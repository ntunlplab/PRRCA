{"year": "2019", "forum": "S1E3Ko09F7", "title": "L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data", "decision": "Accept (Poster)", "meta_review": "The paper presents two new methods for model-agnostic interpretation of instance-wise feature importance. \n\nPros:\nUnlike previous approaches based on the Shapley value, which had an exponential complexity in the number of features, the proposed methods have a linear-complexity when the data have a graph structure, which allows approximation based on graph-structured factorization. The proposed methods present solid technical novelty to study the important challenge of instance-wise, model-agnostic, linear-complexity interpretation of features. \n\nCons:\nAll reviewers wanted to see more extensive experimental results. Authors responded with most experiments requested. One issue raised by R3 was the need for comparing the proposed model-agnostic methods to existing model-specific methods. The proposed linear-complexity algorithm relies on the markov assumption, which some reviewers commented to be a potentially invalid assumption to make, but this does not seem to be a deal breaker since it is a relatively common assumption to make when deriving a polynomial-complexity approximation algorithm. Overall, the rebuttal addressed the reviewers' concerns well enough, leading to increased scores.\n\nVerdict:\nAccept. Solid technical novelty with convincing empirical results.", "reviews": [{"review_id": "S1E3Ko09F7-0", "review_text": "The paper proposes two approximations to the Shapley value used for generating feature scores for interpretability. Both exploit a graph structure over the features by considering only subsets of neighborhoods of features (rather than all subsets). The authors give some approximation guarantees under certain Markovian assumptions on the graph. The paper concludes with experiments on text and images. The paper is generally well written, albeit somewhat lengthy and at times repetitive (I would also swap 2.1 and 2.2 for better early motivation). The problem is important, and exploiting graphical structure is only natural. The authors might benefit from relating to other fields where similar problems are solved (e.g., inference in graphical models). The approximation guarantees are nice, but the assumptions may be too strict. The experimental evaluation seems valid but could be easily strengthened (see comments). Comments: 1. The coefficients in Eq. (6) could be better explained. 2. The theorems seem sound, but the Markovian assumption is rather strict, as it requires that a feature i has an S that \"separates\" over *all* x (in expectation). This goes against the original motivation that different examples are likely to have different explanations. When would this hold in practice? 3. While considering chains for text is valid, the authors should consider exploring other graph structures (e.g., parsing trees). 4. For Eqs. (8) and (9), I could not find the definition of Y. Is this also a random variable representing examples? 5. The authors postulate that sampling-based methods are susceptible to high variance. Showing this empirically would have strengthened their claim. 6. Can the authors empirically quantify Eqs. (8) and (9)? This might shed light as to how realistic the assumptions are. 7. In the experiments, it would have been nice to see how performance and runtime vary with increased neighborhood sizes. This would have quantified the importance of neighborhood size and robustness to hyper-parameters. 8. For the image experiments, since C-Shapley considers connected subsets, it is perhaps not surprising that Fig. 4 shows clusters for this method (and not others). Why did the authors not use superpixels as features? This would have also let them compare to LIME and L-Shapley. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the detailed comments and encouraging title ! We have included three experiments in the updated version to address Point 5 , 6 , and 7 of the reviewer \u2019 s comments , and also omit unnecessary details in the original paper . We will respond the reviewer 's comments concretely below . \u201c The paper is generally well written , somewhat lengthy and at times repetitive ( I would also swap 2.1 and 2.2 for better early motivation ) \u201d Based on the reviewer \u2019 s request , we have shortened the paper by deleting unnecessary repetitions and details in Section 4.3 and the experiment section , and putting some of them to appendix . For example , the description of datasets is deferred to the appendix . As a replacement , we have included a new experiment with human evaluation . On the other hand , we still keep the order of 2.1 and 2.2 . The main reason is that it seems more natural to explain how importance of a feature subset is quantified first ( section 2.1 ) before we motivate the Shapley value , which incorporating interaction based on this quantification ( section 2.2 ) ."}, {"review_id": "S1E3Ko09F7-1", "review_text": "This paper proposes two methods for instance-wise feature importance scoring, which is the task of ranking the importance of each feature in a particular example (in contrast to class-wise or overall feature importance). The approach uses Shapely values, which are a principled way of measuring the contribution of a feature, and have been previously used in feature importance ranking. The difficulty with Shapely values is they are extremely (exponentially) expensive to compute, and the contribution of this paper is to provide two efficient methods of computing approximate Shapely values when there is a known structure (a graph) relating the features to each other. The paper first introduces the L(ocal)-Shapely value, which arises by restricting the Shapely value to a neighbourhood of the feature of interest. The L-Shapely value is still expensive to compute for large neighbourhoods, but can be tractable for small neighbourhoods. The second approximation is the C(onnected)-Shapely value, which further restricts the L-Shapely computation to only consider connected subgraphs of local neighbourhoods. The justification for restricting to connected neighbourhoods is given through a connection to the Myerson value, which is somewhat obscure to me, since I am not familiar with the relevant literature. Nonetheless, it is clear that for the graphs of interest in this paper (chains and lattices) restricting to connected neighbourhoods is a substantial savings. I have understood the scores presented in Figures 2 and 3 as follows: For each feature of each example, rank the features according to importance, using the plugin estimate for P(Y|X_S) where needed. For each \"percent of features masked\" compute log(P(y_true | x_{S\\top features})) - log(P(y_true | x)) using the plugin estimate, and average these values over the dataset. Based on this understanding the results are quite good. The approximate Shapely values do a much better job than their competitors of identifying highly relevant features based on this measure. The qualitative results are also quite compelling, especially on images where C-Shapely tends to select contiguous regions which is intuitively correct behavior. Comparing the different methods in Figure 4, there is quite some variability in the features selected by using different estimators of Shapley values. I wonder is there some way to attack the problem of distinguishing when a feature is ranked highly when its (exact) Shapley value is high versus when it is ranked highly as an artifact of the estimator? ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the detailed and encouraging comments ! Based on the suggestions from the reviewer , we have included an experiment in the updated version that measures the correlation between L-Shapley , C-Shapley and the Shapley value . \u201c Understanding of the evaluation metric \u201d : The evaluation metric we use is the following : log ( P ( y_pred | x ) ) - log ( P ( y_pred | x_ { top features MASKED } ) ) . The reviewer 's understanding is in general correct except that we use the predicted label instead of the true label in the data set , because we hope to find key features for why the model makes its own decision . \u201c I wonder is there some way to attack the problem of distinguishing when a feature is ranked highly when its ( exact ) Shapley value is high versus when it is ranked highly as an artifact of the estimator ? \u201d We have added a new experiment in the updated version to address the problem of how the rank of features correlates with the rank produced by the true Shapley value . We sample a subset of test data from Yahoo ! Answers with 9-12 words , so that the underlying Shapley scores can be accurately computed . We employ two common metrics , Kendall 's Tau and Spearman 's Rho to measure the similarity ( correlation ) between two ranks . We have observed a high rank correlation between our algorithms and the Shapley value . See the figure in the link below , and also Appendix C for more details : https : //drive.google.com/open ? id=1oWsWyA4IkDIbaOjwOOwMAYJzu6kUuQSa"}, {"review_id": "S1E3Ko09F7-2", "review_text": "This paper provides new methods for estimating Shapley values for feature importance that include notions of locality and connectedness. The methods proposed here could be very useful for model explainability purposes, specifically in the model-agnostic case. The results seem promising, and it seems like a reasonable and theoretically sound methodology. In addition to the theoretical properties of the proposed algorithms, they do show a few quantitative and qualitative improvements over other black-box methods. They might strengthen their paper with a more thorough quantitative evaluation. I think the KernelSHAP paper you compare against (Lundberg & Lee 2017) does more quantitative evaluation than what\u2019s presented here, including human judgement comparisons. Is there a way to compare against KernelSHAP using the same evaluation methods from the original paper? Also, you mention throughout the paper that the L-shapley and C-shapley methods can easily complement other sampling/regression-based methods. It's a little ambiguous to me whether this was actually something you tried in your experiments or not. Can you please clarify?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the detailed suggestions and encouraging comments ! We have included an experiment with human evaluation in the updated version . Below we respond to Reviewer 1 \u2019 s questions in details . \u201c Is there a way to compare against KernelSHAP using the same ( human ) evaluation methods from the original paper ? \u201d We agree with the reviewer that human evaluation is important in this area , and we have added a new experiment with human evaluation in the updated version . In KernelSHAP paper , the authors designed experiments to argue for the use of Shapley value instead of LIME , which shows Shapley value is more consistent with human intuition on a data set with only a few number of features . Both KernelSHAP and our algorithms are ways of approximating Shapley value when there is a large number of features , under which case the exact same experiment is difficult to replicate . We have designed two experiments by ourselves involving human evaluation for our methods and KernelSHAP on IMDB in the updated version . We assume that the key words contain an attitude toward a movie and can be used to infer the sentiment of a review . In the first experiment , we ask humans to infer the sentiment of a review within a range of -2 to 2 , given the key words selected by different model interpretation approaches . Second , we also ask humans to infer the sentiment of a review with top words being masked , where words are masked until the predicted class gets a probability score of 0.1 . In both experiments , we evaluate the consistency with truth , the agreement between humans on a single review by standard deviation , and the confidence of their decision via the absolute value of the score . We observe L-Shapley and C-Shapley take the lead respectively in two experiments . See the table and an example interface in the links below , and also Section 5.3 for more details : https : //drive.google.com/open ? id=1aHZPP0ZAdyODgTEFLRrQAKyS4uJ8h-XS https : //drive.google.com/file/d/1_HOR28DGlKqEQVplGahv47o2xPe5lT5e/view ? usp=sharing \u201c It 's a little ambiguous to me whether you tried to complement other sampling/regression-based methods in your experiments or not . Can you please clarify ? \u201d In the experiments , we did n't combine our approach with sampling based methods as the number of model evaluations is already small enough in the setting ( linear in the number of features ) ."}], "0": {"review_id": "S1E3Ko09F7-0", "review_text": "The paper proposes two approximations to the Shapley value used for generating feature scores for interpretability. Both exploit a graph structure over the features by considering only subsets of neighborhoods of features (rather than all subsets). The authors give some approximation guarantees under certain Markovian assumptions on the graph. The paper concludes with experiments on text and images. The paper is generally well written, albeit somewhat lengthy and at times repetitive (I would also swap 2.1 and 2.2 for better early motivation). The problem is important, and exploiting graphical structure is only natural. The authors might benefit from relating to other fields where similar problems are solved (e.g., inference in graphical models). The approximation guarantees are nice, but the assumptions may be too strict. The experimental evaluation seems valid but could be easily strengthened (see comments). Comments: 1. The coefficients in Eq. (6) could be better explained. 2. The theorems seem sound, but the Markovian assumption is rather strict, as it requires that a feature i has an S that \"separates\" over *all* x (in expectation). This goes against the original motivation that different examples are likely to have different explanations. When would this hold in practice? 3. While considering chains for text is valid, the authors should consider exploring other graph structures (e.g., parsing trees). 4. For Eqs. (8) and (9), I could not find the definition of Y. Is this also a random variable representing examples? 5. The authors postulate that sampling-based methods are susceptible to high variance. Showing this empirically would have strengthened their claim. 6. Can the authors empirically quantify Eqs. (8) and (9)? This might shed light as to how realistic the assumptions are. 7. In the experiments, it would have been nice to see how performance and runtime vary with increased neighborhood sizes. This would have quantified the importance of neighborhood size and robustness to hyper-parameters. 8. For the image experiments, since C-Shapley considers connected subsets, it is perhaps not surprising that Fig. 4 shows clusters for this method (and not others). Why did the authors not use superpixels as features? This would have also let them compare to LIME and L-Shapley. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the detailed comments and encouraging title ! We have included three experiments in the updated version to address Point 5 , 6 , and 7 of the reviewer \u2019 s comments , and also omit unnecessary details in the original paper . We will respond the reviewer 's comments concretely below . \u201c The paper is generally well written , somewhat lengthy and at times repetitive ( I would also swap 2.1 and 2.2 for better early motivation ) \u201d Based on the reviewer \u2019 s request , we have shortened the paper by deleting unnecessary repetitions and details in Section 4.3 and the experiment section , and putting some of them to appendix . For example , the description of datasets is deferred to the appendix . As a replacement , we have included a new experiment with human evaluation . On the other hand , we still keep the order of 2.1 and 2.2 . The main reason is that it seems more natural to explain how importance of a feature subset is quantified first ( section 2.1 ) before we motivate the Shapley value , which incorporating interaction based on this quantification ( section 2.2 ) ."}, "1": {"review_id": "S1E3Ko09F7-1", "review_text": "This paper proposes two methods for instance-wise feature importance scoring, which is the task of ranking the importance of each feature in a particular example (in contrast to class-wise or overall feature importance). The approach uses Shapely values, which are a principled way of measuring the contribution of a feature, and have been previously used in feature importance ranking. The difficulty with Shapely values is they are extremely (exponentially) expensive to compute, and the contribution of this paper is to provide two efficient methods of computing approximate Shapely values when there is a known structure (a graph) relating the features to each other. The paper first introduces the L(ocal)-Shapely value, which arises by restricting the Shapely value to a neighbourhood of the feature of interest. The L-Shapely value is still expensive to compute for large neighbourhoods, but can be tractable for small neighbourhoods. The second approximation is the C(onnected)-Shapely value, which further restricts the L-Shapely computation to only consider connected subgraphs of local neighbourhoods. The justification for restricting to connected neighbourhoods is given through a connection to the Myerson value, which is somewhat obscure to me, since I am not familiar with the relevant literature. Nonetheless, it is clear that for the graphs of interest in this paper (chains and lattices) restricting to connected neighbourhoods is a substantial savings. I have understood the scores presented in Figures 2 and 3 as follows: For each feature of each example, rank the features according to importance, using the plugin estimate for P(Y|X_S) where needed. For each \"percent of features masked\" compute log(P(y_true | x_{S\\top features})) - log(P(y_true | x)) using the plugin estimate, and average these values over the dataset. Based on this understanding the results are quite good. The approximate Shapely values do a much better job than their competitors of identifying highly relevant features based on this measure. The qualitative results are also quite compelling, especially on images where C-Shapely tends to select contiguous regions which is intuitively correct behavior. Comparing the different methods in Figure 4, there is quite some variability in the features selected by using different estimators of Shapley values. I wonder is there some way to attack the problem of distinguishing when a feature is ranked highly when its (exact) Shapley value is high versus when it is ranked highly as an artifact of the estimator? ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the detailed and encouraging comments ! Based on the suggestions from the reviewer , we have included an experiment in the updated version that measures the correlation between L-Shapley , C-Shapley and the Shapley value . \u201c Understanding of the evaluation metric \u201d : The evaluation metric we use is the following : log ( P ( y_pred | x ) ) - log ( P ( y_pred | x_ { top features MASKED } ) ) . The reviewer 's understanding is in general correct except that we use the predicted label instead of the true label in the data set , because we hope to find key features for why the model makes its own decision . \u201c I wonder is there some way to attack the problem of distinguishing when a feature is ranked highly when its ( exact ) Shapley value is high versus when it is ranked highly as an artifact of the estimator ? \u201d We have added a new experiment in the updated version to address the problem of how the rank of features correlates with the rank produced by the true Shapley value . We sample a subset of test data from Yahoo ! Answers with 9-12 words , so that the underlying Shapley scores can be accurately computed . We employ two common metrics , Kendall 's Tau and Spearman 's Rho to measure the similarity ( correlation ) between two ranks . We have observed a high rank correlation between our algorithms and the Shapley value . See the figure in the link below , and also Appendix C for more details : https : //drive.google.com/open ? id=1oWsWyA4IkDIbaOjwOOwMAYJzu6kUuQSa"}, "2": {"review_id": "S1E3Ko09F7-2", "review_text": "This paper provides new methods for estimating Shapley values for feature importance that include notions of locality and connectedness. The methods proposed here could be very useful for model explainability purposes, specifically in the model-agnostic case. The results seem promising, and it seems like a reasonable and theoretically sound methodology. In addition to the theoretical properties of the proposed algorithms, they do show a few quantitative and qualitative improvements over other black-box methods. They might strengthen their paper with a more thorough quantitative evaluation. I think the KernelSHAP paper you compare against (Lundberg & Lee 2017) does more quantitative evaluation than what\u2019s presented here, including human judgement comparisons. Is there a way to compare against KernelSHAP using the same evaluation methods from the original paper? Also, you mention throughout the paper that the L-shapley and C-shapley methods can easily complement other sampling/regression-based methods. It's a little ambiguous to me whether this was actually something you tried in your experiments or not. Can you please clarify?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the detailed suggestions and encouraging comments ! We have included an experiment with human evaluation in the updated version . Below we respond to Reviewer 1 \u2019 s questions in details . \u201c Is there a way to compare against KernelSHAP using the same ( human ) evaluation methods from the original paper ? \u201d We agree with the reviewer that human evaluation is important in this area , and we have added a new experiment with human evaluation in the updated version . In KernelSHAP paper , the authors designed experiments to argue for the use of Shapley value instead of LIME , which shows Shapley value is more consistent with human intuition on a data set with only a few number of features . Both KernelSHAP and our algorithms are ways of approximating Shapley value when there is a large number of features , under which case the exact same experiment is difficult to replicate . We have designed two experiments by ourselves involving human evaluation for our methods and KernelSHAP on IMDB in the updated version . We assume that the key words contain an attitude toward a movie and can be used to infer the sentiment of a review . In the first experiment , we ask humans to infer the sentiment of a review within a range of -2 to 2 , given the key words selected by different model interpretation approaches . Second , we also ask humans to infer the sentiment of a review with top words being masked , where words are masked until the predicted class gets a probability score of 0.1 . In both experiments , we evaluate the consistency with truth , the agreement between humans on a single review by standard deviation , and the confidence of their decision via the absolute value of the score . We observe L-Shapley and C-Shapley take the lead respectively in two experiments . See the table and an example interface in the links below , and also Section 5.3 for more details : https : //drive.google.com/open ? id=1aHZPP0ZAdyODgTEFLRrQAKyS4uJ8h-XS https : //drive.google.com/file/d/1_HOR28DGlKqEQVplGahv47o2xPe5lT5e/view ? usp=sharing \u201c It 's a little ambiguous to me whether you tried to complement other sampling/regression-based methods in your experiments or not . Can you please clarify ? \u201d In the experiments , we did n't combine our approach with sampling based methods as the number of model evaluations is already small enough in the setting ( linear in the number of features ) ."}}