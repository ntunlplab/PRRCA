{"year": "2019", "forum": "SJ4Z72Rctm", "title": "Composing Entropic Policies using Divergence Correction", "decision": "Reject", "meta_review": "Multiple reviewers had concerns about the clarity of the presentation and the significance of the results.\n", "reviews": [{"review_id": "SJ4Z72Rctm-0", "review_text": "The authors introduce Divergence Correction (DC) for the problem of transfer learning by composing policies. There approach builds on GPI with a maximum entropy objective. They also prove that DC solves for the max-entropy optimal interpolation between two policies and derive a practical approximation for this algorithm. They provide experimental results in a gridworld problem and study their approximate algorithm in two continuous control problems. While this paper has some interesting ideas (combining GPI with a Max-Entropy objective and DC), these ideas are not properly motivated. The main problem seems to be clarity. One big problem is that the paper never defines the notion of a notion of optimality (or near-optimality). Also, considering that the DC algorithm is one of the main contributions of the paper it is barely motivated. Theorem 3.2 is presented with almost no explanation about how DC was derived. Why do the authors believe that DC is a good idea on a conceptual level? It's very interesting that the paper presents cases where previous approaches (Optimistic and GPI) don't perform well. But the authors don't explain why they believe DC should perform well in these cases. The authors make the unjustified claim in the abstract that their approach has \"near-optimal performance and requires less information\". I say this is unjustified because they only try this approach on three benchmarks. In addition, there should be situations where DC also performs poorly since there are known hardness results for solving MDPs. Admittedly, those results may not apply if the authors are making assumptions that are not being clearly discussed in the paper. Minor Comments: 1. In the abstract, \"requiring less information\" is very imprecise. Are you referring to sample complexity? 2. In the introduction, \"can consistently achieve good performance\" is imprecise. What is the notion of near-optimality? What does consistent mean? Having experimental results on 3 tasks doesn't seem to be enough to me to justify this claim. 3. In the introduction (and rest of the paper), please don't call Haarnoja et al.'s approach optimistic. Optimism already has another widely used meaning in RL literature. Maybe call it \"Uncorrected\". 4. In section 2.2, the authors introduce \\pi_1, \\pi_2, ... , \\pi_n but never actually use that notation. This section does not clearly explain how GPI works. 5. In Theorem 3.1, the authors should introduce Q^1, Q^2, ... , Q^n and define the policies in terms of the action-value functions. Also, the statement of this theorem is not self contained, what is the reward function of the MDP? The proof below should be called a proof sketch. 6. The paper mentions that extending to multiple tasks is possible. Is it trivial? What is the basic idea? It seems straightforward but it might be helpful to explicitly state the idea. 7. In Theorem 3.2, how was C derived? Please add some commentary explaining the conceptual idea. 8. In Table 1, what is f(s, a|b)? I don't see where this was defined? 9. CondQ is usually referred to as UVFA in the literature. 10. Section 3 really needs a conclusion statement. 11. Section 4 is very unclear and hard to follow. 12. In figure 1f, what is LTD? It's never defined. I'm guessing it's DC. 13. All of the figures are too small and some are not clear in black and white.", "rating": "4: Ok but not good enough - rejection", "reply_text": "> their approximate algorithm in two continuous control problems . To clarify : our submission included results for 4 continuous control tasks : 2-D point mass , 5 DOF planar manipulator , 3 DOF jumping ball and 8 DOF ant . We have now added an additional ant task in the appendix ( see response later ) . > While this paper has some interesting ideas \u2026 these ideas are not properly motivated . > The main problem seems to be clarity . One big problem is that the paper never > defines the notion of a notion of optimality ( or near-optimality ) . We are interested in 0-shot transfer where we combine existing policies trained on other tasks to provide a solution for a new task . Our notion of optimality is hence with respect to the performance of the optimal policy for the new task . In that sense a policy composition for the transfer task is optimal if it achieved the same performance as the optimal policy for the transfer task . When we say near-optimal , we simply mean the return is nearly that of an optimal policy . We have tried to clarify this notion in our formalism of the transfer problem ( section 2.1 ) . For the tabular tasks we can solve exactly ( within numerical precision ) for the optimal policy . For the control tasks , we do not have access to the true optimal policy , so we compare the returns of different compositional approaches with one another . We do , however , know roughly the optimal trajectory ( i.e.in the \u201c tricky \u201d tasks this corresponds to heading towards the upper right square ) . We have edited the text to try and clarify these claims . > \u2026 considering that the DC algorithm is one of the main contributions of the paper > it is barely motivated . > Theorem 3.2 is presented with almost no explanation about how DC was derived ... > why they believe DC should perform well in these cases . We have added a motivating paragraph before the introduction of DC and have edited this section to provide an intuitive notation of DC before introducing it formally . In short , we want a method that is , in principle ( if all components are known exactly ) optimal . Theorem 3.2 is our basis for believing that DC should perform well . It shows that , if all of the terms of Q^ { DC } are known exactly , the resulting policy is the optimal policy on the transfer task . The intuition is that the correction term to ( compositional optimism ) CO is ( roughly ) the expected divergences between policies along their trajectories . If the two policies have low-divergences , the CO assumption is approximately correct . If they have high-divergences , this means the policies don \u2019 t agree about what actions to take , and thus can not both achieve their expected returns simultaneously . > The authors make the unjustified claim in the abstract that their approach has `` near-optimal > performance and requires less information '' ... Firstly , we agree the claim regarding \u2018 less information \u2019 was ambiguous . The information was not data efficiency ( all methods here are trained using the same data ) . We do not refer here to sample complexity ( all methods are trained on the same amount of data , and tested on 0-shot transfer ) . What is meant by information here is that , under the formalism for transfer used here , GPI and CondQ/UVFA \u2019 s require that , while learning policies for each task i , the rewards on all tasks be observed ( i.e.\\phi is observable ) . Compositional Optimism and DC do not require access to this information , hence the claim of less information . We have now modified the abstract to explicitly state `` despite not requiring simultaneous observation of all task rewards. \u2019 \u2019 As discussed in our response above , we have edited the text to clarify the notion of optimality , which is the standard RL definition ( a policy is optimal if it has an expected return the same as the optimal policy for task ) , but on the compositional transfer task . The DC theorem , like many RL results , makes the claim of optimality when the components are known exactly . That is , we state in the theorem conditions that $ Q^i $ , $ Q^j $ are the action-value functions of the optimal policies of the base tasks and the correction term C_b^\\infty is the solution to the given fixed point . Thus , in some sense , the known hardness results are inside this assumption that we known optimal solutions to the base tasks and the need to know C everywhere . Again , we want to highlight that prior methods do not recover the optimal transfer policy , even when assuming all of their components are known exactly . In the tabular case , where we can ( within numerical precision ) compute all the components , we do indeed see DC recovers the optimal policy in all tasks we considered . Practically , our experimental results show that DC does generate better transfer policies than GPI and CO . In these experiments , as with almost all DeepRL , of course we do not have access to the exact action-value \u2019 s , but that approximating the DC correction term can result in qualitatively better transfer policies ."}, {"review_id": "SJ4Z72Rctm-1", "review_text": " -- Contribution, Originality, and Quality -- This paper has presented two approaches for transfer learning in the reinforcement learning (RL) setting: max-ent GPI (Section 3.1) and DC (Section 3.2). The authors have also established some theoretical results for these two approaches (Theorem 3.1 and 3.2), and also demonstrated some experiment results (Section 5). These two developed approaches are interesting. However, based on existing literature (Barreto et al. 2017; 2018, Haarnoja et al. 2018a), neither of them seems to contain *significant* novelty. The derivations of the theoretical results (Theorem 3.1 and 3.2) are also relatively straightforward. The experiment results in Section 5 are interesting. -- Clarity -- I have two major complaints about the clarity of this paper. 1) Section 4 of the paper is not well written and is hard to follow. 2) Some notations in the paper are not well defined. For instance 2a) In page 3, the notation \\delta has not been defined. 2b) In page 6, both notation V_{\\theta'_V} and V'_{\\theta_V} have been used. I do not think either of them has been defined. -- Pros and Cons -- Pros: 1) The proposed approaches and the experiment results are interesting. Cons: 1) Neither the algorithm design nor the analysis has sufficient novelty, compared to the typical standard of a top-tier conference. 2) The paper is not very well written, especially Section 4. 3) For Theorem 3.2, why not prove a variant of it for the general multi-task case? 4) It would be better to provide the pseudocode of the proposed algorithm in the main body of the paper.", "rating": "5: Marginally below acceptance threshold", "reply_text": "> ... neither of them seems to contain * significant * novelty . The derivations of the theoretical results > ( Theorem 3.1 and 3.2 ) are also relatively straightforward . The experiment results in Section 5 are interesting . While we acknowledge that our results build on prior work we feel that the reviewer \u2019 s assessment undervalues our contributions . To clarify : Successor Representations/Features have only ever been used in small , discrete action spaces . To the best of our knowledge , we are the first to provide a method for using successor features in continuous action spaces . The extension of the GPI theorem to the max-ent RL objective is a non-trivial extension . This brings an important and general idea on value iteration to a new , and important RL framework . It shows that there is a simple , principled way to combine max-ent policies in a way that ensures the result composition at worst retains the performance of the best policy . The derivation of theorem 3.2 is , as we state in the paper , similar to the approach used in Haarnoja et al. , 2018 . However , there is an important conceptual leap to show that this term can be practically learned and used to improve performance . Indeed the final lines of Haarnoja are \u201c An interesting avenue for future work would be to further study the implications of this bound on compositionality . For example , can we derive a correction that can be applied to the composed Q-function to reduce bias ? Answering such questions would make it more practical to construct new robotic skills out of previously trained building blocks , making it easier to endow robots with large repertoires of behaviors learned via reinforcement learning. \u201d We have taken a first step in this direction and demonstrated that this correction term can be learned in a practical manner . Additionally , we introduced a very simple heuristic , DC-Cheap . In many cases , this heuristic suffices to get similar performance to DC . Finally , we also introduce an algorithm for practically using these methods , including , to our knowledge the first example of online zero-shot transfer ( in the context defined here ) in continuous action spaces . The only other work we are aware of that does this ( Haarnoja et al. , 2018 ) , requires an offline retraining of the sampler . > 1 ) Section 4 of the paper is not well written and is hard to follow . We have re-written section 4 . Hopefully it is clearer now . We have also edited the rest of the paper to improve readability . We welcome additional feedback . > 2a ) ... notation \\delta has not been defined . This was the Dirac delta to communicate the GPI policy is deterministic ( in this prior work ) . We have modified this to simplify the notation by explicitly stating the policy is deterministic . > 2b ) ... notation V_ { \\theta'_V } and V ' _ { \\theta_V } have been used . I do not think > either of them has been defined . V_ { \\theta \u2019 _V } is the target network for V_ { \\theta_V } . This was defined in the paragraph below eq 14 . We have now re-phrased this to make it more clear this is the definition . V \u2019 _ { \\theta_V } was a notational mistake , it should have been V_ { \\theta \u2019 _V } . Thank you for pointing that out . > Pros : > 1 ) The proposed approaches and the experiment results are interesting . > Cons : > 1 ) Neither the algorithm design nor the analysis has sufficient novelty , compared to the typical standard > of a top-tier conference . We addressed this point in detail above , where we enumerated what we believe are the contributions of this work . At the risk of belaboring the point : many papers are simply a more scalable algorithm , or novel theoretical ideas . Here we introduced 2 new theoretical ideas , and a new practical algorithm , is to the best of our knowledge the first demonstration of online zero-shot transfer for task composition in continuous action spaces . > 2 ) The paper is not very well written , especially Section 4 . We have edited the paper throughout to provide additional clarify and substantially revised section 4 and are happy to make further revisions . Our paper builds upon and extends several lines of work and it has been a challenge to introduce and discuss all relevant concepts in the limited space available . > 3 ) For Theorem 3.2 , why not prove a variant of it for the general multi-task case ? Thank you for the suggestion . We have added a proof in the appendix for the n-policy case of Theorem 3.2 . Due to space constraints we have included this in the appendix . he derivation is very similar to the two policy case which we discuss in the main text .. > 4 ) It would be better to provide the pseudocode of the proposed algorithm in the main body of > the paper . As part of our revision of section 4 , we have moved this algorithm into the main text ( and modified it to include all the losses.We thank the reviewer for their time and feedback . We hope we have been able to address many of your concerns and you may reconsider your rating in light of our changes . We welcome additional feedback ."}, {"review_id": "SJ4Z72Rctm-2", "review_text": "This paper proposes using Divergence Correction to compose max ent policies. Based on successor features, this method corrects the optimistic bias of Haarnoja 2018. The motivation for composing policies is sound. This paper addresses the problem statement where policies must accomplish different linear combinations of different reward functions. This method does not require observation the reward weights. As shown in the experiments, this method outperforms or equally performs past work in both tabular and continuous environments. The paper is well written and discusses prior work in an informative manner. The tabular examples provide good visualizations of why the methods perform differently. Minor: - Figure 1.e: Why does the Optimistic transfer have high regret when the caption says that \"on the LU task, optimistic transfers well\" - Figure 1.i states \"Neither GPI nor the optimistic policies (j shows GPI, by the Optimistic policy is similar)\" but Figure1.j is labeled DC T, is this a typo? - Figure 2: Many typos: \"(b) Finger position at the en (of the trajectoriesstard ting from randomly sampled start states)\" ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their time and feedback . We address the minor corrections below . > - Figure 1.e : Why does the Optimistic transfer have high regret when the caption says that `` on the LU task , optimistic transfers well '' Optimistic transfers much better than GPI on this task . The regret scale is logarithmic , so while Optimistic ( red ) does not recover the optimal policy , it has a regret approximately 10^4 lower than GPI ( blue ) . We \u2019 ve edited the caption to make this clearer . Note in response to another reviewer we have renamed Optimistic to Compositional Optimism ( CO ) to avoid confusion . > - Figure 1.i states `` Neither GPI nor the optimistic policies ( j shows GPI , by the Optimistic policy is similar ) '' but Figure1.j is labeled DC T , is this a typo ? Apologies , that is indeed a typo . We \u2019 ve now reworded this caption . > - Figure 2 : Many typos : `` ( b ) Finger position at the en ( of the trajectoriesstard ting from randomly sampled start states ) '' Thank you for pointing this out , this has been fixed . In general , we made a number of minor edits throughout , and particularly revised section 4 to improve the readability of the paper ."}], "0": {"review_id": "SJ4Z72Rctm-0", "review_text": "The authors introduce Divergence Correction (DC) for the problem of transfer learning by composing policies. There approach builds on GPI with a maximum entropy objective. They also prove that DC solves for the max-entropy optimal interpolation between two policies and derive a practical approximation for this algorithm. They provide experimental results in a gridworld problem and study their approximate algorithm in two continuous control problems. While this paper has some interesting ideas (combining GPI with a Max-Entropy objective and DC), these ideas are not properly motivated. The main problem seems to be clarity. One big problem is that the paper never defines the notion of a notion of optimality (or near-optimality). Also, considering that the DC algorithm is one of the main contributions of the paper it is barely motivated. Theorem 3.2 is presented with almost no explanation about how DC was derived. Why do the authors believe that DC is a good idea on a conceptual level? It's very interesting that the paper presents cases where previous approaches (Optimistic and GPI) don't perform well. But the authors don't explain why they believe DC should perform well in these cases. The authors make the unjustified claim in the abstract that their approach has \"near-optimal performance and requires less information\". I say this is unjustified because they only try this approach on three benchmarks. In addition, there should be situations where DC also performs poorly since there are known hardness results for solving MDPs. Admittedly, those results may not apply if the authors are making assumptions that are not being clearly discussed in the paper. Minor Comments: 1. In the abstract, \"requiring less information\" is very imprecise. Are you referring to sample complexity? 2. In the introduction, \"can consistently achieve good performance\" is imprecise. What is the notion of near-optimality? What does consistent mean? Having experimental results on 3 tasks doesn't seem to be enough to me to justify this claim. 3. In the introduction (and rest of the paper), please don't call Haarnoja et al.'s approach optimistic. Optimism already has another widely used meaning in RL literature. Maybe call it \"Uncorrected\". 4. In section 2.2, the authors introduce \\pi_1, \\pi_2, ... , \\pi_n but never actually use that notation. This section does not clearly explain how GPI works. 5. In Theorem 3.1, the authors should introduce Q^1, Q^2, ... , Q^n and define the policies in terms of the action-value functions. Also, the statement of this theorem is not self contained, what is the reward function of the MDP? The proof below should be called a proof sketch. 6. The paper mentions that extending to multiple tasks is possible. Is it trivial? What is the basic idea? It seems straightforward but it might be helpful to explicitly state the idea. 7. In Theorem 3.2, how was C derived? Please add some commentary explaining the conceptual idea. 8. In Table 1, what is f(s, a|b)? I don't see where this was defined? 9. CondQ is usually referred to as UVFA in the literature. 10. Section 3 really needs a conclusion statement. 11. Section 4 is very unclear and hard to follow. 12. In figure 1f, what is LTD? It's never defined. I'm guessing it's DC. 13. All of the figures are too small and some are not clear in black and white.", "rating": "4: Ok but not good enough - rejection", "reply_text": "> their approximate algorithm in two continuous control problems . To clarify : our submission included results for 4 continuous control tasks : 2-D point mass , 5 DOF planar manipulator , 3 DOF jumping ball and 8 DOF ant . We have now added an additional ant task in the appendix ( see response later ) . > While this paper has some interesting ideas \u2026 these ideas are not properly motivated . > The main problem seems to be clarity . One big problem is that the paper never > defines the notion of a notion of optimality ( or near-optimality ) . We are interested in 0-shot transfer where we combine existing policies trained on other tasks to provide a solution for a new task . Our notion of optimality is hence with respect to the performance of the optimal policy for the new task . In that sense a policy composition for the transfer task is optimal if it achieved the same performance as the optimal policy for the transfer task . When we say near-optimal , we simply mean the return is nearly that of an optimal policy . We have tried to clarify this notion in our formalism of the transfer problem ( section 2.1 ) . For the tabular tasks we can solve exactly ( within numerical precision ) for the optimal policy . For the control tasks , we do not have access to the true optimal policy , so we compare the returns of different compositional approaches with one another . We do , however , know roughly the optimal trajectory ( i.e.in the \u201c tricky \u201d tasks this corresponds to heading towards the upper right square ) . We have edited the text to try and clarify these claims . > \u2026 considering that the DC algorithm is one of the main contributions of the paper > it is barely motivated . > Theorem 3.2 is presented with almost no explanation about how DC was derived ... > why they believe DC should perform well in these cases . We have added a motivating paragraph before the introduction of DC and have edited this section to provide an intuitive notation of DC before introducing it formally . In short , we want a method that is , in principle ( if all components are known exactly ) optimal . Theorem 3.2 is our basis for believing that DC should perform well . It shows that , if all of the terms of Q^ { DC } are known exactly , the resulting policy is the optimal policy on the transfer task . The intuition is that the correction term to ( compositional optimism ) CO is ( roughly ) the expected divergences between policies along their trajectories . If the two policies have low-divergences , the CO assumption is approximately correct . If they have high-divergences , this means the policies don \u2019 t agree about what actions to take , and thus can not both achieve their expected returns simultaneously . > The authors make the unjustified claim in the abstract that their approach has `` near-optimal > performance and requires less information '' ... Firstly , we agree the claim regarding \u2018 less information \u2019 was ambiguous . The information was not data efficiency ( all methods here are trained using the same data ) . We do not refer here to sample complexity ( all methods are trained on the same amount of data , and tested on 0-shot transfer ) . What is meant by information here is that , under the formalism for transfer used here , GPI and CondQ/UVFA \u2019 s require that , while learning policies for each task i , the rewards on all tasks be observed ( i.e.\\phi is observable ) . Compositional Optimism and DC do not require access to this information , hence the claim of less information . We have now modified the abstract to explicitly state `` despite not requiring simultaneous observation of all task rewards. \u2019 \u2019 As discussed in our response above , we have edited the text to clarify the notion of optimality , which is the standard RL definition ( a policy is optimal if it has an expected return the same as the optimal policy for task ) , but on the compositional transfer task . The DC theorem , like many RL results , makes the claim of optimality when the components are known exactly . That is , we state in the theorem conditions that $ Q^i $ , $ Q^j $ are the action-value functions of the optimal policies of the base tasks and the correction term C_b^\\infty is the solution to the given fixed point . Thus , in some sense , the known hardness results are inside this assumption that we known optimal solutions to the base tasks and the need to know C everywhere . Again , we want to highlight that prior methods do not recover the optimal transfer policy , even when assuming all of their components are known exactly . In the tabular case , where we can ( within numerical precision ) compute all the components , we do indeed see DC recovers the optimal policy in all tasks we considered . Practically , our experimental results show that DC does generate better transfer policies than GPI and CO . In these experiments , as with almost all DeepRL , of course we do not have access to the exact action-value \u2019 s , but that approximating the DC correction term can result in qualitatively better transfer policies ."}, "1": {"review_id": "SJ4Z72Rctm-1", "review_text": " -- Contribution, Originality, and Quality -- This paper has presented two approaches for transfer learning in the reinforcement learning (RL) setting: max-ent GPI (Section 3.1) and DC (Section 3.2). The authors have also established some theoretical results for these two approaches (Theorem 3.1 and 3.2), and also demonstrated some experiment results (Section 5). These two developed approaches are interesting. However, based on existing literature (Barreto et al. 2017; 2018, Haarnoja et al. 2018a), neither of them seems to contain *significant* novelty. The derivations of the theoretical results (Theorem 3.1 and 3.2) are also relatively straightforward. The experiment results in Section 5 are interesting. -- Clarity -- I have two major complaints about the clarity of this paper. 1) Section 4 of the paper is not well written and is hard to follow. 2) Some notations in the paper are not well defined. For instance 2a) In page 3, the notation \\delta has not been defined. 2b) In page 6, both notation V_{\\theta'_V} and V'_{\\theta_V} have been used. I do not think either of them has been defined. -- Pros and Cons -- Pros: 1) The proposed approaches and the experiment results are interesting. Cons: 1) Neither the algorithm design nor the analysis has sufficient novelty, compared to the typical standard of a top-tier conference. 2) The paper is not very well written, especially Section 4. 3) For Theorem 3.2, why not prove a variant of it for the general multi-task case? 4) It would be better to provide the pseudocode of the proposed algorithm in the main body of the paper.", "rating": "5: Marginally below acceptance threshold", "reply_text": "> ... neither of them seems to contain * significant * novelty . The derivations of the theoretical results > ( Theorem 3.1 and 3.2 ) are also relatively straightforward . The experiment results in Section 5 are interesting . While we acknowledge that our results build on prior work we feel that the reviewer \u2019 s assessment undervalues our contributions . To clarify : Successor Representations/Features have only ever been used in small , discrete action spaces . To the best of our knowledge , we are the first to provide a method for using successor features in continuous action spaces . The extension of the GPI theorem to the max-ent RL objective is a non-trivial extension . This brings an important and general idea on value iteration to a new , and important RL framework . It shows that there is a simple , principled way to combine max-ent policies in a way that ensures the result composition at worst retains the performance of the best policy . The derivation of theorem 3.2 is , as we state in the paper , similar to the approach used in Haarnoja et al. , 2018 . However , there is an important conceptual leap to show that this term can be practically learned and used to improve performance . Indeed the final lines of Haarnoja are \u201c An interesting avenue for future work would be to further study the implications of this bound on compositionality . For example , can we derive a correction that can be applied to the composed Q-function to reduce bias ? Answering such questions would make it more practical to construct new robotic skills out of previously trained building blocks , making it easier to endow robots with large repertoires of behaviors learned via reinforcement learning. \u201d We have taken a first step in this direction and demonstrated that this correction term can be learned in a practical manner . Additionally , we introduced a very simple heuristic , DC-Cheap . In many cases , this heuristic suffices to get similar performance to DC . Finally , we also introduce an algorithm for practically using these methods , including , to our knowledge the first example of online zero-shot transfer ( in the context defined here ) in continuous action spaces . The only other work we are aware of that does this ( Haarnoja et al. , 2018 ) , requires an offline retraining of the sampler . > 1 ) Section 4 of the paper is not well written and is hard to follow . We have re-written section 4 . Hopefully it is clearer now . We have also edited the rest of the paper to improve readability . We welcome additional feedback . > 2a ) ... notation \\delta has not been defined . This was the Dirac delta to communicate the GPI policy is deterministic ( in this prior work ) . We have modified this to simplify the notation by explicitly stating the policy is deterministic . > 2b ) ... notation V_ { \\theta'_V } and V ' _ { \\theta_V } have been used . I do not think > either of them has been defined . V_ { \\theta \u2019 _V } is the target network for V_ { \\theta_V } . This was defined in the paragraph below eq 14 . We have now re-phrased this to make it more clear this is the definition . V \u2019 _ { \\theta_V } was a notational mistake , it should have been V_ { \\theta \u2019 _V } . Thank you for pointing that out . > Pros : > 1 ) The proposed approaches and the experiment results are interesting . > Cons : > 1 ) Neither the algorithm design nor the analysis has sufficient novelty , compared to the typical standard > of a top-tier conference . We addressed this point in detail above , where we enumerated what we believe are the contributions of this work . At the risk of belaboring the point : many papers are simply a more scalable algorithm , or novel theoretical ideas . Here we introduced 2 new theoretical ideas , and a new practical algorithm , is to the best of our knowledge the first demonstration of online zero-shot transfer for task composition in continuous action spaces . > 2 ) The paper is not very well written , especially Section 4 . We have edited the paper throughout to provide additional clarify and substantially revised section 4 and are happy to make further revisions . Our paper builds upon and extends several lines of work and it has been a challenge to introduce and discuss all relevant concepts in the limited space available . > 3 ) For Theorem 3.2 , why not prove a variant of it for the general multi-task case ? Thank you for the suggestion . We have added a proof in the appendix for the n-policy case of Theorem 3.2 . Due to space constraints we have included this in the appendix . he derivation is very similar to the two policy case which we discuss in the main text .. > 4 ) It would be better to provide the pseudocode of the proposed algorithm in the main body of > the paper . As part of our revision of section 4 , we have moved this algorithm into the main text ( and modified it to include all the losses.We thank the reviewer for their time and feedback . We hope we have been able to address many of your concerns and you may reconsider your rating in light of our changes . We welcome additional feedback ."}, "2": {"review_id": "SJ4Z72Rctm-2", "review_text": "This paper proposes using Divergence Correction to compose max ent policies. Based on successor features, this method corrects the optimistic bias of Haarnoja 2018. The motivation for composing policies is sound. This paper addresses the problem statement where policies must accomplish different linear combinations of different reward functions. This method does not require observation the reward weights. As shown in the experiments, this method outperforms or equally performs past work in both tabular and continuous environments. The paper is well written and discusses prior work in an informative manner. The tabular examples provide good visualizations of why the methods perform differently. Minor: - Figure 1.e: Why does the Optimistic transfer have high regret when the caption says that \"on the LU task, optimistic transfers well\" - Figure 1.i states \"Neither GPI nor the optimistic policies (j shows GPI, by the Optimistic policy is similar)\" but Figure1.j is labeled DC T, is this a typo? - Figure 2: Many typos: \"(b) Finger position at the en (of the trajectoriesstard ting from randomly sampled start states)\" ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their time and feedback . We address the minor corrections below . > - Figure 1.e : Why does the Optimistic transfer have high regret when the caption says that `` on the LU task , optimistic transfers well '' Optimistic transfers much better than GPI on this task . The regret scale is logarithmic , so while Optimistic ( red ) does not recover the optimal policy , it has a regret approximately 10^4 lower than GPI ( blue ) . We \u2019 ve edited the caption to make this clearer . Note in response to another reviewer we have renamed Optimistic to Compositional Optimism ( CO ) to avoid confusion . > - Figure 1.i states `` Neither GPI nor the optimistic policies ( j shows GPI , by the Optimistic policy is similar ) '' but Figure1.j is labeled DC T , is this a typo ? Apologies , that is indeed a typo . We \u2019 ve now reworded this caption . > - Figure 2 : Many typos : `` ( b ) Finger position at the en ( of the trajectoriesstard ting from randomly sampled start states ) '' Thank you for pointing this out , this has been fixed . In general , we made a number of minor edits throughout , and particularly revised section 4 to improve the readability of the paper ."}}