{"year": "2020", "forum": "BkggGREKvS", "title": "Promoting Coordination through Policy Regularization in Multi-Agent Deep Reinforcement Learning", "decision": "Reject", "meta_review": "After reading the reviews and discussing this paper with the reviewers, I believe that this paper is not quite ready for publication at this time. While there was some enthusiasm from the reviewers about the paper, there were also major concerns raised about the comparisons and experimental evaluation, as well as some concerns about novelty. The major concerns about experimental evaluation center around the experiments being restricted to continuous action settings where there is a limited set of baselines (see R3). While I see the authors' point that the method is not restricted to this setting, showing more experiments with more baselines would be important: the demonstrated experiments do strike me as somewhat simplistic, and the standardized comparisons are limited.\n\nThis might not by itself be that large of an issue, if it wasn't for the other problem: the contribution strikes me as somewhat ad-hoc. While I can see the intuition behind why these two auxiliary objectives might work well, since there is only intuition, then the burden in terms of showing that this is a good idea falls entirely on the experiments. And this is where in my opinion the work comes up short: if we are going to judge the efficacy of the method entirely on the experimental evaluation without any theoretical motivation, then the experimental evaluation does not seem to me to be sufficient.\n\nThis issue could be addressed either with more extensive and complete experiments and comparisons, or a more convincing conceptual or theoretical argument explaining why we should expect these two particular auxiliary objectives to make a big difference.", "reviews": [{"review_id": "BkggGREKvS-0", "review_text": "This paper proposed two approaches to encourage cooperation among multi-agents under the *centralized training decentralized execution* framework. The main contribution of this paper is that they propose to allow agents to predict the behavior of others and introduce this prediction loss into the RL learning objective. One approach is teammate-regularization where each agent predicts the behavior of all others (and therefore makes the total model complexity increasing quadratically with the number of agents), while the other is a centralized coach-regularization. The performance of these two approaches are compared with MADDPG and its two variants on 4 games. Experiments show that their approaches surpass a few baselines in certain game settings. However, the novelty of this algorithm is not strong, given that the similar idea of 'predicting the behavior of other agents' can be found in related work with discrete action spaces. Experimental results also show unstable advantages of their approaches. Methodology: Team-reg: One main difference between this approach and Jaques work: * Intrinsic social motivation via causal influence in multi-agent* (or other MARL algorithms for discrete action spaces) is that in this work, each agent predict the action of other peers instead of the policy, since this algorithm is only based on DDPG with continuous action space. However, It seems that this idea is transferrable to discrete action spaces as well, so long as we change the MSE loss between actions to KL loss between policy over states. The novelty of this approach is not strong. Also, one hypothesis on which their work is based is that promoting agents\u2019 predictability could foster such team structure and lead to more coordinated behaviors. Does predictability really improve cooperation? Could it be the other way around? Experiment results in the right-below penal (CHASE Game ) and left-above penal (SPREAD game) of Figure 5 actually strength my concern. Coach-reg: Descriptions of the coach regularization approach is quite vague. I doubt the motivation and effectiveness of this approach. More questions are as follows: * What is the role of the policy mask in deriving a better action? Is it just a dropout before the activation layer with a fixed $p$ proportion (which is related to the choice of $K$)? If so, this is like you first assume the policy network is somewhat overfitting, then alleviate this issue by letting the coach adjust which part to keep and which part to drop. How would different $K$ values affect the performance? * Does this framework really improve cooperation? Intuitively, all agents will finally reach a point where they agree on the same policy mask distribution (which is the one generated by the coach). How does the same policy mask lead to an improvement in cooperation? Empirical or theoretical explanations are strongly needed in the main results, not in the appendix section. Experiments: - All variants of MADDPG in the experiments are weak baselines, assuming that $MADDPG + sharing$ means all agents share the same policy and Q-function. However, since this algorithm only works for continuous action space, available relate work to compare with is quite limited. All environments only include two agents. Experiments with more than two agents should be implemented. - Even for only 3 games (excluding the adversarial game setting), both branches failed to beat the *sharing* baseline for the CHASE game. For the ablation study, *agent-moduling* even works better than *TeamReg* for the CHASE game, so as the case when *policy mask* beats *CoachReg* in the SPREAD game (Figure 5). They seem to show that under these two proposed frameworks, the predictability of agents does not always encourage cooperation, at least for 2 out of 4 game settings mentioned in this section. - This work makes a lot of effort on the hyper-parameter tuning part, which however does not provide a systematic solution to the tradeoff between improving predictability and maximizing reward. ", "rating": "6: Weak Accept", "reply_text": "$ \\textbf { 16 . Conclusion } $ We thank Reviewer1 for taking the time to read our full response . We hope that our clarifications above , as well as the additional experiments and conclusions including the revised manuscript , address your concerns to the point where you are able to more clearly assess the totality of our contributions . $ \\textbf { 17 . References } $ [ 1 ] Iqbal , S. , & Sha , F. Actor-Attention-Critic for Multi-Agent Reinforcement Learning Supplementary Material . [ 2 ] Zheng , Lianmin , et al . `` MAgent : A many-agent reinforcement learning platform for artificial collective intelligence . '' Thirty-Second AAAI Conference on Artificial Intelligence . 2018 . [ 3 ] Suarez , J. , Du , Y. , Isola , P. , & Mordatch , I . ( 2019 ) .Neural MMO : A Massively Multiagent Game Environment for Training and Evaluating Intelligent Agents . arXiv preprint arXiv:1903.00784 . [ 4 ] Jaques , Natasha , Angeliki Lazaridou , Edward Hughes , Caglar Gulcehre , Pedro Ortega , Dj Strouse , Joel Z. Leibo , and Nando De Freitas . `` Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning . '' In International Conference on Machine Learning , pp . 3040-3049 . 2019 . [ 5 ] Li , Shihui , Yi Wu , Xinyue Cui , Honghua Dong , Fei Fang , and Stuart Russell . `` Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient . '' In AAAI Conference on Artificial Intelligence ( AAAI ) . 2019 . [ 6 ] Gupta , Jayesh K. , Maxim Egorov , and Mykel Kochenderfer . `` Cooperative multi-agent control using deep reinforcement learning . '' In International Conference on Autonomous Agents and Multiagent Systems , pp . 66-83.Springer , Cham , 2017 ."}, {"review_id": "BkggGREKvS-1", "review_text": "This paper proposes two methods of biasing agents towards learning coordinated behaviours and evaluates both rigorously across two existing and two novel multi-agent domains of suitable complexity. The rigour of the empirical evaluation and its documentation is exemplary. The related work section could be improved by including a section on the closely related work in the area of opponent modelling. This literature is touched on shortly afterwards alongside the related proposed method in Section 4.1 but only by a single citation. A more thorough review in Section 3 would improve the positioning of the paper. From figures 4 and 5 it is interesting to observe that only parameter sharing performs significantly better than the existing baseline method MADDPG on the CHASE environment. In particular I think it would improve the discussion of the results to elaborate further on why the proposed methods do not improve, but also do not detrimentally affect the learning performance of agents in this environment. In Section 6.2 the authors note \"MADDPG + policy mask performs similarly or worse than MADDPG on all but one environment and never outperforms the full CoachReg approach.\" I found it also interesting to note that in that one environment (SPREAD) adding a policy mask both outperformed MADDPG and was more sample efficient than CoachReg. Again what is it about this specific environment that causes the policy mask ablation to be sufficient, enabling it to match the asymptotic performance and learn quicker than the full CoachReg method proposed? All references to papers both published and available on arxiv should cite the published version (e.g. Jacques et al. 2018 was published at ICML). Please revise all references if accepted. Minor Comments: 1) Page 2 \"two standards multi-agent tasks\" -> standard 2) Page 7 \"improve on MADDPG apart on the SPREAD environment\" -> apart from on 3) Page 9 \"each sampled configuration allows to empirically evaluate\" -> allows us ", "rating": "8: Accept", "reply_text": "We wish to thank Reviewer2 for the feedback and for promoting discussions on some of the more intriguing results that fall from our experiments . We address you specific comments , questions and concerns , below . $ \\textbf { 1 . \u201c The related work section could be improved by including a section on the closely related work in the area } $ $ \\textbf { of opponent modelling. \u201d } $ We agree : opponent modelling is closely related to our approach . We have reordered and added references to previous works in agent modelling ( and opponent modelling ) in our revised related work section , as per your suggestion . We also clarified how our use of agent modelling differs from this prior art . $ \\textbf { 2 . \u201c From figures 4 and 5 it is interesting to observe that only parameter sharing performs significantly better } $ $ \\textbf { than the existing baseline method MADDPG on the CHASE environment . In particular I think } $ $ \\textbf { it would improve the discussion of the results to elaborate further on why the proposed methods } $ $ \\textbf { do not improve , but also do not detrimentally affect the learning performance of agents } $ $ \\textbf { in this environment. \u201c } $ This is a very interesting comment . The discovery of coordinated behavior in this environment seems simpler than in our two proposed environments . Indeed , the presence of boundaries in the environment permits selfish and independent agent behaviors , where rushing independently towards the prey can perform surprisingly well given the right trajectory angles . This could explain why our methods , which aim to encourage the discovery of coordinated policies , provide no particular benefits on this task ; however , as you mention , our mechanisms do not reduce performance . For TeamReg , we believe that any additional regularization pressure is negligible since the \u201c rush towards the prey \u201d behavior is already quite predictable . For CoachReg , the question is more complex ( as illustrated in Figure 5 ) : policy masks alone hurt performance whereas the full CoachReg approach does not . We suspect that the policy masks alone makes the sub-policies of the teammate alternate in an unpredictable manner , and so abrupt changes in its policy can perturb the prey ( i.e. , that is moved by repulsive forces ) and prevent agents from trapping the prey when they are both close to it . Using CoachReg , sub-policies are jointly optimized and change synchronously , therefore the prey perturbation due to an agent changing its policy is immediately accounted for by the other agent ( who is also changing its policy ) . Moreover , parameter sharing might be efficient on CHASE since this task is highly symmetric ( one agent rushes from the left , another from the right , and they trap the prey ) and thus we suspect that policy-specialisation may not be necessary to succeed for this task . For such problems where a shared policy is sufficient , it is to be expected that parameter sharing might prevail as it benefits from a more varied data collection process : transitions collected by both agents are used to train the same models . $ \\textbf { 3 . \u201c In Section 6.2 the authors note `` MADDPG + policy mask performs similarly or worse than MADDPG } $ $ \\textbf { on all but one environment and never outperforms the full CoachReg approach . '' I found it also interesting to note } $ $ \\textbf { that in that one environment ( SPREAD ) adding a policy mask both outperformed MADDPG and was more } $ $ \\textbf { sample efficient than CoachReg . Again what is it about this specific environment that causes the policy mask ablation } $ $ \\textbf { to be sufficient , enabling it to match the asymptotic performance and learn quicker than the full CoachReg } $ $ \\textbf { method proposed ? \u201d } $ By inspecting the mask transitions sequences of CoachReg in that environment , we found that successful runs often result from sub-policy combinations such as ( 1 ) \u201c rush towards landmark \u201d and ( 2 ) \u201c decelerate and stay on landmark \u201d ( see Figure 16 , Appendix F2 ) . The remaining difficulty lies in avoiding collisions with teammates . Synchronicity is not strictly necessary to put such a sequential strategy in place . At this point , sub-policy masks alone might be leveraged as a single-agent hierarchical component , easing the process of cutting down an episode into sequential segments . This may explain why \u2014 without the regularization pressure imposed by synchronicity \u2014 the \u201c MADDPG+policy masks \u201d ablation is able to match CoachReg final performance ( while learning slightly faster ) . $ \\textbf { 4 . Outdated ( arXiv ) citations and grammatical errors } $ Thank you for pointing these issues : we have addressed them in our revised manuscript . $ \\textbf { 5 . Conclusion } $ We hope that our elaborations above satisfy your questions and concerns . We remain available to further discussions and/or clarifying any remaining ambiguities ."}, {"review_id": "BkggGREKvS-2", "review_text": "Contribution: This paper proposes two methods building upon MADDPG to encourage collaboration amongst decentralized MARL agents: - TeamReg: the agents are trained with an additional objectives of predicting other agents' actions, and make their own action predictable to them - CoachReg: the agents use a form of structured dropout, and at training time a central agent (the \"coach\") predicts the mask dropout mask that should be applied to all of the agents. An additional loss is provided so that the agent learn to mimic the coach's output from their own input so they can still apply the dropout at test time, when the coach is not available anymore. These two methods are evaluated in 4 different MARL environments, and compared against their ablations and vanilla MADDPG. Review: The paper is well written and easy to follow. It generally motivates well the design choices, both intuitively and experimentally, using numerous ablations. It includes the analysis and explanations of the failure modes, which is valuable in my opinion. The two main limitations of the work are the following: - Limited scale of the experiments. The majority of the experiments contain only 2 agents, and the last one merely contains 3. It is unclear whether the additional losses proposed in this work would still perform correctly with more agents, since the regularization pressure will increase. - No comparison to SOTA MARL methods. The only baseline method presented here is MADDPG, upon which this work is built. Recent work tend to show that it is easily outperformed, hence comparison to stronger baselines (QMIX, COMA, M3DDPG, ...) would be advisable to assess the quality of the policy found. About the policy masks: - what are the value chosen for K and C? - Is there a reason why this particular form of dropout was chosen? Since it occurs after a fully-connected, it should be equivalent and more straight-forward to mask out C contiguous values. Finally, it seems to me that the two methods presented here (TeamReg and CoachReg) are not mutually incompatible. Is there a reason why you didn't to apply both of them simultaneously? ", "rating": "3: Weak Reject", "reply_text": "$ \\textbf { 3 . \u201c About the policy masks : what are the value chosen for K and C ? \u201d } $ Thank you for bringing this to our attention : we set $ K $ to 4 , meaning that agents can choose between 4 sub-policies . Since policies \u2019 hidden layers have size 128 , the corresponding value for $ C $ is 32 . We updated Appendix B in our revised manuscript to clarify this point . $ \\textbf { 4 . \u201c About the policy masks : is there a reason why this particular form of dropout was chosen ? Since it occurs } $ $ \\textbf { after a fully-connected , it should be equivalent and more straight-forward to mask out C contiguous values. \u201d } $ No , there is no particular reason . As you noted , since it is preceded and followed by fully-connected layers , the masking procedure that you suggest ( masking an entire block of contiguous units of size $ M/K $ , where $ M $ is the dimensionality of the layer and $ K $ is the number of possible masks ) is equivalent to the approach that we present in the paper ( masking the $ k $ -th unit of every contiguous block of size $ K $ on that layer ) . Since the layer does not encode any structural information , this operation can indeed be carried out in different ways , boiling down to a relatively minor implementation choice . $ \\textbf { 5 . \u201c it seems to me that the two methods presented here ( TeamReg and CoachReg ) are not mutually incompatible . } $ $ \\textbf { Is there a reason why you did n't to apply both of them simultaneously ? \u201d } $ You are right that the two approaches both aim at promoting coordination through different mechanisms and , so , could applied simultaneously . One would , however , need to account for the fact that \u2014 since each approach is independently implemented \u2014 their combination would require 5 additional hyperparameters on top of the base algorithm ( as opposed to 2 hyperparameters for TeamReg or 3 for CoachReg ) . Since the hyperparameter space grows exponentially with the number of hyperparameters , tuning an MADDPG+TeamReg+CoachReg algorithm could become burdensome . Furthermore , analysing how these two secondary objectives interact would require many additional experiments . We leave such an exploration to future work and focus here on separately evaluating and analysing these two approaches . $ \\textbf { 6 . Conclusion } $ We thank Reviewer3 for very constructive feedback . We hope to have addressed the concerns that were raised in the review and that Reviewer3 will consider updating the rating to support the publication of this work . We remain available to further discussions and/or clarifying any remaining ambiguities . $ \\textbf { 7 . References } $ [ 1 ] Iqbal , S. , & Sha , F. Actor-Attention-Critic for Multi-Agent Reinforcement Learning Supplementary Material . [ 2 ] Zheng , Lianmin , et al . `` MAgent : A many-agent reinforcement learning platform for artificial collective intelligence . '' Thirty-Second AAAI Conference on Artificial Intelligence . 2018 . [ 3 ] Suarez , J. , Du , Y. , Isola , P. , & Mordatch , I . ( 2019 ) .Neural MMO : A Massively Multiagent Game Environment for Training and Evaluating Intelligent Agents . arXiv preprint arXiv:1903.00784 . [ 4 ] Foerster , J. N. , Farquhar , G. , Afouras , T. , Nardelli , N. , & Whiteson , S. ( 2018 , April ) . Counterfactual multi-agent policy gradients . In Thirty-Second AAAI Conference on Artificial Intelligence . [ 5 ] Rashid , T. , Samvelyan , M. , De Witt , C. S. , Farquhar , G. , Foerster , J. , & Whiteson , S. ( 2018 ) . QMIX : monotonic value function factorisation for deep multi-agent reinforcement learning . arXiv preprint arXiv:1803.11485 . [ 6 ] Hu , D. , Jiang , X. , Wei , X. , & Wang , J . ( 2019 , August ) . State Representation Learning for Minimax Deep Deterministic Policy Gradient . In International Conference on Knowledge Science , Engineering and Management ( pp.481-487 ) .Springer , Cham ."}], "0": {"review_id": "BkggGREKvS-0", "review_text": "This paper proposed two approaches to encourage cooperation among multi-agents under the *centralized training decentralized execution* framework. The main contribution of this paper is that they propose to allow agents to predict the behavior of others and introduce this prediction loss into the RL learning objective. One approach is teammate-regularization where each agent predicts the behavior of all others (and therefore makes the total model complexity increasing quadratically with the number of agents), while the other is a centralized coach-regularization. The performance of these two approaches are compared with MADDPG and its two variants on 4 games. Experiments show that their approaches surpass a few baselines in certain game settings. However, the novelty of this algorithm is not strong, given that the similar idea of 'predicting the behavior of other agents' can be found in related work with discrete action spaces. Experimental results also show unstable advantages of their approaches. Methodology: Team-reg: One main difference between this approach and Jaques work: * Intrinsic social motivation via causal influence in multi-agent* (or other MARL algorithms for discrete action spaces) is that in this work, each agent predict the action of other peers instead of the policy, since this algorithm is only based on DDPG with continuous action space. However, It seems that this idea is transferrable to discrete action spaces as well, so long as we change the MSE loss between actions to KL loss between policy over states. The novelty of this approach is not strong. Also, one hypothesis on which their work is based is that promoting agents\u2019 predictability could foster such team structure and lead to more coordinated behaviors. Does predictability really improve cooperation? Could it be the other way around? Experiment results in the right-below penal (CHASE Game ) and left-above penal (SPREAD game) of Figure 5 actually strength my concern. Coach-reg: Descriptions of the coach regularization approach is quite vague. I doubt the motivation and effectiveness of this approach. More questions are as follows: * What is the role of the policy mask in deriving a better action? Is it just a dropout before the activation layer with a fixed $p$ proportion (which is related to the choice of $K$)? If so, this is like you first assume the policy network is somewhat overfitting, then alleviate this issue by letting the coach adjust which part to keep and which part to drop. How would different $K$ values affect the performance? * Does this framework really improve cooperation? Intuitively, all agents will finally reach a point where they agree on the same policy mask distribution (which is the one generated by the coach). How does the same policy mask lead to an improvement in cooperation? Empirical or theoretical explanations are strongly needed in the main results, not in the appendix section. Experiments: - All variants of MADDPG in the experiments are weak baselines, assuming that $MADDPG + sharing$ means all agents share the same policy and Q-function. However, since this algorithm only works for continuous action space, available relate work to compare with is quite limited. All environments only include two agents. Experiments with more than two agents should be implemented. - Even for only 3 games (excluding the adversarial game setting), both branches failed to beat the *sharing* baseline for the CHASE game. For the ablation study, *agent-moduling* even works better than *TeamReg* for the CHASE game, so as the case when *policy mask* beats *CoachReg* in the SPREAD game (Figure 5). They seem to show that under these two proposed frameworks, the predictability of agents does not always encourage cooperation, at least for 2 out of 4 game settings mentioned in this section. - This work makes a lot of effort on the hyper-parameter tuning part, which however does not provide a systematic solution to the tradeoff between improving predictability and maximizing reward. ", "rating": "6: Weak Accept", "reply_text": "$ \\textbf { 16 . Conclusion } $ We thank Reviewer1 for taking the time to read our full response . We hope that our clarifications above , as well as the additional experiments and conclusions including the revised manuscript , address your concerns to the point where you are able to more clearly assess the totality of our contributions . $ \\textbf { 17 . References } $ [ 1 ] Iqbal , S. , & Sha , F. Actor-Attention-Critic for Multi-Agent Reinforcement Learning Supplementary Material . [ 2 ] Zheng , Lianmin , et al . `` MAgent : A many-agent reinforcement learning platform for artificial collective intelligence . '' Thirty-Second AAAI Conference on Artificial Intelligence . 2018 . [ 3 ] Suarez , J. , Du , Y. , Isola , P. , & Mordatch , I . ( 2019 ) .Neural MMO : A Massively Multiagent Game Environment for Training and Evaluating Intelligent Agents . arXiv preprint arXiv:1903.00784 . [ 4 ] Jaques , Natasha , Angeliki Lazaridou , Edward Hughes , Caglar Gulcehre , Pedro Ortega , Dj Strouse , Joel Z. Leibo , and Nando De Freitas . `` Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning . '' In International Conference on Machine Learning , pp . 3040-3049 . 2019 . [ 5 ] Li , Shihui , Yi Wu , Xinyue Cui , Honghua Dong , Fei Fang , and Stuart Russell . `` Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient . '' In AAAI Conference on Artificial Intelligence ( AAAI ) . 2019 . [ 6 ] Gupta , Jayesh K. , Maxim Egorov , and Mykel Kochenderfer . `` Cooperative multi-agent control using deep reinforcement learning . '' In International Conference on Autonomous Agents and Multiagent Systems , pp . 66-83.Springer , Cham , 2017 ."}, "1": {"review_id": "BkggGREKvS-1", "review_text": "This paper proposes two methods of biasing agents towards learning coordinated behaviours and evaluates both rigorously across two existing and two novel multi-agent domains of suitable complexity. The rigour of the empirical evaluation and its documentation is exemplary. The related work section could be improved by including a section on the closely related work in the area of opponent modelling. This literature is touched on shortly afterwards alongside the related proposed method in Section 4.1 but only by a single citation. A more thorough review in Section 3 would improve the positioning of the paper. From figures 4 and 5 it is interesting to observe that only parameter sharing performs significantly better than the existing baseline method MADDPG on the CHASE environment. In particular I think it would improve the discussion of the results to elaborate further on why the proposed methods do not improve, but also do not detrimentally affect the learning performance of agents in this environment. In Section 6.2 the authors note \"MADDPG + policy mask performs similarly or worse than MADDPG on all but one environment and never outperforms the full CoachReg approach.\" I found it also interesting to note that in that one environment (SPREAD) adding a policy mask both outperformed MADDPG and was more sample efficient than CoachReg. Again what is it about this specific environment that causes the policy mask ablation to be sufficient, enabling it to match the asymptotic performance and learn quicker than the full CoachReg method proposed? All references to papers both published and available on arxiv should cite the published version (e.g. Jacques et al. 2018 was published at ICML). Please revise all references if accepted. Minor Comments: 1) Page 2 \"two standards multi-agent tasks\" -> standard 2) Page 7 \"improve on MADDPG apart on the SPREAD environment\" -> apart from on 3) Page 9 \"each sampled configuration allows to empirically evaluate\" -> allows us ", "rating": "8: Accept", "reply_text": "We wish to thank Reviewer2 for the feedback and for promoting discussions on some of the more intriguing results that fall from our experiments . We address you specific comments , questions and concerns , below . $ \\textbf { 1 . \u201c The related work section could be improved by including a section on the closely related work in the area } $ $ \\textbf { of opponent modelling. \u201d } $ We agree : opponent modelling is closely related to our approach . We have reordered and added references to previous works in agent modelling ( and opponent modelling ) in our revised related work section , as per your suggestion . We also clarified how our use of agent modelling differs from this prior art . $ \\textbf { 2 . \u201c From figures 4 and 5 it is interesting to observe that only parameter sharing performs significantly better } $ $ \\textbf { than the existing baseline method MADDPG on the CHASE environment . In particular I think } $ $ \\textbf { it would improve the discussion of the results to elaborate further on why the proposed methods } $ $ \\textbf { do not improve , but also do not detrimentally affect the learning performance of agents } $ $ \\textbf { in this environment. \u201c } $ This is a very interesting comment . The discovery of coordinated behavior in this environment seems simpler than in our two proposed environments . Indeed , the presence of boundaries in the environment permits selfish and independent agent behaviors , where rushing independently towards the prey can perform surprisingly well given the right trajectory angles . This could explain why our methods , which aim to encourage the discovery of coordinated policies , provide no particular benefits on this task ; however , as you mention , our mechanisms do not reduce performance . For TeamReg , we believe that any additional regularization pressure is negligible since the \u201c rush towards the prey \u201d behavior is already quite predictable . For CoachReg , the question is more complex ( as illustrated in Figure 5 ) : policy masks alone hurt performance whereas the full CoachReg approach does not . We suspect that the policy masks alone makes the sub-policies of the teammate alternate in an unpredictable manner , and so abrupt changes in its policy can perturb the prey ( i.e. , that is moved by repulsive forces ) and prevent agents from trapping the prey when they are both close to it . Using CoachReg , sub-policies are jointly optimized and change synchronously , therefore the prey perturbation due to an agent changing its policy is immediately accounted for by the other agent ( who is also changing its policy ) . Moreover , parameter sharing might be efficient on CHASE since this task is highly symmetric ( one agent rushes from the left , another from the right , and they trap the prey ) and thus we suspect that policy-specialisation may not be necessary to succeed for this task . For such problems where a shared policy is sufficient , it is to be expected that parameter sharing might prevail as it benefits from a more varied data collection process : transitions collected by both agents are used to train the same models . $ \\textbf { 3 . \u201c In Section 6.2 the authors note `` MADDPG + policy mask performs similarly or worse than MADDPG } $ $ \\textbf { on all but one environment and never outperforms the full CoachReg approach . '' I found it also interesting to note } $ $ \\textbf { that in that one environment ( SPREAD ) adding a policy mask both outperformed MADDPG and was more } $ $ \\textbf { sample efficient than CoachReg . Again what is it about this specific environment that causes the policy mask ablation } $ $ \\textbf { to be sufficient , enabling it to match the asymptotic performance and learn quicker than the full CoachReg } $ $ \\textbf { method proposed ? \u201d } $ By inspecting the mask transitions sequences of CoachReg in that environment , we found that successful runs often result from sub-policy combinations such as ( 1 ) \u201c rush towards landmark \u201d and ( 2 ) \u201c decelerate and stay on landmark \u201d ( see Figure 16 , Appendix F2 ) . The remaining difficulty lies in avoiding collisions with teammates . Synchronicity is not strictly necessary to put such a sequential strategy in place . At this point , sub-policy masks alone might be leveraged as a single-agent hierarchical component , easing the process of cutting down an episode into sequential segments . This may explain why \u2014 without the regularization pressure imposed by synchronicity \u2014 the \u201c MADDPG+policy masks \u201d ablation is able to match CoachReg final performance ( while learning slightly faster ) . $ \\textbf { 4 . Outdated ( arXiv ) citations and grammatical errors } $ Thank you for pointing these issues : we have addressed them in our revised manuscript . $ \\textbf { 5 . Conclusion } $ We hope that our elaborations above satisfy your questions and concerns . We remain available to further discussions and/or clarifying any remaining ambiguities ."}, "2": {"review_id": "BkggGREKvS-2", "review_text": "Contribution: This paper proposes two methods building upon MADDPG to encourage collaboration amongst decentralized MARL agents: - TeamReg: the agents are trained with an additional objectives of predicting other agents' actions, and make their own action predictable to them - CoachReg: the agents use a form of structured dropout, and at training time a central agent (the \"coach\") predicts the mask dropout mask that should be applied to all of the agents. An additional loss is provided so that the agent learn to mimic the coach's output from their own input so they can still apply the dropout at test time, when the coach is not available anymore. These two methods are evaluated in 4 different MARL environments, and compared against their ablations and vanilla MADDPG. Review: The paper is well written and easy to follow. It generally motivates well the design choices, both intuitively and experimentally, using numerous ablations. It includes the analysis and explanations of the failure modes, which is valuable in my opinion. The two main limitations of the work are the following: - Limited scale of the experiments. The majority of the experiments contain only 2 agents, and the last one merely contains 3. It is unclear whether the additional losses proposed in this work would still perform correctly with more agents, since the regularization pressure will increase. - No comparison to SOTA MARL methods. The only baseline method presented here is MADDPG, upon which this work is built. Recent work tend to show that it is easily outperformed, hence comparison to stronger baselines (QMIX, COMA, M3DDPG, ...) would be advisable to assess the quality of the policy found. About the policy masks: - what are the value chosen for K and C? - Is there a reason why this particular form of dropout was chosen? Since it occurs after a fully-connected, it should be equivalent and more straight-forward to mask out C contiguous values. Finally, it seems to me that the two methods presented here (TeamReg and CoachReg) are not mutually incompatible. Is there a reason why you didn't to apply both of them simultaneously? ", "rating": "3: Weak Reject", "reply_text": "$ \\textbf { 3 . \u201c About the policy masks : what are the value chosen for K and C ? \u201d } $ Thank you for bringing this to our attention : we set $ K $ to 4 , meaning that agents can choose between 4 sub-policies . Since policies \u2019 hidden layers have size 128 , the corresponding value for $ C $ is 32 . We updated Appendix B in our revised manuscript to clarify this point . $ \\textbf { 4 . \u201c About the policy masks : is there a reason why this particular form of dropout was chosen ? Since it occurs } $ $ \\textbf { after a fully-connected , it should be equivalent and more straight-forward to mask out C contiguous values. \u201d } $ No , there is no particular reason . As you noted , since it is preceded and followed by fully-connected layers , the masking procedure that you suggest ( masking an entire block of contiguous units of size $ M/K $ , where $ M $ is the dimensionality of the layer and $ K $ is the number of possible masks ) is equivalent to the approach that we present in the paper ( masking the $ k $ -th unit of every contiguous block of size $ K $ on that layer ) . Since the layer does not encode any structural information , this operation can indeed be carried out in different ways , boiling down to a relatively minor implementation choice . $ \\textbf { 5 . \u201c it seems to me that the two methods presented here ( TeamReg and CoachReg ) are not mutually incompatible . } $ $ \\textbf { Is there a reason why you did n't to apply both of them simultaneously ? \u201d } $ You are right that the two approaches both aim at promoting coordination through different mechanisms and , so , could applied simultaneously . One would , however , need to account for the fact that \u2014 since each approach is independently implemented \u2014 their combination would require 5 additional hyperparameters on top of the base algorithm ( as opposed to 2 hyperparameters for TeamReg or 3 for CoachReg ) . Since the hyperparameter space grows exponentially with the number of hyperparameters , tuning an MADDPG+TeamReg+CoachReg algorithm could become burdensome . Furthermore , analysing how these two secondary objectives interact would require many additional experiments . We leave such an exploration to future work and focus here on separately evaluating and analysing these two approaches . $ \\textbf { 6 . Conclusion } $ We thank Reviewer3 for very constructive feedback . We hope to have addressed the concerns that were raised in the review and that Reviewer3 will consider updating the rating to support the publication of this work . We remain available to further discussions and/or clarifying any remaining ambiguities . $ \\textbf { 7 . References } $ [ 1 ] Iqbal , S. , & Sha , F. Actor-Attention-Critic for Multi-Agent Reinforcement Learning Supplementary Material . [ 2 ] Zheng , Lianmin , et al . `` MAgent : A many-agent reinforcement learning platform for artificial collective intelligence . '' Thirty-Second AAAI Conference on Artificial Intelligence . 2018 . [ 3 ] Suarez , J. , Du , Y. , Isola , P. , & Mordatch , I . ( 2019 ) .Neural MMO : A Massively Multiagent Game Environment for Training and Evaluating Intelligent Agents . arXiv preprint arXiv:1903.00784 . [ 4 ] Foerster , J. N. , Farquhar , G. , Afouras , T. , Nardelli , N. , & Whiteson , S. ( 2018 , April ) . Counterfactual multi-agent policy gradients . In Thirty-Second AAAI Conference on Artificial Intelligence . [ 5 ] Rashid , T. , Samvelyan , M. , De Witt , C. S. , Farquhar , G. , Foerster , J. , & Whiteson , S. ( 2018 ) . QMIX : monotonic value function factorisation for deep multi-agent reinforcement learning . arXiv preprint arXiv:1803.11485 . [ 6 ] Hu , D. , Jiang , X. , Wei , X. , & Wang , J . ( 2019 , August ) . State Representation Learning for Minimax Deep Deterministic Policy Gradient . In International Conference on Knowledge Science , Engineering and Management ( pp.481-487 ) .Springer , Cham ."}}