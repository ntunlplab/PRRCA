{"year": "2021", "forum": "tv8n52XbO4p", "title": "Learning to Generate Noise for Multi-Attack Robustness", "decision": "Reject", "meta_review": "This is a borderline case. The paper seems solid although some of the numbers are likely incorrect because in some results tables in the appendix the error taken over all attacks is higher than for the best individual attack (which should never happen).\n\nThe main contribution of this paper is to augment a standard adversarial loss (against attacks from different norms) with a \u201cconsistency\u201d term (consistency between clean, adversarial and noise augmented samples). The relatively large jump in robustness compared to existing schemes that do adversarial training against multiple norms is a bit surprising. A possible explanation could be that the additional consistency term smoothes the landscape around the clean samples a little bit, which could help to find better adversarial examples. The latter would be very similar to a paper by Pushmeet and colleagues (https://arxiv.org/pdf/1907.02610.pdf) which is not cited, but definitely should. It might also be worthwhile to compare to this paper. \n\nTaken together, this work is interesting but not sufficiently convincing yet to belong to the top papers to be selected for publication at ICLR. \n", "reviews": [{"review_id": "tv8n52XbO4p-0", "review_text": "Summary = The authors propose a number of techniques to learn models which are adversarially robust to multiple perturbations . These involve a noise generator , a loss to enforce consistency , as well as a stochastic variant of adversarial training . With these changes , they are able to produce improvements to robust accuracy to multiple perturbation types . Overall , I get the idea and the empirical results seem promising . However , the structure and writing of the paper is at times rather confusing , and there are a lot of missing details . If the code were not supplied , it would be difficult in the current state to reproduce the method from the paper . Perhaps due to this , the specifics of the key component , the meta noise generator , are still rather opaque to me . Perhaps the authors can clarify , and I am happy to follow up afterwards . Comments for discussion == The majority of my confusion lies in section 4 , for the specifics of the meta noise generator and parts of the algorithm in general . I am otherwise well acquainted with the relevant literature . 1 ) Augmented examples ( x_aug ) are generated by adding noise from the MNG and projecting it onto some ball B . It is not clear to me what ball this is since the authors are considering multiple perturbations . Is it a random type ? Or a joint projection ? I assume it is at least one of the perturbations being considered , or is that incorrect ? 2 ) Similarly , in the algorithm , the authors generate adversarial examples ( x_adv ) by sampling a random attack . I could not find what set of attacks were being sampled from , or what the sampling distribution is ( I checked the appendix as well ) . 3 ) The generator is apparently updated to minimize the classifier loss on the adversarial examples as written in Equation ( 8 ) . However , the adversarial examples are generated from some unspecified set of attacks , which implies that the set of attacks actually depends on the generator somehow . Is this supposed to be the classifier loss on the augmented samples ? If not , then how do the adversarial examples depend on the generator ? 4 ) The consistency loss involves clean , adversarial , and augmented posterior distributions . There are no details on these distributions : are these simply the softmax of the logits ? Or is a generative model that outputs a distribution being used ? 5 ) On a more fundamental level , what is the motivation behind training the generator to minimize the classifier loss ? Why would we want to do this over random sampling ? What 's to prevent a degenerate solution of simply learning to produce a zero perturbation ( and thus always producing clean examples , which can achieve low loss ) ? Minor comments == I have checked the supplementary material and the authors have included the code for running their experiments . Ideally , this would also include pre-trained model weights . Update After much effort , I can say that I understand the paper . The edits appear to have incorporated all the identified missing information . I have thus updated my confidence and slightly improved my score , however I am not confident that the current presentation of the approach will be understandable by a reader without contacting the authors , given that the difficulty I had in understanding the paper ( and my initial confidence ) stemmed primarily from missing information and poor presentation for the approach . Although the results do seem to improve upon past work , its impact will suffer if it is difficult to understand for a non-reviewer reader . I would be more confident if a fresh set of eyes could understand the details of the work without having to go to the authors to clarify so many details .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We sincerely appreciate your constructive comments . We respond to your main concerns below : 1 . Augmented examples ( x_aug ) are generated by adding noise from the MNG and projecting it onto some ball B . It is not clear to me what ball this is since the authors are considering multiple perturbations . Is it a random type ? Or a joint projection ? I assume it is at least one of the perturbations being considered , or is that incorrect ? - $ \\mathcal { B } ( x , \\varepsilon ) $ refers to the * * norm-ball of the specific attack * * sampled by Stochastic Adversarial Training ( SAT ) . That is , if the sampled attack is an $ \\ell_2 $ attack , then $ \\mathcal { B } $ denotes the $ \\ell_2 $ norm ball , and if the sampled attack is an $ \\ell_1 $ attack , then $ \\mathcal { B } $ is the $ \\ell_1 $ norm ball . As MNG learns the noise to minimize the adversarial loss , it is essential to project the generated noise on the same norm-ball . We have clarified this point in the revision . 2.Similarly , in the algorithm , the authors generate adversarial examples ( x_adv ) by sampling a random attack . I could not find what set of attacks were being sampled from , or what the sampling distribution is ( I checked the appendix as well ) . - We apologize for the confusion . In this work , the sampling distribution corresponds to the * * $ \\ell_p $ -bounded perturbations . * * Still , it is important to note that unlike the average and max strategy , MNG + SAT can be applied to any distribution of attacks with a constant cost . We have clarified this point in the revision . 3.The generator is apparently updated to minimize the classifier loss on the adversarial examples as written in Equation ( 8 ) . However , the adversarial examples are generated from some unspecified set of attacks , which implies that the set of attacks actually depends on the generator somehow . Is this supposed to be the classifier loss on the augmented samples ? If not , then how do the adversarial examples depend on the generator ? - This is a critical misunderstanding . * * Adversarial examples do not depend on the generator ; * * instead , the one-step update in Eq . ( 8 ) is essential to do a lookahead for adapting the model parameters in the presence of the noise-augmented samples . Note that augmented samples are different from the adversarial examples ( please refer Figure 1 ) and our contribution is to optimally generate augmented examples to improve the robustness against multiple perturbations explicitly . 4.The consistency loss involves clean , adversarial , and augmented posterior distributions . There are no details on these distributions : are these simply the softmax of the logits ? Or is a generative model that outputs a distribution being used ? - As you mentioned , the distributions are the softmax of the logits of the clean , adversarial and augmented samples where the augmented samples are the output of the Meta Noise Generator ( MNG ) . We have incorporated this point in the revision . 5.What is the motivation behind training the generator to minimize the classifier loss ? Why would we want to do this over random sampling ? What 's to prevent a degenerate solution of simply learning to produce a zero perturbation ( and thus always producing clean examples , which can achieve low loss ) ? - Firstly , we would like to clarify that we * * do not train the generator to minimize the classifier loss ; * * instead , the generator learns an optimal noise distribution in a meta-learning training scheme to * * minimize the adversarial classification loss * * where adversarial classification loss is the loss on the sampled attack from the distribution of attacks . - Secondly , the motivation behind training the generator to minimize this objective is to explicitly learn the noise distribution essential for generalization across multiple perturbations , that might not necessarily correspond to any of the attack perturbations . Furthermore , our algorithm to improve the generalization across multiple perturbations is also motivated by the popular phenomenon of noise regularization being a common technique to improve the generalization performance of deep neural networks . In contrast , even though random sampling helps in generalization , it leads to a suboptimal solution ( see Table 2 ) . - Lastly , our meta-learning training scheme prevents the degenerate solution , as producing clean examples would not result in a lower loss on multiple adversarial perturbations . 6.I have checked the supplementary material , and the authors have included the code for running their experiments . Ideally , this would also include pre-trained model weights . - Thank you for pointing this out . Due to the size limit of the supplementary material , we could not provide the pre-trained models . We provide the pre-trained model weights here : https : //drive.google.com/file/d/1kVfOZ2CrhSzgzlS6gK4AntNZhIUosvfz/view ? usp=sharing"}, {"review_id": "tv8n52XbO4p-1", "review_text": "This paper addresses a timely issue in adversarial robustness - efficient training of robust models against multiple adversarial perturbations . The authors propose a combination of three techniques : stochastic adversarial training ( SAT ) , meta noise generator ( MNG ) , and adversarial consistency ( AC ) loss for efficient training , and evaluate the robustness using multiple L1 , L2 , and Linf norm-bounded attacks and three datasets ( CIFAR-10 , SVHN , and Tiny Imagenet ) . The results show improved multi-attack robustness over several baselines ( including single-attack and multiple-attack models ) and reduced training time . Ablation studies are also performed to illustrate the utility of each component of the proposed model . Overall , this paper provides very detailed evaluations involving multiple datasets , attacks , baselines , and robustness metrics . I find the results convincing and important , and also find sufficient novelty in the proposed training method . The strengths ( S ) and weaknesses ( W ) of this submission are summarized below . S1.The proposal of MNG and AC is effective and novel . S2.The evaluation is thorough and convincing . S3.The proposal improves both robustness and training efficiency in most cases . W1.The adversarial consistency ( AC ) loss is never defined explicitly . Based on equation ( 5 ) , it is hard to understand how AC `` represents the Jensen-Shannon Divergence ( JSD ) among the posterior distributions '' when considering three distributions , P_clean , P_adv , and P_aug . More clarification is needed . W2.Although the results show improved multi-attack robustness , it will be great if the authors can add more intuition on why the proposed training method leads to performance improvement . Based on the ablation study , it seems that the role of SAT and MNG is to reduce overfitting in robustness to encourage generalization , rather than optimization over the worst-case scenarios . W3.The considered multi-attack setting is still limited to different Lp norm perturbation constraints . Although the authors showed improved robustness over unforeseen attacks , the authors should also discuss how the proposed method can generalize to different attacks beyond Lp norms .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your constructive comments . We respond to your main concerns below : 1 ) The adversarial consistency ( AC ) loss is never defined explicitly . - We apologize for the confusion . We have updated the revision with the explicit definition of the Adversarial Consistency ( AC ) loss in Equation 6 . 2 ) Although the results show improved multi-attack robustness , it will be great if the authors can add more intuition on why the proposed training method leads to performance improvement . Based on the ablation study , it seems that the role of SAT and MNG is to reduce overfitting in robustness to encourage generalization , rather than optimization over the worst-case scenarios . - As you mentioned , SAT and MNG indeed play a critical role to reduce overfitting in robustness to encourage generalization . Intuitively , MNG acts as a * * noise regularization technique , * * and SAT promotes generalization across multiple perturbations * * due to its stochasticity . * * Further , we have added a separate paragraph to highlight the illustration of our training scheme in Section 4 of the revision of our paper . - Additionally , we would like to clarify that unlike the max strategy , MNG and SAT * * do not optimize over the worst-case scenarios . * * MNG learns an * * input-dependent optimal noise distribution to lower adversarial error across all the perturbations * * that does not necessarily correspond to any of the attack perturbations . 3 ) The considered multi-attack setting is still limited to different Lp norm perturbation constraints . Although the authors showed improved robustness over unforeseen attacks , the authors should also discuss how the proposed method can generalize to different attacks beyond Lp norms . - We agree that the evaluation of attacks beyond $ \\ell_p $ norms is interesting , and we would like to point out that the unforeseen adversaries consist of Elastic attack and JPEG attacks which * * do not belong to the standard family of $ \\ell_p $ attacks . * *"}, {"review_id": "tv8n52XbO4p-2", "review_text": "In this paper , the authors propose a novel meta-learning framework that explicitly learns to generate noise to improve model robustness ( against multiple types of attacks ) . The results indicate that the proposed approach improves on the state-of-the-art . Overall , the paper is well written . However some details are missing and this could make the paper hard to reproduce . The experiments could be expanded . 1 ) There is a significant amount of work about using generative models to build adversarial examples . The literature review only focuses on classical adversarial robustness and robustness against multiple adversaries . I 'd recommend making a review of these approaches , even if they are orthogonal to the one proposed in this paper ( e.g. , [ 1,2,3 ] ) 2 ) In Eq . ( 6 ) , what is \\mathcal { B } ( x , \\epsilon ) . Since there is multiple threat models , I am assuming that it is selected at random between l_1 , l_2 and l_inf ( like SAT ) . 3 ) The number of inner steps T seems to be critical ( as it will trade-off gradient precision with compute ) . However , I do n't see any study on this in the paper . Also , it is not clear which value was used for the experiments . 4 ) Looking at Eq . ( 7 ) , it seems like backpropagation through the T inner steps is necessary to compute the gradients w.r.t.\\phi.This seems overly expensive and I find surprising that adv_avg and adv_max take so much longer to train . 5 ) Concerning Eq . ( 7 ) , as a curiousity , have authors considered implicit differentiation [ 4 ] ? 6 ) The experiments are run using 30 epochs which is rather on slim side . E.g. , RST_inf should reach about 59 % robust accuracy with 200 epochs of training ( with 30 epochs it only reaches 55 % ) . I 'm curious as to whether the comparison with the proposed approach is unfair ( e.g. , Adv_inf sees a single adv example per batch , whereas MNG-AC sees 2 ) . 7 ) It 's not entirely clear to me why beta negatively affects l_2 robustness . I 'd assume that if the model was only trained against l_2 , then there might be an optimal value for beta that is different that the one from Fig.2.In general , it would be interesting to see on MNG-AC does if different subsets of threats are used . 8 ) The l_2 loss landscapes seem more noisy that what they should be . Also it 's unclear why the axes are centered for l_inf and not for l_2 ( explain how these are generated ) . 9 ) In Table 5 , MNG-AC achieves 35.1 % against all l_inf attacks , but only 33.7 % against AutoAttack . Am I missing something ? Details : A ) It would helpful to the reader to have the epsilon values written on top of the different tables . The captions could be expanded to include more details . B ) Visuaization - > Visualization [ 1 ] https : //openreview.net/pdf ? id=SJeQEp4YDH : GAT : Generative Adversarial Training for Adversarial Example Detection and Robust Classification [ 2 ] https : //arxiv.org/pdf/1801.02610 : Generating Adversarial Examples with Adversarial Networks [ 3 ] https : //arxiv.org/pdf/1710.10766 : PixelDefend : Leveraging Generative Models to Understand and Defend against Adversarial Examples [ 4 ] https : //arxiv.org/pdf/1911.02590 : Optimizing Millions of Hyperparameters by Implicit Differentiation", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your constructive comments . We respond to your main concerns below : 1 . I 'd recommend making a review of generative models to build adversarial examples , even if they are orthogonal to the one proposed in this paper ( e.g. , [ 1,2,3 ] ) - Thank you for the helpful suggestion . We have provided a detailed review of generative models for adversarial robustness in the revision . 2.In Eq . ( 6 ) , what is $ \\mathcal { B } ( x , \\epsilon ) $ . Since there are multiple threat models , I am assuming that it is selected at random between l_1 , l_2 and l_inf ( like SAT ) . - As you rightly mentioned , $ \\mathcal { B } ( x , \\varepsilon ) $ refers to a * * random norm-ball ( like SAT ) * * . That is , if the sampled attack is an $ \\ell_2 $ attack , then $ \\mathcal { B } $ denotes the $ \\ell_2 $ norm ball , and if the sampled attack is an $ \\ell_1 $ attack , then $ \\mathcal { B } $ is the $ \\ell_1 $ norm ball . Still , it is important to note that $ \\mathcal { B } ( x , \\varepsilon ) $ is the norm-ball of the attack sampled by Stochastic Adversarial Training ( SAT ) , as MNG learns the noise to minimize the adversarial loss , it is essential to project the generated noise on the same norm-ball . We have clarified this in the revision . 3.The number of inner steps T seems to be critical . However , I do n't see any study on this in the paper . It is not clear which value was used for the experiments . - We used $ T=2 $ for all our experiments to keep the training cost minimum . We empirically found that larger values of T do not provide a significant increase in the robustness while leading to a significant increase in the training cost . We provide a comparison with different values of $ T $ below : | Model | $ \\ell_\\infty $ | $ \\ell_1 $ | $ \\ell_2 $ | Time ( h ) | | | | -- | -- |- | | $ T = 1 $ | 41.5+-0.8 | 55.1+-0.9 | 71.8+-0.2 | 9.4 | | $ T = 2 $ | 42.2+-0.9 | 55.0+-1.2 | 71.5+-0.1 | 11.2 | | $ T = 4 $ | 42.4+-0.8 | 55.6+-1.1 | 71.0+-0.2 | 14.6 | | $ T = 8 $ | 42.6+-1.0 | 55.3+-1.2 | 71.0+-0.1 | 18.9 | 4 . The experiments are run using 30 epochs which is rather on slim side . E.g. , RST_inf should reach about 59 % robust accuracy with 200 epochs of training ( with 30 epochs it only reaches 55 % ) . I 'm curious as to whether the comparison with the proposed approach is unfair ( e.g. , Adv_inf sees a single adv example per batch , whereas MNG-AC sees 2 ) . - It is essential to note that RST uses ~5 million data points for CIFAR-10 and SVHN , and it took us 4 days with four GeForce RTX 2080Ti to train with 30 epochs . Since it takes * * more than 24 days * * to finish RST with 200 epochs , we did not evaluate it . Furthermore , we would like to clarify that MNG-AC does not see 2 examples per batch , the lookahead in Equation 8. occurs with a meta-model , and the classifier update occurs only once . ( Please see Line 393 in train_MNG.py in our code for more details ) . 5.It 's not entirely clear to me why beta negatively affects l_2 robustness . In general , it would be interesting to see what MNG-AC does if different subsets of threats are used . - We would like to clarify that it is not a general statement that beta negatively affects $ \\ell_2 $ robustness , instead $ \\beta $ controls the trade-off between multiple perturbations . We will do our best to get the results for different subsets of threats by the end of the rebuttal deadline . 6.The l_2 loss landscapes seem more noisy that what they should be . Also it 's unclear why the axes are centred for l_inf and not for l_2 ( explain how these are generated ) . - We apologize for the confusion . The axes are centred for all the $ \\ell_p $ norms ; we will further clean the plots in the revision . To generate these plots , we vary the input along a linear space defined by the $ \\ell_p $ norm of the gradient where x and y-axes represent the perturbation added in each direction , and the z-axis represents the loss . 7.In Table 5 , MNG-AC achieves 35.1 % against all l_inf attacks , but only 33.7 % against AutoAttack . Am I missing something ? - Thank you for pointing this out . We have fixed this discrepancy in the revision . 8.Concerning Eq . ( 7 ) , as a curiosity , have authors considered implicit differentiation [ 4 ] ? - Thank you for suggesting relevant work . However , due to the * * high computational cost of hypergradients , * * we did not evaluate this direction of work . We will add a reference to this work in the final draft , and a comparison with it should be an interesting problem for future work . 9.The caption could be expanded to include epsilon values . B ) Visuaization - > Visualization - Thank you for the suggestions , we have updated the caption and typo in the revision . Additionally , to promote reproducibility of our work , we provide the pre-trained model weights here : https : //drive.google.com/file/d/1kVfOZ2CrhSzgzlS6gK4AntNZhIUosvfz/view ? usp=sharing"}, {"review_id": "tv8n52XbO4p-3", "review_text": "1.Summary The authors propose a new method to improve robustness to adversarial examples under various norms ( L1 , L2 and LInf ) . Their method combines adversarial training with an adversarial noise generator . They improve upon adversarial training in a multi norm setting by choosing one norm at random for each sample , instead of computing an adversarial for all norms , thus significantly reducing the training time . They additionally improve robustness by regularizing model features between the standard image , the adversarially perturbed image and a perturbation of the image created with an adversarial noise generator . 2.Strengths + The method is based on adversarial training . As far as I know and as the authors note this is the only method that reliably leads to more robust models . + The authors attack their models with a range of attacks that to the best of my knowledge are state-of-the art . + The method apparently works in the multi norm setting . 3.Weaknesses - I was missing an intuitive description why the adversarial noise should improve robustness to adversarial attacks . I was only aware of it as a method to improve corruption robustness . - I was not always sure if I got everything correctly in sections 4 and 5.3 . I think I got it but I sometimes missed a figure . It may e.g.be helpful to include the losses in Figure 1 or make a separate figure . Especially why the MNG was trained the way it is was a bit unclear for me . 4.Recommendation I think this paper is an accept but as I do n't work with adversarial examples I am not at all confident in that assessment . From the discussions with people who work on adversarial examples new defenses are usually broken very quickly and there is a number of papers which break numerous defenses . The method is however based on adversarial training which to my knowledge is the only robust method so far and the used attacks seem valid . So I am definitely leaning towards accept but the opinion of a real expert would be highly appreciated as I feel not at all qualified to assess the validity of papers on adversarial examples . 5.Questions/Recommendations - Is there a difference between the M ( eta ) NG and the A ( dversarial ) NG from Rusak et . al.2020 ? 6.Additional feedback - None as the paper is pretty well written .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your constructive comments . We respond to your main concerns below : 1 . I was missing an intuitive description of why the adversarial noise should improve robustness to adversarial attacks . I was only aware of it as a method to improve corruption robustness . - It is important to note that simply adding noise * * would not improve the robustness to adversarial attacks * * ( see our comparison with A ( dversarial ) NG below ) , MNG * * explicitly learns an optimal noise distribution * * to prevent overfitting and to promote the generalization across multiple perturbations . Additionally , adversarial noise acts as a noise regularization technique , which is a common technique to improve the generalization in deep neural networks . Further , we have added a separate paragraph to highlight the illustration of our training scheme in Section 4 of the revision of our paper . 2.Is there a difference between the M ( eta ) NG and the A ( dversarial ) NG from Rusak et al.2020 ? - It is important to note that A ( dversarial ) NG ( Rusak et al.20 ) learns the noise projected on $ \\ell_2 $ norm-ball to confuse the classifier , in contrast maximally , we * * meta-learn the noise distribution to compliment the generalization across multiple $ \\ell_p $ perturbations . * * Further , we show that * * A ( dversarial ) NG fails to defend against multiple adversarial perturbations * * below , which demonstrates the efficiency of M ( eta ) NG over A ( dversarial ) NG : | Dataset | Model | Acc $ _ { \\rm clean } $ | $ \\ell_\\infty $ | $ \\ell_1 $ | $ \\ell_2 $ | | | -- | : -- : | : - : | : : | : : | | CIFAR-10 | MNG-AC | 81.5+-0.3 | 42.2+-0.9 | 55.0+-1.2 | 71.5+-0.1 | | CIFAR-10 | ANG | 94.6+-0.0 | 0.1+-0.00 | 0.1+-0.0 | 2.9+-0.9 | | SVHN | MNG-AC | 93.7+-0.1 | 35.1+-1.9 | 47.4+-2.2 | 77.6+-1.0 | | SVHN | ANG | 96.8+-0.1 | 0.2+-0.0 | 7.3+-0.5 | 33.9+-1.5 | | Tiny-ImageNet | MNG-AC | 53.1+-0.3 | 27.4+-0.7 | 39.6+-0.7 | 44.8+-0.1 | | Tiny-ImageNet | ANG | 62.8+-0.3 | 0.2+-0.1 | 3.4+-0.4 | 13.4+-0.6 | 3 . Why the MNG was trained the way it is was a bit unclear for me . - Our objective was to learn optimal noise distribution that could explicitly minimize the loss of multiple adversarial perturbations and promote label consistency across multiple perturbations . A standard approach was to use a bilevel optimization to train the adversarial classifier with MNG . However , bilevel optimization for adversarial training was * * computationally costly . * * As a result , we adopted an alternative scheme where we first update the model parameters on the augmented samples for $ T $ steps , to explicitly increase the influence of the augmented samples . Then we perform a one-step lookahead to model the adaptaion of the adversarial classifier in the presence of augmented examples . Lastly , after receiving the feedback from the classifier , we update $ \\phi $ to explicitly minimize the adversarial loss to promote the adversarial robustness of the classifier in the next step . 4.I do n't work with adversarial examples I am not at all confident in that assessment . From the discussions with people who work on adversarial examples new defenses are usually broken very quickly and there is a number of papers which break numerous defenses . The method is however based on adversarial training which to my knowledge is the only robust method so far and the used attacks seem valid . So I am definitely leaning towards accept but the opinion of a real expert would be highly appreciated as I feel not at all qualified to assess the validity of papers on adversarial examples . - We understand your concern , and we would like to highlight that we have evaluated our proposed method on all the state-of-the-art attacks that exist in the literature . We believe that our evaluation can be a firm guideline when other researchers pursue the evaluation of defenses that are robust against multiple perturbations in the future . Further , as you rightly mentioned our defense is based on adversarial training , which is the only robust method that has withstood the stronger set of attacks ."}], "0": {"review_id": "tv8n52XbO4p-0", "review_text": "Summary = The authors propose a number of techniques to learn models which are adversarially robust to multiple perturbations . These involve a noise generator , a loss to enforce consistency , as well as a stochastic variant of adversarial training . With these changes , they are able to produce improvements to robust accuracy to multiple perturbation types . Overall , I get the idea and the empirical results seem promising . However , the structure and writing of the paper is at times rather confusing , and there are a lot of missing details . If the code were not supplied , it would be difficult in the current state to reproduce the method from the paper . Perhaps due to this , the specifics of the key component , the meta noise generator , are still rather opaque to me . Perhaps the authors can clarify , and I am happy to follow up afterwards . Comments for discussion == The majority of my confusion lies in section 4 , for the specifics of the meta noise generator and parts of the algorithm in general . I am otherwise well acquainted with the relevant literature . 1 ) Augmented examples ( x_aug ) are generated by adding noise from the MNG and projecting it onto some ball B . It is not clear to me what ball this is since the authors are considering multiple perturbations . Is it a random type ? Or a joint projection ? I assume it is at least one of the perturbations being considered , or is that incorrect ? 2 ) Similarly , in the algorithm , the authors generate adversarial examples ( x_adv ) by sampling a random attack . I could not find what set of attacks were being sampled from , or what the sampling distribution is ( I checked the appendix as well ) . 3 ) The generator is apparently updated to minimize the classifier loss on the adversarial examples as written in Equation ( 8 ) . However , the adversarial examples are generated from some unspecified set of attacks , which implies that the set of attacks actually depends on the generator somehow . Is this supposed to be the classifier loss on the augmented samples ? If not , then how do the adversarial examples depend on the generator ? 4 ) The consistency loss involves clean , adversarial , and augmented posterior distributions . There are no details on these distributions : are these simply the softmax of the logits ? Or is a generative model that outputs a distribution being used ? 5 ) On a more fundamental level , what is the motivation behind training the generator to minimize the classifier loss ? Why would we want to do this over random sampling ? What 's to prevent a degenerate solution of simply learning to produce a zero perturbation ( and thus always producing clean examples , which can achieve low loss ) ? Minor comments == I have checked the supplementary material and the authors have included the code for running their experiments . Ideally , this would also include pre-trained model weights . Update After much effort , I can say that I understand the paper . The edits appear to have incorporated all the identified missing information . I have thus updated my confidence and slightly improved my score , however I am not confident that the current presentation of the approach will be understandable by a reader without contacting the authors , given that the difficulty I had in understanding the paper ( and my initial confidence ) stemmed primarily from missing information and poor presentation for the approach . Although the results do seem to improve upon past work , its impact will suffer if it is difficult to understand for a non-reviewer reader . I would be more confident if a fresh set of eyes could understand the details of the work without having to go to the authors to clarify so many details .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We sincerely appreciate your constructive comments . We respond to your main concerns below : 1 . Augmented examples ( x_aug ) are generated by adding noise from the MNG and projecting it onto some ball B . It is not clear to me what ball this is since the authors are considering multiple perturbations . Is it a random type ? Or a joint projection ? I assume it is at least one of the perturbations being considered , or is that incorrect ? - $ \\mathcal { B } ( x , \\varepsilon ) $ refers to the * * norm-ball of the specific attack * * sampled by Stochastic Adversarial Training ( SAT ) . That is , if the sampled attack is an $ \\ell_2 $ attack , then $ \\mathcal { B } $ denotes the $ \\ell_2 $ norm ball , and if the sampled attack is an $ \\ell_1 $ attack , then $ \\mathcal { B } $ is the $ \\ell_1 $ norm ball . As MNG learns the noise to minimize the adversarial loss , it is essential to project the generated noise on the same norm-ball . We have clarified this point in the revision . 2.Similarly , in the algorithm , the authors generate adversarial examples ( x_adv ) by sampling a random attack . I could not find what set of attacks were being sampled from , or what the sampling distribution is ( I checked the appendix as well ) . - We apologize for the confusion . In this work , the sampling distribution corresponds to the * * $ \\ell_p $ -bounded perturbations . * * Still , it is important to note that unlike the average and max strategy , MNG + SAT can be applied to any distribution of attacks with a constant cost . We have clarified this point in the revision . 3.The generator is apparently updated to minimize the classifier loss on the adversarial examples as written in Equation ( 8 ) . However , the adversarial examples are generated from some unspecified set of attacks , which implies that the set of attacks actually depends on the generator somehow . Is this supposed to be the classifier loss on the augmented samples ? If not , then how do the adversarial examples depend on the generator ? - This is a critical misunderstanding . * * Adversarial examples do not depend on the generator ; * * instead , the one-step update in Eq . ( 8 ) is essential to do a lookahead for adapting the model parameters in the presence of the noise-augmented samples . Note that augmented samples are different from the adversarial examples ( please refer Figure 1 ) and our contribution is to optimally generate augmented examples to improve the robustness against multiple perturbations explicitly . 4.The consistency loss involves clean , adversarial , and augmented posterior distributions . There are no details on these distributions : are these simply the softmax of the logits ? Or is a generative model that outputs a distribution being used ? - As you mentioned , the distributions are the softmax of the logits of the clean , adversarial and augmented samples where the augmented samples are the output of the Meta Noise Generator ( MNG ) . We have incorporated this point in the revision . 5.What is the motivation behind training the generator to minimize the classifier loss ? Why would we want to do this over random sampling ? What 's to prevent a degenerate solution of simply learning to produce a zero perturbation ( and thus always producing clean examples , which can achieve low loss ) ? - Firstly , we would like to clarify that we * * do not train the generator to minimize the classifier loss ; * * instead , the generator learns an optimal noise distribution in a meta-learning training scheme to * * minimize the adversarial classification loss * * where adversarial classification loss is the loss on the sampled attack from the distribution of attacks . - Secondly , the motivation behind training the generator to minimize this objective is to explicitly learn the noise distribution essential for generalization across multiple perturbations , that might not necessarily correspond to any of the attack perturbations . Furthermore , our algorithm to improve the generalization across multiple perturbations is also motivated by the popular phenomenon of noise regularization being a common technique to improve the generalization performance of deep neural networks . In contrast , even though random sampling helps in generalization , it leads to a suboptimal solution ( see Table 2 ) . - Lastly , our meta-learning training scheme prevents the degenerate solution , as producing clean examples would not result in a lower loss on multiple adversarial perturbations . 6.I have checked the supplementary material , and the authors have included the code for running their experiments . Ideally , this would also include pre-trained model weights . - Thank you for pointing this out . Due to the size limit of the supplementary material , we could not provide the pre-trained models . We provide the pre-trained model weights here : https : //drive.google.com/file/d/1kVfOZ2CrhSzgzlS6gK4AntNZhIUosvfz/view ? usp=sharing"}, "1": {"review_id": "tv8n52XbO4p-1", "review_text": "This paper addresses a timely issue in adversarial robustness - efficient training of robust models against multiple adversarial perturbations . The authors propose a combination of three techniques : stochastic adversarial training ( SAT ) , meta noise generator ( MNG ) , and adversarial consistency ( AC ) loss for efficient training , and evaluate the robustness using multiple L1 , L2 , and Linf norm-bounded attacks and three datasets ( CIFAR-10 , SVHN , and Tiny Imagenet ) . The results show improved multi-attack robustness over several baselines ( including single-attack and multiple-attack models ) and reduced training time . Ablation studies are also performed to illustrate the utility of each component of the proposed model . Overall , this paper provides very detailed evaluations involving multiple datasets , attacks , baselines , and robustness metrics . I find the results convincing and important , and also find sufficient novelty in the proposed training method . The strengths ( S ) and weaknesses ( W ) of this submission are summarized below . S1.The proposal of MNG and AC is effective and novel . S2.The evaluation is thorough and convincing . S3.The proposal improves both robustness and training efficiency in most cases . W1.The adversarial consistency ( AC ) loss is never defined explicitly . Based on equation ( 5 ) , it is hard to understand how AC `` represents the Jensen-Shannon Divergence ( JSD ) among the posterior distributions '' when considering three distributions , P_clean , P_adv , and P_aug . More clarification is needed . W2.Although the results show improved multi-attack robustness , it will be great if the authors can add more intuition on why the proposed training method leads to performance improvement . Based on the ablation study , it seems that the role of SAT and MNG is to reduce overfitting in robustness to encourage generalization , rather than optimization over the worst-case scenarios . W3.The considered multi-attack setting is still limited to different Lp norm perturbation constraints . Although the authors showed improved robustness over unforeseen attacks , the authors should also discuss how the proposed method can generalize to different attacks beyond Lp norms .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your constructive comments . We respond to your main concerns below : 1 ) The adversarial consistency ( AC ) loss is never defined explicitly . - We apologize for the confusion . We have updated the revision with the explicit definition of the Adversarial Consistency ( AC ) loss in Equation 6 . 2 ) Although the results show improved multi-attack robustness , it will be great if the authors can add more intuition on why the proposed training method leads to performance improvement . Based on the ablation study , it seems that the role of SAT and MNG is to reduce overfitting in robustness to encourage generalization , rather than optimization over the worst-case scenarios . - As you mentioned , SAT and MNG indeed play a critical role to reduce overfitting in robustness to encourage generalization . Intuitively , MNG acts as a * * noise regularization technique , * * and SAT promotes generalization across multiple perturbations * * due to its stochasticity . * * Further , we have added a separate paragraph to highlight the illustration of our training scheme in Section 4 of the revision of our paper . - Additionally , we would like to clarify that unlike the max strategy , MNG and SAT * * do not optimize over the worst-case scenarios . * * MNG learns an * * input-dependent optimal noise distribution to lower adversarial error across all the perturbations * * that does not necessarily correspond to any of the attack perturbations . 3 ) The considered multi-attack setting is still limited to different Lp norm perturbation constraints . Although the authors showed improved robustness over unforeseen attacks , the authors should also discuss how the proposed method can generalize to different attacks beyond Lp norms . - We agree that the evaluation of attacks beyond $ \\ell_p $ norms is interesting , and we would like to point out that the unforeseen adversaries consist of Elastic attack and JPEG attacks which * * do not belong to the standard family of $ \\ell_p $ attacks . * *"}, "2": {"review_id": "tv8n52XbO4p-2", "review_text": "In this paper , the authors propose a novel meta-learning framework that explicitly learns to generate noise to improve model robustness ( against multiple types of attacks ) . The results indicate that the proposed approach improves on the state-of-the-art . Overall , the paper is well written . However some details are missing and this could make the paper hard to reproduce . The experiments could be expanded . 1 ) There is a significant amount of work about using generative models to build adversarial examples . The literature review only focuses on classical adversarial robustness and robustness against multiple adversaries . I 'd recommend making a review of these approaches , even if they are orthogonal to the one proposed in this paper ( e.g. , [ 1,2,3 ] ) 2 ) In Eq . ( 6 ) , what is \\mathcal { B } ( x , \\epsilon ) . Since there is multiple threat models , I am assuming that it is selected at random between l_1 , l_2 and l_inf ( like SAT ) . 3 ) The number of inner steps T seems to be critical ( as it will trade-off gradient precision with compute ) . However , I do n't see any study on this in the paper . Also , it is not clear which value was used for the experiments . 4 ) Looking at Eq . ( 7 ) , it seems like backpropagation through the T inner steps is necessary to compute the gradients w.r.t.\\phi.This seems overly expensive and I find surprising that adv_avg and adv_max take so much longer to train . 5 ) Concerning Eq . ( 7 ) , as a curiousity , have authors considered implicit differentiation [ 4 ] ? 6 ) The experiments are run using 30 epochs which is rather on slim side . E.g. , RST_inf should reach about 59 % robust accuracy with 200 epochs of training ( with 30 epochs it only reaches 55 % ) . I 'm curious as to whether the comparison with the proposed approach is unfair ( e.g. , Adv_inf sees a single adv example per batch , whereas MNG-AC sees 2 ) . 7 ) It 's not entirely clear to me why beta negatively affects l_2 robustness . I 'd assume that if the model was only trained against l_2 , then there might be an optimal value for beta that is different that the one from Fig.2.In general , it would be interesting to see on MNG-AC does if different subsets of threats are used . 8 ) The l_2 loss landscapes seem more noisy that what they should be . Also it 's unclear why the axes are centered for l_inf and not for l_2 ( explain how these are generated ) . 9 ) In Table 5 , MNG-AC achieves 35.1 % against all l_inf attacks , but only 33.7 % against AutoAttack . Am I missing something ? Details : A ) It would helpful to the reader to have the epsilon values written on top of the different tables . The captions could be expanded to include more details . B ) Visuaization - > Visualization [ 1 ] https : //openreview.net/pdf ? id=SJeQEp4YDH : GAT : Generative Adversarial Training for Adversarial Example Detection and Robust Classification [ 2 ] https : //arxiv.org/pdf/1801.02610 : Generating Adversarial Examples with Adversarial Networks [ 3 ] https : //arxiv.org/pdf/1710.10766 : PixelDefend : Leveraging Generative Models to Understand and Defend against Adversarial Examples [ 4 ] https : //arxiv.org/pdf/1911.02590 : Optimizing Millions of Hyperparameters by Implicit Differentiation", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your constructive comments . We respond to your main concerns below : 1 . I 'd recommend making a review of generative models to build adversarial examples , even if they are orthogonal to the one proposed in this paper ( e.g. , [ 1,2,3 ] ) - Thank you for the helpful suggestion . We have provided a detailed review of generative models for adversarial robustness in the revision . 2.In Eq . ( 6 ) , what is $ \\mathcal { B } ( x , \\epsilon ) $ . Since there are multiple threat models , I am assuming that it is selected at random between l_1 , l_2 and l_inf ( like SAT ) . - As you rightly mentioned , $ \\mathcal { B } ( x , \\varepsilon ) $ refers to a * * random norm-ball ( like SAT ) * * . That is , if the sampled attack is an $ \\ell_2 $ attack , then $ \\mathcal { B } $ denotes the $ \\ell_2 $ norm ball , and if the sampled attack is an $ \\ell_1 $ attack , then $ \\mathcal { B } $ is the $ \\ell_1 $ norm ball . Still , it is important to note that $ \\mathcal { B } ( x , \\varepsilon ) $ is the norm-ball of the attack sampled by Stochastic Adversarial Training ( SAT ) , as MNG learns the noise to minimize the adversarial loss , it is essential to project the generated noise on the same norm-ball . We have clarified this in the revision . 3.The number of inner steps T seems to be critical . However , I do n't see any study on this in the paper . It is not clear which value was used for the experiments . - We used $ T=2 $ for all our experiments to keep the training cost minimum . We empirically found that larger values of T do not provide a significant increase in the robustness while leading to a significant increase in the training cost . We provide a comparison with different values of $ T $ below : | Model | $ \\ell_\\infty $ | $ \\ell_1 $ | $ \\ell_2 $ | Time ( h ) | | | | -- | -- |- | | $ T = 1 $ | 41.5+-0.8 | 55.1+-0.9 | 71.8+-0.2 | 9.4 | | $ T = 2 $ | 42.2+-0.9 | 55.0+-1.2 | 71.5+-0.1 | 11.2 | | $ T = 4 $ | 42.4+-0.8 | 55.6+-1.1 | 71.0+-0.2 | 14.6 | | $ T = 8 $ | 42.6+-1.0 | 55.3+-1.2 | 71.0+-0.1 | 18.9 | 4 . The experiments are run using 30 epochs which is rather on slim side . E.g. , RST_inf should reach about 59 % robust accuracy with 200 epochs of training ( with 30 epochs it only reaches 55 % ) . I 'm curious as to whether the comparison with the proposed approach is unfair ( e.g. , Adv_inf sees a single adv example per batch , whereas MNG-AC sees 2 ) . - It is essential to note that RST uses ~5 million data points for CIFAR-10 and SVHN , and it took us 4 days with four GeForce RTX 2080Ti to train with 30 epochs . Since it takes * * more than 24 days * * to finish RST with 200 epochs , we did not evaluate it . Furthermore , we would like to clarify that MNG-AC does not see 2 examples per batch , the lookahead in Equation 8. occurs with a meta-model , and the classifier update occurs only once . ( Please see Line 393 in train_MNG.py in our code for more details ) . 5.It 's not entirely clear to me why beta negatively affects l_2 robustness . In general , it would be interesting to see what MNG-AC does if different subsets of threats are used . - We would like to clarify that it is not a general statement that beta negatively affects $ \\ell_2 $ robustness , instead $ \\beta $ controls the trade-off between multiple perturbations . We will do our best to get the results for different subsets of threats by the end of the rebuttal deadline . 6.The l_2 loss landscapes seem more noisy that what they should be . Also it 's unclear why the axes are centred for l_inf and not for l_2 ( explain how these are generated ) . - We apologize for the confusion . The axes are centred for all the $ \\ell_p $ norms ; we will further clean the plots in the revision . To generate these plots , we vary the input along a linear space defined by the $ \\ell_p $ norm of the gradient where x and y-axes represent the perturbation added in each direction , and the z-axis represents the loss . 7.In Table 5 , MNG-AC achieves 35.1 % against all l_inf attacks , but only 33.7 % against AutoAttack . Am I missing something ? - Thank you for pointing this out . We have fixed this discrepancy in the revision . 8.Concerning Eq . ( 7 ) , as a curiosity , have authors considered implicit differentiation [ 4 ] ? - Thank you for suggesting relevant work . However , due to the * * high computational cost of hypergradients , * * we did not evaluate this direction of work . We will add a reference to this work in the final draft , and a comparison with it should be an interesting problem for future work . 9.The caption could be expanded to include epsilon values . B ) Visuaization - > Visualization - Thank you for the suggestions , we have updated the caption and typo in the revision . Additionally , to promote reproducibility of our work , we provide the pre-trained model weights here : https : //drive.google.com/file/d/1kVfOZ2CrhSzgzlS6gK4AntNZhIUosvfz/view ? usp=sharing"}, "3": {"review_id": "tv8n52XbO4p-3", "review_text": "1.Summary The authors propose a new method to improve robustness to adversarial examples under various norms ( L1 , L2 and LInf ) . Their method combines adversarial training with an adversarial noise generator . They improve upon adversarial training in a multi norm setting by choosing one norm at random for each sample , instead of computing an adversarial for all norms , thus significantly reducing the training time . They additionally improve robustness by regularizing model features between the standard image , the adversarially perturbed image and a perturbation of the image created with an adversarial noise generator . 2.Strengths + The method is based on adversarial training . As far as I know and as the authors note this is the only method that reliably leads to more robust models . + The authors attack their models with a range of attacks that to the best of my knowledge are state-of-the art . + The method apparently works in the multi norm setting . 3.Weaknesses - I was missing an intuitive description why the adversarial noise should improve robustness to adversarial attacks . I was only aware of it as a method to improve corruption robustness . - I was not always sure if I got everything correctly in sections 4 and 5.3 . I think I got it but I sometimes missed a figure . It may e.g.be helpful to include the losses in Figure 1 or make a separate figure . Especially why the MNG was trained the way it is was a bit unclear for me . 4.Recommendation I think this paper is an accept but as I do n't work with adversarial examples I am not at all confident in that assessment . From the discussions with people who work on adversarial examples new defenses are usually broken very quickly and there is a number of papers which break numerous defenses . The method is however based on adversarial training which to my knowledge is the only robust method so far and the used attacks seem valid . So I am definitely leaning towards accept but the opinion of a real expert would be highly appreciated as I feel not at all qualified to assess the validity of papers on adversarial examples . 5.Questions/Recommendations - Is there a difference between the M ( eta ) NG and the A ( dversarial ) NG from Rusak et . al.2020 ? 6.Additional feedback - None as the paper is pretty well written .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely appreciate your constructive comments . We respond to your main concerns below : 1 . I was missing an intuitive description of why the adversarial noise should improve robustness to adversarial attacks . I was only aware of it as a method to improve corruption robustness . - It is important to note that simply adding noise * * would not improve the robustness to adversarial attacks * * ( see our comparison with A ( dversarial ) NG below ) , MNG * * explicitly learns an optimal noise distribution * * to prevent overfitting and to promote the generalization across multiple perturbations . Additionally , adversarial noise acts as a noise regularization technique , which is a common technique to improve the generalization in deep neural networks . Further , we have added a separate paragraph to highlight the illustration of our training scheme in Section 4 of the revision of our paper . 2.Is there a difference between the M ( eta ) NG and the A ( dversarial ) NG from Rusak et al.2020 ? - It is important to note that A ( dversarial ) NG ( Rusak et al.20 ) learns the noise projected on $ \\ell_2 $ norm-ball to confuse the classifier , in contrast maximally , we * * meta-learn the noise distribution to compliment the generalization across multiple $ \\ell_p $ perturbations . * * Further , we show that * * A ( dversarial ) NG fails to defend against multiple adversarial perturbations * * below , which demonstrates the efficiency of M ( eta ) NG over A ( dversarial ) NG : | Dataset | Model | Acc $ _ { \\rm clean } $ | $ \\ell_\\infty $ | $ \\ell_1 $ | $ \\ell_2 $ | | | -- | : -- : | : - : | : : | : : | | CIFAR-10 | MNG-AC | 81.5+-0.3 | 42.2+-0.9 | 55.0+-1.2 | 71.5+-0.1 | | CIFAR-10 | ANG | 94.6+-0.0 | 0.1+-0.00 | 0.1+-0.0 | 2.9+-0.9 | | SVHN | MNG-AC | 93.7+-0.1 | 35.1+-1.9 | 47.4+-2.2 | 77.6+-1.0 | | SVHN | ANG | 96.8+-0.1 | 0.2+-0.0 | 7.3+-0.5 | 33.9+-1.5 | | Tiny-ImageNet | MNG-AC | 53.1+-0.3 | 27.4+-0.7 | 39.6+-0.7 | 44.8+-0.1 | | Tiny-ImageNet | ANG | 62.8+-0.3 | 0.2+-0.1 | 3.4+-0.4 | 13.4+-0.6 | 3 . Why the MNG was trained the way it is was a bit unclear for me . - Our objective was to learn optimal noise distribution that could explicitly minimize the loss of multiple adversarial perturbations and promote label consistency across multiple perturbations . A standard approach was to use a bilevel optimization to train the adversarial classifier with MNG . However , bilevel optimization for adversarial training was * * computationally costly . * * As a result , we adopted an alternative scheme where we first update the model parameters on the augmented samples for $ T $ steps , to explicitly increase the influence of the augmented samples . Then we perform a one-step lookahead to model the adaptaion of the adversarial classifier in the presence of augmented examples . Lastly , after receiving the feedback from the classifier , we update $ \\phi $ to explicitly minimize the adversarial loss to promote the adversarial robustness of the classifier in the next step . 4.I do n't work with adversarial examples I am not at all confident in that assessment . From the discussions with people who work on adversarial examples new defenses are usually broken very quickly and there is a number of papers which break numerous defenses . The method is however based on adversarial training which to my knowledge is the only robust method so far and the used attacks seem valid . So I am definitely leaning towards accept but the opinion of a real expert would be highly appreciated as I feel not at all qualified to assess the validity of papers on adversarial examples . - We understand your concern , and we would like to highlight that we have evaluated our proposed method on all the state-of-the-art attacks that exist in the literature . We believe that our evaluation can be a firm guideline when other researchers pursue the evaluation of defenses that are robust against multiple perturbations in the future . Further , as you rightly mentioned our defense is based on adversarial training , which is the only robust method that has withstood the stronger set of attacks ."}}