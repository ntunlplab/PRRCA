{"year": "2019", "forum": "ByeNFoRcK7", "title": "PA-GAN: Improving GAN Training by Progressive Augmentation", "decision": "Reject", "meta_review": "The submission hypothesizes that in typical GAN training the discriminator is too strong, too fast, and thus suggests a modification by which they gradually increases the task difficulty of the discriminator. This is done by introducing (effectively) a new random variable -- which has an effect on the label -- and which prevents the discriminator from solving its task too quickly. \n\nThere was a healthy amount of back-and-forth between the authors and the reviewers which allowed for a number of important clarifications to be made (esp. with regards to proofs, comparison with baselines, etc). My judgment of this paper is that it provides a neat way to overcome a particular difficulty of training GANs, but that there is a lot of confusion about the similarities (of lack thereof) with various potentially simpler alternatives such as input dropout, adding noise to the input etc. I was sometimes confused by the author response as well (they at once suggest that the proposed method reduces overfitting of the discriminator but also state that \"We believe our method does not even try to \u201cregularize\u201d the discriminator\"). Because of all this, the significance of this work is unclear and thus I do not recommend acceptance.", "reviews": [{"review_id": "ByeNFoRcK7-0", "review_text": "This paper proposes a new trick to improve the stability of GANs. In particular the authors try to tackle the vanishing gradient problem in GANs, when the discriminator becomes to strong and is able to perfectly separate the distribution early in training, resulting in almost zero gradient for the generator. The authors propose to increase the difficulty of the task during training to avoid the discriminator to become too strong. The paper is quite well written and clear. However there is several unsupported claims (see below). A lot of work has been proposed to regularize the discriminator, it's not clear how different this approach is to adding noise to the input or adding dropout to the discriminator. Pros: - The experimental section is quite thorough and the results seem overall good. - The paper is quite clear. Cons: - There is a major mistake in the derivation of the proposed method. In eq. (6) & (7), (c) is not an equivalence, minimizing the KL divergence is not the same as minimizing the Jensen-Shannon divergence. The only thing we have is that: KL(p||q) = 0 <=> JSD(p||q) = 0 <=> p=q . The same kind of mistake is made for (d). Note that the KL-divergence can also be approximated with a GAN see [1]. Since the equivalence between (6) and (7) doesn't hold, the equation (11) doesn't hold either. - The authors say that the discriminator can detect the class of a sample by using checksum, the checksum is quite easy for a neural networks to learn so I don't really see how the method proposed actually increase the difficulty of the task for the discriminator. Especially if the last layer of the discriminator learns to perform a checksum, and the discriminator architecture has residual connections, then it should be straight-forward for the discriminator to solve the new task given it can already solve the previous task. So I'm not sure the method would still works if we use ResNet architecture for the discriminator. - I believe the approach is really similar to adding noise to the input. I think the method should be compared to this kind of baseline. Indeed the method seems almost equivalent to resetting some of the weights of the first layer of the discriminator when the discriminator becomes too strong, so I think it should also be compared to other regularization such as dropout noise on the discriminator. - The authors claim that their method doesn't \"just memorize the true data distribution\". It's not clear to me why this should be the case and this is neither shown theoretically or empirically. I encourage the author to think about some way to support this claim. - The authors states that \"adding high-dimensional noise introduces significant variance in the parameter estimation, which slows down training\", can the author give some references to support that statement ? - According to the author: \"Regularizing the discriminator with the gradient penalty depends on the model distribution, which changes during training and thus results in increased runtime\". While I agree that computing the gradient penalty slightly increase the runtime because we need to compute some second order derivatives, I don't see how these increase of runtime is due to change in the model distribution. The authors should clarify what they mean. Others: - It would be very interesting to study when does the level number increase and what happens when it increase ? Also what is the final number of level at the end of training ? Conclusion: The idea has some major flaws that need to be fixed. I believe the idea has similar effect to adding dropout on the first layer of the discriminator. I don't think the paper should be accepted unless those major concerns are resolved. References: [1] Nowozin, S., Cseke, B., & Tomioka, R. (2016). f-gan: Training generative neural samplers using variational divergence minimization. NIPS", "rating": "5: Marginally below acceptance threshold", "reply_text": "Apart from the questions that have been addressed in our general response ( A-C ) , here are answers to remaining questions of R1 : 1 ) `` adding high-dimensional noise introduces significant variance in the parameter estimation , which slows down training '' , can the author give some references to support that statement ? We would like to refer R1 to [ Roth et al.NIPS \u2019 17 ] .This work argues that \u201c high-dimensional noise introduces significant variance in the parameter estimation process \u201c , \u201c counteracting this requires a lot of samples and therefore ultimately leads to a costly or impractical solution \u201c and that \u201c explicitly adding noise in high-dimensional ambient spaces introduces additional sampling variance \u201d , which leads to the increase of the overall training time . 2 ) According to the author : `` Regularizing the discriminator with the gradient penalty depends on the model distribution , which changes during training and thus results in increased runtime '' . I do n't see how these increase of runtime is due to change in the model distribution . The authors should clarify what they mean . Here we meant that there are two drawbacks of employing the gradient penalty [ Kurach et al.2018 ] .First , it can depend on the model distribution , which changes during training . Second , computing the gradient norms results in increased runtime . 3 ) The authors claim that their method does n't `` just memorize the true data distribution '' . It 's not clear to me why this should be the case and this is neither shown theoretically or empirically . I encourage the author to think about some way to support this claim . Our aim was to provide the intuition to the reader why we think employing PA result in better FID and inception scores , which are known to correspond to the variation of generated images . We believe that structurally augmenting the input sample space and mapping it to higher dimensions encourages the generator to explore various paths towards the true data distribution , leading to the improved variation of generated images , which we observe in the improved FID and IS metrics . We thank R1 for pointing out the confusing statements mentioned above , we adjusted those statements in the in the revision to improve the overall clarity ."}, {"review_id": "ByeNFoRcK7-1", "review_text": "This paper modifies the GAN objective by defining the TRUE and FAKE labels in terms of both the training sample, and a newly introduced random variable s. The intuition is that by progressively changing the definition of s, and its effect on the label, we can prevent the discriminator network from immediately learning to separate the two classes. The paper doesn't give any strong theoretical support for this intuition. And it I found it a bit surprising that the discriminator doesn't immediately learn the one extra bit of information introduced by every new level of augmentation. However, the results do seem to show that this augmentation has a beneficial effect on two different architectures in different data scenarios, although the increase is not uniform over all settings. The approach presented in this paper is motivated primarily as a method of increasing stability of training but this is not directly investigated. Figure 3 and Table 2 both suggest that the augmentation does nothing to reduce variance between runs. There is also no direct comparison to other methods of weakening the discriminator, although these are mentioned in the related work. I think the paper would be much improved by a thorough investigation of the method's effect on training stability, to go along with the current set of evaluations.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank R3 for the reviewing effort and comments that allowed us to improve the quality of our work . We have addressed R3 's questions in our general response ( A-C ) ."}, {"review_id": "ByeNFoRcK7-2", "review_text": "Authors argue that the main issue with stability in GANs is due to the discriminator becoming too powerful too quickly. To address this issue they propose to make the task progressively more difficult: Instead of providing only the samples to the discriminator, an additional (processed) bitstring is provided. The idea is that the bitstring in combination with the sample determines whether the sample should be considered true or fake. This in turn requires the decision boundary of the discriminator to become more complicated for increasing lengths of the bitstring. In a limited set of experiments the authors show that the proposed approach can improve the FID scores. Pro: - A simple idea to make the problem progressively more difficult. - The writing is relatively easy to follow. - Standardized experimental setup. Con: - Ablation study of the training tricks is missing: (1) How does the proposed approach perform when no progressive scheduling is used? (2) How does it perform without the linear model for increasing p? (3) How does the learning rate of G impact the quality? Does one need all of these tricks? Arguably, if one includes the FID/KID to modify the learning rates in the competing approaches, one could find a good setup which yields improved results. This is my major issue with this approach. - Clarity can be improved: several pages of theory can really be summarized into \u201clearning the joint distribution implies that the marginals are also correctly learned\u2019 (similar to ALI/BIGAN). This would leave much more space to perform necessary ablation studies. - Comparison to [1] is missing: In that model, it seems that the same effect can be achieved and strongly improves the FID. Namely, they introduce a model in which observed samples pass through a \"lens\" before being revealed to the discriminator thus balancing the generator and discriminator by gradually revealing more detailed features. - Can you provide more convincing arguments that the strength of the discriminator is a major factor we should be fixing? In some approaches such as Wasserstein GAN, we should train the discriminator to optimality in each round. Why is the proposed approach more practical then approaches such as [2]? [1] http://proceedings.mlr.press/v80/sajjadi18a.html [2] https://arxiv.org/abs/1706.08500", "rating": "5: Marginally below acceptance threshold", "reply_text": "Apart from the questions that have been addressed in our general response ( A-C ) , here are answers to R2 's remaining questions : 1 ) Several pages of theory can really be summarized into \u201c learning the joint distribution implies that the marginals are also correctly learned \u2019 ( similar to ALI/BIGAN ) . From the perspective of joint distribution matching , ALI/BiGAN constructs two joint distributions which marginals are with respect to the data and model distribution . Therefore , when two joint distributions are mutually matched , the model distribution approaches the data distribution . In our case , as mathematically shown in Appendix A.1 , the two joint distributions have identical marginals by construction . Upon a completely different line , we prove its equivalence to the original problem of GAN . Furthermore , ALI/BIGAN aim at generative latent modeling , whereas we aim at stabilizing the training process of GAN ( generative modeling without the latent code ) . The augmentation bit sequence s in our case is not a latent code of the input image . The generator does not take s as its input to generate the synthetic data . A successful training of ALI/BIGAN requires x and z being mutually dependent , e.g. , its variant ALICE enforcing the dependence through conditional entropy . We are in the opposite situation , namely , x and s being mutually independent implies a perfect generator . 2 ) \u201c Can you provide more convincing arguments that the strength of the discriminator is a major factor we should be fixing ? In some approaches such as Wasserstein GAN , we should train the discriminator to optimality in each round. \u201d We first would like refer the reviewer to works of [ Arjovsky & Bottou ICLR \u2019 17 and S\u00f8nderby et al.ICLR \u2019 17 ] .In these works , the authors provide the explanation of the fundamental problem of instability of GANs and explain why additional techniques are required to weaken the discriminator . In particular , the problem is that the support of the real data distribution and the generative model distribution are often non-overlapping ( in image modelling , distribution of natural images is often assumed to be concentrated on or around a lower-dimensional manifold ) . In such situations , the divergences which GANs are minimizing become meaningless ( the Jensen-Shannon divergence is saturated so its maximum value ) , and the discriminator is extremely prone to overfitting , which can lead to instabilities . One of the ways to avoid this behavior is to weaken the discriminator by making its job harder , which we successfully achieve by performing PA . Recent work address the problem of weakening the discriminator in two ways , which are orthogonal to our approach , either by regularizing the discriminator via different variations of the gradient penalty [ Gulrajani et al.2017 , Roth et al.2017 , Fedus et al.2018 ] or altering directly the data samples [ Arjovsky & Bottou 2017 , S\u00f8nderby et al.2017 , Sajjadi et al.2018 ] .The Wasserstein distance belongs to the family of integral probability metrics , which are well defined in contrast to f-divergences [ Arjovsky & Bottou 2017 ] . Thus , [ Arjovsky et al.2017 ] advises to train the discriminator to optimality . However , in WGAN the class of discriminators is restricted to Lipschitz continuous functions , which yields a hard constraint on the function class that is empirically hard to satisfy . Moreover , [ Mescheder et al.2018 ] show that WGANs ( and WGAN-GPs ) do not converge as in practice the discriminator is trained with a fixed number of discriminator updates per generator update and thus the discriminator optimality is not guaranteed . Therefore , preserving a healthy competition between the generator and discriminator remains challenging ( the escalation of signal magnitudes in the generator and discriminator is still observed in practice [ Karras et al.2018 ] ) .PA can serve as a regularization technique in this case . Empirically we show that employing PA with WGAN and WGAN-GP leads to better performance . 3 ) Why is the proposed approach more practical then approaches such as https : //arxiv.org/abs/1706.08500 Different learning rates of the generator and discriminator ( or different number of updates ) introduce an additional hyper-parameter which requires a careful tuning to achieve a performance improvement . In contrast , our progressive augmentation is done automatically by a simple threshold test of the KID score . We would like to point out , that it is possible to combine both approaches , which potentially might be benefiting to each other . We consider the comparison/combination with the two time-scale update rule as part of the future work . We thank R2 for the comments and suggestions that allowed us to greatly improve the quality of the work ."}], "0": {"review_id": "ByeNFoRcK7-0", "review_text": "This paper proposes a new trick to improve the stability of GANs. In particular the authors try to tackle the vanishing gradient problem in GANs, when the discriminator becomes to strong and is able to perfectly separate the distribution early in training, resulting in almost zero gradient for the generator. The authors propose to increase the difficulty of the task during training to avoid the discriminator to become too strong. The paper is quite well written and clear. However there is several unsupported claims (see below). A lot of work has been proposed to regularize the discriminator, it's not clear how different this approach is to adding noise to the input or adding dropout to the discriminator. Pros: - The experimental section is quite thorough and the results seem overall good. - The paper is quite clear. Cons: - There is a major mistake in the derivation of the proposed method. In eq. (6) & (7), (c) is not an equivalence, minimizing the KL divergence is not the same as minimizing the Jensen-Shannon divergence. The only thing we have is that: KL(p||q) = 0 <=> JSD(p||q) = 0 <=> p=q . The same kind of mistake is made for (d). Note that the KL-divergence can also be approximated with a GAN see [1]. Since the equivalence between (6) and (7) doesn't hold, the equation (11) doesn't hold either. - The authors say that the discriminator can detect the class of a sample by using checksum, the checksum is quite easy for a neural networks to learn so I don't really see how the method proposed actually increase the difficulty of the task for the discriminator. Especially if the last layer of the discriminator learns to perform a checksum, and the discriminator architecture has residual connections, then it should be straight-forward for the discriminator to solve the new task given it can already solve the previous task. So I'm not sure the method would still works if we use ResNet architecture for the discriminator. - I believe the approach is really similar to adding noise to the input. I think the method should be compared to this kind of baseline. Indeed the method seems almost equivalent to resetting some of the weights of the first layer of the discriminator when the discriminator becomes too strong, so I think it should also be compared to other regularization such as dropout noise on the discriminator. - The authors claim that their method doesn't \"just memorize the true data distribution\". It's not clear to me why this should be the case and this is neither shown theoretically or empirically. I encourage the author to think about some way to support this claim. - The authors states that \"adding high-dimensional noise introduces significant variance in the parameter estimation, which slows down training\", can the author give some references to support that statement ? - According to the author: \"Regularizing the discriminator with the gradient penalty depends on the model distribution, which changes during training and thus results in increased runtime\". While I agree that computing the gradient penalty slightly increase the runtime because we need to compute some second order derivatives, I don't see how these increase of runtime is due to change in the model distribution. The authors should clarify what they mean. Others: - It would be very interesting to study when does the level number increase and what happens when it increase ? Also what is the final number of level at the end of training ? Conclusion: The idea has some major flaws that need to be fixed. I believe the idea has similar effect to adding dropout on the first layer of the discriminator. I don't think the paper should be accepted unless those major concerns are resolved. References: [1] Nowozin, S., Cseke, B., & Tomioka, R. (2016). f-gan: Training generative neural samplers using variational divergence minimization. NIPS", "rating": "5: Marginally below acceptance threshold", "reply_text": "Apart from the questions that have been addressed in our general response ( A-C ) , here are answers to remaining questions of R1 : 1 ) `` adding high-dimensional noise introduces significant variance in the parameter estimation , which slows down training '' , can the author give some references to support that statement ? We would like to refer R1 to [ Roth et al.NIPS \u2019 17 ] .This work argues that \u201c high-dimensional noise introduces significant variance in the parameter estimation process \u201c , \u201c counteracting this requires a lot of samples and therefore ultimately leads to a costly or impractical solution \u201c and that \u201c explicitly adding noise in high-dimensional ambient spaces introduces additional sampling variance \u201d , which leads to the increase of the overall training time . 2 ) According to the author : `` Regularizing the discriminator with the gradient penalty depends on the model distribution , which changes during training and thus results in increased runtime '' . I do n't see how these increase of runtime is due to change in the model distribution . The authors should clarify what they mean . Here we meant that there are two drawbacks of employing the gradient penalty [ Kurach et al.2018 ] .First , it can depend on the model distribution , which changes during training . Second , computing the gradient norms results in increased runtime . 3 ) The authors claim that their method does n't `` just memorize the true data distribution '' . It 's not clear to me why this should be the case and this is neither shown theoretically or empirically . I encourage the author to think about some way to support this claim . Our aim was to provide the intuition to the reader why we think employing PA result in better FID and inception scores , which are known to correspond to the variation of generated images . We believe that structurally augmenting the input sample space and mapping it to higher dimensions encourages the generator to explore various paths towards the true data distribution , leading to the improved variation of generated images , which we observe in the improved FID and IS metrics . We thank R1 for pointing out the confusing statements mentioned above , we adjusted those statements in the in the revision to improve the overall clarity ."}, "1": {"review_id": "ByeNFoRcK7-1", "review_text": "This paper modifies the GAN objective by defining the TRUE and FAKE labels in terms of both the training sample, and a newly introduced random variable s. The intuition is that by progressively changing the definition of s, and its effect on the label, we can prevent the discriminator network from immediately learning to separate the two classes. The paper doesn't give any strong theoretical support for this intuition. And it I found it a bit surprising that the discriminator doesn't immediately learn the one extra bit of information introduced by every new level of augmentation. However, the results do seem to show that this augmentation has a beneficial effect on two different architectures in different data scenarios, although the increase is not uniform over all settings. The approach presented in this paper is motivated primarily as a method of increasing stability of training but this is not directly investigated. Figure 3 and Table 2 both suggest that the augmentation does nothing to reduce variance between runs. There is also no direct comparison to other methods of weakening the discriminator, although these are mentioned in the related work. I think the paper would be much improved by a thorough investigation of the method's effect on training stability, to go along with the current set of evaluations.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank R3 for the reviewing effort and comments that allowed us to improve the quality of our work . We have addressed R3 's questions in our general response ( A-C ) ."}, "2": {"review_id": "ByeNFoRcK7-2", "review_text": "Authors argue that the main issue with stability in GANs is due to the discriminator becoming too powerful too quickly. To address this issue they propose to make the task progressively more difficult: Instead of providing only the samples to the discriminator, an additional (processed) bitstring is provided. The idea is that the bitstring in combination with the sample determines whether the sample should be considered true or fake. This in turn requires the decision boundary of the discriminator to become more complicated for increasing lengths of the bitstring. In a limited set of experiments the authors show that the proposed approach can improve the FID scores. Pro: - A simple idea to make the problem progressively more difficult. - The writing is relatively easy to follow. - Standardized experimental setup. Con: - Ablation study of the training tricks is missing: (1) How does the proposed approach perform when no progressive scheduling is used? (2) How does it perform without the linear model for increasing p? (3) How does the learning rate of G impact the quality? Does one need all of these tricks? Arguably, if one includes the FID/KID to modify the learning rates in the competing approaches, one could find a good setup which yields improved results. This is my major issue with this approach. - Clarity can be improved: several pages of theory can really be summarized into \u201clearning the joint distribution implies that the marginals are also correctly learned\u2019 (similar to ALI/BIGAN). This would leave much more space to perform necessary ablation studies. - Comparison to [1] is missing: In that model, it seems that the same effect can be achieved and strongly improves the FID. Namely, they introduce a model in which observed samples pass through a \"lens\" before being revealed to the discriminator thus balancing the generator and discriminator by gradually revealing more detailed features. - Can you provide more convincing arguments that the strength of the discriminator is a major factor we should be fixing? In some approaches such as Wasserstein GAN, we should train the discriminator to optimality in each round. Why is the proposed approach more practical then approaches such as [2]? [1] http://proceedings.mlr.press/v80/sajjadi18a.html [2] https://arxiv.org/abs/1706.08500", "rating": "5: Marginally below acceptance threshold", "reply_text": "Apart from the questions that have been addressed in our general response ( A-C ) , here are answers to R2 's remaining questions : 1 ) Several pages of theory can really be summarized into \u201c learning the joint distribution implies that the marginals are also correctly learned \u2019 ( similar to ALI/BIGAN ) . From the perspective of joint distribution matching , ALI/BiGAN constructs two joint distributions which marginals are with respect to the data and model distribution . Therefore , when two joint distributions are mutually matched , the model distribution approaches the data distribution . In our case , as mathematically shown in Appendix A.1 , the two joint distributions have identical marginals by construction . Upon a completely different line , we prove its equivalence to the original problem of GAN . Furthermore , ALI/BIGAN aim at generative latent modeling , whereas we aim at stabilizing the training process of GAN ( generative modeling without the latent code ) . The augmentation bit sequence s in our case is not a latent code of the input image . The generator does not take s as its input to generate the synthetic data . A successful training of ALI/BIGAN requires x and z being mutually dependent , e.g. , its variant ALICE enforcing the dependence through conditional entropy . We are in the opposite situation , namely , x and s being mutually independent implies a perfect generator . 2 ) \u201c Can you provide more convincing arguments that the strength of the discriminator is a major factor we should be fixing ? In some approaches such as Wasserstein GAN , we should train the discriminator to optimality in each round. \u201d We first would like refer the reviewer to works of [ Arjovsky & Bottou ICLR \u2019 17 and S\u00f8nderby et al.ICLR \u2019 17 ] .In these works , the authors provide the explanation of the fundamental problem of instability of GANs and explain why additional techniques are required to weaken the discriminator . In particular , the problem is that the support of the real data distribution and the generative model distribution are often non-overlapping ( in image modelling , distribution of natural images is often assumed to be concentrated on or around a lower-dimensional manifold ) . In such situations , the divergences which GANs are minimizing become meaningless ( the Jensen-Shannon divergence is saturated so its maximum value ) , and the discriminator is extremely prone to overfitting , which can lead to instabilities . One of the ways to avoid this behavior is to weaken the discriminator by making its job harder , which we successfully achieve by performing PA . Recent work address the problem of weakening the discriminator in two ways , which are orthogonal to our approach , either by regularizing the discriminator via different variations of the gradient penalty [ Gulrajani et al.2017 , Roth et al.2017 , Fedus et al.2018 ] or altering directly the data samples [ Arjovsky & Bottou 2017 , S\u00f8nderby et al.2017 , Sajjadi et al.2018 ] .The Wasserstein distance belongs to the family of integral probability metrics , which are well defined in contrast to f-divergences [ Arjovsky & Bottou 2017 ] . Thus , [ Arjovsky et al.2017 ] advises to train the discriminator to optimality . However , in WGAN the class of discriminators is restricted to Lipschitz continuous functions , which yields a hard constraint on the function class that is empirically hard to satisfy . Moreover , [ Mescheder et al.2018 ] show that WGANs ( and WGAN-GPs ) do not converge as in practice the discriminator is trained with a fixed number of discriminator updates per generator update and thus the discriminator optimality is not guaranteed . Therefore , preserving a healthy competition between the generator and discriminator remains challenging ( the escalation of signal magnitudes in the generator and discriminator is still observed in practice [ Karras et al.2018 ] ) .PA can serve as a regularization technique in this case . Empirically we show that employing PA with WGAN and WGAN-GP leads to better performance . 3 ) Why is the proposed approach more practical then approaches such as https : //arxiv.org/abs/1706.08500 Different learning rates of the generator and discriminator ( or different number of updates ) introduce an additional hyper-parameter which requires a careful tuning to achieve a performance improvement . In contrast , our progressive augmentation is done automatically by a simple threshold test of the KID score . We would like to point out , that it is possible to combine both approaches , which potentially might be benefiting to each other . We consider the comparison/combination with the two time-scale update rule as part of the future work . We thank R2 for the comments and suggestions that allowed us to greatly improve the quality of the work ."}}