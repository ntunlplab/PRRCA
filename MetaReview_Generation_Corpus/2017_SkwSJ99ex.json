{"year": "2017", "forum": "SkwSJ99ex", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "decision": "Reject", "meta_review": "The proposed method doesn't have enough novelty to be accepted to ICLR.", "reviews": [{"review_id": "SkwSJ99ex-0", "review_text": "One of the main idea of this paper is to replace pooling layers with convolutions of stride 2 and retraining the model. Authors merge this into a new layer and brand it as a new type of layer. This is very misleading and adding noise to the field. And using strided convolutions rather than pooling is not actually novel (e.g. https://arxiv.org/abs/1605.02346). While the speed-up obtained are good, the lack of novelty and the rebranding attempt make this paper not a good fit for ICLR.", "rating": "4: Ok but not good enough - rejection", "reply_text": "The reviewer missed the key points in our paper . There exist clear and significant differences between our work and previous works . We are not `` replacing a pooling layer to a convolution layer of stride 2 '' which was in fact proposed in another paper `` \u201c STRIVING FOR SIMPLICITY : THE ALL CONVOLUTIONAL NET \u201d . Instead , our method merges various types of non-tensor layers ( i.e. , pooling , LRN etc . ) and their bottom convolution layer to a new convolution layer ( i.e. , the `` rebirth '' layer ) . This is how we achieve significantly improved efficiency and accelerated model execution . We view this process as `` rebirth '' of a new layer instead of `` a new * type * of layer '' as an analogy to help readers ' catch up the idea and understand the merit of work . In addition , from the reference ( https : //arxiv.org/abs/1605.02346 ) , we did not find the similar operation as our algorithm which optimizes multiple layers with a new layer . As indicted in Section 4 ( tasks and datasets section in reference ) , \u201c First , a shallow CNN which consists of six layers each followed by a rectified linear unit [ 32 ] and Batch Normalization [ 33 ] . The first four layers include max pooling with stride 2 , leading to an effective stride of 16 , \u201d the method in reference relied on traditional network structure without applying the optimization schemes as ours . Regarding the novelty , to the best of our knowledge , this is * first * one that leverages merging of different types of layers ( including * non-tensor * layers ) for acceleration of CNN models on mobile devices . This is also the * best * method ( see Table below ) reported in accelerating the CNN on mobile devices . It has technical novelty , and truly works pretty well in reality . If anyone has questions , we are very happy to do competitions on mobile devices . Giving all the above considerations , we are very disappointed to see the reviewer viewing our work as `` misleading and adding noise to the field. ``"}, {"review_id": "SkwSJ99ex-1", "review_text": "This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model. The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model. Reducing model sizes and speeding up model evaluation are important in many applications. I have several concerns: 1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them. 2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. 3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy. BTW, the DNN low-rank approximation technique was first proposed in speech recognition. e.g., Xue, J., Li, J. and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369). ", "rating": "4: Ok but not good enough - rejection", "reply_text": "( 1 ) There are many techniques that can reduce model sizes . For example , it has been shown by several groups that using the teacher-student approach , people can achieve the same and sometimes even better accuracy than the teacher ( big model ) using a much smaller model . However , this paper does not compare any one of them . Ans : We agree that there are many different kinds of model compression techniques . However , these algorithms mainly focus on reducing the model size instead of improving the running speed , especially , the acceleration of the execution time of deep model on mobile devices is not evident . We already compared with several state-of-the-art works in the paper . As listed in Table 5 , in terms of running speed , runtime memory , energy , our approach performs significant better than GoogleNet-Tucker ( ICLR'16 ) , Squeezenet ( https : //openreview.net/forum ? id=S1xh5sYgx ) and others . The details of comparison are listed as follows : -- -- -- -- -- -- -- -- -- -- -- -| -- -- -DeepRebirth -- -- -| -- -- SqueezeNet -- | -- Tucker ( ICLR'16 ) * -- -- | -- GoogleNet -- -| Acc | 86.5 % | 80.3 % | 85.7 % | 88.9 % | Execution time | 65.34 ms | 75.34 ms | 342.5 ms | 424.7 ms | Energy | 226 mJ | 288 mJ | 902 mJ | 984 mJ | Memory size | 14.8 MB | 36.5 MB | 35.8 MB | 33.2 MB | Parameter size | 11.99 MB | 4.72 MB | 14.38 MB | 26.72 MB | * The number is based on our implementation of the algorithm in ICLR'16 paper . ( 2 ) The technique proposed in this paper is limited in its applicability since it 's designed specifically for the models discussed in the paper . Ans : Our proposed approach can be applied to current general deep neural network structures that consist of different layers , e.g. , convolution , pooling , LRN , etc . The reason why we used GoogleNet , Resnet and Alexnet is due to their popularity , as evident in current model compression work , e.g , Song Han . et al . [ ICLR \u2019 16 ] , Yong-Deok Kim et al . [ ICLR \u2019 16 ] . ( 3 ) Replacing several layers with single layer is a relatively standard procedure . For example , the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy . Ans : As far as we know , only mean variance normalization layer and batch normalization layer can all be absorbed without retraining , however , the operation can be only applied to two-layers and needs specific structure \u2013 normalization layer right after convolution . To address these limitations , we proposed a general approach that can be applied to pooling , LRN , batchnorm and convolution , etc . ( 4 ) BTW , the DNN low-rank approximation technique was first proposed in speech recognition . Ans : Thanks for the comment . We have already included this reference in revision ."}, {"review_id": "SkwSJ99ex-2", "review_text": "This paper looks at the idea of fusing multiple layers (typically a convolution and a LRN or pooling layer) into a single convolution via retraining of just that layer, and shows that simpler, faster models can be constructed that way at minimal loss in accuracy. This idea is fine. Several issues: - The paper introduces the concept of a 'Deeprebirth layer', and for a while it seems like it's going to be some new architecture. Mid-way, we discover that 1) it's just a convolution 2) it's actually a different kind of convolution depending on whether one fuses serial or parallel pooling layers. I understand the desire to give a name to the technique, but in this case naming the layer itself, when it's actually multiple things, non of which are new architecturally, confuses the argument a lot. - There are ways to perform this kind of operator fusion without retraining, and some deep learning framework such as Theano and the upcoming TensorFlow XLA implement them. It would have been nice to have a baseline that implements it, especially since most of the additional energy cost from non-fused operators comes from the extra intermediate memory writes that operator fusion removes. - Batchnorm can be folded into convolution layers without retraining by scaling the weights. Were they folded into the baseline figures reported in Table 7? - At the time of writing, the authors have not provided the details that would make this research reproducible, in particular how the depth of the fused layers relates to the depth of the original layers in each of the experiments. - Retraining: how much time (epochs) does the retraining take? Did you consider using any form of distillation? Interesting set of experiments. This paper needs a lot of improvements to be suitable for publication. - Open-sourcing: having the implementation be open-source always enhances the usefulness of such paper. Not a requirement obviously. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "( 1 ) I understand the desire to give a name to the technique , but in this case naming the layer itself , when it 's actually multiple things , non of which are new architecturally , confuses the argument a lot . Ans : Well , we name our approach as \u201c deep-rebirth \u201d because in our proposed speed optimization pipeline , we have replaced multiple deep network layers which run much slower compared to the newly generated and faster layer ( e.g. , convolution layer in our experiment ) like a rebirth . ( 2 ) There are ways to perform this kind of operator fusion without retraining , and some deep learning framework such as Theano and the upcoming TensorFlow XLA implement them . It would have been nice to have a baseline that implements it , especially since most of the additional energy cost from non-fused operators comes from the extra intermediate memory writes that operator fusion removes . Ans : As far as we know , the operator fusion without retraining can only be applied to some specific structure , i.e. , batch normalization or mean normalization after convolution . This kind of operator fusion can \u2019 t be applied to other kinds of layers , such as pooling , LRN , etc . Our pipeline is much more general . The deep learning framework such as Theano and XLA performs the operator fusion to reduce the memory . However , the execution time of these models may not be accelerated due to the use of same architectures ( i.e. , performing exactly the same set of floating-point operations ) . If we view them as the baseline , their execution time would be much \u201c higher \u201d than deep-rebirth . In contrast , our method re-trained the rebirth layers and significantly speeds up the execution time . ( 3 ) Batchnorm can be folded into convolution layers without retraining by scaling the weights . Were they folded into the baseline figures reported in Table 7 ? Ans : We agree that batchnorm can be folded into convolution layer without retraining by scaling the weights . The results listed in Table 7 also include other layers \u2019 optimization , e.g. , pooling , not limited to batchnorm . ( 4 ) At the time of writing , the authors have not provided the details that would make this research reproducible , in particular how the depth of the fused layers relates to the depth of the original layers in each of the experiments . Open-sourcing : having the implementation be open-source always enhances the usefulness of such paper . Not a requirement obviously . Ans : We have listed the details in our revision . We plan to release the code after final decision . Our first step for open-source would be the release of the model architectures ( in caffe \u2019 s prototxt format ) used in our paper . ( 5 ) Retraining : how much time ( epochs ) does the retraining take ? Did you consider using any form of distillation ? Ans : Each step of our optimization pipeline takes around 2 epochs ( in section 4.1.1 ) . For GoogleNet , it takes 9 steps and 18 epochs . We regard this training time ( or rebirth time ) as a one-time cost and once trained it can be deployed to any mobile devices . Our \u201c rebirth \u201d method can be considered as a special form of distillation that transfers the knowledge from the cumbersome substructure of multiple layers to the rebirthed accelerated substructure . Different from traditional distillation method , our approach adopts layer-wise optimization and maintains the knowledge of the rest layers . Also , our method utilizes a softmax at the end of our CNN to optimize classification accuracy . ( 6 ) Interesting set of experiments . This paper needs a lot of improvements to be suitable for publication . Ans : Thanks for your appreciation of our paper . We are constantly improving the quality of our paper ."}], "0": {"review_id": "SkwSJ99ex-0", "review_text": "One of the main idea of this paper is to replace pooling layers with convolutions of stride 2 and retraining the model. Authors merge this into a new layer and brand it as a new type of layer. This is very misleading and adding noise to the field. And using strided convolutions rather than pooling is not actually novel (e.g. https://arxiv.org/abs/1605.02346). While the speed-up obtained are good, the lack of novelty and the rebranding attempt make this paper not a good fit for ICLR.", "rating": "4: Ok but not good enough - rejection", "reply_text": "The reviewer missed the key points in our paper . There exist clear and significant differences between our work and previous works . We are not `` replacing a pooling layer to a convolution layer of stride 2 '' which was in fact proposed in another paper `` \u201c STRIVING FOR SIMPLICITY : THE ALL CONVOLUTIONAL NET \u201d . Instead , our method merges various types of non-tensor layers ( i.e. , pooling , LRN etc . ) and their bottom convolution layer to a new convolution layer ( i.e. , the `` rebirth '' layer ) . This is how we achieve significantly improved efficiency and accelerated model execution . We view this process as `` rebirth '' of a new layer instead of `` a new * type * of layer '' as an analogy to help readers ' catch up the idea and understand the merit of work . In addition , from the reference ( https : //arxiv.org/abs/1605.02346 ) , we did not find the similar operation as our algorithm which optimizes multiple layers with a new layer . As indicted in Section 4 ( tasks and datasets section in reference ) , \u201c First , a shallow CNN which consists of six layers each followed by a rectified linear unit [ 32 ] and Batch Normalization [ 33 ] . The first four layers include max pooling with stride 2 , leading to an effective stride of 16 , \u201d the method in reference relied on traditional network structure without applying the optimization schemes as ours . Regarding the novelty , to the best of our knowledge , this is * first * one that leverages merging of different types of layers ( including * non-tensor * layers ) for acceleration of CNN models on mobile devices . This is also the * best * method ( see Table below ) reported in accelerating the CNN on mobile devices . It has technical novelty , and truly works pretty well in reality . If anyone has questions , we are very happy to do competitions on mobile devices . Giving all the above considerations , we are very disappointed to see the reviewer viewing our work as `` misleading and adding noise to the field. ``"}, "1": {"review_id": "SkwSJ99ex-1", "review_text": "This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model. The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model. Reducing model sizes and speeding up model evaluation are important in many applications. I have several concerns: 1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them. 2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. 3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy. BTW, the DNN low-rank approximation technique was first proposed in speech recognition. e.g., Xue, J., Li, J. and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369). ", "rating": "4: Ok but not good enough - rejection", "reply_text": "( 1 ) There are many techniques that can reduce model sizes . For example , it has been shown by several groups that using the teacher-student approach , people can achieve the same and sometimes even better accuracy than the teacher ( big model ) using a much smaller model . However , this paper does not compare any one of them . Ans : We agree that there are many different kinds of model compression techniques . However , these algorithms mainly focus on reducing the model size instead of improving the running speed , especially , the acceleration of the execution time of deep model on mobile devices is not evident . We already compared with several state-of-the-art works in the paper . As listed in Table 5 , in terms of running speed , runtime memory , energy , our approach performs significant better than GoogleNet-Tucker ( ICLR'16 ) , Squeezenet ( https : //openreview.net/forum ? id=S1xh5sYgx ) and others . The details of comparison are listed as follows : -- -- -- -- -- -- -- -- -- -- -- -| -- -- -DeepRebirth -- -- -| -- -- SqueezeNet -- | -- Tucker ( ICLR'16 ) * -- -- | -- GoogleNet -- -| Acc | 86.5 % | 80.3 % | 85.7 % | 88.9 % | Execution time | 65.34 ms | 75.34 ms | 342.5 ms | 424.7 ms | Energy | 226 mJ | 288 mJ | 902 mJ | 984 mJ | Memory size | 14.8 MB | 36.5 MB | 35.8 MB | 33.2 MB | Parameter size | 11.99 MB | 4.72 MB | 14.38 MB | 26.72 MB | * The number is based on our implementation of the algorithm in ICLR'16 paper . ( 2 ) The technique proposed in this paper is limited in its applicability since it 's designed specifically for the models discussed in the paper . Ans : Our proposed approach can be applied to current general deep neural network structures that consist of different layers , e.g. , convolution , pooling , LRN , etc . The reason why we used GoogleNet , Resnet and Alexnet is due to their popularity , as evident in current model compression work , e.g , Song Han . et al . [ ICLR \u2019 16 ] , Yong-Deok Kim et al . [ ICLR \u2019 16 ] . ( 3 ) Replacing several layers with single layer is a relatively standard procedure . For example , the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy . Ans : As far as we know , only mean variance normalization layer and batch normalization layer can all be absorbed without retraining , however , the operation can be only applied to two-layers and needs specific structure \u2013 normalization layer right after convolution . To address these limitations , we proposed a general approach that can be applied to pooling , LRN , batchnorm and convolution , etc . ( 4 ) BTW , the DNN low-rank approximation technique was first proposed in speech recognition . Ans : Thanks for the comment . We have already included this reference in revision ."}, "2": {"review_id": "SkwSJ99ex-2", "review_text": "This paper looks at the idea of fusing multiple layers (typically a convolution and a LRN or pooling layer) into a single convolution via retraining of just that layer, and shows that simpler, faster models can be constructed that way at minimal loss in accuracy. This idea is fine. Several issues: - The paper introduces the concept of a 'Deeprebirth layer', and for a while it seems like it's going to be some new architecture. Mid-way, we discover that 1) it's just a convolution 2) it's actually a different kind of convolution depending on whether one fuses serial or parallel pooling layers. I understand the desire to give a name to the technique, but in this case naming the layer itself, when it's actually multiple things, non of which are new architecturally, confuses the argument a lot. - There are ways to perform this kind of operator fusion without retraining, and some deep learning framework such as Theano and the upcoming TensorFlow XLA implement them. It would have been nice to have a baseline that implements it, especially since most of the additional energy cost from non-fused operators comes from the extra intermediate memory writes that operator fusion removes. - Batchnorm can be folded into convolution layers without retraining by scaling the weights. Were they folded into the baseline figures reported in Table 7? - At the time of writing, the authors have not provided the details that would make this research reproducible, in particular how the depth of the fused layers relates to the depth of the original layers in each of the experiments. - Retraining: how much time (epochs) does the retraining take? Did you consider using any form of distillation? Interesting set of experiments. This paper needs a lot of improvements to be suitable for publication. - Open-sourcing: having the implementation be open-source always enhances the usefulness of such paper. Not a requirement obviously. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "( 1 ) I understand the desire to give a name to the technique , but in this case naming the layer itself , when it 's actually multiple things , non of which are new architecturally , confuses the argument a lot . Ans : Well , we name our approach as \u201c deep-rebirth \u201d because in our proposed speed optimization pipeline , we have replaced multiple deep network layers which run much slower compared to the newly generated and faster layer ( e.g. , convolution layer in our experiment ) like a rebirth . ( 2 ) There are ways to perform this kind of operator fusion without retraining , and some deep learning framework such as Theano and the upcoming TensorFlow XLA implement them . It would have been nice to have a baseline that implements it , especially since most of the additional energy cost from non-fused operators comes from the extra intermediate memory writes that operator fusion removes . Ans : As far as we know , the operator fusion without retraining can only be applied to some specific structure , i.e. , batch normalization or mean normalization after convolution . This kind of operator fusion can \u2019 t be applied to other kinds of layers , such as pooling , LRN , etc . Our pipeline is much more general . The deep learning framework such as Theano and XLA performs the operator fusion to reduce the memory . However , the execution time of these models may not be accelerated due to the use of same architectures ( i.e. , performing exactly the same set of floating-point operations ) . If we view them as the baseline , their execution time would be much \u201c higher \u201d than deep-rebirth . In contrast , our method re-trained the rebirth layers and significantly speeds up the execution time . ( 3 ) Batchnorm can be folded into convolution layers without retraining by scaling the weights . Were they folded into the baseline figures reported in Table 7 ? Ans : We agree that batchnorm can be folded into convolution layer without retraining by scaling the weights . The results listed in Table 7 also include other layers \u2019 optimization , e.g. , pooling , not limited to batchnorm . ( 4 ) At the time of writing , the authors have not provided the details that would make this research reproducible , in particular how the depth of the fused layers relates to the depth of the original layers in each of the experiments . Open-sourcing : having the implementation be open-source always enhances the usefulness of such paper . Not a requirement obviously . Ans : We have listed the details in our revision . We plan to release the code after final decision . Our first step for open-source would be the release of the model architectures ( in caffe \u2019 s prototxt format ) used in our paper . ( 5 ) Retraining : how much time ( epochs ) does the retraining take ? Did you consider using any form of distillation ? Ans : Each step of our optimization pipeline takes around 2 epochs ( in section 4.1.1 ) . For GoogleNet , it takes 9 steps and 18 epochs . We regard this training time ( or rebirth time ) as a one-time cost and once trained it can be deployed to any mobile devices . Our \u201c rebirth \u201d method can be considered as a special form of distillation that transfers the knowledge from the cumbersome substructure of multiple layers to the rebirthed accelerated substructure . Different from traditional distillation method , our approach adopts layer-wise optimization and maintains the knowledge of the rest layers . Also , our method utilizes a softmax at the end of our CNN to optimize classification accuracy . ( 6 ) Interesting set of experiments . This paper needs a lot of improvements to be suitable for publication . Ans : Thanks for your appreciation of our paper . We are constantly improving the quality of our paper ."}}