{"year": "2020", "forum": "BkxUvnEYDH", "title": "Program Guided Agent", "decision": "Accept (Spotlight)", "meta_review": "This paper provides a fascinating hybridization approach to incorporating programs as priors over policies which are then refined using deep RL. The reviewers were, at the end of the discussion, all in favour of acceptance (with the majority strongly in favour). An excellent paper I hope to see included in the conference.", "reviews": [{"review_id": "BkxUvnEYDH-0", "review_text": "This paper presents a reinforcement learning agent that learns to execute tasks specified in a form of programs with an architecture consisting of three modules. The (fixed) interpreter module interprets the program, by issuing queries to a (pre-trained) vision module and giving goals to a policy module that executes them in the environment. The paper also introduces a policy modulation technique, with the goal of modulating the current state with the expected (symbolic) goal. The model is evaluated on a 2D approximation of Minecraft, where it outperforms a set of baselines. In addition, the authors modified the program dataset and re-expressed it in terms of natural language and showed that the baselines perform better with programs than with the natural language instructions. Though I think the general idea of the paper is worth exploring, I find the concrete contribution of the paper a bit thin to the point that I am hesitant to recommend this paper for acceptance. Allow me to explain my objections: First and foremost, this work is very close to work by Denil et al where the execute (in a differentiable fashion) programs in an RL agent. Their work does not have a discrete interpreter per-se, but it does have a differentiable execution of commands. The major difference between these two works would be that Denil et al do not have the vision module (they do mention learning from pixels as future work). However, that is not entirely true. The model presented here uses a pretrained vision module, which by itself is not a problem and is used in related work [1], but this vision module does not operate on visual input but the symbolic representation of the map. A crucial thing here is that if all of a sudden we want to include a new object on the map, the model won\u2019t be able to use some learned similarity since it would require introducing a new object (slice of the input), as it would should it have been trained on pixels. So technically speaking, this is not a vision module but a symbolic state processing module. Then, the modulation mentioned in the paper does not seem particularly novel. Sure, the exact architecture of the model is probably unique, but the idea of modulating a state with a goal is not and has been seen in other work such as [2] and [3] among others. The paper does not mention why, for example, this modulation technique is useful and why any other similar architecture would not be as successful, nor does it mention related modulation techniques in other work. A big issue I have with the evaluation in the paper is that I do not see the benefit of having the experiments with natural language at all. The focal point of the paper are agents able to execute tasks in the form of programs. The (though manually) generated natural language instructions from those same programs cannot even be used by the proposed agent as there is no natural language interpreter, so they are a dangling part of the paper which is there just to showcase that programs should be easier for baselines to learn to execute than natural language (the hypothesis would be that a simpler and more structured/formal language is easier to learn than the natural language). Hence the seq-LSTM results on language which are a tad lower than the results on programs are expected, though the performance of transformers is the opposite---they are better on language, and that is something that is puzzling and left unexplained, as well as the unexpectedly low performance of Tree-RNNs. One would expect them to perform a bit better than LSTMs, but that might be contingent on the size of the dataset more than the structure of the inputs. However, none of these curious findings have been explained. Moreover, the comparison to baselines is not particularly fair as these baselines had to learn the symbolic state interpretation, whereas your model did not. You could have provided the same to the baselines for a better comparison. In addition to that, 5.4.2 goes into detail of analysing the baselines, and ignoring the proposed model. Why didn\u2019t you include the same statistics for your agent in Figure 5 and said subsubsection? The paper is missing some notable related work: - S.R.K. Branavan\u2019s work (all but one cited language-instructed agent papers are post 2017) as well as [4] - object-oriented and hierarchical RL - [5], where they train the neural programmer interpreter from the final reward only, which brings NPI close to this work Questions: - Figure 5 - There\u2019s a mark for Tree-RNN, but Tree-RNNs are not in any figure. Why didn\u2019t you plot the performance of your model? - The setup naturally induces a curriculum - how does it do that if the programs are randomly sampled? - You say that your model learns to comprehending the program flow. I\u2019m not sure I would agree with that because what your model learns is to execute single commands. From what it seems, the interpreter is the only part of the model (which is fixed btw) which sees the control flow, whereas the policy just executes singular commands. Did you mean something else by that statement? Minor issues: - You say twice interpreter (i.e. compiler). Given that they\u2019re not the same, and you\u2019re using an interpreter, I suggest omitting the word compiler. - Figure 2 is lacking detail. There is no difference between r and i (both being positive integers) other than their descriptions, - operators agent[_], env and is_there lack parameters (non-terminal nodes), and where\u2019s river, bridge, etc? [1] COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration [2] FeUdal Networks for Hierarchical Reinforcement Learning [3] Universal value function approximators [4] Vogel et al Learning to follow navigational directions [5] Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction", "rating": "6: Weak Accept", "reply_text": "Q : 5.4.2 goes into detail of analysing the baselines , and ignoring the proposed model . Why didn \u2019 t you include the same statistics for your agent in Figure 5 and said subsubsection ? A : The performance drop of evaluating our agent on the programs that are longer or have a higher number of control flows is less significant than the drop shown on the end-to-end learning baselines . We believe this is because our framework utilizes a rule-based parser ( i.e.the program interpreter ) which is not affected by longer or more complex tasks . Therefore , to analyze the cause of failures , we provided failure analyses in Section E.5.1 and Section E.5.2 detailing the failure rates and the average time cost of each subtask . Q : The paper is missing some notable related work : - S.R.K . Branavan \u2019 s work ( all but one cited language-instructed agent papers are post 2017 ) as well as [ 4 ] - object-oriented and hierarchical RL - [ 5 ] , where they train the neural programmer interpreter from the final reward only , which brings NPI close to this work A : Thanks for pointing out these relevant works . We have incorporated these works and some of their follow-up works in the revision . Q : Figure 5 - There \u2019 s a mark for Tree-RNN , but Tree-RNNs are not in any figure . Why didn \u2019 t you plot the performance of your model ? A : Figure 5 presents all the combinations of models { Seq-LSTM , Transformer , Tree-RNN } and task representations { programs , natural language instructions } . While Seq-LSTM and Transformer can be applied to programs and natural language instructions , Tree-RNN requires the input representation with a tree structure and therefore is only evaluated on programs . The bottom-left subfigures of Figure 5 ( a ) and 5 ( b ) in the original paper both show the performance of Tree-RNN in dark blue . However , the dark red color was reserved for Tree-RNN applied to natural language instructions , which is not evaluated and therefore is not shown in this figure . We have removed the dark red color from the legend and we are sorry for the confusion . Q : The setup naturally induces a curriculum - how does it do that if the programs are randomly sampled ? A : We did not explicitly set up a curriculum ; instead , we always randomly sampled programs regardless of their difficulty . We found that this setup naturally induces a curriculum . At the beginning of the training , the policy first learns to solve programs that require a less number of subtasks but fails to complete the harder programs . Yet , completing simpler programs and obtaining the task completion reward still allows the model to obtain a better understanding of the subtasks , which eventually allows the policy to complete more complex programs . We have revised the paper to make it clear that we did not explicitly set up a curriculum . Q : You say that your model learns to comprehending the program flow . I \u2019 m not sure I would agree with that because what your model learns is to execute single commands . From what it seems , the interpreter is the only part of the model ( which is fixed btw ) which sees the control flow , whereas the policy just executes singular commands . Did you mean something else by that statement ? A : We appreciate the comment . We have replaced the term \u201c comprehend \u201d with \u201c execute \u201d to make it is clear that our framework does not learn to read programs but utilizing a rule-based parser ( i.e.the program interpreter ) . Q : You say twice interpreter ( i.e.compiler ) . Given that they \u2019 re not the same , and you \u2019 re using an interpreter , I suggest omitting the word compiler . A : Thanks for the suggestion . We were intended to give an intuition when writing the paper but you are absolutely correct that they are not the same . We have removed it from the revised paper . Q : Figure 2 is lacking detail . There is no difference between r and i ( both being positive integers ) other than their descriptions , - operators agent [ _ ] , env and is_there lack parameters ( non-terminal nodes ) , and where \u2019 s river , bridge , etc ? A : We have fixed the DSL according to the suggestion . Specifically , we have made the following changes : ( 1 ) $ r $ and $ i $ were merged into $ i $ , which represents a constant , and ( 2 ) we added terrain ( $ u $ ) to represent different types of terrains ."}, {"review_id": "BkxUvnEYDH-1", "review_text": "This paper provides a method for instructing an agent using programs as input instructions, so that the agent should learn to contextually execute this program in a specified environment, learning to generalise as needed from perception, and to satisfy concerns that in the language of planning would be called monitoring and execution. The authors provide a method that breaks this problem down into one of interpreting the program (which is crafted separately as a compiler that benefits from a DSL), learning to identify contextual features and then adapting and applying the policy. The arguments in this paper are well made but the paper would benefit from better clarifying several points: 1. To start at the very beginning, the authors begin in the first page by giving the impression that the agent has gone directly from an NL instruction and otherwise uninterpreted sensory input to a solution, in the spirit of typical end to end systems, whereas what the authors are proposing is a very different and more pragmatic approach wherein the interpretation of the task is handled prior to learning, so that learning is only applied to smaller subproblems. This could be made clearer in the introduction. 2. In particular, it was unclear how the DSL comes about and what restrictions it places on the problem. The DSL will clearly have an influence because a very different task from MineCraft, say, robot manipulation, would have quite different needs of sensor-driven control and hence the information flows (specifically, the separation between goal identification, context perception and motion execution) would be different. What one puts into the DSL will significantly influence how well the overall framework performs (e.g., the ability to crisply ask is_there is powerful). Have the authors systematically explored this axis of design? Can we hear more in the setup about this? 3. The influence of the domain is once again seen in the modulation mechanism for the goal and the way in which affine transformations enable generalisation. This is of course sensible in 2D spatial navigation but may be less straight forward in other decision making contexts. The authors have been clear enough about what they have done, but I would have found it interesting to understand how much we should expect this particular concept to stretch and where its limitations become more apparent - perhaps in the discussion. Overall, this is good work and the writing is clear with suitable references. I would note that the authors are revisiting concerns well studied in the planning literature. While the authors do acknowledge HAMs and so on from the HRL literature, they'd make the paper stronger by also tracing the roots of some of these ideas into the rest of planning. I'll end this review by asking about the relationships between NL instructions and the formal programs. In some domains, the number of realisable programs that map to an ambiguous NL instruction can be large. Equally, not all lay users can write good programs. So, it is worth noting this gap and making clear that this paper does not really address this bit of the overall problem.", "rating": "8: Accept", "reply_text": "Q : Overall , this is good work and the writing is clear with suitable references . I would note that the authors are revisiting concerns well studied in the planning literature . While the authors do acknowledge HAMs and so on from the HRL literature , they 'd make the paper stronger by also tracing the roots of some of these ideas into the rest of planning . A : Classical symbolic planning ( Ghallab et al. , 2004 ) considers a planning problem contains an initial state , a goal , and a set of operators where the agent should act accordingly . Each operator consists of the name of the operator ( and necessary arguments ) , a precondition for this operator , and the effect on the environment if this operator is executed . As a whole , our framework shares many similarities to classical symbolic planning . Our policy ( action module ) which conditions on a symbolically represented goal is similar to an operator , as both the policy and an operator interacts with the environment to fulfill the given task . On the other hand , perceptions ( conditions ) in our DSL and preconditions both require perceiving environments ( e.g.the presence/absence of objects ) . However , perceptions differ from preconditions in that perceptions determine which branches of a program should be taken while preconditions determine if an operator/action/skill is applicable . Q : I 'll end this review by asking about the relationships between NL instructions and the formal programs . In some domains , the number of realisable programs that map to an ambiguous NL instruction can be large . Equally , not all lay users can write good programs . So , it is worth noting this gap and making clear that this paper does not really address this bit of the overall problem . A : As pointed out by the reviewer , there is a trade-off between the accessibility to the instructions ( NL or programs ) and the difficulty of learning from them . In this work , we are mainly interested in advocating programs as a good alternative representation for instructing an agent especially when the tasks are diverse ( i.e.more branches ) . We also agree that the accessibility of natural languages is enjoyable . It is worth noting there has been growing interest in synthesizing programs from natural language ( Lin et al. , 2018 ; Raza et al. , 2015 ; Desai et al. , 2016 ) and other representations ( e.g.images , strings , videos/execution traces . etc . ) .We would like to believe that the difficulty of accessing programs could be alleviated . On the other hand , semantic parsing has been an active research area to bridge unstructured languages and structural formal languages ( Wang et al. , 2017 ) ."}, {"review_id": "BkxUvnEYDH-2", "review_text": "Update: I thank the reviewers for their extensive rebuttal and revision of the paper addressing all of my concerns. I have increased my score. Summary This paper investigates an important direction: How can RL agents make use of high-level instructions and task decompositions formalized as programs? The authors propose a model for a program guided agent that, conditioned on a program, interprets the program, executes it to query a perception module and subsequently proposes subgoals to a low-level action module. The method outperforms LSTM and Transformer baselines on a Minecraft-like task and generalizes to programs larger than the one seen during training. Strengths Contribution in the important direction of training RL agents with instructions and prior knowledge, here in the form of programs Clearly written paper with good illustrations of the model Good performance on generalization task of acting in environments where the programmatic instructions are longer than those seen during training Weaknesses One of the contributions of the paper is a modulation mechanism (Section 4.3) on the state features that incorporates a goal-conditioned policy. However, a very related approach has been proposed by Bahdanau, Dzmitry, et al. \"Learning to Understand Goal Specifications by Modelling Reward.\" ICLR 2019. They introduced FILM layers that modulate the layers in a ConvNet conditioned on a goal representation. This should be discussed and compared to in the paper. I am surprised there is no comparison to other work that conditions on programs or hierarchical RL approaches. For example, the authors mention various works in Section 2, but fail to compare to them or at least explain why a comparison would not be possible. Another point of criticism is that the authors do not use an existing environment, but instead a Minecraft-inspired one similar to Andreas et al, Oh et al. and Sohn et al. This makes a comparison to prior work hard and I would like to understand in what way previous environments were inadequate for the research carried out here. One aspect that I found most interesting in this paper is that the authors also let annotators map the given programs into natural language form. However, there is no discussion of these results. Similarly, there are interesting qualitative analyses in the appendix of the paper that I only stumbled upon by chance. I believe these should be referenced and a short summary should be integrated into the main part of the paper. I would particularly like to see a discussion of limitations already in the main part of the paper. Minor Comments p1: I like the motivation of cooking recipes for work on program conditioned learning. There is in fact a paper (probably multiple) from the NLP community that I think could be cited here. The one that comes to my mind is: Malmaud, Jonathan, et al. \"Cooking with semantics.\" Proceedings of the ACL 2014 Workshop on Semantic Parsing. 2014. p1: I agree with the argument that programs might be favored over natural language to specify goals as they are unambiguous. However, I think this can also be seen as a drawback. Natural language allows us to very efficiently share information, maybe sometimes information that is only disambiguated through observations in the environment. Another advantage is that natural language for instructing learning agents (like people) is abundant on the web, while programs are not. p2: \"that leverages grammar\" -> \"that leverages a grammar\" p2: \"we propose to utilize an precise\" -> \"we propose to utilize a precise\" p2: For learning from video demonstrations, an important prior work is Aytar, Yusuf, et al. \"Playing hard exploration games by watching youtube.\" Advances in Neural Information Processing Systems. 2018. p3: A deep learning program synthesis work prior to the ones mentioned here is Bo\u0161njak, Matko, et al. \"Programming with a differentiable forth interpreter.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. p5: Would it make sense to also compare to a purely hand-crafted programmatic policy? I am missing a justification why learning is strictly necessary in the environment considered in this work. p6 Section 4.4.1: I believe the explanation of the perception module would benefit from a concrete example. Questions to Authors", "rating": "8: Accept", "reply_text": "Q : Would it make sense to also compare to a purely hand-crafted programmatic policy ? I am missing a justification why learning is strictly necessary in the environment considered in this work . A : We aim to develop a program guided agent that perceives and interacts with the environment using high-dimensional sensory inputs ( e.g.RGB images ) . We propose to learn both the policy and the perception module because ( 1 ) learning methods ( e.g.neural networks ) have been shown effective in dealing with high-dimensional data and ( 2 ) a hand-crafted programmatic policy or perception module can be difficult to obtain when the agent 's observation is high-dimensional . While we tried to provide this intuition at the beginning of Section 4 , we apologize that it was not clear and have revised to paper to emphasize this . To further show that our framework can be extended to a high-dimensional state representation where a hand-crafted policy or perception module might not be easy to obtain , we performed an additional experiment where the perception module and the policy are trained on raw RGB inputs instead of symbolic state representation . The results suggest that our framework can utilize RGB inputs while maintaining similar performance ( 93.2 % on the test set ) and generalization ability ( 91.4 % on the test-complex set ) . We have revised the paper to incorporate this finding . Please see Section E.5 for the details . Q : Section 4.4.1 : I believe the explanation of the perception module would benefit from a concrete example . A : Thank you for the suggestion . We have added an example when explaining the perception module in the revision ."}], "0": {"review_id": "BkxUvnEYDH-0", "review_text": "This paper presents a reinforcement learning agent that learns to execute tasks specified in a form of programs with an architecture consisting of three modules. The (fixed) interpreter module interprets the program, by issuing queries to a (pre-trained) vision module and giving goals to a policy module that executes them in the environment. The paper also introduces a policy modulation technique, with the goal of modulating the current state with the expected (symbolic) goal. The model is evaluated on a 2D approximation of Minecraft, where it outperforms a set of baselines. In addition, the authors modified the program dataset and re-expressed it in terms of natural language and showed that the baselines perform better with programs than with the natural language instructions. Though I think the general idea of the paper is worth exploring, I find the concrete contribution of the paper a bit thin to the point that I am hesitant to recommend this paper for acceptance. Allow me to explain my objections: First and foremost, this work is very close to work by Denil et al where the execute (in a differentiable fashion) programs in an RL agent. Their work does not have a discrete interpreter per-se, but it does have a differentiable execution of commands. The major difference between these two works would be that Denil et al do not have the vision module (they do mention learning from pixels as future work). However, that is not entirely true. The model presented here uses a pretrained vision module, which by itself is not a problem and is used in related work [1], but this vision module does not operate on visual input but the symbolic representation of the map. A crucial thing here is that if all of a sudden we want to include a new object on the map, the model won\u2019t be able to use some learned similarity since it would require introducing a new object (slice of the input), as it would should it have been trained on pixels. So technically speaking, this is not a vision module but a symbolic state processing module. Then, the modulation mentioned in the paper does not seem particularly novel. Sure, the exact architecture of the model is probably unique, but the idea of modulating a state with a goal is not and has been seen in other work such as [2] and [3] among others. The paper does not mention why, for example, this modulation technique is useful and why any other similar architecture would not be as successful, nor does it mention related modulation techniques in other work. A big issue I have with the evaluation in the paper is that I do not see the benefit of having the experiments with natural language at all. The focal point of the paper are agents able to execute tasks in the form of programs. The (though manually) generated natural language instructions from those same programs cannot even be used by the proposed agent as there is no natural language interpreter, so they are a dangling part of the paper which is there just to showcase that programs should be easier for baselines to learn to execute than natural language (the hypothesis would be that a simpler and more structured/formal language is easier to learn than the natural language). Hence the seq-LSTM results on language which are a tad lower than the results on programs are expected, though the performance of transformers is the opposite---they are better on language, and that is something that is puzzling and left unexplained, as well as the unexpectedly low performance of Tree-RNNs. One would expect them to perform a bit better than LSTMs, but that might be contingent on the size of the dataset more than the structure of the inputs. However, none of these curious findings have been explained. Moreover, the comparison to baselines is not particularly fair as these baselines had to learn the symbolic state interpretation, whereas your model did not. You could have provided the same to the baselines for a better comparison. In addition to that, 5.4.2 goes into detail of analysing the baselines, and ignoring the proposed model. Why didn\u2019t you include the same statistics for your agent in Figure 5 and said subsubsection? The paper is missing some notable related work: - S.R.K. Branavan\u2019s work (all but one cited language-instructed agent papers are post 2017) as well as [4] - object-oriented and hierarchical RL - [5], where they train the neural programmer interpreter from the final reward only, which brings NPI close to this work Questions: - Figure 5 - There\u2019s a mark for Tree-RNN, but Tree-RNNs are not in any figure. Why didn\u2019t you plot the performance of your model? - The setup naturally induces a curriculum - how does it do that if the programs are randomly sampled? - You say that your model learns to comprehending the program flow. I\u2019m not sure I would agree with that because what your model learns is to execute single commands. From what it seems, the interpreter is the only part of the model (which is fixed btw) which sees the control flow, whereas the policy just executes singular commands. Did you mean something else by that statement? Minor issues: - You say twice interpreter (i.e. compiler). Given that they\u2019re not the same, and you\u2019re using an interpreter, I suggest omitting the word compiler. - Figure 2 is lacking detail. There is no difference between r and i (both being positive integers) other than their descriptions, - operators agent[_], env and is_there lack parameters (non-terminal nodes), and where\u2019s river, bridge, etc? [1] COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration [2] FeUdal Networks for Hierarchical Reinforcement Learning [3] Universal value function approximators [4] Vogel et al Learning to follow navigational directions [5] Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction", "rating": "6: Weak Accept", "reply_text": "Q : 5.4.2 goes into detail of analysing the baselines , and ignoring the proposed model . Why didn \u2019 t you include the same statistics for your agent in Figure 5 and said subsubsection ? A : The performance drop of evaluating our agent on the programs that are longer or have a higher number of control flows is less significant than the drop shown on the end-to-end learning baselines . We believe this is because our framework utilizes a rule-based parser ( i.e.the program interpreter ) which is not affected by longer or more complex tasks . Therefore , to analyze the cause of failures , we provided failure analyses in Section E.5.1 and Section E.5.2 detailing the failure rates and the average time cost of each subtask . Q : The paper is missing some notable related work : - S.R.K . Branavan \u2019 s work ( all but one cited language-instructed agent papers are post 2017 ) as well as [ 4 ] - object-oriented and hierarchical RL - [ 5 ] , where they train the neural programmer interpreter from the final reward only , which brings NPI close to this work A : Thanks for pointing out these relevant works . We have incorporated these works and some of their follow-up works in the revision . Q : Figure 5 - There \u2019 s a mark for Tree-RNN , but Tree-RNNs are not in any figure . Why didn \u2019 t you plot the performance of your model ? A : Figure 5 presents all the combinations of models { Seq-LSTM , Transformer , Tree-RNN } and task representations { programs , natural language instructions } . While Seq-LSTM and Transformer can be applied to programs and natural language instructions , Tree-RNN requires the input representation with a tree structure and therefore is only evaluated on programs . The bottom-left subfigures of Figure 5 ( a ) and 5 ( b ) in the original paper both show the performance of Tree-RNN in dark blue . However , the dark red color was reserved for Tree-RNN applied to natural language instructions , which is not evaluated and therefore is not shown in this figure . We have removed the dark red color from the legend and we are sorry for the confusion . Q : The setup naturally induces a curriculum - how does it do that if the programs are randomly sampled ? A : We did not explicitly set up a curriculum ; instead , we always randomly sampled programs regardless of their difficulty . We found that this setup naturally induces a curriculum . At the beginning of the training , the policy first learns to solve programs that require a less number of subtasks but fails to complete the harder programs . Yet , completing simpler programs and obtaining the task completion reward still allows the model to obtain a better understanding of the subtasks , which eventually allows the policy to complete more complex programs . We have revised the paper to make it clear that we did not explicitly set up a curriculum . Q : You say that your model learns to comprehending the program flow . I \u2019 m not sure I would agree with that because what your model learns is to execute single commands . From what it seems , the interpreter is the only part of the model ( which is fixed btw ) which sees the control flow , whereas the policy just executes singular commands . Did you mean something else by that statement ? A : We appreciate the comment . We have replaced the term \u201c comprehend \u201d with \u201c execute \u201d to make it is clear that our framework does not learn to read programs but utilizing a rule-based parser ( i.e.the program interpreter ) . Q : You say twice interpreter ( i.e.compiler ) . Given that they \u2019 re not the same , and you \u2019 re using an interpreter , I suggest omitting the word compiler . A : Thanks for the suggestion . We were intended to give an intuition when writing the paper but you are absolutely correct that they are not the same . We have removed it from the revised paper . Q : Figure 2 is lacking detail . There is no difference between r and i ( both being positive integers ) other than their descriptions , - operators agent [ _ ] , env and is_there lack parameters ( non-terminal nodes ) , and where \u2019 s river , bridge , etc ? A : We have fixed the DSL according to the suggestion . Specifically , we have made the following changes : ( 1 ) $ r $ and $ i $ were merged into $ i $ , which represents a constant , and ( 2 ) we added terrain ( $ u $ ) to represent different types of terrains ."}, "1": {"review_id": "BkxUvnEYDH-1", "review_text": "This paper provides a method for instructing an agent using programs as input instructions, so that the agent should learn to contextually execute this program in a specified environment, learning to generalise as needed from perception, and to satisfy concerns that in the language of planning would be called monitoring and execution. The authors provide a method that breaks this problem down into one of interpreting the program (which is crafted separately as a compiler that benefits from a DSL), learning to identify contextual features and then adapting and applying the policy. The arguments in this paper are well made but the paper would benefit from better clarifying several points: 1. To start at the very beginning, the authors begin in the first page by giving the impression that the agent has gone directly from an NL instruction and otherwise uninterpreted sensory input to a solution, in the spirit of typical end to end systems, whereas what the authors are proposing is a very different and more pragmatic approach wherein the interpretation of the task is handled prior to learning, so that learning is only applied to smaller subproblems. This could be made clearer in the introduction. 2. In particular, it was unclear how the DSL comes about and what restrictions it places on the problem. The DSL will clearly have an influence because a very different task from MineCraft, say, robot manipulation, would have quite different needs of sensor-driven control and hence the information flows (specifically, the separation between goal identification, context perception and motion execution) would be different. What one puts into the DSL will significantly influence how well the overall framework performs (e.g., the ability to crisply ask is_there is powerful). Have the authors systematically explored this axis of design? Can we hear more in the setup about this? 3. The influence of the domain is once again seen in the modulation mechanism for the goal and the way in which affine transformations enable generalisation. This is of course sensible in 2D spatial navigation but may be less straight forward in other decision making contexts. The authors have been clear enough about what they have done, but I would have found it interesting to understand how much we should expect this particular concept to stretch and where its limitations become more apparent - perhaps in the discussion. Overall, this is good work and the writing is clear with suitable references. I would note that the authors are revisiting concerns well studied in the planning literature. While the authors do acknowledge HAMs and so on from the HRL literature, they'd make the paper stronger by also tracing the roots of some of these ideas into the rest of planning. I'll end this review by asking about the relationships between NL instructions and the formal programs. In some domains, the number of realisable programs that map to an ambiguous NL instruction can be large. Equally, not all lay users can write good programs. So, it is worth noting this gap and making clear that this paper does not really address this bit of the overall problem.", "rating": "8: Accept", "reply_text": "Q : Overall , this is good work and the writing is clear with suitable references . I would note that the authors are revisiting concerns well studied in the planning literature . While the authors do acknowledge HAMs and so on from the HRL literature , they 'd make the paper stronger by also tracing the roots of some of these ideas into the rest of planning . A : Classical symbolic planning ( Ghallab et al. , 2004 ) considers a planning problem contains an initial state , a goal , and a set of operators where the agent should act accordingly . Each operator consists of the name of the operator ( and necessary arguments ) , a precondition for this operator , and the effect on the environment if this operator is executed . As a whole , our framework shares many similarities to classical symbolic planning . Our policy ( action module ) which conditions on a symbolically represented goal is similar to an operator , as both the policy and an operator interacts with the environment to fulfill the given task . On the other hand , perceptions ( conditions ) in our DSL and preconditions both require perceiving environments ( e.g.the presence/absence of objects ) . However , perceptions differ from preconditions in that perceptions determine which branches of a program should be taken while preconditions determine if an operator/action/skill is applicable . Q : I 'll end this review by asking about the relationships between NL instructions and the formal programs . In some domains , the number of realisable programs that map to an ambiguous NL instruction can be large . Equally , not all lay users can write good programs . So , it is worth noting this gap and making clear that this paper does not really address this bit of the overall problem . A : As pointed out by the reviewer , there is a trade-off between the accessibility to the instructions ( NL or programs ) and the difficulty of learning from them . In this work , we are mainly interested in advocating programs as a good alternative representation for instructing an agent especially when the tasks are diverse ( i.e.more branches ) . We also agree that the accessibility of natural languages is enjoyable . It is worth noting there has been growing interest in synthesizing programs from natural language ( Lin et al. , 2018 ; Raza et al. , 2015 ; Desai et al. , 2016 ) and other representations ( e.g.images , strings , videos/execution traces . etc . ) .We would like to believe that the difficulty of accessing programs could be alleviated . On the other hand , semantic parsing has been an active research area to bridge unstructured languages and structural formal languages ( Wang et al. , 2017 ) ."}, "2": {"review_id": "BkxUvnEYDH-2", "review_text": "Update: I thank the reviewers for their extensive rebuttal and revision of the paper addressing all of my concerns. I have increased my score. Summary This paper investigates an important direction: How can RL agents make use of high-level instructions and task decompositions formalized as programs? The authors propose a model for a program guided agent that, conditioned on a program, interprets the program, executes it to query a perception module and subsequently proposes subgoals to a low-level action module. The method outperforms LSTM and Transformer baselines on a Minecraft-like task and generalizes to programs larger than the one seen during training. Strengths Contribution in the important direction of training RL agents with instructions and prior knowledge, here in the form of programs Clearly written paper with good illustrations of the model Good performance on generalization task of acting in environments where the programmatic instructions are longer than those seen during training Weaknesses One of the contributions of the paper is a modulation mechanism (Section 4.3) on the state features that incorporates a goal-conditioned policy. However, a very related approach has been proposed by Bahdanau, Dzmitry, et al. \"Learning to Understand Goal Specifications by Modelling Reward.\" ICLR 2019. They introduced FILM layers that modulate the layers in a ConvNet conditioned on a goal representation. This should be discussed and compared to in the paper. I am surprised there is no comparison to other work that conditions on programs or hierarchical RL approaches. For example, the authors mention various works in Section 2, but fail to compare to them or at least explain why a comparison would not be possible. Another point of criticism is that the authors do not use an existing environment, but instead a Minecraft-inspired one similar to Andreas et al, Oh et al. and Sohn et al. This makes a comparison to prior work hard and I would like to understand in what way previous environments were inadequate for the research carried out here. One aspect that I found most interesting in this paper is that the authors also let annotators map the given programs into natural language form. However, there is no discussion of these results. Similarly, there are interesting qualitative analyses in the appendix of the paper that I only stumbled upon by chance. I believe these should be referenced and a short summary should be integrated into the main part of the paper. I would particularly like to see a discussion of limitations already in the main part of the paper. Minor Comments p1: I like the motivation of cooking recipes for work on program conditioned learning. There is in fact a paper (probably multiple) from the NLP community that I think could be cited here. The one that comes to my mind is: Malmaud, Jonathan, et al. \"Cooking with semantics.\" Proceedings of the ACL 2014 Workshop on Semantic Parsing. 2014. p1: I agree with the argument that programs might be favored over natural language to specify goals as they are unambiguous. However, I think this can also be seen as a drawback. Natural language allows us to very efficiently share information, maybe sometimes information that is only disambiguated through observations in the environment. Another advantage is that natural language for instructing learning agents (like people) is abundant on the web, while programs are not. p2: \"that leverages grammar\" -> \"that leverages a grammar\" p2: \"we propose to utilize an precise\" -> \"we propose to utilize a precise\" p2: For learning from video demonstrations, an important prior work is Aytar, Yusuf, et al. \"Playing hard exploration games by watching youtube.\" Advances in Neural Information Processing Systems. 2018. p3: A deep learning program synthesis work prior to the ones mentioned here is Bo\u0161njak, Matko, et al. \"Programming with a differentiable forth interpreter.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. p5: Would it make sense to also compare to a purely hand-crafted programmatic policy? I am missing a justification why learning is strictly necessary in the environment considered in this work. p6 Section 4.4.1: I believe the explanation of the perception module would benefit from a concrete example. Questions to Authors", "rating": "8: Accept", "reply_text": "Q : Would it make sense to also compare to a purely hand-crafted programmatic policy ? I am missing a justification why learning is strictly necessary in the environment considered in this work . A : We aim to develop a program guided agent that perceives and interacts with the environment using high-dimensional sensory inputs ( e.g.RGB images ) . We propose to learn both the policy and the perception module because ( 1 ) learning methods ( e.g.neural networks ) have been shown effective in dealing with high-dimensional data and ( 2 ) a hand-crafted programmatic policy or perception module can be difficult to obtain when the agent 's observation is high-dimensional . While we tried to provide this intuition at the beginning of Section 4 , we apologize that it was not clear and have revised to paper to emphasize this . To further show that our framework can be extended to a high-dimensional state representation where a hand-crafted policy or perception module might not be easy to obtain , we performed an additional experiment where the perception module and the policy are trained on raw RGB inputs instead of symbolic state representation . The results suggest that our framework can utilize RGB inputs while maintaining similar performance ( 93.2 % on the test set ) and generalization ability ( 91.4 % on the test-complex set ) . We have revised the paper to incorporate this finding . Please see Section E.5 for the details . Q : Section 4.4.1 : I believe the explanation of the perception module would benefit from a concrete example . A : Thank you for the suggestion . We have added an example when explaining the perception module in the revision ."}}