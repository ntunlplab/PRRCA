{"year": "2019", "forum": "HkeILsRqFQ", "title": "An experimental study of layer-level training speed and its impact on generalization", "decision": "Reject", "meta_review": "Dear authors,\n\nThe reviewers all appreciated the question you are asking and the study of the impact of each layer is definitely an interesting one.\n\nThey were however uncertain about the actual metrics you used to emphasize your points. Further, as you noted, there were quite a few presentation issues that led to skepticism of the reviewers, despite them spending quite a bit of time reading the paper and engaging in discussion.\n\nHence, I regret to inform you that your work is not yet ready for publication. A more focused analysis would be a great addition to the questions you raise.", "reviews": [{"review_id": "HkeILsRqFQ-0", "review_text": "Pros: Overall, this is a nice empirical paper with a reasonably extensive set of experiments. It is interesting that, among networks that train to ~100% with Layca, the best generalizing ones tend to have balanced training between layers (Fig. 2), and that tuned SGD does not generalize as well as Layca (Fig. 4). I think this paper\u2019s focus on discrepancies in training & generalization originating from layers of a deep network is an interesting and important topic of study that warrants further empirical and theoretical investigation from the community. I think the work already has some interesting results and will encourage further investigation. Cons: --Would appreciate greater discussion of the originality of the results; in particular, a more upfront discussion (which is currently concisely presented in the supplementary) regarding algorithms that are similar to Layca when less crucial steps are dropped, e.g. Yu et al 2017 and Ginsburg et al 2018. --After reading the paper, I don\u2019t feel especially convinced that rotation (of the flattened weight matrix) is the best quantity to analyze training dynamics of a single layer. Could there be greater discussion & motivation for this, and in particular, relationship to work where weights are parameterized using orthogonal matrices, or even orthogonal initialization? Some minor comments: --Would have appreciated a discussion of the learning rate schedule (as well as other experimental details, e.g. loss function used and what role this plays) and whether networks with lower learning rates would need to be trained longer. --Greater discussion of why the first and last layer(s) do not experience the same rotation rate as other layers and if there would be better generalization if they did. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your encouraging comments , we really appreciate it . The bulk of our rebuttal is contained in the two comments addressed to all reviewers . We add here answers to your specific questions . Replies to \u2018 Cons \u2019 : -- Algorithms similar to Layca are now mentioned in the related work section , with explicit mention that they may even be equivalent in some practical applications . Note that the similarity between Layca and previous works does not really affect the originality of our contribution since the novelty of our work lies in the claims derived from the extensive analysis performed based on Layca , and not in the design of Layca itself . -- The pertinence of measuring layer-level training speed through the rotation rate of weight vectors is discussed in depth in the comment addressed to all reviewers . Using orthogonal weights in deep networks was proposed to deal with the vanishing/exploding gradients problem , because such weights better preserve the norm of signals through multiplication . This assumes implicitly that only the norm of input and gradient signals matter to measure layer-level training speed , and not the norm of each layer \u2019 s weights . The section ( Section 6 ) we \u2019 ve added to the paper shows that such assumption is probably wrong . Indeed , metrics of layer-level training speed that do not depend on the norm of the weights do not demonstrate a consistent relation to generalization . Replies to \u2018 Some minor comments \u2019 : -- The learning rate schedules are now described in the Supplementary Material . The approach is very standard : reduce the learning rate by a factor of 5 or 10 at specific epochs during training ( e.g . [ 1 ] ) .We don \u2019 t believe it is necessary to train networks with lower learning rates for a longer period . First , as shown in Figure 3 , lower learning rates sometimes required shorter training time ( which is quite unintuitive ) , because the network doesn \u2019 t get stuck in high loss curve plateaus in such cases . Second , as detailed in Section B.4 , the networks trained with lower learning rates , and for which layer-wise angle deviation curves are presented in the paper , all achieve ~100 % training accuracy . -- This is an interesting research question , that we hope to elucidate in future work . We hope our rebuttal answers the reviewer \u2019 s doubts , and are open to further discussion . Thank you again for your efforts and positivity . [ 1 ] Deep Residual Learning for Image Recognition ; K. He et al. , CVPR 2016"}, {"review_id": "HkeILsRqFQ-1", "review_text": "Paper summary: The authors propose layer rotation speed as a measure of layer-wise training speed and introduce the Layca optimizer as a means of enforcing uniform layer rotation speed throughout the network. They show empirically that layer rotation speed is linked to the generalization performance of deep neural networks and that weight decay induces uniform layer rotation speeds. Detailed comments: Overall, I felt that the paper introduced some interesting ideas but I was not left convinced that layer rotation speed is the correct measure of layer training speed. I hope that the authors can clarify this based on my questions and comments below. 1) In the introduction you refer to input and feedback signals to a layer, I assume this refers to the forward and backward pass. As I understand it, this intuition and the findings of Figure 1 do not immediately relate to the notion of layer rotation speed during training. Could you clarify what you mean by \"the input and feedback signals that a layer receives could also influence the generalization ability induced by the layer's training\", to me this statement seems obvious as input+feedback signals contains training entirely. 2) In Figure 1 you show that when training one chosen layer and keeping the others fixed, if the chosen layer is deeper into the network the test accuracy is worse. I wonder to what extent this might be remedied by initialization. For example, one might expect that when sampling random square matrices there are some very small eigenvalues which \"kill\" information in the forward pass. If we train a layer deep in the network it may have access to less information from the data than one earlier on. Whereas training an earlier layer could allow this layer to shift mass into the parts of the eigenspace which are well represented (so-to-speak) in the future layers. Have you thought about this at all? One simple way to evaluate this would be to initialize the weights to be random orthogonal matrices, ensuring that the eigenvalues are equal. With that said, I thought that this was an interesting experiment with a fairly surprising outcome! 3) In related work you discuss the vanishing and exploding gradients problems in terms of layer-level training speed. I think that another relevant research direction may be dynamical isometry [1] which solves this problem by restricting all singular values of the Jacobian matrix to be close to 1. These ideas may also be relevant when discussing Layca and layer-rotation. 4) I found section 3.1. a little unconvincing. It is not obvious to me that layer rotation speed is necessarily a good measure of training speed. In fact, there are many updates which have large cosine distance (as you define it) but do not change the network function (for example, permuting the weight matrices in fully-connected networks). Why is the rotation defined through a vectorization of the weight matrix as opposed to e.g. the polar decomposition? Is this a computational issue? Similarly, in section 3.2 you liken Layca to optimization on a manifold but I am not convinced that this makes sense for matrices which inherently have some structure (e.g, perhaps the Stiefel manifold would be more meaningful). 6) Figure 2 shows that uniform rotation leads to improved test accuracy. But could it be the case that controlling the effective learning rate is sufficient (and layer rotation is one way to achieve this)? For example, we might take the sign of the update and use this to ensure that each weight matrix has the same effective learning rate (something like [2]). Do you expect this would have a similar effect? If not, what is unique about layer rotation that provides good test accuracy? 7) You claim that SGD and adaptive methods with weight decay works without taking extra care to control the layer-rotation rate, as weight decay provides a similar effect. Firstly, you use weight decay and L2 regularization interchangeable, could you be explicit about exactly which you mean (see e.g. [3]). Assuming you mean weight decay (and not L2 regularization), then this could also be due to the effective learning rate ([4,5,6]) which may have some interaction with layer rotation rate (i.e. Figure 4). In summary, I would have liked to see an explanation for why weight decay leads to uniform rotation speeds. 8) If I understand correctly, Figure 5 shows 5 tasks and reportedly 5 optimization schemes - each on a different task? It seems more reasonable to compare these on the same task. Overall I felt that the paper had some interesting contributions and a fairly comprehensive empirical study. However, I do not feel that the paper gives adequate attention to the notion of effective learning rate induced by weight decay and I was not totally convinced that the way layer rotation speed is defined is the correct way. Minor comments: - A lot of white space and a large caption for Figure 1. - Section 4 opens with \"monitor and control\", but I think the latter is really presented in section 4 and not section 3. - I think a diagram of the projection step of the Layca algorithm would be informative (for 2D weight vector). - Why does `5` appear in equation 1? Is this an arbitrary choice? - Some of the lighter colors in e.g. Figure 2(b) made some lines hard to read when printed. I do not believe that this affected the image significantly. Clarity: The paper is well written and is easy to understand. Some of the figures in the experiments are a little cluttered and the lighter colors can be hard to see (e.g. Fig 2(b)), but this is minor. Significance: The paper presents an interesting view point but I am not convinced that it offers as strong an explanation for these phenomena as other approaches. I believe with some more clarification the results could become more significant. My review score hinges mostly on the interaction between layer rotation speed and the effective layer-wise learning rate. Originality: To my knowledge, the ideas are presented in the paper are original. In particular, this is a novel way to characterize layer-wise training speed. References: [1] Pennington et al. \"Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\" https://arxiv.org/abs/1711.04735 [2] Bernstein et al. \"signSGD: Compressed Optimisation for Non-Convex Problems\" https://arxiv.org/abs/1802.04434 [3] Loshchilov et al. \"Fixing Weight Decay Regularization in Adam\", https://arxiv.org/pdf/1711.05101.pdf [4] Laarhoven, \"L2 Regularization versus Batch and Weight Normalization\" https://arxiv.org/abs/1706.05350 [5] Hoffer et al. \"Norm matters: efficient and accurate normalization schemes in deep networks\" https://arxiv.org/abs/1803.01814 [6] Anonymous, \"Three Mechanisms of Weight Decay Regularization\" https://openreview.net/forum?id=B1lz-3Rct7 (Another ICLR 2019 submission)", "rating": "5: Marginally below acceptance threshold", "reply_text": "R7 ) Please note that our analysis of weight decay is only performed on SGD , and not on adaptive gradient methods ( cfr.general comment to all reviewers that clarifies our analysis of adaptive gradient methods ) . In the case of SGD , weight decay and L2 regularization are equivalent , which explains why we use both interchangeably . As mentioned by the reviewer , several works have recently argued that weight decay 's regularization effect emerged from its ability to increase the effective learning rate . These works , however , do not provide a concise description of when and to what extent weight decay changes the effective learning rate , such that using weight decay is still necessary to benefit from its regularization effect in practice . Our work also analyses weight decay , but from the perspective of layer rotation rates instead of effective learning rates . These two perspectives differ in two important ways . First , learning rates are not monitored on a per layer basis , while layer rotation rates are . Second , learning rates are independent of the gradient signal , while layer rotation rates measure weight updates , which depend on the learning rate and the gradient signal . We show that our new perspective enables a more succinct description of weight decay 's regularizing effect , that we are able to reproduce without any additional meta-parameter tuning when using Layca , our tool for controlling layer rotation rates . This discussion has been added to the related work section . We thank the reviewer for highlighting this pertinent line of work . Whilst going beyond the scope of our paper , understanding why weight decay leads to uniform rotation rates is definitely an interesting question to investigate in the future . We believe it will require a precise study of the equilibrium point to which the norm of each layer \u2019 s weights converges ( i.e.the point where weight decay and the gradients cancel each other , such that the norm of a layer \u2019 s weights remains constant ) . In a sense , even if we are still unable to answer this question , we appreciate that our work raised such question in reviewer \u2019 s mind . Indeed , this question reveals that our work , by pointing the tight connection between rotation rates and generalization , opens several novel research avenues that could end up in disclosing key ingredients and methods regarding the control of network generalization . R8 ) We hope that the clarification of our analysis of adaptive gradient methods ( cfr.comment to all reviewers ) makes our methodology clearer . The reason why the five different adaptive gradient methods are applied on 5 different tasks , is because we do not compare the different adaptive methods to each other , but we rather compare each adaptive gradient method to SGD , which has been applied on the 5 tasks ( cfr.Figure 4 ) . We hope that this rather long rebuttal will convince the reviewer of the significance of our work . We of course are open to further discussion . We thank the reviewer again for his extensive review of our paper ."}, {"review_id": "HkeILsRqFQ-2", "review_text": "This paper insists layer-level training speed is crucial for generalization ability. The layer-level training speed is measured by angle between weights at different time stamps in this paper. To control the amount of weight rotation, which means the degree of angle movement, this paper proposes a new algorithm, Layca. This algorithm projects the gradient vector of SGD (or update vector of other variants) onto the space orthogonal to the current weight vector, and adjust the length of the update vector to achieve the desirable angle movement. This paper conducted several experiments to verify the helpfulness of Layca. This paper have an impressive theme, the layer-level training speed is important to have a strong generalization power for CNNs. To verify this hypothesis, this paper proposes a simply SGD-variant to control the amount of weight rotation for showing its impact on generalization. This experimental study shows many insights about how the amount of weight rotation affect the generalization power of CNN family. However, the contribution of this paper is limited. I thought this paper lacks the discussion of how much the layer-level training speed is important. This paper shows the Figure 1 as one clue, but this figure shows the importance of each layer for generalization, not the importance of the layer-level training speed. It is better to show how and how much it is important to consider the layer-level training speed carefully, especially compared with the current state-of-the-art CNN optimization methods or plain SGD (like performance difference). In addition, figures shown in this paper are quite hard to read. Too many figures, too many lines, no legends, and these lines are heavily overlapped. If this paper is accepted and will be published, I strongly recommend authors choose some important figures and lines to make these visible, and move others to supplementary material.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We are happy that the reviewer appreciates the research question introduced by our paper . The bulk of our rebuttal is contained in the two comments addressed to all reviewers . We add here answers to your specific questions . If we understand the comment well , the reviewer believes our contribution is limited because we study layer rotation rates and the training of layers in isolation , instead of layer-level training speed . As can be deduced from the other reviews and the bulk of our rebuttal , studying layer-level training speed is a difficult task because there is no clear way to measure it . As long as no exact measure has been found , studying layer-level training speed directly is simply impossible . In our paper , we thus make the choice of measuring it indirectly/approximately , by training layers in isolation , or by monitoring layer rotation rates . We believe this approach provides important contributions to the community . First , although indirectly , our provides substantial evidence that layer-level training speed \u2019 s impact on generalization is ubiquitous in current deep learning applications , and thus that its study should be taken seriously ( to our knowledge , no paper has been published on this research question yet ) . Second , we provide evidence that layer rotation rates are a pertinent measure of layer-level training speed , as they have a remarkably consistent relation to generalization . Third , even if we completely ignore our contributions around layer-level training speed , our paper still provides useful guidelines for meta-parameter tuning and fundamental insights around weight decay and adaptive gradient methods . We hope our rebuttal clarifies our methodology , and highlights the significance of our contributions . We thank the reviewer again for his efforts for reviewing our paper ."}], "0": {"review_id": "HkeILsRqFQ-0", "review_text": "Pros: Overall, this is a nice empirical paper with a reasonably extensive set of experiments. It is interesting that, among networks that train to ~100% with Layca, the best generalizing ones tend to have balanced training between layers (Fig. 2), and that tuned SGD does not generalize as well as Layca (Fig. 4). I think this paper\u2019s focus on discrepancies in training & generalization originating from layers of a deep network is an interesting and important topic of study that warrants further empirical and theoretical investigation from the community. I think the work already has some interesting results and will encourage further investigation. Cons: --Would appreciate greater discussion of the originality of the results; in particular, a more upfront discussion (which is currently concisely presented in the supplementary) regarding algorithms that are similar to Layca when less crucial steps are dropped, e.g. Yu et al 2017 and Ginsburg et al 2018. --After reading the paper, I don\u2019t feel especially convinced that rotation (of the flattened weight matrix) is the best quantity to analyze training dynamics of a single layer. Could there be greater discussion & motivation for this, and in particular, relationship to work where weights are parameterized using orthogonal matrices, or even orthogonal initialization? Some minor comments: --Would have appreciated a discussion of the learning rate schedule (as well as other experimental details, e.g. loss function used and what role this plays) and whether networks with lower learning rates would need to be trained longer. --Greater discussion of why the first and last layer(s) do not experience the same rotation rate as other layers and if there would be better generalization if they did. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your encouraging comments , we really appreciate it . The bulk of our rebuttal is contained in the two comments addressed to all reviewers . We add here answers to your specific questions . Replies to \u2018 Cons \u2019 : -- Algorithms similar to Layca are now mentioned in the related work section , with explicit mention that they may even be equivalent in some practical applications . Note that the similarity between Layca and previous works does not really affect the originality of our contribution since the novelty of our work lies in the claims derived from the extensive analysis performed based on Layca , and not in the design of Layca itself . -- The pertinence of measuring layer-level training speed through the rotation rate of weight vectors is discussed in depth in the comment addressed to all reviewers . Using orthogonal weights in deep networks was proposed to deal with the vanishing/exploding gradients problem , because such weights better preserve the norm of signals through multiplication . This assumes implicitly that only the norm of input and gradient signals matter to measure layer-level training speed , and not the norm of each layer \u2019 s weights . The section ( Section 6 ) we \u2019 ve added to the paper shows that such assumption is probably wrong . Indeed , metrics of layer-level training speed that do not depend on the norm of the weights do not demonstrate a consistent relation to generalization . Replies to \u2018 Some minor comments \u2019 : -- The learning rate schedules are now described in the Supplementary Material . The approach is very standard : reduce the learning rate by a factor of 5 or 10 at specific epochs during training ( e.g . [ 1 ] ) .We don \u2019 t believe it is necessary to train networks with lower learning rates for a longer period . First , as shown in Figure 3 , lower learning rates sometimes required shorter training time ( which is quite unintuitive ) , because the network doesn \u2019 t get stuck in high loss curve plateaus in such cases . Second , as detailed in Section B.4 , the networks trained with lower learning rates , and for which layer-wise angle deviation curves are presented in the paper , all achieve ~100 % training accuracy . -- This is an interesting research question , that we hope to elucidate in future work . We hope our rebuttal answers the reviewer \u2019 s doubts , and are open to further discussion . Thank you again for your efforts and positivity . [ 1 ] Deep Residual Learning for Image Recognition ; K. He et al. , CVPR 2016"}, "1": {"review_id": "HkeILsRqFQ-1", "review_text": "Paper summary: The authors propose layer rotation speed as a measure of layer-wise training speed and introduce the Layca optimizer as a means of enforcing uniform layer rotation speed throughout the network. They show empirically that layer rotation speed is linked to the generalization performance of deep neural networks and that weight decay induces uniform layer rotation speeds. Detailed comments: Overall, I felt that the paper introduced some interesting ideas but I was not left convinced that layer rotation speed is the correct measure of layer training speed. I hope that the authors can clarify this based on my questions and comments below. 1) In the introduction you refer to input and feedback signals to a layer, I assume this refers to the forward and backward pass. As I understand it, this intuition and the findings of Figure 1 do not immediately relate to the notion of layer rotation speed during training. Could you clarify what you mean by \"the input and feedback signals that a layer receives could also influence the generalization ability induced by the layer's training\", to me this statement seems obvious as input+feedback signals contains training entirely. 2) In Figure 1 you show that when training one chosen layer and keeping the others fixed, if the chosen layer is deeper into the network the test accuracy is worse. I wonder to what extent this might be remedied by initialization. For example, one might expect that when sampling random square matrices there are some very small eigenvalues which \"kill\" information in the forward pass. If we train a layer deep in the network it may have access to less information from the data than one earlier on. Whereas training an earlier layer could allow this layer to shift mass into the parts of the eigenspace which are well represented (so-to-speak) in the future layers. Have you thought about this at all? One simple way to evaluate this would be to initialize the weights to be random orthogonal matrices, ensuring that the eigenvalues are equal. With that said, I thought that this was an interesting experiment with a fairly surprising outcome! 3) In related work you discuss the vanishing and exploding gradients problems in terms of layer-level training speed. I think that another relevant research direction may be dynamical isometry [1] which solves this problem by restricting all singular values of the Jacobian matrix to be close to 1. These ideas may also be relevant when discussing Layca and layer-rotation. 4) I found section 3.1. a little unconvincing. It is not obvious to me that layer rotation speed is necessarily a good measure of training speed. In fact, there are many updates which have large cosine distance (as you define it) but do not change the network function (for example, permuting the weight matrices in fully-connected networks). Why is the rotation defined through a vectorization of the weight matrix as opposed to e.g. the polar decomposition? Is this a computational issue? Similarly, in section 3.2 you liken Layca to optimization on a manifold but I am not convinced that this makes sense for matrices which inherently have some structure (e.g, perhaps the Stiefel manifold would be more meaningful). 6) Figure 2 shows that uniform rotation leads to improved test accuracy. But could it be the case that controlling the effective learning rate is sufficient (and layer rotation is one way to achieve this)? For example, we might take the sign of the update and use this to ensure that each weight matrix has the same effective learning rate (something like [2]). Do you expect this would have a similar effect? If not, what is unique about layer rotation that provides good test accuracy? 7) You claim that SGD and adaptive methods with weight decay works without taking extra care to control the layer-rotation rate, as weight decay provides a similar effect. Firstly, you use weight decay and L2 regularization interchangeable, could you be explicit about exactly which you mean (see e.g. [3]). Assuming you mean weight decay (and not L2 regularization), then this could also be due to the effective learning rate ([4,5,6]) which may have some interaction with layer rotation rate (i.e. Figure 4). In summary, I would have liked to see an explanation for why weight decay leads to uniform rotation speeds. 8) If I understand correctly, Figure 5 shows 5 tasks and reportedly 5 optimization schemes - each on a different task? It seems more reasonable to compare these on the same task. Overall I felt that the paper had some interesting contributions and a fairly comprehensive empirical study. However, I do not feel that the paper gives adequate attention to the notion of effective learning rate induced by weight decay and I was not totally convinced that the way layer rotation speed is defined is the correct way. Minor comments: - A lot of white space and a large caption for Figure 1. - Section 4 opens with \"monitor and control\", but I think the latter is really presented in section 4 and not section 3. - I think a diagram of the projection step of the Layca algorithm would be informative (for 2D weight vector). - Why does `5` appear in equation 1? Is this an arbitrary choice? - Some of the lighter colors in e.g. Figure 2(b) made some lines hard to read when printed. I do not believe that this affected the image significantly. Clarity: The paper is well written and is easy to understand. Some of the figures in the experiments are a little cluttered and the lighter colors can be hard to see (e.g. Fig 2(b)), but this is minor. Significance: The paper presents an interesting view point but I am not convinced that it offers as strong an explanation for these phenomena as other approaches. I believe with some more clarification the results could become more significant. My review score hinges mostly on the interaction between layer rotation speed and the effective layer-wise learning rate. Originality: To my knowledge, the ideas are presented in the paper are original. In particular, this is a novel way to characterize layer-wise training speed. References: [1] Pennington et al. \"Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\" https://arxiv.org/abs/1711.04735 [2] Bernstein et al. \"signSGD: Compressed Optimisation for Non-Convex Problems\" https://arxiv.org/abs/1802.04434 [3] Loshchilov et al. \"Fixing Weight Decay Regularization in Adam\", https://arxiv.org/pdf/1711.05101.pdf [4] Laarhoven, \"L2 Regularization versus Batch and Weight Normalization\" https://arxiv.org/abs/1706.05350 [5] Hoffer et al. \"Norm matters: efficient and accurate normalization schemes in deep networks\" https://arxiv.org/abs/1803.01814 [6] Anonymous, \"Three Mechanisms of Weight Decay Regularization\" https://openreview.net/forum?id=B1lz-3Rct7 (Another ICLR 2019 submission)", "rating": "5: Marginally below acceptance threshold", "reply_text": "R7 ) Please note that our analysis of weight decay is only performed on SGD , and not on adaptive gradient methods ( cfr.general comment to all reviewers that clarifies our analysis of adaptive gradient methods ) . In the case of SGD , weight decay and L2 regularization are equivalent , which explains why we use both interchangeably . As mentioned by the reviewer , several works have recently argued that weight decay 's regularization effect emerged from its ability to increase the effective learning rate . These works , however , do not provide a concise description of when and to what extent weight decay changes the effective learning rate , such that using weight decay is still necessary to benefit from its regularization effect in practice . Our work also analyses weight decay , but from the perspective of layer rotation rates instead of effective learning rates . These two perspectives differ in two important ways . First , learning rates are not monitored on a per layer basis , while layer rotation rates are . Second , learning rates are independent of the gradient signal , while layer rotation rates measure weight updates , which depend on the learning rate and the gradient signal . We show that our new perspective enables a more succinct description of weight decay 's regularizing effect , that we are able to reproduce without any additional meta-parameter tuning when using Layca , our tool for controlling layer rotation rates . This discussion has been added to the related work section . We thank the reviewer for highlighting this pertinent line of work . Whilst going beyond the scope of our paper , understanding why weight decay leads to uniform rotation rates is definitely an interesting question to investigate in the future . We believe it will require a precise study of the equilibrium point to which the norm of each layer \u2019 s weights converges ( i.e.the point where weight decay and the gradients cancel each other , such that the norm of a layer \u2019 s weights remains constant ) . In a sense , even if we are still unable to answer this question , we appreciate that our work raised such question in reviewer \u2019 s mind . Indeed , this question reveals that our work , by pointing the tight connection between rotation rates and generalization , opens several novel research avenues that could end up in disclosing key ingredients and methods regarding the control of network generalization . R8 ) We hope that the clarification of our analysis of adaptive gradient methods ( cfr.comment to all reviewers ) makes our methodology clearer . The reason why the five different adaptive gradient methods are applied on 5 different tasks , is because we do not compare the different adaptive methods to each other , but we rather compare each adaptive gradient method to SGD , which has been applied on the 5 tasks ( cfr.Figure 4 ) . We hope that this rather long rebuttal will convince the reviewer of the significance of our work . We of course are open to further discussion . We thank the reviewer again for his extensive review of our paper ."}, "2": {"review_id": "HkeILsRqFQ-2", "review_text": "This paper insists layer-level training speed is crucial for generalization ability. The layer-level training speed is measured by angle between weights at different time stamps in this paper. To control the amount of weight rotation, which means the degree of angle movement, this paper proposes a new algorithm, Layca. This algorithm projects the gradient vector of SGD (or update vector of other variants) onto the space orthogonal to the current weight vector, and adjust the length of the update vector to achieve the desirable angle movement. This paper conducted several experiments to verify the helpfulness of Layca. This paper have an impressive theme, the layer-level training speed is important to have a strong generalization power for CNNs. To verify this hypothesis, this paper proposes a simply SGD-variant to control the amount of weight rotation for showing its impact on generalization. This experimental study shows many insights about how the amount of weight rotation affect the generalization power of CNN family. However, the contribution of this paper is limited. I thought this paper lacks the discussion of how much the layer-level training speed is important. This paper shows the Figure 1 as one clue, but this figure shows the importance of each layer for generalization, not the importance of the layer-level training speed. It is better to show how and how much it is important to consider the layer-level training speed carefully, especially compared with the current state-of-the-art CNN optimization methods or plain SGD (like performance difference). In addition, figures shown in this paper are quite hard to read. Too many figures, too many lines, no legends, and these lines are heavily overlapped. If this paper is accepted and will be published, I strongly recommend authors choose some important figures and lines to make these visible, and move others to supplementary material.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We are happy that the reviewer appreciates the research question introduced by our paper . The bulk of our rebuttal is contained in the two comments addressed to all reviewers . We add here answers to your specific questions . If we understand the comment well , the reviewer believes our contribution is limited because we study layer rotation rates and the training of layers in isolation , instead of layer-level training speed . As can be deduced from the other reviews and the bulk of our rebuttal , studying layer-level training speed is a difficult task because there is no clear way to measure it . As long as no exact measure has been found , studying layer-level training speed directly is simply impossible . In our paper , we thus make the choice of measuring it indirectly/approximately , by training layers in isolation , or by monitoring layer rotation rates . We believe this approach provides important contributions to the community . First , although indirectly , our provides substantial evidence that layer-level training speed \u2019 s impact on generalization is ubiquitous in current deep learning applications , and thus that its study should be taken seriously ( to our knowledge , no paper has been published on this research question yet ) . Second , we provide evidence that layer rotation rates are a pertinent measure of layer-level training speed , as they have a remarkably consistent relation to generalization . Third , even if we completely ignore our contributions around layer-level training speed , our paper still provides useful guidelines for meta-parameter tuning and fundamental insights around weight decay and adaptive gradient methods . We hope our rebuttal clarifies our methodology , and highlights the significance of our contributions . We thank the reviewer again for his efforts for reviewing our paper ."}}