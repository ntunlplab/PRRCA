{"year": "2018", "forum": "BkJ3ibb0-", "title": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models", "decision": "Accept (Poster)", "meta_review": "The paper studied defenses against adversarial examples by training a GAN and, at inference time, finding the GAN-generated sample that is nearest to the (adversarial) input example. Next, it classifies the generated example rather than the input example. This defense is interesting and novel. The CelebA experiments the authors added in their revision suggest that the defense can be effective on high-resolution RGB images.", "reviews": [{"review_id": "BkJ3ibb0--0", "review_text": "This paper presents Defense-GAN: a GAN that used at test time to map the input generate an image (G(z)) close (in MSE(G(z), x)) to the input image (x), by applying several steps of gradient descent of this MSE. The GAN is a WGAN trained on the train set (only to keep the generator). The goal of the whole approach is to be robust to adversarial examples, without having to change the (downstream task) classifier, only swapping in the G(z) for the x. + The paper is easy to follow. + It seems (but I am not an expert in adversarial examples) to cite the relevant litterature (that I know of) and compare to reasonably established attacks and defenses. + Simple/directly applicable approach that seems to work experimentally, but - A missing baseline is to take the nearest neighbour of the (perturbed) x from the training set. - Only MNIST-sized images, and MNIST-like (60k train set, 10 labels) datasets: MNIST and F-MNIST. - Between 0.043sec and 0.825 sec to reconstruct an MNIST-sized image. ? MagNet results were very often worse than no defense in Table 4, could you comment on that? - In white-box attacks, it seems to me like L steps of gradient descent on MSE(G(z), x) should be directly extended to L steps of (at least) FGSM-based attacks, at least as a control.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate the constructive criticism and detailed analysis of our paper . A ) Nearest-neighbor baseline : Taking the nearest neighbor of the potentially perturbed x from the training set can be seen as a simple way of removing adversarial noise , and is tantamount to a 1-nearest-neighbor ( 1-NN ) classifier . On MNIST , a 1-NN classifier achieves an 88.6 % accuracy on FGSM adversarial examples with epsilon = 0.3 , found using the B substitute network . Defense-GAN-Rec and Defense-GAN-Orig average about 92.5 % across the four different classifier networks when the substitute model is fixed to B . Similar trends are found for other substitute models . There is an improvement of about 4 % by using Defense-GAN . It is also worth noting that in the case of MNIST , a 1-NN classifier works reasonably well ( achieving around 95 % on clean images ) . This is not the case for more complex datasets : for example , if the problem at hand is face attributes classification , nearest neighbors may not necessarily belong to the same class , and therefore NN classifiers will perform poorly . B ) Only MNIST-sized images : Based on the reviewer \u2019 s suggestion , we have added additional white-box results on the Large-scale CelebFaces Attributes ( CelebA ) dataset in the appendix of the paper . The results show that Defense-GAN can still be used with more complex datasets including larger and RGB images . For further details , please refer to Appendix F in the revised version . C ) Time to reconstruct images : We agree with the reviewer that Defense-GAN introduces additional inference time by reconstructing images using GD on the MSE loss . However , we show its effectiveness against various attacks , especially in comparison to other simpler defenses . Furthermore , we have not optimized the running time of our algorithm , as it was not the focus of this work . This is a worthwhile effort to pursue in the future by trying to better utilize computational resources . Per the reviewer \u2019 s comment , we have timed some reconstruction steps for CelebA images ( which are 15.6 times larger than MNIST/F-MNIST ) . For R = 2 , we have : L = 10 , 0.132 sec L = 25 , 0.106 sec L = 50 , 0.210 sec L = 100 , 0.413 sec L = 200 , 0.824 sec The reconstruction time for CelebA did not scale with the size of the image . D ) MagNet results are sometimes worse than no defense in Table 4 : Even though it seems counter-intuitive that a defense mechanism can sometimes cause a decrease in performance , this stems from the fact that white-box attackers also know the exact defense mechanism used . In the case of MagNet , the defense mechanism is another feedforward network which , in conjunction with the original classifier , can be viewed as a new deeper feedforward network . Attacks on this bigger network can sometimes be more successful than attacks on the original network . Furthermore , MagNet was not designed to be robust against white-box attacks . E ) Using L steps of white-box FGSM : Per our understanding , the reviewer is suggesting using iterative FGSM . We do agree that for a fair comparison , L steps of iterative FGSM could be used . However , we note that CW is an iterative optimization-based attack , and is more powerful than iterative FGSM . Since we have shown robustness against CW attacks in Table 4 , we believe iterative FGSM results will be similar ."}, {"review_id": "BkJ3ibb0--1", "review_text": "This paper presents a method to cope with adversarial examples in classification tasks, leveraging a generative model of the inputs. Given an accurate generative model of the input, this approach first projects the input onto the manifold learned by the generative model (the idea being that inputs on this manifold reflect the non-adversarial input distribution). This projected input is then used to produce the classification probabilities. The authors test their method on various adversarially constructed inputs (with varying degrees of noise). Questions/Comments: - I am interested in unpacking the improvement of Defense-GAN over the MagNet auto-encoder based method. Is the MagNet auto-encoder suffering lower accuracy because the projection of an adversarial image is based on an encoding function that is learned only on true data? If the decoder from the MagNet approach were treated purely as a generative model, and the same optimization-based projection approach (proposed in this work) was followed, would the results be comparable? - Is there anything special about the GAN approach, versus other generative approaches? - In the black-box vs. white-box scenarios, can the attacker know the GAN parameters? Is that what is meant by the \"defense network\" (in experiments bullet 2)? - How computationally expensive is this approach take compared to MagNet or other adversarial approaches? Quality: The method appears to be technically correct. Clarity: This paper clearly written; both method and experiments are presented well. Originality: I am not familiar enough with adversarial learning to assess the novelty of this approach. Significance: I believe the main contribution of this method is the optimization-based approach to project onto a generative model's manifold. I think this kernel has the potential to be explored further (e.g. computational speed-up, projection metrics).", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the insightful comments and discussions . A ) Defense-GAN vs . MagNet vs. other generative approaches : We believe that the MagNet auto-encoder suffers lower accuracy compared to Defense-GAN due to the fact that the \u201c reconstruction \u201d step in MagNet is a feed-forward network as opposed to an optimization-based projection as in Defense-GAN . Overall , the combination of MagNet and the classifier can be seen as one deeper classification network , and has a wide attack surface compared to Defense-GAN . As suggested by the reviewer , if the MagNet decoder ( or another generative approach ) was treated as a generative model , and the same optimization-based projection approach was followed , the model with more representative power would perform better . From our experience , GANs tend to have more representative power , but this is still an active area of research and discussion . We believe that , since GANs are specifically designed to optimize for generative tasks , using a GAN in conjunction with our proposed optimization-based projection would outperform an encoder with the same projection method . However , this would be an interesting future research direction . In addition , we were able to show some theoretical guarantees regarding the use and representative power of GANs in equation ( 7 ) . B ) Black- and white-box attacks : In our work and previous literature , it is assumed that in black-box scenarios the attacker does not know the classifier network nor the defense mechanism ( and any parameters thereof ) . The only information the attacker can use is the classifier output . In white-box scenarios , the attacker knows the entire system including the classifier network , defense mechanisms , and all parameters ( which in our case , include GAN parameters ) . By \u201c defense network \u201d in Experiments bullet 2 , we mean the generator network . C ) Computational complexity : Defense-GAN adds inference-time complexity to the classifier . As discussed in Appendix G ( Appendix F in the original version of the paper ) , this complexity depends on L , the number of GD steps used to reconstruct images , and ( to a lesser extent ) R , the number of random restarts . At training time , Defense-GAN requires training a GAN , but no retraining of the classifier is necessary . In comparison , MagNet also adds inference-time complexity . However , the time overhead is much smaller than Defense-GAN as MagNet is simply a feedforward network . At training time , the overhead is similar to Defense-GAN ( training the encoder , no retraining of the classifier ) . Adversarial training adds no inference-time complexity . However , training time can be significantly larger than for other methods since re-training the classifier is required ( preceded by generating the adversarial examples to augment the training dataset ) ."}, {"review_id": "BkJ3ibb0--2", "review_text": "The authors describe a new defense mechanism against adversarial attacks on classifiers (e.g., FGSM). They propose utilizing Generative Adversarial Networks (GAN), which are usually used for training generative models for an unknown distribution, but have a natural adversarial interpretation. In particular, a GAN consists of a generator NN G which maps a random vector z to an example x, and a discriminator NN D which seeks to discriminate between an examples produced by G and examples drawn from the true distribution. The GAN is trained to minimize the max min loss of D on this discrimination task, thereby producing a G (in the limit) whose outputs are indistinguishable from the true distribution by the best discriminator. Utilizing a trained GAN, the authors propose the following defense at inference time. Given a sample x (which has been adversarially perturbed), first project x onto the range of G by solving the minimization problem z* = argmin_z ||G(z) - x||_2. This is done by SGD. Then apply any classifier trained on the true distribution on the resulting x* = G(z*). In the case of existing black-box attacks, the authors argue (convincingly) that the method is both flexible and empirically effective. In particular, the defense can be applied in conjunction with any classifier (including already hardened classifiers), and does not assume any specific attack model. Nevertheless, it appears to be effective against FGSM attacks, and competitive with adversarial training specifically to defend against FGSM. The authors provide less-convincing evidence that the defense is effective against white-box attacks. In particular, the method is shown to be robust against FGSM, RAND+FGSM, and CW white-box attacks. However, it is not clear to me that the method is invulnerable to novel white-box attacks. In particular, it seems that the attacker can design an x which projects onto some desired x* (using some other method entirely), which then fools the classifier downstream. Nevertheless, the method is shown to be an effective tool for hardening any classifier against existing black-box attacks (which is arguably of great practical value). It is novel and should generate further research with respect to understanding its vulnerabilities more completely. Minor Comments: The sentence starting \u201cUnless otherwise specified\u2026\u201d at the top of page 7 is confusing given the actual contents of Tables 1 and 2, which are clarified only by looking at Table 5 in the appendix. This should be fixed. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the constructive review and comments . A ) Regarding the effectiveness against white-box attacks : As the reviewer has pointed out , we have shown the robustness of our method to existing white-box attacks such as FGSM , RAND+FGSM , and CW . Indeed , a good attack strategy could be to design an x which projects onto a desired x * = G ( z * ) . However , this requires solving for : Find x s.t . the output of the gradient-descent block is z * . Per our understanding , the reviewer \u2019 s suggestion is the following : Find a desired x * in the range of the generator which fools the classifier . Find an x which projects onto x * , i.e. , such that the output of the GD block is z * , where G ( z * ) = x * . Step 1 is a more challenging version of existing attacks , due to the constraint that the adversarial example should lie in the range of the generator . While step 1 could potentially be solvable , the real difficulty lies in step 2 . In fact , it is not obvious how to find such an x given x * . What comes to mind is attempting to solve step 2 using an optimization framework , e.g . : Minimize ( over x , z * ) 1 Subject to G ( z * ) = x * z * is the output of the GD block after L steps . We have shown in Appendix B that solving this problem using GD gets more and more prohibitive as L increases . Furthermore , since we use random initializations of z , if the random seed is not accessible by the attacker , there is no guarantee that a fixed x will result in the same fixed z every time after L steps of GD on the MSE . Due to these factors , we believe that our method is robust to a wide range of gradient-based white-box attacks . However , we are very much interested in further research of novel attack methods . B ) We have fixed the minor comments by specifically mentioning the classifier and substitute models for every Table and Figure throughout the paper ."}], "0": {"review_id": "BkJ3ibb0--0", "review_text": "This paper presents Defense-GAN: a GAN that used at test time to map the input generate an image (G(z)) close (in MSE(G(z), x)) to the input image (x), by applying several steps of gradient descent of this MSE. The GAN is a WGAN trained on the train set (only to keep the generator). The goal of the whole approach is to be robust to adversarial examples, without having to change the (downstream task) classifier, only swapping in the G(z) for the x. + The paper is easy to follow. + It seems (but I am not an expert in adversarial examples) to cite the relevant litterature (that I know of) and compare to reasonably established attacks and defenses. + Simple/directly applicable approach that seems to work experimentally, but - A missing baseline is to take the nearest neighbour of the (perturbed) x from the training set. - Only MNIST-sized images, and MNIST-like (60k train set, 10 labels) datasets: MNIST and F-MNIST. - Between 0.043sec and 0.825 sec to reconstruct an MNIST-sized image. ? MagNet results were very often worse than no defense in Table 4, could you comment on that? - In white-box attacks, it seems to me like L steps of gradient descent on MSE(G(z), x) should be directly extended to L steps of (at least) FGSM-based attacks, at least as a control.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate the constructive criticism and detailed analysis of our paper . A ) Nearest-neighbor baseline : Taking the nearest neighbor of the potentially perturbed x from the training set can be seen as a simple way of removing adversarial noise , and is tantamount to a 1-nearest-neighbor ( 1-NN ) classifier . On MNIST , a 1-NN classifier achieves an 88.6 % accuracy on FGSM adversarial examples with epsilon = 0.3 , found using the B substitute network . Defense-GAN-Rec and Defense-GAN-Orig average about 92.5 % across the four different classifier networks when the substitute model is fixed to B . Similar trends are found for other substitute models . There is an improvement of about 4 % by using Defense-GAN . It is also worth noting that in the case of MNIST , a 1-NN classifier works reasonably well ( achieving around 95 % on clean images ) . This is not the case for more complex datasets : for example , if the problem at hand is face attributes classification , nearest neighbors may not necessarily belong to the same class , and therefore NN classifiers will perform poorly . B ) Only MNIST-sized images : Based on the reviewer \u2019 s suggestion , we have added additional white-box results on the Large-scale CelebFaces Attributes ( CelebA ) dataset in the appendix of the paper . The results show that Defense-GAN can still be used with more complex datasets including larger and RGB images . For further details , please refer to Appendix F in the revised version . C ) Time to reconstruct images : We agree with the reviewer that Defense-GAN introduces additional inference time by reconstructing images using GD on the MSE loss . However , we show its effectiveness against various attacks , especially in comparison to other simpler defenses . Furthermore , we have not optimized the running time of our algorithm , as it was not the focus of this work . This is a worthwhile effort to pursue in the future by trying to better utilize computational resources . Per the reviewer \u2019 s comment , we have timed some reconstruction steps for CelebA images ( which are 15.6 times larger than MNIST/F-MNIST ) . For R = 2 , we have : L = 10 , 0.132 sec L = 25 , 0.106 sec L = 50 , 0.210 sec L = 100 , 0.413 sec L = 200 , 0.824 sec The reconstruction time for CelebA did not scale with the size of the image . D ) MagNet results are sometimes worse than no defense in Table 4 : Even though it seems counter-intuitive that a defense mechanism can sometimes cause a decrease in performance , this stems from the fact that white-box attackers also know the exact defense mechanism used . In the case of MagNet , the defense mechanism is another feedforward network which , in conjunction with the original classifier , can be viewed as a new deeper feedforward network . Attacks on this bigger network can sometimes be more successful than attacks on the original network . Furthermore , MagNet was not designed to be robust against white-box attacks . E ) Using L steps of white-box FGSM : Per our understanding , the reviewer is suggesting using iterative FGSM . We do agree that for a fair comparison , L steps of iterative FGSM could be used . However , we note that CW is an iterative optimization-based attack , and is more powerful than iterative FGSM . Since we have shown robustness against CW attacks in Table 4 , we believe iterative FGSM results will be similar ."}, "1": {"review_id": "BkJ3ibb0--1", "review_text": "This paper presents a method to cope with adversarial examples in classification tasks, leveraging a generative model of the inputs. Given an accurate generative model of the input, this approach first projects the input onto the manifold learned by the generative model (the idea being that inputs on this manifold reflect the non-adversarial input distribution). This projected input is then used to produce the classification probabilities. The authors test their method on various adversarially constructed inputs (with varying degrees of noise). Questions/Comments: - I am interested in unpacking the improvement of Defense-GAN over the MagNet auto-encoder based method. Is the MagNet auto-encoder suffering lower accuracy because the projection of an adversarial image is based on an encoding function that is learned only on true data? If the decoder from the MagNet approach were treated purely as a generative model, and the same optimization-based projection approach (proposed in this work) was followed, would the results be comparable? - Is there anything special about the GAN approach, versus other generative approaches? - In the black-box vs. white-box scenarios, can the attacker know the GAN parameters? Is that what is meant by the \"defense network\" (in experiments bullet 2)? - How computationally expensive is this approach take compared to MagNet or other adversarial approaches? Quality: The method appears to be technically correct. Clarity: This paper clearly written; both method and experiments are presented well. Originality: I am not familiar enough with adversarial learning to assess the novelty of this approach. Significance: I believe the main contribution of this method is the optimization-based approach to project onto a generative model's manifold. I think this kernel has the potential to be explored further (e.g. computational speed-up, projection metrics).", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the insightful comments and discussions . A ) Defense-GAN vs . MagNet vs. other generative approaches : We believe that the MagNet auto-encoder suffers lower accuracy compared to Defense-GAN due to the fact that the \u201c reconstruction \u201d step in MagNet is a feed-forward network as opposed to an optimization-based projection as in Defense-GAN . Overall , the combination of MagNet and the classifier can be seen as one deeper classification network , and has a wide attack surface compared to Defense-GAN . As suggested by the reviewer , if the MagNet decoder ( or another generative approach ) was treated as a generative model , and the same optimization-based projection approach was followed , the model with more representative power would perform better . From our experience , GANs tend to have more representative power , but this is still an active area of research and discussion . We believe that , since GANs are specifically designed to optimize for generative tasks , using a GAN in conjunction with our proposed optimization-based projection would outperform an encoder with the same projection method . However , this would be an interesting future research direction . In addition , we were able to show some theoretical guarantees regarding the use and representative power of GANs in equation ( 7 ) . B ) Black- and white-box attacks : In our work and previous literature , it is assumed that in black-box scenarios the attacker does not know the classifier network nor the defense mechanism ( and any parameters thereof ) . The only information the attacker can use is the classifier output . In white-box scenarios , the attacker knows the entire system including the classifier network , defense mechanisms , and all parameters ( which in our case , include GAN parameters ) . By \u201c defense network \u201d in Experiments bullet 2 , we mean the generator network . C ) Computational complexity : Defense-GAN adds inference-time complexity to the classifier . As discussed in Appendix G ( Appendix F in the original version of the paper ) , this complexity depends on L , the number of GD steps used to reconstruct images , and ( to a lesser extent ) R , the number of random restarts . At training time , Defense-GAN requires training a GAN , but no retraining of the classifier is necessary . In comparison , MagNet also adds inference-time complexity . However , the time overhead is much smaller than Defense-GAN as MagNet is simply a feedforward network . At training time , the overhead is similar to Defense-GAN ( training the encoder , no retraining of the classifier ) . Adversarial training adds no inference-time complexity . However , training time can be significantly larger than for other methods since re-training the classifier is required ( preceded by generating the adversarial examples to augment the training dataset ) ."}, "2": {"review_id": "BkJ3ibb0--2", "review_text": "The authors describe a new defense mechanism against adversarial attacks on classifiers (e.g., FGSM). They propose utilizing Generative Adversarial Networks (GAN), which are usually used for training generative models for an unknown distribution, but have a natural adversarial interpretation. In particular, a GAN consists of a generator NN G which maps a random vector z to an example x, and a discriminator NN D which seeks to discriminate between an examples produced by G and examples drawn from the true distribution. The GAN is trained to minimize the max min loss of D on this discrimination task, thereby producing a G (in the limit) whose outputs are indistinguishable from the true distribution by the best discriminator. Utilizing a trained GAN, the authors propose the following defense at inference time. Given a sample x (which has been adversarially perturbed), first project x onto the range of G by solving the minimization problem z* = argmin_z ||G(z) - x||_2. This is done by SGD. Then apply any classifier trained on the true distribution on the resulting x* = G(z*). In the case of existing black-box attacks, the authors argue (convincingly) that the method is both flexible and empirically effective. In particular, the defense can be applied in conjunction with any classifier (including already hardened classifiers), and does not assume any specific attack model. Nevertheless, it appears to be effective against FGSM attacks, and competitive with adversarial training specifically to defend against FGSM. The authors provide less-convincing evidence that the defense is effective against white-box attacks. In particular, the method is shown to be robust against FGSM, RAND+FGSM, and CW white-box attacks. However, it is not clear to me that the method is invulnerable to novel white-box attacks. In particular, it seems that the attacker can design an x which projects onto some desired x* (using some other method entirely), which then fools the classifier downstream. Nevertheless, the method is shown to be an effective tool for hardening any classifier against existing black-box attacks (which is arguably of great practical value). It is novel and should generate further research with respect to understanding its vulnerabilities more completely. Minor Comments: The sentence starting \u201cUnless otherwise specified\u2026\u201d at the top of page 7 is confusing given the actual contents of Tables 1 and 2, which are clarified only by looking at Table 5 in the appendix. This should be fixed. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the constructive review and comments . A ) Regarding the effectiveness against white-box attacks : As the reviewer has pointed out , we have shown the robustness of our method to existing white-box attacks such as FGSM , RAND+FGSM , and CW . Indeed , a good attack strategy could be to design an x which projects onto a desired x * = G ( z * ) . However , this requires solving for : Find x s.t . the output of the gradient-descent block is z * . Per our understanding , the reviewer \u2019 s suggestion is the following : Find a desired x * in the range of the generator which fools the classifier . Find an x which projects onto x * , i.e. , such that the output of the GD block is z * , where G ( z * ) = x * . Step 1 is a more challenging version of existing attacks , due to the constraint that the adversarial example should lie in the range of the generator . While step 1 could potentially be solvable , the real difficulty lies in step 2 . In fact , it is not obvious how to find such an x given x * . What comes to mind is attempting to solve step 2 using an optimization framework , e.g . : Minimize ( over x , z * ) 1 Subject to G ( z * ) = x * z * is the output of the GD block after L steps . We have shown in Appendix B that solving this problem using GD gets more and more prohibitive as L increases . Furthermore , since we use random initializations of z , if the random seed is not accessible by the attacker , there is no guarantee that a fixed x will result in the same fixed z every time after L steps of GD on the MSE . Due to these factors , we believe that our method is robust to a wide range of gradient-based white-box attacks . However , we are very much interested in further research of novel attack methods . B ) We have fixed the minor comments by specifically mentioning the classifier and substitute models for every Table and Figure throughout the paper ."}}