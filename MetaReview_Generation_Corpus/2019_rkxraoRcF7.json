{"year": "2019", "forum": "rkxraoRcF7", "title": "Learning Disentangled Representations with Reference-Based Variational Autoencoders", "decision": "Reject", "meta_review": "This is a proposed method that studies learning of disentangled representations in a relatively specific setting, defined as follows: given two datasets, one unlabeled and another that has a particular factor of variation fixed, the method will disentangle the factor of variation from the others. The reviewers found the method promising, with interesting results (qual & quant).\n\nThe weaknesses of the method as discussed in the reviews and after:\n\n- the quantitative results with weak supervision are not a big improvement over beta-vae-like methods or mathieu et al.\n- a red flag of sorts to me is that it is not very clear where the gains are coming from: the authors claim to have done a fair comparison with the various baselines, but they introduce an entirely new encoder/decoder architecture that was likely (involuntarily, but still) tuned more to their method than others.\n- the setup as presented is somewhat artificial and less general than it could be (however, this was not a major factor in my decision). It is easy to get confused by the kind of disentagled representations that this work is aiming to get.\n\nI think this has the potential to be a solid paper, but at this stage it's missing a number of ablation studies to truly understand what sets it apart from the previous work. At the very least, there is a number of architectural and training choices in Appendix D -- like the 0.25 dropout -- that require more explanation / empirical understanding and how they generalize to other datasets.\n\nGiven all of this, at this point it is hard for me to recommend acceptance of this work. I encourage the authors to take all this feedback into account, extend their work to more domains (the artistic-style disentangling that they mention seems like a good idea) and provide more empirical evidence about their architectural choices and their effect on the results.", "reviews": [{"review_id": "rkxraoRcF7-0", "review_text": "The authors address the problem of representation learning in which data-generative factors of variation are separated, or disentangled, from each other. Pointing out that unsupervised disentangling is hard despite recent breakthroughs, and that supervised disentangling needs a large number of carefully labeled data, they propose a \u201cweakly supervised\u201d approach that does not require explicit factor labels, but instead divides the training data in to two subsets. One set, the \u201creference set\u201d is known to the learning algorithm to leave a set of generative \u201ctarget factors\u201d fixed at one specific value per factor, while the other set is known to the learning algorithm to vary across all generative factors. The problem setup posed by the authors is to separate the corresponding two sets of factors into two non-overlapping sets of latents. Pros: To address this problem, the authors propose an architecture that includes a reverse KL-term in the loss, and they show convincingly that this approach is indeed successful in separating the two sets of generative factors from each other. This is demonstrated in two different ways. First, quantitatively on an a modified MNIST dataset, showing that the information about the target factors is indeed (mostly) in the set of latents that are meant to capture them. Second, qualitatively on the modified MNIST and on a further dataset, AffectNet, which has been carefully curated by the authors to improve the quality of the reference set. The qualitative results are impressive and show that this approach can be used to transfer the target factors from one image, onto another image. Technically, this work combines and extends a set of interesting techniques into a novel framework, applied to a new way of disentangling two sets of factors of variation with a VAE approach. Cons: The problem that this work solves seems somewhat artificial, and the training data, while less burdensome than having explicit labels, is still difficult to obtain in practice. More importantly, though, both the title and the start of the both the abstract and the introduction are somewhat misleading. That\u2019s because this work does not actually address disentangling in the sense of \u201cLearning disentangled representations from visual data, where high-level generative factors correspond to independent dimensions of feature vectors\u2026\u201d What it really addresses is separating two sets of factors into different parts of the representation, within each of which the factors can be, are very likely are, entangled with each other. Related to the point that this work is not really about disentangling, the quantitative comparisons with completely unsupervised baselines are not really that meaningful, at least not in terms of what this work sets out to do. All it shows is whether information about the target factors is easily (linearly) decodable from the latents, which, while related to disentangling, says little about the quality of it. On the positive side, this kind of quantitative comparison (where the authors approach has to show that the information exists in the correct part of the space) is not pitted unfairly against the unsupervised baselines. === Update: The authors have made a good effort to address the concerns raised, and I believe the paper should be accepted in its current form. I have increased my rating from 6 to 7, accordingly. ", "rating": "7: Good paper, accept", "reply_text": "We would like to thank the reviewer for his useful comments and remarks . Detailed discussion about his specific concerns are addressed below . * * * ( R1.Q0 ) Clarification about quantitative results First of all , we would like to clarify that our experiments also include quantitative evaluation over the AffectNet . From the reviewer 's description in the \u201c Pros \u201d section , it could be interpreted that we only provide quantitative results over this dataset . * * * ( R1.Q1 ) - Practical applications of the learning setting : \u201c .. The problem that this work solves seems somewhat artificial ... \u201d We strongly believe that addressing the introduced problem can be useful in different scenarios . One of the motivation of our experiments on the AffectNet was to show a concrete advantage of this type of supervision in a practical case . Note that in facial behavior analysis/synthesis large-scale datasets are typically very hard to annotate . The reason is that facial gestures depend on a combination of a large number of facial muscle activations and their corresponding intensities ( i.e.Action Units ) [ Ekman , 1997 ] . Therefore , fine-grained annotation of facial gestures is very tedious and require expert coders . By contrast , collecting a large data set of neutral faces is much easier and can be carried out by non-expert annotators . Another interesting application that we plan to explore in future work is \u201c weakly-supervised \u201d artistic-style disentangling . In this case , we will consider the unlabelled dataset to be a collection of paintings ( containing a large-number of styles that do not need to be labelled ) . On the other hand , we will consider the reference samples as images with a \u201c constant \u201d style ( real photographs ) . Note that in this case , the reference dataset would be almost free to collect . By training our model on this data , we would be able to learn a latent representation of the painting styles with no supervision and manipulate it in order to transfer styles , interpolate them or synthetically generate new ones . Following the same idea , another potential application where the reference-based supervision could be useful is automatic colorization of grayscale photographs . In this case , multiple colorizations for the same picture could be synthetized by injecting random noise into the latent variable e. Note that in this scenario , the reference images would be obtained by removing the color of natural images ( forming the unlabelled set ) . Again , in this application the reference-set would be very easy to collect ."}, {"review_id": "rkxraoRcF7-1", "review_text": "Summary: Given two sets of data, where one is unlabelled and the other is a reference data set with a particular factor of variation that is fixed, the approach disentangles this factor of variation from the others. The approach uses a VAE whose latents are split into e that represents the factor of variation and z that represents the remaining factors. A symmetric KL loss that is approximated using the density-ratio trick is optimised for the learning, and the method is applied to MNIST digit style disentangling and AffectNet facial expression disentangling. Pros: - Clearly written - Results look promising, both quantitative and qualitative. Cons: - Mathieu et al disentangle a specific factor from others without explicit labels but by drawing two images with the same value of the specified factor (i.e. drawing from the reference set) and also drawing a third image with a any value of the specified factor (i.e. drawing from the unlabelled set). Hence their approach is directly applicable to the problem at hand in the paper. Although Mathieu et al use digit/face identity as the shared factor, their method is directly applicable to the case where the shared factor is digit style/facial expression. Hence it appears to me that it should be compared against. - missing reference - Bouchacourt - explicit labels aren\u2019t given and data is grouped where each group shares a factor of var. But here the data is assumed to be partitioned into groups, so there is no equivalent to the unlablled set, hence difficult to compare against for the outlined tasks. - Regarding comparison against unsupervised disentangling methods, there have been more recent approaches since betaVAE and DIP-VAE (e.g. FactorVAE (Kim et al) TCVAE (Chen et al)). It would be nice to compare against these methods, not only via predictive accuracy of target factors but also using disentangling metrics specified in these papers. Other Qs/comments - the KL terms in (5) are intractable due to the densities p^u(x) and p^r(x), hence two separate discriminators need to be used to approximate two separate density ratios, making the model rather large and complicated with many moving parts. What would happen if these KL terms in (5) are dropped and one simply uses SGVB to optimise the resulting loss without the need for discriminators? Usually discriminators tend to heavily underestimate density ratios (See e.g. Rosca et al), especially densities defined on high dimensions, so it might be best to avoid them whenever possible. The requirement of adding reconstruction terms to the loss in (10) is perhaps evidence of this, because these reconstruction terms are already present in the loss (3) & (5) that the discriminator should be approximating. So the necessity of extra regularisation of these reconstruction terms suggests that the discriminator is giving poor estimates of them. The reconstruction terms for z,e in (5) appear sufficient to force the model to use e (which is the motivation given in the paper for using the symmetric KL), akin to how InfoGAN forces the model to use the latents, so the necessity of the KL terms in (5) is questionable and appears to need further justification and/or ablation studies. - (minor) why not learn the likelihood variance lambda? ************* Revision ************* I am convinced by the rebuttal of the authors, hence have modified my score accordingly.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for his useful comments and suggestions . Detailed comments about specific concerns are addressed below . * * * ( R2.Q1 ) Comparison with [ Mathieu et . al , 2016 ] We would like to clarify that the type of supervision used in [ Mathieu et . al , 2016 ] is not equivalent to the one assumed in our problem . As discussed in R3.Q8 , \u201c the reference-based setting is different from the scenario where information about samples sharing the same target factors is available . In particular , in our case we only know that reference images share the same label . In contrast , for the unlabelled distribution we do not have access to this information \u201d . Note that this fact renders the original learning algorithm proposed in [ Mathieu et . al , 2016 ] inapplicable in our context . The reason is that for any unlabelled image , we should be able to sample another image with the same generative factor ( e.g.the same expression ) , given that we need to reconstruct them by swapping their latent representation e. Intuitively , this shows that reference-based disentangling is a more challenging problem than the one addressed in [ Mathieu et . al , 2016 ] . The reason is that the amount of available supervision is lower ( following the original paper nomenclature , only one type of \u201c id \u201d is labelled ) . Beyond the discussed difference , we agree with the reviewer that it is interesting to evaluate how the approach presented in [ Mathieu et . al , 2016 ] behaves if only a single id is available ( i.e , the reference label ) . For this reason , we have implemented this method using the same network architectures as in the rest of our models . Following the experimental evaluation described in Sec.5.3 , we have trained it by using the procedure suggested by the reviewer ( i.e , only pairs of reference images are used during training and no labels for unlabelled images are assumed ) . Note that this implies a modification over the original learning algorithm . We have added the results in Table 1 . As can be seen , with the method of [ Mathieu et . al , 2016 ] we obtain reasonable results in the AffectNet dataset . However , sRBD-VAE achieves better average accuracy . On the other hand , in the MNIST dataset , using the approach of [ Mathieu et . al , 2016 ] obtains poor performance compared to most methods . These results confirm that our approach is better suited to exploit the weak-supervision provided by the reference set of images . We have added this evaluation into the revised paper ( see baselines in Sec 5.2 and discussion of the results in Sec 5.3 ) It is also worth mentioning that an advantage of our model compared to [ Mathieu et . al , 2016 ] is that we are able to naturally address conditional image generation ( Sec 5.4 ) by sampling latent variables from p ( e ) . Note that in [ Mathieu et . al ] this is not possible given that no prior over the target factors e is forced . * * * ( R2.Q2 ) \u201c missing reference - Bouchacourt \u201d We have added the reference in Related Work section . * * * ( R2.Q3 ) \u201c ... there have been more recent approaches since betaVAE and DIP-VAE ( e.g.FactorVAE ( Kim et al ) TCVAE ( Chen et al ) ) . It would be nice to compare against these methods , not only via predictive accuracy of target factors but also using disentangling metrics specified in these papers\u2026 \u201d We thank the reviewer for pointing out these recent unsupervised methods . We have added these references to the updated version of the paper . For the sake of completeness of our evaluation , we have implemented the method proposed by [ Chen et . al,2018 ] ( note that TCVAE and FactorVAE minimize the same objective function ) and run the same experiments described in Sec.5.3 of our paper . We have added the quantitative results in Table 1 of the updated paper . Note that TCVAE obtain a similar average performance compared to other unsupervised approaches like bVAE or DIP-VAE . Moreover , the average results obtained by our method in both datasets are better . Therefore , our conclusions in this experiment remain unchanged . On the other hand , we would like to clarify that the metrics proposed in [ Chen et . al , 2018 , Kim et . al , 2018 ] are specifically designed for evaluating how a single dimension of the latent representation corresponds to a single ground-truth generative factor . As we have discussed in our response to Reviewer 1 ( see Q1.R1 ) , this is not the goal of our work and we believe that the one-to-one mapping assumption is not realistic when modelling high-level generative factors . For example , it is not reasonable to expect that a single dimension of the latent vector e can convey all the information about a complex generative factor such as the facial expression . Therefore , we think that the metrics proposed in the cited works are not appropriate in our context ."}, {"review_id": "rkxraoRcF7-2", "review_text": "The paper proposes reference based VAEs, which considers learning semantically meaningful feature with weak supervision. The latent variable contains two parts, one related to the reference set and the other irrelevant. To prevent degenerate solutions, the paper proposed to use reverse KL resulting in a ALICE-style objective. The paper demonstrates interesting empirical results on feature prediction, conditional image generation and image synthesis. I don\u2019t really see how Equation (5) in symmetric KL prevents learning redundant z (i.e. z contains all information of e). It seems one could have both KL terms near zero but also have p(x|z, e) = p(x|z)? One scenario would be the case where z contains all the information about e (which learns the reference latent features), so we have redundant information in z. In this case, the learned features e are informative but the decoder does not use e anyways. To ensure that z does not contain information about e, one could add an adversarial predictor that tries to predict e from z. Note that this cannot be detected by the feature learning metric because it ignores z for RbVAE during training. The experiments on conditional image generation look interesting, but I wonder if the ground truth transformation for MNIST can be simply described as in some linear transformation on the original image. I wonder if the proposed method works on SVHN, where you can use label information as reference supervision. Moreover, I wonder if it is possible to use multiple types of reference images, but fewer images in each type, to reach comparable or even better performance. Minor points: - Why assume that the reference distribution is delta distribution whose support has measure zero, instead of a regular Gaussian? - (6), (8), (10) seems over complicated due to the semi-supervised nature of the objective. I wonder if having an additional figure would make things clearer. - Maybe it is helpful to cite the ALICE paper (Li et al) for Equation (10). - Table 1, maybe add the word \u201crespectively\u201d so it is clearer which metric you use for which dataset. - I wonder if it is fair enough to compare feature prediction with VAE and other models since they do not use any \u201cweak supervision\u201d; a fairer baseline could consider learning with the weak supervision labels (containing the information that some images have the same label). The improvement on AffectNet compared to regular VAE does not look amazing given the additional weak supervision. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for his detailed feedback . Following , we address his concerns . * * * ( R3.Q1 ) \u201c I don \u2019 t really see how Equation ( 5 ) in symmetric KL prevents learning redundant z ( i.e.z contains all information of e ) \u201d ... \u201d To ensure that z does not contain information about e , one could add an adversarial predictor that tries to predict e from z \u201d We thank the reviewer for arising this question . We didn \u2019 t think about this potential \u201c degenerate \u201d solution before ( was not observed in our experiments ) and we have concluded that our model naturally avoids the case where redundant information from e is encoded into z . The rationale is as follows . Note that the classifier d ( x , z , e ) is trained in order to discriminate triplets { x , z , e } obtained from the distributions q ( z , e|x ) ( x ) and p ( x|z , e ) p ( z ) p ( e ) . On the other hand , the model encoder and generator try to make these distributions as similar as possible . Consider the scenario where a latent z sampled from q ( z|x ) contains ( redundant ) information about a sample e generated from q ( e|x ) . In this case , z and e would be conditionally dependent . In contrast , latent variables e and z generated by p ( z ) p ( e ) are independent ( given that the priors are defined by an isotropic Gaussian distribution ) . Therefore , our model is penalized in this case since the the discriminator d ( x , z , e ) would easily differentiate between both distributions ( by exploiting the dependency present in q ( z , e|x ) but not in p ( z ) p ( e ) ) . Interestingly , note that the \u201c adversarial predictor \u201d suggested by the reviewer is already implicitly implemented by the discriminator d ( x , z , e ) . We have discussed this issue in the updated version ( before last paragraph of Sec 4.3 \u201c Optimization via Adversarial Learning \u201d ) . * * * ( R3.Q2 ) \u201c I wonder if the ground truth transformation for MNIST can be simply described as in some linear transformation on the original image ... \u201c The transformations applied to the MNIST datasets are : ( i ) Colorization , ( ii ) Modification of the stroke width and ( iii ) Resizing + zero-padding . Apart from ( i ) , ( ii ) and ( iii ) are not linearly dependent on the transformation parameter . * * * ( R3.Q3 ) \u201c I wonder if the proposed method works on SVHN , where you can use label information as reference supervision ... \u201c Note that our main motivation is to learn a disentangled representation without explicit labelling of the underlying target factors . Using the SVHN as suggested ( i.e , considering the digit labels ) would imply that the factors of interest are annotated and , therefore , that full or semi-supervision is provided during training . We would like to emphasize that we are focused in the weakly-supervised setting , where explicit annotations are not needed in order to disentangle the target factors . * * * ( R3.Q4 ) \u201c I wonder if it is possible to use multiple types of reference images ... \u201c If we understand correctly , in the described setting each reference set would contain images with a specific set of \u201c constant \u201d factors . In this case , we think that our model could easily adress the suggested scenario by splitting the latent variables into more than two subsets and using different discriminators for each reference distribution . If the reviewer has a concrete idea about a potential scenario where this setting is interesting , we would be grateful to know it . We are currently exploring potential extensions and applications of our proposed model for future work . * * * ( R3.Q5 ) \u201c Why assume that the reference distribution is delta distribution whose support has measure zero , instead of a regular Gaussian ? \u201c Using a \u201c delta shaped \u201d prior over latents e allows us to model the assumption that variation factors are constant across reference images . In contrast , note that for unlabelled images the prior p ( e ) is indeed modelled as a regular Gaussian . * * * ( R3.Q6 ) \u201c ( 6 ) , ( 8 ) , ( 10 ) seems over complicated due to the semi-supervised nature of the objective . I wonder if having an additional figure would make things clearer ... \u201c Thank you for the suggestion . We have added Fig.5 in the Appendix B in order to clarify the formulation and illustrate the training process . * * * ( R3.Q7 ) \u201c Maybe it is helpful to cite the ALICE paper ( Li et al ) for Equation ( 10 ) . Table 1 , maybe add the word \u201c respectively \u201d so it is clearer which metric you use for which dataset ... \u201c Suggested changes are added in the updated version of the paper ."}], "0": {"review_id": "rkxraoRcF7-0", "review_text": "The authors address the problem of representation learning in which data-generative factors of variation are separated, or disentangled, from each other. Pointing out that unsupervised disentangling is hard despite recent breakthroughs, and that supervised disentangling needs a large number of carefully labeled data, they propose a \u201cweakly supervised\u201d approach that does not require explicit factor labels, but instead divides the training data in to two subsets. One set, the \u201creference set\u201d is known to the learning algorithm to leave a set of generative \u201ctarget factors\u201d fixed at one specific value per factor, while the other set is known to the learning algorithm to vary across all generative factors. The problem setup posed by the authors is to separate the corresponding two sets of factors into two non-overlapping sets of latents. Pros: To address this problem, the authors propose an architecture that includes a reverse KL-term in the loss, and they show convincingly that this approach is indeed successful in separating the two sets of generative factors from each other. This is demonstrated in two different ways. First, quantitatively on an a modified MNIST dataset, showing that the information about the target factors is indeed (mostly) in the set of latents that are meant to capture them. Second, qualitatively on the modified MNIST and on a further dataset, AffectNet, which has been carefully curated by the authors to improve the quality of the reference set. The qualitative results are impressive and show that this approach can be used to transfer the target factors from one image, onto another image. Technically, this work combines and extends a set of interesting techniques into a novel framework, applied to a new way of disentangling two sets of factors of variation with a VAE approach. Cons: The problem that this work solves seems somewhat artificial, and the training data, while less burdensome than having explicit labels, is still difficult to obtain in practice. More importantly, though, both the title and the start of the both the abstract and the introduction are somewhat misleading. That\u2019s because this work does not actually address disentangling in the sense of \u201cLearning disentangled representations from visual data, where high-level generative factors correspond to independent dimensions of feature vectors\u2026\u201d What it really addresses is separating two sets of factors into different parts of the representation, within each of which the factors can be, are very likely are, entangled with each other. Related to the point that this work is not really about disentangling, the quantitative comparisons with completely unsupervised baselines are not really that meaningful, at least not in terms of what this work sets out to do. All it shows is whether information about the target factors is easily (linearly) decodable from the latents, which, while related to disentangling, says little about the quality of it. On the positive side, this kind of quantitative comparison (where the authors approach has to show that the information exists in the correct part of the space) is not pitted unfairly against the unsupervised baselines. === Update: The authors have made a good effort to address the concerns raised, and I believe the paper should be accepted in its current form. I have increased my rating from 6 to 7, accordingly. ", "rating": "7: Good paper, accept", "reply_text": "We would like to thank the reviewer for his useful comments and remarks . Detailed discussion about his specific concerns are addressed below . * * * ( R1.Q0 ) Clarification about quantitative results First of all , we would like to clarify that our experiments also include quantitative evaluation over the AffectNet . From the reviewer 's description in the \u201c Pros \u201d section , it could be interpreted that we only provide quantitative results over this dataset . * * * ( R1.Q1 ) - Practical applications of the learning setting : \u201c .. The problem that this work solves seems somewhat artificial ... \u201d We strongly believe that addressing the introduced problem can be useful in different scenarios . One of the motivation of our experiments on the AffectNet was to show a concrete advantage of this type of supervision in a practical case . Note that in facial behavior analysis/synthesis large-scale datasets are typically very hard to annotate . The reason is that facial gestures depend on a combination of a large number of facial muscle activations and their corresponding intensities ( i.e.Action Units ) [ Ekman , 1997 ] . Therefore , fine-grained annotation of facial gestures is very tedious and require expert coders . By contrast , collecting a large data set of neutral faces is much easier and can be carried out by non-expert annotators . Another interesting application that we plan to explore in future work is \u201c weakly-supervised \u201d artistic-style disentangling . In this case , we will consider the unlabelled dataset to be a collection of paintings ( containing a large-number of styles that do not need to be labelled ) . On the other hand , we will consider the reference samples as images with a \u201c constant \u201d style ( real photographs ) . Note that in this case , the reference dataset would be almost free to collect . By training our model on this data , we would be able to learn a latent representation of the painting styles with no supervision and manipulate it in order to transfer styles , interpolate them or synthetically generate new ones . Following the same idea , another potential application where the reference-based supervision could be useful is automatic colorization of grayscale photographs . In this case , multiple colorizations for the same picture could be synthetized by injecting random noise into the latent variable e. Note that in this scenario , the reference images would be obtained by removing the color of natural images ( forming the unlabelled set ) . Again , in this application the reference-set would be very easy to collect ."}, "1": {"review_id": "rkxraoRcF7-1", "review_text": "Summary: Given two sets of data, where one is unlabelled and the other is a reference data set with a particular factor of variation that is fixed, the approach disentangles this factor of variation from the others. The approach uses a VAE whose latents are split into e that represents the factor of variation and z that represents the remaining factors. A symmetric KL loss that is approximated using the density-ratio trick is optimised for the learning, and the method is applied to MNIST digit style disentangling and AffectNet facial expression disentangling. Pros: - Clearly written - Results look promising, both quantitative and qualitative. Cons: - Mathieu et al disentangle a specific factor from others without explicit labels but by drawing two images with the same value of the specified factor (i.e. drawing from the reference set) and also drawing a third image with a any value of the specified factor (i.e. drawing from the unlabelled set). Hence their approach is directly applicable to the problem at hand in the paper. Although Mathieu et al use digit/face identity as the shared factor, their method is directly applicable to the case where the shared factor is digit style/facial expression. Hence it appears to me that it should be compared against. - missing reference - Bouchacourt - explicit labels aren\u2019t given and data is grouped where each group shares a factor of var. But here the data is assumed to be partitioned into groups, so there is no equivalent to the unlablled set, hence difficult to compare against for the outlined tasks. - Regarding comparison against unsupervised disentangling methods, there have been more recent approaches since betaVAE and DIP-VAE (e.g. FactorVAE (Kim et al) TCVAE (Chen et al)). It would be nice to compare against these methods, not only via predictive accuracy of target factors but also using disentangling metrics specified in these papers. Other Qs/comments - the KL terms in (5) are intractable due to the densities p^u(x) and p^r(x), hence two separate discriminators need to be used to approximate two separate density ratios, making the model rather large and complicated with many moving parts. What would happen if these KL terms in (5) are dropped and one simply uses SGVB to optimise the resulting loss without the need for discriminators? Usually discriminators tend to heavily underestimate density ratios (See e.g. Rosca et al), especially densities defined on high dimensions, so it might be best to avoid them whenever possible. The requirement of adding reconstruction terms to the loss in (10) is perhaps evidence of this, because these reconstruction terms are already present in the loss (3) & (5) that the discriminator should be approximating. So the necessity of extra regularisation of these reconstruction terms suggests that the discriminator is giving poor estimates of them. The reconstruction terms for z,e in (5) appear sufficient to force the model to use e (which is the motivation given in the paper for using the symmetric KL), akin to how InfoGAN forces the model to use the latents, so the necessity of the KL terms in (5) is questionable and appears to need further justification and/or ablation studies. - (minor) why not learn the likelihood variance lambda? ************* Revision ************* I am convinced by the rebuttal of the authors, hence have modified my score accordingly.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for his useful comments and suggestions . Detailed comments about specific concerns are addressed below . * * * ( R2.Q1 ) Comparison with [ Mathieu et . al , 2016 ] We would like to clarify that the type of supervision used in [ Mathieu et . al , 2016 ] is not equivalent to the one assumed in our problem . As discussed in R3.Q8 , \u201c the reference-based setting is different from the scenario where information about samples sharing the same target factors is available . In particular , in our case we only know that reference images share the same label . In contrast , for the unlabelled distribution we do not have access to this information \u201d . Note that this fact renders the original learning algorithm proposed in [ Mathieu et . al , 2016 ] inapplicable in our context . The reason is that for any unlabelled image , we should be able to sample another image with the same generative factor ( e.g.the same expression ) , given that we need to reconstruct them by swapping their latent representation e. Intuitively , this shows that reference-based disentangling is a more challenging problem than the one addressed in [ Mathieu et . al , 2016 ] . The reason is that the amount of available supervision is lower ( following the original paper nomenclature , only one type of \u201c id \u201d is labelled ) . Beyond the discussed difference , we agree with the reviewer that it is interesting to evaluate how the approach presented in [ Mathieu et . al , 2016 ] behaves if only a single id is available ( i.e , the reference label ) . For this reason , we have implemented this method using the same network architectures as in the rest of our models . Following the experimental evaluation described in Sec.5.3 , we have trained it by using the procedure suggested by the reviewer ( i.e , only pairs of reference images are used during training and no labels for unlabelled images are assumed ) . Note that this implies a modification over the original learning algorithm . We have added the results in Table 1 . As can be seen , with the method of [ Mathieu et . al , 2016 ] we obtain reasonable results in the AffectNet dataset . However , sRBD-VAE achieves better average accuracy . On the other hand , in the MNIST dataset , using the approach of [ Mathieu et . al , 2016 ] obtains poor performance compared to most methods . These results confirm that our approach is better suited to exploit the weak-supervision provided by the reference set of images . We have added this evaluation into the revised paper ( see baselines in Sec 5.2 and discussion of the results in Sec 5.3 ) It is also worth mentioning that an advantage of our model compared to [ Mathieu et . al , 2016 ] is that we are able to naturally address conditional image generation ( Sec 5.4 ) by sampling latent variables from p ( e ) . Note that in [ Mathieu et . al ] this is not possible given that no prior over the target factors e is forced . * * * ( R2.Q2 ) \u201c missing reference - Bouchacourt \u201d We have added the reference in Related Work section . * * * ( R2.Q3 ) \u201c ... there have been more recent approaches since betaVAE and DIP-VAE ( e.g.FactorVAE ( Kim et al ) TCVAE ( Chen et al ) ) . It would be nice to compare against these methods , not only via predictive accuracy of target factors but also using disentangling metrics specified in these papers\u2026 \u201d We thank the reviewer for pointing out these recent unsupervised methods . We have added these references to the updated version of the paper . For the sake of completeness of our evaluation , we have implemented the method proposed by [ Chen et . al,2018 ] ( note that TCVAE and FactorVAE minimize the same objective function ) and run the same experiments described in Sec.5.3 of our paper . We have added the quantitative results in Table 1 of the updated paper . Note that TCVAE obtain a similar average performance compared to other unsupervised approaches like bVAE or DIP-VAE . Moreover , the average results obtained by our method in both datasets are better . Therefore , our conclusions in this experiment remain unchanged . On the other hand , we would like to clarify that the metrics proposed in [ Chen et . al , 2018 , Kim et . al , 2018 ] are specifically designed for evaluating how a single dimension of the latent representation corresponds to a single ground-truth generative factor . As we have discussed in our response to Reviewer 1 ( see Q1.R1 ) , this is not the goal of our work and we believe that the one-to-one mapping assumption is not realistic when modelling high-level generative factors . For example , it is not reasonable to expect that a single dimension of the latent vector e can convey all the information about a complex generative factor such as the facial expression . Therefore , we think that the metrics proposed in the cited works are not appropriate in our context ."}, "2": {"review_id": "rkxraoRcF7-2", "review_text": "The paper proposes reference based VAEs, which considers learning semantically meaningful feature with weak supervision. The latent variable contains two parts, one related to the reference set and the other irrelevant. To prevent degenerate solutions, the paper proposed to use reverse KL resulting in a ALICE-style objective. The paper demonstrates interesting empirical results on feature prediction, conditional image generation and image synthesis. I don\u2019t really see how Equation (5) in symmetric KL prevents learning redundant z (i.e. z contains all information of e). It seems one could have both KL terms near zero but also have p(x|z, e) = p(x|z)? One scenario would be the case where z contains all the information about e (which learns the reference latent features), so we have redundant information in z. In this case, the learned features e are informative but the decoder does not use e anyways. To ensure that z does not contain information about e, one could add an adversarial predictor that tries to predict e from z. Note that this cannot be detected by the feature learning metric because it ignores z for RbVAE during training. The experiments on conditional image generation look interesting, but I wonder if the ground truth transformation for MNIST can be simply described as in some linear transformation on the original image. I wonder if the proposed method works on SVHN, where you can use label information as reference supervision. Moreover, I wonder if it is possible to use multiple types of reference images, but fewer images in each type, to reach comparable or even better performance. Minor points: - Why assume that the reference distribution is delta distribution whose support has measure zero, instead of a regular Gaussian? - (6), (8), (10) seems over complicated due to the semi-supervised nature of the objective. I wonder if having an additional figure would make things clearer. - Maybe it is helpful to cite the ALICE paper (Li et al) for Equation (10). - Table 1, maybe add the word \u201crespectively\u201d so it is clearer which metric you use for which dataset. - I wonder if it is fair enough to compare feature prediction with VAE and other models since they do not use any \u201cweak supervision\u201d; a fairer baseline could consider learning with the weak supervision labels (containing the information that some images have the same label). The improvement on AffectNet compared to regular VAE does not look amazing given the additional weak supervision. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for his detailed feedback . Following , we address his concerns . * * * ( R3.Q1 ) \u201c I don \u2019 t really see how Equation ( 5 ) in symmetric KL prevents learning redundant z ( i.e.z contains all information of e ) \u201d ... \u201d To ensure that z does not contain information about e , one could add an adversarial predictor that tries to predict e from z \u201d We thank the reviewer for arising this question . We didn \u2019 t think about this potential \u201c degenerate \u201d solution before ( was not observed in our experiments ) and we have concluded that our model naturally avoids the case where redundant information from e is encoded into z . The rationale is as follows . Note that the classifier d ( x , z , e ) is trained in order to discriminate triplets { x , z , e } obtained from the distributions q ( z , e|x ) ( x ) and p ( x|z , e ) p ( z ) p ( e ) . On the other hand , the model encoder and generator try to make these distributions as similar as possible . Consider the scenario where a latent z sampled from q ( z|x ) contains ( redundant ) information about a sample e generated from q ( e|x ) . In this case , z and e would be conditionally dependent . In contrast , latent variables e and z generated by p ( z ) p ( e ) are independent ( given that the priors are defined by an isotropic Gaussian distribution ) . Therefore , our model is penalized in this case since the the discriminator d ( x , z , e ) would easily differentiate between both distributions ( by exploiting the dependency present in q ( z , e|x ) but not in p ( z ) p ( e ) ) . Interestingly , note that the \u201c adversarial predictor \u201d suggested by the reviewer is already implicitly implemented by the discriminator d ( x , z , e ) . We have discussed this issue in the updated version ( before last paragraph of Sec 4.3 \u201c Optimization via Adversarial Learning \u201d ) . * * * ( R3.Q2 ) \u201c I wonder if the ground truth transformation for MNIST can be simply described as in some linear transformation on the original image ... \u201c The transformations applied to the MNIST datasets are : ( i ) Colorization , ( ii ) Modification of the stroke width and ( iii ) Resizing + zero-padding . Apart from ( i ) , ( ii ) and ( iii ) are not linearly dependent on the transformation parameter . * * * ( R3.Q3 ) \u201c I wonder if the proposed method works on SVHN , where you can use label information as reference supervision ... \u201c Note that our main motivation is to learn a disentangled representation without explicit labelling of the underlying target factors . Using the SVHN as suggested ( i.e , considering the digit labels ) would imply that the factors of interest are annotated and , therefore , that full or semi-supervision is provided during training . We would like to emphasize that we are focused in the weakly-supervised setting , where explicit annotations are not needed in order to disentangle the target factors . * * * ( R3.Q4 ) \u201c I wonder if it is possible to use multiple types of reference images ... \u201c If we understand correctly , in the described setting each reference set would contain images with a specific set of \u201c constant \u201d factors . In this case , we think that our model could easily adress the suggested scenario by splitting the latent variables into more than two subsets and using different discriminators for each reference distribution . If the reviewer has a concrete idea about a potential scenario where this setting is interesting , we would be grateful to know it . We are currently exploring potential extensions and applications of our proposed model for future work . * * * ( R3.Q5 ) \u201c Why assume that the reference distribution is delta distribution whose support has measure zero , instead of a regular Gaussian ? \u201c Using a \u201c delta shaped \u201d prior over latents e allows us to model the assumption that variation factors are constant across reference images . In contrast , note that for unlabelled images the prior p ( e ) is indeed modelled as a regular Gaussian . * * * ( R3.Q6 ) \u201c ( 6 ) , ( 8 ) , ( 10 ) seems over complicated due to the semi-supervised nature of the objective . I wonder if having an additional figure would make things clearer ... \u201c Thank you for the suggestion . We have added Fig.5 in the Appendix B in order to clarify the formulation and illustrate the training process . * * * ( R3.Q7 ) \u201c Maybe it is helpful to cite the ALICE paper ( Li et al ) for Equation ( 10 ) . Table 1 , maybe add the word \u201c respectively \u201d so it is clearer which metric you use for which dataset ... \u201c Suggested changes are added in the updated version of the paper ."}}