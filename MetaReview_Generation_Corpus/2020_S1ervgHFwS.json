{"year": "2020", "forum": "S1ervgHFwS", "title": "Adversarial Training Generalizes Data-dependent Spectral Norm Regularization", "decision": "Reject", "meta_review": "This paper shows an theoretical equivalence between the L2 PGD adversarial training and operator norm regularization. It gives an interesting observation and support it from both theoretical arguments and practical experiments. There has been a significant discussion between the reviewers and authors. Although the authors made efforts in rebuttal, it still leaves many places to improve and clarify, especially in improving the mathematical rigor of the  proof and experiments using state-of-the-art networks. \n\n", "reviews": [{"review_id": "S1ervgHFwS-0", "review_text": "Adversarial training generalizes data-dependent spectral norm regularization This paper shows that, projected gradient descent based adversarial training is similar to the data-dependent spectral norm regularization, and under very restrictive condition, the authors show that this two methods are the same. Some experiments are conducted to support the theory. Overall, I think this paper is marginal, while the experiments are not convincing. First, the relation between spectral normalization and adversarial training have been investigated by [1], while the fast computational of maximum singular value with power methods have also been proposed in [1]. The authors only give a data-dependent version of the spectral normalization based on the Jacobian of the neural networks, which I think is somewhat weak. The experiments are limited with specific settings that are not generally used in practice, which alleviate my confidence on this paper\u2019s results. Also, the experiment section contain several not so important information. I think the authors should do far more experiments to support the main claim, while move these additional justification to the appendix. Detailed comments: 1. I think the claim of theorem 1 is somewhat ambiguous. How to guarantee there exists such epsilon satisfies this condition? Is this the case we face in the real world? What will happen if \\alpha is not sufficiently large? If we don\u2019t use logits pairing and \\ell_2 norm constraint, will the claim hold? I think the correlation behinds the spectral norm and adversarial training is well investigated and use this correlation as the intuition behinds work is enough. This theorem cannot convince me that the proposed methods have a strong theoretical basis. 2. Generally, the neural networks have a large number of parameters (~ millions) for image classification task. The global spectral norm regularization only needs to calculate the spectral norm of each layer\u2019s weight matrix, whose computational cost is acceptable. However, to calculate the Jacobian and use the power methods, we will additionally do several forward pass and backward pass just as adversarial training. As a regularization technique, is this calculation tolerable? If this is some variant of the adversarial training, I don\u2019t find the experiment results support the claim that it will outperform the adversarial training consistantly. 3. Why don\u2019t use some standard neural network architecture like ResNet? As this results is not comparable to other existing work, I\u2019m not sure if this result is meaningful. Also, are the comparisons fair? For example, the regularization coefficient of global spectral norm regularization and data-dependent spectral norm regularization are far more different. And the authors use only 1 iteration to calculate the singular value in global spectral norm regularization, why to do that? Also, what\u2019s the result compared with \\ell_p norm constraint adversarial training? 4. The evaluation of some assumption on the network is better moved to appendix, as this is only some sanity check, not the core contribution. More experiments with ResNet, WideResNet, MobileNet etc. on CIFAR100 and ImageNet are more convincing. 5. What\u2019s the attack method in the main context? 6. I think the discussion in Appendix A.5 is somewhat confusing. If the authors want to argue that the network is locally linear so that we can approximate with linear regression, why should we use the power methods? Still, I feel the contribution of this paper is somewhat weak. I don\u2019t see any improvements of the proposed algorithms compared with the standard adversarial training, as well as the theoretical contribution like adversarial generalization. The experiments are not convincing, as the setting is different from the general setting the community used in adversarial training. I\u2019m not familiar with the results in global spectral normalization and it\u2019s possible that the global spectral normalization may have little gain in adversarial robustness, but in my opinion, the main contribution [1] is the generalization analysis of spectral normalized adversarial trained neural networks, which this paper lacks. On the empirical side, the computation efficiency and performance of the proposed algorithms don\u2019t outperform adversarial training much. So I tend to reject this paper. [1] Farnia, Farzan, Jesse Zhang, and David Tse. \"Generalizable Adversarial Training via Spectral Normalization.\" International Conference on Learning Representations, 2019.", "rating": "1: Reject", "reply_text": "* This is part 3 of 3 of our response * > > 3 . Why don \u2019 t use some standard neural network architecture like ResNet ? Also , are the comparisons fair ? For example , the regularization coefficient of global SNR and d.d . SNR are different . And the authors use only 1 iteration to calculate the singular value in global spectral norm regularization , why to do that ? \u201c The regularization constants were chosen such that the models achieve roughly the same test set accuracy on clean examples as the adversarially trained model does. \u201d as was clearly stated in our paper . Hence , yes , the comparisons are fair . For global SNR , we try to stay as close as possible to the original authors ' suggestions . Yoshida & Miato write \u201c One [ power method ] iteration [ per parameter update ] was adequate in our experiments \u201d and \u201c we performed only one [ power method ] iteration [ per parameter update ] because it was adequate for obtaining a sufficiently good approximation \u201d . Note , the computation of the data-independent regularizer decouples from the empirical loss , hence the power-method iterations can be amortized across data-points . As stated above , our network architecture is standard , is publically available and is used throughout research . > > 4.The evaluation of some assumption on the network is better moved to appendix , as this is only some sanity check , not the core contribution . More experiments with ResNet , WideResNet , MobileNet etc . on CIFAR100 and ImageNet are more convincing . Firstly , it is unclear what evaluation of assumptions the reviewer is referring to . Secondly , we sincerely do not expect to see any difference regarding the correspondence \u201c AT < - > d.d . SNR \u201d on other architectures / data sets . Our theorem proves that \\ell_2-norm constrained PGA-based AT and d.d . SNR are equivalent for small enough epsilon , while our extensive experiments show that in practice , the correspondence between AT and d.d . SNR holds approximately in a region much larger than proved in the Theorem , the region being roughly the size of the epsilon * -ball used during adversarial training ( epsilon * = 1.75 > > epsilon in Theorem ) , see Figure 2 ( left ) and discussion in Section 5.3 \u201c Validity of linear approximation \u201d . Sure , more experiments can always be requested , but we believe that confirming our main claims in practice is more important and valuable than including one further architecture or dataset . Please also consider our comments from the `` general comments '' section at the beginning of this review on this topic . > > 5.What \u2019 s the attack method in the main context ? We evaluated against \\ell_2-norm constrained PGA in the main text , as stated in Section 5.1 and Table 1 . Additional results for \\ell_\\infty PGA attack are provided in the Appendix . > > 6.The discussion in Appendix A.5 is somewhat confusing . If the authors want to argue that the network is locally linear so that we can approximate with linear regression , why should we use the power methods ? We use the power method during training , since we only need access to the dominant singular vector . In the experiment section , we more generally study the spectral properties of the Jacobian , requiring us to compute the full spectrum and not just the dominant singular value / vector pair . The full spectral decomposition requires much more computation and is only viable when evaluating / investigating certain properties , not during training . We very clearly stated this in the first paragraph of Section 5.1 . > > `` Still , I feel the contribution of this paper is somewhat weak . I don \u2019 t see any improvements of the proposed algorithms compared with the standard adversarial training , as well as the theoretical contribution like adversarial generalization . The experiments are not convincing , as the setting is different from the general setting the community used in adversarial training . I \u2019 m not familiar with the results in global spectral normalization and it \u2019 s possible that the global spectral normalization may have little gain in adversarial robustness , but in my opinion , the main contribution [ 1 ] is the generalization analysis of spectral normalized adversarial trained neural networks , which this paper lacks . On the empirical side , the computation efficiency and performance of the proposed algorithms don \u2019 t outperform adversarial training much . So I tend to reject this paper . '' Again , we 1. do not claim to outperform AT , we claim to show its correspondence to d.d . SNR and 2. we show this correspondence in a theoretical way that no previous work has managed to establish . Our experimental section reflects and supports these points very well . Also , we are not in a competition with [ 1 ] ."}, {"review_id": "S1ervgHFwS-1", "review_text": "This paper studies the link between adversarial training and the proposed data-dependent operator norm regularization for ReLU network. Under specific conditions, in theory, the authors show the equivalence between the l2 PGD training and the regularization method. Empirical experiments are conducted to support the theory. While this paper gives interesting observation on both theory and empirical study, I think this paper is not qualified for publishing in ICLR due to the following reasons: (1) limited theoretical results; (2) No significant improvement for practical algorithm; Main argument: The main theorem 1 seems to be weak as it is only valid for small perturbation region \\epsilon and it is unclear how this assumption is consistent to the practice. It is also unclear how the assumption that \\alpha \\to \\infty influences the practical algorithm. It would be better to generalize the theorem to other \\ell_p attack, instead of just \\ell_2. Discussion of computational complexity of the proposed regularization method compared with PGD is missed. The adversarial robustness is related to the (local) Lipschitz continuity and many other types of regularization decreases the (local) Lipschitz constant. Could you further give result that distinguishes the proposed norm? It would be better to give improved algorithm for adversarial training based on the current result. The current contribution for further theoretical is too weak as the main theorem requires strong assumption. And I don\u2019t see significant contribution to empirical algorithm. Minor Equ (10) seems not the typical one used and seems not the one studied later. ", "rating": "3: Weak Reject", "reply_text": "* This is part 2 of 2 of our response * > > Discussion of computational complexity of the proposed regularization method compared to PGD . In the last sentence of Section 4.1 we compare the computational complexity of d.d . SNR with that of global ( data-independent ) SNR . Compared to PGD , d.d . SNR is equally computationally expensive . As stated in Table 1 , both PGD and d.d . SNR were implemented with 10 iterations . Since our main claim is to show equivalence of the two methods , this is a valuable piece of information . We will add a sentence to the paper to emphasize this point . > > Many other types of regularization decreases the ( local ) Lipschitz constant . Could you further give result that distinguishes the proposed norm ? Indeed there are many works that aim at regularizing the Lipschitz constant . However , these works mostly focus on decreasing the global Lipschitz constant , which corresponds to data-independent SNR and gives only a loose bound on adversarial robustness . We would like to stress that this is different from ( and weaker than ) our presented data-dependent SNR . One of the main points in our paper , especially our experiments , is that the Lipschitz regularization of previous work can not account for / does not explain adversarial robustness ( see Section 5.4 , Figures 3 ( left ) and 5 ( left ) ) . The data-dependent SNR variant introduced in our paper is a novel and significant generalization and the first type of SNR that is equivalent to AT . > > It would be better to give improved algorithm for adversarial training based on the current result . The current contribution for further theoretical is too weak and I don \u2019 t see significant contribution to empirical algorithm . As we stated above , it is not our goal to improve the practical algorithm of adversarial training , but to show its correspondence to data-dependent SNR . In fact , it would be contrary to our main result to try to improve the practical algorithm . Other than that , we do not understand what the reviewer means by his or her request . Please elaborate . > > Equ ( 10 ) seems not the typical one used and seems not the one studied later . We do study this equation for p=2 . See also Equations ( 33 ) and ( 34 ) in the Appendix , where we show that Equation ( 10 ) reduces to Equation ( 7 ) under the conditions of our Theorem . Perhaps the reviewer refers to the setting in which the network is trained to only minimize the adversarially perturbed empirical loss . It is however customary to train the network to minimize a convex combination of a clean empirical loss and an adversarially perturbed empirical loss , see the equation on page 5 in Goodfellow et al. \u201c Explaining and harnessing adversarial examples \u201d . In conclusion , we agree that our Theorem makes strong assumptions , but we believe that 1. it is valuable to be on record and theoretically confirm this long-standing hypothesis and 2. we show extensively that the claim of the Theorem holds well beyond its assumptions in practice . As for improving over AT in the practical sense , we never claim to do so , and it would actually run contrary to our claim . We hope that these comments provide clarification and we look forward to continuing the discussion ."}, {"review_id": "S1ervgHFwS-2", "review_text": "This largely theretical paper establishes a theoretical link between adversarial training and operator norm regularization for DNNs. It is well written and structured, and it falls squarely within the the remit of the conference. The experimental apparatus is thorough and the derivations, proofs and the maths at large seem sound to me, even if I have not checked them in full detail. The study delivers a data-dependent variant of spectral norm regularization affecting large singular values of the DNN. It is proved to be equivalent to adversarial training based on a type of norm-constrained projected gradient ascent attack. Results are novel and relevant and, in my opinion, they merit acceptance.", "rating": "8: Accept", "reply_text": "Dear Reviewer We would like to thank you for your comments . Your feedback is highly appreciated ."}], "0": {"review_id": "S1ervgHFwS-0", "review_text": "Adversarial training generalizes data-dependent spectral norm regularization This paper shows that, projected gradient descent based adversarial training is similar to the data-dependent spectral norm regularization, and under very restrictive condition, the authors show that this two methods are the same. Some experiments are conducted to support the theory. Overall, I think this paper is marginal, while the experiments are not convincing. First, the relation between spectral normalization and adversarial training have been investigated by [1], while the fast computational of maximum singular value with power methods have also been proposed in [1]. The authors only give a data-dependent version of the spectral normalization based on the Jacobian of the neural networks, which I think is somewhat weak. The experiments are limited with specific settings that are not generally used in practice, which alleviate my confidence on this paper\u2019s results. Also, the experiment section contain several not so important information. I think the authors should do far more experiments to support the main claim, while move these additional justification to the appendix. Detailed comments: 1. I think the claim of theorem 1 is somewhat ambiguous. How to guarantee there exists such epsilon satisfies this condition? Is this the case we face in the real world? What will happen if \\alpha is not sufficiently large? If we don\u2019t use logits pairing and \\ell_2 norm constraint, will the claim hold? I think the correlation behinds the spectral norm and adversarial training is well investigated and use this correlation as the intuition behinds work is enough. This theorem cannot convince me that the proposed methods have a strong theoretical basis. 2. Generally, the neural networks have a large number of parameters (~ millions) for image classification task. The global spectral norm regularization only needs to calculate the spectral norm of each layer\u2019s weight matrix, whose computational cost is acceptable. However, to calculate the Jacobian and use the power methods, we will additionally do several forward pass and backward pass just as adversarial training. As a regularization technique, is this calculation tolerable? If this is some variant of the adversarial training, I don\u2019t find the experiment results support the claim that it will outperform the adversarial training consistantly. 3. Why don\u2019t use some standard neural network architecture like ResNet? As this results is not comparable to other existing work, I\u2019m not sure if this result is meaningful. Also, are the comparisons fair? For example, the regularization coefficient of global spectral norm regularization and data-dependent spectral norm regularization are far more different. And the authors use only 1 iteration to calculate the singular value in global spectral norm regularization, why to do that? Also, what\u2019s the result compared with \\ell_p norm constraint adversarial training? 4. The evaluation of some assumption on the network is better moved to appendix, as this is only some sanity check, not the core contribution. More experiments with ResNet, WideResNet, MobileNet etc. on CIFAR100 and ImageNet are more convincing. 5. What\u2019s the attack method in the main context? 6. I think the discussion in Appendix A.5 is somewhat confusing. If the authors want to argue that the network is locally linear so that we can approximate with linear regression, why should we use the power methods? Still, I feel the contribution of this paper is somewhat weak. I don\u2019t see any improvements of the proposed algorithms compared with the standard adversarial training, as well as the theoretical contribution like adversarial generalization. The experiments are not convincing, as the setting is different from the general setting the community used in adversarial training. I\u2019m not familiar with the results in global spectral normalization and it\u2019s possible that the global spectral normalization may have little gain in adversarial robustness, but in my opinion, the main contribution [1] is the generalization analysis of spectral normalized adversarial trained neural networks, which this paper lacks. On the empirical side, the computation efficiency and performance of the proposed algorithms don\u2019t outperform adversarial training much. So I tend to reject this paper. [1] Farnia, Farzan, Jesse Zhang, and David Tse. \"Generalizable Adversarial Training via Spectral Normalization.\" International Conference on Learning Representations, 2019.", "rating": "1: Reject", "reply_text": "* This is part 3 of 3 of our response * > > 3 . Why don \u2019 t use some standard neural network architecture like ResNet ? Also , are the comparisons fair ? For example , the regularization coefficient of global SNR and d.d . SNR are different . And the authors use only 1 iteration to calculate the singular value in global spectral norm regularization , why to do that ? \u201c The regularization constants were chosen such that the models achieve roughly the same test set accuracy on clean examples as the adversarially trained model does. \u201d as was clearly stated in our paper . Hence , yes , the comparisons are fair . For global SNR , we try to stay as close as possible to the original authors ' suggestions . Yoshida & Miato write \u201c One [ power method ] iteration [ per parameter update ] was adequate in our experiments \u201d and \u201c we performed only one [ power method ] iteration [ per parameter update ] because it was adequate for obtaining a sufficiently good approximation \u201d . Note , the computation of the data-independent regularizer decouples from the empirical loss , hence the power-method iterations can be amortized across data-points . As stated above , our network architecture is standard , is publically available and is used throughout research . > > 4.The evaluation of some assumption on the network is better moved to appendix , as this is only some sanity check , not the core contribution . More experiments with ResNet , WideResNet , MobileNet etc . on CIFAR100 and ImageNet are more convincing . Firstly , it is unclear what evaluation of assumptions the reviewer is referring to . Secondly , we sincerely do not expect to see any difference regarding the correspondence \u201c AT < - > d.d . SNR \u201d on other architectures / data sets . Our theorem proves that \\ell_2-norm constrained PGA-based AT and d.d . SNR are equivalent for small enough epsilon , while our extensive experiments show that in practice , the correspondence between AT and d.d . SNR holds approximately in a region much larger than proved in the Theorem , the region being roughly the size of the epsilon * -ball used during adversarial training ( epsilon * = 1.75 > > epsilon in Theorem ) , see Figure 2 ( left ) and discussion in Section 5.3 \u201c Validity of linear approximation \u201d . Sure , more experiments can always be requested , but we believe that confirming our main claims in practice is more important and valuable than including one further architecture or dataset . Please also consider our comments from the `` general comments '' section at the beginning of this review on this topic . > > 5.What \u2019 s the attack method in the main context ? We evaluated against \\ell_2-norm constrained PGA in the main text , as stated in Section 5.1 and Table 1 . Additional results for \\ell_\\infty PGA attack are provided in the Appendix . > > 6.The discussion in Appendix A.5 is somewhat confusing . If the authors want to argue that the network is locally linear so that we can approximate with linear regression , why should we use the power methods ? We use the power method during training , since we only need access to the dominant singular vector . In the experiment section , we more generally study the spectral properties of the Jacobian , requiring us to compute the full spectrum and not just the dominant singular value / vector pair . The full spectral decomposition requires much more computation and is only viable when evaluating / investigating certain properties , not during training . We very clearly stated this in the first paragraph of Section 5.1 . > > `` Still , I feel the contribution of this paper is somewhat weak . I don \u2019 t see any improvements of the proposed algorithms compared with the standard adversarial training , as well as the theoretical contribution like adversarial generalization . The experiments are not convincing , as the setting is different from the general setting the community used in adversarial training . I \u2019 m not familiar with the results in global spectral normalization and it \u2019 s possible that the global spectral normalization may have little gain in adversarial robustness , but in my opinion , the main contribution [ 1 ] is the generalization analysis of spectral normalized adversarial trained neural networks , which this paper lacks . On the empirical side , the computation efficiency and performance of the proposed algorithms don \u2019 t outperform adversarial training much . So I tend to reject this paper . '' Again , we 1. do not claim to outperform AT , we claim to show its correspondence to d.d . SNR and 2. we show this correspondence in a theoretical way that no previous work has managed to establish . Our experimental section reflects and supports these points very well . Also , we are not in a competition with [ 1 ] ."}, "1": {"review_id": "S1ervgHFwS-1", "review_text": "This paper studies the link between adversarial training and the proposed data-dependent operator norm regularization for ReLU network. Under specific conditions, in theory, the authors show the equivalence between the l2 PGD training and the regularization method. Empirical experiments are conducted to support the theory. While this paper gives interesting observation on both theory and empirical study, I think this paper is not qualified for publishing in ICLR due to the following reasons: (1) limited theoretical results; (2) No significant improvement for practical algorithm; Main argument: The main theorem 1 seems to be weak as it is only valid for small perturbation region \\epsilon and it is unclear how this assumption is consistent to the practice. It is also unclear how the assumption that \\alpha \\to \\infty influences the practical algorithm. It would be better to generalize the theorem to other \\ell_p attack, instead of just \\ell_2. Discussion of computational complexity of the proposed regularization method compared with PGD is missed. The adversarial robustness is related to the (local) Lipschitz continuity and many other types of regularization decreases the (local) Lipschitz constant. Could you further give result that distinguishes the proposed norm? It would be better to give improved algorithm for adversarial training based on the current result. The current contribution for further theoretical is too weak as the main theorem requires strong assumption. And I don\u2019t see significant contribution to empirical algorithm. Minor Equ (10) seems not the typical one used and seems not the one studied later. ", "rating": "3: Weak Reject", "reply_text": "* This is part 2 of 2 of our response * > > Discussion of computational complexity of the proposed regularization method compared to PGD . In the last sentence of Section 4.1 we compare the computational complexity of d.d . SNR with that of global ( data-independent ) SNR . Compared to PGD , d.d . SNR is equally computationally expensive . As stated in Table 1 , both PGD and d.d . SNR were implemented with 10 iterations . Since our main claim is to show equivalence of the two methods , this is a valuable piece of information . We will add a sentence to the paper to emphasize this point . > > Many other types of regularization decreases the ( local ) Lipschitz constant . Could you further give result that distinguishes the proposed norm ? Indeed there are many works that aim at regularizing the Lipschitz constant . However , these works mostly focus on decreasing the global Lipschitz constant , which corresponds to data-independent SNR and gives only a loose bound on adversarial robustness . We would like to stress that this is different from ( and weaker than ) our presented data-dependent SNR . One of the main points in our paper , especially our experiments , is that the Lipschitz regularization of previous work can not account for / does not explain adversarial robustness ( see Section 5.4 , Figures 3 ( left ) and 5 ( left ) ) . The data-dependent SNR variant introduced in our paper is a novel and significant generalization and the first type of SNR that is equivalent to AT . > > It would be better to give improved algorithm for adversarial training based on the current result . The current contribution for further theoretical is too weak and I don \u2019 t see significant contribution to empirical algorithm . As we stated above , it is not our goal to improve the practical algorithm of adversarial training , but to show its correspondence to data-dependent SNR . In fact , it would be contrary to our main result to try to improve the practical algorithm . Other than that , we do not understand what the reviewer means by his or her request . Please elaborate . > > Equ ( 10 ) seems not the typical one used and seems not the one studied later . We do study this equation for p=2 . See also Equations ( 33 ) and ( 34 ) in the Appendix , where we show that Equation ( 10 ) reduces to Equation ( 7 ) under the conditions of our Theorem . Perhaps the reviewer refers to the setting in which the network is trained to only minimize the adversarially perturbed empirical loss . It is however customary to train the network to minimize a convex combination of a clean empirical loss and an adversarially perturbed empirical loss , see the equation on page 5 in Goodfellow et al. \u201c Explaining and harnessing adversarial examples \u201d . In conclusion , we agree that our Theorem makes strong assumptions , but we believe that 1. it is valuable to be on record and theoretically confirm this long-standing hypothesis and 2. we show extensively that the claim of the Theorem holds well beyond its assumptions in practice . As for improving over AT in the practical sense , we never claim to do so , and it would actually run contrary to our claim . We hope that these comments provide clarification and we look forward to continuing the discussion ."}, "2": {"review_id": "S1ervgHFwS-2", "review_text": "This largely theretical paper establishes a theoretical link between adversarial training and operator norm regularization for DNNs. It is well written and structured, and it falls squarely within the the remit of the conference. The experimental apparatus is thorough and the derivations, proofs and the maths at large seem sound to me, even if I have not checked them in full detail. The study delivers a data-dependent variant of spectral norm regularization affecting large singular values of the DNN. It is proved to be equivalent to adversarial training based on a type of norm-constrained projected gradient ascent attack. Results are novel and relevant and, in my opinion, they merit acceptance.", "rating": "8: Accept", "reply_text": "Dear Reviewer We would like to thank you for your comments . Your feedback is highly appreciated ."}}