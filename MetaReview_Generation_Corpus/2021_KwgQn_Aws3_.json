{"year": "2021", "forum": "KwgQn_Aws3_", "title": "Interpretable Sequence Classification Via Prototype Trajectory", "decision": "Reject", "meta_review": "The authors introduce an RNN model, ProtoryNet, which uses trajectories of sentence protoypes to illuminate the semantics of text data.\n\nGood points were brought up and addressed in discussion, which have improved the paper - including a helpful suggestion from Rev 3 to fine-tune BERT sentence embeddings in ProtoryNet, which led to significant performance gains.\n\nUnfortunately the tone of discussion with one reviewer slipped below the respectful standards to which we aspire, but rest assured that only substantive points on the paper were considered.\n\nReviewers were split but in discussion converged to leaning against acceptance, allowing the authors to reflect on, and incorporate new results carefully in an updated manuscript.", "reviews": [{"review_id": "KwgQn_Aws3_-0", "review_text": "The authors propose ProtoryNet , a prototype-based model for paragraph classification that associates each sentence in the paragraph with a relevant prototypical sentence from the training data . The idea is interesting and the ability to decompose sentiment scores over each sentence + find prototypes for each helps to build user understanding of the model prediction . Thank you to the authors for the submission . However , I have two concernsbaseline comparisons and model detailsthat prevent me from assigning a higher rating . * * Baseline comparisons * * 1 . Across 5 sentiment classification datasets , the authors find that ProtoryNet substantially underperforms a standard BERT model , in some cases obtaining more than double the error . This seems like a substantial price to pay the ability to associate each sentence with a prototype . The authors write that `` Note , however , that DistilBERT was pre-trained on a massive corpus of text data ... Hence , [ it ] should only be used for [ a ] sanity check '' . However , ProtoryNet also seems to build on top of standard pre-trained BERT embeddings , which have also been derived from a massive corpus of text data ( and actually , the comparison should favor ProtoryNet , since DistilBERT is a smaller model than the standard BERT model ) . Could the authors elaborate on why they believe that this comparison is unfair ? If it is unfair , the authors should perform a comparison that is as similar as possible to ProtoryNet but without the prototype parts , i.e. , train a model on top of the same BERT embeddings and see how that performs . 2.Related to the above question , it is common to fine-tune BERT models on the dataset of interest . Was this done here for DistilBERT ? What about for ProtoryNet ? And if not , why not ? * * Model details * * 3 . It was difficult to follow all of the model details ; perhaps consider reorganizing and clarifying the writing . For example , it was unclear how the prototypes are actually chosen until late in the paper , whereas it should have been explained in S3.1 . The notation in S3.1 has a few minor errors . For example , if the entire sentence is encoded as $ \\mathbb { R } ^V $ then it seems like $ V $ is not just the size of the vocabulary , but the size of the vocabulary to the power of the length of the sentence ? Also , how were the hyperparameter values and coefficient values chosen ? Is prototype projection also done at the end of training ? 4.There are many modeling decisions that seem somewhat ad-hoc or non-standard . It seems like it might be possible to simplify the model significantly , or if not , it would be nice for the effects of these decisions to be better studied . For example : ( a ) mean-squared error is used even on binary classification problems ; ( b ) the loss function is complicated by diversity and prototypicality terms , but the sensitivity analysis reveals that the accuracies are basically indistinguishable even when we completely remove those terms ; ( c ) the sparsity transformation was approximated by a softmax that seems basically indistinguishable from a step function since $ \\gamma \\geq 10^6 $ , so does it actually matter ? ( Note that ReLUs are also not differentiable . ) ( d ) How useful is the LSTM at the end , if it generally goes over only ~4 sentences ? * * Update * * Thank you to the authors for the revisions , and great to know that the experimental results have improved significantly . In the absence of an updated manuscript , it is difficult to update my score appropriately , so I will leave it as it currently is . However , I think the work is promising and that an updated manuscript that incorporates the new experimental results and more carefully teases apart the contributions of the different components ( as the authors have started to do in this rebuttal period ) would be impactful . Thank you to the authors again for all of their hard work .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your valuable feedback ! Here we address your comments and clarify the paper a little more . * * * * * Q1 * * : Performance comparison with DistilBERT * * A1 * * : you are right that DistilBERT outperforms ProtoryNet . We did not claim it was an \u201c unfair \u201d comparison and we acknowledged that ProtoryNet did not perform as well as the SOTA , paying the price for interpretability . However , ProtoryNet outperforms the latest prototype-based baseline ProSeNet . So the message we try to deliver is that if one desires such an understandable model , then ProtoryNet is a better choice than the other interpretable baseline . We understand that many researchers may still view predictive performance as a dominating factor when evaluating a new method , but we hope the interpretability of the model can be appreciated . Please also see our discussion of this issue in the response to all reviewers . * * * * * Q2 * * : fine-tuning BERT * * A2 * * : We did fine-tune DistilBERT on each dataset of interest but we didn \u2019 t fine-tune BERT sentence embedding in ProtoryNet , using BERT as a service . You actually made a very good point ! Thank you for that ! We are re-running our experiments now , allowing the BERT sentence embedding to be trained with the rest of the model . But the training takes a long time so please allow us some time . * * * * * Q3 * * : model details * * A3 * * : Thank you for the suggestion . Section 3.1 describes the architecture and * * forward * * functions from component a to d. We added explanations about the prototypes when we describe the prototype layer but we will leave the prototype initialization and projection to section 3.3 since it is part of the training process . Each sentence is coded as $ \\mathbb { R } ^V $ because each sentence is a vector of size $ V $ , where each element in $ V $ represents whether the corresponding word appears in the sentence . In the submitted paper we set $ \\alpha= { 0.1 } $ , $ \\beta=1e^ { -4 } $ , and $ K = 200 $ to generate the results in Table 1 . We are running new experiments tuning these parameters in order to improve the performance . The prototype projection is done every 10 epochs . * * * * * Q4 * * : modeling decisions * * A4 * * : a ) There exist many choices of loss functions such as MSE , cross-entropy , etc . MSE is one common choice . We do not see any particular reason for choosing or not choosing a specific loss function . So for the purpose of presenting the model , we just pick MSE as it can also be used for both classification and regression , which our framework can also be applied . If you feel there \u2019 s a specific loss function that needs to be tested , please let us know and we can run more experiments . b ) the diversity and prototypicality terms are designed not for the purpose of improving accuracy , but for improving interpretability . The reason it doesn \u2019 t hurt the predictive performance can be explained by the recent research on \u201c Rashomon Set \u201d [ 1 ] , that there exist many models with very similar performance , so one can add customized constraints to the model to achieve additional benefits , such as interpretability . Here , to achieve good explanations , we desire prototypes that are different from each other to avoid redundancy , thus the diversity term . We also want each input sentence to be mapped to a prototype that is similar enough to make the explanation convincing , thus the prototypicality term . In fact , similar terms have been introduced in other prototype based DNN models [ 2,3 ] . c ) We apply a softmax not only to approximate a step function but most importantly , to select the most similar prototype ( with the largest similarity ) . So it is necessary to apply a softmax instead of a step function . d ) Our experiments actually show the performance of long paragraphs ( > 25 words , which are often over 4 sentences ) . We are running experiments now where we simply averaging the sentiments of the sentences . The results we have collected so far suggest that the performance is worse than using LSTM at the end to capture the temporal patterns . * * * [ 1 ] Rudin , Cynthia . `` Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead . '' Nature Machine Intelligence 1.5 ( 2019 ) : 206-215 . [ 2 ] Chen , Chaofan , et al . `` This looks like that : deep learning for interpretable image recognition . '' Advances in neural information processing systems . 2019 . [ 3 ] Ming , Yao , et al . `` Interpretable and steerable sequence learning via prototypes . '' Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2019 . * * * * * Please let us know if there 's anything unclear or need more clarification * * Thank you again for your comments !"}, {"review_id": "KwgQn_Aws3_-1", "review_text": "Summary : this paper presents an RNN sequence classifying model that generates a prototype for each sentence in a paragraph . The generated prototypes help explain the model 's prediction . The method embeds each sentence , matches to prototypes , then runs through an LSTM before making a prediction . Experiments found improved accuracy compared to a previous model that generates only one prototype for a paragraph . A user evaluation also found improvement in interpretability . Strengths : - The paper is well written and easy to understand . - Generating prototypes is a promising direction for improving the interpretability of RNNs and other neural nets models . The idea of generating a prototype trajectory for sentences in a paragraph is interesting and novel to my knowledge . - The architecture and training methods are technically sound . - The experiments show positive improvement in prediction accuracy . Concern on user evaluation : - There is some improvement but the error bars are large , it is not clear if the differences are statistically significant . - With a prototype for each sentence , users will need to read more . So a more fine-grained explanation will increase cognitive load . User evaluation can potentially investigate this .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your positive comments . We are very glad you like our paper . Here we address your concerns on user evaluation . * * * * * Q1 * * : Error bar being too large * * A1 * * : The results were collected from 58 subjects in the original paper . In the last couple of months , our survey continued to collect data and now we have a total of 111 subjects who participated in the survey . We have updated the paper with the new results and report the p-value for comparing the two models . Results show that the difference is statistically significant . * * * * * Q2 * * : Fine-grained explanation increases the cognitive load * * A2 * * : Thank you for bringing up this discussion . Here \u2019 s our thoughts on this topic . We think that users ' comprehension of a new model follows a Bell curve , where as information is provided , they understand the model better . However , when the information saturates or overloads , their understandability is inhibited , just as you concerned . Without user study , it is unknown where the amount of information provided lies in the Bell curve . So that is why we designed the experiments in the paper , comparing ProtoryNet with ProSeNet , knowing that ProtoryNet presents more information than ProSeNet . The results already show that users feel ProtoryNet is easier to understand than ProSeNet . According to the comments we collected from the survey , users feel that prototypes provided at the document level are difficult to understand or map to the input since the semantics are more complicated ; while in our model , prototypes at the sentence level are easier to make such links . In addition , we feel interpretability is quite subjective and has to be determined by the application and the specific user . Reviewer 2 proposed Bag-of-words , which is on the word level , ProSeNet is on the document level , and our model , ProtoryNet , sits right in the middle . In practice , it will be helpful to provide users different options so they can choose based on their specific task and application . * * * * * Please let us know if there 's anything unclear or need more clarification . * * Thank you again for your comments !"}, {"review_id": "KwgQn_Aws3_-2", "review_text": "This paper addresses the problem of explainable AI by trying to build models which are inherently interpretable ( as opposed to post-hoc methods that interpret black-box models after being trained ) . In particular , they focus on a recent paper on prototype-based networks , and a model called prosenet . They introduce an extension , protorynet , which fares better on longer documents by applying prototypes to each sentence ( rather than the whole document ) , leading to a `` prototype trajectory '' , showing how the prediction evolves over the course of the sentence . They show improvements in prediction accuracy across 5 datasets , analyse the hyperparameters and conduct a human study . Strengths : I really like the concept of prototype networks , as it is simple and I could see it being accessible to non-technical users . The authors are to commended for undertaking a proper evaluation , including human experiments , which are often painful to conduct but quite helpful . The proposed extension is straightforward and easy to understand . As someone unfamiliar with the original paper from Ming et al , I found it easy to get up to speed . The authors are also good at plainly describing the details of their approach , and running the requisite ablation studies ( e.g.between sigmoid/exponential ) . Weaknesses : 1 . The paper 's prior work section is incomplete , and makes factually incorrect statements about the state of the field . In particular , the authors focus on attention-based techniques , ignoring the considerable amount of work on other approaches , and incorrectly saying that other approaches `` often turn out to be gibberish '' . Attention is a particularly strange subset of interpretations to focus on , given that the community has recently started to argue about whether attention is an explanation at all [ 7 ] In general , I would suggest the authors look at papers from the \u201c Interpretability and Analysis of Models for NLP \u201d track at ACL and the blackboxNLP workshop at ACL . For concrete starting points , I 'll restrict myself to ICLR papers , dating back to 2017 [ 2-5 ] . General attribution methods , such as integrated gradients [ 6 ] are also pertinent . I do n't think the paper can be published without a reasonable related work section , hence the `` clear reject '' score . If this were fixed , I 'd upgrade to a `` weak reject '' , i.e.5.2.I am concerned that this model may be outperformed by simple , bag of words approaches . For IMDB , the original paper [ 1 ] has a variety of results hitting 88 % accuracy , while the results reported for protorynet are 85 % . If a bag of words model outperforms protorynet , that makes it a less desirable model . It is possible that preprocessing differences account for this , though . Can the authors give some clarity on this , for IMDB as well as the other datasets ? Ideally , there would be an additional column in the results table providing results for a simple , non-neural , baseline . 3.I worry that a lot of the added accuracy does not help the model 's accuracy . In particular , the ablation charts in Figures 5 and 8 show that when alpha=beta=0 , the model 's accuracy is within ~.2 % of the best accuracy , so do we need those additional loss terms ? I also suspect that averaging the outputs , rather than feeding them through an RNN would be comparably accurate , and simpler . 4.While I appreciate the human studies , the confidence bars are quite wide , so that most of the findings are not statistically significant . 5.For the model diagnosis human experiment , the users were only shown three examples . How were these examples chosen ? That is not very many , so I worry that either those examples were chosen to be ones where protorynet was better . Ideally they would be chosen randomly , subject to some reasonable criteria . Nitpicks : - Distillbert is an odd SOTA to choose , as it is designed to have fewer parameters . Something like RoBERTA would likely have stronger results , and be more representative of SOTA . [ 1 ] https : //ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf [ 2 ] https : //arxiv.org/abs/1911.06194 [ 3 ] https : //arxiv.org/abs/1812.04801 [ 4 ] https : //arxiv.org/abs/1801.05453 [ 5 ] https : //arxiv.org/abs/1801.05453 [ 6 ] https : //arxiv.org/abs/1703.01365 [ 7 ] https : //arxiv.org/abs/1902.10186", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your positive feedback on our idea and concept ! We apologize for not presenting enough material and let us try if we can address your concerns in the response . * * * * * Q1 * * : lacks related work * * A1 * * : We apologize for missing the important related work . We want to clarify that `` often turn out to be gibberish '' refers to attention methods . We agree with you attention methods have been under heated debate and are particularly controversial . But from the feedback we have got so far , we realize many researchers would still immediately think of attention methods when it comes to interpretability of RNN . So we feel it is necessary to include it , even just to make a distinction . We will lightly touch attention methods in the updated version and cite the recent paper [ 4 ] . In addition , thank you very much for pointing us to the papers . We have added one paragraph in the related work section discussions the papers listed in the review . Also , we do want to make a distinction here , that the papers listed in your review are posthoc explanation methods while ProtoryNet itself is interpretable and does not rely on any external explainers . * * * * * Q2 * * : Concern about the predictive performance and adding an interpretable baseline * * A2 * * : We understand your concern about the predictive performance . We are working on re-running all experiments , this time allowing the BERT sentence embedder to be fine tuned ( per Reviewer 3 \u2019 s suggestion ) . We have added bag-of-words as an interpretable , non-neural baseline . ( Note that in the paper you point us to , they used a dataset with 50k instances while our model was applied to the original version of the data with only 25k instances ) . We have run bag-of-words on our datasets for a meaningful comparison . On average ProtoryNet is better in predictive performance . Also , bag-of-words ' explanations are on word-level , which might be too fine-grained . ProtoryNet provides sentence level interpretations and also track how sentiment changes . * * * * * Q3.1 * * : the purpose of the diversity and prototypicality terms * * A3.1 * * : We apologize for not explaining the purpose of the two terms clearly in the paper . These two terms are NOT designed for improving accuracy , but for the purpose of improving interpretability . The fact that it doesn \u2019 t hurt the predictive performance can be explained by the recent research on \u201c Rashomon Set \u201d [ 1 ] , that there exist many models with very similar performance , so one can add customized constraints to the model to achieve additional benefits , such as interpretability . Here , to achieve good explanations , we desire prototypes that are different from each other to avoid redundancy , thus the diversity term . We also want each input sentence to be mapped to a prototype that is similar enough to make the explanation convincing ( if `` I love the food '' is mapped to the prototype `` terrible service '' , users would n't believe this model ) , thus the prototypicality term . In fact , similar terms have been introduced in other prototype based DNN models [ 2,3 ] . We have added more explanations of the two terms in the paper . * * * * * Q3.2 * * : Averaging the outputs * * A3.2 * * : Thank you for proposing the easy but interesting baseline . We have been running experiments . The results we have collected so far show that this baseline is worse than ProtoryNet . We will update the paper once we have complete results for all datasets . * * * * * Q4 * * : Statistically insignificant results * * A4 * * : The results were collected from 58 subjects in the original paper . In the last a couple of months , our survey continued to collect data and now we have a total of 111 subjects who participated in the survey . We will update the paper with the new results and report the p-value for comparing the two models . * * * * * Q5 * * : Survey design * * A5 * * : Each user was shown three examples , but the three examples were randomly drawn from a pool of 20 examples where both ProtoryNet and ProSeNet misclassified . The 20 examples were selected from the common set of misclassification and having less than or equal to 5 sentences . * * * [ 1 ] Rudin , Cynthia . `` Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead . '' Nature Machine Intelligence 1.5 ( 2019 ) : 206-215 . [ 2 ] Chen , Chaofan , et al . `` This looks like that : deep learning for interpretable image recognition . '' Advances in neural information processing systems . 2019 . [ 3 ] Ming , Yao , et al . `` Interpretable and steerable sequence learning via prototypes . '' Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2019 . [ 4 ] Jain , Sarthak , and Byron C. Wallace . `` Attention is not explanation . '' arXiv preprint arXiv:1902.10186 ( 2019 ) . * * * * * Please let us know if there 's anything unclear or need more clarification * * Thank you again for your comments !"}, {"review_id": "KwgQn_Aws3_-3", "review_text": "This paper presents ProtoryNet , a framework for text data that classifies and explains the prototypes ' results . The key concept , that is the novelty of the work , is that this framework is based on sentence prototypes , called prototype trajectory in the paper . In particular , instead of working at the entity of the text level , the text is split into sentences and each sentence is analyzed by itself . The structure of the framework is composed of a layer that encodes a text 's sequences , followed by a prototype layer in which is computed the similarity among each sentence and the prototype trajectories . At this point , the sentences are represented in one-hot encoding : for each sentence , there is a bunch of zero and then a one for the most similar sentence prototype . This representation is used for the classification of the sentence , done using an LSTM structure . In this setting , the interpretation is given by exploiting the prototypes matched for the text under analysis . The idea presented in the paper is really interesting : it allows for an interpretation based on prototypes that is simple and understandable while achieving acceptable prediction performance . Pros It is well written and easy to read . There is only a repetition of a \u201c the \u201d at the end of page 7 . The idea of the trajectory prototypes is quite interesting , especially because it can be employed for other kinds of sequence data . The explanations obtained are compelling : the results obtained from the human evaluation showed excellent results , especially in the context of local explanations for non-experts . I appreciated the presentation of the framework using a picture due to the complicated structure . Cons A few words more about the diversity and the prototypically would have been useful in the objective functions . The reader is able to get a general idea , but maybe in the appendix , there could be something more to understand the claims fully . The prediction performance of the model is fine , but not excellent . In my opinion , some experiments more about the prototype initialization would have been helpful in understanding if it can be a possible source of errors . The evaluation of the explanations is only w.r.t.ProSeNet.However , there are other methods to compare with , such as LIME and SHAP , to name agnostic methods , but also attention-based explanations , such as LRP ( Layerwise-Relevance-Propagation ) , NeuroX and Integrated Gradients ( that are not cited even in the related work section ) . What about the robustness of the explanations ? Similar sentences are going to be represented by the same prototype or by prototypes that are similar ? An analysis of the robustness of the explanations would be useful ( fidelity , hit , and similar metrics could be used , but also methods such as ROAR-RemOve And Retrain or KAR-Keep and Retrain ) What about other classificators ? The last part of the framework uses a LSTM to predict the sentiment of the sentence . However , due to the structure based on similarity , other classificators such as logistic regression or decision tree may be tested in the same fashion of shapelet-based classifiers . The explanation may also benefit from the use of such methods due to the additional interpretable information they provide . [ LIME ] Ribeiro , M. T. , Singh , S. , & Guestrin , C. ( 2016 , August ) . `` Why should I trust you ? '' Explaining the predictions of any classifier . In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining ( pp.1135-1144 ) . [ SHAP ] Lundberg , S. M. , & Lee , S. I . ( 2017 ) .A unified approach to interpreting model predictions . In Advances in neural information processing systems ( pp.4765-4774 ) . [ LRP ] Patil , A. , Wadekar , A. , Gupta , T. , Vijan , R. , & Kazi , F. ( 2019 , July ) . Explainable LSTM Model for Anomaly Detection in HDFS Log File using Layerwise Relevance Propagation . In 2019 IEEE Bombay Section Signature Conference ( IBSSC ) ( pp.1-6 ) .IEEE . [ LRP ] Bach , S. , Binder , A. , Montavon , G. , Klauschen , F. , M\u00fcller , K. R. , & Samek , W. ( 2015 ) . On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation . PloS one , 10 ( 7 ) , e0130140 . [ INTGRAD ] Sundararajan , M. , Taly , A. , & Yan , Q . ( 2017 ) .Axiomatic attribution for deep networks . arXiv preprint arXiv:1703.01365 . [ SHAPELET ] Lines , J. , Davis , L. M. , Hills , J. , & Bagnall , A . ( 2012 , August ) . A shapelet transform for time series classification . In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining ( pp.289-297 ) .", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your valuable feedback ! We are enthusiastic that you like our idea . Please let us do a little more explanation here to address your concern . * * * * * Q1 * * : typo * * A1 * * : fixed . Thank you for your careful reading . * * * * * Q2 * * : a few more words about the diversity and prototypicality * * A2 * * : We have added a paragraph in Section 3 to discuss the two terms in more detail . Please see the updated paper . * * * * * Q3 * * : Predictive performance is fine but not excellent * * A3 * * : Yes we agree there is a gap compared to SOTA . We have tested not initializing the prototypes but the performance got slightly worse , so we can rule out it as an explanation . We are now tuning the hyper-parameters and the number of LSTM layers in order to improve the performance . ( We set them to fixed values in the original submission ) We are also following Reviewer 3 \u2019 s advice of fine-tuning the BERT embedding part ( component a ) together with the rest of the model ( component bcd ) during training to see if there \u2019 s an improvement . On the other hand , we want to get reviewers \u2019 attention that the main advantage of ProtoryNet is the interpretability , for both technical users and non-technical users . So we went into great detail comparing ProtoryNet with the other very recent interpretable RNN model to show that our model is both more interpretable and accurate . We have also added comparisons with two more baselines proposed by Reviewer 2 . The results are shown in the updated paper . Please also see our comment to all reviewers at the top of the page . * * * * * Q4 * * : Comparison with model agnostic methods such as LIME and SHAP , and attention-based explanations * * A4 * * : We have added the methods you pointed out in the paper in the related work . The main reason we did not compare with LIME and SHAP is that they are post-hoc explanation methods . Posthoc explainers explain a black-box predictive model . Here , ProtoryNet is a predictive model itself , so they are two types of models that are not directly comparable . Meanwhile , attention-based explanations are not comparable either , since recent research seems to be against using the attention weights as explanations [ 1 ] . Reviewer 2 also pointed out similar things that attention-based methods are not good comparison here . The most directly comparable baseline is ProSeNet , that is why we went into great detail to compare it with our model with both experimental evaluations and human evaluation . Please also see our discussion in response to all reviewers at the top of the page . * * * * * Q5 * * : Robustness evaluation * * A5 * * : You made a very good point about the robustness evaluation ! We will add a new metric in the paper , called epsilon-robustness , which measures the probability that two sentences are mapped to different prototypes if their distance is less than epsilon . We upload the results once the experiments are done . We have too many experiments running at the same time . Please allow us some time to get the new results . Thank you ! * * * * * Q6 * * : Using other classifiers such as decision trees and logistic regression * * A6 * * : While each sentence is mapped to a prototype to be represented by a sparse vector , it is still sequence data with varying lengths , so decision tree or logistic regression models can not be applied . But we have implemented the Reviewer 2 's idea of directly averaging the sentiments of prototypes the input sentences are mapped to , and the performance is worse than using an LSTM ( please see the results in the updated paper ) . If you have a specific idea of how to apply decision tree or logistic regression to replace LSTM , please let us know . Thank you ! * * * [ 1 ] Jain , Sarthak , and Byron C. Wallace . `` Attention is not explanation . '' arXiv preprint arXiv:1902.10186 ( 2019 ) ."}], "0": {"review_id": "KwgQn_Aws3_-0", "review_text": "The authors propose ProtoryNet , a prototype-based model for paragraph classification that associates each sentence in the paragraph with a relevant prototypical sentence from the training data . The idea is interesting and the ability to decompose sentiment scores over each sentence + find prototypes for each helps to build user understanding of the model prediction . Thank you to the authors for the submission . However , I have two concernsbaseline comparisons and model detailsthat prevent me from assigning a higher rating . * * Baseline comparisons * * 1 . Across 5 sentiment classification datasets , the authors find that ProtoryNet substantially underperforms a standard BERT model , in some cases obtaining more than double the error . This seems like a substantial price to pay the ability to associate each sentence with a prototype . The authors write that `` Note , however , that DistilBERT was pre-trained on a massive corpus of text data ... Hence , [ it ] should only be used for [ a ] sanity check '' . However , ProtoryNet also seems to build on top of standard pre-trained BERT embeddings , which have also been derived from a massive corpus of text data ( and actually , the comparison should favor ProtoryNet , since DistilBERT is a smaller model than the standard BERT model ) . Could the authors elaborate on why they believe that this comparison is unfair ? If it is unfair , the authors should perform a comparison that is as similar as possible to ProtoryNet but without the prototype parts , i.e. , train a model on top of the same BERT embeddings and see how that performs . 2.Related to the above question , it is common to fine-tune BERT models on the dataset of interest . Was this done here for DistilBERT ? What about for ProtoryNet ? And if not , why not ? * * Model details * * 3 . It was difficult to follow all of the model details ; perhaps consider reorganizing and clarifying the writing . For example , it was unclear how the prototypes are actually chosen until late in the paper , whereas it should have been explained in S3.1 . The notation in S3.1 has a few minor errors . For example , if the entire sentence is encoded as $ \\mathbb { R } ^V $ then it seems like $ V $ is not just the size of the vocabulary , but the size of the vocabulary to the power of the length of the sentence ? Also , how were the hyperparameter values and coefficient values chosen ? Is prototype projection also done at the end of training ? 4.There are many modeling decisions that seem somewhat ad-hoc or non-standard . It seems like it might be possible to simplify the model significantly , or if not , it would be nice for the effects of these decisions to be better studied . For example : ( a ) mean-squared error is used even on binary classification problems ; ( b ) the loss function is complicated by diversity and prototypicality terms , but the sensitivity analysis reveals that the accuracies are basically indistinguishable even when we completely remove those terms ; ( c ) the sparsity transformation was approximated by a softmax that seems basically indistinguishable from a step function since $ \\gamma \\geq 10^6 $ , so does it actually matter ? ( Note that ReLUs are also not differentiable . ) ( d ) How useful is the LSTM at the end , if it generally goes over only ~4 sentences ? * * Update * * Thank you to the authors for the revisions , and great to know that the experimental results have improved significantly . In the absence of an updated manuscript , it is difficult to update my score appropriately , so I will leave it as it currently is . However , I think the work is promising and that an updated manuscript that incorporates the new experimental results and more carefully teases apart the contributions of the different components ( as the authors have started to do in this rebuttal period ) would be impactful . Thank you to the authors again for all of their hard work .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your valuable feedback ! Here we address your comments and clarify the paper a little more . * * * * * Q1 * * : Performance comparison with DistilBERT * * A1 * * : you are right that DistilBERT outperforms ProtoryNet . We did not claim it was an \u201c unfair \u201d comparison and we acknowledged that ProtoryNet did not perform as well as the SOTA , paying the price for interpretability . However , ProtoryNet outperforms the latest prototype-based baseline ProSeNet . So the message we try to deliver is that if one desires such an understandable model , then ProtoryNet is a better choice than the other interpretable baseline . We understand that many researchers may still view predictive performance as a dominating factor when evaluating a new method , but we hope the interpretability of the model can be appreciated . Please also see our discussion of this issue in the response to all reviewers . * * * * * Q2 * * : fine-tuning BERT * * A2 * * : We did fine-tune DistilBERT on each dataset of interest but we didn \u2019 t fine-tune BERT sentence embedding in ProtoryNet , using BERT as a service . You actually made a very good point ! Thank you for that ! We are re-running our experiments now , allowing the BERT sentence embedding to be trained with the rest of the model . But the training takes a long time so please allow us some time . * * * * * Q3 * * : model details * * A3 * * : Thank you for the suggestion . Section 3.1 describes the architecture and * * forward * * functions from component a to d. We added explanations about the prototypes when we describe the prototype layer but we will leave the prototype initialization and projection to section 3.3 since it is part of the training process . Each sentence is coded as $ \\mathbb { R } ^V $ because each sentence is a vector of size $ V $ , where each element in $ V $ represents whether the corresponding word appears in the sentence . In the submitted paper we set $ \\alpha= { 0.1 } $ , $ \\beta=1e^ { -4 } $ , and $ K = 200 $ to generate the results in Table 1 . We are running new experiments tuning these parameters in order to improve the performance . The prototype projection is done every 10 epochs . * * * * * Q4 * * : modeling decisions * * A4 * * : a ) There exist many choices of loss functions such as MSE , cross-entropy , etc . MSE is one common choice . We do not see any particular reason for choosing or not choosing a specific loss function . So for the purpose of presenting the model , we just pick MSE as it can also be used for both classification and regression , which our framework can also be applied . If you feel there \u2019 s a specific loss function that needs to be tested , please let us know and we can run more experiments . b ) the diversity and prototypicality terms are designed not for the purpose of improving accuracy , but for improving interpretability . The reason it doesn \u2019 t hurt the predictive performance can be explained by the recent research on \u201c Rashomon Set \u201d [ 1 ] , that there exist many models with very similar performance , so one can add customized constraints to the model to achieve additional benefits , such as interpretability . Here , to achieve good explanations , we desire prototypes that are different from each other to avoid redundancy , thus the diversity term . We also want each input sentence to be mapped to a prototype that is similar enough to make the explanation convincing , thus the prototypicality term . In fact , similar terms have been introduced in other prototype based DNN models [ 2,3 ] . c ) We apply a softmax not only to approximate a step function but most importantly , to select the most similar prototype ( with the largest similarity ) . So it is necessary to apply a softmax instead of a step function . d ) Our experiments actually show the performance of long paragraphs ( > 25 words , which are often over 4 sentences ) . We are running experiments now where we simply averaging the sentiments of the sentences . The results we have collected so far suggest that the performance is worse than using LSTM at the end to capture the temporal patterns . * * * [ 1 ] Rudin , Cynthia . `` Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead . '' Nature Machine Intelligence 1.5 ( 2019 ) : 206-215 . [ 2 ] Chen , Chaofan , et al . `` This looks like that : deep learning for interpretable image recognition . '' Advances in neural information processing systems . 2019 . [ 3 ] Ming , Yao , et al . `` Interpretable and steerable sequence learning via prototypes . '' Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2019 . * * * * * Please let us know if there 's anything unclear or need more clarification * * Thank you again for your comments !"}, "1": {"review_id": "KwgQn_Aws3_-1", "review_text": "Summary : this paper presents an RNN sequence classifying model that generates a prototype for each sentence in a paragraph . The generated prototypes help explain the model 's prediction . The method embeds each sentence , matches to prototypes , then runs through an LSTM before making a prediction . Experiments found improved accuracy compared to a previous model that generates only one prototype for a paragraph . A user evaluation also found improvement in interpretability . Strengths : - The paper is well written and easy to understand . - Generating prototypes is a promising direction for improving the interpretability of RNNs and other neural nets models . The idea of generating a prototype trajectory for sentences in a paragraph is interesting and novel to my knowledge . - The architecture and training methods are technically sound . - The experiments show positive improvement in prediction accuracy . Concern on user evaluation : - There is some improvement but the error bars are large , it is not clear if the differences are statistically significant . - With a prototype for each sentence , users will need to read more . So a more fine-grained explanation will increase cognitive load . User evaluation can potentially investigate this .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your positive comments . We are very glad you like our paper . Here we address your concerns on user evaluation . * * * * * Q1 * * : Error bar being too large * * A1 * * : The results were collected from 58 subjects in the original paper . In the last couple of months , our survey continued to collect data and now we have a total of 111 subjects who participated in the survey . We have updated the paper with the new results and report the p-value for comparing the two models . Results show that the difference is statistically significant . * * * * * Q2 * * : Fine-grained explanation increases the cognitive load * * A2 * * : Thank you for bringing up this discussion . Here \u2019 s our thoughts on this topic . We think that users ' comprehension of a new model follows a Bell curve , where as information is provided , they understand the model better . However , when the information saturates or overloads , their understandability is inhibited , just as you concerned . Without user study , it is unknown where the amount of information provided lies in the Bell curve . So that is why we designed the experiments in the paper , comparing ProtoryNet with ProSeNet , knowing that ProtoryNet presents more information than ProSeNet . The results already show that users feel ProtoryNet is easier to understand than ProSeNet . According to the comments we collected from the survey , users feel that prototypes provided at the document level are difficult to understand or map to the input since the semantics are more complicated ; while in our model , prototypes at the sentence level are easier to make such links . In addition , we feel interpretability is quite subjective and has to be determined by the application and the specific user . Reviewer 2 proposed Bag-of-words , which is on the word level , ProSeNet is on the document level , and our model , ProtoryNet , sits right in the middle . In practice , it will be helpful to provide users different options so they can choose based on their specific task and application . * * * * * Please let us know if there 's anything unclear or need more clarification . * * Thank you again for your comments !"}, "2": {"review_id": "KwgQn_Aws3_-2", "review_text": "This paper addresses the problem of explainable AI by trying to build models which are inherently interpretable ( as opposed to post-hoc methods that interpret black-box models after being trained ) . In particular , they focus on a recent paper on prototype-based networks , and a model called prosenet . They introduce an extension , protorynet , which fares better on longer documents by applying prototypes to each sentence ( rather than the whole document ) , leading to a `` prototype trajectory '' , showing how the prediction evolves over the course of the sentence . They show improvements in prediction accuracy across 5 datasets , analyse the hyperparameters and conduct a human study . Strengths : I really like the concept of prototype networks , as it is simple and I could see it being accessible to non-technical users . The authors are to commended for undertaking a proper evaluation , including human experiments , which are often painful to conduct but quite helpful . The proposed extension is straightforward and easy to understand . As someone unfamiliar with the original paper from Ming et al , I found it easy to get up to speed . The authors are also good at plainly describing the details of their approach , and running the requisite ablation studies ( e.g.between sigmoid/exponential ) . Weaknesses : 1 . The paper 's prior work section is incomplete , and makes factually incorrect statements about the state of the field . In particular , the authors focus on attention-based techniques , ignoring the considerable amount of work on other approaches , and incorrectly saying that other approaches `` often turn out to be gibberish '' . Attention is a particularly strange subset of interpretations to focus on , given that the community has recently started to argue about whether attention is an explanation at all [ 7 ] In general , I would suggest the authors look at papers from the \u201c Interpretability and Analysis of Models for NLP \u201d track at ACL and the blackboxNLP workshop at ACL . For concrete starting points , I 'll restrict myself to ICLR papers , dating back to 2017 [ 2-5 ] . General attribution methods , such as integrated gradients [ 6 ] are also pertinent . I do n't think the paper can be published without a reasonable related work section , hence the `` clear reject '' score . If this were fixed , I 'd upgrade to a `` weak reject '' , i.e.5.2.I am concerned that this model may be outperformed by simple , bag of words approaches . For IMDB , the original paper [ 1 ] has a variety of results hitting 88 % accuracy , while the results reported for protorynet are 85 % . If a bag of words model outperforms protorynet , that makes it a less desirable model . It is possible that preprocessing differences account for this , though . Can the authors give some clarity on this , for IMDB as well as the other datasets ? Ideally , there would be an additional column in the results table providing results for a simple , non-neural , baseline . 3.I worry that a lot of the added accuracy does not help the model 's accuracy . In particular , the ablation charts in Figures 5 and 8 show that when alpha=beta=0 , the model 's accuracy is within ~.2 % of the best accuracy , so do we need those additional loss terms ? I also suspect that averaging the outputs , rather than feeding them through an RNN would be comparably accurate , and simpler . 4.While I appreciate the human studies , the confidence bars are quite wide , so that most of the findings are not statistically significant . 5.For the model diagnosis human experiment , the users were only shown three examples . How were these examples chosen ? That is not very many , so I worry that either those examples were chosen to be ones where protorynet was better . Ideally they would be chosen randomly , subject to some reasonable criteria . Nitpicks : - Distillbert is an odd SOTA to choose , as it is designed to have fewer parameters . Something like RoBERTA would likely have stronger results , and be more representative of SOTA . [ 1 ] https : //ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf [ 2 ] https : //arxiv.org/abs/1911.06194 [ 3 ] https : //arxiv.org/abs/1812.04801 [ 4 ] https : //arxiv.org/abs/1801.05453 [ 5 ] https : //arxiv.org/abs/1801.05453 [ 6 ] https : //arxiv.org/abs/1703.01365 [ 7 ] https : //arxiv.org/abs/1902.10186", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your positive feedback on our idea and concept ! We apologize for not presenting enough material and let us try if we can address your concerns in the response . * * * * * Q1 * * : lacks related work * * A1 * * : We apologize for missing the important related work . We want to clarify that `` often turn out to be gibberish '' refers to attention methods . We agree with you attention methods have been under heated debate and are particularly controversial . But from the feedback we have got so far , we realize many researchers would still immediately think of attention methods when it comes to interpretability of RNN . So we feel it is necessary to include it , even just to make a distinction . We will lightly touch attention methods in the updated version and cite the recent paper [ 4 ] . In addition , thank you very much for pointing us to the papers . We have added one paragraph in the related work section discussions the papers listed in the review . Also , we do want to make a distinction here , that the papers listed in your review are posthoc explanation methods while ProtoryNet itself is interpretable and does not rely on any external explainers . * * * * * Q2 * * : Concern about the predictive performance and adding an interpretable baseline * * A2 * * : We understand your concern about the predictive performance . We are working on re-running all experiments , this time allowing the BERT sentence embedder to be fine tuned ( per Reviewer 3 \u2019 s suggestion ) . We have added bag-of-words as an interpretable , non-neural baseline . ( Note that in the paper you point us to , they used a dataset with 50k instances while our model was applied to the original version of the data with only 25k instances ) . We have run bag-of-words on our datasets for a meaningful comparison . On average ProtoryNet is better in predictive performance . Also , bag-of-words ' explanations are on word-level , which might be too fine-grained . ProtoryNet provides sentence level interpretations and also track how sentiment changes . * * * * * Q3.1 * * : the purpose of the diversity and prototypicality terms * * A3.1 * * : We apologize for not explaining the purpose of the two terms clearly in the paper . These two terms are NOT designed for improving accuracy , but for the purpose of improving interpretability . The fact that it doesn \u2019 t hurt the predictive performance can be explained by the recent research on \u201c Rashomon Set \u201d [ 1 ] , that there exist many models with very similar performance , so one can add customized constraints to the model to achieve additional benefits , such as interpretability . Here , to achieve good explanations , we desire prototypes that are different from each other to avoid redundancy , thus the diversity term . We also want each input sentence to be mapped to a prototype that is similar enough to make the explanation convincing ( if `` I love the food '' is mapped to the prototype `` terrible service '' , users would n't believe this model ) , thus the prototypicality term . In fact , similar terms have been introduced in other prototype based DNN models [ 2,3 ] . We have added more explanations of the two terms in the paper . * * * * * Q3.2 * * : Averaging the outputs * * A3.2 * * : Thank you for proposing the easy but interesting baseline . We have been running experiments . The results we have collected so far show that this baseline is worse than ProtoryNet . We will update the paper once we have complete results for all datasets . * * * * * Q4 * * : Statistically insignificant results * * A4 * * : The results were collected from 58 subjects in the original paper . In the last a couple of months , our survey continued to collect data and now we have a total of 111 subjects who participated in the survey . We will update the paper with the new results and report the p-value for comparing the two models . * * * * * Q5 * * : Survey design * * A5 * * : Each user was shown three examples , but the three examples were randomly drawn from a pool of 20 examples where both ProtoryNet and ProSeNet misclassified . The 20 examples were selected from the common set of misclassification and having less than or equal to 5 sentences . * * * [ 1 ] Rudin , Cynthia . `` Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead . '' Nature Machine Intelligence 1.5 ( 2019 ) : 206-215 . [ 2 ] Chen , Chaofan , et al . `` This looks like that : deep learning for interpretable image recognition . '' Advances in neural information processing systems . 2019 . [ 3 ] Ming , Yao , et al . `` Interpretable and steerable sequence learning via prototypes . '' Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2019 . [ 4 ] Jain , Sarthak , and Byron C. Wallace . `` Attention is not explanation . '' arXiv preprint arXiv:1902.10186 ( 2019 ) . * * * * * Please let us know if there 's anything unclear or need more clarification * * Thank you again for your comments !"}, "3": {"review_id": "KwgQn_Aws3_-3", "review_text": "This paper presents ProtoryNet , a framework for text data that classifies and explains the prototypes ' results . The key concept , that is the novelty of the work , is that this framework is based on sentence prototypes , called prototype trajectory in the paper . In particular , instead of working at the entity of the text level , the text is split into sentences and each sentence is analyzed by itself . The structure of the framework is composed of a layer that encodes a text 's sequences , followed by a prototype layer in which is computed the similarity among each sentence and the prototype trajectories . At this point , the sentences are represented in one-hot encoding : for each sentence , there is a bunch of zero and then a one for the most similar sentence prototype . This representation is used for the classification of the sentence , done using an LSTM structure . In this setting , the interpretation is given by exploiting the prototypes matched for the text under analysis . The idea presented in the paper is really interesting : it allows for an interpretation based on prototypes that is simple and understandable while achieving acceptable prediction performance . Pros It is well written and easy to read . There is only a repetition of a \u201c the \u201d at the end of page 7 . The idea of the trajectory prototypes is quite interesting , especially because it can be employed for other kinds of sequence data . The explanations obtained are compelling : the results obtained from the human evaluation showed excellent results , especially in the context of local explanations for non-experts . I appreciated the presentation of the framework using a picture due to the complicated structure . Cons A few words more about the diversity and the prototypically would have been useful in the objective functions . The reader is able to get a general idea , but maybe in the appendix , there could be something more to understand the claims fully . The prediction performance of the model is fine , but not excellent . In my opinion , some experiments more about the prototype initialization would have been helpful in understanding if it can be a possible source of errors . The evaluation of the explanations is only w.r.t.ProSeNet.However , there are other methods to compare with , such as LIME and SHAP , to name agnostic methods , but also attention-based explanations , such as LRP ( Layerwise-Relevance-Propagation ) , NeuroX and Integrated Gradients ( that are not cited even in the related work section ) . What about the robustness of the explanations ? Similar sentences are going to be represented by the same prototype or by prototypes that are similar ? An analysis of the robustness of the explanations would be useful ( fidelity , hit , and similar metrics could be used , but also methods such as ROAR-RemOve And Retrain or KAR-Keep and Retrain ) What about other classificators ? The last part of the framework uses a LSTM to predict the sentiment of the sentence . However , due to the structure based on similarity , other classificators such as logistic regression or decision tree may be tested in the same fashion of shapelet-based classifiers . The explanation may also benefit from the use of such methods due to the additional interpretable information they provide . [ LIME ] Ribeiro , M. T. , Singh , S. , & Guestrin , C. ( 2016 , August ) . `` Why should I trust you ? '' Explaining the predictions of any classifier . In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining ( pp.1135-1144 ) . [ SHAP ] Lundberg , S. M. , & Lee , S. I . ( 2017 ) .A unified approach to interpreting model predictions . In Advances in neural information processing systems ( pp.4765-4774 ) . [ LRP ] Patil , A. , Wadekar , A. , Gupta , T. , Vijan , R. , & Kazi , F. ( 2019 , July ) . Explainable LSTM Model for Anomaly Detection in HDFS Log File using Layerwise Relevance Propagation . In 2019 IEEE Bombay Section Signature Conference ( IBSSC ) ( pp.1-6 ) .IEEE . [ LRP ] Bach , S. , Binder , A. , Montavon , G. , Klauschen , F. , M\u00fcller , K. R. , & Samek , W. ( 2015 ) . On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation . PloS one , 10 ( 7 ) , e0130140 . [ INTGRAD ] Sundararajan , M. , Taly , A. , & Yan , Q . ( 2017 ) .Axiomatic attribution for deep networks . arXiv preprint arXiv:1703.01365 . [ SHAPELET ] Lines , J. , Davis , L. M. , Hills , J. , & Bagnall , A . ( 2012 , August ) . A shapelet transform for time series classification . In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining ( pp.289-297 ) .", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your valuable feedback ! We are enthusiastic that you like our idea . Please let us do a little more explanation here to address your concern . * * * * * Q1 * * : typo * * A1 * * : fixed . Thank you for your careful reading . * * * * * Q2 * * : a few more words about the diversity and prototypicality * * A2 * * : We have added a paragraph in Section 3 to discuss the two terms in more detail . Please see the updated paper . * * * * * Q3 * * : Predictive performance is fine but not excellent * * A3 * * : Yes we agree there is a gap compared to SOTA . We have tested not initializing the prototypes but the performance got slightly worse , so we can rule out it as an explanation . We are now tuning the hyper-parameters and the number of LSTM layers in order to improve the performance . ( We set them to fixed values in the original submission ) We are also following Reviewer 3 \u2019 s advice of fine-tuning the BERT embedding part ( component a ) together with the rest of the model ( component bcd ) during training to see if there \u2019 s an improvement . On the other hand , we want to get reviewers \u2019 attention that the main advantage of ProtoryNet is the interpretability , for both technical users and non-technical users . So we went into great detail comparing ProtoryNet with the other very recent interpretable RNN model to show that our model is both more interpretable and accurate . We have also added comparisons with two more baselines proposed by Reviewer 2 . The results are shown in the updated paper . Please also see our comment to all reviewers at the top of the page . * * * * * Q4 * * : Comparison with model agnostic methods such as LIME and SHAP , and attention-based explanations * * A4 * * : We have added the methods you pointed out in the paper in the related work . The main reason we did not compare with LIME and SHAP is that they are post-hoc explanation methods . Posthoc explainers explain a black-box predictive model . Here , ProtoryNet is a predictive model itself , so they are two types of models that are not directly comparable . Meanwhile , attention-based explanations are not comparable either , since recent research seems to be against using the attention weights as explanations [ 1 ] . Reviewer 2 also pointed out similar things that attention-based methods are not good comparison here . The most directly comparable baseline is ProSeNet , that is why we went into great detail to compare it with our model with both experimental evaluations and human evaluation . Please also see our discussion in response to all reviewers at the top of the page . * * * * * Q5 * * : Robustness evaluation * * A5 * * : You made a very good point about the robustness evaluation ! We will add a new metric in the paper , called epsilon-robustness , which measures the probability that two sentences are mapped to different prototypes if their distance is less than epsilon . We upload the results once the experiments are done . We have too many experiments running at the same time . Please allow us some time to get the new results . Thank you ! * * * * * Q6 * * : Using other classifiers such as decision trees and logistic regression * * A6 * * : While each sentence is mapped to a prototype to be represented by a sparse vector , it is still sequence data with varying lengths , so decision tree or logistic regression models can not be applied . But we have implemented the Reviewer 2 's idea of directly averaging the sentiments of prototypes the input sentences are mapped to , and the performance is worse than using an LSTM ( please see the results in the updated paper ) . If you have a specific idea of how to apply decision tree or logistic regression to replace LSTM , please let us know . Thank you ! * * * [ 1 ] Jain , Sarthak , and Byron C. Wallace . `` Attention is not explanation . '' arXiv preprint arXiv:1902.10186 ( 2019 ) ."}}