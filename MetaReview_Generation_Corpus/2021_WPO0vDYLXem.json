{"year": "2021", "forum": "WPO0vDYLXem", "title": "Hyperparameter Transfer Across Developer Adjustments", "decision": "Reject", "meta_review": "The paper has been actively discussed, both during and after the rebuttal phase. I enjoyed, and I am thankful for, the active communication that took place between the authors and the reviewers.\n\nOn the one hand, the reviewers agreed on several pros of the paper, e.g.,\n* Clear, well presented manuscript\n* The presentation of practically-relevant setting\n* A work that fosters reproducible research (both BO data and algorithms are made available)\n* Careful experiments\n\nOn the other hand, several important weaknesses were also outlined, e.g.,\n* _Novelty_: While the authors claim they \u201cintroduce a practically relevant and fundamentally novel research problem\u201d, existing commercial HPO solutions already mention, and propose solutions for, the very same problem, e.g., [AWS](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-warm-start.html) (section \u201cTypes of Warm Start Tuning Jobs\u201d) and [Google cloud](https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-on-google-cloud-platform-is-now-faster-and-smarter) (section \u201cLearning from previous trials\u201d). The reviewers all agreed on the fact that this down-weights the novelty aspect (claimed many times in the rebuttal and the manuscript): The paper formalizes an already existing framework rather than introducing it.\n* In the light of the weakened \"novelty\" contribution (see above), the reviewers regretted the absence of a novel transfer method _tailored to HT-AA_, which would have certainly strengthened the submission.\n* _\u201cDynamic range\u201d of the benchmark_: It is difficult to evaluate the capacity of the benchmark to discriminate between different approaches (e.g., see new Fig. 3 showing the violin plot with all three methods for transfer, as suggested by Reviewer 1: the improvements over \"best first\" seem marginal at best). To better understand the benchmark, it would be nice to illustrate its \u201cdynamic range\u201d by exhibiting a more powerful method that would substantially improve over \u201cbest first\u201d.\n\nAs illustrated by its scores, the paper is extremely borderline. Given the mixed perspectives of pros and cons, we decided with the reviewers to recommend the rejection of the paper. ", "reviews": [{"review_id": "WPO0vDYLXem-0", "review_text": "Major weaknesses of the paper : - My understanding is that these are surrogate models that are meant to simulate a real-world task . However there is no description as to how these surrogates were created or trained , nor how their fidelity to the original task was vetted . - The most simple and naive algorithm seems to provide similar speed-ups to the much more complicated proposed T2PE ; especially when considering the much larger improvement of the naive method ( see Fig.11 ) , none of the other proposed methods seem justified to me . - Furthermore , I do n't consider this naive approach ( Best First ) as being an HPO approach that leverages transfer , since it does exactly what anyone would do when faced with a slightly altered set of hyperparameters . - This last point suggests that at least one of the following must be true : * non-trivial transfer is not as important as intuition would lead us to think ; * this benchmark suite does not provide a good testbed for assessing an HPO method 's ability to transfer ; or * none of the non-trivial proposed algorithms do a good job transferring and can therefore not argue against the previous point . Given this important contradiction , I must recommend a rejection . My recommendations would be to : - focus on creating a good benchmark suite ( perhaps focus on a single or two domains as introduce many variants , instead of four domains with only two variants ) ; - focus on vetting the surrogates in terms of their fidelity to the task they are meant to simulate ; and - focus on demonstrating that accounting for the slight modifications in the subsequent HPO does indeed provide a benefit over the naive thing to do . For the record , I do n't think this is an easy task . Minor points : - Justify geometric mean . I 'm not saying it 's the wrong way to compare these , I just think it requires at least a sentence of justification . - Same for the violin plots . For such simple plots , simple boxes and whiskers , with perhaps data points to show the spread of measurements across seeds , would do just fine . - Figure 4 , and indeed any mention of the two methods therein , can be entirely removed from the paper ; other than to perhaps mention that they were tried and failedresults in the appendix . - A much more interesting replacement for that figure would be Figure 11 . - Not sure what is the point of comparing random search to TPE in the appendix unless this means Best-first then Random-search/TPE ? If the latter is true , please clarify .", "rating": "5: Marginally below acceptance threshold", "reply_text": "3. \u201c The most simple and naive algorithm seems to provide similar speed-ups to the much more complicated proposed T2PE ; especially when considering the much larger improvement of the naive method ( see Fig.11 ) , none of the other proposed methods seem justified to me . [ ... ] My recommendations would be to : [ ... ] focus on demonstrating that accounting for the slight modifications in the subsequent HPO does indeed provide a benefit over the naive thing to do . For the record , I do n't think this is an easy task. \u201d -- > First , we refer to the reply to all reviewers where we elaborate on the goals of our empirical evaluation , and explain in detail why the empirical evaluation of only-optimize-new and drop-unimportant is quite valuable to the community . Second , we have dropped the range adjustment part of T2PE ( in the revised version TGP as we use GPs instead of TPE now , based on the reviews ) to make it much simpler . TGP is now like only-optimize-new , but instead of using the best previous values of previously existing hyperparameters at each step , the algorithm samples from a model fitted on the projected results of the previous HPO . Finally , we actually do provide an approach that does indeed yield a benefit over best-first ( which is not an easy task ! ) , i.e. , the combination of best-first and T2PE ( in the revised version TGP ) . This approach provides an additional 0.1-0.3 average speedup where the number of evaluations for the previous HPO is larger than 10 ( Table 2 ) . We made changes to feature this result more prominently and added per-benchmark results for the combination of best-first and TGP to Appendix B ( showing that the speedup of TGP+best-first over best-first is up to a factor of 1.5x in some benchmarks ; see e.g. , Figure 10 , FCN-B and NAS-A ) . 4. \u201c My recommendations would be to : focus on creating a good benchmark suite ( perhaps focus on a single or two domains as introduce many variants , instead of four domains with only two variants ) ; [ ... ] \u201c -- > Our benchmark suite ( a ) is based on code and data from commonly to widely used benchmarks in HPO research , ( b ) covers a wide range of algorithms , ( c ) includes developer adjustments of many different types and with many motivations [ see our response to AnonReviewer4 point 4 ( which includes the large appendix comment \u201c Appendix : Motivations for Developer Adjustments \u201d ) ] , ( d ) includes many tasks for each algorithm and adjustment , and ( e ) is independent of hardware and comparatively cheap to evaluate . We think these are the attributes of a high quality benchmark suite . 5. \u201c Justify geometric mean . I 'm not saying it 's the wrong way to compare these , I just think it requires at least a sentence of justification. \u201d -- > We agree and have added a justification to our paper . Intuitively , the geometric mean is an average of speedup values . E.g. , two speedups of 0.1x and 10x intuitively average to 1x , and not 5.05x . We want to note that the arithmetic mean is an upper bound for the geometric mean , so using the geometric mean in fact makes our speedups slightly less impressive than had we used the standard arithmetic mean . 6. \u201c Same for the violin plots [ justification ] . For such simple plots , simple boxes and whiskers , with perhaps data points to show the spread of measurements across seeds , would do just fine. \u201d -- > We agree that we should explain why we use violin plots and have added a justification to the results paragraph in Section 5 . The main advantage of using violin plots is being able to take into account potential multi-modality of the data distribution . E.g. , in Figure 3 the geometric mean of the best-first approach is not always at a point of high density and the distribution over 8 benchmarks has multiple modes . While stated in the respective captions , here , we also want to note that our plots show either violins over benchmark geometric means , or violins over task geometric means for each benchmark . The seeds are averaged for each task individually . 7. \u201c Figure 4 , and indeed any mention of the two methods therein , can be entirely removed from the paper ; other than to perhaps mention that they were tried and failedresults in the appendix. \u201d . A much more interesting replacement for that figure would be Figure 11 . -- > In the interest of brevity , we refer to the reply to all reviewers , where we have answered this question in detail . 8 . `` Not sure what is the point of comparing random search to TPE in the appendix unless this means Best-first then Random-search/TPE ? If the latter is true , please clarify . '' -- > One reviewer asked how reliable TPE is . A comparison to random search answers this . We now explicitly state this in the description . Please note that we now also include GPs in our evaluation which provide much larger speedups over random search . Thanks again for all your comments ! If we cleared up some of your concerns , we would kindly ask you to update your assessment ."}, {"review_id": "WPO0vDYLXem-1", "review_text": "This paper is addressing a problem which is quite relevant in practice , namely how to warmstart HP optimization after small changes have been done to the ML model . Such changes may modify the HP search space , both by adding/removing HPs , or by changing their value ranges . The paper is clearly written . It introduces 3 potential baselines , as well as a simple transfer strategy . All work is based on TPE , which is frequently used , but not SotA for HPO . The paper does not elaborate on the motivations of these `` developer changes '' . One could suspect many are attempts to modify/improve the HPO process itself , over the * same * model . And this is not really new , there is lots of prior work to help shaping search spaces , both by quantifying HP relevance or by learning search ranges . Say , a developer modifies the value range of an HP . What other motivation would there be than mistrust in the previous range , but no change of algorithm . Same for adding/removing an HP , which normally just means going from fixed default to HPO or back . In fact , the 8 benchmarks are all of that sort . My feeling is that by viewing the problem in this way ( namely , just HP search space optimization ) , there is suddenly a lot more related work not taken into account here . More difficult problems , such as learning ensembles from a range of models , and then adding/removing model types , are not tackled here . These would call for more difficult transfer strategies . This paper does not really propose new methodology , except maybe T2PE , which is a pretty basic heuristic . There is a lot of prior work on transfer HPO , some of which could cewrtainly be adopted . Given that the paper is mainly empirical , one would expect a more thorough and wider evaluation . On the positive side , the paper introduces 8 new benchmarks , even though they are pretty simple setups . Their empirical evaluations are a little thin . Only the best-first baseline works well , results for the others are not shown . It should be noted that best-first is standard in HPO practice , this is the first thing one does for transfer . Their T2PE essentially works just as well , and a combination of the two works slightly better . While the paper categorizes types of modification , the empirical evaluation does not differentiate among them anymore . Also , the restriction to TPE is questionable . Why not also use GP-BO ? All baselines would work just the same . My main recommendation for this work would be to be clear about the modification for such limited developer changes . If this is just about the developer trying to twist HPO in itself , this work would have to compare against previous work for optimizing search spaces . Otherwise , please address more complex scenarios , such as ensemble learning , where HPO transfer becomes really difficult .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Heterogeneous adjustments that change X by changing the range for one or multiple hyperparameters ( and optionally other changes to X , A , or H ) : * Motivations in the case of numerical hyperparameters could be : optimizing the search space , moving to a GPU with larger memory so that e.g. , the batch size or model size can now be larger , changing from a discrete range to a continuous range after being made aware of the possibility of a relaxation of a hyperparameter , etc . * Motivations in the case of categorical hyperparameters could be : removing a choice that has a bug , adding a choice after implementing the corresponding code in A ( e.g. , new type of optimizer or kernel ) , moving to hardware H that allows for additional choices ( e.g. , different arithmetic precisions or representations ) , \u2026 * The following benchmarks include such adjustments for numerical hyperparameters : * SVM-B ( Increase range for cost hyperparameter ) : E.g. , search space optimization . * The following benchmarks include such adjustments for categorical hyperparameters * FCN-B ( Add per-layer choice of activation function ; Change learning rate schedule ) E.g. , when a developer implements additional activation functions and now wants to optimize over them , and at the same time performs a homogeneous adjustment . * NAS-A ( Add 3x3 average pooling as choice of operation to each edge ) : E.g. , after learning about the idea of average pooling , the developer adds this to the NAS search space . Heterogeneous adjustments that change X by adding or removing one or multiple hyperparameters ( and optionally other changes to X , A , or H ) : * Motivations could be : unfixing/exposing an existing hyperparameter , fixing an existing hyperparameter like the batch size after moving to a GPU with less memory , fixing an existing categorical hyperparameter because a bug has been found in one of two choices , a part of the algorithm A was changed and the new version of that part includes one or multiple new hyperparameters ( e.g. , when changing SGD to ADAM , when updating to a GPU that has support for low precision arithmetics , or when changing the NAS cell template ) , a part of algorithm A was changed and the new version of that part does not include certain hyperparameters anymore . * The following benchmarks include such adjustments : * FCN-A ( Increase # units-per-layer 16\u00d7 ; Double # epochs ; Fix batch size hyperparameter ) E.g. , a developer received a more powerful GPU , and could hence increase the model size and training time , but had to fix the batch size to fit the larger model into GPU memory . * FCN-B ( Add per-layer choice of activation function ; Change learning rate schedule ) : E.g. , when a developer implements additional activation functions and now wants to optimize over them , and at the same time performs a homogeneous adjustment . * XGB-A ( Expose four booster hyperparameters ) : The developer learns that certain hyperparameters are important to tune , and hence does not use the default values of the library anymore . * NAS-B ( Add node to cell template ( adds 3 hyperparameters ) ) . E.g. , when moving to a larger GPU that can fit a larger neural network into memory . * SVM-A ( Change kernel ; Remove hyperparameter for old kernel ; Add hyperparameter for new kernel ) . E.g. , when a visualization of the data clearly shows that a radial kernel makes no sense , but a polynomial kernel does , however , now the degree of the polynomial needs to be tuned ( which did not exist as a hyperparameter before ) , and the hyperparameter for the radial kernel is dropped . We hope that this extensive list demonstrates the broad applicability of HT-AA , and that it is by no means limited to search space optimization . If this broad applicability of our problem formulation changes a reviewer \u2019 s mind , we would kindly ask them to update their score accordingly ."}, {"review_id": "WPO0vDYLXem-2", "review_text": "The authors propose a new framework for hyperparameter optimization and transfer across incremental modifications to a given algorithm and its search space , a process called developer adjustments in the paper . The authors then propose a few strategies to transfer knowledge from previous HPO runs and evaluate them on a series of simulated benchmarks . Results show the value added by transferring information from previous runs , as well as the surprising efficiency of simply reusing the best found hyperparameters from the previous run . Strong points : - The framework is simple and clearly introduced . - Extensive experiments help bring to light the advantage of transferring across adjustments . - The paper is very well written . Weak points : - Not enough details on benchmarks , more on this below - The use of simulated benchmarks with surrogate models introduces noise in the evaluation - Comparisons with more baselines would be beneficial . RF and/or GP-based HPO methods are extremely popular and would have been easy to integrate with the best-first baseline . Recommendation : The contributions are simple and incremental , and clearly rooted in machine learning engineering , however I still think they could be beneficial as a whole to the community given the extensive experiments realized . I have some issues with experiments , lack of details and baselines , but those issues are mostly fixable . I 'll give the paper a weak accept for now . Extra comments : You do not specify which benchmarks are based on lookup tables and which ones are based on surrogate models . From looking at the search spaces , I would assume that the SVM and XGB benchmarks are modeled via surrogate benchmarks and the FCN and NAS benchmarks are lookup tables , but this should be explicited in the paper ( or appendix ) . Parameters used for the benchmark surrogate model should also be given ( if defaults of Eggensperger are used , simply mention this ) . It is also not clear what underlying datasets are used , this bears some importance and should be mentioned , even if only in the Appendix . On surrogate model benchmarks : It can be seen in ( Eggensperger et al.2015. , Figure 2 ) that ordering of methods can shift due to noise in the surrogate model ( a random forest ? ) . This is likely going to have a bigger impact when trying to measure the speedup , which is measured when a method reaches a certain threshold of performance . This threshold is likely to be met during the convergence phase of algorithms , and this phase appears noisier ( i.e.looking at how the phases of transition differ between the true benchmark and the RF surrogate benchmark differ in Eggensperger et al.2015 ) .Have you given this any thought ? Have you compared experiments with a few runs on a real benchmark ? The method you end up recommending only has its detailed performance shown in the appendix . This feels counterintuitive to me . This result should be featured in the paper itself . This is perhaps due to the used of those split violin plots , which force you to display only two methods per plot . Maybe you should display a group of X single-sided violin plots where X is the number of methods you are trying to compare . I think it is misleading to portray everything in terms of speedup or improvement over the `` TPE solution with X iterations '' . A more strictly meaningful metric here is accuracy ( assuming there is only one dataset per benchmark ) . Assuming the performance to beat by original TPE was an 11 % error rate , there is a big difference between a method which was able to achieve a 10 % error rate and a method which was able to achieve a 5 % error rate , yet both will be assessed by how quickly they achieved x < 10 % error rate . I ca n't seem to find such figures in the appendices . Typos : - Section 3.1 page 3 , argmax g ( x ) / b ( x ) < < you mean g ( x ) / l ( x ) ? - appendix G , you wrote TPE2 instead of T2PE", "rating": "6: Marginally above acceptance threshold", "reply_text": "5. \u201c The contributions are simple and incremental , and clearly rooted in machine learning engineering , however I still think they could be beneficial as a whole to the community given the extensive experiments realized. \u201d -- > Thank you for the characterization as beneficial as a whole to the community . However , we consider our contributions not as incremental , as we introduce a practically relevant and fundamentally novel research problem . 6. \u201c The method you end up recommending only has its detailed performance shown in the appendix . This feels counterintuitive to me . This result should be featured in the paper itself. \u201d -- > We agree . We changed the paper to feature the recommended method more prominently . 7. \u201c I think it is misleading to portray everything in terms of speedup or improvement over the `` TPE solution with X iterations '' . A more strictly meaningful metric here is accuracy ( assuming there is only one dataset per benchmark ) . [ ... ] I ca n't seem to find such figures in the appendices. \u201d -- > We consider multiple datasets per benchmark ( see Table 3 in Appendix A ) and our benchmarks even use different metrics ( now also in Table 3 ) , so we chose speedup as a metric to bridge all these . Additionally , besides the speedup evaluation , we also provide an evaluation of standardized objective improvement over the `` TPE/GP solution with X iterations '' ( with a small delta added , as some standard deviations were 0 ) in Appendix D ( referenced at the end of the experiments section ) . While this type of evaluation is more common in research on the related hyperparameter transfer across tasks problem , in the main paper we decided to focus on the speedup instead , as the viewpoint of reducing computational demands is ethically stronger ( reducing the carbon footprint , etc ) , and as speedup is easier to interpret than some normalized average objective improvement with a small delta added . 8. \u201c Appendix G , you wrote TPE2 instead of T2PE \u201d -- > Actually , this is not a mistake . In Appendix G we compare TPE with two different seed ranges ( denoted TPE and TPE2 ) to measure the influence of seeds in our evaluation . But we realized that this may have been confusing and renamed the two seed ranges to TPE_1 and TPE_2 . Thanks again for all your comments ! If we cleared up some of your concerns , we would kindly ask you to update your assessment ."}, {"review_id": "WPO0vDYLXem-3", "review_text": "The paper is motivated by the situation where a machine learning algorithm has development adjustment and we would like to reuse the tuning results of previous hyperparameter optimization . The paper calls it HT-AA problem , which certainly is an interesting problem given software can get updated often . The paper proposes four simple baseline algorithms for the HT-AA problem . For the empirical study , a set of eight benchmarks for basic HT-AA problem are presented . The experiment results show the transfer TPE ( T2PE ) and best-first strategy produce good speed up . To reach given objective values , T2PE can be 1.0\u20131.7x faster than TPE , and best-first 1.2\u20132.6x faster comparing with old HPO . The pros of the paper include : 1 . Although the topic of the paper is not one of the most popular , some readers might find it interesting and can be benefited from it . 2.The proposed methods are reasonable and acceptable . 3.Numerical results show some of the proposed methods can help to speed up reoptimizing hyperparameter . The cons include : 1 . Need explain when the only-optimize-new and drop-unimportant methods can be useful . If not useful as the experiments demonstrate , why propose them ? 2.Look like TPE is the method that show speedup for the benchmarks . How reliable the method is ? Is the saving justifying use an extra tuning tool ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your helpful comments , questions , and the positive feedback for our approaches and numerical results , calling our proposed problem setting interesting , and recognizing the potential benefit our paper can bring to ICLR readers . In the following we provide detailed replies to your questions . 3. \u201c Need explain when the only-optimize-new and drop-unimportant methods can be useful . If not useful as the experiments demonstrate , why propose them ? \u201d -- > In the interest of brevity , we refer to the general reply to all reviewers above , where we have answered this question in detail . 4. \u201c Look like TPE is the method that show speedup for the benchmarks . How reliable the method is ? Is the saving justifying use an extra tuning tool ? \u201d -- > Based on a suggestion by other reviewers , we added Gaussian processes approaches which perform better than TPE . To measure how reliable the basic HPO without transfer is , we compare it to random search in Appendix F. As the cost for HPO scales with the cost of the algorithm it is tuning , an average speedup of 1.2 -- 3.6x ( depending on the budgets involved ) and a maximum benchmark-average speedup of over 10x can be a significant cost and CO2 saver . As our transfer algorithms fold into the basic hyperparameter tuning tool itself , and do not add any additional required user actions ( other than optionally choosing where to save results ) , using the across-adjustments transfer algorithms does not add any overhead for the user compared to using a tool for basic HPO . As for the maintainer of the tool : the complexity of the approaches we evaluated is deliberately chosen as low , so implementing these approaches is certainly feasible . The simple code for all our tools is open-sourced . Thanks again for all your comments ! If we cleared up some of your concerns , we would kindly ask you to update your assessment ."}], "0": {"review_id": "WPO0vDYLXem-0", "review_text": "Major weaknesses of the paper : - My understanding is that these are surrogate models that are meant to simulate a real-world task . However there is no description as to how these surrogates were created or trained , nor how their fidelity to the original task was vetted . - The most simple and naive algorithm seems to provide similar speed-ups to the much more complicated proposed T2PE ; especially when considering the much larger improvement of the naive method ( see Fig.11 ) , none of the other proposed methods seem justified to me . - Furthermore , I do n't consider this naive approach ( Best First ) as being an HPO approach that leverages transfer , since it does exactly what anyone would do when faced with a slightly altered set of hyperparameters . - This last point suggests that at least one of the following must be true : * non-trivial transfer is not as important as intuition would lead us to think ; * this benchmark suite does not provide a good testbed for assessing an HPO method 's ability to transfer ; or * none of the non-trivial proposed algorithms do a good job transferring and can therefore not argue against the previous point . Given this important contradiction , I must recommend a rejection . My recommendations would be to : - focus on creating a good benchmark suite ( perhaps focus on a single or two domains as introduce many variants , instead of four domains with only two variants ) ; - focus on vetting the surrogates in terms of their fidelity to the task they are meant to simulate ; and - focus on demonstrating that accounting for the slight modifications in the subsequent HPO does indeed provide a benefit over the naive thing to do . For the record , I do n't think this is an easy task . Minor points : - Justify geometric mean . I 'm not saying it 's the wrong way to compare these , I just think it requires at least a sentence of justification . - Same for the violin plots . For such simple plots , simple boxes and whiskers , with perhaps data points to show the spread of measurements across seeds , would do just fine . - Figure 4 , and indeed any mention of the two methods therein , can be entirely removed from the paper ; other than to perhaps mention that they were tried and failedresults in the appendix . - A much more interesting replacement for that figure would be Figure 11 . - Not sure what is the point of comparing random search to TPE in the appendix unless this means Best-first then Random-search/TPE ? If the latter is true , please clarify .", "rating": "5: Marginally below acceptance threshold", "reply_text": "3. \u201c The most simple and naive algorithm seems to provide similar speed-ups to the much more complicated proposed T2PE ; especially when considering the much larger improvement of the naive method ( see Fig.11 ) , none of the other proposed methods seem justified to me . [ ... ] My recommendations would be to : [ ... ] focus on demonstrating that accounting for the slight modifications in the subsequent HPO does indeed provide a benefit over the naive thing to do . For the record , I do n't think this is an easy task. \u201d -- > First , we refer to the reply to all reviewers where we elaborate on the goals of our empirical evaluation , and explain in detail why the empirical evaluation of only-optimize-new and drop-unimportant is quite valuable to the community . Second , we have dropped the range adjustment part of T2PE ( in the revised version TGP as we use GPs instead of TPE now , based on the reviews ) to make it much simpler . TGP is now like only-optimize-new , but instead of using the best previous values of previously existing hyperparameters at each step , the algorithm samples from a model fitted on the projected results of the previous HPO . Finally , we actually do provide an approach that does indeed yield a benefit over best-first ( which is not an easy task ! ) , i.e. , the combination of best-first and T2PE ( in the revised version TGP ) . This approach provides an additional 0.1-0.3 average speedup where the number of evaluations for the previous HPO is larger than 10 ( Table 2 ) . We made changes to feature this result more prominently and added per-benchmark results for the combination of best-first and TGP to Appendix B ( showing that the speedup of TGP+best-first over best-first is up to a factor of 1.5x in some benchmarks ; see e.g. , Figure 10 , FCN-B and NAS-A ) . 4. \u201c My recommendations would be to : focus on creating a good benchmark suite ( perhaps focus on a single or two domains as introduce many variants , instead of four domains with only two variants ) ; [ ... ] \u201c -- > Our benchmark suite ( a ) is based on code and data from commonly to widely used benchmarks in HPO research , ( b ) covers a wide range of algorithms , ( c ) includes developer adjustments of many different types and with many motivations [ see our response to AnonReviewer4 point 4 ( which includes the large appendix comment \u201c Appendix : Motivations for Developer Adjustments \u201d ) ] , ( d ) includes many tasks for each algorithm and adjustment , and ( e ) is independent of hardware and comparatively cheap to evaluate . We think these are the attributes of a high quality benchmark suite . 5. \u201c Justify geometric mean . I 'm not saying it 's the wrong way to compare these , I just think it requires at least a sentence of justification. \u201d -- > We agree and have added a justification to our paper . Intuitively , the geometric mean is an average of speedup values . E.g. , two speedups of 0.1x and 10x intuitively average to 1x , and not 5.05x . We want to note that the arithmetic mean is an upper bound for the geometric mean , so using the geometric mean in fact makes our speedups slightly less impressive than had we used the standard arithmetic mean . 6. \u201c Same for the violin plots [ justification ] . For such simple plots , simple boxes and whiskers , with perhaps data points to show the spread of measurements across seeds , would do just fine. \u201d -- > We agree that we should explain why we use violin plots and have added a justification to the results paragraph in Section 5 . The main advantage of using violin plots is being able to take into account potential multi-modality of the data distribution . E.g. , in Figure 3 the geometric mean of the best-first approach is not always at a point of high density and the distribution over 8 benchmarks has multiple modes . While stated in the respective captions , here , we also want to note that our plots show either violins over benchmark geometric means , or violins over task geometric means for each benchmark . The seeds are averaged for each task individually . 7. \u201c Figure 4 , and indeed any mention of the two methods therein , can be entirely removed from the paper ; other than to perhaps mention that they were tried and failedresults in the appendix. \u201d . A much more interesting replacement for that figure would be Figure 11 . -- > In the interest of brevity , we refer to the reply to all reviewers , where we have answered this question in detail . 8 . `` Not sure what is the point of comparing random search to TPE in the appendix unless this means Best-first then Random-search/TPE ? If the latter is true , please clarify . '' -- > One reviewer asked how reliable TPE is . A comparison to random search answers this . We now explicitly state this in the description . Please note that we now also include GPs in our evaluation which provide much larger speedups over random search . Thanks again for all your comments ! If we cleared up some of your concerns , we would kindly ask you to update your assessment ."}, "1": {"review_id": "WPO0vDYLXem-1", "review_text": "This paper is addressing a problem which is quite relevant in practice , namely how to warmstart HP optimization after small changes have been done to the ML model . Such changes may modify the HP search space , both by adding/removing HPs , or by changing their value ranges . The paper is clearly written . It introduces 3 potential baselines , as well as a simple transfer strategy . All work is based on TPE , which is frequently used , but not SotA for HPO . The paper does not elaborate on the motivations of these `` developer changes '' . One could suspect many are attempts to modify/improve the HPO process itself , over the * same * model . And this is not really new , there is lots of prior work to help shaping search spaces , both by quantifying HP relevance or by learning search ranges . Say , a developer modifies the value range of an HP . What other motivation would there be than mistrust in the previous range , but no change of algorithm . Same for adding/removing an HP , which normally just means going from fixed default to HPO or back . In fact , the 8 benchmarks are all of that sort . My feeling is that by viewing the problem in this way ( namely , just HP search space optimization ) , there is suddenly a lot more related work not taken into account here . More difficult problems , such as learning ensembles from a range of models , and then adding/removing model types , are not tackled here . These would call for more difficult transfer strategies . This paper does not really propose new methodology , except maybe T2PE , which is a pretty basic heuristic . There is a lot of prior work on transfer HPO , some of which could cewrtainly be adopted . Given that the paper is mainly empirical , one would expect a more thorough and wider evaluation . On the positive side , the paper introduces 8 new benchmarks , even though they are pretty simple setups . Their empirical evaluations are a little thin . Only the best-first baseline works well , results for the others are not shown . It should be noted that best-first is standard in HPO practice , this is the first thing one does for transfer . Their T2PE essentially works just as well , and a combination of the two works slightly better . While the paper categorizes types of modification , the empirical evaluation does not differentiate among them anymore . Also , the restriction to TPE is questionable . Why not also use GP-BO ? All baselines would work just the same . My main recommendation for this work would be to be clear about the modification for such limited developer changes . If this is just about the developer trying to twist HPO in itself , this work would have to compare against previous work for optimizing search spaces . Otherwise , please address more complex scenarios , such as ensemble learning , where HPO transfer becomes really difficult .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Heterogeneous adjustments that change X by changing the range for one or multiple hyperparameters ( and optionally other changes to X , A , or H ) : * Motivations in the case of numerical hyperparameters could be : optimizing the search space , moving to a GPU with larger memory so that e.g. , the batch size or model size can now be larger , changing from a discrete range to a continuous range after being made aware of the possibility of a relaxation of a hyperparameter , etc . * Motivations in the case of categorical hyperparameters could be : removing a choice that has a bug , adding a choice after implementing the corresponding code in A ( e.g. , new type of optimizer or kernel ) , moving to hardware H that allows for additional choices ( e.g. , different arithmetic precisions or representations ) , \u2026 * The following benchmarks include such adjustments for numerical hyperparameters : * SVM-B ( Increase range for cost hyperparameter ) : E.g. , search space optimization . * The following benchmarks include such adjustments for categorical hyperparameters * FCN-B ( Add per-layer choice of activation function ; Change learning rate schedule ) E.g. , when a developer implements additional activation functions and now wants to optimize over them , and at the same time performs a homogeneous adjustment . * NAS-A ( Add 3x3 average pooling as choice of operation to each edge ) : E.g. , after learning about the idea of average pooling , the developer adds this to the NAS search space . Heterogeneous adjustments that change X by adding or removing one or multiple hyperparameters ( and optionally other changes to X , A , or H ) : * Motivations could be : unfixing/exposing an existing hyperparameter , fixing an existing hyperparameter like the batch size after moving to a GPU with less memory , fixing an existing categorical hyperparameter because a bug has been found in one of two choices , a part of the algorithm A was changed and the new version of that part includes one or multiple new hyperparameters ( e.g. , when changing SGD to ADAM , when updating to a GPU that has support for low precision arithmetics , or when changing the NAS cell template ) , a part of algorithm A was changed and the new version of that part does not include certain hyperparameters anymore . * The following benchmarks include such adjustments : * FCN-A ( Increase # units-per-layer 16\u00d7 ; Double # epochs ; Fix batch size hyperparameter ) E.g. , a developer received a more powerful GPU , and could hence increase the model size and training time , but had to fix the batch size to fit the larger model into GPU memory . * FCN-B ( Add per-layer choice of activation function ; Change learning rate schedule ) : E.g. , when a developer implements additional activation functions and now wants to optimize over them , and at the same time performs a homogeneous adjustment . * XGB-A ( Expose four booster hyperparameters ) : The developer learns that certain hyperparameters are important to tune , and hence does not use the default values of the library anymore . * NAS-B ( Add node to cell template ( adds 3 hyperparameters ) ) . E.g. , when moving to a larger GPU that can fit a larger neural network into memory . * SVM-A ( Change kernel ; Remove hyperparameter for old kernel ; Add hyperparameter for new kernel ) . E.g. , when a visualization of the data clearly shows that a radial kernel makes no sense , but a polynomial kernel does , however , now the degree of the polynomial needs to be tuned ( which did not exist as a hyperparameter before ) , and the hyperparameter for the radial kernel is dropped . We hope that this extensive list demonstrates the broad applicability of HT-AA , and that it is by no means limited to search space optimization . If this broad applicability of our problem formulation changes a reviewer \u2019 s mind , we would kindly ask them to update their score accordingly ."}, "2": {"review_id": "WPO0vDYLXem-2", "review_text": "The authors propose a new framework for hyperparameter optimization and transfer across incremental modifications to a given algorithm and its search space , a process called developer adjustments in the paper . The authors then propose a few strategies to transfer knowledge from previous HPO runs and evaluate them on a series of simulated benchmarks . Results show the value added by transferring information from previous runs , as well as the surprising efficiency of simply reusing the best found hyperparameters from the previous run . Strong points : - The framework is simple and clearly introduced . - Extensive experiments help bring to light the advantage of transferring across adjustments . - The paper is very well written . Weak points : - Not enough details on benchmarks , more on this below - The use of simulated benchmarks with surrogate models introduces noise in the evaluation - Comparisons with more baselines would be beneficial . RF and/or GP-based HPO methods are extremely popular and would have been easy to integrate with the best-first baseline . Recommendation : The contributions are simple and incremental , and clearly rooted in machine learning engineering , however I still think they could be beneficial as a whole to the community given the extensive experiments realized . I have some issues with experiments , lack of details and baselines , but those issues are mostly fixable . I 'll give the paper a weak accept for now . Extra comments : You do not specify which benchmarks are based on lookup tables and which ones are based on surrogate models . From looking at the search spaces , I would assume that the SVM and XGB benchmarks are modeled via surrogate benchmarks and the FCN and NAS benchmarks are lookup tables , but this should be explicited in the paper ( or appendix ) . Parameters used for the benchmark surrogate model should also be given ( if defaults of Eggensperger are used , simply mention this ) . It is also not clear what underlying datasets are used , this bears some importance and should be mentioned , even if only in the Appendix . On surrogate model benchmarks : It can be seen in ( Eggensperger et al.2015. , Figure 2 ) that ordering of methods can shift due to noise in the surrogate model ( a random forest ? ) . This is likely going to have a bigger impact when trying to measure the speedup , which is measured when a method reaches a certain threshold of performance . This threshold is likely to be met during the convergence phase of algorithms , and this phase appears noisier ( i.e.looking at how the phases of transition differ between the true benchmark and the RF surrogate benchmark differ in Eggensperger et al.2015 ) .Have you given this any thought ? Have you compared experiments with a few runs on a real benchmark ? The method you end up recommending only has its detailed performance shown in the appendix . This feels counterintuitive to me . This result should be featured in the paper itself . This is perhaps due to the used of those split violin plots , which force you to display only two methods per plot . Maybe you should display a group of X single-sided violin plots where X is the number of methods you are trying to compare . I think it is misleading to portray everything in terms of speedup or improvement over the `` TPE solution with X iterations '' . A more strictly meaningful metric here is accuracy ( assuming there is only one dataset per benchmark ) . Assuming the performance to beat by original TPE was an 11 % error rate , there is a big difference between a method which was able to achieve a 10 % error rate and a method which was able to achieve a 5 % error rate , yet both will be assessed by how quickly they achieved x < 10 % error rate . I ca n't seem to find such figures in the appendices . Typos : - Section 3.1 page 3 , argmax g ( x ) / b ( x ) < < you mean g ( x ) / l ( x ) ? - appendix G , you wrote TPE2 instead of T2PE", "rating": "6: Marginally above acceptance threshold", "reply_text": "5. \u201c The contributions are simple and incremental , and clearly rooted in machine learning engineering , however I still think they could be beneficial as a whole to the community given the extensive experiments realized. \u201d -- > Thank you for the characterization as beneficial as a whole to the community . However , we consider our contributions not as incremental , as we introduce a practically relevant and fundamentally novel research problem . 6. \u201c The method you end up recommending only has its detailed performance shown in the appendix . This feels counterintuitive to me . This result should be featured in the paper itself. \u201d -- > We agree . We changed the paper to feature the recommended method more prominently . 7. \u201c I think it is misleading to portray everything in terms of speedup or improvement over the `` TPE solution with X iterations '' . A more strictly meaningful metric here is accuracy ( assuming there is only one dataset per benchmark ) . [ ... ] I ca n't seem to find such figures in the appendices. \u201d -- > We consider multiple datasets per benchmark ( see Table 3 in Appendix A ) and our benchmarks even use different metrics ( now also in Table 3 ) , so we chose speedup as a metric to bridge all these . Additionally , besides the speedup evaluation , we also provide an evaluation of standardized objective improvement over the `` TPE/GP solution with X iterations '' ( with a small delta added , as some standard deviations were 0 ) in Appendix D ( referenced at the end of the experiments section ) . While this type of evaluation is more common in research on the related hyperparameter transfer across tasks problem , in the main paper we decided to focus on the speedup instead , as the viewpoint of reducing computational demands is ethically stronger ( reducing the carbon footprint , etc ) , and as speedup is easier to interpret than some normalized average objective improvement with a small delta added . 8. \u201c Appendix G , you wrote TPE2 instead of T2PE \u201d -- > Actually , this is not a mistake . In Appendix G we compare TPE with two different seed ranges ( denoted TPE and TPE2 ) to measure the influence of seeds in our evaluation . But we realized that this may have been confusing and renamed the two seed ranges to TPE_1 and TPE_2 . Thanks again for all your comments ! If we cleared up some of your concerns , we would kindly ask you to update your assessment ."}, "3": {"review_id": "WPO0vDYLXem-3", "review_text": "The paper is motivated by the situation where a machine learning algorithm has development adjustment and we would like to reuse the tuning results of previous hyperparameter optimization . The paper calls it HT-AA problem , which certainly is an interesting problem given software can get updated often . The paper proposes four simple baseline algorithms for the HT-AA problem . For the empirical study , a set of eight benchmarks for basic HT-AA problem are presented . The experiment results show the transfer TPE ( T2PE ) and best-first strategy produce good speed up . To reach given objective values , T2PE can be 1.0\u20131.7x faster than TPE , and best-first 1.2\u20132.6x faster comparing with old HPO . The pros of the paper include : 1 . Although the topic of the paper is not one of the most popular , some readers might find it interesting and can be benefited from it . 2.The proposed methods are reasonable and acceptable . 3.Numerical results show some of the proposed methods can help to speed up reoptimizing hyperparameter . The cons include : 1 . Need explain when the only-optimize-new and drop-unimportant methods can be useful . If not useful as the experiments demonstrate , why propose them ? 2.Look like TPE is the method that show speedup for the benchmarks . How reliable the method is ? Is the saving justifying use an extra tuning tool ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your helpful comments , questions , and the positive feedback for our approaches and numerical results , calling our proposed problem setting interesting , and recognizing the potential benefit our paper can bring to ICLR readers . In the following we provide detailed replies to your questions . 3. \u201c Need explain when the only-optimize-new and drop-unimportant methods can be useful . If not useful as the experiments demonstrate , why propose them ? \u201d -- > In the interest of brevity , we refer to the general reply to all reviewers above , where we have answered this question in detail . 4. \u201c Look like TPE is the method that show speedup for the benchmarks . How reliable the method is ? Is the saving justifying use an extra tuning tool ? \u201d -- > Based on a suggestion by other reviewers , we added Gaussian processes approaches which perform better than TPE . To measure how reliable the basic HPO without transfer is , we compare it to random search in Appendix F. As the cost for HPO scales with the cost of the algorithm it is tuning , an average speedup of 1.2 -- 3.6x ( depending on the budgets involved ) and a maximum benchmark-average speedup of over 10x can be a significant cost and CO2 saver . As our transfer algorithms fold into the basic hyperparameter tuning tool itself , and do not add any additional required user actions ( other than optionally choosing where to save results ) , using the across-adjustments transfer algorithms does not add any overhead for the user compared to using a tool for basic HPO . As for the maintainer of the tool : the complexity of the approaches we evaluated is deliberately chosen as low , so implementing these approaches is certainly feasible . The simple code for all our tools is open-sourced . Thanks again for all your comments ! If we cleared up some of your concerns , we would kindly ask you to update your assessment ."}}