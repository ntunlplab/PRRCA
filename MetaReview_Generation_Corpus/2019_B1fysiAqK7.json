{"year": "2019", "forum": "B1fysiAqK7", "title": "Probabilistic Binary Neural Networks", "decision": "Reject", "meta_review": "The paper proposes a probabilistic training method for binary Neural Network with stochastic versions of Batch Normalization and max pooling.\n\nThe reviewers and AC note the following potential weaknesses: (1) limited novelty and (2) preliminary experimental results.\n\nAC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.", "reviews": [{"review_id": "B1fysiAqK7-0", "review_text": "1. This paper presents a novel approach for training efficient binary networks with both binary weights and binary activations and verifies the capability of the proposed method on benchmark. In addition, related bach normalization and pooling layers are also improved. 2. How long do we need for training BNNs using the proposed method? 3. The paper only compares their performance with only one related work (Hubara et al., 2016), which is somewhat insufficient. 4. Another only concern is that whether the proposed method can be well applied on large scale datasets such as ImageNet.", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # On training time Figure 3c in our paper shows a plot of the validation loss . This plot shows that training on cifar-10 converges in approximately 50 epochs . Of course , during training we do have to perform two convolutions ( one for the mean and one for the variance of the Gaussian approximation ) , which may result in a longer wall clock time , if the hardware does not permit parallelization of these two operations . # # On the comparison to other methods We compare to Hubara et al. \u2019 s Binarized Neural Networks as it is the natural alternative to train Binary Neural Networks . The binary network that one obtains after training user our method is equivalent to the Binary Network obtained using the method by Hubara et al.Moreover , in both approaches no new computations are introduced ( such as , for example , in XNOR-net ( Rastegari et al. , 2016 ) or ABC-net ( Lin et al. , 2017 ) ) . For this reason , we believe it is fair to compare our method to that of Hubara et al. , however , we do agree that is may be interesting to explore the probabilistic counterparts of , for example , XNOR-Net and ABC-Net in future work . # On large scale datasets Although we agree that experiments on larger scale datasets are valuable , we also believe that the results currently presented in our paper are already valuable . We did perform some preliminary experiments on larger scale datasets and found evidence that we can scale to larger datasets , however , some careful tuning may be required , as is often the case for binary neural networks ."}, {"review_id": "B1fysiAqK7-1", "review_text": "To reduce the deep neural networks' reliance on memory and high power consumption, the paper proposed a kind of probabilistic neural networks with both binary hidden node and binary weights. The paper presents a probabilistic way to binarize the activation and weight values. Also, it proposed a random version of batch-normalization and max-pooling. The binarization of hidden node is achieved through stochastic sampling according to the sign of the stochastic pre-activation values. The weight binarization is analogously done by sampling from a binary distribution. There is no too much new here, and is a standard way to obtain binary values probabilistically. The paper said in the introduction that the binary model will be trained with the re-parameterization trick, through either propagating the distributions or the samples from concrete distribution. But I am still not very clear how this training process is done, especially for the training of weight parameters. Overall, the problem investigated in this paper is very interesting and is of practical importance, the experimental results are preliminary but encouraging. But all the techniques used in this paper to binarize neural networks are standard, and no too much new here.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear reviewer , We want to clarify that we never sample from the binary weight distributions directly . We only sample the activations ( i.e. , the output of a layer ) in some of our experiments . Moreover , it is technically correct that the stochastic binary activations are related to the sign of the stochastic pre-activations , however , we \u2019 d like to stress that the stochastic binary activation distributions are obtained from the full density of the pre-activation distribution by applying the sign function to the random variables . # # On the contributions of our paper First , we \u2019 d like to clarify that our paper is not only about binary neural networks , but also about probabilistc neural networks . Interestingly , by restricting ourselves to binary random variables , we get a closed-form forward pass . Although this has been explored before ( e.g. , Soudry et al . ( 2014 ) or Baldassi et al . ( 2018 ) ) , we also introduce stochastic versions of pooling and batch normalization . As a result , our method allows for more \u2018 modern \u2019 or complicated probabilistic architectures . Other than that , we summarize the contributions of our paper below : - We propose a method for training Binary Neural Networks without sampling or gradient approximations ( PBNet ) ; - We propose a training procedure for Binary Neural Networks that has various favorable properties . E.g. , the probabilistic formulation allows for any-time ensemble predictions , it allows for more complex network architectures than earlier work ( on probabilistic networks ) , and it is easily implemented in existing deep learning frameworks ; - We propose random/stochastic versions ( as summarized in the review ) of max-pooling and batch normalization , which are required in order to propagate distributions through the layer and/or network ( e.g. , our abblation results show that batch normalization is required and thus we need a stochastic version of batch normalization ) ; - We emprically show that sampling during training is crucial for obtaining a performant deterministic Binary Neural Network , but not required when one is interested in a stochastic binary neural network ( see the propagate columns in table 1 ) . # # On the use of the ( local ) reparametrization trick The local reparametrization trick , as introduced in ( Kingma et al. , 2015 ) , translates noises from global parameters to local parameters . Specifically , in ( Kingma et al. , 2015 ) , instead of sampling weights from the Gaussian weight distributions , the uncertainty ( or variance ) of the weights is propagated to the ( pre- ) activations . The pre-activations are subsequently sampled from the pre-activation distributions using the reparametrization trick . In ( Kingma et al. , 2015 ) , the weights are Gaussian and the activations are scalars , hence the pre-activations are Gaussian too , however , it is also possible to obtain Gaussian approximations to the pre-activation distributions by following the Lyapunov Central Limit Theorem . If we would simply follow the reparametrization trick , we would sample from this distribution at this point ( as was done by Shayer et al . ( 2018 ) ) .However , based on the observation that we can back-propagate through binarization ( or step-wise ) functions when applied to random variables ( if the CDF of the corresponding distribution is differentiable ) , we choose to apply the binarization function ( i.e. , non-linearity ) before sampling . In the case of PBNet-S we then sample the binary activation using the concrete relaxation of the Bernoulli distribution ( using the reparametrization trick ) . In the case of PBNet , we do not sample the activation but propagate the binary activation distribution to the next layer . As such , technically , PBNet does not make use of the local-reparametrization trick . We have updated the introduction to make this distinction more clear ."}, {"review_id": "B1fysiAqK7-2", "review_text": "## Summary This work presents a probabilistic training method for binary Neural Network with stochastic versions of Batch Normalization and max pooling. By sampling from the weight distribution an ensemble of Binary Neural Networks could further improve the performance. In the experimental section, the authors compare proposed PBNet with Binarized NN (Hubara et al., 2016) in two image datasets (MNIST and CIFAR10). In general, the paper was written in poor quality and without enough details. The idea behind the paper is not novel. Stochastic binarization and the (local) reparametrization trick were used to training binary (quantized) neural networks in previous works. The empirical results are not significant. ## Detail comments Issues with the training algorithm of stochastic neural network The authors did not give details of the training method and vaguely mentioned that the variational optimization framework (Staines & Barber, 2012). I do not understand equation 1. Since B is binary, the left part of equation 2 is a combination optimization problem. If B is sampled during the training, the gradient would suffer from high variance. Issues with propagating distributions throughout the network Equation 3 is based on the assumption of that the activations are random variables from Bernoulli distribution. In equation 4, the activations of the current layer become random variables from Gaussian distribution. How the activations to further propagate? Issues with ternary Neural Networks in section 2.4 For a ternary NN, the weight will be from a multinomial distribution, I think it will break the assumption used by equation 3. Issues with empirical evidences Since the activations are sampled in PBNET-S, a more appropriate baseline should be BNN with stochastic binarization (Hubara et al., 2016) which achieved 89.85% accuracy on CIFAR-10. It means that the proposed methods did not show any significant improvements. By the way BNN with stochastic binarization (Hubara et al., 2016) can also allow for ensemble predictions to improve performance. ", "rating": "3: Clear rejection", "reply_text": "Dear reviewer , We have tried to address the questions/remarks raised in the review . Moreover , we have updated the writing in the paper and hope the presentation is now easier to follow . Our response follows the structure of the original review such that it is easy to refer back to the original remarks . # On the general remarks We agree that the local reparametrization trick has been used before in order to train binary ( or quantized ) neural networks , however , we binarize both the weights and activations . Moreover , we propagate the activation distribution throughout the network/layer in order to backpropagate through binarization functions . By doing so , the gradient of the binarization function with respect to the parameters of the pre-activation distribution exists and can easily be computed using standard tools . Although our method doesn \u2019 t achieve better performance when compared to the Binarized Neural Networks by Hubara et al. , the performance is on par . For this reason , we also do not make claims about outperforming existing methods , however , we do argue that our stochastic training method has various favorable properties , i.e. , we obtain a distribution over binary network parameters that allow for any-time ensembles without retraining anything , it allows for more complex network architectures than earlier work ( on probabilistic networks ) , and it is easily implemented in existing deep learning frameworks . Moreover , the probabilistic approach allows for straightforward inclusion of priors on the weights and/or activations which can help to impose more structure on the Binary Neural Network ( e.g. , sparsity priors ) that can lead to even more efficient networks . # # Response to \u201c Issues with the training algorithm of stochastic neural network \u201d : We indeed didn \u2019 t elaborate in much detail on the training method because in many aspects our training method follows the standard approach in the current literature . Since we ensured that all the operations in the PBNet ( -S ) are differentiable ( w.r.t.the parameters of the input distributions for most of the operations ) , we can train the PBNet as any other network ( and thus leverage existing Deep Learning frameworks ) . For more specifics , see algorithm 1 , which outlines the forward pass for a single layer . # # Response to equation 1 being unclear Equation 1 states the upper bound on the training objective . Our actual objective is to obtain the binary weights that minimize the loss as states on the left-hand side . This is indeed a combinatorial problem . As such we make use of the variational optimization framework ( Staines & Barber , 2012 ) in order to obtain an upper bound on the training objective . I.e. , we introduce a distribution over the binary parameters of the network and instead of optimizing the binary parameters directly , we optimize the parameters of the binary distributions . As pointed out by the reviewer , optimizing this upper bound may result in high variance on the gradients , but we deal with this in the following way : - For PBNet , we never sample weights but instead propagate the variance throughout the network , i.e. , the forward pass is deterministic and we don \u2019 t suffer from high variance on the gradients - For PBNet-S , we leverage the local reparametrization trick , which is known to have lower gradient variance compared to simply sampling the weights during training . However , instead of sampling the ( pre- ) activations directly after computing the linear operation of a layer , we sample the activations at the very last operation of the layer . Moreover , the gradient variance is related to the variance of the weight distribution . For this reason , we initialize the parameters from a pre-trained network and specifically initialize the weight distribution q ( B ) to have low variance ( compared to a random initialization ) . Following this , we empirically find no issues with training these networks ."}], "0": {"review_id": "B1fysiAqK7-0", "review_text": "1. This paper presents a novel approach for training efficient binary networks with both binary weights and binary activations and verifies the capability of the proposed method on benchmark. In addition, related bach normalization and pooling layers are also improved. 2. How long do we need for training BNNs using the proposed method? 3. The paper only compares their performance with only one related work (Hubara et al., 2016), which is somewhat insufficient. 4. Another only concern is that whether the proposed method can be well applied on large scale datasets such as ImageNet.", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # On training time Figure 3c in our paper shows a plot of the validation loss . This plot shows that training on cifar-10 converges in approximately 50 epochs . Of course , during training we do have to perform two convolutions ( one for the mean and one for the variance of the Gaussian approximation ) , which may result in a longer wall clock time , if the hardware does not permit parallelization of these two operations . # # On the comparison to other methods We compare to Hubara et al. \u2019 s Binarized Neural Networks as it is the natural alternative to train Binary Neural Networks . The binary network that one obtains after training user our method is equivalent to the Binary Network obtained using the method by Hubara et al.Moreover , in both approaches no new computations are introduced ( such as , for example , in XNOR-net ( Rastegari et al. , 2016 ) or ABC-net ( Lin et al. , 2017 ) ) . For this reason , we believe it is fair to compare our method to that of Hubara et al. , however , we do agree that is may be interesting to explore the probabilistic counterparts of , for example , XNOR-Net and ABC-Net in future work . # On large scale datasets Although we agree that experiments on larger scale datasets are valuable , we also believe that the results currently presented in our paper are already valuable . We did perform some preliminary experiments on larger scale datasets and found evidence that we can scale to larger datasets , however , some careful tuning may be required , as is often the case for binary neural networks ."}, "1": {"review_id": "B1fysiAqK7-1", "review_text": "To reduce the deep neural networks' reliance on memory and high power consumption, the paper proposed a kind of probabilistic neural networks with both binary hidden node and binary weights. The paper presents a probabilistic way to binarize the activation and weight values. Also, it proposed a random version of batch-normalization and max-pooling. The binarization of hidden node is achieved through stochastic sampling according to the sign of the stochastic pre-activation values. The weight binarization is analogously done by sampling from a binary distribution. There is no too much new here, and is a standard way to obtain binary values probabilistically. The paper said in the introduction that the binary model will be trained with the re-parameterization trick, through either propagating the distributions or the samples from concrete distribution. But I am still not very clear how this training process is done, especially for the training of weight parameters. Overall, the problem investigated in this paper is very interesting and is of practical importance, the experimental results are preliminary but encouraging. But all the techniques used in this paper to binarize neural networks are standard, and no too much new here.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear reviewer , We want to clarify that we never sample from the binary weight distributions directly . We only sample the activations ( i.e. , the output of a layer ) in some of our experiments . Moreover , it is technically correct that the stochastic binary activations are related to the sign of the stochastic pre-activations , however , we \u2019 d like to stress that the stochastic binary activation distributions are obtained from the full density of the pre-activation distribution by applying the sign function to the random variables . # # On the contributions of our paper First , we \u2019 d like to clarify that our paper is not only about binary neural networks , but also about probabilistc neural networks . Interestingly , by restricting ourselves to binary random variables , we get a closed-form forward pass . Although this has been explored before ( e.g. , Soudry et al . ( 2014 ) or Baldassi et al . ( 2018 ) ) , we also introduce stochastic versions of pooling and batch normalization . As a result , our method allows for more \u2018 modern \u2019 or complicated probabilistic architectures . Other than that , we summarize the contributions of our paper below : - We propose a method for training Binary Neural Networks without sampling or gradient approximations ( PBNet ) ; - We propose a training procedure for Binary Neural Networks that has various favorable properties . E.g. , the probabilistic formulation allows for any-time ensemble predictions , it allows for more complex network architectures than earlier work ( on probabilistic networks ) , and it is easily implemented in existing deep learning frameworks ; - We propose random/stochastic versions ( as summarized in the review ) of max-pooling and batch normalization , which are required in order to propagate distributions through the layer and/or network ( e.g. , our abblation results show that batch normalization is required and thus we need a stochastic version of batch normalization ) ; - We emprically show that sampling during training is crucial for obtaining a performant deterministic Binary Neural Network , but not required when one is interested in a stochastic binary neural network ( see the propagate columns in table 1 ) . # # On the use of the ( local ) reparametrization trick The local reparametrization trick , as introduced in ( Kingma et al. , 2015 ) , translates noises from global parameters to local parameters . Specifically , in ( Kingma et al. , 2015 ) , instead of sampling weights from the Gaussian weight distributions , the uncertainty ( or variance ) of the weights is propagated to the ( pre- ) activations . The pre-activations are subsequently sampled from the pre-activation distributions using the reparametrization trick . In ( Kingma et al. , 2015 ) , the weights are Gaussian and the activations are scalars , hence the pre-activations are Gaussian too , however , it is also possible to obtain Gaussian approximations to the pre-activation distributions by following the Lyapunov Central Limit Theorem . If we would simply follow the reparametrization trick , we would sample from this distribution at this point ( as was done by Shayer et al . ( 2018 ) ) .However , based on the observation that we can back-propagate through binarization ( or step-wise ) functions when applied to random variables ( if the CDF of the corresponding distribution is differentiable ) , we choose to apply the binarization function ( i.e. , non-linearity ) before sampling . In the case of PBNet-S we then sample the binary activation using the concrete relaxation of the Bernoulli distribution ( using the reparametrization trick ) . In the case of PBNet , we do not sample the activation but propagate the binary activation distribution to the next layer . As such , technically , PBNet does not make use of the local-reparametrization trick . We have updated the introduction to make this distinction more clear ."}, "2": {"review_id": "B1fysiAqK7-2", "review_text": "## Summary This work presents a probabilistic training method for binary Neural Network with stochastic versions of Batch Normalization and max pooling. By sampling from the weight distribution an ensemble of Binary Neural Networks could further improve the performance. In the experimental section, the authors compare proposed PBNet with Binarized NN (Hubara et al., 2016) in two image datasets (MNIST and CIFAR10). In general, the paper was written in poor quality and without enough details. The idea behind the paper is not novel. Stochastic binarization and the (local) reparametrization trick were used to training binary (quantized) neural networks in previous works. The empirical results are not significant. ## Detail comments Issues with the training algorithm of stochastic neural network The authors did not give details of the training method and vaguely mentioned that the variational optimization framework (Staines & Barber, 2012). I do not understand equation 1. Since B is binary, the left part of equation 2 is a combination optimization problem. If B is sampled during the training, the gradient would suffer from high variance. Issues with propagating distributions throughout the network Equation 3 is based on the assumption of that the activations are random variables from Bernoulli distribution. In equation 4, the activations of the current layer become random variables from Gaussian distribution. How the activations to further propagate? Issues with ternary Neural Networks in section 2.4 For a ternary NN, the weight will be from a multinomial distribution, I think it will break the assumption used by equation 3. Issues with empirical evidences Since the activations are sampled in PBNET-S, a more appropriate baseline should be BNN with stochastic binarization (Hubara et al., 2016) which achieved 89.85% accuracy on CIFAR-10. It means that the proposed methods did not show any significant improvements. By the way BNN with stochastic binarization (Hubara et al., 2016) can also allow for ensemble predictions to improve performance. ", "rating": "3: Clear rejection", "reply_text": "Dear reviewer , We have tried to address the questions/remarks raised in the review . Moreover , we have updated the writing in the paper and hope the presentation is now easier to follow . Our response follows the structure of the original review such that it is easy to refer back to the original remarks . # On the general remarks We agree that the local reparametrization trick has been used before in order to train binary ( or quantized ) neural networks , however , we binarize both the weights and activations . Moreover , we propagate the activation distribution throughout the network/layer in order to backpropagate through binarization functions . By doing so , the gradient of the binarization function with respect to the parameters of the pre-activation distribution exists and can easily be computed using standard tools . Although our method doesn \u2019 t achieve better performance when compared to the Binarized Neural Networks by Hubara et al. , the performance is on par . For this reason , we also do not make claims about outperforming existing methods , however , we do argue that our stochastic training method has various favorable properties , i.e. , we obtain a distribution over binary network parameters that allow for any-time ensembles without retraining anything , it allows for more complex network architectures than earlier work ( on probabilistic networks ) , and it is easily implemented in existing deep learning frameworks . Moreover , the probabilistic approach allows for straightforward inclusion of priors on the weights and/or activations which can help to impose more structure on the Binary Neural Network ( e.g. , sparsity priors ) that can lead to even more efficient networks . # # Response to \u201c Issues with the training algorithm of stochastic neural network \u201d : We indeed didn \u2019 t elaborate in much detail on the training method because in many aspects our training method follows the standard approach in the current literature . Since we ensured that all the operations in the PBNet ( -S ) are differentiable ( w.r.t.the parameters of the input distributions for most of the operations ) , we can train the PBNet as any other network ( and thus leverage existing Deep Learning frameworks ) . For more specifics , see algorithm 1 , which outlines the forward pass for a single layer . # # Response to equation 1 being unclear Equation 1 states the upper bound on the training objective . Our actual objective is to obtain the binary weights that minimize the loss as states on the left-hand side . This is indeed a combinatorial problem . As such we make use of the variational optimization framework ( Staines & Barber , 2012 ) in order to obtain an upper bound on the training objective . I.e. , we introduce a distribution over the binary parameters of the network and instead of optimizing the binary parameters directly , we optimize the parameters of the binary distributions . As pointed out by the reviewer , optimizing this upper bound may result in high variance on the gradients , but we deal with this in the following way : - For PBNet , we never sample weights but instead propagate the variance throughout the network , i.e. , the forward pass is deterministic and we don \u2019 t suffer from high variance on the gradients - For PBNet-S , we leverage the local reparametrization trick , which is known to have lower gradient variance compared to simply sampling the weights during training . However , instead of sampling the ( pre- ) activations directly after computing the linear operation of a layer , we sample the activations at the very last operation of the layer . Moreover , the gradient variance is related to the variance of the weight distribution . For this reason , we initialize the parameters from a pre-trained network and specifically initialize the weight distribution q ( B ) to have low variance ( compared to a random initialization ) . Following this , we empirically find no issues with training these networks ."}}