{"year": "2018", "forum": "S1vuO-bCW", "title": "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "This paper is an easy accept -- three reviewers have above threshold scores, while one reviewer is slightly below threshold, but based on the submitted manuscript.  It appears that the paper has substantially improved based on reviewer comments.\n\nPros:\n\nAll reviews had positive sentiment: \"very elegant and general idea\" (Reviewer4); \"idea is interesting and potentially very useful\" (Reviewer2); \"method is novel, the explanation is clear, and has good experimental results\" (Reviewer3); \"a good way to learn a policy for resetting while learning a policy for solving the problem.  Seems like a fairly small but well considered and executed piece of work.\" (Reviewer1)\n\nCons:\n\nOne reviewer found that testing in only three artificial tasks was a limitation.\n\nThe initial reviews noted several issues where clarification of the text and/or figures was needed.  There were also a bunch of statements where the reviewers questioned the technical correctness / accuracy of the discussion.  Most of these points appear to have been adequately addressed in the revised manuscript.", "reviews": [{"review_id": "S1vuO-bCW-0", "review_text": "The paper solves the problem of how to do autonomous resets, which is an important problem in real world RL. The method is novel, the explanation is clear, and has good experimental results. Pros: 1. The approach is simple, solves a task of practical importance, and performs well in the experiments. 2. The experimental section performs good ablation studies wrt fewer reset thresholds, reset attempts, use of ensembles. Cons: 1. The method is evaluated only for 3 tasks, which are all in simulation, and on no real world tasks. Additional tasks could be useful, especially for qualitative analysis of the learned reset policies. 2. It seems that while the method does reduce hard resets, it would be more convincing if it can solve tasks which a model without a reset policy couldnt. Right now, the methods without the reset policy perform about equally well on final reward. 3. The method wont be applicable to RL environments where we will need to take multiple non-invertible actions to achieve the goal (an analogy would be multiple levels in a game). In such situations, one might want to use the reset policy to go back to intermediate \u201cstart\u201d states from where we can continue again, rather than the original start state always. Conclusion/Significance: The approach is a step in the right direction, and further refinements can make it a significant contribution to robotics work. Revision: Thanks to the authors for addressing the issues I raised, I revise my review to 7", "rating": "7: Good paper, accept", "reply_text": "We thank AnonReviewer3 for recognizing the importance of the problem we aim to solve , and for noting that our simple method is supported with \u201c good ablation studies. \u201d We have addressed the issues raised by the reviewer , as discussed below : 1 . We have run experiments on two additional environments ( ball in cup and peg insertion ) , so the revised version of the paper shows experiments on 5 simulated environments ( Section 6 , paragraph 1 ) . Videos of the learned policies visualize the learned forward + reset policies are available on the project website : https : //sites.google.com/site/mlleavenotrace/ Experimental evaluation of five distinct domains compares favorably to most RL papers that have appeared in ICLR in the past . While we agree that real-world evaluation of our method would be excellent , this is going substantially beyond the typical evaluation for ICLR RL work . 2.We ran additional environments that show that , in certain difficult situations , our method can solve tasks which a model without a reset policy can not . Newly-added Sections 6.1 and 6.6 demonstrate this result in two settings . We summarize these results below in a separate post entitled \u201c Additional Experiments. \u201d 3 . We expanded Section 4 paragraph 2 to clarify our assumption that there exists a reversible goal state . This is indeed a limitation of our approach , which we note in the paper ( Section 4 paragraph 2 ) . We show experimentally that our method can be applied to a number of realistic tasks , such as locomotion and manipulation . Extending our work to tasks with irreversible goal states by resetting to intermediate goals is a great idea , and would make for interesting future work ."}, {"review_id": "S1vuO-bCW-1", "review_text": "If one is committed to doing value-function or policy-based RL for an episodic task on a real physical system, then one has to come up with a way of resetting the domain for new trials. This paper proposes a good way of doing this: learn a policy for resetting at the same time as learning a policy for solving the problem. As a side effect, the Q values associated with the reset policy can be used to predict when the system is about to enter an unrecoverable state and \"forbid\" the action. It is, of course, necessary that the domain be, in fact, reversible (or, at least, that it be possible to reach a starting state from at least one goal state--and it's better if that goal state is not significantly harder to reach than other goal states. There were a couple of places in the paper that seemed to be to be not strictly technically correct. It says that the reset policy is designed to achieve a distribution of final states that is equivalent to a starting distribution on the problem. This is technically fairly difficult, as a problem, and I don't think it can be achieved through standard RL methods. Later, it is clearer that there is a set of possible start states and they are all treated as goal states from the perspective of the reset policy. That is a start set, not a distribution. And, there's no particular reason to think that the reset policy will not, for example, always end up returning to a particular state. Another point is that training a set of Q functions from different starting states generates some kind of an ensemble, but I don't think you can guarantee much about what sort of a distribution on values it will really represent. Q learning + function approximation can go wrong in a variety of ways, and so some of these values might be really gross over or under estimates of what can be achieved even by the policies associated with those values. A final, higher-level, methodological concern is that, it seems to me, as the domains become more complex, rather than trying to learn two (or more) policies, it might be more effective to take a model-based approach, learn one model, and do reasoning to decide how to return home (and even to select from a distribution of start states) and/or to decide if a step is likely to remove the robot from the \"resettable\" space. All this aside, this seems like a fairly small but well considered and executed piece of work. I'm rating it as marginally above threshold, but I indeed find it very close to the threshold.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank AnonReviewer1 for noting the main goal of our paper and recognizing how we incorporate safety using the learned reset policy , and further thank the reviewer for finding our paper a \u201c well considered and executed piece of work. \u201d We \u2019 ve addressed the concerns raised by the reviewer with clarifications in the main text , which we detail below . Assumption that environment is reversible - This assumption is indeed a limitation of our approach , which we note in the paper ( Section 4 paragraph 2 ) . We have expanded this section to clarify this detail . We show experimentally that our method can be applied to a number of realistic tasks , such as locomotion and manipulation . Extending our work to tasks with irreversible goal states is a great idea , and would make for interesting future work . Initial state distribution - You correctly note that the reset policy might always reset to the same state , thus failing to sample from the full initial state distribution . We have corrected this technical error in the revised version of the paper ( Section 4 , paragraph 2 ) by adding the additional assumption that the initial state distribution be unimodal and have narrow support . We also expanded the discussion of \u201c safe sets \u201d in Section 4.2 paragraph 1 to clarify the difference between the initial state distribution , the reset policy \u2019 s reward , and the safe set . We also describe a method to detect if there is mismatch between the the initial state distribution and the reset policy final state distribution . Q functions - We learn an ensemble of Q functions , each of which is a sampled from the posterior distribution over Q functions given the observed data . We expanded Section 4.4 paragraph 1 to note how this technique has been established in previous work ( \u201c Deep Exploration via Bootstrapped DQN \u201d [ Osband 2016 ] and \u201c UCB Exploration via Q-Ensembles \u201d [ Chen 2017 ] ) . In general , we are not guaranteed that samples from a distribution are close to its mean . However , our experiments on ensemble aggregation ( taking the min , mean or max over the Q functions ) had little effect on policy reward . If \u201c gross under/over-estimation \u201d had occurred , taking the min/max over the ensemble would have resulted in markedly lower reward . We expanded Appendix A paragraph 2 to explain this finding . Model-based alternative - We appreciate the comment regarding a potential model-based alternative to our method . However , we are not aware of any past model-based methods for solving this task . We would be happy to attempt a comparison or add a discussion if the reviewer has a particular prior method in mind . The early aborts in our method provide one method of identifying irreversible states . A model-based alternative could also serve this function . We believe that our early aborts , which only require learning a single reset policy , are simpler than learning a model of the environment dynamics and hypothesize that our approach will scale better to complex environments ."}, {"review_id": "S1vuO-bCW-2", "review_text": "(This delayed review is based on the deadline version of the paper.) This paper proposes to learn by RL a reset policy at the same time that we learn the forward policy, and use the learned reset Q-function to predict and avoid actions that would prevent reset \u2014 an indication that they are \"unsafe\" in some sense. This idea (both parts) is interesting and potentially very useful, particularly in physical domains where reset is expensive and exploration is risky. While I'm sure the community can benefit from ideas of this kind, it really needs clearer presentations of such ideas. I can appreciate the very intuitive and colloquial style of the paper, however the discussion of the core idea would benefit from some rigor and formal definitions. Examples of intuitive language that could be hiding the necessary complexities of a more formal treatment: 1. In the penultimate paragraph of Section 1, actions are described as \"reversible\", while a stochastic environment may be lacking such a notion altogether (i.e. there's no clear inverse if state transitions are not deterministic functions). 2. It's not clear whether the authors suggest that the ability to reset is a good notion of safety, or just a proxy to such a notion. This should be made more explicit, making it clearer what this proxy misses: states where the learned reset policy fails (whether due to limited controllability or errors in the policy), that are nonetheless safe. 3. In the last paragraph of Section 3, a reset policy is defined as reaching p_0 from *any* state. This is a very strong requirement, which isn't even satisfiable in most domains, and indeed the reset policies learned in the rest of the paper don't satisfy it. 4. What are p_0 and r_r in the experiments? What is the relation between S_{reset} and p_0? Is there a discount factor? 5. In the first paragraph of Section 4.1, states are described as \"irreversible\" or \"irrecoverable\". Again, in a stochastic environment a more nuanced notion is needed, as there may be policies that take a long time to reset from some states, but do so eventually. 6. A definition of a \"hard\" reset would make the paper clearer. 7. After (1), states are described as \"allowed\". Again, preventing actions that are likely to hinder reset cannot completely prevent any given state in a stochastic environment. It also seems that (2) describes states where some allowed action can be taken, rather than states reachable by some allowed action. For both reasons, Algorithm 1 does not prevent reaching states outside S*, so what is the point of that definition? 8. The paper is not explicit about the learning dynamics of the reset policy. It should include a figure showing the learning curve of this policy (or some other visualization), and explain how the reset policy can ever gain experience and learn to reset from states that it initially avoids as unsafe. 9. Algorithm 1 is unclear on how a failed reset is identified, and what happens in such case \u2014 do we run another forward episode? Another reset episode?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the comments ! It seems that all the concerns have to do with the writing in the paper and are straightforward to fix . We have addressed all the concerns raised about the paper in this review . Given that all issues have been addressed , we would appreciate if the reviewer could take another look at the paper . 1.We have clarified our definition of reversible action in Section 1 paragraph 4 . For deterministic MDPs , we say an action is reversible if it leads to a state from which there exists a reset policy that can return to a state with high density under the initial state distribution . For stochastic MDPs , we say an action is reversible if the probability that an oracle reset policy that can reset from the next state is greater than some safety threshold . Note that definition for deterministic MDPs is a special case of the definition for stochastic MDPs . 2.The ability of an oracle reset policy to reset is a good notion of safety . In our algorithm , we approximate this notion of safety , assuming that whether our learned reset policy can reset in N episodes is a good proxy for whether an oracle reset policy can reset . We have clarified Section 1 paragraph 4 to make this distinction clear . We also added Appendix B to discuss handling errors in Q value estimation . In this section , we describe how Leave No Trace copes with overestimates and underestimates of Q values . 3.We have corrected this technical error in Section 3 paragraph 2 by redefining the reset policy as being able to reach p_0 from any state reached by the forward policy . That our learned reset policy only learns to reset from states reached by the forward policy is indeed a limitation of our method . However , note that early aborts help the forward policy avoid visiting states from which the reset policy is unable to reach p_0 . 4.For the continuous control environments , the initial state distribution p_0 is uniform distribution centered at a \u201c start pose. \u201d We use a discount factor \\gamma = 0.99 . Both details have been noted in Appendix F.3 paragraph 2 . The reset reward r_r is a hand-crafted approximation to p_0 . For example , in the Ball in Cup environment , r_r is proportional to the negative L2 distance from the ball to the origin ( below the cup ) . For cliff cheetah , r_r includes one term that is proportional to the distance of the cheetah to the origin , and another term indicating whether the cheetah is standing . S_ { reset } is the set of states where r_r ( s ) is greater than 0.7 ( Appendix C.3 paragraph 2 ) 5 . We have clarified Section 4.1 paragraph 1 to explain how our proposed algorithm handles both cases : states from which it is impossible to reset and states from which resetting would take prohibitively many steps . In both cases , the cumulative discounted reward ( and hence the value function ) will be low . By performing an early abort when the value function is low , we avoid both cases . 6.We added a definition of \u201c hard reset \u201d to Section 4.2 paragraph 1 : A hard reset is an action that resamples that state from the initial state distribution . Hard resets are available to an external agent ( e.g.a human ) but not the learned agent . 7.We acknowledge that the proposed algorithm does not guarantee that we never visit unsafe states . In Appendix A , we have added a proof that Leave No Trace would only visit states that are safe in expectation if it had access to the true Q values . Appendix A.3 discusses the approximations we make in practice that can cause Leave No Trace to visit unsafe states . Finally , Appendix B discusses how Leave No Trace handles errors incurred by over/under-estimates of Q values . 8.Newly added Appendix D visualizes the training dynamics by plotting the number of time steps in each episode before an early abort . Initially , early aborts occur near the initial state distribution , so the forward episode lengths are quite short . As the reset policy improves , early aborts occur further from the initial state , as indicated by longer forward episode lengths . Newly added Appendix B discusses how Leave No Trace handles errors incurred by over/under-estimates of Q values . It describes how Leave No Trace learns that an \u201c unsafe \u201d state is actually safe . 9.We detect failed resets in line 12 of Algorithm 1 . We have added a comment to help clarify this . When a failed reset is detected , a hard reset occurs ( line 13 ) ."}, {"review_id": "S1vuO-bCW-3", "review_text": "This paper proposes the idea of having an agent learning a policy that resets the agent's state to one of the states drawn from the distribution of starting states. The agent learns such policy while also learning how to solve the actual task. This approach generates more autonomous agents that require fewer human interventions in the learning process. This is a very elegant and general idea, where the value function learned in the reset task also encodes some measure of safety in the environment. All that being said, I gave this paper a score of 6 because two aspects that seem fundamental to me are not clear in the paper. If clarified, I'd happily increase my score. 1) *Defining state visitation/equality in the function approximation setting:* The main idea behind the proposed algorithm is to ensure that \"when the reset policy is executed from any state, the distribution over final states matches the initial state distribution p_0\". This is formally described, for example, in line 13 of Algorithm 1. The authors \"define a set of safe states S_{reset} \\subseteq S, and say that we are in an irreversible state if the set of states visited by the reset policy over the past N episodes is disjoint from S_{reset}.\" However, it is not clear to me how one can uniquely identify a state in the function approximation case. Obviously, it is straightforward to apply such definition in the tabular case, where counting state visitation is easy. However, how do we count state visitation in continuous domains? Did the authors manually define the range of each joint/torque/angle that characterizes the start state? In a control task from pixels, for example, would the exact configuration of pixels seen at the beginning be the start state? Defining state visitation in the function approximation setting is not trivial and it seems to me the authors just glossed over it, despite being essential to your work. 2) *Experimental design for Figure 5*: This setup is not clear to me at all and in fact, my first reaction is to say it is wrong. An episodic task is generally defined as: the agent starts in a state drawn from the distribution of starting states and at the moment it reaches the goal state, the task is reset and the agent starts again. It doesn't seem to be what the authors did, is that right? The sentence: \"our method learns to solve this task by automatically resetting the environment after each episode, so the forward policy can practice catching the ball when initialized below the cup\" is confusion. When is the task reset to the \"status quo\" approach? Also, let's say an agent takes 50 time steps to reach the goal and then it decides to do a soft-reset. Are the time steps it is spending on its soft-reset being taken into account when generating the reported results? Some other minor points are: - The authors should standardize their use of citations in the paper. Sometimes there are way too many parentheses in a reference. For example: \"manual resets are necessary when the robot or environment breaks (e.g. Gandhi et al. (2017))\", or \"Our methods can also be used directly with any other Q-learning methods ((Watkins & Dayan, 1992; Mnih et al., 2013; Gu et al., 2017; Amos et al., 2016; Metz et al., 2017))\" - There is a whole line of work in safe RL that is not acknowledged in the related work section. Representative papers are: [1] Philip S. Thomas, Georgios Theocharous, Mohammad Ghavamzadeh: High-Confidence Off-Policy Evaluation. AAAI 2015: 3000-3006 [2] Philip S. Thomas, Georgios Theocharous, Mohammad Ghavamzadeh: High Confidence Policy Improvement. ICML 2015: 2380-2388 - In the Preliminaries Section the next state is said to be drawn from s_{t+1} ~ P(s'| s, a). However, this hides the fact the next state is dependent on the environment dynamics and on the policy being followed. I think it would be clearer if written: s_{t+1} ~ P(s'| s, \\pi(a|s)). - It seems to me that, in Algorithm 1, the name 'Act' is misleading. Shouldn't it be 'ChooseAction' or 'EpsilonGreedy'? If I understand correctly, the function 'Act' just returns the action to be executed, while the function 'Step' is the one that actually executes the action. - It is absolutely essential to depict the confidence intervals in the plots in Figure 3. Ideally we should have confidence intervals in all the plots in the paper.", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 4 for the comments and for finding our paper a \u201c very elegant and general idea. \u201d The main comments had to do with clarity - we have addressed these in the revised version . We would appreciate of the reviewer would reevaluate the paper given the new clarifications . 1 . ( State visitation ) We implement our algorithm in a way that avoids having to test whether two states are equal ( indeed , a challenging problem ) . In Equation 4 , we define the set of safe states S_ { reset } implicitly using the reset reward function r_r ( s ) . In particular , we say a state is safe if the the reset reward is greater than some threshold ( 0.7 in our experiments ) . For example , in the pusher task , the reset reward is the distance from a certain ( x , y , z ) point , so S_ { reset } is the set of points within some distance of this point . We added a comment to line 13 of the algorithm clarifying how we test if a state is in S_ { reset } . 2 . ( Figure 5 ) We clarified our description of the environments in Section 6 paragraph 1 to note that the episode is not terminated when the agent reaches a goal state . For the experiment in Figure 5 , the \u2018 forward-only \u2019 baseline and our method are non-episodic - the environment is never reset and no hard resets are used ( Section 6.1 ) . The \u2018 status quo \u2019 baseline is episodic , doing a hard reset every T time steps ( for ball in cup , T = 200 steps ) . We reworded the confusing sentence about our method as follows : \u201c In contrast , our method learns to solve this task by automatically resetting the environment after each attempt , so the forward policy can practice catching the ball without hard resets. \u201d All results in the paper include time steps for both the forward task and the reset task . We clarified this in Section 6.1 paragraph 1 . This highlights a strength of our method : even though the agent spends a considerable amount of time learning the reset task , it still learns to do the forward task in roughly the same number of total steps ( steps for forward task + steps for reset task ) . Minor points : 1 . Citations . We \u2019 ve removed the extra parenthesis for the Q-learning references . Generally , we use \u201c e.g. \u201d in citations when the citation is an example of the described behavior . 2.Thanks for the additional references ! We \u2019 ve included them in Section 2 paragraph 1 . 3.We chose to separate out the policy from the transition dynamics . Action a_ { t } is sampled from \\pi ( a_ { t } | s_ { t } ) and depends on the policy ; next state s_ { t } is sampled from P ( s_ { t+1 } | s_ { t } , a_ { t } ) and depends on the transition dynamics . 4.Good idea . We \u2019 ve changed \u201c Act ( ) \u201d to \u201c ChooseAction ( ) \u201d in Algorithm 1 . 5.For Figure 3 , we agree confidence intervals would be helpful . We can \u2019 t regenerate the plot in the next 24 hours before the rebuttal deadline , but will include confidence intervals in the camera-ready version ."}], "0": {"review_id": "S1vuO-bCW-0", "review_text": "The paper solves the problem of how to do autonomous resets, which is an important problem in real world RL. The method is novel, the explanation is clear, and has good experimental results. Pros: 1. The approach is simple, solves a task of practical importance, and performs well in the experiments. 2. The experimental section performs good ablation studies wrt fewer reset thresholds, reset attempts, use of ensembles. Cons: 1. The method is evaluated only for 3 tasks, which are all in simulation, and on no real world tasks. Additional tasks could be useful, especially for qualitative analysis of the learned reset policies. 2. It seems that while the method does reduce hard resets, it would be more convincing if it can solve tasks which a model without a reset policy couldnt. Right now, the methods without the reset policy perform about equally well on final reward. 3. The method wont be applicable to RL environments where we will need to take multiple non-invertible actions to achieve the goal (an analogy would be multiple levels in a game). In such situations, one might want to use the reset policy to go back to intermediate \u201cstart\u201d states from where we can continue again, rather than the original start state always. Conclusion/Significance: The approach is a step in the right direction, and further refinements can make it a significant contribution to robotics work. Revision: Thanks to the authors for addressing the issues I raised, I revise my review to 7", "rating": "7: Good paper, accept", "reply_text": "We thank AnonReviewer3 for recognizing the importance of the problem we aim to solve , and for noting that our simple method is supported with \u201c good ablation studies. \u201d We have addressed the issues raised by the reviewer , as discussed below : 1 . We have run experiments on two additional environments ( ball in cup and peg insertion ) , so the revised version of the paper shows experiments on 5 simulated environments ( Section 6 , paragraph 1 ) . Videos of the learned policies visualize the learned forward + reset policies are available on the project website : https : //sites.google.com/site/mlleavenotrace/ Experimental evaluation of five distinct domains compares favorably to most RL papers that have appeared in ICLR in the past . While we agree that real-world evaluation of our method would be excellent , this is going substantially beyond the typical evaluation for ICLR RL work . 2.We ran additional environments that show that , in certain difficult situations , our method can solve tasks which a model without a reset policy can not . Newly-added Sections 6.1 and 6.6 demonstrate this result in two settings . We summarize these results below in a separate post entitled \u201c Additional Experiments. \u201d 3 . We expanded Section 4 paragraph 2 to clarify our assumption that there exists a reversible goal state . This is indeed a limitation of our approach , which we note in the paper ( Section 4 paragraph 2 ) . We show experimentally that our method can be applied to a number of realistic tasks , such as locomotion and manipulation . Extending our work to tasks with irreversible goal states by resetting to intermediate goals is a great idea , and would make for interesting future work ."}, "1": {"review_id": "S1vuO-bCW-1", "review_text": "If one is committed to doing value-function or policy-based RL for an episodic task on a real physical system, then one has to come up with a way of resetting the domain for new trials. This paper proposes a good way of doing this: learn a policy for resetting at the same time as learning a policy for solving the problem. As a side effect, the Q values associated with the reset policy can be used to predict when the system is about to enter an unrecoverable state and \"forbid\" the action. It is, of course, necessary that the domain be, in fact, reversible (or, at least, that it be possible to reach a starting state from at least one goal state--and it's better if that goal state is not significantly harder to reach than other goal states. There were a couple of places in the paper that seemed to be to be not strictly technically correct. It says that the reset policy is designed to achieve a distribution of final states that is equivalent to a starting distribution on the problem. This is technically fairly difficult, as a problem, and I don't think it can be achieved through standard RL methods. Later, it is clearer that there is a set of possible start states and they are all treated as goal states from the perspective of the reset policy. That is a start set, not a distribution. And, there's no particular reason to think that the reset policy will not, for example, always end up returning to a particular state. Another point is that training a set of Q functions from different starting states generates some kind of an ensemble, but I don't think you can guarantee much about what sort of a distribution on values it will really represent. Q learning + function approximation can go wrong in a variety of ways, and so some of these values might be really gross over or under estimates of what can be achieved even by the policies associated with those values. A final, higher-level, methodological concern is that, it seems to me, as the domains become more complex, rather than trying to learn two (or more) policies, it might be more effective to take a model-based approach, learn one model, and do reasoning to decide how to return home (and even to select from a distribution of start states) and/or to decide if a step is likely to remove the robot from the \"resettable\" space. All this aside, this seems like a fairly small but well considered and executed piece of work. I'm rating it as marginally above threshold, but I indeed find it very close to the threshold.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank AnonReviewer1 for noting the main goal of our paper and recognizing how we incorporate safety using the learned reset policy , and further thank the reviewer for finding our paper a \u201c well considered and executed piece of work. \u201d We \u2019 ve addressed the concerns raised by the reviewer with clarifications in the main text , which we detail below . Assumption that environment is reversible - This assumption is indeed a limitation of our approach , which we note in the paper ( Section 4 paragraph 2 ) . We have expanded this section to clarify this detail . We show experimentally that our method can be applied to a number of realistic tasks , such as locomotion and manipulation . Extending our work to tasks with irreversible goal states is a great idea , and would make for interesting future work . Initial state distribution - You correctly note that the reset policy might always reset to the same state , thus failing to sample from the full initial state distribution . We have corrected this technical error in the revised version of the paper ( Section 4 , paragraph 2 ) by adding the additional assumption that the initial state distribution be unimodal and have narrow support . We also expanded the discussion of \u201c safe sets \u201d in Section 4.2 paragraph 1 to clarify the difference between the initial state distribution , the reset policy \u2019 s reward , and the safe set . We also describe a method to detect if there is mismatch between the the initial state distribution and the reset policy final state distribution . Q functions - We learn an ensemble of Q functions , each of which is a sampled from the posterior distribution over Q functions given the observed data . We expanded Section 4.4 paragraph 1 to note how this technique has been established in previous work ( \u201c Deep Exploration via Bootstrapped DQN \u201d [ Osband 2016 ] and \u201c UCB Exploration via Q-Ensembles \u201d [ Chen 2017 ] ) . In general , we are not guaranteed that samples from a distribution are close to its mean . However , our experiments on ensemble aggregation ( taking the min , mean or max over the Q functions ) had little effect on policy reward . If \u201c gross under/over-estimation \u201d had occurred , taking the min/max over the ensemble would have resulted in markedly lower reward . We expanded Appendix A paragraph 2 to explain this finding . Model-based alternative - We appreciate the comment regarding a potential model-based alternative to our method . However , we are not aware of any past model-based methods for solving this task . We would be happy to attempt a comparison or add a discussion if the reviewer has a particular prior method in mind . The early aborts in our method provide one method of identifying irreversible states . A model-based alternative could also serve this function . We believe that our early aborts , which only require learning a single reset policy , are simpler than learning a model of the environment dynamics and hypothesize that our approach will scale better to complex environments ."}, "2": {"review_id": "S1vuO-bCW-2", "review_text": "(This delayed review is based on the deadline version of the paper.) This paper proposes to learn by RL a reset policy at the same time that we learn the forward policy, and use the learned reset Q-function to predict and avoid actions that would prevent reset \u2014 an indication that they are \"unsafe\" in some sense. This idea (both parts) is interesting and potentially very useful, particularly in physical domains where reset is expensive and exploration is risky. While I'm sure the community can benefit from ideas of this kind, it really needs clearer presentations of such ideas. I can appreciate the very intuitive and colloquial style of the paper, however the discussion of the core idea would benefit from some rigor and formal definitions. Examples of intuitive language that could be hiding the necessary complexities of a more formal treatment: 1. In the penultimate paragraph of Section 1, actions are described as \"reversible\", while a stochastic environment may be lacking such a notion altogether (i.e. there's no clear inverse if state transitions are not deterministic functions). 2. It's not clear whether the authors suggest that the ability to reset is a good notion of safety, or just a proxy to such a notion. This should be made more explicit, making it clearer what this proxy misses: states where the learned reset policy fails (whether due to limited controllability or errors in the policy), that are nonetheless safe. 3. In the last paragraph of Section 3, a reset policy is defined as reaching p_0 from *any* state. This is a very strong requirement, which isn't even satisfiable in most domains, and indeed the reset policies learned in the rest of the paper don't satisfy it. 4. What are p_0 and r_r in the experiments? What is the relation between S_{reset} and p_0? Is there a discount factor? 5. In the first paragraph of Section 4.1, states are described as \"irreversible\" or \"irrecoverable\". Again, in a stochastic environment a more nuanced notion is needed, as there may be policies that take a long time to reset from some states, but do so eventually. 6. A definition of a \"hard\" reset would make the paper clearer. 7. After (1), states are described as \"allowed\". Again, preventing actions that are likely to hinder reset cannot completely prevent any given state in a stochastic environment. It also seems that (2) describes states where some allowed action can be taken, rather than states reachable by some allowed action. For both reasons, Algorithm 1 does not prevent reaching states outside S*, so what is the point of that definition? 8. The paper is not explicit about the learning dynamics of the reset policy. It should include a figure showing the learning curve of this policy (or some other visualization), and explain how the reset policy can ever gain experience and learn to reset from states that it initially avoids as unsafe. 9. Algorithm 1 is unclear on how a failed reset is identified, and what happens in such case \u2014 do we run another forward episode? Another reset episode?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the comments ! It seems that all the concerns have to do with the writing in the paper and are straightforward to fix . We have addressed all the concerns raised about the paper in this review . Given that all issues have been addressed , we would appreciate if the reviewer could take another look at the paper . 1.We have clarified our definition of reversible action in Section 1 paragraph 4 . For deterministic MDPs , we say an action is reversible if it leads to a state from which there exists a reset policy that can return to a state with high density under the initial state distribution . For stochastic MDPs , we say an action is reversible if the probability that an oracle reset policy that can reset from the next state is greater than some safety threshold . Note that definition for deterministic MDPs is a special case of the definition for stochastic MDPs . 2.The ability of an oracle reset policy to reset is a good notion of safety . In our algorithm , we approximate this notion of safety , assuming that whether our learned reset policy can reset in N episodes is a good proxy for whether an oracle reset policy can reset . We have clarified Section 1 paragraph 4 to make this distinction clear . We also added Appendix B to discuss handling errors in Q value estimation . In this section , we describe how Leave No Trace copes with overestimates and underestimates of Q values . 3.We have corrected this technical error in Section 3 paragraph 2 by redefining the reset policy as being able to reach p_0 from any state reached by the forward policy . That our learned reset policy only learns to reset from states reached by the forward policy is indeed a limitation of our method . However , note that early aborts help the forward policy avoid visiting states from which the reset policy is unable to reach p_0 . 4.For the continuous control environments , the initial state distribution p_0 is uniform distribution centered at a \u201c start pose. \u201d We use a discount factor \\gamma = 0.99 . Both details have been noted in Appendix F.3 paragraph 2 . The reset reward r_r is a hand-crafted approximation to p_0 . For example , in the Ball in Cup environment , r_r is proportional to the negative L2 distance from the ball to the origin ( below the cup ) . For cliff cheetah , r_r includes one term that is proportional to the distance of the cheetah to the origin , and another term indicating whether the cheetah is standing . S_ { reset } is the set of states where r_r ( s ) is greater than 0.7 ( Appendix C.3 paragraph 2 ) 5 . We have clarified Section 4.1 paragraph 1 to explain how our proposed algorithm handles both cases : states from which it is impossible to reset and states from which resetting would take prohibitively many steps . In both cases , the cumulative discounted reward ( and hence the value function ) will be low . By performing an early abort when the value function is low , we avoid both cases . 6.We added a definition of \u201c hard reset \u201d to Section 4.2 paragraph 1 : A hard reset is an action that resamples that state from the initial state distribution . Hard resets are available to an external agent ( e.g.a human ) but not the learned agent . 7.We acknowledge that the proposed algorithm does not guarantee that we never visit unsafe states . In Appendix A , we have added a proof that Leave No Trace would only visit states that are safe in expectation if it had access to the true Q values . Appendix A.3 discusses the approximations we make in practice that can cause Leave No Trace to visit unsafe states . Finally , Appendix B discusses how Leave No Trace handles errors incurred by over/under-estimates of Q values . 8.Newly added Appendix D visualizes the training dynamics by plotting the number of time steps in each episode before an early abort . Initially , early aborts occur near the initial state distribution , so the forward episode lengths are quite short . As the reset policy improves , early aborts occur further from the initial state , as indicated by longer forward episode lengths . Newly added Appendix B discusses how Leave No Trace handles errors incurred by over/under-estimates of Q values . It describes how Leave No Trace learns that an \u201c unsafe \u201d state is actually safe . 9.We detect failed resets in line 12 of Algorithm 1 . We have added a comment to help clarify this . When a failed reset is detected , a hard reset occurs ( line 13 ) ."}, "3": {"review_id": "S1vuO-bCW-3", "review_text": "This paper proposes the idea of having an agent learning a policy that resets the agent's state to one of the states drawn from the distribution of starting states. The agent learns such policy while also learning how to solve the actual task. This approach generates more autonomous agents that require fewer human interventions in the learning process. This is a very elegant and general idea, where the value function learned in the reset task also encodes some measure of safety in the environment. All that being said, I gave this paper a score of 6 because two aspects that seem fundamental to me are not clear in the paper. If clarified, I'd happily increase my score. 1) *Defining state visitation/equality in the function approximation setting:* The main idea behind the proposed algorithm is to ensure that \"when the reset policy is executed from any state, the distribution over final states matches the initial state distribution p_0\". This is formally described, for example, in line 13 of Algorithm 1. The authors \"define a set of safe states S_{reset} \\subseteq S, and say that we are in an irreversible state if the set of states visited by the reset policy over the past N episodes is disjoint from S_{reset}.\" However, it is not clear to me how one can uniquely identify a state in the function approximation case. Obviously, it is straightforward to apply such definition in the tabular case, where counting state visitation is easy. However, how do we count state visitation in continuous domains? Did the authors manually define the range of each joint/torque/angle that characterizes the start state? In a control task from pixels, for example, would the exact configuration of pixels seen at the beginning be the start state? Defining state visitation in the function approximation setting is not trivial and it seems to me the authors just glossed over it, despite being essential to your work. 2) *Experimental design for Figure 5*: This setup is not clear to me at all and in fact, my first reaction is to say it is wrong. An episodic task is generally defined as: the agent starts in a state drawn from the distribution of starting states and at the moment it reaches the goal state, the task is reset and the agent starts again. It doesn't seem to be what the authors did, is that right? The sentence: \"our method learns to solve this task by automatically resetting the environment after each episode, so the forward policy can practice catching the ball when initialized below the cup\" is confusion. When is the task reset to the \"status quo\" approach? Also, let's say an agent takes 50 time steps to reach the goal and then it decides to do a soft-reset. Are the time steps it is spending on its soft-reset being taken into account when generating the reported results? Some other minor points are: - The authors should standardize their use of citations in the paper. Sometimes there are way too many parentheses in a reference. For example: \"manual resets are necessary when the robot or environment breaks (e.g. Gandhi et al. (2017))\", or \"Our methods can also be used directly with any other Q-learning methods ((Watkins & Dayan, 1992; Mnih et al., 2013; Gu et al., 2017; Amos et al., 2016; Metz et al., 2017))\" - There is a whole line of work in safe RL that is not acknowledged in the related work section. Representative papers are: [1] Philip S. Thomas, Georgios Theocharous, Mohammad Ghavamzadeh: High-Confidence Off-Policy Evaluation. AAAI 2015: 3000-3006 [2] Philip S. Thomas, Georgios Theocharous, Mohammad Ghavamzadeh: High Confidence Policy Improvement. ICML 2015: 2380-2388 - In the Preliminaries Section the next state is said to be drawn from s_{t+1} ~ P(s'| s, a). However, this hides the fact the next state is dependent on the environment dynamics and on the policy being followed. I think it would be clearer if written: s_{t+1} ~ P(s'| s, \\pi(a|s)). - It seems to me that, in Algorithm 1, the name 'Act' is misleading. Shouldn't it be 'ChooseAction' or 'EpsilonGreedy'? If I understand correctly, the function 'Act' just returns the action to be executed, while the function 'Step' is the one that actually executes the action. - It is absolutely essential to depict the confidence intervals in the plots in Figure 3. Ideally we should have confidence intervals in all the plots in the paper.", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 4 for the comments and for finding our paper a \u201c very elegant and general idea. \u201d The main comments had to do with clarity - we have addressed these in the revised version . We would appreciate of the reviewer would reevaluate the paper given the new clarifications . 1 . ( State visitation ) We implement our algorithm in a way that avoids having to test whether two states are equal ( indeed , a challenging problem ) . In Equation 4 , we define the set of safe states S_ { reset } implicitly using the reset reward function r_r ( s ) . In particular , we say a state is safe if the the reset reward is greater than some threshold ( 0.7 in our experiments ) . For example , in the pusher task , the reset reward is the distance from a certain ( x , y , z ) point , so S_ { reset } is the set of points within some distance of this point . We added a comment to line 13 of the algorithm clarifying how we test if a state is in S_ { reset } . 2 . ( Figure 5 ) We clarified our description of the environments in Section 6 paragraph 1 to note that the episode is not terminated when the agent reaches a goal state . For the experiment in Figure 5 , the \u2018 forward-only \u2019 baseline and our method are non-episodic - the environment is never reset and no hard resets are used ( Section 6.1 ) . The \u2018 status quo \u2019 baseline is episodic , doing a hard reset every T time steps ( for ball in cup , T = 200 steps ) . We reworded the confusing sentence about our method as follows : \u201c In contrast , our method learns to solve this task by automatically resetting the environment after each attempt , so the forward policy can practice catching the ball without hard resets. \u201d All results in the paper include time steps for both the forward task and the reset task . We clarified this in Section 6.1 paragraph 1 . This highlights a strength of our method : even though the agent spends a considerable amount of time learning the reset task , it still learns to do the forward task in roughly the same number of total steps ( steps for forward task + steps for reset task ) . Minor points : 1 . Citations . We \u2019 ve removed the extra parenthesis for the Q-learning references . Generally , we use \u201c e.g. \u201d in citations when the citation is an example of the described behavior . 2.Thanks for the additional references ! We \u2019 ve included them in Section 2 paragraph 1 . 3.We chose to separate out the policy from the transition dynamics . Action a_ { t } is sampled from \\pi ( a_ { t } | s_ { t } ) and depends on the policy ; next state s_ { t } is sampled from P ( s_ { t+1 } | s_ { t } , a_ { t } ) and depends on the transition dynamics . 4.Good idea . We \u2019 ve changed \u201c Act ( ) \u201d to \u201c ChooseAction ( ) \u201d in Algorithm 1 . 5.For Figure 3 , we agree confidence intervals would be helpful . We can \u2019 t regenerate the plot in the next 24 hours before the rebuttal deadline , but will include confidence intervals in the camera-ready version ."}}