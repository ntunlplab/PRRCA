{"year": "2021", "forum": "7nfCtKep-v", "title": "EXPLORING VULNERABILITIES OF BERT-BASED APIS", "decision": "Reject", "meta_review": "The paper presents novel model stealing attacks against BERT API. The attacks are split in two phases. In the first phase, the black-box BERT model is recovered by submission of specially crafted data. In the second phase, the inferred model can be used for identifying sensitive attributes or to generate adversarial examples against the basic BERT model.\n\nDespite the novelty of presented attacks against BERT models, the current version of the paper has some problems with clarity and motivation. The presentation of attacks is very short, and some technical details are not adequately covered. The practical motivation of adversarial example transfer attacks is not very clear, and the authors' response on this issue did not provide a convincing clarifications. Furthermore, creation of surrogate models for generation of adversarial examples is a well-known technique and the difference of the proposed AET attack from this conceptual approach is not clear.\n\nOverall, the paper reveals a solid and interesting work but a substantial revision would be necessary to make it suitable for the ACLR audience.  ", "reviews": [{"review_id": "7nfCtKep-v-0", "review_text": "Summary : This paper is studying the vulnerabilities of modern BERT-based classifiers , which a service provider is hosting using a black-box inference API . Consistent with prior work [ 2 ] , the authors succeed in extracting high performing copies of the APIs , by training models using the outputs of the API to queries ( akin to distillation ) . The authors then study two attacks on the copy model private attribute identification of sentences in the API 's training data & adversarial example transfer from the white-box copy model to the black-box API . The authors report high attack success rates , better than those from competitive baselines ( which do not require constructing a copy model ) . A few defences are also explored but are ineffective to prevent these attacks . - Strengths of the Paper : 1 . While model extraction on BERT models has been studied previously [ 2 ] , this paper goes beyond the setting of utility theft and explores information leakage and adversarial example transfer . These are extremely practical real-world settings . Moreover , the paper uses modern NLP techniques ( finetuning BERT ) , which is ubiquitous in NLP systems these days . 2.The reported attacks seem to significantly outperform some competitive baselines which did n't use an extracted model . While I have concerns about the experimental setup ( below ) , these are very interesting results highlighting vulnerabilities of the models . This can encourage more research in defending against model extraction . - Weaknesses of the Paper : 1 . Query distribution : These distributions seem fairly similar to the downstream task for all datasets , for instance , `` reviews '' contains Yelp reviews , which is one of the datasets the victim model was trained on ( I suspect some amount of overlap at the very least ) . The best MEA scores are observed when the domains are aligned , which might not be a practical setting for an attacker who has no knowledge of the victim model 's training distribution . I suggest , at the very least , authors to provide n-gram overlap statistics between their preferred query distribution and downstream test set ( the GPT2 paper [ 3 ] had similar statistics ) . The paper 's story will be stronger if a corpus like Wikipedia is used for the query distribution , with the same set of downstream datasets . 2.AIA Attacks : I have a few concerns here . First , is n't access to private attributes in half of the victim data ( D_a ) too strong an assumption ? In a more practical setting , an attacker will have no access to D_a . It 's even possible that the attacker does n't know the output space of attributes . I think the more interesting setting is where the attacker is able to infer some information about the training data without supervising a classifier with gold data ( D_a ) , perhaps using something like model inversion . This information need not be a private binary label , it could even be some canary string like a credit card number [ 4 ] . One more concern I had here was regarding the main baseline in this experiment , `` BERT ( w/o fine-tuning ) '' . I find it quite strange that this is much worse than the majority class in two datasets . What happens when you fine-tune it on D_a ? ( using the standard practice of [ CLS ] vector for classification ) . This is a valid baseline if access to D_a is assumed , I think this will do quite well if it is possible to infer the private variable from the text . 3.Adversarial example transfer : My main concern here is that `` transfer rate '' by itself is insufficient . You can make transfer rate 100 % by retrieving examples from the target adversarial class . The more interesting evaluation is , what fraction of adversarial examples are both ( 1 ) transferred correctly ; ( 2 ) not adversarial to a human ( the changes are so minor that humans ignore them ) . Some kind of human evaluation for ( 2 ) will be helpful . Also , a good baseline here would be using adv-bert but with randomly chosen words ( instead of white-box gradients ) , and an upper bound with adv-bert attacks on the victim model itself . - Overall Recommendation : While this is a very practically important setting , I 'm not entirely convinced the proposed attacks work . My main concerns are regarding some of the experimental decisions and lack of baselines while comparing attacks . Overall I think the paper needs more work to be ready for publication . - Other Feedback : While these points are not a make or break for me , they will make the paper stronger . It will be nice to include some fine-grained qualitative analysis of the adversarial examples ( along with samples ) , perhaps highlighting why generating that example would only be possible with access to an extracted model , and confirming the victim API model generates the same example . It will also be nice to see work beyond classification setting . Setups like question answering , machine translation , unconditional text generation are exciting testbeds which might be a lot more vulnerable to AIA style attacks than classifiers . With GPT3 , black-box text generation APIs are probably going to get very common in the next 2-3 years ! - Errors / Typos / Stylistic : I had some trouble understanding parts of the paper . I think with a bit more polishing and careful proof-reading , the paper will be easier to understand . There were also a few incorrect statements . I 've pointed them below along with typos / stylistic suggestions , `` commercial NLP models such as Google \u2019 s BERT and OpenAI \u2019 s GPT-2 ( Radford et al. , 2019 ) are often made indirectly accessible through pay-per-query prediction APIs . '' -- > This is not a correct statement , both pretrained models are freely available `` and NLP tasks ( Chandrasekaran et al. , 2020 ) . '' is a mis-citation , you probably wanted to cite Pal et al.2019 [ 1 ] or Krishna et al.2020 [ 2 ] here ? In 3.2 and the Abstract / Intro I would remove the claim that `` architecture , hyperparameter is not known '' , since both the victim / attacker are finetuning BERT . There 's some unnecessary mathiness in 3.2 ( variables which are not referred to later on , like f_ { bert } _theta * ) . I would suggest avoiding variables unless you plan to re-use them to reduce confusion . In Table 3 I would suggest reporting attack success rather than privacy , to be consistent with other tables in paper ( higher means more attack success ) Table 4 caption , `` Transferability is the ratio '' -- > `` Transferability is the percentage '' ? - * * After Author Response * * : I really appreciate the author 's efforts over the course of the rebuttal period for rigorously testing their method with several new baselines in such a short period of time . For AIA attacks , the baseline numbers provided in the rebuttal are helpful but raise concerns about whether the proposed AIA attacks are working . I find it hard to believe that victim models have less private information than extracted models in 2 out of 3 datasets , and I suspect some other factors are contributing to this counterintuitive trend ( like you said , maybe dark knowledge ) . I will stick with my stance that the AIA setting is broken since you are inferring private attributes using information from an identically distributed D_a ( I think model inversion is a more valid setting to measure leakage ) . For adversarial attack baselines , I agree with your argument that conducting black-box attacks directly on the victim models may need minimal difference queries which can be detected on the API side . However , you are going to need several orders of magnitude more queries to do extraction in the first place ( which may or may not be easy to detect ) . I still encourage you to run this baseline in the next version of the paper , instead of only doing black-box attacks on extracted models . These minimal difference checks may not be in place , and directly doing black-box attacks on the victim model are much easier than extracting and then constructing adversarial examples . It is good to know what additional benefit you get by doing model extraction . Overall , I have decided to raise my score to 6 ( more like ~5.5-6 ) . This is conditional on the authors performing much more rigorous hypothesis-driven testing in the next version of the paper ( just like they did in the rebuttal ) to really validate the hypothesis `` extracting models make APIs more vulnerable to adversarial attacks '' . - References [ 1 ] - https : //arxiv.org/abs/1905.09165 [ 2 ] - https : //arxiv.org/abs/1910.12366 [ 3 ] - https : //cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf [ 4 ] - https : //arxiv.org/abs/1802.08232", "rating": "6: Marginally above acceptance threshold", "reply_text": "First of all , We would like to appreciate the reviewer \u2019 s suggestion . [ Re : query distribution ] Sorry for the miscommunication caused in our paper . We would like to clarify/correct a few key points in the reviewer 's comments . First , we removed the Yelp dataset from the queries , when attacking the Yelp task . In addition , when attackers conduct an attack to steal the model , they don \u2019 t have access to the in-house training dataset , but they definitely have some prior knowledge of the task as the commercial pay-per-query prediction NLP systems are responsible for publishing a step-by-step document to guide end-users which task the API is used for and how to use their APIs . Thus we selected the datasets relevant to the task of interest . Following the reviewer \u2019 s suggestion , we also calculate the uni-gram and 5-gram overlapping between test sets and different queries on the 1x setting . According to table3 , AG news is close to news data , while TP-US , Blog , and Yelp are similar to reviews data in terms of the lexicons , which corroborates the results of table 2 . [ Re : AIA setting ] In terms of AIA , we would like to kindly correct some misunderstandings of the reviewer , with the hope that this will help clarify the experimental setting . We never assume that the attacker can access the private attributes in half of the victim data . In Sec.4.3 , we have clearly stated that for each dataset , the first half ( denoted as D_V ) is used to train a victim model , whereas the second half ( denoted as D_A ) is specifically reserved as public data for the training of AIA attack model . There is no overlap between D_V and D_A . We also pointed out the difference between our AIA attack and model inversion attack in Sec.3.3 . Definitely , model inversion attack can also be conducted upon the extracted model as well , which however is not our main focus in this work . In Table 3 , we take BERT ( w/o fine-tuning ) as a baseline as we want to demonstrate that the attack model built on the BERT representation of the extracted model indeed largely enhances the attribute inference of the training data of the victim model . And the reason is quite straightforward : the extracted model trains on the queries and the returned predictions from the victim model , while BERT ( w/o fine-tuning ) is a plain model that did not contain any information about the target model training data . Hence , it \u2019 s not surprising that BERT ( w/o fine-tuning ) may even get worse performance than the majority class on AG new and TP-US . We have added the extra explanation in our revision in Sec.4.3 . [ Re : Adversarial example transfer ] The first question is about the transfer rate . Many previous works have studied and provided results about the transfer rate between different models [ 1 ] [ 2 ] [ 3 ] . From these studies , we can see that it is hard to achieve a 100 % transfer rate due to the general modification budget . The second question is about human evaluation and randomly chosen words . This is also studied by previous studies [ 1 ] . Previous results show the randomly chosen word would decrease the attack rate with the limited modification budget . We think we have provided enough results to demonstrate the effectiveness of our attack strategies . However , following your suggestions , we did additional experiments on a variant of adv-bert , where the tokens are selected randomly on table 5 ( c.f.the revised version ) . In this case , our white-box setting still demonstrates superiority . We will include a human evaluation in the final revision as what previous works have done [ 1 ] [ 3 ] [ Other Feedback ] We want to correct the point in \u201c victim API model generates the same example \u201d . There is no reason and motivation for the victim API model to generate the adversarial examples , as victim API is the service provider who needs to ensure utility for benign users . In the black-box attacks against the victim API , adversarial examples are generated by the attacker with the aim of compromising the integrity of the victim API [ Papernot et al. , 2017 ] . We conduct MEA first in order to steal the victim API , then based on the extracted model , we can generate natural adversarial examples [ 1 ] with high transferability to the victim API . Hope this clarifies . In terms of other tasks , we agree question answering , machine translation , unconditional text generation are also exciting testbeds . However , we want to emphasize that our work is a pioneering work validating that the subsequent attacks can be done after the model extraction , hence we only investigated several representative classification tasks . It should be easy to extend to other tasks with a minor alteration , which is our concurrent work in another paper ."}, {"review_id": "7nfCtKep-v-1", "review_text": "# # # Overview The authors propose a pipeline to attack and steal sensitive information from a BERT-based API service , and can subsequently perform adversarial attack to the victim model by creating white-box adversarial samples on the stolen model . The pipeline can be summarized as the followings : 1 . Using distillation to train ( steal ) a model from the API . 2.Conduct model inversion attack to the stolen model in step 1 to expose sensitive information of the training data . 3.Create adversarial samples for the stolen model in step 1 and use them to attack the original API . Some of the assumptions of the experiment settings are too strong and far from the real situation , but the idea of using this pipeline to conduct model inversion and adversarial transfer attack is very interesting . # # # Pros The pipeline proposed by the authors is very insightful . The experiment results also show the effectiveness of model inversion and adversarial attack . # # # Cons The assumption of the Model Extraction Attack part is too strong . The authors use the same pre-trained BERT parameters for both victim and stealer model . However in real practice we are not able to know which pre-trained BERT parameter set is to be used for fine-tuning , nevertheless to get the pre-trained model . What if we use different pre-trained BERT parameters ? What if we use a pre-trained BERT with different size ( num of layers , hidden dim , etc . ) ? Besides , the stealing method is just a conventional distillation . Although the authors claims three differences between their method and distillation : ( 1 ) the goal ( 2 ) the accessibility of original data ( 3 ) the accessibility of hard labels , only the first one is appropriately claimed . For ( 2 ) , distillation is also broadly used in transfer learning w/o the access of original data . For ( 3 ) , distillation w/ only soft labels is also very popular and useful , from conventional distillation for compression , to self-distillation . I 'd like to hear the reason why the authors make the assumption that the stealer would have the same pre-trained BERT when attacking , and also curious about the results of using different pre-trained BERT model . I might change the rating if the authors may address these questions .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to appreciate the reviewer \u2019 s suggestion . Although in the main content , we hold a strong assumption , we also provided more experimental results on more realistic scenarios , where the victim and imitated model have a different architecture , see Appendix Section D ( c.f.original version Table 8 ) or Section 4.5 ( c.f.revised version Table 6 ) . As shown in these tables , albeit the performance degradation of all attacks , the architectural difference between the victim and the extracted model can not prevent the attacks launched by the attacker . Although we only experimented with BERT , RoBERTa , and XLNET , we believe the proposed scenario is a practical concern that may adversely affect all cloud APIs . We justify this belief with the following reasons : To the best of our knowledge , despite the different training objectives ( ELECTRA v.s BERT ) , the amount of the training data and training time ( RoBERTa v.s.BERT ) , and so on , all the publicly available pre-trained models share the same architecture , i.e.transformer . Hence experiments on BERT , RoBERTa , and XLNET are sufficiently representative . Moreover , the gist of our work is not about knowledge distillation ( KD ) . Instead , KD is merely a vehicle that bridges the victim model and the extracted model . Our key contribution is to leverage a surrogate to conduct successive attacks . If one directly studies some black-box attacks on cloud APIs , the owner of the victim model can spot the intensive abnormal querying actions and ban any queries from the attackers . In our revision , we have deleted the comparison to KD to avoid any confusion ."}, {"review_id": "7nfCtKep-v-2", "review_text": "The paper is motivated by a challenging problem in deploying a neural network-based model for sensitive domain and research in this direction is essential for making such model usable for sensitive domains . The paper presents a model extraction attack , where the adversary can steal a BERT- based API ( i.e.the victim model ) , without knowing the victim model \u2019 s architecture , parameters or the training data distribution . The model extraction attack , where the adversary queries the target model with the goal to steal it and turn it into a white-box model . They demonstrated using simulated experiments that how the extracted model can be exploited to develop effective attribute inference attack to expose sensitive information of the training data . They claimed that the extracted model can lead to highly transferable adversarial attacks against the original model ( victim model ) . The model extraction step of the proposed method is the main concern for me . Conclusions maid by simulated experiments on model extraction attack might not hold for a real experiment . The simulated experiments make both victim model and extracted model accessible and thus measuring functional similarity is fairly easy . However , without the knowledge of the victim model and with limited query budget , the simulated experiment might not resemble a real-scenario . Some explanations with real scenarios would make the claim more realistic . Some thoughts : Re : \u201c Modern NLP systems are typically based on a pre-trained BERT . \u201d : provide references or evidence to support the statement . Re : \u201c Model extraction attack aims to steal an intellectual model from cloud services. \u201d : provide references or evidence to support the statement . Re : \u201c Most existing adversarial attacks on BERT are white-box settings , requiring knowledge of either the model internals ( e.g. , model architecture , hyperparameters ) or training data. \u201d : provide references or evidence to support the statement . Re : \u201c The intuition lies in the fact that the similarity of our extracted model and the victim model allows for direct transfer of adversarial examples obtained via gradient-based attacks. \u201d \u2014 BERT part is same for both victim and extracted model but rest is still unknown and how the complexity of the similarity measurement increases for a real scenario ? Re : \u201c We measure the accuracy ( on the same held-out test set for evaluation purposes ) between the outputs of the victim model and the extracted model to assess their functional similarity. \u201d \u2014 Can this be arbitrarily true by accident ? Is there a robust way that we can use to measure the similarity ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "First of all , We would like to appreciate the reviewer \u2019 s suggestion . [ Re : realisticity ] We would like to clarify/correct a few key points in the reviewer 's comments . Our primary claim is that in spite of the achievements made by BERT-based models , there are lots of security issues when companies directly deploy them as APIs . This work raises three issues : 1 ) model extraction , 2 ) privacy leakage and 3 ) pseudo adversarial attack . Our paper has shown that BERT-based models are vulnerable to all these attacks . Due to the space constraint , in the main content , we only described a simpler case , where both victim and imitated model use BERT base . However , we want to stress that the BERT part is not always the same for both victim and extracted model in our paper . In Appendix Section D ( c.f.original version Table 8 ) or Section 4.5 ( c.f.revised version Table 6 ) , we did show that without the prior knowledge of the architecture of the victim model , our attacks are still effective . We also provided limited query budgets in Appendix Table 7 by varying the number of queries from 0.1X-5X . Thus the assumption made in this paper is relatively practical , universal instead of hypothetical . Given the success of the proposed attacks , our next step is to explore a more realistic setting , i.e.how to fulfill the comparable performance to the current setting , with limited query budgets . [ Re : complexity for a real scenario ] We admit that the complexity of the similarity measurement between two models is intractable , especially for those that are distant from each other . However , we hold an assumption that for a given sentence , a rational model should rely on the salient words , when assigning a label . Hence , if two models function similarly , the rationale should be highly overlapped . The black-box adversarial attack leverages the brittleness of the deep learning model , whereas the white-box attack seeks the most informative words affecting the decision according to the gradient [ 1 ] . According to our assumption , the informative words should be easily transferred to the victim , which is also verified by Table 5 and Table 6 in the revised version . Of course , this assumption requires further rigorous study . [ thoughts1-5 ] For model extraction attack , we have clearly cited [ 2 , 3 , 4 , 5 ] in Sec.2.1 . We added these references to the statement mentioned by the reviewer to make it more clear . We have modified the statement \u201c Most existing adversarial attacks on BERT are white-box settings ... \u201d to the following : However , most recent works for adversarial example transfer focus on the black-box setting [ 6 , 7 ] . In such a setting , the adversary attacks the model via the query feedback only in Sec.3.4 . In Table 3 , Table 8 ( c.f.Appendix Section D in the original version ) or Table 6 in our revision , Table 7 ( in Appendix ) , we have shown how the complexity of the similarity measurement increases for more realistic scenarios : when the attacker has no prior knowledge of the architecture of the victim model , no prior knowledge of the victim model training data distribution , and only has limited query budget . Definitely there should be a robust way that we can use to measure the similarity , which however is out of the scope of this paper . Our main focus is to exploit the privacy leakage and vulnerability of transferred adversarial examples from the extracted model . [ 1 ] Sun , Lichao , et al . `` Adv-BERT : BERT is not robust on misspellings ! Generating nature adversarial samples on BERT . '' arXiv preprint arXiv:2003.04985 ( 2020 ) . [ 2 ] Kalpesh Krishna , Gaurav Singh Tomar , Ankur P Parikh , Nicolas Papernot , and Mohit Iyyer . Thieves on sesame street ! model extraction of bert-based apis.arXiv preprint arXiv:1910.12366 , 2019 [ 3 ] Knockoff nets : Stealing functionality of black-box models . InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp . 4954\u20134963 , 2019 [ 4 ] Stealing machine learning models via prediction apis . In25th { USENIX } Security Symposium ( { USENIX } Security16 ) , pp . 601\u2013618 , 2016 [ 5 ] Eric Wallace , Mitchell Stern , and Dawn Song . Imitation attacks and defenses for black-box machine translation systems.arXiv preprint arXiv:2004.15015 , 2020 [ 6 ] Ji Gao , Jack Lanchantin , Mary Lou Soffa , and Yanjun Qi . Black-box generation of adversarial text sequences to evade deep learning classifiers . In2018 IEEE Security and Privacy Workshops ( SPW ) , pp . 50\u201356.IEEE , 2018 . [ 7 ] Javid Ebrahimi , Daniel Lowd , and Dejing Dou . On adversarial examples for character-level neural machine translation.arXiv preprint arXiv:1806.09030 , 2018a"}, {"review_id": "7nfCtKep-v-3", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper presents a model extraction attack ( MEA ) for BERT-based models that are hosted behind an API . Using the model obtained in this step , the work aims to subsequently demonstrate attribute inference attacks ( AIA ) to expose sensitive information of the underlying data used during fine-tuning and adversarial example transfer ( AET ) that can be used to attack the hosted model . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I lean toward reject . The underlying idea is interesting and timely , and core to this interest is that `` the adversary can steal a BERT-based API ( the victim model ) , without knowing the victim model 's architecture , parameters or the training data distribution . '' As demonstrated , a substantial portion of the architecture ( BERT ) is known and the exploration of fine-tuning as the only mechanism for tailoring the model ( rather than continual pretraining ) limits potential impact . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Strengths : - Broad interest . The underlying ideas are of general interest , especially given recent examples of language models hosted behind APIs . The notion that they can be efficiently reproduced from that API and that they may in turn leak training information is an emerging concern . - Clear differentiation from prior work . In particular , the section on comparison to knowledge distillation is helpful in grounding the setting for experimentation . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Weaknesses : - Limitations . There is an implicit assumption that the models being hosted behind APIs are fine-tuned BERT models . Limitations of this should be more explicitly discussed . Many works in specific domains ( e.g. , legal , biomedical , etc . ) appear to rely on continual pretraining to integrate sensitive data rather than fine-tuning toward a single task . Others even appear to train these models from scratch on data . It 's unclear how common the case of fine-tuned BERT models behind APIs are from this paper . - Motivation of AET . The motivation of adversarial attacks against a pay-per-query API are unclear . Yes , it 's possible to cause the API to create incorrect predictions , but why is that problematic for the owner of the model ? It 's clearly undesirable with respect to creating robust models , but as presented it 's unclear why this is problematic . - Impact.Similar to the point above , the assertion that `` modern NLP systems typically leverage the fine-tuning methodology by adding a few task-specific layers on top of the publicly available BERT base '' is not substantiated by this work or by citation . While BERT has certainly become abundant , many recent advances are either not BERT-based ( though perhaps the underlying transformer architecture ) or do more than fine-tuning . - Knowledge of black-box model . While a stated goal is that a knowledge of the architecture and training data is not required , the experiments leverage a knowledge of the architecture ( BERT ) and appear to share an architecture for layers used during fine-tuning . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions : - Given the positioning of `` stealing '' a model , how many queries are required to obtain an approximate model ? How many are required if knowledge of the previously issued queries is known ? - Can you provide pointers to models that are BERT-based and fine-tuned for a specific task only ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "First of all , We would like to appreciate the reviewer \u2019 s suggestion . [ Re : Limitation ] We agree with the weakness pointed by the reviewer . However , we would like to highlight our main contribution : we aim to raise general issues related to BERT models , it is not restricted to the original BERT models , but can also be applied to any domain-specific models ( SciBERT ( Beltagy et al.2019 ) , FinBERT ( Yang et al.2020 ) , BioBERT ( Lee et al.2019 ) , and so on ) . Since all these models are publicly available , the attacker can easily adapt them for the task of interest under the same attacking protocol introduced in this work . Additionally , domain-specific unlabelled datasets can be crawled easily as well . As we described in Table 2 , model extraction can also be beneficial to different datasets at the cost of more queries . Krishna et al.2019 have shown that using even dummy queries can steal fine-tuned task-specific BERT models with competitive performance . Finally , we experimented with different architectures in Appendix Section D ( c.f.original version Table 8 ) or Section 4.5 ( c.f.revised version Table 6 ) . As shown in these tables , even though the victim models are different from the extracted model , the proposed attacks are still effective , which draws an analogy to knowledge distillation , where the teacher model and the student model can be different . To sum up , the proposed attacks are model-agnostic . One can plug and play any publicly available pretrained models and dataset for a particular task . [ Re : Motivation of AET ] Malicious users could be one of the business competitors . Hence if they manage to spot incorrect predictions , they can improve the robustness of their model while launching an advertising campaign against the victim model with these adversarial examples . If a rival company directly studies adversarial attacks on the victim model , its owner can spot the suspicious querying and ban it . We have made it clear in our revision . For the motivation of AET , it \u2019 s also recommended to refer to the most recent work [ Wallace et al.2020 ] , which transfers adversarial examples to machine translation systems . Lastly , a malicious user can utilise misclassification or mistranslation to raise any controversial quarrel or even disturbance , if there are political , ethical , gender , or race biases being involved . We have experienced such situations over the past years . [ Re : Impact and Knowledge of black-box model ] We would like to emphasize that we did experiment with different architectures in Appendix Section D ( c.f.original version Table 8 ) or Section 4.5 ( c.f.revised version Table 6 ) . As shown in these tables , albeit the performance degradation of all attacks , the architectural difference between the victim and the extracted model can not prevent the attacks launched by malicious users . Although we only experimented with BERT , RoBERTa , and XLNET , we believe the proposed scenario is a practical concern adversely affecting and being applicable to all cloud APIs . We justify this belief with the following reasons : To the best of our knowledge , despite the different training objectives ( ELECTRA v.s BERT ) , the amount of the training data and training time ( RoBERTa v.s.BERT ) , and so on , all the publicly available pre-trained models share the same architecture , i.e.transformer . Hence experiments on BERT , RoBERTa , and XLNET are sufficiently representative . [ Re : Questions ] As shown in Table 2 , Table 7 ( in Appendix ) and all previous works ( Krishna et al 2019 , Orekondy et al.2019 ) , the number of queries to obtain an approximate model is varied across different tasks , and generally more queries result in better extraction . The study on the size of queries is also a very interesting research direction , which would be our future work and we did not take it as the main focus of this work . We want to emphasize that this work pays more attention to the subsequent attacks based on the extracted model , which is only served as a prerequisite , but not the gist of this work . Here are a list of papers fine-tuning on BERT : SemEval-2019 Task 6 : Identifying and Categorizing Offensive Language in Social Media ( OffensEval ) ( https : //arxiv.org/pdf/1903.08983.pdf ) Passage Re-ranking with BERT ( https : //arxiv.org/abs/1901.04085 ) Text Summarization with Pretrained Encoders ( https : //arxiv.org/abs/1908.08345 ) SpanBERT : Improving Pre-training by Representing and Predicting Spans ( https : //arxiv.org/abs/1907.10529 ) In addition , according to huggingface website , bert-base-uncase is the most downloaded model among all the pretrained model ( https : //huggingface.co/models ) Although recent works have modified the objective or architecture to fit their tasks , they are still derived from BERT . Our work leverages BERT-based as a case study , but both the methodology and security issues are universal and model-agnostic . Other works can alter the proposed attacks for a particular scenario ."}], "0": {"review_id": "7nfCtKep-v-0", "review_text": "Summary : This paper is studying the vulnerabilities of modern BERT-based classifiers , which a service provider is hosting using a black-box inference API . Consistent with prior work [ 2 ] , the authors succeed in extracting high performing copies of the APIs , by training models using the outputs of the API to queries ( akin to distillation ) . The authors then study two attacks on the copy model private attribute identification of sentences in the API 's training data & adversarial example transfer from the white-box copy model to the black-box API . The authors report high attack success rates , better than those from competitive baselines ( which do not require constructing a copy model ) . A few defences are also explored but are ineffective to prevent these attacks . - Strengths of the Paper : 1 . While model extraction on BERT models has been studied previously [ 2 ] , this paper goes beyond the setting of utility theft and explores information leakage and adversarial example transfer . These are extremely practical real-world settings . Moreover , the paper uses modern NLP techniques ( finetuning BERT ) , which is ubiquitous in NLP systems these days . 2.The reported attacks seem to significantly outperform some competitive baselines which did n't use an extracted model . While I have concerns about the experimental setup ( below ) , these are very interesting results highlighting vulnerabilities of the models . This can encourage more research in defending against model extraction . - Weaknesses of the Paper : 1 . Query distribution : These distributions seem fairly similar to the downstream task for all datasets , for instance , `` reviews '' contains Yelp reviews , which is one of the datasets the victim model was trained on ( I suspect some amount of overlap at the very least ) . The best MEA scores are observed when the domains are aligned , which might not be a practical setting for an attacker who has no knowledge of the victim model 's training distribution . I suggest , at the very least , authors to provide n-gram overlap statistics between their preferred query distribution and downstream test set ( the GPT2 paper [ 3 ] had similar statistics ) . The paper 's story will be stronger if a corpus like Wikipedia is used for the query distribution , with the same set of downstream datasets . 2.AIA Attacks : I have a few concerns here . First , is n't access to private attributes in half of the victim data ( D_a ) too strong an assumption ? In a more practical setting , an attacker will have no access to D_a . It 's even possible that the attacker does n't know the output space of attributes . I think the more interesting setting is where the attacker is able to infer some information about the training data without supervising a classifier with gold data ( D_a ) , perhaps using something like model inversion . This information need not be a private binary label , it could even be some canary string like a credit card number [ 4 ] . One more concern I had here was regarding the main baseline in this experiment , `` BERT ( w/o fine-tuning ) '' . I find it quite strange that this is much worse than the majority class in two datasets . What happens when you fine-tune it on D_a ? ( using the standard practice of [ CLS ] vector for classification ) . This is a valid baseline if access to D_a is assumed , I think this will do quite well if it is possible to infer the private variable from the text . 3.Adversarial example transfer : My main concern here is that `` transfer rate '' by itself is insufficient . You can make transfer rate 100 % by retrieving examples from the target adversarial class . The more interesting evaluation is , what fraction of adversarial examples are both ( 1 ) transferred correctly ; ( 2 ) not adversarial to a human ( the changes are so minor that humans ignore them ) . Some kind of human evaluation for ( 2 ) will be helpful . Also , a good baseline here would be using adv-bert but with randomly chosen words ( instead of white-box gradients ) , and an upper bound with adv-bert attacks on the victim model itself . - Overall Recommendation : While this is a very practically important setting , I 'm not entirely convinced the proposed attacks work . My main concerns are regarding some of the experimental decisions and lack of baselines while comparing attacks . Overall I think the paper needs more work to be ready for publication . - Other Feedback : While these points are not a make or break for me , they will make the paper stronger . It will be nice to include some fine-grained qualitative analysis of the adversarial examples ( along with samples ) , perhaps highlighting why generating that example would only be possible with access to an extracted model , and confirming the victim API model generates the same example . It will also be nice to see work beyond classification setting . Setups like question answering , machine translation , unconditional text generation are exciting testbeds which might be a lot more vulnerable to AIA style attacks than classifiers . With GPT3 , black-box text generation APIs are probably going to get very common in the next 2-3 years ! - Errors / Typos / Stylistic : I had some trouble understanding parts of the paper . I think with a bit more polishing and careful proof-reading , the paper will be easier to understand . There were also a few incorrect statements . I 've pointed them below along with typos / stylistic suggestions , `` commercial NLP models such as Google \u2019 s BERT and OpenAI \u2019 s GPT-2 ( Radford et al. , 2019 ) are often made indirectly accessible through pay-per-query prediction APIs . '' -- > This is not a correct statement , both pretrained models are freely available `` and NLP tasks ( Chandrasekaran et al. , 2020 ) . '' is a mis-citation , you probably wanted to cite Pal et al.2019 [ 1 ] or Krishna et al.2020 [ 2 ] here ? In 3.2 and the Abstract / Intro I would remove the claim that `` architecture , hyperparameter is not known '' , since both the victim / attacker are finetuning BERT . There 's some unnecessary mathiness in 3.2 ( variables which are not referred to later on , like f_ { bert } _theta * ) . I would suggest avoiding variables unless you plan to re-use them to reduce confusion . In Table 3 I would suggest reporting attack success rather than privacy , to be consistent with other tables in paper ( higher means more attack success ) Table 4 caption , `` Transferability is the ratio '' -- > `` Transferability is the percentage '' ? - * * After Author Response * * : I really appreciate the author 's efforts over the course of the rebuttal period for rigorously testing their method with several new baselines in such a short period of time . For AIA attacks , the baseline numbers provided in the rebuttal are helpful but raise concerns about whether the proposed AIA attacks are working . I find it hard to believe that victim models have less private information than extracted models in 2 out of 3 datasets , and I suspect some other factors are contributing to this counterintuitive trend ( like you said , maybe dark knowledge ) . I will stick with my stance that the AIA setting is broken since you are inferring private attributes using information from an identically distributed D_a ( I think model inversion is a more valid setting to measure leakage ) . For adversarial attack baselines , I agree with your argument that conducting black-box attacks directly on the victim models may need minimal difference queries which can be detected on the API side . However , you are going to need several orders of magnitude more queries to do extraction in the first place ( which may or may not be easy to detect ) . I still encourage you to run this baseline in the next version of the paper , instead of only doing black-box attacks on extracted models . These minimal difference checks may not be in place , and directly doing black-box attacks on the victim model are much easier than extracting and then constructing adversarial examples . It is good to know what additional benefit you get by doing model extraction . Overall , I have decided to raise my score to 6 ( more like ~5.5-6 ) . This is conditional on the authors performing much more rigorous hypothesis-driven testing in the next version of the paper ( just like they did in the rebuttal ) to really validate the hypothesis `` extracting models make APIs more vulnerable to adversarial attacks '' . - References [ 1 ] - https : //arxiv.org/abs/1905.09165 [ 2 ] - https : //arxiv.org/abs/1910.12366 [ 3 ] - https : //cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf [ 4 ] - https : //arxiv.org/abs/1802.08232", "rating": "6: Marginally above acceptance threshold", "reply_text": "First of all , We would like to appreciate the reviewer \u2019 s suggestion . [ Re : query distribution ] Sorry for the miscommunication caused in our paper . We would like to clarify/correct a few key points in the reviewer 's comments . First , we removed the Yelp dataset from the queries , when attacking the Yelp task . In addition , when attackers conduct an attack to steal the model , they don \u2019 t have access to the in-house training dataset , but they definitely have some prior knowledge of the task as the commercial pay-per-query prediction NLP systems are responsible for publishing a step-by-step document to guide end-users which task the API is used for and how to use their APIs . Thus we selected the datasets relevant to the task of interest . Following the reviewer \u2019 s suggestion , we also calculate the uni-gram and 5-gram overlapping between test sets and different queries on the 1x setting . According to table3 , AG news is close to news data , while TP-US , Blog , and Yelp are similar to reviews data in terms of the lexicons , which corroborates the results of table 2 . [ Re : AIA setting ] In terms of AIA , we would like to kindly correct some misunderstandings of the reviewer , with the hope that this will help clarify the experimental setting . We never assume that the attacker can access the private attributes in half of the victim data . In Sec.4.3 , we have clearly stated that for each dataset , the first half ( denoted as D_V ) is used to train a victim model , whereas the second half ( denoted as D_A ) is specifically reserved as public data for the training of AIA attack model . There is no overlap between D_V and D_A . We also pointed out the difference between our AIA attack and model inversion attack in Sec.3.3 . Definitely , model inversion attack can also be conducted upon the extracted model as well , which however is not our main focus in this work . In Table 3 , we take BERT ( w/o fine-tuning ) as a baseline as we want to demonstrate that the attack model built on the BERT representation of the extracted model indeed largely enhances the attribute inference of the training data of the victim model . And the reason is quite straightforward : the extracted model trains on the queries and the returned predictions from the victim model , while BERT ( w/o fine-tuning ) is a plain model that did not contain any information about the target model training data . Hence , it \u2019 s not surprising that BERT ( w/o fine-tuning ) may even get worse performance than the majority class on AG new and TP-US . We have added the extra explanation in our revision in Sec.4.3 . [ Re : Adversarial example transfer ] The first question is about the transfer rate . Many previous works have studied and provided results about the transfer rate between different models [ 1 ] [ 2 ] [ 3 ] . From these studies , we can see that it is hard to achieve a 100 % transfer rate due to the general modification budget . The second question is about human evaluation and randomly chosen words . This is also studied by previous studies [ 1 ] . Previous results show the randomly chosen word would decrease the attack rate with the limited modification budget . We think we have provided enough results to demonstrate the effectiveness of our attack strategies . However , following your suggestions , we did additional experiments on a variant of adv-bert , where the tokens are selected randomly on table 5 ( c.f.the revised version ) . In this case , our white-box setting still demonstrates superiority . We will include a human evaluation in the final revision as what previous works have done [ 1 ] [ 3 ] [ Other Feedback ] We want to correct the point in \u201c victim API model generates the same example \u201d . There is no reason and motivation for the victim API model to generate the adversarial examples , as victim API is the service provider who needs to ensure utility for benign users . In the black-box attacks against the victim API , adversarial examples are generated by the attacker with the aim of compromising the integrity of the victim API [ Papernot et al. , 2017 ] . We conduct MEA first in order to steal the victim API , then based on the extracted model , we can generate natural adversarial examples [ 1 ] with high transferability to the victim API . Hope this clarifies . In terms of other tasks , we agree question answering , machine translation , unconditional text generation are also exciting testbeds . However , we want to emphasize that our work is a pioneering work validating that the subsequent attacks can be done after the model extraction , hence we only investigated several representative classification tasks . It should be easy to extend to other tasks with a minor alteration , which is our concurrent work in another paper ."}, "1": {"review_id": "7nfCtKep-v-1", "review_text": "# # # Overview The authors propose a pipeline to attack and steal sensitive information from a BERT-based API service , and can subsequently perform adversarial attack to the victim model by creating white-box adversarial samples on the stolen model . The pipeline can be summarized as the followings : 1 . Using distillation to train ( steal ) a model from the API . 2.Conduct model inversion attack to the stolen model in step 1 to expose sensitive information of the training data . 3.Create adversarial samples for the stolen model in step 1 and use them to attack the original API . Some of the assumptions of the experiment settings are too strong and far from the real situation , but the idea of using this pipeline to conduct model inversion and adversarial transfer attack is very interesting . # # # Pros The pipeline proposed by the authors is very insightful . The experiment results also show the effectiveness of model inversion and adversarial attack . # # # Cons The assumption of the Model Extraction Attack part is too strong . The authors use the same pre-trained BERT parameters for both victim and stealer model . However in real practice we are not able to know which pre-trained BERT parameter set is to be used for fine-tuning , nevertheless to get the pre-trained model . What if we use different pre-trained BERT parameters ? What if we use a pre-trained BERT with different size ( num of layers , hidden dim , etc . ) ? Besides , the stealing method is just a conventional distillation . Although the authors claims three differences between their method and distillation : ( 1 ) the goal ( 2 ) the accessibility of original data ( 3 ) the accessibility of hard labels , only the first one is appropriately claimed . For ( 2 ) , distillation is also broadly used in transfer learning w/o the access of original data . For ( 3 ) , distillation w/ only soft labels is also very popular and useful , from conventional distillation for compression , to self-distillation . I 'd like to hear the reason why the authors make the assumption that the stealer would have the same pre-trained BERT when attacking , and also curious about the results of using different pre-trained BERT model . I might change the rating if the authors may address these questions .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to appreciate the reviewer \u2019 s suggestion . Although in the main content , we hold a strong assumption , we also provided more experimental results on more realistic scenarios , where the victim and imitated model have a different architecture , see Appendix Section D ( c.f.original version Table 8 ) or Section 4.5 ( c.f.revised version Table 6 ) . As shown in these tables , albeit the performance degradation of all attacks , the architectural difference between the victim and the extracted model can not prevent the attacks launched by the attacker . Although we only experimented with BERT , RoBERTa , and XLNET , we believe the proposed scenario is a practical concern that may adversely affect all cloud APIs . We justify this belief with the following reasons : To the best of our knowledge , despite the different training objectives ( ELECTRA v.s BERT ) , the amount of the training data and training time ( RoBERTa v.s.BERT ) , and so on , all the publicly available pre-trained models share the same architecture , i.e.transformer . Hence experiments on BERT , RoBERTa , and XLNET are sufficiently representative . Moreover , the gist of our work is not about knowledge distillation ( KD ) . Instead , KD is merely a vehicle that bridges the victim model and the extracted model . Our key contribution is to leverage a surrogate to conduct successive attacks . If one directly studies some black-box attacks on cloud APIs , the owner of the victim model can spot the intensive abnormal querying actions and ban any queries from the attackers . In our revision , we have deleted the comparison to KD to avoid any confusion ."}, "2": {"review_id": "7nfCtKep-v-2", "review_text": "The paper is motivated by a challenging problem in deploying a neural network-based model for sensitive domain and research in this direction is essential for making such model usable for sensitive domains . The paper presents a model extraction attack , where the adversary can steal a BERT- based API ( i.e.the victim model ) , without knowing the victim model \u2019 s architecture , parameters or the training data distribution . The model extraction attack , where the adversary queries the target model with the goal to steal it and turn it into a white-box model . They demonstrated using simulated experiments that how the extracted model can be exploited to develop effective attribute inference attack to expose sensitive information of the training data . They claimed that the extracted model can lead to highly transferable adversarial attacks against the original model ( victim model ) . The model extraction step of the proposed method is the main concern for me . Conclusions maid by simulated experiments on model extraction attack might not hold for a real experiment . The simulated experiments make both victim model and extracted model accessible and thus measuring functional similarity is fairly easy . However , without the knowledge of the victim model and with limited query budget , the simulated experiment might not resemble a real-scenario . Some explanations with real scenarios would make the claim more realistic . Some thoughts : Re : \u201c Modern NLP systems are typically based on a pre-trained BERT . \u201d : provide references or evidence to support the statement . Re : \u201c Model extraction attack aims to steal an intellectual model from cloud services. \u201d : provide references or evidence to support the statement . Re : \u201c Most existing adversarial attacks on BERT are white-box settings , requiring knowledge of either the model internals ( e.g. , model architecture , hyperparameters ) or training data. \u201d : provide references or evidence to support the statement . Re : \u201c The intuition lies in the fact that the similarity of our extracted model and the victim model allows for direct transfer of adversarial examples obtained via gradient-based attacks. \u201d \u2014 BERT part is same for both victim and extracted model but rest is still unknown and how the complexity of the similarity measurement increases for a real scenario ? Re : \u201c We measure the accuracy ( on the same held-out test set for evaluation purposes ) between the outputs of the victim model and the extracted model to assess their functional similarity. \u201d \u2014 Can this be arbitrarily true by accident ? Is there a robust way that we can use to measure the similarity ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "First of all , We would like to appreciate the reviewer \u2019 s suggestion . [ Re : realisticity ] We would like to clarify/correct a few key points in the reviewer 's comments . Our primary claim is that in spite of the achievements made by BERT-based models , there are lots of security issues when companies directly deploy them as APIs . This work raises three issues : 1 ) model extraction , 2 ) privacy leakage and 3 ) pseudo adversarial attack . Our paper has shown that BERT-based models are vulnerable to all these attacks . Due to the space constraint , in the main content , we only described a simpler case , where both victim and imitated model use BERT base . However , we want to stress that the BERT part is not always the same for both victim and extracted model in our paper . In Appendix Section D ( c.f.original version Table 8 ) or Section 4.5 ( c.f.revised version Table 6 ) , we did show that without the prior knowledge of the architecture of the victim model , our attacks are still effective . We also provided limited query budgets in Appendix Table 7 by varying the number of queries from 0.1X-5X . Thus the assumption made in this paper is relatively practical , universal instead of hypothetical . Given the success of the proposed attacks , our next step is to explore a more realistic setting , i.e.how to fulfill the comparable performance to the current setting , with limited query budgets . [ Re : complexity for a real scenario ] We admit that the complexity of the similarity measurement between two models is intractable , especially for those that are distant from each other . However , we hold an assumption that for a given sentence , a rational model should rely on the salient words , when assigning a label . Hence , if two models function similarly , the rationale should be highly overlapped . The black-box adversarial attack leverages the brittleness of the deep learning model , whereas the white-box attack seeks the most informative words affecting the decision according to the gradient [ 1 ] . According to our assumption , the informative words should be easily transferred to the victim , which is also verified by Table 5 and Table 6 in the revised version . Of course , this assumption requires further rigorous study . [ thoughts1-5 ] For model extraction attack , we have clearly cited [ 2 , 3 , 4 , 5 ] in Sec.2.1 . We added these references to the statement mentioned by the reviewer to make it more clear . We have modified the statement \u201c Most existing adversarial attacks on BERT are white-box settings ... \u201d to the following : However , most recent works for adversarial example transfer focus on the black-box setting [ 6 , 7 ] . In such a setting , the adversary attacks the model via the query feedback only in Sec.3.4 . In Table 3 , Table 8 ( c.f.Appendix Section D in the original version ) or Table 6 in our revision , Table 7 ( in Appendix ) , we have shown how the complexity of the similarity measurement increases for more realistic scenarios : when the attacker has no prior knowledge of the architecture of the victim model , no prior knowledge of the victim model training data distribution , and only has limited query budget . Definitely there should be a robust way that we can use to measure the similarity , which however is out of the scope of this paper . Our main focus is to exploit the privacy leakage and vulnerability of transferred adversarial examples from the extracted model . [ 1 ] Sun , Lichao , et al . `` Adv-BERT : BERT is not robust on misspellings ! Generating nature adversarial samples on BERT . '' arXiv preprint arXiv:2003.04985 ( 2020 ) . [ 2 ] Kalpesh Krishna , Gaurav Singh Tomar , Ankur P Parikh , Nicolas Papernot , and Mohit Iyyer . Thieves on sesame street ! model extraction of bert-based apis.arXiv preprint arXiv:1910.12366 , 2019 [ 3 ] Knockoff nets : Stealing functionality of black-box models . InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp . 4954\u20134963 , 2019 [ 4 ] Stealing machine learning models via prediction apis . In25th { USENIX } Security Symposium ( { USENIX } Security16 ) , pp . 601\u2013618 , 2016 [ 5 ] Eric Wallace , Mitchell Stern , and Dawn Song . Imitation attacks and defenses for black-box machine translation systems.arXiv preprint arXiv:2004.15015 , 2020 [ 6 ] Ji Gao , Jack Lanchantin , Mary Lou Soffa , and Yanjun Qi . Black-box generation of adversarial text sequences to evade deep learning classifiers . In2018 IEEE Security and Privacy Workshops ( SPW ) , pp . 50\u201356.IEEE , 2018 . [ 7 ] Javid Ebrahimi , Daniel Lowd , and Dejing Dou . On adversarial examples for character-level neural machine translation.arXiv preprint arXiv:1806.09030 , 2018a"}, "3": {"review_id": "7nfCtKep-v-3", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper presents a model extraction attack ( MEA ) for BERT-based models that are hosted behind an API . Using the model obtained in this step , the work aims to subsequently demonstrate attribute inference attacks ( AIA ) to expose sensitive information of the underlying data used during fine-tuning and adversarial example transfer ( AET ) that can be used to attack the hosted model . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I lean toward reject . The underlying idea is interesting and timely , and core to this interest is that `` the adversary can steal a BERT-based API ( the victim model ) , without knowing the victim model 's architecture , parameters or the training data distribution . '' As demonstrated , a substantial portion of the architecture ( BERT ) is known and the exploration of fine-tuning as the only mechanism for tailoring the model ( rather than continual pretraining ) limits potential impact . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Strengths : - Broad interest . The underlying ideas are of general interest , especially given recent examples of language models hosted behind APIs . The notion that they can be efficiently reproduced from that API and that they may in turn leak training information is an emerging concern . - Clear differentiation from prior work . In particular , the section on comparison to knowledge distillation is helpful in grounding the setting for experimentation . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Weaknesses : - Limitations . There is an implicit assumption that the models being hosted behind APIs are fine-tuned BERT models . Limitations of this should be more explicitly discussed . Many works in specific domains ( e.g. , legal , biomedical , etc . ) appear to rely on continual pretraining to integrate sensitive data rather than fine-tuning toward a single task . Others even appear to train these models from scratch on data . It 's unclear how common the case of fine-tuned BERT models behind APIs are from this paper . - Motivation of AET . The motivation of adversarial attacks against a pay-per-query API are unclear . Yes , it 's possible to cause the API to create incorrect predictions , but why is that problematic for the owner of the model ? It 's clearly undesirable with respect to creating robust models , but as presented it 's unclear why this is problematic . - Impact.Similar to the point above , the assertion that `` modern NLP systems typically leverage the fine-tuning methodology by adding a few task-specific layers on top of the publicly available BERT base '' is not substantiated by this work or by citation . While BERT has certainly become abundant , many recent advances are either not BERT-based ( though perhaps the underlying transformer architecture ) or do more than fine-tuning . - Knowledge of black-box model . While a stated goal is that a knowledge of the architecture and training data is not required , the experiments leverage a knowledge of the architecture ( BERT ) and appear to share an architecture for layers used during fine-tuning . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions : - Given the positioning of `` stealing '' a model , how many queries are required to obtain an approximate model ? How many are required if knowledge of the previously issued queries is known ? - Can you provide pointers to models that are BERT-based and fine-tuned for a specific task only ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "First of all , We would like to appreciate the reviewer \u2019 s suggestion . [ Re : Limitation ] We agree with the weakness pointed by the reviewer . However , we would like to highlight our main contribution : we aim to raise general issues related to BERT models , it is not restricted to the original BERT models , but can also be applied to any domain-specific models ( SciBERT ( Beltagy et al.2019 ) , FinBERT ( Yang et al.2020 ) , BioBERT ( Lee et al.2019 ) , and so on ) . Since all these models are publicly available , the attacker can easily adapt them for the task of interest under the same attacking protocol introduced in this work . Additionally , domain-specific unlabelled datasets can be crawled easily as well . As we described in Table 2 , model extraction can also be beneficial to different datasets at the cost of more queries . Krishna et al.2019 have shown that using even dummy queries can steal fine-tuned task-specific BERT models with competitive performance . Finally , we experimented with different architectures in Appendix Section D ( c.f.original version Table 8 ) or Section 4.5 ( c.f.revised version Table 6 ) . As shown in these tables , even though the victim models are different from the extracted model , the proposed attacks are still effective , which draws an analogy to knowledge distillation , where the teacher model and the student model can be different . To sum up , the proposed attacks are model-agnostic . One can plug and play any publicly available pretrained models and dataset for a particular task . [ Re : Motivation of AET ] Malicious users could be one of the business competitors . Hence if they manage to spot incorrect predictions , they can improve the robustness of their model while launching an advertising campaign against the victim model with these adversarial examples . If a rival company directly studies adversarial attacks on the victim model , its owner can spot the suspicious querying and ban it . We have made it clear in our revision . For the motivation of AET , it \u2019 s also recommended to refer to the most recent work [ Wallace et al.2020 ] , which transfers adversarial examples to machine translation systems . Lastly , a malicious user can utilise misclassification or mistranslation to raise any controversial quarrel or even disturbance , if there are political , ethical , gender , or race biases being involved . We have experienced such situations over the past years . [ Re : Impact and Knowledge of black-box model ] We would like to emphasize that we did experiment with different architectures in Appendix Section D ( c.f.original version Table 8 ) or Section 4.5 ( c.f.revised version Table 6 ) . As shown in these tables , albeit the performance degradation of all attacks , the architectural difference between the victim and the extracted model can not prevent the attacks launched by malicious users . Although we only experimented with BERT , RoBERTa , and XLNET , we believe the proposed scenario is a practical concern adversely affecting and being applicable to all cloud APIs . We justify this belief with the following reasons : To the best of our knowledge , despite the different training objectives ( ELECTRA v.s BERT ) , the amount of the training data and training time ( RoBERTa v.s.BERT ) , and so on , all the publicly available pre-trained models share the same architecture , i.e.transformer . Hence experiments on BERT , RoBERTa , and XLNET are sufficiently representative . [ Re : Questions ] As shown in Table 2 , Table 7 ( in Appendix ) and all previous works ( Krishna et al 2019 , Orekondy et al.2019 ) , the number of queries to obtain an approximate model is varied across different tasks , and generally more queries result in better extraction . The study on the size of queries is also a very interesting research direction , which would be our future work and we did not take it as the main focus of this work . We want to emphasize that this work pays more attention to the subsequent attacks based on the extracted model , which is only served as a prerequisite , but not the gist of this work . Here are a list of papers fine-tuning on BERT : SemEval-2019 Task 6 : Identifying and Categorizing Offensive Language in Social Media ( OffensEval ) ( https : //arxiv.org/pdf/1903.08983.pdf ) Passage Re-ranking with BERT ( https : //arxiv.org/abs/1901.04085 ) Text Summarization with Pretrained Encoders ( https : //arxiv.org/abs/1908.08345 ) SpanBERT : Improving Pre-training by Representing and Predicting Spans ( https : //arxiv.org/abs/1907.10529 ) In addition , according to huggingface website , bert-base-uncase is the most downloaded model among all the pretrained model ( https : //huggingface.co/models ) Although recent works have modified the objective or architecture to fit their tasks , they are still derived from BERT . Our work leverages BERT-based as a case study , but both the methodology and security issues are universal and model-agnostic . Other works can alter the proposed attacks for a particular scenario ."}}