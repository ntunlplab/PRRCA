{"year": "2019", "forum": "rJlnB3C5Ym", "title": "Rethinking the Value of Network Pruning", "decision": "Accept (Poster)", "meta_review": "The paper presents a lot of empirical evidence that fine tuning pruned networks is inferior to training them from scratch. These results seem unsurprising in retrospect, but hindsight is 20-20.  The reviewers raised a wide range of issues, some of which were addressed and some which were not. I recommend to the authors that they make sure that any claims they draw from their experiments are sufficiently prescribed. E.g., the lottery ticket experiments done by Anonymous in response to this paper show that the random initialization does poorer than restarting with the initial weights (other than in resnet, though this seems possibly due to the learning rate). There is something different in their setting, and so your claims should be properly circumscribed. I don't think the \"standard\" versus \"nonstandard\" terminology is appropriate until the actual boundary between these two behaviors is identified. I would recommend the authors make guarded claims here.", "reviews": [{"review_id": "rJlnB3C5Ym-0", "review_text": "This paper proposes to investigate recent popular approaches to pruning networks, which have roots in works by Lecun \u201890, and are mostly rooted in a recent series of papers by Song Han (2015-2016). The methods proposed in these papers consist of the following pipeline: (i) train a neural network, (ii) then prune the weights, typically by trimming the those connections corresponding to weights with lowest magnitude, (iii) fine tune the resulting sparsely-connected neural network. The authors of the present work assert that traditionally, \u201ceach of the three stages is considered as indispensable\u201d. The authors go on to investigate the contribution of each step to the overall pipeline. Among their findings, they report that fine-tuning appears no better than training the resulting pruned network from scratch. The assertion then is that the important aspect of pruning is not that it identifies the \u201cimportant weights\u201d but rather that it identifies a useful sparse architecture. One problem here is that the authors may overstate the extent to which previous papers emphasize the fine-tuning, and they may understate the extent to which previous papers emphasize the learning of the architecture. Re-reading Han 2015, it seems clear enough that the key point is \u201clearning the connections\u201d (it\u2019s right there in the title) and that the \u201cimportant weights\u201d are a means to achieve this end. Moreover the authors may miss the actual point of fine-tuning. The chief benefit of fine-tuning is that it is faster than training from scratch at each round of retraining, so that even if it achieves the same performance as training from scratch, that\u2019s still a key benefit. In general, when making claims about other people\u2019s beliefs, the authors need to provide citations. References are not just about credit attribution but also about providing evidence and here that evidence is missing. I\u2019d like to see sweeping statements like \u201cThis is usually reported to be superior to directly training a smaller network from scratch\u201d supported by precise references, perhaps even a quote, to spare the reader some time. To this reader, the most interesting finding in the paper by far is surprisingly understated in the abstract and introduction, buried at the end of the paper. Here, the authors investigate what are the properties of the resulting sparse architectures that make them useful. They find that by looking at convolutional kernels from pruned architectures, they can obtain for each connection, a probability that a connection is \u201ckept\u201d. Using these probabilities, they can create new sparse architectures that match the sparsity pattern of the pruned architectures, a technique that they call \u201cguided sparsification\u201d. The method yields similar benefits to pruning. Note that while obtaining the sparsity patterns does require running a pruning algorithm in the first place, ***the learned sparsity patterns generalize well across architectures and datasets***. This result is interesting and useful, and to my knowledge novel. I think the authors should go deeper here, investigating the idea on yet more datasets and architectures (ImageNet would be nice). I also think that this result should be given greater emphasis and raised to the level of a major focal point of the paper. With convincing results and some hard-work to reshape the narrative to support this more important finding, I will consider revising my score. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "[ 1 ] Channel Pruning for Accelerating Very Deep Neural Networks . He et al. , ICCV 2017 . [ 2 ] ThiNet : A Filter Level Pruning Method for Deep Neural Network Compression . Luo et al. , ICCV 2017 . [ 3 ] Pruning Filters for Efficient ConvNets . Li et al. , ICLR 2017 . [ 4 ] Learning both Weights and Connections for Efficient Neural Networks . Han et al. , NIPS 2015 . [ 5 ] Learning Efficient Convolutional Networks through Network Slimming . Liu et al. , ICCV 2017 . [ 6 ] Data-Driven Sparse Structure Selection for Deep Neural Networks . Huang et al. , ECCV 2018 . [ 7 ] Deep Compression : Compressing Deep Neural Networks with Pruning , Trained Quantization and Huffman Coding . Han et al. , ICLR 2016 . [ 8 ] Pruning Convolutional Neural Networks for Resource Efficient Inference . Molchanov et al. , ICLR 2017 . [ 9 ] AutoPruner : An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference . Luo et al.arXiv , 2018 . [ 10 ] NISP : Pruning Networks using Neuron Importance Score Propagation . Yu et al. , CVPR 2018 . [ 11 ] \u201c Learning-Compression \u201d Algorithms for Neural Net Pruning . Carreira-Perpinan et al. , CVPR 2018 ."}, {"review_id": "rJlnB3C5Ym-1", "review_text": "This paper reinvestigate several recent works on network pruning and find that the common belief about the necessity to train a large network before pruning may not hold. The authors find that training the pruned model from scratch can achieve similar, if not better, performance given enough time of training. Based on these observations, the author conclude that training a larger model followed by pruning is not necessary for obtaining an efficient model with similar performance. In other words, the pruned architecture is more important than the weights inherited from the large model. It reminds researchers to perform stronger baselines before showing complex pruning methods. The paper is well organized and written. It re-evaluate the recent progresses made on this topic. Instead of comparing approaches by simply using the numbers from previous paper, the authors perform extensive experiments to verify whether training the pruned network from scratch would work. The results are very interesting, it suggests the researchers to tune the baseline \u201chardly\u201d and stick to simple approach. However, here are some places that I have concerns with: 1. The two \u201ccommon beliefs\u201d actually state one thing, that is the weights of a pre-trained larger model can potentially help optimization for a smaller model. 2. I don\u2019t quite agree with that \u201ctraining\u201d is the first step of a pruning pipeline as illustrated in Figure 1. Actually the motivation or the common assumption for pruning is that there are already existing trained models (training is already finished) with good performance. If a trained model does not even exist, then one can certainly train various thin/smaller model from scratch as before, this is still a trial and error process. 3. \u201cThe value of pruning\u201d. The goal of pruning is to explore a \u201cthin\u201d or \u201cshallower\u201d version of it with similar accuracy while avoiding the exhaustive architecture search with heavy training processes. Thus the first value of pruning is to explore efficient architecture while avoiding heavy training. Therefore, it should be fast and efficient, ideally with no retraining or little fine-tuning. When the pruning method is too complex to implement or requires much more time than training from scratch, it could be an overkill and adds little value, especially when the performance is not better enough. Therefore, it is more informative if the authors would report the time/complexities for pruning/fine-tuning . 4. The second value of pruning lies at understand the redundancy of the model and providing insights for more efficient architecture designs. 5. Comparing to random initialization, pruning simply provide an initialization point inherited from the larger network. The essential question the author asked is whether a subset of pre-trained weights can outperform random initialization. This seems to be a common belief in transfer learning, knowledge distillation and the studies on initialization. The authors conclude that the accuracy of an architecture is determined by the architecture itself, but not the initialization. If this is true, training from scratch should have similar (but not better) result as fine-tuning a pruned model. As the inherited weights can also be viewed as a \u201crandom\u201d initialization. Both methods should reach equivalent good solution if they are trained with enough number of epochs. Can this be verified with experiments? 6. The experiments might not be enough to reject the common belief. The experiments only spoke that the pruned architectures can still be easily trained and encounter no difficulties during the optimization. One conjecture is that the pruned models in the previous work still have enough capacity for keeping good accuracy. What if the models are significantly pruned (say more than 70% of channels got pruned), is training from scratch still working well? It would add much value if the author can identify when training from scratch fails to match the performance obtained by pruning and fine-tuning. 7. In Section 4.1, \u201cscratch-trained models achieve at least the same level of accuracy as fine-tuned models\u201d. First, the ResNet-34-pruned A/B for this comparison does not have significant FLOPs reduction (10% and 24% FLOPs reduction). Fine-tuning still has advantage as it only takes \u00bc of training time compare to scratch-E. Second, it is interesting that fine-tuning has generally smaller variance than stratch-E (except VGG-19). Would this imply that fine-tuning a pruned model produce more stable result? It would be more complete if there is variance analysis for the imagenet result. 8. What is the training/fine-tuning hyperparameters used in section 4.1? Note that in the experiment of Li et al, 2017, scratch-E takes 164 epochs to train from scratch, while fine-tuning takes only 40 epochs. Like suggested above, if we fine-tune it with more epochs, would it achieve equivalent performance? Also, what is the hyperparameter used in scratch-E? Note that the original paper use batch size 128. If the authors adopts a smaller batch-size for scratch-E, then it has in more iterations and could certainly result in better performance according to recent belief that small batch-size generates better. 9. The conclusion of section 5 is not quite clear or novel. Using uniform pruning ratio for pruning is expected to perform worse than automatic pruning methods as it does not consider the importance difference of each layer and. This comes back to my point 3 & 4 about the value of pruning, that is the value of pruning lies at the analysis of the redundancy of the network. There are a number of works worked on analyzing the importance of different layers of filters. So I think the \u201chypothesis\u201d of \u201cthe value of automatic pruning methods actually lies in the resulting architecture rather than the inherited weight\u201d is kind of straightforward. Also, why not use FLOPs as x-axis in Figure 3? Minor: It might be more accurate to use \u201cL1-norm based Filter Pruning (Li et al., 2017)\u201d as literally \u201cchannels\u201d usually refers to feature maps, which are by-products of the model but not the model itself. I will revise my score if authors can address above concerns. --------- review after rebuttal---------- #1#2 It would be great if the authors can make it clear that training is not the always the first step and the value of pruning in introduction rather than mentioning in conclusion. Saving training time is still an important factor when training from scratch is expensive. #5 \u201cfine-tuning with enough epochs\u201d. I understand that the authors are mainly questioning about whether training from scratch is necessarily bad than pruning and fine-tuning. The author do find that \u201ctraining from scratch is better when the number of epochs is large enough\u201d. But we see that fine-tuning ResNet-56 A/B with 20 epochs does outperform (or is equivalent to) scratch training for the first 160 epochs, which validates \u201cfine-tuning is faster to converge\u201d. However, training 320 epochs (16x more comparing to 20 epochs fine-tuning and 2x comparing with normal training from scratch) is not quite coherent with the setting of \u201cscratch B\u201d, as ResNet-56 B just reduce 27% FLOPs. The other part of the question is still unclear, i.e., the author claimed that the accuracy of an architecture is determined by the architecture itself, but not the initialization, then both fine-tuning and scratch training should reach equivalent solution if they are well trained enough, regardless of the initialization or pruning method. The learning rate for scratch training is already well known (learning rate drop brings boost the accuracy). However, learning rate schedule for fine-tuning (especially for significantly pruned model as for reply#6) is not well explored. I wonder whether that a carefully tuned learning rate/hyperparameters for fine-tuning may get the same or better performance as scratch training. Questions: - Are both methods using the same learning rate schedule between epoch 160 and epoch 320? - The ResNets-56 A/B results in the reply#8 does not match the reported performance in reply#5. e.g., it shows 92.67(0.09) for ResNet-56-B with 40-epochs fine-tuning in reply5, but it turns out to be 92.68(\u00b10.19) in reply#8. - It would be great if the authors can add convergence curves for fine-tuning and scratch training for easier comparison. #6 The failure case for sparse pruning on ImageNet is interesting and it would be great to have the imageNet result reported and discussed. The authors find that \u201cwhen the pruned ratio is large enough, training from scratch is better by a even larger margin than fine-tuning\u201d. This could be due to following reasons: 1. When the pruning ratio is large, the pruned model with preserved weights is significantly different from the original model, and fine-tuning with small learning rate and limited number of epochs is not enough to recover the accuracy. As mentioned earlier, tuning the hyperparameters for fine-tuning based on pruning ratio might improve the performance of fine-tuning. 2. Though the pruning ratio is large, the model used in this experiment may still have large capacity to reach good performance. How about pruning ResNet-56 with significant pruning ratios? Finally, based on above observations, it seems to me that the preserved weights is more essential for fast fine-tuning but less useful for significant pruning ratios. -------- update ---------------- The authors addressed most of my concerns. Some questions are still remaining in my comment \u201cReview after rebuttal\u201d, specifically, fine-tuning a pruned network may still get good performance if the hyperparameters are carefully tuned based on the pruning ratios, or in other words, the preserved weights is more essential for fast fine-tuning but less useful for significant pruning ratios. The authors may need to carefully made the conclusion from the observations. I would hope the authors can address these concerns in the future version. However, I think the paper is overall well-written and existing content is inspiring enough for readers to further explore the trainability of the pruned network. Therefore I raised my score to 7. ", "rating": "7: Good paper, accept", "reply_text": "For fine-tuning epochs , we have run experiments to show that more epochs don \u2019 t improve fine-tuning noticeably . This is because very small learning rate is used to preserve the inherited weights , as also mentioned in point 5 . The results are as follows : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Pruned Model Fine-tune-40 Fine-tune-80 Fine-tune-160 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- VGG-16 93.40 ( \u00b10.12 ) 93.45 ( \u00b10.06 ) 93.45 ( \u00b10.08 ) ResNet-56-A 92.97 ( \u00b10.17 ) 92.92 ( \u00b10.15 ) 92.94 ( \u00b10.16 ) ResNet-56-B 92.68 ( \u00b10.19 ) 92.67 ( \u00b10.14 ) 92.76 ( \u00b10.16 ) ResNet-110-A 93.14 ( \u00b10.16 ) 93.12 ( \u00b10.19 ) 93.04 ( \u00b10.22 ) ResNet-110-B 92.69 ( \u00b10.09 ) 92.75 ( \u00b10.15 ) 92.76 ( \u00b10.16 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- It can be seen that fine-tuning for more epochs gives negligible accuracy increase and sometimes small decrease . 9. # # Conclusion of Section 5 # # Yes , we agree with your points in 3 & 4 , and our experiments in Section 4 and 5 are to verify these points . We also agree that the conclusion of Section 5 may seem straightforward to some audience , but we think pruning is not very widely recognized as architecture search . Conventional network pruning and architecture search works still use totally different techniques , with the former focus on selecting important weights from a larger network and the later typically uses reinforcement learning or evolutionary algorithms to search an architecture through iterations . Pruning is usually mentioned as a model compression technique , in a resource-saving context , instead of being treated as an architecture search method . To our knowledge , our work is one of the first to draw a distinction between predefined and automatic pruning methods , and also one of the first to compare automatically pruned architecture with uniform pruning . Previous works compare pruned and fine-tuned model with the original large model , this is not sufficient to for `` pruning can be seen as architecture search '' : 1 . The benefit could be from the inherited weights , not the architecture . 2.Comparison with uniform pruning is missing . In Section 4 , we show that the performance of training from scratch can be on par with pruning , however , comparison with uniform pruning is still needed . In Section 5 , we break the tie between inherited weights and the resulting architecture , training the pruned architecture from scratch and comparing with uniform pruning . This provides further evidence that the value lies in searching efficient architecture . Also , in Section 5 , we have shown that we can transfer the sparsity pattern in the pruned model to a different architecture on a different dataset . This implies that in these cases , we don \u2019 t need to train a large model on the target dataset to find the efficient model and transferred design patterns can help us design an efficient model from scratch . This experiment was also not investigated in prior works and the conclusion is not obvious . We will include more results on this point in the revision . We will also add some discussions about the differences and similarities between pruning as architecture and conventional architecture search . We will mention that some previous works have made connections between pruning and architecture search as well . The main reason why we didn \u2019 t include figures with FLOPs as x-axis is mainly the space limit . We have included figures with FLOPs as x-axis in Section 2 of this anonymous pdf link ( https : //drive.google.com/open ? id=1BjGJQASV-CuGoq-nVErIRihHMdVwCZxl ) . Minor : Thank you for reminding , we will change the name to `` filter pruning '' as suggested . We will upload a revision after we address other reviewers ' concerns , and please advise on which part of this response you would like to see to be reflected in the revision ( other than the content we already plan to include ) . Again , thank you for your detailed review . If you have any further questions , we are happy to answer . [ 1 ] Channel Pruning for Accelerating Very Deep Neural Networks . He et al. , ICCV 2017 . [ 2 ] ThiNet : A Filter Level Pruning Method for Deep Neural Network Compression . Luo et al. , ICCV 2017 . [ 3 ] Learning Efficient Convolutional Networks through Network Slimming . Liu et al. , ICCV 2017 . [ 4 ] Pruning Filters for Efficient ConvNets . Li et al. , ICLR 2017 . [ 5 ] Learning both Weights and Connections for Efficient Neural Networks . Han et al. , NIPS 2015 . [ 6 ] Deep Compression : Compressing Deep Neural Networks with Pruning , Trained Quantization and Huffman Coding . Han et al. , ICLR 2016 . [ 7 ] Data-Driven Sparse Structure Selection for Deep Neural Networks . Huang et al. , ECCV 2018 . [ 8 ] Deep Residual Learning for Image Recognition . He et al. , CVPR 2016 . [ 9 ] Densely Connected Convolutional Networks . Liu et al. , CVPR 2017 ."}, {"review_id": "rJlnB3C5Ym-2", "review_text": "This paper shows through a set of experiments that the common belief that a large neural network trained, then pruned and fine-tuned performs better than another network that has the same size of the pruned one, but trained from scratch, is actually false. That is, a pruned network does not perform better than a network with the same dimensions but trained from scratch. Also, the authors consider that what is important for good performance is to know how many weights/filters are needed at each layer, while the actual values of the weights do not matter. Then, what happens in a standard large neural network training can be seen as an architecture search, in which the algorithm learns what is the right amount of weights for each layer. Pros: - If these results are generally true, then, most of the pruning techniques are not really needed. This is an important result. - If these results hold, there is no need for training larger models and prune them. Best results can be obtained by training from scratch the right architecture. - the intuition that the neural network pruning is actually performing architecture search is quite interesting. Cons: - It is still difficult to believe that most of the previous work and previous experiments (as in Zhu & Gupta 2018) are faulty. - Another paper with opposing results is [1]. There the authors have an explicit control experiment in which they evaluate the training of a pruned network with random initialization and obtain worse performance than when pruned and pruned and retrained with the correct initialization. - Soft pruning techniques as [2] obtain even better results than the original network. These approaches are not considered in the analysis. For instance, in their tab. 1, ResNet-56 pruned 30% obtained a gain of 0.19% while your ResNet-50 pruned 30% obtains a loss of 4.56 from tab. 2. This is a significant difference in performance. Global evaluation: In general, the paper is well written and give good insides about pruning techniques. However, considering the vast literature that contradicts this paper results, it is not easy to understand which results to believe. It would be useful to see if the authors can obtain good results without pruning also on the control experiment in [1]. Finally, it seems that the proposed method is worse than soft pruning. In soft pruning, we do not gain in training speed, but if the main objective is performance, it is a very relevant result and makes the claims of the paper weaker. Additional comments: - top pag.4: \"in practice, we found that increasing the training epochs within a reasonable range is rarely harmful\". If you use early stopping results should not be affected by the number of training epochs (if trained until convergence). [1] The Lottery Ticket Hypothesis: Finding Small, Trainable Neural Networks, Jonathan Frankle, Michael Carbin, arXiv2018 [2] Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks, Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, Yi Yang, arXiv 2018 ", "rating": "6: Marginally above acceptance threshold", "reply_text": "[ 1 ] The Lottery Ticket Hypothesis : Finding Small , Trainable Neural Networks , Jonathan Frankle , Michael Carbin , arXiv 2018. https : //openreview.net/forum ? id=rJl-b3RcF7 [ 2 ] Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks , Yang He , Guoliang Kang , Xuanyi Dong , Yanwei Fu , Yi Yang , arXiv 2018 . [ 3 ] To prune , or not to prune : exploring the efficacy of pruning for model compression . Zhu et al. , NIPS workshop 2017 . [ 4 ] Channel Pruning for Accelerating Very Deep Neural Networks . He et al. , ICCV 2017 . [ 5 ] ThiNet : A Filter Level Pruning Method for Deep Neural Network Compression . Luo et al. , ICCV 2017 . [ 6 ] Learning both Weights and Connections for Efficient Neural Networks . Han et al. , NIPS 2015 . [ 7 ] Pruning Filters for Efficient ConvNets . Li et al. , ICLR 2017 . [ 8 ] https : //github.com/he-y/soft-filter-pruning [ 9 ] Data-Driven Sparse Structure Selection for Deep Neural Networks . Huang et al. , ECCV 2018 . [ 10 ] Learning Efficient Convolutional Networks through Network Slimming . Liu et al. , ICCV 2017 ."}], "0": {"review_id": "rJlnB3C5Ym-0", "review_text": "This paper proposes to investigate recent popular approaches to pruning networks, which have roots in works by Lecun \u201890, and are mostly rooted in a recent series of papers by Song Han (2015-2016). The methods proposed in these papers consist of the following pipeline: (i) train a neural network, (ii) then prune the weights, typically by trimming the those connections corresponding to weights with lowest magnitude, (iii) fine tune the resulting sparsely-connected neural network. The authors of the present work assert that traditionally, \u201ceach of the three stages is considered as indispensable\u201d. The authors go on to investigate the contribution of each step to the overall pipeline. Among their findings, they report that fine-tuning appears no better than training the resulting pruned network from scratch. The assertion then is that the important aspect of pruning is not that it identifies the \u201cimportant weights\u201d but rather that it identifies a useful sparse architecture. One problem here is that the authors may overstate the extent to which previous papers emphasize the fine-tuning, and they may understate the extent to which previous papers emphasize the learning of the architecture. Re-reading Han 2015, it seems clear enough that the key point is \u201clearning the connections\u201d (it\u2019s right there in the title) and that the \u201cimportant weights\u201d are a means to achieve this end. Moreover the authors may miss the actual point of fine-tuning. The chief benefit of fine-tuning is that it is faster than training from scratch at each round of retraining, so that even if it achieves the same performance as training from scratch, that\u2019s still a key benefit. In general, when making claims about other people\u2019s beliefs, the authors need to provide citations. References are not just about credit attribution but also about providing evidence and here that evidence is missing. I\u2019d like to see sweeping statements like \u201cThis is usually reported to be superior to directly training a smaller network from scratch\u201d supported by precise references, perhaps even a quote, to spare the reader some time. To this reader, the most interesting finding in the paper by far is surprisingly understated in the abstract and introduction, buried at the end of the paper. Here, the authors investigate what are the properties of the resulting sparse architectures that make them useful. They find that by looking at convolutional kernels from pruned architectures, they can obtain for each connection, a probability that a connection is \u201ckept\u201d. Using these probabilities, they can create new sparse architectures that match the sparsity pattern of the pruned architectures, a technique that they call \u201cguided sparsification\u201d. The method yields similar benefits to pruning. Note that while obtaining the sparsity patterns does require running a pruning algorithm in the first place, ***the learned sparsity patterns generalize well across architectures and datasets***. This result is interesting and useful, and to my knowledge novel. I think the authors should go deeper here, investigating the idea on yet more datasets and architectures (ImageNet would be nice). I also think that this result should be given greater emphasis and raised to the level of a major focal point of the paper. With convincing results and some hard-work to reshape the narrative to support this more important finding, I will consider revising my score. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "[ 1 ] Channel Pruning for Accelerating Very Deep Neural Networks . He et al. , ICCV 2017 . [ 2 ] ThiNet : A Filter Level Pruning Method for Deep Neural Network Compression . Luo et al. , ICCV 2017 . [ 3 ] Pruning Filters for Efficient ConvNets . Li et al. , ICLR 2017 . [ 4 ] Learning both Weights and Connections for Efficient Neural Networks . Han et al. , NIPS 2015 . [ 5 ] Learning Efficient Convolutional Networks through Network Slimming . Liu et al. , ICCV 2017 . [ 6 ] Data-Driven Sparse Structure Selection for Deep Neural Networks . Huang et al. , ECCV 2018 . [ 7 ] Deep Compression : Compressing Deep Neural Networks with Pruning , Trained Quantization and Huffman Coding . Han et al. , ICLR 2016 . [ 8 ] Pruning Convolutional Neural Networks for Resource Efficient Inference . Molchanov et al. , ICLR 2017 . [ 9 ] AutoPruner : An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference . Luo et al.arXiv , 2018 . [ 10 ] NISP : Pruning Networks using Neuron Importance Score Propagation . Yu et al. , CVPR 2018 . [ 11 ] \u201c Learning-Compression \u201d Algorithms for Neural Net Pruning . Carreira-Perpinan et al. , CVPR 2018 ."}, "1": {"review_id": "rJlnB3C5Ym-1", "review_text": "This paper reinvestigate several recent works on network pruning and find that the common belief about the necessity to train a large network before pruning may not hold. The authors find that training the pruned model from scratch can achieve similar, if not better, performance given enough time of training. Based on these observations, the author conclude that training a larger model followed by pruning is not necessary for obtaining an efficient model with similar performance. In other words, the pruned architecture is more important than the weights inherited from the large model. It reminds researchers to perform stronger baselines before showing complex pruning methods. The paper is well organized and written. It re-evaluate the recent progresses made on this topic. Instead of comparing approaches by simply using the numbers from previous paper, the authors perform extensive experiments to verify whether training the pruned network from scratch would work. The results are very interesting, it suggests the researchers to tune the baseline \u201chardly\u201d and stick to simple approach. However, here are some places that I have concerns with: 1. The two \u201ccommon beliefs\u201d actually state one thing, that is the weights of a pre-trained larger model can potentially help optimization for a smaller model. 2. I don\u2019t quite agree with that \u201ctraining\u201d is the first step of a pruning pipeline as illustrated in Figure 1. Actually the motivation or the common assumption for pruning is that there are already existing trained models (training is already finished) with good performance. If a trained model does not even exist, then one can certainly train various thin/smaller model from scratch as before, this is still a trial and error process. 3. \u201cThe value of pruning\u201d. The goal of pruning is to explore a \u201cthin\u201d or \u201cshallower\u201d version of it with similar accuracy while avoiding the exhaustive architecture search with heavy training processes. Thus the first value of pruning is to explore efficient architecture while avoiding heavy training. Therefore, it should be fast and efficient, ideally with no retraining or little fine-tuning. When the pruning method is too complex to implement or requires much more time than training from scratch, it could be an overkill and adds little value, especially when the performance is not better enough. Therefore, it is more informative if the authors would report the time/complexities for pruning/fine-tuning . 4. The second value of pruning lies at understand the redundancy of the model and providing insights for more efficient architecture designs. 5. Comparing to random initialization, pruning simply provide an initialization point inherited from the larger network. The essential question the author asked is whether a subset of pre-trained weights can outperform random initialization. This seems to be a common belief in transfer learning, knowledge distillation and the studies on initialization. The authors conclude that the accuracy of an architecture is determined by the architecture itself, but not the initialization. If this is true, training from scratch should have similar (but not better) result as fine-tuning a pruned model. As the inherited weights can also be viewed as a \u201crandom\u201d initialization. Both methods should reach equivalent good solution if they are trained with enough number of epochs. Can this be verified with experiments? 6. The experiments might not be enough to reject the common belief. The experiments only spoke that the pruned architectures can still be easily trained and encounter no difficulties during the optimization. One conjecture is that the pruned models in the previous work still have enough capacity for keeping good accuracy. What if the models are significantly pruned (say more than 70% of channels got pruned), is training from scratch still working well? It would add much value if the author can identify when training from scratch fails to match the performance obtained by pruning and fine-tuning. 7. In Section 4.1, \u201cscratch-trained models achieve at least the same level of accuracy as fine-tuned models\u201d. First, the ResNet-34-pruned A/B for this comparison does not have significant FLOPs reduction (10% and 24% FLOPs reduction). Fine-tuning still has advantage as it only takes \u00bc of training time compare to scratch-E. Second, it is interesting that fine-tuning has generally smaller variance than stratch-E (except VGG-19). Would this imply that fine-tuning a pruned model produce more stable result? It would be more complete if there is variance analysis for the imagenet result. 8. What is the training/fine-tuning hyperparameters used in section 4.1? Note that in the experiment of Li et al, 2017, scratch-E takes 164 epochs to train from scratch, while fine-tuning takes only 40 epochs. Like suggested above, if we fine-tune it with more epochs, would it achieve equivalent performance? Also, what is the hyperparameter used in scratch-E? Note that the original paper use batch size 128. If the authors adopts a smaller batch-size for scratch-E, then it has in more iterations and could certainly result in better performance according to recent belief that small batch-size generates better. 9. The conclusion of section 5 is not quite clear or novel. Using uniform pruning ratio for pruning is expected to perform worse than automatic pruning methods as it does not consider the importance difference of each layer and. This comes back to my point 3 & 4 about the value of pruning, that is the value of pruning lies at the analysis of the redundancy of the network. There are a number of works worked on analyzing the importance of different layers of filters. So I think the \u201chypothesis\u201d of \u201cthe value of automatic pruning methods actually lies in the resulting architecture rather than the inherited weight\u201d is kind of straightforward. Also, why not use FLOPs as x-axis in Figure 3? Minor: It might be more accurate to use \u201cL1-norm based Filter Pruning (Li et al., 2017)\u201d as literally \u201cchannels\u201d usually refers to feature maps, which are by-products of the model but not the model itself. I will revise my score if authors can address above concerns. --------- review after rebuttal---------- #1#2 It would be great if the authors can make it clear that training is not the always the first step and the value of pruning in introduction rather than mentioning in conclusion. Saving training time is still an important factor when training from scratch is expensive. #5 \u201cfine-tuning with enough epochs\u201d. I understand that the authors are mainly questioning about whether training from scratch is necessarily bad than pruning and fine-tuning. The author do find that \u201ctraining from scratch is better when the number of epochs is large enough\u201d. But we see that fine-tuning ResNet-56 A/B with 20 epochs does outperform (or is equivalent to) scratch training for the first 160 epochs, which validates \u201cfine-tuning is faster to converge\u201d. However, training 320 epochs (16x more comparing to 20 epochs fine-tuning and 2x comparing with normal training from scratch) is not quite coherent with the setting of \u201cscratch B\u201d, as ResNet-56 B just reduce 27% FLOPs. The other part of the question is still unclear, i.e., the author claimed that the accuracy of an architecture is determined by the architecture itself, but not the initialization, then both fine-tuning and scratch training should reach equivalent solution if they are well trained enough, regardless of the initialization or pruning method. The learning rate for scratch training is already well known (learning rate drop brings boost the accuracy). However, learning rate schedule for fine-tuning (especially for significantly pruned model as for reply#6) is not well explored. I wonder whether that a carefully tuned learning rate/hyperparameters for fine-tuning may get the same or better performance as scratch training. Questions: - Are both methods using the same learning rate schedule between epoch 160 and epoch 320? - The ResNets-56 A/B results in the reply#8 does not match the reported performance in reply#5. e.g., it shows 92.67(0.09) for ResNet-56-B with 40-epochs fine-tuning in reply5, but it turns out to be 92.68(\u00b10.19) in reply#8. - It would be great if the authors can add convergence curves for fine-tuning and scratch training for easier comparison. #6 The failure case for sparse pruning on ImageNet is interesting and it would be great to have the imageNet result reported and discussed. The authors find that \u201cwhen the pruned ratio is large enough, training from scratch is better by a even larger margin than fine-tuning\u201d. This could be due to following reasons: 1. When the pruning ratio is large, the pruned model with preserved weights is significantly different from the original model, and fine-tuning with small learning rate and limited number of epochs is not enough to recover the accuracy. As mentioned earlier, tuning the hyperparameters for fine-tuning based on pruning ratio might improve the performance of fine-tuning. 2. Though the pruning ratio is large, the model used in this experiment may still have large capacity to reach good performance. How about pruning ResNet-56 with significant pruning ratios? Finally, based on above observations, it seems to me that the preserved weights is more essential for fast fine-tuning but less useful for significant pruning ratios. -------- update ---------------- The authors addressed most of my concerns. Some questions are still remaining in my comment \u201cReview after rebuttal\u201d, specifically, fine-tuning a pruned network may still get good performance if the hyperparameters are carefully tuned based on the pruning ratios, or in other words, the preserved weights is more essential for fast fine-tuning but less useful for significant pruning ratios. The authors may need to carefully made the conclusion from the observations. I would hope the authors can address these concerns in the future version. However, I think the paper is overall well-written and existing content is inspiring enough for readers to further explore the trainability of the pruned network. Therefore I raised my score to 7. ", "rating": "7: Good paper, accept", "reply_text": "For fine-tuning epochs , we have run experiments to show that more epochs don \u2019 t improve fine-tuning noticeably . This is because very small learning rate is used to preserve the inherited weights , as also mentioned in point 5 . The results are as follows : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Pruned Model Fine-tune-40 Fine-tune-80 Fine-tune-160 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- VGG-16 93.40 ( \u00b10.12 ) 93.45 ( \u00b10.06 ) 93.45 ( \u00b10.08 ) ResNet-56-A 92.97 ( \u00b10.17 ) 92.92 ( \u00b10.15 ) 92.94 ( \u00b10.16 ) ResNet-56-B 92.68 ( \u00b10.19 ) 92.67 ( \u00b10.14 ) 92.76 ( \u00b10.16 ) ResNet-110-A 93.14 ( \u00b10.16 ) 93.12 ( \u00b10.19 ) 93.04 ( \u00b10.22 ) ResNet-110-B 92.69 ( \u00b10.09 ) 92.75 ( \u00b10.15 ) 92.76 ( \u00b10.16 ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- It can be seen that fine-tuning for more epochs gives negligible accuracy increase and sometimes small decrease . 9. # # Conclusion of Section 5 # # Yes , we agree with your points in 3 & 4 , and our experiments in Section 4 and 5 are to verify these points . We also agree that the conclusion of Section 5 may seem straightforward to some audience , but we think pruning is not very widely recognized as architecture search . Conventional network pruning and architecture search works still use totally different techniques , with the former focus on selecting important weights from a larger network and the later typically uses reinforcement learning or evolutionary algorithms to search an architecture through iterations . Pruning is usually mentioned as a model compression technique , in a resource-saving context , instead of being treated as an architecture search method . To our knowledge , our work is one of the first to draw a distinction between predefined and automatic pruning methods , and also one of the first to compare automatically pruned architecture with uniform pruning . Previous works compare pruned and fine-tuned model with the original large model , this is not sufficient to for `` pruning can be seen as architecture search '' : 1 . The benefit could be from the inherited weights , not the architecture . 2.Comparison with uniform pruning is missing . In Section 4 , we show that the performance of training from scratch can be on par with pruning , however , comparison with uniform pruning is still needed . In Section 5 , we break the tie between inherited weights and the resulting architecture , training the pruned architecture from scratch and comparing with uniform pruning . This provides further evidence that the value lies in searching efficient architecture . Also , in Section 5 , we have shown that we can transfer the sparsity pattern in the pruned model to a different architecture on a different dataset . This implies that in these cases , we don \u2019 t need to train a large model on the target dataset to find the efficient model and transferred design patterns can help us design an efficient model from scratch . This experiment was also not investigated in prior works and the conclusion is not obvious . We will include more results on this point in the revision . We will also add some discussions about the differences and similarities between pruning as architecture and conventional architecture search . We will mention that some previous works have made connections between pruning and architecture search as well . The main reason why we didn \u2019 t include figures with FLOPs as x-axis is mainly the space limit . We have included figures with FLOPs as x-axis in Section 2 of this anonymous pdf link ( https : //drive.google.com/open ? id=1BjGJQASV-CuGoq-nVErIRihHMdVwCZxl ) . Minor : Thank you for reminding , we will change the name to `` filter pruning '' as suggested . We will upload a revision after we address other reviewers ' concerns , and please advise on which part of this response you would like to see to be reflected in the revision ( other than the content we already plan to include ) . Again , thank you for your detailed review . If you have any further questions , we are happy to answer . [ 1 ] Channel Pruning for Accelerating Very Deep Neural Networks . He et al. , ICCV 2017 . [ 2 ] ThiNet : A Filter Level Pruning Method for Deep Neural Network Compression . Luo et al. , ICCV 2017 . [ 3 ] Learning Efficient Convolutional Networks through Network Slimming . Liu et al. , ICCV 2017 . [ 4 ] Pruning Filters for Efficient ConvNets . Li et al. , ICLR 2017 . [ 5 ] Learning both Weights and Connections for Efficient Neural Networks . Han et al. , NIPS 2015 . [ 6 ] Deep Compression : Compressing Deep Neural Networks with Pruning , Trained Quantization and Huffman Coding . Han et al. , ICLR 2016 . [ 7 ] Data-Driven Sparse Structure Selection for Deep Neural Networks . Huang et al. , ECCV 2018 . [ 8 ] Deep Residual Learning for Image Recognition . He et al. , CVPR 2016 . [ 9 ] Densely Connected Convolutional Networks . Liu et al. , CVPR 2017 ."}, "2": {"review_id": "rJlnB3C5Ym-2", "review_text": "This paper shows through a set of experiments that the common belief that a large neural network trained, then pruned and fine-tuned performs better than another network that has the same size of the pruned one, but trained from scratch, is actually false. That is, a pruned network does not perform better than a network with the same dimensions but trained from scratch. Also, the authors consider that what is important for good performance is to know how many weights/filters are needed at each layer, while the actual values of the weights do not matter. Then, what happens in a standard large neural network training can be seen as an architecture search, in which the algorithm learns what is the right amount of weights for each layer. Pros: - If these results are generally true, then, most of the pruning techniques are not really needed. This is an important result. - If these results hold, there is no need for training larger models and prune them. Best results can be obtained by training from scratch the right architecture. - the intuition that the neural network pruning is actually performing architecture search is quite interesting. Cons: - It is still difficult to believe that most of the previous work and previous experiments (as in Zhu & Gupta 2018) are faulty. - Another paper with opposing results is [1]. There the authors have an explicit control experiment in which they evaluate the training of a pruned network with random initialization and obtain worse performance than when pruned and pruned and retrained with the correct initialization. - Soft pruning techniques as [2] obtain even better results than the original network. These approaches are not considered in the analysis. For instance, in their tab. 1, ResNet-56 pruned 30% obtained a gain of 0.19% while your ResNet-50 pruned 30% obtains a loss of 4.56 from tab. 2. This is a significant difference in performance. Global evaluation: In general, the paper is well written and give good insides about pruning techniques. However, considering the vast literature that contradicts this paper results, it is not easy to understand which results to believe. It would be useful to see if the authors can obtain good results without pruning also on the control experiment in [1]. Finally, it seems that the proposed method is worse than soft pruning. In soft pruning, we do not gain in training speed, but if the main objective is performance, it is a very relevant result and makes the claims of the paper weaker. Additional comments: - top pag.4: \"in practice, we found that increasing the training epochs within a reasonable range is rarely harmful\". If you use early stopping results should not be affected by the number of training epochs (if trained until convergence). [1] The Lottery Ticket Hypothesis: Finding Small, Trainable Neural Networks, Jonathan Frankle, Michael Carbin, arXiv2018 [2] Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks, Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, Yi Yang, arXiv 2018 ", "rating": "6: Marginally above acceptance threshold", "reply_text": "[ 1 ] The Lottery Ticket Hypothesis : Finding Small , Trainable Neural Networks , Jonathan Frankle , Michael Carbin , arXiv 2018. https : //openreview.net/forum ? id=rJl-b3RcF7 [ 2 ] Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks , Yang He , Guoliang Kang , Xuanyi Dong , Yanwei Fu , Yi Yang , arXiv 2018 . [ 3 ] To prune , or not to prune : exploring the efficacy of pruning for model compression . Zhu et al. , NIPS workshop 2017 . [ 4 ] Channel Pruning for Accelerating Very Deep Neural Networks . He et al. , ICCV 2017 . [ 5 ] ThiNet : A Filter Level Pruning Method for Deep Neural Network Compression . Luo et al. , ICCV 2017 . [ 6 ] Learning both Weights and Connections for Efficient Neural Networks . Han et al. , NIPS 2015 . [ 7 ] Pruning Filters for Efficient ConvNets . Li et al. , ICLR 2017 . [ 8 ] https : //github.com/he-y/soft-filter-pruning [ 9 ] Data-Driven Sparse Structure Selection for Deep Neural Networks . Huang et al. , ECCV 2018 . [ 10 ] Learning Efficient Convolutional Networks through Network Slimming . Liu et al. , ICCV 2017 ."}}