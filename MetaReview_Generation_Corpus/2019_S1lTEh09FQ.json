{"year": "2019", "forum": "S1lTEh09FQ", "title": "Combinatorial Attacks on Binarized Neural Networks", "decision": "Accept (Poster)", "meta_review": "The paper provides a novel attack method and contributes to evaluating the robustness of neural networks with recently proposed defenses. The evaluation is convincing overall and the authors have answered most questions from the reviewers. We recommend acceptance. ", "reviews": [{"review_id": "S1lTEh09FQ-0", "review_text": "The authors study the problem of generating strong adversarial attacks on binarized neural networks (networks whose weights are binary valued and have a sign function nonlinearity). Since these networks are not continuous (due to the sign function nonlinearity), it is possible that standard gradient-based attack algorithms are not effective at producing adversarial examples. While this problem can be encoded as a mixed integer linear program, off-the-shelf MILP solvers are not scalable to larger/deeper networks. Thus, the authors propose a new target propagation style algorithm that attempts to infer desired activations at each layer (from the perspective of maximizing the adversary's objective) starting at the final layer and moving towards the input. The propagation at each layer requires solving another MILP (albeit a much smaller one). Further, in order to prevent the target propagation from discovering assignments at upper layers that are unachievable given the constraints at lower layers, the authors propose two heuristics (making small moves and penalizing deviations from the previous target values) to obtain an effective attack algorithm. The authors validate their approach experimentally on MNIST/Fashion MNIST image classifiers. Quality: The paper is reasonably well written and the key ideas are communicated well. However, the experimental section needs to be improved significantly. Clarity: The paper is easy to understand and organized well. Originality: The application of target propagation in the context of adversarial examples is certainly novel and so are the specific enhancements proposed in the context of adversarial example generation. The Significance: The study of adversarial examples for binarized networks is novel and important and effective attack generation algorithms are a significant first step towards training robust models of this type - this could enable deployment of robust and compact binarized classifiers in on-device settings (where model size is important). Cons My main concerns with this paper are regarding the experimental evaluation - I do not feel these are sufficient to justify the strength of the attack method proposed. Here are my broad concerns: 1. Even though the datasets used are small (MNIST/Fashion MNIST), the experimental validation of adversarial attacks is only performed on 100 test examples. This is not sufficiently representative (given experimental evidence with adversarial attacks on non-binarized models) and this needs to be addressed for the results to be considered conclusive. 2. The attack method is only compared to FSGM, which is known to be a rather poor attack even on non-binarized networks. The authors should compare to stronger gradient based attacks (like PGD) and gradient free attacks which have been used to break adversarial defenses that are nondifferentiable in prior work - https://arxiv.org/abs/1802.00420 and https://arxiv.org/abs/1802.05666). Further, the MILP approach used can be strengthened by doing better bound propagation (like in https://arxiv.org/pdf/1711.00455.pdf) 3. The attack radii used are very small compared to what has been used in non-binarized networks, where networks have been trained to even be verifiably robust to adversarial pertrubations of much larger radii (see for example https://arxiv.org/pdf/1805.12514.pdf). Given the existence of this work, it is important to evaluate the algorithms proposed on larger radii (since it is possible to construct non-binarized networks that are indeed robust to perburbations of eps=.1-.3 on MNIST). 4. Motivation for binarization: I assume that motivation for binarized models arising from faster training/inference times and smaller model sizes. However, to justify this, the authors need to compare their BNNs to comparable non-binarized neural networks (for example,ones that are similar in terms of number of bits used to represent the model) on training time, inference time and adversarial robustness. Otherwise, it seems hard to see why binarized networks are valuable from a robustness. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the detailed comments - we believe most of your concerns are clarified below . In particular , our FGSM is the same as the PGD you refer to , as we explain below . 1.We are currently running the same experiments reported in the paper on a much larger set of test images , and will report the updated results as soon as they become available . 2.- Regarding PGD : Thanks for raising this point - we already use PGD and will clarify this in writing . In our paper , FGSM refers to \u201c iterated FGSM \u201d or \u201c multi-step FGSM \u201d or PGD ( these are all referring to the same method , e.g.see page 4 of https : //arxiv.org/abs/1706.06083 ) . We make that clear in section 2 : \u201c Soon thereafter , an iterative variant of FGSM was shown to produce much more effective attacks ( Kurakin et al. , 2016 ) ; it is this version of FGSM that we will compare against in this work. \u201d . In fact , we run iterated FGSM/PGD for 3 minutes ( same as MIP and IProp ) with random restarts every 100 iterations . This provides FGSM with the same computational budget as IProp . We will update the paper to clarify this point in the experiments section . - Regarding gradient-free attacks : Thanks for bringing those papers to our attention . The first paper ( https : //arxiv.org/abs/1802.00420 ) proposes a method that uses the straight-through estimator to approximate gradients of a non-differentiable network ; this is indeed the same trick used for FGSM/PGD on BNNs , and so our comparison with PGD already covers the method BPDA proposed in the paper . Regarding the second paper ( https : //arxiv.org/pdf/1802.05666.pdf ) , we are now implementing it and will report on results as soon as they become available . - Regarding bound propagation : indeed , we already do perform bound propagation since the input images are bounded in a small epsilon-box ; the reported MIP results already use bound propagation . We will explicitly mention this in the updated paper . 3.Thank you for the reference to this recent paper . We will consider these additional experiments . 4.The point you raise relates to BNNs in general , rather than to our particular work . BNNs are amenable to fast hardware implementations as in the papers [ a-c ] , which are much harder to achieve for non-binarized networks . As such , we believe it is important to study the robustness of BNNs to attacks , regardless of whether there exists robust non-binarized counterparts of similar size . [ a ] Liang , Shuang , et al . `` FP-BNN : Binarized neural network on FPGA . '' Neurocomputing 275 ( 2018 ) : 1072-1086 . [ b ] McDanel , Bradley , Surat Teerapittayanon , and H. T. Kung . `` Embedded binarized neural networks . '' arXiv preprint arXiv:1709.02260 ( 2017 ) . [ c ] Yang , Li , Zhezhi He , and Deliang Fan . `` A Fully Onchip Binarized Convolutional Neural Network FPGA Impelmentation with Accurate Inference . '' Proceedings of the International Symposium on Low Power Electronics and Design . ACM , 2018 ."}, {"review_id": "S1lTEh09FQ-1", "review_text": "This paper proposed a new attack algorithm based on MILP on binary neural networks. In addition to the full MILP formulation, the authors proposed an integer target propagation algorithm (IProp) to find adversarial examples by solving a smaller (instead of the full) MILP. The topic is important but the clarity should be improved. It is less clear when describing the Iprop algorithm. Questions: 1. Can IProp work for other architectures? It looks like the propagation steps work on only fully connected layers (or conv layers) with activation functions. Does it work for pooling layers? 2. The results in Figure 2 look weird and might be wrong: since MIP is the exact solution (green bar), how is it possible that the prediction flip rate of IProp larger than MIP? See top row figures where some red bars are larger than green bars. 3. Also, is the FGSM method comparing in Figure 2 operating on the approximate BNN as described in the related work? How does the performance of PGD (Madry etal) compared to IProp? 4. How are the big M parameters in equation 4 and 5 computed? Is the formulation eq (1) to (8) the same as that in Tjeng 2018? Since BNN is a special case of general neural networks. Please elaborate. 5. In Sec 2 related work, why \"there's no objective function\" for verification method? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for taking the time to review our paper . Our answers to your questions are numbered in the same order as your review : 1 . Yes , IProp does work for pooling layers , as the layer-to-layer satisfaction problem ( section 4.1 ) can be modified to compute a pooling transformation by adding constraints appropriately . For instance , max/mean pooling are easily implemented with linear inequalities and/or binary variables . 2. we discuss this point at length in section 5.2 , page 7 . The MIP solver fails to scale to the wider/deeper networks , and thus times out at the 3-minute cutoff . The final solution returned by MIP may thus be suboptimal , which results in green bars being smaller than red bars . 3 . ( same reply as to reviewer 1 ) Thanks for raising this point - we already use PGD and will clarify this in writing . In our paper , FGSM refers to \u201c iterated FGSM \u201d or \u201c multi-step FGSM \u201d or PGD ( these are all referring to the same method , e.g.see page 4 of https : //arxiv.org/abs/1706.06083 ) . We make that clear in section 2 : \u201c Soon thereafter , an iterative variant of FGSM was shown to produce much more effective attacks ( Kurakin et al. , 2016 ) ; it is this version of FGSM that we will compare against in this work. \u201d . In fact , we run iterated FGSM/PGD for 3 minutes ( same as MIP and IProp ) with random restarts every 100 iterations . This provides FGSM with the same computational budget as IProp . We will update the paper to clarify this point in the experiments section . 4. the big-M values are computed by simply bounding the a_ { 1 , j } variables at the first hidden layer , since the input image is in an epsilon-box . Then , those bounds are passed on to the h_ { 1 , j } variables , i.e.if the lower and upper bounds on a given a_ { 1 , j } are negative , then h_ { 1 , j } must be -1 . Those bounds on h_ { 1 , j } are then propagated to the a_ { 2 , j } variables , and so on and so forth . This procedure is simple and runs in time linear in the size of the network . We are happy to describe it in the paper , if the reviewer thinks that would be useful . Our formulation differs from that of Tjeng in that our constraints ( 4 ) , ( 5 ) and ( 7 ) encode the discrete sign activation function and the binary weights . 5. in Narodytska et al . ( 2018 ) , the goal is to prove that an input to a network can not be fooled with epsilon perturbations , or provide a counter-example to that . As such , they do not care about maximizing the difference between the incorrect class and the true class as we do . In other words , the verification problem in Narodytska et al . ( 2018 ) is a feasibility problem rather than an optimization problem , and so it does not have an explicit objective function ."}, {"review_id": "S1lTEh09FQ-2", "review_text": "This paper presents an algorithm to find adversarial attacks to binary neural networks. Binary neural networks uses sign functions as nonlinearities, making the network essentially discrete. Previous attempts at finding adversarial attacks for binary neural networks either rely on relaxation which cannot find very good adversarial examples, or calling a mixed integer linear programming (MILP) solver which doesn\u2019t scale. This paper proposes to decompose the problem and iteratively find desired representations layer by layer from the top to the input. This so called Integer Propagation (IProp) algorithm is more efficient than solving the full MILP as it solves much smaller MILP problems, one for each layer, thus each step can be solved relatively quickly. The authors then proposed a few more improvements to the IProp algorithm, including ways to do local adjustments to the solutions, and warming starting from an existing solution. Experiments on binary neural nets trained for MNIST and Fashion MNIST show the superiority of the proposed method over MILP and relaxation based algorithms. Overall I found the paper to be very clear and the proposed method is sound. I think combining ideas from discrete / combinatorial optimization with deep learning is an important research direction and can shed light on training and verifying models with discrete components, like the hard nonlinearities in the binary neural nets studied in this paper. In terms of the particular proposed approach, it is hard for me to imagine the blind IProp that does not take the input into account until the last layer is ever going to work. The small step size modifications make a lot more sense. Regarding the selection of the set S, in the paper the authors simply sampled elements to be in S uniformly, but it seems possible to make use of the information from the forward pass, and choose the hidden units that are the closed to reaching the desired activations. Would that be any better? A few minor comments: - when reporting warm start results, it would be good to also show the performance of the FGSM solution used for warm starting, in addition to the other two results shown in Figure 6 to have a more complete comparison - the hidden units h_{l,j} were formulated to be in {0, 1} in equation (7), but everywhere else in the paper they are assumed to be in {-1, +1}, which is not consistent and slightly confusing. Overall I think this is a solid paper and support accepting it for publication.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the positive comments and suggestions ! Regarding the set S : indeed , your suggestion is valid and we have tried it early on . We sampled neurons closer to the threshold ( zero ) with higher probability than others . We did not observe much improvement over uniform sampling at the time , and thus decided to stick with simple uniform sampling . Regarding warmstart results : that \u2019 s a great point ; we will do so in the final version of the paper . Regarding notation : thanks for catching that ; we will make the notation consistent throughout ."}], "0": {"review_id": "S1lTEh09FQ-0", "review_text": "The authors study the problem of generating strong adversarial attacks on binarized neural networks (networks whose weights are binary valued and have a sign function nonlinearity). Since these networks are not continuous (due to the sign function nonlinearity), it is possible that standard gradient-based attack algorithms are not effective at producing adversarial examples. While this problem can be encoded as a mixed integer linear program, off-the-shelf MILP solvers are not scalable to larger/deeper networks. Thus, the authors propose a new target propagation style algorithm that attempts to infer desired activations at each layer (from the perspective of maximizing the adversary's objective) starting at the final layer and moving towards the input. The propagation at each layer requires solving another MILP (albeit a much smaller one). Further, in order to prevent the target propagation from discovering assignments at upper layers that are unachievable given the constraints at lower layers, the authors propose two heuristics (making small moves and penalizing deviations from the previous target values) to obtain an effective attack algorithm. The authors validate their approach experimentally on MNIST/Fashion MNIST image classifiers. Quality: The paper is reasonably well written and the key ideas are communicated well. However, the experimental section needs to be improved significantly. Clarity: The paper is easy to understand and organized well. Originality: The application of target propagation in the context of adversarial examples is certainly novel and so are the specific enhancements proposed in the context of adversarial example generation. The Significance: The study of adversarial examples for binarized networks is novel and important and effective attack generation algorithms are a significant first step towards training robust models of this type - this could enable deployment of robust and compact binarized classifiers in on-device settings (where model size is important). Cons My main concerns with this paper are regarding the experimental evaluation - I do not feel these are sufficient to justify the strength of the attack method proposed. Here are my broad concerns: 1. Even though the datasets used are small (MNIST/Fashion MNIST), the experimental validation of adversarial attacks is only performed on 100 test examples. This is not sufficiently representative (given experimental evidence with adversarial attacks on non-binarized models) and this needs to be addressed for the results to be considered conclusive. 2. The attack method is only compared to FSGM, which is known to be a rather poor attack even on non-binarized networks. The authors should compare to stronger gradient based attacks (like PGD) and gradient free attacks which have been used to break adversarial defenses that are nondifferentiable in prior work - https://arxiv.org/abs/1802.00420 and https://arxiv.org/abs/1802.05666). Further, the MILP approach used can be strengthened by doing better bound propagation (like in https://arxiv.org/pdf/1711.00455.pdf) 3. The attack radii used are very small compared to what has been used in non-binarized networks, where networks have been trained to even be verifiably robust to adversarial pertrubations of much larger radii (see for example https://arxiv.org/pdf/1805.12514.pdf). Given the existence of this work, it is important to evaluate the algorithms proposed on larger radii (since it is possible to construct non-binarized networks that are indeed robust to perburbations of eps=.1-.3 on MNIST). 4. Motivation for binarization: I assume that motivation for binarized models arising from faster training/inference times and smaller model sizes. However, to justify this, the authors need to compare their BNNs to comparable non-binarized neural networks (for example,ones that are similar in terms of number of bits used to represent the model) on training time, inference time and adversarial robustness. Otherwise, it seems hard to see why binarized networks are valuable from a robustness. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the detailed comments - we believe most of your concerns are clarified below . In particular , our FGSM is the same as the PGD you refer to , as we explain below . 1.We are currently running the same experiments reported in the paper on a much larger set of test images , and will report the updated results as soon as they become available . 2.- Regarding PGD : Thanks for raising this point - we already use PGD and will clarify this in writing . In our paper , FGSM refers to \u201c iterated FGSM \u201d or \u201c multi-step FGSM \u201d or PGD ( these are all referring to the same method , e.g.see page 4 of https : //arxiv.org/abs/1706.06083 ) . We make that clear in section 2 : \u201c Soon thereafter , an iterative variant of FGSM was shown to produce much more effective attacks ( Kurakin et al. , 2016 ) ; it is this version of FGSM that we will compare against in this work. \u201d . In fact , we run iterated FGSM/PGD for 3 minutes ( same as MIP and IProp ) with random restarts every 100 iterations . This provides FGSM with the same computational budget as IProp . We will update the paper to clarify this point in the experiments section . - Regarding gradient-free attacks : Thanks for bringing those papers to our attention . The first paper ( https : //arxiv.org/abs/1802.00420 ) proposes a method that uses the straight-through estimator to approximate gradients of a non-differentiable network ; this is indeed the same trick used for FGSM/PGD on BNNs , and so our comparison with PGD already covers the method BPDA proposed in the paper . Regarding the second paper ( https : //arxiv.org/pdf/1802.05666.pdf ) , we are now implementing it and will report on results as soon as they become available . - Regarding bound propagation : indeed , we already do perform bound propagation since the input images are bounded in a small epsilon-box ; the reported MIP results already use bound propagation . We will explicitly mention this in the updated paper . 3.Thank you for the reference to this recent paper . We will consider these additional experiments . 4.The point you raise relates to BNNs in general , rather than to our particular work . BNNs are amenable to fast hardware implementations as in the papers [ a-c ] , which are much harder to achieve for non-binarized networks . As such , we believe it is important to study the robustness of BNNs to attacks , regardless of whether there exists robust non-binarized counterparts of similar size . [ a ] Liang , Shuang , et al . `` FP-BNN : Binarized neural network on FPGA . '' Neurocomputing 275 ( 2018 ) : 1072-1086 . [ b ] McDanel , Bradley , Surat Teerapittayanon , and H. T. Kung . `` Embedded binarized neural networks . '' arXiv preprint arXiv:1709.02260 ( 2017 ) . [ c ] Yang , Li , Zhezhi He , and Deliang Fan . `` A Fully Onchip Binarized Convolutional Neural Network FPGA Impelmentation with Accurate Inference . '' Proceedings of the International Symposium on Low Power Electronics and Design . ACM , 2018 ."}, "1": {"review_id": "S1lTEh09FQ-1", "review_text": "This paper proposed a new attack algorithm based on MILP on binary neural networks. In addition to the full MILP formulation, the authors proposed an integer target propagation algorithm (IProp) to find adversarial examples by solving a smaller (instead of the full) MILP. The topic is important but the clarity should be improved. It is less clear when describing the Iprop algorithm. Questions: 1. Can IProp work for other architectures? It looks like the propagation steps work on only fully connected layers (or conv layers) with activation functions. Does it work for pooling layers? 2. The results in Figure 2 look weird and might be wrong: since MIP is the exact solution (green bar), how is it possible that the prediction flip rate of IProp larger than MIP? See top row figures where some red bars are larger than green bars. 3. Also, is the FGSM method comparing in Figure 2 operating on the approximate BNN as described in the related work? How does the performance of PGD (Madry etal) compared to IProp? 4. How are the big M parameters in equation 4 and 5 computed? Is the formulation eq (1) to (8) the same as that in Tjeng 2018? Since BNN is a special case of general neural networks. Please elaborate. 5. In Sec 2 related work, why \"there's no objective function\" for verification method? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for taking the time to review our paper . Our answers to your questions are numbered in the same order as your review : 1 . Yes , IProp does work for pooling layers , as the layer-to-layer satisfaction problem ( section 4.1 ) can be modified to compute a pooling transformation by adding constraints appropriately . For instance , max/mean pooling are easily implemented with linear inequalities and/or binary variables . 2. we discuss this point at length in section 5.2 , page 7 . The MIP solver fails to scale to the wider/deeper networks , and thus times out at the 3-minute cutoff . The final solution returned by MIP may thus be suboptimal , which results in green bars being smaller than red bars . 3 . ( same reply as to reviewer 1 ) Thanks for raising this point - we already use PGD and will clarify this in writing . In our paper , FGSM refers to \u201c iterated FGSM \u201d or \u201c multi-step FGSM \u201d or PGD ( these are all referring to the same method , e.g.see page 4 of https : //arxiv.org/abs/1706.06083 ) . We make that clear in section 2 : \u201c Soon thereafter , an iterative variant of FGSM was shown to produce much more effective attacks ( Kurakin et al. , 2016 ) ; it is this version of FGSM that we will compare against in this work. \u201d . In fact , we run iterated FGSM/PGD for 3 minutes ( same as MIP and IProp ) with random restarts every 100 iterations . This provides FGSM with the same computational budget as IProp . We will update the paper to clarify this point in the experiments section . 4. the big-M values are computed by simply bounding the a_ { 1 , j } variables at the first hidden layer , since the input image is in an epsilon-box . Then , those bounds are passed on to the h_ { 1 , j } variables , i.e.if the lower and upper bounds on a given a_ { 1 , j } are negative , then h_ { 1 , j } must be -1 . Those bounds on h_ { 1 , j } are then propagated to the a_ { 2 , j } variables , and so on and so forth . This procedure is simple and runs in time linear in the size of the network . We are happy to describe it in the paper , if the reviewer thinks that would be useful . Our formulation differs from that of Tjeng in that our constraints ( 4 ) , ( 5 ) and ( 7 ) encode the discrete sign activation function and the binary weights . 5. in Narodytska et al . ( 2018 ) , the goal is to prove that an input to a network can not be fooled with epsilon perturbations , or provide a counter-example to that . As such , they do not care about maximizing the difference between the incorrect class and the true class as we do . In other words , the verification problem in Narodytska et al . ( 2018 ) is a feasibility problem rather than an optimization problem , and so it does not have an explicit objective function ."}, "2": {"review_id": "S1lTEh09FQ-2", "review_text": "This paper presents an algorithm to find adversarial attacks to binary neural networks. Binary neural networks uses sign functions as nonlinearities, making the network essentially discrete. Previous attempts at finding adversarial attacks for binary neural networks either rely on relaxation which cannot find very good adversarial examples, or calling a mixed integer linear programming (MILP) solver which doesn\u2019t scale. This paper proposes to decompose the problem and iteratively find desired representations layer by layer from the top to the input. This so called Integer Propagation (IProp) algorithm is more efficient than solving the full MILP as it solves much smaller MILP problems, one for each layer, thus each step can be solved relatively quickly. The authors then proposed a few more improvements to the IProp algorithm, including ways to do local adjustments to the solutions, and warming starting from an existing solution. Experiments on binary neural nets trained for MNIST and Fashion MNIST show the superiority of the proposed method over MILP and relaxation based algorithms. Overall I found the paper to be very clear and the proposed method is sound. I think combining ideas from discrete / combinatorial optimization with deep learning is an important research direction and can shed light on training and verifying models with discrete components, like the hard nonlinearities in the binary neural nets studied in this paper. In terms of the particular proposed approach, it is hard for me to imagine the blind IProp that does not take the input into account until the last layer is ever going to work. The small step size modifications make a lot more sense. Regarding the selection of the set S, in the paper the authors simply sampled elements to be in S uniformly, but it seems possible to make use of the information from the forward pass, and choose the hidden units that are the closed to reaching the desired activations. Would that be any better? A few minor comments: - when reporting warm start results, it would be good to also show the performance of the FGSM solution used for warm starting, in addition to the other two results shown in Figure 6 to have a more complete comparison - the hidden units h_{l,j} were formulated to be in {0, 1} in equation (7), but everywhere else in the paper they are assumed to be in {-1, +1}, which is not consistent and slightly confusing. Overall I think this is a solid paper and support accepting it for publication.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the positive comments and suggestions ! Regarding the set S : indeed , your suggestion is valid and we have tried it early on . We sampled neurons closer to the threshold ( zero ) with higher probability than others . We did not observe much improvement over uniform sampling at the time , and thus decided to stick with simple uniform sampling . Regarding warmstart results : that \u2019 s a great point ; we will do so in the final version of the paper . Regarding notation : thanks for catching that ; we will make the notation consistent throughout ."}}