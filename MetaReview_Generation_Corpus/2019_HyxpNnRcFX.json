{"year": "2019", "forum": "HyxpNnRcFX", "title": "Modulating transfer between tasks in gradient-based meta-learning", "decision": "Reject", "meta_review": "This paper is extending the meta-learning MAML method to the mixture case. Specifically, the global parameters of the method are now modeled as a mixture. The authors also derive the elaborate associated inference for this approach.\n\nThe paper is well written although Rev2 raises some presentation issues that can surely improve the quality of the paper, if addressed in depth. \n\nThe results do not convince any of the three reviewers. Rev3 asks for a clearer exposition of the results to increase convincingness. Rev2 and Rev1 also make similar comments.  \n\nRev1 also questions the motivation of the approach, although the other two reviewers seem to find the approach well motivated. Although it certainly helps to prove the motivation within a very tailored to the method application, the AC weighted the opinion of all reviewers and did not consider the paper to lack in the motivation aspect. \n\nThe reviewers were overall not very impressed with this paper and that does not seem to stem from lack of novelty or technical correctness. Instead, it seems that this work is rather inconclusive (or at least it is presented in an inconclusive manner): Rev1 says that the important questions (like trade-offs and other practical issues) are not answered, Rev2 suggests that maybe this paper is trying to address too much, and all three reviewers are not convinced by the experiments and derived insights. \n\nFinally, Rev2 points out some inherent caveats of the method; although they do not seem to be severe enough to undermine the overall quality of the approach, it would be instructive to have them investigated more thoroughly (even if not completely solving them).", "reviews": [{"review_id": "HyxpNnRcFX-0", "review_text": "This paper presents a mixture of hierarchical Bayesian models for meta-learning to modulate transfer between various tasks to be learned. A non-parametric variant is also developed to capture the evolution of a task distribution over time. These are very fundamental and important problems for meta-learning. However, while the proposed model appears to be interesting, the evaluation is less convincing. 1. The performance of few-shot classification on MiniImageNet is not comparable to the state of the art (Table 2, Table 1). Especially, by Table, the proposed model performs much worse than existing methods (50% vs 60%). More discussions and explanations on this experiment are clearly required. 2. A more systematic and realistic evaluation is necessary to justify the proposed method. As a method that aims to cope with heterogeneous or even evolving task distributions, it is expected to work well in practice and outperform those baselines that are designed for a single task distribution. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their comments . We are in agreement with the reviewer that both modulating transfer and ensuring robustness to a changing task distribution are important and timely problems in meta-learning . We respond below to specific comments but please also see the general `` response to all reviewers '' above . > `` The performance of few-shot classification on MiniImageNet is not comparable to the state of the art ( Table 2 , Table 1 ) ... More discussions and explanations on this experiment are clearly required . '' For the standard homogeneous miniImageNet benchmark , we would first like to refer the reviewer to the \u201c response to all reviewers \u201d where we emphasize that our primary goal is not necessarily to achieve state-of-the-art results on these traditional datasets , and , moreover , benchmarking on this dataset is difficult due to nonstandard practices . However , as reported in the paper at submission time , our model does achieve the highest 1-shot accuracy for comparable architectures . The reported higher accuracies in the lower half of Table 2 use different and significantly more powerful architectures . > `` A more systematic and realistic evaluation is necessary to justify the proposed method . As a method that aims to cope with heterogeneous or even evolving task distributions , it is expected to work well in practice and outperform those baselines that are designed for a single task distribution . '' We apologize for the confusion caused by the original version of this figure : Notably , what we represent in Figure 5 is the validation loss values for each task , on a logarithmic scale . Accordingly , Figure 5 confirms that our model presents a substantial improvement over MAML that justifies the added complexity of our method . We would also like to emphasize that Figure 6 , as well as Figure 5 , demonstrate task differentiation to a reasonable degree . Note that the total cluster responsibility reported in Figure 5 and Figure 6 is the sum of cluster responsibilities across the different tasks in a single mini-batch . Figure 5 shows that the spawned clusters were sufficiently differentiated ( and at most one type of task was assigned per component ) . In Figure 6 , at each moment , one or two clusters are assigned tasks ( from the minibatch of 4 tasks ) . In a later version , we will add a table with the final loss values , and we will present two figures for each experiment , one for the losses and one for the cluster responsibilities , to avoid further confusion . We are also working on a more visually informative and less overwhelming presentation of the cluster assignment probabilities per task to emphasize the capability of our approach to differentiate between tasks and spawn new clusters when needed , in a task-agnostic setting . We would welcome more specific comments from the reviewer on what would constitute a more systematic and realistic evaluation ."}, {"review_id": "HyxpNnRcFX-1", "review_text": "This paper proposes a mixture of MAMLs (Finn et al., 2017) by exploiting the interpretation of MAML as a hierarchical Bayesian model (Grant et al. 2018). They propose an EM algorithm for joint training of parameter initializations and assignment of tasks to initializations. They further propose a non-parametric approach to dynamically increase the capacity of the meta learner in continual learning problems. The proposed method is tested in a few-shot learning setup on miniImagenet, on a synthetic continual learning problem, and an evolutionary version of miniImagenet. [Strengths] + Modeling the initialization space is an open research question and the authors make a sound proposal to tackle this. + The extension to continual learning is particularly interesting, as current methods for avoiding catastrophic forgetting. inevitably saturate model parameters. By dynamically increasing the meta-learner's capacity, this approach can in principle bypass catastrophic forgetting. [Weaknesses] - There is nothing in the algorithm that prevents mode collapse, and the only thing breaking symmetry is random initialization. In fact, figure 5 and 6 suggest mode collapse occurs even in the non-parametric case. A closely related paper that may be of interest ( Kim et al., 2018, https://arxiv.org/abs/1806.03836 ) address this issue by using Stein Variational SGD. - Results on miniImagenet are not encouraging; the gains on MAML are small and similar methods that generalize MAML (Kim et al., 2018, Rusu et al., 2018) achieve significantly better performance. - Experiments on evolving tasks suggest the method is not able to capture task diversity. In the synthetic experiment (figure 5), the model suffers mode collapse when a sufficiently difficult task is introduced. Ultimately, it performs on par with MAML, despite having three times the capacity. Similarly, on the evolving miniImagenet dataset, figure 6 indicates there is no cluster differentiation across tasks. - The paper needs major polishing.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their comments . We respond below to specific comments below but please also see the general `` response to all reviewers '' above . > `` Results on miniImagenet are not encouraging ; the gains on MAML are small and similar methods that generalize MAML ( Kim et al. , 2018 , Rusu et al. , 2018 ) achieve significantly better performance . '' For the standard homogeneous miniImageNet benchmark , we would first like to refer the reviewer to the \u201c response to all reviewers \u201d where we emphasize that our primary goal is not necessarily to achieve state-of-the-art results on these traditional datasets , and , moreover , benchmarking on this dataset is difficult due to nonstandard practices . However , as reported in the paper at submission time , our model does achieve the highest 1-shot accuracy for comparable architectures . The reported higher accuracies in the lower half of Table 2 use different and significantly more powerful architectures . > `` There is nothing in the algorithm that prevents mode collapse , and the only thing breaking symmetry is random initialization\u2026 A closely related paper that may be of interest ( Kim et al. , 2018 , https : //arxiv.org/abs/1806.03836 ) address this issue by using Stein Variational SGD . '' tl ; dr : Auxiliary mode collapse penalties ( analogous to the repulsion term in BMAML [ Kim18 ] ) might not be appropriate for clustering in the stochastic setting . Regarding the use of Stein Variational Gradient Descent in Kim et al . [ Kim2018 ] : The second term in Eq . ( 1 ) represents a repulsive force which might deter mode collapse to some degree . However , their approach does not necessarily handle multimodality in the case of heterogeneous tasks better than our proposed approach with a similar number of particles ( to our number of components ) , as a small number of particles could still concentrate around one large mode and ignore the narrower ones . In particular , their repulsion term does not guarantee differentiation , nor do they investigate whether the phenomenon of mode collapse occurs in their experiments ( either with the repulsion term or with an ablation of the repulsion term ) . Regarding our method : We confirm the reviewer 's assessment that symmetry-breaking in the method described in the submission is only due to the random seeding of the cluster initializations . Using random initialization alone to break symmetry is a common practice in the clustering and latent mixture modelling literature due to its simplicity [ e.g. , Pen99 ] ; more sophisticated approaches , such as data-dependent initialization schemes , would be orthogonal to our approach . A complication regarding imposing auxiliary regularization during training to break symmetries is that we are working in the stochastic setting , where task assignments from previous batches are not kept in memory ; therefore , any such regularization terms must be evaluated per batch . However , it is difficult to impose a principled , batch-wise regularization term that encourages mode differentiation without making assumptions about the task distribution within a mini-batch . Since our meta-learning training formulation assumes tasks are sampled uniformly with replacement from a potentially non-stationary task distribution , it would be disadvantageous in the general case to make such assumptions . In particular , an artificial penalty to enforce differentiation of assignments within a batch could hinder information transfer between two ( somewhat similar ) tasks by forcing their assignments into two different clusters . This assumption also crucially falls apart in the case of evolutionary miniImagenet ( Figure 6 ) , where the tasks in a batch do share the same stylization , and therefore may benefit from being assigned to the same cluster . For these reasons , although it is straightforward to include a batch-wise regularization term ( one that , for example , penalizes the entropy of a categorical distribution , which would be analogous to the batch-wise repulsion term used in BMAML ) , we do not believe that this is appropriate for the general problem setting that we consider . Ideally , we want only to enforce/diminish transfer between tasks that share/lack similar properties , which is realized via our underlying probabilistic model ."}, {"review_id": "HyxpNnRcFX-2", "review_text": "Summary: This work tackles few-shot (or meta) learning, providing an extension of the gradient-based MAML method to using a mixture over global hyperparameters. Each task stochastically picks a mixture component, giving rise to task clustering. Stochastic EM is used for end-to-end learning, an algorithm that is L times more expensive than MAML, where L is the number of mixture components. There is also a nonparametric version, based on Dirichlet process mixtures, but a large number of approximations render this somewhat heuristic. Comparative results are presented on miniImageNet (5-way, 1-shot). These results are not near the state-of-the art anymore, and some of the state-of-art methods are simpler and faster than even MAML. If expensive gradient-based meta-learning methods are to be consider in the future, the authors have to provide compelling arguments why the additional computations pay off. - Quality: Paper is technically complex, but based on simple ideas. In the case of infinite mixtures, it is not clear what is done in the end in the experiments. Experimental results are rather poor, given state-of-the-art. - Clarity: The paper is not hard to understand. What is done, is done cleanly. - Originality: The idea of putting a mixture model on the global parameters is not surprising. Important questions, such as how to make this faster, are not addressed. - Significance: The only comparative results on miniImageNet are worse than the state-of-the-art by quite a margin (admittedly, the field moves fast here, but it is also likely these benchmarks are not all that hard). This is even though better performing methods, like Versa, are much cheaper to run While the idea of task clustering is potentially useful, and may be important in practical use cases, I feel the proposed method is simply just too expensive to run in order to justify mild gains. The experiments do not show benefits of the idea. State of the art results on miniImageNet 5-way, 1-shot, the only experiments here which compare to others, show accuracies better than 53: - Versa: https://arxiv.org/abs/1805.09921. Importantly, this method uses a simpler model (logistic regression head models) and is quite a bit faster than MAML, so much faster than what is proposed here - BMAML: https://arxiv.org/abs/1806.03836. This is also quite complex and expensive, compared to Versa, but provides good results. Other points: - You use a set of size N+M per task update. In your 5-way, 1-shot experiments, what is N and M? I'd guess N=5 (1 shot per class), but what is M? If N+M > 5, then I wonder why results are branded as 5-way, 1-shot, which to mean means that each update can use exactly 5 labeled points. Please just be exact in the main paper about what you do, and what main competitors do, in particular about the number of points to use in each task update. - Nonparametric extension via Dirichlet process mixture. This is quite elaborate, and uses further approximations (ICM, instead of Gibbs sampling). Can be seen as a heuristic to evolve the number of components. What is given in Algorithm 2, is not compatible with Section 4. How do you merge your Section 4 algorithm with stochastic EM? In Algorithm 2, how do you avoid that there is always one more (L -> L+1) components? Some threshold must be applied somewhere. An alternative would be to use split&merge heuristics for EM. - Results reported in Section 5 are potentially interesting, but entirely lack a reference point. The first is artificial, and surely does not need an algorithm of this complexity. The setup in Section 5.2 is potentially interesting, but needs more work, in particular a proper comparison to related work. This type of effort is needed to motivate an extension of MAML which makes everything quite a bit more expensive, and lacks behind the state-of-art, which uses amortized inference networks (Versa, neural processes) rather than gradient-based. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for an extensive and detailed review . We respond below to some specific comments but please also refer to the general `` response to all reviewers '' above . We encourage the reviewer to follow up with any other points that would improve the paper . > `` Stochastic EM is used for end-to-end learning , an algorithm that is L times more expensive than MAML , where L is the number of mixture components . '' > \u201c Important questions , such as how to make this faster , are not addressed \u201d > `` [ VERSA ] uses a simpler model ( logistic regression head models ) and is quite a bit faster than MAML , so much faster than what is proposed here\u2026 [ BMAML ] is also quite complex and expensive , compared to Versa , but provides good results . '' We will clarify in a revised version of the paper that this approach is easily parallelizable by assigning the computation of the MAP estimate \\hat { \\phi } , as well as the computation of the gradient with respect to the hyperparameter \\theta , to independent workers . Moreover , keeping the structure of the underlying probabilistic model fixed , our maximum a posteriori ( MAP ) procedure ( which directly optimizes the negative log posterior via gradient descent ) is the most straightforward approach to point estimation . In contrast , approaches like VERSA [ Gor18 ] that use a hyper network to compute task-specific parameters or approaches that make use of amortized inference require a heavily parameterized hyper/inference network in order to compute the task-specific parameter values . The training of this hyper/inference network imposes an additional computational cost , even though test-time computation/inference of task-specific parameters can be performed via a single feedforward pas . As such , these different approaches present alternative trade-offs in speed at training versus test time . One modelling change we did experiment with on the homogeneous miniImageNet benchmark was learning the mixture model only on the last layer of the neural network initialization . In this case , we achieved significant speedups : The runtime for L = 2 , \u2026 , 5 components was not much more than the original MAML runtime ; this variant of our method is therefore quite simple and fast . The corresponding drop in generalization performance from clustering all the layers was not substantial . However , in the submission , we focused on the extensible ( non ) parametric mixture modeling aspect for the full set of neural network weights to demonstrate the generality and scalability of our method . Therefore , we did not prioritize reporting such results within the space constraints but will add them in an updated version of the paper . We welcome further clarification on drawbacks related to the runtime of our method with respect to alternative approaches . Are there further details we can provide ?"}], "0": {"review_id": "HyxpNnRcFX-0", "review_text": "This paper presents a mixture of hierarchical Bayesian models for meta-learning to modulate transfer between various tasks to be learned. A non-parametric variant is also developed to capture the evolution of a task distribution over time. These are very fundamental and important problems for meta-learning. However, while the proposed model appears to be interesting, the evaluation is less convincing. 1. The performance of few-shot classification on MiniImageNet is not comparable to the state of the art (Table 2, Table 1). Especially, by Table, the proposed model performs much worse than existing methods (50% vs 60%). More discussions and explanations on this experiment are clearly required. 2. A more systematic and realistic evaluation is necessary to justify the proposed method. As a method that aims to cope with heterogeneous or even evolving task distributions, it is expected to work well in practice and outperform those baselines that are designed for a single task distribution. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their comments . We are in agreement with the reviewer that both modulating transfer and ensuring robustness to a changing task distribution are important and timely problems in meta-learning . We respond below to specific comments but please also see the general `` response to all reviewers '' above . > `` The performance of few-shot classification on MiniImageNet is not comparable to the state of the art ( Table 2 , Table 1 ) ... More discussions and explanations on this experiment are clearly required . '' For the standard homogeneous miniImageNet benchmark , we would first like to refer the reviewer to the \u201c response to all reviewers \u201d where we emphasize that our primary goal is not necessarily to achieve state-of-the-art results on these traditional datasets , and , moreover , benchmarking on this dataset is difficult due to nonstandard practices . However , as reported in the paper at submission time , our model does achieve the highest 1-shot accuracy for comparable architectures . The reported higher accuracies in the lower half of Table 2 use different and significantly more powerful architectures . > `` A more systematic and realistic evaluation is necessary to justify the proposed method . As a method that aims to cope with heterogeneous or even evolving task distributions , it is expected to work well in practice and outperform those baselines that are designed for a single task distribution . '' We apologize for the confusion caused by the original version of this figure : Notably , what we represent in Figure 5 is the validation loss values for each task , on a logarithmic scale . Accordingly , Figure 5 confirms that our model presents a substantial improvement over MAML that justifies the added complexity of our method . We would also like to emphasize that Figure 6 , as well as Figure 5 , demonstrate task differentiation to a reasonable degree . Note that the total cluster responsibility reported in Figure 5 and Figure 6 is the sum of cluster responsibilities across the different tasks in a single mini-batch . Figure 5 shows that the spawned clusters were sufficiently differentiated ( and at most one type of task was assigned per component ) . In Figure 6 , at each moment , one or two clusters are assigned tasks ( from the minibatch of 4 tasks ) . In a later version , we will add a table with the final loss values , and we will present two figures for each experiment , one for the losses and one for the cluster responsibilities , to avoid further confusion . We are also working on a more visually informative and less overwhelming presentation of the cluster assignment probabilities per task to emphasize the capability of our approach to differentiate between tasks and spawn new clusters when needed , in a task-agnostic setting . We would welcome more specific comments from the reviewer on what would constitute a more systematic and realistic evaluation ."}, "1": {"review_id": "HyxpNnRcFX-1", "review_text": "This paper proposes a mixture of MAMLs (Finn et al., 2017) by exploiting the interpretation of MAML as a hierarchical Bayesian model (Grant et al. 2018). They propose an EM algorithm for joint training of parameter initializations and assignment of tasks to initializations. They further propose a non-parametric approach to dynamically increase the capacity of the meta learner in continual learning problems. The proposed method is tested in a few-shot learning setup on miniImagenet, on a synthetic continual learning problem, and an evolutionary version of miniImagenet. [Strengths] + Modeling the initialization space is an open research question and the authors make a sound proposal to tackle this. + The extension to continual learning is particularly interesting, as current methods for avoiding catastrophic forgetting. inevitably saturate model parameters. By dynamically increasing the meta-learner's capacity, this approach can in principle bypass catastrophic forgetting. [Weaknesses] - There is nothing in the algorithm that prevents mode collapse, and the only thing breaking symmetry is random initialization. In fact, figure 5 and 6 suggest mode collapse occurs even in the non-parametric case. A closely related paper that may be of interest ( Kim et al., 2018, https://arxiv.org/abs/1806.03836 ) address this issue by using Stein Variational SGD. - Results on miniImagenet are not encouraging; the gains on MAML are small and similar methods that generalize MAML (Kim et al., 2018, Rusu et al., 2018) achieve significantly better performance. - Experiments on evolving tasks suggest the method is not able to capture task diversity. In the synthetic experiment (figure 5), the model suffers mode collapse when a sufficiently difficult task is introduced. Ultimately, it performs on par with MAML, despite having three times the capacity. Similarly, on the evolving miniImagenet dataset, figure 6 indicates there is no cluster differentiation across tasks. - The paper needs major polishing.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their comments . We respond below to specific comments below but please also see the general `` response to all reviewers '' above . > `` Results on miniImagenet are not encouraging ; the gains on MAML are small and similar methods that generalize MAML ( Kim et al. , 2018 , Rusu et al. , 2018 ) achieve significantly better performance . '' For the standard homogeneous miniImageNet benchmark , we would first like to refer the reviewer to the \u201c response to all reviewers \u201d where we emphasize that our primary goal is not necessarily to achieve state-of-the-art results on these traditional datasets , and , moreover , benchmarking on this dataset is difficult due to nonstandard practices . However , as reported in the paper at submission time , our model does achieve the highest 1-shot accuracy for comparable architectures . The reported higher accuracies in the lower half of Table 2 use different and significantly more powerful architectures . > `` There is nothing in the algorithm that prevents mode collapse , and the only thing breaking symmetry is random initialization\u2026 A closely related paper that may be of interest ( Kim et al. , 2018 , https : //arxiv.org/abs/1806.03836 ) address this issue by using Stein Variational SGD . '' tl ; dr : Auxiliary mode collapse penalties ( analogous to the repulsion term in BMAML [ Kim18 ] ) might not be appropriate for clustering in the stochastic setting . Regarding the use of Stein Variational Gradient Descent in Kim et al . [ Kim2018 ] : The second term in Eq . ( 1 ) represents a repulsive force which might deter mode collapse to some degree . However , their approach does not necessarily handle multimodality in the case of heterogeneous tasks better than our proposed approach with a similar number of particles ( to our number of components ) , as a small number of particles could still concentrate around one large mode and ignore the narrower ones . In particular , their repulsion term does not guarantee differentiation , nor do they investigate whether the phenomenon of mode collapse occurs in their experiments ( either with the repulsion term or with an ablation of the repulsion term ) . Regarding our method : We confirm the reviewer 's assessment that symmetry-breaking in the method described in the submission is only due to the random seeding of the cluster initializations . Using random initialization alone to break symmetry is a common practice in the clustering and latent mixture modelling literature due to its simplicity [ e.g. , Pen99 ] ; more sophisticated approaches , such as data-dependent initialization schemes , would be orthogonal to our approach . A complication regarding imposing auxiliary regularization during training to break symmetries is that we are working in the stochastic setting , where task assignments from previous batches are not kept in memory ; therefore , any such regularization terms must be evaluated per batch . However , it is difficult to impose a principled , batch-wise regularization term that encourages mode differentiation without making assumptions about the task distribution within a mini-batch . Since our meta-learning training formulation assumes tasks are sampled uniformly with replacement from a potentially non-stationary task distribution , it would be disadvantageous in the general case to make such assumptions . In particular , an artificial penalty to enforce differentiation of assignments within a batch could hinder information transfer between two ( somewhat similar ) tasks by forcing their assignments into two different clusters . This assumption also crucially falls apart in the case of evolutionary miniImagenet ( Figure 6 ) , where the tasks in a batch do share the same stylization , and therefore may benefit from being assigned to the same cluster . For these reasons , although it is straightforward to include a batch-wise regularization term ( one that , for example , penalizes the entropy of a categorical distribution , which would be analogous to the batch-wise repulsion term used in BMAML ) , we do not believe that this is appropriate for the general problem setting that we consider . Ideally , we want only to enforce/diminish transfer between tasks that share/lack similar properties , which is realized via our underlying probabilistic model ."}, "2": {"review_id": "HyxpNnRcFX-2", "review_text": "Summary: This work tackles few-shot (or meta) learning, providing an extension of the gradient-based MAML method to using a mixture over global hyperparameters. Each task stochastically picks a mixture component, giving rise to task clustering. Stochastic EM is used for end-to-end learning, an algorithm that is L times more expensive than MAML, where L is the number of mixture components. There is also a nonparametric version, based on Dirichlet process mixtures, but a large number of approximations render this somewhat heuristic. Comparative results are presented on miniImageNet (5-way, 1-shot). These results are not near the state-of-the art anymore, and some of the state-of-art methods are simpler and faster than even MAML. If expensive gradient-based meta-learning methods are to be consider in the future, the authors have to provide compelling arguments why the additional computations pay off. - Quality: Paper is technically complex, but based on simple ideas. In the case of infinite mixtures, it is not clear what is done in the end in the experiments. Experimental results are rather poor, given state-of-the-art. - Clarity: The paper is not hard to understand. What is done, is done cleanly. - Originality: The idea of putting a mixture model on the global parameters is not surprising. Important questions, such as how to make this faster, are not addressed. - Significance: The only comparative results on miniImageNet are worse than the state-of-the-art by quite a margin (admittedly, the field moves fast here, but it is also likely these benchmarks are not all that hard). This is even though better performing methods, like Versa, are much cheaper to run While the idea of task clustering is potentially useful, and may be important in practical use cases, I feel the proposed method is simply just too expensive to run in order to justify mild gains. The experiments do not show benefits of the idea. State of the art results on miniImageNet 5-way, 1-shot, the only experiments here which compare to others, show accuracies better than 53: - Versa: https://arxiv.org/abs/1805.09921. Importantly, this method uses a simpler model (logistic regression head models) and is quite a bit faster than MAML, so much faster than what is proposed here - BMAML: https://arxiv.org/abs/1806.03836. This is also quite complex and expensive, compared to Versa, but provides good results. Other points: - You use a set of size N+M per task update. In your 5-way, 1-shot experiments, what is N and M? I'd guess N=5 (1 shot per class), but what is M? If N+M > 5, then I wonder why results are branded as 5-way, 1-shot, which to mean means that each update can use exactly 5 labeled points. Please just be exact in the main paper about what you do, and what main competitors do, in particular about the number of points to use in each task update. - Nonparametric extension via Dirichlet process mixture. This is quite elaborate, and uses further approximations (ICM, instead of Gibbs sampling). Can be seen as a heuristic to evolve the number of components. What is given in Algorithm 2, is not compatible with Section 4. How do you merge your Section 4 algorithm with stochastic EM? In Algorithm 2, how do you avoid that there is always one more (L -> L+1) components? Some threshold must be applied somewhere. An alternative would be to use split&merge heuristics for EM. - Results reported in Section 5 are potentially interesting, but entirely lack a reference point. The first is artificial, and surely does not need an algorithm of this complexity. The setup in Section 5.2 is potentially interesting, but needs more work, in particular a proper comparison to related work. This type of effort is needed to motivate an extension of MAML which makes everything quite a bit more expensive, and lacks behind the state-of-art, which uses amortized inference networks (Versa, neural processes) rather than gradient-based. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for an extensive and detailed review . We respond below to some specific comments but please also refer to the general `` response to all reviewers '' above . We encourage the reviewer to follow up with any other points that would improve the paper . > `` Stochastic EM is used for end-to-end learning , an algorithm that is L times more expensive than MAML , where L is the number of mixture components . '' > \u201c Important questions , such as how to make this faster , are not addressed \u201d > `` [ VERSA ] uses a simpler model ( logistic regression head models ) and is quite a bit faster than MAML , so much faster than what is proposed here\u2026 [ BMAML ] is also quite complex and expensive , compared to Versa , but provides good results . '' We will clarify in a revised version of the paper that this approach is easily parallelizable by assigning the computation of the MAP estimate \\hat { \\phi } , as well as the computation of the gradient with respect to the hyperparameter \\theta , to independent workers . Moreover , keeping the structure of the underlying probabilistic model fixed , our maximum a posteriori ( MAP ) procedure ( which directly optimizes the negative log posterior via gradient descent ) is the most straightforward approach to point estimation . In contrast , approaches like VERSA [ Gor18 ] that use a hyper network to compute task-specific parameters or approaches that make use of amortized inference require a heavily parameterized hyper/inference network in order to compute the task-specific parameter values . The training of this hyper/inference network imposes an additional computational cost , even though test-time computation/inference of task-specific parameters can be performed via a single feedforward pas . As such , these different approaches present alternative trade-offs in speed at training versus test time . One modelling change we did experiment with on the homogeneous miniImageNet benchmark was learning the mixture model only on the last layer of the neural network initialization . In this case , we achieved significant speedups : The runtime for L = 2 , \u2026 , 5 components was not much more than the original MAML runtime ; this variant of our method is therefore quite simple and fast . The corresponding drop in generalization performance from clustering all the layers was not substantial . However , in the submission , we focused on the extensible ( non ) parametric mixture modeling aspect for the full set of neural network weights to demonstrate the generality and scalability of our method . Therefore , we did not prioritize reporting such results within the space constraints but will add them in an updated version of the paper . We welcome further clarification on drawbacks related to the runtime of our method with respect to alternative approaches . Are there further details we can provide ?"}}