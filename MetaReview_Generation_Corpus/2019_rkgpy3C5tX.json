{"year": "2019", "forum": "rkgpy3C5tX", "title": "Amortized Bayesian Meta-Learning", "decision": "Accept (Poster)", "meta_review": "This paper combines two ideas: MAML, and the hierarchical Bayesian inference approach of Amit and Meir (2018). The idea is fairly straightforward but well-motivated, and it seems to work well in practice.  The paper is well-written and includes good discussion of the relevant literature. The experiments show improvements on various tests of Bayesian inference, and include some good analysis beyond simply reporting better numbers.\n\nOn the whole, the reviewers are fairly positive about the paper. (While the numerical scores are slightly below the cutoff, the reviewers are more positive in the discussion.) The reviewers' main complaint is the lack of comparisons against recently published methods, especially Gordon et al. (2018). The lack of comparison to this paper doesn't strike me as a big problem; the preprint was released only a few months before the deadline, their approach was very different from the proposed one, and the proposed approach has some plausible advantages (simplicity, computational efficiency), so I don't think a direct comparison is required for acceptance.\n\nOverall, I recommend acceptance.\n", "reviews": [{"review_id": "rkgpy3C5tX-0", "review_text": "This work proposes an adaptation to MAML-type models that accounts for posterior uncertainty in task specific latent variables. This is achieved via a hierarchical Bayesian view of MAML, employing variational inference for the task-specific parameters. The key intuition of this paper is that one can perform fast and efficient test-time variational inference for the task-specific latent variables by learning a good initialization during meta-training. This is achieved in a very similar fashion to MAML, and allows for an interesting form of amortization of test-time inference. Pros: - For the most part, the approach presented is principled and well justified. - The motivation is clear: in the few-shot learning regime we expect to have little data to infer the task-specific latent variables, and so we should perform posterior inference to account for uncertainty. - The paper is well written, clear, and easy to follow. Cons (more details below): - It is not clear what the significant contributions of this paper are, as a number of methods have been proposed to account for uncertainty in the task-specific latent variables, and results for many of these methods appear to be better than those presented here. - Experimental section does compare to many of the existing related methods - There are some conceptual issues that need to be addressed by the authors. I enjoyed reading this paper, and I think the ideas and work presented are, for the most part, solid. However, I am not sure to what extent the novel contribution of this paper is significant. Several papers, including Grant et al. (2018), but going back to Heskes (2000), have proposed the hierarchical Bayesian view of meta-learning. Grant et al. (2018) used a Laplace approximation to learn in such a model with MAML-type settings, presenting a method that accounts for uncertainty in this family of models. More recently, Finn et al. (2018) and Kim et al. (2018) have done this in a variational manner, albeit with variations in the implementation details. Gordon et al. (2018) proposed a more general presentation, unifying the above works (and others) in a Bayesian framework that allows for different functional forms of posterior inference (both point estimates and distributional) of the task-specific parameters, including gradient based procedures. All of these papers have been publicly available for a few months at the time of submission, such that this view of meta-learning as (amortized) Bayesian inference is not novel. Here are some points that I would ask the authors to address during the rebuttal period: - The method presented in the paper does not account for the meta-training splits into query and test sets, other than to mention that these led to empirical performance gains (this is somewhat typical of probabilistic meta-learning papers). However, it not clear that this is justified from a probabilistic inference perspective, which would favour conditioning on all available data at inference time. Further, in the experimental section, the authors state that \"For the few-shot learning experiments, we found it necessary to downweight the inner KL term for better performance in our model\". Put together, it is not quite clear exactly what form of approximate inference is being conducted here. Can the authors comment on this? - I am not sure I agree with the authors' interpretation of the term \"amortized Bayesian inference\", at least in that it deviates from the way the term is typically used in the related literature. The method negates the need to maintain variational parameters for each latent variable, and approximate posterior inference for unseen tasks may be performed relatively efficiently, which is highly desirable. However, a gradient optimization procedure must still be performed for inference of task-specific variables for new tasks at test time. Thus, new variational parameters must be introduced and optimized at test time. It is true that by finding good global initializations the authors may drastically reduce the computational cost of the inference process, but this implies that the cost of inference at test time has been reduced, not fully amortized to a fixed cost (unless one fixes the number of gradient steps, which is a further deviation from variational inference and requires a prior of the form used in Grant et al. (2018)). Full amortization of inference for the task-specific variables is proposed by Garnelo et al. (2018) and Gordon et al. (2018), as well as Edwards and Storkey (2016), all of which employ inference networks mapping directly from the query sets to the variational parameters of the latent variables. In these cases, posterior inference of the latent variables for unseen tasks has the constant cost of a pass through an inference network, rather than several forward-backward passes, and does not require introducing new variational parameters to be optimized. Further, these methods negate the need for differentiating through gradient-based procedures at meta-training time, which is not avoided in this paper, but rather dealt with in the standard Hessian-vector product form. It would be highly useful in the paper (perhaps in the related work section) for the authors to conduct a more thorough comparison of their proposed method and the existing literature employing amortized inference for meta-learning, to put their work in context. I also have a number of concerns regarding the experimental section of the paper, which I find to be lacking both in details and the empirical comparison of the method to existing works. - The authors' cite recent works on meta-learning that take into account uncertainty in the local latent variables (e.g., Grant et al. (2018), Finn et al. (2018), Kim et al. (2018)), but do not compare to these methods. - Results from Garnelo et al. (2018) are not provided for the contextual bandits experiment. Their results seem to be comparable or better to those presented in this paper. Can the authors comment on this? - The same is true for the few-shot learning case, where MAML is the only method compared to, despite there being, at the time of submission, many papers which have significantly improved upon these results. - In terms of details, it is unclear how many gradient steps were taken at test time, and how this affects performance of the model. - In terms of accuracy, the proposed method appears to be under-performing significantly (i.e., below confidence bounds in almost all cases). - The statement \"...we believe improvements could be made with better variance reduction methods for stochastic gradients\" should, in my opinion, either be investigated or omitted from the paper. - In terms of uncertainty quantification, I find this experimental evaluation highly interesting. However, there is not a comparison to much of the existing work. The comparison to MAML is only of moderate interest in this case, as MAML is a deterministic method and is not expected to perform well in this regard. A comparison to Probabilistic or Bayesian MAML (at the least) would be more convincing if uncertainty calibration proved to be better for this method. Overall, the paper proposes a principled approach to performing approximate posterior inference for task specific latent variables in meta-learning settings. The paper is well-written, and the method is clearly derived. However, it is my impression that the paper does not make significant novel contributions to the existing research in (probabilistic) meta-learning, does not properly acknowledge all existing work (much of which covers the main ideas presented in the paper), has a number of conceptual issues that might need addressing, and its experimental section lacks evaluation and comparisons to the existing similar works. As the method is, for the most part, principled and well-derived, and the paper well written, I am willing to reconsider my overall score if the authors can demonstrate either (i) significant novelty or (ii) that this particular flavour of inference for the task-specific parameters provides significant benefits over existing approaches. [1] - T. Heskes. Empirical Bayes for learning to learn. 2000. [2] - E. Grant, C. Finn, S. Levine, T. Darrell, and T. Griffiths. Recasting gradient-based meta-learning as hierarchical Bayes. 2018. [3] - C. Finn, K. Xu, and S. Levine. Probabilistic model-agnostic meta-learning. 2018. [4] - T. Kim, J. Yoon, O. Dia, S. Kim, Y. Bengio, and S. Ahn. Bayesian model-agnostic meta-learning. 2018. [5] - J. Gordon, J. Bronskill, M. Bauer, S. Nowozin, and R. Turner. Decision-theoretic meta-learning: versatile and efficient amortization of few-shot learning. 2018. [6] - M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. Eslami, and Y. W. Teh. Neural processes. 2018. [7] - H. Edwards, and A. Storkey. Towards a neural statistician. 2016.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and detailed feedback ! We \u2019 ve addressed the points you talked about below . = > \u201c The method presented ... does not account for the meta-training splits into query and test sets\u2026 \u201d We are free to choose any parameterized procedure to produce an approximate posterior . Therefore , we can choose to condition on only the support set when computing the approximate posterior . It is true that conditioning on less information may give a looser lower bound whereas conditioning on all the data would make the bound tighter . However , we only care about obtaining a tight bound during training insofar as it allows computing a good approximate posterior from the support set at test time . Therefore , we choose to condition our approximate posterior on only the support set . We wish our model \u2019 s performance to generalize from training to test , and so we ensure the training and test conditions are similar . This is supported by empirical evidence - the model performs better during testing when the variational distribution is computed the same way during training and testing . Also one can view the objective in Eqn 5 ( specifically the part inside the brackets corresponding to each episode i ) as the KL between the approximate posterior conditioned on the support set and the true posterior over task-specific latent variables phi conditioned on the support & query sets and theta . This is similar to loss used in Bayesian MAML , which aims to minimize a dissimilarity function between an approximate posterior over task-specific parameters given the support set , and an approximate posterior over task-specific parameters given the support & query sets . The loss there is presented without derivation from a probabilistic inference perspective and justified by empirical performance . It \u2019 s interesting that our derived loss connects to theirs . = > \u201c Further ... the authors state that \u2018 For the few-shot learning experiments , we found it necessary to downweight the inner KL term for better performance in our model \u2019 ... \u201d Downweighting the KL term can be justified from a probabilistic perspective as accounting for confidence in the data ( controlled by label noise and # of examples ) . The weight on the KL term allows us to control the assumed noise in the observation model . We choose the weight on the KL term to maximize the validation performance . = > \u201c I am not sure I agree with the authors ' interpretation of the term \u2018 amortized Bayesian inference \u2019 ... \u201d We agree that our method could be characterized as \u201c semi-amortized \u201d , a la Semi-Amortized VAEs ( Kim et al ) . In practice , we do fix a small number of gradient steps , which effectively does mean that finding a task-specific posterior is a fixed cost . Kim , Yoon et al.Semi-Amortized Variational Autoencoders . 2018.= > \u201c It is true that by finding good global initializations the authors may drastically reduce the computational cost of the inference process , but this implies that the cost of inference at test time has been reduced , not fully amortized to a fixed cost ( unless one fixes the number of gradient steps , which is a further deviation from variational inference and requires a prior of the form used in Grant et al . ) . \u201d We disagree that the parameterized procedure we use ( with fixed number of gradient steps ) to produce an approximate posterior requires changing the structure of the prior . Automatic-differentiation based variational inference ( for example , the VAE ) is based on the idea that given a parametric differentiable procedure ( such as the forward pass through an encoder network ) that produces an approximate posterior , we can train the parameters of that procedure in order to to produce a better approximate posterior ( through maximizing the ELBO ) . Importantly , this does not require changing the structure of the prior and is valid for any prior we define - as long as we can estimate the KL divergence . Using gradient descent with a fixed number of steps from initial variational weights can be thought of analogously to using an encoder network . It could be possible that our method suffers from an amortization gap ( Cremer et al ) , as do encoder networks , because of using a fixed number of updates . However , we didn \u2019 t observe this in practice - we experimented with different amounts of steps and found diminishing returns after a certain point . Please do let us know if we have misunderstood what you meant and we \u2019 d be happy to discuss further . Cremer , Chris et al.Inference Suboptimality in Variational Autoencoders . 2018.= > \u201c Full amortization of inference for the task-specific variables is proposed by Garnelo et al and Gordon et al , as well as Edwards & Storkey , all of which employ inference networks mapping directly from the query sets to the variational parameters of the latent variables . In these cases , posterior inference of the latent variables for unseen tasks has the constant cost of a pass through an inference network , rather than several forward-backward passes ... \u201d [ cont . ]"}, {"review_id": "rkgpy3C5tX-1", "review_text": "The authors consider meta-learning to learn a prior over neural network weights. This is done via amortized variational inference. This means that a good initialisation of the variational parameters are learned across tasks, such that a good set of hyperparameters per task can be found in a few gradient steps. The proposed approach is evaluated on a toy and several popular benchmarks (like miniImagenet). The topic is timely. The contribution is modest, essentially applying the same idea as the one proposed in MAML to a variational objective, but well executed. The paper is relatively well-written and the contributions clearly stated/motivated. Section 2 and 3 could be written in a more compact way (in particular the math), but it does not harm the flow. The authors conducted a good set of experiments, but are missing comparisons Bayesian versions of MAML. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate your comments ! With regard to comparisons with bayesian versions of MAML , unfortunately , we could not find any source code available for these models . For the cifar-100 and mini-Imagenet experiments , we can not just take the results reported in these papers - our evaluation focuses on specific metrics used to quantify uncertainty so we would need to have the code for the models to calculate these metrics . However , taking the reviewers \u2019 feedback into account , we have worked on our own implementation of Probabilistic MAML and based on preliminary experiments on mini-Imagenet , it does appear that our model has better predictive uncertainty ( both in terms of calibration of the predictive distribution and confidence on out-of-distribution examples ) . Our implementation does not exactly reproduce the results from the Probabilistic MAML paper and so we are in touch with the authors trying to make sure our implementation matches theirs . We hope to add the results of these experiments to the paper by the end of the revision period . For the contextual bandit experiments , even with the source code , it is not straightforward to apply the aforementioned techniques . Probabilistic MAML does not maintain uncertainty in task-specific weights - all uncertainty in the posterior comes from uncertainty in global parameters which has fixed variance . Therefore , as experience is accumulated on a new task , it is not possible to compute a posterior for the task that will become more certain . This makes Probabilistic MAML inappropriate for the contextual bandit Thompson sampling setup . Bayesian MAML could be applied to the contextual bandit experiments ; however , it would likely require significant effort to tune the appropriate size of the ensemble ( as Bayesian MAML maintains an approximate posterior consisting of M different copies of the model ) and amount of parameter sharing so as to make experiments feasible and to have the appropriate amount of exploration . If M were too small , then Thompson sampling would provide very limited exploration . We would like to stress that our method is easy to apply to the problem because we can easily sample from our model \u2019 s approximate posterior and because the total number of parameters are only increased 2-fold ( weights and variances ) . We can calculate an approximate posterior over any commonly used network architecture for which we can compute gradients , making our method model-agnostic without introducing new hyperparameters such as the ensemble size or amount of parameter sharing which must be carefully tuned ."}, {"review_id": "rkgpy3C5tX-2", "review_text": "The authors proposed a meta-learning approach which amortizes hierarchical variational inference across tasks, learning an initial variational distribution such that, after a few steps of stochastic optimization with the reparametrization trick, they obtain a good task-specific approximate posterior. The optimization is performed by applying backpropagation through gradient updates. Experiments on a contextual bandit setting and on miniImage net show how the proposed approach can outperform a baseline based on the method MAML. Although in miniImagenet the proposed method does not produce gains in terms of accuracy, it does produce gains in terms of uncertainty estimation. Quality: The derivation of the proposed method is rigorous and well justified. The experiments performed show that the proposed method can result in gains. However, the comparison is only with respect to MAML and other techniques could have also be included to make it more meaningful. For example, Gordon, Jonathan, et al. \"Decision-Theoretic Meta-Learning: Versatile and Efficient Amortization of Few-Shot Learning.\" arXiv preprint arXiv:1805.09921 (2018). or the methods included in the related work section, or Garnelo et al. 2018. The authors do not comment on the computational cost of the proposed method. Clarity: The paper is clearly written and easy to read. Novelty: The proposed method is new up to my knowledge. This is one of the first methods to do Bayesian meta-learning. Significance: The experimental results show that the proposed method can produce gains. However, because the authors only compare with a non-Bayesian meta-learning method (MAML), it is not clear how significant the results are. Furthermore, the computational cost of the proposed method is described well enough.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your feedback ! We have addressed the two points you mentioned below . = > Baselines for comparison Please see our response to Reviewer 1 with regards to comparisons to Bayesian versions of MAML for cifar and mini-Imagenet . As mentioned , we have worked on our own implementation of Probabilistic MAML and based on preliminary experiments on mini-Imagenet , it does appear that our model has better predictive uncertainty . Our implementation does not exactly reproduce the results from the Probabilistic MAML paper and so we are in touch with the authors trying to make sure our implementation matches theirs . We hope to add the results of these experiments to the paper by the end of the revision period . For the contextual bandit experiments , even with the source code , it is not straightforward to apply the aforementioned techniques . Probabilistic MAML does not maintain uncertainty in task-specific weights - all uncertainty in the posterior comes from uncertainty in global parameters which has fixed variance . Therefore , as experience is accumulated on a new task , it is not possible to compute a posterior for the task that will become more certain . This makes Probabilistic MAML inappropriate for the contextual bandit Thompson sampling setup . Bayesian MAML could be applied to the contextual bandit experiments ; however , it would likely require significant effort to tune the appropriate size of the ensemble ( as Bayesian MAML maintains an approximate posterior consisting of M different copies of the model ) and amount of parameter sharing so as to make experiments feasible and to have the appropriate amount of exploration . If M were too small , then Thompson sampling would provide very limited exploration . Additionally , for Gordon et al , we have a similar issue , as the amortization network they use outputs the distribution of weights over the final layer ( whereas the rest of the network weights are shared ) and would require tinkering to get the appropriate network architecture to work in the contextual bandits setting . We would like to stress that our method is easy to apply to the problem because we can easily sample from our model \u2019 s approximate posterior and because the total number of parameters are only increased 2-fold ( weights and variances ) . We can calculate an approximate posterior over any commonly used network architecture for which we can compute gradients , making our method model-agnostic without introducing new hyperparameters such as the ensemble size or amount of parameter sharing which must be carefully tuned . As for Garnelo et al , we could not find details of their setup for the contextual bandit experiment ( such as the network architecture , how often to update the models in each trial , how many batches to use for each update , what optimizer to use , etc ) , which prevented a fair comparison of inference methods . We emailed the authors several times with these questions but received no response . If the reviewers wish , we are happy to include the results from Garnelo et al in the paper , with an asterisk indicating we do not know the design of their experiment and can not fairly compare MAML or our method to them because of the different hyperparameters we likely used . = > Computation cost of method The computation cost of our method is similar to MAML except for the fact we need to compute stochastic gradients in the inner loop . To reduce the variance of the stochastic gradients , we do the following ( as has been commonly done in previous work involving bayesian neural networks ) : a . Use fully-independent ( or close to fully-independent ) weight samples for each example in an episode . b.Average over multiple weight samples when computing the expectation . ( a ) is achieved using the local reparameterization trick for fully-connected layers and flipout for convolutional layers . Both of these methods increase the complexity of the forward pass by 2 because they require two weight multiplications ( or convolutions ) rather than one for normal fully-connected or convolutional layers . ( b ) is achieved by replicating the data . Because we are in the few-shot learning setting , we can simply replicate the episode data enough times to get different samples and average and the replicated data still fits in a forward pass on the GPU . Thus ( b ) doesn \u2019 t increase the time complexity too much because it corresponds to using a bigger batch of data for each episode while using the same amount of forward passes . For example , for the cifar-100 experiments , our model took 2.6 times as long to train than MAML on a single GPU . This is typical of the time tradeoff between training a bayesian vs non-bayesian deep network . We will add more details about the computational cost to a new revision ."}], "0": {"review_id": "rkgpy3C5tX-0", "review_text": "This work proposes an adaptation to MAML-type models that accounts for posterior uncertainty in task specific latent variables. This is achieved via a hierarchical Bayesian view of MAML, employing variational inference for the task-specific parameters. The key intuition of this paper is that one can perform fast and efficient test-time variational inference for the task-specific latent variables by learning a good initialization during meta-training. This is achieved in a very similar fashion to MAML, and allows for an interesting form of amortization of test-time inference. Pros: - For the most part, the approach presented is principled and well justified. - The motivation is clear: in the few-shot learning regime we expect to have little data to infer the task-specific latent variables, and so we should perform posterior inference to account for uncertainty. - The paper is well written, clear, and easy to follow. Cons (more details below): - It is not clear what the significant contributions of this paper are, as a number of methods have been proposed to account for uncertainty in the task-specific latent variables, and results for many of these methods appear to be better than those presented here. - Experimental section does compare to many of the existing related methods - There are some conceptual issues that need to be addressed by the authors. I enjoyed reading this paper, and I think the ideas and work presented are, for the most part, solid. However, I am not sure to what extent the novel contribution of this paper is significant. Several papers, including Grant et al. (2018), but going back to Heskes (2000), have proposed the hierarchical Bayesian view of meta-learning. Grant et al. (2018) used a Laplace approximation to learn in such a model with MAML-type settings, presenting a method that accounts for uncertainty in this family of models. More recently, Finn et al. (2018) and Kim et al. (2018) have done this in a variational manner, albeit with variations in the implementation details. Gordon et al. (2018) proposed a more general presentation, unifying the above works (and others) in a Bayesian framework that allows for different functional forms of posterior inference (both point estimates and distributional) of the task-specific parameters, including gradient based procedures. All of these papers have been publicly available for a few months at the time of submission, such that this view of meta-learning as (amortized) Bayesian inference is not novel. Here are some points that I would ask the authors to address during the rebuttal period: - The method presented in the paper does not account for the meta-training splits into query and test sets, other than to mention that these led to empirical performance gains (this is somewhat typical of probabilistic meta-learning papers). However, it not clear that this is justified from a probabilistic inference perspective, which would favour conditioning on all available data at inference time. Further, in the experimental section, the authors state that \"For the few-shot learning experiments, we found it necessary to downweight the inner KL term for better performance in our model\". Put together, it is not quite clear exactly what form of approximate inference is being conducted here. Can the authors comment on this? - I am not sure I agree with the authors' interpretation of the term \"amortized Bayesian inference\", at least in that it deviates from the way the term is typically used in the related literature. The method negates the need to maintain variational parameters for each latent variable, and approximate posterior inference for unseen tasks may be performed relatively efficiently, which is highly desirable. However, a gradient optimization procedure must still be performed for inference of task-specific variables for new tasks at test time. Thus, new variational parameters must be introduced and optimized at test time. It is true that by finding good global initializations the authors may drastically reduce the computational cost of the inference process, but this implies that the cost of inference at test time has been reduced, not fully amortized to a fixed cost (unless one fixes the number of gradient steps, which is a further deviation from variational inference and requires a prior of the form used in Grant et al. (2018)). Full amortization of inference for the task-specific variables is proposed by Garnelo et al. (2018) and Gordon et al. (2018), as well as Edwards and Storkey (2016), all of which employ inference networks mapping directly from the query sets to the variational parameters of the latent variables. In these cases, posterior inference of the latent variables for unseen tasks has the constant cost of a pass through an inference network, rather than several forward-backward passes, and does not require introducing new variational parameters to be optimized. Further, these methods negate the need for differentiating through gradient-based procedures at meta-training time, which is not avoided in this paper, but rather dealt with in the standard Hessian-vector product form. It would be highly useful in the paper (perhaps in the related work section) for the authors to conduct a more thorough comparison of their proposed method and the existing literature employing amortized inference for meta-learning, to put their work in context. I also have a number of concerns regarding the experimental section of the paper, which I find to be lacking both in details and the empirical comparison of the method to existing works. - The authors' cite recent works on meta-learning that take into account uncertainty in the local latent variables (e.g., Grant et al. (2018), Finn et al. (2018), Kim et al. (2018)), but do not compare to these methods. - Results from Garnelo et al. (2018) are not provided for the contextual bandits experiment. Their results seem to be comparable or better to those presented in this paper. Can the authors comment on this? - The same is true for the few-shot learning case, where MAML is the only method compared to, despite there being, at the time of submission, many papers which have significantly improved upon these results. - In terms of details, it is unclear how many gradient steps were taken at test time, and how this affects performance of the model. - In terms of accuracy, the proposed method appears to be under-performing significantly (i.e., below confidence bounds in almost all cases). - The statement \"...we believe improvements could be made with better variance reduction methods for stochastic gradients\" should, in my opinion, either be investigated or omitted from the paper. - In terms of uncertainty quantification, I find this experimental evaluation highly interesting. However, there is not a comparison to much of the existing work. The comparison to MAML is only of moderate interest in this case, as MAML is a deterministic method and is not expected to perform well in this regard. A comparison to Probabilistic or Bayesian MAML (at the least) would be more convincing if uncertainty calibration proved to be better for this method. Overall, the paper proposes a principled approach to performing approximate posterior inference for task specific latent variables in meta-learning settings. The paper is well-written, and the method is clearly derived. However, it is my impression that the paper does not make significant novel contributions to the existing research in (probabilistic) meta-learning, does not properly acknowledge all existing work (much of which covers the main ideas presented in the paper), has a number of conceptual issues that might need addressing, and its experimental section lacks evaluation and comparisons to the existing similar works. As the method is, for the most part, principled and well-derived, and the paper well written, I am willing to reconsider my overall score if the authors can demonstrate either (i) significant novelty or (ii) that this particular flavour of inference for the task-specific parameters provides significant benefits over existing approaches. [1] - T. Heskes. Empirical Bayes for learning to learn. 2000. [2] - E. Grant, C. Finn, S. Levine, T. Darrell, and T. Griffiths. Recasting gradient-based meta-learning as hierarchical Bayes. 2018. [3] - C. Finn, K. Xu, and S. Levine. Probabilistic model-agnostic meta-learning. 2018. [4] - T. Kim, J. Yoon, O. Dia, S. Kim, Y. Bengio, and S. Ahn. Bayesian model-agnostic meta-learning. 2018. [5] - J. Gordon, J. Bronskill, M. Bauer, S. Nowozin, and R. Turner. Decision-theoretic meta-learning: versatile and efficient amortization of few-shot learning. 2018. [6] - M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. Eslami, and Y. W. Teh. Neural processes. 2018. [7] - H. Edwards, and A. Storkey. Towards a neural statistician. 2016.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and detailed feedback ! We \u2019 ve addressed the points you talked about below . = > \u201c The method presented ... does not account for the meta-training splits into query and test sets\u2026 \u201d We are free to choose any parameterized procedure to produce an approximate posterior . Therefore , we can choose to condition on only the support set when computing the approximate posterior . It is true that conditioning on less information may give a looser lower bound whereas conditioning on all the data would make the bound tighter . However , we only care about obtaining a tight bound during training insofar as it allows computing a good approximate posterior from the support set at test time . Therefore , we choose to condition our approximate posterior on only the support set . We wish our model \u2019 s performance to generalize from training to test , and so we ensure the training and test conditions are similar . This is supported by empirical evidence - the model performs better during testing when the variational distribution is computed the same way during training and testing . Also one can view the objective in Eqn 5 ( specifically the part inside the brackets corresponding to each episode i ) as the KL between the approximate posterior conditioned on the support set and the true posterior over task-specific latent variables phi conditioned on the support & query sets and theta . This is similar to loss used in Bayesian MAML , which aims to minimize a dissimilarity function between an approximate posterior over task-specific parameters given the support set , and an approximate posterior over task-specific parameters given the support & query sets . The loss there is presented without derivation from a probabilistic inference perspective and justified by empirical performance . It \u2019 s interesting that our derived loss connects to theirs . = > \u201c Further ... the authors state that \u2018 For the few-shot learning experiments , we found it necessary to downweight the inner KL term for better performance in our model \u2019 ... \u201d Downweighting the KL term can be justified from a probabilistic perspective as accounting for confidence in the data ( controlled by label noise and # of examples ) . The weight on the KL term allows us to control the assumed noise in the observation model . We choose the weight on the KL term to maximize the validation performance . = > \u201c I am not sure I agree with the authors ' interpretation of the term \u2018 amortized Bayesian inference \u2019 ... \u201d We agree that our method could be characterized as \u201c semi-amortized \u201d , a la Semi-Amortized VAEs ( Kim et al ) . In practice , we do fix a small number of gradient steps , which effectively does mean that finding a task-specific posterior is a fixed cost . Kim , Yoon et al.Semi-Amortized Variational Autoencoders . 2018.= > \u201c It is true that by finding good global initializations the authors may drastically reduce the computational cost of the inference process , but this implies that the cost of inference at test time has been reduced , not fully amortized to a fixed cost ( unless one fixes the number of gradient steps , which is a further deviation from variational inference and requires a prior of the form used in Grant et al . ) . \u201d We disagree that the parameterized procedure we use ( with fixed number of gradient steps ) to produce an approximate posterior requires changing the structure of the prior . Automatic-differentiation based variational inference ( for example , the VAE ) is based on the idea that given a parametric differentiable procedure ( such as the forward pass through an encoder network ) that produces an approximate posterior , we can train the parameters of that procedure in order to to produce a better approximate posterior ( through maximizing the ELBO ) . Importantly , this does not require changing the structure of the prior and is valid for any prior we define - as long as we can estimate the KL divergence . Using gradient descent with a fixed number of steps from initial variational weights can be thought of analogously to using an encoder network . It could be possible that our method suffers from an amortization gap ( Cremer et al ) , as do encoder networks , because of using a fixed number of updates . However , we didn \u2019 t observe this in practice - we experimented with different amounts of steps and found diminishing returns after a certain point . Please do let us know if we have misunderstood what you meant and we \u2019 d be happy to discuss further . Cremer , Chris et al.Inference Suboptimality in Variational Autoencoders . 2018.= > \u201c Full amortization of inference for the task-specific variables is proposed by Garnelo et al and Gordon et al , as well as Edwards & Storkey , all of which employ inference networks mapping directly from the query sets to the variational parameters of the latent variables . In these cases , posterior inference of the latent variables for unseen tasks has the constant cost of a pass through an inference network , rather than several forward-backward passes ... \u201d [ cont . ]"}, "1": {"review_id": "rkgpy3C5tX-1", "review_text": "The authors consider meta-learning to learn a prior over neural network weights. This is done via amortized variational inference. This means that a good initialisation of the variational parameters are learned across tasks, such that a good set of hyperparameters per task can be found in a few gradient steps. The proposed approach is evaluated on a toy and several popular benchmarks (like miniImagenet). The topic is timely. The contribution is modest, essentially applying the same idea as the one proposed in MAML to a variational objective, but well executed. The paper is relatively well-written and the contributions clearly stated/motivated. Section 2 and 3 could be written in a more compact way (in particular the math), but it does not harm the flow. The authors conducted a good set of experiments, but are missing comparisons Bayesian versions of MAML. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate your comments ! With regard to comparisons with bayesian versions of MAML , unfortunately , we could not find any source code available for these models . For the cifar-100 and mini-Imagenet experiments , we can not just take the results reported in these papers - our evaluation focuses on specific metrics used to quantify uncertainty so we would need to have the code for the models to calculate these metrics . However , taking the reviewers \u2019 feedback into account , we have worked on our own implementation of Probabilistic MAML and based on preliminary experiments on mini-Imagenet , it does appear that our model has better predictive uncertainty ( both in terms of calibration of the predictive distribution and confidence on out-of-distribution examples ) . Our implementation does not exactly reproduce the results from the Probabilistic MAML paper and so we are in touch with the authors trying to make sure our implementation matches theirs . We hope to add the results of these experiments to the paper by the end of the revision period . For the contextual bandit experiments , even with the source code , it is not straightforward to apply the aforementioned techniques . Probabilistic MAML does not maintain uncertainty in task-specific weights - all uncertainty in the posterior comes from uncertainty in global parameters which has fixed variance . Therefore , as experience is accumulated on a new task , it is not possible to compute a posterior for the task that will become more certain . This makes Probabilistic MAML inappropriate for the contextual bandit Thompson sampling setup . Bayesian MAML could be applied to the contextual bandit experiments ; however , it would likely require significant effort to tune the appropriate size of the ensemble ( as Bayesian MAML maintains an approximate posterior consisting of M different copies of the model ) and amount of parameter sharing so as to make experiments feasible and to have the appropriate amount of exploration . If M were too small , then Thompson sampling would provide very limited exploration . We would like to stress that our method is easy to apply to the problem because we can easily sample from our model \u2019 s approximate posterior and because the total number of parameters are only increased 2-fold ( weights and variances ) . We can calculate an approximate posterior over any commonly used network architecture for which we can compute gradients , making our method model-agnostic without introducing new hyperparameters such as the ensemble size or amount of parameter sharing which must be carefully tuned ."}, "2": {"review_id": "rkgpy3C5tX-2", "review_text": "The authors proposed a meta-learning approach which amortizes hierarchical variational inference across tasks, learning an initial variational distribution such that, after a few steps of stochastic optimization with the reparametrization trick, they obtain a good task-specific approximate posterior. The optimization is performed by applying backpropagation through gradient updates. Experiments on a contextual bandit setting and on miniImage net show how the proposed approach can outperform a baseline based on the method MAML. Although in miniImagenet the proposed method does not produce gains in terms of accuracy, it does produce gains in terms of uncertainty estimation. Quality: The derivation of the proposed method is rigorous and well justified. The experiments performed show that the proposed method can result in gains. However, the comparison is only with respect to MAML and other techniques could have also be included to make it more meaningful. For example, Gordon, Jonathan, et al. \"Decision-Theoretic Meta-Learning: Versatile and Efficient Amortization of Few-Shot Learning.\" arXiv preprint arXiv:1805.09921 (2018). or the methods included in the related work section, or Garnelo et al. 2018. The authors do not comment on the computational cost of the proposed method. Clarity: The paper is clearly written and easy to read. Novelty: The proposed method is new up to my knowledge. This is one of the first methods to do Bayesian meta-learning. Significance: The experimental results show that the proposed method can produce gains. However, because the authors only compare with a non-Bayesian meta-learning method (MAML), it is not clear how significant the results are. Furthermore, the computational cost of the proposed method is described well enough.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your feedback ! We have addressed the two points you mentioned below . = > Baselines for comparison Please see our response to Reviewer 1 with regards to comparisons to Bayesian versions of MAML for cifar and mini-Imagenet . As mentioned , we have worked on our own implementation of Probabilistic MAML and based on preliminary experiments on mini-Imagenet , it does appear that our model has better predictive uncertainty . Our implementation does not exactly reproduce the results from the Probabilistic MAML paper and so we are in touch with the authors trying to make sure our implementation matches theirs . We hope to add the results of these experiments to the paper by the end of the revision period . For the contextual bandit experiments , even with the source code , it is not straightforward to apply the aforementioned techniques . Probabilistic MAML does not maintain uncertainty in task-specific weights - all uncertainty in the posterior comes from uncertainty in global parameters which has fixed variance . Therefore , as experience is accumulated on a new task , it is not possible to compute a posterior for the task that will become more certain . This makes Probabilistic MAML inappropriate for the contextual bandit Thompson sampling setup . Bayesian MAML could be applied to the contextual bandit experiments ; however , it would likely require significant effort to tune the appropriate size of the ensemble ( as Bayesian MAML maintains an approximate posterior consisting of M different copies of the model ) and amount of parameter sharing so as to make experiments feasible and to have the appropriate amount of exploration . If M were too small , then Thompson sampling would provide very limited exploration . Additionally , for Gordon et al , we have a similar issue , as the amortization network they use outputs the distribution of weights over the final layer ( whereas the rest of the network weights are shared ) and would require tinkering to get the appropriate network architecture to work in the contextual bandits setting . We would like to stress that our method is easy to apply to the problem because we can easily sample from our model \u2019 s approximate posterior and because the total number of parameters are only increased 2-fold ( weights and variances ) . We can calculate an approximate posterior over any commonly used network architecture for which we can compute gradients , making our method model-agnostic without introducing new hyperparameters such as the ensemble size or amount of parameter sharing which must be carefully tuned . As for Garnelo et al , we could not find details of their setup for the contextual bandit experiment ( such as the network architecture , how often to update the models in each trial , how many batches to use for each update , what optimizer to use , etc ) , which prevented a fair comparison of inference methods . We emailed the authors several times with these questions but received no response . If the reviewers wish , we are happy to include the results from Garnelo et al in the paper , with an asterisk indicating we do not know the design of their experiment and can not fairly compare MAML or our method to them because of the different hyperparameters we likely used . = > Computation cost of method The computation cost of our method is similar to MAML except for the fact we need to compute stochastic gradients in the inner loop . To reduce the variance of the stochastic gradients , we do the following ( as has been commonly done in previous work involving bayesian neural networks ) : a . Use fully-independent ( or close to fully-independent ) weight samples for each example in an episode . b.Average over multiple weight samples when computing the expectation . ( a ) is achieved using the local reparameterization trick for fully-connected layers and flipout for convolutional layers . Both of these methods increase the complexity of the forward pass by 2 because they require two weight multiplications ( or convolutions ) rather than one for normal fully-connected or convolutional layers . ( b ) is achieved by replicating the data . Because we are in the few-shot learning setting , we can simply replicate the episode data enough times to get different samples and average and the replicated data still fits in a forward pass on the GPU . Thus ( b ) doesn \u2019 t increase the time complexity too much because it corresponds to using a bigger batch of data for each episode while using the same amount of forward passes . For example , for the cifar-100 experiments , our model took 2.6 times as long to train than MAML on a single GPU . This is typical of the time tradeoff between training a bayesian vs non-bayesian deep network . We will add more details about the computational cost to a new revision ."}}