{"year": "2021", "forum": "c9-WeM-ceB", "title": "Saliency is a Possible Red Herring When Diagnosing Poor Generalization", "decision": "Accept (Poster)", "meta_review": "The reviewers all agreed on accepting this paper, stating that it makes a compelling point about the usefulness of saliency methods to diagnose generalization.  The reviewers found that the experiments were a strong point and applauded the thorough hyperparameter tuning and re-runs for statistical significance.  One reviewer commented that the paper was too dense with information, so much so as to make it difficult to digest.  However, overall this seems like an interesting paper that is relevant to the community and will hopefully foster some good discussion about the shortcomings and future directions of saliency methods.", "reviews": [{"review_id": "c9-WeM-ceB-0", "review_text": "The reviewed paper explores the relationship between the quality and spatial distribution of the saliency maps produced at inference time and the model 's generalization performance . The authors employed a number of existing methods as well as proposed and implemented their own technique ( ActDiff ) to align saliency maps with causally plausible regions . All methods were applied on synthetic and real-world data in a series of clever experiments , showing little correlation between saliency map spatial alignment and performance on unseen data . Overall , the paper seems to be technically sound , claims justified , and supported by the evidence presented in tables and figures . Importantly , the paper raises a very interesting point , challenging the status quo in the field . The manuscript is relatively easy to read and understand . For all these reasons , I vote for accepting . Major questions-concerns : * Figures 6 and 7 show mean saliency maps from randomly selected test images . Columns of these images show outputs from different algorithms used in this work . My question is why do saliency maps produced by methods such as Masked on test images show significant activity in the regions that were explicitly made useless for training ( by randomly shuffling pixels outside masks ) ? I believe this requires a more elaborate and explicit answer . * Perhaps it makes sense to openly recognize that before completely dismissing the validity of using saliency maps for diagnosing overfitting a lot more datasets must be studied as the two presented in the paper , may not be enough to provide conclusive evidence . Several minor comments and questions : * Some acronyms , e.g.PAC on the first page is not defined before being used . * Caption for Figure 1 can be made more clear , as there is no explanation of what `` pathology correlation with site/view '' means . It is only later in the text , the authors add that they have intentionally biased positive cases by sampling them mostly from either one site or one view . But before reading this part , the caption remains confusing for the reader . * It might be a good idea to reformulate contributions of the paper into nouns instead of verbs e.g.instead of `` Create a dataset '' - `` A dataset '' . * A sentence from the related work section , namely : `` Zhuang et al . ( 2019 ) was additionally designed ... '' should be reformulated . Updates : Thanks for the authors ' response . I believe this paper is valuable for the field and community and therefore I recommend this paper to be accepted .", "rating": "7: Good paper, accept", "reply_text": "> Figures 6 and 7 show mean saliency maps from randomly selected test images . Columns of these images show outputs from different algorithms used in this work . My question is why do saliency maps produced by methods such as Masked on test images show significant activity in the regions that were explicitly made useless for training ( by randomly shuffling pixels outside masks ) ? I believe this requires a more elaborate and explicit answer . Thank you for pointing this out . We have updated the text to explain what we believe to be going on here . Briefly , since the model is convolutional , during training , the model learns to build features from within the masks . During testing , any texture or shape learned from inside of the mask might also match local regions of the image outside of the mask . There is no restriction on the regions of the image that can be predicted from during test time . This is a failure of less sophisticated methods like masking . > Perhaps it makes sense to openly recognize that before completely dismissing the validity of using saliency maps for diagnosing overfitting a lot more datasets must be studied as the two presented in the paper , may not be enough to provide conclusive evidence . Yes we agree that these datasets are not sufficient to make a conclusive claim broadly , and we hope this work encouraged future research . We will make this limitation more explicit in the conclusions with a dedicated paragraph . > Some acronyms , e.g.PAC on the first page is not defined before being used . Thanks , we have fixed this . > Caption for Figure 1 can be made more clear , as there is no explanation of what `` pathology correlation with site/view '' means . It is only later in the text , the authors add that they have intentionally biased positive cases by sampling them mostly from either one site or one view . But before reading this part , the caption remains confusing for the reader . This figure has been moved to the experimental section of the article , which we think makes it easier to understand . > It might be a good idea to reformulate contributions of the paper into nouns instead of verbs e.g.instead of `` Create a dataset '' - `` A dataset '' . Thanks , good idea . > A sentence from the related work section , namely : `` Zhuang et al . ( 2019 ) was additionally designed ... '' should be reformulated . Thanks , we have done this ."}, {"review_id": "c9-WeM-ceB-1", "review_text": "Updated recommendation after major changes to the submission : thank you for addressing my comments . Short summary - The authors investigate the relationship between model generalization under distribution shifts and attribution techniques . They hypothesize that imposing \u201c better \u201d attributions in a model ( which they define as being more aligned to a mask selected by domain experts ) would increase model generalizability . However , they observe that such constraints hurt the performance under no shift , and do not necessarily lead to increased performance or \u201c better \u201d feature attribution maps under shift . Strengths -- This work investigates 3 datasets : one synthetic dataset where the effects are well-controlled for , as well as 2 manipulations of real-world data in medical imaging . It includes different and recent techniques to constraint model learning by masked representations , and does not make bold claims about the results . Weaknesses - There are however a few weaknesses that represent major concerns to me : 1 ) The main assumption underlying this work is that improving feature attributions will help generalizability . I however do not think that this is the message that was relayed in past publications on the topic ( contrarily to what is mentioned in the introduction ) and do not think that \u201c better \u201d attributions is sufficient for better generalizability . My reading on this topic is that attribution methods can help in highlighting confounding factors when models do not generalize under distribution shifts . Making this a sufficiency condition is a step that is not well-motivated to me . 2 ) The reason why I do not believe this assumption is valid is because attribution methods have been shown to not correctly represent model \u2019 s decisions and to be sensitive to different factors ( including shift-variance , Kindermans et al. , 2017 , or being susceptible to adversarial perturbations ) . In addition , different attribution methods will likely display different patterns . 3 ) Which is why I am wondering whether other attributions , which are more theoretically grounded ( e.g.integrated gradients [ Sundararajan et al. , 2017 ] or more recent gradient-based techniques like DeepLIFT , or from non-gradients based techniques like SHAP or occlusion ) were investigated . 4 ) The authors mention that masks represent a \u201c good \u201d attribution map . However , there is no guarantee that features in those masks are not affected by distribution shifts . This should be discussed as a limitation of mask-based regularizers . 5 ) The authors seem to have missed that the \u201c masked \u201d trained classifier also highlights the confounder on the synthetic data , despite it correctly predicting the outputs . I believe this is related to the choice of saliency maps , as \u201c raw \u201d gradients do not \u201c explain \u201d the decision of the model , i.e.it does not display the effect the features have between a bad or neutral decision and the current prediction . For this reason , I would suggest using integrated gradients or another technique . 6 ) The purpose of the study is quite vague and its results and conclusions lack of depth and reflexion . Novelty : The authors propose two novel methods to constrain models by using masked representations . While in theory they seem interesting ( e.g.activations have been successfully used for OOD detection ) , the results obtained vary per dataset and there is no clear advantage in terms of model generalization to be using their technique over other methods . I feel that the main assumption is not well-grounded , and hence , to me , novelty does not reside in the field tackled . Clarity : - Mainly , I found the paper well written and the experiments clear . I would suggest some proof-reading ( see minor comments , some repeated or missing words ) . Mostly I would suggest to revise the introduction as it wasn \u2019 t clear to me what the purpose of the study was until well into the experiments . I also found that the introduction is not well matched to the main message of the paper ( e.g.it does not mention mask-related constraints ) . Rigor : I found that the comparison between the proposed technique and the literature was well explained and sufficient . Hyper-parameters were described and confidence intervals were provided on all results . I enjoyed the fact that three datasets were included . Overall , I think the experiments were well executed . Detailed comments : -- - \u201c A critical assumption underlying these aforementioned works is that properties of the saliency map are indicative of generalization performance. \u201d I do not agree with that statement given my major concerns ( 1 ) . - Intro : masking is mentioned in the key contributions but not before and no justification is used . Such masking also assumes that the shifts across train and test distributions do not impact features in this mask . This seems like a strong hypothesis to me , especially when considering e.g.different imaging sites , or image resolutions . - It is unclear whether the goal is to obtain models that generalize better or whether it is to obtain feature attribution maps that are consistent with expert input . These two goals can be misaligned , as displayed in the results and should not be conflated . - actdiff method : can lambda be mentioned in the equation ? How would such a regularizer perform in a high dimensional layer ( curse of dimensionality ) ? Why use pre-activation outputs ? Were any other versions of this formulation tried ? - How were the masks defined ? Is there variability per sample ( e.g.different experts ) in their definition ? - The authors refer to the synthetic dataset as the changes in the chest x-ray but then have a separate synthetic dataset . The language is confusing and datasets should be defined before being referenced to . - Synthetic data results : the masked model performs best , ignoring the confounder . However , the saliency map reflects that the attributions are high for the confounder . Therefore , I do not see the same \u201c high correlation \u201d between IoU and AUC that the authors mention . In addition , this , to me , reflects the main limitation of attribution maps : they do not reflect the model \u2019 s decision . They rather reflect the local effect of a feature on the label ( Lipton , 2016 , Ancona et al. , 2018 ) . I am wondering why the authors select saliency maps compared to e.g.integrated gradients ( Sundararajan et al. , 2017 ) or other gradient-based but more mathematically grounded techniques . Given my own experience of attribution techniques , it is likely that using different attribution methods will lead to different conclusions with respect to the correlation between IoU and AUC . - I understand the choice in the setup of confounders for both datasets . However I am unsure how this represents real-world settings . For example , training sets might indeed be site-specific , but it would be surprising to me that the test presents the inverse of the confounder . We could for example expect the absence of that confounder ( which could be simulated by removing the confounder in the test set for synthetic data ) , or lower correlations between a feature and the label . - ActDiff substantially decreases model performance in the absence of confounders - If the goal is to obtain more generalizable models , other techniques could be envisaged when the relationship between label and confounder is not deterministic , like resampling or reweighting . Were these considered ? Minor : - Intro : PAC undefined - intro : the authors conflate the behavior of a model and which inductive biases it relies on , with the obtained saliency maps . This is however a complex and , in my opinion , unanswered question . - Figure 1 : NIH , PC , PA and AP not defined . SPC and VPC are presented succinctly without intuition and we only understand them much later . Maybe this figure should be moved in the results . - contributions : point 1 needs proof-reading - \u201c out of distribution feature attribution phenomena \u201d : I searched DeGrave et al for this term but could not find it . If not used elsewhere , I would rephrase as this is a confusing formulation : samples/images can be OOD , and these samples can provide feature attributions , but the feature attributions themselves are not OOD . - proof-reading of the text is required . Some explanations are poorly framed and can be rephrased ( e.g.related works , paragraph 2 ) . - Zeiler & Fergus , 2013 ( published at ECCV 2014 ) refers to the work on occlusion , where inputs are masked by a baseline value and the changes in predicted risk is defined as their attribution . While this technique provides one attribution per feature , it is not based on the gradients of the network . This paper also does not refer to the term \u201c saliency \u201d . - Formulas 4 and 5 , consistency in the formulations is desirable : if showing for binary classification in most cases , it is better to keep this set up for other formulations ( even if they could be extended to multiclass ) , especially as the experiments are run on binary classification . The limitation of GradMask to binary cases could however be mentioned . - Figure 5 can be difficult to investigate without colormaps . For instance , it looks to me like the \u201c masked \u201d training model does highlight the confounding factor .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for all the great points . We have performed new experiments which will be uploaded in a revised document early this week . We wanted to respond to your points now to ensure that these changes have satisfied your concerns with our paper . > 1.The main assumption underlying this work is that improving feature attributions will help generalizability . I however do not think that this is the message that was relayed in past publications on the topic ( contrarily to what is mentioned in the introduction ) and do not think that \u201c better \u201d attributions is sufficient for better generalizability . My reading on this topic is that attribution methods can help in highlighting confounding factors when models do not generalize under distribution shifts . Making this a sufficiency condition is a step that is not well-motivated to me . We shared the belief that saliency could help in highlighting confounding factors . However in this work we demonstrate models which do not indicate incorrect feature attribution yet fail to generalize dramatically ( in fact they look at the right area ) . Therefore our paper is now questioning this line of thinking . This impression that a good saliency map implies a good prediction is common in applied domains ( such as medical ) which is the reason we wrote this paper https : //www.nature.com/articles/s41598-019-42557-4 https : //www.nature.com/articles/s41591-020-0942-0 . We believe the evidence that we present in this paper will serve to help guide those researchers to evaluate models with more rigor . The idea that correcting models to have `` good '' saliency would yield good generalization is present in the ML literature as well [ RRR , https : //arxiv.org/abs/1906.10670 ] . > 2.The reason why I do not believe this assumption is valid is because attribution methods have been shown to not correctly represent model \u2019 s decisions [ ... & ] different attribution methods will likely display different patterns . Thank you for raising this important point . We maintain that this paper actually supports your claim , and provides experimental evidence in a real-world application that the assumption you describe is not valid . We performed this research to challenge that assumption to have a citable example that this is not valid . Your concern that Input gradients might not be representative of the model \u2019 s attributions is a good one , and to address this we have calculated both Integrated Gradients and Occlusion-based attribution maps for all experiments . In the main text , we present the Integrated Gradients images , and show all three methods side-by-side in the appendix . Interestingly , we found all methods gave similar but not identical attribution patterns . > 3.Which is why I am wondering whether other attributions , which are more theoretically grounded ( e.g.integrated gradients [ Sundararajan et al. , 2017 ] or more recent gradient-based techniques like DeepLIFT , or from non-gradients based techniques like SHAP or occlusion ) were investigated . We originally focused on input gradient in line with previous work [ GradMask , RRR ] , but have now also computed the integrated gradients and occlusion-based metrics for all experiments and included them in the Appendix Figures A3-7 and A13 for the overview plots ( now uploaded ) . We believe that this paper is now much stronger with these included . Thank you ! + Example Synthetic Images : https : //figshare.com/s/84ebabd78fe7d3b0b271 + Example XRay Images : https : //figshare.com/s/d1dc30ca2241853cf219 > 4 . The authors mention that masks represent a \u201c good \u201d attribution map . However , there is no guarantee that features in those masks are not affected by distribution shifts . This should be discussed as a limitation of mask-based regularizers . We agree and have added this in a limitations section . > 5.The authors seem to have missed that the \u201c masked \u201d trained classifier also highlights the confounder on the synthetic data , despite it correctly predicting the outputs . I believe this is related to the choice of saliency maps , as \u201c raw \u201d gradients do not \u201c explain \u201d the decision of the model [ ... ] I would suggest using integrated gradients or another technique . While it is true that the masked approach appears to highlight the confounder , it does also do a slightly better job at distributing attribution to the \u2018 + \u2019 symbols . Interestingly , we see the same behaviour for Input Gradients , Integrated Gradients , and the Occlusion approach . We have added experiments using different methods to generate saliency maps and present them side-by-side in the appendix , showing integrated gradients in the main text . > 6.The purpose of the study is quite vague and its results and conclusions lack of depth and reflexion . Aside from adding new experiments as you suggested , we have tried to clarify the purpose of this work , and expand upon the conclusions , by expanding both sections to make the purpose of the work clearer ."}, {"review_id": "c9-WeM-ceB-2", "review_text": "This paper addresses the potential correlation between saliency map values and model generalization ability in image analysis with a focus on medical image . The paper falls well within the scope of the conference . It is overall well written , but it is so crammed with information that , in order to comply with paper length limits , its 'digestion ' and interpretation becomes difficult at times ( this is clear in the case of images , whose meaning often has to be half-guessed due to lack of details ) The problem this study deals with is definitely important in the context of medical decision making on the basis of image , and the problems it pinpoints are more than real ( small sample sizes , inter-site heterogeneity , bias due to human intervention in the masking process , etc ) The proposal seems sound to me and the experiments convincing . My main qualm concerns their specificity , given that they address a very specific domain .", "rating": "7: Good paper, accept", "reply_text": "> This paper addresses the potential correlation between saliency map values and model generalization ability in image analysis with a focus on medical image . The paper falls well within the scope of the conference . It is overall well written , but it is so crammed with information that , in order to comply with paper length limits , its 'digestion ' and interpretation becomes difficult at times ( this is clear in the case of images , whose meaning often has to be half-guessed due to lack of details ) Thank you for your kind assessment of our manuscript ! We will take your suggestion to clarify the text as much as possible seriously . In particular , we will take advantage of the appendix to flesh out experimental details that are briefly described in the main text . > The problem this study deals with is definitely important in the context of medical decision making on the basis of image , and the problems it pinpoints are more than real ( small sample sizes , inter-site heterogeneity , bias due to human intervention in the masking process , etc ) The proposal seems sound to me and the experiments convincing . My main qualm concerns their specificity , given that they address a very specific domain . While we understand that we tackled the problem of medical imaging generalization specifically in this paper , the methods and problems addressed are applicable to any imaging application where there might exist a covariate shift between the training and test distributions , and our masking approaches could be used in those contexts . We hope the reader focuses less on the specifics of our masking approach , however , and focus more on the problem of relying on these attribution methods to diagnose model correctness . We have expanded the text to focus the attention on these findings ."}, {"review_id": "c9-WeM-ceB-3", "review_text": "Summary This paper focuses on the confounder problem that spatially-seperated image regions ( e.g.shoulders of xray images ) might spuriously correlated with the target ( e.g.pneumonia ) . If given a human-labeled region that is deemed important , we can decrease this spuriousness by regularizing the model toward the important region . They not only compare with several existing saliency-based methods ( RRR and GradMask ) , but also propose 2 new methods ( ActDiff , Adversarial ) inspired from domain adapataion literature that the representation of the classifier should be similar between original image and the masked image ( the image that the non-important region is shuffled ) . They compare in 1 synthetic dataset and 2 xray datasets . They show that ( 1 ) these methods ( sometimes ) hurt generalization when spuriousness does not exist , and ( 2 ) the model 's saliency map is only weakly correlated with generalization performance , and thus doubting the validtiy of using saliency maps for diagnosing whether a model is overfit to spurious features . Pros 1.Very detailed hyperparameter search and lots of repeated run ( 10 ) to get good standard deviation 2 . Visualizations of average test images are very intriguing . Major comments 1 . It seems that when there is covariate shift , the ActDiff performs the best but hurts the performance when no shift exists , but saliency-based methods do not suffer this . But given that you can tune the lambda of the regularization , why could it happen ? Ca n't you just pick lambda close to 0 ? 2.Some visualization looks suspiciously abnormal . For example , RRR in Figure 5 has a big blur in the middle . Why is that ? Also , in Figure 7 , Masked focus on the neck to predict but neck is clearly outside of the bounding box . How should we explain this phenomenon ? Besides , in Figure 7 upper rows ( VPC ) , Adversarial seems to have better masks by focusing on the lungs , and ActDiff does not . But their IOU is reversed : Adversarial has lower IOU and ActDiff has higher IOU in Table 2 . 3.In all the experiments , the validation set is always assumed correct without any background shift . While in real scenario it might not always be easy to access to a validation dataset without any shift . When we instead only have a biased validation dataset , which method will perform better ? Will the result change ? Minor Comments 1 . Some hyperparameters for visualization like Gaussian smoothing and thersholding might need justifications . Especially the maximum value capped in 50th percentile seems to be a bit excess . 2.When visualizing the gradients ( Fig.5 , 6 , 7 ) , maybe we should only include images that model predicts correctly ? The gradient of extremely wrong predictions might not be very meaningful . 3.The Figure 8 shows the scatter plot between Test AUC and IOU . It seems Synthetic and Xray SPC have much higher IOU overall . Maybe because their bounding box or confounder is location-wise fixed while only the RSNA VPC has varying bounding boxes ? 4.The masked baseline perform much worse in No VPC with AUC=0.5 which is random guessing . But it does not happen in other datasets . Maybe it 's because the VPC has smaller bounding box and thus only access to such region is too difficult ? 5.The lambda should be included in all the equations ( eq.1 to 5 ) .6.The final hyperparameter should be reported in the appendix . Overall evaluations Overall I like this paper . The hyperparameter tuning is very thorough . The conclusion is good that great saliency map does not mean better accuracy and vice versa . The experimental results are a bit unsatisfying that no real data is improved . And sometimes the simple masked baseline outperform others . I am happy to increase my score if my major concerns are addressed .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> 1.Some hyperparameters for visualization like Gaussian smoothing and thersholding might need justifications . Especially the maximum value capped in 50th percentile seems to be a bit excess . The gaussian sigma of 1 is small , especially for images with 224x224 resolution , and was done primarily to remove speckly noise from the attribution maps , which distract from the underlying pattern . The threshold was a more arbitrary choice , but we thought the 50th percentile would be easier for the reader to visualize : without this threshold , the anatomy is harder to see . For the values in the tables , we binarized at a percentile that matches the size of the supplied mask , not the 50th percentile , so the qualitative decision made for the figures does not impact the interpretation of the numbers in the tables . > 2.When visualizing the gradients ( Fig.5 , 6 , 7 ) , maybe we should only include images that model predicts correctly ? The gradient of extremely wrong predictions might not be very meaningful . Thank you for this helpful suggestion . We have generated figures for samples where A ) the model is correct , and B ) the model is incorrect on the test set , to determine how different these attributions are . These plots are found in the appendix A6 , computed using Integrated Gradients . > 3.The Figure 8 shows the scatter plot between Test AUC and IOU . It seems Synthetic and Xray SPC have much higher IOU overall . Maybe because their bounding box or confounder is location-wise fixed while only the RSNA VPC has varying bounding boxes ? Yes , you are correct . For the synthetic dataset , the task is extremely easy , so a high IOU isn \u2019 t surprising . The saliency maps are quite precise ( see Appendix A9 ) . Meanwhile for XRay SPC , the task is simply to predict away from the image border : one can imagine that many saliency maps that are simply focused away from the shoulder will perform well in this case . In contrast , we evaluated the RSNA images by their overlap with bounding boxes around the disease , which is variable across images . This is a harder task for the model to localize well . > 4.The masked baseline perform much worse in No VPC with AUC=0.5 which is random guessing . But it does not happen in other datasets . Maybe it 's because the VPC has smaller bounding box and thus only access to such region is too difficult ? This is reasonable and likely correct although we can \u2019 t know for sure . The masked task for the synthetic dataset is quite easy : the network only needs to learn two shapes ( + , - ) and with masked dataset , it learns how to count the + symbols during training . With the XRay dataset , the gross anatomy of the entire lung is available including the context around . Meanwhile , for the RSNA data , the model is only able to consistently see what is inside the lung , without context . The model is likely learning image textures that might appear elsewhere in the image , leading to bad generalization . > 5.The lambda should be included in all the equations ( eq.1 to 5 ) .We have done this in the updated PDF . > 6.The final hyperparameter should be reported in the appendix . We have done this in the Appendix . > Overall I like this paper . The hyperparameter tuning is very thorough . The conclusion is good that great saliency map does not mean better accuracy and vice versa . The experimental results are a bit unsatisfying that no real data is improved . And sometimes the simple masked baseline outperform others . I am happy to increase my score if my major concerns are addressed . Thank you for your thoughtful review . We would like to point out the following : where there exists a covariate shift between the training and test sets , the domain invariance approaches we introduce improve over the baseline for that experiment , which we believe is a valuable contribution . It is true that in the absence of a covariate shift , these methods do not help the model , but we also would not expect them to as the model would not be as likely to learn to use features that are only predictive in the training set distribution ( since the test set distribution is the same ) . Furthermore , we were equally surprised that the masked baseline performed so well on the X-Ray dataset , but perhaps this is less surprising because the masked region is far from the anatomy that determines the class , and it is also worth noting the high variance in performance of that method across seeds . It would be therefore hard to recommend the masked approach be used on datasets , especially when the masks are smaller ( note the far worse performance of the RSNA experiments ) . We have highlighted this in the text ."}], "0": {"review_id": "c9-WeM-ceB-0", "review_text": "The reviewed paper explores the relationship between the quality and spatial distribution of the saliency maps produced at inference time and the model 's generalization performance . The authors employed a number of existing methods as well as proposed and implemented their own technique ( ActDiff ) to align saliency maps with causally plausible regions . All methods were applied on synthetic and real-world data in a series of clever experiments , showing little correlation between saliency map spatial alignment and performance on unseen data . Overall , the paper seems to be technically sound , claims justified , and supported by the evidence presented in tables and figures . Importantly , the paper raises a very interesting point , challenging the status quo in the field . The manuscript is relatively easy to read and understand . For all these reasons , I vote for accepting . Major questions-concerns : * Figures 6 and 7 show mean saliency maps from randomly selected test images . Columns of these images show outputs from different algorithms used in this work . My question is why do saliency maps produced by methods such as Masked on test images show significant activity in the regions that were explicitly made useless for training ( by randomly shuffling pixels outside masks ) ? I believe this requires a more elaborate and explicit answer . * Perhaps it makes sense to openly recognize that before completely dismissing the validity of using saliency maps for diagnosing overfitting a lot more datasets must be studied as the two presented in the paper , may not be enough to provide conclusive evidence . Several minor comments and questions : * Some acronyms , e.g.PAC on the first page is not defined before being used . * Caption for Figure 1 can be made more clear , as there is no explanation of what `` pathology correlation with site/view '' means . It is only later in the text , the authors add that they have intentionally biased positive cases by sampling them mostly from either one site or one view . But before reading this part , the caption remains confusing for the reader . * It might be a good idea to reformulate contributions of the paper into nouns instead of verbs e.g.instead of `` Create a dataset '' - `` A dataset '' . * A sentence from the related work section , namely : `` Zhuang et al . ( 2019 ) was additionally designed ... '' should be reformulated . Updates : Thanks for the authors ' response . I believe this paper is valuable for the field and community and therefore I recommend this paper to be accepted .", "rating": "7: Good paper, accept", "reply_text": "> Figures 6 and 7 show mean saliency maps from randomly selected test images . Columns of these images show outputs from different algorithms used in this work . My question is why do saliency maps produced by methods such as Masked on test images show significant activity in the regions that were explicitly made useless for training ( by randomly shuffling pixels outside masks ) ? I believe this requires a more elaborate and explicit answer . Thank you for pointing this out . We have updated the text to explain what we believe to be going on here . Briefly , since the model is convolutional , during training , the model learns to build features from within the masks . During testing , any texture or shape learned from inside of the mask might also match local regions of the image outside of the mask . There is no restriction on the regions of the image that can be predicted from during test time . This is a failure of less sophisticated methods like masking . > Perhaps it makes sense to openly recognize that before completely dismissing the validity of using saliency maps for diagnosing overfitting a lot more datasets must be studied as the two presented in the paper , may not be enough to provide conclusive evidence . Yes we agree that these datasets are not sufficient to make a conclusive claim broadly , and we hope this work encouraged future research . We will make this limitation more explicit in the conclusions with a dedicated paragraph . > Some acronyms , e.g.PAC on the first page is not defined before being used . Thanks , we have fixed this . > Caption for Figure 1 can be made more clear , as there is no explanation of what `` pathology correlation with site/view '' means . It is only later in the text , the authors add that they have intentionally biased positive cases by sampling them mostly from either one site or one view . But before reading this part , the caption remains confusing for the reader . This figure has been moved to the experimental section of the article , which we think makes it easier to understand . > It might be a good idea to reformulate contributions of the paper into nouns instead of verbs e.g.instead of `` Create a dataset '' - `` A dataset '' . Thanks , good idea . > A sentence from the related work section , namely : `` Zhuang et al . ( 2019 ) was additionally designed ... '' should be reformulated . Thanks , we have done this ."}, "1": {"review_id": "c9-WeM-ceB-1", "review_text": "Updated recommendation after major changes to the submission : thank you for addressing my comments . Short summary - The authors investigate the relationship between model generalization under distribution shifts and attribution techniques . They hypothesize that imposing \u201c better \u201d attributions in a model ( which they define as being more aligned to a mask selected by domain experts ) would increase model generalizability . However , they observe that such constraints hurt the performance under no shift , and do not necessarily lead to increased performance or \u201c better \u201d feature attribution maps under shift . Strengths -- This work investigates 3 datasets : one synthetic dataset where the effects are well-controlled for , as well as 2 manipulations of real-world data in medical imaging . It includes different and recent techniques to constraint model learning by masked representations , and does not make bold claims about the results . Weaknesses - There are however a few weaknesses that represent major concerns to me : 1 ) The main assumption underlying this work is that improving feature attributions will help generalizability . I however do not think that this is the message that was relayed in past publications on the topic ( contrarily to what is mentioned in the introduction ) and do not think that \u201c better \u201d attributions is sufficient for better generalizability . My reading on this topic is that attribution methods can help in highlighting confounding factors when models do not generalize under distribution shifts . Making this a sufficiency condition is a step that is not well-motivated to me . 2 ) The reason why I do not believe this assumption is valid is because attribution methods have been shown to not correctly represent model \u2019 s decisions and to be sensitive to different factors ( including shift-variance , Kindermans et al. , 2017 , or being susceptible to adversarial perturbations ) . In addition , different attribution methods will likely display different patterns . 3 ) Which is why I am wondering whether other attributions , which are more theoretically grounded ( e.g.integrated gradients [ Sundararajan et al. , 2017 ] or more recent gradient-based techniques like DeepLIFT , or from non-gradients based techniques like SHAP or occlusion ) were investigated . 4 ) The authors mention that masks represent a \u201c good \u201d attribution map . However , there is no guarantee that features in those masks are not affected by distribution shifts . This should be discussed as a limitation of mask-based regularizers . 5 ) The authors seem to have missed that the \u201c masked \u201d trained classifier also highlights the confounder on the synthetic data , despite it correctly predicting the outputs . I believe this is related to the choice of saliency maps , as \u201c raw \u201d gradients do not \u201c explain \u201d the decision of the model , i.e.it does not display the effect the features have between a bad or neutral decision and the current prediction . For this reason , I would suggest using integrated gradients or another technique . 6 ) The purpose of the study is quite vague and its results and conclusions lack of depth and reflexion . Novelty : The authors propose two novel methods to constrain models by using masked representations . While in theory they seem interesting ( e.g.activations have been successfully used for OOD detection ) , the results obtained vary per dataset and there is no clear advantage in terms of model generalization to be using their technique over other methods . I feel that the main assumption is not well-grounded , and hence , to me , novelty does not reside in the field tackled . Clarity : - Mainly , I found the paper well written and the experiments clear . I would suggest some proof-reading ( see minor comments , some repeated or missing words ) . Mostly I would suggest to revise the introduction as it wasn \u2019 t clear to me what the purpose of the study was until well into the experiments . I also found that the introduction is not well matched to the main message of the paper ( e.g.it does not mention mask-related constraints ) . Rigor : I found that the comparison between the proposed technique and the literature was well explained and sufficient . Hyper-parameters were described and confidence intervals were provided on all results . I enjoyed the fact that three datasets were included . Overall , I think the experiments were well executed . Detailed comments : -- - \u201c A critical assumption underlying these aforementioned works is that properties of the saliency map are indicative of generalization performance. \u201d I do not agree with that statement given my major concerns ( 1 ) . - Intro : masking is mentioned in the key contributions but not before and no justification is used . Such masking also assumes that the shifts across train and test distributions do not impact features in this mask . This seems like a strong hypothesis to me , especially when considering e.g.different imaging sites , or image resolutions . - It is unclear whether the goal is to obtain models that generalize better or whether it is to obtain feature attribution maps that are consistent with expert input . These two goals can be misaligned , as displayed in the results and should not be conflated . - actdiff method : can lambda be mentioned in the equation ? How would such a regularizer perform in a high dimensional layer ( curse of dimensionality ) ? Why use pre-activation outputs ? Were any other versions of this formulation tried ? - How were the masks defined ? Is there variability per sample ( e.g.different experts ) in their definition ? - The authors refer to the synthetic dataset as the changes in the chest x-ray but then have a separate synthetic dataset . The language is confusing and datasets should be defined before being referenced to . - Synthetic data results : the masked model performs best , ignoring the confounder . However , the saliency map reflects that the attributions are high for the confounder . Therefore , I do not see the same \u201c high correlation \u201d between IoU and AUC that the authors mention . In addition , this , to me , reflects the main limitation of attribution maps : they do not reflect the model \u2019 s decision . They rather reflect the local effect of a feature on the label ( Lipton , 2016 , Ancona et al. , 2018 ) . I am wondering why the authors select saliency maps compared to e.g.integrated gradients ( Sundararajan et al. , 2017 ) or other gradient-based but more mathematically grounded techniques . Given my own experience of attribution techniques , it is likely that using different attribution methods will lead to different conclusions with respect to the correlation between IoU and AUC . - I understand the choice in the setup of confounders for both datasets . However I am unsure how this represents real-world settings . For example , training sets might indeed be site-specific , but it would be surprising to me that the test presents the inverse of the confounder . We could for example expect the absence of that confounder ( which could be simulated by removing the confounder in the test set for synthetic data ) , or lower correlations between a feature and the label . - ActDiff substantially decreases model performance in the absence of confounders - If the goal is to obtain more generalizable models , other techniques could be envisaged when the relationship between label and confounder is not deterministic , like resampling or reweighting . Were these considered ? Minor : - Intro : PAC undefined - intro : the authors conflate the behavior of a model and which inductive biases it relies on , with the obtained saliency maps . This is however a complex and , in my opinion , unanswered question . - Figure 1 : NIH , PC , PA and AP not defined . SPC and VPC are presented succinctly without intuition and we only understand them much later . Maybe this figure should be moved in the results . - contributions : point 1 needs proof-reading - \u201c out of distribution feature attribution phenomena \u201d : I searched DeGrave et al for this term but could not find it . If not used elsewhere , I would rephrase as this is a confusing formulation : samples/images can be OOD , and these samples can provide feature attributions , but the feature attributions themselves are not OOD . - proof-reading of the text is required . Some explanations are poorly framed and can be rephrased ( e.g.related works , paragraph 2 ) . - Zeiler & Fergus , 2013 ( published at ECCV 2014 ) refers to the work on occlusion , where inputs are masked by a baseline value and the changes in predicted risk is defined as their attribution . While this technique provides one attribution per feature , it is not based on the gradients of the network . This paper also does not refer to the term \u201c saliency \u201d . - Formulas 4 and 5 , consistency in the formulations is desirable : if showing for binary classification in most cases , it is better to keep this set up for other formulations ( even if they could be extended to multiclass ) , especially as the experiments are run on binary classification . The limitation of GradMask to binary cases could however be mentioned . - Figure 5 can be difficult to investigate without colormaps . For instance , it looks to me like the \u201c masked \u201d training model does highlight the confounding factor .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for all the great points . We have performed new experiments which will be uploaded in a revised document early this week . We wanted to respond to your points now to ensure that these changes have satisfied your concerns with our paper . > 1.The main assumption underlying this work is that improving feature attributions will help generalizability . I however do not think that this is the message that was relayed in past publications on the topic ( contrarily to what is mentioned in the introduction ) and do not think that \u201c better \u201d attributions is sufficient for better generalizability . My reading on this topic is that attribution methods can help in highlighting confounding factors when models do not generalize under distribution shifts . Making this a sufficiency condition is a step that is not well-motivated to me . We shared the belief that saliency could help in highlighting confounding factors . However in this work we demonstrate models which do not indicate incorrect feature attribution yet fail to generalize dramatically ( in fact they look at the right area ) . Therefore our paper is now questioning this line of thinking . This impression that a good saliency map implies a good prediction is common in applied domains ( such as medical ) which is the reason we wrote this paper https : //www.nature.com/articles/s41598-019-42557-4 https : //www.nature.com/articles/s41591-020-0942-0 . We believe the evidence that we present in this paper will serve to help guide those researchers to evaluate models with more rigor . The idea that correcting models to have `` good '' saliency would yield good generalization is present in the ML literature as well [ RRR , https : //arxiv.org/abs/1906.10670 ] . > 2.The reason why I do not believe this assumption is valid is because attribution methods have been shown to not correctly represent model \u2019 s decisions [ ... & ] different attribution methods will likely display different patterns . Thank you for raising this important point . We maintain that this paper actually supports your claim , and provides experimental evidence in a real-world application that the assumption you describe is not valid . We performed this research to challenge that assumption to have a citable example that this is not valid . Your concern that Input gradients might not be representative of the model \u2019 s attributions is a good one , and to address this we have calculated both Integrated Gradients and Occlusion-based attribution maps for all experiments . In the main text , we present the Integrated Gradients images , and show all three methods side-by-side in the appendix . Interestingly , we found all methods gave similar but not identical attribution patterns . > 3.Which is why I am wondering whether other attributions , which are more theoretically grounded ( e.g.integrated gradients [ Sundararajan et al. , 2017 ] or more recent gradient-based techniques like DeepLIFT , or from non-gradients based techniques like SHAP or occlusion ) were investigated . We originally focused on input gradient in line with previous work [ GradMask , RRR ] , but have now also computed the integrated gradients and occlusion-based metrics for all experiments and included them in the Appendix Figures A3-7 and A13 for the overview plots ( now uploaded ) . We believe that this paper is now much stronger with these included . Thank you ! + Example Synthetic Images : https : //figshare.com/s/84ebabd78fe7d3b0b271 + Example XRay Images : https : //figshare.com/s/d1dc30ca2241853cf219 > 4 . The authors mention that masks represent a \u201c good \u201d attribution map . However , there is no guarantee that features in those masks are not affected by distribution shifts . This should be discussed as a limitation of mask-based regularizers . We agree and have added this in a limitations section . > 5.The authors seem to have missed that the \u201c masked \u201d trained classifier also highlights the confounder on the synthetic data , despite it correctly predicting the outputs . I believe this is related to the choice of saliency maps , as \u201c raw \u201d gradients do not \u201c explain \u201d the decision of the model [ ... ] I would suggest using integrated gradients or another technique . While it is true that the masked approach appears to highlight the confounder , it does also do a slightly better job at distributing attribution to the \u2018 + \u2019 symbols . Interestingly , we see the same behaviour for Input Gradients , Integrated Gradients , and the Occlusion approach . We have added experiments using different methods to generate saliency maps and present them side-by-side in the appendix , showing integrated gradients in the main text . > 6.The purpose of the study is quite vague and its results and conclusions lack of depth and reflexion . Aside from adding new experiments as you suggested , we have tried to clarify the purpose of this work , and expand upon the conclusions , by expanding both sections to make the purpose of the work clearer ."}, "2": {"review_id": "c9-WeM-ceB-2", "review_text": "This paper addresses the potential correlation between saliency map values and model generalization ability in image analysis with a focus on medical image . The paper falls well within the scope of the conference . It is overall well written , but it is so crammed with information that , in order to comply with paper length limits , its 'digestion ' and interpretation becomes difficult at times ( this is clear in the case of images , whose meaning often has to be half-guessed due to lack of details ) The problem this study deals with is definitely important in the context of medical decision making on the basis of image , and the problems it pinpoints are more than real ( small sample sizes , inter-site heterogeneity , bias due to human intervention in the masking process , etc ) The proposal seems sound to me and the experiments convincing . My main qualm concerns their specificity , given that they address a very specific domain .", "rating": "7: Good paper, accept", "reply_text": "> This paper addresses the potential correlation between saliency map values and model generalization ability in image analysis with a focus on medical image . The paper falls well within the scope of the conference . It is overall well written , but it is so crammed with information that , in order to comply with paper length limits , its 'digestion ' and interpretation becomes difficult at times ( this is clear in the case of images , whose meaning often has to be half-guessed due to lack of details ) Thank you for your kind assessment of our manuscript ! We will take your suggestion to clarify the text as much as possible seriously . In particular , we will take advantage of the appendix to flesh out experimental details that are briefly described in the main text . > The problem this study deals with is definitely important in the context of medical decision making on the basis of image , and the problems it pinpoints are more than real ( small sample sizes , inter-site heterogeneity , bias due to human intervention in the masking process , etc ) The proposal seems sound to me and the experiments convincing . My main qualm concerns their specificity , given that they address a very specific domain . While we understand that we tackled the problem of medical imaging generalization specifically in this paper , the methods and problems addressed are applicable to any imaging application where there might exist a covariate shift between the training and test distributions , and our masking approaches could be used in those contexts . We hope the reader focuses less on the specifics of our masking approach , however , and focus more on the problem of relying on these attribution methods to diagnose model correctness . We have expanded the text to focus the attention on these findings ."}, "3": {"review_id": "c9-WeM-ceB-3", "review_text": "Summary This paper focuses on the confounder problem that spatially-seperated image regions ( e.g.shoulders of xray images ) might spuriously correlated with the target ( e.g.pneumonia ) . If given a human-labeled region that is deemed important , we can decrease this spuriousness by regularizing the model toward the important region . They not only compare with several existing saliency-based methods ( RRR and GradMask ) , but also propose 2 new methods ( ActDiff , Adversarial ) inspired from domain adapataion literature that the representation of the classifier should be similar between original image and the masked image ( the image that the non-important region is shuffled ) . They compare in 1 synthetic dataset and 2 xray datasets . They show that ( 1 ) these methods ( sometimes ) hurt generalization when spuriousness does not exist , and ( 2 ) the model 's saliency map is only weakly correlated with generalization performance , and thus doubting the validtiy of using saliency maps for diagnosing whether a model is overfit to spurious features . Pros 1.Very detailed hyperparameter search and lots of repeated run ( 10 ) to get good standard deviation 2 . Visualizations of average test images are very intriguing . Major comments 1 . It seems that when there is covariate shift , the ActDiff performs the best but hurts the performance when no shift exists , but saliency-based methods do not suffer this . But given that you can tune the lambda of the regularization , why could it happen ? Ca n't you just pick lambda close to 0 ? 2.Some visualization looks suspiciously abnormal . For example , RRR in Figure 5 has a big blur in the middle . Why is that ? Also , in Figure 7 , Masked focus on the neck to predict but neck is clearly outside of the bounding box . How should we explain this phenomenon ? Besides , in Figure 7 upper rows ( VPC ) , Adversarial seems to have better masks by focusing on the lungs , and ActDiff does not . But their IOU is reversed : Adversarial has lower IOU and ActDiff has higher IOU in Table 2 . 3.In all the experiments , the validation set is always assumed correct without any background shift . While in real scenario it might not always be easy to access to a validation dataset without any shift . When we instead only have a biased validation dataset , which method will perform better ? Will the result change ? Minor Comments 1 . Some hyperparameters for visualization like Gaussian smoothing and thersholding might need justifications . Especially the maximum value capped in 50th percentile seems to be a bit excess . 2.When visualizing the gradients ( Fig.5 , 6 , 7 ) , maybe we should only include images that model predicts correctly ? The gradient of extremely wrong predictions might not be very meaningful . 3.The Figure 8 shows the scatter plot between Test AUC and IOU . It seems Synthetic and Xray SPC have much higher IOU overall . Maybe because their bounding box or confounder is location-wise fixed while only the RSNA VPC has varying bounding boxes ? 4.The masked baseline perform much worse in No VPC with AUC=0.5 which is random guessing . But it does not happen in other datasets . Maybe it 's because the VPC has smaller bounding box and thus only access to such region is too difficult ? 5.The lambda should be included in all the equations ( eq.1 to 5 ) .6.The final hyperparameter should be reported in the appendix . Overall evaluations Overall I like this paper . The hyperparameter tuning is very thorough . The conclusion is good that great saliency map does not mean better accuracy and vice versa . The experimental results are a bit unsatisfying that no real data is improved . And sometimes the simple masked baseline outperform others . I am happy to increase my score if my major concerns are addressed .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> 1.Some hyperparameters for visualization like Gaussian smoothing and thersholding might need justifications . Especially the maximum value capped in 50th percentile seems to be a bit excess . The gaussian sigma of 1 is small , especially for images with 224x224 resolution , and was done primarily to remove speckly noise from the attribution maps , which distract from the underlying pattern . The threshold was a more arbitrary choice , but we thought the 50th percentile would be easier for the reader to visualize : without this threshold , the anatomy is harder to see . For the values in the tables , we binarized at a percentile that matches the size of the supplied mask , not the 50th percentile , so the qualitative decision made for the figures does not impact the interpretation of the numbers in the tables . > 2.When visualizing the gradients ( Fig.5 , 6 , 7 ) , maybe we should only include images that model predicts correctly ? The gradient of extremely wrong predictions might not be very meaningful . Thank you for this helpful suggestion . We have generated figures for samples where A ) the model is correct , and B ) the model is incorrect on the test set , to determine how different these attributions are . These plots are found in the appendix A6 , computed using Integrated Gradients . > 3.The Figure 8 shows the scatter plot between Test AUC and IOU . It seems Synthetic and Xray SPC have much higher IOU overall . Maybe because their bounding box or confounder is location-wise fixed while only the RSNA VPC has varying bounding boxes ? Yes , you are correct . For the synthetic dataset , the task is extremely easy , so a high IOU isn \u2019 t surprising . The saliency maps are quite precise ( see Appendix A9 ) . Meanwhile for XRay SPC , the task is simply to predict away from the image border : one can imagine that many saliency maps that are simply focused away from the shoulder will perform well in this case . In contrast , we evaluated the RSNA images by their overlap with bounding boxes around the disease , which is variable across images . This is a harder task for the model to localize well . > 4.The masked baseline perform much worse in No VPC with AUC=0.5 which is random guessing . But it does not happen in other datasets . Maybe it 's because the VPC has smaller bounding box and thus only access to such region is too difficult ? This is reasonable and likely correct although we can \u2019 t know for sure . The masked task for the synthetic dataset is quite easy : the network only needs to learn two shapes ( + , - ) and with masked dataset , it learns how to count the + symbols during training . With the XRay dataset , the gross anatomy of the entire lung is available including the context around . Meanwhile , for the RSNA data , the model is only able to consistently see what is inside the lung , without context . The model is likely learning image textures that might appear elsewhere in the image , leading to bad generalization . > 5.The lambda should be included in all the equations ( eq.1 to 5 ) .We have done this in the updated PDF . > 6.The final hyperparameter should be reported in the appendix . We have done this in the Appendix . > Overall I like this paper . The hyperparameter tuning is very thorough . The conclusion is good that great saliency map does not mean better accuracy and vice versa . The experimental results are a bit unsatisfying that no real data is improved . And sometimes the simple masked baseline outperform others . I am happy to increase my score if my major concerns are addressed . Thank you for your thoughtful review . We would like to point out the following : where there exists a covariate shift between the training and test sets , the domain invariance approaches we introduce improve over the baseline for that experiment , which we believe is a valuable contribution . It is true that in the absence of a covariate shift , these methods do not help the model , but we also would not expect them to as the model would not be as likely to learn to use features that are only predictive in the training set distribution ( since the test set distribution is the same ) . Furthermore , we were equally surprised that the masked baseline performed so well on the X-Ray dataset , but perhaps this is less surprising because the masked region is far from the anatomy that determines the class , and it is also worth noting the high variance in performance of that method across seeds . It would be therefore hard to recommend the masked approach be used on datasets , especially when the masks are smaller ( note the far worse performance of the RSNA experiments ) . We have highlighted this in the text ."}}