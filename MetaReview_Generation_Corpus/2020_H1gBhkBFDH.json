{"year": "2020", "forum": "H1gBhkBFDH", "title": "B-Spline CNNs on Lie groups", "decision": "Accept (Poster)", "meta_review": "The paper describes principles for endowing a neural architecture with invariance with respect to a Lie group. The contribution is that these principles can accommodate discrete and continuous groups, through approximation via a base family (B-splines). \n\nThe main criticisms were related to the intelligibility of the paper and the practicality of the approach, implementation-wise. Significant improvements have been done and the paper has been partially rewritten during the rebuttal period.\n\nOther criticisms were related to the efficiency of the approach, regarding how the property of invariance holds under the approximations done. These comments were addressed in the rebuttal and the empirical comparison with data augmentation also supports the merits of the approach.\n\nThis leads me to recommend acceptance. I urge the authors to extend the description and discussion about the experimental validation. \n", "reviews": [{"review_id": "H1gBhkBFDH-0", "review_text": "In this paper, a framework for building group CNN with an arbitrary Lie group G is proposed. Generally, such a group CNN consists of 3 types of layers: a lifting layer which lifts a 2D image to a 3D data (G-image) whose domain is G; a group correlation layer which computes a 3D G-image from a 3D G-image; and a projection layer from a 3D G-image to a 2D image. To implement the convolutions in the lifting layer and group correlation layer which are defined in the continuous setting, the B-Spline basis functions are applied to expand the convolution kernels. Experimental results on tumor clarification and landmark localization show the superiority over CNN. Advantages: 1. A flexible framework for group convolutional neural network is proposed with strong theoretical support in Theorem 1. 2. Familiar properties of convolutions from classical CNN design (like localized, atrous, and deformable convolutions) can also be implemented in G-CNN using specified B-Spline basis functions. 3. In comparison with standard CNN, the effectiveness of the B-Spline-based G-CNN is validated through experiments on two typical data sets. Weakness: 1. [Readability] For readers who are not familiar with Lie groups, this paper is very hard to follow. (1) For Theorem 1, the authors are suggested to give some illustrative explanation. Besides, what is \u201cStab_G\u201d? (2) The architecture of G-CNN, i.e., the 3 types of layers, are directly given in Eqs. (5)-(7) without examples, illustrative examinations, or visual illustrations. (3) Fig. 1 can be modified for better readability. 2. [Experiments] The proposed G-CNN has some similarities with data augmentation (like rotation, scaling) based CNN. Then, how better can the G-CNN perform than CNN with data augmentation? More experiments on this point are suggested, and relevant theoretical explanations will be appreciated. 3. [Implementation] Considering the complicated mathematics in this paper, I am afraid that implementation of the proposed G-CNN is also very hard. It would be better for the authors to discuss the implementation. In my mind, if the implementation is not so hard, then the formulation of G-CNN can also be simplified for better readability. ", "rating": "8: Accept", "reply_text": "Thank you for such a careful analysis of the paper ! We also thank you for identifying some points for improvement ; we address these in our revision and believe it leads to a much improved paper . We discuss them below . We are currently working on updating the manuscript . If in the meantime you have additional questions we would be happy to respond to them ! * * * \u201c [ Readability ] For readers who are not familiar with Lie groups , this paper is very hard to follow . ( 1 ) For Theorem 1 , the authors are suggested to give some illustrative explanation . Besides , what is \u201c Stab_G \u201d ? ( 2 ) The architecture of G-CNN , i.e. , the 3 types of layers , are directly given in Eqs . ( 5 ) - ( 7 ) without examples , illustrative examinations , or visual illustrations . ( 3 ) Fig.1 can be modified for better readability . \u201c ( 1 ) We are working on an illustration of theorem 1 for the case of roto-translation equivariant networks , and will place this in appendix B.2 and refer to it in the main text . We will provide extra explanation for each layer to give a more context . ( 2 ) We will subsequently add a paragraph in which equations ( 5 ) - ( 7 ) are given explicitly for the roto-translation group and write out the equations for several other groups in Appendix B . ( 3 ) We are working on an improved introductory figure . All in all these modifications will probably add another page to the main body of the paper , but of course we still aim to stay within the 10 page limit . Stay tuned for the revision . * * * \u201c [ Experiments ] The proposed G-CNN has some similarities with data augmentation ( like rotation , scaling ) based CNN . Then , how better can the G-CNN perform than CNN with data augmentation ? More experiments on this point are suggested , and relevant theoretical explanations will be appreciated . \u201c We initially left out discussions regarding augmentation as these are addressed in prior work on G-CNNs , but we realize that it is a too important connection to ignore . So we are currently trying to find a way to fit this into the revision . There are mainly two arguments why G-CNNs are preferred over data-augmentations : 1 . Data augmentations transform the inputs globally and are not able to deal with local transformations/symmetries . G-CNNs handle both local and global symmetries . 2.By using data augmentations you let the network learn how to deal with such transformations . It thus has to spend valuable network capacity on this . G-CNNs on the other hand have the appropriate geometric structure encoded in them and therefore do not have to spend valuable network capacity on learning geometric behavior , but rather can spend it all on learning effective representations . We do remark however , that data-augmentations and G-CNNs happily live together , and that data augmentations can still be used to improve performance , in particular when the augmentations include transformations that are not covered by the Lie group . * * * `` [ Implementation ] Considering the complicated mathematics in this paper , I am afraid that implementation of the proposed G-CNN is also very hard . It would be better for the authors to discuss the implementation . In my mind , if the implementation is not so hard , then the formulation of G-CNN can also be simplified for better readability . \u201c In order to achieve a generic viewpoint on equivariance we make an abstraction step ( and speak of representations of groups ) , and this step is indeed somewhat mathematically demanding , but it eventually allows us to develop the code in a modular ( object oriented ) and generic way . The specific equations for Lie group CNNs layers , e.g.for roto-translation equivariance , are however very readable and similar to the conventional convolution operators . We will provide such explicit examples in the revision in Appendix B , but we are trying to fit a concrete example in the main body of the text as well for the revision . Via the abstractions made in this paper a developer/researcher interested in implementing G-CNNs for a particular transformation group only has to define the group structure of the sub-group H that he/she wants to combine with translations ( e.g.to build translation+rotation networks , translation+scalings networks , translations+skewing networks , \u2026 ) and all the layers are automatically derived . We hope to be able to convince you of the tractability of implementing the theory by anonymously providing examples of implementations for the 2D roto-translation and scale-translation groups ( as python classes ) , together with the g-splinets ( as it is currently called ) tensorflow library via the link above ( here on openreview.net ) . We are working on this give an update when we submit the final revision . The code will appear on GitHub after the accept/reject decision is made , with minimal working examples and the script used to generate the results ."}, {"review_id": "H1gBhkBFDH-1", "review_text": "The paper proposes an (approximately) equivariant neural network architecture for data lying on homogeneous spaces of Lie groups. In contrast to the Gauge equivariant and Fourier approaches that have recently appeared, here the authors simply put a B-spline basis on local patches of the homogeneous space and move the basis elements around explicitly by applying the group action. The approach is appealing in its simplicity and generality. No need to worry about irreducible representations and Fourier transforms, the formalism works for virtually any Lie group, no problem with non-compact groups. However, there is a constant need for interpolation. What is more more significant is that both the homogeneous space and the group need to be discretized and in general that cannot be done in a regular manner (no notion of a uniform grid on SO(3) for example). The authors assure us that \"we find that it is possible to find approximately uniform B-splines... e.g. by using a repulsion model\". I am not sure that it is so simple. This is one of those things where the idea is straightforward but the devil is in the details. Theorem 1 seems important but it is a bit cryptic. What is the statement \"a kernel satisfying such and such properties gives rise to an equivariant CNN\"? Or \"A CNN is equivariant if and only the kernel satisfies such and such properties\"? Concerningly, the paper is closely related to a few other papers using the spline CNN idea or at least the idea of taking a fixed set of functions and moving it around on the homogeneous space by acting on it with select group elements, most notably \"Roto-translational convolutional neural networks for medical image analysis\" by Bekkers et al.. The main difference of the present paper relative to that one is that the idea is fleshed out in a little more detail and is generalized from SE(2) to arbitrary Lie groups. However, conceptually there is little that is new. In such a situation it would be important to present convincing experiments. Unfortunately in the present paper, results are only presented on 2 datasets, and the algorithm is basically only compared to different versions of itself, rather than state of the art competitors. The paper is clearly written but the intuitive nature of the core ideas could be better conveyed e.g. by fancy diagrams.", "rating": "3: Weak Reject", "reply_text": "Thank you for your thorough analysis of the paper and for raising points for discussion which we are happy to address in the following . We are currently working on updating the manuscript . If in the meantime you have additional questions we would be happy to respond to them ! * * * \u201c In contrast to the Gauge equivariant and Fourier approaches that have recently appeared , here the authors simply put a B-spline basis on local patches of the homogeneous space and move the basis elements around explicitly by applying the group action . \u201c We provided a detailed discussion about the connection of this work to the theory of gauge equivariant CNNs in appendix C.2 and summarized this in the introduction . It turns out that the two viewpoints are equivalent in certain settings : we choose the gauge frames to be left-invariant vector fields generated by the Lie group structure . In a related way as is done in our paper , gauge equivariant CNNs also \u201c simply move a kernel around \u201d and align it with a particular vector field ( gauge field ) . In the gauge paper , however , a particular grid/manifold is chosen that allows for discrete convolutions and as such avoid interpolation . In this respect , we prefer to invert the \u201c in contrast to \u2026 simply\u2026 \u201d statement , and remark that in order to apply the gauge CNN framework to other cases ( such as meshes or manifolds in general ) , one has at some point to resort ( analytic ) kernel representations that can be sampled at arbitrary points on the manifold . The proposed B-splines enable that . We agree that they are simple functions , but that is precisely why they are nice to work with . Fourier methods are a different story . These are also wonderful techniques that do not necessarily require a specific discretization grid . I would say that such methods are your method of choice when dealing with compact ( unimodular ) groups , but these methods do not generalize well to other types of manifolds . The purpose of this paper is to explore new ways to represent data and build learning architectures . A particular result is that in the B-spline Lie G-CNN viewpoint we can adopt conventional engineering heuristics such as working with localized , deformable and atrous convolutions , which is simply not possible in a Fourier basis . * * * \u201c However , there is a constant need for interpolation . What is more more significant is that both the homogeneous space and the group need to be discretized and in general that can not be done in a regular manner ( no notion of a uniform grid on SO ( 3 ) for example ) . The authors assure us that `` we find that it is possible to find approximately uniform B-splines ... e.g.by using a repulsion model '' . I am not sure that it is so simple . This is one of those things where the idea is straightforward but the devil is in the details . \u201c We are a big fan of Fourier methods and irreps to steer convolution kernels ( w.r.t.trafo parameters ) , they allow to work exclusively with the coefficients without ever having to sample them . This , however , requires specialized activation functions ( several are proposed e.g.in the works by Worrall et al.2017 , Weiler et al.2018a , Kondor 2018 and others alike ) . Again , these methods work well on rotation groups , but do not generalize well to other groups . Interestingly , however , in popular techniques for spherical convolutions ( both Cohen 2018b and Esteves et al 2018a ) one does in fact rely on sampling of the data on the sphere ( with grids that are non-uniform ) . They rely on a sequence of spherical harmonic fits , exact convolutions in \u201c Fourier \u201d domain , followed by sampling again on the sphere such that element-wise nonlinearities can be applied in a conventional way . They are highly effective despite the fact that after applying such nonlinearities ( 1 ) the functions leave the spherical harmonic basis in which they were expressed and ( 2 ) the networks are not fully equivariant anymore due to the non-uniform grid . As in many real world applications one has to make a trade-off between mathematical beauty and computational efficiency or pragmatism . Regarding discretizations on uniform grids . As remarked in the main body of the paper , uniform local grids can always be constructed on Lie groups . However , on compact groups one has to be careful that the grid does not start to overlap with itself , as can happen with SO ( d ) . Luckily on compact groups repulsion models also always work as due to the periodic nature one has that the repulsing forces do not send elements outside of the domain . Finally , in response to \u201c the constant need for interpolation \u201d . We do not regard the need for interpolation as a limitation . Computationally , interpolation ( in our case actually just sampling of the kernels ) only occurs with every transformation in the sub-group H that is sampled , and only on for the convolution kernels ."}, {"review_id": "H1gBhkBFDH-2", "review_text": "This paper proposes a neural network architecture which that enables the implementation of group convolutional neural networks for arbitrary Lie groups. This lifts a significant limitation of such models which were previously confined to discrete or continuous compact groups due to tractability issues. I'm afraid that this paper is over my head. It relies heavily on field-specific terminology and as such is likely to be accessible to a relatively small subset of researchers. This looks to me like a solid contribution, however I'm really not qualified to judge.", "rating": "6: Weak Accept", "reply_text": "Thank you for reading and for providing your high-level summary ( which is correct ; ) ) . We agree that the paper relies on advanced mathematical/geometrical concepts . We found it important to build up the proposed framework in a mathematically coherent and solid way , and the abstractions help us to make generalizations , grasp the broader picture ( see also paragraphs and appendices on related work ) and eventually implement the theory in an accessible , object-oriented way . Nevertheless , we also find it important that the paper is accessible to a wide audience . As such , we will open-source the code ( see also the code snipped as a response to reviewer 3 ) and work on new figures and add extra clarifications of the theory in the main text . We are currently working on updating the manuscript . If in the meantime you have additional questions we would be happy to respond to them !"}], "0": {"review_id": "H1gBhkBFDH-0", "review_text": "In this paper, a framework for building group CNN with an arbitrary Lie group G is proposed. Generally, such a group CNN consists of 3 types of layers: a lifting layer which lifts a 2D image to a 3D data (G-image) whose domain is G; a group correlation layer which computes a 3D G-image from a 3D G-image; and a projection layer from a 3D G-image to a 2D image. To implement the convolutions in the lifting layer and group correlation layer which are defined in the continuous setting, the B-Spline basis functions are applied to expand the convolution kernels. Experimental results on tumor clarification and landmark localization show the superiority over CNN. Advantages: 1. A flexible framework for group convolutional neural network is proposed with strong theoretical support in Theorem 1. 2. Familiar properties of convolutions from classical CNN design (like localized, atrous, and deformable convolutions) can also be implemented in G-CNN using specified B-Spline basis functions. 3. In comparison with standard CNN, the effectiveness of the B-Spline-based G-CNN is validated through experiments on two typical data sets. Weakness: 1. [Readability] For readers who are not familiar with Lie groups, this paper is very hard to follow. (1) For Theorem 1, the authors are suggested to give some illustrative explanation. Besides, what is \u201cStab_G\u201d? (2) The architecture of G-CNN, i.e., the 3 types of layers, are directly given in Eqs. (5)-(7) without examples, illustrative examinations, or visual illustrations. (3) Fig. 1 can be modified for better readability. 2. [Experiments] The proposed G-CNN has some similarities with data augmentation (like rotation, scaling) based CNN. Then, how better can the G-CNN perform than CNN with data augmentation? More experiments on this point are suggested, and relevant theoretical explanations will be appreciated. 3. [Implementation] Considering the complicated mathematics in this paper, I am afraid that implementation of the proposed G-CNN is also very hard. It would be better for the authors to discuss the implementation. In my mind, if the implementation is not so hard, then the formulation of G-CNN can also be simplified for better readability. ", "rating": "8: Accept", "reply_text": "Thank you for such a careful analysis of the paper ! We also thank you for identifying some points for improvement ; we address these in our revision and believe it leads to a much improved paper . We discuss them below . We are currently working on updating the manuscript . If in the meantime you have additional questions we would be happy to respond to them ! * * * \u201c [ Readability ] For readers who are not familiar with Lie groups , this paper is very hard to follow . ( 1 ) For Theorem 1 , the authors are suggested to give some illustrative explanation . Besides , what is \u201c Stab_G \u201d ? ( 2 ) The architecture of G-CNN , i.e. , the 3 types of layers , are directly given in Eqs . ( 5 ) - ( 7 ) without examples , illustrative examinations , or visual illustrations . ( 3 ) Fig.1 can be modified for better readability . \u201c ( 1 ) We are working on an illustration of theorem 1 for the case of roto-translation equivariant networks , and will place this in appendix B.2 and refer to it in the main text . We will provide extra explanation for each layer to give a more context . ( 2 ) We will subsequently add a paragraph in which equations ( 5 ) - ( 7 ) are given explicitly for the roto-translation group and write out the equations for several other groups in Appendix B . ( 3 ) We are working on an improved introductory figure . All in all these modifications will probably add another page to the main body of the paper , but of course we still aim to stay within the 10 page limit . Stay tuned for the revision . * * * \u201c [ Experiments ] The proposed G-CNN has some similarities with data augmentation ( like rotation , scaling ) based CNN . Then , how better can the G-CNN perform than CNN with data augmentation ? More experiments on this point are suggested , and relevant theoretical explanations will be appreciated . \u201c We initially left out discussions regarding augmentation as these are addressed in prior work on G-CNNs , but we realize that it is a too important connection to ignore . So we are currently trying to find a way to fit this into the revision . There are mainly two arguments why G-CNNs are preferred over data-augmentations : 1 . Data augmentations transform the inputs globally and are not able to deal with local transformations/symmetries . G-CNNs handle both local and global symmetries . 2.By using data augmentations you let the network learn how to deal with such transformations . It thus has to spend valuable network capacity on this . G-CNNs on the other hand have the appropriate geometric structure encoded in them and therefore do not have to spend valuable network capacity on learning geometric behavior , but rather can spend it all on learning effective representations . We do remark however , that data-augmentations and G-CNNs happily live together , and that data augmentations can still be used to improve performance , in particular when the augmentations include transformations that are not covered by the Lie group . * * * `` [ Implementation ] Considering the complicated mathematics in this paper , I am afraid that implementation of the proposed G-CNN is also very hard . It would be better for the authors to discuss the implementation . In my mind , if the implementation is not so hard , then the formulation of G-CNN can also be simplified for better readability . \u201c In order to achieve a generic viewpoint on equivariance we make an abstraction step ( and speak of representations of groups ) , and this step is indeed somewhat mathematically demanding , but it eventually allows us to develop the code in a modular ( object oriented ) and generic way . The specific equations for Lie group CNNs layers , e.g.for roto-translation equivariance , are however very readable and similar to the conventional convolution operators . We will provide such explicit examples in the revision in Appendix B , but we are trying to fit a concrete example in the main body of the text as well for the revision . Via the abstractions made in this paper a developer/researcher interested in implementing G-CNNs for a particular transformation group only has to define the group structure of the sub-group H that he/she wants to combine with translations ( e.g.to build translation+rotation networks , translation+scalings networks , translations+skewing networks , \u2026 ) and all the layers are automatically derived . We hope to be able to convince you of the tractability of implementing the theory by anonymously providing examples of implementations for the 2D roto-translation and scale-translation groups ( as python classes ) , together with the g-splinets ( as it is currently called ) tensorflow library via the link above ( here on openreview.net ) . We are working on this give an update when we submit the final revision . The code will appear on GitHub after the accept/reject decision is made , with minimal working examples and the script used to generate the results ."}, "1": {"review_id": "H1gBhkBFDH-1", "review_text": "The paper proposes an (approximately) equivariant neural network architecture for data lying on homogeneous spaces of Lie groups. In contrast to the Gauge equivariant and Fourier approaches that have recently appeared, here the authors simply put a B-spline basis on local patches of the homogeneous space and move the basis elements around explicitly by applying the group action. The approach is appealing in its simplicity and generality. No need to worry about irreducible representations and Fourier transforms, the formalism works for virtually any Lie group, no problem with non-compact groups. However, there is a constant need for interpolation. What is more more significant is that both the homogeneous space and the group need to be discretized and in general that cannot be done in a regular manner (no notion of a uniform grid on SO(3) for example). The authors assure us that \"we find that it is possible to find approximately uniform B-splines... e.g. by using a repulsion model\". I am not sure that it is so simple. This is one of those things where the idea is straightforward but the devil is in the details. Theorem 1 seems important but it is a bit cryptic. What is the statement \"a kernel satisfying such and such properties gives rise to an equivariant CNN\"? Or \"A CNN is equivariant if and only the kernel satisfies such and such properties\"? Concerningly, the paper is closely related to a few other papers using the spline CNN idea or at least the idea of taking a fixed set of functions and moving it around on the homogeneous space by acting on it with select group elements, most notably \"Roto-translational convolutional neural networks for medical image analysis\" by Bekkers et al.. The main difference of the present paper relative to that one is that the idea is fleshed out in a little more detail and is generalized from SE(2) to arbitrary Lie groups. However, conceptually there is little that is new. In such a situation it would be important to present convincing experiments. Unfortunately in the present paper, results are only presented on 2 datasets, and the algorithm is basically only compared to different versions of itself, rather than state of the art competitors. The paper is clearly written but the intuitive nature of the core ideas could be better conveyed e.g. by fancy diagrams.", "rating": "3: Weak Reject", "reply_text": "Thank you for your thorough analysis of the paper and for raising points for discussion which we are happy to address in the following . We are currently working on updating the manuscript . If in the meantime you have additional questions we would be happy to respond to them ! * * * \u201c In contrast to the Gauge equivariant and Fourier approaches that have recently appeared , here the authors simply put a B-spline basis on local patches of the homogeneous space and move the basis elements around explicitly by applying the group action . \u201c We provided a detailed discussion about the connection of this work to the theory of gauge equivariant CNNs in appendix C.2 and summarized this in the introduction . It turns out that the two viewpoints are equivalent in certain settings : we choose the gauge frames to be left-invariant vector fields generated by the Lie group structure . In a related way as is done in our paper , gauge equivariant CNNs also \u201c simply move a kernel around \u201d and align it with a particular vector field ( gauge field ) . In the gauge paper , however , a particular grid/manifold is chosen that allows for discrete convolutions and as such avoid interpolation . In this respect , we prefer to invert the \u201c in contrast to \u2026 simply\u2026 \u201d statement , and remark that in order to apply the gauge CNN framework to other cases ( such as meshes or manifolds in general ) , one has at some point to resort ( analytic ) kernel representations that can be sampled at arbitrary points on the manifold . The proposed B-splines enable that . We agree that they are simple functions , but that is precisely why they are nice to work with . Fourier methods are a different story . These are also wonderful techniques that do not necessarily require a specific discretization grid . I would say that such methods are your method of choice when dealing with compact ( unimodular ) groups , but these methods do not generalize well to other types of manifolds . The purpose of this paper is to explore new ways to represent data and build learning architectures . A particular result is that in the B-spline Lie G-CNN viewpoint we can adopt conventional engineering heuristics such as working with localized , deformable and atrous convolutions , which is simply not possible in a Fourier basis . * * * \u201c However , there is a constant need for interpolation . What is more more significant is that both the homogeneous space and the group need to be discretized and in general that can not be done in a regular manner ( no notion of a uniform grid on SO ( 3 ) for example ) . The authors assure us that `` we find that it is possible to find approximately uniform B-splines ... e.g.by using a repulsion model '' . I am not sure that it is so simple . This is one of those things where the idea is straightforward but the devil is in the details . \u201c We are a big fan of Fourier methods and irreps to steer convolution kernels ( w.r.t.trafo parameters ) , they allow to work exclusively with the coefficients without ever having to sample them . This , however , requires specialized activation functions ( several are proposed e.g.in the works by Worrall et al.2017 , Weiler et al.2018a , Kondor 2018 and others alike ) . Again , these methods work well on rotation groups , but do not generalize well to other groups . Interestingly , however , in popular techniques for spherical convolutions ( both Cohen 2018b and Esteves et al 2018a ) one does in fact rely on sampling of the data on the sphere ( with grids that are non-uniform ) . They rely on a sequence of spherical harmonic fits , exact convolutions in \u201c Fourier \u201d domain , followed by sampling again on the sphere such that element-wise nonlinearities can be applied in a conventional way . They are highly effective despite the fact that after applying such nonlinearities ( 1 ) the functions leave the spherical harmonic basis in which they were expressed and ( 2 ) the networks are not fully equivariant anymore due to the non-uniform grid . As in many real world applications one has to make a trade-off between mathematical beauty and computational efficiency or pragmatism . Regarding discretizations on uniform grids . As remarked in the main body of the paper , uniform local grids can always be constructed on Lie groups . However , on compact groups one has to be careful that the grid does not start to overlap with itself , as can happen with SO ( d ) . Luckily on compact groups repulsion models also always work as due to the periodic nature one has that the repulsing forces do not send elements outside of the domain . Finally , in response to \u201c the constant need for interpolation \u201d . We do not regard the need for interpolation as a limitation . Computationally , interpolation ( in our case actually just sampling of the kernels ) only occurs with every transformation in the sub-group H that is sampled , and only on for the convolution kernels ."}, "2": {"review_id": "H1gBhkBFDH-2", "review_text": "This paper proposes a neural network architecture which that enables the implementation of group convolutional neural networks for arbitrary Lie groups. This lifts a significant limitation of such models which were previously confined to discrete or continuous compact groups due to tractability issues. I'm afraid that this paper is over my head. It relies heavily on field-specific terminology and as such is likely to be accessible to a relatively small subset of researchers. This looks to me like a solid contribution, however I'm really not qualified to judge.", "rating": "6: Weak Accept", "reply_text": "Thank you for reading and for providing your high-level summary ( which is correct ; ) ) . We agree that the paper relies on advanced mathematical/geometrical concepts . We found it important to build up the proposed framework in a mathematically coherent and solid way , and the abstractions help us to make generalizations , grasp the broader picture ( see also paragraphs and appendices on related work ) and eventually implement the theory in an accessible , object-oriented way . Nevertheless , we also find it important that the paper is accessible to a wide audience . As such , we will open-source the code ( see also the code snipped as a response to reviewer 3 ) and work on new figures and add extra clarifications of the theory in the main text . We are currently working on updating the manuscript . If in the meantime you have additional questions we would be happy to respond to them !"}}