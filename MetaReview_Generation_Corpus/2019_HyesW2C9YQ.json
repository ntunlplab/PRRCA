{"year": "2019", "forum": "HyesW2C9YQ", "title": "I Know the Feeling: Learning to Converse with Empathy", "decision": "Reject", "meta_review": "The reviewers raised a number of concerns including the usefulness of the presented dataset given that the collected data is acted rather than naturalistic (and the large body of research in affective computing explains that models trained on acted data cannot generalise to naturalistic data), no methodological novelty in the presented work, and relatively uninteresting application with very limited real-world application (it remains unclear whether having better empathetic dialogues would be truly crucial for any real-life application and, in addition, all work is based on acted rather than real-world data). The authors\u2019 rebuttal addressed some of the reviewers\u2019 concerns but not fully (especially when it comes to usefulness of the data). Overall, I believe that the effort to collect the presented database is noble and may be useful to the community to a small extent. However, given the unrealism of the data and, in turn, very limited (if any) generalisability of the presented to real-world scenarios, and lack of methodological contribution, I cannot recommend this paper for presentation at ICLR.", "reviews": [{"review_id": "HyesW2C9YQ-0", "review_text": "The overall goal of the paper is to make end-to-end dialogue systems more empathetic, so that they can respond more appropriately and in ways that acknowledge how the users are feeling. The authors make two contributions towards that goal: (1) they introduce a crowdsourced dataset (EmpatheticDialogue) annotated with fine-grained emotion labels. (2) They show improvements on dialogue generation (in terms of empathy, but also relevance and fluency) using a multi-task objective, ensemble of encoders, and a more ad-hoc technique that consists of prepending inferred emotion labels to the input. In terms of technical novelty, the work is relatively incremental: (A) The use of multi-task objectives in sequence models [1] is relatively common nowadays (there is little mathematical details in the paper, so it\u2019s hard to see how the approach of the paper really differs from extensive related work.). (B) Prepending predictions: prepending class labels to the input is also relatively common (e.g., in multilingual NMT to select a language). [2] presents a similar approach for polite response generation, where they prepend a label using a politeness classifier. I also have some doubts about the two claimed contributions of the paper (the authors actually list 3 contributions in the introduction, but for convenience I lump the 2 non-data ones together): (1) Dataset: The dataset was crowdsourced by giving workers an emotion label (e.g., afraid) and asking them to define a situation in which that emotion might occur and inviting them to have a conversation on that situation. The problem with prompting workers for specific emotions is that this assumes they are good actors and this is likely to produce exchanges that are rather clich\u00e9 and overdone (e.g., Table 1: the label \u201cafraid\u201d yields a situation that is rather spooky and unlikely in the real world, and the conversations themselves are rather clich\u00e9 and incorporate little details that would make them sound real). The authors justify this dataset by pointing out that existing real-world datasets underrepresent rare emotions (e.g., afraid), but that\u2019s just a reflection of how these emotions are distributed in the real world. Better subsampling strategies would enable a better balance in the distribution without having to give up on real-world data (filtering using emojis, hashtags, etc.). As the paper shows quantitative gains using this dataset, it is probably ok to use but, qualitatively, this dataset is probably not for everyone working on emotion in NLP. (2) Improvement in empathetic dialogue generation: The paper shows improvements across the board compared to a Transformer baseline, but the question the authors do not satisfactorily address is whether their explicit (and I would say sometimes ad-hoc) treatment of empathy (e.g., using emotion classifier, etc.) is crucially needed to get better empathetic dialogues, since the authors did not control for training data size and model capacity. Indeed, the authors exploited different amounts of data (out of-domain, or both in- and out-of-domain), different model capacities (going from baseline Transformer to model ensembles), and sometimes richer input (e.g., pre-trained emotion classifier). The results might only be showing that more data or more model capacity helps, which would of course not be surprising at all. The fact that generated outputs improve in all aspects (not only empathy, but in attributes completely unrelated to empathy such as fluency and relevance) suggests that the improvement is due to more data or capacity (e.g., perhaps yielding better encoder). More statistics in the table in terms of number of parameters and amount of in- and out-of-domain data used for each experiment would help draw a clearer picture. About the use of Reddit: this might not be the best background dataset, as it\u2019s mostly strangers talking to other strangers, presumably causing the baseline to be weak on empathy. Twitter or other social-network type datasets (letting you follow people rather topics) *might* be better suited as it comparatively involves more exchanges between people who actually know each other and who are thus more likely to behave empathetically. Overall, the paper doesn\u2019t really attempt to make major technical contribution, and instead (1) introduces a dataset and (2) makes empirical contributions, but I think there are problems with both. Typos: Introduction: \u201cfro\u201d References: Elizaa [1] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser Multi-task Sequence to Sequence Learning https://arxiv.org/abs/1511.06114 [2] Tong Niu and Mohit Bansal Polite Dialogue Generation Without Parallel Data https://arxiv.org/pdf/1805.03162.pdf", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your insightful and detailed review . While we respectfully disagree with some of the points ( and will detail why below ) , we appreciate that in most cases the fault was lying with us not having clarified those arguments in the manuscript , which we have now done . For others , your insightful questions led us to organize our experiments better and supplement them with new ones which collectively make for a clearer picture . To your detailed points , let us start with one of your last observations : \u201c About the use of Reddit : this might not be the best background dataset , as it \u2019 s mostly strangers talking to other strangers , presumably causing the baseline to be weak on empathy . \u201c We definitely agree that we would expect Reddit to be weak on empathy , but ( 1 ) as stated in our response in the main thread , there isn \u2019 t currently an empathy benchmark that we know of to actually quantify that , and ( 2 ) we wish for publicly available resources to train a conversation system that would respond with empathy in a reproducible way for the community . The Reddit data has the advantage of being easily publicly available , of a very large scale ( 1.7B comments ) , and has already been used to train dialogue systems ( a few refs in the paper ) , so it \u2019 s good for the publicly available reproducible part . From interacting with Reddit-trained dialogue systems and looking at Reddit data , it indeed doesn \u2019 t seem very empathetic . There are two options from there : try and make that system more empathetic , and this is the approach we took in our work . The other one is to look for another background dataset , as you suggest . Unfortunately , there aren \u2019 t many publicly available corpora for training dialogue in a general domain , and their scale is at least an order of magnitude smaller than the Reddit one . We like your specific suggestion of \u201c Twitter or other social-network type datasets ( letting you follow people rather topics ) \u201d , and we indeed know of existing datasets from Twitter of a large scale , but they have shortcomings . First , tweets have low character limits , which is a constraint that doesn \u2019 t match the setting of general dialogue . Second , existing publicly released datasets are orders of magnitude lower in scale , and the conversations are very short . The Twitter corpus from Ritter et al 2010 [ 1 ] has 1.3 million conversations , 69 % of which have only 2 turns . Sordoni et al 2015 [ 2 ] used more than 100 million longer Twitter conversations , but the released Triples Twitter corpus unfortunately has fewer than 5k dialogues . A last point that we also clarified in our updated paper ( see response in common thread ) is that by nature , publicly available datasets from social media are quite different from one-on-one conversations , so even with a better background dataset from public social media , we would want to release data in a one-on-one setting . \u201c exchanges that are rather clich\u00e9 and overdone ( e.g. , Table 1 : the label \u201c afraid \u201d yields a situation that is rather spooky and unlikely in the real world , and the conversations themselves are rather clich\u00e9 and incorporate little details that would make them sound real ) . \u201d : the example we had used in Table 1 indeed gives that impression . Thanks for drawing our attention to that ; this impression is fortunately not representative of our dataset , which to us seems quite colorful . As stated in the general thread response , we have now added a sample of 10 randomly drawn conversations to give a better sense of what our dataset looks like . That random sample talks about 4 chicken species ( Australorps , Rhode Island Reds , Barred Plymouth Rocks , Welsummer ) , rabies causing sensitivity to light , enchiladas , English majors , too much coffee to start a shift , Tuesday breaks to buy lottery tickets with coworkers , etc . \u201c existing real-world datasets underrepresent rare emotions ( e.g. , afraid ) , but that \u2019 s just a reflection of how these emotions are distributed in the real world. \u201d : thank you for making us realize that we had not done a good enough job explaining the limitations of existing datasets for training empathetic conversations . We have updated the manuscript to clarify the nature of existing datasets and why they did not meet our needs . Please also refer to our response in the general thread for more clarifications on this point . [ 1 ] A. Ritter , C. Cherry , and B. Dolan . Unsupervised modeling of twitter conversations . In North American Chapter of the Association for Computational Linguistics ( NAACL 2010 ) , 2010 . [ 2 ] A. Sordoni , M. Galley , M. Auli , C. Brockett , Y. Ji , M. Mitchell , J. Nie , J. Gao , and B. Dolan . A neural network approach to context-sensitive generation of conversational responses . In Conference of the North American Chapter of the Association for Computational Linguistics ( NAACL-HLT 2015 ) , 2015 ."}, {"review_id": "HyesW2C9YQ-1", "review_text": "The paper describes a new study about how to make dialogs more empathetic. The work introduced a new dataset of 25k dialogs designed to evaluate the role that empathy recognition may play in generating better responses tuned to the feeling of the conversation partner. Several model set-ups, and many secondary options of the set-ups are evaluated. Pros: A lot of good thoughts were put into the work, and even though the techniques tried are relatively unsophisticated, the work represents a serious attempt on the subject and is of good reference value. The linkage between the use of emotion supervision and better relevancy is interesting. The dataset by itself is a good contribution to the community conducting studies in this area. Cons: The conclusions are somewhat fuzzy as there are too many effects interacting, and as a result no clear cut recommendations can be made (perhaps with the exception that ensembling a classifier model trained for emotion recognition together with the response selector is seen as having advantages). There are some detailed questions that are unaddressed or unclear from the writing. See the Misc. items below. Misc. P.1, 6th line from bottom: \"fro\" -> \"from\" Table 1: How is the \"situation description\" supposed to be related to the opening sentence of the speaker? In the examples there seems to be substantial overlap. Figure 2, distribution of the 32 emotion labels used: this is a very refined set that could get blurred at the boundaries between similar emotions. As for the creators of those dialogs, does everyone interpret the same emotion label the same way? e.g. angry, furious; confident, prepared; ...; will such potential ambiguities impact the work? One way to learn more about this is to aggregate related emotions to make a coarser set, and compare the results. Also, often an event may trigger multiple emotions, which one the speaker chooses to focus on may vary from person to person. How may ignoring the secondary emotions impact the results? To some extent this is leveraged by the prepending method (with top-K emotion predictions). What about the other two methods? P. 6, on using an existing emotion predictor: does it predict the same set of emotions that you are using in this work? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thoughtful feedback , which was very helpful to improve the paper . Please see our response in the general thread , which details our updates . Regarding your specific notes and questions : \u201c The conclusions are somewhat fuzzy as there are , and as a result no clear cut recommendations can be made \u201d : thanks for pointing that out -- we have extensively reorganized our paper to make the motivations and results clearer ; please also see our response in the general thread . \u201c How is the `` situation description '' supposed to be related to the opening sentence of the speaker ? In the examples there seems to be substantial Overlap. \u201d This was indeed unclear , thanks for pointing that out -- we have now clarified this . We asked the crowdsourced workers to start the conversation by describing their situation in a conversational way . Because of this , there often is overlap . Workers sometimes stuck to the situation description closely , while others were more creative about re-wording things ."}, {"review_id": "HyesW2C9YQ-2", "review_text": "Overall this paper contributes many interesting insights into the specific application of empathetic dialog into chatbot responses. The paper in particular is contributing its collected set of 25k empathetic dialogs, short semi-staged conversations around a particular seeded emotion and the results of various ways of incorporating this training set into a generative chatbot. While the results clearly do not solve the problem of automating emapthy, the paper does give insights into which methods perform better than others (Generation vs Retrieval) and explicitly adding emotion predictions vs using an ensemble of encoders. There is a lot in this paper, and I think it could have been better organized. I am more familiar with emotion related research and not language to language translation, so I would have appreciated a better explanation of the rationale for using BLEU scores. I did some online research to understand these Bilingual Evaluation Understudy Scores and while it seems like they measure sentence similarity, it is unclear how they capture \u201drelevance\u201d at least according to the brief tutorial that I read (https://machinelearningmastery.com/calculate-bleu-score-for-text-python/). I did not see the paper describing the use of this score in the references but perhaps I missed it \u2013 could you please clarify why this is a good metric for relevance? It seems that these scores are very sensitive to sentence variation. I am not sure if you can measure empathy or appropriateness of a response using this metric. For your data collection you have 810 participants and 24,850 conversations. Are the 810 participants all speakers or speakers and listeners combined? How many conversations did each speaker/listener pair perform 32? (one for each emotion) or 64? (two for each emotion) Was the number variable? If so what is the distribution of the contribution \u2013 e.g. did one worker generate 10,000 while several hundred workers did only three of four? Was it about even? Just for clarity \u2013 how did you enroll participants? Was it through AMT? What were the criteria for the workers? E.g. Native English speaker, etc. In your supplemental material, I found the interchanging of the words \u201ccontext\u201d and \u201cemotion\u201d confusing. The word context is used frequently throughout your manuscript: \u201cdialog context,\u201d \u201csituational context\u201d - emotions are different from situations, the situational utterance is the first utterance describing the emotion if I read your manuscript correctly. Table 6 should use \u201cLabel\u201d or \u201cEmotion\u201d instead of the more ambiguous \u201cContext.\u201d My understanding is that speakers were asked to write about a time when they experienced a particular feeling and they were given a choice of three feelings that they could write about. You then say that workers are forced to select from contexts they had not chosen before to ensure that all of the categories were used. From this I am assuming that each speaker/listener worker pair had to write about all 32 emotions \u2013 is this correct? Another interpretation of this is that you asked new workers to describe situations involving feelings that had not been chosen by other workers as data collection progressed to ensure that you had a balanced data set. This would imply that some emotional situations were less preferred and potentially more difficult to write about. It would be interesting if this data was presented. It might imply that some emotion labels are not as strong if people were forced to write about them rather than being able to choose to write about them. Were these dialogs ever actually annotated? You state in section 2, Related Work \u201cwe train models for emotion detection on conversation data that has been explicitly labeled by annotators\u201d \u2013 please describe how this was done. Did independent third party annotators review the dialogs for label correctness? Was a single rater or a majority vote used to decide the final label. For example, in Table 1, the label \u201cAfraid\u201d is given to a conversation that could also have reasonable been generated by the label \u201cAnxious\u201d a word explicitly used in the dialog. I am guessing that the dialogs are just labeled according to the label / provocation word and that they were not annotated beyond that, but please make this clear. In the last paragraph you state \u201cA few works focus..\u201d and then list 5. This should rather be \u201cseveral other works have focused on \u201c \u2026 Conversely, you later state in section 3 \u201cSpeaker and Listener\u201d, \u201cWe include a few example conversations from the training data in Table 1,\u201d this should more explicitly be \u201ctwo.\u201d Also in section 3 when you describe your cross validation process, you state \u201cWe split the conversations into approximately 80/10/10 partitions. To prevent overlap of <<discussed topics>> we split the data so that all the sets of conversations with the same speaker providing the prompt would be in the same partition. In your supplemental material you state that workers were paired. Each worker is asked to write a prompt, which also seems to be the first utterance in the dialog they will start. You state each worker selects one emotion word from a list of three which is somehow generated (randomly?) form your list of 32 . I am assuming each worker in the pair does this, then the pair has a two \u201cconversations\u201d one where the first worker is the speaker and another where the second worker is the speaker \u2013 is this correct? It is not entirely clear from the description. Given that you have 810 workers and 24,850 conversations, I am assuming that each worker had more than one conversation. My question is - did they generate a new prompt / first utterance for each conversations. I am assuming yes since you say there are 24,850 prompts/conversations. For each user are all of the situation/prompts they generate describing the same emotion context? E.g. would one worker write ~30 conversations on the same emotion. This seems unlikely, and it seems more likely that given the number of conversations ~30 per participant is similar to the number of emotion words that you asked each worker to cycle through nearly all of the emotions or that given they were able to select, they might describe the same emotion, e.g. \u201cfear\u201d several times. If the same worker was allowed to select the same emotion context multiple times was it found that they re-used the same prompt several times? I am assuming that this is the case and that this is what you mean when you say that you \u201cprevent overlap of discussed topics\u201d between sets when you exclude particular workers. Is this correct? Or did you actually look and code the discussed topics to ensure no overlap even across workers (e.g. several people might have expressed fear of heights or fear of the dark). In section 4, Empathetic dialog generator, you state that the dialog model has access to the situation description given by the speaker (also later called the situational prompt) but not the emotion word prompt. Calling these both prompts makes the statement about 24,850 prompts/conversations a bit ambiguous. A better statement would be 24,850 conversations based on unique situational prompts/descriptions (if they are in fact unique situational prompts. I am assuming they are not if you are worried about overlapping \u201cdiscussed topics\u201d which I am assuming are the situational prompts since the dialogs are very short and heavily keyed off these initial situational prompts) In your evaluation of the models with Human ratings you describe two sets of tests. In one test you say you collect 100 annotations per model. More explicitly, did you select 100 situational prompts and then ask workers to rate the response of each model? Was how many responses was each worker shown? How many workers were used? Are the highlighted numbers the only significant findings or just the max scores? Annotations is probably not the correct word here. Please also describe your process for assigning workers to the second human ratings task. Since the two novel aspects of your paper are the new dataset and the use of this dataset to create more empathetic chatbot responses (\"I know the feeling\") I have focused on these aspects of the paper in my review. I found the inclusion of Table 7 underexplained in the text. The emotion labels for all these datasets are not directly comparable so I would have liked to have seen more explanation around how these classifications were compared. It would also be helpful to know how more similar emotions such as \"afraid\" and \"anxious\" were scored vs \"happy\" and \"sad\" confusions ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your thoughtful feedback and pointing out many places where more experimental details or clarifications would be useful , and where we had been inconsistent in our terminology . We addressed your points and incorporated your corrections to the updated manuscript ; please find detailed responses below . \u201c I think it could have been better organized. \u201d : we indeed have extensively re-organized the paper , as detailed in our response in the general thread . \u201c \u201c I would have appreciated a better explanation of the rationale for using BLEU scores . I did some online research to understand these Bilingual Evaluation Understudy Scores and while it seems like they measure sentence similarity , it is unclear how they capture \u201d relevance \u201d at least according to the brief tutorial that I read ( https : //machinelearningmastery.com/calculate-bleu-score-for-text-python/ ) . \u201d : : We truly appreciate your thoughtfulness in taking the time to consult background information about BLEU -- we indeed should have included the standard reference for BLEU , Papineni et al [ 4 ] , which we are adding to the manuscript . We use BLEU as an evaluation metric because it has been frequently used in other dialogue generation papers as an automated evaluation ( Li et.al 2016 [ 1 ] cited in our manuscript , Wen et al 2015 [ 2 ] , Li et al 2015 [ 3 ] , to name just a few ) , and we are adding this as well to the manuscript . However , we definitely agree that word overlap scores do not always align with human judgement , which has been documented in other works such as the \u201c How Not to Evaluate your Dialogue System \u201d paper [ Liu et al.2016 ] that we mentioned in our discussion of the human evaluation set-up . This is why we include human evaluation in addition to the commonly used automated metrics . Crowdsourcing process : Yes , we did use Amazon Mturk for recruiting workers , and required that all of our participants came from the US . Each pair of workers contributed to at least two conversations , that could have been about the same or different emotion labels , depending on which words they were offered and which words they selected . Individual workers did not have to have conversations about all 32 emotions ; instead , coverage of all emotions was ensured by offering more often the emotions that had been selected less overall . We have added a lot of details about the procedure in the manuscript ( in the Appendix A2 and A4 sections , as well as in section 3 ) . As to your specific questions : the median number of conversations per worker was 8 , while the average was 61 . Thus , there were definitely a handful of workers who were more actively participating . To ensure quality , we hand-checked random subsets of conversations by our most-frequent workers . They were allowed to participate in as many of these HITs as they wanted for the first ~15k conversations , then we added qualifications to limit the more \u201c frequently active \u201d workers to a set number of conversations ( 100 per worker ) . We have added that information to the crowdsourcing description in the appendix , as well as a larger random sample of conversations from the dataset . We would like to clarify what the HITs look like . In each HIT a worker is first taken to a screen where they are shown 3 emotion words . At first the 3 words were sampled randomly , but as the crowdsourcing data generation process went on , we showed the 3 words that had overall been picked the least so far for a first-time worker , or the 3 that had been used the least for that worker if the worker had already performed the task before , so as to ensure better coverage of all emotion labels . It is true that this makes worker select emotions that they might not spontaneously have preferred , but we observed an initial bias for situations that were easier to describe ( e.g , a situation causing surprise ) , and we thought our dataset would be more useful for training versatile dialogue models if all emotion words were covered in a more balanced way . \u201c This would imply that some emotional situations were less preferred and potentially more difficult to write about . It would be interesting if this data was presented . \u201c : While we are not presenting the initial imbalance , or commenting on it in the paper , its residual effect ( as well as the fact that workers still had a choice between 3 words , so could effectively exclude ever working on 2 ) can still be observed in the slight imbalance of our set , in Table 7 of the Appendix , where the labels are ordered by decreasing frequencies . Workers picked one emotion word among the 3 offered ( their own choice ) and wrote a description of a time they felt that way . Then , they were taken to another screen where they were paired randomly with another worker who had just completed the same process . They took turns starting two conversations . Each worker had to describe their situation as part of the conversation they started . After that , they answered a few brief feedback questions which helped monitor quality ."}], "0": {"review_id": "HyesW2C9YQ-0", "review_text": "The overall goal of the paper is to make end-to-end dialogue systems more empathetic, so that they can respond more appropriately and in ways that acknowledge how the users are feeling. The authors make two contributions towards that goal: (1) they introduce a crowdsourced dataset (EmpatheticDialogue) annotated with fine-grained emotion labels. (2) They show improvements on dialogue generation (in terms of empathy, but also relevance and fluency) using a multi-task objective, ensemble of encoders, and a more ad-hoc technique that consists of prepending inferred emotion labels to the input. In terms of technical novelty, the work is relatively incremental: (A) The use of multi-task objectives in sequence models [1] is relatively common nowadays (there is little mathematical details in the paper, so it\u2019s hard to see how the approach of the paper really differs from extensive related work.). (B) Prepending predictions: prepending class labels to the input is also relatively common (e.g., in multilingual NMT to select a language). [2] presents a similar approach for polite response generation, where they prepend a label using a politeness classifier. I also have some doubts about the two claimed contributions of the paper (the authors actually list 3 contributions in the introduction, but for convenience I lump the 2 non-data ones together): (1) Dataset: The dataset was crowdsourced by giving workers an emotion label (e.g., afraid) and asking them to define a situation in which that emotion might occur and inviting them to have a conversation on that situation. The problem with prompting workers for specific emotions is that this assumes they are good actors and this is likely to produce exchanges that are rather clich\u00e9 and overdone (e.g., Table 1: the label \u201cafraid\u201d yields a situation that is rather spooky and unlikely in the real world, and the conversations themselves are rather clich\u00e9 and incorporate little details that would make them sound real). The authors justify this dataset by pointing out that existing real-world datasets underrepresent rare emotions (e.g., afraid), but that\u2019s just a reflection of how these emotions are distributed in the real world. Better subsampling strategies would enable a better balance in the distribution without having to give up on real-world data (filtering using emojis, hashtags, etc.). As the paper shows quantitative gains using this dataset, it is probably ok to use but, qualitatively, this dataset is probably not for everyone working on emotion in NLP. (2) Improvement in empathetic dialogue generation: The paper shows improvements across the board compared to a Transformer baseline, but the question the authors do not satisfactorily address is whether their explicit (and I would say sometimes ad-hoc) treatment of empathy (e.g., using emotion classifier, etc.) is crucially needed to get better empathetic dialogues, since the authors did not control for training data size and model capacity. Indeed, the authors exploited different amounts of data (out of-domain, or both in- and out-of-domain), different model capacities (going from baseline Transformer to model ensembles), and sometimes richer input (e.g., pre-trained emotion classifier). The results might only be showing that more data or more model capacity helps, which would of course not be surprising at all. The fact that generated outputs improve in all aspects (not only empathy, but in attributes completely unrelated to empathy such as fluency and relevance) suggests that the improvement is due to more data or capacity (e.g., perhaps yielding better encoder). More statistics in the table in terms of number of parameters and amount of in- and out-of-domain data used for each experiment would help draw a clearer picture. About the use of Reddit: this might not be the best background dataset, as it\u2019s mostly strangers talking to other strangers, presumably causing the baseline to be weak on empathy. Twitter or other social-network type datasets (letting you follow people rather topics) *might* be better suited as it comparatively involves more exchanges between people who actually know each other and who are thus more likely to behave empathetically. Overall, the paper doesn\u2019t really attempt to make major technical contribution, and instead (1) introduces a dataset and (2) makes empirical contributions, but I think there are problems with both. Typos: Introduction: \u201cfro\u201d References: Elizaa [1] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser Multi-task Sequence to Sequence Learning https://arxiv.org/abs/1511.06114 [2] Tong Niu and Mohit Bansal Polite Dialogue Generation Without Parallel Data https://arxiv.org/pdf/1805.03162.pdf", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your insightful and detailed review . While we respectfully disagree with some of the points ( and will detail why below ) , we appreciate that in most cases the fault was lying with us not having clarified those arguments in the manuscript , which we have now done . For others , your insightful questions led us to organize our experiments better and supplement them with new ones which collectively make for a clearer picture . To your detailed points , let us start with one of your last observations : \u201c About the use of Reddit : this might not be the best background dataset , as it \u2019 s mostly strangers talking to other strangers , presumably causing the baseline to be weak on empathy . \u201c We definitely agree that we would expect Reddit to be weak on empathy , but ( 1 ) as stated in our response in the main thread , there isn \u2019 t currently an empathy benchmark that we know of to actually quantify that , and ( 2 ) we wish for publicly available resources to train a conversation system that would respond with empathy in a reproducible way for the community . The Reddit data has the advantage of being easily publicly available , of a very large scale ( 1.7B comments ) , and has already been used to train dialogue systems ( a few refs in the paper ) , so it \u2019 s good for the publicly available reproducible part . From interacting with Reddit-trained dialogue systems and looking at Reddit data , it indeed doesn \u2019 t seem very empathetic . There are two options from there : try and make that system more empathetic , and this is the approach we took in our work . The other one is to look for another background dataset , as you suggest . Unfortunately , there aren \u2019 t many publicly available corpora for training dialogue in a general domain , and their scale is at least an order of magnitude smaller than the Reddit one . We like your specific suggestion of \u201c Twitter or other social-network type datasets ( letting you follow people rather topics ) \u201d , and we indeed know of existing datasets from Twitter of a large scale , but they have shortcomings . First , tweets have low character limits , which is a constraint that doesn \u2019 t match the setting of general dialogue . Second , existing publicly released datasets are orders of magnitude lower in scale , and the conversations are very short . The Twitter corpus from Ritter et al 2010 [ 1 ] has 1.3 million conversations , 69 % of which have only 2 turns . Sordoni et al 2015 [ 2 ] used more than 100 million longer Twitter conversations , but the released Triples Twitter corpus unfortunately has fewer than 5k dialogues . A last point that we also clarified in our updated paper ( see response in common thread ) is that by nature , publicly available datasets from social media are quite different from one-on-one conversations , so even with a better background dataset from public social media , we would want to release data in a one-on-one setting . \u201c exchanges that are rather clich\u00e9 and overdone ( e.g. , Table 1 : the label \u201c afraid \u201d yields a situation that is rather spooky and unlikely in the real world , and the conversations themselves are rather clich\u00e9 and incorporate little details that would make them sound real ) . \u201d : the example we had used in Table 1 indeed gives that impression . Thanks for drawing our attention to that ; this impression is fortunately not representative of our dataset , which to us seems quite colorful . As stated in the general thread response , we have now added a sample of 10 randomly drawn conversations to give a better sense of what our dataset looks like . That random sample talks about 4 chicken species ( Australorps , Rhode Island Reds , Barred Plymouth Rocks , Welsummer ) , rabies causing sensitivity to light , enchiladas , English majors , too much coffee to start a shift , Tuesday breaks to buy lottery tickets with coworkers , etc . \u201c existing real-world datasets underrepresent rare emotions ( e.g. , afraid ) , but that \u2019 s just a reflection of how these emotions are distributed in the real world. \u201d : thank you for making us realize that we had not done a good enough job explaining the limitations of existing datasets for training empathetic conversations . We have updated the manuscript to clarify the nature of existing datasets and why they did not meet our needs . Please also refer to our response in the general thread for more clarifications on this point . [ 1 ] A. Ritter , C. Cherry , and B. Dolan . Unsupervised modeling of twitter conversations . In North American Chapter of the Association for Computational Linguistics ( NAACL 2010 ) , 2010 . [ 2 ] A. Sordoni , M. Galley , M. Auli , C. Brockett , Y. Ji , M. Mitchell , J. Nie , J. Gao , and B. Dolan . A neural network approach to context-sensitive generation of conversational responses . In Conference of the North American Chapter of the Association for Computational Linguistics ( NAACL-HLT 2015 ) , 2015 ."}, "1": {"review_id": "HyesW2C9YQ-1", "review_text": "The paper describes a new study about how to make dialogs more empathetic. The work introduced a new dataset of 25k dialogs designed to evaluate the role that empathy recognition may play in generating better responses tuned to the feeling of the conversation partner. Several model set-ups, and many secondary options of the set-ups are evaluated. Pros: A lot of good thoughts were put into the work, and even though the techniques tried are relatively unsophisticated, the work represents a serious attempt on the subject and is of good reference value. The linkage between the use of emotion supervision and better relevancy is interesting. The dataset by itself is a good contribution to the community conducting studies in this area. Cons: The conclusions are somewhat fuzzy as there are too many effects interacting, and as a result no clear cut recommendations can be made (perhaps with the exception that ensembling a classifier model trained for emotion recognition together with the response selector is seen as having advantages). There are some detailed questions that are unaddressed or unclear from the writing. See the Misc. items below. Misc. P.1, 6th line from bottom: \"fro\" -> \"from\" Table 1: How is the \"situation description\" supposed to be related to the opening sentence of the speaker? In the examples there seems to be substantial overlap. Figure 2, distribution of the 32 emotion labels used: this is a very refined set that could get blurred at the boundaries between similar emotions. As for the creators of those dialogs, does everyone interpret the same emotion label the same way? e.g. angry, furious; confident, prepared; ...; will such potential ambiguities impact the work? One way to learn more about this is to aggregate related emotions to make a coarser set, and compare the results. Also, often an event may trigger multiple emotions, which one the speaker chooses to focus on may vary from person to person. How may ignoring the secondary emotions impact the results? To some extent this is leveraged by the prepending method (with top-K emotion predictions). What about the other two methods? P. 6, on using an existing emotion predictor: does it predict the same set of emotions that you are using in this work? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thoughtful feedback , which was very helpful to improve the paper . Please see our response in the general thread , which details our updates . Regarding your specific notes and questions : \u201c The conclusions are somewhat fuzzy as there are , and as a result no clear cut recommendations can be made \u201d : thanks for pointing that out -- we have extensively reorganized our paper to make the motivations and results clearer ; please also see our response in the general thread . \u201c How is the `` situation description '' supposed to be related to the opening sentence of the speaker ? In the examples there seems to be substantial Overlap. \u201d This was indeed unclear , thanks for pointing that out -- we have now clarified this . We asked the crowdsourced workers to start the conversation by describing their situation in a conversational way . Because of this , there often is overlap . Workers sometimes stuck to the situation description closely , while others were more creative about re-wording things ."}, "2": {"review_id": "HyesW2C9YQ-2", "review_text": "Overall this paper contributes many interesting insights into the specific application of empathetic dialog into chatbot responses. The paper in particular is contributing its collected set of 25k empathetic dialogs, short semi-staged conversations around a particular seeded emotion and the results of various ways of incorporating this training set into a generative chatbot. While the results clearly do not solve the problem of automating emapthy, the paper does give insights into which methods perform better than others (Generation vs Retrieval) and explicitly adding emotion predictions vs using an ensemble of encoders. There is a lot in this paper, and I think it could have been better organized. I am more familiar with emotion related research and not language to language translation, so I would have appreciated a better explanation of the rationale for using BLEU scores. I did some online research to understand these Bilingual Evaluation Understudy Scores and while it seems like they measure sentence similarity, it is unclear how they capture \u201drelevance\u201d at least according to the brief tutorial that I read (https://machinelearningmastery.com/calculate-bleu-score-for-text-python/). I did not see the paper describing the use of this score in the references but perhaps I missed it \u2013 could you please clarify why this is a good metric for relevance? It seems that these scores are very sensitive to sentence variation. I am not sure if you can measure empathy or appropriateness of a response using this metric. For your data collection you have 810 participants and 24,850 conversations. Are the 810 participants all speakers or speakers and listeners combined? How many conversations did each speaker/listener pair perform 32? (one for each emotion) or 64? (two for each emotion) Was the number variable? If so what is the distribution of the contribution \u2013 e.g. did one worker generate 10,000 while several hundred workers did only three of four? Was it about even? Just for clarity \u2013 how did you enroll participants? Was it through AMT? What were the criteria for the workers? E.g. Native English speaker, etc. In your supplemental material, I found the interchanging of the words \u201ccontext\u201d and \u201cemotion\u201d confusing. The word context is used frequently throughout your manuscript: \u201cdialog context,\u201d \u201csituational context\u201d - emotions are different from situations, the situational utterance is the first utterance describing the emotion if I read your manuscript correctly. Table 6 should use \u201cLabel\u201d or \u201cEmotion\u201d instead of the more ambiguous \u201cContext.\u201d My understanding is that speakers were asked to write about a time when they experienced a particular feeling and they were given a choice of three feelings that they could write about. You then say that workers are forced to select from contexts they had not chosen before to ensure that all of the categories were used. From this I am assuming that each speaker/listener worker pair had to write about all 32 emotions \u2013 is this correct? Another interpretation of this is that you asked new workers to describe situations involving feelings that had not been chosen by other workers as data collection progressed to ensure that you had a balanced data set. This would imply that some emotional situations were less preferred and potentially more difficult to write about. It would be interesting if this data was presented. It might imply that some emotion labels are not as strong if people were forced to write about them rather than being able to choose to write about them. Were these dialogs ever actually annotated? You state in section 2, Related Work \u201cwe train models for emotion detection on conversation data that has been explicitly labeled by annotators\u201d \u2013 please describe how this was done. Did independent third party annotators review the dialogs for label correctness? Was a single rater or a majority vote used to decide the final label. For example, in Table 1, the label \u201cAfraid\u201d is given to a conversation that could also have reasonable been generated by the label \u201cAnxious\u201d a word explicitly used in the dialog. I am guessing that the dialogs are just labeled according to the label / provocation word and that they were not annotated beyond that, but please make this clear. In the last paragraph you state \u201cA few works focus..\u201d and then list 5. This should rather be \u201cseveral other works have focused on \u201c \u2026 Conversely, you later state in section 3 \u201cSpeaker and Listener\u201d, \u201cWe include a few example conversations from the training data in Table 1,\u201d this should more explicitly be \u201ctwo.\u201d Also in section 3 when you describe your cross validation process, you state \u201cWe split the conversations into approximately 80/10/10 partitions. To prevent overlap of <<discussed topics>> we split the data so that all the sets of conversations with the same speaker providing the prompt would be in the same partition. In your supplemental material you state that workers were paired. Each worker is asked to write a prompt, which also seems to be the first utterance in the dialog they will start. You state each worker selects one emotion word from a list of three which is somehow generated (randomly?) form your list of 32 . I am assuming each worker in the pair does this, then the pair has a two \u201cconversations\u201d one where the first worker is the speaker and another where the second worker is the speaker \u2013 is this correct? It is not entirely clear from the description. Given that you have 810 workers and 24,850 conversations, I am assuming that each worker had more than one conversation. My question is - did they generate a new prompt / first utterance for each conversations. I am assuming yes since you say there are 24,850 prompts/conversations. For each user are all of the situation/prompts they generate describing the same emotion context? E.g. would one worker write ~30 conversations on the same emotion. This seems unlikely, and it seems more likely that given the number of conversations ~30 per participant is similar to the number of emotion words that you asked each worker to cycle through nearly all of the emotions or that given they were able to select, they might describe the same emotion, e.g. \u201cfear\u201d several times. If the same worker was allowed to select the same emotion context multiple times was it found that they re-used the same prompt several times? I am assuming that this is the case and that this is what you mean when you say that you \u201cprevent overlap of discussed topics\u201d between sets when you exclude particular workers. Is this correct? Or did you actually look and code the discussed topics to ensure no overlap even across workers (e.g. several people might have expressed fear of heights or fear of the dark). In section 4, Empathetic dialog generator, you state that the dialog model has access to the situation description given by the speaker (also later called the situational prompt) but not the emotion word prompt. Calling these both prompts makes the statement about 24,850 prompts/conversations a bit ambiguous. A better statement would be 24,850 conversations based on unique situational prompts/descriptions (if they are in fact unique situational prompts. I am assuming they are not if you are worried about overlapping \u201cdiscussed topics\u201d which I am assuming are the situational prompts since the dialogs are very short and heavily keyed off these initial situational prompts) In your evaluation of the models with Human ratings you describe two sets of tests. In one test you say you collect 100 annotations per model. More explicitly, did you select 100 situational prompts and then ask workers to rate the response of each model? Was how many responses was each worker shown? How many workers were used? Are the highlighted numbers the only significant findings or just the max scores? Annotations is probably not the correct word here. Please also describe your process for assigning workers to the second human ratings task. Since the two novel aspects of your paper are the new dataset and the use of this dataset to create more empathetic chatbot responses (\"I know the feeling\") I have focused on these aspects of the paper in my review. I found the inclusion of Table 7 underexplained in the text. The emotion labels for all these datasets are not directly comparable so I would have liked to have seen more explanation around how these classifications were compared. It would also be helpful to know how more similar emotions such as \"afraid\" and \"anxious\" were scored vs \"happy\" and \"sad\" confusions ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your thoughtful feedback and pointing out many places where more experimental details or clarifications would be useful , and where we had been inconsistent in our terminology . We addressed your points and incorporated your corrections to the updated manuscript ; please find detailed responses below . \u201c I think it could have been better organized. \u201d : we indeed have extensively re-organized the paper , as detailed in our response in the general thread . \u201c \u201c I would have appreciated a better explanation of the rationale for using BLEU scores . I did some online research to understand these Bilingual Evaluation Understudy Scores and while it seems like they measure sentence similarity , it is unclear how they capture \u201d relevance \u201d at least according to the brief tutorial that I read ( https : //machinelearningmastery.com/calculate-bleu-score-for-text-python/ ) . \u201d : : We truly appreciate your thoughtfulness in taking the time to consult background information about BLEU -- we indeed should have included the standard reference for BLEU , Papineni et al [ 4 ] , which we are adding to the manuscript . We use BLEU as an evaluation metric because it has been frequently used in other dialogue generation papers as an automated evaluation ( Li et.al 2016 [ 1 ] cited in our manuscript , Wen et al 2015 [ 2 ] , Li et al 2015 [ 3 ] , to name just a few ) , and we are adding this as well to the manuscript . However , we definitely agree that word overlap scores do not always align with human judgement , which has been documented in other works such as the \u201c How Not to Evaluate your Dialogue System \u201d paper [ Liu et al.2016 ] that we mentioned in our discussion of the human evaluation set-up . This is why we include human evaluation in addition to the commonly used automated metrics . Crowdsourcing process : Yes , we did use Amazon Mturk for recruiting workers , and required that all of our participants came from the US . Each pair of workers contributed to at least two conversations , that could have been about the same or different emotion labels , depending on which words they were offered and which words they selected . Individual workers did not have to have conversations about all 32 emotions ; instead , coverage of all emotions was ensured by offering more often the emotions that had been selected less overall . We have added a lot of details about the procedure in the manuscript ( in the Appendix A2 and A4 sections , as well as in section 3 ) . As to your specific questions : the median number of conversations per worker was 8 , while the average was 61 . Thus , there were definitely a handful of workers who were more actively participating . To ensure quality , we hand-checked random subsets of conversations by our most-frequent workers . They were allowed to participate in as many of these HITs as they wanted for the first ~15k conversations , then we added qualifications to limit the more \u201c frequently active \u201d workers to a set number of conversations ( 100 per worker ) . We have added that information to the crowdsourcing description in the appendix , as well as a larger random sample of conversations from the dataset . We would like to clarify what the HITs look like . In each HIT a worker is first taken to a screen where they are shown 3 emotion words . At first the 3 words were sampled randomly , but as the crowdsourcing data generation process went on , we showed the 3 words that had overall been picked the least so far for a first-time worker , or the 3 that had been used the least for that worker if the worker had already performed the task before , so as to ensure better coverage of all emotion labels . It is true that this makes worker select emotions that they might not spontaneously have preferred , but we observed an initial bias for situations that were easier to describe ( e.g , a situation causing surprise ) , and we thought our dataset would be more useful for training versatile dialogue models if all emotion words were covered in a more balanced way . \u201c This would imply that some emotional situations were less preferred and potentially more difficult to write about . It would be interesting if this data was presented . \u201c : While we are not presenting the initial imbalance , or commenting on it in the paper , its residual effect ( as well as the fact that workers still had a choice between 3 words , so could effectively exclude ever working on 2 ) can still be observed in the slight imbalance of our set , in Table 7 of the Appendix , where the labels are ordered by decreasing frequencies . Workers picked one emotion word among the 3 offered ( their own choice ) and wrote a description of a time they felt that way . Then , they were taken to another screen where they were paired randomly with another worker who had just completed the same process . They took turns starting two conversations . Each worker had to describe their situation as part of the conversation they started . After that , they answered a few brief feedback questions which helped monitor quality ."}}