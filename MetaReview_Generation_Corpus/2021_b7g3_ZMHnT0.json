{"year": "2021", "forum": "b7g3_ZMHnT0", "title": "Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation", "decision": "Accept (Poster)", "meta_review": "The paper's main message is that some existing NLP techniques that claim to improve performance by the use of a knowledge graph may not achieve this improved performance because of the knowledge graph or at least the explanation given may be questionable.  This is thought provoking and it will incite the community to think more carefully about the real factors of improved performance.  The initial version of the paper was not well written, but the authors improved the writing significantly.  The paper includes a thorough empirical evaluation to support the main message.  I have read the paper and I believe that this work will be of interest to a diverse audience.", "reviews": [{"review_id": "b7g3_ZMHnT0-0", "review_text": "The paper presents an interesting finding that some of the existing KG-augmented models , such as those for QA and item recommendation , may not actually capture or leverage the semantics in KGs , and their performance improvement can not be attributed to the usage of additional knowledge . I think this finding is of some significance . Pros : 1 ) The paper presents four heuristic methods and an RL-based method for KG perturbation . It obtains some interesting and noteworthy findings based on the experimental results in terms of QA and recommender systems . Cons : 1 ) In my opinion , perturbing KGs by randomly or heuristically changing existing edges is not well-motivated . Although it supports the experimental study in this paper and reveals several interesting findings , it has very little practical significance , because the perturbed KG would inevitably contain many invalid and incorrect facts without positive effects for downstream applications . Hence , the technical contribution of the proposed heuristic strategies and RL model for KG perturbation is not significant . 2 ) The analysis for experimental results is somewhat superficial . It fails to provide deep insights into why some models can still work with the perturbed KG . The authors should provide more analysis to explain the experimental results that are against common sense . For example , an experiment conclusion says that `` it is the ( false ) connection between entities instead of the semantic stored in the KG that leads to the improvement over non-KG baseline . '' Why ? I think this is an interesting and noteworthy finding , but no in-depth analysis is given . Besides , I also have a minor question , i.e. , how does the edge deletion method perturb all the triples ( 100 % ) in a KG ? How many triples are left after edge deletion ? Some typos : 1 ) these false connection - > these false connections 2 ) Relation Swapping ( RS ) - > Relation Swapping ( RS ) Anyway , the paper presents a noteworthy finding , which calls for further investigation into what information from the KGs is actually captured to improve the neural-symbolic models . So , I would like to recommend a weak acceptance of this paper . after rebuttal Thank the authors for their response which addressed my concerns . Based on the response , the revision , and other reviews , I would like to keep my score unchanged at 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback ! Below are responses to your comments/questions . -- * * 3.1 : In my opinion , perturbing KGs by randomly or heuristically changing existing edges is not well-motivated . Although it supports the experimental study in this paper and reveals several interesting findings , it has very little practical significance , because the perturbed KG would inevitably contain many invalid and incorrect facts without positive effects for downstream applications . Hence , the technical contribution of the proposed heuristic strategies and RL model for KG perturbation is not significant . * * By showing that KG-augmented neural-symbolic models perform well on perturbed KGs , we disprove existing assumptions about how models use KG info and about the plausibility of explanations provided by such models . Contrary to popular belief , KG-augmented models can still work even when the KG has been perturbed so much that humans can not understand the models \u2019 explanations . In light of this , the motivation of our paper is to guide future work on designing models that use KG info effectively and provide plausible explanations . Note that the proposed KG perturbation methods merely serve as analytical tools and are not intended to directly improve performance or explanation quality . Additionally , our results suggest that KG-augmented neural-symbolic models can be robust to noisy KG data . Even when the KG contains a fairly small amount of signal , the models are somehow able to leverage it . This could be a useful property in situations where it is not practical to obtain fully clean KG annotations . We have updated our paper to explicitly describe this motivation . Please refer to * Section 7 : Conclusion * . * * 3.2 : The analysis for experimental results is somewhat superficial . It fails to provide deep insights into why some models can still work with the perturbed KG . The authors should provide more analysis to explain the experimental results that are against common sense . For example , an experiment conclusion says that `` it is the ( false ) connection between entities instead of the semantic stored in the KG that leads to the improvement over non-KG baseline . '' Why ? I think this is an interesting and noteworthy finding , but no in-depth analysis is given . * * Good question ! First , subgraph extraction is heuristic-based , so some relevant info in the original KG may not be retrievable unless the KG itself is changed . Meanwhile , the RL-based KG perturbation methods are trained to maximize performance , which may enable the KG to be modified such that more relevant info is retrievable using the subgraph extraction heuristics . Second , in our experiments , relation-based perturbations ( RS , RR , RL-RR ) generally outperform edge-based perturbations ( ER , ED ) . Also , we have found that the original KG can contain noisy relation annotations which are sometimes \u201c corrected \u201d by relation-based perturbations . In certain cases , this may result in the perturbed KG achieving slightly higher performance than the original KG ( RR and RL-RR for RN-CSQA ; RL-RR for Last.FM ) . Similarly , in our user study , despite all questions being correctly answered by the model , there were some RL-RR explanations that received higher readability/usability ratings than their original KG counterparts . Although the original KG achieved higher human ratings than the RL-RR KG did overall , both KGs still achieved relatively low ratings with respect to our scales . While our main argument centers on NSKG models ' flaws , this counterintuitive finding suggests that KGs themselves are flawed too , but in a way that can be systematically corrected . We \u2019 ve updated the paper to include this analysis , which we agree would make our claims more convincing . Please refer to * Section 5.3 : Auxiliary Experiments and Analysis -- Paragraph \u201c Why do perturbed KGs sometimes perform better than the original KG ? \u201d * . * * 3.3 : Besides , I also have a minor question , i.e. , how does the edge deletion method perturb all the triples ( 100 % ) in a KG ? How many triples are left after edge deletion ? * * Good catch . For edge deletion , at 100 % perturbation , we delete all but 10 edges . We \u2019 ve updated the paper to include this info . Please refer to * Section 3.1 : Heuristic-Based KG Perturbation * ."}, {"review_id": "b7g3_ZMHnT0-1", "review_text": "The paper provides a number of adversarial attacks on hybrid neural-symbolic systems . The systems are recommender and QA systems which use an underlying knowledge-graph ( KG ) such as ConceptNet . Previous work has suggested that the KGs are important for good performance , and moreover that the use of KGs lends the system a degree of interpretability . The attacks are successful - maintaining performance whilst seriously degrading the KG - throwing doubt on these claims . Two main approaches to attacking the systems are followed : a simple heuristic method in which labels in the KG are modified randomly , and a more sophisticated method in which deep reinforcement learning is used to learn an optimal policy to change the KG whilst maintaining good performance on the task . Overall I have a lot of sympathy for the motivation of this paper . There is increasing evidence that the attempted use of symbolic methods in hybrid systems , and in particular the use of symbolic methods to provide explanations , is just picking up on incidental properties of the data and exploiting the power of the deep network in ways unrelated to the symbolic representations . This paper provides further compelling evidence . However , the paper is currently an extremely frustrating read , and it took me a number of attempts to get through the paper to write this review . Part of the problem is that the authors have tried to cram in too much material . I would have preferred to have seen fewer experiments described , but in more detail , with the remainder briefly mentioned in a paragraph or two , or perhaps in an appendix ( if that 's allowed for ICLR ) . The other problem is that the presentation in the paper is poor . Part of that is down to the non-native English in parts ( which is not the fault of the authors ) , but part of it is also just down to sloppiness in the presentation . More detailed comments -- A KG is denoted as G = ( E , R , T ) , where E and R are the entity set and relation set respectively - this is a little confusing since T is also the set of relations ( tuples over E ) . R is the set of relation * labels * ? Finally , both c and k are concatenated for calculating the plausibility score - [ presuambly the concatenation is then put through an MLP ? ] In both graph encoders , the subgraph G ( q , a ) together with the aggregation weights - [ neither of the descriptions mention aggregation weights ] while the task is to predict the unobserved interaction ( yuv = 0 ) . - [ does the zero here mean that the interaction is unobserved , or that the user has not engaged with the item ? ( or both ? ) ] where the aggregation weights are personalized for u - [ you need to define how the aggregation works ] We randomly choose two triples ( edges ) in the KG and swap their relations . - [ it would be good to see some actual examples here from one of the applications/KGs . ] where sG ( \u00b7 ) is a KG scoring function trained on G. - [ is this defined anywhere ? ] The font in table 1 is really small . If you get more room in a final version please turn this into two tables . in-house test accuracy - what 's an `` in-house '' accuracy ? We then compute the relation specific clustering coefficient vector c^r - do you define this anywhere ? OpenBookQA ( OBQA ) - do you give a reference ? We randomly select 10 questions - this seems like a small sample on which to peform the human evaluation . This evaluation would be more persuasive with a larger sample ( even 20 or 30 ) . Typos etc . ( not exhaustive ) -- despite a deceptive symbolic structure - [ I 'm not sure that `` deceptive '' is the right word here ( and lots of places elsewhere ) , just something like `` incorrect '' may be better . ] that KG - > the KG The preliminary results indicate that KG can be easily manipulated and lost its benefit - > lose It brings more worrisome scenario - [ rephrase ] without noticeable performance drop - > without a noticeable performance drop In specific - > More specifically e.g. , commonsense question answering ( QA ) and recommender system , - > i.e.aggregating its neighbors \u2019 embedding - > aggregating its neighbors \u2019 embeddings This heuristic changes the semantic - > semantics This heuristic also does not perturb KG \u2019 s structure but its semantic - > This heuristic also does not perturb KG \u2019 s structure but its semantics on both of commonsense QA and recommendation system tasks - > on both commonsense QA and recommendation system tasks since one of our goal - > goals have captured the information of KG . - > the KG sequentially to LSTM - > sequentially to the LSTM Evaluating the KG on downstream task - > Evaluating the KG on a downstream task Relation Swapping ( RS ) - > Relation Swapping ( RS ) but keep the relation distribution - > keeps We also leverage the validity scores given by human - > humans ranging from gradient based ( Chen - [ space ] AutoEncoder based ( Chen - [ space ] which can lead to corrupt explanations - > which can lead to incorrect explanations our RL-RR method always yield - > yields between entities instead of the semantic - > semantics that investigate into the problem - > that investigate the problem", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback ! Below are responses to each of your comments . Also , we \u2019 ve made major updates to the paper , incorporating your suggestions . -- * * 2.1 : A KG is denoted as G = ( E , R , T ) , where E and R are the entity set and relation set respectively - this is a little confusing since T is also the set of relations ( tuples over E ) . R is the set of relation labels ? * * Yes , R is the set of relation labels ( a.k.a.relation types ) , while T is the set of facts ( a.k.a.edges , triples ) , which are of the form ( entity1 , relation label , entity2 ) . In ConceptNet , R contains relation labels such as \u201c AtLocation \u201d and \u201c Desires \u201d , while T contains fact triples like \u201c ( Desk , AtLocation , Classroom ) \u201d and \u201c ( Child , Desires , Learn ) \u201d . We have clarified this in the updated paper . Please refer to * Section 2 : Problem Setting -- Paragraph `` Notation '' * . * * 2.2 : Finally , both c and k are concatenated for calculating the plausibility score - [ presumably the concatenation is then put through an MLP ? ] * * Yes , the concatenation is fed into a one-layer MLP , which outputs the plausibility score . We have updated the paper to state that the concatenation is fed into an MLP classifier F^ { cls } , as well as that the text embedding is computed using a Transformer text encoder F^ { text } . Please refer to * Section 2 : Problem Setting -- Paragraph `` Commonsense QA '' * . * * 2.3 : In both graph encoders , the subgraph G ( q , a ) together with the aggregation weights - [ neither of the descriptions mention aggregation weights ] * * To improve the presentation here , we have incorporated the following changes into the updated paper : - In the Commonsense QA part of Section 2 , we removed the description of GNNs and only consider path-based models ( which include RN and MHGRN ) , since path-based models are the ones designed for interpretability . - We compressed our description of path-based models to be self-contained and convey only the essential message : * \u201c ... path-based models compute the graph embedding by using attention to selectively aggregate paths in the subgraph . The attention scores can help explain which paths the model focused on most for a given prediction. \u201d * Given our paper \u2019 s focus , we felt that detailed explanation of path-based models would not be very beneficial . Also , in the original submission , by \u201c aggregation weights \u201d , we were referring to the attention scores used for path aggregation . Please refer to * Section 2 : Problem Setting -- Paragraph `` Commonsense QA '' * . * * 2.4 : while the task is to predict the unobserved interaction ( yuv = 0 ) . - [ does the zero here mean that the interaction is unobserved , or that the user has not engaged with the item ? ( or both ? ) ] * * y_ { uv } = 0 means that the interaction between user u and item v has not been observed . In this case , user u may or may not have engaged with item v in the past . For ( u , v ) pairs where y_ { uv } = 0 , our goal is to predict how likely user u is to want to engage with item v. We have clarified this in the updated paper . Please refer to * Section 2 : Problem Setting -- Paragraph `` Item Recommendation '' * ."}, {"review_id": "b7g3_ZMHnT0-2", "review_text": "This paper proposed to learn a RL model to modified the KG . They showed that their model can successfully deceive the KG-augmented models with most relations replaced , compared to heuristic-based strategies . I am not convinced by the motivation of this paper . The authors claimed that their model can learn to modified a KB so that a KG-augmented model can yield similar performance as before . This is under the assumption that similar performance in predicting the correct answer ( e.g.accuracy ) will lead to similar quality of explanation . This assumption does not always hold . If the authors really care about explanation , you should experiment with the explanation model that used KGs . There 's also a discrepancy between the authors ' motivation and their evaluation . If the authors assume KG is really important in generating explanations and can be easily fooled by their framework , they should also evaluated on KG only tasks , e.g.KBQA , KBC etc , besides these KG-augmented models . Some detailed questions : 1 . Many of the downstream models that the authors evaluated on are based on GCN approaches . GCN is good at filtering irrelevant information from their neighbors . How did you make sure that embeddings from the replaced relations are not simply filtered out at the aggregation steps ? 2.I would like to argue the LSTM is not appropriate here . LSTM is designed for `` ordered '' input . Making tricks to adapt it to un-ordered inputs is not a very good option . In each update step , the LSTM basically learns a gate variable \\sigma on the input . Can you do something like : G += \\sum \\alpha \\dot g , where g is the embedding of your single updates ? 3.In some cases , with replaced relations , the model can have better performance as the original model w/ KG ( e.g.OBQA RN Acc in Table 1 ) . Do you have an intuition why that could happen ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your feedback ! Below , we \u2019 ve clarified our motivation and why we believe this motivation is aligned with our experiments . Also , we \u2019 ve provided responses to the three questions you listed . -- * * 1.1 : Clarification about motivation * * We would like to clarify what we think may be a fundamental misunderstanding here . In this paper , we demonstrate that KG-augmented models can still maintain their downstream performance even when the KG has been perturbed significantly . Our motivation for doing so is to show that high model performance does NOT imply high explanation faithfulness . In other words , we do NOT assume that \u201c similar performance in predicting the correct answer ( e.g. , accuracy ) will lead to similar quality of explanation \u201d , but show the opposite instead . Note that all of the models we evaluated were claimed to provide KG-based explanations for their predictions . The surprising fact that KG-augmented models can still work on perturbed KGs raises doubts about how these models actually use KG info . As demonstrated in the human evaluation , such models apparently use KG info in a way that is not aligned with how humans interpret their explanations . We believe our findings are an important step in guiding future work on designing models that more effectively use KG info to both improve performance and provide better explanations . * * 1.2 : Clarification about experiments * * We believe that our experiments support the motivation described in 1.1 . As suggested , KG-only tasks could also be a reasonable setting . However , we specifically consider KG-augmented tasks because they naturally provide a non-KG baseline to compare against ( i.e. , the \u201c No KG \u201d results ) . Since our goal is to demonstrate that a perturbed KG can still be useful to neural-symbolic models , we need to be able to show that using a perturbed KG is better than not using a KG at all . * * 1.3 : Many of the downstream models that the authors evaluated on are based on GCN approaches . GCN is good at filtering irrelevant information from their neighbors . How did you make sure that embeddings from the replaced relations are not simply filtered out at the aggregation steps ? * * Given a fixed model , our goal is to measure how well the model performs on a perturbed KG , compared to on the original KG . Thus , if GCN is good at filtering irrelevant information from node neighbors , we would like to evaluate this ability on both original and perturbed KGs . To facilitate a fair comparison , we feel it would be best not to handicap the GCN to perform worse on the perturbed KG by \u201c mak [ ing ] sure that embeddings from the replaced relations are not simply filtered out at the aggregation steps \u201d . Nonetheless , we agree that the effects of perturbed KGs on GCN \u2019 s internal mechanisms should be further analyzed in future work ( e.g. , at each layer , which KG facts are being emphasized ? ) ."}, {"review_id": "b7g3_ZMHnT0-3", "review_text": "This paper shows that knowledge graph augmented question answering and recommendation system models are so graph structure/content change invariant that by using a simple heuristic or more sophisticated reinforcement learning-based approach , we can change the knowledge graphs without significant change in the model performance . This phenomenon leads to corrupt non-sense explanations for such models ' decisions/outputs . Strengths : - The paper is very well-written and crystal clear . - The idea is very interesting and novel . - The evaluations are relatively strong . Weak points : - I suggest the authors make their arguments about the invalidity of model explanations more clear by providing some explanation samples . - I am wondering what is the source of a more significant gap in the VALIDITY of explanations for original kg vs RL-PR in OBQA compared to CSQA . I am suggesting the authors provide an explanation for this difference . Overall , a study on the behavior of knowledge graph augmented models when they encounter the perturbed graphs is an interesting idea . The paper is also very well-written . I 'd like to see the revised manuscript being accepted by the conference .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your feedback ! Below are responses to the two concerns you expressed . - * * 4.1 : I suggest the authors make their arguments about the invalidity of model explanations more clear by providing some explanation samples . * * We agree that providing concrete examples of faithful and unfaithful explanations would improve our claims . Following your suggestion , In Figure X of the updated paper , we further illustrate our human evaluation by including real examples of fact path explanations produced by MHGRN using the original KG and RL-RR on CSQA , along with the consensus user ratings given to each explanation . Below is the example we \u2019 ve added to the paper : Example 1 : Original KG seems plausible . RL-RR does not seem plausible . - Question : \u201c James chose to not to print the cards , because he wanted to be more personal . What type of cards did he choose , instead ? \u201d ( Answer : \u201c handwritten \u201d ) - Original KG : \u201c print [ Antonym ] > handwritten \u201d ( Read : 1.0 ; Use : 2.0 ) - RL-RR : \u201c print [ NotDesires ] > handwritten \u201d ( Read : 0.0 ; Use : 0.0 ) Please refer to * Section 5.3 : Auxiliary Experiments and Analysis -- Paragraph \u201c Human Evaluation of KG Explanations \u201d * . For reference , we also provide the second example below ( not in the paper ) : Example 2 : Original KG and RL-RR both do not seem plausible . - Question : \u201c The middle of the day usually involves the bright star nearest to the earth to be straight overhead why ? \u201d ( Answer : \u201c human planet rotation \u201d ) - Original KG : \u201c wind [ IsA ] > rotation \u201d ( Read : 0.2 ; Use : 0.0 ) - RL-RR : \u201c straight < -- [ NotCapableOf ] turning [ NotDesires ] > rotation \u201d ( Read : 0.4 ; Use : 0.0 ) * * 4.2 : I am wondering what is the source of a more significant gap in the VALIDITY of explanations for original kg vs RL-RR in OBQA compared to CSQA . I am suggesting the authors provide an explanation for this difference . * * Good question ! First , note that we \u2019 ve renamed the user study dimensions as follows : [ Interpretability -- > Readability ] and [ Validity -- > Usability ] . OBQA 's much larger usability gap between Original KG and RL-RR can be explained by the fact that CSQA is constructed from ConceptNet . Every CSQA question-answer is based on ConceptNet entities/relations , so a random ConceptNet subgraph is more likely to have semantic overlap with a CSQA question-answer than with an OBQA question-answer . Hence , a perturbed ConceptNet subgraph may also be more likely to overlap with a CSQA question-answer , and so perturbing the KG might have a smaller impact on human judgments of CSQA path usability . This does not say anything about the model \u2019 s performance on CSQA and OBQA , as high model performance does not necessarily imply high explanation quality . In fact , our user study disproves this connection . We \u2019 ve updated our paper to include this analysis . Please refer to * Section 5.3 : Auxiliary Experiments and Analysis -- Paragraph \u201c Human Evaluation of KG Explanations \u201d * ."}], "0": {"review_id": "b7g3_ZMHnT0-0", "review_text": "The paper presents an interesting finding that some of the existing KG-augmented models , such as those for QA and item recommendation , may not actually capture or leverage the semantics in KGs , and their performance improvement can not be attributed to the usage of additional knowledge . I think this finding is of some significance . Pros : 1 ) The paper presents four heuristic methods and an RL-based method for KG perturbation . It obtains some interesting and noteworthy findings based on the experimental results in terms of QA and recommender systems . Cons : 1 ) In my opinion , perturbing KGs by randomly or heuristically changing existing edges is not well-motivated . Although it supports the experimental study in this paper and reveals several interesting findings , it has very little practical significance , because the perturbed KG would inevitably contain many invalid and incorrect facts without positive effects for downstream applications . Hence , the technical contribution of the proposed heuristic strategies and RL model for KG perturbation is not significant . 2 ) The analysis for experimental results is somewhat superficial . It fails to provide deep insights into why some models can still work with the perturbed KG . The authors should provide more analysis to explain the experimental results that are against common sense . For example , an experiment conclusion says that `` it is the ( false ) connection between entities instead of the semantic stored in the KG that leads to the improvement over non-KG baseline . '' Why ? I think this is an interesting and noteworthy finding , but no in-depth analysis is given . Besides , I also have a minor question , i.e. , how does the edge deletion method perturb all the triples ( 100 % ) in a KG ? How many triples are left after edge deletion ? Some typos : 1 ) these false connection - > these false connections 2 ) Relation Swapping ( RS ) - > Relation Swapping ( RS ) Anyway , the paper presents a noteworthy finding , which calls for further investigation into what information from the KGs is actually captured to improve the neural-symbolic models . So , I would like to recommend a weak acceptance of this paper . after rebuttal Thank the authors for their response which addressed my concerns . Based on the response , the revision , and other reviews , I would like to keep my score unchanged at 6 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback ! Below are responses to your comments/questions . -- * * 3.1 : In my opinion , perturbing KGs by randomly or heuristically changing existing edges is not well-motivated . Although it supports the experimental study in this paper and reveals several interesting findings , it has very little practical significance , because the perturbed KG would inevitably contain many invalid and incorrect facts without positive effects for downstream applications . Hence , the technical contribution of the proposed heuristic strategies and RL model for KG perturbation is not significant . * * By showing that KG-augmented neural-symbolic models perform well on perturbed KGs , we disprove existing assumptions about how models use KG info and about the plausibility of explanations provided by such models . Contrary to popular belief , KG-augmented models can still work even when the KG has been perturbed so much that humans can not understand the models \u2019 explanations . In light of this , the motivation of our paper is to guide future work on designing models that use KG info effectively and provide plausible explanations . Note that the proposed KG perturbation methods merely serve as analytical tools and are not intended to directly improve performance or explanation quality . Additionally , our results suggest that KG-augmented neural-symbolic models can be robust to noisy KG data . Even when the KG contains a fairly small amount of signal , the models are somehow able to leverage it . This could be a useful property in situations where it is not practical to obtain fully clean KG annotations . We have updated our paper to explicitly describe this motivation . Please refer to * Section 7 : Conclusion * . * * 3.2 : The analysis for experimental results is somewhat superficial . It fails to provide deep insights into why some models can still work with the perturbed KG . The authors should provide more analysis to explain the experimental results that are against common sense . For example , an experiment conclusion says that `` it is the ( false ) connection between entities instead of the semantic stored in the KG that leads to the improvement over non-KG baseline . '' Why ? I think this is an interesting and noteworthy finding , but no in-depth analysis is given . * * Good question ! First , subgraph extraction is heuristic-based , so some relevant info in the original KG may not be retrievable unless the KG itself is changed . Meanwhile , the RL-based KG perturbation methods are trained to maximize performance , which may enable the KG to be modified such that more relevant info is retrievable using the subgraph extraction heuristics . Second , in our experiments , relation-based perturbations ( RS , RR , RL-RR ) generally outperform edge-based perturbations ( ER , ED ) . Also , we have found that the original KG can contain noisy relation annotations which are sometimes \u201c corrected \u201d by relation-based perturbations . In certain cases , this may result in the perturbed KG achieving slightly higher performance than the original KG ( RR and RL-RR for RN-CSQA ; RL-RR for Last.FM ) . Similarly , in our user study , despite all questions being correctly answered by the model , there were some RL-RR explanations that received higher readability/usability ratings than their original KG counterparts . Although the original KG achieved higher human ratings than the RL-RR KG did overall , both KGs still achieved relatively low ratings with respect to our scales . While our main argument centers on NSKG models ' flaws , this counterintuitive finding suggests that KGs themselves are flawed too , but in a way that can be systematically corrected . We \u2019 ve updated the paper to include this analysis , which we agree would make our claims more convincing . Please refer to * Section 5.3 : Auxiliary Experiments and Analysis -- Paragraph \u201c Why do perturbed KGs sometimes perform better than the original KG ? \u201d * . * * 3.3 : Besides , I also have a minor question , i.e. , how does the edge deletion method perturb all the triples ( 100 % ) in a KG ? How many triples are left after edge deletion ? * * Good catch . For edge deletion , at 100 % perturbation , we delete all but 10 edges . We \u2019 ve updated the paper to include this info . Please refer to * Section 3.1 : Heuristic-Based KG Perturbation * ."}, "1": {"review_id": "b7g3_ZMHnT0-1", "review_text": "The paper provides a number of adversarial attacks on hybrid neural-symbolic systems . The systems are recommender and QA systems which use an underlying knowledge-graph ( KG ) such as ConceptNet . Previous work has suggested that the KGs are important for good performance , and moreover that the use of KGs lends the system a degree of interpretability . The attacks are successful - maintaining performance whilst seriously degrading the KG - throwing doubt on these claims . Two main approaches to attacking the systems are followed : a simple heuristic method in which labels in the KG are modified randomly , and a more sophisticated method in which deep reinforcement learning is used to learn an optimal policy to change the KG whilst maintaining good performance on the task . Overall I have a lot of sympathy for the motivation of this paper . There is increasing evidence that the attempted use of symbolic methods in hybrid systems , and in particular the use of symbolic methods to provide explanations , is just picking up on incidental properties of the data and exploiting the power of the deep network in ways unrelated to the symbolic representations . This paper provides further compelling evidence . However , the paper is currently an extremely frustrating read , and it took me a number of attempts to get through the paper to write this review . Part of the problem is that the authors have tried to cram in too much material . I would have preferred to have seen fewer experiments described , but in more detail , with the remainder briefly mentioned in a paragraph or two , or perhaps in an appendix ( if that 's allowed for ICLR ) . The other problem is that the presentation in the paper is poor . Part of that is down to the non-native English in parts ( which is not the fault of the authors ) , but part of it is also just down to sloppiness in the presentation . More detailed comments -- A KG is denoted as G = ( E , R , T ) , where E and R are the entity set and relation set respectively - this is a little confusing since T is also the set of relations ( tuples over E ) . R is the set of relation * labels * ? Finally , both c and k are concatenated for calculating the plausibility score - [ presuambly the concatenation is then put through an MLP ? ] In both graph encoders , the subgraph G ( q , a ) together with the aggregation weights - [ neither of the descriptions mention aggregation weights ] while the task is to predict the unobserved interaction ( yuv = 0 ) . - [ does the zero here mean that the interaction is unobserved , or that the user has not engaged with the item ? ( or both ? ) ] where the aggregation weights are personalized for u - [ you need to define how the aggregation works ] We randomly choose two triples ( edges ) in the KG and swap their relations . - [ it would be good to see some actual examples here from one of the applications/KGs . ] where sG ( \u00b7 ) is a KG scoring function trained on G. - [ is this defined anywhere ? ] The font in table 1 is really small . If you get more room in a final version please turn this into two tables . in-house test accuracy - what 's an `` in-house '' accuracy ? We then compute the relation specific clustering coefficient vector c^r - do you define this anywhere ? OpenBookQA ( OBQA ) - do you give a reference ? We randomly select 10 questions - this seems like a small sample on which to peform the human evaluation . This evaluation would be more persuasive with a larger sample ( even 20 or 30 ) . Typos etc . ( not exhaustive ) -- despite a deceptive symbolic structure - [ I 'm not sure that `` deceptive '' is the right word here ( and lots of places elsewhere ) , just something like `` incorrect '' may be better . ] that KG - > the KG The preliminary results indicate that KG can be easily manipulated and lost its benefit - > lose It brings more worrisome scenario - [ rephrase ] without noticeable performance drop - > without a noticeable performance drop In specific - > More specifically e.g. , commonsense question answering ( QA ) and recommender system , - > i.e.aggregating its neighbors \u2019 embedding - > aggregating its neighbors \u2019 embeddings This heuristic changes the semantic - > semantics This heuristic also does not perturb KG \u2019 s structure but its semantic - > This heuristic also does not perturb KG \u2019 s structure but its semantics on both of commonsense QA and recommendation system tasks - > on both commonsense QA and recommendation system tasks since one of our goal - > goals have captured the information of KG . - > the KG sequentially to LSTM - > sequentially to the LSTM Evaluating the KG on downstream task - > Evaluating the KG on a downstream task Relation Swapping ( RS ) - > Relation Swapping ( RS ) but keep the relation distribution - > keeps We also leverage the validity scores given by human - > humans ranging from gradient based ( Chen - [ space ] AutoEncoder based ( Chen - [ space ] which can lead to corrupt explanations - > which can lead to incorrect explanations our RL-RR method always yield - > yields between entities instead of the semantic - > semantics that investigate into the problem - > that investigate the problem", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your feedback ! Below are responses to each of your comments . Also , we \u2019 ve made major updates to the paper , incorporating your suggestions . -- * * 2.1 : A KG is denoted as G = ( E , R , T ) , where E and R are the entity set and relation set respectively - this is a little confusing since T is also the set of relations ( tuples over E ) . R is the set of relation labels ? * * Yes , R is the set of relation labels ( a.k.a.relation types ) , while T is the set of facts ( a.k.a.edges , triples ) , which are of the form ( entity1 , relation label , entity2 ) . In ConceptNet , R contains relation labels such as \u201c AtLocation \u201d and \u201c Desires \u201d , while T contains fact triples like \u201c ( Desk , AtLocation , Classroom ) \u201d and \u201c ( Child , Desires , Learn ) \u201d . We have clarified this in the updated paper . Please refer to * Section 2 : Problem Setting -- Paragraph `` Notation '' * . * * 2.2 : Finally , both c and k are concatenated for calculating the plausibility score - [ presumably the concatenation is then put through an MLP ? ] * * Yes , the concatenation is fed into a one-layer MLP , which outputs the plausibility score . We have updated the paper to state that the concatenation is fed into an MLP classifier F^ { cls } , as well as that the text embedding is computed using a Transformer text encoder F^ { text } . Please refer to * Section 2 : Problem Setting -- Paragraph `` Commonsense QA '' * . * * 2.3 : In both graph encoders , the subgraph G ( q , a ) together with the aggregation weights - [ neither of the descriptions mention aggregation weights ] * * To improve the presentation here , we have incorporated the following changes into the updated paper : - In the Commonsense QA part of Section 2 , we removed the description of GNNs and only consider path-based models ( which include RN and MHGRN ) , since path-based models are the ones designed for interpretability . - We compressed our description of path-based models to be self-contained and convey only the essential message : * \u201c ... path-based models compute the graph embedding by using attention to selectively aggregate paths in the subgraph . The attention scores can help explain which paths the model focused on most for a given prediction. \u201d * Given our paper \u2019 s focus , we felt that detailed explanation of path-based models would not be very beneficial . Also , in the original submission , by \u201c aggregation weights \u201d , we were referring to the attention scores used for path aggregation . Please refer to * Section 2 : Problem Setting -- Paragraph `` Commonsense QA '' * . * * 2.4 : while the task is to predict the unobserved interaction ( yuv = 0 ) . - [ does the zero here mean that the interaction is unobserved , or that the user has not engaged with the item ? ( or both ? ) ] * * y_ { uv } = 0 means that the interaction between user u and item v has not been observed . In this case , user u may or may not have engaged with item v in the past . For ( u , v ) pairs where y_ { uv } = 0 , our goal is to predict how likely user u is to want to engage with item v. We have clarified this in the updated paper . Please refer to * Section 2 : Problem Setting -- Paragraph `` Item Recommendation '' * ."}, "2": {"review_id": "b7g3_ZMHnT0-2", "review_text": "This paper proposed to learn a RL model to modified the KG . They showed that their model can successfully deceive the KG-augmented models with most relations replaced , compared to heuristic-based strategies . I am not convinced by the motivation of this paper . The authors claimed that their model can learn to modified a KB so that a KG-augmented model can yield similar performance as before . This is under the assumption that similar performance in predicting the correct answer ( e.g.accuracy ) will lead to similar quality of explanation . This assumption does not always hold . If the authors really care about explanation , you should experiment with the explanation model that used KGs . There 's also a discrepancy between the authors ' motivation and their evaluation . If the authors assume KG is really important in generating explanations and can be easily fooled by their framework , they should also evaluated on KG only tasks , e.g.KBQA , KBC etc , besides these KG-augmented models . Some detailed questions : 1 . Many of the downstream models that the authors evaluated on are based on GCN approaches . GCN is good at filtering irrelevant information from their neighbors . How did you make sure that embeddings from the replaced relations are not simply filtered out at the aggregation steps ? 2.I would like to argue the LSTM is not appropriate here . LSTM is designed for `` ordered '' input . Making tricks to adapt it to un-ordered inputs is not a very good option . In each update step , the LSTM basically learns a gate variable \\sigma on the input . Can you do something like : G += \\sum \\alpha \\dot g , where g is the embedding of your single updates ? 3.In some cases , with replaced relations , the model can have better performance as the original model w/ KG ( e.g.OBQA RN Acc in Table 1 ) . Do you have an intuition why that could happen ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your feedback ! Below , we \u2019 ve clarified our motivation and why we believe this motivation is aligned with our experiments . Also , we \u2019 ve provided responses to the three questions you listed . -- * * 1.1 : Clarification about motivation * * We would like to clarify what we think may be a fundamental misunderstanding here . In this paper , we demonstrate that KG-augmented models can still maintain their downstream performance even when the KG has been perturbed significantly . Our motivation for doing so is to show that high model performance does NOT imply high explanation faithfulness . In other words , we do NOT assume that \u201c similar performance in predicting the correct answer ( e.g. , accuracy ) will lead to similar quality of explanation \u201d , but show the opposite instead . Note that all of the models we evaluated were claimed to provide KG-based explanations for their predictions . The surprising fact that KG-augmented models can still work on perturbed KGs raises doubts about how these models actually use KG info . As demonstrated in the human evaluation , such models apparently use KG info in a way that is not aligned with how humans interpret their explanations . We believe our findings are an important step in guiding future work on designing models that more effectively use KG info to both improve performance and provide better explanations . * * 1.2 : Clarification about experiments * * We believe that our experiments support the motivation described in 1.1 . As suggested , KG-only tasks could also be a reasonable setting . However , we specifically consider KG-augmented tasks because they naturally provide a non-KG baseline to compare against ( i.e. , the \u201c No KG \u201d results ) . Since our goal is to demonstrate that a perturbed KG can still be useful to neural-symbolic models , we need to be able to show that using a perturbed KG is better than not using a KG at all . * * 1.3 : Many of the downstream models that the authors evaluated on are based on GCN approaches . GCN is good at filtering irrelevant information from their neighbors . How did you make sure that embeddings from the replaced relations are not simply filtered out at the aggregation steps ? * * Given a fixed model , our goal is to measure how well the model performs on a perturbed KG , compared to on the original KG . Thus , if GCN is good at filtering irrelevant information from node neighbors , we would like to evaluate this ability on both original and perturbed KGs . To facilitate a fair comparison , we feel it would be best not to handicap the GCN to perform worse on the perturbed KG by \u201c mak [ ing ] sure that embeddings from the replaced relations are not simply filtered out at the aggregation steps \u201d . Nonetheless , we agree that the effects of perturbed KGs on GCN \u2019 s internal mechanisms should be further analyzed in future work ( e.g. , at each layer , which KG facts are being emphasized ? ) ."}, "3": {"review_id": "b7g3_ZMHnT0-3", "review_text": "This paper shows that knowledge graph augmented question answering and recommendation system models are so graph structure/content change invariant that by using a simple heuristic or more sophisticated reinforcement learning-based approach , we can change the knowledge graphs without significant change in the model performance . This phenomenon leads to corrupt non-sense explanations for such models ' decisions/outputs . Strengths : - The paper is very well-written and crystal clear . - The idea is very interesting and novel . - The evaluations are relatively strong . Weak points : - I suggest the authors make their arguments about the invalidity of model explanations more clear by providing some explanation samples . - I am wondering what is the source of a more significant gap in the VALIDITY of explanations for original kg vs RL-PR in OBQA compared to CSQA . I am suggesting the authors provide an explanation for this difference . Overall , a study on the behavior of knowledge graph augmented models when they encounter the perturbed graphs is an interesting idea . The paper is also very well-written . I 'd like to see the revised manuscript being accepted by the conference .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your feedback ! Below are responses to the two concerns you expressed . - * * 4.1 : I suggest the authors make their arguments about the invalidity of model explanations more clear by providing some explanation samples . * * We agree that providing concrete examples of faithful and unfaithful explanations would improve our claims . Following your suggestion , In Figure X of the updated paper , we further illustrate our human evaluation by including real examples of fact path explanations produced by MHGRN using the original KG and RL-RR on CSQA , along with the consensus user ratings given to each explanation . Below is the example we \u2019 ve added to the paper : Example 1 : Original KG seems plausible . RL-RR does not seem plausible . - Question : \u201c James chose to not to print the cards , because he wanted to be more personal . What type of cards did he choose , instead ? \u201d ( Answer : \u201c handwritten \u201d ) - Original KG : \u201c print [ Antonym ] > handwritten \u201d ( Read : 1.0 ; Use : 2.0 ) - RL-RR : \u201c print [ NotDesires ] > handwritten \u201d ( Read : 0.0 ; Use : 0.0 ) Please refer to * Section 5.3 : Auxiliary Experiments and Analysis -- Paragraph \u201c Human Evaluation of KG Explanations \u201d * . For reference , we also provide the second example below ( not in the paper ) : Example 2 : Original KG and RL-RR both do not seem plausible . - Question : \u201c The middle of the day usually involves the bright star nearest to the earth to be straight overhead why ? \u201d ( Answer : \u201c human planet rotation \u201d ) - Original KG : \u201c wind [ IsA ] > rotation \u201d ( Read : 0.2 ; Use : 0.0 ) - RL-RR : \u201c straight < -- [ NotCapableOf ] turning [ NotDesires ] > rotation \u201d ( Read : 0.4 ; Use : 0.0 ) * * 4.2 : I am wondering what is the source of a more significant gap in the VALIDITY of explanations for original kg vs RL-RR in OBQA compared to CSQA . I am suggesting the authors provide an explanation for this difference . * * Good question ! First , note that we \u2019 ve renamed the user study dimensions as follows : [ Interpretability -- > Readability ] and [ Validity -- > Usability ] . OBQA 's much larger usability gap between Original KG and RL-RR can be explained by the fact that CSQA is constructed from ConceptNet . Every CSQA question-answer is based on ConceptNet entities/relations , so a random ConceptNet subgraph is more likely to have semantic overlap with a CSQA question-answer than with an OBQA question-answer . Hence , a perturbed ConceptNet subgraph may also be more likely to overlap with a CSQA question-answer , and so perturbing the KG might have a smaller impact on human judgments of CSQA path usability . This does not say anything about the model \u2019 s performance on CSQA and OBQA , as high model performance does not necessarily imply high explanation quality . In fact , our user study disproves this connection . We \u2019 ve updated our paper to include this analysis . Please refer to * Section 5.3 : Auxiliary Experiments and Analysis -- Paragraph \u201c Human Evaluation of KG Explanations \u201d * ."}}