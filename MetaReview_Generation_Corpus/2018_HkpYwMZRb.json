{"year": "2018", "forum": "HkpYwMZRb", "title": "Gradients explode - Deep Networks are shallow - ResNet explained", "decision": "Invite to Workshop Track", "meta_review": "The paper sets out to analyze the problem of exploding gradients in deep nets which is of fundamental importance. Reviewers largely acknowledge the novelty of the main ideas in the paper towards this goal, however it is also strongly felt that the writing/presentation of the paper needs significant improvement to make it into a coherent and clean story before it can be published. There are also some concerns on networks used in the experiments not being close to practice.\n\nI recommend invitation to the workshop track as it has novel ideas and will likely generate interesting discussion. ", "reviews": [{"review_id": "HkpYwMZRb-0", "review_text": "Summary of paper - The paper introduces the Gradient Scale Coefficient and uses it to demonstrate issues with the current understanding of where and why exploding gradients occur. Review - The paper attempts to contribute to the discussion about the exploding gradient problem by both introducing a metric for discussing this issue and by showing that current understanding of the exploding gradient problem may be incorrect. It is admirable that the authors are seeking to add to the understanding about theory of neural nets instead of contributing a new architecture with better error rates but without understanding why said error rates are lower. While the authors list 7 contributions, the current version of the text is a challenge to read and makes it challenging to distill an overarching theme or narrative to these contributions. The authors do mention experiments on page 8, but confess that some of the results are somewhat underwhelming. Unfortunately, all tables with the experimental results are left to the appendix. As this is a mostly theoretical paper, pushing experimental results to the appendix does make sense, but the repeated references to these tables suggest that these experimental results are crucial for the authors\u2019 overall points. While the authors do attempt to accomplish a lot in these nearly 16 pages of text, the authors' main points and overall narrative gets lost due to the writing that is a bit jumbled at times and that relies heavily on the supplement. There are several places where it is not immediately clear why a certain block of text is included (i.e. the proof outlines on pages 8 and 10). At other points the authors default to an chronological narrative that can be useful at times (i.e. page 9), but here seems to distract from their overall narrative. This paper has a lot of content, but not all of it appears to be relevant to the authors\u2019 central points. Furthermore, the paper is nearly double the recommended page length and has a nearly 30 page supplement. My biggest recommendations for this paper are for the authors to 1) articulate one theme and then 2) look at each part (whether that be section, paragraph, or sentence) and ask what does that part contribute to that theme. Pros - * This paper attempts to add the understanding of neural nets instead of only contributing better error rates on benchmark datasets. * At several points, the authors seek to make the work accessible by offering lay explanations for their more technical points. * The practical suggestions on page 16 are a true highlight and could provide an outline for possible revisions. Cons - * The main narrative is lost in the text, leaving a reader unsure of the authors main points and contributions as they read. For example, the authors\u2019 first contribution is hidden among the text presentation of section 2. * The paper relies heavily on the supplement to make their central points. * It is nearly double the recommended page length with a nearly 30 page supplement Minor issues - * Use one style for introducing and defining terms either use italics or single quotes. The latter is not recommended since the authors use double quotes in the abstract to express that the exploding gradient problem is not solved. * The citation style of Authors (YEAR) at times leads to awkward sentence parsing. * Given that many figures have several subfigures, the authors should consider using a package that will denote subfigures with letters. * The block quotes in the introduction may be quite important for points later in the paper, but summarizing the points of these quotes may be a better use of space. The authors more successfully did this in paragraph 2 of the introduction. * All long descriptions of the appendix should be carefully revisited and possibly removed due to page length considerations. * In the text, figure 4 (which is in the supplement) is referenced before figure 3 (which is in the text). =-=-=-= Response to the authors During the initial reviewing period, I was unable to distill the significance of the authors\u2019 contributions from the current literature in large part due to the nature of the writing style. After reading the authors responses and consulting the differences between the versions of the paper, my review remains the same. It should be noted that all three reviewers pointed out the length of the paper as a weakness of the paper, and that in the most recent draft, the authors made the main text of the paper longer. Consulting the differences between the paper revisions, I was initially intrigued with the volume of differences that shown in the summary bar. Upon closer inspection, I read a much stronger introduction and appreciated the summaries at the ends of sections 4.4 and 6. However, I did notice that the majority of these changes were superficial re-orderings of the original text. Given the limited substantive changes to the main text, I did not deeply re-read the text of the paper beyond the introduction.", "rating": "3: Clear rejection", "reply_text": "# # # `` The paper relies heavily on the supplement to make their central points . '' We moved both table 1 and table 2 to the main body of the paper in the revision . Because this paper is detail-oriented and each reader cares about a different set of details , we chose ( a ) to provide as much detail as possible and ( b ) move those details to the appendix into dedicated sections so that they would be easy to find by specific interested parties . Do you think there is any particular section , paragraph or detail from the appendix that should still be moved to the main body ? If so , we would be glad to know and to fulfil such a request if it could be aligned with the preferences of the other reviewers . # # # `` ... confess that some of the results are somewhat underwhelming . '' The goal of sections 3 through 6 is to demonstrate the pathologies of exploding gradients and collapsing domains . We made our neural networks very deep precisely so that these pathologies would be very clear and measurable . Pathological architectures , by definition , suffer from high errors . We include this information explicitly in the revision . Using very deep MLPs to study gradient pathologies is a well-established practice from previous works closely related to this paper ( e.g . [ 1,2,4 ] ) .Note that we contrast these high error values with those achieved by ResNet and looks-linear initialized ReLU networks , which acheive much lower error , in section 7 / table 2 . # # # `` Unfortunately , all tables with the experimental results are left to the appendix . '' Tables 1 and 2 have been moved to the main body in the revision as per this request . # # # `` There are several places where it is not immediately clear why a certain block of text is included ( i.e.the proof outlines on pages 8 and 10 ) . '' In the revision , the proof outline of theorem 1 was removed and replaced by an informal explanation of the underlying mechanisms preceding the theorem . The proof outline of theorem 2 exists to highlight the important intermediate result that surjective endomorphisms exhibit an expected absolute determinant of 1 , which leads to an expected qm norm greater than 1 , which causes exploding gradients . We 've added more references to this important relationship throughout the revision . # # # Minor issues : - We used single quotes to define terms and italic to highlight important concepts . In the revision , we use single quotes to define terms AND important concepts for increased consistency . We still use italic to highlight important concepts . - We use the citation style provided by the ICLR latex template . I would prefer not to alter this setting . Also , the vast majority of ICLR 2018 submission use ( YEAR ) in their citations . However , I did miss some brackets around citations in the original version of the paper . Those brackets have been added in the revision . - Letters have been added to the subfigures . Thank you for this advice . - We removed 2 of the 4 block quotes in the introduction . We would like to keep the remaining ones to underscore the difference between our results and popular wisdom . - Appendix length : see above - Figures are numbered according to the order in which they appear in the paper , not the order in which they are referenced . Again , this is the default of the ICLR latex template / latex itself . Let me know if you would like me to alter this . # # # We hope that we have addressed your concerns in this comment and the revised version of the paper . If you agree that the contributions of our paper are significant and have been sufficiently demonstrated , we hope that you agree that our paper is well-placed at ICLR . We look forward to hearing your thoughts and comments . [ 1 ] Schoenholz et al.Deep Information Propagation . ICLR 2107. https : //arxiv.org/abs/1611.01232 [ 2 ] Balduzzi et al.The Shattered Gradients Problem : If resnets are the answer , then what is the question ? . ICML , 2017. https : //arxiv.org/abs/1702.08591 [ 3 ] Saxe et al.Exact solutions to the nonlinear dynamics of learning in deep linear neural networks . 2014. https : //arxiv.org/abs/1312.6120 [ 4 ] Yang & Schoenholz . Mean field residual networks : on the edge of chaos . NIPS , 2017. https : //papers.nips.cc/paper/6879-mean-field-residual-networks-on-the-edge-of-chaos"}, {"review_id": "HkpYwMZRb-1", "review_text": "Paper Summary: This is a very long paper (55 pages), and I did not read it in its entirety. The first part (up to page 11), focuses on better understanding the exploding gradients problem, and challenges the fact that current techniques to address gradient explosion work as claimed. To do so, they first motivate a new measure of gradient size, the Gradient Scale Coefficient which averages the singular values of the Jacobian and takes a ratio of different layers. The motivation for this measure is that it is invariant to simple rescaling of layers that preserves the function. (I would have liked to have seen what was meant by preserved the function here -- did you mean preserve the same class outputs e.g.?) They focus on linear MLPs in the paper for computational simplicity. With this setup, and assuming the Jacobian decomposes, they prove that the GSC increases exponentially (Proposition 5). They empirically test this out for networks 50 layers deep and 100 layers wide, where they find that some architectures have exploding gradients after random initialization, and others do not, but those that do not have other drawbacks. They then overview the notion of effective depth for a residual network: a linear residual network can be written as a product of terms of the form (I + r_i). Expanding out, each term is a product of some of the r_i and some of the identities I. If all r_i have a norm < 1, then the terms that dominate will be those that consist of fewer r_i, resulting in a lower effective depth. This is described in Veit et al, 2016. While this analysis was originally used for residual networks, they relate this to any network by letting I turn into an arbitrary initial function. Their main theoretical result from this is that deeper networks take exponentially longer to train (under certain conditions), which they test out with (linear?) networks of depth 50 and width 100. They also propose that the reason gradients explode is because networks try to preserve their domain going forward, which requires Jacobians to have determinant 1 and leads to a higher Q-norm. Main Comments: This could potentially be a very nice paper, but I feel the current presentation is not ready for acceptance. In particular, the paper would benefit greatly from being made much shorter, and having more of the important details or proof outlines for the various propositions in the main text. Right now, it is quite confusing to follow, and I fail to see the motivation for some of the analysis. For example, the Gradient Scale Coefficient appears to be motivated because (bottom page 3), with other norm measurements, we could take any architecture and rescale the parameters, and inversely scale the gradients to make it \"easy to train\". But typically easy to train does not involve a specific preprocessing of gradients. Other propositions e.g. Theorem 1, proposition 6, could do with clearer intuition leading to them. I think the assumptions made in the results should also be clearer. (It's fine to have results, but currently I can't tell under what conditions the results apply and under what conditions they don't. E.g. are there any extensions of this that apply to non-linear networks?) I also have issues with their experimental setup: why choose to experiment on networks of depth 50 and width 100? This doesn't really look anything like networks that are trained in practice. Calling these \"popular architectures\" is misleading. In summary, I think this paper needs more work on the presentation to make clear what they are proving and under what conditions, and with experiments that are closer to those used in practice to support their claims. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "# # # `` I would have liked to have seen what was meant by preserved the function here -- did you mean preserve the same class outputs e.g '' Yes , we mean the value of the prediction and error layers is invariant . In the revision , we have amended the text to reflect this . # # # `` They focus on linear MLPs in the paper for computational simplicity . '' When you say `` linear MLPs '' , do you mean MLPs containing only linear layers ? Note that all of the MLPs we study in this paper contain nonlinear layers ( ReLU , tanh , SeLU , batch normalization and layer normalization ) and many also contain skip connections . We do not study linear MLPs in this paper . # # # `` But typically easy to train does not involve a specific preprocessing of gradients . '' In the revision , we replace `` easy to train '' with `` can be successfully trained '' and make clear that this includes gradient rescaling . In the paper , we aim to contrast training difficulty that can be overcome by rescaling the gradient versus training difficulty that can not be overcome in this way , as encapsulated by theorem 1 . We agree that it is not always obvious how to scale the gradient in practice , but point out that techniques such as Adam , vSGD [ 4 ] or heuristics such as `` scale the gradient to be proportial to the size of the weight matrix '' are often quite successful . # # # `` Other propositions e.g.Theorem 1 , proposition 6 , could do with clearer intuition leading to them . I think the assumptions made in the results should also be clearer . '' We added additional explanations to the leadup of both theorem 1 and proposition 6 in the revision . Unfortunately , we were unable to include the assumptions made in theoretical results in the main body of the paper due to space reason , and because we think it would significantly detract from the readability of the paper . For example , consider the full statement of theorem 1 . While some readers will be interested in this full statement , other readers may find it distracting . However , the assumptions are given and discussed in detail in sections E and F. Is there a specific section , paragraph or detail from the appendix you believe we should include in the main body ? # # # We hope that we have addressed your concerns in this comment and the revised version of the paper . We also refer you to our new introduction and conclusion section that make the contributions and implications of our paper even more clear . If you agree that our paper makes important contributions that are also well-supported ( taking into account that we do not just use linear MLPs ) we hope that you agree our paper is well-placed at ICLR . We look forward to hearing your thoughts and comments . [ 1 ] Schoenholz et al.Deep Information Propagation . ICLR 2107. https : //arxiv.org/abs/1611.01232 [ 2 ] Balduzzi et al.The Shattered Gradients Problem : If resnets are the answer , then what is the question ? . ICML , 2017. https : //arxiv.org/abs/1702.08591 [ 3 ] Yang & Schoenholz . Mean field residual networks : on the edge of chaos . NIPS , 2017. https : //papers.nips.cc/paper/6879-mean-field-residual-networks-on-the-edge-of-chaos [ 4 ] Schaul et al.No More Pesky Learning Rates . ICML , 2013. https : //arxiv.org/abs/1206.1106 [ 5 ] Saxe et al.Exact solutions to the nonlinear dynamics of learning in deep linear neural networks . 2014. https : //arxiv.org/abs/1312.6120"}, {"review_id": "HkpYwMZRb-2", "review_text": "The paper makes some bold claims. In particular about commonly accepted intuition for avoiding exploding/vanishing gradients and why all the recent bag of tricks (BN, Adam) do not actually address the problems they set out to alleviate. This is either a very important paper or the analysis is incorrect but it's not my area of expertise. Actually understanding it at depth and validating the proofs and validity of the experiments will require some digestion. It's possible some of the issues arise from the particular architectures they choose to investigate and demonstrate on (eg I have mostly seen ResNets in the context of CNNs but they analyze on FC topologies, the form of the loss, etc) but that's a guess and there are some further analysis in the supp material for these networks which I haven't looked at in detail. Regardless - an important note to the authors is that it's a particularly long and verbose paper, coming in at 16 pages of the main paper(!) with nearly 50 (!) pages of supplementary material where the heart and meat of the proofs and experiments reside. As such it's not even clear if this is proper for a conference. The authors have already provided several pages worth of additional comments on the website on further related work. I view this as an issue in and of itself. Being succinct and applying rigour in editing is part of doing science and reporting findings, and a wise guideline to follow. While the authors may claim it's necessary to use that much space to make their point I will argue that this length is uncalibrated to standards. I've seen many papers that need to go through much more complicated derivations and theory and remain within a 8-10 page limit by being precise and strictly to the point. Perhaps Godel could be a good inspiration here, with a 21 page PhD thesis that fundamentally changed mathematics. In addition to being quite bold in claims, it is also somewhat confrontational in style. I understand the authors are trying to make a very serious claim about much of the common wisdom, but again, having reviewed papers for many years, this is highly unusual and it is questionable whether it is necessary. So, while I cannot vouch for the correctness, I think it can and should go through a serious revision to make it succinct and that will likely considerably help in making it accessible to a wider readership and aligned to the expectations from a conference paper in the field. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Dear Reviewer , Thank you for your review and for your honesty in stating that this paper does not fall within your area of expertise . I just uploaded a revised version of the paper . We address the points raised in your review in this revision as well as in the comments below . # # # `` an important note to the authors is that it 's a particularly long and verbose paper '' In the revision , we removed some of the less central results ( propositions 7 through 9 ) as well as high-level commentary to make the paper more focused . This paper addresses many issues in detail that other papers often gloss over , such as the rigorous definition of exploding gradients or effective depth and the careful setting of layerwise step sizes . This rigor is what enables us to obtain important results . Also note that an important predecessor work [ 3 ] from NIPS 2017 is also 55 pages long . Much of our appendix is strictly optional for readers interested in certain specifics , such as implementation details for those interested in replicating our results or the extended related work section for those interested in pursuing research in deep learning theory . # # # `` It 's possible some of the issues arise from the particular architectures they choose to investigate and demonstrate on '' While we believe that all results discussed in the paper apply to convolutional and other networks in a similar fashion , we do not discuss or test the applicability to these networks specifically , for space reasons . However , using very deep MLPs as a testbed to advance the study of exploding gradients and related problems is a well-established practice ( e.g . [ 1,2,3,4 ] ) . # # # `` it is also somewhat confrontational in style '' I apologize if my writing style appeared confrontational . Do you mean the paragraph that starts with `` These claims are mistaken . ... '' ? I reformulated that paragraph in the revision . It now starts with `` We argue that these claims are overly optimistic ... '' # # # `` making it accessible to a wider readership and aligned to the expectations from a conference paper in the field '' We do not necessarily agree that ICLR papers should appeal to a wide readership . Many program synthesis papers are targeted at those interested in program synthesis . Many machine translation papers are targeted at those interested in NLP etc . Our paper is targeted at those interested in the theory of neural networks and foundational principles of neural network architecture design . We accept that this is a subset of the entire ICLR audience and do not see anything wrong with that . [ 1 ] Schoenholz et al.Deep Information Propagation . ICLR 2107. https : //arxiv.org/abs/1611.01232 [ 2 ] Balduzzi et al.The Shattered Gradients Problem : If resnets are the answer , then what is the question ? . ICML , 2017. https : //arxiv.org/abs/1702.08591 [ 3 ] Yang & Schoenholz . Mean field residual networks : on the edge of chaos . NIPS , 2017. https : //papers.nips.cc/paper/6879-mean-field-residual-networks-on-the-edge-of-chaos [ 4 ] Saxe et al.Exact solutions to the nonlinear dynamics of learning in deep linear neural networks . 2014. https : //arxiv.org/abs/1312.6120"}], "0": {"review_id": "HkpYwMZRb-0", "review_text": "Summary of paper - The paper introduces the Gradient Scale Coefficient and uses it to demonstrate issues with the current understanding of where and why exploding gradients occur. Review - The paper attempts to contribute to the discussion about the exploding gradient problem by both introducing a metric for discussing this issue and by showing that current understanding of the exploding gradient problem may be incorrect. It is admirable that the authors are seeking to add to the understanding about theory of neural nets instead of contributing a new architecture with better error rates but without understanding why said error rates are lower. While the authors list 7 contributions, the current version of the text is a challenge to read and makes it challenging to distill an overarching theme or narrative to these contributions. The authors do mention experiments on page 8, but confess that some of the results are somewhat underwhelming. Unfortunately, all tables with the experimental results are left to the appendix. As this is a mostly theoretical paper, pushing experimental results to the appendix does make sense, but the repeated references to these tables suggest that these experimental results are crucial for the authors\u2019 overall points. While the authors do attempt to accomplish a lot in these nearly 16 pages of text, the authors' main points and overall narrative gets lost due to the writing that is a bit jumbled at times and that relies heavily on the supplement. There are several places where it is not immediately clear why a certain block of text is included (i.e. the proof outlines on pages 8 and 10). At other points the authors default to an chronological narrative that can be useful at times (i.e. page 9), but here seems to distract from their overall narrative. This paper has a lot of content, but not all of it appears to be relevant to the authors\u2019 central points. Furthermore, the paper is nearly double the recommended page length and has a nearly 30 page supplement. My biggest recommendations for this paper are for the authors to 1) articulate one theme and then 2) look at each part (whether that be section, paragraph, or sentence) and ask what does that part contribute to that theme. Pros - * This paper attempts to add the understanding of neural nets instead of only contributing better error rates on benchmark datasets. * At several points, the authors seek to make the work accessible by offering lay explanations for their more technical points. * The practical suggestions on page 16 are a true highlight and could provide an outline for possible revisions. Cons - * The main narrative is lost in the text, leaving a reader unsure of the authors main points and contributions as they read. For example, the authors\u2019 first contribution is hidden among the text presentation of section 2. * The paper relies heavily on the supplement to make their central points. * It is nearly double the recommended page length with a nearly 30 page supplement Minor issues - * Use one style for introducing and defining terms either use italics or single quotes. The latter is not recommended since the authors use double quotes in the abstract to express that the exploding gradient problem is not solved. * The citation style of Authors (YEAR) at times leads to awkward sentence parsing. * Given that many figures have several subfigures, the authors should consider using a package that will denote subfigures with letters. * The block quotes in the introduction may be quite important for points later in the paper, but summarizing the points of these quotes may be a better use of space. The authors more successfully did this in paragraph 2 of the introduction. * All long descriptions of the appendix should be carefully revisited and possibly removed due to page length considerations. * In the text, figure 4 (which is in the supplement) is referenced before figure 3 (which is in the text). =-=-=-= Response to the authors During the initial reviewing period, I was unable to distill the significance of the authors\u2019 contributions from the current literature in large part due to the nature of the writing style. After reading the authors responses and consulting the differences between the versions of the paper, my review remains the same. It should be noted that all three reviewers pointed out the length of the paper as a weakness of the paper, and that in the most recent draft, the authors made the main text of the paper longer. Consulting the differences between the paper revisions, I was initially intrigued with the volume of differences that shown in the summary bar. Upon closer inspection, I read a much stronger introduction and appreciated the summaries at the ends of sections 4.4 and 6. However, I did notice that the majority of these changes were superficial re-orderings of the original text. Given the limited substantive changes to the main text, I did not deeply re-read the text of the paper beyond the introduction.", "rating": "3: Clear rejection", "reply_text": "# # # `` The paper relies heavily on the supplement to make their central points . '' We moved both table 1 and table 2 to the main body of the paper in the revision . Because this paper is detail-oriented and each reader cares about a different set of details , we chose ( a ) to provide as much detail as possible and ( b ) move those details to the appendix into dedicated sections so that they would be easy to find by specific interested parties . Do you think there is any particular section , paragraph or detail from the appendix that should still be moved to the main body ? If so , we would be glad to know and to fulfil such a request if it could be aligned with the preferences of the other reviewers . # # # `` ... confess that some of the results are somewhat underwhelming . '' The goal of sections 3 through 6 is to demonstrate the pathologies of exploding gradients and collapsing domains . We made our neural networks very deep precisely so that these pathologies would be very clear and measurable . Pathological architectures , by definition , suffer from high errors . We include this information explicitly in the revision . Using very deep MLPs to study gradient pathologies is a well-established practice from previous works closely related to this paper ( e.g . [ 1,2,4 ] ) .Note that we contrast these high error values with those achieved by ResNet and looks-linear initialized ReLU networks , which acheive much lower error , in section 7 / table 2 . # # # `` Unfortunately , all tables with the experimental results are left to the appendix . '' Tables 1 and 2 have been moved to the main body in the revision as per this request . # # # `` There are several places where it is not immediately clear why a certain block of text is included ( i.e.the proof outlines on pages 8 and 10 ) . '' In the revision , the proof outline of theorem 1 was removed and replaced by an informal explanation of the underlying mechanisms preceding the theorem . The proof outline of theorem 2 exists to highlight the important intermediate result that surjective endomorphisms exhibit an expected absolute determinant of 1 , which leads to an expected qm norm greater than 1 , which causes exploding gradients . We 've added more references to this important relationship throughout the revision . # # # Minor issues : - We used single quotes to define terms and italic to highlight important concepts . In the revision , we use single quotes to define terms AND important concepts for increased consistency . We still use italic to highlight important concepts . - We use the citation style provided by the ICLR latex template . I would prefer not to alter this setting . Also , the vast majority of ICLR 2018 submission use ( YEAR ) in their citations . However , I did miss some brackets around citations in the original version of the paper . Those brackets have been added in the revision . - Letters have been added to the subfigures . Thank you for this advice . - We removed 2 of the 4 block quotes in the introduction . We would like to keep the remaining ones to underscore the difference between our results and popular wisdom . - Appendix length : see above - Figures are numbered according to the order in which they appear in the paper , not the order in which they are referenced . Again , this is the default of the ICLR latex template / latex itself . Let me know if you would like me to alter this . # # # We hope that we have addressed your concerns in this comment and the revised version of the paper . If you agree that the contributions of our paper are significant and have been sufficiently demonstrated , we hope that you agree that our paper is well-placed at ICLR . We look forward to hearing your thoughts and comments . [ 1 ] Schoenholz et al.Deep Information Propagation . ICLR 2107. https : //arxiv.org/abs/1611.01232 [ 2 ] Balduzzi et al.The Shattered Gradients Problem : If resnets are the answer , then what is the question ? . ICML , 2017. https : //arxiv.org/abs/1702.08591 [ 3 ] Saxe et al.Exact solutions to the nonlinear dynamics of learning in deep linear neural networks . 2014. https : //arxiv.org/abs/1312.6120 [ 4 ] Yang & Schoenholz . Mean field residual networks : on the edge of chaos . NIPS , 2017. https : //papers.nips.cc/paper/6879-mean-field-residual-networks-on-the-edge-of-chaos"}, "1": {"review_id": "HkpYwMZRb-1", "review_text": "Paper Summary: This is a very long paper (55 pages), and I did not read it in its entirety. The first part (up to page 11), focuses on better understanding the exploding gradients problem, and challenges the fact that current techniques to address gradient explosion work as claimed. To do so, they first motivate a new measure of gradient size, the Gradient Scale Coefficient which averages the singular values of the Jacobian and takes a ratio of different layers. The motivation for this measure is that it is invariant to simple rescaling of layers that preserves the function. (I would have liked to have seen what was meant by preserved the function here -- did you mean preserve the same class outputs e.g.?) They focus on linear MLPs in the paper for computational simplicity. With this setup, and assuming the Jacobian decomposes, they prove that the GSC increases exponentially (Proposition 5). They empirically test this out for networks 50 layers deep and 100 layers wide, where they find that some architectures have exploding gradients after random initialization, and others do not, but those that do not have other drawbacks. They then overview the notion of effective depth for a residual network: a linear residual network can be written as a product of terms of the form (I + r_i). Expanding out, each term is a product of some of the r_i and some of the identities I. If all r_i have a norm < 1, then the terms that dominate will be those that consist of fewer r_i, resulting in a lower effective depth. This is described in Veit et al, 2016. While this analysis was originally used for residual networks, they relate this to any network by letting I turn into an arbitrary initial function. Their main theoretical result from this is that deeper networks take exponentially longer to train (under certain conditions), which they test out with (linear?) networks of depth 50 and width 100. They also propose that the reason gradients explode is because networks try to preserve their domain going forward, which requires Jacobians to have determinant 1 and leads to a higher Q-norm. Main Comments: This could potentially be a very nice paper, but I feel the current presentation is not ready for acceptance. In particular, the paper would benefit greatly from being made much shorter, and having more of the important details or proof outlines for the various propositions in the main text. Right now, it is quite confusing to follow, and I fail to see the motivation for some of the analysis. For example, the Gradient Scale Coefficient appears to be motivated because (bottom page 3), with other norm measurements, we could take any architecture and rescale the parameters, and inversely scale the gradients to make it \"easy to train\". But typically easy to train does not involve a specific preprocessing of gradients. Other propositions e.g. Theorem 1, proposition 6, could do with clearer intuition leading to them. I think the assumptions made in the results should also be clearer. (It's fine to have results, but currently I can't tell under what conditions the results apply and under what conditions they don't. E.g. are there any extensions of this that apply to non-linear networks?) I also have issues with their experimental setup: why choose to experiment on networks of depth 50 and width 100? This doesn't really look anything like networks that are trained in practice. Calling these \"popular architectures\" is misleading. In summary, I think this paper needs more work on the presentation to make clear what they are proving and under what conditions, and with experiments that are closer to those used in practice to support their claims. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "# # # `` I would have liked to have seen what was meant by preserved the function here -- did you mean preserve the same class outputs e.g '' Yes , we mean the value of the prediction and error layers is invariant . In the revision , we have amended the text to reflect this . # # # `` They focus on linear MLPs in the paper for computational simplicity . '' When you say `` linear MLPs '' , do you mean MLPs containing only linear layers ? Note that all of the MLPs we study in this paper contain nonlinear layers ( ReLU , tanh , SeLU , batch normalization and layer normalization ) and many also contain skip connections . We do not study linear MLPs in this paper . # # # `` But typically easy to train does not involve a specific preprocessing of gradients . '' In the revision , we replace `` easy to train '' with `` can be successfully trained '' and make clear that this includes gradient rescaling . In the paper , we aim to contrast training difficulty that can be overcome by rescaling the gradient versus training difficulty that can not be overcome in this way , as encapsulated by theorem 1 . We agree that it is not always obvious how to scale the gradient in practice , but point out that techniques such as Adam , vSGD [ 4 ] or heuristics such as `` scale the gradient to be proportial to the size of the weight matrix '' are often quite successful . # # # `` Other propositions e.g.Theorem 1 , proposition 6 , could do with clearer intuition leading to them . I think the assumptions made in the results should also be clearer . '' We added additional explanations to the leadup of both theorem 1 and proposition 6 in the revision . Unfortunately , we were unable to include the assumptions made in theoretical results in the main body of the paper due to space reason , and because we think it would significantly detract from the readability of the paper . For example , consider the full statement of theorem 1 . While some readers will be interested in this full statement , other readers may find it distracting . However , the assumptions are given and discussed in detail in sections E and F. Is there a specific section , paragraph or detail from the appendix you believe we should include in the main body ? # # # We hope that we have addressed your concerns in this comment and the revised version of the paper . We also refer you to our new introduction and conclusion section that make the contributions and implications of our paper even more clear . If you agree that our paper makes important contributions that are also well-supported ( taking into account that we do not just use linear MLPs ) we hope that you agree our paper is well-placed at ICLR . We look forward to hearing your thoughts and comments . [ 1 ] Schoenholz et al.Deep Information Propagation . ICLR 2107. https : //arxiv.org/abs/1611.01232 [ 2 ] Balduzzi et al.The Shattered Gradients Problem : If resnets are the answer , then what is the question ? . ICML , 2017. https : //arxiv.org/abs/1702.08591 [ 3 ] Yang & Schoenholz . Mean field residual networks : on the edge of chaos . NIPS , 2017. https : //papers.nips.cc/paper/6879-mean-field-residual-networks-on-the-edge-of-chaos [ 4 ] Schaul et al.No More Pesky Learning Rates . ICML , 2013. https : //arxiv.org/abs/1206.1106 [ 5 ] Saxe et al.Exact solutions to the nonlinear dynamics of learning in deep linear neural networks . 2014. https : //arxiv.org/abs/1312.6120"}, "2": {"review_id": "HkpYwMZRb-2", "review_text": "The paper makes some bold claims. In particular about commonly accepted intuition for avoiding exploding/vanishing gradients and why all the recent bag of tricks (BN, Adam) do not actually address the problems they set out to alleviate. This is either a very important paper or the analysis is incorrect but it's not my area of expertise. Actually understanding it at depth and validating the proofs and validity of the experiments will require some digestion. It's possible some of the issues arise from the particular architectures they choose to investigate and demonstrate on (eg I have mostly seen ResNets in the context of CNNs but they analyze on FC topologies, the form of the loss, etc) but that's a guess and there are some further analysis in the supp material for these networks which I haven't looked at in detail. Regardless - an important note to the authors is that it's a particularly long and verbose paper, coming in at 16 pages of the main paper(!) with nearly 50 (!) pages of supplementary material where the heart and meat of the proofs and experiments reside. As such it's not even clear if this is proper for a conference. The authors have already provided several pages worth of additional comments on the website on further related work. I view this as an issue in and of itself. Being succinct and applying rigour in editing is part of doing science and reporting findings, and a wise guideline to follow. While the authors may claim it's necessary to use that much space to make their point I will argue that this length is uncalibrated to standards. I've seen many papers that need to go through much more complicated derivations and theory and remain within a 8-10 page limit by being precise and strictly to the point. Perhaps Godel could be a good inspiration here, with a 21 page PhD thesis that fundamentally changed mathematics. In addition to being quite bold in claims, it is also somewhat confrontational in style. I understand the authors are trying to make a very serious claim about much of the common wisdom, but again, having reviewed papers for many years, this is highly unusual and it is questionable whether it is necessary. So, while I cannot vouch for the correctness, I think it can and should go through a serious revision to make it succinct and that will likely considerably help in making it accessible to a wider readership and aligned to the expectations from a conference paper in the field. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Dear Reviewer , Thank you for your review and for your honesty in stating that this paper does not fall within your area of expertise . I just uploaded a revised version of the paper . We address the points raised in your review in this revision as well as in the comments below . # # # `` an important note to the authors is that it 's a particularly long and verbose paper '' In the revision , we removed some of the less central results ( propositions 7 through 9 ) as well as high-level commentary to make the paper more focused . This paper addresses many issues in detail that other papers often gloss over , such as the rigorous definition of exploding gradients or effective depth and the careful setting of layerwise step sizes . This rigor is what enables us to obtain important results . Also note that an important predecessor work [ 3 ] from NIPS 2017 is also 55 pages long . Much of our appendix is strictly optional for readers interested in certain specifics , such as implementation details for those interested in replicating our results or the extended related work section for those interested in pursuing research in deep learning theory . # # # `` It 's possible some of the issues arise from the particular architectures they choose to investigate and demonstrate on '' While we believe that all results discussed in the paper apply to convolutional and other networks in a similar fashion , we do not discuss or test the applicability to these networks specifically , for space reasons . However , using very deep MLPs as a testbed to advance the study of exploding gradients and related problems is a well-established practice ( e.g . [ 1,2,3,4 ] ) . # # # `` it is also somewhat confrontational in style '' I apologize if my writing style appeared confrontational . Do you mean the paragraph that starts with `` These claims are mistaken . ... '' ? I reformulated that paragraph in the revision . It now starts with `` We argue that these claims are overly optimistic ... '' # # # `` making it accessible to a wider readership and aligned to the expectations from a conference paper in the field '' We do not necessarily agree that ICLR papers should appeal to a wide readership . Many program synthesis papers are targeted at those interested in program synthesis . Many machine translation papers are targeted at those interested in NLP etc . Our paper is targeted at those interested in the theory of neural networks and foundational principles of neural network architecture design . We accept that this is a subset of the entire ICLR audience and do not see anything wrong with that . [ 1 ] Schoenholz et al.Deep Information Propagation . ICLR 2107. https : //arxiv.org/abs/1611.01232 [ 2 ] Balduzzi et al.The Shattered Gradients Problem : If resnets are the answer , then what is the question ? . ICML , 2017. https : //arxiv.org/abs/1702.08591 [ 3 ] Yang & Schoenholz . Mean field residual networks : on the edge of chaos . NIPS , 2017. https : //papers.nips.cc/paper/6879-mean-field-residual-networks-on-the-edge-of-chaos [ 4 ] Saxe et al.Exact solutions to the nonlinear dynamics of learning in deep linear neural networks . 2014. https : //arxiv.org/abs/1312.6120"}}