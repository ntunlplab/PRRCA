{"year": "2020", "forum": "SylUzpNFDS", "title": "SoftLoc: Robust Temporal Localization under Label Misalignment", "decision": "Reject", "meta_review": "Main content:\n\nBlind review #3 summarizes it well:\n\nThis paper proposes a new loss for training models that predict where events occur in a sequence when the training sequence has noisy labels. The central idea is to smooth the label sequence and prediction sequence and compare these rather than to force the model to treat all errors as equally serious.\n\nThe proposed problem seems sensible, and the method is a reasonable approach. The evaluations are carried out on a variety of different tasks (piano onset detection, drum detection, smoking detection, video action segmentation).\n\n--\n\nDiscussion:\n\nThe reviewers were concerned about the relatively low level of novelty, simplicity of the proposed approach (which the authors argue could be seen as a feature rather than a flaw, given its good performance), and inadequate motivation.\n\n--\n\nRecommendation and justification:\n\nAfter the authors' revision in response to the reviews, this paper could be a weak accept if not for the large number of stronger submissions.", "reviews": [{"review_id": "SylUzpNFDS-0", "review_text": "This paper proposes a new loss for training models that predict where events occur in a sequence when the training sequence has noisy labels. The central idea is to smooth the label sequence and prediction sequence and compare these rather than to force the model to treat all errors as equally serious. The proposed problem seems sensible, and the method is a reasonable approach. The evaluations are carried out on a variety of different tasks (piano onset detection, drum detection, smoking detection, video action segmentation). Suggestions * While the authors do bring up lots of related work on learning from noisy labels, the insights from that work, and its relationship to this proposed technique could be more productively explored * The connections between the assumptions of the evaluation metric and the motivation for the smoothing methodology could be more productively elucidated * The task explored in this paper, and the task-specific problem, should be described more generally since ICLR has a generalist readership. For instance, the paper gets off on a rather strange footing discussing large data (indeed, but evident to the vast majority of ICLR readers), but little is said in terms of the specifics of the temporal localisation problem except via citations to other papers. In particular, the task set up, the problem posed by label misalignment could be described elegantly right in the introduction with a carefully designed figure. In section 2, the description is hard to follow, the mathematical notation is vague and hard to parse. For instance, the label sequence Y, is d-dimensional- the meaning of d should be clarified. Calling it \u201cdiscretised\u201d is also a bit strange, for most readers- it\u2019s a label sequence. The evaluation objective needs to be clarified, at least qualitatively; ideally in the introduction. Introducing a new objective is meaningful not only in light of noisy labels (always a problem), but in light of how the evaluation is carried out. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your insightful remarks . Based on your suggestions , Sections 1 , 3 and 4.1 have been thoroughly updated and improved . For instance , the Introduction focuses now more on task specific matters , the Related works is more proactive , and the methodology is more clearly motivated . Figures 1 and 3 have also been added respectively to better illustrate the task at hand and to display the main conceptual differences with the classical approaches . ( 1 ) Related Works We agree with your assessment and have rewritten parts of the Related Works section to more proactively explain the differences between past works and our proposed approach . Figure 3 has also been added to more clearly illustrate the conceptual novelties . In addition , we took out the paragraph about classification noise , as the two problems are not closely related and setting the focus on a difference task might dilute the message about the objective of this work . Overall , we feel that these changes make the section more direct and proactive . ( 2 ) Methodology The chosen smoothing methodology builds on top of the intuitive classical approach of smoothing the labels only . The design choices are motivated by the need to improve these classical approaches . In the updated version of the text , Sections 3 and 4.1 as well as the new Figures 2 , 3 and 4 highlight more clearly the issues caused by these widespread classical methods as well as the solutions provided by the proposed loss function . Overall , as the general motivation underlying the new methodology is mainly based on intuitive concepts , the paper presents a large experimental evaluation with 4 different experiments in 3 different domains ( audio sequences , wearable sensors timeseries , video ) to support the main claim of the work . The results confirm the effectiveness of the newly introduced loss function . ( 3 ) Introduction We agree that the specific problem addressed in this work was introduced too briefly in the initial version of the paper . Therefore , Figure 1 has been added to the first page to have a direct visual illustration of the task addressed in this work . Additionally , a full paragraph ( 2 ) focusing on the temporal localization task has been included in the updated text . While being trivial for the majority of the ICLR readership , we feel that the initial paragraph might be useful for reader coming from a more general audience . Indeed , the first few sentences gradually set the context ( e.g.deep learning , large datasets , label noise ) , before the specifics of the task are addressed . Overall , after adding a new paragraph on temporal localization and including Figure ( 1 ) based on your suggestions , we feel that this new introduction sets the stage for the rest of the paper more completely . Finally , we have made some clarifications in Section 2 . ( 4 ) Evaluation Objective In all honesty , we are not sure to fully understand the suggestion . In the event that the modifications made in the introduction and in Section 3 and 4.1 do not answer this , would it be possible to clarify this point , so that we can improve the text accordingly ? We hope that we have properly included your suggestions in the updated version of the text . If you have any other suggestions , please let us know , so we can take them into account ."}, {"review_id": "SylUzpNFDS-1", "review_text": "This paper deals with temporal label noise, or label misalignment problem in localization, which is the long-standing problem of robust event localization regarding the temporally misaligned labels in sequential data. Some existing works are constrained to hardly fit the annotations of data during training. The work models the temporal label misalignment with perturbations to the ground-truth timestamps, and presents a soft localization learning scheme which relaxes the reliance on the exact position of labels. The idea introduced to temporal data is well-motivated since the label for temporal data is often dispersed. Technically speaking, the proposed SoftLoc loss comprises two terms: 1). $\\mathscr{L}_{SLL}$: a soft learning loss that relaxes the prediction mass concentration, by symmetrically filtering the labels and predictions. This term helps to relax the model\u2019s reliance on exact label locations. 2). $\\mathscr{L}_{MC}$: a mass convergence loss that acts as a regularizer to facilitate the model with precise impulse-like localizations. With a trade of factor to balance two terms, the SoftLoc model can achieve precise impulse-like localization performance without weakening the model robustness. Various applications, such as PIANO ONSET, DRUM DETECTION and TIME SERIES DETECTION are performed to verify the effectiveness of the proposed method. And state-of-the-art performance is achieved. Despite the achievement indicated by the experiments, I still have some concerns about this paper. (1) The authors propose the relaxed loss L_{SLL} for the soft learning of the location. And the label smoothing idea (applying a \u02dcS^2-Gaussian filter to the labels) has been introduced to increase the robustness to temporal misalignment of annotations [2]. Although the authors discuss the several inherent drawbacks of it in the 2nd paragraph of Related Work, they still follow the label smoothing idea in their model (Eq.2) in Section 4.1 in a two-side smoothed process. The reason should be clarified. (2) The authors claim that their method has the advantage of generalizing some regimes of weakly-supervised learning. And the adopted L_{MC} is a weakly-supervised loss. Does the advantage come from L_{MC}? (3) The authors propose the two-side relaxed loss L_{SLL} for the soft learning of temporal localization problem. However, in the ablation study, the authors do not give the results with different level of label noise for one-side variant. Adding these results could help to demonstrate the necessity of the two-side relaxed loss L_{SLL}. References: [1] Improved musical onset detection with convolutional neural networks [2] Onsets and frames: Dual-objective piano transcription ", "rating": "6: Weak Accept", "reply_text": "Thank you for your constructive remarks . Based on your comments , we have updated Sections 3 and 4.1 to focus more on the choice of methodology and have added Figure 3 ( and also Figures 2 and 4 ) to highlight even more clearly the difference between our model and the classical approaches . ( 1 ) Method Clarification We agree that the motivation could have been expressed more clearly . The text ( Sections 3 and 4.1 ) has been updated to provide more clarity on the fundamental conceptual difference between our approach and the classical ones . In short , there is no issue with smoothing per say , problems arise when smoothing the labels * * only * * . However , most of these issues can simply be solved by not only smoothing the labels , but also the predictions . While this change might seem subtle , it has quite large implications in terms of modelling . A new figure ( 3 ) illustrates the main conceptual differences . Overall , instead of transforming the point prediction problem into a distribution prediction problem , smoothing both the labels and predictions allows the model to infer point predictions directly . It is only the loss function that views these point predictions as distributions and thus that deals with the temporal uncertainty of the labels . ( See also discussion on novelty in the reply to Reviewer 1 . ) ( 2 ) Generalization of Past Models Obviously , as the proposed loss is a linear combination of a soft fully supervised loss ( $ L_ { SLL } $ ) and a weakly supervised loss ( $ L_ { MC } $ ) , setting $ \\alpha=1 $ would result in a weakly-supervised loss ( i.e. $ L_ { MC } $ ) . However , the comment about the generalization of regimes from fully supervised to weakly supervised ( in Section 4.3 . ) focuses on the power of the smoothing parameter $ \\mathcal { S } _ { M } $ . On the one side of the spectrum , setting $ \\mathcal { S } _ { M } =0 $ yields a loss that strictly takes the location of the labels into account ( e.g.stepwise cross-entropy ) . One the other side , $ \\mathcal { S } _ { M } \\to\\infty $ yields a loss that totally discard all prior location information ( i.e. $ L_ { MC } $ ) . Therefore , varying $ \\mathcal { S } _ { M } $ allows to control the amount of relaxation of the model reliance on the exact localization of the labels ( ranging from fully supervised to fully supervised . ) Overall , while the $ \\alpha $ parameter regulates the predominance of the mass convergence over the soft learning , the softness parameter can define a wide range of models ranging from a strict fully-supervised model to a weakly-supervised model . ( 3 ) Ablation Study Both benchmarks from Hawthorn et al . ( Figure 1 in Section 5.1.1 ) are using one-sided smoothing with some peak picking . Their performance in moderate to high noise settings demonstrate the necessity for another approach . Additionally , the ablation study was designed to evaluate the subcomponents of our full model . Since peak picking is not part of our final approach , we chose not to include the Hawthorn et al.benchmarks in the ablation study . Second , one could further be interested in a model trained solely with one sided smoothing loss without peak picking . This approach is however bound to yield very poor results since the model would infer a distribution and the evaluation metric requires point predictions . Finally , the performance of a one-sided smoothing combined with the weakly supervised $ L_ { MC } $ could be assessed . However , these two losses have conflicting objectives , as the former requires the prediction to be spread out over time , while the latter requires the prediction to be point-like . The training of models with conflicting loss functions is very unstable and , in this case , would certainly lead to poor results . Overall , we preferred to only include the two components of the final loss function in our ablation study and feel that including weak benchmark ( e.g.one sided without peak picking ) would only be viewed as an attempt to artificially embellish our results . We hope that this completely answers your questions . If not , feel free to ask for more detailed explanations ."}, {"review_id": "SylUzpNFDS-2", "review_text": "The authors propose SoftLoc, an interpolation of two temporal event prediction losses (one based on label & prediction smoothing, and one based on weakly-supervised count-based loss) that makes the predictor more robust to noisy training data. They demonstrate on various temporal alignment datasets from music to wearable sensors to video action segmentations, that their method is performs well both on noisy-free and noisy settings compared to prior approaches. Strengths: (1) relatively thorough experimental valuations: using 4 datasets comparing with sufficient number of prior approaches (One potential improvement could be to try noise distributions other than Gaussian) (2) simple objective and consistent improvements. It is encouraging that the simple modification enables such consistent empirical improvements. Weaknesses: (1) the novelty of the method appears limited. The weakly-supervised loss is borrowed from the prior work, so it seems like the main algorithmic novelty is to add noise to predictions (as opposed to just adding to labels as done in prior work). Other comments: (1) in 5.1.1, is there justification for \\alpha_\\tau expression? How did you pick that? (2) is Issue 1 an actual problem? Even with just label smoothing, I would expect predictor will find the mode and peak-picking can produce the correct predictions. Similarly, discussions of how the proposed method can solve the issues listed may benefit from some rewriting, or even simple toy experiments to demo that those are actually the concerns that are not addressed by prior work. Such toy experiments may complement the existing end-to-end experiments to demonstrate the precise properties of the proposed method. ", "rating": "3: Weak Reject", "reply_text": "-- -- Other comments ( 1 ) The general motivation for the choice of $ \\alpha_ { \\tau } $ is briefly mentioned in Section 4.2. : \u201c From a practical standpoint , starting with a moderate $ \\alpha_ { \\tau } $ allows an initial relaxed localization learning , before performing stronger mass convergence \u201d . More precisely , this setting allows for an initial long ( > 1e5 iterations ) learning phase with $ \\alpha_ { \\tau } =0.2 $ , where the approximate location can be learnt without much consideration for the sparsity of the predictions . The core of the representation learning is also achieved during this phase . After this stage , the predictions are generally scattered over several time-steps . The $ \\alpha_ { \\tau } $ is then linearly increased until obtaining precise point predictions . Almost no hyperparameter tuning was performed in this paper as the central objective of the work is about robustness to noise and not raw performance . We simply ensured that 1 ) $ \\alpha_ { \\tau } $ is initially small enough for the smooth location learning loss to drive the training , 2 ) the initial phase lasts long enough for the location learning loss to plateau and 3 ) the number of iteration is large enough to allow for the convergence of the full loss . ( 2 ) Issue 1 is certainly a problem in moderate to high noise settings as demonstrated by the experiments . In Section 5.1.1. the benchmarks from Hawthorne et al.performs label smoothing and peak picking . While their performance is overall very good when the standard deviation of the normal noise is under 25ms , their score plummets after that point . In noisy settings , the main issue comes from the fact that the performance of such approaches relies heavily on peak picking , which is not part of the learning process itself . In contrast , our approach learns precise localization in an end-to-end fashion . Following your recommendation , Sections 3 , 4.1 , 4.2 have been rewritten to explain in more details the benefits of our model against classical approaches , while illustrations ( Figures 2 and 4 ) have been added to the text for a more visual presentation of the model \u2019 s properties . We hope we were able to address your concern . If you have any further concern or question , do not hesitate to ask ."}], "0": {"review_id": "SylUzpNFDS-0", "review_text": "This paper proposes a new loss for training models that predict where events occur in a sequence when the training sequence has noisy labels. The central idea is to smooth the label sequence and prediction sequence and compare these rather than to force the model to treat all errors as equally serious. The proposed problem seems sensible, and the method is a reasonable approach. The evaluations are carried out on a variety of different tasks (piano onset detection, drum detection, smoking detection, video action segmentation). Suggestions * While the authors do bring up lots of related work on learning from noisy labels, the insights from that work, and its relationship to this proposed technique could be more productively explored * The connections between the assumptions of the evaluation metric and the motivation for the smoothing methodology could be more productively elucidated * The task explored in this paper, and the task-specific problem, should be described more generally since ICLR has a generalist readership. For instance, the paper gets off on a rather strange footing discussing large data (indeed, but evident to the vast majority of ICLR readers), but little is said in terms of the specifics of the temporal localisation problem except via citations to other papers. In particular, the task set up, the problem posed by label misalignment could be described elegantly right in the introduction with a carefully designed figure. In section 2, the description is hard to follow, the mathematical notation is vague and hard to parse. For instance, the label sequence Y, is d-dimensional- the meaning of d should be clarified. Calling it \u201cdiscretised\u201d is also a bit strange, for most readers- it\u2019s a label sequence. The evaluation objective needs to be clarified, at least qualitatively; ideally in the introduction. Introducing a new objective is meaningful not only in light of noisy labels (always a problem), but in light of how the evaluation is carried out. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your insightful remarks . Based on your suggestions , Sections 1 , 3 and 4.1 have been thoroughly updated and improved . For instance , the Introduction focuses now more on task specific matters , the Related works is more proactive , and the methodology is more clearly motivated . Figures 1 and 3 have also been added respectively to better illustrate the task at hand and to display the main conceptual differences with the classical approaches . ( 1 ) Related Works We agree with your assessment and have rewritten parts of the Related Works section to more proactively explain the differences between past works and our proposed approach . Figure 3 has also been added to more clearly illustrate the conceptual novelties . In addition , we took out the paragraph about classification noise , as the two problems are not closely related and setting the focus on a difference task might dilute the message about the objective of this work . Overall , we feel that these changes make the section more direct and proactive . ( 2 ) Methodology The chosen smoothing methodology builds on top of the intuitive classical approach of smoothing the labels only . The design choices are motivated by the need to improve these classical approaches . In the updated version of the text , Sections 3 and 4.1 as well as the new Figures 2 , 3 and 4 highlight more clearly the issues caused by these widespread classical methods as well as the solutions provided by the proposed loss function . Overall , as the general motivation underlying the new methodology is mainly based on intuitive concepts , the paper presents a large experimental evaluation with 4 different experiments in 3 different domains ( audio sequences , wearable sensors timeseries , video ) to support the main claim of the work . The results confirm the effectiveness of the newly introduced loss function . ( 3 ) Introduction We agree that the specific problem addressed in this work was introduced too briefly in the initial version of the paper . Therefore , Figure 1 has been added to the first page to have a direct visual illustration of the task addressed in this work . Additionally , a full paragraph ( 2 ) focusing on the temporal localization task has been included in the updated text . While being trivial for the majority of the ICLR readership , we feel that the initial paragraph might be useful for reader coming from a more general audience . Indeed , the first few sentences gradually set the context ( e.g.deep learning , large datasets , label noise ) , before the specifics of the task are addressed . Overall , after adding a new paragraph on temporal localization and including Figure ( 1 ) based on your suggestions , we feel that this new introduction sets the stage for the rest of the paper more completely . Finally , we have made some clarifications in Section 2 . ( 4 ) Evaluation Objective In all honesty , we are not sure to fully understand the suggestion . In the event that the modifications made in the introduction and in Section 3 and 4.1 do not answer this , would it be possible to clarify this point , so that we can improve the text accordingly ? We hope that we have properly included your suggestions in the updated version of the text . If you have any other suggestions , please let us know , so we can take them into account ."}, "1": {"review_id": "SylUzpNFDS-1", "review_text": "This paper deals with temporal label noise, or label misalignment problem in localization, which is the long-standing problem of robust event localization regarding the temporally misaligned labels in sequential data. Some existing works are constrained to hardly fit the annotations of data during training. The work models the temporal label misalignment with perturbations to the ground-truth timestamps, and presents a soft localization learning scheme which relaxes the reliance on the exact position of labels. The idea introduced to temporal data is well-motivated since the label for temporal data is often dispersed. Technically speaking, the proposed SoftLoc loss comprises two terms: 1). $\\mathscr{L}_{SLL}$: a soft learning loss that relaxes the prediction mass concentration, by symmetrically filtering the labels and predictions. This term helps to relax the model\u2019s reliance on exact label locations. 2). $\\mathscr{L}_{MC}$: a mass convergence loss that acts as a regularizer to facilitate the model with precise impulse-like localizations. With a trade of factor to balance two terms, the SoftLoc model can achieve precise impulse-like localization performance without weakening the model robustness. Various applications, such as PIANO ONSET, DRUM DETECTION and TIME SERIES DETECTION are performed to verify the effectiveness of the proposed method. And state-of-the-art performance is achieved. Despite the achievement indicated by the experiments, I still have some concerns about this paper. (1) The authors propose the relaxed loss L_{SLL} for the soft learning of the location. And the label smoothing idea (applying a \u02dcS^2-Gaussian filter to the labels) has been introduced to increase the robustness to temporal misalignment of annotations [2]. Although the authors discuss the several inherent drawbacks of it in the 2nd paragraph of Related Work, they still follow the label smoothing idea in their model (Eq.2) in Section 4.1 in a two-side smoothed process. The reason should be clarified. (2) The authors claim that their method has the advantage of generalizing some regimes of weakly-supervised learning. And the adopted L_{MC} is a weakly-supervised loss. Does the advantage come from L_{MC}? (3) The authors propose the two-side relaxed loss L_{SLL} for the soft learning of temporal localization problem. However, in the ablation study, the authors do not give the results with different level of label noise for one-side variant. Adding these results could help to demonstrate the necessity of the two-side relaxed loss L_{SLL}. References: [1] Improved musical onset detection with convolutional neural networks [2] Onsets and frames: Dual-objective piano transcription ", "rating": "6: Weak Accept", "reply_text": "Thank you for your constructive remarks . Based on your comments , we have updated Sections 3 and 4.1 to focus more on the choice of methodology and have added Figure 3 ( and also Figures 2 and 4 ) to highlight even more clearly the difference between our model and the classical approaches . ( 1 ) Method Clarification We agree that the motivation could have been expressed more clearly . The text ( Sections 3 and 4.1 ) has been updated to provide more clarity on the fundamental conceptual difference between our approach and the classical ones . In short , there is no issue with smoothing per say , problems arise when smoothing the labels * * only * * . However , most of these issues can simply be solved by not only smoothing the labels , but also the predictions . While this change might seem subtle , it has quite large implications in terms of modelling . A new figure ( 3 ) illustrates the main conceptual differences . Overall , instead of transforming the point prediction problem into a distribution prediction problem , smoothing both the labels and predictions allows the model to infer point predictions directly . It is only the loss function that views these point predictions as distributions and thus that deals with the temporal uncertainty of the labels . ( See also discussion on novelty in the reply to Reviewer 1 . ) ( 2 ) Generalization of Past Models Obviously , as the proposed loss is a linear combination of a soft fully supervised loss ( $ L_ { SLL } $ ) and a weakly supervised loss ( $ L_ { MC } $ ) , setting $ \\alpha=1 $ would result in a weakly-supervised loss ( i.e. $ L_ { MC } $ ) . However , the comment about the generalization of regimes from fully supervised to weakly supervised ( in Section 4.3 . ) focuses on the power of the smoothing parameter $ \\mathcal { S } _ { M } $ . On the one side of the spectrum , setting $ \\mathcal { S } _ { M } =0 $ yields a loss that strictly takes the location of the labels into account ( e.g.stepwise cross-entropy ) . One the other side , $ \\mathcal { S } _ { M } \\to\\infty $ yields a loss that totally discard all prior location information ( i.e. $ L_ { MC } $ ) . Therefore , varying $ \\mathcal { S } _ { M } $ allows to control the amount of relaxation of the model reliance on the exact localization of the labels ( ranging from fully supervised to fully supervised . ) Overall , while the $ \\alpha $ parameter regulates the predominance of the mass convergence over the soft learning , the softness parameter can define a wide range of models ranging from a strict fully-supervised model to a weakly-supervised model . ( 3 ) Ablation Study Both benchmarks from Hawthorn et al . ( Figure 1 in Section 5.1.1 ) are using one-sided smoothing with some peak picking . Their performance in moderate to high noise settings demonstrate the necessity for another approach . Additionally , the ablation study was designed to evaluate the subcomponents of our full model . Since peak picking is not part of our final approach , we chose not to include the Hawthorn et al.benchmarks in the ablation study . Second , one could further be interested in a model trained solely with one sided smoothing loss without peak picking . This approach is however bound to yield very poor results since the model would infer a distribution and the evaluation metric requires point predictions . Finally , the performance of a one-sided smoothing combined with the weakly supervised $ L_ { MC } $ could be assessed . However , these two losses have conflicting objectives , as the former requires the prediction to be spread out over time , while the latter requires the prediction to be point-like . The training of models with conflicting loss functions is very unstable and , in this case , would certainly lead to poor results . Overall , we preferred to only include the two components of the final loss function in our ablation study and feel that including weak benchmark ( e.g.one sided without peak picking ) would only be viewed as an attempt to artificially embellish our results . We hope that this completely answers your questions . If not , feel free to ask for more detailed explanations ."}, "2": {"review_id": "SylUzpNFDS-2", "review_text": "The authors propose SoftLoc, an interpolation of two temporal event prediction losses (one based on label & prediction smoothing, and one based on weakly-supervised count-based loss) that makes the predictor more robust to noisy training data. They demonstrate on various temporal alignment datasets from music to wearable sensors to video action segmentations, that their method is performs well both on noisy-free and noisy settings compared to prior approaches. Strengths: (1) relatively thorough experimental valuations: using 4 datasets comparing with sufficient number of prior approaches (One potential improvement could be to try noise distributions other than Gaussian) (2) simple objective and consistent improvements. It is encouraging that the simple modification enables such consistent empirical improvements. Weaknesses: (1) the novelty of the method appears limited. The weakly-supervised loss is borrowed from the prior work, so it seems like the main algorithmic novelty is to add noise to predictions (as opposed to just adding to labels as done in prior work). Other comments: (1) in 5.1.1, is there justification for \\alpha_\\tau expression? How did you pick that? (2) is Issue 1 an actual problem? Even with just label smoothing, I would expect predictor will find the mode and peak-picking can produce the correct predictions. Similarly, discussions of how the proposed method can solve the issues listed may benefit from some rewriting, or even simple toy experiments to demo that those are actually the concerns that are not addressed by prior work. Such toy experiments may complement the existing end-to-end experiments to demonstrate the precise properties of the proposed method. ", "rating": "3: Weak Reject", "reply_text": "-- -- Other comments ( 1 ) The general motivation for the choice of $ \\alpha_ { \\tau } $ is briefly mentioned in Section 4.2. : \u201c From a practical standpoint , starting with a moderate $ \\alpha_ { \\tau } $ allows an initial relaxed localization learning , before performing stronger mass convergence \u201d . More precisely , this setting allows for an initial long ( > 1e5 iterations ) learning phase with $ \\alpha_ { \\tau } =0.2 $ , where the approximate location can be learnt without much consideration for the sparsity of the predictions . The core of the representation learning is also achieved during this phase . After this stage , the predictions are generally scattered over several time-steps . The $ \\alpha_ { \\tau } $ is then linearly increased until obtaining precise point predictions . Almost no hyperparameter tuning was performed in this paper as the central objective of the work is about robustness to noise and not raw performance . We simply ensured that 1 ) $ \\alpha_ { \\tau } $ is initially small enough for the smooth location learning loss to drive the training , 2 ) the initial phase lasts long enough for the location learning loss to plateau and 3 ) the number of iteration is large enough to allow for the convergence of the full loss . ( 2 ) Issue 1 is certainly a problem in moderate to high noise settings as demonstrated by the experiments . In Section 5.1.1. the benchmarks from Hawthorne et al.performs label smoothing and peak picking . While their performance is overall very good when the standard deviation of the normal noise is under 25ms , their score plummets after that point . In noisy settings , the main issue comes from the fact that the performance of such approaches relies heavily on peak picking , which is not part of the learning process itself . In contrast , our approach learns precise localization in an end-to-end fashion . Following your recommendation , Sections 3 , 4.1 , 4.2 have been rewritten to explain in more details the benefits of our model against classical approaches , while illustrations ( Figures 2 and 4 ) have been added to the text for a more visual presentation of the model \u2019 s properties . We hope we were able to address your concern . If you have any further concern or question , do not hesitate to ask ."}}