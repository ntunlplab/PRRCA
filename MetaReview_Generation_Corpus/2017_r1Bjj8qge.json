{"year": "2017", "forum": "r1Bjj8qge", "title": "Encoding and Decoding Representations with Sum- and Max-Product Networks", "decision": "Reject", "meta_review": "Dear authors, in general the reviewers found that the paper was interesting and has potential but needs additional work in the presentation and experiments. Unfortunately, even if all reviews had been a weak accept (i.e. all 6s) it would not have met the very competitive standard for this year.\n \n A general concern among the reviewers was the presentation of the research, the paper and the experiments. Too much of the text was dedicated to the explanation of concepts which should be considered to be general knowledge to the ICLR audience (for example the justification for and description of generative models). That text could be replaced with further analysis and justification.\n \n The choice of baseline comparisons and benchmarks did not seem appropriate given the presented model and text. Specifically, it is difficult to determine how good of a generative model it is if the authors don't compare it to other generative models in terms of data likelihood under the model. Similarly, it's difficult to place it in the literature as a model for representation learning if it isn't compared to the state-of-the-art for RL on standard benchmarks.\n \n The clarifications of the authors and revisions to the manuscript are greatly appreciated. Hopefully this will help the authors to improve the manuscript and submit to another conference in the near future.", "reviews": [{"review_id": "r1Bjj8qge-0", "review_text": "This paper tries to solve the problem of interpretable representations with focus on Sum Product Networks. The authors argue that SPNs are a powerful linear models that are able to learn parts and their combinations, however, their representations havent been fully exploited by generating embeddings. Pros: -The idea is interesting and interpretable models/representations is an important topic. -Generating embeddings to interpret SPNs is a novel idea. -The experiments are interesting but could be extended. Cons: -The author's contribution isn't fully clear and there are multiple claims that need support. For example, SPNs are indeed interpretable as is, since the bottom-up propagation of information from the visible inputs could be visualized at every stage, and the top-down parse could be also visualized as it has been done before (Amer & Todorovic, 2015). Another example, Proposition one claims that MPNs are perfect encoder decoders since the max nodes always have one max value, however, what if it was uniformally distributed node, or there are two equal values? Did the authors run into such cases? Did they address all edge cases? -A good comparison could have been against Generative Adversarial Networks (GANs), Generative Stochastic Networks (GSNs) and Variational Autoencoders too since they are the state-of-the-art generative models, rather than comparing with RBMs and Nade. I would suggest that the authors take sometime to evaluate their approach against the suggested methods, and make sure to clarify their contributions and eliminate over claiming statements. I agree with the other comments raised by Anon-Reviewer1.", "rating": "6: Marginally above acceptance threshold", "reply_text": "-- -- -- -- Dear reviewer , thank you for your comments and for showing interest in our approach . In the following we clarify our choices and claims . > SPNs are indeed interpretable as is , since the bottom-up propagation > of information from the visible inputs could be visualized at every > stage , and the top-down parse could be also visualized as it has > been done before ( Amer & Todorovic , 2015 ) Indeed , SPNs have a clear , full probabilistic semantics but up to now they have been employed as classical probabilistic models in which interpretability is tackled only by looking at the results of single probabilistic queries , be them full evidence or MPE inference . To substantiate how our aim is different , consider the the work in ( Amer & Todorovic , 2015 ) . By applying an approximate MPE inference routine they i ) infer the most probable activity from a video sequence and ii ) map that activity to a foreground region in the video . They are weakly/fully supervised to infer the values of the indicator variables they set in a fixed-a-priori grid to locate these foreground/background regions . Instead , we train our SPNs in an unsupervised fashion , extracting structured embeddings from inner nodes jointly and employing them for predictive tasks on unseen variables . Moreover , by effectively visualizing the features extracted by each node , we provide an evidence that each node acts a meaningful part-based filter . We already investigated visualizing how inference propagates in SPNs at different stages in our unpublished arxiv paper ( Vergari et al.2016 ) .If needed , we could add to this work more visualizations from it . > Proposition one claims that MPNs are perfect encoder decoders since the max nodes always have one max value , however , what if it was uniformally distributed node , or there are two equal values ? Please note that Proposition one claims that MPNs are perfect encoder-decoders * only if * there are no ties among child distributions . This is always true for deterministic ( selective ) MPNs ( Proposition 2 ) but may not be true in practice for non-deterministic MPNs learned from data . > Did the authors run into such cases ? Did they address all edge cases ? Indeed , we investigated this issue empirically for our networks in Section 5.1 when employing full embeddings . Please refer to tables 5 and 6 to see how close to perfect encoder-decoders our MPNs were on the X and Y . > -A good comparison could have been against Generative Adversarial Networks ( GANs ) , Generative Stochastic Networks ( GSNs ) and Variational Autoencoders too since they are the state-of-the-art generative models , rather than comparing with RBMs and Nade To support our claim that probabilistic representations from tractable models like SPNs/MPNs are indeed meaningful , we devised our experimentation to provide fair competitors . While GANs , VAEs and even GSNs are state-of-the art models * for generating * high quality samples , their likelihood is intractable ( even implicit for GANs ) and exact inference is unfeasible with them . As a consequence , computing probabilistic features for unsupervised learning comparable to those extracted from SPNs in Section 3 would be not immediate . On the other hand , MADEs admit tractable exact full evidence inference , and they provide a way to decode the learned representations . This makes them a very natural competitor for our SPNs/MPNs encoding and decoding schemes . RBMs , while having an intractable joint likelihood , admit tractable conditionals of the hidden variables given the evidence and such probabilistic features have proved to be very competitive in the literature . We will discuss these differences in more detail in a new revision ."}, {"review_id": "r1Bjj8qge-1", "review_text": " The paper's aim is - as argued in the paper and the responses to other reviewers comments - that SPN and MPN can be interpreted as encoders and decoders of RL. Well - this is an interesting perspective and could be (potentially) worth a paper. However - the current draft is far from being convincing in that respect - and I am talking about the updated / improved version as of now. The paper does not require minor revisions to make this point apparent - but a significant and major rewrite which seems beyond what the authors have done so far. - the experiments are (as also pointed out by other reviewers) rather unstructured and difficult to see much of an insight. I should probably also list (as the other other reviewers) flaws and issues with the experiments - but given the detailed comments by the other reviewers there seems to be little additional value in doing so So in essence the paper simply does not deliver at this point on its promise (as far as I am concerned) and in that sense I suggest a very clear reject for this conference. As for the dataset employed: MNIST should be considered a toy-dataset for pretty much all purposes these days - and in that sense the dataset choice is not helping me (and many other people that you might want to convince) to safe this paper. ", "rating": "3: Clear rejection", "reply_text": "Dear reviewer , thanks for your comments up to now and for your appreciation of our idea to exploit SPNs/MPNs for RL . > the current draft is far from being convincing in that respect - and I am talking about the updated / improved version as of now . The paper does not require minor revisions to make this point apparent - but a significant and major rewrite which seems beyond what the authors have done so far . We acknowledge that the paper can be improved but we would appreciate somewhat more detailed comments . We are always eager to revise parts of the paper or add experiments . We believe that this is a key advantage of conferences such as ICLR and the open review process . A revision of this work could surely benefit from constructive feedback . > the experiments are ( as also pointed out by other reviewers ) rather unstructured and difficult to see much of an insight . I should probably also list ( as the other other reviewers ) flaws and issues with the experiments - but given the detailed comments by the other reviewers there seems to be little additional value in doing so Please note that there is only one other reviewer , whose score is `` Marginally above acceptance threshold '' . While we accept your opinion concerning our experiments , we indeed would appreciate an objective list of flaws in order to substantiate your rating . Please also note the proposed revision of our experiment section ( see reply to Reviewer 1 ) . > As for the dataset employed : MNIST should be considered a toy-dataset for pretty much all purposes these days - and in that sense the dataset choice is not helping me ( and many other people that you might want to convince ) to safe this paper . Please note that we employed MNIST only to show one aspect of the whole work : the visualizations of the part-based learned filters , for which an image dataset was needed . However , we conducted our experiments on 10 more standard benchmark datasets for Multilabel-classification , since our major aim is to test embeddings for discriminative structured output prediction . Please see our answer to your pre-review question on this matter . Again , we are open for suggestions concerning additional datasets to be included in our experiments . As a side note on learning the structure of an SPN on much larger ( image ) datasets , please consider that up to now , in the SPN literature , only ( image ) datasets on par with MNIST have been considered . Scaling structure learning would be worth a publication by itself . All in all , we believe it is time to start researching tractable generative models for RL , even if we have to cope with all the ( computational , theoretical , etc ) limitations for these architectures that have been overcome for other now-used-everyday deep learning models ."}, {"review_id": "r1Bjj8qge-2", "review_text": "The authors propose and evaluate using SPN's to generate embeddings of input and output variables, and using MPN to decode output embeddings to output variables. The advantage of predicting label embeddings is to decouple dependencies in the predicted space. The authors show experimentally that using SPN based embeddings is better than those produced by RBM's. This paper is fairly dense and a bit hard to read. After the discussion, the main contributions of the authors are: 1. They propose the scheme of learning SPN's over Y and then using MPN's to decode the output, or just SPNs to embed X. 2. They propose how to decode MPN's with partial data. 3. They perform some analysis of when their scheme will lead to perfect encoding/decodings. 4. They run many, many experiments comparing various ways of using their proposed method to make predictions on multi-label classification datasets. My main concerns with this paper are as follows: - The point of this paper is about using generative models for representation learning. In their experiments, the main task is discriminative; e.g. predict multiple Y from X. The only discriminative baseline is a L2 regularized logistic regression, which does not have any structure on the output; it'd be nice to see how a discriminative structured prediction method would do, such as CRF or belief propagation. - The many experiments suggest that their encoder/decoder scheme is working better than the alternatives; can you please give more details on the relative computation complexity of each method? - One thing I'm still having trouble understanding is *why* this method works better than MADE and the other alternatives. Is it learning a better model of the distribution of Y? Is it better at separating out correlations in the output into individual nodes? Does it have larger representations? - I think the experiments are overkill and if anything, they way they are presented detract from the paper. There's already far too many numbers and graphs presented to be easy to understand. If I have to dig through hundreds of numbers to figure out if your claim is correct, the paper is not clear enough. And, I said this before in my comments, please do not refer to Q1, Q2, etc. -- these shortcuts let you make the paper more dense with fewer words but at the cost of readability. I *think* I convinced myself that your method works...I would love to see a table that shows, for each condition: (A) a baseline X->Y, (B) one *average* result across datasets for your method, and (C) one *average* result from a reasonable best competitor method. Please show for both the exact match and hamming losses, as that will demonstrate the gap between independent linear prediction and structured prediction. That would still be plenty of numbers but would make it much easier for the reader to verify your claims and you can put everything else in the Appendix. E.g. something like: Input | Predicted Output | Decoder | Hamming | Exact Match ---- X | P(Y) | CRF | xx.xx | xx.xx (this is your baseline) SPN E_X | P(Y) | n/a | xx.xx | xx.xx X | SPN E_Y | MPN | xx.xx | xx.xx (given X, predict E_Y, then decode it with an MPN) Does a presentation like that make sense? It's just really hard and time-consuming for me as a reviewer to verify your results, the way you've laid them out currently. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "-- -- -- -- -- -- -- -- -- -- -- -- -- Dear reviewer , thanks for your time and suggestions . We will try to answer your questions more in detail , hoping to improve the paper towards a full acceptance . > The point of this paper is about using generative models for representation learning . In their experiments , the main task is discriminative ; e.g.predict multiple Y from X . The only discriminative baseline is a L2 regularized logistic regression , which does not have any structure on the output ; it 'd be nice to see how a discriminative structured prediction method would do , such as CRF or belief propagation . It is true that the embeddings from our models ( SPNs/MPNs , RBMs and MADEs ) come from generative models , in an unsupervised way . However , once these embeddings are extracted for the X/Y , a supervised linear model is trained on them ( be it either a logistic or a ridge regressor ) . This is the main reason why we compare all our results against the 'plain ' L2 logistic regressor baseline : to state how useful are the new embeddings compared to using none ( the original X and Y ) , i.e.how decoupled and now linearly separable are the new representations learned in the previous unsupervised stage . If needed , we can look into 'fully'-supervised non-linear discriminate models to provide additional comparisons . One way to cope with the structure in the Y would be to look at Classifier Chains [ 1 ] , we will also investigate CRFs as you suggest . All in all , if they directly optimize a loss to predict the Y , we expect them to perform better . > The many experiments suggest that their encoder/decoder scheme is working better than the alternatives ; can you please give more details on the relative computation complexity of each method ? To extract an embedding from an SPN/MPN we simply evaluate them bottom-up , thus the complexity is linear in the size of the network , as it is for the other neural models . Therefore , the main computational cost is learning the structure of the network with LearnSPN-b ( see Appendix C ) . In that iterative scheme , the cost of clustering features ( insert a product node ) is quadratic in the number of the features , while clustering instances ( insert a sum node ) can be quadratic in the number of instances [ 2 ] . However , no more learning is required for SPNs/MPNs : since both structure and weights are already learned . We argue that such a learning step is 'cheaper ' than performing intensive tuning for all the hyperparameters needed for specifying the structure of a neural model and learning its weight . E.g number of layers , hidden units , activation type , learning rate scheduling algorithm and coefficient , skipping connections and so on for MADEs , see the Appendix E.1.3 . > - One thing I 'm still having trouble understanding is * why * this method works better than MADE and the other alternatives . Is it learning a better model of the distribution of Y ? If we take the log-likelihood as the criterion to measure how good a distribution is fit , the answer is no . For instance MADE models have better log-likelihoods than SPNs in our experiments , since they directly optimize this loss while training , however their embeddings are less discriminative features . We can report in additional tables the log-likelihoods achieved on the test sets by MADEs and SPNs . From this perspective RBMs are not directly comparable since their log-likelihood is intractable and we optimize the pseudo-log-likelihood instead . MANIAC , on the other hand , is not probabilitstic at all . > Is it better at separating out correlations in the output into individual nodes ? We say yes , as we argue in the Introduction and Section 3 and 4 . Our point is that SPN embeddings are inherently ( probabilistic ) * part-based * features that are extracted while performing that sort of hierachical co-clustering by Learn-SPN . From this point of view , we expect them to better capture local variations . > Does it have larger representations ? We do not believe it to be the cause , either . To demonstrate this , we accurately selected our model capacities in the experiments . Since LearnSPN-b adaptively grows a network , we first measured the embedding size from the learned SPNs and therefore trained RBMs/MADEs to have similar or bigger size . Hence the need for models with different hidden unit sizes . The sizes of SPN embeddings are to be found in Appendix D. The shortest and longest SPN embeddings on the X are ~21 and ~3530 respectively ( Table 3 , inner column ) , while on the Y we have ~25 and ~500 ( Table 4 , edges column ) . > I said this before in my comments , please do not refer to Q1 , Q2 , etc . these shortcuts let you make the paper more dense with fewer words but at the cost of readability . We will rewrite the experiment section to make it more clear with no shorthand notations for questions . > I would love to see a table that shows , for each condition : ( A ) a baseline X- > Y , ( B ) one * average * result across datasets for your method , and ( C ) one * average * result from a reasonable best competitor method . Please show for both the exact match and hamming losses , as that will demonstrate the gap between independent linear prediction and structured prediction . That would still be plenty of numbers but would make it much easier for the reader to verify your claims and you can put everything else in the Appendix . We can refactor the table as suggested , adding a little of redundancy to the columns . Concerning ( A ) , at the moment we can provide the logistic regression baseline ( LR , X- > Y ) since it actually is the most natural method to compare against , as stated above . Concerning ( B ) , the average rank we provided as the last column in Tables 1,7,8 is already an aggregate result that shows the best method across all datasets . Nevertheless , we can average the result across all datasets as suggested . This is how the result table will look like : http : //oi68.tinypic.com/2u786cp.jpg Alternatively , to stress the improvement over the LR baseline we can report the average relative improvement of each score as follows : avg_imp ( j , s ) = \\sum_ { d\\in datasets } ( s_ { j } ( d ) - ( s_ { LR } ( d ) ) ) * 100 / ( s_ { LR } ( d ) ) where s ( . ) is the score function , j a model and s_ { LR } ( d ) the score of the logistic regression baseline on the dataset d. The result table would look like this : http : //oi67.tinypic.com/ve4osl.jpg Concerning ( C ) , we could aggregate rows across same methods ( eg : RBM-500/1000/5000 ) by taking the max value , however losing the information that determining the embedding size does not come for free for all models but SPN/MPN . Here is how the table would look like : http : //oi65.tinypic.com/301mx5e.jpg What do you think ? After this discussion we will insert the best table layour in a new revision . [ 1 ] Read et al . ( 2011 ) `` Classifier chains for multi-label classification . '' Machine learning 85.3 [ 2 ] Gens et al . ( 2013 ) `` Learning the Structure of Sum-Product Networks '' ICML 2013"}], "0": {"review_id": "r1Bjj8qge-0", "review_text": "This paper tries to solve the problem of interpretable representations with focus on Sum Product Networks. The authors argue that SPNs are a powerful linear models that are able to learn parts and their combinations, however, their representations havent been fully exploited by generating embeddings. Pros: -The idea is interesting and interpretable models/representations is an important topic. -Generating embeddings to interpret SPNs is a novel idea. -The experiments are interesting but could be extended. Cons: -The author's contribution isn't fully clear and there are multiple claims that need support. For example, SPNs are indeed interpretable as is, since the bottom-up propagation of information from the visible inputs could be visualized at every stage, and the top-down parse could be also visualized as it has been done before (Amer & Todorovic, 2015). Another example, Proposition one claims that MPNs are perfect encoder decoders since the max nodes always have one max value, however, what if it was uniformally distributed node, or there are two equal values? Did the authors run into such cases? Did they address all edge cases? -A good comparison could have been against Generative Adversarial Networks (GANs), Generative Stochastic Networks (GSNs) and Variational Autoencoders too since they are the state-of-the-art generative models, rather than comparing with RBMs and Nade. I would suggest that the authors take sometime to evaluate their approach against the suggested methods, and make sure to clarify their contributions and eliminate over claiming statements. I agree with the other comments raised by Anon-Reviewer1.", "rating": "6: Marginally above acceptance threshold", "reply_text": "-- -- -- -- Dear reviewer , thank you for your comments and for showing interest in our approach . In the following we clarify our choices and claims . > SPNs are indeed interpretable as is , since the bottom-up propagation > of information from the visible inputs could be visualized at every > stage , and the top-down parse could be also visualized as it has > been done before ( Amer & Todorovic , 2015 ) Indeed , SPNs have a clear , full probabilistic semantics but up to now they have been employed as classical probabilistic models in which interpretability is tackled only by looking at the results of single probabilistic queries , be them full evidence or MPE inference . To substantiate how our aim is different , consider the the work in ( Amer & Todorovic , 2015 ) . By applying an approximate MPE inference routine they i ) infer the most probable activity from a video sequence and ii ) map that activity to a foreground region in the video . They are weakly/fully supervised to infer the values of the indicator variables they set in a fixed-a-priori grid to locate these foreground/background regions . Instead , we train our SPNs in an unsupervised fashion , extracting structured embeddings from inner nodes jointly and employing them for predictive tasks on unseen variables . Moreover , by effectively visualizing the features extracted by each node , we provide an evidence that each node acts a meaningful part-based filter . We already investigated visualizing how inference propagates in SPNs at different stages in our unpublished arxiv paper ( Vergari et al.2016 ) .If needed , we could add to this work more visualizations from it . > Proposition one claims that MPNs are perfect encoder decoders since the max nodes always have one max value , however , what if it was uniformally distributed node , or there are two equal values ? Please note that Proposition one claims that MPNs are perfect encoder-decoders * only if * there are no ties among child distributions . This is always true for deterministic ( selective ) MPNs ( Proposition 2 ) but may not be true in practice for non-deterministic MPNs learned from data . > Did the authors run into such cases ? Did they address all edge cases ? Indeed , we investigated this issue empirically for our networks in Section 5.1 when employing full embeddings . Please refer to tables 5 and 6 to see how close to perfect encoder-decoders our MPNs were on the X and Y . > -A good comparison could have been against Generative Adversarial Networks ( GANs ) , Generative Stochastic Networks ( GSNs ) and Variational Autoencoders too since they are the state-of-the-art generative models , rather than comparing with RBMs and Nade To support our claim that probabilistic representations from tractable models like SPNs/MPNs are indeed meaningful , we devised our experimentation to provide fair competitors . While GANs , VAEs and even GSNs are state-of-the art models * for generating * high quality samples , their likelihood is intractable ( even implicit for GANs ) and exact inference is unfeasible with them . As a consequence , computing probabilistic features for unsupervised learning comparable to those extracted from SPNs in Section 3 would be not immediate . On the other hand , MADEs admit tractable exact full evidence inference , and they provide a way to decode the learned representations . This makes them a very natural competitor for our SPNs/MPNs encoding and decoding schemes . RBMs , while having an intractable joint likelihood , admit tractable conditionals of the hidden variables given the evidence and such probabilistic features have proved to be very competitive in the literature . We will discuss these differences in more detail in a new revision ."}, "1": {"review_id": "r1Bjj8qge-1", "review_text": " The paper's aim is - as argued in the paper and the responses to other reviewers comments - that SPN and MPN can be interpreted as encoders and decoders of RL. Well - this is an interesting perspective and could be (potentially) worth a paper. However - the current draft is far from being convincing in that respect - and I am talking about the updated / improved version as of now. The paper does not require minor revisions to make this point apparent - but a significant and major rewrite which seems beyond what the authors have done so far. - the experiments are (as also pointed out by other reviewers) rather unstructured and difficult to see much of an insight. I should probably also list (as the other other reviewers) flaws and issues with the experiments - but given the detailed comments by the other reviewers there seems to be little additional value in doing so So in essence the paper simply does not deliver at this point on its promise (as far as I am concerned) and in that sense I suggest a very clear reject for this conference. As for the dataset employed: MNIST should be considered a toy-dataset for pretty much all purposes these days - and in that sense the dataset choice is not helping me (and many other people that you might want to convince) to safe this paper. ", "rating": "3: Clear rejection", "reply_text": "Dear reviewer , thanks for your comments up to now and for your appreciation of our idea to exploit SPNs/MPNs for RL . > the current draft is far from being convincing in that respect - and I am talking about the updated / improved version as of now . The paper does not require minor revisions to make this point apparent - but a significant and major rewrite which seems beyond what the authors have done so far . We acknowledge that the paper can be improved but we would appreciate somewhat more detailed comments . We are always eager to revise parts of the paper or add experiments . We believe that this is a key advantage of conferences such as ICLR and the open review process . A revision of this work could surely benefit from constructive feedback . > the experiments are ( as also pointed out by other reviewers ) rather unstructured and difficult to see much of an insight . I should probably also list ( as the other other reviewers ) flaws and issues with the experiments - but given the detailed comments by the other reviewers there seems to be little additional value in doing so Please note that there is only one other reviewer , whose score is `` Marginally above acceptance threshold '' . While we accept your opinion concerning our experiments , we indeed would appreciate an objective list of flaws in order to substantiate your rating . Please also note the proposed revision of our experiment section ( see reply to Reviewer 1 ) . > As for the dataset employed : MNIST should be considered a toy-dataset for pretty much all purposes these days - and in that sense the dataset choice is not helping me ( and many other people that you might want to convince ) to safe this paper . Please note that we employed MNIST only to show one aspect of the whole work : the visualizations of the part-based learned filters , for which an image dataset was needed . However , we conducted our experiments on 10 more standard benchmark datasets for Multilabel-classification , since our major aim is to test embeddings for discriminative structured output prediction . Please see our answer to your pre-review question on this matter . Again , we are open for suggestions concerning additional datasets to be included in our experiments . As a side note on learning the structure of an SPN on much larger ( image ) datasets , please consider that up to now , in the SPN literature , only ( image ) datasets on par with MNIST have been considered . Scaling structure learning would be worth a publication by itself . All in all , we believe it is time to start researching tractable generative models for RL , even if we have to cope with all the ( computational , theoretical , etc ) limitations for these architectures that have been overcome for other now-used-everyday deep learning models ."}, "2": {"review_id": "r1Bjj8qge-2", "review_text": "The authors propose and evaluate using SPN's to generate embeddings of input and output variables, and using MPN to decode output embeddings to output variables. The advantage of predicting label embeddings is to decouple dependencies in the predicted space. The authors show experimentally that using SPN based embeddings is better than those produced by RBM's. This paper is fairly dense and a bit hard to read. After the discussion, the main contributions of the authors are: 1. They propose the scheme of learning SPN's over Y and then using MPN's to decode the output, or just SPNs to embed X. 2. They propose how to decode MPN's with partial data. 3. They perform some analysis of when their scheme will lead to perfect encoding/decodings. 4. They run many, many experiments comparing various ways of using their proposed method to make predictions on multi-label classification datasets. My main concerns with this paper are as follows: - The point of this paper is about using generative models for representation learning. In their experiments, the main task is discriminative; e.g. predict multiple Y from X. The only discriminative baseline is a L2 regularized logistic regression, which does not have any structure on the output; it'd be nice to see how a discriminative structured prediction method would do, such as CRF or belief propagation. - The many experiments suggest that their encoder/decoder scheme is working better than the alternatives; can you please give more details on the relative computation complexity of each method? - One thing I'm still having trouble understanding is *why* this method works better than MADE and the other alternatives. Is it learning a better model of the distribution of Y? Is it better at separating out correlations in the output into individual nodes? Does it have larger representations? - I think the experiments are overkill and if anything, they way they are presented detract from the paper. There's already far too many numbers and graphs presented to be easy to understand. If I have to dig through hundreds of numbers to figure out if your claim is correct, the paper is not clear enough. And, I said this before in my comments, please do not refer to Q1, Q2, etc. -- these shortcuts let you make the paper more dense with fewer words but at the cost of readability. I *think* I convinced myself that your method works...I would love to see a table that shows, for each condition: (A) a baseline X->Y, (B) one *average* result across datasets for your method, and (C) one *average* result from a reasonable best competitor method. Please show for both the exact match and hamming losses, as that will demonstrate the gap between independent linear prediction and structured prediction. That would still be plenty of numbers but would make it much easier for the reader to verify your claims and you can put everything else in the Appendix. E.g. something like: Input | Predicted Output | Decoder | Hamming | Exact Match ---- X | P(Y) | CRF | xx.xx | xx.xx (this is your baseline) SPN E_X | P(Y) | n/a | xx.xx | xx.xx X | SPN E_Y | MPN | xx.xx | xx.xx (given X, predict E_Y, then decode it with an MPN) Does a presentation like that make sense? It's just really hard and time-consuming for me as a reviewer to verify your results, the way you've laid them out currently. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "-- -- -- -- -- -- -- -- -- -- -- -- -- Dear reviewer , thanks for your time and suggestions . We will try to answer your questions more in detail , hoping to improve the paper towards a full acceptance . > The point of this paper is about using generative models for representation learning . In their experiments , the main task is discriminative ; e.g.predict multiple Y from X . The only discriminative baseline is a L2 regularized logistic regression , which does not have any structure on the output ; it 'd be nice to see how a discriminative structured prediction method would do , such as CRF or belief propagation . It is true that the embeddings from our models ( SPNs/MPNs , RBMs and MADEs ) come from generative models , in an unsupervised way . However , once these embeddings are extracted for the X/Y , a supervised linear model is trained on them ( be it either a logistic or a ridge regressor ) . This is the main reason why we compare all our results against the 'plain ' L2 logistic regressor baseline : to state how useful are the new embeddings compared to using none ( the original X and Y ) , i.e.how decoupled and now linearly separable are the new representations learned in the previous unsupervised stage . If needed , we can look into 'fully'-supervised non-linear discriminate models to provide additional comparisons . One way to cope with the structure in the Y would be to look at Classifier Chains [ 1 ] , we will also investigate CRFs as you suggest . All in all , if they directly optimize a loss to predict the Y , we expect them to perform better . > The many experiments suggest that their encoder/decoder scheme is working better than the alternatives ; can you please give more details on the relative computation complexity of each method ? To extract an embedding from an SPN/MPN we simply evaluate them bottom-up , thus the complexity is linear in the size of the network , as it is for the other neural models . Therefore , the main computational cost is learning the structure of the network with LearnSPN-b ( see Appendix C ) . In that iterative scheme , the cost of clustering features ( insert a product node ) is quadratic in the number of the features , while clustering instances ( insert a sum node ) can be quadratic in the number of instances [ 2 ] . However , no more learning is required for SPNs/MPNs : since both structure and weights are already learned . We argue that such a learning step is 'cheaper ' than performing intensive tuning for all the hyperparameters needed for specifying the structure of a neural model and learning its weight . E.g number of layers , hidden units , activation type , learning rate scheduling algorithm and coefficient , skipping connections and so on for MADEs , see the Appendix E.1.3 . > - One thing I 'm still having trouble understanding is * why * this method works better than MADE and the other alternatives . Is it learning a better model of the distribution of Y ? If we take the log-likelihood as the criterion to measure how good a distribution is fit , the answer is no . For instance MADE models have better log-likelihoods than SPNs in our experiments , since they directly optimize this loss while training , however their embeddings are less discriminative features . We can report in additional tables the log-likelihoods achieved on the test sets by MADEs and SPNs . From this perspective RBMs are not directly comparable since their log-likelihood is intractable and we optimize the pseudo-log-likelihood instead . MANIAC , on the other hand , is not probabilitstic at all . > Is it better at separating out correlations in the output into individual nodes ? We say yes , as we argue in the Introduction and Section 3 and 4 . Our point is that SPN embeddings are inherently ( probabilistic ) * part-based * features that are extracted while performing that sort of hierachical co-clustering by Learn-SPN . From this point of view , we expect them to better capture local variations . > Does it have larger representations ? We do not believe it to be the cause , either . To demonstrate this , we accurately selected our model capacities in the experiments . Since LearnSPN-b adaptively grows a network , we first measured the embedding size from the learned SPNs and therefore trained RBMs/MADEs to have similar or bigger size . Hence the need for models with different hidden unit sizes . The sizes of SPN embeddings are to be found in Appendix D. The shortest and longest SPN embeddings on the X are ~21 and ~3530 respectively ( Table 3 , inner column ) , while on the Y we have ~25 and ~500 ( Table 4 , edges column ) . > I said this before in my comments , please do not refer to Q1 , Q2 , etc . these shortcuts let you make the paper more dense with fewer words but at the cost of readability . We will rewrite the experiment section to make it more clear with no shorthand notations for questions . > I would love to see a table that shows , for each condition : ( A ) a baseline X- > Y , ( B ) one * average * result across datasets for your method , and ( C ) one * average * result from a reasonable best competitor method . Please show for both the exact match and hamming losses , as that will demonstrate the gap between independent linear prediction and structured prediction . That would still be plenty of numbers but would make it much easier for the reader to verify your claims and you can put everything else in the Appendix . We can refactor the table as suggested , adding a little of redundancy to the columns . Concerning ( A ) , at the moment we can provide the logistic regression baseline ( LR , X- > Y ) since it actually is the most natural method to compare against , as stated above . Concerning ( B ) , the average rank we provided as the last column in Tables 1,7,8 is already an aggregate result that shows the best method across all datasets . Nevertheless , we can average the result across all datasets as suggested . This is how the result table will look like : http : //oi68.tinypic.com/2u786cp.jpg Alternatively , to stress the improvement over the LR baseline we can report the average relative improvement of each score as follows : avg_imp ( j , s ) = \\sum_ { d\\in datasets } ( s_ { j } ( d ) - ( s_ { LR } ( d ) ) ) * 100 / ( s_ { LR } ( d ) ) where s ( . ) is the score function , j a model and s_ { LR } ( d ) the score of the logistic regression baseline on the dataset d. The result table would look like this : http : //oi67.tinypic.com/ve4osl.jpg Concerning ( C ) , we could aggregate rows across same methods ( eg : RBM-500/1000/5000 ) by taking the max value , however losing the information that determining the embedding size does not come for free for all models but SPN/MPN . Here is how the table would look like : http : //oi65.tinypic.com/301mx5e.jpg What do you think ? After this discussion we will insert the best table layour in a new revision . [ 1 ] Read et al . ( 2011 ) `` Classifier chains for multi-label classification . '' Machine learning 85.3 [ 2 ] Gens et al . ( 2013 ) `` Learning the Structure of Sum-Product Networks '' ICML 2013"}}