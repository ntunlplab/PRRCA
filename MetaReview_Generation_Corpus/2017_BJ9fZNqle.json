{"year": "2017", "forum": "BJ9fZNqle", "title": "Multi-modal Variational Encoder-Decoders", "decision": "Reject", "meta_review": "This paper explores a variational autoencoder variant.\n \n ICLR gives authors some respect that other conferences don't. It is flexible about the length of the paper, and allows revisions to be submitted. The understanding should be that authors should in turn treat reviewers with respect. The paper should still be finished. Reviewers can't be expected to read a churn of large revisions. The final paper should be roughly the right length, unless with very good reason.\n \n This paper was clearly not finished, and now is too long, with issues remaining. I hope that it will be submitted again, but not until it is actually ready.", "reviews": [{"review_id": "BJ9fZNqle-0", "review_text": "UPDATE: I have read the authors' rebuttal and also the other comments in this paper's thread. My thoughts have not changed. The authors propose using a mixture prior rather than a uni-modal prior for variational auto-encoders. They argue that the simple uni-modal prior \"hinders the overall expressivity of the learned model as it cannot possibly capture more complex aspects of the data distribution.\" I find the motivation of the paper suspicious because while the prior may be uni-modal, the posterior distribution is certainly not. Furthermore, a uni-modal distribution on the latent variable space can certainly still lead to the capturing of complex, multi-modal data distributions. (As the most trivial case, take the latent variable space to be a uniform distribution; take the likelihood to be a point mass given by applying the true data distribution's inverse CDF to the uniform. Such a model can capture any distribution.) In addition, multi-modality is arguably an overfocused concept in the literature, where the (latent variable) space is hardly anymore worth capturing from a mixture of simple distributions when it is often a complex nonlinear space. It is unclear from the experiments how much the influence of the prior's multimodality influences the posterior to capture more complex phenomena, and whether this is any better than considering a more complex (but still reparameterizable) distribution on the latent space. I recommend that this paper be rejected, and encourage the authors to more extensively study the effect of different priors. I'd also like to make two additional comments: While there is no length restriction at ICLR, the 14 page document can be significantly condensed without loss of describing their innovation or clarity. I recommend the authors do so. Finally, I think it's important to note the controversy in this paper. It was submitted with many significant incomplete details (e.g., no experiments, many missing citations, a figure placed inside that was pencilled in by hand, and several missing paragraphs). These details were not completed until roughly a week(?) later. I recommend the chairs discuss this in light of what should be allowed next year.", "rating": "3: Clear rejection", "reply_text": "Thank you for your review > I find the motivation of the paper suspicious ... Indeed , it may appear that many researchers now focus on multi-modality , but we believe it is an important area of research . In fact , our quantitative results on the document modeling tasks strongly support this claim , where the multi-modal latent variable models significantly outperform the uni-modal Gaussian latent variable models across three different tasks . In fact , multi-modality is one of the major motivations for incorporating discrete latent variables into VAEs , which is what at least 3 other ICLR submissions focus on : \u201c Discrete Variational Autoencoders \u201d by Rolfe The Concrete Distribution : A Continuous Relaxation of Discrete Random Variables \u201d by Maddison et al. \u201c Categorical Reparameterization with Gumbel-Softmax \u201d by Jang et al.Of course , our approach differs from these , because our goal is to learn continuous latent variables with multi-modal probability density functions . For a lot of NLP applications , there are plenty of intuitive arguments for why one would like multi-modality in the latent variable space . Suppose we have a generative model over sports articles . Some of the generated articles might be related to football and others to hockey . But clearly there exists no sport that lies in between football and hockey ( at least not yet ! ) , so if the latent variable contains different regions for football and hockey articles , then there must be a region of low probability mass between them . By definition , such a probability density is multi-modal and can not be represented by a uni-modal Gaussian variable . In theory , it could be captured in the decoder module which takes as input a uni-modal latent variable sample , but in practice this is going to be very difficult to learn as our experiments have shown . > the 14 page document can be significantly condensed without loss We agree , and will shorten the paper for the camera-ready version ( see our response to Reviewer # 3 above ) . > It was submitted with many significant incomplete details We sincerely apologize for the incomplete submission . We did not plan for this , but found bugs in our experiments and had to re-launch several of them just before the deadline , which is why it took a whole week extra before we had the final results . -Alex And Iulian"}, {"review_id": "BJ9fZNqle-1", "review_text": "The authors introduce some new prior and approximate posterior families for variational autoencoders, which are compatible with the reparameterization trick, as well as being capable of expressing multiple modes. They also introduce a gating mechanism between prior and posterior. They show improvements on bag of words document modeling, and dialogue response generation. The original abstract is overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz with a DNN response model p(x|z) (\"it cannot possibly capture more complex aspects of the data distribution\", \"critical restriction\", etc). While the assertion that a unimodal latent prior is necessary to model multimodal observations is false, there are sensible motivations for the piecewise constant prior and posterior. For example, if we think of a VAE as a sort of regularized autoencoder where codes are constrained to \"fill up\" parts of the prior latent space, then there is a sphere-packing argument to be made that filling a Gaussian prior with Gaussian posteriors is a bad use of code space. Although the authors don't explore this much, a hypercube-based tiling of latent code space is a sensible idea. As stated, I found the message of the paper to be quite sloppy with respect to the concept of \"multi-modality.\" There are 3 types of multimodality at play here: multimodality in the observed marginal distribution p(x), which can be captured by any deep latent Gaussian model, multimodality in the prior p(z), which makes sense in some situations (e.g. a model of MNIST digits could have 10 prior modes corresponding to latent codes for each digit class), and multimodality in the posterior z for a given observation x_i, q(z_i|x_i). The final type of multimodality is harder to argue for, except in so far as it allows the expression of flexibly shaped distributions without highly separated modes. I believe flexible posterior approximations are important to enable fine-grained and efficient tiling of latent space, but I don't think these need to have multiple strong modes. I would be interested to see experiments demonstrating otherwise for real world data. I think this paper should be more clear about the different types of multi-modality and which parts of their analysis demonstrate which ones. I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words, but rather just a separation between the Gaussian and the piecewise variables. As I mention in my earlier questions, I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood when the powerful networks transforming to and from latent space should make it scale-invariant. Explicitly separating out the contributions of a reimplemented base model, prior-posterior interpolation and the learned prior parameters would strengthen these experiments. Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one. The fact that adding more constant components helps for document modeling is interesting, and it would be nice to see more qualitative analysis of what the prior modes represent. I also would be surprised if posterior modes were highly separated, and if they were it would be interesting to explore if they corresponded to e.g. ambiguous word-senses. The experiments on dialog modeling are mostly negative results, quantitatively. The observation that the the piecewise constant variables encode time-related words and the Gaussian variables encode sentiment is interesting, especially since it occurs in both sets of experiments. This is actually quite interesting, and I would be interested in seeing analysis of why this is the case. As above, I would like to see an analysis of the sorts of words that are encoded in the different prior modes and whether they correspond to e.g. groups of similar holidays or days. In conclusion, I think the piecewise constant variational family is a good idea, although it is not well-motivated by the paper. The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM. The fact that H-NVDM performs better is interesting, though. This paper should better motivate the need for different types of multi-modality, and demonstrate that those sorts of things are actually being captured by the model. As it is, the paper introduces an interesting variational family and shows that it performs better for some tasks, but the motivation and analysis is not clearly focused. To demonstrate that this is a broadly applicable family, it would also be good to do experiments on a more standard datasets like MNIST. Even without an absolute log-likelihood improvement, if the method yielded interpretable multiple modes this would be a valuable contribution.", "rating": "4: Ok but not good enough - rejection", "reply_text": "> `` The original abstract is overly strong in its assertion that a unimodal latent prior p ( z ) can not fit a multimodal marginal int_z p ( x|z ) p ( x ) dz '' We did not argue for multi-modality of the output space . Naturally , with a powerful decoder , the marginal p ( x ) and the conditional p ( x|z ) could potentially be multi-modal . However , using a Gaussian prior , by definition the latent variable prior p ( z ) is uni-modal . This could in principle hurt the representations learned . If the latent factors are truly multi-modal , trying to represent them or compress them down to a uni-modal space will act as a strong regularization and make the training process more difficult . That 's why our goal is to learn a multi-modal prior p ( z ) . We will clarify this distinction in the final version of the paper . > `` I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood '' There are other differences between NVDM and G-NVDM beyond learning the prior parameters . One important difference is the gating mechanism which allows the model to interpolate between the posterior and prior in order to calculate the final generated posterior parameters ( beyond minor details such as different activation functions , technical modifications , etc.as mentioned in the paper ) . This is simply something that we found helped improve performance in preliminary experiments ( as well as in some experiments in the past ) . We felt that a fairer comparison would be to first improve the baseline model ( i.e. , the NVDM ) and then compare our proposed hybrid models against the improved baseline ( as opposed to only/exclusively reporting a previously published as is commonly done ) . This was especially important given that we intended to jointly learn the priors and use the gating mechanism in the proposed models as well . The mission of the paper was to show that the proposed prior improved the encoder-decoder models in the challenging text problems we chose to explore ( or at least uncovered interesting information using the piecewise variables ) . Many of the models trained in the paper ( especially the dialogue models ) are fairly expensive to train , and we further felt that reporting various degradations of the models would clutter the paper and detract from focusing on the piecewise variables themselves . However , we do agree that an ablation test would be appropriate and will add an appendix in the final version exploring the effects of each modification . > `` ... a hypercube-based tiling of latent code space is a sensible idea . '' We appreciate this interpretation , and will give it more thought . > `` I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words '' This is exactly what our analysis using gradients was aimed at . We will try to visualize the impact of the latent variable components in another way . > `` The experiments on dialog modeling are mostly negative results '' That is not accurate . The standard G-VHRED model is neither better than nor worse than the H-VHRED . Human subjects simply can not tell them apart . -Alex And Iulian"}, {"review_id": "BJ9fZNqle-2", "review_text": "This paper proposes a piecewise constant parameterisation for neural variational models so that it could explore the multi-modality of the latent variables and develop more powerful neural models. The experiments of neural variational document models and variational hierarchical recurrent encoder-decoder models show that the introduction of the piecewise constant distribution helps achieve better perplexity on modelling documents and seemly better performance on modelling dialogues. The idea of having a piecewise constant prior for latent variables is interesting, but the paper is not well-written (even 14 pages long) and the design of the experiments fails to demonstrate the most of the claims. The detailed comments are as follows: --The author explains the limitations of the VAEs with standard Gaussian prior in the last paragraph of 3.1 and the last paragraph of 5.1. Hence, a multimodal prior would help the VAEs overcome the issues of optimisation. However, there is a lack of evidence showing the multimodality of the prior helps break the bottleneck. --In the last paragraph of 6.1, the author claimed the decoder parameter matrix is directly affected by the latent variables. But what the connects the decoder is a combination of a piecewise constant and Gaussian latent variables. No matter what is discovered in the experiments, it only shows z=<z_gaussian, z_piecewise> is multimodal. However, z=<z_gaussian1, z_gaussian2> can be multimodal as well. None of the claims in this paragraph stands. --In the quantitative evaluation of NVDM, there is an incremental model from z=z_gaussian to z=<z_gaussian, z_piecewise>. As the prior is learned together with the variational posterior, a more flexible prior would alleviate the regularisation imposed by the KL term. Certainly, more parameters are applied as well, so a fair comparison would at least be z=<z_gaussian, z_piecewise> and z=<z_gaussian1, z_gaussian2> which equals to a double sized z_gaussian. --The results shown in Table 3 are implausible. I cannot believe the author used gradients to evaluate the model. --Eq. 5 is confusing, adding a multiplication sign might help. --3.1 can be deleted because people attending ICLR are familiar with VAEs. Typos: as well as the well as the generated prior-> as well as the generated prior", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your detailed review . > paper is not well-written ( even 14 pages long ) We agree that the paper is too long . We aim to shorten it by moving the mathematical derivations and the equations for the NVDM and VHRED models to the appendix . Hopefully , this will also make the ideas more clear . > the experiments fails to demonstrate the most of the claims \u2026 We disagree with this conclusion . We have carried out experiments on four different tasks ( 20-Newsgroup , RCV1 , CADE , and Twitter dialogues ) , while comparing to the most competitive baseline model ( a similar model with multivariate Gaussian latent variables ) . On all document modeling tasks , we demonstrate significant performance improvements w.r.t.perplexity . On the document modeling tasks we further demonstrate the utility of the piecewise constant variables through word query similarity and gradient analysis . On the dialogue task , we demonstrate their utility through gradient analysis and qualitatively using examples . We believe our experimental evaluations are at least as well-designed and conclusive -- - if not more conclusive -- - compared to several other ICLR submissions . OUR EXPERIMENTS ARE ON COMPLEX , REAL-WORLD TEXT DATASETS , which contain plenty of MULTI-MODAL STRUCTURE . Most of these experiments took several days ( sometimes weeks ) to execute on machines with TitanX GPUs . Please compare the impact and computational complexity of these experiments to other ICLR submissions such as : \u201c The Concrete Distribution : A Continuous Relaxation of Discrete Random Variables \u201d and \u201c Categorical Reparameterization with Gumbel-Softmax \u201d . These submissions focus their experiments on MNIST and OMNIGLOT , which their reviewers seem to be satisfied with . > However , z= < z_gaussian1 , z_gaussian2 > can be multimodal as well ... All Gaussian distributions are uni-modal by definition , so concatenating two samples from multivariate Gaussian distributions is still uni-modal . Therefore , z= < z_gaussian1 , z_gaussian2 > can not be multi-modal . > a fair comparison would at least be z= < z_gaussian , z_piecewise > and z= < z_gaussian1 , z_gaussian2 > which equals to a double sized z_gaussian . We did take this into account during hyper-parameter search . For the G-NVDM model , we experimented with up to 200 latent Gaussians variables , but found that this performed worse due to overfitting . > The results shown in Table 3 are implausible\u2026 Researchers routinely analyse hidden unit activations in neural network models . For example , see Miao et al ( 2015 ) and Karpathy et al . ( 2015 ) .In our case , we were interested in seeing how much changing a word would affect the latent variable models , which is why we compute the gradient w.r.t.the word embedding . Of course , this does not yield quantitative evidence showing how well the model performs on different tasks , but it does illustrate what words the latent variables are sensitive to which , in turn , shows what they have learned to encode . References Miao et al , Neural Variational Inference for Text Processing . 2015.Karpathy et al , Visualizing and Understanding Recurrent Networks , 2015 . - Alex And Iulian"}], "0": {"review_id": "BJ9fZNqle-0", "review_text": "UPDATE: I have read the authors' rebuttal and also the other comments in this paper's thread. My thoughts have not changed. The authors propose using a mixture prior rather than a uni-modal prior for variational auto-encoders. They argue that the simple uni-modal prior \"hinders the overall expressivity of the learned model as it cannot possibly capture more complex aspects of the data distribution.\" I find the motivation of the paper suspicious because while the prior may be uni-modal, the posterior distribution is certainly not. Furthermore, a uni-modal distribution on the latent variable space can certainly still lead to the capturing of complex, multi-modal data distributions. (As the most trivial case, take the latent variable space to be a uniform distribution; take the likelihood to be a point mass given by applying the true data distribution's inverse CDF to the uniform. Such a model can capture any distribution.) In addition, multi-modality is arguably an overfocused concept in the literature, where the (latent variable) space is hardly anymore worth capturing from a mixture of simple distributions when it is often a complex nonlinear space. It is unclear from the experiments how much the influence of the prior's multimodality influences the posterior to capture more complex phenomena, and whether this is any better than considering a more complex (but still reparameterizable) distribution on the latent space. I recommend that this paper be rejected, and encourage the authors to more extensively study the effect of different priors. I'd also like to make two additional comments: While there is no length restriction at ICLR, the 14 page document can be significantly condensed without loss of describing their innovation or clarity. I recommend the authors do so. Finally, I think it's important to note the controversy in this paper. It was submitted with many significant incomplete details (e.g., no experiments, many missing citations, a figure placed inside that was pencilled in by hand, and several missing paragraphs). These details were not completed until roughly a week(?) later. I recommend the chairs discuss this in light of what should be allowed next year.", "rating": "3: Clear rejection", "reply_text": "Thank you for your review > I find the motivation of the paper suspicious ... Indeed , it may appear that many researchers now focus on multi-modality , but we believe it is an important area of research . In fact , our quantitative results on the document modeling tasks strongly support this claim , where the multi-modal latent variable models significantly outperform the uni-modal Gaussian latent variable models across three different tasks . In fact , multi-modality is one of the major motivations for incorporating discrete latent variables into VAEs , which is what at least 3 other ICLR submissions focus on : \u201c Discrete Variational Autoencoders \u201d by Rolfe The Concrete Distribution : A Continuous Relaxation of Discrete Random Variables \u201d by Maddison et al. \u201c Categorical Reparameterization with Gumbel-Softmax \u201d by Jang et al.Of course , our approach differs from these , because our goal is to learn continuous latent variables with multi-modal probability density functions . For a lot of NLP applications , there are plenty of intuitive arguments for why one would like multi-modality in the latent variable space . Suppose we have a generative model over sports articles . Some of the generated articles might be related to football and others to hockey . But clearly there exists no sport that lies in between football and hockey ( at least not yet ! ) , so if the latent variable contains different regions for football and hockey articles , then there must be a region of low probability mass between them . By definition , such a probability density is multi-modal and can not be represented by a uni-modal Gaussian variable . In theory , it could be captured in the decoder module which takes as input a uni-modal latent variable sample , but in practice this is going to be very difficult to learn as our experiments have shown . > the 14 page document can be significantly condensed without loss We agree , and will shorten the paper for the camera-ready version ( see our response to Reviewer # 3 above ) . > It was submitted with many significant incomplete details We sincerely apologize for the incomplete submission . We did not plan for this , but found bugs in our experiments and had to re-launch several of them just before the deadline , which is why it took a whole week extra before we had the final results . -Alex And Iulian"}, "1": {"review_id": "BJ9fZNqle-1", "review_text": "The authors introduce some new prior and approximate posterior families for variational autoencoders, which are compatible with the reparameterization trick, as well as being capable of expressing multiple modes. They also introduce a gating mechanism between prior and posterior. They show improvements on bag of words document modeling, and dialogue response generation. The original abstract is overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz with a DNN response model p(x|z) (\"it cannot possibly capture more complex aspects of the data distribution\", \"critical restriction\", etc). While the assertion that a unimodal latent prior is necessary to model multimodal observations is false, there are sensible motivations for the piecewise constant prior and posterior. For example, if we think of a VAE as a sort of regularized autoencoder where codes are constrained to \"fill up\" parts of the prior latent space, then there is a sphere-packing argument to be made that filling a Gaussian prior with Gaussian posteriors is a bad use of code space. Although the authors don't explore this much, a hypercube-based tiling of latent code space is a sensible idea. As stated, I found the message of the paper to be quite sloppy with respect to the concept of \"multi-modality.\" There are 3 types of multimodality at play here: multimodality in the observed marginal distribution p(x), which can be captured by any deep latent Gaussian model, multimodality in the prior p(z), which makes sense in some situations (e.g. a model of MNIST digits could have 10 prior modes corresponding to latent codes for each digit class), and multimodality in the posterior z for a given observation x_i, q(z_i|x_i). The final type of multimodality is harder to argue for, except in so far as it allows the expression of flexibly shaped distributions without highly separated modes. I believe flexible posterior approximations are important to enable fine-grained and efficient tiling of latent space, but I don't think these need to have multiple strong modes. I would be interested to see experiments demonstrating otherwise for real world data. I think this paper should be more clear about the different types of multi-modality and which parts of their analysis demonstrate which ones. I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words, but rather just a separation between the Gaussian and the piecewise variables. As I mention in my earlier questions, I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood when the powerful networks transforming to and from latent space should make it scale-invariant. Explicitly separating out the contributions of a reimplemented base model, prior-posterior interpolation and the learned prior parameters would strengthen these experiments. Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one. The fact that adding more constant components helps for document modeling is interesting, and it would be nice to see more qualitative analysis of what the prior modes represent. I also would be surprised if posterior modes were highly separated, and if they were it would be interesting to explore if they corresponded to e.g. ambiguous word-senses. The experiments on dialog modeling are mostly negative results, quantitatively. The observation that the the piecewise constant variables encode time-related words and the Gaussian variables encode sentiment is interesting, especially since it occurs in both sets of experiments. This is actually quite interesting, and I would be interested in seeing analysis of why this is the case. As above, I would like to see an analysis of the sorts of words that are encoded in the different prior modes and whether they correspond to e.g. groups of similar holidays or days. In conclusion, I think the piecewise constant variational family is a good idea, although it is not well-motivated by the paper. The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM. The fact that H-NVDM performs better is interesting, though. This paper should better motivate the need for different types of multi-modality, and demonstrate that those sorts of things are actually being captured by the model. As it is, the paper introduces an interesting variational family and shows that it performs better for some tasks, but the motivation and analysis is not clearly focused. To demonstrate that this is a broadly applicable family, it would also be good to do experiments on a more standard datasets like MNIST. Even without an absolute log-likelihood improvement, if the method yielded interpretable multiple modes this would be a valuable contribution.", "rating": "4: Ok but not good enough - rejection", "reply_text": "> `` The original abstract is overly strong in its assertion that a unimodal latent prior p ( z ) can not fit a multimodal marginal int_z p ( x|z ) p ( x ) dz '' We did not argue for multi-modality of the output space . Naturally , with a powerful decoder , the marginal p ( x ) and the conditional p ( x|z ) could potentially be multi-modal . However , using a Gaussian prior , by definition the latent variable prior p ( z ) is uni-modal . This could in principle hurt the representations learned . If the latent factors are truly multi-modal , trying to represent them or compress them down to a uni-modal space will act as a strong regularization and make the training process more difficult . That 's why our goal is to learn a multi-modal prior p ( z ) . We will clarify this distinction in the final version of the paper . > `` I found it surprising that the learned variance and mean for the Gaussian prior helps so dramatically with G-NVDM likelihood '' There are other differences between NVDM and G-NVDM beyond learning the prior parameters . One important difference is the gating mechanism which allows the model to interpolate between the posterior and prior in order to calculate the final generated posterior parameters ( beyond minor details such as different activation functions , technical modifications , etc.as mentioned in the paper ) . This is simply something that we found helped improve performance in preliminary experiments ( as well as in some experiments in the past ) . We felt that a fairer comparison would be to first improve the baseline model ( i.e. , the NVDM ) and then compare our proposed hybrid models against the improved baseline ( as opposed to only/exclusively reporting a previously published as is commonly done ) . This was especially important given that we intended to jointly learn the priors and use the gating mechanism in the proposed models as well . The mission of the paper was to show that the proposed prior improved the encoder-decoder models in the challenging text problems we chose to explore ( or at least uncovered interesting information using the piecewise variables ) . Many of the models trained in the paper ( especially the dialogue models ) are fairly expensive to train , and we further felt that reporting various degradations of the models would clutter the paper and detract from focusing on the piecewise variables themselves . However , we do agree that an ablation test would be appropriate and will add an appendix in the final version exploring the effects of each modification . > `` ... a hypercube-based tiling of latent code space is a sensible idea . '' We appreciate this interpretation , and will give it more thought . > `` I also found it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior corresponding to different words '' This is exactly what our analysis using gradients was aimed at . We will try to visualize the impact of the latent variable components in another way . > `` The experiments on dialog modeling are mostly negative results '' That is not accurate . The standard G-VHRED model is neither better than nor worse than the H-VHRED . Human subjects simply can not tell them apart . -Alex And Iulian"}, "2": {"review_id": "BJ9fZNqle-2", "review_text": "This paper proposes a piecewise constant parameterisation for neural variational models so that it could explore the multi-modality of the latent variables and develop more powerful neural models. The experiments of neural variational document models and variational hierarchical recurrent encoder-decoder models show that the introduction of the piecewise constant distribution helps achieve better perplexity on modelling documents and seemly better performance on modelling dialogues. The idea of having a piecewise constant prior for latent variables is interesting, but the paper is not well-written (even 14 pages long) and the design of the experiments fails to demonstrate the most of the claims. The detailed comments are as follows: --The author explains the limitations of the VAEs with standard Gaussian prior in the last paragraph of 3.1 and the last paragraph of 5.1. Hence, a multimodal prior would help the VAEs overcome the issues of optimisation. However, there is a lack of evidence showing the multimodality of the prior helps break the bottleneck. --In the last paragraph of 6.1, the author claimed the decoder parameter matrix is directly affected by the latent variables. But what the connects the decoder is a combination of a piecewise constant and Gaussian latent variables. No matter what is discovered in the experiments, it only shows z=<z_gaussian, z_piecewise> is multimodal. However, z=<z_gaussian1, z_gaussian2> can be multimodal as well. None of the claims in this paragraph stands. --In the quantitative evaluation of NVDM, there is an incremental model from z=z_gaussian to z=<z_gaussian, z_piecewise>. As the prior is learned together with the variational posterior, a more flexible prior would alleviate the regularisation imposed by the KL term. Certainly, more parameters are applied as well, so a fair comparison would at least be z=<z_gaussian, z_piecewise> and z=<z_gaussian1, z_gaussian2> which equals to a double sized z_gaussian. --The results shown in Table 3 are implausible. I cannot believe the author used gradients to evaluate the model. --Eq. 5 is confusing, adding a multiplication sign might help. --3.1 can be deleted because people attending ICLR are familiar with VAEs. Typos: as well as the well as the generated prior-> as well as the generated prior", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your detailed review . > paper is not well-written ( even 14 pages long ) We agree that the paper is too long . We aim to shorten it by moving the mathematical derivations and the equations for the NVDM and VHRED models to the appendix . Hopefully , this will also make the ideas more clear . > the experiments fails to demonstrate the most of the claims \u2026 We disagree with this conclusion . We have carried out experiments on four different tasks ( 20-Newsgroup , RCV1 , CADE , and Twitter dialogues ) , while comparing to the most competitive baseline model ( a similar model with multivariate Gaussian latent variables ) . On all document modeling tasks , we demonstrate significant performance improvements w.r.t.perplexity . On the document modeling tasks we further demonstrate the utility of the piecewise constant variables through word query similarity and gradient analysis . On the dialogue task , we demonstrate their utility through gradient analysis and qualitatively using examples . We believe our experimental evaluations are at least as well-designed and conclusive -- - if not more conclusive -- - compared to several other ICLR submissions . OUR EXPERIMENTS ARE ON COMPLEX , REAL-WORLD TEXT DATASETS , which contain plenty of MULTI-MODAL STRUCTURE . Most of these experiments took several days ( sometimes weeks ) to execute on machines with TitanX GPUs . Please compare the impact and computational complexity of these experiments to other ICLR submissions such as : \u201c The Concrete Distribution : A Continuous Relaxation of Discrete Random Variables \u201d and \u201c Categorical Reparameterization with Gumbel-Softmax \u201d . These submissions focus their experiments on MNIST and OMNIGLOT , which their reviewers seem to be satisfied with . > However , z= < z_gaussian1 , z_gaussian2 > can be multimodal as well ... All Gaussian distributions are uni-modal by definition , so concatenating two samples from multivariate Gaussian distributions is still uni-modal . Therefore , z= < z_gaussian1 , z_gaussian2 > can not be multi-modal . > a fair comparison would at least be z= < z_gaussian , z_piecewise > and z= < z_gaussian1 , z_gaussian2 > which equals to a double sized z_gaussian . We did take this into account during hyper-parameter search . For the G-NVDM model , we experimented with up to 200 latent Gaussians variables , but found that this performed worse due to overfitting . > The results shown in Table 3 are implausible\u2026 Researchers routinely analyse hidden unit activations in neural network models . For example , see Miao et al ( 2015 ) and Karpathy et al . ( 2015 ) .In our case , we were interested in seeing how much changing a word would affect the latent variable models , which is why we compute the gradient w.r.t.the word embedding . Of course , this does not yield quantitative evidence showing how well the model performs on different tasks , but it does illustrate what words the latent variables are sensitive to which , in turn , shows what they have learned to encode . References Miao et al , Neural Variational Inference for Text Processing . 2015.Karpathy et al , Visualizing and Understanding Recurrent Networks , 2015 . - Alex And Iulian"}}