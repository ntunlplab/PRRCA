{"year": "2019", "forum": "SJVmjjR9FX", "title": "Variational Bayesian Phylogenetic Inference", "decision": "Accept (Poster)", "meta_review": "The reviewers lean to accept, and the authors clearly put a significant amount of time into their response. I will also lean to accept. However, the comments of reviewer 2 should be taken seriously, and addressed if possible, including an attempt to cut the paper length down.", "reviews": [{"review_id": "SJVmjjR9FX-0", "review_text": "This paper explores an approximate inference solution to the challenging problem of Bayesian inference of phylogenetic trees. Its leverages recently proposed subsplit Bayesian networks (SBNs) as a variational approximation over tree space and combines this with modern gradient estimators for VI. It is thorough in its evaluation of both methodological considerations and different datasets. The main advantage would seem to be a large speedup over MCMC-based methods (Figure 4), which could be of significant value to the phylogenetics community. This point would benefit from more discussion. How do the number of iterations (reported in Figures 3&4, which was done carefully) correspond to wallclock time? Can this new method scale to numbers of sites and sequences that were previously unfeasible? The main technical contribution is the use of SBNs as variational approximations over tree-space, but it is difficult to follow their implementation and parameter sharing without the explanation of the original paper. Additionally, the issue of estimating the support of the subsplit CPTs needs more discussion. As the authors acknowledge, complete parameterizations of these models scale in a combinatorial way with \u201call possible parent-child subsplit pairs\u201d, and they deal with this by shrinking the support up front with various heuristics. It seems that these support estimation approaches would be feasible when the data are strong but would become challenging to scale when the data are weak. Since VB is often concerned with the limited-data regime, more discussion of when support estimation is feasible and when it is difficult would clarify how widely applicable the method is. Overall, this work is an interesting extension of variational Bayes to a tree-structured inference problem and is thorough in its evaluation. While it is a bit focused on classical inference for ICLR, it could be interesting both for the VI community and as a significant application advancement. Other notes: In table 1, is the point that all methods are basically the same with different variance? This is not clear from the text. What about the variational bounds? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your thoughtful review and valuable feedback . Below are the answers to your comments : 1 ) `` The main advantage would seem to be a large speedup over MCMC-based methods ( Figure 4 ) , which could be of significant value to the phylogenetics community . This point would benefit from more discussion . How do the number of iterations ( reported in Figures 3 & 4 , which was done carefully ) correspond to wallclock time ? Can this new method scale to numbers of sites and sequences that were previously unfeasible ? '' We are glad that this reviewer appreciates the care with which we crafted the comparison in terms of number of likelihood evaluations . Our motivation in doing a comparison in terms of likelihood evaluations is because our current implementation is in Python , whereas while MrBayes is in C that has been optimized for many years . This is the first paper introducing the ideas and initial implementation of variational Bayes phylogenetic inference , and we think that this level of comparison is appropriate . We will soon begin developing a highly optimized implementation , for which we are planning a more applications-driven paper which will include a wallclock comparison . Regarding large data sets , given that the learned SBNs can provide guided exploration in tree space and variational approaches naturally incorporate stochastic gradients , we believe it is much easier for VBPI to scale to datasets with large numbers of sequences and sites . However , we have not tried out our initial Python implementation on especially big data sets . 2 ) `` The main technical contribution is the use of SBNs as variational approximations over tree-space , but it is difficult to follow their implementation and parameter sharing without the explanation of the original paper . '' As explained in point 2 to reviewer 3 , this is mainly due to the page limit of the conference . We will definitely add more detailed explanation in our revision if there is room after trimming proposed by Reviewer 2 . 3 ) `` Additionally , the issue of estimating the support of the subsplit CPTs needs more discussion . As the authors acknowledge , complete parameterizations of these models scale in a combinatorial way with ? all possible parent-child subsplit pairs ? , and they deal with this by shrinking the support up front with various heuristics . It seems that these support estimation approaches would be feasible when the data are strong but would become challenging to scale when the data are weak . Since VB is often concerned with the limited-data regime , more discussion of when support estimation is feasible and when it is difficult would clarify how widely applicable the method is . '' This is indeed an important point . We agree that when the data are weak , the posterior on subsplit pairs could have a large support . However , the SBN approach actually has a strong natural advantage in the weak-data regime . When data is weak , the support of the posterior distribution on complete trees , as evaluated by classical MCMC approaches , is enormous . For example , if there is uncertainty in multiple different parts of the tree , the support on complete trees scales as the product of these local uncertainties . The SBN parameterization alleviates this issue by factorizing the uncertainty into local structures . Thus , if the support of parent-child pairs is too large , then one should certainly not be trying to assign posterior support to each tree individually as in classical MCMC . Regarding heuristics for support estimation , we show in section 4.2 that bootstrap-based support estimation is effective even for diffuse posteriors across four data sets ( DS5 , DS6 , DS7 , DS8 ) . See below for the numbers of unique trees in the standard MCMC run samples for all data sets ( which is an indicator of the diffusivity of the posteriors ) . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - datasets | DS1 DS2 DS3 DS4 DS5 DS6 DS7 DS8 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - # sample trees | 1228 7 43 828 33752 35407 1125 3067 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - We agree that a further discussion of the weak-data regime is important and we look forward to adding to the discussion in a revision ."}, {"review_id": "SJVmjjR9FX-1", "review_text": "This paper proposes a variational approach to Bayesian posterior inference in phylogenetic trees. The novel part of the approach (using subsplit Bayesian networks as a variational distribution) is intelligently combined with recent ideas from the approximate-inference literature (reweighted wake-sleep, VIMCO, reparameterization gradients, and multiple-sample ELBO estimators) to yield what seems to be an effective approach to a very hard inference problem. My score would be higher were it not for two issues: * The paper is 10 pages long, and I'm not convinced it needs to be. The reviewer guidelines (https://iclr.cc/Conferences/2019/Reviewer_Guidelines) say that \"the overall time to read a paper should be comparable to that of a typical 8-page conference paper. Reviewers may apply a higher reviewing standard to papers that substantially exceed this length.\" So I recommend trying to cut it down a bit during the revision phase. * The empirical comparisons are all likelihood/ELBO-based. These metrics are important, but it would be nice to see some kind of qualitative summary of the inferences made by different methods\u2014two methods can produce similar log-likelihoods or KL divergences but suggest different scientific conclusions. One final comment: it's not clear to me that ICLR is the most relevant venue for this work, which is purely about Bayesian inference rather than deep learning. This isn't a huge deal\u2014certainly there's plenty of variational inference at ICLR these days\u2014but I suspect many ICLR attendees may tune out when they realize there aren't any neural nets in the paper.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for your review and time . We would like to incorporate the suggestions into our revision and think we would benefit from some clarifications on your part . 1 ) `` The paper is 10 pages long , and I 'm not convinced it needs to be . The reviewer guidelines ( https : //iclr.cc/Conferences/2019/Reviewer_Guidelines ) say that `` the overall time to read a paper should be comparable to that of a typical 8-page conference paper . Reviewers may apply a higher reviewing standard to papers that substantially exceed this length . '' So I recommend trying to cut it down a bit during the revision phase . '' The main reason we took 10 pages for the paper is that phylogenetic inference is probably not well known to the machine learning community and much space is devoted to putting the phylogenetic models and experiments in context . We have tried to balance between being short and being a little bit long ( but more self-contained ) and thought the latter would eventually save the reviewers ' time . However , we would like to cut down our paper as suggested and would appreciate it very much if the reviewer can point to us which parts of the paper that you find are redundant and can be made more brief . 2 ) `` The empirical comparisons are all likelihood/ELBO-based . These metrics are important , but it would be nice to see some kind of qualitative summary of the inferences made by different methods ? two methods can produce similar log-likelihoods or KL divergences but suggest different scientific conclusions . '' First , we would like to make sure the reviewer is aware how the KL results show the SBN-based approximations to be very close in distribution on the discrete space of phylogenetic tree structures . We have emphasized in point 3 to reviewer 3 , and realize that we should have been more clear on this point . However , we are happy to incorporate any qualitative summaries the reviewer would like to suggest . We could certainly add , for example , tree shape summaries , but such a comparison would be significantly weaker than the current comparison on tree structures . 3 ) `` One final comment : it 's not clear to me that ICLR is the most relevant venue for this work , which is purely about Bayesian inference rather than deep learning . This is n't a huge deal ? certainly there 's plenty of variational inference at ICLR these days ? but I suspect many ICLR attendees may tune out when they realize there are n't any neural nets in the paper . '' We think ICLR is an excellent venue for this work because : ( i ) Representation learning on discrete/structured objects has received increasing attention from the machine learning community , and our work represents an important advance in variational inference on complex structured models . ( ii ) Our variational framework admits many extensions that can incorporate the approximating power of neural networks ( e.g , using normalizing flow and deep networks for more flexible within-tree and between-tree approximations , as mentioned in the discussion section of our paper ) ."}, {"review_id": "SJVmjjR9FX-2", "review_text": "This paper is well written, appears to be well executed, and the results look good. I am not particularly well informed about the area, but the work appears to be novel. MCMC for phylogenetic inference is hugely expensive, and anything we can do to reduce that cost would be beneficial (the computational expense is not given, or I've missed it, for the variational approach - presumably it is relatively small compared to MCMC?). My main criticism is that I found the details of subsplit Bayesian networks difficult to follow. Googling them suggests they are a relatively new model, which has not been well studied or used (there are no citations of the paper that introduces them for example!). The paper would be stronger if it discussed these in more detail - how close can they come to approximating the models usually used in phylogenetic analyses? Often the inferred phylogeny is itself of interest - how similar are the trees inferred here to those found from MrBayes?", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and feedback . We address your specific questions and comments below : 1 ) `` the computational expense is not given , or I 've missed it , for the variational approach - presumably it is relatively small compared to MCMC ? '' We present the computational expense for the variational approach in terms of the number of likelihood evaluations , and compare to MCMC . We direct the reviewer to Figure 4 in section 4.2 , where we show the KL divergence to the ground truth as a function of the number of iterations of different methods ( including MCMC via MrBayes ) . For a fair comparison , the number of iterations for MCMC is mapped to the number of iterations of variational methods that take the same number of likelihood evaluations . 2 ) `` My main criticism is that I found the details of subsplit Bayesian networks difficult to follow . Googling them suggests they are a relatively new model , which has not been well studied or used ( there are no citations of the paper that introduces them for example ! ) . '' SBNs are indeed a new model . The relatively short discussion of subsplit Bayesian networks ( SBNs ) is mainly due to the page limit of the conference , but we would like to present a more detailed discussion of SBNs if there is room in our revision . For a more detailed discussion , we refer the reviewer to the original paper [ 1 ] that introduced SBNs , which has been accepted to NIPS this year . 3 ) `` The paper would be stronger if it discussed these in more detail - how close can they come to approximating the models usually used in phylogenetic analyses ? Often the inferred phylogeny is itself of interest - how similar are the trees inferred here to those found from MrBayes ? '' First , we would like to ensure that our means of evaluating the SBN approximation is clear . We compute KL divergence over the discrete collection of phylogenetic tree structures , from the SBN distribution to the ground truth distribution on phylogenetic tree models obtained from extremely long MCMC runs using MrBayes . In order to get a low KL divergence to this ground truth , it is not enough to have similar trees : one must find practically the same set of trees as MrBayes , with nearly identical probability weights . Based on the low KL divergence reported in [ 1 ] and our experiments , SBNs can indeed provide accurate approximations to the phylogenetic posteriors inferred from real data ( see Table 1 in [ 1 ] and section 4.2 in our paper . ) . Therefore , we believe SBN-based phylogenetic inference represents an important advance in this field , especially on structural learning of phylogenies . Reference [ 1 ] C. Zhang and FA . Matsen.Generalizing tree probability estimation via Bayesian networks . arXiv preprint arXiv:1805.07834 , 2018"}], "0": {"review_id": "SJVmjjR9FX-0", "review_text": "This paper explores an approximate inference solution to the challenging problem of Bayesian inference of phylogenetic trees. Its leverages recently proposed subsplit Bayesian networks (SBNs) as a variational approximation over tree space and combines this with modern gradient estimators for VI. It is thorough in its evaluation of both methodological considerations and different datasets. The main advantage would seem to be a large speedup over MCMC-based methods (Figure 4), which could be of significant value to the phylogenetics community. This point would benefit from more discussion. How do the number of iterations (reported in Figures 3&4, which was done carefully) correspond to wallclock time? Can this new method scale to numbers of sites and sequences that were previously unfeasible? The main technical contribution is the use of SBNs as variational approximations over tree-space, but it is difficult to follow their implementation and parameter sharing without the explanation of the original paper. Additionally, the issue of estimating the support of the subsplit CPTs needs more discussion. As the authors acknowledge, complete parameterizations of these models scale in a combinatorial way with \u201call possible parent-child subsplit pairs\u201d, and they deal with this by shrinking the support up front with various heuristics. It seems that these support estimation approaches would be feasible when the data are strong but would become challenging to scale when the data are weak. Since VB is often concerned with the limited-data regime, more discussion of when support estimation is feasible and when it is difficult would clarify how widely applicable the method is. Overall, this work is an interesting extension of variational Bayes to a tree-structured inference problem and is thorough in its evaluation. While it is a bit focused on classical inference for ICLR, it could be interesting both for the VI community and as a significant application advancement. Other notes: In table 1, is the point that all methods are basically the same with different variance? This is not clear from the text. What about the variational bounds? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your thoughtful review and valuable feedback . Below are the answers to your comments : 1 ) `` The main advantage would seem to be a large speedup over MCMC-based methods ( Figure 4 ) , which could be of significant value to the phylogenetics community . This point would benefit from more discussion . How do the number of iterations ( reported in Figures 3 & 4 , which was done carefully ) correspond to wallclock time ? Can this new method scale to numbers of sites and sequences that were previously unfeasible ? '' We are glad that this reviewer appreciates the care with which we crafted the comparison in terms of number of likelihood evaluations . Our motivation in doing a comparison in terms of likelihood evaluations is because our current implementation is in Python , whereas while MrBayes is in C that has been optimized for many years . This is the first paper introducing the ideas and initial implementation of variational Bayes phylogenetic inference , and we think that this level of comparison is appropriate . We will soon begin developing a highly optimized implementation , for which we are planning a more applications-driven paper which will include a wallclock comparison . Regarding large data sets , given that the learned SBNs can provide guided exploration in tree space and variational approaches naturally incorporate stochastic gradients , we believe it is much easier for VBPI to scale to datasets with large numbers of sequences and sites . However , we have not tried out our initial Python implementation on especially big data sets . 2 ) `` The main technical contribution is the use of SBNs as variational approximations over tree-space , but it is difficult to follow their implementation and parameter sharing without the explanation of the original paper . '' As explained in point 2 to reviewer 3 , this is mainly due to the page limit of the conference . We will definitely add more detailed explanation in our revision if there is room after trimming proposed by Reviewer 2 . 3 ) `` Additionally , the issue of estimating the support of the subsplit CPTs needs more discussion . As the authors acknowledge , complete parameterizations of these models scale in a combinatorial way with ? all possible parent-child subsplit pairs ? , and they deal with this by shrinking the support up front with various heuristics . It seems that these support estimation approaches would be feasible when the data are strong but would become challenging to scale when the data are weak . Since VB is often concerned with the limited-data regime , more discussion of when support estimation is feasible and when it is difficult would clarify how widely applicable the method is . '' This is indeed an important point . We agree that when the data are weak , the posterior on subsplit pairs could have a large support . However , the SBN approach actually has a strong natural advantage in the weak-data regime . When data is weak , the support of the posterior distribution on complete trees , as evaluated by classical MCMC approaches , is enormous . For example , if there is uncertainty in multiple different parts of the tree , the support on complete trees scales as the product of these local uncertainties . The SBN parameterization alleviates this issue by factorizing the uncertainty into local structures . Thus , if the support of parent-child pairs is too large , then one should certainly not be trying to assign posterior support to each tree individually as in classical MCMC . Regarding heuristics for support estimation , we show in section 4.2 that bootstrap-based support estimation is effective even for diffuse posteriors across four data sets ( DS5 , DS6 , DS7 , DS8 ) . See below for the numbers of unique trees in the standard MCMC run samples for all data sets ( which is an indicator of the diffusivity of the posteriors ) . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - datasets | DS1 DS2 DS3 DS4 DS5 DS6 DS7 DS8 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - # sample trees | 1228 7 43 828 33752 35407 1125 3067 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - We agree that a further discussion of the weak-data regime is important and we look forward to adding to the discussion in a revision ."}, "1": {"review_id": "SJVmjjR9FX-1", "review_text": "This paper proposes a variational approach to Bayesian posterior inference in phylogenetic trees. The novel part of the approach (using subsplit Bayesian networks as a variational distribution) is intelligently combined with recent ideas from the approximate-inference literature (reweighted wake-sleep, VIMCO, reparameterization gradients, and multiple-sample ELBO estimators) to yield what seems to be an effective approach to a very hard inference problem. My score would be higher were it not for two issues: * The paper is 10 pages long, and I'm not convinced it needs to be. The reviewer guidelines (https://iclr.cc/Conferences/2019/Reviewer_Guidelines) say that \"the overall time to read a paper should be comparable to that of a typical 8-page conference paper. Reviewers may apply a higher reviewing standard to papers that substantially exceed this length.\" So I recommend trying to cut it down a bit during the revision phase. * The empirical comparisons are all likelihood/ELBO-based. These metrics are important, but it would be nice to see some kind of qualitative summary of the inferences made by different methods\u2014two methods can produce similar log-likelihoods or KL divergences but suggest different scientific conclusions. One final comment: it's not clear to me that ICLR is the most relevant venue for this work, which is purely about Bayesian inference rather than deep learning. This isn't a huge deal\u2014certainly there's plenty of variational inference at ICLR these days\u2014but I suspect many ICLR attendees may tune out when they realize there aren't any neural nets in the paper.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for your review and time . We would like to incorporate the suggestions into our revision and think we would benefit from some clarifications on your part . 1 ) `` The paper is 10 pages long , and I 'm not convinced it needs to be . The reviewer guidelines ( https : //iclr.cc/Conferences/2019/Reviewer_Guidelines ) say that `` the overall time to read a paper should be comparable to that of a typical 8-page conference paper . Reviewers may apply a higher reviewing standard to papers that substantially exceed this length . '' So I recommend trying to cut it down a bit during the revision phase . '' The main reason we took 10 pages for the paper is that phylogenetic inference is probably not well known to the machine learning community and much space is devoted to putting the phylogenetic models and experiments in context . We have tried to balance between being short and being a little bit long ( but more self-contained ) and thought the latter would eventually save the reviewers ' time . However , we would like to cut down our paper as suggested and would appreciate it very much if the reviewer can point to us which parts of the paper that you find are redundant and can be made more brief . 2 ) `` The empirical comparisons are all likelihood/ELBO-based . These metrics are important , but it would be nice to see some kind of qualitative summary of the inferences made by different methods ? two methods can produce similar log-likelihoods or KL divergences but suggest different scientific conclusions . '' First , we would like to make sure the reviewer is aware how the KL results show the SBN-based approximations to be very close in distribution on the discrete space of phylogenetic tree structures . We have emphasized in point 3 to reviewer 3 , and realize that we should have been more clear on this point . However , we are happy to incorporate any qualitative summaries the reviewer would like to suggest . We could certainly add , for example , tree shape summaries , but such a comparison would be significantly weaker than the current comparison on tree structures . 3 ) `` One final comment : it 's not clear to me that ICLR is the most relevant venue for this work , which is purely about Bayesian inference rather than deep learning . This is n't a huge deal ? certainly there 's plenty of variational inference at ICLR these days ? but I suspect many ICLR attendees may tune out when they realize there are n't any neural nets in the paper . '' We think ICLR is an excellent venue for this work because : ( i ) Representation learning on discrete/structured objects has received increasing attention from the machine learning community , and our work represents an important advance in variational inference on complex structured models . ( ii ) Our variational framework admits many extensions that can incorporate the approximating power of neural networks ( e.g , using normalizing flow and deep networks for more flexible within-tree and between-tree approximations , as mentioned in the discussion section of our paper ) ."}, "2": {"review_id": "SJVmjjR9FX-2", "review_text": "This paper is well written, appears to be well executed, and the results look good. I am not particularly well informed about the area, but the work appears to be novel. MCMC for phylogenetic inference is hugely expensive, and anything we can do to reduce that cost would be beneficial (the computational expense is not given, or I've missed it, for the variational approach - presumably it is relatively small compared to MCMC?). My main criticism is that I found the details of subsplit Bayesian networks difficult to follow. Googling them suggests they are a relatively new model, which has not been well studied or used (there are no citations of the paper that introduces them for example!). The paper would be stronger if it discussed these in more detail - how close can they come to approximating the models usually used in phylogenetic analyses? Often the inferred phylogeny is itself of interest - how similar are the trees inferred here to those found from MrBayes?", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and feedback . We address your specific questions and comments below : 1 ) `` the computational expense is not given , or I 've missed it , for the variational approach - presumably it is relatively small compared to MCMC ? '' We present the computational expense for the variational approach in terms of the number of likelihood evaluations , and compare to MCMC . We direct the reviewer to Figure 4 in section 4.2 , where we show the KL divergence to the ground truth as a function of the number of iterations of different methods ( including MCMC via MrBayes ) . For a fair comparison , the number of iterations for MCMC is mapped to the number of iterations of variational methods that take the same number of likelihood evaluations . 2 ) `` My main criticism is that I found the details of subsplit Bayesian networks difficult to follow . Googling them suggests they are a relatively new model , which has not been well studied or used ( there are no citations of the paper that introduces them for example ! ) . '' SBNs are indeed a new model . The relatively short discussion of subsplit Bayesian networks ( SBNs ) is mainly due to the page limit of the conference , but we would like to present a more detailed discussion of SBNs if there is room in our revision . For a more detailed discussion , we refer the reviewer to the original paper [ 1 ] that introduced SBNs , which has been accepted to NIPS this year . 3 ) `` The paper would be stronger if it discussed these in more detail - how close can they come to approximating the models usually used in phylogenetic analyses ? Often the inferred phylogeny is itself of interest - how similar are the trees inferred here to those found from MrBayes ? '' First , we would like to ensure that our means of evaluating the SBN approximation is clear . We compute KL divergence over the discrete collection of phylogenetic tree structures , from the SBN distribution to the ground truth distribution on phylogenetic tree models obtained from extremely long MCMC runs using MrBayes . In order to get a low KL divergence to this ground truth , it is not enough to have similar trees : one must find practically the same set of trees as MrBayes , with nearly identical probability weights . Based on the low KL divergence reported in [ 1 ] and our experiments , SBNs can indeed provide accurate approximations to the phylogenetic posteriors inferred from real data ( see Table 1 in [ 1 ] and section 4.2 in our paper . ) . Therefore , we believe SBN-based phylogenetic inference represents an important advance in this field , especially on structural learning of phylogenies . Reference [ 1 ] C. Zhang and FA . Matsen.Generalizing tree probability estimation via Bayesian networks . arXiv preprint arXiv:1805.07834 , 2018"}}