{"year": "2020", "forum": "rJx2slSKDS", "title": "Latent Variables on Spheres for Sampling and Inference", "decision": "Reject", "meta_review": "This paper proposes to improve VAE/GAN by performing variational inference with a constraint that the latent variables lie on a sphere. The reviewers find some technical issues with the paper (R3's comment regarding theorem 3). They also found that the method is not motivated well, and the paper is not convincing. Based on this feedback, I recommend to reject the paper.", "reviews": [{"review_id": "rJx2slSKDS-0", "review_text": "This paper proposed an interesting idea that by regularizing the structure of the latent space into a sphere, we can free VAE from variantional inference framework. However, here are several concerns about this paper: 1. is this a variant of the Wasserstein auto-encoder? 2. the image quality of VAE (CelebA) is not that bad in other VAE papers, maybe tuning the \\beta-VAE can also achieve the same quantitative and qualitative results. 3. Can you visualize the latent space (z) for the CelebA dataset, also comparing with the results from VAE?", "rating": "6: Weak Accept", "reply_text": "Q1 : \u201c is this a variant of the Wasserstein auto-encoder ? \u201d A1 : Our SAE algorithm is not a variant of WAE proposed in the following paper . Wasserstein Auto-Encoders https : //arxiv.org/abs/1711.01558 WAE minimized Wasserstein distance between the model distribution and the prior distribution . The algorithm reduces to adversarial learning via a discriminator in the latent space , which is similar to the following paper Adversarial Autoencoders https : //arxiv.org/abs/1511.05644 Like VAE , both Wasserstein autoencoder and adversarial autoencoder need a prior distribution to match . However , there is no loss imposed on the latent space to optimize for SAE . SAE does not need priors either . The object function is the reconstruction loss || x - \\tilde { x } || with the spherical constraint shown in equation ( 10 ) . It is much simpler than Wasserstein autoencoder . In order to elucidate the unique property of random variables on spheres , we leveraged Wasserstein distance to derive Theorem 2 . The Wasserstein distance here serves to establish a circumstance that the algorithm with the spherical constraint can be distribution-agnostic . We did not use Wasserstein distance for computation in SAE . Both Wasserstein autoencoder and adversarial autoencoder are very interesting and inspiring algorithms . We like these two works very much . Q2 : \u201c the image quality of VAE ( CelebA ) is not that bad in other VAE papers , maybe tuning the \\beta-VAE can also achieve the same quantitative and qualitative results. \u201d Q3 : \u201c Can you visualize the latent space ( z ) for the CelebA dataset , also comparing with the results from VAE ? \u201d A2 : For data factors , the image quality of VAE depends on the image size and the image diversity . For face images , the large image size and more backgrounds in the image will make the data difficult to fit . We used the more challenging data of FFHQ available at https : //github.com/NVlabs/stylegan and the image size we used is 128x128 . A3 : We are conducting the experiment on CelebA of size 64x64 according to your advice . We will update the results when this complementary experiment is completed ."}, {"review_id": "rJx2slSKDS-1", "review_text": "Summary This paper considers the L2 normalization of samples \u201cz\u201d from a given prior p(z) in Generative Adversarial Netowks (GAN) and autoencoders. The L2 normalization corresponds to projecting samples onto the surface of a unit-hypersphere. Hence, to attempt to justify this normalization, the authors rely on some already established results regarding high dimensional hyperspheres. In particular, the focus is on the fact that, the Euclidean distance between any given point on a hypersphere and another randomly sampled point on the hypersphere tends to a constant, when the number of dimensions goes to infinity. This result is then used to show that the Wasserstein distance between two arbitrary distributions on a hypersphere converges to a constant when the number of dimensions grows. Based on this result, the authors claim that projecting the latent samples onto the surface of a hypersphere would make GAN less sensitive to the choice of the prior distribution. Moreover, they claim that such normalization would also benefits inference, and that it addresses the issue of variational inference in VAE. Main comments. This paper is hard to follow and requires substantial improvements in terms of writing, owing to several grammatical and semantic issues. Moreover, there is a lack rigor; some important claims are supported neither by experiments nor by theoretical analysis. Experiments in the main paper are also weak. I can therefore not recommend acceptance. My detailed comments are below. - An important claim in this paper is that the proposed approach \u201calleviates variational inference in VAE\u201d. However, this requires clarification as well as theoretical/empirical justifications. - In the introduction, it is stated that generated samples from VAE may deviate from real data samples, because \u201cthe posterior q(z|x) cannot match the prior p(z) perfectly\u201d. However, in VAE we do not expect the posterior to match the prior perfectly, as this would result in useless data representations or inference. Generation issues in VAE may rather be explained by the fact that, in this context we optimize a lower bound on the KL-divergence between the empirical data distribution and the model distribution. The latter objective does not penalize the model distribution if it puts some of its mass in regions where the empirical data distribution is very low or even zero. - Theorem 2 (on the convergence of the Wasserstein distance (W2) on high dimensional hyperspheres) does not seem to hold if, for instance, P and P\u2019 are empirical distributions with overlapping supports. Further, even when the above Theorem holds, the W2 distance may be relatively high since it is proportional to the square root of the number of samples. - Moreover, why and how would Theorem 2 justify improved inference when projecting latent samples onto a hypersphere? - Please consider revising the following statement in the introduction: \u201cThe encoder f in VAE approximates the posterior q(z|x)\u201d. The encoder \u201cf\u201d in VAE parametrizes the variational posterior. - Some typos, - Abstract, \u201c\u2026 by sampling and inference tasks\u201d -- \u201con sampling \u2026\u201d - Introduction second paragraph after eq 2. \u201c\u2026 it also causes the new problems\u201d \u2013 \u201c \u2026 causes new problems\u201d - Section 2.1, \u201cFor convenient analysis \u2026\u201d \u2013 \u201cFor a convenient \u2026\u201d - Second paragraph after Theorem 1. \u201c\u2026 perform probabilistic optimizations \u2026 \u201d \u2013 \u201c\u2026 optimization \u2026\u201d - Section 5.2, second paragraph. Is it Figure 9? The main recommendations I would make are as follows. - Consider revising the paper to improve its writing. - Provide rigorous theoretical analysis and discussions to support the main claims. - Improve experiments by including more datasets and baselines (e.g., hyperspherical VAE [1]), as well conduct more targeted experiments to give more insights regarding the effect of the L2 normalization on inference and generation. [1] Davidson, Tim R., et al. \"Hyperspherical variational auto-encoders.\" UAI, 2018. ", "rating": "1: Reject", "reply_text": "Q1 : \u201c An important claim in this paper is that the proposed approach \u201c alleviates variational inference in VAE \u201d . However , this requires clarification as well as theoretical/empirical justifications \u201d Q2 : \u201c Moreover , why and how would Theorem 2 justify improved inference when projecting latent samples onto a hypersphere ? \u201d A1 and A2 : These two questions and related comments might be due to our inappropriate use of \u201c alleviate \u201d and the extensive meaning of inference beyond probability . Actually , there is no posterior inference and any priors involved in our SAE algorithm . It is the vanilla autoencoder subject to the spherical constraint shown in equation ( 10 ) . So we said \u201c thus freeing VAE from the approximate optimization of posterior probability via variational inference \u201d and \u201c Our algorithm is geometric and free from posterior probability optimization \u201d . Indeed , \u201c alleviates variational inference in VAE \u201d is an inappropriate use in this scenario . We will correct this in the revised version . Besides , we use \u201c inference \u201d to refer to inferring ( obtaining ) z from the encoder , not only for \u201c variational \u201d inference or \u201c probabilistic \u201d inference . This might cause misunderstanding with habitual thinking in this field . This misunderstanding might be avoided by using \u201c geometric inference \u201d . We will note this meaning clearly in the revised version . Q3 : \u201c However , in VAE we do not expect the posterior to match the prior perfectly , as this would result in useless data representations or inference. \u201d A3 : We understand your viewpoint about the model distribution and the prior distribution . \u201c match the prior perfectly \u201d does not mean the point-to-point correspondence . We refer to fitting distributions . The word \u201c match \u201d is also used in Wasserstein autoencoder ( https : //arxiv.org/abs/1711.01558 ) , which is the same scenario to ours . Q4 : \u201c Theorem 2 ( on the convergence of the Wasserstein distance ( W2 ) on high dimensional hyperspheres ) does not seem to hold if , for instance , P and P \u2019 are empirical distributions with overlapping supports. \u201d Q5 : \u201c Further , even when the above Theorem holds , the W2 distance may be relatively high since it is proportional to the square root of the number of samples. \u201d A4 : To make our theory much easier to understand , we directly gave the computational definition of Wasserstein distance in ( 8 ) and ( 9 ) rather than its original integral form . Thus , Theorem 2 is the direct result by substituting the conclusion of Lemma 1 into ( 8 ) . It is very easy . About the correctness of Lemma 1 , please refer to the elegant proof at http : //faculty.madisoncollege.edu/alehnen/sphere/hypers.htm . Most theorems only hold under some conditions . Both Lemma 1 and Theorem 2 need a basic condition . The condition is that the points are drawn from spheres at RANDOM . To satisfy the condition , we use the operation of centerization in our SAE algorithm , which is motivated from central limit theorem in probability . In fact , it is straightforward to design the case to deny Lemma 1 and Theorem 2 if we bypass the condition . For instance , let Z1 be the set sampled from the spherical part in the open positive orthant and Z2 sampled from the spherical part in the open negative orthant . The third set Z3 is derived from Z2 by the small perturbation . Both Lemma 1 and Theorem 2 do not hold for the dataset { Z1 , Z2 , Z3 } . But such samping violates the randomness needed . For SAE , the centerization is used to prevent such cases . A5 : \u201c the W2 distance may be relatively high since it is proportional to the square root of the number of samples. \u201d is correct . However , it is logically wrong to use it to deny our theory , because all the W2 distances between two arbitrary random datasets still converge to be the same constant in Theorem 2 when the number of samples increases . The conclusion still holds in our paper . Q6 : \u201c Improve experiments by including more datasets and baselines ( e.g. , hyperspherical VAE [ 1 ] ) , as well conduct more targeted experiments to give more insights regarding the effect of the L2 normalization on inference and generation . \u201d A6 : We failed to get the convergent results of hyper-Spherical VAE ( S-VAE ) on FFHQ faces of size 128x128 . So we did not compare it in the current version . We are now running it on MNIST . The results will be updated in the revised version within several days ."}, {"review_id": "rJx2slSKDS-2", "review_text": "This paper proposes a novel autoencoder algorithm, named Spherical AutoEncoder (SAE). In this paper, the authors argue that the sphere structure has good properties in high-dimensional. To leverage the properties, proposed algorithm centerizes latent variables and projects them onto unit sphere. To show the empirical performance of the proposed approach, the authors perform image reconstruction and generation using FFHQ dataset and MNIST dataset. Comments: I think the proposed approach, using spherical latent space, is interesting and make sense. - As mentioned in section 3.2, the proposed algorithm is reduced to standard autoencoder since it is free from posterior inference. Then, to clarify the algorithm, it seems necessary to provide the formulation of objective functions. - Is the objective still valid or reasonable even it is derived from the equation (10) without posterior inference? - How does the objective change when centerization and spherization are applied to the GAN? - Compared with using von Mises-Fisher distribution in the vanilla VAE, the advantage of the proposed method is not clear. To my understanding, the main difference seems to be whether using lower bound with posterior inference or deterministic framework without such approximation. However, there are no theoretical or empirical results to show the benefit of the proposed method. If theoretical or empirical results with reasonable intuition is provided, it will make the proposed algorithm more valuable. Questions: - Compare to ProGAN and StyleGAN, is the contribution of the paper to applying centerization to GAN and centerization and spherization to autoencoder? - What dimension do you use as latent dimension in the experiments? - Does the choice of prior distribution affect the experimental results? If so, is there any compatible reason with the intuition of SAE? Typo: Under equation (10) in page 5: \\tilde{z} should be \\hat{z}. ", "rating": "6: Weak Accept", "reply_text": "Q1 : \u201c Then , to clarify the algorithm , it seems necessary to provide the formulation of objective functions. \u201d , \u201c Is the objective still valid or reasonable even it is derived from the equation ( 10 ) without posterior inference ? \u201d A1 : The objective function will be provided in the revised version . It is the reconstruction loss || x - \\tilde { x } || subject to the spherical constraint on z ( equation ( 10 ) ) . There are no posterior inference and no KL-divergence involved in our algorithm . It is very simple . Q2 : \u201c How does the objective change when centerization and spherization are applied to the GAN ? \u201d A2 : There is no extra objective when applied to GANs . Only centerization and spherization are needed . Q3 : \u201c Compared with using von Mises-Fisher distribution in the vanilla VAE , the advantage of the proposed method is not clear . To my understanding , the main difference seems to be whether using lower bound with posterior inference or deterministic framework without such approximation . However , there are no theoretical or empirical results to show the benefit of the proposed method. \u201d A3 : This might be the misunderstanding caused by that we didn \u2019 t explicitly write the objective function in the paper . We explain this in Q1 . Our SAE algorithm is essentially different from S-VAE ( hyper-Spherical VAE ) . The S-VAE is established on the principle of VAE . So , S-VAE has the drawbacks posed by VAE such as the approximation of posterior inference , the prior dependence , and the reparameterization trick for random variables . But SAE is distribution-agnostic with respect to Wasserstein distance , which is rigorously guaranteed by Theorem 2 . Actually , we failed to get the convergent results of S-VAE on FFHQ faces of size 128x128 . We are now running it on MNIST . The results will be updated in the revised version within several days . Q4 : \u201c Compare to ProGAN and StyleGAN , is the contribution of the paper to applying centerization to GAN and centerization and spherization to autoencoder ? \u201d A4 : Both GAN and autoencoder need to use centerization and spherization on random variables . For ProGAN and StyleGAN , the authors empirically applied spherization on z in their code , which motivated our work . We also made it clear in the context of equation ( 3 ) . Q4 : \u201c What dimension do you use as latent dimension in the experiments ? \u201d A4 : We followed the experimental setting of StyleGAN . The 512-dimensional latent vectors are used for StyleGAN , VAE , and SAE on the face datasets including FFHQ and CelebA . For MNIST , we take the 10-dimensional latent codes . Q5 : \u201c Does the choice of prior distribution affect the experimental results ? If so , is there any compatible reason with the intuition of SAE ? \u201d A5 : This question might be another misunderstanding caused by Q1 . Actually , there are no any priors involved in SAE during training . We used different priors to test the robustness of SAE and VAE after training was completed . We will make it clear in the revised version ."}], "0": {"review_id": "rJx2slSKDS-0", "review_text": "This paper proposed an interesting idea that by regularizing the structure of the latent space into a sphere, we can free VAE from variantional inference framework. However, here are several concerns about this paper: 1. is this a variant of the Wasserstein auto-encoder? 2. the image quality of VAE (CelebA) is not that bad in other VAE papers, maybe tuning the \\beta-VAE can also achieve the same quantitative and qualitative results. 3. Can you visualize the latent space (z) for the CelebA dataset, also comparing with the results from VAE?", "rating": "6: Weak Accept", "reply_text": "Q1 : \u201c is this a variant of the Wasserstein auto-encoder ? \u201d A1 : Our SAE algorithm is not a variant of WAE proposed in the following paper . Wasserstein Auto-Encoders https : //arxiv.org/abs/1711.01558 WAE minimized Wasserstein distance between the model distribution and the prior distribution . The algorithm reduces to adversarial learning via a discriminator in the latent space , which is similar to the following paper Adversarial Autoencoders https : //arxiv.org/abs/1511.05644 Like VAE , both Wasserstein autoencoder and adversarial autoencoder need a prior distribution to match . However , there is no loss imposed on the latent space to optimize for SAE . SAE does not need priors either . The object function is the reconstruction loss || x - \\tilde { x } || with the spherical constraint shown in equation ( 10 ) . It is much simpler than Wasserstein autoencoder . In order to elucidate the unique property of random variables on spheres , we leveraged Wasserstein distance to derive Theorem 2 . The Wasserstein distance here serves to establish a circumstance that the algorithm with the spherical constraint can be distribution-agnostic . We did not use Wasserstein distance for computation in SAE . Both Wasserstein autoencoder and adversarial autoencoder are very interesting and inspiring algorithms . We like these two works very much . Q2 : \u201c the image quality of VAE ( CelebA ) is not that bad in other VAE papers , maybe tuning the \\beta-VAE can also achieve the same quantitative and qualitative results. \u201d Q3 : \u201c Can you visualize the latent space ( z ) for the CelebA dataset , also comparing with the results from VAE ? \u201d A2 : For data factors , the image quality of VAE depends on the image size and the image diversity . For face images , the large image size and more backgrounds in the image will make the data difficult to fit . We used the more challenging data of FFHQ available at https : //github.com/NVlabs/stylegan and the image size we used is 128x128 . A3 : We are conducting the experiment on CelebA of size 64x64 according to your advice . We will update the results when this complementary experiment is completed ."}, "1": {"review_id": "rJx2slSKDS-1", "review_text": "Summary This paper considers the L2 normalization of samples \u201cz\u201d from a given prior p(z) in Generative Adversarial Netowks (GAN) and autoencoders. The L2 normalization corresponds to projecting samples onto the surface of a unit-hypersphere. Hence, to attempt to justify this normalization, the authors rely on some already established results regarding high dimensional hyperspheres. In particular, the focus is on the fact that, the Euclidean distance between any given point on a hypersphere and another randomly sampled point on the hypersphere tends to a constant, when the number of dimensions goes to infinity. This result is then used to show that the Wasserstein distance between two arbitrary distributions on a hypersphere converges to a constant when the number of dimensions grows. Based on this result, the authors claim that projecting the latent samples onto the surface of a hypersphere would make GAN less sensitive to the choice of the prior distribution. Moreover, they claim that such normalization would also benefits inference, and that it addresses the issue of variational inference in VAE. Main comments. This paper is hard to follow and requires substantial improvements in terms of writing, owing to several grammatical and semantic issues. Moreover, there is a lack rigor; some important claims are supported neither by experiments nor by theoretical analysis. Experiments in the main paper are also weak. I can therefore not recommend acceptance. My detailed comments are below. - An important claim in this paper is that the proposed approach \u201calleviates variational inference in VAE\u201d. However, this requires clarification as well as theoretical/empirical justifications. - In the introduction, it is stated that generated samples from VAE may deviate from real data samples, because \u201cthe posterior q(z|x) cannot match the prior p(z) perfectly\u201d. However, in VAE we do not expect the posterior to match the prior perfectly, as this would result in useless data representations or inference. Generation issues in VAE may rather be explained by the fact that, in this context we optimize a lower bound on the KL-divergence between the empirical data distribution and the model distribution. The latter objective does not penalize the model distribution if it puts some of its mass in regions where the empirical data distribution is very low or even zero. - Theorem 2 (on the convergence of the Wasserstein distance (W2) on high dimensional hyperspheres) does not seem to hold if, for instance, P and P\u2019 are empirical distributions with overlapping supports. Further, even when the above Theorem holds, the W2 distance may be relatively high since it is proportional to the square root of the number of samples. - Moreover, why and how would Theorem 2 justify improved inference when projecting latent samples onto a hypersphere? - Please consider revising the following statement in the introduction: \u201cThe encoder f in VAE approximates the posterior q(z|x)\u201d. The encoder \u201cf\u201d in VAE parametrizes the variational posterior. - Some typos, - Abstract, \u201c\u2026 by sampling and inference tasks\u201d -- \u201con sampling \u2026\u201d - Introduction second paragraph after eq 2. \u201c\u2026 it also causes the new problems\u201d \u2013 \u201c \u2026 causes new problems\u201d - Section 2.1, \u201cFor convenient analysis \u2026\u201d \u2013 \u201cFor a convenient \u2026\u201d - Second paragraph after Theorem 1. \u201c\u2026 perform probabilistic optimizations \u2026 \u201d \u2013 \u201c\u2026 optimization \u2026\u201d - Section 5.2, second paragraph. Is it Figure 9? The main recommendations I would make are as follows. - Consider revising the paper to improve its writing. - Provide rigorous theoretical analysis and discussions to support the main claims. - Improve experiments by including more datasets and baselines (e.g., hyperspherical VAE [1]), as well conduct more targeted experiments to give more insights regarding the effect of the L2 normalization on inference and generation. [1] Davidson, Tim R., et al. \"Hyperspherical variational auto-encoders.\" UAI, 2018. ", "rating": "1: Reject", "reply_text": "Q1 : \u201c An important claim in this paper is that the proposed approach \u201c alleviates variational inference in VAE \u201d . However , this requires clarification as well as theoretical/empirical justifications \u201d Q2 : \u201c Moreover , why and how would Theorem 2 justify improved inference when projecting latent samples onto a hypersphere ? \u201d A1 and A2 : These two questions and related comments might be due to our inappropriate use of \u201c alleviate \u201d and the extensive meaning of inference beyond probability . Actually , there is no posterior inference and any priors involved in our SAE algorithm . It is the vanilla autoencoder subject to the spherical constraint shown in equation ( 10 ) . So we said \u201c thus freeing VAE from the approximate optimization of posterior probability via variational inference \u201d and \u201c Our algorithm is geometric and free from posterior probability optimization \u201d . Indeed , \u201c alleviates variational inference in VAE \u201d is an inappropriate use in this scenario . We will correct this in the revised version . Besides , we use \u201c inference \u201d to refer to inferring ( obtaining ) z from the encoder , not only for \u201c variational \u201d inference or \u201c probabilistic \u201d inference . This might cause misunderstanding with habitual thinking in this field . This misunderstanding might be avoided by using \u201c geometric inference \u201d . We will note this meaning clearly in the revised version . Q3 : \u201c However , in VAE we do not expect the posterior to match the prior perfectly , as this would result in useless data representations or inference. \u201d A3 : We understand your viewpoint about the model distribution and the prior distribution . \u201c match the prior perfectly \u201d does not mean the point-to-point correspondence . We refer to fitting distributions . The word \u201c match \u201d is also used in Wasserstein autoencoder ( https : //arxiv.org/abs/1711.01558 ) , which is the same scenario to ours . Q4 : \u201c Theorem 2 ( on the convergence of the Wasserstein distance ( W2 ) on high dimensional hyperspheres ) does not seem to hold if , for instance , P and P \u2019 are empirical distributions with overlapping supports. \u201d Q5 : \u201c Further , even when the above Theorem holds , the W2 distance may be relatively high since it is proportional to the square root of the number of samples. \u201d A4 : To make our theory much easier to understand , we directly gave the computational definition of Wasserstein distance in ( 8 ) and ( 9 ) rather than its original integral form . Thus , Theorem 2 is the direct result by substituting the conclusion of Lemma 1 into ( 8 ) . It is very easy . About the correctness of Lemma 1 , please refer to the elegant proof at http : //faculty.madisoncollege.edu/alehnen/sphere/hypers.htm . Most theorems only hold under some conditions . Both Lemma 1 and Theorem 2 need a basic condition . The condition is that the points are drawn from spheres at RANDOM . To satisfy the condition , we use the operation of centerization in our SAE algorithm , which is motivated from central limit theorem in probability . In fact , it is straightforward to design the case to deny Lemma 1 and Theorem 2 if we bypass the condition . For instance , let Z1 be the set sampled from the spherical part in the open positive orthant and Z2 sampled from the spherical part in the open negative orthant . The third set Z3 is derived from Z2 by the small perturbation . Both Lemma 1 and Theorem 2 do not hold for the dataset { Z1 , Z2 , Z3 } . But such samping violates the randomness needed . For SAE , the centerization is used to prevent such cases . A5 : \u201c the W2 distance may be relatively high since it is proportional to the square root of the number of samples. \u201d is correct . However , it is logically wrong to use it to deny our theory , because all the W2 distances between two arbitrary random datasets still converge to be the same constant in Theorem 2 when the number of samples increases . The conclusion still holds in our paper . Q6 : \u201c Improve experiments by including more datasets and baselines ( e.g. , hyperspherical VAE [ 1 ] ) , as well conduct more targeted experiments to give more insights regarding the effect of the L2 normalization on inference and generation . \u201d A6 : We failed to get the convergent results of hyper-Spherical VAE ( S-VAE ) on FFHQ faces of size 128x128 . So we did not compare it in the current version . We are now running it on MNIST . The results will be updated in the revised version within several days ."}, "2": {"review_id": "rJx2slSKDS-2", "review_text": "This paper proposes a novel autoencoder algorithm, named Spherical AutoEncoder (SAE). In this paper, the authors argue that the sphere structure has good properties in high-dimensional. To leverage the properties, proposed algorithm centerizes latent variables and projects them onto unit sphere. To show the empirical performance of the proposed approach, the authors perform image reconstruction and generation using FFHQ dataset and MNIST dataset. Comments: I think the proposed approach, using spherical latent space, is interesting and make sense. - As mentioned in section 3.2, the proposed algorithm is reduced to standard autoencoder since it is free from posterior inference. Then, to clarify the algorithm, it seems necessary to provide the formulation of objective functions. - Is the objective still valid or reasonable even it is derived from the equation (10) without posterior inference? - How does the objective change when centerization and spherization are applied to the GAN? - Compared with using von Mises-Fisher distribution in the vanilla VAE, the advantage of the proposed method is not clear. To my understanding, the main difference seems to be whether using lower bound with posterior inference or deterministic framework without such approximation. However, there are no theoretical or empirical results to show the benefit of the proposed method. If theoretical or empirical results with reasonable intuition is provided, it will make the proposed algorithm more valuable. Questions: - Compare to ProGAN and StyleGAN, is the contribution of the paper to applying centerization to GAN and centerization and spherization to autoencoder? - What dimension do you use as latent dimension in the experiments? - Does the choice of prior distribution affect the experimental results? If so, is there any compatible reason with the intuition of SAE? Typo: Under equation (10) in page 5: \\tilde{z} should be \\hat{z}. ", "rating": "6: Weak Accept", "reply_text": "Q1 : \u201c Then , to clarify the algorithm , it seems necessary to provide the formulation of objective functions. \u201d , \u201c Is the objective still valid or reasonable even it is derived from the equation ( 10 ) without posterior inference ? \u201d A1 : The objective function will be provided in the revised version . It is the reconstruction loss || x - \\tilde { x } || subject to the spherical constraint on z ( equation ( 10 ) ) . There are no posterior inference and no KL-divergence involved in our algorithm . It is very simple . Q2 : \u201c How does the objective change when centerization and spherization are applied to the GAN ? \u201d A2 : There is no extra objective when applied to GANs . Only centerization and spherization are needed . Q3 : \u201c Compared with using von Mises-Fisher distribution in the vanilla VAE , the advantage of the proposed method is not clear . To my understanding , the main difference seems to be whether using lower bound with posterior inference or deterministic framework without such approximation . However , there are no theoretical or empirical results to show the benefit of the proposed method. \u201d A3 : This might be the misunderstanding caused by that we didn \u2019 t explicitly write the objective function in the paper . We explain this in Q1 . Our SAE algorithm is essentially different from S-VAE ( hyper-Spherical VAE ) . The S-VAE is established on the principle of VAE . So , S-VAE has the drawbacks posed by VAE such as the approximation of posterior inference , the prior dependence , and the reparameterization trick for random variables . But SAE is distribution-agnostic with respect to Wasserstein distance , which is rigorously guaranteed by Theorem 2 . Actually , we failed to get the convergent results of S-VAE on FFHQ faces of size 128x128 . We are now running it on MNIST . The results will be updated in the revised version within several days . Q4 : \u201c Compare to ProGAN and StyleGAN , is the contribution of the paper to applying centerization to GAN and centerization and spherization to autoencoder ? \u201d A4 : Both GAN and autoencoder need to use centerization and spherization on random variables . For ProGAN and StyleGAN , the authors empirically applied spherization on z in their code , which motivated our work . We also made it clear in the context of equation ( 3 ) . Q4 : \u201c What dimension do you use as latent dimension in the experiments ? \u201d A4 : We followed the experimental setting of StyleGAN . The 512-dimensional latent vectors are used for StyleGAN , VAE , and SAE on the face datasets including FFHQ and CelebA . For MNIST , we take the 10-dimensional latent codes . Q5 : \u201c Does the choice of prior distribution affect the experimental results ? If so , is there any compatible reason with the intuition of SAE ? \u201d A5 : This question might be another misunderstanding caused by Q1 . Actually , there are no any priors involved in SAE during training . We used different priors to test the robustness of SAE and VAE after training was completed . We will make it clear in the revised version ."}}