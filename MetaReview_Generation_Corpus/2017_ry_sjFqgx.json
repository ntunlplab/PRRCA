{"year": "2017", "forum": "ry_sjFqgx", "title": "Program Synthesis for Character Level Language Modeling", "decision": "Accept (Poster)", "meta_review": "This work was a very controversial submission, with two strong accepts and one initial strong reject. There was significant discussion about the replicability of the paper, although all reviewers seem interested in the methods. \n \n Pro:\n - There is strong originality to this work. One of the few submissions in the area not just using RNNs for LM.\n - The paper shows strong empirical results on PL language modeling, comparative results in natural language modeling, fast lookup compared to neural models, and presents a significantly different approach then the currently accepted NN methods. \n \n \n Cons:\n - The original draft has clarity issues. In particular the MCMC approach is very difficult to follow and lacks clear explanation for this community, the interpretability of the method is not demonstrated, and too much of the work is devoted to laying out the language itself. (Note The authors have gone out of their way to include an appendix which caused one reviewer to go from recommending strong rejection to weak rejection, but also requires a much larger submission size. While two reviewers do like the work, their reviews are mainly based on the empirical sucess at program language modeling.)\n \n Overall, the PCs have determined that this work deserves to appear at the conference.", "reviews": [{"review_id": "ry_sjFqgx-0", "review_text": "This paper proposes an approach to character language modeling (CLMs) based on developing a domain specific language to represent CLMs. The experiments show mixed performance versus neural CLM approaches to modeling linux kernel data and wikipedia text, however the proposed DSL models are slightly more compact and fast to query as compared with neural CLMs. The proposed approach is difficult to understand overall and perhaps is aimed towards the sub-community already working on this sort of approach but lacks sufficient explanation for the ICLR audience. Critically the paper glosses over the major issues of demonstrating the proposed DSL is a valid probabilistic model and how training is performed to fit the model to data (there is clearly not a gradient-based training approach used). FInally the experiments feel incomplete without showing samples drawn from the generative model or analyzing the learned model to determine what it has learned. Overall I feel this paper does not describe the approach in enough depth for readers to understand or re-implement it. Almost all of the model section is devoted to exposition of the DSL without specifying how probabilities are computed using this model and how training is performed. How are probabilities actually encoded? The DSL description seems to have only discrete decisions rather than probabilities. Training is perhaps covered in previous papers but there needs to be some discussion of how it works here. Section 2.5 does not do enough to explain how training works or how any measure of optimality is achieved. Given this model is quite a different hypothesis space from neural models or n-grams, looking and samples drawn from the model seems critical. The current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example long-range syntax constraints like brackets)", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the suggestions . In the Appendix , we clarify the training procedure as well as how the probabilistic model is built . It is true the model is based on discrete and deterministic decisions obtained by executing the learned program . This is also true for the probabilistic model which uses maximum likelihood estimation based on counting ( as in n-gram models ) . It is an interesting future work direction to extend to model with continuous word representations as done in neural networks ."}, {"review_id": "ry_sjFqgx-1", "review_text": "The authors propose a method for language modeling by first generating a program from a DSL, then learning the count-based parameters of that program. Pros include: The proposed method is innovative and highly different from standard LSTM-based approaches of late. The model should also be much quicker to apply at query time. Strong empirical results are obtained on modeling code, though there is some gap between the synthesis method and neural methods on the Hutter task. A detailed description of the language syntax is provided. Cons/suggestions: - The synthesis procedure using MCMC is left very vague, even though being able to make this procedure efficient is one of the key questions. - The work builds on work from the PL literature; surely the related work could also be expanded and this work better put in context. - More compact/convincing examples of human interpretability would be helpful. Other comments - Training time evaluation in Table 1 should give basic information such as whether training was done on GPU/CPU, CPU specs, etc. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the suggestions , we added a thorough Appendix that provides all details . We also included details about the used hardware in our experimental setup in the updated version . All of our experiments were performed on a machine with Intel ( R ) Xeon ( R ) CPU E5-2690 with 14 cores . All training times are reported for parallel training on CPU . Using GPUs for training of the neural networks is likely to provide additional improvement in training time ."}, {"review_id": "ry_sjFqgx-2", "review_text": "This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which rely on gradient descent, and thus are extremely slow to estimate. Count-based method such as regular n-gram models suffer because of their simplicity, i.e. not being able to model large context, and scaling badly as context increases. The proposed approach synthesizes programs using MCMC which learn context-sensitive probabilities using count-based estimation, and thus is both fast and able to model long-range context. Experiments on a programming language datasets, the linux kernel corpus, show that this method is vastly better than both LSTM and n-gram language models. Experiments on the Wikipedia corpus show that the method is competitive, but not better, to SOTA models. Both estimation and query time are significantly better than LSTM LMs, and competitive to n-gram LMs. It's debatable whether this paper is suitable for ICLR, due to ICLR's focus on neural network-based approaches. However, in the interest of diversity and novelty, such \"outside\" papers should be accepted to ICLR. This paper is likely to inspire more research into fusion of program synthesis and machine learning methods, which was a popular theme at NIPS 2016. *Pros* 1. Novel approach. 2. Good results. *Cons* 1. Some significant algorithmic details are not included in the paper. They should at least be included in an appendix for comprehensiveness. *Comments* 1. Please include n-gram results in the table for Wikipedia results.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Given the limits on the length of the submission it was not possible to include all details . However , as suggested by all reviewers , we added an Appendix describing all relevant details of the synthesis , formal semantics and how the probability distributions are built . We also updated Table 2 in the main paper to include results for n-gram ( 1.94 BPC ) ."}], "0": {"review_id": "ry_sjFqgx-0", "review_text": "This paper proposes an approach to character language modeling (CLMs) based on developing a domain specific language to represent CLMs. The experiments show mixed performance versus neural CLM approaches to modeling linux kernel data and wikipedia text, however the proposed DSL models are slightly more compact and fast to query as compared with neural CLMs. The proposed approach is difficult to understand overall and perhaps is aimed towards the sub-community already working on this sort of approach but lacks sufficient explanation for the ICLR audience. Critically the paper glosses over the major issues of demonstrating the proposed DSL is a valid probabilistic model and how training is performed to fit the model to data (there is clearly not a gradient-based training approach used). FInally the experiments feel incomplete without showing samples drawn from the generative model or analyzing the learned model to determine what it has learned. Overall I feel this paper does not describe the approach in enough depth for readers to understand or re-implement it. Almost all of the model section is devoted to exposition of the DSL without specifying how probabilities are computed using this model and how training is performed. How are probabilities actually encoded? The DSL description seems to have only discrete decisions rather than probabilities. Training is perhaps covered in previous papers but there needs to be some discussion of how it works here. Section 2.5 does not do enough to explain how training works or how any measure of optimality is achieved. Given this model is quite a different hypothesis space from neural models or n-grams, looking and samples drawn from the model seems critical. The current experiments show it can score utterances relatively well but it would be very interesting if the model can sample more structured samples than neural approaches (for example long-range syntax constraints like brackets)", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the suggestions . In the Appendix , we clarify the training procedure as well as how the probabilistic model is built . It is true the model is based on discrete and deterministic decisions obtained by executing the learned program . This is also true for the probabilistic model which uses maximum likelihood estimation based on counting ( as in n-gram models ) . It is an interesting future work direction to extend to model with continuous word representations as done in neural networks ."}, "1": {"review_id": "ry_sjFqgx-1", "review_text": "The authors propose a method for language modeling by first generating a program from a DSL, then learning the count-based parameters of that program. Pros include: The proposed method is innovative and highly different from standard LSTM-based approaches of late. The model should also be much quicker to apply at query time. Strong empirical results are obtained on modeling code, though there is some gap between the synthesis method and neural methods on the Hutter task. A detailed description of the language syntax is provided. Cons/suggestions: - The synthesis procedure using MCMC is left very vague, even though being able to make this procedure efficient is one of the key questions. - The work builds on work from the PL literature; surely the related work could also be expanded and this work better put in context. - More compact/convincing examples of human interpretability would be helpful. Other comments - Training time evaluation in Table 1 should give basic information such as whether training was done on GPU/CPU, CPU specs, etc. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the suggestions , we added a thorough Appendix that provides all details . We also included details about the used hardware in our experimental setup in the updated version . All of our experiments were performed on a machine with Intel ( R ) Xeon ( R ) CPU E5-2690 with 14 cores . All training times are reported for parallel training on CPU . Using GPUs for training of the neural networks is likely to provide additional improvement in training time ."}, "2": {"review_id": "ry_sjFqgx-2", "review_text": "This paper introduces a novel method for language modeling which is suitable for both modeling programming language as well as natural language. The approach uses a program synthesis algorithm to search over program space and uses count-based estimation of the weights of the program. This is a departure from neural network-based approaches which rely on gradient descent, and thus are extremely slow to estimate. Count-based method such as regular n-gram models suffer because of their simplicity, i.e. not being able to model large context, and scaling badly as context increases. The proposed approach synthesizes programs using MCMC which learn context-sensitive probabilities using count-based estimation, and thus is both fast and able to model long-range context. Experiments on a programming language datasets, the linux kernel corpus, show that this method is vastly better than both LSTM and n-gram language models. Experiments on the Wikipedia corpus show that the method is competitive, but not better, to SOTA models. Both estimation and query time are significantly better than LSTM LMs, and competitive to n-gram LMs. It's debatable whether this paper is suitable for ICLR, due to ICLR's focus on neural network-based approaches. However, in the interest of diversity and novelty, such \"outside\" papers should be accepted to ICLR. This paper is likely to inspire more research into fusion of program synthesis and machine learning methods, which was a popular theme at NIPS 2016. *Pros* 1. Novel approach. 2. Good results. *Cons* 1. Some significant algorithmic details are not included in the paper. They should at least be included in an appendix for comprehensiveness. *Comments* 1. Please include n-gram results in the table for Wikipedia results.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Given the limits on the length of the submission it was not possible to include all details . However , as suggested by all reviewers , we added an Appendix describing all relevant details of the synthesis , formal semantics and how the probability distributions are built . We also updated Table 2 in the main paper to include results for n-gram ( 1.94 BPC ) ."}}