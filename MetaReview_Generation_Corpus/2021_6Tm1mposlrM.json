{"year": "2021", "forum": "6Tm1mposlrM", "title": "Sharpness-aware Minimization for Efficiently Improving Generalization", "decision": "Accept (Spotlight)", "meta_review": "The paper proposes to minimize the loss while regularizing its sharpness: so that the minimum will lie in a region with uniformly low loss.\nThe reviewers uniformly appreciated the paper. They have made a number of suggestion for improving the paper, which the authors should consider incorporating in their final version.\n\n", "reviews": [{"review_id": "6Tm1mposlrM-0", "review_text": "Motivated by the connection between the flatness of minima and its generalization ability , the authors propose Sharpness-aware Minimization ( SAM ) , which explicitly minimizes both loss value and loss sharpness during training deep neural networks . They find SAM improves generalization for a range of image classification tasks and provide robustness to label noise as well . They also introduce a new notion of sharpness named m-sharpness . Strength : * The paper is overall well written with clear motivation . * The experiments are comprehensive and the results show clear improvement over non-SAM approaches or previous SOTA . Weakness : * There is no clear definition of the \u201c sharpness \u201c that the algorithm tries to optimize . Given many existing definitions of the sharpness ( e.g. , [ 1 ] ) , it is not clear how the proposed measurement connects or differs with previous works . * My major concern is about the usage of hyperparameter $ \\rho $ : a ) The introduction of the dataset and model dependent hyperparameter $ \\rho $ and the need of grid-search before training makes the algorithm more tricky to work and sensitive to other hyperparameters and scale of $ w $ , e.g. , when weight decay is applied , the norm of $ w $ usually shrinks during training , and the same radius $ \\rho $ could be too large for a small scaled $ w $ at the end of training in comparison with the $ w $ at the beginning . This discrepancy would become larger when the number of epochs training gets larger . b ) The details for how to obtain the optimal $ rho $ is not quite clear , e.g. , smaller $ \\rho $ in sec 3.3 . An ablation study on the sensitivity of $ \\rho $ regarding different dataset , model and noisy level would be useful . c ) The wall-clock training time of the SAM method is not discussed . A comprehensive of the cost ( including hyperparameter search for $ rho $ . ) would be helpful to have for evaluating the complexity of the method . * The message conveyed in section 4.1 is not quite clear . Does each accelerator perform independent $ \\epsilon $ estimation ? Is $ epsilon $ obtained on each accelerator synchronized after their estimation ? Does it indicate the SAM training is done better in model-parallel in small batches rather than data-parallel with large batches ? Suggestions : 1 ) To avoid the scaling issue of $ \\rho $ , one suggestion would be considering optimizing the sharpness metric on the normalized loss landscape as described in [ 2 ] . In Figure 1 , the authors adopt [ 2 ] for comparing the landscape of minimas obtained by non-SAM and SAM , so it might be intuitive to optimize this normalized sharpness directly , in which $ \\rho $ can be fixed and random direction is sufficient ? 2 ) The benefit of flatness to the robustness to label noise is not well discussed . What is the performance when the label noise is over 90 % or even 100 % . Eventually all models should not generalize given 100 % corruption but it would be interesting to know where the limit of SAM is . Minor : * Some figures are not well described , e.g. , the meaning of Figure 1 left is not quite clear . Figure 2 is not intuitive as the loss contour value is not clear . It is not straightforward to know why w_ { t+1 } ^ { SAM } is a better or \u201c flatter \u201d move . The notion $ w_ { adv } $ is also not defined anywhere . [ 1 ] Keskar et al , On large-batch training for deep learning : Generalization gap and sharp minima , ICLR 2017 [ 2 ] Li et al , Visualizing the Loss Landscape of Neural Nets , NIPS 2018 After Rebuttal Thanks for the detailed reply and additional experiments . I increased my score accordingly and I hope the authors could further address following issues : - While the results in C.3 shows default $ \\rho $ improves over SGD on most experiments ( may also add SVHN and Fashion ) , I can still see its sensitivity to datasets , architecture , noise level and number of accelerators as shown in Table 6 , 7 , 8 and Fig.3.For example , 0.05 is not close to optimal with labe noise 20 % ~60 % in Table 8 . It is unclear whether $ \\rho $ is robust to other hyperparameter changes ( e.g. , weight decay that controls weight scales ) . So an ablation study on the sensitivity of $ \\rho $ and further explanation would be necessary and much valuable for practitioners . - It would be also helpful if the authors can provide more details about how to get the flat minima of Fig .1 ( right ) when optimizing deep non-residual networks , such as $ \\rho $ and other hyperparameters . - Minor : Table 8 should be validation errors rather than accuracy .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your encouraging and valuable feedback . Below , we address your comments : # # # Definition of sharpness : In the paragraph after Theorem 1 , we note that * The term in square brackets captures the sharpness of $ L_S $ at $ w $ by measuring how quickly the training loss can be increased by moving from $ w $ to a nearby parameter value ; this sharpness term [ ... ] * , giving the definition of the sharpness that we use . The same definition of sharpness is used in the theorem : as we defined \u201c sharpness \u201d as $ max_\\ { ||\\epsilon||_2\\leq \\rho } L ( w+eps ) - L ( w ) $ , theorem 1 can read \u201c $ L_D < = L_s $ + sharpness + h ( ... ) . We have reworded the explanation of theorem 1 to make it more explicit that this is how we define sharpness . # # # Shrinking of $ w $ While we agree that that could in principle occur , our empirical results ( including several state of the art results on widely-studied benchmarks ) unequivocally show that using a value of $ \\rho $ that is constant during training is sufficient to significantly increase the generalization of the learned model ( all experiments hold $ \\rho $ constant throughout training ) . Of course , modifying $ \\rho $ during training could potentially provide yet more gains ; we believe that studying this possibility would be an excellent direction for future work . Regarding the scaling of the loss , the reviewer write : * To avoid the scaling issue of \u03c1 , one suggestion would be considering optimizing the sharpness metric on the normalized loss landscape as described in [ 2 ] . In Figure 1 , the authors adopt [ 2 ] for comparing the landscape of minimas obtained by non-SAM and SAM , so it might be intuitive to optimize this normalized sharpness directly , in which \u03c1 can be fixed and random direction is sufficient ? * : This is a great suggestion and we agree that there is probably some invariants that we could leverage in order to make the tuning of $ \\rho $ even easier . Because the optimal value of $ \\rho $ says something about the geometry of the loss landscape , we believe this is a very interesting direction for future research . We however don \u2019 t believe that random directions are promising ; for example , Figure 6 in the appendix shows that random directions are inferior to adversarial directions in the setting considered . # # # About the tuning of $ \\rho $ Apologies for the lack of clarity . For the cifar experiments , we perform a standard hyper-parameter search on $ \\rho $ ( from 3.1 : _we tune via a grid search over { 0.01,0.02,0.05,0.1,0.2,0.5 } using 10 % of the training set as a validation set_ ) . All other hyper-parameters are held constant when tuning $ \\rho $ ( such that we tune the learning rate and weight decay first , for the SGD baseline , then we only tune $ \\rho $ for SAM ) . For the Imagenet experiments , we searched for the optimal $ \\rho $ using the smallest model ( Resnet-50 ) trained for only 100 epochs ( from 3.1 : _we use $ \\rho $ = 0.05 determined via a grid search on ResNet-50 trained for 100 epochs_ ) . We then used the same value of $ \\rho $ for all our imagenet experiments . For the fine tuning experiments , we also grid-searched $ \\rho $ for the Efficientnet-b7 model in the same fashion , using the validation set when available . We then used the best value of $ \\rho $ found for Efficientnet-l2 , without additional tuning . For the noisy label experiment , we also found $ \\rho $ by cross-validation . Feel free to check the scores on the validation set ( table 8 of the revised paper ) if that is of interest . SAM introduces only a single additional scalar-valued hyperparameter , $ \\rho $ , and our results show that $ \\rho $ can be simply and effectively set via a standard grid search using a validation set . This means that setting the value of SAM \u2019 s hyperparameter ( $ \\rho $ ) is identical to the means by which hyperparameters are commonly set for a wide variety of other widely used and impactful procedures , such as dropout ( which requires tuning dropout probability ) , MixUp ( which requires tuning the mixing hyperparameter alpha ) , and even SGD ( which requires tuning learning rate ) . For instance , for the noisy label experiments , all of the methods to which we compare SAM similarly require their own hyperparameters to be tuned . However we understand the concerns of the reviewer and agree that a big sensitivity to the choice of hyper-parameters would make a method less easy to use . * * To demonstrate that SAM performs even when $ \\rho $ is not finely tuned , we compiled the table for the cifar experiment , the imagenet experiment , and the fine tuning experiment using $ \\rho $ =0.05 everywhere * * and added those to our rebuttal . These results can be seen in appendix C.3 and show that * * SAM is superior to SGD even when $ \\rho $ is set to a default value . * *"}, {"review_id": "6Tm1mposlrM-1", "review_text": "# # Summary This paper proposes and empirically evaluates SAM , an optimization method that is designed to seek out regions of uniformly low training loss . The method is derived from a bound on the generalization performance of parameters $ w $ in terms of the maximal training loss in a region around $ w $ . After various approximations , minimizing this upper bound gives rise to a simple method , which first performs a ( normalized ) gradient ascent step ; computes the gradient at that perturbed location ; and uses that gradient to update the weights . The empirical evaluation shows that SAM improves generalization performance across a wide range of settings . # # Rating The paper is well-structured and easy to read . The proposed method is motivated from a PAC-Bayesian generalization bound , but involves approximations which are only vaguely justified . The empirical evaluation is extensive and rigorous . Given its simplicity , scalability , and the convincing results , I believe that this method could have significant impact . With a small exception ( see below ) all derivations are , to the best of my knowledge , mathematically sound . Overall , this is a good paper and I recommend acceptance , but there are various aspects that could be improved . If these are addressed during the rebuttal , I will consider increasing my score . # # Major Comments 1 ) The proposed method is motivated by a generalization bound , but this bound is never evaluated numerically . Prior work has shown that many generalization bounds are numerically vacuous in deep learning settings . The bound in Theorem 1 should be really straight-forward to evaluate after the end of training and I think this would be a vital addition to this paper . The paper shows convincing practical benefits of SAM , but it is important to know whether this benefit actually stems from the ( approximate ) minimization of a non-vacuous bound . This is my single biggest concern with this work . 2 ) The derivation of the SAM update relies on various approximations to turn the intractable bi-level optimization problem of Eq . ( 1 ) into a practical method . My concern here is that the paper does not even attempt to justify these approximations and/or to discuss their limitations and possible pitfalls . I \u2019 m not saying that these approximations are not justified ; I just think that a bit of discussion would significantly strengthen the paper . A few thoughts : a ) The linear approximation that facilitates the closed-form solution of the inner optimization problem ( top of page 4 ) is really drastic . It would be accurate if and only if the curvature in a $ \\rho $ -sized region around w would be negligibly small . That , however , is exactly the type of region the method wants to find , so this is somewhat circular . Contrasting the first-order with a second-order approximation may be a useful exercise . An interesting ablation would be to solve the inner loop optimization in Eq . ( 1 ) more accurately , say , by performing multiple iterations of ( projected ) gradient descent . ( When dropping the second-order term , this wouldn \u2019 t be exceedingly costly and could provide further insight . ) b ) Later on , the \u201c second order \u201d term , which derives through the solution $ \\hat { \\epsilon } $ of the inner-loop optimization , is dropped . The authors write that this is done to \u201c further accelerate the computation \u201d and \u201c does not adversely affect model training and in fact improves numerical stability \u201d . However , as shown in Appendix C.3 , dropping the second-order term yields a quite considerable * improvement * . It always makes me a bit nervous , when a ( relatively crude ) approximation works better than the quantity you actually want to compute . I don \u2019 t think it is adequate to sweep this under the rug with a vague reference to numerical stability . At least , I thank that ( i ) the fact should be stated clearly in the main text , and ( ii ) the reference to numerical stability should be marked as speculation , unless there \u2019 s evidence to back it up . 3 ) Even if the approximations involved are only vaguely justified , the SAM update itself makes a lot of intuitive sense , and I think the reader might benefit from discussing that in a bit more detail . For example , one simple observation would be that , for a sharp minimum whose basin of attraction is of radius smaller than rho , the ascent step of SAM ( by virtue of the normalisation to length rho ) would take you outside of that basin . If the basin of attraction is larger than rho , the SAM update stays inside . Maybe this intuition could be formalized ? 4 ) Connection to Entropy-SGD : a ) Is it really fair to characterize EntropySGD as \u201c only suitable to small models and datasets \u201d ? Of course , it requires a number L of additional gradient steps to obtain an MC-estimate of the gradient of the local entropy . But , if my understanding is correct , these could be parallelized . The original paper uses L=20 but , to my knowledge , nobody has tested the ( lower ) limits of that number . If L < =5 does something useful , it wouldn \u2019 t be too outlandish in terms of computational cost . b ) On that note : It would have been great to see an empirical comparison to EntropySGD for at least one of the experiments . c ) Looking more closely at the proof of the generalization bound , the term involving maximization over a local region actually originates from a bound on the * integral * over that region , which makes the connection to EntropySGD even closer . Is it fair to say that the maximization step in SAM is actually just a clever and cheap surrogate for integrating the gradient the over the local region as done in EntropySGD ? d ) An interesting piece of related work that you might want to add is [ 1 ] , which shows that EntropySGD ( in a specific sense ) also optimizes a generalization bound . 5 ) You clearly show the benefits of SAM in pushing the state-of-the-art , which is fantastic ! Personally , I would have also valued experiments that are more illustrative and maybe test the limits of the method : a ) What does SAM do in a convex setting , say , a logistic regression on some ( maybe synthetic ) dataset that is easily overfit ? Does it converge to the unique minimizer of the training loss ? Does it plateau before reaching that ? b ) What happens if you apply SAM in the setting of [ 2 ] with random labels assigned to images ? Will it converge to a point that closely fits each ( nonsense ) training sample ? c ) With regards to the finding on m-sharpness , it would be great to see a small-scale experiments with full-batch optimization . 6 ) The finding on m-sharpness is intriguing , though somewhat counter to the motivation for SAM laid out in the paper . The results in Figure 3 make me wonder , whether SAM would even gives practical improvements with large m ? This is an important question , but I totally agree with the authors decision to leave it for future work . # # Minor Comments 7 ) SAM is motivated from the generalization bound in Theorem 1 . The bound includes a term $ h ( ||w||^2 / \\rho^2 ) $ , which is later replaced with a simple L2 regularization term with coefficient lambda . From the Theorem , that coefficient $ \\lambda $ should be coupled with the neighborhood size $ \\rho $ in an inversely proportional manner . Of course , for a fixed choice of $ \\rho $ , this can be subsumed into the regularization parameter lambda . However , it would make a difference when comparing various choices for rho , as is done in the experiments . Did you keep the regularization parameter lambda constant in those experiments or did it vary with rho , as suggested by the Theorem ? Furthermore , it would be interesting to know to what extent SAM depends on that L2 regularization . Did all experiments use L2 regularization and did you , by any chance , test SAM \u2019 s performance without the regularization parameter ? 8 ) The bib file could really need some love : a ) You cite arXiv versions of of multiple papers that have been published at peer-reviewed venues . b ) Capitalize properly : \u201c nesterov momentum \u201d , \u201c pac-bayesian \u201d , \u201c journal of machine learning research \u201d , etc\u2026 c ) Cite identical venues consistently . 9 ) An interesting connection ( or non-connection ) to discuss in the paper would be adversarial training . Some of these methods perform an adversarial perturbation step to the * data * , followed by a weight update computed on that perturbed data . This is an interesting parallel to SAM , where the first step is sort of an adversarial perturbation of the weights . 10 ) If I am not mistaken , there is a minor flaw in the proof of Theorem 1 . The very last equation gives a high-probability bound on the norm of $ ||\\epsilon||^2 $ , which is then plugged into the high-probability generalization bound of Eq . ( ) .Both bounds hold with probability 1-delta individually , but that does not imply that the combined bound holds with probability 1-delta . However , that should easily be fixed with a slight adaptation of the constants . # # Typos / Style - At least according to some style guides , I think that you should write \u201c Sharpness-Aware Minimization \u201d instead of \u201c Sharpness-aware Minimization \u201d . - Capitalize references to sections , equations , figures : \u201c in Appendix C.3 \u201d , \u201c using Equation 3 \u201d , et cetera\u2026 - Typo \u201c opefrations \u201d near the bottom of page 4 - Footnote markers that refer to an entire sentence go after the punctuation mark ( e.g. , for footnotes 3 and 5 ) . - In Table 3 , the boldface is assigned to the wrong method in the Stanford Cars row . # # References [ 1 ] Dziugaite , G. K. , & Roy , D. ( 2018 ) . Entropy-SGD optimizes the prior of a PAC-Bayes bound : Generalization properties of Entropy-SGD and data-dependent priors . In International Conference on Machine Learning ( pp.1377-1386 ) . PMLR . [ 2 ] Zhang , C. , Bengio , S. , Hardt , M. , Recht , B. , & Vinyals , O . ( 2016 ) .Understanding deep learning requires rethinking generalization . arXiv preprint arXiv:1611.03530 . # # Update after Rebuttal Thanks for the detailed replies to my questions and comments . I think the paper has been improved substantially and I have increased my rating . Congratulations on the good work !", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "* * What happens if you apply SAM in the setting of [ 2 ] with random labels assigned to images ? Will it converge to a point that closely fits each ( nonsense ) training sample ? * * Our experiments from section 3.3 explore this ( though not using 100 % random labels , which could also be interesting ) , although in the paper we only report the test accuracy . With respect to training accuracy , what we observe is that when using SAM , the network interpolates the \u201c clean \u201d part of the training dataset , while not learning the corrupted labels ( it is also worth noting that the same phenomenon can be obtained with other regularizers , such as Mixup with high alpha ) . For SAM , this might suggest that the minima that fit random labels have a smaller width than the minima that fit the true labels , although this is speculative at this point and we decided to leave this out of the final manuscript . We however believe that there is some very interesting research to be done on the intersection between SAM and semi-supervised / noisy or random labels , and that this could provide more insight into the geometry of the loss landscape . * * With regards to the finding on m-sharpness , it would be great to see a small-scale experiments with full-batch optimization . * * Given the results of the m-sharpness experiment , and the fact that non stochastic gradient descent performs poorly for deep networks , we believe that full batch optimization would not perform well . It would be interesting to see how much SAM manages to mitigate the generalization issue of full batch gradient descent though . * * The finding on m-sharpness is intriguing , though somewhat counter to the motivation for SAM laid out in the paper . * * We initially thought the same thing as the reviewer , as it seemed that branching away from the definition of sharpness used in the theorem ( m=dataset size ) makes the connection between SAM and the generalization bound more tenuous . However , we now believe that the results presented in the right side of figure 3 ( Predictive power of m-sharpness for the generalization gap ) changes this perspective , and shows that there might be better bounds to be discovered that rely on a \u201c per example sharpness \u201d . In that sense , SAM could be an argument to explore new types of sharpness-based bounds . * * From the Theorem , that coefficient \u03bb should be coupled with the neighborhood size \u03c1 in an inversely proportional manner . * * This is a great point . In our experiments , we wanted to be i ) mimicking the way most practitioners would tune the parameters and ii ) being conservative in our choice of rho , avoiding large gridsearch on rho x lambda x learning rate for instance . As a result , we first tune the baseline to find the optimal learning rate and the weight decay ( jointly ) , and only after that tune rho when using SAM . It is however true that we could have used lambda/rho instead of lambda when searching for rho , and it would be interesting to see the impact of this on the results . * * An interesting connection ( or non-connection ) to discuss in the paper would be adversarial training . * * Some of these methods perform an adversarial perturbation step to the data - > SAM can indeed been described as a parameter space analogue to the fast gradient sign method ( FGSM ) ( although the FGSM projection is done on the l_inf sphere and not the l_2 sphere ) , as they both solve min max problems . It would be interesting to see how networks performs when mixing both , considering adversarial attacks on both the input and the weights , but we leave this to future work . * * The bib file could really need some love . * * You are correct , we will clean it in the next version of the paper . * * If I am not mistaken , there is a minor flaw in the proof of Theorem 1 . * * We added a corrected version of the theorem . * * In Table 3 , the boldface is assigned to the wrong method in the Stanford Cars row . * * The boldface is actually correct , it is the score that should read 5.0 instead of 5.3 in this version of the paper . We will correct the table . References : [ 1 ] Exploring the Vulnerability of Deep Neural Networks : A Study of Parameter Corruption ( https : //arxiv.org/pdf/2006.05620.pdf ) [ 2 ] Regularizing Neural Networks via Adversarial Model Perturbation ( https : //arxiv.org/abs/2010.04925 ) < - maybe not needed [ 3 ] AdaNet : Adaptive Structural Learning of Artificial Neural Networks ,"}, {"review_id": "6Tm1mposlrM-2", "review_text": "- Overview This paper proposed a learning algorithm using the idea of flatness . Basically , the algorithm was constructed by a first-order Taylor approximation of flatness and using only the main term . The developed algorithm uses the gradient of the loss with steepest direction when updating the parameters . The proposed algorithm is simple and fast to work . It is also easy to incorporate into existing networks and algorithms and has updated accuracy in many models . - Comments . This is an interesting study , as it is the first application of flatness to a practical algorithm . It is also excellent in that it is easy to implement and immediately applicable . The performance is also nice . The validity of this approximation is a concern . When the authors perform a first-order Taylor approximation , they are looking only at the gradient of the parameter to determine the adversarial direction . So , if the loss function is very spiky , there is a concern that the approximation will not work because the local gradient alone will not find the appropriate direction . The nice networks used in their experiments is supposed to use smoothing techniques for the loss surface , such as the batch normalization and the skip connections , but the concern is whether the proposed method works properly in neural networks with a more less smooth loss surface . To address this concern , my suggestion is to use the algorithm in without the Taylor approximation make comparisons on a more primitive network with a less smooth loss surface . That way , users would feel more comfortable using the method .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We are glad that you liked our paper . Below , we address your only concern with the paper . If you find our response adequate , we would appreciate it if you increase your score . _The validity of this approximation is a concern . [ ... ] So , if the loss function is very spiky , there is a concern that the approximation will not work because the local gradient alone will not find the appropriate direction_ : We agree that there is no worst-case guarantee for this approximation for arbitrary networks . However , we follow here in the footsteps of algorithms such as SGD in the context of training neural networks . SGD , for example , arises from principled theoretical underpinnings , but the theory largely does not apply when using SGD to train neural networks . Indeed , there is no known guarantee that SGD will be able to find a high-quality local minimum of a non-convex loss landscape . Nonetheless , SGD is widely considered to be a valuable method due to its empirically observed effectiveness in finding neural network parameters of high quality . Analogously , we motivate SAM from principled theoretical underpinnings , and while the practical application of SAM ( Algorithm 1 ) involves approximations that we haven \u2019 t ( yet ) fully analyzed theoretically ( a great direction for future work ! ) , our empirical results show that these approximations yield an effective algorithm of substantial practical utility . Furthermore : - As the reviewer noted , we achieve state of the art results with modern architectures ; the approximation unequivocally works well for a range of settings . - The current literature also suggests that the smoothness of the loss landscape is necessary to obtain good results with SGD ( as the reviewer pointed out , this is a motivation behind recurrent connections ) . As a result , it seems that SAM will benefit from all the \u201c tricks \u201d researchers have developed to make SGD work in the first place . - Our work can be connected to adversarial training , except that we perform adversarial attacks on the weight space instead of the input space . One common method for adversarial training is the fast gradient sign method ( FGSM ) , which is the equivalent of SAM in the input space ( although the projection is done on the l_inf sphere and not the l_2 sphere ) . This method is widely used due to its empirical efficacy , even if there is also no guarantee that the adversarial attack is the best possible . _The nice networks used in their experiments is supposed to use smoothing techniques for the loss surface , such as the batch normalization and the skip connections , but the concern is whether the proposed method works properly in neural networks with a more less smooth loss surface . To address this concern , my suggestion is to use the algorithm in without the Taylor approximation make comparisons on a more primitive network with a less smooth loss surface . That way , users would feel more comfortable using the method_ : We added a new appendix analyzing SAM when several steps of projected gradient ascent are used to compute epsilon ( thereby computing the inner maximization with higher fidelity ) . The conclusion of this additional experiment is that i ) for most of the training , one projected gradient step ( i.e. , the Taylor approximation ) is enough to get a good approximation of the epsilon found without the Taylor approximation , ii ) this approximation becomes weaker near convergence , where doing several iterations of projected gradient ascent yields a better epsilon , and iii ) the test accuracy is not strongly affected by the number of project gradient iterations , thus the Taylor approximation is enough in practical settings . Additionally , the loss landscape presented on the right-hand side of Figure 1 is obtained for a CNN without skip connections . In this representation of the loss , it is possible to see that SAM actually managed to find a wider minimum in the absence of skip connections , and that the minimum found when not using SAM is very \u201c spiky \u201d and not smooth . Similarly , the Hessian spectra presented in Section 4.2 were computed for a WideResNet40-10 trained on CIFAR-10 with and without SAM , without using batch normalization . These spectra again confirm that SAM finds a lower-curvature minimum of the loss landscape . We additionally run further experiments for CNNs without batch normalization and without residual connections ( models from Frankle and Carbin , https : //arxiv.org/abs/1803.03635 ) . We used the same hyperparameters as for the paper \u2019 s WideResnet-28x10 experiments on CIFAR and obtain the following results : Conv-2 : * * 14.5 * * ( SAM ) , 15.2 ( SGD ) ; Conv-4 : * * 8.2 * * ( SAM ) , 9.1 ( SGD ) ; Conv-6 : * * 6.8 * * ( SAM ) , 7.8 ( SGD ) ; As seen in these results , SAM again performs well in the absence of batch normalization or residual connections ."}, {"review_id": "6Tm1mposlrM-3", "review_text": "# # Summary # # The paper proposes a modified loss function for supervised learning , in which the original loss at w is replaced with a maximum of the loss in a small p-norm ball around w. An approximate way to compute gradients for this loss is presented , and evaluated in high detail on a variety of supervised learning problems where it is shown to consistently improve the overall generalization error . Furthermore , based on PAC-Bayes theory , a generalization bound for learning under that loss is presented . # # Explanation of Rating # # The main strengths of the paper are the simplicity and convincing evaluation of the method . Another strength is the sound theoretical generalization bound . The paper is also very well written , and I therefore clearly recommend acceptance . Perhaps a weakness of the paper is the lack of a `` broader/high-level perspective '' on the method ( see detailed comment # 1 ) and its comparison to variational inference methods which also optimize PAC-Bayes bounds ( see detailed comment # 2 ) . # # Detailed Comments # # 1 . In the past , Gaussian convolution smoothings of loss functions have been considered a lot in the context of homotopy continuation methods or variational inference . To me , the proposed loss function \\max_\\eps L ( w + \\eps ) - \\delta_ { |\\eps| < \\rho } can be seen as a convolution of the loss , but on the tropical semiring ( max , + ) with the convolution kernel being the indicator function \\delta_ { |\\eps| < \\rho } . Such types of convolution have been heavily studied in the field of convex analysis , where they are known under the name infimal convolution / epi-addition . This opens up the question on why this specific convolution kernel has been chosen . The practical results in this paper are quite strong and have been elusive so far for variational inference and Gaussian smoothing methods , even though they have been around for a long time . Do these good results mainly stem from the efficiently implementable algorithm , or are they more due to favorable geometrical properties of the loss due to performing the convolution in the tropical geometry ? 2.What is the main advantage of the proposed approach over a variational inference ( VI ) method ( e.g.with mean-field Gaussian approximation ) to minimize the PAC-Bayes bound ? In variational inference , the free parameter \\rho is also be optimized by minimizing the right-hand side of the PAC-Bayes bound . Have you tried minimizing over \\rho , or perhaps consider a diagonal approximation of \\rho ? Minor comments / typos : - p.12 `` Adding a Gaussian perturbation should increase the test error '' - > `` Adding a Gaussian perturbation should not decrease the test error . '' This is expected to hold in practice at a minimum . But it is not clear if it holds for any w. - p.12 Typo : `` Then we KL divergence '' - > `` Then the KL divergence '' . - The steps ( 12 ) and ( 13 ) in the proof are not easy to follow . In particular , it was unclear to me what meant with `` each bound '' before Eq.12 , and why the bounds should hold with this specific choice of probability . Furthermore , I could n't follow what happens from ( 12 ) to ( 13 ) . - I did not notice where the assumption ||w|| > = 1 is used , perhaps it can be mentioned in the proof . - \\lambda is another hyperparameter , but it is not mentioned later on how it is chosen .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your encouraging and thoughtful review . Below we address your comments : 1 ) Thanks for pointing to the infimal convolution view in convex analysis . This is a very interesting reformulation and we look forward to exploring its implications . About your question , we believe that while the objective function is important , the specific choice of the training algorithm has a great impact on the final performance . 2 ) We believe that SAM improves over VI methods in terms of final performance ( and perhaps even efficiency ) . Even though we have not made a formal comparison in the paper , we have tried optimizing PAC-Bayesian bounds in the past and have not observed such significant benefits . In our experiments , instead of optimizing \\rho directly , we treat it as a hyperparameter . Minor comments / typos : Thanks for pointing these out . We will fix them all in the final revision ."}], "0": {"review_id": "6Tm1mposlrM-0", "review_text": "Motivated by the connection between the flatness of minima and its generalization ability , the authors propose Sharpness-aware Minimization ( SAM ) , which explicitly minimizes both loss value and loss sharpness during training deep neural networks . They find SAM improves generalization for a range of image classification tasks and provide robustness to label noise as well . They also introduce a new notion of sharpness named m-sharpness . Strength : * The paper is overall well written with clear motivation . * The experiments are comprehensive and the results show clear improvement over non-SAM approaches or previous SOTA . Weakness : * There is no clear definition of the \u201c sharpness \u201c that the algorithm tries to optimize . Given many existing definitions of the sharpness ( e.g. , [ 1 ] ) , it is not clear how the proposed measurement connects or differs with previous works . * My major concern is about the usage of hyperparameter $ \\rho $ : a ) The introduction of the dataset and model dependent hyperparameter $ \\rho $ and the need of grid-search before training makes the algorithm more tricky to work and sensitive to other hyperparameters and scale of $ w $ , e.g. , when weight decay is applied , the norm of $ w $ usually shrinks during training , and the same radius $ \\rho $ could be too large for a small scaled $ w $ at the end of training in comparison with the $ w $ at the beginning . This discrepancy would become larger when the number of epochs training gets larger . b ) The details for how to obtain the optimal $ rho $ is not quite clear , e.g. , smaller $ \\rho $ in sec 3.3 . An ablation study on the sensitivity of $ \\rho $ regarding different dataset , model and noisy level would be useful . c ) The wall-clock training time of the SAM method is not discussed . A comprehensive of the cost ( including hyperparameter search for $ rho $ . ) would be helpful to have for evaluating the complexity of the method . * The message conveyed in section 4.1 is not quite clear . Does each accelerator perform independent $ \\epsilon $ estimation ? Is $ epsilon $ obtained on each accelerator synchronized after their estimation ? Does it indicate the SAM training is done better in model-parallel in small batches rather than data-parallel with large batches ? Suggestions : 1 ) To avoid the scaling issue of $ \\rho $ , one suggestion would be considering optimizing the sharpness metric on the normalized loss landscape as described in [ 2 ] . In Figure 1 , the authors adopt [ 2 ] for comparing the landscape of minimas obtained by non-SAM and SAM , so it might be intuitive to optimize this normalized sharpness directly , in which $ \\rho $ can be fixed and random direction is sufficient ? 2 ) The benefit of flatness to the robustness to label noise is not well discussed . What is the performance when the label noise is over 90 % or even 100 % . Eventually all models should not generalize given 100 % corruption but it would be interesting to know where the limit of SAM is . Minor : * Some figures are not well described , e.g. , the meaning of Figure 1 left is not quite clear . Figure 2 is not intuitive as the loss contour value is not clear . It is not straightforward to know why w_ { t+1 } ^ { SAM } is a better or \u201c flatter \u201d move . The notion $ w_ { adv } $ is also not defined anywhere . [ 1 ] Keskar et al , On large-batch training for deep learning : Generalization gap and sharp minima , ICLR 2017 [ 2 ] Li et al , Visualizing the Loss Landscape of Neural Nets , NIPS 2018 After Rebuttal Thanks for the detailed reply and additional experiments . I increased my score accordingly and I hope the authors could further address following issues : - While the results in C.3 shows default $ \\rho $ improves over SGD on most experiments ( may also add SVHN and Fashion ) , I can still see its sensitivity to datasets , architecture , noise level and number of accelerators as shown in Table 6 , 7 , 8 and Fig.3.For example , 0.05 is not close to optimal with labe noise 20 % ~60 % in Table 8 . It is unclear whether $ \\rho $ is robust to other hyperparameter changes ( e.g. , weight decay that controls weight scales ) . So an ablation study on the sensitivity of $ \\rho $ and further explanation would be necessary and much valuable for practitioners . - It would be also helpful if the authors can provide more details about how to get the flat minima of Fig .1 ( right ) when optimizing deep non-residual networks , such as $ \\rho $ and other hyperparameters . - Minor : Table 8 should be validation errors rather than accuracy .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your encouraging and valuable feedback . Below , we address your comments : # # # Definition of sharpness : In the paragraph after Theorem 1 , we note that * The term in square brackets captures the sharpness of $ L_S $ at $ w $ by measuring how quickly the training loss can be increased by moving from $ w $ to a nearby parameter value ; this sharpness term [ ... ] * , giving the definition of the sharpness that we use . The same definition of sharpness is used in the theorem : as we defined \u201c sharpness \u201d as $ max_\\ { ||\\epsilon||_2\\leq \\rho } L ( w+eps ) - L ( w ) $ , theorem 1 can read \u201c $ L_D < = L_s $ + sharpness + h ( ... ) . We have reworded the explanation of theorem 1 to make it more explicit that this is how we define sharpness . # # # Shrinking of $ w $ While we agree that that could in principle occur , our empirical results ( including several state of the art results on widely-studied benchmarks ) unequivocally show that using a value of $ \\rho $ that is constant during training is sufficient to significantly increase the generalization of the learned model ( all experiments hold $ \\rho $ constant throughout training ) . Of course , modifying $ \\rho $ during training could potentially provide yet more gains ; we believe that studying this possibility would be an excellent direction for future work . Regarding the scaling of the loss , the reviewer write : * To avoid the scaling issue of \u03c1 , one suggestion would be considering optimizing the sharpness metric on the normalized loss landscape as described in [ 2 ] . In Figure 1 , the authors adopt [ 2 ] for comparing the landscape of minimas obtained by non-SAM and SAM , so it might be intuitive to optimize this normalized sharpness directly , in which \u03c1 can be fixed and random direction is sufficient ? * : This is a great suggestion and we agree that there is probably some invariants that we could leverage in order to make the tuning of $ \\rho $ even easier . Because the optimal value of $ \\rho $ says something about the geometry of the loss landscape , we believe this is a very interesting direction for future research . We however don \u2019 t believe that random directions are promising ; for example , Figure 6 in the appendix shows that random directions are inferior to adversarial directions in the setting considered . # # # About the tuning of $ \\rho $ Apologies for the lack of clarity . For the cifar experiments , we perform a standard hyper-parameter search on $ \\rho $ ( from 3.1 : _we tune via a grid search over { 0.01,0.02,0.05,0.1,0.2,0.5 } using 10 % of the training set as a validation set_ ) . All other hyper-parameters are held constant when tuning $ \\rho $ ( such that we tune the learning rate and weight decay first , for the SGD baseline , then we only tune $ \\rho $ for SAM ) . For the Imagenet experiments , we searched for the optimal $ \\rho $ using the smallest model ( Resnet-50 ) trained for only 100 epochs ( from 3.1 : _we use $ \\rho $ = 0.05 determined via a grid search on ResNet-50 trained for 100 epochs_ ) . We then used the same value of $ \\rho $ for all our imagenet experiments . For the fine tuning experiments , we also grid-searched $ \\rho $ for the Efficientnet-b7 model in the same fashion , using the validation set when available . We then used the best value of $ \\rho $ found for Efficientnet-l2 , without additional tuning . For the noisy label experiment , we also found $ \\rho $ by cross-validation . Feel free to check the scores on the validation set ( table 8 of the revised paper ) if that is of interest . SAM introduces only a single additional scalar-valued hyperparameter , $ \\rho $ , and our results show that $ \\rho $ can be simply and effectively set via a standard grid search using a validation set . This means that setting the value of SAM \u2019 s hyperparameter ( $ \\rho $ ) is identical to the means by which hyperparameters are commonly set for a wide variety of other widely used and impactful procedures , such as dropout ( which requires tuning dropout probability ) , MixUp ( which requires tuning the mixing hyperparameter alpha ) , and even SGD ( which requires tuning learning rate ) . For instance , for the noisy label experiments , all of the methods to which we compare SAM similarly require their own hyperparameters to be tuned . However we understand the concerns of the reviewer and agree that a big sensitivity to the choice of hyper-parameters would make a method less easy to use . * * To demonstrate that SAM performs even when $ \\rho $ is not finely tuned , we compiled the table for the cifar experiment , the imagenet experiment , and the fine tuning experiment using $ \\rho $ =0.05 everywhere * * and added those to our rebuttal . These results can be seen in appendix C.3 and show that * * SAM is superior to SGD even when $ \\rho $ is set to a default value . * *"}, "1": {"review_id": "6Tm1mposlrM-1", "review_text": "# # Summary This paper proposes and empirically evaluates SAM , an optimization method that is designed to seek out regions of uniformly low training loss . The method is derived from a bound on the generalization performance of parameters $ w $ in terms of the maximal training loss in a region around $ w $ . After various approximations , minimizing this upper bound gives rise to a simple method , which first performs a ( normalized ) gradient ascent step ; computes the gradient at that perturbed location ; and uses that gradient to update the weights . The empirical evaluation shows that SAM improves generalization performance across a wide range of settings . # # Rating The paper is well-structured and easy to read . The proposed method is motivated from a PAC-Bayesian generalization bound , but involves approximations which are only vaguely justified . The empirical evaluation is extensive and rigorous . Given its simplicity , scalability , and the convincing results , I believe that this method could have significant impact . With a small exception ( see below ) all derivations are , to the best of my knowledge , mathematically sound . Overall , this is a good paper and I recommend acceptance , but there are various aspects that could be improved . If these are addressed during the rebuttal , I will consider increasing my score . # # Major Comments 1 ) The proposed method is motivated by a generalization bound , but this bound is never evaluated numerically . Prior work has shown that many generalization bounds are numerically vacuous in deep learning settings . The bound in Theorem 1 should be really straight-forward to evaluate after the end of training and I think this would be a vital addition to this paper . The paper shows convincing practical benefits of SAM , but it is important to know whether this benefit actually stems from the ( approximate ) minimization of a non-vacuous bound . This is my single biggest concern with this work . 2 ) The derivation of the SAM update relies on various approximations to turn the intractable bi-level optimization problem of Eq . ( 1 ) into a practical method . My concern here is that the paper does not even attempt to justify these approximations and/or to discuss their limitations and possible pitfalls . I \u2019 m not saying that these approximations are not justified ; I just think that a bit of discussion would significantly strengthen the paper . A few thoughts : a ) The linear approximation that facilitates the closed-form solution of the inner optimization problem ( top of page 4 ) is really drastic . It would be accurate if and only if the curvature in a $ \\rho $ -sized region around w would be negligibly small . That , however , is exactly the type of region the method wants to find , so this is somewhat circular . Contrasting the first-order with a second-order approximation may be a useful exercise . An interesting ablation would be to solve the inner loop optimization in Eq . ( 1 ) more accurately , say , by performing multiple iterations of ( projected ) gradient descent . ( When dropping the second-order term , this wouldn \u2019 t be exceedingly costly and could provide further insight . ) b ) Later on , the \u201c second order \u201d term , which derives through the solution $ \\hat { \\epsilon } $ of the inner-loop optimization , is dropped . The authors write that this is done to \u201c further accelerate the computation \u201d and \u201c does not adversely affect model training and in fact improves numerical stability \u201d . However , as shown in Appendix C.3 , dropping the second-order term yields a quite considerable * improvement * . It always makes me a bit nervous , when a ( relatively crude ) approximation works better than the quantity you actually want to compute . I don \u2019 t think it is adequate to sweep this under the rug with a vague reference to numerical stability . At least , I thank that ( i ) the fact should be stated clearly in the main text , and ( ii ) the reference to numerical stability should be marked as speculation , unless there \u2019 s evidence to back it up . 3 ) Even if the approximations involved are only vaguely justified , the SAM update itself makes a lot of intuitive sense , and I think the reader might benefit from discussing that in a bit more detail . For example , one simple observation would be that , for a sharp minimum whose basin of attraction is of radius smaller than rho , the ascent step of SAM ( by virtue of the normalisation to length rho ) would take you outside of that basin . If the basin of attraction is larger than rho , the SAM update stays inside . Maybe this intuition could be formalized ? 4 ) Connection to Entropy-SGD : a ) Is it really fair to characterize EntropySGD as \u201c only suitable to small models and datasets \u201d ? Of course , it requires a number L of additional gradient steps to obtain an MC-estimate of the gradient of the local entropy . But , if my understanding is correct , these could be parallelized . The original paper uses L=20 but , to my knowledge , nobody has tested the ( lower ) limits of that number . If L < =5 does something useful , it wouldn \u2019 t be too outlandish in terms of computational cost . b ) On that note : It would have been great to see an empirical comparison to EntropySGD for at least one of the experiments . c ) Looking more closely at the proof of the generalization bound , the term involving maximization over a local region actually originates from a bound on the * integral * over that region , which makes the connection to EntropySGD even closer . Is it fair to say that the maximization step in SAM is actually just a clever and cheap surrogate for integrating the gradient the over the local region as done in EntropySGD ? d ) An interesting piece of related work that you might want to add is [ 1 ] , which shows that EntropySGD ( in a specific sense ) also optimizes a generalization bound . 5 ) You clearly show the benefits of SAM in pushing the state-of-the-art , which is fantastic ! Personally , I would have also valued experiments that are more illustrative and maybe test the limits of the method : a ) What does SAM do in a convex setting , say , a logistic regression on some ( maybe synthetic ) dataset that is easily overfit ? Does it converge to the unique minimizer of the training loss ? Does it plateau before reaching that ? b ) What happens if you apply SAM in the setting of [ 2 ] with random labels assigned to images ? Will it converge to a point that closely fits each ( nonsense ) training sample ? c ) With regards to the finding on m-sharpness , it would be great to see a small-scale experiments with full-batch optimization . 6 ) The finding on m-sharpness is intriguing , though somewhat counter to the motivation for SAM laid out in the paper . The results in Figure 3 make me wonder , whether SAM would even gives practical improvements with large m ? This is an important question , but I totally agree with the authors decision to leave it for future work . # # Minor Comments 7 ) SAM is motivated from the generalization bound in Theorem 1 . The bound includes a term $ h ( ||w||^2 / \\rho^2 ) $ , which is later replaced with a simple L2 regularization term with coefficient lambda . From the Theorem , that coefficient $ \\lambda $ should be coupled with the neighborhood size $ \\rho $ in an inversely proportional manner . Of course , for a fixed choice of $ \\rho $ , this can be subsumed into the regularization parameter lambda . However , it would make a difference when comparing various choices for rho , as is done in the experiments . Did you keep the regularization parameter lambda constant in those experiments or did it vary with rho , as suggested by the Theorem ? Furthermore , it would be interesting to know to what extent SAM depends on that L2 regularization . Did all experiments use L2 regularization and did you , by any chance , test SAM \u2019 s performance without the regularization parameter ? 8 ) The bib file could really need some love : a ) You cite arXiv versions of of multiple papers that have been published at peer-reviewed venues . b ) Capitalize properly : \u201c nesterov momentum \u201d , \u201c pac-bayesian \u201d , \u201c journal of machine learning research \u201d , etc\u2026 c ) Cite identical venues consistently . 9 ) An interesting connection ( or non-connection ) to discuss in the paper would be adversarial training . Some of these methods perform an adversarial perturbation step to the * data * , followed by a weight update computed on that perturbed data . This is an interesting parallel to SAM , where the first step is sort of an adversarial perturbation of the weights . 10 ) If I am not mistaken , there is a minor flaw in the proof of Theorem 1 . The very last equation gives a high-probability bound on the norm of $ ||\\epsilon||^2 $ , which is then plugged into the high-probability generalization bound of Eq . ( ) .Both bounds hold with probability 1-delta individually , but that does not imply that the combined bound holds with probability 1-delta . However , that should easily be fixed with a slight adaptation of the constants . # # Typos / Style - At least according to some style guides , I think that you should write \u201c Sharpness-Aware Minimization \u201d instead of \u201c Sharpness-aware Minimization \u201d . - Capitalize references to sections , equations , figures : \u201c in Appendix C.3 \u201d , \u201c using Equation 3 \u201d , et cetera\u2026 - Typo \u201c opefrations \u201d near the bottom of page 4 - Footnote markers that refer to an entire sentence go after the punctuation mark ( e.g. , for footnotes 3 and 5 ) . - In Table 3 , the boldface is assigned to the wrong method in the Stanford Cars row . # # References [ 1 ] Dziugaite , G. K. , & Roy , D. ( 2018 ) . Entropy-SGD optimizes the prior of a PAC-Bayes bound : Generalization properties of Entropy-SGD and data-dependent priors . In International Conference on Machine Learning ( pp.1377-1386 ) . PMLR . [ 2 ] Zhang , C. , Bengio , S. , Hardt , M. , Recht , B. , & Vinyals , O . ( 2016 ) .Understanding deep learning requires rethinking generalization . arXiv preprint arXiv:1611.03530 . # # Update after Rebuttal Thanks for the detailed replies to my questions and comments . I think the paper has been improved substantially and I have increased my rating . Congratulations on the good work !", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "* * What happens if you apply SAM in the setting of [ 2 ] with random labels assigned to images ? Will it converge to a point that closely fits each ( nonsense ) training sample ? * * Our experiments from section 3.3 explore this ( though not using 100 % random labels , which could also be interesting ) , although in the paper we only report the test accuracy . With respect to training accuracy , what we observe is that when using SAM , the network interpolates the \u201c clean \u201d part of the training dataset , while not learning the corrupted labels ( it is also worth noting that the same phenomenon can be obtained with other regularizers , such as Mixup with high alpha ) . For SAM , this might suggest that the minima that fit random labels have a smaller width than the minima that fit the true labels , although this is speculative at this point and we decided to leave this out of the final manuscript . We however believe that there is some very interesting research to be done on the intersection between SAM and semi-supervised / noisy or random labels , and that this could provide more insight into the geometry of the loss landscape . * * With regards to the finding on m-sharpness , it would be great to see a small-scale experiments with full-batch optimization . * * Given the results of the m-sharpness experiment , and the fact that non stochastic gradient descent performs poorly for deep networks , we believe that full batch optimization would not perform well . It would be interesting to see how much SAM manages to mitigate the generalization issue of full batch gradient descent though . * * The finding on m-sharpness is intriguing , though somewhat counter to the motivation for SAM laid out in the paper . * * We initially thought the same thing as the reviewer , as it seemed that branching away from the definition of sharpness used in the theorem ( m=dataset size ) makes the connection between SAM and the generalization bound more tenuous . However , we now believe that the results presented in the right side of figure 3 ( Predictive power of m-sharpness for the generalization gap ) changes this perspective , and shows that there might be better bounds to be discovered that rely on a \u201c per example sharpness \u201d . In that sense , SAM could be an argument to explore new types of sharpness-based bounds . * * From the Theorem , that coefficient \u03bb should be coupled with the neighborhood size \u03c1 in an inversely proportional manner . * * This is a great point . In our experiments , we wanted to be i ) mimicking the way most practitioners would tune the parameters and ii ) being conservative in our choice of rho , avoiding large gridsearch on rho x lambda x learning rate for instance . As a result , we first tune the baseline to find the optimal learning rate and the weight decay ( jointly ) , and only after that tune rho when using SAM . It is however true that we could have used lambda/rho instead of lambda when searching for rho , and it would be interesting to see the impact of this on the results . * * An interesting connection ( or non-connection ) to discuss in the paper would be adversarial training . * * Some of these methods perform an adversarial perturbation step to the data - > SAM can indeed been described as a parameter space analogue to the fast gradient sign method ( FGSM ) ( although the FGSM projection is done on the l_inf sphere and not the l_2 sphere ) , as they both solve min max problems . It would be interesting to see how networks performs when mixing both , considering adversarial attacks on both the input and the weights , but we leave this to future work . * * The bib file could really need some love . * * You are correct , we will clean it in the next version of the paper . * * If I am not mistaken , there is a minor flaw in the proof of Theorem 1 . * * We added a corrected version of the theorem . * * In Table 3 , the boldface is assigned to the wrong method in the Stanford Cars row . * * The boldface is actually correct , it is the score that should read 5.0 instead of 5.3 in this version of the paper . We will correct the table . References : [ 1 ] Exploring the Vulnerability of Deep Neural Networks : A Study of Parameter Corruption ( https : //arxiv.org/pdf/2006.05620.pdf ) [ 2 ] Regularizing Neural Networks via Adversarial Model Perturbation ( https : //arxiv.org/abs/2010.04925 ) < - maybe not needed [ 3 ] AdaNet : Adaptive Structural Learning of Artificial Neural Networks ,"}, "2": {"review_id": "6Tm1mposlrM-2", "review_text": "- Overview This paper proposed a learning algorithm using the idea of flatness . Basically , the algorithm was constructed by a first-order Taylor approximation of flatness and using only the main term . The developed algorithm uses the gradient of the loss with steepest direction when updating the parameters . The proposed algorithm is simple and fast to work . It is also easy to incorporate into existing networks and algorithms and has updated accuracy in many models . - Comments . This is an interesting study , as it is the first application of flatness to a practical algorithm . It is also excellent in that it is easy to implement and immediately applicable . The performance is also nice . The validity of this approximation is a concern . When the authors perform a first-order Taylor approximation , they are looking only at the gradient of the parameter to determine the adversarial direction . So , if the loss function is very spiky , there is a concern that the approximation will not work because the local gradient alone will not find the appropriate direction . The nice networks used in their experiments is supposed to use smoothing techniques for the loss surface , such as the batch normalization and the skip connections , but the concern is whether the proposed method works properly in neural networks with a more less smooth loss surface . To address this concern , my suggestion is to use the algorithm in without the Taylor approximation make comparisons on a more primitive network with a less smooth loss surface . That way , users would feel more comfortable using the method .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We are glad that you liked our paper . Below , we address your only concern with the paper . If you find our response adequate , we would appreciate it if you increase your score . _The validity of this approximation is a concern . [ ... ] So , if the loss function is very spiky , there is a concern that the approximation will not work because the local gradient alone will not find the appropriate direction_ : We agree that there is no worst-case guarantee for this approximation for arbitrary networks . However , we follow here in the footsteps of algorithms such as SGD in the context of training neural networks . SGD , for example , arises from principled theoretical underpinnings , but the theory largely does not apply when using SGD to train neural networks . Indeed , there is no known guarantee that SGD will be able to find a high-quality local minimum of a non-convex loss landscape . Nonetheless , SGD is widely considered to be a valuable method due to its empirically observed effectiveness in finding neural network parameters of high quality . Analogously , we motivate SAM from principled theoretical underpinnings , and while the practical application of SAM ( Algorithm 1 ) involves approximations that we haven \u2019 t ( yet ) fully analyzed theoretically ( a great direction for future work ! ) , our empirical results show that these approximations yield an effective algorithm of substantial practical utility . Furthermore : - As the reviewer noted , we achieve state of the art results with modern architectures ; the approximation unequivocally works well for a range of settings . - The current literature also suggests that the smoothness of the loss landscape is necessary to obtain good results with SGD ( as the reviewer pointed out , this is a motivation behind recurrent connections ) . As a result , it seems that SAM will benefit from all the \u201c tricks \u201d researchers have developed to make SGD work in the first place . - Our work can be connected to adversarial training , except that we perform adversarial attacks on the weight space instead of the input space . One common method for adversarial training is the fast gradient sign method ( FGSM ) , which is the equivalent of SAM in the input space ( although the projection is done on the l_inf sphere and not the l_2 sphere ) . This method is widely used due to its empirical efficacy , even if there is also no guarantee that the adversarial attack is the best possible . _The nice networks used in their experiments is supposed to use smoothing techniques for the loss surface , such as the batch normalization and the skip connections , but the concern is whether the proposed method works properly in neural networks with a more less smooth loss surface . To address this concern , my suggestion is to use the algorithm in without the Taylor approximation make comparisons on a more primitive network with a less smooth loss surface . That way , users would feel more comfortable using the method_ : We added a new appendix analyzing SAM when several steps of projected gradient ascent are used to compute epsilon ( thereby computing the inner maximization with higher fidelity ) . The conclusion of this additional experiment is that i ) for most of the training , one projected gradient step ( i.e. , the Taylor approximation ) is enough to get a good approximation of the epsilon found without the Taylor approximation , ii ) this approximation becomes weaker near convergence , where doing several iterations of projected gradient ascent yields a better epsilon , and iii ) the test accuracy is not strongly affected by the number of project gradient iterations , thus the Taylor approximation is enough in practical settings . Additionally , the loss landscape presented on the right-hand side of Figure 1 is obtained for a CNN without skip connections . In this representation of the loss , it is possible to see that SAM actually managed to find a wider minimum in the absence of skip connections , and that the minimum found when not using SAM is very \u201c spiky \u201d and not smooth . Similarly , the Hessian spectra presented in Section 4.2 were computed for a WideResNet40-10 trained on CIFAR-10 with and without SAM , without using batch normalization . These spectra again confirm that SAM finds a lower-curvature minimum of the loss landscape . We additionally run further experiments for CNNs without batch normalization and without residual connections ( models from Frankle and Carbin , https : //arxiv.org/abs/1803.03635 ) . We used the same hyperparameters as for the paper \u2019 s WideResnet-28x10 experiments on CIFAR and obtain the following results : Conv-2 : * * 14.5 * * ( SAM ) , 15.2 ( SGD ) ; Conv-4 : * * 8.2 * * ( SAM ) , 9.1 ( SGD ) ; Conv-6 : * * 6.8 * * ( SAM ) , 7.8 ( SGD ) ; As seen in these results , SAM again performs well in the absence of batch normalization or residual connections ."}, "3": {"review_id": "6Tm1mposlrM-3", "review_text": "# # Summary # # The paper proposes a modified loss function for supervised learning , in which the original loss at w is replaced with a maximum of the loss in a small p-norm ball around w. An approximate way to compute gradients for this loss is presented , and evaluated in high detail on a variety of supervised learning problems where it is shown to consistently improve the overall generalization error . Furthermore , based on PAC-Bayes theory , a generalization bound for learning under that loss is presented . # # Explanation of Rating # # The main strengths of the paper are the simplicity and convincing evaluation of the method . Another strength is the sound theoretical generalization bound . The paper is also very well written , and I therefore clearly recommend acceptance . Perhaps a weakness of the paper is the lack of a `` broader/high-level perspective '' on the method ( see detailed comment # 1 ) and its comparison to variational inference methods which also optimize PAC-Bayes bounds ( see detailed comment # 2 ) . # # Detailed Comments # # 1 . In the past , Gaussian convolution smoothings of loss functions have been considered a lot in the context of homotopy continuation methods or variational inference . To me , the proposed loss function \\max_\\eps L ( w + \\eps ) - \\delta_ { |\\eps| < \\rho } can be seen as a convolution of the loss , but on the tropical semiring ( max , + ) with the convolution kernel being the indicator function \\delta_ { |\\eps| < \\rho } . Such types of convolution have been heavily studied in the field of convex analysis , where they are known under the name infimal convolution / epi-addition . This opens up the question on why this specific convolution kernel has been chosen . The practical results in this paper are quite strong and have been elusive so far for variational inference and Gaussian smoothing methods , even though they have been around for a long time . Do these good results mainly stem from the efficiently implementable algorithm , or are they more due to favorable geometrical properties of the loss due to performing the convolution in the tropical geometry ? 2.What is the main advantage of the proposed approach over a variational inference ( VI ) method ( e.g.with mean-field Gaussian approximation ) to minimize the PAC-Bayes bound ? In variational inference , the free parameter \\rho is also be optimized by minimizing the right-hand side of the PAC-Bayes bound . Have you tried minimizing over \\rho , or perhaps consider a diagonal approximation of \\rho ? Minor comments / typos : - p.12 `` Adding a Gaussian perturbation should increase the test error '' - > `` Adding a Gaussian perturbation should not decrease the test error . '' This is expected to hold in practice at a minimum . But it is not clear if it holds for any w. - p.12 Typo : `` Then we KL divergence '' - > `` Then the KL divergence '' . - The steps ( 12 ) and ( 13 ) in the proof are not easy to follow . In particular , it was unclear to me what meant with `` each bound '' before Eq.12 , and why the bounds should hold with this specific choice of probability . Furthermore , I could n't follow what happens from ( 12 ) to ( 13 ) . - I did not notice where the assumption ||w|| > = 1 is used , perhaps it can be mentioned in the proof . - \\lambda is another hyperparameter , but it is not mentioned later on how it is chosen .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your encouraging and thoughtful review . Below we address your comments : 1 ) Thanks for pointing to the infimal convolution view in convex analysis . This is a very interesting reformulation and we look forward to exploring its implications . About your question , we believe that while the objective function is important , the specific choice of the training algorithm has a great impact on the final performance . 2 ) We believe that SAM improves over VI methods in terms of final performance ( and perhaps even efficiency ) . Even though we have not made a formal comparison in the paper , we have tried optimizing PAC-Bayesian bounds in the past and have not observed such significant benefits . In our experiments , instead of optimizing \\rho directly , we treat it as a hyperparameter . Minor comments / typos : Thanks for pointing these out . We will fix them all in the final revision ."}}