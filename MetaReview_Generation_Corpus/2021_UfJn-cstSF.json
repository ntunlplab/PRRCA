{"year": "2021", "forum": "UfJn-cstSF", "title": "Learned ISTA with Error-based Thresholding for Adaptive Sparse Coding", "decision": "Reject", "meta_review": "The paper received mixed reviews, with one review voting for acceptance, one strongly opposed, and two borderline ones. The discussion essentially involved R1 and R2, who gave the most informative reviews. After discussion, they did not update their score, even though they appreciated the work and effort done by the authors during the rebuttal. \n\nIn short, the paper has some merit, but several concerns were raised, which the area chair agrees with, leading to a rejection recommendation. The innovation was found to be limited and the discussion between practice and theory (meaning assumptions made in this work) are not discussed in a convincing manner, and these concerns remained after the rebuttal. The experiments were also subject to improvements.\n\nIt is however likely that with a major revision, this work may become publishable to a another venue.", "reviews": [{"review_id": "UfJn-cstSF-0", "review_text": "# # # Strengh - The idea makes sense to adapt the thresholding mechanism to an input distribution with various reconstruction error . It might bring a much better empirical performance compared to thresholds fixed globally and it seems to be adapted in a denoising setting . # # # Weakness - The motivation for this work is not sufficiently furnished . The authors claim that it is useful when there is a discrepancy between train/test distribution but do not provide reference to realistic situation where such a problem arise . - Also , this method can not really be used for real sparse coding problems as the network needs to be trained with the ground truth which is often not known in practice . - The theoretical contribution is marginal as it is almost straightforwardly adapted from Chen et al . ( 2018 ) and Liu et al . ( 2019 ) .- The theoretical results are not precise enough ( ` s small enough ` ) while this assumption might very well render all the results only applicable to toyish case . Moreover , these assumptions are stronger than the one in ` Chen et al . ( 2018 ) ` and ` Liu et al . ( 2019 ) ` for $ x_s $ realizing the sup in the expression of $ b^t $ ( Eq.7 ) . - Almost all the experiments use the same setting as previous work , failing to highlight the advantage of the proposed method . - The performance advantage over LISTA seems to be minor from Figure.2 . # # Extra remarks - The proposed goal is to adapt LISTA in a setting where the input training distribution is * different * from the testing one . However , in this setting , using an algorithm like LISTA does not make sense as the learned weights have no reason to be adapted to the new distribution if the distribution do not overlap at all . There might be some degree to which it is possible to adapt but this should be made more explicit and better discussed . In particular , what type of distribution shift are considered and would make sense -- sparsity is mentionned but it is unclear . - p.3 : ` normal training of LISTA leads to it. ` I do n't think there is any results showing that SGD over LISTA achieves such threshold in theory and I have n't seen any proper empirical validation . If it exists , a proper citation is needed . Else , the statement should be updated . - p.3 : ` According to some prior works , we also know that U ( t ) \u2208 W ( A ) ` : in the three cited papers , there seems to be no results showing that the learned ` U ( t ) ` verifies this . The statement is once again too strong . - Eq . ( 8 ) : Would it be interesting to evaluate the usage of $ \\rho^ { ( t ) } = \\mu ( A ) $ ? - p.4 : ` the main results are obtained under a mild assumption of the ground-truth sparse code ` - > The assumption is not mild . For instance , it seems to never be verified in any of the experiments . The statement $ \\mu ( A ) s \\ll 1 $ seems not backed by any experimental evidence and I do n't think this is true . - p.4 : ` the above assumption gives a more detailed description of the distribution for $ x_s $ ` while it is true that it sets a distribution on the space $ \\mathcal X $ , it is not more precise as in the assumptions by ` Chen et al . ( 2018 ) ` , I believe no distribution is mentionned . so overall it constrains the type of distribution while it is not required in ` Chen et al . ( 2018 ) ` # # Minor comments , nitpicks and typos - citations in ( ) could use ` citealt ` to remove the extra parenthesis . - p.1 : Lasso can also be solved using CD algorithms , which are typically state of the art . - p.1 : In Gregor & LeCun ( 2010 ) , the thresholding is not modified compared to ISTA . - p.2 : ` with W ( t ) =I\u2212U ( t ) A holds for any layer ` - > ` in the case where W ( t ) =I\u2212U ( t ) A holds for any layer ` . - Eq . ( 7 ) : - ` Liu et al . ( 2018 ) ` : The proper citation is ` Liu , J. , Chen , X. , Wang , Z . & Yin , W. ALISTA : Analytic Weights are as good as Learned weigths in LISTA . in International Conference on Learning Representation ( ICLR ) 1113\u20131117 ( 2019 ) . ` - p.4 : ` a truncated distribution ` - > I assume the authors mean ` truncated gaussian distribution ` ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the constructive comments . Our responses are given as below : Q1 & Q2 : Motivation of the work when there is a discrepancy between train/test distribution , and the usage in real sparse coding problems . A : Similar to LISTA and its variants , our work considers a setting for model training where a set of observations paired with the corresponding ground-truth sparse codes are used . We believe that the discrepancy between train/test distribution arises in many practical applications . For instance , in the task of photometric stereo analysis , if the training and test observations are obtained from two different devices , it is highly likely that the sparsity of the training and test noise vectors are different . In addition , in many practical applications , as has been mentioned by the reviewer , we may not be given the ground-truth sparse representations of the observations , yet , with the dictionary matrix $ A $ , we can still simulate a set of sparse codes together with the observations $ \\ { ( y , x_s ) \\ } $ for training , which leads to a discrepancy between the train and test distribution . Our method can be used to handle such scenarios . Q3 : Theory results are not precise and the assumptions are not mild . A3 : We appreciate the pointer to imprecise statements . We aimed to say $ \\mu ( A ) s \\ll 1 $ when mentioning $ s $ being sufficiently small . Note that in the prior work , $ s $ is also assumed to be sufficiently small to satisfy $ \\mu ( A ) ( 2s-1 ) < 1 $ ( see Theorem 2 in [ 2 ] and Theorem 1 in [ 3 ] ) . Our assumption is given in a similar manner . Experimental results in Figure 6 ( a ) ( $ p_b=0.95 $ ) , Figure 9 ( a ) ( $ p_b=0.9 $ ) , and Figure 9 ( b ) ( $ p_b=0.8 $ ) also show that our EBT mechanism leads to better performance when $ p_b $ is larger , i.e. , when $ s $ is smaller , which well validates our theory . In addition , even without such an additional assumption , we still have some theoretical results that our EBT-LISTA and EBT-LISTA-SS converge faster than LISTA and LISTA-SS with probability ( see our proof ) . Overall , we argue that the theoretical contribution of our work is significant . We not only demonstrate how much performance gain our EBT can achieve in theory but also shed light on the two convergence phases of the variants of LISTA . Q4 : The experiments use the same setting as previous work , failing to highlight the advantage of the proposed method . A4 : We followed some common practice to design experiments for fair comparisons with prior work , and , in order to show the advantages of our method more clearly in some concerned settings , we have shown more results in the two paragraphs of \u201c Adaptivity to unknown sparsity \u201d ( page 8 ) and \u201c Random sparsity \u201d ( page 11 ) in our paper . Experimental results demonstrate that our EBT mechanism leads to superior performance than that of all competitors . We encourage the reviewer to take a closer look at the two paragraphs for more details . Q5 : Comment on Figure 2 for our empirical advantage . A5 : Figure 2 is shown to validate our theoretical results that many variants of LISTA have two convergence phases and our EBT accelerates , in particular , the first phase . We have revised the caption of Figure 2 to make our statement more concrete . Q6 : Comment on \u201c Normal training of LISTA leads to it \u201d . A6 : We thank the reviewer for pointing out the confusing statement . We aimed to say that linear convergence can be obtained if $ b^ { ( t ) } = \\mu ( A ) \\mathrm { sup } _ { i=0,1 , \\ldots , n } \\Vert x_i^ { ( t ) } -x_ { s_i } \\Vert_p $ and some other conditions ( including $ U^ { ( t ) } \\in \\mathcal { W } ( A ) $ as mentioned ) hold . This is a theoretical result of prior work ( see Eq . ( 30 ) in [ 2 ] ) , and we have added proper reference there in the updated version of our paper . Q7 : Comment on $ U^ { ( t ) } \\in \\mathcal { W } ( A ) $ . A7 : $ U^ { ( t ) } \\in \\mathcal { W } ( A ) $ is also one of the assumptions for guaranteeing linear convergence of LISTA [ 2 ] [ 3 ] [ 4 ] ( see Theorem 2 in [ 2 ] , Theorem 1 in [ 3 ] , and Proposition 1 in [ 4 ] for more details ) . We have revised the statement in the updated version of our paper ."}, {"review_id": "UfJn-cstSF-1", "review_text": "In the paper , authors propose a new error-based thresholding mechanism for LISTA which introduces a function of the evolving estimation error to provide each threshold in the shrinkage functions . They provided the theoretical analysis for EBT-LISTA and EBT-LISTA with support selection and proved that the estimation error of the proposed algorithm is theoretically lower than compared methods . The authors also evaluated the proposed method on multiple synthetic or real tasks . Experimental results show that the proposed method achieves a better estimation error and higher adaptivity to different observations with a variety of sparsity .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the positive feedback ."}, {"review_id": "UfJn-cstSF-2", "review_text": "This paper disentangles the threshold parameters in LISTA-type models from the reconstruction errors , proposing the Error-Based Threholding ( EBT ) mechanism which mainly follows a theoretical results in ( Chen et al. , 2019 ; Liu et al. , 2018 ) , where the threshold at one layer is proportional to the recovery error of current iterate . The benefits brought by the proposed EBT method are faster convergence and better adaptivity to a wider range of samples . To bypass the requirement of ground truth sparse signals , EBT uses the reconstruction error following a learned linear transform , which in theory has good coherence property with the dictionary and therefore can approximate the recovery error well . The authors theoretically show that the proposed EBT mechanism enjoys faster convergence in both cases with and without the support selection technique . Emprirical experiments on standard synthetic setting and cross-sparsity setting are shown to support the efficacy of EBT . The authors also do real-world photometric stereo analysis to show the superiority of EBT . Pros : - This paper is a successful extention of basic LISTA-type models by disentangling the learnable threshold parameters . - Theoretical analysis of the benefits of EBT is provided and looks correct to me . - The empirical experiments are solid enough to show the superiority of EBT . Cons : - My main concern is about that this paper is a little bit incremental , as it seems to be a very direct extension based on previous theoretical results . Other comments : - In Figure 1 , it can be observed that the thresholds learned LISTA-SS have a bumpy curve , which is also observed in previous works . In contrast , the $ \\rho^ { ( t ) } $ parameters look much stabilized . It might be better to also show the curves of reconstruction error term , i.e.the $ \\ell_p $ term in eqn ( 12 ) to see how they look like . This would help to understand if the new parameterization used in EBT changes the training dynamics or not . - In the basic settings part , the numbers of validation and testing samples are provided . How many samples are used for training ? - In eqn ( 15 ) , is the term about $ \\rho n $ omitted on purpose , or mistakenly ? In summary , I think this is a good extension paper but I have a little concern about its incremental contribution . Overall I think it is slightly above the threshold . I am open to more opinions from other reviewers .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the positive feedback and comments . Our responses are given as below : Q1 : Whether the new parameterization used in EBT changes the training dynamics or not . A1 : Following the suggestion , we have redrawn Figure 3 in the paper for better illustration of the training dynamics . In the figure , we can see that the learned thresholds in our EBT-based methods and the original LISTA and LISTA-SS are similar , which indicates that the introduced EBT mechanism does not modify the training dynamics of the original methods , and it works by disentangling the reconstruction error and learnable parameters . Q2 : How many samples are used for training ? A2 : Following prior work [ 1 ] [ 2 ] [ 3 ] , we synthesized in-stream data for training all models in comparison , thus the number of training samples grows as the training proceeds . Since all models were trained for the same number of iterations , they all share the same training complexity . Q3 : Mistakes in eqn ( 15 ) . A3 : Thanks for pointing out . In our paper , $ L^ { \\dagger } $ is the orthogonal complement of $ L $ rather than the pseudo-inverse , and thus Eq . ( 14 ) can be rewritten as $ $ L^ { \\dagger } o = \\rho L^ { \\dagger } Ln + L^ { \\dagger } e = L^ { \\dagger } e. $ $ We have revised Eq . ( 15 ) in the latest version of the paper for better clarity . [ 1 ] Xiaohan Chen , Jialin Liu , Zhangyang Wang , and Wotao Yin . Theoretical linear convergence of unfolded ISTA and its practical weights and thresholds . In Advances in Neural Information Processing Systems ( NeurIPS ) , pp . 9061\u20139071 , 2018 . [ 2 ] Jialin Liu , Xiaohan Chen , Zhangyang Wang , and Wotao Yin . ALISTA : Analytic weights are as good as learned weights in LISTA . In Proceedings of the International Conference on Learning Representations ( ICLR ) , pp . 1113\u20131117 , 2019 . [ 3 ] Kailun Wu , Yiwen Guo , Ziang Li , and Changshui Zhang . Sparse coding with gated learned ISTA . In Proceedings of the International Conference on Learning Representations ( ICLR ) , 2020 ."}, {"review_id": "UfJn-cstSF-3", "review_text": "Summary : The authors propose an automatic threshold selection for LISTA-style neural nets . The threshold introduces negligible number of parameters ( either 0 , or one per layer ) . This choice is shown theoretically and empirically to have faster convergence than methods without it and popular alternatives . Clarity : The clarity of your idea would be improved using a graphic : for example showing the LISTA architecture , and what sort of computations are performed in a `` feed in '' direction toward the shrinkage operator . Just a thought , not a criticism . The information in Table 1 might be more succinctly reported using exponential notation , or speedup as a percentage ( I count 39 extraneous 0 's ) Quality : The quality would be improved if you applied this adaptive thresholding idea to classic ( F ) ISTA , i.e.updating ( F ) ISTA 's thresholds via the parameterless form of EBT . How would this impact standard ( F ) ISTA convergence , i.e. , compared to using a fixed threshold run on a whole test set ? ( is ebt-threshold selection better than say , doing a parameter search for fixed threshold `` b '' , over a training set on ( F ) ISTA ? ) The experiment variety is of good quality : experiments across condition numbers and sparsities , validation of theorems , etc . Originality : I am not familiar with other works like this , and I like the idea . I wish we were given insight to how it affects nonlearned ( F ) ISTA algorithms , which are still widely used in practice . Significance : The result is significant in the specific application of LISTA .", "rating": "7: Good paper, accept", "reply_text": "Thanks for the positive feedback and constructive suggestions . We have revised the paper accordingly . The architecture of LISTA and our proposed EBT-LISTA has been shown in Section 3 , and Table 1 has been modified . The results of EBT-ISTA and EBT-FISTA have been shown in Appendix A.1 ( Figure 11 ) . Specifically , introducing our EBT mechanism in the classical ISTA and FISTA leads to faster initial convergences but worse final performance . This is because ISTA and FISTA converge in a sub-linear manner , but our EBT is mostly proposed for methods that converge linearly , which might cause a mismatch to our assumptions ."}], "0": {"review_id": "UfJn-cstSF-0", "review_text": "# # # Strengh - The idea makes sense to adapt the thresholding mechanism to an input distribution with various reconstruction error . It might bring a much better empirical performance compared to thresholds fixed globally and it seems to be adapted in a denoising setting . # # # Weakness - The motivation for this work is not sufficiently furnished . The authors claim that it is useful when there is a discrepancy between train/test distribution but do not provide reference to realistic situation where such a problem arise . - Also , this method can not really be used for real sparse coding problems as the network needs to be trained with the ground truth which is often not known in practice . - The theoretical contribution is marginal as it is almost straightforwardly adapted from Chen et al . ( 2018 ) and Liu et al . ( 2019 ) .- The theoretical results are not precise enough ( ` s small enough ` ) while this assumption might very well render all the results only applicable to toyish case . Moreover , these assumptions are stronger than the one in ` Chen et al . ( 2018 ) ` and ` Liu et al . ( 2019 ) ` for $ x_s $ realizing the sup in the expression of $ b^t $ ( Eq.7 ) . - Almost all the experiments use the same setting as previous work , failing to highlight the advantage of the proposed method . - The performance advantage over LISTA seems to be minor from Figure.2 . # # Extra remarks - The proposed goal is to adapt LISTA in a setting where the input training distribution is * different * from the testing one . However , in this setting , using an algorithm like LISTA does not make sense as the learned weights have no reason to be adapted to the new distribution if the distribution do not overlap at all . There might be some degree to which it is possible to adapt but this should be made more explicit and better discussed . In particular , what type of distribution shift are considered and would make sense -- sparsity is mentionned but it is unclear . - p.3 : ` normal training of LISTA leads to it. ` I do n't think there is any results showing that SGD over LISTA achieves such threshold in theory and I have n't seen any proper empirical validation . If it exists , a proper citation is needed . Else , the statement should be updated . - p.3 : ` According to some prior works , we also know that U ( t ) \u2208 W ( A ) ` : in the three cited papers , there seems to be no results showing that the learned ` U ( t ) ` verifies this . The statement is once again too strong . - Eq . ( 8 ) : Would it be interesting to evaluate the usage of $ \\rho^ { ( t ) } = \\mu ( A ) $ ? - p.4 : ` the main results are obtained under a mild assumption of the ground-truth sparse code ` - > The assumption is not mild . For instance , it seems to never be verified in any of the experiments . The statement $ \\mu ( A ) s \\ll 1 $ seems not backed by any experimental evidence and I do n't think this is true . - p.4 : ` the above assumption gives a more detailed description of the distribution for $ x_s $ ` while it is true that it sets a distribution on the space $ \\mathcal X $ , it is not more precise as in the assumptions by ` Chen et al . ( 2018 ) ` , I believe no distribution is mentionned . so overall it constrains the type of distribution while it is not required in ` Chen et al . ( 2018 ) ` # # Minor comments , nitpicks and typos - citations in ( ) could use ` citealt ` to remove the extra parenthesis . - p.1 : Lasso can also be solved using CD algorithms , which are typically state of the art . - p.1 : In Gregor & LeCun ( 2010 ) , the thresholding is not modified compared to ISTA . - p.2 : ` with W ( t ) =I\u2212U ( t ) A holds for any layer ` - > ` in the case where W ( t ) =I\u2212U ( t ) A holds for any layer ` . - Eq . ( 7 ) : - ` Liu et al . ( 2018 ) ` : The proper citation is ` Liu , J. , Chen , X. , Wang , Z . & Yin , W. ALISTA : Analytic Weights are as good as Learned weigths in LISTA . in International Conference on Learning Representation ( ICLR ) 1113\u20131117 ( 2019 ) . ` - p.4 : ` a truncated distribution ` - > I assume the authors mean ` truncated gaussian distribution ` ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the constructive comments . Our responses are given as below : Q1 & Q2 : Motivation of the work when there is a discrepancy between train/test distribution , and the usage in real sparse coding problems . A : Similar to LISTA and its variants , our work considers a setting for model training where a set of observations paired with the corresponding ground-truth sparse codes are used . We believe that the discrepancy between train/test distribution arises in many practical applications . For instance , in the task of photometric stereo analysis , if the training and test observations are obtained from two different devices , it is highly likely that the sparsity of the training and test noise vectors are different . In addition , in many practical applications , as has been mentioned by the reviewer , we may not be given the ground-truth sparse representations of the observations , yet , with the dictionary matrix $ A $ , we can still simulate a set of sparse codes together with the observations $ \\ { ( y , x_s ) \\ } $ for training , which leads to a discrepancy between the train and test distribution . Our method can be used to handle such scenarios . Q3 : Theory results are not precise and the assumptions are not mild . A3 : We appreciate the pointer to imprecise statements . We aimed to say $ \\mu ( A ) s \\ll 1 $ when mentioning $ s $ being sufficiently small . Note that in the prior work , $ s $ is also assumed to be sufficiently small to satisfy $ \\mu ( A ) ( 2s-1 ) < 1 $ ( see Theorem 2 in [ 2 ] and Theorem 1 in [ 3 ] ) . Our assumption is given in a similar manner . Experimental results in Figure 6 ( a ) ( $ p_b=0.95 $ ) , Figure 9 ( a ) ( $ p_b=0.9 $ ) , and Figure 9 ( b ) ( $ p_b=0.8 $ ) also show that our EBT mechanism leads to better performance when $ p_b $ is larger , i.e. , when $ s $ is smaller , which well validates our theory . In addition , even without such an additional assumption , we still have some theoretical results that our EBT-LISTA and EBT-LISTA-SS converge faster than LISTA and LISTA-SS with probability ( see our proof ) . Overall , we argue that the theoretical contribution of our work is significant . We not only demonstrate how much performance gain our EBT can achieve in theory but also shed light on the two convergence phases of the variants of LISTA . Q4 : The experiments use the same setting as previous work , failing to highlight the advantage of the proposed method . A4 : We followed some common practice to design experiments for fair comparisons with prior work , and , in order to show the advantages of our method more clearly in some concerned settings , we have shown more results in the two paragraphs of \u201c Adaptivity to unknown sparsity \u201d ( page 8 ) and \u201c Random sparsity \u201d ( page 11 ) in our paper . Experimental results demonstrate that our EBT mechanism leads to superior performance than that of all competitors . We encourage the reviewer to take a closer look at the two paragraphs for more details . Q5 : Comment on Figure 2 for our empirical advantage . A5 : Figure 2 is shown to validate our theoretical results that many variants of LISTA have two convergence phases and our EBT accelerates , in particular , the first phase . We have revised the caption of Figure 2 to make our statement more concrete . Q6 : Comment on \u201c Normal training of LISTA leads to it \u201d . A6 : We thank the reviewer for pointing out the confusing statement . We aimed to say that linear convergence can be obtained if $ b^ { ( t ) } = \\mu ( A ) \\mathrm { sup } _ { i=0,1 , \\ldots , n } \\Vert x_i^ { ( t ) } -x_ { s_i } \\Vert_p $ and some other conditions ( including $ U^ { ( t ) } \\in \\mathcal { W } ( A ) $ as mentioned ) hold . This is a theoretical result of prior work ( see Eq . ( 30 ) in [ 2 ] ) , and we have added proper reference there in the updated version of our paper . Q7 : Comment on $ U^ { ( t ) } \\in \\mathcal { W } ( A ) $ . A7 : $ U^ { ( t ) } \\in \\mathcal { W } ( A ) $ is also one of the assumptions for guaranteeing linear convergence of LISTA [ 2 ] [ 3 ] [ 4 ] ( see Theorem 2 in [ 2 ] , Theorem 1 in [ 3 ] , and Proposition 1 in [ 4 ] for more details ) . We have revised the statement in the updated version of our paper ."}, "1": {"review_id": "UfJn-cstSF-1", "review_text": "In the paper , authors propose a new error-based thresholding mechanism for LISTA which introduces a function of the evolving estimation error to provide each threshold in the shrinkage functions . They provided the theoretical analysis for EBT-LISTA and EBT-LISTA with support selection and proved that the estimation error of the proposed algorithm is theoretically lower than compared methods . The authors also evaluated the proposed method on multiple synthetic or real tasks . Experimental results show that the proposed method achieves a better estimation error and higher adaptivity to different observations with a variety of sparsity .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the positive feedback ."}, "2": {"review_id": "UfJn-cstSF-2", "review_text": "This paper disentangles the threshold parameters in LISTA-type models from the reconstruction errors , proposing the Error-Based Threholding ( EBT ) mechanism which mainly follows a theoretical results in ( Chen et al. , 2019 ; Liu et al. , 2018 ) , where the threshold at one layer is proportional to the recovery error of current iterate . The benefits brought by the proposed EBT method are faster convergence and better adaptivity to a wider range of samples . To bypass the requirement of ground truth sparse signals , EBT uses the reconstruction error following a learned linear transform , which in theory has good coherence property with the dictionary and therefore can approximate the recovery error well . The authors theoretically show that the proposed EBT mechanism enjoys faster convergence in both cases with and without the support selection technique . Emprirical experiments on standard synthetic setting and cross-sparsity setting are shown to support the efficacy of EBT . The authors also do real-world photometric stereo analysis to show the superiority of EBT . Pros : - This paper is a successful extention of basic LISTA-type models by disentangling the learnable threshold parameters . - Theoretical analysis of the benefits of EBT is provided and looks correct to me . - The empirical experiments are solid enough to show the superiority of EBT . Cons : - My main concern is about that this paper is a little bit incremental , as it seems to be a very direct extension based on previous theoretical results . Other comments : - In Figure 1 , it can be observed that the thresholds learned LISTA-SS have a bumpy curve , which is also observed in previous works . In contrast , the $ \\rho^ { ( t ) } $ parameters look much stabilized . It might be better to also show the curves of reconstruction error term , i.e.the $ \\ell_p $ term in eqn ( 12 ) to see how they look like . This would help to understand if the new parameterization used in EBT changes the training dynamics or not . - In the basic settings part , the numbers of validation and testing samples are provided . How many samples are used for training ? - In eqn ( 15 ) , is the term about $ \\rho n $ omitted on purpose , or mistakenly ? In summary , I think this is a good extension paper but I have a little concern about its incremental contribution . Overall I think it is slightly above the threshold . I am open to more opinions from other reviewers .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the positive feedback and comments . Our responses are given as below : Q1 : Whether the new parameterization used in EBT changes the training dynamics or not . A1 : Following the suggestion , we have redrawn Figure 3 in the paper for better illustration of the training dynamics . In the figure , we can see that the learned thresholds in our EBT-based methods and the original LISTA and LISTA-SS are similar , which indicates that the introduced EBT mechanism does not modify the training dynamics of the original methods , and it works by disentangling the reconstruction error and learnable parameters . Q2 : How many samples are used for training ? A2 : Following prior work [ 1 ] [ 2 ] [ 3 ] , we synthesized in-stream data for training all models in comparison , thus the number of training samples grows as the training proceeds . Since all models were trained for the same number of iterations , they all share the same training complexity . Q3 : Mistakes in eqn ( 15 ) . A3 : Thanks for pointing out . In our paper , $ L^ { \\dagger } $ is the orthogonal complement of $ L $ rather than the pseudo-inverse , and thus Eq . ( 14 ) can be rewritten as $ $ L^ { \\dagger } o = \\rho L^ { \\dagger } Ln + L^ { \\dagger } e = L^ { \\dagger } e. $ $ We have revised Eq . ( 15 ) in the latest version of the paper for better clarity . [ 1 ] Xiaohan Chen , Jialin Liu , Zhangyang Wang , and Wotao Yin . Theoretical linear convergence of unfolded ISTA and its practical weights and thresholds . In Advances in Neural Information Processing Systems ( NeurIPS ) , pp . 9061\u20139071 , 2018 . [ 2 ] Jialin Liu , Xiaohan Chen , Zhangyang Wang , and Wotao Yin . ALISTA : Analytic weights are as good as learned weights in LISTA . In Proceedings of the International Conference on Learning Representations ( ICLR ) , pp . 1113\u20131117 , 2019 . [ 3 ] Kailun Wu , Yiwen Guo , Ziang Li , and Changshui Zhang . Sparse coding with gated learned ISTA . In Proceedings of the International Conference on Learning Representations ( ICLR ) , 2020 ."}, "3": {"review_id": "UfJn-cstSF-3", "review_text": "Summary : The authors propose an automatic threshold selection for LISTA-style neural nets . The threshold introduces negligible number of parameters ( either 0 , or one per layer ) . This choice is shown theoretically and empirically to have faster convergence than methods without it and popular alternatives . Clarity : The clarity of your idea would be improved using a graphic : for example showing the LISTA architecture , and what sort of computations are performed in a `` feed in '' direction toward the shrinkage operator . Just a thought , not a criticism . The information in Table 1 might be more succinctly reported using exponential notation , or speedup as a percentage ( I count 39 extraneous 0 's ) Quality : The quality would be improved if you applied this adaptive thresholding idea to classic ( F ) ISTA , i.e.updating ( F ) ISTA 's thresholds via the parameterless form of EBT . How would this impact standard ( F ) ISTA convergence , i.e. , compared to using a fixed threshold run on a whole test set ? ( is ebt-threshold selection better than say , doing a parameter search for fixed threshold `` b '' , over a training set on ( F ) ISTA ? ) The experiment variety is of good quality : experiments across condition numbers and sparsities , validation of theorems , etc . Originality : I am not familiar with other works like this , and I like the idea . I wish we were given insight to how it affects nonlearned ( F ) ISTA algorithms , which are still widely used in practice . Significance : The result is significant in the specific application of LISTA .", "rating": "7: Good paper, accept", "reply_text": "Thanks for the positive feedback and constructive suggestions . We have revised the paper accordingly . The architecture of LISTA and our proposed EBT-LISTA has been shown in Section 3 , and Table 1 has been modified . The results of EBT-ISTA and EBT-FISTA have been shown in Appendix A.1 ( Figure 11 ) . Specifically , introducing our EBT mechanism in the classical ISTA and FISTA leads to faster initial convergences but worse final performance . This is because ISTA and FISTA converge in a sub-linear manner , but our EBT is mostly proposed for methods that converge linearly , which might cause a mismatch to our assumptions ."}}