{"year": "2020", "forum": "SJeq9JBFvH", "title": "Deep probabilistic subsampling for task-adaptive compressed sensing", "decision": "Accept (Poster)", "meta_review": "This paper introduces a probabilistic data subsampling scheme that can be optimized end-to-end.  The experimental evaluation is a bit weak, focusing mostly on toy-scale problems, and I would have liked to see a discussion of bias in the Gumbel-max gradient estimator.  \n\nIt's also not clear how the free hyperparameters for this method were chosen, which makes me suspect they were tuned on the test set.\n\nHowever, the overall idea is sensible, and the area seems under-explored.", "reviews": [{"review_id": "SJeq9JBFvH-0", "review_text": "This paper introduces a novel DPS(Deep Probabilistic Subsampling) framework for the task-adaptive subsampling case, which attempts to resolve the issue of end-to-end optimization of an optimal subset of signal with jointly learning a sub-Nyquist sampling scheme and a predictive model for downstream tasks. The parameterization is used to simplify the subsampling distribution and ensure an expressive yet tractable distribution. The new approach contribution is applied to both reconstruction and classification tasks and demonstrated with a suite of experiments in a toy dataset, MINIST, and COFAR10. Overall, the paper requires significant improvement. 1. The approach is not well justified either by theory or practice. There is no experiment clearly shows convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches (Xie & Ermon (2019); Kool et al. (2019); Pl\u00c2\u0161otz & Roth (2018) ). 2. The paper never clearly demonstrates the problem they are trying to solve (nor well differentiates it from the compressed sensing problem or sample selection problem) The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution. 3. The paper is not nicely written or rather easy to follow. The model is not well motivated and the optimization algorithm is also not well described. 4. A theoretical analysis of the convergence of the optimization algorithm could be needed. 5. The paper is imprecise and unpolished and the presentation needs improvement. **There are so many missing details or questions to answer** 1. What is the Gumbel-max trick? 2. How to tune the parameters discussed in training details in the experiments? 3. Why to use experience replay for the linear experiments? 4. Are there evaluations on the utility of proposed compared to existing approaches? 5. Does the proposed approach work in real-world problems? 6. Was there any concrete theoretical guarantee to ensure the convergence of the algorithm. [Post Review after discussion]\u0010: The uploaded version has significantly improved over the first submission. It is now acceptable. ", "rating": "6: Weak Accept", "reply_text": "Question 3 : We did our best to write the paper such that it includes all details needed to fully understand the proposed method and its theoretical background . The referee indicates ( in sharp contrast to referee 3 ) that the paper is not nicely written , nor easy to follow . We invite the referee to be specific about the sections of the original manuscript that need more clarification , allowing us to revise these sections . Question 4 : The optimization algorithm used is the ADAM optimizer ( Kingma & Ba , 2014 ) . We refer to section 4 of Kingma & Ba ( 2014 ) for a proof of convergence for convex functions . It is known that that loss surfaces of deep neural networks are typically non-convex , however the gap between global and local minima is believed to be small for ( see our answer to the referee \u2019 s last question for more detail on this statement ) . Question 5 : We kindly ask the reviewer to elaborate on the given statement . Could the reviewer indicate which sections are found to be imprecise and unpolished , and which parts of the manuscript need a better presentation ? ===================================== Second part of the review ===================================== Question 1 : We specify the Gumbel-max trick in the paragraph below equation 4 . To make the paper more self-contained , we will extend this paragraph to further clarify the Gumbel-max trick . We also refer to our answer to the first question of this referee , in which we elaborated more on the Gumbel-max trick as well . Question 2 : All training parameters were tuned empirically . However , we agree it is worth elaborating on our insights regarding the influence on performance of some of them . We experienced that performance was most sensitive to the learning rates for the sampling and task models , and the temperature parameter tau of the softmax relaxation . We augmented the discussion of our revised manuscript to share these insights . .Question 3 : We know experience replay as a reinforcement learning technique for storing previous state/action pairs . However , our method does not make use of reinforcement learning , so could the reviewer please elaborate how experience replay would relate to our method ? Question 4 : In Section 4.1 ( MNIST classification ) we already compared our proposed sampling method to used Gumbel top-K sampling for data subsampling . We are currently also running experiments that allow for extensive comparison with the recently proposed LOUPE method by Bahadir et al . ( 2019 ) .Question 5 : A large part of the experiments in this work are focusing on compressive/partial Fourier measurements . This adequately reflects the measurement setup in many real-world problems , such as k-space measurement in magnetic resonance imaging ( Lustig et al . ) , Xampling for ultrasound imaging ( Eldar et al . ) , and non-uniform step frequency radar ( Huang , 2014 ) . In addition , we cover direct pixel sampling , related to real-world applications such as compressive cameras . We would like to emphasize that the proposed approach is measurement-domain agnostic , and therefore can be applied across a vast amount of real-world problem . In addition , our ongoing research already shows promising results for real-world applications such as magnetic resonance imaging and ultrasound imaging . This is part of future work . Question 6 : The trends towards using deep learning for data-driven compressed sensing indeed has the downside of not having guarantees on finding a global minimum , as the loss surface of a NN is highly non-linear and non-convex . Still , these data-driven results have shown to be very promising ( Gregor et al. , 2010 ; Jin,2019 ; Bahadir et al. , 2019 ; Mousavi , 2019 ) However due to the weight space symmetry problem ( Goodfellow et al. , 2016 ) the loss surface contains a vast amount of local minima with the same error value . The size of the gap between local and the global minima remains an open field of research . However , citing from Goodfellow et al . ( 2016 ) : \u201c The problem remains an active area of research , but experts now suspect that , for su\ufb03ciently large neural networks , most local minima have a low cost function value , and that it is not important to \ufb01nd a true global minimum rather than to \ufb01nd a point in parameter space that has low but not minimal cost ( Saxe et al. , 2013 ; Dauphin et al. , 2014 ; Goodfellow et al. , 2015 ; Choromanska et al. , 2014 ) \u201d As such , we leverage the empirically-shown ability of stochastic gradient descent to optimize this non-convex function in our NN for finding local minima . Indeed there is no guarantee on finding a global optimum ."}, {"review_id": "SJeq9JBFvH-1", "review_text": "The paper proposes a learning-based adaptive compressed sensing framework in which both the sampling and the task functions (e.g., classification) are learned jointly end-to-end. The main contribution includes using the Gumbel-softmax trick to relax categorical distributions and use back-propagation to estimate the gradient jointly with the tas neural network. The proposed solution has the flexibility of able to be used in several different tasks, such as inverse problems ( super-resolution or image completion) or classification tasks. The paper is very well written. The paper locates itself well in current baselines and explains Experiments mostly well. However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique: 1) The only comparison to another non-fixed sampling baseline is Kool et al. 2019. The visualization and a thorough comparison were missing in MNIST classification. This baseline was also missing in image reconstruction. 2) Compressive Sensing incorporates vast literature of algorithms focusing on different aspects of improvements; algorithms focused on classification and inverse problems. Even if done disjointly, how does the proposed joint learning is compared to those algorithms in these domains? 3) Top row of Figure 3 nicely explains how the learned sampling paradigm performs compared to other mechanisms (such as uniform, random, low-pass). But there is no comparision against other non-fixed techniques. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the positive and constructive feedback . Below we answer the questions and concerns : Question 1 : We agree with the referee and will therefore include a visualization of the trained distributions using Gumbel top-k sampling and a realization of the sampling pattern . We are currently running experiments to obtain Gumbel top-k results for the \u2018 lines and circles \u2019 and CIFAR10 experiments as well . Since we did not sufficiently emphasize that leveraging Gumbel top-k sampling for learning signal subsampling matrices is part of the novelty of the present work , we clarified this in the revised manuscript . In fact , using Gumbel top-k sampling in this context can be seen as a constrained version of DPS , with shared weights across the M distributions . To also include previously-published baselines , we are currently running experiments with the recently proposed LOUPE method by Bahadir et al . ( 2019 ) .Question 2 : Indeed , the notion of compressed sensing has spurred vast work , ranging from sensing strategies to signal recovery algorithms . On the sensing side , sampling strategies are typically designed to satisfy the Restricted Isometry Property ( RIP ) ; describing isometry of the sensing matrix given K-sparse vectors , and thereby providing signal recovery guarantees , given an appropriate algorithm . On the algorithm side , sparsity in some basis transform is typically exploited , leveraging a wide variety of optimization algorithms spanning from proximal gradient methods to projection-over-convex-set and greedy algorithms . More recently , deep learning methods have been proposed for fast signal recovery from CS measurements , yielding state-of-the-art results . In this context , DPS adopts current practices in data-driven CS recovery , but extends this to incorporate subsampling ( the sensing ) in an end-to-end pipeline . Such an end-to-end ( sampling-to-any-task ) learning strategy opens up opportunities for data-driven optimization of sensing strategies beyond theoretically-established results . As pointed out by the referee , the shortcomings of disjoint optimization in classical CS are perhaps most evident when high-level tasks such as classification are part of the pipeline . As such , we are currently running additional experiments to better illustrate this . Question 3 : We agree with the reviewer that such a comparison might be of interest . As such , we are currently running additional experiments to include a comparison to Gumbel top-k ( as we did for the MNIST classification case ) as well as the method proposed by Bahadir et al . ( 2019 ) .Notably , and unlike our method , the latter approach does not permit setting a specific subsampling rate , with this rate is only being indirectly controlled via hyperparameter settings ."}, {"review_id": "SJeq9JBFvH-2", "review_text": "The authors propose a new approach of deep probabilistic subsampling for compressed sensing, based on Gumbel-softmax, which is interesting. A few points should be clarified: - in compressed sensing one has e.g. the restricted isometry property (RIP) related to recovery. How does the new method relate to such theoretical results? Are the results and findings along similar lines as (classical) compressed sensing theory? - Methods in compressed sensing are typically convex, e.g. using l1-regularization. What are the drawbacks of using deep learning in this context, e.g. related to non-convexity? What is the role of initialization? - Does the method both work for underdetermined and overdetermined problems (number of data versus number of unknowns)? - What is the influence of the hyper-parameters mu and lambda in eq (14)? How should the model selection be done (currently lambda is set to 0.004 without further motivation)? - MNIST: 60,000 instead of 70,000? ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the interesting questions regarding our work . Below we respond to these questions of the referee : Question 1 : In compressed sensing , RIP is indeed used to provide a measure for isometry when \u201c restricted \u201d to k columns , i.e.given a measurement $ \\mathbf { y } =\\mathbf { A } \\mathbf { x } $ , of a k-sparse vector $ \\mathbf { x } $ . For many practical problems of interest , analysis of measurements of sparse vectors is achieved by reformulating the measurement as $ \\mathbf { y } =\\mathbf { A } \\mathbf { \\Psi } \\mathbf { x } $ , with $ \\mathbf { \\Psi } $ being a sparsifying basis , and $ \\mathbf { z } =\\mathbf { \\Psi } \\mathbf { x } $ being the quantity of interest . Then , RIP should be evaluated for $ \\mathbf { A } \\mathbf { \\Psi } $ . A common requirement posed for sparse bases is therefore incoherence of the columns . Instead , we here directly learn a mapping to $ \\mathbf { z } $ from data , with no explicit notion of such a sparsifying basis . While this makes theoretical assessment more challenging , it alleviates the need for manual identification of a proper sparse basis for each new problem . We augmented parts of the discussion of the revised manuscript to better reflect this . Question 2 : The reviewer raises a fundamental and interesting question regarding the typical loss surface in CS compared to the one of a neural network . Indeed the loss surface of a NN is highly non-linear and non-convex , it typically contains a vast amount of local minima , as a consequence of the weight space symmetry property ( Goodfellow et al. , 2016 ) , i.e.having the same loss value for a different ordering of the same weights . The size of the gap between local and the global minima remains an open field of research . However , citing from Goodfellow et al . ( 2016 ) : \u201c The problem remains an active area of research , but experts now suspect that , for su\ufb03ciently large neural networks , most local minima have a low cost function value , and that it is not important to \ufb01nd a true global minimum rather than to \ufb01nd a point in parameter space that has low but not minimal cost ( Saxe et al. , 2013 ; Dauphin et al. , 2014 ; Goodfellow et al. , 2015 ; Choromanska et al. , 2014 ) \u201d As such , we leverage the empirically-shown ability of stochastic gradient descent to optimize this non-convex function . Indeed there are no global convergence guarantees , but we have the strong advantage compared to typical L1-reconstruction algorithms that we do not need explicit knowledge on the , in practice often unknown , sparsifying basis . We followed standard practice in deep learning by initializing all layers with their default Keras initializations , i.e.glorot uniform ( Glorot , 2010 ) , which we found to be working well . The logits of the distributions to be trained in the subsampling part of the network were initialized as a uniform , i.e.high-entropy , distribution , enabling most freedom for explorability of the sampling pattern by not explicitly setting a prior . We now detail on the initializations for each layer in the appendix of the revised manuscript . Question 3 : For linear problems such as reconstruction , there is a clear relationship between the number of input data versus number of unknowns . In this paper we focus on non-linear reconstruction , as well as other tasks such as object classification . Under this scope , it becomes unclear if we can still consider problems to be over- or underdetermined from a traditional point of view , and a more general information theoretic standpoint might proof fruitful . Concretely , as deep learning methods are optimized stochastically , they are expected to be drawn towards solutions that carry the largest signal for the downstream task . In the face of redundant input samples and under pressure of a small number of output samples , the method is thus expected to randomly select just one of these redundant samples as this would improve the loss of the model . As some tentative evidence to support this claim , we would refer you to figure 2a ( 96.8 % removed ) , where almost no directly neighbouring pixels are sampled , showing a clear preference of the model for skipping redundant samples ."}], "0": {"review_id": "SJeq9JBFvH-0", "review_text": "This paper introduces a novel DPS(Deep Probabilistic Subsampling) framework for the task-adaptive subsampling case, which attempts to resolve the issue of end-to-end optimization of an optimal subset of signal with jointly learning a sub-Nyquist sampling scheme and a predictive model for downstream tasks. The parameterization is used to simplify the subsampling distribution and ensure an expressive yet tractable distribution. The new approach contribution is applied to both reconstruction and classification tasks and demonstrated with a suite of experiments in a toy dataset, MINIST, and COFAR10. Overall, the paper requires significant improvement. 1. The approach is not well justified either by theory or practice. There is no experiment clearly shows convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches (Xie & Ermon (2019); Kool et al. (2019); Pl\u00c2\u0161otz & Roth (2018) ). 2. The paper never clearly demonstrates the problem they are trying to solve (nor well differentiates it from the compressed sensing problem or sample selection problem) The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution. 3. The paper is not nicely written or rather easy to follow. The model is not well motivated and the optimization algorithm is also not well described. 4. A theoretical analysis of the convergence of the optimization algorithm could be needed. 5. The paper is imprecise and unpolished and the presentation needs improvement. **There are so many missing details or questions to answer** 1. What is the Gumbel-max trick? 2. How to tune the parameters discussed in training details in the experiments? 3. Why to use experience replay for the linear experiments? 4. Are there evaluations on the utility of proposed compared to existing approaches? 5. Does the proposed approach work in real-world problems? 6. Was there any concrete theoretical guarantee to ensure the convergence of the algorithm. [Post Review after discussion]\u0010: The uploaded version has significantly improved over the first submission. It is now acceptable. ", "rating": "6: Weak Accept", "reply_text": "Question 3 : We did our best to write the paper such that it includes all details needed to fully understand the proposed method and its theoretical background . The referee indicates ( in sharp contrast to referee 3 ) that the paper is not nicely written , nor easy to follow . We invite the referee to be specific about the sections of the original manuscript that need more clarification , allowing us to revise these sections . Question 4 : The optimization algorithm used is the ADAM optimizer ( Kingma & Ba , 2014 ) . We refer to section 4 of Kingma & Ba ( 2014 ) for a proof of convergence for convex functions . It is known that that loss surfaces of deep neural networks are typically non-convex , however the gap between global and local minima is believed to be small for ( see our answer to the referee \u2019 s last question for more detail on this statement ) . Question 5 : We kindly ask the reviewer to elaborate on the given statement . Could the reviewer indicate which sections are found to be imprecise and unpolished , and which parts of the manuscript need a better presentation ? ===================================== Second part of the review ===================================== Question 1 : We specify the Gumbel-max trick in the paragraph below equation 4 . To make the paper more self-contained , we will extend this paragraph to further clarify the Gumbel-max trick . We also refer to our answer to the first question of this referee , in which we elaborated more on the Gumbel-max trick as well . Question 2 : All training parameters were tuned empirically . However , we agree it is worth elaborating on our insights regarding the influence on performance of some of them . We experienced that performance was most sensitive to the learning rates for the sampling and task models , and the temperature parameter tau of the softmax relaxation . We augmented the discussion of our revised manuscript to share these insights . .Question 3 : We know experience replay as a reinforcement learning technique for storing previous state/action pairs . However , our method does not make use of reinforcement learning , so could the reviewer please elaborate how experience replay would relate to our method ? Question 4 : In Section 4.1 ( MNIST classification ) we already compared our proposed sampling method to used Gumbel top-K sampling for data subsampling . We are currently also running experiments that allow for extensive comparison with the recently proposed LOUPE method by Bahadir et al . ( 2019 ) .Question 5 : A large part of the experiments in this work are focusing on compressive/partial Fourier measurements . This adequately reflects the measurement setup in many real-world problems , such as k-space measurement in magnetic resonance imaging ( Lustig et al . ) , Xampling for ultrasound imaging ( Eldar et al . ) , and non-uniform step frequency radar ( Huang , 2014 ) . In addition , we cover direct pixel sampling , related to real-world applications such as compressive cameras . We would like to emphasize that the proposed approach is measurement-domain agnostic , and therefore can be applied across a vast amount of real-world problem . In addition , our ongoing research already shows promising results for real-world applications such as magnetic resonance imaging and ultrasound imaging . This is part of future work . Question 6 : The trends towards using deep learning for data-driven compressed sensing indeed has the downside of not having guarantees on finding a global minimum , as the loss surface of a NN is highly non-linear and non-convex . Still , these data-driven results have shown to be very promising ( Gregor et al. , 2010 ; Jin,2019 ; Bahadir et al. , 2019 ; Mousavi , 2019 ) However due to the weight space symmetry problem ( Goodfellow et al. , 2016 ) the loss surface contains a vast amount of local minima with the same error value . The size of the gap between local and the global minima remains an open field of research . However , citing from Goodfellow et al . ( 2016 ) : \u201c The problem remains an active area of research , but experts now suspect that , for su\ufb03ciently large neural networks , most local minima have a low cost function value , and that it is not important to \ufb01nd a true global minimum rather than to \ufb01nd a point in parameter space that has low but not minimal cost ( Saxe et al. , 2013 ; Dauphin et al. , 2014 ; Goodfellow et al. , 2015 ; Choromanska et al. , 2014 ) \u201d As such , we leverage the empirically-shown ability of stochastic gradient descent to optimize this non-convex function in our NN for finding local minima . Indeed there is no guarantee on finding a global optimum ."}, "1": {"review_id": "SJeq9JBFvH-1", "review_text": "The paper proposes a learning-based adaptive compressed sensing framework in which both the sampling and the task functions (e.g., classification) are learned jointly end-to-end. The main contribution includes using the Gumbel-softmax trick to relax categorical distributions and use back-propagation to estimate the gradient jointly with the tas neural network. The proposed solution has the flexibility of able to be used in several different tasks, such as inverse problems ( super-resolution or image completion) or classification tasks. The paper is very well written. The paper locates itself well in current baselines and explains Experiments mostly well. However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique: 1) The only comparison to another non-fixed sampling baseline is Kool et al. 2019. The visualization and a thorough comparison were missing in MNIST classification. This baseline was also missing in image reconstruction. 2) Compressive Sensing incorporates vast literature of algorithms focusing on different aspects of improvements; algorithms focused on classification and inverse problems. Even if done disjointly, how does the proposed joint learning is compared to those algorithms in these domains? 3) Top row of Figure 3 nicely explains how the learned sampling paradigm performs compared to other mechanisms (such as uniform, random, low-pass). But there is no comparision against other non-fixed techniques. ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the positive and constructive feedback . Below we answer the questions and concerns : Question 1 : We agree with the referee and will therefore include a visualization of the trained distributions using Gumbel top-k sampling and a realization of the sampling pattern . We are currently running experiments to obtain Gumbel top-k results for the \u2018 lines and circles \u2019 and CIFAR10 experiments as well . Since we did not sufficiently emphasize that leveraging Gumbel top-k sampling for learning signal subsampling matrices is part of the novelty of the present work , we clarified this in the revised manuscript . In fact , using Gumbel top-k sampling in this context can be seen as a constrained version of DPS , with shared weights across the M distributions . To also include previously-published baselines , we are currently running experiments with the recently proposed LOUPE method by Bahadir et al . ( 2019 ) .Question 2 : Indeed , the notion of compressed sensing has spurred vast work , ranging from sensing strategies to signal recovery algorithms . On the sensing side , sampling strategies are typically designed to satisfy the Restricted Isometry Property ( RIP ) ; describing isometry of the sensing matrix given K-sparse vectors , and thereby providing signal recovery guarantees , given an appropriate algorithm . On the algorithm side , sparsity in some basis transform is typically exploited , leveraging a wide variety of optimization algorithms spanning from proximal gradient methods to projection-over-convex-set and greedy algorithms . More recently , deep learning methods have been proposed for fast signal recovery from CS measurements , yielding state-of-the-art results . In this context , DPS adopts current practices in data-driven CS recovery , but extends this to incorporate subsampling ( the sensing ) in an end-to-end pipeline . Such an end-to-end ( sampling-to-any-task ) learning strategy opens up opportunities for data-driven optimization of sensing strategies beyond theoretically-established results . As pointed out by the referee , the shortcomings of disjoint optimization in classical CS are perhaps most evident when high-level tasks such as classification are part of the pipeline . As such , we are currently running additional experiments to better illustrate this . Question 3 : We agree with the reviewer that such a comparison might be of interest . As such , we are currently running additional experiments to include a comparison to Gumbel top-k ( as we did for the MNIST classification case ) as well as the method proposed by Bahadir et al . ( 2019 ) .Notably , and unlike our method , the latter approach does not permit setting a specific subsampling rate , with this rate is only being indirectly controlled via hyperparameter settings ."}, "2": {"review_id": "SJeq9JBFvH-2", "review_text": "The authors propose a new approach of deep probabilistic subsampling for compressed sensing, based on Gumbel-softmax, which is interesting. A few points should be clarified: - in compressed sensing one has e.g. the restricted isometry property (RIP) related to recovery. How does the new method relate to such theoretical results? Are the results and findings along similar lines as (classical) compressed sensing theory? - Methods in compressed sensing are typically convex, e.g. using l1-regularization. What are the drawbacks of using deep learning in this context, e.g. related to non-convexity? What is the role of initialization? - Does the method both work for underdetermined and overdetermined problems (number of data versus number of unknowns)? - What is the influence of the hyper-parameters mu and lambda in eq (14)? How should the model selection be done (currently lambda is set to 0.004 without further motivation)? - MNIST: 60,000 instead of 70,000? ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the interesting questions regarding our work . Below we respond to these questions of the referee : Question 1 : In compressed sensing , RIP is indeed used to provide a measure for isometry when \u201c restricted \u201d to k columns , i.e.given a measurement $ \\mathbf { y } =\\mathbf { A } \\mathbf { x } $ , of a k-sparse vector $ \\mathbf { x } $ . For many practical problems of interest , analysis of measurements of sparse vectors is achieved by reformulating the measurement as $ \\mathbf { y } =\\mathbf { A } \\mathbf { \\Psi } \\mathbf { x } $ , with $ \\mathbf { \\Psi } $ being a sparsifying basis , and $ \\mathbf { z } =\\mathbf { \\Psi } \\mathbf { x } $ being the quantity of interest . Then , RIP should be evaluated for $ \\mathbf { A } \\mathbf { \\Psi } $ . A common requirement posed for sparse bases is therefore incoherence of the columns . Instead , we here directly learn a mapping to $ \\mathbf { z } $ from data , with no explicit notion of such a sparsifying basis . While this makes theoretical assessment more challenging , it alleviates the need for manual identification of a proper sparse basis for each new problem . We augmented parts of the discussion of the revised manuscript to better reflect this . Question 2 : The reviewer raises a fundamental and interesting question regarding the typical loss surface in CS compared to the one of a neural network . Indeed the loss surface of a NN is highly non-linear and non-convex , it typically contains a vast amount of local minima , as a consequence of the weight space symmetry property ( Goodfellow et al. , 2016 ) , i.e.having the same loss value for a different ordering of the same weights . The size of the gap between local and the global minima remains an open field of research . However , citing from Goodfellow et al . ( 2016 ) : \u201c The problem remains an active area of research , but experts now suspect that , for su\ufb03ciently large neural networks , most local minima have a low cost function value , and that it is not important to \ufb01nd a true global minimum rather than to \ufb01nd a point in parameter space that has low but not minimal cost ( Saxe et al. , 2013 ; Dauphin et al. , 2014 ; Goodfellow et al. , 2015 ; Choromanska et al. , 2014 ) \u201d As such , we leverage the empirically-shown ability of stochastic gradient descent to optimize this non-convex function . Indeed there are no global convergence guarantees , but we have the strong advantage compared to typical L1-reconstruction algorithms that we do not need explicit knowledge on the , in practice often unknown , sparsifying basis . We followed standard practice in deep learning by initializing all layers with their default Keras initializations , i.e.glorot uniform ( Glorot , 2010 ) , which we found to be working well . The logits of the distributions to be trained in the subsampling part of the network were initialized as a uniform , i.e.high-entropy , distribution , enabling most freedom for explorability of the sampling pattern by not explicitly setting a prior . We now detail on the initializations for each layer in the appendix of the revised manuscript . Question 3 : For linear problems such as reconstruction , there is a clear relationship between the number of input data versus number of unknowns . In this paper we focus on non-linear reconstruction , as well as other tasks such as object classification . Under this scope , it becomes unclear if we can still consider problems to be over- or underdetermined from a traditional point of view , and a more general information theoretic standpoint might proof fruitful . Concretely , as deep learning methods are optimized stochastically , they are expected to be drawn towards solutions that carry the largest signal for the downstream task . In the face of redundant input samples and under pressure of a small number of output samples , the method is thus expected to randomly select just one of these redundant samples as this would improve the loss of the model . As some tentative evidence to support this claim , we would refer you to figure 2a ( 96.8 % removed ) , where almost no directly neighbouring pixels are sampled , showing a clear preference of the model for skipping redundant samples ."}}