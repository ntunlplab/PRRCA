{"year": "2019", "forum": "HkgEQnRqYQ", "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space", "decision": "Accept (Poster)", "meta_review": "This paper proposes a knowledge graph completion approach that represents relations as rotations in a complex space; an idea that the reviewers found quite interesting and novel. The authors provide analysis to show how this model can capture symmetry/assymmetry, inversions, and composition. The authors also introduce a separate contribution of self-adversarial negative sampling, which, combined with complex rotational embeddings, obtains state of the art results on the benchmarks for this task.\n\nThe reviewers and the AC identified a number of potential weaknesses in the initial paper: (1) the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, (2) citation and comparison to a closely related approach (TorusE), and (3) a number of presentation issues early on in the paper.\n\nThe reviewers appreciated the author's comments and the revision, which addressed all of the concerns by including (1) additional experiments to performance with and without self-adversarial sampling, and comparisons to TorusE, (2) improved presentation.\n\nWith the revision, the reviewers agreed that this is a worthy paper to include in the conference.\n", "reviews": [{"review_id": "HkgEQnRqYQ-0", "review_text": "# Summary This paper presents a neural link prediction scoring function that can infer symmetry, anti-symmetry, inversion and composition patterns of relations in a knowledge base, whereas previous methods were only able to support a subset. The method achieves state of the art on FB15k-237, WN18RR and Countries benchmark knowledge bases. I think this will be interesting to the ICLR community. I particularly enjoyed the analysis of existing methods regarding the expressiveness of relational patterns mentioned above. # Strengths - Improvements over prior neural link prediction methods - Clearly written paper - Interesting analysis of existing neural link prediction methods # Weaknesses - As the authors not only propose a new scoring function for neural link prediction but also an adversarial sampling mechanism for negative data, I believe a more careful ablation study should have been carried out. There is an ablation study showing the impact of the negative sampling on the baseline TransE, as well as another ablation in the appendix demonstrating the impact of negative sampling on TransE and the proposed method, RotatE, for the FB15k-237. However, from Table 10 in the appendix, one can see that the two competing methods, TransE and RotatE, in fact, perform fairly similarly once both use adversarial sampling it still remains unclear whether the gains observed in table 4 and 5 are due to adversarial sampling or a better scoring function. Particularly, I want to see results of a stronger baseline, ComplEx, equipped with the adversarial sampling approach. Ideally, I would also like to see multiple repeats of the experiments to get a sense of the variance of the results (as it has been done for Countries in Table 6). # Minor Comments - Eq 5: Already introduce gamma (the fixed margin) here. - While I understand that this paper focuses on knowledge graph embeddings, I believe the large body of other relational AI approaches should be mention as some of them can also model symmetry, anti-symmetry, inversion and composition patterns of relations as well (though they might be less scalable and therefore of less practical relevance), e.g. the following come to mind: - Lao et al. (2011). Random walk inference and learning in a large scale knowledge base. - Neelakantan et al. (2015). Compositional vector space models for knowledge base completion. - Das et al. (2016). Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks. - Rocktaschel and Riedel (2017). End-to-end Differentiable Proving. - Yang et al. (2017). Differentiable Learning of Logical Rules for Knowledge Base Completion. - Table 6: How many repeats were used for estimating the standard deviation? Update: I thank the authors for their response and additional experiments. I am increasing my score to 7.", "rating": "7: Good paper, accept", "reply_text": "\u201c Particularly , I want to see results of a stronger baseline , ComplEx , equipped with the adversarial sampling approach\u2026. \u201d We have added the experimental results of TransE and ComplEx on three datasets in our paper ( Table 8 ) . We can see that our proposed approach still outperforms ComplEx with the new adversarial approach , especially on the data set FB15k-237 and Countries . The reason is that FB15k-237 and Countries contain many composition patterns , which can not be modeled by ComplEx but can be effectively modeled by RotatE . \u201c Ideally , I would also like to see multiple repeats of the experiments to get a sense of the variance of the results ... \u201d We also added the variance of the results of our model on different data sets , which are summarized into Table 12 in the appendix . We can see that the variance of the results are very small , 0.001 at maximum . \u201c Table 6 : How many repeats were used for estimating the standard deviation ? \u201d Only 3 are used . Since the variance are very small , the same results are obtained with more repeats . \u201c While I understand that this paper focuses on knowledge graph embeddings , I believe the large body of other relational AI approaches should be mention\u2026. \u201d We have added some discussion on these methods in the related work section ."}, {"review_id": "HkgEQnRqYQ-1", "review_text": "The authors propose to model the relations as a rotation in the complex vector space. They show that this way one can model symmetry/antisymmetry, inversion and composition. Another contribution is the so-called self-adversarial negative sampling. Pros: The problem that they raise is important and the solution is relevant. The results considering the simplicity of the proposed model are impressive. The experiments, proof of lemmas and general overview are easy to follow, well-written and well-organized. The improvement given the negative sampling approach is also noteworthy. Cons: Nevertheless, this approach is very similar to TorusE [1], since the element-wise rotation on the complex plane is somehow related to transformation on high-dimensional Torus. Therefore, it is expected from the authors to investigate the differences between these two approaches. Suggestions: Also, it is important to note the result of ablation study on Table 10 in supplementary materials, since part of the improvement does not come only from how the authors model the relation but also from the negative sampling(which could improve the results of other works as well). Maybe it is even better if Table 10 is presented in the main paper. Another suggestion is to mention the negative sampling contribution also in the abstract. [1] Ebisu, Takuma, and Ryutaro Ichise. \"Toruse: Knowledge graph embedding on a lie group.\" arXiv preprint arXiv:1711.05435 (2017).\" ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your appreciation to our work and your great comments on improving the paper . We have added the experimental results of TransE and ComplEx with self-adversarial negative sampling on three datasets in our paper ( Table 8 ) . We have also added the contribution of the self-adversarial negative sampling into both the abstract and introduction . Regarding TorusE , thanks again for bringing it to our attention , which we did not notice before . It is indeed relevant to our model , which is a concurrent work . We have discussed this model in the related work section . The difference between TorusE and RotatE can be summarized as below : ( 1 ) The TorusE model constraints the embedding of objects on a torus , and models relations as translations , while the RotatE model embeds objects on the entire complex vector space , and models relations as rotations . ( 2 ) The TorusE model requires embedding objects on a compact Lie group [ 2 ] while the RotatE model allows embedding objects on a non-compact Lie group , which has much more representation capacity . The TorusE model is actually very close to a special case of our model , i.e. , pRotatE , which constraints the modulus of the head and entity embeddings fixed . As shown in Table 5 , it is very important for modeling and inferring the composition patterns by embedding the entities on a non-compact Lie group . We can also compare the results of TorusE and RotatE on the FB15k and WN18 data sets ( Table 3 in the TorusE paper and Table 4 in our paper ) , we can see that our RotatE model significantly outperforms TorusE on the two data sets . ( 3 ) The motivations of the TorusE paper and this paper are quite different . The TorusE paper aims to solve the regularization problem of TransE , while our paper focuses on inferring and modeling three important and popular relation patterns . [ 1 ] Ebisu , Takuma , and Ryutaro Ichise . `` Toruse : Knowledge graph embedding on a lie group . '' arXiv preprint arXiv:1711.05435 ( 2017 ) . '' [ 2 ] https : //en.wikipedia.org/wiki/Compact_group # Compact_Lie_groups"}, {"review_id": "HkgEQnRqYQ-2", "review_text": "The paper proposes a method for graph embedding to be used for link prediction, in which each entity is represented as a vector in complex space and each relation is modeled as a rotation from the head entity to the tale entity. From the modeling perspective, the proposed model is rich as many type of relations can be modeled with it. In particular, symmetric and anti-symmetric relations can be modeled. It is also possible to model the inverse of a relation and the composition of two relations with this setup. Empirical evaluation demonstrates that method is effective and beats a number of well known competitors. This is a solid work and could be of interest in the community. Modeling is elegant and experimental results are strong. I have not seen it proposed before. - The presentation of paper could be improved, in particular the first paragraph of page 2 where the representation in complex domain is introduced is hard to follow and could be improved by inserting formulations instead of merely text. It would be nice to explicitly mention the number of real and imaginary dimensions of the complex vectors and provide explicit formulation for the Hadamard product on the complex domain, since the term elementwise could be ambiguous. - The optimization section does not mention how constraints are imposed. This is an important technicality and should be clarified. - In experiments, how does the effective number of parameters that are used to express representations compare when the representations are a complex vs a real number? Each complex number is presented with two parameters and each real number with one parameter. How is that taken into account in experiments - Since the method is reported to beat several number of competitors, it is useful to provide the code. Based on the results above, I vote for the paper to be accepted. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your appreciation to our work and the great comments . We \u2019 ve revised the introduction part on the representations in complex domain . \u201c The optimization section does not mention how constraints are imposed. \u201d Since each relation is modeled as a rotation in the complex vector space , we represent each relation r according to its polar form with its modulus as 1 , i.e. , Re ( r ) = sine ( \\theta ) , and Im ( r ) = cosine ( \\theta ) , where \\theta is the phase of relation r. With the polar form representation , the constraints can be easily satisfied . \u201c In experiments , how does the effective number of parameters that are used to express representations compare when the representations are a complex vs a real number \u2026. \u201d If the same number of dimension is used for both the real and imaginary parts of the complex number as the real number , the number of parameters for the complex embedding would be twice the number of parameters for the embeddings in the real space . To make a fair comparison , in the process of grid search for finding the optimum embedding dimension , we double the range of the search space for models represented in real space such as TransE . \u201c Since the method is reported to beat several number of competitors , it is useful to provide the code. \u201d Yes , we will definitely release our code and share it with the entire community ."}], "0": {"review_id": "HkgEQnRqYQ-0", "review_text": "# Summary This paper presents a neural link prediction scoring function that can infer symmetry, anti-symmetry, inversion and composition patterns of relations in a knowledge base, whereas previous methods were only able to support a subset. The method achieves state of the art on FB15k-237, WN18RR and Countries benchmark knowledge bases. I think this will be interesting to the ICLR community. I particularly enjoyed the analysis of existing methods regarding the expressiveness of relational patterns mentioned above. # Strengths - Improvements over prior neural link prediction methods - Clearly written paper - Interesting analysis of existing neural link prediction methods # Weaknesses - As the authors not only propose a new scoring function for neural link prediction but also an adversarial sampling mechanism for negative data, I believe a more careful ablation study should have been carried out. There is an ablation study showing the impact of the negative sampling on the baseline TransE, as well as another ablation in the appendix demonstrating the impact of negative sampling on TransE and the proposed method, RotatE, for the FB15k-237. However, from Table 10 in the appendix, one can see that the two competing methods, TransE and RotatE, in fact, perform fairly similarly once both use adversarial sampling it still remains unclear whether the gains observed in table 4 and 5 are due to adversarial sampling or a better scoring function. Particularly, I want to see results of a stronger baseline, ComplEx, equipped with the adversarial sampling approach. Ideally, I would also like to see multiple repeats of the experiments to get a sense of the variance of the results (as it has been done for Countries in Table 6). # Minor Comments - Eq 5: Already introduce gamma (the fixed margin) here. - While I understand that this paper focuses on knowledge graph embeddings, I believe the large body of other relational AI approaches should be mention as some of them can also model symmetry, anti-symmetry, inversion and composition patterns of relations as well (though they might be less scalable and therefore of less practical relevance), e.g. the following come to mind: - Lao et al. (2011). Random walk inference and learning in a large scale knowledge base. - Neelakantan et al. (2015). Compositional vector space models for knowledge base completion. - Das et al. (2016). Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks. - Rocktaschel and Riedel (2017). End-to-end Differentiable Proving. - Yang et al. (2017). Differentiable Learning of Logical Rules for Knowledge Base Completion. - Table 6: How many repeats were used for estimating the standard deviation? Update: I thank the authors for their response and additional experiments. I am increasing my score to 7.", "rating": "7: Good paper, accept", "reply_text": "\u201c Particularly , I want to see results of a stronger baseline , ComplEx , equipped with the adversarial sampling approach\u2026. \u201d We have added the experimental results of TransE and ComplEx on three datasets in our paper ( Table 8 ) . We can see that our proposed approach still outperforms ComplEx with the new adversarial approach , especially on the data set FB15k-237 and Countries . The reason is that FB15k-237 and Countries contain many composition patterns , which can not be modeled by ComplEx but can be effectively modeled by RotatE . \u201c Ideally , I would also like to see multiple repeats of the experiments to get a sense of the variance of the results ... \u201d We also added the variance of the results of our model on different data sets , which are summarized into Table 12 in the appendix . We can see that the variance of the results are very small , 0.001 at maximum . \u201c Table 6 : How many repeats were used for estimating the standard deviation ? \u201d Only 3 are used . Since the variance are very small , the same results are obtained with more repeats . \u201c While I understand that this paper focuses on knowledge graph embeddings , I believe the large body of other relational AI approaches should be mention\u2026. \u201d We have added some discussion on these methods in the related work section ."}, "1": {"review_id": "HkgEQnRqYQ-1", "review_text": "The authors propose to model the relations as a rotation in the complex vector space. They show that this way one can model symmetry/antisymmetry, inversion and composition. Another contribution is the so-called self-adversarial negative sampling. Pros: The problem that they raise is important and the solution is relevant. The results considering the simplicity of the proposed model are impressive. The experiments, proof of lemmas and general overview are easy to follow, well-written and well-organized. The improvement given the negative sampling approach is also noteworthy. Cons: Nevertheless, this approach is very similar to TorusE [1], since the element-wise rotation on the complex plane is somehow related to transformation on high-dimensional Torus. Therefore, it is expected from the authors to investigate the differences between these two approaches. Suggestions: Also, it is important to note the result of ablation study on Table 10 in supplementary materials, since part of the improvement does not come only from how the authors model the relation but also from the negative sampling(which could improve the results of other works as well). Maybe it is even better if Table 10 is presented in the main paper. Another suggestion is to mention the negative sampling contribution also in the abstract. [1] Ebisu, Takuma, and Ryutaro Ichise. \"Toruse: Knowledge graph embedding on a lie group.\" arXiv preprint arXiv:1711.05435 (2017).\" ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your appreciation to our work and your great comments on improving the paper . We have added the experimental results of TransE and ComplEx with self-adversarial negative sampling on three datasets in our paper ( Table 8 ) . We have also added the contribution of the self-adversarial negative sampling into both the abstract and introduction . Regarding TorusE , thanks again for bringing it to our attention , which we did not notice before . It is indeed relevant to our model , which is a concurrent work . We have discussed this model in the related work section . The difference between TorusE and RotatE can be summarized as below : ( 1 ) The TorusE model constraints the embedding of objects on a torus , and models relations as translations , while the RotatE model embeds objects on the entire complex vector space , and models relations as rotations . ( 2 ) The TorusE model requires embedding objects on a compact Lie group [ 2 ] while the RotatE model allows embedding objects on a non-compact Lie group , which has much more representation capacity . The TorusE model is actually very close to a special case of our model , i.e. , pRotatE , which constraints the modulus of the head and entity embeddings fixed . As shown in Table 5 , it is very important for modeling and inferring the composition patterns by embedding the entities on a non-compact Lie group . We can also compare the results of TorusE and RotatE on the FB15k and WN18 data sets ( Table 3 in the TorusE paper and Table 4 in our paper ) , we can see that our RotatE model significantly outperforms TorusE on the two data sets . ( 3 ) The motivations of the TorusE paper and this paper are quite different . The TorusE paper aims to solve the regularization problem of TransE , while our paper focuses on inferring and modeling three important and popular relation patterns . [ 1 ] Ebisu , Takuma , and Ryutaro Ichise . `` Toruse : Knowledge graph embedding on a lie group . '' arXiv preprint arXiv:1711.05435 ( 2017 ) . '' [ 2 ] https : //en.wikipedia.org/wiki/Compact_group # Compact_Lie_groups"}, "2": {"review_id": "HkgEQnRqYQ-2", "review_text": "The paper proposes a method for graph embedding to be used for link prediction, in which each entity is represented as a vector in complex space and each relation is modeled as a rotation from the head entity to the tale entity. From the modeling perspective, the proposed model is rich as many type of relations can be modeled with it. In particular, symmetric and anti-symmetric relations can be modeled. It is also possible to model the inverse of a relation and the composition of two relations with this setup. Empirical evaluation demonstrates that method is effective and beats a number of well known competitors. This is a solid work and could be of interest in the community. Modeling is elegant and experimental results are strong. I have not seen it proposed before. - The presentation of paper could be improved, in particular the first paragraph of page 2 where the representation in complex domain is introduced is hard to follow and could be improved by inserting formulations instead of merely text. It would be nice to explicitly mention the number of real and imaginary dimensions of the complex vectors and provide explicit formulation for the Hadamard product on the complex domain, since the term elementwise could be ambiguous. - The optimization section does not mention how constraints are imposed. This is an important technicality and should be clarified. - In experiments, how does the effective number of parameters that are used to express representations compare when the representations are a complex vs a real number? Each complex number is presented with two parameters and each real number with one parameter. How is that taken into account in experiments - Since the method is reported to beat several number of competitors, it is useful to provide the code. Based on the results above, I vote for the paper to be accepted. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for your appreciation to our work and the great comments . We \u2019 ve revised the introduction part on the representations in complex domain . \u201c The optimization section does not mention how constraints are imposed. \u201d Since each relation is modeled as a rotation in the complex vector space , we represent each relation r according to its polar form with its modulus as 1 , i.e. , Re ( r ) = sine ( \\theta ) , and Im ( r ) = cosine ( \\theta ) , where \\theta is the phase of relation r. With the polar form representation , the constraints can be easily satisfied . \u201c In experiments , how does the effective number of parameters that are used to express representations compare when the representations are a complex vs a real number \u2026. \u201d If the same number of dimension is used for both the real and imaginary parts of the complex number as the real number , the number of parameters for the complex embedding would be twice the number of parameters for the embeddings in the real space . To make a fair comparison , in the process of grid search for finding the optimum embedding dimension , we double the range of the search space for models represented in real space such as TransE . \u201c Since the method is reported to beat several number of competitors , it is useful to provide the code. \u201d Yes , we will definitely release our code and share it with the entire community ."}}