{"year": "2017", "forum": "HyoST_9xl", "title": "DSD: Dense-Sparse-Dense Training for Deep Neural Networks", "decision": "Accept (Poster)", "meta_review": "Important problem, simple (in a positive way) idea, broad experimental evaluation; all reviewers recommend accepting the paper, and the AC agrees. Please incorporate any remaining reviewer feedback.", "reviews": [{"review_id": "HyoST_9xl-0", "review_text": "Summary: The paper proposes a model training strategy to achieve higher accuracy. The issue is train a too large model and you going to over-fit and your model will capture noise. Prune models or make it too small then it will miss important connections and under-fit. Thus, the proposed method involves various training steps: first they train a dense network, then prune it making it sparse then train a sparse network and finally they add connections back and train the model as dense again (DSD). The DSD method is generic method that can be used in CNN/RNN/LSTM. The reasons why models have better accuracy after DSD are: escape of saddle point, sparsity makes model more robust to noise and symmetry break allowing richer representations. Pro: The main point that this paper wants to show is that a model has the capacity to achieve higher accuracy, because it was shown that it is possible to compress a model without losing accuracy. And lossless compression means that there\u2019s significant redundancy in the models that were trained using current training methods. This is an important observation that large models can get better accuracies as better training schemes are used. Cons & Questions: The issue is that the accuracy is slightly increased (2 or 3%) for most models. And the question is what is the price paid for this improvement? Resource and performance concerns arises because training a large model is computationally expensive (hours or even days using high performance GPUs). Second question, can I keep adding Dense, Sparse and Dense training iterations to get higher and higher accuracy improvement? Are there limitations to this DSDSD\u2026 approach? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the kind words in the review . 1.a ) The 2 % or 3 % increase of accuracy for a well-designed model is non-trivial . The baseline ResNet-50 top5 error is already below 7 % , improving on such good model is not an easy task . I have chatted with a notable web company in China , they pay 1Million CNY to buy more dataset just to increase the relative accuracy by only 5 % , while the performance improvement of DSD training is 3.6 % -13.7 % . 1.b ) Fine-tuning a model with DSD is indeed extra time , but the longer training time is dwarfed by the time it took to do design space exploration for new topology/architecture and tune hyperparameters . For the tens to hundreds of models that were trained and then discarded , only one is fine-tuned with DSD . 1.c ) We also explored DSD training with shorter epochs , this is possible by pruning early , i.e.prune before the dense model fully converges . In the DeepSpeech experiments , weights are pruned early after training the initial model prematurely for only 50 epochs . Shown in Table 10 , the baseline training converged to WER of 28.03 % /33.55 % with 150 epochs , while DSD training converged to 27.90 % /32.20 % WER with 50+50+50=150 epochs . So , same epochs , better accuracy . With another SD iteration it can do even better . 2 ) Yes , we can do DSDSD . We have already listed the DSDSD result in Table 10 and Table 12 , where we have five rows of result for each DSDSD step . The limitation to this approach is the diminishing returns as we add more iterations ."}, {"review_id": "HyoST_9xl-1", "review_text": "Training highly non-convex deep neural networks is a very important practical problem, and this paper provides a great exploration of an interesting new idea for more effective training. The empirical evaluation both in the paper itself and in the authors\u2019 comments during discussion convincingly demonstrates that the method achieves consistent improvements in accuracy across multiple architectures, tasks and datasets. The algorithm is very simple (alternating between training the full dense network and a sparse version of it), which is actually a positive since that means it may get adapted in practice by the research community. The paper should be revised to incorporate the additional experiments and comments from the discussion, particularly the accuracy comparisons with the same number of epochs. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the comments . The same # epochs experimental results are addressed in the reply to reviewer1 , to facilitate reading let me paste here : We have compared DSD v.s . conventional training for the same number of epochs in Deep Speech ( Table 7 ) and Deep Speech2 ( Table 9 ) . Baseline training fully converged to WER of 28.03 % /33.55 % with 150 epochs , while DSD training achieved a better 27.90 % /32.20 % WER with the same total 50+50+50=150 epochs . Similar for Deep Speech2 ( Table 9 ) , baseline training fully converged to WER of 9.55 % /14.52 % with 60 epochs , while DSD training achieved a better 9.11 % /13.96 % WER with the same total 20+20+20=60 epochs . For both Deep Speech and Deep Speech2 , we trained the baseline models by ourselves so we make sure it 's fully converged . Thus it is a fair comparison that DSD is advantageous . Even worked on a model that is not fully converged on imagenet , DSD can win the * fully converged * model by a large margin ( 27.2 % compared with 29.3 % top1 error ) . To further verify the advantage of DSD training is statistically significant , we did T-test by repeating the experiment 16 times on cifar10 using ResNet-20 to compare the error rate of DSD training and conventional fine-tuning ( by dropping the learning rate upon `` convergence '' and continuing to learn ) under the same number of epochs . The DSD training on average achieved Top-1 testing error of 7.89 % , which is 0.37 % absolute improvements ( 4.5 % relative improvements ) from the baseline model and relatively 1.1 % better than what the conventional fine-tuning achieves . The results demonstrate the DSD training achieves significant improvements from both the baseline model ( T-test result with p < 0.001 ) and conventional fine tuning ( T-test result with p < 0.001 ) . The above T-test results are in the appendix . The T-test experiment also shows that DSD could reduce the variance of learning . In the 16 repeated experiments , DSD training has lower standard deviation of errors ( sparse phase std=0.05 % , dense phase std=0.03 % ) compared with their counterparts using conventional fine-tuning ( first phase std=0.08 % , second phase std=0.04 % ) . Each phase corresponds to the same number of epochs . Hope these empirical result helps clarify the question . We already incorporated related results at Table 7/9 and at the end of page 6 and also in appendix-A , and we 'd be happy to add more discussion to the paper ."}, {"review_id": "HyoST_9xl-2", "review_text": "This paper presents a training strategy for deep networks. First, the network is trained in a standard fashion. Second, small magnitude weights are clamped to 0; the rest of the weights continue to be trained. Finally, all the weights are again jointly trained. Experiments on a variety of image, text, and speech datasets demonstrate the approach can obtain high-quality results. The proposed idea is novel and interesting. In a sense it is close to Dropout, though as noted in the paper the deterministic weight clamping method is different. The main advantage of the proposed method is its simplicity. Three hyper-parameters are needed: the number of weights to clamp to 0, and the numbers of epochs of training used in the first dense phase and the sparse phase. Given these, it can be plugged in to training a range of networks, as shown in the experiments. The concern I have is regarding the current empirical evaluation. As noted in the question phase, it seems the baseline methods are not trained for as many epochs as the proposed method. Standard tricks, such as dropping the learning rate upon \"convergence\" and continuing to learn, can be employed. The response seems to indicate that these approaches can be effective. I think a more thorough empirical analysis of performance over epochs, learning rates, etc. would strengthen the paper. An exploration regarding the sparsity hyper-parameter would also be interesting. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments . The reviewer 's concern is unnecessary . We have compared using DSD method and baseline method for the same number of epochs in Deep Speech ( Table 7 ) and Deep Speech2 ( table 9 ) . Baseline training fully converged to WER of 28.03 % /33.55 % with 150 epochs , while DSD training achieved a better 27.90 % /32.20 % WER with the same total 50+50+50=150 epochs . Similar for Deep Speech2 ( Table 9 ) , baseline training fully converged to WER of 9.55 % /14.52 % with 60 epochs , while DSD training achieved a better 9.11 % /13.96 % WER with the same total 20+20+20=60 epochs . For both Deep Speech and Deep Speech2 we trained the baseline models by ourselves so we make sure it 's fully converged . Thus it is a fair comparison that DSD is advantageous . The overall observation is : training for same # epochs , DSD can perform better ; training for another DSD iteration ( DSDSD ) , DSD can perform even better . Even worked on a model that is not fully converged on imagenet , DSD can win the * fully converged * model by a large margin ( 27.2 % compared with 29.3 % top1 error ) . To further verify the advantage of DSD training is statistically significant , we did T-test by repeating the experiment 16 times on cifar10 using ResNet-20 to compare the error rate of DSD training and conventional fine-tuning ( by dropping the learning rate upon `` convergence '' and continuing to learn ) under the same number of epochs . The DSD training on average achieved Top-1 testing error of 7.89 % , which is 0.37 % absolute improvements ( 4.5 % relative improvements ) from the baseline model and relatively 1.1 % better than what the conventional fine-tuning achieves . The results demonstrate the DSD training achieves significant improvements from both the baseline model ( T-test result with p < 0.001 ) and conventional fine tuning ( T-test result with p < 0.001 ) . The above T-test results are in the appendix . The T-test experiment also shows that DSD could reduce the variance of learning . In the 16 repeated experiments , DSD training has lower standard deviation of errors ( sparse phase std=0.05 % , dense phase std=0.03 % ) compared with their counterparts using conventional fine-tuning ( first phase std=0.08 % , second phase std=0.04 % ) . Each phase corresponds to the same number of epochs . Hope these empirical result helps clarify the question . We already incorporated related results at Table 7/9 and at the end of page 6 and also in appendix-A , and we 'd be happy to add more discussion to the paper ."}], "0": {"review_id": "HyoST_9xl-0", "review_text": "Summary: The paper proposes a model training strategy to achieve higher accuracy. The issue is train a too large model and you going to over-fit and your model will capture noise. Prune models or make it too small then it will miss important connections and under-fit. Thus, the proposed method involves various training steps: first they train a dense network, then prune it making it sparse then train a sparse network and finally they add connections back and train the model as dense again (DSD). The DSD method is generic method that can be used in CNN/RNN/LSTM. The reasons why models have better accuracy after DSD are: escape of saddle point, sparsity makes model more robust to noise and symmetry break allowing richer representations. Pro: The main point that this paper wants to show is that a model has the capacity to achieve higher accuracy, because it was shown that it is possible to compress a model without losing accuracy. And lossless compression means that there\u2019s significant redundancy in the models that were trained using current training methods. This is an important observation that large models can get better accuracies as better training schemes are used. Cons & Questions: The issue is that the accuracy is slightly increased (2 or 3%) for most models. And the question is what is the price paid for this improvement? Resource and performance concerns arises because training a large model is computationally expensive (hours or even days using high performance GPUs). Second question, can I keep adding Dense, Sparse and Dense training iterations to get higher and higher accuracy improvement? Are there limitations to this DSDSD\u2026 approach? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the kind words in the review . 1.a ) The 2 % or 3 % increase of accuracy for a well-designed model is non-trivial . The baseline ResNet-50 top5 error is already below 7 % , improving on such good model is not an easy task . I have chatted with a notable web company in China , they pay 1Million CNY to buy more dataset just to increase the relative accuracy by only 5 % , while the performance improvement of DSD training is 3.6 % -13.7 % . 1.b ) Fine-tuning a model with DSD is indeed extra time , but the longer training time is dwarfed by the time it took to do design space exploration for new topology/architecture and tune hyperparameters . For the tens to hundreds of models that were trained and then discarded , only one is fine-tuned with DSD . 1.c ) We also explored DSD training with shorter epochs , this is possible by pruning early , i.e.prune before the dense model fully converges . In the DeepSpeech experiments , weights are pruned early after training the initial model prematurely for only 50 epochs . Shown in Table 10 , the baseline training converged to WER of 28.03 % /33.55 % with 150 epochs , while DSD training converged to 27.90 % /32.20 % WER with 50+50+50=150 epochs . So , same epochs , better accuracy . With another SD iteration it can do even better . 2 ) Yes , we can do DSDSD . We have already listed the DSDSD result in Table 10 and Table 12 , where we have five rows of result for each DSDSD step . The limitation to this approach is the diminishing returns as we add more iterations ."}, "1": {"review_id": "HyoST_9xl-1", "review_text": "Training highly non-convex deep neural networks is a very important practical problem, and this paper provides a great exploration of an interesting new idea for more effective training. The empirical evaluation both in the paper itself and in the authors\u2019 comments during discussion convincingly demonstrates that the method achieves consistent improvements in accuracy across multiple architectures, tasks and datasets. The algorithm is very simple (alternating between training the full dense network and a sparse version of it), which is actually a positive since that means it may get adapted in practice by the research community. The paper should be revised to incorporate the additional experiments and comments from the discussion, particularly the accuracy comparisons with the same number of epochs. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the comments . The same # epochs experimental results are addressed in the reply to reviewer1 , to facilitate reading let me paste here : We have compared DSD v.s . conventional training for the same number of epochs in Deep Speech ( Table 7 ) and Deep Speech2 ( Table 9 ) . Baseline training fully converged to WER of 28.03 % /33.55 % with 150 epochs , while DSD training achieved a better 27.90 % /32.20 % WER with the same total 50+50+50=150 epochs . Similar for Deep Speech2 ( Table 9 ) , baseline training fully converged to WER of 9.55 % /14.52 % with 60 epochs , while DSD training achieved a better 9.11 % /13.96 % WER with the same total 20+20+20=60 epochs . For both Deep Speech and Deep Speech2 , we trained the baseline models by ourselves so we make sure it 's fully converged . Thus it is a fair comparison that DSD is advantageous . Even worked on a model that is not fully converged on imagenet , DSD can win the * fully converged * model by a large margin ( 27.2 % compared with 29.3 % top1 error ) . To further verify the advantage of DSD training is statistically significant , we did T-test by repeating the experiment 16 times on cifar10 using ResNet-20 to compare the error rate of DSD training and conventional fine-tuning ( by dropping the learning rate upon `` convergence '' and continuing to learn ) under the same number of epochs . The DSD training on average achieved Top-1 testing error of 7.89 % , which is 0.37 % absolute improvements ( 4.5 % relative improvements ) from the baseline model and relatively 1.1 % better than what the conventional fine-tuning achieves . The results demonstrate the DSD training achieves significant improvements from both the baseline model ( T-test result with p < 0.001 ) and conventional fine tuning ( T-test result with p < 0.001 ) . The above T-test results are in the appendix . The T-test experiment also shows that DSD could reduce the variance of learning . In the 16 repeated experiments , DSD training has lower standard deviation of errors ( sparse phase std=0.05 % , dense phase std=0.03 % ) compared with their counterparts using conventional fine-tuning ( first phase std=0.08 % , second phase std=0.04 % ) . Each phase corresponds to the same number of epochs . Hope these empirical result helps clarify the question . We already incorporated related results at Table 7/9 and at the end of page 6 and also in appendix-A , and we 'd be happy to add more discussion to the paper ."}, "2": {"review_id": "HyoST_9xl-2", "review_text": "This paper presents a training strategy for deep networks. First, the network is trained in a standard fashion. Second, small magnitude weights are clamped to 0; the rest of the weights continue to be trained. Finally, all the weights are again jointly trained. Experiments on a variety of image, text, and speech datasets demonstrate the approach can obtain high-quality results. The proposed idea is novel and interesting. In a sense it is close to Dropout, though as noted in the paper the deterministic weight clamping method is different. The main advantage of the proposed method is its simplicity. Three hyper-parameters are needed: the number of weights to clamp to 0, and the numbers of epochs of training used in the first dense phase and the sparse phase. Given these, it can be plugged in to training a range of networks, as shown in the experiments. The concern I have is regarding the current empirical evaluation. As noted in the question phase, it seems the baseline methods are not trained for as many epochs as the proposed method. Standard tricks, such as dropping the learning rate upon \"convergence\" and continuing to learn, can be employed. The response seems to indicate that these approaches can be effective. I think a more thorough empirical analysis of performance over epochs, learning rates, etc. would strengthen the paper. An exploration regarding the sparsity hyper-parameter would also be interesting. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments . The reviewer 's concern is unnecessary . We have compared using DSD method and baseline method for the same number of epochs in Deep Speech ( Table 7 ) and Deep Speech2 ( table 9 ) . Baseline training fully converged to WER of 28.03 % /33.55 % with 150 epochs , while DSD training achieved a better 27.90 % /32.20 % WER with the same total 50+50+50=150 epochs . Similar for Deep Speech2 ( Table 9 ) , baseline training fully converged to WER of 9.55 % /14.52 % with 60 epochs , while DSD training achieved a better 9.11 % /13.96 % WER with the same total 20+20+20=60 epochs . For both Deep Speech and Deep Speech2 we trained the baseline models by ourselves so we make sure it 's fully converged . Thus it is a fair comparison that DSD is advantageous . The overall observation is : training for same # epochs , DSD can perform better ; training for another DSD iteration ( DSDSD ) , DSD can perform even better . Even worked on a model that is not fully converged on imagenet , DSD can win the * fully converged * model by a large margin ( 27.2 % compared with 29.3 % top1 error ) . To further verify the advantage of DSD training is statistically significant , we did T-test by repeating the experiment 16 times on cifar10 using ResNet-20 to compare the error rate of DSD training and conventional fine-tuning ( by dropping the learning rate upon `` convergence '' and continuing to learn ) under the same number of epochs . The DSD training on average achieved Top-1 testing error of 7.89 % , which is 0.37 % absolute improvements ( 4.5 % relative improvements ) from the baseline model and relatively 1.1 % better than what the conventional fine-tuning achieves . The results demonstrate the DSD training achieves significant improvements from both the baseline model ( T-test result with p < 0.001 ) and conventional fine tuning ( T-test result with p < 0.001 ) . The above T-test results are in the appendix . The T-test experiment also shows that DSD could reduce the variance of learning . In the 16 repeated experiments , DSD training has lower standard deviation of errors ( sparse phase std=0.05 % , dense phase std=0.03 % ) compared with their counterparts using conventional fine-tuning ( first phase std=0.08 % , second phase std=0.04 % ) . Each phase corresponds to the same number of epochs . Hope these empirical result helps clarify the question . We already incorporated related results at Table 7/9 and at the end of page 6 and also in appendix-A , and we 'd be happy to add more discussion to the paper ."}}