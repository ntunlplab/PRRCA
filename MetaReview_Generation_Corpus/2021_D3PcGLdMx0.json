{"year": "2021", "forum": "D3PcGLdMx0", "title": "MELR: Meta-Learning via Modeling Episode-Level Relationships for Few-Shot Learning", "decision": "Accept (Poster)", "meta_review": "This paper explores the effect of poorly sampled episodes in few-shot learning, and its effect on trained models. The improvements from the additional attention module (CEAM) and regularizer (CECR) are strong, and the ablations are thorough. The reviewers are not fully convinced that poor sampling is indeed the main issue. That is, it could be that CEAM and CECR improve performance for other reasons, but the hypothesis is sensible, and the reviewers believe a more thorough investigation is beyond the scope of this work.\n\nDuring discussions, one note that came up is whether CEAM works because of cross-episode attention, or if the idea of an instance-level FEAT is itself a good one. One ablation to sort this out would be to apply FEAT and an instance-level FEAT on episodes that are twice as large as those seen by CEAM so that the effective episode size is the same. This would help answer: is it the reduced noise due to effectively larger episodes, a stronger attention mechanism using instance-level information, or is the idea of crossover episodes indeed the important factor? The reviewers agree that this baseline, or an analogous baseline, should be included in the final version.\n", "reviews": [{"review_id": "D3PcGLdMx0-0", "review_text": "Summary of Paper : This paper proposes to improve Prototypical Networks by a method MELR which aims to fix the problem caused by poorly represented classes in sampled episodes and to reap benefits from enforcing cross-episode consistency . The proposed method achieves State of Art results on commonly used benchmarks miniImageNet and tieredImageNet . Reasons for score : Overall , I tend towards rejecting this paper . I think cross-episode relationship is an important topic of study in the context of few-shot learning , and the experimental results in this paper are strong . However , I am unconvinced that this \u2018 hammer \u2019 is the right tool for the \u2018 nail \u2019 that the authors claim to solve . A more thorough study of the motivating problem and how & why MELR works would greatly improve this paper . Pros : 1.The proposed method performs well in standard benchmark datasets miniImageNet , tieredImageNet , and CUB200 . Assuming correctness of experimental protocol , the improvement over previous methods is significant . 2.Visualization of embedding space by t-SNE lends further credibility to the strong performance . Samples from each class are well clustered yet still disperse . 3.Ablation of hyperparameters and algorithmic alternatives is mostly complete and honestly presented . Cons : 1.The paper is motivated by the supposed \u201c poorly sampled episodes \u201d problem . While it intuitively makes sense that some data points are more representative than others , whether this has a disparate impact on episode few-shot learning is unclear . In my opinion , the small sample problem in few-shot episodes is no worse than that encountered in standard batch training . In standard supervised learning tasks , batch size as small as 1 has been used successfully to train deep networks given sufficient training epochs . Without empirical or theoretical illustration , I am not convinced that the problem the authors seek to address is a real problem . 2.The reasoning behind equation 2 is unclear . It is not clear why the attention module output in CEAM is added to the embedding F if the goal is to ignore bad examples and emphasize good examples . The choice to ignore class labels when doing attention is also surprising as it doesn \u2019 t use the fact that both episodes have the same classes . 3.The proposed method aims to use inter-episode information to stabilize representation learning , hence samples two episodes at a time and apply CEAM and CECR to the joint episode . I don \u2019 t see why the authors restrict the method to just two episodes . Classifier consistency should hold transductively across any number of episodes with the same base classes . Thus , you could make the number of episodes an hyperparameter and experimentally verify what is the best choice . 4.Grammar mistakes are common and writing generally lacks polish . Questions : Please address the points in cons above . Minor points and additional feedback : Introduction \u201c even may be impossible \u201d - > may even be impossible \u201c reliance of deep neural networks on sufficient annotated training data \u201d : you always need \u201c sufficient \u201d data , FSL aims to make fewer data be sufficient . \u201c Meta-training \u201d , \u201c episode \u201d and \u201c base class \u201d used before definition \u201c Concretely , MELR consists of two key components : a Cross-Episode Attention Module ( CEAM ) and a Cross-Episode Consistency Regularization ( CECR ) \u201d - > remove \u2018 a \u2019 \u201c two cross-episode components ( i.e. , CEAM and CECR ) to explicitly enforcing \u201d - > to explicitly enforce Related Work Very few model-based methods are mentioned but I guess that \u2019 s beyond the point here . \u201c Almost all existing meta-learning based FSL methods ignore the relationships across episodes \u201d - > \u201c Ignore \u201d is probably not true since many works ( incl.MAML ) frame the problem of Meta-learning as an inter-task learning process ( as presented in this review https : //arxiv.org/pdf/2004.05439.pdf ) . There \u2019 s also this work ( https : //arxiv.org/abs/1909.11722 ) that looks at the role of shots when building episodes during and after meta-training . Methodology Too many inline equations . Even with a dedicated definition section there is still a new definition almost every paragraph . Important equations should be made standalone , definitions placed into its own section , and fluff math be removed . Why does CEAM take S as argument twice ? Should they be S_k and S_v instead ? Eqn 2 : Is the softmax taken over rows or columns ( or both ) of F_qS_k^T ? [ Post rebuttal ] I have increased the score of my review to 7 . Below is a copy-pasta of my comments post discussion : While my original concern about how much sampling affects FSL is still not fully addressed , I think it is not a trivial question to answer and a full exploration of the topic could constitute its own paper . So although I 'm not fully convinced about the motivation of this paper , I think the thorough experimental evaluation along with the strong empirical results together warrants publication . From my perspective , a particular important strength of this paper is its ablations . I 'm fairly convinced that the presented implementation is likely the best way to implement the idea of cross-episode attention + distillation . I think the baseline proposed by the AC makes sense . It would be great if that could be incorporated into the final version of the paper . A possible explanation for the drop in performance when using 3 or more episodes is due to the relative decrease in episode diversity . Results on wider datasets could corroborate this hypothesis .", "rating": "7: Good paper, accept", "reply_text": "Thanks for the reviewer \u2019 s constructive comments and suggestions . * * Q1 : I am not convinced that the problem the authors seek to address ( \u2018 poorly sampled episodes \u2019 problem ) is a real problem \u2026 In my opinion , the small sample problem in few-shot episodes is no worse than that encountered in standard batch training . * * \\ A1 : Sorry for the confusion . Indeed , in standard supervised learning , the model updating in each mini-batch iteration suffers little from the poorly sampled mini-batches problem . However , it is very different from the \u2018 poorly sampled episodes \u2019 problem studied in this paper under the few-shot learning setting . More specifically , in mini-batch based supervised learning , the training instances of the same classes will be sampled in different mini-batches . That is , the overall classification task remains unchanged across different mini-batches . So any mini-batch with poor samples will have a small impact on the final trained model \u2013 there are much more iterations/mini-batches that have no outliers to recover the negative effect . In FSL , however , each episode contains a new meta-training task sampled from a pool of seen classes , and in the next episode the task will be completely different . So when there are outliers in each episode , their negative impact on the task must be dealt with immediately . This is because the same model will be deployed for meta-test where * an unseen task is presented only once * , potentially corrupted by outliers . The purpose of our MELR is to meta-learn a mechanism ( i.e. , meta-learn the \u2018 poor sampling problem \u2019 -solver ) so that the negative impact of outliers can be dealt with for any unseen new tasks during meta-test . * * Q2 : It is not clear why the attention module output in CEAM is added to the embedding F if the goal is to ignore bad examples and emphasize good examples . The choice to ignore class labels when doing attention is also surprising as it doesn \u2019 t use the fact that both episodes have the same classes . * * \\ A2 : The attention module used in CEAM is essentially a transformer as those used in NLP ( e.g. , BERT and GPT ) . So it is designed to update the embeddings of a set of instances through inter-instance attention , such that outliers can be identified and their effect minimized . Adding attention on top of the original embedding of each instance ( i.e. , adopting a residual structure ) is thus a common practice . Moreover , we present the results in the following table when the residual structure is removed from our CEAM ( i.e. , we remove the original embedding F in Eqs . ( 2 ) and ( 6 ) ) . It can be clearly seen that the residual structure is important for solving the original FSL problem ( not only the poor sampling problem needs to be solved ) since the original embedding contains descriptive information . |Method & nbsp ; | & nbsp ; Backbone & nbsp ; | & nbsp ; 5-way 1-shot & nbsp ; | & nbsp ; 5-way 5-shot| |-| : - : | : - : | : - : | |ProtoNet+CEAM ( w/o residual struct . ) & nbsp ; |Conv4-64|49.45 $ \\pm $ 0.42|64.81 $ \\pm $ 0.39| |ProtoNet+CEAM ( w/ residual struct . ) & nbsp ; |Conv4-64|55.01 $ \\pm $ 0.43|72.01 $ \\pm $ 0.35| The comment on the class label is an interesting one . We could add a class embedding and combine it to the feature embedding F as input to the attention module . However , this is not needed in our model : the output of our CEAM ( i.e. , updated embeddings ) will be subject to the few-shot classification loss in Eq . ( 9 ) , which clearly needs to use the class labels of each support set instance . We find that this loss can help to guide the attention module to maintain class separation in the updated embedding space . Moreover , we choose to devise the CECR module that aligns the distributions of predicted scores w.r.t.the two episodes for each query sample . That is , the fact that the two episodes have the same classes is implicitly explored in our CECR . * * Q3 : I don \u2019 t see why the authors restrict the method to just two episodes . * * \\ A3 : Thanks for the suggestion . We have added the experiments in Appendix A.6 by varying the number of episodes $ N_e $ . Concretely , for each episode $ e^ { ( i ) } $ ( $ i = 1 , \\cdots , N_e $ ) , the output of CEAM is defined as : $ $ \\mathbf { \\hat { F } } ^ { ( i ) } = \\frac { 1 } { N_e-1 } \\sum_ { j = 1 , \\cdots , N_e , j \\ne i } \\text { CEAM } ( \\mathbf { F } ^ { ( i ) } , \\mathbf { S } ^ { ( j ) } , \\mathbf { S } ^ { ( j ) } ) . $ $ As for CECR , we determine the episode with the best accuracy and distill knowledge to the rest $ N_e-1 $ episodes . The results on * mini * ImageNet using Conv4-64 in the following table show that the performance drops slightly as the number of episodes increases . One possible explanation is that too much training data make the model fit better on the training set but fail to improve its generalization ability on novel classes . |Method & nbsp ; | & nbsp ; # episodes & nbsp ; | & nbsp ; 5-way 1-shot & nbsp ; | & nbsp ; 5-way 5-shot & nbsp ; | |-| : - : | : - : | : - : | |MELR|2|55.35 $ \\pm $ 0.43|72.27 $ \\pm $ 0.35| |MELR|3|55.26 $ \\pm $ 0.44|71.88 $ \\pm $ 0.35| |MELR|4|55.15 $ \\pm $ 0.43|71.63 $ \\pm $ 0.35|"}, {"review_id": "D3PcGLdMx0-1", "review_text": "# # # Summary This paper proposes a way to exploit relationships across tasks in episodic training with the goal of improving the trained models who might be susceptible to poor sampling in for few-shot learning scenarios . The proposed model consists of two components : a cross-attention transformer ( CEAM ) which is used to observe details across two episodes , and a regularization term ( CECR ) which imposes that two different instances of the same task ( which have the exact same classes ) are consistent in terms of prediction . Cross-attention is computed via a scaled-attention transformer using both support and query set . The consistency loss is a knowledge distillation that imposes an agreement on the two episodes . The soft target is chosen among the two predictions selecting the classifier with the highest accuracy . # # # Considerations : - I like the idea of exploiting the information across tasks to improve the performance of episodic meta training . This is an interesting direction that should might definitely help disambiguate in the case of poor sampling . - The ablation study is accurately performed giving the impression of a careful examination of the components of the model proposed . - I 'm not sure the authors can claim sota results : here some of the latest models that perform best on mini-imagenet https : //paperswithcode.com/sota/few-shot-image-classification-on-mini-1 I would prefer to restate the contribution as an improvement of x % over the baseline . It is obvious that sota performance requires higher capacity models such as dense-net . I think that other experiments are needed in order to make the claim of achieving sota , otherwise , if the claim is changed , I 'm satisfied with the experiments . - I suggest the authors moving algorithm 1 in the main paper , maybe replacing the verbose description of each step with actual formulas and pseudocode . - I think there is still room for improvement on the manuscript . The paper might be a good contribution to the scientific community , but I 'll wait for the authors ' response on my doubts before my final decision . # # # Questions : - Q1 : Why only considering tasks with the same classes for consistency ? Why not considering also partial overlapping of classes across tasks ? I guess it is only for simplicity , but it might be beneficial to consider other types of relationships . - Q2 : It is not exactly clear how the meta-test evaluation is performed . I understand that during training you always consider a pair of episodes that are used to transform the features , but how does this translate at meta-test time ? Do you always need a pair of episodes ? My guess is no , but not using the transformer should change the distribution of the features at test-time and I do n't find it trivial to see how this is taken into account . Maybe I 'm just misreading the paper . I suggest the authors clarifying this point in the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We \u2019 d like to thank the reviewer for the constructive comments and suggestions . We have accordingly made changes in the revision . * * Q1 : I 'm not sure the authors can claim SOTA results . * * \\ A1 : Thanks for pointing this out . To avoid misunderstanding on the SOTA claim , we have now modified the claim in the revision as suggested by the reviewer . Indeed , higher results have been reported elsewhere but achieved with larger backbones , external data , and/or the transductive setting . Under the most standard FSL setting ( with three commonly used backbones , no external data , non-transductive ) , our MELR indeed achieves the best performance on the two benchmarks . * * Q2 : I suggest the authors moving algorithm 1 in the main paper . * * \\ A2 : Thanks for the suggestion . We have moved Algorithm 1 to the main paper and modified the descriptions . * * Q3 : Why only considering tasks with the same classes for consistency ? Why not considering also partial overlapping of classes across tasks ? * * \\ A3 : Great suggestion ! After the ICLR paper submission , we have actually considered episodes with partially overlapped sets of classes . Our main idea is to modify the CECR part but with CEAM unchanged . Concretely , given a pair of randomly sampled episodes , for support samples from the disjointed classes ( if any ) that come from only one episode , we apply data augmentation ( e.g. , random crops/horizontal flip ) to them so that each class from the two episodes now has two sets of $ K $ shots for us to implement CECR . The obtained results under the 5-way 1-shot and 5-shot settings on * mini * ImageNet ( with Conv4-64 as the backbone ) are 54.72 % $ \\pm $ 0.43 % and 71.51 % $ \\pm $ 0.35 % , respectively . These are good results . However , it seems that exploiting partial overlapping of classes across tasks does not lead to performance improvements over our main results in Table 1 ( 55.35 % $ \\pm $ 0.43 % for 1-shot and 72.27 % $ \\pm $ 0.35 % for 5-shot ) . Perhaps new algorithms need to be designed to model other types of cross-episode relationships more effectively \u2013 we will leave it to the future work . * * Q4 : It is not exactly clear how the meta-test evaluation is performed . * * \\ A4 : Sorry for the confusion about evaluation protocols . Indeed , only one episode is needed during meta-test , which is the same as previous works . We have stated in Section 4.1 that our MELR is evaluated over meta-test episodes independently ( i.e. , one episode at a time ) . Concretely , we apply the trained CEAM within each episode $ e^ { ( test ) } $ by inputting the triplet $ ( \\mathbf { F } ^ { ( test ) } , \\mathbf { S } ^ { ( test ) } , \\mathbf { S } ^ { ( test ) } ) $ , where $ \\mathbf { S } ^ { ( test ) } \\in \\mathbb { R } ^ { NK \\times d } $ and $ \\mathbf { F } ^ { ( test ) } \\in \\mathbb { R } ^ { N ( K+Q ) \\times d } $ are the feature matrices of support samples and all samples in $ e^ { ( test ) } $ , respectively . That is , for each meta-test episode , we take its own support samples ( instead of those from another episode ) as keys and values for the trained CEAM . Note that we strictly follow the * non-transductive * evaluation setting since the embedding of each query sample in $ e^ { ( test ) } $ is transformed by the trained CEAM independently according to the support sets ."}, {"review_id": "D3PcGLdMx0-2", "review_text": "Summary : - This paper proposes a meta-learning method ( MELR ) to alleviate the negative impact of poor sampling of support sets . MLER consists of two main modules . The first module ( CEAM ) applies the attention mechanism to two sampled episodes and it alleviates the negative impact of badly-sampled instances . The second module ( CECR ) enhances the consistency of classifiers obtained by using two episodes to deal with the sensitivity for the badly-sampled instances . Specifically , to realize this , CECR utilizes a knowledge distillation framework . Experiments with two real-world datasets demonstrate that MELR works well . Pros : - This paper proposes a new meta-learning method to alleviate the negative effect of poor sampling of support sets . - Experimental results show that MELR can improve the baseline method ( Propnet ) . These results show some evidence of the effectiveness of two proposed modules ( CEAM and CECR ) . Cons : - It is not clear why CEAM can alleviate the negative effect of badly-sampled support instances . - Hyper-parameter candidates used for MELR are not described . Detailed comments and questions : - Although empirical results seem to support the effectiveness of CEAM , I do not understand why CEAM alleviates the negative effect of badly-sampled few shots for a given query instance . Can the authors qualitatively explain the reason for this ? - In the last row of fig 4 , it seems that CEAM creates obvious outlier instances . Why does this happen ? - How much does the proposed method depend on hyperparameters such as $ T $ and $ \\lambda $ ? Minor comments : - In eqs . ( 1 ) and ( 9 ) , although argmax is taken in the loss function $ L $ , it is not correct when using the cross-entropy loss .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the reviewer \u2019 s constructive comments and suggestions . Our responses are as follows . * * Q1 : It is not clear why CEAM can alleviate the negative effect of badly-sampled support instances . Can the authors qualitatively explain the reason for this ? In the last row of fig 4 ( * now Figure 5 in the revision * ) , it seems that CEAM creates obvious outlier instances . Why does this happen ? * * \\ A1 : Under the few-shot setting , the biggest negative effect of having an outlying support instance is that the class mean/prototype will be heavily biased by the outlier . This would lead to prototypes of different classes to be close to each other , causing problems for classifying query samples using these prototypes . Our CEAM is essentially a self-attention based transformer . It transforms the latent embedding of each support set sample by allowing it to examine ( attend to ) other samples , both from the same class and different classes , in the support set , and update its embedding as a weighted sum of all samples . The weight is determined mostly by similarity or proximity in the original embedding space . Based on this understanding , it is now easy to understand the usefulness as well as the limitation of CEAM for countering the negative effect of bad samples . In particular , the learned transformer is subject to the cross-entropy loss computed on the query using the updated prototypes . This encourages the transformer in CEAM to update the embeddings so that different classes become well separable , therefore largely neutralizing the main negative effect of the outlying samples , as supported by Figure 3 ( b ) vs. 3 ( a ) and more visualizations in Figure 5 in the revision . However , the effect of CEAM on the relationship of outliers and inliers of the same class is limited , which is determined by the nature of the transformer : the inliers are far away from the outliers so would have limited effect in pulling them toward the inlier majority . This is shown clearly in Figure 5 bottom row \u2013 when the outliers are extremely different from the inliers , not being able to pull them close to the inliers would hinder CEAM \u2019 s main objective of preventing class overlapping . That is why in this extreme case we need CECR : the predictions based on two sets of prototypes need to be consistent using CECR , which means that the learned embeddings must pull the outliers closer to the inliers . Therefore , both CEAM and CECR are necessary for our full model . * * Q2 : Hyper-parameter candidates used for MELR are not described . How much does the proposed method depend on hyper-parameters such as $ T $ and $ \\lambda $ ? * * \\ A2 : Thanks . We have added the hyper-parameter candidates in Section 4.1 of the main paper . Concretely , we select $ T $ from { 16 , 32 , 64 , 128 } and $ \\lambda $ from { 0.02 , 0.05 , 0.1 , 0.2 } based on the validation performances . Moreover , we have also added a hyper-parameter analysis in Appendix A.5 . Our method is shown to be insensitive to the hyper-parameters . * * Q3 : In eqs . ( 1 ) and ( 9 ) , although argmax is taken in the loss function , it is not correct when using the cross-entropy loss . * * \\ A3 : Thanks . We have made the correction in the revision ."}, {"review_id": "D3PcGLdMx0-3", "review_text": "Summary One problem of few-shot episodic learning is a poor sampling resulting in negative impacts on the learned model . The paper proposes a new episodic training by exploiting inter-episode relationships to deal with poor sampling problem and improve the learned model by enforcing consistency regularization . Cross Episode Attention Module ( CEAM ) is proposed to alleviate the effect of poorly-sampled shots and Cross-Episode Consistency Regularization ( CECR ) is proposed to enforce robustness of the classifiers . Strength - The paper proposes a novel idea of how to improve few-shot learning by exploiting inter-episode relationships . Using multiple episodes and exploiting inter-episode is a new attempt . - There have been attempts to improve few-shot training by batch construction , but the proposed method outperforms the previous approaches with a sizable margin . - The extensive ablative studies provide comprehensive comparisons among possible design choices . ( including supplementary materials ) Weakness - I could not find a significant weakness of the paper . Rating I like the overall idea of using inter-episode relationships for few-shot training . The proposed approach shows strong performance and technically straightforward and easy to understand . Another strength of the method is that no additional hyper-parameter is used to tune the performance . The paper is clear and extensive experiments support the effectiveness of the paper including supplementary materials .", "rating": "7: Good paper, accept", "reply_text": "We \u2019 d like to greatly thank the reviewer for the positive comments !"}], "0": {"review_id": "D3PcGLdMx0-0", "review_text": "Summary of Paper : This paper proposes to improve Prototypical Networks by a method MELR which aims to fix the problem caused by poorly represented classes in sampled episodes and to reap benefits from enforcing cross-episode consistency . The proposed method achieves State of Art results on commonly used benchmarks miniImageNet and tieredImageNet . Reasons for score : Overall , I tend towards rejecting this paper . I think cross-episode relationship is an important topic of study in the context of few-shot learning , and the experimental results in this paper are strong . However , I am unconvinced that this \u2018 hammer \u2019 is the right tool for the \u2018 nail \u2019 that the authors claim to solve . A more thorough study of the motivating problem and how & why MELR works would greatly improve this paper . Pros : 1.The proposed method performs well in standard benchmark datasets miniImageNet , tieredImageNet , and CUB200 . Assuming correctness of experimental protocol , the improvement over previous methods is significant . 2.Visualization of embedding space by t-SNE lends further credibility to the strong performance . Samples from each class are well clustered yet still disperse . 3.Ablation of hyperparameters and algorithmic alternatives is mostly complete and honestly presented . Cons : 1.The paper is motivated by the supposed \u201c poorly sampled episodes \u201d problem . While it intuitively makes sense that some data points are more representative than others , whether this has a disparate impact on episode few-shot learning is unclear . In my opinion , the small sample problem in few-shot episodes is no worse than that encountered in standard batch training . In standard supervised learning tasks , batch size as small as 1 has been used successfully to train deep networks given sufficient training epochs . Without empirical or theoretical illustration , I am not convinced that the problem the authors seek to address is a real problem . 2.The reasoning behind equation 2 is unclear . It is not clear why the attention module output in CEAM is added to the embedding F if the goal is to ignore bad examples and emphasize good examples . The choice to ignore class labels when doing attention is also surprising as it doesn \u2019 t use the fact that both episodes have the same classes . 3.The proposed method aims to use inter-episode information to stabilize representation learning , hence samples two episodes at a time and apply CEAM and CECR to the joint episode . I don \u2019 t see why the authors restrict the method to just two episodes . Classifier consistency should hold transductively across any number of episodes with the same base classes . Thus , you could make the number of episodes an hyperparameter and experimentally verify what is the best choice . 4.Grammar mistakes are common and writing generally lacks polish . Questions : Please address the points in cons above . Minor points and additional feedback : Introduction \u201c even may be impossible \u201d - > may even be impossible \u201c reliance of deep neural networks on sufficient annotated training data \u201d : you always need \u201c sufficient \u201d data , FSL aims to make fewer data be sufficient . \u201c Meta-training \u201d , \u201c episode \u201d and \u201c base class \u201d used before definition \u201c Concretely , MELR consists of two key components : a Cross-Episode Attention Module ( CEAM ) and a Cross-Episode Consistency Regularization ( CECR ) \u201d - > remove \u2018 a \u2019 \u201c two cross-episode components ( i.e. , CEAM and CECR ) to explicitly enforcing \u201d - > to explicitly enforce Related Work Very few model-based methods are mentioned but I guess that \u2019 s beyond the point here . \u201c Almost all existing meta-learning based FSL methods ignore the relationships across episodes \u201d - > \u201c Ignore \u201d is probably not true since many works ( incl.MAML ) frame the problem of Meta-learning as an inter-task learning process ( as presented in this review https : //arxiv.org/pdf/2004.05439.pdf ) . There \u2019 s also this work ( https : //arxiv.org/abs/1909.11722 ) that looks at the role of shots when building episodes during and after meta-training . Methodology Too many inline equations . Even with a dedicated definition section there is still a new definition almost every paragraph . Important equations should be made standalone , definitions placed into its own section , and fluff math be removed . Why does CEAM take S as argument twice ? Should they be S_k and S_v instead ? Eqn 2 : Is the softmax taken over rows or columns ( or both ) of F_qS_k^T ? [ Post rebuttal ] I have increased the score of my review to 7 . Below is a copy-pasta of my comments post discussion : While my original concern about how much sampling affects FSL is still not fully addressed , I think it is not a trivial question to answer and a full exploration of the topic could constitute its own paper . So although I 'm not fully convinced about the motivation of this paper , I think the thorough experimental evaluation along with the strong empirical results together warrants publication . From my perspective , a particular important strength of this paper is its ablations . I 'm fairly convinced that the presented implementation is likely the best way to implement the idea of cross-episode attention + distillation . I think the baseline proposed by the AC makes sense . It would be great if that could be incorporated into the final version of the paper . A possible explanation for the drop in performance when using 3 or more episodes is due to the relative decrease in episode diversity . Results on wider datasets could corroborate this hypothesis .", "rating": "7: Good paper, accept", "reply_text": "Thanks for the reviewer \u2019 s constructive comments and suggestions . * * Q1 : I am not convinced that the problem the authors seek to address ( \u2018 poorly sampled episodes \u2019 problem ) is a real problem \u2026 In my opinion , the small sample problem in few-shot episodes is no worse than that encountered in standard batch training . * * \\ A1 : Sorry for the confusion . Indeed , in standard supervised learning , the model updating in each mini-batch iteration suffers little from the poorly sampled mini-batches problem . However , it is very different from the \u2018 poorly sampled episodes \u2019 problem studied in this paper under the few-shot learning setting . More specifically , in mini-batch based supervised learning , the training instances of the same classes will be sampled in different mini-batches . That is , the overall classification task remains unchanged across different mini-batches . So any mini-batch with poor samples will have a small impact on the final trained model \u2013 there are much more iterations/mini-batches that have no outliers to recover the negative effect . In FSL , however , each episode contains a new meta-training task sampled from a pool of seen classes , and in the next episode the task will be completely different . So when there are outliers in each episode , their negative impact on the task must be dealt with immediately . This is because the same model will be deployed for meta-test where * an unseen task is presented only once * , potentially corrupted by outliers . The purpose of our MELR is to meta-learn a mechanism ( i.e. , meta-learn the \u2018 poor sampling problem \u2019 -solver ) so that the negative impact of outliers can be dealt with for any unseen new tasks during meta-test . * * Q2 : It is not clear why the attention module output in CEAM is added to the embedding F if the goal is to ignore bad examples and emphasize good examples . The choice to ignore class labels when doing attention is also surprising as it doesn \u2019 t use the fact that both episodes have the same classes . * * \\ A2 : The attention module used in CEAM is essentially a transformer as those used in NLP ( e.g. , BERT and GPT ) . So it is designed to update the embeddings of a set of instances through inter-instance attention , such that outliers can be identified and their effect minimized . Adding attention on top of the original embedding of each instance ( i.e. , adopting a residual structure ) is thus a common practice . Moreover , we present the results in the following table when the residual structure is removed from our CEAM ( i.e. , we remove the original embedding F in Eqs . ( 2 ) and ( 6 ) ) . It can be clearly seen that the residual structure is important for solving the original FSL problem ( not only the poor sampling problem needs to be solved ) since the original embedding contains descriptive information . |Method & nbsp ; | & nbsp ; Backbone & nbsp ; | & nbsp ; 5-way 1-shot & nbsp ; | & nbsp ; 5-way 5-shot| |-| : - : | : - : | : - : | |ProtoNet+CEAM ( w/o residual struct . ) & nbsp ; |Conv4-64|49.45 $ \\pm $ 0.42|64.81 $ \\pm $ 0.39| |ProtoNet+CEAM ( w/ residual struct . ) & nbsp ; |Conv4-64|55.01 $ \\pm $ 0.43|72.01 $ \\pm $ 0.35| The comment on the class label is an interesting one . We could add a class embedding and combine it to the feature embedding F as input to the attention module . However , this is not needed in our model : the output of our CEAM ( i.e. , updated embeddings ) will be subject to the few-shot classification loss in Eq . ( 9 ) , which clearly needs to use the class labels of each support set instance . We find that this loss can help to guide the attention module to maintain class separation in the updated embedding space . Moreover , we choose to devise the CECR module that aligns the distributions of predicted scores w.r.t.the two episodes for each query sample . That is , the fact that the two episodes have the same classes is implicitly explored in our CECR . * * Q3 : I don \u2019 t see why the authors restrict the method to just two episodes . * * \\ A3 : Thanks for the suggestion . We have added the experiments in Appendix A.6 by varying the number of episodes $ N_e $ . Concretely , for each episode $ e^ { ( i ) } $ ( $ i = 1 , \\cdots , N_e $ ) , the output of CEAM is defined as : $ $ \\mathbf { \\hat { F } } ^ { ( i ) } = \\frac { 1 } { N_e-1 } \\sum_ { j = 1 , \\cdots , N_e , j \\ne i } \\text { CEAM } ( \\mathbf { F } ^ { ( i ) } , \\mathbf { S } ^ { ( j ) } , \\mathbf { S } ^ { ( j ) } ) . $ $ As for CECR , we determine the episode with the best accuracy and distill knowledge to the rest $ N_e-1 $ episodes . The results on * mini * ImageNet using Conv4-64 in the following table show that the performance drops slightly as the number of episodes increases . One possible explanation is that too much training data make the model fit better on the training set but fail to improve its generalization ability on novel classes . |Method & nbsp ; | & nbsp ; # episodes & nbsp ; | & nbsp ; 5-way 1-shot & nbsp ; | & nbsp ; 5-way 5-shot & nbsp ; | |-| : - : | : - : | : - : | |MELR|2|55.35 $ \\pm $ 0.43|72.27 $ \\pm $ 0.35| |MELR|3|55.26 $ \\pm $ 0.44|71.88 $ \\pm $ 0.35| |MELR|4|55.15 $ \\pm $ 0.43|71.63 $ \\pm $ 0.35|"}, "1": {"review_id": "D3PcGLdMx0-1", "review_text": "# # # Summary This paper proposes a way to exploit relationships across tasks in episodic training with the goal of improving the trained models who might be susceptible to poor sampling in for few-shot learning scenarios . The proposed model consists of two components : a cross-attention transformer ( CEAM ) which is used to observe details across two episodes , and a regularization term ( CECR ) which imposes that two different instances of the same task ( which have the exact same classes ) are consistent in terms of prediction . Cross-attention is computed via a scaled-attention transformer using both support and query set . The consistency loss is a knowledge distillation that imposes an agreement on the two episodes . The soft target is chosen among the two predictions selecting the classifier with the highest accuracy . # # # Considerations : - I like the idea of exploiting the information across tasks to improve the performance of episodic meta training . This is an interesting direction that should might definitely help disambiguate in the case of poor sampling . - The ablation study is accurately performed giving the impression of a careful examination of the components of the model proposed . - I 'm not sure the authors can claim sota results : here some of the latest models that perform best on mini-imagenet https : //paperswithcode.com/sota/few-shot-image-classification-on-mini-1 I would prefer to restate the contribution as an improvement of x % over the baseline . It is obvious that sota performance requires higher capacity models such as dense-net . I think that other experiments are needed in order to make the claim of achieving sota , otherwise , if the claim is changed , I 'm satisfied with the experiments . - I suggest the authors moving algorithm 1 in the main paper , maybe replacing the verbose description of each step with actual formulas and pseudocode . - I think there is still room for improvement on the manuscript . The paper might be a good contribution to the scientific community , but I 'll wait for the authors ' response on my doubts before my final decision . # # # Questions : - Q1 : Why only considering tasks with the same classes for consistency ? Why not considering also partial overlapping of classes across tasks ? I guess it is only for simplicity , but it might be beneficial to consider other types of relationships . - Q2 : It is not exactly clear how the meta-test evaluation is performed . I understand that during training you always consider a pair of episodes that are used to transform the features , but how does this translate at meta-test time ? Do you always need a pair of episodes ? My guess is no , but not using the transformer should change the distribution of the features at test-time and I do n't find it trivial to see how this is taken into account . Maybe I 'm just misreading the paper . I suggest the authors clarifying this point in the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We \u2019 d like to thank the reviewer for the constructive comments and suggestions . We have accordingly made changes in the revision . * * Q1 : I 'm not sure the authors can claim SOTA results . * * \\ A1 : Thanks for pointing this out . To avoid misunderstanding on the SOTA claim , we have now modified the claim in the revision as suggested by the reviewer . Indeed , higher results have been reported elsewhere but achieved with larger backbones , external data , and/or the transductive setting . Under the most standard FSL setting ( with three commonly used backbones , no external data , non-transductive ) , our MELR indeed achieves the best performance on the two benchmarks . * * Q2 : I suggest the authors moving algorithm 1 in the main paper . * * \\ A2 : Thanks for the suggestion . We have moved Algorithm 1 to the main paper and modified the descriptions . * * Q3 : Why only considering tasks with the same classes for consistency ? Why not considering also partial overlapping of classes across tasks ? * * \\ A3 : Great suggestion ! After the ICLR paper submission , we have actually considered episodes with partially overlapped sets of classes . Our main idea is to modify the CECR part but with CEAM unchanged . Concretely , given a pair of randomly sampled episodes , for support samples from the disjointed classes ( if any ) that come from only one episode , we apply data augmentation ( e.g. , random crops/horizontal flip ) to them so that each class from the two episodes now has two sets of $ K $ shots for us to implement CECR . The obtained results under the 5-way 1-shot and 5-shot settings on * mini * ImageNet ( with Conv4-64 as the backbone ) are 54.72 % $ \\pm $ 0.43 % and 71.51 % $ \\pm $ 0.35 % , respectively . These are good results . However , it seems that exploiting partial overlapping of classes across tasks does not lead to performance improvements over our main results in Table 1 ( 55.35 % $ \\pm $ 0.43 % for 1-shot and 72.27 % $ \\pm $ 0.35 % for 5-shot ) . Perhaps new algorithms need to be designed to model other types of cross-episode relationships more effectively \u2013 we will leave it to the future work . * * Q4 : It is not exactly clear how the meta-test evaluation is performed . * * \\ A4 : Sorry for the confusion about evaluation protocols . Indeed , only one episode is needed during meta-test , which is the same as previous works . We have stated in Section 4.1 that our MELR is evaluated over meta-test episodes independently ( i.e. , one episode at a time ) . Concretely , we apply the trained CEAM within each episode $ e^ { ( test ) } $ by inputting the triplet $ ( \\mathbf { F } ^ { ( test ) } , \\mathbf { S } ^ { ( test ) } , \\mathbf { S } ^ { ( test ) } ) $ , where $ \\mathbf { S } ^ { ( test ) } \\in \\mathbb { R } ^ { NK \\times d } $ and $ \\mathbf { F } ^ { ( test ) } \\in \\mathbb { R } ^ { N ( K+Q ) \\times d } $ are the feature matrices of support samples and all samples in $ e^ { ( test ) } $ , respectively . That is , for each meta-test episode , we take its own support samples ( instead of those from another episode ) as keys and values for the trained CEAM . Note that we strictly follow the * non-transductive * evaluation setting since the embedding of each query sample in $ e^ { ( test ) } $ is transformed by the trained CEAM independently according to the support sets ."}, "2": {"review_id": "D3PcGLdMx0-2", "review_text": "Summary : - This paper proposes a meta-learning method ( MELR ) to alleviate the negative impact of poor sampling of support sets . MLER consists of two main modules . The first module ( CEAM ) applies the attention mechanism to two sampled episodes and it alleviates the negative impact of badly-sampled instances . The second module ( CECR ) enhances the consistency of classifiers obtained by using two episodes to deal with the sensitivity for the badly-sampled instances . Specifically , to realize this , CECR utilizes a knowledge distillation framework . Experiments with two real-world datasets demonstrate that MELR works well . Pros : - This paper proposes a new meta-learning method to alleviate the negative effect of poor sampling of support sets . - Experimental results show that MELR can improve the baseline method ( Propnet ) . These results show some evidence of the effectiveness of two proposed modules ( CEAM and CECR ) . Cons : - It is not clear why CEAM can alleviate the negative effect of badly-sampled support instances . - Hyper-parameter candidates used for MELR are not described . Detailed comments and questions : - Although empirical results seem to support the effectiveness of CEAM , I do not understand why CEAM alleviates the negative effect of badly-sampled few shots for a given query instance . Can the authors qualitatively explain the reason for this ? - In the last row of fig 4 , it seems that CEAM creates obvious outlier instances . Why does this happen ? - How much does the proposed method depend on hyperparameters such as $ T $ and $ \\lambda $ ? Minor comments : - In eqs . ( 1 ) and ( 9 ) , although argmax is taken in the loss function $ L $ , it is not correct when using the cross-entropy loss .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the reviewer \u2019 s constructive comments and suggestions . Our responses are as follows . * * Q1 : It is not clear why CEAM can alleviate the negative effect of badly-sampled support instances . Can the authors qualitatively explain the reason for this ? In the last row of fig 4 ( * now Figure 5 in the revision * ) , it seems that CEAM creates obvious outlier instances . Why does this happen ? * * \\ A1 : Under the few-shot setting , the biggest negative effect of having an outlying support instance is that the class mean/prototype will be heavily biased by the outlier . This would lead to prototypes of different classes to be close to each other , causing problems for classifying query samples using these prototypes . Our CEAM is essentially a self-attention based transformer . It transforms the latent embedding of each support set sample by allowing it to examine ( attend to ) other samples , both from the same class and different classes , in the support set , and update its embedding as a weighted sum of all samples . The weight is determined mostly by similarity or proximity in the original embedding space . Based on this understanding , it is now easy to understand the usefulness as well as the limitation of CEAM for countering the negative effect of bad samples . In particular , the learned transformer is subject to the cross-entropy loss computed on the query using the updated prototypes . This encourages the transformer in CEAM to update the embeddings so that different classes become well separable , therefore largely neutralizing the main negative effect of the outlying samples , as supported by Figure 3 ( b ) vs. 3 ( a ) and more visualizations in Figure 5 in the revision . However , the effect of CEAM on the relationship of outliers and inliers of the same class is limited , which is determined by the nature of the transformer : the inliers are far away from the outliers so would have limited effect in pulling them toward the inlier majority . This is shown clearly in Figure 5 bottom row \u2013 when the outliers are extremely different from the inliers , not being able to pull them close to the inliers would hinder CEAM \u2019 s main objective of preventing class overlapping . That is why in this extreme case we need CECR : the predictions based on two sets of prototypes need to be consistent using CECR , which means that the learned embeddings must pull the outliers closer to the inliers . Therefore , both CEAM and CECR are necessary for our full model . * * Q2 : Hyper-parameter candidates used for MELR are not described . How much does the proposed method depend on hyper-parameters such as $ T $ and $ \\lambda $ ? * * \\ A2 : Thanks . We have added the hyper-parameter candidates in Section 4.1 of the main paper . Concretely , we select $ T $ from { 16 , 32 , 64 , 128 } and $ \\lambda $ from { 0.02 , 0.05 , 0.1 , 0.2 } based on the validation performances . Moreover , we have also added a hyper-parameter analysis in Appendix A.5 . Our method is shown to be insensitive to the hyper-parameters . * * Q3 : In eqs . ( 1 ) and ( 9 ) , although argmax is taken in the loss function , it is not correct when using the cross-entropy loss . * * \\ A3 : Thanks . We have made the correction in the revision ."}, "3": {"review_id": "D3PcGLdMx0-3", "review_text": "Summary One problem of few-shot episodic learning is a poor sampling resulting in negative impacts on the learned model . The paper proposes a new episodic training by exploiting inter-episode relationships to deal with poor sampling problem and improve the learned model by enforcing consistency regularization . Cross Episode Attention Module ( CEAM ) is proposed to alleviate the effect of poorly-sampled shots and Cross-Episode Consistency Regularization ( CECR ) is proposed to enforce robustness of the classifiers . Strength - The paper proposes a novel idea of how to improve few-shot learning by exploiting inter-episode relationships . Using multiple episodes and exploiting inter-episode is a new attempt . - There have been attempts to improve few-shot training by batch construction , but the proposed method outperforms the previous approaches with a sizable margin . - The extensive ablative studies provide comprehensive comparisons among possible design choices . ( including supplementary materials ) Weakness - I could not find a significant weakness of the paper . Rating I like the overall idea of using inter-episode relationships for few-shot training . The proposed approach shows strong performance and technically straightforward and easy to understand . Another strength of the method is that no additional hyper-parameter is used to tune the performance . The paper is clear and extensive experiments support the effectiveness of the paper including supplementary materials .", "rating": "7: Good paper, accept", "reply_text": "We \u2019 d like to greatly thank the reviewer for the positive comments !"}}