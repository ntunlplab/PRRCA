{"year": "2019", "forum": "HJlQfnCqKX", "title": "Predicting the Generalization Gap in Deep Networks with Margin Distributions", "decision": "Accept (Poster)", "meta_review": "The paper suggests a new measurement of layer-wise margin distributions for generalization ability. Extensive experiments are conducted. Though there lacks a solid theory to explain the phenomenon. The majority of reviewers suggest acceptance (9,6,5). Therefore, it is proposed as probable accept.", "reviews": [{"review_id": "HJlQfnCqKX-0", "review_text": "After author response, I have increased my score. I'm still not 100% sure about the interpretation the authors provided for the negative distances. The paper is well written and is mostly clear. (1st line on page 4 has a typo, \\bar{x}_k in eq (4) should be \\bar{x}^l?) Novelty: I am not sure whether the paper adds any significant on top of what we know from Bartlett et al., Elsayed et al. since: (i). The fact that \"normalized\" margins are strongly correlated with the test set accuracy was shown in Bartlett et al. (figure 1.). A major part of the definition comes from there or from the reference they cite; (ii). Taylor approximation to compute the margin distribution is in Elsayed et al.; (iii). I think the four points listed in page 2 (which make the distinction between related work) is misleading: the way I see it is that the authors use the margin distribution in Elsayed et al which simply overcomes some of the obstacles that norm based margins may face. The only novelty here seems to be that the authors use the margin distribution at each layer. Technical pitfalls: Computing the d_{f,x,i,j} using Equation (3) is missing an absolute value in the numerator as in equation (7) Elsayed et al.. The authors interpret the negative values as misclassification: why is it true? The margin distribution used in Bartlett et al. (below Figure 4 on page 5 in arxiv:1706.08498) uses labeled data and it is obvious in this case to interpreting negative values as misclassification. I don't see how this is true for eq (3) here in this paper. Secondly, why are negative points ignored?? Misclassified points in my opinion are equally important, ignoring the information that a point is misclassified doesn't sound like a great idea. How do the experiments look if we don't ignore them? Experiments: Good set of experiments. However I find the results to be mildly taking the claims of the authors made in four points listed in page 2 away: Section 4.1, \"Empirically, we found constructing this only on four evenly-spaced layers, input, and 3 hidden layers, leads to good predictors.\". How can the authors explain this? By using linear models, authors implicitly assume that the relationship between generalization gaps and signatures are linear (in Eucledian or log spaces). However, from the experiments (table 1), we see that log models always have better results than linear models. Even assuming linear relationship, I think it is informative to also provide other metrics such as MSE, AIC, BIC etc..", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for your insightful review . # # NOVELTY # # R2 : \u201c The fact that normalized margins are correlated with generalization was shown in Bartlett Fig 1 \u201d . As you pointed out , both works build on the broad notion of \u201c margin distribution \u201d and \u201c normalization \u201d . However , there are significant differences : 1 . Margin in Bartlett uses f_i-f_j that can only reflect output margins , as opposed to ( f_i-f_j ) /||d/dx f_i - d/dx f_j|| that works for any layer . 2.We do not use margin distribution itself to predict the generalization gap , but rather distributional features that involve \u201c nonlinear transform \u201d of the distances ( quartiles or moments ) . 3.Normalization in Bartlett \u2019 s uses norm of weight matrices , which is drastically different from geometric spread of activations ( variance ) we use ( Eqs 4 and 5 ) . Also their can not be used as-is for residual networks , a drawback not present in our normalization . These distinctions result in very different predictions of the generalization , as clearly shown in our Fig 2 and Table 1 . In fact , the choice of distributional features and normalization are crucial for accurate prediction of the generalization gap . Finally , we have conducted a far larger scale of experiments , and will be releasing the 700+ realistic models used in the paper so that researchers can easily test generalization theories . This is the first of its kind . # # TECHNICAL # # # Missing Absolute Value in Eq ( 3 ) # There is no incorrectness ; we deliberately adopt \u201c signed distance \u201d . The polarity reflects which side of the decision boundary the point is . Even Eq ( 7 ) of Elsayed that you mentioned quickly evolves to signed distance in their Eq ( 8 ) . # Why Negative Distance Implies Misclassification # It was our oversight not to mention that \u201c i \u201d in our Eq ( 3 ) corresponds to the ground truth label . We will clarify this in the final version . In this case , f_i-f_j > 0 ( i.e.distance is positive ) implies correct classification and f_i-f_j < 0 implies misclassification . # Why Negative Points are Ignored # We indeed investigated using negative distances . We observed that : 1 . Modern deep architectures often achieve near perfect classification on training data . Hence , the contribution of negative distances to the full distribution is negligible in most trained models . 2.A small fraction of models do have notable misclassification ( due to data augmentation or heavy regularization ) . For these models , we found that margin distribution computed with only positive samples predicted the generalization gap better than ( or at par with ) the full distribution . However , we observed that the latter is indeed a better predictor of test accuracy ( just not the gap ) . Since we focus our narrative on the generalization gap , we decided to omit these results from the main paper ; however , we will include these results in the appendix . We also note that there is no technical problem in using margin distribution with only positive samples , e.g.Bartlett \u2019 s work \u201c The Sample Complexity of Pattern Classification with Neural Networks \u201d develops a generalization bound by such samples ( paragraph above their Theorem 2 ) . # # EXPERIMENTS # # # Why 4 Layers and Why Even Spacing # 1 . This leads to a fixed-length signature vector , hence agnostic to the architecture and depth . 2.Computing signature across all layers is expensive for large deep models . 3.Larger signature would require more pre-trained networks to avoid overfitting in regression phase . Given that each pre-trained network is only one sample in the regression task , creating a large pool of models is prohibitively expensive . Our study with 700 realistic sized pre-trained networks is perhaps already beyond the common practice for such empirical analysis . 4.The even spacing is merely a natural choice of minimal commitment and already achieves near perfect prediction ( CoD close to 1 ) is some scenarios . However , it is possible to examine other configurations . # Log/Linear # We are not sure if we understand the question . We provide an answer below , but if this is not what you meant , please let us know . We investigate the use of signature components in two ways : 1 . Directly as the input to linear regression , 2 . Applying an element-wise log to them before using them as input of the linear regression . In either case , the regression remains linear in optimization variables , but with the log transform we effectively regress the product of signature components to the gap value . # Other Criteria ( MSE , AIC , etc . ) # We have pointed out that the coefficient of determination already captures the MSE along with the scale of the error ; however , for completeness , we will include this result in the appendix . We report k-fold cross validation results as well , which is known to be asymptotically equivalent to AIC ( Stone M. ( 1977 ) An asymptotic equivalence of choice of model by cross-validation and Akaike \u2019 s criterion )"}, {"review_id": "HJlQfnCqKX-1", "review_text": "The author(s) suggest using geometric margin and layer-wise margin distribution in [Elsayed et al. 2018] for predicting generalization gap. pros, a). The author shows large experiments to support their argument. cons, a). No theoretical verification (nor convincing intuition) is provided, especially for the following questions, i) what benefit can be acquired when using geometric margin defined in the paper. ii) why does normalization make sense beyond the simple scaling-free reason. For example, spectral complexity as a normalization factor in [Bartlett et al. 2017] is proposed from the fact, that the Lipschitz constant determines the complexity of network space. iii) why does the middle layer margin can help? iv) why a linear (linear log) relation between the statistic and generalization gap. Further question towards experiment, i) I don't think your comparison with Bartlett's work is fair. Their bounds suggest the gap is approximately Prob(0<X<\\gamma) + Const/\\gamma for a chosen \\gamma, where X is the normalized margin distribution. I think using the extracted signature from margin distribution and a linear predictor don't make sense here. ii) If you do regression analysis on a five layers cnn, can you have a good prediction on a nine layers cnn (or even residue cnn)? Finally, I'm not sure the novelty is strong enough since the margin definition comes from [Elsayed et al. 2018] and the strong linear relationship has been shown in [Bartlett et al. 2017, Liao et al. 2018] though in different settings.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review . We address your concerns below . # What benefit can be acquired when using geometric margin defined in the paper. # The geometric distance is the actual distance between a point \u201c x \u201d and the decision boundary f ( x ) =0 , i.e.d1=min_x ||x|| s.t . f ( x ) =0.This term is usually used in contrast to functional distance defined as d2=f ( x ) . If x is on the decision boundary , d1=d2=0 , but otherwise d1 and d2 can differ . Note that d2 can change by simple reparametrization . For instance , consider a linear decision boundary f ( x ) =w.x . In this case , geometric distance d1=f ( x ) /||w|| and d2=f ( x ) . Let F ( x ) = ( c * w ) .x , i.e.just scaling the weights by factor c. This does not change the decision boundary . For such F , d1 remains the same , but d2 scales with c. One can force a condition to make margins equal in both scenarios : by making the closet point to the decision boundary to have distance 1 . However , this requires introducing an inequality per point , similar to SVMs . With geometric margin , we can work with an unconstrained optimization and directly apply gradient descent or SGD . # Why does normalization make sense ? # Our normalization allows direct analysis of the margins across different models with the same topology ( or different datasets trained on the same network ) , which is otherwise difficult due to the positive homogeneity of ReLU networks . For example , suppose we have two networks with exactly the same weight , and then in one of the networks , we scale weight_i by constant positive factor c and the weight_ { i+1 } by 1/c ( i is a layer index ) , the predictions of the two networks remain the same ; however , their unnormalized margin distribution will be vastly different and the normalized version will be exactly the same . # Why does the middle layer margin can help ? # There is no reason we can assume a-priori that maximizing only input or output margin ( for example ) is enough for good generalization . As shown in our ablation results in Tables 1 and 4 , the combination of multiple layers performs significantly better . If we cut a deep network at any stage , we can treat the first half of the network as a feature extractor and the second half as the classifier . From this perspective , the margins at middle layer can be just as important as the margins in the output layer or input layer . Lastly , we note that Elsayed et . al.show that optimizing margin at multiple layers provides significant benefits for generalization and adversarial robustness . # Why a linear ( linear log ) relation between the statistic and generalization gap. # We are not claiming this is the true relationship between the statistics and the generalization gap . The true relationship may very well be nonlinear and one could perform a nonlinear regression to predict the gap , but it would need regularization and more data to avoid overfitting while a linear combination of simple distributional features already attains high quality prediction ( according to CoD , k-fold cross validation and MSE ) across 700+ pretrained models . This suggests that a linear relationship is indeed a very close * approximation * . # I do n't think your comparison with Bartlett 's work is fair . Their bounds suggest the gap is approximately Prob ( 0 < X < \\gamma ) + Const/\\gamma for a chosen \\gamma , where X is the normalized margin distribution . I think using the extracted signature from margin distribution and a linear predictor do n't make sense here. # We assume the reviewer is referring to theorem 1.1 of Bartlett et al.If one wishes to compute the gap to be the inside of the soft big O , the result will be much larger than the error emitted by our prediction , and will require picking appropriate gamma and delta values . We further note the following : the case study of Bartlett et . al . ( section 2 ) explicitly show in their diagrams ( Figures 2 and 3 ) the normalized distribution as evidence of generalization prediction power ( instead of the bound itself ) and this normalized distribution is closely related to but is not directly their bounds ( they drop the log terms ) ; extracting the statistics in a sense quantifies their case study . Before submitting the paper , we also had personal communication with one of the authors of Bartlett et . al. , and the author agreed that our comparison was fair ."}, {"review_id": "HJlQfnCqKX-2", "review_text": "This paper does not even try to propose yet another \"vacuous\" generalization bounds, but instead empirically convincingly shows an interesting connection between the proposed margin statistics and the generalization gap, which could well be used to provide some \"prescriptive\" insights (per Sanjeev Arora) towards understanding generalization in deep neural nets. I have no major complaints but for a few questions regarding clarifications, 1. From Eq.(5), such distances are defined for only one out of the many possible pairs of labels. So when forming the so-called \"margin signature\", how exactly do you compose it from all such pair-wise distances? Do you pool all the distances together before computing the statistics, or do you aggregate individual statistics from pair-wise distances? And how do you select which pairs to include or exclude? Are you assuming \"i\" is always the ground-truth label class for $x_k$ here? 2. In Eq.(3), the way you define the distance (that flipping i and j would change the sign of the distance) is implying that {i, j} should not be viewed as an unordered pair, in which case a better notation might be (i, j) (i.e. replacing sets \"{}\" with tuples \"()\" to signal that order matters). And why do you \"only consider distances with positive sign\"? I can understand doing this for when neither i nor j corresponds to the ground-truth label of x, because you really can't tell which score should be higher. But when i happens to be the ground-truth label, wouldn't a positive distance and a negative distance be meaningful different and therefore it should only be beneficial to include both of them in the margin samples? And a minor typo: In Eq.(4), $\\bar{x}_k$ should have been $\\bar{x}^l$?", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We would like to thank you for your review and suggestions . We are very glad that you liked the empirical analysis of generalization gap and margin distribution statistics . On that note , while not mentioned in the paper , we are in preparation to release the 700+ models we used in the paper as a dataset where researchers can easily test theories on generalization . We believe this will be one of the first datasets for studying generalization on realistic and modern network architectures and we hope it will be instrumental in the ongoing generalization research . # # Construction of Signature from Pairwise Distances ( i , j ) in Eq ( 5 ) # # For computational efficiency , we picked we pick ground truth label as `` i '' ( as you correctly pointed out ) , and the highest non-ground truth logit as `` j '' , and compute the distance between the two classes . While aggregating all pairwise distance might be more comprehensive , the complexity scales roughly quadratically with the number of classes . As such , we made the design choice to use the top two classes . In cases where the class with the highest logit is not the ground truth ( hence misclassification with negative distance ) , we discard the data point . We will further explain this choice below . We mention this detail in the text but we will make sure it is more clear . # # Notation ( i , j ) instead of { i , j } to Emphasize Orderedness # # Thank you for the suggestion . We agree and will incorporate this in the revision to avoid confusion . # # Why Only Positive Distances in Margin Distribution # # You are right that when \u201c i \u201d is the ground truth label , the sign of the distance indicates whether the point is correctly classifier or is misclassified . We indeed investigated using negative distances when computing the margin distribution . We observed that : 1 . Modern deep architectures often achieve near perfect classification on training data . Hence , the contribution of negative distances to the full distribution is negligible in most trained models . 2.A small fraction of models do have notable misclassification ( due to data augmentation or heavy regularization ) . For these models , we found that margin distribution computed with only positive samples predicted the generalization gap better than ( or at par with ) the full distribution . However , we observed that the latter is indeed a better predictor of test accuracy ( just not the gap ) . Since we focus our narrative on the generalization gap , we decided to omit these results from the main paper ; however , we will include these results in the appendix . We also note that there is no technical problem in using margin distribution with only positive samples , e.g.Bartlett \u2019 s work \u201c The Sample Complexity of Pattern Classification with Neural Networks \u201d develops a generalization bound by such samples ( paragraph above their Theorem 2 ) . # # Typo # # Thank you for pointing out the typo . It will be fixed in revision ."}], "0": {"review_id": "HJlQfnCqKX-0", "review_text": "After author response, I have increased my score. I'm still not 100% sure about the interpretation the authors provided for the negative distances. The paper is well written and is mostly clear. (1st line on page 4 has a typo, \\bar{x}_k in eq (4) should be \\bar{x}^l?) Novelty: I am not sure whether the paper adds any significant on top of what we know from Bartlett et al., Elsayed et al. since: (i). The fact that \"normalized\" margins are strongly correlated with the test set accuracy was shown in Bartlett et al. (figure 1.). A major part of the definition comes from there or from the reference they cite; (ii). Taylor approximation to compute the margin distribution is in Elsayed et al.; (iii). I think the four points listed in page 2 (which make the distinction between related work) is misleading: the way I see it is that the authors use the margin distribution in Elsayed et al which simply overcomes some of the obstacles that norm based margins may face. The only novelty here seems to be that the authors use the margin distribution at each layer. Technical pitfalls: Computing the d_{f,x,i,j} using Equation (3) is missing an absolute value in the numerator as in equation (7) Elsayed et al.. The authors interpret the negative values as misclassification: why is it true? The margin distribution used in Bartlett et al. (below Figure 4 on page 5 in arxiv:1706.08498) uses labeled data and it is obvious in this case to interpreting negative values as misclassification. I don't see how this is true for eq (3) here in this paper. Secondly, why are negative points ignored?? Misclassified points in my opinion are equally important, ignoring the information that a point is misclassified doesn't sound like a great idea. How do the experiments look if we don't ignore them? Experiments: Good set of experiments. However I find the results to be mildly taking the claims of the authors made in four points listed in page 2 away: Section 4.1, \"Empirically, we found constructing this only on four evenly-spaced layers, input, and 3 hidden layers, leads to good predictors.\". How can the authors explain this? By using linear models, authors implicitly assume that the relationship between generalization gaps and signatures are linear (in Eucledian or log spaces). However, from the experiments (table 1), we see that log models always have better results than linear models. Even assuming linear relationship, I think it is informative to also provide other metrics such as MSE, AIC, BIC etc..", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for your insightful review . # # NOVELTY # # R2 : \u201c The fact that normalized margins are correlated with generalization was shown in Bartlett Fig 1 \u201d . As you pointed out , both works build on the broad notion of \u201c margin distribution \u201d and \u201c normalization \u201d . However , there are significant differences : 1 . Margin in Bartlett uses f_i-f_j that can only reflect output margins , as opposed to ( f_i-f_j ) /||d/dx f_i - d/dx f_j|| that works for any layer . 2.We do not use margin distribution itself to predict the generalization gap , but rather distributional features that involve \u201c nonlinear transform \u201d of the distances ( quartiles or moments ) . 3.Normalization in Bartlett \u2019 s uses norm of weight matrices , which is drastically different from geometric spread of activations ( variance ) we use ( Eqs 4 and 5 ) . Also their can not be used as-is for residual networks , a drawback not present in our normalization . These distinctions result in very different predictions of the generalization , as clearly shown in our Fig 2 and Table 1 . In fact , the choice of distributional features and normalization are crucial for accurate prediction of the generalization gap . Finally , we have conducted a far larger scale of experiments , and will be releasing the 700+ realistic models used in the paper so that researchers can easily test generalization theories . This is the first of its kind . # # TECHNICAL # # # Missing Absolute Value in Eq ( 3 ) # There is no incorrectness ; we deliberately adopt \u201c signed distance \u201d . The polarity reflects which side of the decision boundary the point is . Even Eq ( 7 ) of Elsayed that you mentioned quickly evolves to signed distance in their Eq ( 8 ) . # Why Negative Distance Implies Misclassification # It was our oversight not to mention that \u201c i \u201d in our Eq ( 3 ) corresponds to the ground truth label . We will clarify this in the final version . In this case , f_i-f_j > 0 ( i.e.distance is positive ) implies correct classification and f_i-f_j < 0 implies misclassification . # Why Negative Points are Ignored # We indeed investigated using negative distances . We observed that : 1 . Modern deep architectures often achieve near perfect classification on training data . Hence , the contribution of negative distances to the full distribution is negligible in most trained models . 2.A small fraction of models do have notable misclassification ( due to data augmentation or heavy regularization ) . For these models , we found that margin distribution computed with only positive samples predicted the generalization gap better than ( or at par with ) the full distribution . However , we observed that the latter is indeed a better predictor of test accuracy ( just not the gap ) . Since we focus our narrative on the generalization gap , we decided to omit these results from the main paper ; however , we will include these results in the appendix . We also note that there is no technical problem in using margin distribution with only positive samples , e.g.Bartlett \u2019 s work \u201c The Sample Complexity of Pattern Classification with Neural Networks \u201d develops a generalization bound by such samples ( paragraph above their Theorem 2 ) . # # EXPERIMENTS # # # Why 4 Layers and Why Even Spacing # 1 . This leads to a fixed-length signature vector , hence agnostic to the architecture and depth . 2.Computing signature across all layers is expensive for large deep models . 3.Larger signature would require more pre-trained networks to avoid overfitting in regression phase . Given that each pre-trained network is only one sample in the regression task , creating a large pool of models is prohibitively expensive . Our study with 700 realistic sized pre-trained networks is perhaps already beyond the common practice for such empirical analysis . 4.The even spacing is merely a natural choice of minimal commitment and already achieves near perfect prediction ( CoD close to 1 ) is some scenarios . However , it is possible to examine other configurations . # Log/Linear # We are not sure if we understand the question . We provide an answer below , but if this is not what you meant , please let us know . We investigate the use of signature components in two ways : 1 . Directly as the input to linear regression , 2 . Applying an element-wise log to them before using them as input of the linear regression . In either case , the regression remains linear in optimization variables , but with the log transform we effectively regress the product of signature components to the gap value . # Other Criteria ( MSE , AIC , etc . ) # We have pointed out that the coefficient of determination already captures the MSE along with the scale of the error ; however , for completeness , we will include this result in the appendix . We report k-fold cross validation results as well , which is known to be asymptotically equivalent to AIC ( Stone M. ( 1977 ) An asymptotic equivalence of choice of model by cross-validation and Akaike \u2019 s criterion )"}, "1": {"review_id": "HJlQfnCqKX-1", "review_text": "The author(s) suggest using geometric margin and layer-wise margin distribution in [Elsayed et al. 2018] for predicting generalization gap. pros, a). The author shows large experiments to support their argument. cons, a). No theoretical verification (nor convincing intuition) is provided, especially for the following questions, i) what benefit can be acquired when using geometric margin defined in the paper. ii) why does normalization make sense beyond the simple scaling-free reason. For example, spectral complexity as a normalization factor in [Bartlett et al. 2017] is proposed from the fact, that the Lipschitz constant determines the complexity of network space. iii) why does the middle layer margin can help? iv) why a linear (linear log) relation between the statistic and generalization gap. Further question towards experiment, i) I don't think your comparison with Bartlett's work is fair. Their bounds suggest the gap is approximately Prob(0<X<\\gamma) + Const/\\gamma for a chosen \\gamma, where X is the normalized margin distribution. I think using the extracted signature from margin distribution and a linear predictor don't make sense here. ii) If you do regression analysis on a five layers cnn, can you have a good prediction on a nine layers cnn (or even residue cnn)? Finally, I'm not sure the novelty is strong enough since the margin definition comes from [Elsayed et al. 2018] and the strong linear relationship has been shown in [Bartlett et al. 2017, Liao et al. 2018] though in different settings.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review . We address your concerns below . # What benefit can be acquired when using geometric margin defined in the paper. # The geometric distance is the actual distance between a point \u201c x \u201d and the decision boundary f ( x ) =0 , i.e.d1=min_x ||x|| s.t . f ( x ) =0.This term is usually used in contrast to functional distance defined as d2=f ( x ) . If x is on the decision boundary , d1=d2=0 , but otherwise d1 and d2 can differ . Note that d2 can change by simple reparametrization . For instance , consider a linear decision boundary f ( x ) =w.x . In this case , geometric distance d1=f ( x ) /||w|| and d2=f ( x ) . Let F ( x ) = ( c * w ) .x , i.e.just scaling the weights by factor c. This does not change the decision boundary . For such F , d1 remains the same , but d2 scales with c. One can force a condition to make margins equal in both scenarios : by making the closet point to the decision boundary to have distance 1 . However , this requires introducing an inequality per point , similar to SVMs . With geometric margin , we can work with an unconstrained optimization and directly apply gradient descent or SGD . # Why does normalization make sense ? # Our normalization allows direct analysis of the margins across different models with the same topology ( or different datasets trained on the same network ) , which is otherwise difficult due to the positive homogeneity of ReLU networks . For example , suppose we have two networks with exactly the same weight , and then in one of the networks , we scale weight_i by constant positive factor c and the weight_ { i+1 } by 1/c ( i is a layer index ) , the predictions of the two networks remain the same ; however , their unnormalized margin distribution will be vastly different and the normalized version will be exactly the same . # Why does the middle layer margin can help ? # There is no reason we can assume a-priori that maximizing only input or output margin ( for example ) is enough for good generalization . As shown in our ablation results in Tables 1 and 4 , the combination of multiple layers performs significantly better . If we cut a deep network at any stage , we can treat the first half of the network as a feature extractor and the second half as the classifier . From this perspective , the margins at middle layer can be just as important as the margins in the output layer or input layer . Lastly , we note that Elsayed et . al.show that optimizing margin at multiple layers provides significant benefits for generalization and adversarial robustness . # Why a linear ( linear log ) relation between the statistic and generalization gap. # We are not claiming this is the true relationship between the statistics and the generalization gap . The true relationship may very well be nonlinear and one could perform a nonlinear regression to predict the gap , but it would need regularization and more data to avoid overfitting while a linear combination of simple distributional features already attains high quality prediction ( according to CoD , k-fold cross validation and MSE ) across 700+ pretrained models . This suggests that a linear relationship is indeed a very close * approximation * . # I do n't think your comparison with Bartlett 's work is fair . Their bounds suggest the gap is approximately Prob ( 0 < X < \\gamma ) + Const/\\gamma for a chosen \\gamma , where X is the normalized margin distribution . I think using the extracted signature from margin distribution and a linear predictor do n't make sense here. # We assume the reviewer is referring to theorem 1.1 of Bartlett et al.If one wishes to compute the gap to be the inside of the soft big O , the result will be much larger than the error emitted by our prediction , and will require picking appropriate gamma and delta values . We further note the following : the case study of Bartlett et . al . ( section 2 ) explicitly show in their diagrams ( Figures 2 and 3 ) the normalized distribution as evidence of generalization prediction power ( instead of the bound itself ) and this normalized distribution is closely related to but is not directly their bounds ( they drop the log terms ) ; extracting the statistics in a sense quantifies their case study . Before submitting the paper , we also had personal communication with one of the authors of Bartlett et . al. , and the author agreed that our comparison was fair ."}, "2": {"review_id": "HJlQfnCqKX-2", "review_text": "This paper does not even try to propose yet another \"vacuous\" generalization bounds, but instead empirically convincingly shows an interesting connection between the proposed margin statistics and the generalization gap, which could well be used to provide some \"prescriptive\" insights (per Sanjeev Arora) towards understanding generalization in deep neural nets. I have no major complaints but for a few questions regarding clarifications, 1. From Eq.(5), such distances are defined for only one out of the many possible pairs of labels. So when forming the so-called \"margin signature\", how exactly do you compose it from all such pair-wise distances? Do you pool all the distances together before computing the statistics, or do you aggregate individual statistics from pair-wise distances? And how do you select which pairs to include or exclude? Are you assuming \"i\" is always the ground-truth label class for $x_k$ here? 2. In Eq.(3), the way you define the distance (that flipping i and j would change the sign of the distance) is implying that {i, j} should not be viewed as an unordered pair, in which case a better notation might be (i, j) (i.e. replacing sets \"{}\" with tuples \"()\" to signal that order matters). And why do you \"only consider distances with positive sign\"? I can understand doing this for when neither i nor j corresponds to the ground-truth label of x, because you really can't tell which score should be higher. But when i happens to be the ground-truth label, wouldn't a positive distance and a negative distance be meaningful different and therefore it should only be beneficial to include both of them in the margin samples? And a minor typo: In Eq.(4), $\\bar{x}_k$ should have been $\\bar{x}^l$?", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We would like to thank you for your review and suggestions . We are very glad that you liked the empirical analysis of generalization gap and margin distribution statistics . On that note , while not mentioned in the paper , we are in preparation to release the 700+ models we used in the paper as a dataset where researchers can easily test theories on generalization . We believe this will be one of the first datasets for studying generalization on realistic and modern network architectures and we hope it will be instrumental in the ongoing generalization research . # # Construction of Signature from Pairwise Distances ( i , j ) in Eq ( 5 ) # # For computational efficiency , we picked we pick ground truth label as `` i '' ( as you correctly pointed out ) , and the highest non-ground truth logit as `` j '' , and compute the distance between the two classes . While aggregating all pairwise distance might be more comprehensive , the complexity scales roughly quadratically with the number of classes . As such , we made the design choice to use the top two classes . In cases where the class with the highest logit is not the ground truth ( hence misclassification with negative distance ) , we discard the data point . We will further explain this choice below . We mention this detail in the text but we will make sure it is more clear . # # Notation ( i , j ) instead of { i , j } to Emphasize Orderedness # # Thank you for the suggestion . We agree and will incorporate this in the revision to avoid confusion . # # Why Only Positive Distances in Margin Distribution # # You are right that when \u201c i \u201d is the ground truth label , the sign of the distance indicates whether the point is correctly classifier or is misclassified . We indeed investigated using negative distances when computing the margin distribution . We observed that : 1 . Modern deep architectures often achieve near perfect classification on training data . Hence , the contribution of negative distances to the full distribution is negligible in most trained models . 2.A small fraction of models do have notable misclassification ( due to data augmentation or heavy regularization ) . For these models , we found that margin distribution computed with only positive samples predicted the generalization gap better than ( or at par with ) the full distribution . However , we observed that the latter is indeed a better predictor of test accuracy ( just not the gap ) . Since we focus our narrative on the generalization gap , we decided to omit these results from the main paper ; however , we will include these results in the appendix . We also note that there is no technical problem in using margin distribution with only positive samples , e.g.Bartlett \u2019 s work \u201c The Sample Complexity of Pattern Classification with Neural Networks \u201d develops a generalization bound by such samples ( paragraph above their Theorem 2 ) . # # Typo # # Thank you for pointing out the typo . It will be fixed in revision ."}}