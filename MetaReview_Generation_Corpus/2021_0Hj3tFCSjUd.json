{"year": "2021", "forum": "0Hj3tFCSjUd", "title": "Energy-based View of Retrosynthesis", "decision": "Reject", "meta_review": "Before the discussion phase nearly all reviewers had doubts about the comparison of the current work with state-of-the-art works (notably Yan et al., 2020, RetroXpert, and GraphRETRO). The authors then compared with these works and emphasized that these works rely on hand-crafted features. They argue that the fairest comparison is the one where each method uses the same sort of features during train/test time. This is because in certain real world settings we may not have accurate estimates of such features (e.g., atom mappings, templates, reaction centers). However, in the revised version of the paper the authors did not adhere to this concept of fair comparison in Table 4 of Appendix A.4. Here their method uses reaction centers as input while baselines do not. While the authors claimed that the comparison here was designed to show how reaction centers provided as input improved performance, this doesn't seem like a good way to show it: to isolate the improvement due to reaction center inputs you should fix everything else, i.e., the rest of the method. \n\nApart from the above contradiction, I buy the arguments of reviewers that distinguishing between methods that use hand-crafted features and those that do not is not a meaningful distinction. One can apply atom-mapping or reaction center discovery algorithms as data preprocessing before applying other methods. Ablation studies where such preprocessing is added or removed are interesting, but it is completely fair for any method to use such preprocessing before applying their method, it is up to the modeller. \n\nI would have argued for acceptance had the authors either (a) just included results from SOTA methods (one, RetroXpert was published 1 month after the ICLR submission deadline), and/or (b) reran their approach with such preprocessing. However the authors ended up hurting the submission by emphasizing a difference between using handcrafted features and not, then contradicting their experimental setup in Table 4.\n\nThis is a good paper, but I agree it is not ready to be accepted at ICLR. I recommend the authors do the following: (a) use any preprocessing they want for their method and compare with the state-of-the-art, (b) if they want they can run their method without any preprocessing as an interesting ablation study, (c) remove Table 4 (as (b) already does this type of an ablation study), (d) describe recent work through the lense of EBM, (e) resubmit to a strong ML conference. The new submission will be much stronger.", "reviews": [{"review_id": "0Hj3tFCSjUd-0", "review_text": "# # # Summary of the paper This paper proposes an energy based model ( EBM ) for retrosynthesis . The best model ( dual model ) leverages the duality of retrosynthesis and reaction prediction . The EBM contains three factors : prior on reactants $ p ( X ) $ , forward reaction probability $ p ( y | X ) and backward posterior $ P ( X|y ) $ . The duality loss penalizes the KL divergence of the two directions . The forward predictor is trained on a mixture of original data and samples drawn from the backward predictor . The dual model shows improvement over template-based and template-free baselines . # # # Strength 1 . The dual model is novel and interesting . EBM provides a unified view of forward and backward reaction prediction . It will be interesting to see how this improves the forward prediction performance . 2.EBM is a flexible framework , which can be applied to both template-based and template-free approaches . # # # Weakness ( and questions ) 1 . Clarity : The paper describes EBM view of many methods , ranging from sequence and graph based methods . As a result , each subsection is too sketchy and lots of details are missing . For example : 1 . For Dual-TB ( ours ) , what is the exact model architecture ? I understand that the candidates are generated from reaction templates , but what 's the parametrization of $ p_\\alpha , p_\\gamma , p_\\eta $ ? Is it GLN ? 2.For Dual-TF ( ours ) , what is the model architecture ? Is it transformer ? 3.What is the augmented USPTO 50k ? Is it randomized SMILES ? For Dual-TB ( ours ) , is it trained on augmented USPTO 50k ? Why SMILES randomization matters for templated based methods ? 4. $ X $ is a set of compounds , how do you generate a set of compounds in an order-invariant manner ? 5.For duality constraint , you sample from backward predictor using beam search . Why not the other way around ? Why not sample from forward predictor and train your backward predictor on the additional samples ? 2.At inference time , it is hard to sample from EBM . Therefore , authors propose to rank the candidates generated from other models ( reaction templates or transformers ) . This really limits the performance ( and applicability ) of the approach . To my knowledge , you can do MCMC based on Langevin dynamics to sample from EBM . Is this not possible for retrosynthesis ? 3.The best model is this paper actually performs much worse than the state-of-the-art models . For instance , RetroXpert ( Yan et al. , NeurIPS 2020 ) achieves 65.6 % top-1 accuracy ( reaction type unknown ) and 70.4 % top-1 accuracy ( reaction type known ) . This is much better than the dual model ( 55.2 % , 67.7 % ) . Somnath et al , 2020 also achieves 64.2 % top-1 accuracy ( reaction type unknown ) , which is much higher than dual model ( 55.2 % ) . # # # Overall evaluation I vote for weak reject of the paper , primarily due to the weaker result and lack of clarity . I believe the dual formulation can be applied to these above state-of-the-art models ( if code is available ) . I suspect the weak result is primarily due to the base model ( e.g. , transformer ) . I am happy to adjust my score if there are stronger results and the clarity can be improved . One suggestion for clarity is to put perturbed / bidirectional models into appendix since they are not helpful anyway ... # # # Post Rebuttal I would like to thank authors for their response . I think the paper needs to be improved further to get accepted . I do believe that the proposed dual learning method is promising , but empirical evaluation is still lacking . So my review score stays the same . [ 1 ] Yan et al. , RetroXpert : Decompose Retrosynthesis Prediction like A Chemist , NeurIPS 2020 . [ 2 ] Somnath et al. , Learning Graph Models for Template-Free Retrosynthesis , 2020", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the thoughtful comments . We uploaded the revised paper and changes are highlighted in blue . Please see our response to \u201c Weakness \u201d : * * Q1 , 2 , 3 * * : The model architecture for the dual model under template-based or template-free setup are both transformers ( not GLN ) ( See the paragraph under eq4 on page 4 ) . $ p_ { \\alpha } $ , $ p_ { \\gamma } $ , $ p_ { \\eta } $ are transformer parameters . Dual-TB and Dual-TF have the same training procedure ( including model architecture ) , but different inferences . In other words , we train the same dual model in TB or TF set up , which are used to rank a list of reactant candidates , e.g. $ [ X1 , X2 , X3 .. ] $ that are associated with a given product $ y $ during inference . The proposal reactant list can be generated with templates or without templates , leading to template-based or template-free methods . * * Q4 * * : Yes . The augment procedures of USPTO50K are as follows . For each reaction , 1 . Replace each molecule in reactants or product using random SMILES . 2.Random permute the order of reactant molecules . We updated it in the revised draft . We trained Dual-TB on both USPTO 50K ( Table 2 upper ) and augmented USPTO 50k ( Table 2 bottom ) . Random SMILES matters for \u201c sequence-based model \u201d , as opposed to \u201c graph-based model \u201d . Sequence-based models linearize the 2d molecule to a 1d sequence . The linear compression losses a rich local environment of a molecule . Random SMILES are different linearizations of the same molecule , which rescue some of the information loss . Using random SMILES also prevents transformers from overfitting . Note that as we don \u2019 t add new molecules , random SMILES augmentation won \u2019 t influence graph-based models \u2019 performance , as graph-based models are permutation invariant . Random SMILES doesn \u2019 t have a distinguished effect on template-based versus template-free models -- it benefits both template-based and template-free models -- as long as the model is sequenced based ( Table 2 template-based vs Table 3 template-free ) . * * Q5 * * : There are two ways : 1 . Data level ( we used ) : As stated above , the Random SMILES data augmentation we used includes permuting the order of reactants , which achieves order invariant during training stochastically . 2.NN level : Modify the position encoding in transformer implementation : Transformer has a position encoding to mark the different locations on an input sequence . We modified the position encoding such that each molecule starts with 0 encodings , as opposed to the concatenated position in the reactants sequence . We didn \u2019 t use it in our paper , because ( 1 ) data level has superior performance and is easy . See details in Appendix A12 . Table 7 of revised paper . * * Q6 * * : Yes , we can draw samples from the forward model and train backward on the additional samples . We can also do both ( the paper plus this suggestion ) , or even repeat them a few times . However drawing samples from the forward direction would either require empirical reactants from data , or from the learned $ p ( X ) $ , which is harder than drawing samples from the backward direction . * * Q7 * * : It is true drawing samples from EBMs can be generally challenging . Langevin requires differentiability w.r.t the observations , i.e. , molecules , which is NOT applicable here since the molecule SMILES string is discrete . Fortunately , using proposal distribution to sample from EBM is a principled way in general , such as the Metropolis Hastings algorithm . In our case , using the $ p ( X|y ) $ as a proposal is a good trade-off , as it has full support in the molecule space , while being close to the EBM distribution . In practice , $ p ( X|y ) $ can be efficiently obtained by K-size beam search . * * Q8 : Not SOTA compared with RetroXpert and GraphRETRO * * Please see the response to all reviewers with the title `` Reply to R1 , R2 , R3 , R4 : Not SOTA ... \u201d . * * Q9 : The dual formulation can be applied to these above state-of-the-art models ( if code is available ) . * * Yes.The generalization is an advantage of the dual framework , as it can be adapted to different models . RetroXpert 's code is not uploaded yet : https : //github.com/uta-smile/RetroXpert ( only data is uploaded.Not code ) . We are happy to adapt RetroXpert into the dual model framework if the authors upload code . * * Q10 : Are weak results due to transformer ? * * See the response titled '' reply to all ; NOT SOTA ... '' and Table 4 in Appendix A . 4 of our revised paper . The weak results are due to that we are not using extra chemical features . If we add those features to the dual model , we get better results than RetroXpert . Dual : 70.1 % ; 73.2 % RetroXpert : 65.6 % ; 70.4 % * * Q11 : Move perturbed / bidirectional to appendix * * Good point . Thanks.But our goal is to present a full picture of different model designs . In our opinions , both good and bad performances are valuable to trigger helpful discussions and inspire better models in the community . But since we run out of space , we plan to move some parts into the appendix ."}, {"review_id": "0Hj3tFCSjUd-1", "review_text": "# Summary # This paper introduces a re-interpretation of seq2seq and graph2graph retrosynthesis models with energy based models ( EBMs ) . EBM is a general log-linear framework to model joint distributions of a feature variable X and a target variable y , first introduced to connect discriminative ( DNN ) models with generative ( DNN ) models . This paper shows the reformulations of typical seq2seq retrosynthesis models and graph2graph models . Also , the paper proposes a dual training model that optimizes both the forward path model and the backward path model . Experimental results show that the dual training models perform better than conventional retrosynthesis models in template-based and template-free retrosynthesis frameworks . # Comments # I have two major concerns . First , I can not clearly understand a new insight or knowledge that is brought by the EBM-based re-formulation of retrosynthesis models . Any probabilistic models can be described by the full joint distribution of all variables . In my understanding , the EBM is a way of modeling the joints with the log-linear model plus the potential function . It seems for me that the current manuscript is successful in re-wiring the existing retrosynthesis frameworks into EBMs , but that is all . Please clarify what readers can learn about `` connections and ... differences between models , ... understanding of model design [ abstract ] '' from the EMB modelings . Second , the State-of-the-Art of the retrosynthesis in the manuscript is somewhat outdated . To the best of my knowledge , the current best-performing retrosynthesis models are [ 1 ] and [ 2 ] , and the scores of these models are higher than the reported results of the proposed dual models . I think the current manuscript needs modifications to appropriately cite these papers and replace the standpoint of the submitted work . Personally , I am interested in how the EMB can interpret these latest models . [ 1 ] Somnath+ , `` Learning graph models for template-free retrosynthesis '' , arXiv:2006.07038 , June 2020 . [ 2 ] Yan+ , `` RetroXpert : Decompose retrosynthesis prediction like a chemist '' , chemrxiv:11869692.v3 , June 2020 . # Evaluation points # ( + ) First to apply the EBM for retrosynthesis prediction models ( + ) Proposed a forward/backward simultaneous ( dual ) training ( -- ) It is unclear what we can learn by rewriting the retrosynthesis models with EBM . ( - ) Reported results do not update the State-of-the-Art prediction accuracy of retrosynthesis models in the literature", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the insightful comments . We have loaded the revised paper to address the comments , and the changes are highlighted in blue . Please see our response : # # # Q1 : Why is the EBM framework beneficial ? What do we get from unifying models into EBM ? Thank you for the question . EBMs provide a way to map high dimensional data ( X , y ) into an unnormalized probability distribution . EBM works on a wide range of problems and demonstrates good performance . The community has accumulated lots of knowledge of how to design energy scores and to train them efficiently . That prior knowledge can generalize to new problems or new model designs . EBM provides the following : * * 1 . Principled ways for training * * . EBMs have many off-the-shelf training methods that are ready to use , including but limited to contrastive divergence ( Hinton , 2002 ) , pseudo-likelihood ( PL ) ( Besag , 1975 ) , conditional composite likelihood ( CL ) ( Lindsay , 1988 ) , score matching ( SM ) ( Hyv\u00a8arinen , 2005 ) , minimum ( diffusion ) Stein kernel discrepancy estimator ( DSKD ) ( Barp et al.,2019 ) , non-local contrastive objectives ( NLCO ) ( Vickrey et al. , 2010 ) , minimum probability flow ( MPF ) ( SohlDickstein et al. , 2011 ) , and noise-contrastive estimation ( NCE ) ( Gutmann and Hyvarinen , 2010 ) . For example , if we design the energy score function as the \u201c bidirectional model \u201d ( eq11 in Sec 3.1.5 ) , with EBM in mind , it is natural to think of using pseudo-likelihood ( PL ) as the training approach . Additionally , there are different ways to design loss see Sec 2.2 of LeCun et al tutorial http : //yann.lecun.com/exdb/publis/pdf/lecun-06.pdf page 6 * * 2 . Better inference * * EBM model \u2019 s inference is flexible . LeCun et al tutorial Page 6 ( inference ) . EBM can not only make inferences such as \u201c classification \u201d or \u201c prediction \u201d , e.g.what is best y ( label ) for X ( input image ) , but also EBM \u2019 s inference can be used to rank possible ( X , y ) -pairs . From the EBM point of view , we can decouple the model , learning ( training ) , and inference ( testing ) . For example , one can use the autoregressive model for modeling and learning , while during inference we can use it as an energy function to get better samples ( via re-ranking as we used , or doing Gibbs-sampling to refine the samples ) . * * 3.Advance understanding of different model designs . * * * Extract commonalities and differences between EBM variants . One example , RetroXpert , GraphRETRO , and G2G share the same energy score format ( eq 14 ) but the differences are the various instantionans of $ p ( X|y , c ) $ . Another example is comparing the energy score design ordered model ( eq3 ) , perturbed model ( eq8 ) , and bidirectional model ( eq11 ) . We see their bidirectional degree : ordered < perturbed < bidirectional The experiments show although bidirectional provides more information to the model , at the same time adds challenges in modeling $ p ( X|y ) $ ( from $ p ( X_i| X_ { \\setminus i } , y ) $ to $ p ( X|y ) $ ) and training . In addition , some variants share the same training procedure ( ordered model and perturbed model both use MLE ) . * Understand the strengths and limitations in model design . Full model versus ordered model . The full model is more flexible that do not impose restrictions on model design , however , the expressional power adds challenges in training , say enumerating all possible molecules . In contrast , the ordered model is restricted to the autoregressive model , but the training is much tractable ( MLE ) * Compare the complexity of learning or inference . See appendix A.7 `` TIME AND SPACE COMPLEXITY ANALYSIS '' # # # Q2 : Not SOTA , compared with RetroXpert ( Yan et al , NeurIPS 2020 ) and GraphRETRO ( Somnath et al , 2020 ) . Please kindly refer to the response to all reviewers with the title `` Reply to R1 , R2 , R3 , R4 : Not SOTA ... \u201d . We have a different setup . These RetroXpert and GraphRETRO are not template free . The lower results are due to that we are not using extra chemical features . If we add those features to the dual model , we get better results than RetroXpert . Dual : 70.1 % ; 73.2 % RetroXpert : 65.6 % ; 70.4 % ( Table 4 in Appendix A.4 of our revised paper ) . # # # Q3 : \u201c I am interested in how the EMB can interpret these latest models. \u201d How to interpret the latest model such as RetroXpert and GraphRETRO into the EBM framework ? Please also see Appendix A.1 and A2 of revised paper . Yes , RetroXpert and GraphRETRO can be integrated into our EBM framework in a way that is similar to G2G ( Sec 3.2.3 in our paper.eq14 ) .The difference between the three is parameterizing $ p ( X|c , y ) $ , which is the step that generates reactants from synthons . Synthons are subgraphs extracted from the products by breaking the bonds in the reaction centers . Reactants are generated by completing the \u201c missing pieces \u201d ( \u201c leaving groups \u201d in GraphRETRO ) in synthon . RetroXpert uses transformers to complete the missing pieces of reactant from synthons ; GraphRETRO uses a predefined list of leaving groups and selects the missing pieces ; G2G uses graph generation ."}, {"review_id": "0Hj3tFCSjUd-2", "review_text": "In this paper , the authors use the framework of energy based models to describe several known approaches for ML-based retrosynthesis in a unified way . This allows to combine retrosynthetic ( backward ) and reaction prediction ( forward ) in a principled way . Based on this analysis , a dual model is proposed which used a duality constraint as a regulariser , leading to improved performance over several baselines models , but not over SOTA ( see below ! ) . The theoretical analysis alone is very interesting , and well described . However , I have a few concerns with missing ablation experiments and the positioning , which could be addressed to make the paper stronger in my opinion . Overall , I think this paper should be accepted at ICLR after the following points have been addressed ( or the authors commit to provide the additional data in the camera ready version if the time of the rebuttal phase is too short to run additional experiments ) . However , in the current form , the paper is not ready . My evaluation is for the current form of the paper , and I am happy to change it significantly during the rebuttal . Ablation Experiments : Is the duality constraint actually needed ? It would be important to perform the ablation experiment where $ \\beta $ is set to 0 . In other words , what happens if you just use $ \\log p ( X|y ) + \\log p ( y|X ) $ to rank the candidates ? If the authors perform these additional experiments and report the numbers , I will increase my score regardless of the outcome of the experiments . Prior work : The state of the art claim is not correct . Yan et al achieve higher performance on the same dataset https : //chemrxiv.org/articles/preprint/Interpretable_Retrosynthesis_Prediction_in_Two_Steps/11869692/2 which has already been published last February ! Therefore , please remove the SOTA claim from the paper , and acknowledge the Yan et al work . For this reviewer well-motivated modelling , honest analysis and proper experimentation is more important than chasing SOTA , and not achieving SOTA will not affect the evaluation negatively . Also , the connection of forward reaction and retrosynthesis prediction via Bayes Theorem has been studied here https : //arxiv.org/abs/2003.03190 which should be acknowledged . Other work preceding has also used a combination of backward and forward prediction . For example , Segler et al Nature 2018 and Coley et al Science 2019 use a model for p ( X|y ) to propose disconnections combined with a model for p ( X , y ) to remove low probability solution . The usefulness of SMILES augmentation has been previously shown , please cite the previous work by Arus Pous et al https : //jcheminf.biomedcentral.com/articles/10.1186/s13321-019-0393-0 The Coley et al 2017a paper that the authors cite in the introduction is a wonderful paper , however , it is not concerned with retrosynthesis . I would suggest to cite the review article by Strieth-Kalthoff et al https : //doi.org/10.1039/C9CS00786E instead , which provides a good overview over ML approaches for ( retro ) synthesis . Furthermore , I would suggest to cite Segler & Waller 2017 in the introduction , which was the first paper to suggest deep neural networks for both retrosynthesis & reaction prediction ( see also https : //doi.org/10.1016/j.ddtec.2020.06.002 ) Coley 2017a should be cited later in the context of forward prediction . Comments : Table 3 is a bit unclear . With Type , do you mean the reaction type is given ? small things : - in eq 13 , the is a low dot , should this have been a centred dot for multiplication ? - Please do n't use Google house fonts for your figures , to maintain anonymity .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the insightful comments and helpful suggestions ! We have loaded a revised paper to address the comments , and the changes are highlighted in blue . Please see our response : * * Q1 : Ablation Experiments on with or without dual constraint . * * Thank you for the invaluable suggestions . Your suggestions have led to precious insights . We have added the following ablation experiments in Appendix A6 of the revised paper : a. Dual model ( with dual constraint ) : ( eq6 ) $ E_ { dual } [ log p ( X ) + log\u2061p ( X|y ) ] + log\u2061p ( y|X ) $ b . Without dual constraint : $ log p ( X ) + log\u2061p ( X|y ) + log\u2061p ( y|X ) $ c. Without prior : $ log\u2061p ( X|y ) + log\u2061p ( y|X ) $ d. Only backward : $ log\u2061p ( X|y ) $ The results in table 5 ( Appendix A6 ) show that every component of the dual loss contributes to the performance positively . For top 1 accuracy : ( a ) dual 67.7 % ; ( b ) 67.0 % ; ( c ) 66.1 % ; ( d ) 60.9 % . Comparing ( a ) and ( b ) is the contribution of dual constraint , which is 0.7 % . It is harder to improve upon a higher accuracy range ( i.e . ( b ) ) than a lower range ( i.e . ( d ) ) . * * Q2 : Please cite relevant papers * * Thank you for the great suggestions . We have cited all the suggested papers in the revised version . Segler Nature 2018 , Segler & Waller 2017 , Strieth-Kalthoff et al 2020 , Johansson et al 2020 , Yan et al 2020 , Somnath et al 2020 , Guo et al 2020 , Coley et al Science 2019 ; Arus-Pous et al 2019 . In our first edition , we have already cited Segler & Waller 2017 ( \u201c NeuralSym \u201d in Table 1 ) , and Segler Nature 2018 ( Introduction ) . Segler Nature 2018 focuses on multiple-step retrosynthesis . We focus on the single-step retrosynthesis , which is a critical building block of the multi-step process . We have included a discussion about Segler & Waller 2017 in the result section . We will include more discussions with other methods in the camera-ready edition . * * Q3 : Table 3 . With Type , do you mean the reaction type is given ? * * Yes * * Q4 : eq13 should be a centered dot for multiplication , not a dot . * * Yes.Thanks.Fixed . * * Q5 : Please change fonts for your figures . * * Apologize . We changed the fonts to \u201c courier \u201d . * * Q6 : Not SOTA , compared with RetroXpert ( Yan et al , NeurIPS 2020 ) and GraphRETRO ( Somnath et al , 2020 ) [ from other reviewers ] . * * Please kindly refer to the response to all reviewers with the title `` Reply to R1 , R2 , R3 , R4 : Not SOTA ... \u201d . We have a different setup . RetroXpert and GraphRETRO are not template free methods . The lower results are due to the fact that we are not using extra chemical features . If we add those features to the dual model , we get better results than RetroXpert . Dual : 70.1 % ; 73.2 % RetroXpert : 65.6 % ; 70.4 % ( Table 4 in Appendix A.4 of our revised paper ) ."}, {"review_id": "0Hj3tFCSjUd-3", "review_text": "SUMMARY This paper uses the statistical physics-inspired energy-based model formalism to study the by now `` canonical '' problem of retrosynthesis using deep learning . The authors use an interesting variant that combines forward and backward prediction . The authors use template-based and template-free models . PROS - This reviewer believes that energy-based models have an elegance and connection to statistical mechanics that should be explored more in the area of machine learning . This work goes in this interesting direction . - Based on the above , and as far as the reviewer is appraised , this is a unique , non-derivative direction in the field and therefore deserving of consideration for acceptance . - The dual model seems to be very useful given the increase in template-based and non-template-based model performance . This could be applied to other transformer-based tasks in chemistry and graph-based ML - The authors compared their models to a variety of SOTA models and approaches , they also were thorough and explored both DeepSMILES and SELFIES . CONS - Some of the mathematical formalism could be moved to supplementary to allow for better discussion . MINOR FORMATTING - The authors may want to give the manuscript a pass for grammar . There are missing articles in a few sentences .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for the helpful suggestions . We thank the reviewer for the positive feedbacks and for highlighting the advantages of our work . We have uploaded the revised paper , and the changes are highlighted in blue . \u201c Moving some mathematical details into supplementary \u201d is a very helpful suggestion , as it can ( 1 ) make the paper more readable , ( 2 ) allow in-depth discussion , and ( 3 ) avoid distracting the readers from unnecessary details . We plan to do so for the camera-ready version . However , currently , we are debating on which parts should be put into the appendix , as different readers seem to be interested in different parts ."}], "0": {"review_id": "0Hj3tFCSjUd-0", "review_text": "# # # Summary of the paper This paper proposes an energy based model ( EBM ) for retrosynthesis . The best model ( dual model ) leverages the duality of retrosynthesis and reaction prediction . The EBM contains three factors : prior on reactants $ p ( X ) $ , forward reaction probability $ p ( y | X ) and backward posterior $ P ( X|y ) $ . The duality loss penalizes the KL divergence of the two directions . The forward predictor is trained on a mixture of original data and samples drawn from the backward predictor . The dual model shows improvement over template-based and template-free baselines . # # # Strength 1 . The dual model is novel and interesting . EBM provides a unified view of forward and backward reaction prediction . It will be interesting to see how this improves the forward prediction performance . 2.EBM is a flexible framework , which can be applied to both template-based and template-free approaches . # # # Weakness ( and questions ) 1 . Clarity : The paper describes EBM view of many methods , ranging from sequence and graph based methods . As a result , each subsection is too sketchy and lots of details are missing . For example : 1 . For Dual-TB ( ours ) , what is the exact model architecture ? I understand that the candidates are generated from reaction templates , but what 's the parametrization of $ p_\\alpha , p_\\gamma , p_\\eta $ ? Is it GLN ? 2.For Dual-TF ( ours ) , what is the model architecture ? Is it transformer ? 3.What is the augmented USPTO 50k ? Is it randomized SMILES ? For Dual-TB ( ours ) , is it trained on augmented USPTO 50k ? Why SMILES randomization matters for templated based methods ? 4. $ X $ is a set of compounds , how do you generate a set of compounds in an order-invariant manner ? 5.For duality constraint , you sample from backward predictor using beam search . Why not the other way around ? Why not sample from forward predictor and train your backward predictor on the additional samples ? 2.At inference time , it is hard to sample from EBM . Therefore , authors propose to rank the candidates generated from other models ( reaction templates or transformers ) . This really limits the performance ( and applicability ) of the approach . To my knowledge , you can do MCMC based on Langevin dynamics to sample from EBM . Is this not possible for retrosynthesis ? 3.The best model is this paper actually performs much worse than the state-of-the-art models . For instance , RetroXpert ( Yan et al. , NeurIPS 2020 ) achieves 65.6 % top-1 accuracy ( reaction type unknown ) and 70.4 % top-1 accuracy ( reaction type known ) . This is much better than the dual model ( 55.2 % , 67.7 % ) . Somnath et al , 2020 also achieves 64.2 % top-1 accuracy ( reaction type unknown ) , which is much higher than dual model ( 55.2 % ) . # # # Overall evaluation I vote for weak reject of the paper , primarily due to the weaker result and lack of clarity . I believe the dual formulation can be applied to these above state-of-the-art models ( if code is available ) . I suspect the weak result is primarily due to the base model ( e.g. , transformer ) . I am happy to adjust my score if there are stronger results and the clarity can be improved . One suggestion for clarity is to put perturbed / bidirectional models into appendix since they are not helpful anyway ... # # # Post Rebuttal I would like to thank authors for their response . I think the paper needs to be improved further to get accepted . I do believe that the proposed dual learning method is promising , but empirical evaluation is still lacking . So my review score stays the same . [ 1 ] Yan et al. , RetroXpert : Decompose Retrosynthesis Prediction like A Chemist , NeurIPS 2020 . [ 2 ] Somnath et al. , Learning Graph Models for Template-Free Retrosynthesis , 2020", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the thoughtful comments . We uploaded the revised paper and changes are highlighted in blue . Please see our response to \u201c Weakness \u201d : * * Q1 , 2 , 3 * * : The model architecture for the dual model under template-based or template-free setup are both transformers ( not GLN ) ( See the paragraph under eq4 on page 4 ) . $ p_ { \\alpha } $ , $ p_ { \\gamma } $ , $ p_ { \\eta } $ are transformer parameters . Dual-TB and Dual-TF have the same training procedure ( including model architecture ) , but different inferences . In other words , we train the same dual model in TB or TF set up , which are used to rank a list of reactant candidates , e.g. $ [ X1 , X2 , X3 .. ] $ that are associated with a given product $ y $ during inference . The proposal reactant list can be generated with templates or without templates , leading to template-based or template-free methods . * * Q4 * * : Yes . The augment procedures of USPTO50K are as follows . For each reaction , 1 . Replace each molecule in reactants or product using random SMILES . 2.Random permute the order of reactant molecules . We updated it in the revised draft . We trained Dual-TB on both USPTO 50K ( Table 2 upper ) and augmented USPTO 50k ( Table 2 bottom ) . Random SMILES matters for \u201c sequence-based model \u201d , as opposed to \u201c graph-based model \u201d . Sequence-based models linearize the 2d molecule to a 1d sequence . The linear compression losses a rich local environment of a molecule . Random SMILES are different linearizations of the same molecule , which rescue some of the information loss . Using random SMILES also prevents transformers from overfitting . Note that as we don \u2019 t add new molecules , random SMILES augmentation won \u2019 t influence graph-based models \u2019 performance , as graph-based models are permutation invariant . Random SMILES doesn \u2019 t have a distinguished effect on template-based versus template-free models -- it benefits both template-based and template-free models -- as long as the model is sequenced based ( Table 2 template-based vs Table 3 template-free ) . * * Q5 * * : There are two ways : 1 . Data level ( we used ) : As stated above , the Random SMILES data augmentation we used includes permuting the order of reactants , which achieves order invariant during training stochastically . 2.NN level : Modify the position encoding in transformer implementation : Transformer has a position encoding to mark the different locations on an input sequence . We modified the position encoding such that each molecule starts with 0 encodings , as opposed to the concatenated position in the reactants sequence . We didn \u2019 t use it in our paper , because ( 1 ) data level has superior performance and is easy . See details in Appendix A12 . Table 7 of revised paper . * * Q6 * * : Yes , we can draw samples from the forward model and train backward on the additional samples . We can also do both ( the paper plus this suggestion ) , or even repeat them a few times . However drawing samples from the forward direction would either require empirical reactants from data , or from the learned $ p ( X ) $ , which is harder than drawing samples from the backward direction . * * Q7 * * : It is true drawing samples from EBMs can be generally challenging . Langevin requires differentiability w.r.t the observations , i.e. , molecules , which is NOT applicable here since the molecule SMILES string is discrete . Fortunately , using proposal distribution to sample from EBM is a principled way in general , such as the Metropolis Hastings algorithm . In our case , using the $ p ( X|y ) $ as a proposal is a good trade-off , as it has full support in the molecule space , while being close to the EBM distribution . In practice , $ p ( X|y ) $ can be efficiently obtained by K-size beam search . * * Q8 : Not SOTA compared with RetroXpert and GraphRETRO * * Please see the response to all reviewers with the title `` Reply to R1 , R2 , R3 , R4 : Not SOTA ... \u201d . * * Q9 : The dual formulation can be applied to these above state-of-the-art models ( if code is available ) . * * Yes.The generalization is an advantage of the dual framework , as it can be adapted to different models . RetroXpert 's code is not uploaded yet : https : //github.com/uta-smile/RetroXpert ( only data is uploaded.Not code ) . We are happy to adapt RetroXpert into the dual model framework if the authors upload code . * * Q10 : Are weak results due to transformer ? * * See the response titled '' reply to all ; NOT SOTA ... '' and Table 4 in Appendix A . 4 of our revised paper . The weak results are due to that we are not using extra chemical features . If we add those features to the dual model , we get better results than RetroXpert . Dual : 70.1 % ; 73.2 % RetroXpert : 65.6 % ; 70.4 % * * Q11 : Move perturbed / bidirectional to appendix * * Good point . Thanks.But our goal is to present a full picture of different model designs . In our opinions , both good and bad performances are valuable to trigger helpful discussions and inspire better models in the community . But since we run out of space , we plan to move some parts into the appendix ."}, "1": {"review_id": "0Hj3tFCSjUd-1", "review_text": "# Summary # This paper introduces a re-interpretation of seq2seq and graph2graph retrosynthesis models with energy based models ( EBMs ) . EBM is a general log-linear framework to model joint distributions of a feature variable X and a target variable y , first introduced to connect discriminative ( DNN ) models with generative ( DNN ) models . This paper shows the reformulations of typical seq2seq retrosynthesis models and graph2graph models . Also , the paper proposes a dual training model that optimizes both the forward path model and the backward path model . Experimental results show that the dual training models perform better than conventional retrosynthesis models in template-based and template-free retrosynthesis frameworks . # Comments # I have two major concerns . First , I can not clearly understand a new insight or knowledge that is brought by the EBM-based re-formulation of retrosynthesis models . Any probabilistic models can be described by the full joint distribution of all variables . In my understanding , the EBM is a way of modeling the joints with the log-linear model plus the potential function . It seems for me that the current manuscript is successful in re-wiring the existing retrosynthesis frameworks into EBMs , but that is all . Please clarify what readers can learn about `` connections and ... differences between models , ... understanding of model design [ abstract ] '' from the EMB modelings . Second , the State-of-the-Art of the retrosynthesis in the manuscript is somewhat outdated . To the best of my knowledge , the current best-performing retrosynthesis models are [ 1 ] and [ 2 ] , and the scores of these models are higher than the reported results of the proposed dual models . I think the current manuscript needs modifications to appropriately cite these papers and replace the standpoint of the submitted work . Personally , I am interested in how the EMB can interpret these latest models . [ 1 ] Somnath+ , `` Learning graph models for template-free retrosynthesis '' , arXiv:2006.07038 , June 2020 . [ 2 ] Yan+ , `` RetroXpert : Decompose retrosynthesis prediction like a chemist '' , chemrxiv:11869692.v3 , June 2020 . # Evaluation points # ( + ) First to apply the EBM for retrosynthesis prediction models ( + ) Proposed a forward/backward simultaneous ( dual ) training ( -- ) It is unclear what we can learn by rewriting the retrosynthesis models with EBM . ( - ) Reported results do not update the State-of-the-Art prediction accuracy of retrosynthesis models in the literature", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the insightful comments . We have loaded the revised paper to address the comments , and the changes are highlighted in blue . Please see our response : # # # Q1 : Why is the EBM framework beneficial ? What do we get from unifying models into EBM ? Thank you for the question . EBMs provide a way to map high dimensional data ( X , y ) into an unnormalized probability distribution . EBM works on a wide range of problems and demonstrates good performance . The community has accumulated lots of knowledge of how to design energy scores and to train them efficiently . That prior knowledge can generalize to new problems or new model designs . EBM provides the following : * * 1 . Principled ways for training * * . EBMs have many off-the-shelf training methods that are ready to use , including but limited to contrastive divergence ( Hinton , 2002 ) , pseudo-likelihood ( PL ) ( Besag , 1975 ) , conditional composite likelihood ( CL ) ( Lindsay , 1988 ) , score matching ( SM ) ( Hyv\u00a8arinen , 2005 ) , minimum ( diffusion ) Stein kernel discrepancy estimator ( DSKD ) ( Barp et al.,2019 ) , non-local contrastive objectives ( NLCO ) ( Vickrey et al. , 2010 ) , minimum probability flow ( MPF ) ( SohlDickstein et al. , 2011 ) , and noise-contrastive estimation ( NCE ) ( Gutmann and Hyvarinen , 2010 ) . For example , if we design the energy score function as the \u201c bidirectional model \u201d ( eq11 in Sec 3.1.5 ) , with EBM in mind , it is natural to think of using pseudo-likelihood ( PL ) as the training approach . Additionally , there are different ways to design loss see Sec 2.2 of LeCun et al tutorial http : //yann.lecun.com/exdb/publis/pdf/lecun-06.pdf page 6 * * 2 . Better inference * * EBM model \u2019 s inference is flexible . LeCun et al tutorial Page 6 ( inference ) . EBM can not only make inferences such as \u201c classification \u201d or \u201c prediction \u201d , e.g.what is best y ( label ) for X ( input image ) , but also EBM \u2019 s inference can be used to rank possible ( X , y ) -pairs . From the EBM point of view , we can decouple the model , learning ( training ) , and inference ( testing ) . For example , one can use the autoregressive model for modeling and learning , while during inference we can use it as an energy function to get better samples ( via re-ranking as we used , or doing Gibbs-sampling to refine the samples ) . * * 3.Advance understanding of different model designs . * * * Extract commonalities and differences between EBM variants . One example , RetroXpert , GraphRETRO , and G2G share the same energy score format ( eq 14 ) but the differences are the various instantionans of $ p ( X|y , c ) $ . Another example is comparing the energy score design ordered model ( eq3 ) , perturbed model ( eq8 ) , and bidirectional model ( eq11 ) . We see their bidirectional degree : ordered < perturbed < bidirectional The experiments show although bidirectional provides more information to the model , at the same time adds challenges in modeling $ p ( X|y ) $ ( from $ p ( X_i| X_ { \\setminus i } , y ) $ to $ p ( X|y ) $ ) and training . In addition , some variants share the same training procedure ( ordered model and perturbed model both use MLE ) . * Understand the strengths and limitations in model design . Full model versus ordered model . The full model is more flexible that do not impose restrictions on model design , however , the expressional power adds challenges in training , say enumerating all possible molecules . In contrast , the ordered model is restricted to the autoregressive model , but the training is much tractable ( MLE ) * Compare the complexity of learning or inference . See appendix A.7 `` TIME AND SPACE COMPLEXITY ANALYSIS '' # # # Q2 : Not SOTA , compared with RetroXpert ( Yan et al , NeurIPS 2020 ) and GraphRETRO ( Somnath et al , 2020 ) . Please kindly refer to the response to all reviewers with the title `` Reply to R1 , R2 , R3 , R4 : Not SOTA ... \u201d . We have a different setup . These RetroXpert and GraphRETRO are not template free . The lower results are due to that we are not using extra chemical features . If we add those features to the dual model , we get better results than RetroXpert . Dual : 70.1 % ; 73.2 % RetroXpert : 65.6 % ; 70.4 % ( Table 4 in Appendix A.4 of our revised paper ) . # # # Q3 : \u201c I am interested in how the EMB can interpret these latest models. \u201d How to interpret the latest model such as RetroXpert and GraphRETRO into the EBM framework ? Please also see Appendix A.1 and A2 of revised paper . Yes , RetroXpert and GraphRETRO can be integrated into our EBM framework in a way that is similar to G2G ( Sec 3.2.3 in our paper.eq14 ) .The difference between the three is parameterizing $ p ( X|c , y ) $ , which is the step that generates reactants from synthons . Synthons are subgraphs extracted from the products by breaking the bonds in the reaction centers . Reactants are generated by completing the \u201c missing pieces \u201d ( \u201c leaving groups \u201d in GraphRETRO ) in synthon . RetroXpert uses transformers to complete the missing pieces of reactant from synthons ; GraphRETRO uses a predefined list of leaving groups and selects the missing pieces ; G2G uses graph generation ."}, "2": {"review_id": "0Hj3tFCSjUd-2", "review_text": "In this paper , the authors use the framework of energy based models to describe several known approaches for ML-based retrosynthesis in a unified way . This allows to combine retrosynthetic ( backward ) and reaction prediction ( forward ) in a principled way . Based on this analysis , a dual model is proposed which used a duality constraint as a regulariser , leading to improved performance over several baselines models , but not over SOTA ( see below ! ) . The theoretical analysis alone is very interesting , and well described . However , I have a few concerns with missing ablation experiments and the positioning , which could be addressed to make the paper stronger in my opinion . Overall , I think this paper should be accepted at ICLR after the following points have been addressed ( or the authors commit to provide the additional data in the camera ready version if the time of the rebuttal phase is too short to run additional experiments ) . However , in the current form , the paper is not ready . My evaluation is for the current form of the paper , and I am happy to change it significantly during the rebuttal . Ablation Experiments : Is the duality constraint actually needed ? It would be important to perform the ablation experiment where $ \\beta $ is set to 0 . In other words , what happens if you just use $ \\log p ( X|y ) + \\log p ( y|X ) $ to rank the candidates ? If the authors perform these additional experiments and report the numbers , I will increase my score regardless of the outcome of the experiments . Prior work : The state of the art claim is not correct . Yan et al achieve higher performance on the same dataset https : //chemrxiv.org/articles/preprint/Interpretable_Retrosynthesis_Prediction_in_Two_Steps/11869692/2 which has already been published last February ! Therefore , please remove the SOTA claim from the paper , and acknowledge the Yan et al work . For this reviewer well-motivated modelling , honest analysis and proper experimentation is more important than chasing SOTA , and not achieving SOTA will not affect the evaluation negatively . Also , the connection of forward reaction and retrosynthesis prediction via Bayes Theorem has been studied here https : //arxiv.org/abs/2003.03190 which should be acknowledged . Other work preceding has also used a combination of backward and forward prediction . For example , Segler et al Nature 2018 and Coley et al Science 2019 use a model for p ( X|y ) to propose disconnections combined with a model for p ( X , y ) to remove low probability solution . The usefulness of SMILES augmentation has been previously shown , please cite the previous work by Arus Pous et al https : //jcheminf.biomedcentral.com/articles/10.1186/s13321-019-0393-0 The Coley et al 2017a paper that the authors cite in the introduction is a wonderful paper , however , it is not concerned with retrosynthesis . I would suggest to cite the review article by Strieth-Kalthoff et al https : //doi.org/10.1039/C9CS00786E instead , which provides a good overview over ML approaches for ( retro ) synthesis . Furthermore , I would suggest to cite Segler & Waller 2017 in the introduction , which was the first paper to suggest deep neural networks for both retrosynthesis & reaction prediction ( see also https : //doi.org/10.1016/j.ddtec.2020.06.002 ) Coley 2017a should be cited later in the context of forward prediction . Comments : Table 3 is a bit unclear . With Type , do you mean the reaction type is given ? small things : - in eq 13 , the is a low dot , should this have been a centred dot for multiplication ? - Please do n't use Google house fonts for your figures , to maintain anonymity .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the insightful comments and helpful suggestions ! We have loaded a revised paper to address the comments , and the changes are highlighted in blue . Please see our response : * * Q1 : Ablation Experiments on with or without dual constraint . * * Thank you for the invaluable suggestions . Your suggestions have led to precious insights . We have added the following ablation experiments in Appendix A6 of the revised paper : a. Dual model ( with dual constraint ) : ( eq6 ) $ E_ { dual } [ log p ( X ) + log\u2061p ( X|y ) ] + log\u2061p ( y|X ) $ b . Without dual constraint : $ log p ( X ) + log\u2061p ( X|y ) + log\u2061p ( y|X ) $ c. Without prior : $ log\u2061p ( X|y ) + log\u2061p ( y|X ) $ d. Only backward : $ log\u2061p ( X|y ) $ The results in table 5 ( Appendix A6 ) show that every component of the dual loss contributes to the performance positively . For top 1 accuracy : ( a ) dual 67.7 % ; ( b ) 67.0 % ; ( c ) 66.1 % ; ( d ) 60.9 % . Comparing ( a ) and ( b ) is the contribution of dual constraint , which is 0.7 % . It is harder to improve upon a higher accuracy range ( i.e . ( b ) ) than a lower range ( i.e . ( d ) ) . * * Q2 : Please cite relevant papers * * Thank you for the great suggestions . We have cited all the suggested papers in the revised version . Segler Nature 2018 , Segler & Waller 2017 , Strieth-Kalthoff et al 2020 , Johansson et al 2020 , Yan et al 2020 , Somnath et al 2020 , Guo et al 2020 , Coley et al Science 2019 ; Arus-Pous et al 2019 . In our first edition , we have already cited Segler & Waller 2017 ( \u201c NeuralSym \u201d in Table 1 ) , and Segler Nature 2018 ( Introduction ) . Segler Nature 2018 focuses on multiple-step retrosynthesis . We focus on the single-step retrosynthesis , which is a critical building block of the multi-step process . We have included a discussion about Segler & Waller 2017 in the result section . We will include more discussions with other methods in the camera-ready edition . * * Q3 : Table 3 . With Type , do you mean the reaction type is given ? * * Yes * * Q4 : eq13 should be a centered dot for multiplication , not a dot . * * Yes.Thanks.Fixed . * * Q5 : Please change fonts for your figures . * * Apologize . We changed the fonts to \u201c courier \u201d . * * Q6 : Not SOTA , compared with RetroXpert ( Yan et al , NeurIPS 2020 ) and GraphRETRO ( Somnath et al , 2020 ) [ from other reviewers ] . * * Please kindly refer to the response to all reviewers with the title `` Reply to R1 , R2 , R3 , R4 : Not SOTA ... \u201d . We have a different setup . RetroXpert and GraphRETRO are not template free methods . The lower results are due to the fact that we are not using extra chemical features . If we add those features to the dual model , we get better results than RetroXpert . Dual : 70.1 % ; 73.2 % RetroXpert : 65.6 % ; 70.4 % ( Table 4 in Appendix A.4 of our revised paper ) ."}, "3": {"review_id": "0Hj3tFCSjUd-3", "review_text": "SUMMARY This paper uses the statistical physics-inspired energy-based model formalism to study the by now `` canonical '' problem of retrosynthesis using deep learning . The authors use an interesting variant that combines forward and backward prediction . The authors use template-based and template-free models . PROS - This reviewer believes that energy-based models have an elegance and connection to statistical mechanics that should be explored more in the area of machine learning . This work goes in this interesting direction . - Based on the above , and as far as the reviewer is appraised , this is a unique , non-derivative direction in the field and therefore deserving of consideration for acceptance . - The dual model seems to be very useful given the increase in template-based and non-template-based model performance . This could be applied to other transformer-based tasks in chemistry and graph-based ML - The authors compared their models to a variety of SOTA models and approaches , they also were thorough and explored both DeepSMILES and SELFIES . CONS - Some of the mathematical formalism could be moved to supplementary to allow for better discussion . MINOR FORMATTING - The authors may want to give the manuscript a pass for grammar . There are missing articles in a few sentences .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for the helpful suggestions . We thank the reviewer for the positive feedbacks and for highlighting the advantages of our work . We have uploaded the revised paper , and the changes are highlighted in blue . \u201c Moving some mathematical details into supplementary \u201d is a very helpful suggestion , as it can ( 1 ) make the paper more readable , ( 2 ) allow in-depth discussion , and ( 3 ) avoid distracting the readers from unnecessary details . We plan to do so for the camera-ready version . However , currently , we are debating on which parts should be put into the appendix , as different readers seem to be interested in different parts ."}}