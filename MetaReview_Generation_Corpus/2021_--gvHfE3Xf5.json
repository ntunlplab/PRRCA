{"year": "2021", "forum": "--gvHfE3Xf5", "title": "Meta-Learning of Structured Task Distributions in Humans and Machines", "decision": "Accept (Poster)", "meta_review": "The authors present a study where they investigate whether meta-learning techniques leverage the underlying task distribution. To do so, the authors come up with two conditions, in the first they generate tasks using a grammar and in the second condition, which is the null condition essentially, the tasks have the same statistical properties as the compositional task but they are not derived from a simple grammar. The authors find that while humans are better in the compositional condition, models are better in the null condition.\n\nAll reviewers have been positive with this work, but some concerns were raised regarding clarity around the use of some terms, such as compositionality. The rebuttal period has been very productive and the reviewers have acknowledged the improvements on the manuscript. \n\nAll in all, I think this is a good study to appear on ICLR and I believe researchers would benefit from the design of the study that will perhaps open new opportunities around careful evaluation of meta-learning agents.", "reviews": [{"review_id": "--gvHfE3Xf5-0", "review_text": "This paper sets out to determine something about the form of the bias acquired by a standard meta-learning algorithm , and compare the form of that bias to the inherent bias that humans have . The authors point out , rightly , that meta-learning algorithms have meta-biases and it is important to understand these , from both the scientific and engineering perspectives . It is well written and raises good questions . There is a cleverly constructed test domain and a set of well-executed computer and human experiments ( I thinkI do n't really know about how to construct a human experiment . ) Unfortunately , I ca n't end up agreeing or disagreeing with the claims made in the paper , or really understands how well they are supported by evidence , because I find that they use terms that do n't seem to be sufficiently technically well defined . For example : - what exactly is compositional structure ? - what is statistical structure ? - what is your measure of task complexity ? How can we tell if what the agent learns is compositional ? Is that an externally measurable property of the agent 's behavior and the way it generalizes to new environments ? Or is it a property of the internal representation ? ( It is common to have an intuition that `` compositional '' also implies `` compact '' or `` low complexity '' in some sense . ) It feels like generalization be a way to get more clearly at the presence of a compositional representation : could you train on small grids and have the learned agent generalize to big ones ? It seems like if a fixed-size representation can generalize to very large instances , then that is more clear evidence of compositionality ( but then I 'm thinking of compositionality as a property of a representation , not of externally-measurable behavior . ) I also feel that I do n't quite understand the meta-learning training regime . What exactly constituted a `` task '' from the meta-learning perspective ? Is it a single board ? If so , then the meta-learning problem is to learn the task distribution , in some sense . I was expecting something more `` meta '' : that is , to test whether the system is actually meta-learning the * idea * of compositionality , it seems like set-up would be that a task corresponds to a particular grammar with multiple boards drawn from the distribution induced by the grammar ; then we 'd know that it had meta-learned compositionality if it could learn * new grammars * quickly . Smaller points - I did n't completely understand the production rule ( nor the examples ) for the loop structure . - It would help me understand the task set better if there were a slightly more in-depth description of the chains , trees , and loops and described how the grammar generates the compositional tasks in figure 2 . Is it not the case that every connected configuration of red tiles could be described as a tree ? - Rather than showing just one number for the final performance , It would be helpful to show learning curves for the RL algorithms so the reader can assess the stability , convergence , etc . Similarly , learning curves for humans would be interesting , but less important since I assume they just look flat . - `` is develop ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their helpful comments . Below is our point-by-point by response to their comments along with relevant manuscript changes . > This paper sets out to determine something about the form of the bias acquired by a standard meta-learning algorithm , and compare the form of that bias to the inherent bias that humans have . The authors point out , rightly , that meta-learning algorithms have meta-biases and it is important to understand these , from both the scientific and engineering perspectives . It is well written and raises good questions . There is a cleverly constructed test domain and a set of well-executed computer and human experiments ( I thinkI do n't really know about how to construct a human experiment . ) Unfortunately , I ca n't end up agreeing or disagreeing with the claims made in the paper , or really understands how well they are supported by evidence , because I find that they use terms that do n't seem to be sufficiently technically well defined . > For example : > * what exactly is compositional structure ? > * what is statistical structure ? > * what is your measure of task complexity ? > How can we tell if what the agent learns is compositional ? Is that an externally measurable property of the agent 's behavior and the way it generalizes to new environments ? Or is it a property of the internal representation ? ( It is common to have an intuition that `` compositional '' also implies `` compact '' or `` low complexity '' in some sense . ) We acknowledge the lack of clarity of these technical terms and thank the reviewer for an opportunity to clarify . We briefly summarize our clarifications here and refer to relevant changes in the manuscript . We have reframed the paper significantly to focus more broadly on meta-learning as an approach to learning structured forms of reasoning , and present compositionality as one such example of structure . Compositional structure , for our purposes , is the property of a few low-dimensional , simple rules that can be composed together to generate arbitrary complexity ( as in a grammar ) . The intuition that the reviewer has of it being compact and low-complexity is correct . The term statistical structure , for our purposes , refers to the global statistics as reflected in the conditional distributions of red and blue tiles across the whole board . These statistics can emerge from the compositional boards produced from the generative grammar in Figure 1 , but can also be closely matched without the explicit use of compositional rules ( as reflected in our null distribution ) . Our insight is that humans can recognize these abstract structures and thus do better in the compositional task distribution whereas the preferences is flipped for agents , which prefer the use of global statistics and thus do better in the null task distribution . Finally , we do not directly measure task complexity in this work . Although the two task distributions have similar global statistics , it could very well be that one task distribution is simply more complex than the other . However , the core of our results lies in the double dissociation of performance in human vs agents ( humans do better on compositional , agents do better on null ) . In fact a key point in our paper is that \u201c task complexity \u201d is not an objective measure since humans and agents find different tasks difficult , depending on their biases . The following are related changes to the manuscript : \u201c This control distribution allows us to disentangle statistical pattern matching from structured reasoning ( which , in this specific case , is rule-based compositionality ) and highlights the difference between actually learning and utilizing simple abstract structures ( e.g.low-dimensional compositional rules ) versus using the statistical patterns that may be a downstream consequence of those structures . Our method closely approximates the global statistics that emerge from the compositional rules , by using a neural network to learn the conditional distributions and generating Gibbs samples from these conditionals. \u201d p. 8-9 ( discussion )"}, {"review_id": "--gvHfE3Xf5-1", "review_text": "This work is an exploration of model behaviour upon meta-learning tasks with compositional structure . The authors discover that , unlike humans , machine learning models do not readily pick up on the underlying compositional generative structure of a set of tasks , and hence can not match the performance of humans . Conversely , when the task is structured to leverage other statistical patterns , models do well . Taken as a whole , this is a nice piece of work . The presentation is well crafted , and I believe the experiments are well planned . There are many nice analyses and some welcome statistics , such as shown in Figure 3 . The authors are commended for their work . I wish to lay out a few criticisms , and I 'd like the authors to know that the points are all very easily fixable . The authors design a set of structure-learning tasks using a generative grammar . The exact details of the grammar are not given , and the reader is to rely on rough intuitions based on the figures . I encourage the authors to spell out some more details of the methods . The authors argue that the non-compositional boards could not have been generated by the defined grammar , which seems fair , but they also argue that these boards are necessarily non-compositional . I am having a lot of trouble with this statement , because it is not entirely clear by what the authors mean by compositional , as it has n't been clearly defined . This is a particular problem in the machine learning field as a whole , as it pertains to research on compositionality ; rarely is the term defined in any rigorous sense , and from paper to paper there are seemingly different definitions . I encourage the authors to clearly explain what entails compositionality as they refer to it here , and to explain what makes their non-compositional boards non-compositional according to their definition . The `` non-compositional '' boards might not match the generative grammar as used in the compositional setting , but it does not entail that there does not exist a compositional grammar that can produce the non-compositional boards . In fact , due to the discrete , simplified nature of the game , it would seem almost certainly true that there exists * some * generative grammar that can produce the non-compositional boards seen here . It might not be a `` simple '' , `` interesting '' , or human interpretable one , but it would nonetheless be a grammar , and would be compositional . This fact makes it all the more important to define compositionality . It seems to me that what is more precisely being illustrated is the ability for humans to perceive , and infer the implications of abstract , `` simple '' * structures * , and not compositional rules or grammars per se . Without fully defining compositionality and establishing the non-compositionality in the null setting , I 'm afraid that the results are misstated and misrepresented . A note to the authors : I 'm fully aware that I could be misunderstanding how the MLP+Gibbs sampling method here can entail non-compositionality , and am more than open to being corrected on this matter . I look forward to a discussion in the rebuttal . The previous point leads me to a broader point about the background presented throughout . The authors include many broad , and quite bold statements regarding humans , and how they learn , especially in regards to their capacity for `` learning compositionality '' . It is claimed that humans learn rapidly , with very few samples , which is contrasted with machine models that require an enormous amount of data . This is true in some superficial sense , but does not account for the the entire evolutionary trajectory that produced humans ; indeed , humans at birth are not blank slates to the degree that randomly initialized neural networks are . It 's also claimed that humans learn compositional representations . While this idea is certainly in vogue in some circles in cognitive science , it is certainly not widely agreed upon . I 'm not even sure how such a strong statement can be proven . The citations given point to a cople computational modeling papers , which , in my opinion , insufficiently corroborate such a claim . Moreover , it 's not even clear how many non-artificial pieces of data that humans deal with are even truly compositional ( for a taste of the issues and controversy surrounding one particular example language see the following entry : https : //plato.stanford.edu/entries/compositionality/ ) . Please note that this is not at all to say the views presented in this paper are necessarily false . I merely suggest that the authors take another pass at their writing , and tame a few of the broader , bolder claims about the nature of human learning , because it 's not clear that they are necessarily true , either . I believe the authors have missed out on some possible interpretations to their results . They claim that a meta-learned model can not learn the compositional structure of the tasks , since meta-learning is insufficient to establish the inductive biases required for compositional understanding . However , the inductive bias of a model is determined by more than its parameters ( which in this case , are established via meta-learning ) . The functions a model comes to learn are also dependent on the nature of the computations , manifest through the architecture , and other such things . The authors are well aware of this , since they include a condition wherein the model uses convolutions . But the fact that the convolutional model does better entails that there might exist further architectural variants that do even better than it , and potentially , better than humans . If this were the case , then we 'd no longer be able to claim that meta-learning can not establish the proper inductive bias for compositional understanding . Therefore , the existence of a gradient in model performances warrants more careful wording in regards to the claims ; we can not so broadly categorize meta-learning as insufficient for establishing the right inductive biases for compositional understading without caveating according to the other sources of inductive bias . One piece of proof that the humans understand the compositional structure of the task is that they improve over the course of the task . But could this not simply reflect the fact that humans have a better capacity to improve behaviour over short time periods ? In other words , how do we know that the problem with the models does n't have to do with , say , working memory , rather than compositional understanding per se ? Altogether , this is a well put together paper . There is a lot of interesting work here , and the authors have done well to explore various facets of the setup . My main criticisms have to do with the way the work is pitched , and the way the results are interpreted . It is very much presented with a veneer that speaks to a very particular crowd in cognitive-science inspired machine learning community . But since the views in this community are not necessarily broadly shared , many of the statements , interpretations , and claims come across as quite strong and not fully corroborated .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their helpful comments and have made a response for each of their comments along with suggested changes to the paper : > This work is an exploration of model behaviour upon meta-learning tasks with compositional structure . The authors discover that , unlike humans , machine learning models do not readily pick up on the underlying compositional generative structure of a set of tasks , and hence can not match the performance of humans . Conversely , when the task is structured to leverage other statistical patterns , models do well . Taken as a whole , this is a nice piece of work . The presentation is well crafted , and I believe the experiments are well planned . There are many nice analyses and some welcome statistics , such as shown in Figure 3 . The authors are commended for their work . I wish to lay out a few criticisms , and I 'd like the authors to know that the points are all very easily fixable . We thank the reviewer for their nice summary and commendation of our work . > The authors design a set of structure-learning tasks using a generative grammar . The exact details of the grammar are not given , and the reader is to rely on rough intuitions based on the figures . I encourage the authors to spell out some more details of the methods . We agree our initial description of our generative grammar was too vague . We have included a more detailed description in the appendix ."}, {"review_id": "--gvHfE3Xf5-2", "review_text": "Post revision update - The authors have been very helpful and addressed many of my concerns , and I think the revised paper is a substantial improvement . I have rated the paper as a 7 , although I do have some lingering concerns . Most crucially , it 's still not clear to me that the compositional rules the authors highlight are the correct way to characterize the differences in patterns of behavior , since , for example , both models significantly outperform humans at the tree rule . However , the authors do point out that humans perform better at these tasks than the null distribution . Still , I worry that the authors are focusing on the wrong dimension along which the compositional and null task distributions differ . However , I think the fact that the authors followed the suggestion to include the results in the appendix is helpful in this regard , at least future researchers will be able to see the full pattern of results to draw their own conclusions . Original Review This paper proposes to explore whether meta-learning approaches can exploit a compositional structure in their tasks to generalize , and compares this ability to humans . To do so , the paper introduces a grid dataset consisting of generative grammars for generating compositional grids , as well as a null task distribution which is non-compositional but matches on certain low-order statistics . These tasks are interesting and new . They have humans and agents perform a task to reveal rewarding squares on the grid , and compare to agents that meta-learn this task . Human subjects perform better at the compositional distribution , whereas models perform better at the null distribution . They conclude that `` compositional structure remains difficult for these systems and that they prefer other statistical features '' and that this `` highlights the importance of endowing artificial systems with this bias . '' While the topic is timely , and the tasks are interesting , there are a number of limitations to the model and training which I think seriously limit the conclusions . I think that the task is really only compositional for a model which is able to fixate on different locations on the grid . Thus , I recommend rejecting for now , although I think a revision with a more sophisticated model and more thorough discussion could be a valuable contribution . Strengths : * Interactive tasks for humans and models are a great improvement over prior toy datasets on compositionality , e.g.SCAN . * I think the tasks are very interesting , and offer directions that are n't really address by prior compositional generalization datasets that mostly rely on composition of words , or on composition of visual properties like color and shape . * It 's great to see actual comparisons to human performance , and careful thinking about how to evaluate compositional generalization vs. other types ( though this could be developed further , see below ) . Areas for improvement : * Is generalization equally good for humans under each rule type ? If not , this might affect the conclusions . For example , perhaps humans would be very good at inferring chains , but not the more complex structures ( like trees ) . If so , it could be that `` compositionality '' per se is not the construct underlying their performance , but rather `` chains '' or some other , simpler construct . Because performance is not presented broken down by structure type , it is difficult to determine whether some pattern like this could explain the results . Thus , it is difficult to conclude that compositionality is the factor underlying the results . * There are a number of features of the agent that confound the comparison with the humans . These limit the ability to draw a strong conclusion about e.g . `` the importance of endowing artificial systems with [ a compositional ] bias . '' * Why are the main comparisons run using the non-convolutional model , when the convolutional one is clearly more closely matched to humans ( as the discussion acknowledges ) ? It seems like most ( although not all ) the difference in performance is due to spatial bias , rather than compositionality . * Indeed , this spatial bias is partially addressed on the input ( by the convolutional experiments ) , but it is * never * addressed on the output . Humans know that there is a spatial structure to the tiles they are clicking on , the agent can access this information only implicitly . * To address this , one could build a recurrent attention model ( e.g.Mnih et al , 2014 ; Gregor et al , 2015 ) which can make visual saccades around the grid before deciding whether to reveal the square at the current point of fixation , or whether to fixate to another location . This would likely match the human process better , since the humans are likely fixating their gaze on the locations they are considering , rather than fixating in the center of the grid without moving their eyes . It 's also motivated by the observation that agents generalize better if they receive ego-centric input rather than visual input fixed on the grid ( c.f.Hill et al , 2020 ; Ye et al , 2020 ) . This is an important issue to the claims at stake . For an agent which could fixate on each location it was considering , the compositional rules would be much more consistent than for an agent that perceives the whole grid from a fixed perspective . For a fixating agent , the compositional rules would also be much more consistent than the null distribution . In fact , I would suggest that it is * only from the perspective of a model which can fixate that this distribution can be considered compositional at all . * How can we tell that the difference between the humans and the model is n't due to the human ability to fixate , rather than some abstract bias toward compositionality ? * The paper may not be able to address all the ways in which the model 's experience of the task is unlike humans , but then the there should be a * corresponding tempering of the conclusion that the comparison to humans says something specific about the difference between the model and humans . * That is , given the current experiments , the discussion of this paper should focus at least as much on the limitations of the present model as on general conclusions about failures of the model class and the need for additional inductive biases . * Furthermore , exploring compositionality in toy tasks can be misleading . Hill et al . ( 2020 ) show that compositional generalization is significantly improved in more realistic settings ( for example an RL agent that executes actions over time achieves 100 % compositional generalization on a task that a feed-forward classifier only achieves 80 % generalization on ) . They argue that toy stimuli remove one of the most important elements for training deep models \u2014 the rich environments in which humans , also , are trained . Even if the input and output of the model and humans were better matched on this dataset , it may be misleading to conclude something as general as `` the importance of endowing artificial systems with this [ compositional ] bias '' without giving these models training on a distribution of stimuli and tasks that more closely match the rich variety which humans experience over development . Of course , it is not feasible in practice to do so ( yet ) . But this limitation and its relevance to the conclusions should at least be acknowledged in the discussion . * The paper could use some more discussion of the distinction between statistical patterns and compositional rules . This seems like an important point , but it was n't entirely clear to me . Compositional rules correspond to a certain statistical distribution over grids . The fact that up to 2nd order Ising statistics are matched does not mean that the distributions of outcomes are matched `` statistically , '' it merely means they match in certain low-order statistics . It 's not clear if these are the right statistics by which to compare the distributions ( especially for the non-convolutional model , which has no spatial awareness ) . The paper would be strengthened by justifying the choice of these statistics more carefully , and articulating a clear distinction between what counts as a `` statistical '' pattern vs. a rule . References Gregor , Karol , et al . `` Draw : A recurrent neural network for image generation . '' arXiv preprint arXiv:1502.04623 ( 2015 ) . Hill , Felix , et al . `` Environmental drivers of systematicity and generalization in a situated agent . '' International Conference on Learning Representations , 2020 . Mnih , Volodymyr , Nicolas Heess , and Alex Graves . `` Recurrent models of visual attention . '' Advances in neural information processing systems . 2014.Ye , Chang , et al . `` Rotation , Translation , and Cropping for Zero-Shot Generalization . '' arXiv preprint arXiv:2001.09908 ( 2020 ) .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their helpful comments . Below is our point-by-point by response to their comments along with relevant manuscript changes . > Is generalization equally good for humans under each rule type ? If not , this might affect the conclusions . For example , perhaps humans would be very good at inferring chains , but not the more complex structures ( like trees ) . If so , it could be that `` compositionality '' per se is not the construct underlying their performance , but rather `` chains '' or some other , simpler construct . Because performance is not presented broken down by structure type , it is difficult to determine whether some pattern like this could explain the results . Thus , it is difficult to conclude that compositionality is the factor underlying the results . We agree that our framing of human performance being well explained by \u201c compositionality \u201d has not been adequately hashed out . We believe humans are significantly better than standard machine-learning methods at task distributions that require recognizing abstract structure ( e.g.causal graphs , compositional grammars , discrete symbolic objects , etc ) . We argue that even the \u201c simplicity \u201d of a chain is due to the simplicity of the underlying rule that generates it ( to recursively extend in opposite directions ) . As suggested by the reviewer , we do find that humans do better on chains than trees ( p < .05 t-test ) and on loops than trees ( p < .05 t-test ) . The overarching message we convey is that this very notion of simplicity is based on a structured representation -- i.e.representation in the form of simple recursively applied rules . This is as opposed to the null distribution , which has similar global statistics but is not \u201c simple \u201d in a human sense since it is not underlied by simple recursively applied rules . We agree that the term \u201c compositionality \u201d is confusing in this context and have significantly reframed the manuscript to instead focus more broadly on meta learning approaches to structured representations , with compositionality as a special case . > Why are the main comparisons run using the non-convolutional model , when the convolutional one is clearly more closely matched to humans ( as the discussion acknowledges ) ? It seems like most ( although not all ) the difference in performance is due to spatial bias , rather than compositionality . We agree with the reviewer that the convolutional model is more closely matched to humans . The order of models/comparisons done in this work was mostly to incrementally show the value of adding architectural changes to support inductive bias . A key point in our paper is that \u201c good \u201d performance on meta-test does not indicate that the structure embedded in the task distribution have been internalized by the model or that a standard meta-learner is using these . There are many ways to get decent meta-test performance ( in this case , the use of global statistics ) , not just the structure intentionally embedded during training e.g.spatial or compositional structure . Just as adding convolution makes it easier for the agent , we want to highlight the importance of considering architectural changes that give the meta-learner a compositional inductive bias as well . This ordering also allows us to highlight that simply performing \u201c better or worse \u201d than humans is not the key take-home message from our paper , but that humans and agents perform differently . Of two statistically similar distributions , humans find the one that was generated with low-dimensional composable rules easier , while agents without explicit compositional bias find the other one easier . > Indeed , this spatial bias is partially addressed on the input ( by the convolutional experiments ) , but it is never addressed on the output . Humans know that there is a spatial structure to the tiles they are clicking on , the agent can access this information only implicitly . We first would like to note that , even though spatial proximity is not built-in into the action space , the agent does indeed choose actions close to revealed red tiles . An analysis of the actions of the agent shows that the mean Manhattan distance of an agent \u2019 s action and the most recently revealed red tile has a 95 % bootstrap confidence interval ( confidence interval over different agent runs ) is 1.53-1.75 . The fact that this distance is significantly below 2 is a statistic showing the agent \u2019 s tendency to click on tiles close to the currently revealed red tile , showing that the agent learns to take spatially-proximal actions despite this not being directly told which actions are spatially proximal ."}, {"review_id": "--gvHfE3Xf5-3", "review_text": "* Summarize what the paper claims to contribute . Be positive and generous . * This paper offers an interesting comparison between humans and a meta-learning algorithm [ 1,2 ] learning to uncover structured patterns on a 7x7 grid . The patterns are either generated using simple \u201c compositional rules \u201d ( lines , loops or trees ) or sampled from a distribution that has almost identical 0th , 1st and 2nd order statistics as the ones generated through the \u201c compositional rules \u201d . The board is initially covered , and the agent ( either a human or a RL meta-learning algorithm ) is tasked with selecting one by one which tiles are to be uncovered . The aim is to guess which tiles are covered by the pattern ( red tiles ) and avoid the background ( blue tiles ) , thus uncovering the red tiles as accurately as possible . The authors go on to show supporting evidence that most likely , humans are using a compositional inductive bias when trying to uncover the hidden pattern , whereas the meta-learning algorithms do not . This conclusion is arrived at using the relative performance of both humans and the algorithm on the cases where the pattern was generated using the two different schemes . They then go on to conclude that there is a strong difference between the strategy learned by the algorithm , which seems to use mostly the statistics , compared to humans that seem to make use of a compositional inductive bias . [ 1 ] Yan Duan , John Schulman , Xi Chen , Peter L Bartlett , Ilya Sutskever , and Pieter Abbeel . Rl^2 : Fast reinforcement learning via slow reinforcement learning . arXiv preprint arXiv:1611.02779 , 2016 . [ 2 ] Jane X Wang , Zeb Kurth-Nelson , Dhruva Tirumala , Hubert Soyer , Joel Z Leibo , Remi Munos , Charles Blundell , Dharshan Kumaran , and Matt Botvinick . Learning to reinforcement learn . arXiv preprint arXiv:1611.05763 , 2016 . * List strong and weak points of the paper . Be as comprehensive as possible . * * * Strong points * * - The paper is very clear and very well written . I like the simplicity of the experiment and the approach taken by the authors in trying to uncover whether humans and a particular algorithm follow similar strategies when solving tasks . - I see no reason why this approach should not be re-used by other researchers , and I hope that it will inspire others to go to similar lengths when analysing the operation of their algorithms . Specifically , I consider this a strong point of the paper as I believe it offers an obvious and accessible set of future work . * * Weak points * * - It is not completely clear how the statistically similar patterns are constructed . Is it possible to get patterns that are identical to the compositionally generated ones ? If so , have you checked and removed these ? Please elaborate a bit more on this aspect , show more examples of the non-compositional patterns , and describe how you decided on the Compositional-passing/not compositional passing split in Figure 3C . - One somewhat deeper weakness of this work is that one can claim that there is no inherent notion of compositionality in the patterns of the data per-se , but rather in the manner through which they were generated . This is perhaps an underlying reason why the meta-learning algorithm never picks up the compositionality inductive bias exhibited by humans . My point does not mean to invalidate the findings and conclusion , but rather to emphasise that perhaps we shouldn \u2019 t be looking at this type of experiments , as they are \u201c doomed to fail \u201d . Of course this might be obvious in hindsight , but I would be interested to read what the authors think about this . * Clearly state your recommendation ( accept or reject ) with one or two key reasons for this choice . Provide supporting arguments for your recommendation . * I recommend accepting the paper , mainly for the two strong points earlier . I believe what the length at which authors have gone to for understanding what an algorithm does ( or what it doesn \u2019 t do ) should be an example for our field , which often lacks imagination when it comes to analysing proposed models . I do think that a condition for accepting is at least ensuring that the details of all the generated samples ( compositional or not ) are included in the paper . How many ( unique ) of each were generated/used ? How did you decide the compositional-passing/not compositional passing split . Did you check whether any non-compositional pattern matched the compositional one ? If so , how and did you reject that sample ? * Provide additional feedback with the aim to improve the paper . Make it clear that these points are here to help , and not necessarily part of your decision assessment . * - In the intro : \u201c Second , humans represent this learned information compositionally \u201d . In my opinion this is a very strong statement about the nature of the representation in the human brain and it \u2019 s best avoided unless there \u2019 s neurophysiological evidence . Consider rephrasing to a softer version with a reference . - 4th paragraph in the introduction : \u201c Our methodological contribution in this work is * to * develop novel tasks. \u201d - Last paragraph of introduction : \u201c Since a large swath of real-world tasks contain compositional structure .. \u201d Which ones ? Please include some examples . Also , is it that they contain compositional structure ( i.e.in the way they are generated ) or that the compositional inductive bias that humans exhibit can more efficiently solve them ? - Second sentence of 3.2 Results : \u201c We demonstrate that humans have a clear bias toward compositional distributions , without extensive training and even while directly controlling for statistical complexity. \u201d Couldn \u2019 t we claim that the patterns generated by the compositional distributions are easier/simpler ? They are after all only 3 different rules . Wouldn \u2019 t that be a good enough explanation of why humans solve those much more easily than the statistically matched ones ? You did touch upon the issue of spatial proximity later on . Can you touch upon this point too ? I think it \u2019 s a possible ( and perhaps simpler ) alternative explanation of why humans would be better at them . - Appendix A.1 . The hyperparameters are reported in an unnecessary accuracy - I would assume that results are not sensitive to that many significant figures . Please consider 2 or 3 s.f . in scientific notation for simplicity .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for overall positive comments . Below is a point-by-point response with the relevant changes to the manuscript : > It is not completely clear how the statistically similar patterns are constructed . Is it possible to get patterns that are identical to the compositionally generated ones ? If so , have you checked and removed these ? Please elaborate a bit more on this aspect , show more examples of the non-compositional patterns , and describe how you decided on the Compositional-passing/not compositional passing split in Figure 3C . We apologize for the ambiguity in this analysis . It is indeed possible to get some boards identical to the compositionally generated one . While there is overlap due to the overall distributions being statistically similar , the null distribution is not underlied by a small set of simple rules ( as opposed to the compositional distribution ) . The following change to the manuscript describes the compositional-passing/not compositional-passing to greater detail : \u201c We split the null test set by whether or not the board is ` compositional-passing ' and compare human performance across these . To do this , we generated the set of all possible compositional boards on a $ 7\\times 7 $ grid and labeled any null task distribution board as compositional-passing if it happened to be a part of this set. \u201d p. 6 ( Results ) > One somewhat deeper weakness of this work is that one can claim that there is no inherent notion of compositionality in the patterns of the data per-se , but rather in the manner through which they were generated . This is perhaps an underlying reason why the meta-learning algorithm never picks up the compositionality inductive bias exhibited by humans . My point does not mean to invalidate the findings and conclusion , but rather to emphasise that perhaps we shouldn \u2019 t be looking at this type of experiments , as they are \u201c doomed to fail \u201d . Of course this might be obvious in hindsight , but I would be interested to read what the authors think about this . We agree that the manner in which the boards were generated across the task distributions is where the fundamental difference lies . Since the task involves uncovering the red tiles , an understanding of how the boards are generated would be very useful to perform well . Figures 4 and 5B speak to the specific process that humans use to perform this task and it is consistent with having encoded simple generative rules . By contrast , the Gibbs sampling procedure generates null distribution boards based on the global spatial statistics of the compositional boards through a process that does not involve the use of specific simple composable rules . We see differences in human and agent performance on these two > In the intro : \u201c Second , humans represent this learned information compositionally \u201d . In my opinion this is a very strong statement about the nature of the representation in the human brain and it \u2019 s best avoided unless there \u2019 s neurophysiological evidence . Consider rephrasing to a softer version with a reference . We agree and have rephrased this claim . > Second sentence of 3.2 Results : \u201c We demonstrate that humans have a clear bias toward compositional distributions , without extensive training and even while directly controlling for statistical complexity. \u201d Couldn \u2019 t we claim that the patterns generated by the compositional distributions are easier/simpler ? They are after all only 3 different rules . Wouldn \u2019 t that be a good enough explanation of why humans solve those much more easily than the statistically matched ones ? You did touch upon the issue of spatial proximity later on . Can you touch upon this point too ? I think it \u2019 s a possible ( and perhaps simpler ) alternative explanation of why humans would be better at them . The reviewer is absolutely right , and we make changes to the manuscript to stress this point . We believe that it is the fact that humans are biased towards learning low-dimensional rules ( that can be composed to form abstract structures ) that makes these compositional boards \u201c easier \u201d and leads them to do better . This is as opposed to the null distribution that has similar global statistical properties as those that emerge from the composition of low-dimensional rules , but are not explicitly generated using these simple rules . We have emphasized this point in our revised manuscript ."}], "0": {"review_id": "--gvHfE3Xf5-0", "review_text": "This paper sets out to determine something about the form of the bias acquired by a standard meta-learning algorithm , and compare the form of that bias to the inherent bias that humans have . The authors point out , rightly , that meta-learning algorithms have meta-biases and it is important to understand these , from both the scientific and engineering perspectives . It is well written and raises good questions . There is a cleverly constructed test domain and a set of well-executed computer and human experiments ( I thinkI do n't really know about how to construct a human experiment . ) Unfortunately , I ca n't end up agreeing or disagreeing with the claims made in the paper , or really understands how well they are supported by evidence , because I find that they use terms that do n't seem to be sufficiently technically well defined . For example : - what exactly is compositional structure ? - what is statistical structure ? - what is your measure of task complexity ? How can we tell if what the agent learns is compositional ? Is that an externally measurable property of the agent 's behavior and the way it generalizes to new environments ? Or is it a property of the internal representation ? ( It is common to have an intuition that `` compositional '' also implies `` compact '' or `` low complexity '' in some sense . ) It feels like generalization be a way to get more clearly at the presence of a compositional representation : could you train on small grids and have the learned agent generalize to big ones ? It seems like if a fixed-size representation can generalize to very large instances , then that is more clear evidence of compositionality ( but then I 'm thinking of compositionality as a property of a representation , not of externally-measurable behavior . ) I also feel that I do n't quite understand the meta-learning training regime . What exactly constituted a `` task '' from the meta-learning perspective ? Is it a single board ? If so , then the meta-learning problem is to learn the task distribution , in some sense . I was expecting something more `` meta '' : that is , to test whether the system is actually meta-learning the * idea * of compositionality , it seems like set-up would be that a task corresponds to a particular grammar with multiple boards drawn from the distribution induced by the grammar ; then we 'd know that it had meta-learned compositionality if it could learn * new grammars * quickly . Smaller points - I did n't completely understand the production rule ( nor the examples ) for the loop structure . - It would help me understand the task set better if there were a slightly more in-depth description of the chains , trees , and loops and described how the grammar generates the compositional tasks in figure 2 . Is it not the case that every connected configuration of red tiles could be described as a tree ? - Rather than showing just one number for the final performance , It would be helpful to show learning curves for the RL algorithms so the reader can assess the stability , convergence , etc . Similarly , learning curves for humans would be interesting , but less important since I assume they just look flat . - `` is develop ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their helpful comments . Below is our point-by-point by response to their comments along with relevant manuscript changes . > This paper sets out to determine something about the form of the bias acquired by a standard meta-learning algorithm , and compare the form of that bias to the inherent bias that humans have . The authors point out , rightly , that meta-learning algorithms have meta-biases and it is important to understand these , from both the scientific and engineering perspectives . It is well written and raises good questions . There is a cleverly constructed test domain and a set of well-executed computer and human experiments ( I thinkI do n't really know about how to construct a human experiment . ) Unfortunately , I ca n't end up agreeing or disagreeing with the claims made in the paper , or really understands how well they are supported by evidence , because I find that they use terms that do n't seem to be sufficiently technically well defined . > For example : > * what exactly is compositional structure ? > * what is statistical structure ? > * what is your measure of task complexity ? > How can we tell if what the agent learns is compositional ? Is that an externally measurable property of the agent 's behavior and the way it generalizes to new environments ? Or is it a property of the internal representation ? ( It is common to have an intuition that `` compositional '' also implies `` compact '' or `` low complexity '' in some sense . ) We acknowledge the lack of clarity of these technical terms and thank the reviewer for an opportunity to clarify . We briefly summarize our clarifications here and refer to relevant changes in the manuscript . We have reframed the paper significantly to focus more broadly on meta-learning as an approach to learning structured forms of reasoning , and present compositionality as one such example of structure . Compositional structure , for our purposes , is the property of a few low-dimensional , simple rules that can be composed together to generate arbitrary complexity ( as in a grammar ) . The intuition that the reviewer has of it being compact and low-complexity is correct . The term statistical structure , for our purposes , refers to the global statistics as reflected in the conditional distributions of red and blue tiles across the whole board . These statistics can emerge from the compositional boards produced from the generative grammar in Figure 1 , but can also be closely matched without the explicit use of compositional rules ( as reflected in our null distribution ) . Our insight is that humans can recognize these abstract structures and thus do better in the compositional task distribution whereas the preferences is flipped for agents , which prefer the use of global statistics and thus do better in the null task distribution . Finally , we do not directly measure task complexity in this work . Although the two task distributions have similar global statistics , it could very well be that one task distribution is simply more complex than the other . However , the core of our results lies in the double dissociation of performance in human vs agents ( humans do better on compositional , agents do better on null ) . In fact a key point in our paper is that \u201c task complexity \u201d is not an objective measure since humans and agents find different tasks difficult , depending on their biases . The following are related changes to the manuscript : \u201c This control distribution allows us to disentangle statistical pattern matching from structured reasoning ( which , in this specific case , is rule-based compositionality ) and highlights the difference between actually learning and utilizing simple abstract structures ( e.g.low-dimensional compositional rules ) versus using the statistical patterns that may be a downstream consequence of those structures . Our method closely approximates the global statistics that emerge from the compositional rules , by using a neural network to learn the conditional distributions and generating Gibbs samples from these conditionals. \u201d p. 8-9 ( discussion )"}, "1": {"review_id": "--gvHfE3Xf5-1", "review_text": "This work is an exploration of model behaviour upon meta-learning tasks with compositional structure . The authors discover that , unlike humans , machine learning models do not readily pick up on the underlying compositional generative structure of a set of tasks , and hence can not match the performance of humans . Conversely , when the task is structured to leverage other statistical patterns , models do well . Taken as a whole , this is a nice piece of work . The presentation is well crafted , and I believe the experiments are well planned . There are many nice analyses and some welcome statistics , such as shown in Figure 3 . The authors are commended for their work . I wish to lay out a few criticisms , and I 'd like the authors to know that the points are all very easily fixable . The authors design a set of structure-learning tasks using a generative grammar . The exact details of the grammar are not given , and the reader is to rely on rough intuitions based on the figures . I encourage the authors to spell out some more details of the methods . The authors argue that the non-compositional boards could not have been generated by the defined grammar , which seems fair , but they also argue that these boards are necessarily non-compositional . I am having a lot of trouble with this statement , because it is not entirely clear by what the authors mean by compositional , as it has n't been clearly defined . This is a particular problem in the machine learning field as a whole , as it pertains to research on compositionality ; rarely is the term defined in any rigorous sense , and from paper to paper there are seemingly different definitions . I encourage the authors to clearly explain what entails compositionality as they refer to it here , and to explain what makes their non-compositional boards non-compositional according to their definition . The `` non-compositional '' boards might not match the generative grammar as used in the compositional setting , but it does not entail that there does not exist a compositional grammar that can produce the non-compositional boards . In fact , due to the discrete , simplified nature of the game , it would seem almost certainly true that there exists * some * generative grammar that can produce the non-compositional boards seen here . It might not be a `` simple '' , `` interesting '' , or human interpretable one , but it would nonetheless be a grammar , and would be compositional . This fact makes it all the more important to define compositionality . It seems to me that what is more precisely being illustrated is the ability for humans to perceive , and infer the implications of abstract , `` simple '' * structures * , and not compositional rules or grammars per se . Without fully defining compositionality and establishing the non-compositionality in the null setting , I 'm afraid that the results are misstated and misrepresented . A note to the authors : I 'm fully aware that I could be misunderstanding how the MLP+Gibbs sampling method here can entail non-compositionality , and am more than open to being corrected on this matter . I look forward to a discussion in the rebuttal . The previous point leads me to a broader point about the background presented throughout . The authors include many broad , and quite bold statements regarding humans , and how they learn , especially in regards to their capacity for `` learning compositionality '' . It is claimed that humans learn rapidly , with very few samples , which is contrasted with machine models that require an enormous amount of data . This is true in some superficial sense , but does not account for the the entire evolutionary trajectory that produced humans ; indeed , humans at birth are not blank slates to the degree that randomly initialized neural networks are . It 's also claimed that humans learn compositional representations . While this idea is certainly in vogue in some circles in cognitive science , it is certainly not widely agreed upon . I 'm not even sure how such a strong statement can be proven . The citations given point to a cople computational modeling papers , which , in my opinion , insufficiently corroborate such a claim . Moreover , it 's not even clear how many non-artificial pieces of data that humans deal with are even truly compositional ( for a taste of the issues and controversy surrounding one particular example language see the following entry : https : //plato.stanford.edu/entries/compositionality/ ) . Please note that this is not at all to say the views presented in this paper are necessarily false . I merely suggest that the authors take another pass at their writing , and tame a few of the broader , bolder claims about the nature of human learning , because it 's not clear that they are necessarily true , either . I believe the authors have missed out on some possible interpretations to their results . They claim that a meta-learned model can not learn the compositional structure of the tasks , since meta-learning is insufficient to establish the inductive biases required for compositional understanding . However , the inductive bias of a model is determined by more than its parameters ( which in this case , are established via meta-learning ) . The functions a model comes to learn are also dependent on the nature of the computations , manifest through the architecture , and other such things . The authors are well aware of this , since they include a condition wherein the model uses convolutions . But the fact that the convolutional model does better entails that there might exist further architectural variants that do even better than it , and potentially , better than humans . If this were the case , then we 'd no longer be able to claim that meta-learning can not establish the proper inductive bias for compositional understanding . Therefore , the existence of a gradient in model performances warrants more careful wording in regards to the claims ; we can not so broadly categorize meta-learning as insufficient for establishing the right inductive biases for compositional understading without caveating according to the other sources of inductive bias . One piece of proof that the humans understand the compositional structure of the task is that they improve over the course of the task . But could this not simply reflect the fact that humans have a better capacity to improve behaviour over short time periods ? In other words , how do we know that the problem with the models does n't have to do with , say , working memory , rather than compositional understanding per se ? Altogether , this is a well put together paper . There is a lot of interesting work here , and the authors have done well to explore various facets of the setup . My main criticisms have to do with the way the work is pitched , and the way the results are interpreted . It is very much presented with a veneer that speaks to a very particular crowd in cognitive-science inspired machine learning community . But since the views in this community are not necessarily broadly shared , many of the statements , interpretations , and claims come across as quite strong and not fully corroborated .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their helpful comments and have made a response for each of their comments along with suggested changes to the paper : > This work is an exploration of model behaviour upon meta-learning tasks with compositional structure . The authors discover that , unlike humans , machine learning models do not readily pick up on the underlying compositional generative structure of a set of tasks , and hence can not match the performance of humans . Conversely , when the task is structured to leverage other statistical patterns , models do well . Taken as a whole , this is a nice piece of work . The presentation is well crafted , and I believe the experiments are well planned . There are many nice analyses and some welcome statistics , such as shown in Figure 3 . The authors are commended for their work . I wish to lay out a few criticisms , and I 'd like the authors to know that the points are all very easily fixable . We thank the reviewer for their nice summary and commendation of our work . > The authors design a set of structure-learning tasks using a generative grammar . The exact details of the grammar are not given , and the reader is to rely on rough intuitions based on the figures . I encourage the authors to spell out some more details of the methods . We agree our initial description of our generative grammar was too vague . We have included a more detailed description in the appendix ."}, "2": {"review_id": "--gvHfE3Xf5-2", "review_text": "Post revision update - The authors have been very helpful and addressed many of my concerns , and I think the revised paper is a substantial improvement . I have rated the paper as a 7 , although I do have some lingering concerns . Most crucially , it 's still not clear to me that the compositional rules the authors highlight are the correct way to characterize the differences in patterns of behavior , since , for example , both models significantly outperform humans at the tree rule . However , the authors do point out that humans perform better at these tasks than the null distribution . Still , I worry that the authors are focusing on the wrong dimension along which the compositional and null task distributions differ . However , I think the fact that the authors followed the suggestion to include the results in the appendix is helpful in this regard , at least future researchers will be able to see the full pattern of results to draw their own conclusions . Original Review This paper proposes to explore whether meta-learning approaches can exploit a compositional structure in their tasks to generalize , and compares this ability to humans . To do so , the paper introduces a grid dataset consisting of generative grammars for generating compositional grids , as well as a null task distribution which is non-compositional but matches on certain low-order statistics . These tasks are interesting and new . They have humans and agents perform a task to reveal rewarding squares on the grid , and compare to agents that meta-learn this task . Human subjects perform better at the compositional distribution , whereas models perform better at the null distribution . They conclude that `` compositional structure remains difficult for these systems and that they prefer other statistical features '' and that this `` highlights the importance of endowing artificial systems with this bias . '' While the topic is timely , and the tasks are interesting , there are a number of limitations to the model and training which I think seriously limit the conclusions . I think that the task is really only compositional for a model which is able to fixate on different locations on the grid . Thus , I recommend rejecting for now , although I think a revision with a more sophisticated model and more thorough discussion could be a valuable contribution . Strengths : * Interactive tasks for humans and models are a great improvement over prior toy datasets on compositionality , e.g.SCAN . * I think the tasks are very interesting , and offer directions that are n't really address by prior compositional generalization datasets that mostly rely on composition of words , or on composition of visual properties like color and shape . * It 's great to see actual comparisons to human performance , and careful thinking about how to evaluate compositional generalization vs. other types ( though this could be developed further , see below ) . Areas for improvement : * Is generalization equally good for humans under each rule type ? If not , this might affect the conclusions . For example , perhaps humans would be very good at inferring chains , but not the more complex structures ( like trees ) . If so , it could be that `` compositionality '' per se is not the construct underlying their performance , but rather `` chains '' or some other , simpler construct . Because performance is not presented broken down by structure type , it is difficult to determine whether some pattern like this could explain the results . Thus , it is difficult to conclude that compositionality is the factor underlying the results . * There are a number of features of the agent that confound the comparison with the humans . These limit the ability to draw a strong conclusion about e.g . `` the importance of endowing artificial systems with [ a compositional ] bias . '' * Why are the main comparisons run using the non-convolutional model , when the convolutional one is clearly more closely matched to humans ( as the discussion acknowledges ) ? It seems like most ( although not all ) the difference in performance is due to spatial bias , rather than compositionality . * Indeed , this spatial bias is partially addressed on the input ( by the convolutional experiments ) , but it is * never * addressed on the output . Humans know that there is a spatial structure to the tiles they are clicking on , the agent can access this information only implicitly . * To address this , one could build a recurrent attention model ( e.g.Mnih et al , 2014 ; Gregor et al , 2015 ) which can make visual saccades around the grid before deciding whether to reveal the square at the current point of fixation , or whether to fixate to another location . This would likely match the human process better , since the humans are likely fixating their gaze on the locations they are considering , rather than fixating in the center of the grid without moving their eyes . It 's also motivated by the observation that agents generalize better if they receive ego-centric input rather than visual input fixed on the grid ( c.f.Hill et al , 2020 ; Ye et al , 2020 ) . This is an important issue to the claims at stake . For an agent which could fixate on each location it was considering , the compositional rules would be much more consistent than for an agent that perceives the whole grid from a fixed perspective . For a fixating agent , the compositional rules would also be much more consistent than the null distribution . In fact , I would suggest that it is * only from the perspective of a model which can fixate that this distribution can be considered compositional at all . * How can we tell that the difference between the humans and the model is n't due to the human ability to fixate , rather than some abstract bias toward compositionality ? * The paper may not be able to address all the ways in which the model 's experience of the task is unlike humans , but then the there should be a * corresponding tempering of the conclusion that the comparison to humans says something specific about the difference between the model and humans . * That is , given the current experiments , the discussion of this paper should focus at least as much on the limitations of the present model as on general conclusions about failures of the model class and the need for additional inductive biases . * Furthermore , exploring compositionality in toy tasks can be misleading . Hill et al . ( 2020 ) show that compositional generalization is significantly improved in more realistic settings ( for example an RL agent that executes actions over time achieves 100 % compositional generalization on a task that a feed-forward classifier only achieves 80 % generalization on ) . They argue that toy stimuli remove one of the most important elements for training deep models \u2014 the rich environments in which humans , also , are trained . Even if the input and output of the model and humans were better matched on this dataset , it may be misleading to conclude something as general as `` the importance of endowing artificial systems with this [ compositional ] bias '' without giving these models training on a distribution of stimuli and tasks that more closely match the rich variety which humans experience over development . Of course , it is not feasible in practice to do so ( yet ) . But this limitation and its relevance to the conclusions should at least be acknowledged in the discussion . * The paper could use some more discussion of the distinction between statistical patterns and compositional rules . This seems like an important point , but it was n't entirely clear to me . Compositional rules correspond to a certain statistical distribution over grids . The fact that up to 2nd order Ising statistics are matched does not mean that the distributions of outcomes are matched `` statistically , '' it merely means they match in certain low-order statistics . It 's not clear if these are the right statistics by which to compare the distributions ( especially for the non-convolutional model , which has no spatial awareness ) . The paper would be strengthened by justifying the choice of these statistics more carefully , and articulating a clear distinction between what counts as a `` statistical '' pattern vs. a rule . References Gregor , Karol , et al . `` Draw : A recurrent neural network for image generation . '' arXiv preprint arXiv:1502.04623 ( 2015 ) . Hill , Felix , et al . `` Environmental drivers of systematicity and generalization in a situated agent . '' International Conference on Learning Representations , 2020 . Mnih , Volodymyr , Nicolas Heess , and Alex Graves . `` Recurrent models of visual attention . '' Advances in neural information processing systems . 2014.Ye , Chang , et al . `` Rotation , Translation , and Cropping for Zero-Shot Generalization . '' arXiv preprint arXiv:2001.09908 ( 2020 ) .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their helpful comments . Below is our point-by-point by response to their comments along with relevant manuscript changes . > Is generalization equally good for humans under each rule type ? If not , this might affect the conclusions . For example , perhaps humans would be very good at inferring chains , but not the more complex structures ( like trees ) . If so , it could be that `` compositionality '' per se is not the construct underlying their performance , but rather `` chains '' or some other , simpler construct . Because performance is not presented broken down by structure type , it is difficult to determine whether some pattern like this could explain the results . Thus , it is difficult to conclude that compositionality is the factor underlying the results . We agree that our framing of human performance being well explained by \u201c compositionality \u201d has not been adequately hashed out . We believe humans are significantly better than standard machine-learning methods at task distributions that require recognizing abstract structure ( e.g.causal graphs , compositional grammars , discrete symbolic objects , etc ) . We argue that even the \u201c simplicity \u201d of a chain is due to the simplicity of the underlying rule that generates it ( to recursively extend in opposite directions ) . As suggested by the reviewer , we do find that humans do better on chains than trees ( p < .05 t-test ) and on loops than trees ( p < .05 t-test ) . The overarching message we convey is that this very notion of simplicity is based on a structured representation -- i.e.representation in the form of simple recursively applied rules . This is as opposed to the null distribution , which has similar global statistics but is not \u201c simple \u201d in a human sense since it is not underlied by simple recursively applied rules . We agree that the term \u201c compositionality \u201d is confusing in this context and have significantly reframed the manuscript to instead focus more broadly on meta learning approaches to structured representations , with compositionality as a special case . > Why are the main comparisons run using the non-convolutional model , when the convolutional one is clearly more closely matched to humans ( as the discussion acknowledges ) ? It seems like most ( although not all ) the difference in performance is due to spatial bias , rather than compositionality . We agree with the reviewer that the convolutional model is more closely matched to humans . The order of models/comparisons done in this work was mostly to incrementally show the value of adding architectural changes to support inductive bias . A key point in our paper is that \u201c good \u201d performance on meta-test does not indicate that the structure embedded in the task distribution have been internalized by the model or that a standard meta-learner is using these . There are many ways to get decent meta-test performance ( in this case , the use of global statistics ) , not just the structure intentionally embedded during training e.g.spatial or compositional structure . Just as adding convolution makes it easier for the agent , we want to highlight the importance of considering architectural changes that give the meta-learner a compositional inductive bias as well . This ordering also allows us to highlight that simply performing \u201c better or worse \u201d than humans is not the key take-home message from our paper , but that humans and agents perform differently . Of two statistically similar distributions , humans find the one that was generated with low-dimensional composable rules easier , while agents without explicit compositional bias find the other one easier . > Indeed , this spatial bias is partially addressed on the input ( by the convolutional experiments ) , but it is never addressed on the output . Humans know that there is a spatial structure to the tiles they are clicking on , the agent can access this information only implicitly . We first would like to note that , even though spatial proximity is not built-in into the action space , the agent does indeed choose actions close to revealed red tiles . An analysis of the actions of the agent shows that the mean Manhattan distance of an agent \u2019 s action and the most recently revealed red tile has a 95 % bootstrap confidence interval ( confidence interval over different agent runs ) is 1.53-1.75 . The fact that this distance is significantly below 2 is a statistic showing the agent \u2019 s tendency to click on tiles close to the currently revealed red tile , showing that the agent learns to take spatially-proximal actions despite this not being directly told which actions are spatially proximal ."}, "3": {"review_id": "--gvHfE3Xf5-3", "review_text": "* Summarize what the paper claims to contribute . Be positive and generous . * This paper offers an interesting comparison between humans and a meta-learning algorithm [ 1,2 ] learning to uncover structured patterns on a 7x7 grid . The patterns are either generated using simple \u201c compositional rules \u201d ( lines , loops or trees ) or sampled from a distribution that has almost identical 0th , 1st and 2nd order statistics as the ones generated through the \u201c compositional rules \u201d . The board is initially covered , and the agent ( either a human or a RL meta-learning algorithm ) is tasked with selecting one by one which tiles are to be uncovered . The aim is to guess which tiles are covered by the pattern ( red tiles ) and avoid the background ( blue tiles ) , thus uncovering the red tiles as accurately as possible . The authors go on to show supporting evidence that most likely , humans are using a compositional inductive bias when trying to uncover the hidden pattern , whereas the meta-learning algorithms do not . This conclusion is arrived at using the relative performance of both humans and the algorithm on the cases where the pattern was generated using the two different schemes . They then go on to conclude that there is a strong difference between the strategy learned by the algorithm , which seems to use mostly the statistics , compared to humans that seem to make use of a compositional inductive bias . [ 1 ] Yan Duan , John Schulman , Xi Chen , Peter L Bartlett , Ilya Sutskever , and Pieter Abbeel . Rl^2 : Fast reinforcement learning via slow reinforcement learning . arXiv preprint arXiv:1611.02779 , 2016 . [ 2 ] Jane X Wang , Zeb Kurth-Nelson , Dhruva Tirumala , Hubert Soyer , Joel Z Leibo , Remi Munos , Charles Blundell , Dharshan Kumaran , and Matt Botvinick . Learning to reinforcement learn . arXiv preprint arXiv:1611.05763 , 2016 . * List strong and weak points of the paper . Be as comprehensive as possible . * * * Strong points * * - The paper is very clear and very well written . I like the simplicity of the experiment and the approach taken by the authors in trying to uncover whether humans and a particular algorithm follow similar strategies when solving tasks . - I see no reason why this approach should not be re-used by other researchers , and I hope that it will inspire others to go to similar lengths when analysing the operation of their algorithms . Specifically , I consider this a strong point of the paper as I believe it offers an obvious and accessible set of future work . * * Weak points * * - It is not completely clear how the statistically similar patterns are constructed . Is it possible to get patterns that are identical to the compositionally generated ones ? If so , have you checked and removed these ? Please elaborate a bit more on this aspect , show more examples of the non-compositional patterns , and describe how you decided on the Compositional-passing/not compositional passing split in Figure 3C . - One somewhat deeper weakness of this work is that one can claim that there is no inherent notion of compositionality in the patterns of the data per-se , but rather in the manner through which they were generated . This is perhaps an underlying reason why the meta-learning algorithm never picks up the compositionality inductive bias exhibited by humans . My point does not mean to invalidate the findings and conclusion , but rather to emphasise that perhaps we shouldn \u2019 t be looking at this type of experiments , as they are \u201c doomed to fail \u201d . Of course this might be obvious in hindsight , but I would be interested to read what the authors think about this . * Clearly state your recommendation ( accept or reject ) with one or two key reasons for this choice . Provide supporting arguments for your recommendation . * I recommend accepting the paper , mainly for the two strong points earlier . I believe what the length at which authors have gone to for understanding what an algorithm does ( or what it doesn \u2019 t do ) should be an example for our field , which often lacks imagination when it comes to analysing proposed models . I do think that a condition for accepting is at least ensuring that the details of all the generated samples ( compositional or not ) are included in the paper . How many ( unique ) of each were generated/used ? How did you decide the compositional-passing/not compositional passing split . Did you check whether any non-compositional pattern matched the compositional one ? If so , how and did you reject that sample ? * Provide additional feedback with the aim to improve the paper . Make it clear that these points are here to help , and not necessarily part of your decision assessment . * - In the intro : \u201c Second , humans represent this learned information compositionally \u201d . In my opinion this is a very strong statement about the nature of the representation in the human brain and it \u2019 s best avoided unless there \u2019 s neurophysiological evidence . Consider rephrasing to a softer version with a reference . - 4th paragraph in the introduction : \u201c Our methodological contribution in this work is * to * develop novel tasks. \u201d - Last paragraph of introduction : \u201c Since a large swath of real-world tasks contain compositional structure .. \u201d Which ones ? Please include some examples . Also , is it that they contain compositional structure ( i.e.in the way they are generated ) or that the compositional inductive bias that humans exhibit can more efficiently solve them ? - Second sentence of 3.2 Results : \u201c We demonstrate that humans have a clear bias toward compositional distributions , without extensive training and even while directly controlling for statistical complexity. \u201d Couldn \u2019 t we claim that the patterns generated by the compositional distributions are easier/simpler ? They are after all only 3 different rules . Wouldn \u2019 t that be a good enough explanation of why humans solve those much more easily than the statistically matched ones ? You did touch upon the issue of spatial proximity later on . Can you touch upon this point too ? I think it \u2019 s a possible ( and perhaps simpler ) alternative explanation of why humans would be better at them . - Appendix A.1 . The hyperparameters are reported in an unnecessary accuracy - I would assume that results are not sensitive to that many significant figures . Please consider 2 or 3 s.f . in scientific notation for simplicity .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for overall positive comments . Below is a point-by-point response with the relevant changes to the manuscript : > It is not completely clear how the statistically similar patterns are constructed . Is it possible to get patterns that are identical to the compositionally generated ones ? If so , have you checked and removed these ? Please elaborate a bit more on this aspect , show more examples of the non-compositional patterns , and describe how you decided on the Compositional-passing/not compositional passing split in Figure 3C . We apologize for the ambiguity in this analysis . It is indeed possible to get some boards identical to the compositionally generated one . While there is overlap due to the overall distributions being statistically similar , the null distribution is not underlied by a small set of simple rules ( as opposed to the compositional distribution ) . The following change to the manuscript describes the compositional-passing/not compositional-passing to greater detail : \u201c We split the null test set by whether or not the board is ` compositional-passing ' and compare human performance across these . To do this , we generated the set of all possible compositional boards on a $ 7\\times 7 $ grid and labeled any null task distribution board as compositional-passing if it happened to be a part of this set. \u201d p. 6 ( Results ) > One somewhat deeper weakness of this work is that one can claim that there is no inherent notion of compositionality in the patterns of the data per-se , but rather in the manner through which they were generated . This is perhaps an underlying reason why the meta-learning algorithm never picks up the compositionality inductive bias exhibited by humans . My point does not mean to invalidate the findings and conclusion , but rather to emphasise that perhaps we shouldn \u2019 t be looking at this type of experiments , as they are \u201c doomed to fail \u201d . Of course this might be obvious in hindsight , but I would be interested to read what the authors think about this . We agree that the manner in which the boards were generated across the task distributions is where the fundamental difference lies . Since the task involves uncovering the red tiles , an understanding of how the boards are generated would be very useful to perform well . Figures 4 and 5B speak to the specific process that humans use to perform this task and it is consistent with having encoded simple generative rules . By contrast , the Gibbs sampling procedure generates null distribution boards based on the global spatial statistics of the compositional boards through a process that does not involve the use of specific simple composable rules . We see differences in human and agent performance on these two > In the intro : \u201c Second , humans represent this learned information compositionally \u201d . In my opinion this is a very strong statement about the nature of the representation in the human brain and it \u2019 s best avoided unless there \u2019 s neurophysiological evidence . Consider rephrasing to a softer version with a reference . We agree and have rephrased this claim . > Second sentence of 3.2 Results : \u201c We demonstrate that humans have a clear bias toward compositional distributions , without extensive training and even while directly controlling for statistical complexity. \u201d Couldn \u2019 t we claim that the patterns generated by the compositional distributions are easier/simpler ? They are after all only 3 different rules . Wouldn \u2019 t that be a good enough explanation of why humans solve those much more easily than the statistically matched ones ? You did touch upon the issue of spatial proximity later on . Can you touch upon this point too ? I think it \u2019 s a possible ( and perhaps simpler ) alternative explanation of why humans would be better at them . The reviewer is absolutely right , and we make changes to the manuscript to stress this point . We believe that it is the fact that humans are biased towards learning low-dimensional rules ( that can be composed to form abstract structures ) that makes these compositional boards \u201c easier \u201d and leads them to do better . This is as opposed to the null distribution that has similar global statistical properties as those that emerge from the composition of low-dimensional rules , but are not explicitly generated using these simple rules . We have emphasized this point in our revised manuscript ."}}