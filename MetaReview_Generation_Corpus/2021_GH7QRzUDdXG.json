{"year": "2021", "forum": "GH7QRzUDdXG", "title": "A Geometric Analysis of Deep Generative Image Models and Its Applications", "decision": "Accept (Poster)", "meta_review": "This paper presents several analyses on the geometry of GAN generators through the lens of Riemannian geometry: showing interpretability of the leading eigenvectors of the Hessian, homogeneity of the space, and more efficient latent-space inference through preconditioning. Reviewers found the (revised) paper well-written and clear, with a thorough set of experiments to support their main claims. While there were several concerns around the generality of the approach, the authors performed several experiments in the rebuttal period to address many of the reviewer\u2019s concerns (robustness of findings with different image distance functions, inversion on additional GANs and datasets, user study of perceptual properties of axes, and comparison to previous methods for intepretable axes discovery). I found these experiments extensive and convincing, supporting the claims around robustness of the approach to different image distance metrics, GAN architectures, and interpretability of the axes. \n\nThere were also strong concerns around similarity with recent work (Chiu et al., SIGGRAPH 2020 and Peebles et al., ECCV 2020), but both of these papers were published at most 1 month before the ICLR submission deadline, and thus should be considered as concurrent work. \n\nGiven the strong set of additional experiments and interesting empirical observations, I recommend accepting this paper.\n\nThere remain concerts around the extent to which the findings \u201cunify\u201d previous approaches on interpretable axes, and we encourage the authors to update the paper before the camera ready to address these and additional reviewer concerns (especially expanding the discussion of the relationship with concurrent work in Chiu et al. and Peebles et al.).", "reviews": [{"review_id": "GH7QRzUDdXG-0", "review_text": "_Summary_ : This work intends to explore the geometry of the latent space and proposes to define the distance in latent space as the distance between the corresponding generated images and use the Hessian of that squared distance as metric tensor to define the manifold . Using Learned Perceptual Image Patch Similarity ( LPIPS ) , they show that the Hessian can either be computed through backpropagation or if that is not efficient , it is sufficient to iteratively compute the eigenvectors corresponding to the largest eigenvalues . With the proposed method , the empirical observations showed 1 ) the impact of those eigenvectors through examples , 2 ) consistent geometric local changes over different positions in the latent space , and 3 ) the impact of top eigenvectors on particular layers . Further , the authors discuss three areas of possible application ( gradient-based GAN inversion , gradient-free image search , interpretable axes discovery ) . _Strengths_ : - The paper addresses an interesting topic ( understanding the latent space of GANs ) and proposes a straightforward method . - The empirical observations demonstrates the advantages of using an image similarity metric as latent space distance and the correlation between eigenvalues corresponding to the largest eigenvalues and image perception . _Weaknesses_ : - The method is depending heavily on the distance metric employed , however , does not discuss in what way this could influence the outcome . In this paper , only LPIPS was considered as metric . What are the advantages and disadvantages ? How would the analysis and outcome change when other distance metrics are used ? These are questions that might benefit the work to discuss . - The actual method was barely discussed in the paper , but rather moved to the appendix . I think the authors may want to restructure the paper to include how to compute the eigenvectors are computed . - The actual application of the proposed method was rather discussed than actually performed . I do think this is a very interesting proposal , however , with only showing empirical observations , I believe the scope of this paper is too little . It would have been better if the authors picked one of the applications and applied their method . For more details see below . _Overall assessment_ : Overall , I find the proposed method very interesting and the empirical observations compelling . However , I find the scope of the paper not sufficient as it would have been nice to see that one of the application with the usage of the Hessian metric as distance function would have worked . _Detailed questions and comments_ : - Learned Perceptual Image Patch Similarity ( LLIPS , Section 3 `` Numerical Method '' ) : As this is the main distance metric being used , would it be possible to briefly introduce and define it in the paper ? - Requirement for understanding the latent space ( abstract ) : The abstract mentions inversion and interpretability as requirement for understanding the latent space . It was not clear to me how the method presented is addressing each of the requirement . Further , the abstract also claims `` This geometric understanding unifies previous results of GAN inversion and interpretation . `` . Can you clarify these claims ? - Appendix A.2 Methods for computing the hessian : Parts of how the Hessian is computed should be in the main paper as this is the main method for this paper . The explanation in Section 3 `` Numerical Method '' was superficial and I could have not understood the methodology without reading the the appendix . - Spectrum Structure of GANs ( Section 4 , Figure 2 ) : Can you clarify what has been exactly used to plot Figure 2 ? In the paragraph , Figure 2 was plotted using the mean and 90 % confidence interval , Figure 2 y-axis label says $ \\log ( eig/eigmax ) $ . - Figure 3D : The figure is hard to read , with many data points overlapping each other . I am also confused what the two lines crossing each data point represents . - `` Then we explored linearly the latent space along the eigenvector '' ( Section 4 ) : Why is the exploration linearly ? Does this conform with the manifold ? How is $ \\mu_i $ defined ? The footnote also says that spherical linear exploration is used some spaces . Can you elaborate more on how you performed exploration ? - Top Eigenvectors Capture Perceptual Relevant Changes : Would it be able to quantify this ? As only four samples were shown , how would we know that this generalizes for all samples ? - [ 1 ] - [ 4 ] might be also relevant to the topic of latent space exploration . They are not necessarily w.r.t.to manifold learning , however , with respect to applications in Section 6 , they look very close . _Post-rebuttal_ : I do highly appreciate the authors trying to answer all our questions and adding more details and experiments . However , after also reading through [ Chiu et al. , SIGGRAPH 2020 ] I do find that this paper has a large overlap with the one mentioned . Therefore , agreeing with Reviewer # 2 , the contribution is reduced to applying the idea to GANs . Therefore I am keeping my recommendation . [ 1 ] Lipton , Z.C . and Tripathi , S. , 2017 . Precise Recovery of Latent Vectors from Generative Adversarial Networks . ICLR 2017 workshop . [ 2 ] Albright , M. and McCloskey , S. , 2019 , May . Source Generator Attribution via Inversion . In CVPR Workshops ( Vol.7 ) . [ 3 ] Webster , R. , Rabin , J. , Simon , L. and Jurie , F. , 2019 . Detecting overfitting of deep generative networks via latent recovery . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( pp.11273-11282 ) . [ 4 ] Bojanowski , P. , Joulin , A. , Lopez-Pas , D. and Szlam , A. , 2018 , July . Optimizing the Latent Space of Generative Networks . In International Conference on Machine Learning ( pp.600-609 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank our reviewers for their time in considering our work . These insights have improved our paper considerably . We respond to each point below . > The method is depending heavily on the distance metric employed , however , does not discuss in what way this could influence the outcome . What are the advantages and disadvantages ? How would the analysis and outcome change when other distance metrics are used ? These are questions that might benefit the work to discuss . The reviewer is correct that our work used LPIPS distance as the primary image dissimilarity metric , and thus , we set out to determine if our results were entirely dependent on this choice . We performed a set of additional experiments computing the metric tensor at the same hidden vector using different image distance functions , including the mean squared error in pixel space ( MSE ) and structural similarity index measure ( SSIM ) . We computed the Hessian using each of these three measures ( MSE , SSIM , and LPIPS ) at 100 random sampled vectors in BigGAN , Progressive Growing GAN ( Face ) , StyleGAN2 ( FFHQ 256 resolution ) . We then compared the Hessians obtained with each image distance function . We found that the element-wise Pearson correlation of the Hessian matrices ranged between 0.94-0.99 , the correlation of eigenvalue spectra ranged between 0.987-0.995 . Using our derived statistics , we also measured the Hessian similarity $ C^ { Hlog } $ and $ C^ { Hlin } $ and found this resulted in a similarly high correlation ( ~0.99 ) . Thus we can confirm that the estimated Hessian matrix and its spectra are not strongly dependent on the chosen image distance metric , and their effect on the eigenvector of each other is correlated . We have added the table quantifying this result to the appendix . This result can be interpreted in the context of Section 5 of the manuscript . As the equation $ H ( z ) =J_ { \\varphi\\circ G } ^TJ_ { \\varphi\\circ G } $ there showed , the Riemannian metric of the GAN is the inner product matrix of the Jacobian of the generator composing the representation map for the distance metric $ J_ { \\varphi\\circ G } $ . This Jacobian is the composition of a chain of Jacobian of each layer , and the effect of the image difference metric is to add a few more terms to the top of the chain of Jacobians . In this regard , the Jacobian terms from the layers of the GAN seem to have a dominating effect compared to the few final terms coming from the image dissimilarity metric . Another explanation is that the 3 metric functions are in consensus for the geometry on the GAN manifold , but they may not agree with each other for * * out of manifold transforms * * . For example , as the demo in [ LPIPS ] ( https : //github.com/richzhang/PerceptualSimilarity ) paper showed , for some transforms like image blur or color distortion , MSE , SSIM and LPIPS can yield different answers about which image is more similar . But image blur and color distortion are artifacts that are easily discriminated against , so I believe generators will be trained not to generate blur and color distortion . In this regard , these transforms are out of manifold transforms . Thus , even if LPIPS , SSIM , and MSE are not always the same , they could provide a similar measure for the `` on manifold transforms '' encoded by GAN . > The actual method was barely discussed in the paper , but rather moved to the appendix . I think the authors may want to restructure the paper to include how to compute the eigenvectors are computed . We appreciate the reviewer 's effort to dig into the appendix and understand our method . We agree with the reviewer : although originally we were compelled to move the detailed version of the numerical method to the Appendix due to page limits , now we have expanded the method part in the main text to make it more intuitive . > The actual application of the proposed method was rather discussed than actually performed . I do think this is a very interesting proposal , however , with only showing empirical observations , I believe the scope of this paper is too little . It would have been better if the authors picked one of the applications and applied their method . This is a good recommendation . In this initial study , our goal was to emphasize the multiplicity of applications that this observation can advance . To clarify , we performed experiments in Section 6 and showed that using Hessian can actually assist optimization in GAN space with or without gradient . In this revision , figure 5 is expanded to illustrate this result . However , we take the point that it would be desirable to really focus on one of the applications , and we are pursuing similar efforts . For this manuscript revision , we have chosen to emphasize GAN inversion : we spend more effort to show that the Hessian preconditioning technique will improve GAN inversion for multiple GANs ( StyleGAN face , PGGAN ) for in distribution and out of distribution samples , using multi-start Adam optimizer . Hopefully , this can expand the scope of the paper ."}, {"review_id": "GH7QRzUDdXG-1", "review_text": "The paper performs the analysis of the GAN latent spaces from the geometric perspective , inducing a metric tensor in the latent space from the LPIPS distance in the image space . The main authors ' finding is that under such metric , the latent spaces of typical GANs are highly anisotropic , which can be exploited for more effective GAN inversion . Furthermore , the authors show that eigen vectors of the metric tensor often correspond to interpretable latent transformations . Pros : 1 ) The paper is exceptionally well-written and provides a very interesting read . While the performed analysis is simple and natural , it does reveal several interesting findings about typical latent spaces : LPIPS-anisotropy , global consistency of the metric tensor . 2 ) The authors confirm the usefulness of their analysis by providing immediate practical benefits : more effective GAN inversion , which accounts for the latent space anisotropy . Cons : 1 ) Missing work on interpretable GAN directions : [ A ] The Hessian Penalty : A Weak Prior for Unsupervised Disentanglement , ECCV 2020 2 ) In my opinion , the authors do not provide enough support for their claim `` This finding unifies previous unsupervised methods that discover interpretable axes in the GAN space '' . - While the proposed method does seem to generalize both Ha \u0308rko \u0308nen et al. , 2020 and Shen & Zhou , 2020 , I do not see , how it captures Voynov & Babenko , 2020 and Pebbles et al ( see above [ A ] ) . Furthermore , I believe that such claims should be supported by the experiments . Could the authors experimentally confirm that their method results in the same set of directions as the existing methods ? Overall , I am positive about this submission , since the main analysis is both interesting and practically useful . My main criticism is that in terms of discovery of interpretable directions , the methods should be experimentally compared to existing alternatives . If it does provide a super-set of directions , obtained by existing methods , this would make the submission much stronger . Otherwise , the claim about unification should be toned down in my opinion . == AFTER REBUTTAL == I appreciate the authors ' efforts on additional thorough comparison to existing works on interpretable axes discovery . From the updated manuscript , however , it is not clear what method is superior and the authors ' approach appears to be a yet another method for this task rather than generalization of previous ones . Overall , I am still on the positive side since the observed findings deliver a clear profit for GAN inversion . But I am not increasing my score given that the `` interpretable axes '' part has become less impressive ( in terms of weaker claims and conclusions ) and the competing SIGGRAPH work .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> * Missing work on interpretable GAN directions : * * [ A ] The Hessian Penalty : A Weak Prior for Unsupervised Disentanglement , ECCV 2020 * Thank you for bringing this work to our attention . We have now included it as a reference and as a launching pad for our investigations . > * In my opinion , the authors do not provide enough support for their claim `` This finding unifies previous unsupervised methods that discover interpretable axes in the GAN space '' . * * While the proposed method does seem to generalize both Ha \u0308rko \u0308nen et al. , 2020 and Shen & Zhou , 2020 , I do not see , how it captures Voynov & Babenko , 2020 and Pebbles et al ( see above [ A ] ) . Furthermore , I believe that such claims should be supported by the experiments . Could the authors experimentally confirm that their method results in the same set of directions as the existing methods ? * > > * Overall , I am positive about this submission , since the main analysis is both interesting and practically useful . My main criticism is that in terms of discovery of interpretable directions , the methods should be experimentally compared to existing alternatives . If it does provide a super-set of directions , obtained by existing methods , this would make the submission much stronger . Otherwise , the claim about unification should be toned down in my opinion . * We appreciate these kind comments ! We agree that our submitted draft needed some clarification regarding the claim that our results unified previous unsupervised methods . To address this , we performed additional comparison with previous work on this topic , and have added those to the appendix . Specifically , we acknowledge that the * Voynov & Babenko , 2020 * result may not be deducible from our result . To clarify the relationship , we performed an additional comparison of the axes they discovered and the Hessian eigenvectors we computed on those GANs . In the PGGAN , we found that the 6 axes they annotated have a larger alignment with our top eigenspace , as the $ vHv $ for their axes are significantly larger than those of unit random vectors ( $ p < 0.01 $ for all but one axes $ p < 0.05 $ ) . Further , we projected their axes onto the eigenvectors of Hessian and analyzed how the power ( square of projection coefficient ) is distributed on the spectra . Further , to investigate the alignment of their axes and single eigenvector , we project their axes onto the basis formed by Hessian eigenvectors and compute the power ( squared projection coefficient ) . We computed the histogram of power in different parts of spectra and found that the entropy of this distribution is significantly lower for their axes than for unit random vectors , ( $ p < 0.01 $ for all but one axes ) . In this regard , their method also discovers that the top eigenspace of PGGAN contains many informative transforms , and that alignment with dimensions of similar eigenvalues is better than mixing all the dimensions up . Note that , in their work , they chose to enforce orthonormality on the discovered axes for BigGAN noise space and StyleGAN FFHQ1024 , which was not a constraint for the axes in PGGAN . In those two models , maybe due to orthogonality their axes do not have a uniform relationship with the Hessian structure of GAN . For example , in the BigGAN noise space , 3 out of 6 annotated axes have a significantly lower $ vHv $ value ( p < 0.001 ) comparing to random vectors , namely these axes avoid the top eigenspace . Further , the same 3 out of 6 annotated axes have a significantly lower entropy in the top 15 dimension eigenspace ( $ p < 0.05 $ ) which suggests that although these axes avoid the top eigenspace , they are trained to concentrate power more on one of the top eigenvectors instead of mixing them randomly . This initial comparison has already provided some unexpected insights about the nature of interpretable axes . Further development is needed to draw a clear connection between the Hessian structure and the axes defined by in the previous works . This comparison encourages us to qualify our statements about unifying previous results , especially for Voynov Babenko 2020 ."}, {"review_id": "GH7QRzUDdXG-2", "review_text": "This paper proposes a method for finding the axes of largest variation in the latent space of a generative model . This can be leveraged for better generator inversion and explainability . Several experiments are done to evaluate the latent vectors used by the method quantitatively and qualitatively . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : The analyses are interesting and the method seems novel . This method could prove useful in analysis of latent space properties . Its full significance and practicality could be clearer , though . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : - The problem of understanding the latent space structure better is relevant and timely in the research on generative models equipped with a continuous latent space . - Experiments seem to demonstrate successful dimensional truncation , GAN inversion , and that the eigenvectors behave consistently # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : - I am not sure I see whether each experiment actually supports the corresponding claim . Especially , the claim that interpretable axes are found seems central but it is not much supported in the experiments . It seems mostly represented in Fig.9 in the Appendix , but I do not fully understand how we are supposed interpret the result in this figure . - Fig 2 is hard to parse and clearly too tightly packed . Something should be done about it . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : - It is easy to get lost in the math and in the various measurements . Could the authors summarize , in practise and plain English , how their method should be used to find the most intepretable axes of variation for a new generative model , and how to confirm and measure that we have indeed found them ? Typo : on page 7 around `` ( Fig 5 . ) '' # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Update after rebuttal discussions : - In the light of the considerable overlap with [ Chiu et al. , 2020 ] pointed out by the other reviewers , I decreased my score . I have familiarized myself with it and can confirm the said overlap . However , given remaining differences , I do not find it unreasonable to consider this paper as `` complementary '' to [ Chiu et al. , 2020 ] , * provided that the authors explicitly address the similarities in the final version * . - I consider the sum total of contributions of the paper still tilting towards being sufficient for publication .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate the helpful and constructive reviews that the reviewers have made ! We respond to each of the points below . > * I am not sure I see whether each experiment actually supports the corresponding claim . Especially , the claim that interpretable axes are found seems central but it is not much supported in the experiments . It seems mostly represented in Fig.9 in the Appendix , but I do not fully understand how we are supposed interpret the result in this figure . * We thank the reviewer for this insight . We agree that in our initial draft , we were overreaching in our description of interpretability . In the two weeks since receiving this comment , we set out to formally investigate whether this method reveals axes that are perceptually relevant to individuals other than the authors . Using Amazon \u2019 s Mechanical Turk , we generated images under the identified Hessian directions of four different GANs ( Progressive Growing GAN , BigGAN noise space , StyleGAN-Cat and StyleGAN-Face ) , and presented them to 175 participants , to investigate if the images were interpretable . We operationalized the concept of \u201c interpretable \u201d as follows : in each trial , we randomly sampled five reference images generated by a given GAN , and perturbed them ( linearly or spherically ) along the Hessian axis to be tested , which created five image sequences . These five sequences were shown on the same screen . The subjects viewed all the five sequences , and were asked if they could perceive a change , and if so , to describe the transformation that was common to the majority of the sequences . They were also asked to indicate how many sequences shared the identified transformation . Finally , subjects reported how large the perceived image change was ( 0 % -100 % ) , and the similarity of the image transformations across the five sequences ( score of 1-9.9 most similar ) and the difficulty in interpreting the common change ( InterpretDifficulty score on the scale of 1-9 , 9 most difficult ) . Image perturbations comprised the top 10 eigenvectors , 10 random vectors orthogonal to these eigenvectors , and the bottom 10 eigenvectors in four GANs . Further , we added in photos of real objects as reference stimuli , which were manipulated by well-defined transformations such as rotation , perspective change , eye color , and more . Overall , subjects reported that the image sequences generated by top eigenvectors showed a larger amplitude of image change than both a ) orthogonal directions ( * t * = 3.4 , * P * = 7.0 x 10^-4 ) and b ) bottom eigenvectors ( * t * = 6.4 , * P * = 2.1 x 10^-10 ) ; notably , this was true even though we picked a much smaller step size in the top eigenspace than the orthogonal and bottom eigenspace . On average , the top 10 eigenvectors had a higher perceptual consistency score than a ) the orthogonal-space random vectors ( * t = * 2.8 * , P = * 5.8 x 10-3 ) and b ) the bottom eigenvectors ( * t = * 4.4 * , P = * 1.3 x 10-5 ) . The subjects report that the top eigenvectors are easier to interpret than the bottom eigenvectors when they observe image change in the bottom eigenvectors . ( * t = * -4.6 * , P = 3.8 * x 10-6 ) . Thus , we have found stronger evidence for our initial report that these Hessian eigenvectors \u201c capture perceptually relevant changes. \u201d These results are now featured in Section 3 . > * Fig 2 is hard to parse and clearly too tightly packed . Something should be done about it . * To improve this image , first , we lowered the density by enlarging the canvas of the figure ; we also removed 1-2 GAN curves from each subplot , to improve legend readability ."}, {"review_id": "GH7QRzUDdXG-3", "review_text": "Summary : The paper proposes an analysis way of a latent space of GAN in a GAN architecture agnostic way . They use the Riemannian manifold analysis to investigate image manifold , which leads to a simple algorithm with eigen-decomposition of the Hessian matrix of a local point . Unfortunately , many ideas and some of the findings ( Hessian-based GAN exploration and anisotropy of Hessian in the latent space ) are already well explored in [ C1 ] with much higher quality and various modality applications . But still , there are interesting ideas contained in this paper : 1 ) the latent space is homogeneous ( in a sense that the major directions obtained by a latent position are shared at different positions with similar semantic meanings ) , and 2 ) extensive eigenvalue analyses of Hessian . Based on the findings , they show interesting applications of efficient GAN Inversion , gradient-free search in Image space , and unsupervised discovery of interpretable axes . However , these applications are also already explored by [ C1 ] in a visually pleasing way . [ C1 ] Human-in-the-Loop Differential Subspace Search in High-Dimensional Latent Space , SIGGRAPH 2020 . Reasons for score : I like the idea that leverages the findings of the homogeneous property of Hessian and improves the efficiency of GAN inversion with Hessian preconditioning . Also , I appreciate the analysis of eigenvalues and correlations . However , unfortunately , most of the contributions the authors argued are largely overlapped with [ C1 ] , of which citation is missed by the authors . Thus , with these remaining contributions , this reviewer could not vote for the acceptance of this work at this point . This reviewer would like to ask the significant different contribution from [ C1 ] that this reviewer may miss . Pros : - The paper shows interesting interpretation and ideas , and its applications . - The paper is written very clearly and readily . - The authors validate the ideas with enough effort in the experiments . Cons : - The GAN architecture agnostic latent space exploration with Riemannian interpretation is already done in [ C1 ] in a more general way . Detail comments : - While [ C1 ] presents mainly about the SVD of Jacobian ( but they also show Hessian interpretation ( metric tensor ) in Sec.4.1 as well ) , it is essentially the same because of Eq . ( 3 ) in this paper . Also , [ C1 ] suggested much more diverse exploration methods with diverse search methods . Thus , this reviewer would like to listen to the authors ' responses about this overlap .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank our reviewer for the kind and constructive feedback . We were delighted to read this very recent Chiu et al.SIGGRAPH paper and were glad to find some overlap with our work , which we developed independently and in parallel . This overlap suggests an independent confirmation of the validity of the approach , which is exciting , and we have amended our manuscript to credit their important work . As pointed out by our reviewer , the SIGGRAPH numerical method bears some similarity to ours ; the authors also applied their method to a problem reminiscent of those in our study ( i.e.optimizing the black-box function of human perceptual judgment ) , and their Figures 3 and 4 also showed different rates of image change , proving the robustness of the geometric structure . However , there are strong differences between our studies : first , we provide an explicit , analytical link between the Jacobian of the generator to the Riemannian metric ; second , we provide a numerical method that is faster and more accurate in approximating the full metric tensor , suitable to analyze the geometry of the latent space ; third , we provide insights to the geometry through the homogeneity of the metric tensor . We elaborate on these points below . 1.The SIGGRAPH paper \u2019 s mathematical framework for finding the most informative axes bears some similarity to ours . However , this article had no equation explicitly describing the connection between the Riemannian metric tensor and the Jacobian . In our reviewer \u2019 s example of section 4.1 , Chiu et al.implied a Hessian interpretation of their approach , where the authors created a synthetic function using a quadratic form with an anisotropic spectrum to test their optimization method ; but they did not elaborate on this interpretation for general generative models . Our study presents this analytical connection explicitly \u2014 without our work , readers might not recognize this relationship and understand the geometric structure fully . We further went on to comprehensively measure the geometric properties of multiple state-of-the-art generative models , namely , we measured the local geometry of GAN latent spaces and their globalization . 2.In terms of the numerical method and efficiency , our approach differs substantially from that of Chiu et . al.Their numerical method computes the Jacobian matrix of the generator instead of the Hessian matrices of the squared distance . As recognized by the reviewer , if we have used L2 distance as the image distance function , then the inner product matrix of the Jacobian $ H=J^TJ $ results in the Hessian . In the Jacobian matrix ( $ m $ -by- $ n $ , sample dimension $ m $ , hidden dimension $ n $ ) , $ m $ is usually much larger than $ n $ ( e.g.for a standard image generative network $ n $ is ~200-500 , and $ m $ ranges from ~20,000-3,000,000 ) , thus it is highly inefficient to compute a full Jacobian matrix as its complexity is O ( m * single backprop time ) ; in contrast , the complexity of our method is O ( n * double backprop time ) using common backpropagation . For example , for Progressive Growing GAN at 256 pixel-level , our method could obtain the full Hessian and its spectrum in 58 seconds , while the alternative method needs more than 4800 sec to obtain the spectrum of the full Jacobian . Thus , in their application , the authors approximate the Jacobian by randomly sampling output coordinates in sample space , and perform singular value decomposition ( SVD ) in the restrained space . This is equivalent to sampling rows of the Jacobian matrix to estimate its spectrum . In our approximation method , we perform the Hessian decomposition while doing backprop using Lanczos iteration , thus finding the largest eigenvalues and eigenvectors ( equivalent to their largest right singular vectors ) . We conducted a numerical experiment to prove this . We showed that our numerical method approximates the real Hessian matrix faster and more accurately . Taking the Progressive Growing GAN with latent dimension n=512 as an example , using Lanczos iteration with finite-differencing method ( ForwardIter ) , we can find the top 50 eigenvectors of the Hessian , approximating the full matrix to an accuracy of 0.99999 ( per element-wise Pearson correlation ) in 7.9 secs , while the alternative method , sampling 500 output dimensions , can only achieve 0.92420 correlation to the real Hessian in 12.5 secs . Thus , in terms of algorithmic efficiency , our method can compute the Riemannian metric tensor and the directions that are most informative faster , because the Lanczos iteration is more sample-efficient than a random sampling method ( for more specific information , we will add this comparison to our Appendix ) . Although the alternative is a praiseworthy way to identify promising axes to explore the latent space of GANs , as the null space will not affect the exploration process in their application , our study provides a more efficient tool to interrogate the geometry of the GAN space ."}], "0": {"review_id": "GH7QRzUDdXG-0", "review_text": "_Summary_ : This work intends to explore the geometry of the latent space and proposes to define the distance in latent space as the distance between the corresponding generated images and use the Hessian of that squared distance as metric tensor to define the manifold . Using Learned Perceptual Image Patch Similarity ( LPIPS ) , they show that the Hessian can either be computed through backpropagation or if that is not efficient , it is sufficient to iteratively compute the eigenvectors corresponding to the largest eigenvalues . With the proposed method , the empirical observations showed 1 ) the impact of those eigenvectors through examples , 2 ) consistent geometric local changes over different positions in the latent space , and 3 ) the impact of top eigenvectors on particular layers . Further , the authors discuss three areas of possible application ( gradient-based GAN inversion , gradient-free image search , interpretable axes discovery ) . _Strengths_ : - The paper addresses an interesting topic ( understanding the latent space of GANs ) and proposes a straightforward method . - The empirical observations demonstrates the advantages of using an image similarity metric as latent space distance and the correlation between eigenvalues corresponding to the largest eigenvalues and image perception . _Weaknesses_ : - The method is depending heavily on the distance metric employed , however , does not discuss in what way this could influence the outcome . In this paper , only LPIPS was considered as metric . What are the advantages and disadvantages ? How would the analysis and outcome change when other distance metrics are used ? These are questions that might benefit the work to discuss . - The actual method was barely discussed in the paper , but rather moved to the appendix . I think the authors may want to restructure the paper to include how to compute the eigenvectors are computed . - The actual application of the proposed method was rather discussed than actually performed . I do think this is a very interesting proposal , however , with only showing empirical observations , I believe the scope of this paper is too little . It would have been better if the authors picked one of the applications and applied their method . For more details see below . _Overall assessment_ : Overall , I find the proposed method very interesting and the empirical observations compelling . However , I find the scope of the paper not sufficient as it would have been nice to see that one of the application with the usage of the Hessian metric as distance function would have worked . _Detailed questions and comments_ : - Learned Perceptual Image Patch Similarity ( LLIPS , Section 3 `` Numerical Method '' ) : As this is the main distance metric being used , would it be possible to briefly introduce and define it in the paper ? - Requirement for understanding the latent space ( abstract ) : The abstract mentions inversion and interpretability as requirement for understanding the latent space . It was not clear to me how the method presented is addressing each of the requirement . Further , the abstract also claims `` This geometric understanding unifies previous results of GAN inversion and interpretation . `` . Can you clarify these claims ? - Appendix A.2 Methods for computing the hessian : Parts of how the Hessian is computed should be in the main paper as this is the main method for this paper . The explanation in Section 3 `` Numerical Method '' was superficial and I could have not understood the methodology without reading the the appendix . - Spectrum Structure of GANs ( Section 4 , Figure 2 ) : Can you clarify what has been exactly used to plot Figure 2 ? In the paragraph , Figure 2 was plotted using the mean and 90 % confidence interval , Figure 2 y-axis label says $ \\log ( eig/eigmax ) $ . - Figure 3D : The figure is hard to read , with many data points overlapping each other . I am also confused what the two lines crossing each data point represents . - `` Then we explored linearly the latent space along the eigenvector '' ( Section 4 ) : Why is the exploration linearly ? Does this conform with the manifold ? How is $ \\mu_i $ defined ? The footnote also says that spherical linear exploration is used some spaces . Can you elaborate more on how you performed exploration ? - Top Eigenvectors Capture Perceptual Relevant Changes : Would it be able to quantify this ? As only four samples were shown , how would we know that this generalizes for all samples ? - [ 1 ] - [ 4 ] might be also relevant to the topic of latent space exploration . They are not necessarily w.r.t.to manifold learning , however , with respect to applications in Section 6 , they look very close . _Post-rebuttal_ : I do highly appreciate the authors trying to answer all our questions and adding more details and experiments . However , after also reading through [ Chiu et al. , SIGGRAPH 2020 ] I do find that this paper has a large overlap with the one mentioned . Therefore , agreeing with Reviewer # 2 , the contribution is reduced to applying the idea to GANs . Therefore I am keeping my recommendation . [ 1 ] Lipton , Z.C . and Tripathi , S. , 2017 . Precise Recovery of Latent Vectors from Generative Adversarial Networks . ICLR 2017 workshop . [ 2 ] Albright , M. and McCloskey , S. , 2019 , May . Source Generator Attribution via Inversion . In CVPR Workshops ( Vol.7 ) . [ 3 ] Webster , R. , Rabin , J. , Simon , L. and Jurie , F. , 2019 . Detecting overfitting of deep generative networks via latent recovery . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( pp.11273-11282 ) . [ 4 ] Bojanowski , P. , Joulin , A. , Lopez-Pas , D. and Szlam , A. , 2018 , July . Optimizing the Latent Space of Generative Networks . In International Conference on Machine Learning ( pp.600-609 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank our reviewers for their time in considering our work . These insights have improved our paper considerably . We respond to each point below . > The method is depending heavily on the distance metric employed , however , does not discuss in what way this could influence the outcome . What are the advantages and disadvantages ? How would the analysis and outcome change when other distance metrics are used ? These are questions that might benefit the work to discuss . The reviewer is correct that our work used LPIPS distance as the primary image dissimilarity metric , and thus , we set out to determine if our results were entirely dependent on this choice . We performed a set of additional experiments computing the metric tensor at the same hidden vector using different image distance functions , including the mean squared error in pixel space ( MSE ) and structural similarity index measure ( SSIM ) . We computed the Hessian using each of these three measures ( MSE , SSIM , and LPIPS ) at 100 random sampled vectors in BigGAN , Progressive Growing GAN ( Face ) , StyleGAN2 ( FFHQ 256 resolution ) . We then compared the Hessians obtained with each image distance function . We found that the element-wise Pearson correlation of the Hessian matrices ranged between 0.94-0.99 , the correlation of eigenvalue spectra ranged between 0.987-0.995 . Using our derived statistics , we also measured the Hessian similarity $ C^ { Hlog } $ and $ C^ { Hlin } $ and found this resulted in a similarly high correlation ( ~0.99 ) . Thus we can confirm that the estimated Hessian matrix and its spectra are not strongly dependent on the chosen image distance metric , and their effect on the eigenvector of each other is correlated . We have added the table quantifying this result to the appendix . This result can be interpreted in the context of Section 5 of the manuscript . As the equation $ H ( z ) =J_ { \\varphi\\circ G } ^TJ_ { \\varphi\\circ G } $ there showed , the Riemannian metric of the GAN is the inner product matrix of the Jacobian of the generator composing the representation map for the distance metric $ J_ { \\varphi\\circ G } $ . This Jacobian is the composition of a chain of Jacobian of each layer , and the effect of the image difference metric is to add a few more terms to the top of the chain of Jacobians . In this regard , the Jacobian terms from the layers of the GAN seem to have a dominating effect compared to the few final terms coming from the image dissimilarity metric . Another explanation is that the 3 metric functions are in consensus for the geometry on the GAN manifold , but they may not agree with each other for * * out of manifold transforms * * . For example , as the demo in [ LPIPS ] ( https : //github.com/richzhang/PerceptualSimilarity ) paper showed , for some transforms like image blur or color distortion , MSE , SSIM and LPIPS can yield different answers about which image is more similar . But image blur and color distortion are artifacts that are easily discriminated against , so I believe generators will be trained not to generate blur and color distortion . In this regard , these transforms are out of manifold transforms . Thus , even if LPIPS , SSIM , and MSE are not always the same , they could provide a similar measure for the `` on manifold transforms '' encoded by GAN . > The actual method was barely discussed in the paper , but rather moved to the appendix . I think the authors may want to restructure the paper to include how to compute the eigenvectors are computed . We appreciate the reviewer 's effort to dig into the appendix and understand our method . We agree with the reviewer : although originally we were compelled to move the detailed version of the numerical method to the Appendix due to page limits , now we have expanded the method part in the main text to make it more intuitive . > The actual application of the proposed method was rather discussed than actually performed . I do think this is a very interesting proposal , however , with only showing empirical observations , I believe the scope of this paper is too little . It would have been better if the authors picked one of the applications and applied their method . This is a good recommendation . In this initial study , our goal was to emphasize the multiplicity of applications that this observation can advance . To clarify , we performed experiments in Section 6 and showed that using Hessian can actually assist optimization in GAN space with or without gradient . In this revision , figure 5 is expanded to illustrate this result . However , we take the point that it would be desirable to really focus on one of the applications , and we are pursuing similar efforts . For this manuscript revision , we have chosen to emphasize GAN inversion : we spend more effort to show that the Hessian preconditioning technique will improve GAN inversion for multiple GANs ( StyleGAN face , PGGAN ) for in distribution and out of distribution samples , using multi-start Adam optimizer . Hopefully , this can expand the scope of the paper ."}, "1": {"review_id": "GH7QRzUDdXG-1", "review_text": "The paper performs the analysis of the GAN latent spaces from the geometric perspective , inducing a metric tensor in the latent space from the LPIPS distance in the image space . The main authors ' finding is that under such metric , the latent spaces of typical GANs are highly anisotropic , which can be exploited for more effective GAN inversion . Furthermore , the authors show that eigen vectors of the metric tensor often correspond to interpretable latent transformations . Pros : 1 ) The paper is exceptionally well-written and provides a very interesting read . While the performed analysis is simple and natural , it does reveal several interesting findings about typical latent spaces : LPIPS-anisotropy , global consistency of the metric tensor . 2 ) The authors confirm the usefulness of their analysis by providing immediate practical benefits : more effective GAN inversion , which accounts for the latent space anisotropy . Cons : 1 ) Missing work on interpretable GAN directions : [ A ] The Hessian Penalty : A Weak Prior for Unsupervised Disentanglement , ECCV 2020 2 ) In my opinion , the authors do not provide enough support for their claim `` This finding unifies previous unsupervised methods that discover interpretable axes in the GAN space '' . - While the proposed method does seem to generalize both Ha \u0308rko \u0308nen et al. , 2020 and Shen & Zhou , 2020 , I do not see , how it captures Voynov & Babenko , 2020 and Pebbles et al ( see above [ A ] ) . Furthermore , I believe that such claims should be supported by the experiments . Could the authors experimentally confirm that their method results in the same set of directions as the existing methods ? Overall , I am positive about this submission , since the main analysis is both interesting and practically useful . My main criticism is that in terms of discovery of interpretable directions , the methods should be experimentally compared to existing alternatives . If it does provide a super-set of directions , obtained by existing methods , this would make the submission much stronger . Otherwise , the claim about unification should be toned down in my opinion . == AFTER REBUTTAL == I appreciate the authors ' efforts on additional thorough comparison to existing works on interpretable axes discovery . From the updated manuscript , however , it is not clear what method is superior and the authors ' approach appears to be a yet another method for this task rather than generalization of previous ones . Overall , I am still on the positive side since the observed findings deliver a clear profit for GAN inversion . But I am not increasing my score given that the `` interpretable axes '' part has become less impressive ( in terms of weaker claims and conclusions ) and the competing SIGGRAPH work .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> * Missing work on interpretable GAN directions : * * [ A ] The Hessian Penalty : A Weak Prior for Unsupervised Disentanglement , ECCV 2020 * Thank you for bringing this work to our attention . We have now included it as a reference and as a launching pad for our investigations . > * In my opinion , the authors do not provide enough support for their claim `` This finding unifies previous unsupervised methods that discover interpretable axes in the GAN space '' . * * While the proposed method does seem to generalize both Ha \u0308rko \u0308nen et al. , 2020 and Shen & Zhou , 2020 , I do not see , how it captures Voynov & Babenko , 2020 and Pebbles et al ( see above [ A ] ) . Furthermore , I believe that such claims should be supported by the experiments . Could the authors experimentally confirm that their method results in the same set of directions as the existing methods ? * > > * Overall , I am positive about this submission , since the main analysis is both interesting and practically useful . My main criticism is that in terms of discovery of interpretable directions , the methods should be experimentally compared to existing alternatives . If it does provide a super-set of directions , obtained by existing methods , this would make the submission much stronger . Otherwise , the claim about unification should be toned down in my opinion . * We appreciate these kind comments ! We agree that our submitted draft needed some clarification regarding the claim that our results unified previous unsupervised methods . To address this , we performed additional comparison with previous work on this topic , and have added those to the appendix . Specifically , we acknowledge that the * Voynov & Babenko , 2020 * result may not be deducible from our result . To clarify the relationship , we performed an additional comparison of the axes they discovered and the Hessian eigenvectors we computed on those GANs . In the PGGAN , we found that the 6 axes they annotated have a larger alignment with our top eigenspace , as the $ vHv $ for their axes are significantly larger than those of unit random vectors ( $ p < 0.01 $ for all but one axes $ p < 0.05 $ ) . Further , we projected their axes onto the eigenvectors of Hessian and analyzed how the power ( square of projection coefficient ) is distributed on the spectra . Further , to investigate the alignment of their axes and single eigenvector , we project their axes onto the basis formed by Hessian eigenvectors and compute the power ( squared projection coefficient ) . We computed the histogram of power in different parts of spectra and found that the entropy of this distribution is significantly lower for their axes than for unit random vectors , ( $ p < 0.01 $ for all but one axes ) . In this regard , their method also discovers that the top eigenspace of PGGAN contains many informative transforms , and that alignment with dimensions of similar eigenvalues is better than mixing all the dimensions up . Note that , in their work , they chose to enforce orthonormality on the discovered axes for BigGAN noise space and StyleGAN FFHQ1024 , which was not a constraint for the axes in PGGAN . In those two models , maybe due to orthogonality their axes do not have a uniform relationship with the Hessian structure of GAN . For example , in the BigGAN noise space , 3 out of 6 annotated axes have a significantly lower $ vHv $ value ( p < 0.001 ) comparing to random vectors , namely these axes avoid the top eigenspace . Further , the same 3 out of 6 annotated axes have a significantly lower entropy in the top 15 dimension eigenspace ( $ p < 0.05 $ ) which suggests that although these axes avoid the top eigenspace , they are trained to concentrate power more on one of the top eigenvectors instead of mixing them randomly . This initial comparison has already provided some unexpected insights about the nature of interpretable axes . Further development is needed to draw a clear connection between the Hessian structure and the axes defined by in the previous works . This comparison encourages us to qualify our statements about unifying previous results , especially for Voynov Babenko 2020 ."}, "2": {"review_id": "GH7QRzUDdXG-2", "review_text": "This paper proposes a method for finding the axes of largest variation in the latent space of a generative model . This can be leveraged for better generator inversion and explainability . Several experiments are done to evaluate the latent vectors used by the method quantitatively and qualitatively . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : The analyses are interesting and the method seems novel . This method could prove useful in analysis of latent space properties . Its full significance and practicality could be clearer , though . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : - The problem of understanding the latent space structure better is relevant and timely in the research on generative models equipped with a continuous latent space . - Experiments seem to demonstrate successful dimensional truncation , GAN inversion , and that the eigenvectors behave consistently # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : - I am not sure I see whether each experiment actually supports the corresponding claim . Especially , the claim that interpretable axes are found seems central but it is not much supported in the experiments . It seems mostly represented in Fig.9 in the Appendix , but I do not fully understand how we are supposed interpret the result in this figure . - Fig 2 is hard to parse and clearly too tightly packed . Something should be done about it . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : - It is easy to get lost in the math and in the various measurements . Could the authors summarize , in practise and plain English , how their method should be used to find the most intepretable axes of variation for a new generative model , and how to confirm and measure that we have indeed found them ? Typo : on page 7 around `` ( Fig 5 . ) '' # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Update after rebuttal discussions : - In the light of the considerable overlap with [ Chiu et al. , 2020 ] pointed out by the other reviewers , I decreased my score . I have familiarized myself with it and can confirm the said overlap . However , given remaining differences , I do not find it unreasonable to consider this paper as `` complementary '' to [ Chiu et al. , 2020 ] , * provided that the authors explicitly address the similarities in the final version * . - I consider the sum total of contributions of the paper still tilting towards being sufficient for publication .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate the helpful and constructive reviews that the reviewers have made ! We respond to each of the points below . > * I am not sure I see whether each experiment actually supports the corresponding claim . Especially , the claim that interpretable axes are found seems central but it is not much supported in the experiments . It seems mostly represented in Fig.9 in the Appendix , but I do not fully understand how we are supposed interpret the result in this figure . * We thank the reviewer for this insight . We agree that in our initial draft , we were overreaching in our description of interpretability . In the two weeks since receiving this comment , we set out to formally investigate whether this method reveals axes that are perceptually relevant to individuals other than the authors . Using Amazon \u2019 s Mechanical Turk , we generated images under the identified Hessian directions of four different GANs ( Progressive Growing GAN , BigGAN noise space , StyleGAN-Cat and StyleGAN-Face ) , and presented them to 175 participants , to investigate if the images were interpretable . We operationalized the concept of \u201c interpretable \u201d as follows : in each trial , we randomly sampled five reference images generated by a given GAN , and perturbed them ( linearly or spherically ) along the Hessian axis to be tested , which created five image sequences . These five sequences were shown on the same screen . The subjects viewed all the five sequences , and were asked if they could perceive a change , and if so , to describe the transformation that was common to the majority of the sequences . They were also asked to indicate how many sequences shared the identified transformation . Finally , subjects reported how large the perceived image change was ( 0 % -100 % ) , and the similarity of the image transformations across the five sequences ( score of 1-9.9 most similar ) and the difficulty in interpreting the common change ( InterpretDifficulty score on the scale of 1-9 , 9 most difficult ) . Image perturbations comprised the top 10 eigenvectors , 10 random vectors orthogonal to these eigenvectors , and the bottom 10 eigenvectors in four GANs . Further , we added in photos of real objects as reference stimuli , which were manipulated by well-defined transformations such as rotation , perspective change , eye color , and more . Overall , subjects reported that the image sequences generated by top eigenvectors showed a larger amplitude of image change than both a ) orthogonal directions ( * t * = 3.4 , * P * = 7.0 x 10^-4 ) and b ) bottom eigenvectors ( * t * = 6.4 , * P * = 2.1 x 10^-10 ) ; notably , this was true even though we picked a much smaller step size in the top eigenspace than the orthogonal and bottom eigenspace . On average , the top 10 eigenvectors had a higher perceptual consistency score than a ) the orthogonal-space random vectors ( * t = * 2.8 * , P = * 5.8 x 10-3 ) and b ) the bottom eigenvectors ( * t = * 4.4 * , P = * 1.3 x 10-5 ) . The subjects report that the top eigenvectors are easier to interpret than the bottom eigenvectors when they observe image change in the bottom eigenvectors . ( * t = * -4.6 * , P = 3.8 * x 10-6 ) . Thus , we have found stronger evidence for our initial report that these Hessian eigenvectors \u201c capture perceptually relevant changes. \u201d These results are now featured in Section 3 . > * Fig 2 is hard to parse and clearly too tightly packed . Something should be done about it . * To improve this image , first , we lowered the density by enlarging the canvas of the figure ; we also removed 1-2 GAN curves from each subplot , to improve legend readability ."}, "3": {"review_id": "GH7QRzUDdXG-3", "review_text": "Summary : The paper proposes an analysis way of a latent space of GAN in a GAN architecture agnostic way . They use the Riemannian manifold analysis to investigate image manifold , which leads to a simple algorithm with eigen-decomposition of the Hessian matrix of a local point . Unfortunately , many ideas and some of the findings ( Hessian-based GAN exploration and anisotropy of Hessian in the latent space ) are already well explored in [ C1 ] with much higher quality and various modality applications . But still , there are interesting ideas contained in this paper : 1 ) the latent space is homogeneous ( in a sense that the major directions obtained by a latent position are shared at different positions with similar semantic meanings ) , and 2 ) extensive eigenvalue analyses of Hessian . Based on the findings , they show interesting applications of efficient GAN Inversion , gradient-free search in Image space , and unsupervised discovery of interpretable axes . However , these applications are also already explored by [ C1 ] in a visually pleasing way . [ C1 ] Human-in-the-Loop Differential Subspace Search in High-Dimensional Latent Space , SIGGRAPH 2020 . Reasons for score : I like the idea that leverages the findings of the homogeneous property of Hessian and improves the efficiency of GAN inversion with Hessian preconditioning . Also , I appreciate the analysis of eigenvalues and correlations . However , unfortunately , most of the contributions the authors argued are largely overlapped with [ C1 ] , of which citation is missed by the authors . Thus , with these remaining contributions , this reviewer could not vote for the acceptance of this work at this point . This reviewer would like to ask the significant different contribution from [ C1 ] that this reviewer may miss . Pros : - The paper shows interesting interpretation and ideas , and its applications . - The paper is written very clearly and readily . - The authors validate the ideas with enough effort in the experiments . Cons : - The GAN architecture agnostic latent space exploration with Riemannian interpretation is already done in [ C1 ] in a more general way . Detail comments : - While [ C1 ] presents mainly about the SVD of Jacobian ( but they also show Hessian interpretation ( metric tensor ) in Sec.4.1 as well ) , it is essentially the same because of Eq . ( 3 ) in this paper . Also , [ C1 ] suggested much more diverse exploration methods with diverse search methods . Thus , this reviewer would like to listen to the authors ' responses about this overlap .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank our reviewer for the kind and constructive feedback . We were delighted to read this very recent Chiu et al.SIGGRAPH paper and were glad to find some overlap with our work , which we developed independently and in parallel . This overlap suggests an independent confirmation of the validity of the approach , which is exciting , and we have amended our manuscript to credit their important work . As pointed out by our reviewer , the SIGGRAPH numerical method bears some similarity to ours ; the authors also applied their method to a problem reminiscent of those in our study ( i.e.optimizing the black-box function of human perceptual judgment ) , and their Figures 3 and 4 also showed different rates of image change , proving the robustness of the geometric structure . However , there are strong differences between our studies : first , we provide an explicit , analytical link between the Jacobian of the generator to the Riemannian metric ; second , we provide a numerical method that is faster and more accurate in approximating the full metric tensor , suitable to analyze the geometry of the latent space ; third , we provide insights to the geometry through the homogeneity of the metric tensor . We elaborate on these points below . 1.The SIGGRAPH paper \u2019 s mathematical framework for finding the most informative axes bears some similarity to ours . However , this article had no equation explicitly describing the connection between the Riemannian metric tensor and the Jacobian . In our reviewer \u2019 s example of section 4.1 , Chiu et al.implied a Hessian interpretation of their approach , where the authors created a synthetic function using a quadratic form with an anisotropic spectrum to test their optimization method ; but they did not elaborate on this interpretation for general generative models . Our study presents this analytical connection explicitly \u2014 without our work , readers might not recognize this relationship and understand the geometric structure fully . We further went on to comprehensively measure the geometric properties of multiple state-of-the-art generative models , namely , we measured the local geometry of GAN latent spaces and their globalization . 2.In terms of the numerical method and efficiency , our approach differs substantially from that of Chiu et . al.Their numerical method computes the Jacobian matrix of the generator instead of the Hessian matrices of the squared distance . As recognized by the reviewer , if we have used L2 distance as the image distance function , then the inner product matrix of the Jacobian $ H=J^TJ $ results in the Hessian . In the Jacobian matrix ( $ m $ -by- $ n $ , sample dimension $ m $ , hidden dimension $ n $ ) , $ m $ is usually much larger than $ n $ ( e.g.for a standard image generative network $ n $ is ~200-500 , and $ m $ ranges from ~20,000-3,000,000 ) , thus it is highly inefficient to compute a full Jacobian matrix as its complexity is O ( m * single backprop time ) ; in contrast , the complexity of our method is O ( n * double backprop time ) using common backpropagation . For example , for Progressive Growing GAN at 256 pixel-level , our method could obtain the full Hessian and its spectrum in 58 seconds , while the alternative method needs more than 4800 sec to obtain the spectrum of the full Jacobian . Thus , in their application , the authors approximate the Jacobian by randomly sampling output coordinates in sample space , and perform singular value decomposition ( SVD ) in the restrained space . This is equivalent to sampling rows of the Jacobian matrix to estimate its spectrum . In our approximation method , we perform the Hessian decomposition while doing backprop using Lanczos iteration , thus finding the largest eigenvalues and eigenvectors ( equivalent to their largest right singular vectors ) . We conducted a numerical experiment to prove this . We showed that our numerical method approximates the real Hessian matrix faster and more accurately . Taking the Progressive Growing GAN with latent dimension n=512 as an example , using Lanczos iteration with finite-differencing method ( ForwardIter ) , we can find the top 50 eigenvectors of the Hessian , approximating the full matrix to an accuracy of 0.99999 ( per element-wise Pearson correlation ) in 7.9 secs , while the alternative method , sampling 500 output dimensions , can only achieve 0.92420 correlation to the real Hessian in 12.5 secs . Thus , in terms of algorithmic efficiency , our method can compute the Riemannian metric tensor and the directions that are most informative faster , because the Lanczos iteration is more sample-efficient than a random sampling method ( for more specific information , we will add this comparison to our Appendix ) . Although the alternative is a praiseworthy way to identify promising axes to explore the latent space of GANs , as the null space will not affect the exploration process in their application , our study provides a more efficient tool to interrogate the geometry of the GAN space ."}}