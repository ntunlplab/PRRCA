{"year": "2021", "forum": "j6rILItz4yr", "title": "ALFA: Adversarial Feature Augmentation for Enhanced Image Recognition", "decision": "Reject", "meta_review": "Adversarial training is usually done on the image space by directly optimizing the pixels. This paper suggests the adversarial training over intermediate feature spaces in the neural network. The idea is very simple. The authors have done extensive experiments to justify its performance. But the performance gain though this idea seems to be marginal. Further, the layer to conduct the adversarial training can be optimized within the framework, which aligns with the general autoML idea. The new version L-ALFA has been well introduced, but unfortunately, the practical result can be very straightforward, that is just to select the final layer. The more important ALFA hyperparameters that would most benefit from automatic tuning are not sufficiently treated.  There have been extensive discussions between the authors and the reviewers. After incorporating the reviewers' comments, the paper will have a good chance to be accepted at another venue. \n\n", "reviews": [{"review_id": "j6rILItz4yr-0", "review_text": "Overview of paper : this work tackles the task of adversarial augmentation for better generalization . Instead of augmentation the pixels space , which is expensive and potentially harder , they augment the intermediate feature representation . As the choice of the particular layer for application of the perturbations affects performance , the authors , optimize it jointly with the rest of the parameters . Experiments show this method improves accuracy over standard training . Novelty : although adversarial training on raw image is known to improve generalization , doing so on image features is novel as far as I am aware . Additionally , jointly choosing the layers to be perturbed is also new in my understanding ( although the main benefit is in the analysis , as the fixed strategy of perturbing the last block seems comparable ) . Evaluation : The proposed feature adversarial training seems to consistently improve generalization on several popular datasets , however there are a few limitations : i ) the gap is not huge . ii ) it is not clear that the difference is significant from Xie et al.iii ) nearly all results are on Cifar10 , including the baseline comparisons iv ) the speed up due to operating on features rather than pixels is cited as the main motivation but limited effort exists to evaluate it . Presentation : the paper is nicely written , and is easy to follow . Other questions : did you use auxiliary BNs like Xie et al ? In what experiments did ALFA-L beat ALFA on the last block ? ( Sec 4.4 is a bit hazy there ) Overall : Adversarial feature perturbation for generalization is an interesting ideas and was shown to have some benefits . The evaluation of accuracy and runtime against other reasonable methods ( particularly doing the same on pixels ) is limited . I am positively inclined towards this paper and hope the authors can address by concerns in the rebuttal . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # The response addressed some of my concerns , but I am concerned about L-ALFA taking such a large part of the paper and then shown to not help so much over just picking the final laye nor being much faster than the baseline . The more important ALFA hyperparameters that would most benefit from automatic tuning are not sufficiently treated . Although I do like the objective of this paper and some of the approaches , I think it might need to be revised and resubmitted , incorporating the extensive discussion presented by all the reviewers .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your insightful comments . Below , we provide detailed responses to your concerns . [ Weakness 1 : The gap is not huge . ] It is fair to note that these datasets are highly competitive , and it is impressive to obtain further improvements based on baselines \u2019 almost saturated performance . Our proposed methods ALFA , as an implicit data augmentation or regularization , achieves consistent generalization improvements over strong baselines on CIFAR-10 , CIFAR-100 , and ImageNet benchmarks across different backbone networks for image recognition . [ Weakness 2 : The difference from Xie et al .. ] Our methods ( ALFA , L-ALFA ) have several key differences from AdvProp in Xie et al . ( 1 ) Our methods leverage adversarial perturbations in the feature space to improve generalization , instead of generating computational expensive pixel-level perturbations on raw images ( Xie et al ) . ( 2 ) To efficiently learn an optimal strategy of perturbation injection , we further propose a learnable adversarial feature augmentation ( L-ALFA ) framework , which is capable of automatically adjusting the position and strength of introduced feature perturbations . This is not covered in Xie et al . ( 3 ) We do not use the auxiliary batch normalization , which is the main contribution in Xie et al . [ Weakness 3 : Nearly all results are CIFAR-10 . ] We respectfully do not agree . We verify our proposed ALFA on CIFAR-100 with ResNet-20s and ResNet-56s backbones ( Table 3 ) , on ImageNet with ResNet-18 , ResNet-50 , ResNet-101 , and ResNet-152 backbones ( Table 3 ) . We also report the ablation of PGD steps on ImageNet ( Table 4 ) . In addition , we validate L-ALFA on both CIFAR-100 and ImageNet ( Table 7 ) . Note that , in both Table 3 and Table 4 , we include the comparison with baselines . To further address your concern , we also provide the EfficientNet-B0 results on ImageNet here , Baseline ( 77.04 % ) vs. ALFA ( 77.48 % ) . Extensive experiments demonstrate that ALFA achieves consistent generalization improvement over multiple backbones and diverse datasets ."}, {"review_id": "j6rILItz4yr-1", "review_text": "-- Paper summary -- The authors propose Adversarial Feature Augmentation ( ALFA ) , which augment features at hidden layers by adding adversarial perturbations . Where and how strongly the augmentation is conducted is automatically optimized via training . Experimental results show that the proposed method consistently improves the performance of baselines over several datasets and network architectures . -- Review summary -- Although the motivation of this study is clear , the proposed method is not appropriately designed along with the motivation . Moreover , its novely is merginal . I vote for rejection . -- Details -- Strength - The motivation is clear and seems reasonable . Training with adversarial perturbations is known to be effective but computationally expensive . It can be problematic when the model or training data is large-scale . - The proposed method consistently improves the performance of baselines over several datasets and network architectures . Weakness and concerns - Is the computational complexity of the proposed method really small ? Since the adversarial perturbation is computed for every layer , its computational complexity should be almost same with that of standard adversarial training . - The training objective shown in Eq . ( 3 ) is not reasonable . Since the norm of \\delta is upper-bounded by a certain constant \\epsilon , the effect of the adversarial perturbation can be reduced just by increasing the scale of features . Are features normalized ones ? - The optimization of \\eta in L-ALFA is not reasonable . Since min_\\eta comes after max_\\delta , L-ALFA should choose the layer that corresponds to the smallest increase of loss by adding adversarial perturbation . Therefore , this design minimizes the effect of the augmentation , which is contradictive to the motivation of introducing \\eta . Moreover , since \\epsilon is common for all layers , the optimal \\eta should be sensitive to the scale of features , which indicates that the performance of the proposed method would heavily depend on both how to initialize the model and whether any normalization is conducted in the model or not . - Marginal novelty . An idea of adversarially augmented features has been already presented in [ R1 ] . [ R1 ] `` Training Deep Neural Networks with Adversarially Augmented Features for Small-scale Training Datasets , '' IJCNN 2019 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your insightful comments . Below , we provide detailed responses to your concerns . [ Weakness 1 : Computational complexity . ] For ALFA , we only apply adversarial perturbation to a single layer , which allows ALFA to maintain cost efficiency even for deep networks . Note that the feature perturbation generation only requires backpropagation through a section of the network . The efficiency of L-ALFA , compared to pixel-based methods , mainly depends on the number of perturbed layers ( i.e. , the dimension of $ \\eta $ in equation 5 ) , and it is not necessary to perturb every layer . We agree that if there are hundreds of layers that need to be perturbed , L-ALFA may not be efficient enough . Fortunately , as shown in Table 8 , a moderate number of perturbed layers ( e.g. , dim ( $ \\eta $ ) =9 ) is enough to exhibit the effectiveness of L-ALFA . We provide an updated time comparison with ResNet-18 per training epoch here : AdvProp ( 123s ) vs. ALFA ( 28s ) vs. L-ALFA with dim ( $ \\eta $ ) =9 ( 91s ) vs. Standard training ( 23s ) . [ Weakness 2 : Epsilon upper-bound . ] As mentioned in Section 4 , we adopt unbounded perturbation generation without the epsilon constraint . We will update our manuscript to clarify the confusion . Thanks for pointing out the missing details . Yes , we apply feature perturbations to the normalized feature from batch normalization . To further address your concern , here we report the magnitude of crafted feature perturbation in practice , which steadily stays in the range of 0.97 to 1.10 under the $ \\ell_2 $ norm . [ Weakness 3 : The optimization of $ \\eta $ in L-ALFA is not reasonable . ] In general , our design philosophy of L-ALFA is adversarial training . It maximizes the training objective to craft feature perturbation and minimizes the training objective to optimize model weights and the learnable \u201c hyperparameter \u201d ( e.g. , $ \\eta $ in equation 5 ) . Note that there is a constraint $ \\mathcal { P } $ ball of $ \\eta $ , where the summation of all $ \\eta_i $ is equal to 1 . This constraint prevents the minimization to hurt the effect of feature augmentation and avoids trivial solutions ( i.e. , $ \\eta $ =0 ) . Moreover , as mentioned in the answer to Weakness 2 , we adopt unbound perturbation generation and batch normalization in all networks . In addition , as shown in Table 6 , we conduct five independent runs with different random seeds , and the performance is very stable , i.e. , 94.65 % ( +- 0.08 % ) . [ Weakness 4 : IJCNN 2019 paper . ] Thanks for pointing out the literature . Our work is different from this IJCNN paper in three aspects : motivation , methods , and experiment design . ( 1 ) Motivation . IJCNN crafts strong adversarial perturbations to fool the model ( \u201c the perturbation is designed to be adversarial , which means that they are designed to significantly change the output of the network. \u201d ) . Our method aims to generate moderate adversarially perturbed features to improve the generalization rather than to fool the model . ( 2 ) Methods . IJCNN injects adversarial perturbations in a randomly selected layer by virtual adversarial training and ad-hoc coefficient layers . We utilize PGD-AT to craft gradient-based perturbations , and further design the learnable L-ALFA framework to automatically learn the location and strength of generated feature perturbations . ( 3 ) Experiments . IJCNN only focuses on small-scale datasets ( MNIST , NORB , CIFAR-10 ) with networks no more than eight layers . Our experiments are conducted on diverse datasets from small-scale CIFAR-10 and CIFAR-100 to large-scale ImageNet , with much larger network architectures than IJCNN , including ResNet-20s/56s , ResNet-18/-50/-101/-152 . We hope that you can raise your score if you find our answers that address your questions . Thank you !"}, {"review_id": "j6rILItz4yr-2", "review_text": "Pros : Method is clearly stated , and the learning adaptive perturbation strength seems novel . Provided experimental results for large datasets like ImageNet . Major Concerns : The method proposes crafting adversarial perturbation at different layers of the network . A small change in features space may correspond to a large change in input space for neural networks , hence it is doubtful whether the perturbations crafted will be meaningful . The method which attempts to cause perturbations in the feature space also ensures that perturbed image is similar to original image ( imperceptibility constraint ) [ 3 ] . The authors mention use of unbounded perturbation size in section 4.2 which goes against the goal of adversarial training ( i.e.same prediction in the neighborhood of the image ) . Also , if the perturbation is unbounded then there is no requirement of Projection step in PGD ( Projection is done for enforcing the boundedness of perturbation ) . Could the authors please clarify this ? The intuition why this method works is also not too clear , as it has been shown in [ 1 ] that adversarial training leads to drop in standard accuracy . But in this paper adversarial training has been shown to increase standard accuracy even without using any adaptive parameters ( like batch normalization layers used in AdvProp [ 2 ] ) . This simply goes against the established facts . It is hard to distill what settings work well across different datasets . As seen in Table 5 , for CIFAR-10 , changing from 1 step PGD to 5 step PGD increases accuracy , whereas it decreases in case of ImageNet . Also there exist only a certain range of step size values for which standard accuracy increases . Could the authors kindly offer additional theoretical or intuitive explanations to clarify the same ? This would help substantially improve the submission . Other questions : The authors could provide info on fooling achieved by crafted adversaries which is required for confirming the success of adversary creation . Also , the adversarial training method proposed in this paper shows minimal increase in adversarial robustness which also supplements the need for details on fooling rate . Typically adversarial training increases its robustness [ 5 ] , which is not observed with the proposed method . Could the authors clarify this ? The accuracy reported for the ImageNet models is lower than Torchvision models [ 4 ] as the authors use the same code as PyTorch repo it is unexpected . This may be due to the use of 10 % training data for validation which is not required in ImageNet , as it already has a validation set . ( This is important as the magnitude of improvements are not very large over the standard accuracy in case of ImageNet and is even smaller when compared with Torchvision models ) . The Torchvision ResNet 50 model obtains 76.15 % accuracy which is comparable to ALFA ( 76.23 % ) ( present in Table 3 ) . As the perturbations are calculated for each layer , the complexity of the method would increase with large networks like ResNet-152 . So it is unclear if the proposed method will continue to be cost efficient in comparison to pixel-based methods . Also , could the authors clarify the network architecture used for comparison of the time complexity in Section 4.3 for ALPHA and AdvProp [ 2 ] ? Overall , this seems a very complex method without any theoretical grounding to increase the generalization performance . [ 1 ] Zhang , H. , Yu , Y. , Jiao , J. , Xing , E. , Ghaoui , L.E . & Jordan , M .. ( 2019 ) . Theoretically Principled Trade-off between Robustness and Accuracy . Proceedings of the 36th International Conference on Machine Learning , in PMLR 97:7472-7482 [ 2 ] Xie , C. , Tan , M. , Gong , B. , Wang , J. , Yuille , A. L. , & Le , Q. V. ( 2020 ) . Adversarial examples improve image recognition . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ( pp.819-828 ) . [ 3 ] Ganeshan , A. , & Babu , R. V. ( 2019 ) . FDA : Feature disruptive attack . In Proceedings of the IEEE International Conference on Computer Vision ( pp.8069-8079 ) . [ 4 ] https : //pytorch.org/docs/stable/torchvision/models.html [ 5 ] Madry , A. , Makelov , A. , Schmidt , L. , Tsipras , D. , & Vladu , A . ( 2018 , February ) . Towards Deep Learning Models Resistant to Adversarial Attacks . In International Conference on Learning Representations .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your insightful comments . Below , we provide detailed responses to your concerns . [ Major Concern 1 : It is doubtful whether the perturbations crafted will be meaningful . ] Our work aims to enhance image recognition on the clean test set ; therefore , we do not have the constraint that the perturbed image needs to be similar to the original image ( i.e. , the imperceptibility constraint ) . More specifically , our method performs \u201c adversarial data augmentation \u201d , with a focus on leveraging adversarial perturbations in the feature space for regularization , rather than generating actual adversarially perturbed images . We will make this clear in the revision . [ Major Concern 2 : Unbounded perturbation . ] Our generated perturbations are applied to intermediate feature embeddings . Since we only care about the final clean accuracy , there is no constraint that we need to craft adversarial perturbations in the neighborhood of the image . For the unbounded perturbation generation , we do not adopt the projection process . In practice , the magnitude of crafted feature perturbation steadily stays in a range from 0.97 to 1.10 under the $ \\ell_2 $ norm . We will make this clear in the revision . [ Major Concern 3 : Clarify the intuition . ] Our method is not against the facts in [ 1 ] . TRADES [ 1 ] found that vanilla adversarial training on the input pixel space leads to standard accuracy degradation . AdvProp [ 2 ] utilizes adversarial examples with an auxiliary batch normalization to improve standard accuracy . Our work is different in that we introduce adversarial perturbation to feature embeddings rather than raw pixels , which improves the standard accuracy of image recognition . Our method is also well-motivated since adversarial feature augmentation has been widely proved beneficial for model generalization , such as [ 3 ] in computer vision , [ 4,5 ] in natural language processing , and [ 6 ] in vision+language tasks . [ Major Concern 4 : Insights of hyperparameter selection . ] As mentioned by R2 , to obtain standard accuracy improvement , there is a safe zone of hyperparameter selection for perturbation strength . The immoderate weak perturbation yields marginal effects , while excessive strong perturbations may lead to over-smoothed decision boundaries and incur performance degradation . Thus , learning a proper perturbation strength is important for our proposed ALFA . Moreover , in order to avoid laborious tuning across diverse datasets and backbones , we design learnable adversarial feature augmentation ( L-ALFA ) to automatically learn the best hyperparameter configurations . [ 1 ] Theoretically Principled Trade-off between Robustness and Accuracy , ICML 2019 . [ 2 ] Adversarial examples improve image recognition , CVPR 2020 . [ 3 ] Improved Sample Complexities for Deep Networks and Robust Classification via an All-Layer Margin , ICLR 2020 . [ 4 ] FreeLB : Enhanced Adversarial Training for Natural Language Understanding , ICLR 2020 . [ 5 ] SMART : Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization , ACL 2020 . [ 6 ] Large-Scale Adversarial Training for Vision-and-Language Representation Learning , NeurIPS 2020 ."}, {"review_id": "j6rILItz4yr-3", "review_text": "In this paper , the authors suggest a method for adversarial feature-level augmentation , mainly framed as an approach to improve the clean-set accuracy rather than adversarial example robustness . The authors also propose a learnable version ( LALFA ) , to automatically learn the location and strength of the perturbations . Overall , I think this is an interesting paper that can be considered for publication at ICLR . The following elaborates it further : Strengths : * Even though the adversarial feature augmentations at the feature level are not unprecedented , but the learnable tuning of the location/strength is an interesting approach that can potentially avoid expensive hyperparameter optimization . * The shown improvements of the best-achieved models seem consistent over the baselines , across different datasets and models . Weaknesses : * The obtained improvements are moderate on smaller networks and become marginal with deeper counterparts . * The performance and the offered advantages seem quite sensitive to the choice of hyperparameters in ALFA ( Tables 4 and 5 ) . I wonder , at least , how stable/conclusive the comparisons are when transferring the selected model from one set to another , e.g.validation to test . Further detailed comments : * `` L-ALFA saves toilsome tuning by automatically adjusting the strength and locations of adversarial feature augmentations '' = > It is fair to note that this is achieved at the expense of introducing a new hyper-parameter to tune , namely \\alpha . * An ablation study on the L1 regularization could have been useful . * In Table 1 and some of the other numerical comparisons , except for Table 6 , where standard deviations are reported : I wonder how significant are these comparisons ? Are the differences meaningful when considering the intra-experiment variations ? * Insights on why MoEx , one of the three baselines compared against , is performing worse than both random noise and normal training , would be helpful . * Equation ( 1 ) defines \\delta \\in B_\\epsilon ( x ) and uses it as an offset to x : f ( x+\\delta ; \\theta ) . Later B_\\epsilon ( x ) is defined as `` The norm ball B_\\epsilon is centered at x with radius \\epsilon '' : These are not consistent , if \\delta is used as an offset , it can not be sampled around x . * \\mathcal { L } _ { at } has been used inconsistency across equation ( 1 ) and equations ( 3 ) and ( 4 ) , on the first argument ; f_i ( x ; \\theta^ { ( i ) } +\\delta^ { ( i ) } ) in equation ( 3 ) and ( 4 ) misses the second part of the network , transforming the intermediate feature maps to the required predictions . * \\mathcal { L } _ { at } is used in equation 1 , but is elaborated after equation 2 . * Defining both F ( x ; \\theta ) and f ( x ; \\theta ) is referring to the neural network , and its output seems unnecessary and confusing ; besides , F ( x ; \\theta ) is never referenced before . * f ( x+\\delta , \\theta ) = > f ( x+\\delta ; \\theta ) * Adding a row in table 7 , representing the results from ALFA , will make the direct comparison between ALFA and L-ALFA easier . * `` ResNet-18 has ... and twenty convolutional layers '' = > It has seventeen convolutional layers , I think . Please clarify on dim ( \\eta ) = 20 in Table 8 . Are some of the pooling layers outputs also taken ? * typo : `` a unduly ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your insightful comments . Below , we provide detailed responses to your concerns . [ Weakness 1 : Moderate and Marginal Improvements . ] We respectfully do not agree that the improvements are marginal . Our proposed methods ALFA , as an implicit data augmentation or regularization , achieves consistent generalization improvements over strong baselines on CIFAR-10 , CIFAR-100 , and ImageNet benchmarks across different backbone networks for image recognition . Note that , these datasets are highly competitive , and it is impressive to obtain further improvements based on baselines \u2019 almost saturated performance . [ Weakness 2 : The Stability of Hyperparameters from Validation to Test . ] As mentioned in Section 4.1 , the original training dataset is randomly split into 90 % training and 10 % validation in our experiments . The early stopping technique is applied to find the top-performing checkpoints on the validation set . Then , the selected checkpoints are evaluated on the test set and report the performance . From our observations , the hyperparameters are quite stable from validation to test sets . To further address your concerns , we provide the code as the additional supplementary material that will be uploaded before the end of the rebuttal period ."}], "0": {"review_id": "j6rILItz4yr-0", "review_text": "Overview of paper : this work tackles the task of adversarial augmentation for better generalization . Instead of augmentation the pixels space , which is expensive and potentially harder , they augment the intermediate feature representation . As the choice of the particular layer for application of the perturbations affects performance , the authors , optimize it jointly with the rest of the parameters . Experiments show this method improves accuracy over standard training . Novelty : although adversarial training on raw image is known to improve generalization , doing so on image features is novel as far as I am aware . Additionally , jointly choosing the layers to be perturbed is also new in my understanding ( although the main benefit is in the analysis , as the fixed strategy of perturbing the last block seems comparable ) . Evaluation : The proposed feature adversarial training seems to consistently improve generalization on several popular datasets , however there are a few limitations : i ) the gap is not huge . ii ) it is not clear that the difference is significant from Xie et al.iii ) nearly all results are on Cifar10 , including the baseline comparisons iv ) the speed up due to operating on features rather than pixels is cited as the main motivation but limited effort exists to evaluate it . Presentation : the paper is nicely written , and is easy to follow . Other questions : did you use auxiliary BNs like Xie et al ? In what experiments did ALFA-L beat ALFA on the last block ? ( Sec 4.4 is a bit hazy there ) Overall : Adversarial feature perturbation for generalization is an interesting ideas and was shown to have some benefits . The evaluation of accuracy and runtime against other reasonable methods ( particularly doing the same on pixels ) is limited . I am positively inclined towards this paper and hope the authors can address by concerns in the rebuttal . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # The response addressed some of my concerns , but I am concerned about L-ALFA taking such a large part of the paper and then shown to not help so much over just picking the final laye nor being much faster than the baseline . The more important ALFA hyperparameters that would most benefit from automatic tuning are not sufficiently treated . Although I do like the objective of this paper and some of the approaches , I think it might need to be revised and resubmitted , incorporating the extensive discussion presented by all the reviewers .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your insightful comments . Below , we provide detailed responses to your concerns . [ Weakness 1 : The gap is not huge . ] It is fair to note that these datasets are highly competitive , and it is impressive to obtain further improvements based on baselines \u2019 almost saturated performance . Our proposed methods ALFA , as an implicit data augmentation or regularization , achieves consistent generalization improvements over strong baselines on CIFAR-10 , CIFAR-100 , and ImageNet benchmarks across different backbone networks for image recognition . [ Weakness 2 : The difference from Xie et al .. ] Our methods ( ALFA , L-ALFA ) have several key differences from AdvProp in Xie et al . ( 1 ) Our methods leverage adversarial perturbations in the feature space to improve generalization , instead of generating computational expensive pixel-level perturbations on raw images ( Xie et al ) . ( 2 ) To efficiently learn an optimal strategy of perturbation injection , we further propose a learnable adversarial feature augmentation ( L-ALFA ) framework , which is capable of automatically adjusting the position and strength of introduced feature perturbations . This is not covered in Xie et al . ( 3 ) We do not use the auxiliary batch normalization , which is the main contribution in Xie et al . [ Weakness 3 : Nearly all results are CIFAR-10 . ] We respectfully do not agree . We verify our proposed ALFA on CIFAR-100 with ResNet-20s and ResNet-56s backbones ( Table 3 ) , on ImageNet with ResNet-18 , ResNet-50 , ResNet-101 , and ResNet-152 backbones ( Table 3 ) . We also report the ablation of PGD steps on ImageNet ( Table 4 ) . In addition , we validate L-ALFA on both CIFAR-100 and ImageNet ( Table 7 ) . Note that , in both Table 3 and Table 4 , we include the comparison with baselines . To further address your concern , we also provide the EfficientNet-B0 results on ImageNet here , Baseline ( 77.04 % ) vs. ALFA ( 77.48 % ) . Extensive experiments demonstrate that ALFA achieves consistent generalization improvement over multiple backbones and diverse datasets ."}, "1": {"review_id": "j6rILItz4yr-1", "review_text": "-- Paper summary -- The authors propose Adversarial Feature Augmentation ( ALFA ) , which augment features at hidden layers by adding adversarial perturbations . Where and how strongly the augmentation is conducted is automatically optimized via training . Experimental results show that the proposed method consistently improves the performance of baselines over several datasets and network architectures . -- Review summary -- Although the motivation of this study is clear , the proposed method is not appropriately designed along with the motivation . Moreover , its novely is merginal . I vote for rejection . -- Details -- Strength - The motivation is clear and seems reasonable . Training with adversarial perturbations is known to be effective but computationally expensive . It can be problematic when the model or training data is large-scale . - The proposed method consistently improves the performance of baselines over several datasets and network architectures . Weakness and concerns - Is the computational complexity of the proposed method really small ? Since the adversarial perturbation is computed for every layer , its computational complexity should be almost same with that of standard adversarial training . - The training objective shown in Eq . ( 3 ) is not reasonable . Since the norm of \\delta is upper-bounded by a certain constant \\epsilon , the effect of the adversarial perturbation can be reduced just by increasing the scale of features . Are features normalized ones ? - The optimization of \\eta in L-ALFA is not reasonable . Since min_\\eta comes after max_\\delta , L-ALFA should choose the layer that corresponds to the smallest increase of loss by adding adversarial perturbation . Therefore , this design minimizes the effect of the augmentation , which is contradictive to the motivation of introducing \\eta . Moreover , since \\epsilon is common for all layers , the optimal \\eta should be sensitive to the scale of features , which indicates that the performance of the proposed method would heavily depend on both how to initialize the model and whether any normalization is conducted in the model or not . - Marginal novelty . An idea of adversarially augmented features has been already presented in [ R1 ] . [ R1 ] `` Training Deep Neural Networks with Adversarially Augmented Features for Small-scale Training Datasets , '' IJCNN 2019 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your insightful comments . Below , we provide detailed responses to your concerns . [ Weakness 1 : Computational complexity . ] For ALFA , we only apply adversarial perturbation to a single layer , which allows ALFA to maintain cost efficiency even for deep networks . Note that the feature perturbation generation only requires backpropagation through a section of the network . The efficiency of L-ALFA , compared to pixel-based methods , mainly depends on the number of perturbed layers ( i.e. , the dimension of $ \\eta $ in equation 5 ) , and it is not necessary to perturb every layer . We agree that if there are hundreds of layers that need to be perturbed , L-ALFA may not be efficient enough . Fortunately , as shown in Table 8 , a moderate number of perturbed layers ( e.g. , dim ( $ \\eta $ ) =9 ) is enough to exhibit the effectiveness of L-ALFA . We provide an updated time comparison with ResNet-18 per training epoch here : AdvProp ( 123s ) vs. ALFA ( 28s ) vs. L-ALFA with dim ( $ \\eta $ ) =9 ( 91s ) vs. Standard training ( 23s ) . [ Weakness 2 : Epsilon upper-bound . ] As mentioned in Section 4 , we adopt unbounded perturbation generation without the epsilon constraint . We will update our manuscript to clarify the confusion . Thanks for pointing out the missing details . Yes , we apply feature perturbations to the normalized feature from batch normalization . To further address your concern , here we report the magnitude of crafted feature perturbation in practice , which steadily stays in the range of 0.97 to 1.10 under the $ \\ell_2 $ norm . [ Weakness 3 : The optimization of $ \\eta $ in L-ALFA is not reasonable . ] In general , our design philosophy of L-ALFA is adversarial training . It maximizes the training objective to craft feature perturbation and minimizes the training objective to optimize model weights and the learnable \u201c hyperparameter \u201d ( e.g. , $ \\eta $ in equation 5 ) . Note that there is a constraint $ \\mathcal { P } $ ball of $ \\eta $ , where the summation of all $ \\eta_i $ is equal to 1 . This constraint prevents the minimization to hurt the effect of feature augmentation and avoids trivial solutions ( i.e. , $ \\eta $ =0 ) . Moreover , as mentioned in the answer to Weakness 2 , we adopt unbound perturbation generation and batch normalization in all networks . In addition , as shown in Table 6 , we conduct five independent runs with different random seeds , and the performance is very stable , i.e. , 94.65 % ( +- 0.08 % ) . [ Weakness 4 : IJCNN 2019 paper . ] Thanks for pointing out the literature . Our work is different from this IJCNN paper in three aspects : motivation , methods , and experiment design . ( 1 ) Motivation . IJCNN crafts strong adversarial perturbations to fool the model ( \u201c the perturbation is designed to be adversarial , which means that they are designed to significantly change the output of the network. \u201d ) . Our method aims to generate moderate adversarially perturbed features to improve the generalization rather than to fool the model . ( 2 ) Methods . IJCNN injects adversarial perturbations in a randomly selected layer by virtual adversarial training and ad-hoc coefficient layers . We utilize PGD-AT to craft gradient-based perturbations , and further design the learnable L-ALFA framework to automatically learn the location and strength of generated feature perturbations . ( 3 ) Experiments . IJCNN only focuses on small-scale datasets ( MNIST , NORB , CIFAR-10 ) with networks no more than eight layers . Our experiments are conducted on diverse datasets from small-scale CIFAR-10 and CIFAR-100 to large-scale ImageNet , with much larger network architectures than IJCNN , including ResNet-20s/56s , ResNet-18/-50/-101/-152 . We hope that you can raise your score if you find our answers that address your questions . Thank you !"}, "2": {"review_id": "j6rILItz4yr-2", "review_text": "Pros : Method is clearly stated , and the learning adaptive perturbation strength seems novel . Provided experimental results for large datasets like ImageNet . Major Concerns : The method proposes crafting adversarial perturbation at different layers of the network . A small change in features space may correspond to a large change in input space for neural networks , hence it is doubtful whether the perturbations crafted will be meaningful . The method which attempts to cause perturbations in the feature space also ensures that perturbed image is similar to original image ( imperceptibility constraint ) [ 3 ] . The authors mention use of unbounded perturbation size in section 4.2 which goes against the goal of adversarial training ( i.e.same prediction in the neighborhood of the image ) . Also , if the perturbation is unbounded then there is no requirement of Projection step in PGD ( Projection is done for enforcing the boundedness of perturbation ) . Could the authors please clarify this ? The intuition why this method works is also not too clear , as it has been shown in [ 1 ] that adversarial training leads to drop in standard accuracy . But in this paper adversarial training has been shown to increase standard accuracy even without using any adaptive parameters ( like batch normalization layers used in AdvProp [ 2 ] ) . This simply goes against the established facts . It is hard to distill what settings work well across different datasets . As seen in Table 5 , for CIFAR-10 , changing from 1 step PGD to 5 step PGD increases accuracy , whereas it decreases in case of ImageNet . Also there exist only a certain range of step size values for which standard accuracy increases . Could the authors kindly offer additional theoretical or intuitive explanations to clarify the same ? This would help substantially improve the submission . Other questions : The authors could provide info on fooling achieved by crafted adversaries which is required for confirming the success of adversary creation . Also , the adversarial training method proposed in this paper shows minimal increase in adversarial robustness which also supplements the need for details on fooling rate . Typically adversarial training increases its robustness [ 5 ] , which is not observed with the proposed method . Could the authors clarify this ? The accuracy reported for the ImageNet models is lower than Torchvision models [ 4 ] as the authors use the same code as PyTorch repo it is unexpected . This may be due to the use of 10 % training data for validation which is not required in ImageNet , as it already has a validation set . ( This is important as the magnitude of improvements are not very large over the standard accuracy in case of ImageNet and is even smaller when compared with Torchvision models ) . The Torchvision ResNet 50 model obtains 76.15 % accuracy which is comparable to ALFA ( 76.23 % ) ( present in Table 3 ) . As the perturbations are calculated for each layer , the complexity of the method would increase with large networks like ResNet-152 . So it is unclear if the proposed method will continue to be cost efficient in comparison to pixel-based methods . Also , could the authors clarify the network architecture used for comparison of the time complexity in Section 4.3 for ALPHA and AdvProp [ 2 ] ? Overall , this seems a very complex method without any theoretical grounding to increase the generalization performance . [ 1 ] Zhang , H. , Yu , Y. , Jiao , J. , Xing , E. , Ghaoui , L.E . & Jordan , M .. ( 2019 ) . Theoretically Principled Trade-off between Robustness and Accuracy . Proceedings of the 36th International Conference on Machine Learning , in PMLR 97:7472-7482 [ 2 ] Xie , C. , Tan , M. , Gong , B. , Wang , J. , Yuille , A. L. , & Le , Q. V. ( 2020 ) . Adversarial examples improve image recognition . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ( pp.819-828 ) . [ 3 ] Ganeshan , A. , & Babu , R. V. ( 2019 ) . FDA : Feature disruptive attack . In Proceedings of the IEEE International Conference on Computer Vision ( pp.8069-8079 ) . [ 4 ] https : //pytorch.org/docs/stable/torchvision/models.html [ 5 ] Madry , A. , Makelov , A. , Schmidt , L. , Tsipras , D. , & Vladu , A . ( 2018 , February ) . Towards Deep Learning Models Resistant to Adversarial Attacks . In International Conference on Learning Representations .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your insightful comments . Below , we provide detailed responses to your concerns . [ Major Concern 1 : It is doubtful whether the perturbations crafted will be meaningful . ] Our work aims to enhance image recognition on the clean test set ; therefore , we do not have the constraint that the perturbed image needs to be similar to the original image ( i.e. , the imperceptibility constraint ) . More specifically , our method performs \u201c adversarial data augmentation \u201d , with a focus on leveraging adversarial perturbations in the feature space for regularization , rather than generating actual adversarially perturbed images . We will make this clear in the revision . [ Major Concern 2 : Unbounded perturbation . ] Our generated perturbations are applied to intermediate feature embeddings . Since we only care about the final clean accuracy , there is no constraint that we need to craft adversarial perturbations in the neighborhood of the image . For the unbounded perturbation generation , we do not adopt the projection process . In practice , the magnitude of crafted feature perturbation steadily stays in a range from 0.97 to 1.10 under the $ \\ell_2 $ norm . We will make this clear in the revision . [ Major Concern 3 : Clarify the intuition . ] Our method is not against the facts in [ 1 ] . TRADES [ 1 ] found that vanilla adversarial training on the input pixel space leads to standard accuracy degradation . AdvProp [ 2 ] utilizes adversarial examples with an auxiliary batch normalization to improve standard accuracy . Our work is different in that we introduce adversarial perturbation to feature embeddings rather than raw pixels , which improves the standard accuracy of image recognition . Our method is also well-motivated since adversarial feature augmentation has been widely proved beneficial for model generalization , such as [ 3 ] in computer vision , [ 4,5 ] in natural language processing , and [ 6 ] in vision+language tasks . [ Major Concern 4 : Insights of hyperparameter selection . ] As mentioned by R2 , to obtain standard accuracy improvement , there is a safe zone of hyperparameter selection for perturbation strength . The immoderate weak perturbation yields marginal effects , while excessive strong perturbations may lead to over-smoothed decision boundaries and incur performance degradation . Thus , learning a proper perturbation strength is important for our proposed ALFA . Moreover , in order to avoid laborious tuning across diverse datasets and backbones , we design learnable adversarial feature augmentation ( L-ALFA ) to automatically learn the best hyperparameter configurations . [ 1 ] Theoretically Principled Trade-off between Robustness and Accuracy , ICML 2019 . [ 2 ] Adversarial examples improve image recognition , CVPR 2020 . [ 3 ] Improved Sample Complexities for Deep Networks and Robust Classification via an All-Layer Margin , ICLR 2020 . [ 4 ] FreeLB : Enhanced Adversarial Training for Natural Language Understanding , ICLR 2020 . [ 5 ] SMART : Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization , ACL 2020 . [ 6 ] Large-Scale Adversarial Training for Vision-and-Language Representation Learning , NeurIPS 2020 ."}, "3": {"review_id": "j6rILItz4yr-3", "review_text": "In this paper , the authors suggest a method for adversarial feature-level augmentation , mainly framed as an approach to improve the clean-set accuracy rather than adversarial example robustness . The authors also propose a learnable version ( LALFA ) , to automatically learn the location and strength of the perturbations . Overall , I think this is an interesting paper that can be considered for publication at ICLR . The following elaborates it further : Strengths : * Even though the adversarial feature augmentations at the feature level are not unprecedented , but the learnable tuning of the location/strength is an interesting approach that can potentially avoid expensive hyperparameter optimization . * The shown improvements of the best-achieved models seem consistent over the baselines , across different datasets and models . Weaknesses : * The obtained improvements are moderate on smaller networks and become marginal with deeper counterparts . * The performance and the offered advantages seem quite sensitive to the choice of hyperparameters in ALFA ( Tables 4 and 5 ) . I wonder , at least , how stable/conclusive the comparisons are when transferring the selected model from one set to another , e.g.validation to test . Further detailed comments : * `` L-ALFA saves toilsome tuning by automatically adjusting the strength and locations of adversarial feature augmentations '' = > It is fair to note that this is achieved at the expense of introducing a new hyper-parameter to tune , namely \\alpha . * An ablation study on the L1 regularization could have been useful . * In Table 1 and some of the other numerical comparisons , except for Table 6 , where standard deviations are reported : I wonder how significant are these comparisons ? Are the differences meaningful when considering the intra-experiment variations ? * Insights on why MoEx , one of the three baselines compared against , is performing worse than both random noise and normal training , would be helpful . * Equation ( 1 ) defines \\delta \\in B_\\epsilon ( x ) and uses it as an offset to x : f ( x+\\delta ; \\theta ) . Later B_\\epsilon ( x ) is defined as `` The norm ball B_\\epsilon is centered at x with radius \\epsilon '' : These are not consistent , if \\delta is used as an offset , it can not be sampled around x . * \\mathcal { L } _ { at } has been used inconsistency across equation ( 1 ) and equations ( 3 ) and ( 4 ) , on the first argument ; f_i ( x ; \\theta^ { ( i ) } +\\delta^ { ( i ) } ) in equation ( 3 ) and ( 4 ) misses the second part of the network , transforming the intermediate feature maps to the required predictions . * \\mathcal { L } _ { at } is used in equation 1 , but is elaborated after equation 2 . * Defining both F ( x ; \\theta ) and f ( x ; \\theta ) is referring to the neural network , and its output seems unnecessary and confusing ; besides , F ( x ; \\theta ) is never referenced before . * f ( x+\\delta , \\theta ) = > f ( x+\\delta ; \\theta ) * Adding a row in table 7 , representing the results from ALFA , will make the direct comparison between ALFA and L-ALFA easier . * `` ResNet-18 has ... and twenty convolutional layers '' = > It has seventeen convolutional layers , I think . Please clarify on dim ( \\eta ) = 20 in Table 8 . Are some of the pooling layers outputs also taken ? * typo : `` a unduly ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your insightful comments . Below , we provide detailed responses to your concerns . [ Weakness 1 : Moderate and Marginal Improvements . ] We respectfully do not agree that the improvements are marginal . Our proposed methods ALFA , as an implicit data augmentation or regularization , achieves consistent generalization improvements over strong baselines on CIFAR-10 , CIFAR-100 , and ImageNet benchmarks across different backbone networks for image recognition . Note that , these datasets are highly competitive , and it is impressive to obtain further improvements based on baselines \u2019 almost saturated performance . [ Weakness 2 : The Stability of Hyperparameters from Validation to Test . ] As mentioned in Section 4.1 , the original training dataset is randomly split into 90 % training and 10 % validation in our experiments . The early stopping technique is applied to find the top-performing checkpoints on the validation set . Then , the selected checkpoints are evaluated on the test set and report the performance . From our observations , the hyperparameters are quite stable from validation to test sets . To further address your concerns , we provide the code as the additional supplementary material that will be uploaded before the end of the rebuttal period ."}}