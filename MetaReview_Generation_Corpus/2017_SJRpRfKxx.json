{"year": "2017", "forum": "SJRpRfKxx", "title": "Recurrent Mixture Density Network for Spatiotemporal Visual Attention", "decision": "Accept (Poster)", "meta_review": "The paper describes a model for video saliency prediction using a combination of spatio-temporal ConvNet features and LSTM. The proposed method outperforms the state of the art on the saliency prediction task and is shown to improve the performance of a baseline action classification model.", "reviews": [{"review_id": "SJRpRfKxx-0", "review_text": "The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition. quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated? You claim that your \"results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues. clarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used. At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features. Other issues: You cite K\u00fcmmerer et. al 2015 as a model which \"learns ... indirectly rather than from explicit information of where humans look\", however the their model has been trained on fixation data using maximum-likelihood. Apart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction. If the evaluation issues given above are sorted out, I will happily improve my rating.", "rating": "7: Good paper, accept", "reply_text": "As mentioned in the pre-review answer \u201c Answer to Central Bias \u201d , the trained central bias is reported in Table 1 while Table 2 reports the manually-created central bias from Mathe & Sminchisescu , 2015 . The trained central bias is used as a comparative simple baseline that does not consider the content of the frames . Human performance is computed as described below in the pre-review answer \u201c Answer to empirical distribution \u201d . Since the human performance uses fixations to create the saliency maps , we believe that Mathe & Sminchisescu , 2015 tuned the parameters \\sigma and p ( Sec.4.1 ) .However , this information is missing from their paper . We clarified these points in the new version of the manuscript ( last paragraph of Sec.4.1 ) .Our results on Table 2 are in between the human performance and the trained central bias . We agree that AUC is not the best metric for evaluation because of the saturation problem . In fact we used also NSS , CC and Sim to evaluate the variants of our method . However , we needed to use the AUC to compare with the methods of Mathe & Sminchisescu , 2015 . About the evaluation in Table 3 , we wanted to evaluate two different ways to merge the saliency map . We thank the reviewer for pointing out that one relevant baseline is missing : CONV5 when multiplying the input with the saliency map . We are in the process of running this experiment . We will report the result as soon as we obtain it . K\u00fcmmerer et . al 2015 reference was misplaced by mistake . In fact , their model is trained on fixations for saliency prediction in images . We removed it from the second paragraph of Sec.2 and correctly placed it in the third paragraph of Sec.2 ."}, {"review_id": "SJRpRfKxx-1", "review_text": "This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns, thus helping to prune irrelevant information from the video and improve action recognition. The work is interesting and has shown state-of-the-art results on predicting human attention on action videos. It has also shown promise for helping action clip classification. The paper would benefit from a discussion on the role of context in attention. For instance, if context is important, and people give attention to context, why is it not incorporated automatically in your model? One weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair. The attention weighted feature maps in fact reduce the classification performance, and only improve performance when doubling the feature and associated model complexity by concatenating the weighted maps with the original features. Is there a way to combine the context and attention without concatenation? The rational for concatenating the features extracted from the original clip, and the features extracted from the saliency weighted clip seems to contradict the initial hypothesis that `eliminating or down-weighting pixels that are not important' will improve performance. The authors should also mention the current state-of-the-art results in Table 4, for comparison. # Other comments: # Abstract - Typo: `mixed with irrelevant ...' \"Time consistency in videos ... expands the temporal domain from few frames to seconds\" - These two points are not clear, probably need a re-write. # Contributions - 1) `The model can be trained without having to engineer spatiotemporal features' - you would need to collect training data from humans though.. # Section 3.1 The number of fixation points is controlled to be fixed for each frame - how is this done? In practice we freeze the layers of the C3D network to values pretrained by Tran etal. What happens when you allow gradients to flow back to the C3D layers? Is it not better to allow the features to be best tuned for the final task? The precise way in which the features are concatenated needs to be clarified in section 3.4. Minor typo: `we added them trained central bias'", "rating": "6: Marginally above acceptance threshold", "reply_text": "The context and attention parts are capturing slightly different information ( as confirmed empirically in our study ) . The context features represent the video in its entirety . Therefore they may contain elements from the background , which may be useful for action categorization but at times also potentially distracting . Instead , the attention part focuses only on the most salient spatiotemporal volumes in the video . We agree that there are many other ways to use the saliency maps produced by our model . For example , the saliency map could be used to sample higher-resolution crops of the frames . We opted for the simplest model in order to show that what is contributing to the improvement in action categorization is truly coming from the saliency map and not from a more complex model . \u201c Comparison between [ ( 1 ) , ( 2 ) ] and ( 3 ) seems unfair. \u201d As we mentioned in the pre-review answer \u201c Answer to few questions , comments \u201d , we ran an experiment with PCA to yield the same feature dimensionality as ( 1 ) in order to have a more fair comparison . The resulting testing mAP of this compressed descriptor ( using both context and attention ) is 51.82 % , which is quite a bit better than method ( 1 ) ( based on context only ) , despite having the same dimensionality . We added this relevant result to the paper ( last paragraph in page 8 ) . The number of fixations are randomly subsampled ( first paragraph , Sec.3.1 ) if greater than A. Conversely , they are randomly duplicated if less than A . In our experiments , we froze the layers of the C3D . We ran several fine-tuning experiments for action recognition using the Hollywood2 dataset , however we have never obtained any significant improvement . We think that this might be due to the small size of the dataset and to the fact that the C3D features are already representative and general since they were trained on a huge dataset . This discouraged us to run fine-tuning experiments for saliency prediction . About feature concatenation : for each clip , we stacked the d-dimensional context feature vector on top of the d-dimensional attention-based feature vector to form a ( 2 * d ) -dimensional descriptor ."}, {"review_id": "SJRpRfKxx-2", "review_text": "This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model. Finally, the visual attention map is generated from the Gaussian mixture model. Overall, the idea in this paper is reasonable and the paper is well written. RNN/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN/LSTM for problems where the output is continuous like in this paper. The experimental results have demonstrated the effectiveness of the proposed approach. In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets. It also shows improvement over baselines (e.g. C3D + SVM) on the action recognition task. My only \"gripe\" of this paper is that this paper is missing some important baseline comparisons. In particular, it does not seem to show how the \"recurrent\" part help the overall performance. Although Table 2 shows RMDN outperforms other state-of-the-art, it might be due to the fact that it uses strong C3D features (while other methods in Table 2 use traditional handcrafted features). Since saliency prediction is essentially a dense image labeling problem (similar to semantic segmentation). For dense image labeling, there has been lots of methods proposed in the past two years, e.g. fully convolution neural network (FCN) or deconvnet. A straightforward baseline is to simply take FCN and apply it on each frame. If the proposed method still outperforms this baseline, we can know that the \"recurrent\" part really helps. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We agree with the reviewer that it is instructive to separately assess the importance of the recurrent part of our model . However , we believe that comparing our model with a fully convolutional network or a deconvolutional network would not provide an answer to this question , since these are methods that differ considerably from our proposed model , not only in their lack of recurrence . If the reviewer agrees , we propose instead to run a more informative ablation study , involving the removal of the recurrent link between time t-1 and time t from our model . This will provide a more direct answer concerning the usefulness of recurrency ."}], "0": {"review_id": "SJRpRfKxx-0", "review_text": "The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition. quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated? You claim that your \"results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues. clarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used. At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features. Other issues: You cite K\u00fcmmerer et. al 2015 as a model which \"learns ... indirectly rather than from explicit information of where humans look\", however the their model has been trained on fixation data using maximum-likelihood. Apart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction. If the evaluation issues given above are sorted out, I will happily improve my rating.", "rating": "7: Good paper, accept", "reply_text": "As mentioned in the pre-review answer \u201c Answer to Central Bias \u201d , the trained central bias is reported in Table 1 while Table 2 reports the manually-created central bias from Mathe & Sminchisescu , 2015 . The trained central bias is used as a comparative simple baseline that does not consider the content of the frames . Human performance is computed as described below in the pre-review answer \u201c Answer to empirical distribution \u201d . Since the human performance uses fixations to create the saliency maps , we believe that Mathe & Sminchisescu , 2015 tuned the parameters \\sigma and p ( Sec.4.1 ) .However , this information is missing from their paper . We clarified these points in the new version of the manuscript ( last paragraph of Sec.4.1 ) .Our results on Table 2 are in between the human performance and the trained central bias . We agree that AUC is not the best metric for evaluation because of the saturation problem . In fact we used also NSS , CC and Sim to evaluate the variants of our method . However , we needed to use the AUC to compare with the methods of Mathe & Sminchisescu , 2015 . About the evaluation in Table 3 , we wanted to evaluate two different ways to merge the saliency map . We thank the reviewer for pointing out that one relevant baseline is missing : CONV5 when multiplying the input with the saliency map . We are in the process of running this experiment . We will report the result as soon as we obtain it . K\u00fcmmerer et . al 2015 reference was misplaced by mistake . In fact , their model is trained on fixations for saliency prediction in images . We removed it from the second paragraph of Sec.2 and correctly placed it in the third paragraph of Sec.2 ."}, "1": {"review_id": "SJRpRfKxx-1", "review_text": "This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns, thus helping to prune irrelevant information from the video and improve action recognition. The work is interesting and has shown state-of-the-art results on predicting human attention on action videos. It has also shown promise for helping action clip classification. The paper would benefit from a discussion on the role of context in attention. For instance, if context is important, and people give attention to context, why is it not incorporated automatically in your model? One weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair. The attention weighted feature maps in fact reduce the classification performance, and only improve performance when doubling the feature and associated model complexity by concatenating the weighted maps with the original features. Is there a way to combine the context and attention without concatenation? The rational for concatenating the features extracted from the original clip, and the features extracted from the saliency weighted clip seems to contradict the initial hypothesis that `eliminating or down-weighting pixels that are not important' will improve performance. The authors should also mention the current state-of-the-art results in Table 4, for comparison. # Other comments: # Abstract - Typo: `mixed with irrelevant ...' \"Time consistency in videos ... expands the temporal domain from few frames to seconds\" - These two points are not clear, probably need a re-write. # Contributions - 1) `The model can be trained without having to engineer spatiotemporal features' - you would need to collect training data from humans though.. # Section 3.1 The number of fixation points is controlled to be fixed for each frame - how is this done? In practice we freeze the layers of the C3D network to values pretrained by Tran etal. What happens when you allow gradients to flow back to the C3D layers? Is it not better to allow the features to be best tuned for the final task? The precise way in which the features are concatenated needs to be clarified in section 3.4. Minor typo: `we added them trained central bias'", "rating": "6: Marginally above acceptance threshold", "reply_text": "The context and attention parts are capturing slightly different information ( as confirmed empirically in our study ) . The context features represent the video in its entirety . Therefore they may contain elements from the background , which may be useful for action categorization but at times also potentially distracting . Instead , the attention part focuses only on the most salient spatiotemporal volumes in the video . We agree that there are many other ways to use the saliency maps produced by our model . For example , the saliency map could be used to sample higher-resolution crops of the frames . We opted for the simplest model in order to show that what is contributing to the improvement in action categorization is truly coming from the saliency map and not from a more complex model . \u201c Comparison between [ ( 1 ) , ( 2 ) ] and ( 3 ) seems unfair. \u201d As we mentioned in the pre-review answer \u201c Answer to few questions , comments \u201d , we ran an experiment with PCA to yield the same feature dimensionality as ( 1 ) in order to have a more fair comparison . The resulting testing mAP of this compressed descriptor ( using both context and attention ) is 51.82 % , which is quite a bit better than method ( 1 ) ( based on context only ) , despite having the same dimensionality . We added this relevant result to the paper ( last paragraph in page 8 ) . The number of fixations are randomly subsampled ( first paragraph , Sec.3.1 ) if greater than A. Conversely , they are randomly duplicated if less than A . In our experiments , we froze the layers of the C3D . We ran several fine-tuning experiments for action recognition using the Hollywood2 dataset , however we have never obtained any significant improvement . We think that this might be due to the small size of the dataset and to the fact that the C3D features are already representative and general since they were trained on a huge dataset . This discouraged us to run fine-tuning experiments for saliency prediction . About feature concatenation : for each clip , we stacked the d-dimensional context feature vector on top of the d-dimensional attention-based feature vector to form a ( 2 * d ) -dimensional descriptor ."}, "2": {"review_id": "SJRpRfKxx-2", "review_text": "This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model. Finally, the visual attention map is generated from the Gaussian mixture model. Overall, the idea in this paper is reasonable and the paper is well written. RNN/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN/LSTM for problems where the output is continuous like in this paper. The experimental results have demonstrated the effectiveness of the proposed approach. In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets. It also shows improvement over baselines (e.g. C3D + SVM) on the action recognition task. My only \"gripe\" of this paper is that this paper is missing some important baseline comparisons. In particular, it does not seem to show how the \"recurrent\" part help the overall performance. Although Table 2 shows RMDN outperforms other state-of-the-art, it might be due to the fact that it uses strong C3D features (while other methods in Table 2 use traditional handcrafted features). Since saliency prediction is essentially a dense image labeling problem (similar to semantic segmentation). For dense image labeling, there has been lots of methods proposed in the past two years, e.g. fully convolution neural network (FCN) or deconvnet. A straightforward baseline is to simply take FCN and apply it on each frame. If the proposed method still outperforms this baseline, we can know that the \"recurrent\" part really helps. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We agree with the reviewer that it is instructive to separately assess the importance of the recurrent part of our model . However , we believe that comparing our model with a fully convolutional network or a deconvolutional network would not provide an answer to this question , since these are methods that differ considerably from our proposed model , not only in their lack of recurrence . If the reviewer agrees , we propose instead to run a more informative ablation study , involving the removal of the recurrent link between time t-1 and time t from our model . This will provide a more direct answer concerning the usefulness of recurrency ."}}