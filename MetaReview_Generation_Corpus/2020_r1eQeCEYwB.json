{"year": "2020", "forum": "r1eQeCEYwB", "title": "GRAPH ANALYSIS AND GRAPH POOLING IN THE SPATIAL DOMAIN", "decision": "Reject", "meta_review": "The authors identify a limitation of aggregating GNNs, which is that global structure can be mostly lost. They propose a method which combines a graph embedding with the spatial convolution GNN and show that the resulting GNN can better distinguish between similar local structures. \n\nThe reviewers were mixed in their scores. The proposed approach is clearly motivated and justified and may be relelvant for some graphnet researchers, but the approach is only applicable in some circumstances - in other cases it may be desirable to ignore global structure. This, plus the high computational complexity of the proposed approach, mean that the significance is weaker. Overall the reviewers felt that the contribution was not significant enough and that the results were not statistically convincing.  Decision is to reject.", "reviews": [{"review_id": "r1eQeCEYwB-0", "review_text": "In this work the authors point out an issue related to graph neural networks. Specifically, if two nodes, that may be far apart in the graph, may be represented as (almost) the same vector. This is simply because when no features/labels are associated with nodes, and the local structure around those two nodes is very similar then the local aggregation of information will result in a similar representation. Therefore the authors introduce an embedding first of the graph in the Euclidean space using DeepWalk and then use this embedding in combination with the design of a CNN. The authors propose a pooling method that outperforms several state-of-the-art pooling techniques on real data. Overall, the empirical results are supportive of the fact that the proposed method can help improve the performance of GNNs. Overall I found the results of this paper to be weak, but nonetheless the paper is well-written and contains some interesting ideas. Hence my rating. Some questions follow. - While the authors call this as an \"issue\" it is more like a feature of these methods. For instance, in \"RolX: Structural Role Extraction & Mining in Large Graphs\" by Henderson et al. this \"issue\" could turn out to be an interesting feature of the GNNs in the sense that these nodes may have a similar (structural) role. It would be nice to have a short discussion related to this line of research in social networks' analysis. - Some components of the CNN (e.g., node sampling) could be done using well-developed tools for sampling matrices from numerical linear algebra, or by introducing some randomness when sampling a node as in kmeans++. - Graph downsampling appears to be an expensive operation. Can you please comment on the running times? The issue of scalability is not discussed, and the reader cannot easily infer to what sizes this method can scale up to. - Using other graph tasks, that are classical but also more challenging (e.g., learning 2-connected components of a graph just to mention something that comes up) would be interesting to see in Section 5.2. - It would have been interesting to see the effect of the embedding step on the accuracy on the real data. E.g., using node2vec or standard spectral embeddings. [Edit: The authors have replied to my comments, and the other reviewers' comments in great detail. Therefore I upgrade my score.] ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the comments . 1- \u201c Overall I found the results of this paper to be weak \u201d : Table 1 and Table 2 indicate that the improvements in the results are significant for both real and synthetic data . 2- \u201c Some components of the CNN ( e.g. , node sampling ) could be done using well-developed tools for sampling matrices from numerical linear algebra , or by introducing some randomness when sampling a node as in kmeans++ \u201d : Thanks for this suggestion . We are well aware of column/row sampling techniques and we considered several sampling methods as our candidates including the one used in K-means++ and SRS [ arXiv:1705.03566 ] . The farthest data point sampling method used in our pooling method ( which is very similar to the sampling techniques used in K-means ++ ) ensures that the sampled embedding vectors cover the spatial distribution of all the embedding vectors and this is all we expect the sampling method to do . 3- \u201c Graph downsampling appears to be an expensive operation. \u201d The complexity of the sampling method is linear with the number of sampled embedding vectors ( O ( m * n * de ) where m is the number of sampled vectors and de is the dimension of the embedding vectors ) . In addition , different from node classification , in the graph classification task , we mostly do not have large graphs ( mostly less than 100 nodes ) . In the revised paper , we described the computation complexity of the node sampling method . 4- \u201c It would have been interesting to see the effect of the embedding step on the accuracy on the real data . E.g. , using node2vec or standard spectral embeddings. \u201d \u0651In our initial experiments , we used other embedding methods such as [ arXiv:1710.02971 ] but the results were not much different . In the final version , we will report the results with several different embedding methods . 5- \u201c While the authors call this as an `` issue '' it is more like a feature of these methods . For instance , in `` RolX : Structural Role Extraction & Mining in Large Graphs '' by Henderson et al.this `` issue '' could turn out to be an interesting feature of the GNNs in the sense that these nodes may have a similar ( structural ) role. \u201d It depends on the application that we deal with . In the graph classification problem , we clearly showed that it is not a desirable feature and this feature can make the GNN unable to extract discriminative features . Moreover , our approach addresses another problem of GNNs raised by ( Xu et al . ( ICLR 2019 ) ) : GNNs can map even different local structures to same feature vectors . Since we include the information about the location of the nodes , we help the GNN to distinguish both similar and different local structures . We cited \u201c RolX : Structural Role Extraction & Mining in Large Graphs \u201d and clarified that the point that in some applications mapping node with similar local structures to similar feature vectors can be desirable . We would like to reiterate that our paper is the first work which address this problem and the presented results ( Table 1 and Table2 ) clearly show how effective is the presented solution ."}, {"review_id": "r1eQeCEYwB-1", "review_text": "============ After rebuttal ============ I thank the authors for carefully discussing the points of my review. I have upgraded the score to marginal acceptance (6). ============ Original review ============ In this paper, the authors identify a shortcoming of existing GNN architectures for graph classification tasks -- specifically, the fact that, in the featureless regime, the graph convolutional layers rely on propagating very rudimentary structural information, making it hard (or impossible) to distinguish graphs with similar local structure. To fix the problem, the authors propose to augment the input feature space with graph-structural embeddings (computed by an algorithm like DeepWalk), and processing those in parallel with any other input features available. On existing real-world datasets, as well as synthetic datasets carefully constructed to illustrate this phenomenon, the proposed pipeline matches or exceeds the version without the structural embedding inputs. Further, the authors note that the structural embeddings could be used to propose a novel graph pooling method -- one which attempts to preserve as diverse structural feature sets as possible. It is shown that this method is competitive to other differentiable pooling methods, like DiffPool and Graph U-Nets. Lastly, the authors demonstrate that the addition of pooling layers does not help baseline GNNs on the synthetically constructed tasks, as the fundamental issue of handling similar local structures is still not addressed. I believe that the paper clearly exposes and proposes a nice idea which could hold great potential, and which can be useful to graph representation learning practitioners. I am particularly happy with the design of the synthetic experiments. However, I find that, in its current form, the manuscript is narrowly below the bar for a venue like ICLR. Comments: * The observation that existing GNN layers may struggle with distinguishing featureless graphs is not particularly novel. It's largely the centerpiece of the (already cited) work of Xu et al. (ICLR 2019), and I believe that its relevance and relation to the authors' work should be better stressed in the related work section. * The usage of DeepWalk to encode structural information (and even to be used as initial features for a GNN) is, ultimately, also not necessarily a novel idea. At least, it's something that should be clear to any expert GNN practitioner already: if useful features are missing from the graph, a method like DeepWalk (if applicable; see below!) could be a go-to method for obtaining such features. In its current form, I don't see that the authors are proposing anything substantially architecturally novel, and their contribution is primarily on the data/feature engineering side. * The above point is not necessarily problematic, but if the aim is to stress the importance of the architectural novelties of the proposed GNN-ESR model more, and not just the added features, I would recommend the authors to perform a few ablations: e.g. seeing how well would processing a concatenation of E and F in the same GNN layer perform. * Many of the standard graph classification datasets are known to be noisy and unreliable (see e.g. Luzhnica, Day and Li\u00f2 (ICML GraphReasoning Workshop 2019). This means that it is a must to report error bars of the cross-validation experiments. It's hard to say that many of the improvements depicted here are statistically significant otherwise. * I have concerns about the computational complexity, or even feasibility, of using DeepWalk-like methods in the general case, e.g. for node classification. Namely, if such layers are to be applied in inductive settings (with nodes gradually added to graphs), one would require re-running DeepWalk every time a new node is added. The authors should comment on this adequately, and perhaps discuss the feasibility of other unsupervised embedding techniques for obtaining the e-vectors -- such as VGAE (Kipf and Welling, NIPS BDL 2016), GraphSAGE (Hamilton et al., NIPS 2017), Graph2Gauss (Bojchevski and G\u00fcnnemann, ICLR 2018) or DGI (Veli\u010dkovi\u0107 et al., ICLR 2019). * While I find the proposed pooling method interesting (and more grounded in the graph's structural features than other proposed works), I find that there are many potential limitations to be discussed. For example, the fact that we start from a random first index means that we cannot rely on the obtained pooling to always be the same -- could this cause undesirable variance at test time? Furthermore, the downsampling from A^3 is a sure-fire way to obtain dense graphs after the first pooling -- potentially severely limiting the applicability of the method for large graphs. In my opinion, the authors should appropriately comment on these and perform ablations against pooling with A and A^2 (as was done in Graph U-Nets). It should also be interesting to note that there exist other structurally-informed pooling methods; see e.g. the Clique pooling method from Luzhnica, Day and Li\u00f2 (ICLR RLGM 2019). Given all of the above, I recommend (marginal) rejection, but am open to improving my score if the authors appropriately address the aforementioned comments. Some minor comments and thoughts: * The paper has several typos and grammar issues, and a typo pass is highly encouraged to aid clarity; * The \"attention mechanism\" of Equation (5--6) seems to be nonparametric? If so, it might be interesting to compare with a version that features learnable queries, e.g. using the Transformer-style attention. * In Equation (3), should the first min actually be a max? As we're maximising the overall minimum distances between topological features. * I'm not sure that the paper is anywhere clear on what's the exact GNN layer being used. Could this be clarified and made more explicit? It's critical to reproducibility. * I find it curious that the authors needed to resort to using batch normalisation---I usually found it to either have no meaningful effect on the results on the graph classification benchmarks, or it made results worse. Can the authors comment on this decision? * The idea to concatenate output of all convolutional layers is heavily resembling Jumping Knowledge networks (Xu et al., ICML 2018), and I believe they should be appropriately cited.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the comments . 1- \u201c Xu et al . ( ICLR 2019 ) \u201d : Please note that the approach presented in ( Xu et al . ( ICLR 2019 ) ) does not solve the problem that we address in our paper . The authors of ( Xu et al . ( ICLR 2019 ) ) want to make sure that if two local structures are DIFFERENT , the GNN maps them to different vectors in the feature space and they showed that the sum aggregator satisfies this requirement . The solution that we proposed prevents the GNN from mapping even SIMILAR local structures to the same feature vectors because we make the GNN aware of the location of the nodes . In addition , in all our experiments we used the sum aggregator . The presented experiments clearly show that ( Xu et al . ( ICLR 2019 ) ) can not solve the problem that we addressed in our paper . We clarified this difference in the new paper . 2- \u201c their contribution is primarily on the data/feature engineering side \u201d : We believe that it is an important contribution that our paper shows that the fact that GNN maps similar local structures to same feature vectors is problematic . In addition , we propose a solution to this important problem and our solution is simple and effective ( it also addresses the problem raised in Xu et al . ( ICLR 2019 ) in a more effective way ) . Moreover , the contributions of our paper is not limited to GNN-ESR . We present a new graph pooling method too . 3- \u201c The above point is not necessarily problematic but \u2026 seeing how well would processing a concatenation of E and F in the same GNN layer perform \u201d : We do not claim that we propose a novel architecture for GNNs ( except the new pooling layer ) . In Figure 1 , if we set k1=0 , it is equivalent to the scenario in which we E and F are concatenated . Our initial experiments with few data-sets showed that k=1 is slightly better than k=0 . We will regenerate the results with k=0 and will report them in the final version . 4- \u201c This means that it is a must to report error bars of the cross-validation experiments .. \u201d : Thanks for your suggestion . We added the error bars to the results in the new paper . 5- \u201c computational complexity , or even feasibility , of using DeepWalk-like methods in the general case \u2026 or DGI ( Veli\u010dkovi\u0107 et al. , ICLR 2019 ) . \u201d In this paper , we focus on graph classification . For online cases , there are some online embedding methods . In addition , Deepwalk is scalable to large graphs . In the paper , we mention that the motivation for using the embedding vectors is to provide a point-cloud representation of the graph to the neural networks . Thus , any embedding method which yields embedding vectors such that the distance between the embedding vectors is proportional to the distance of their corresponding nodes is applicable . In our initial experiments , we used other embedding methods such as [ arXiv:1710.02971 ] but the results were not much different . Please note that methods such as VGAE ( Kipf and Welling , NIPS BDL 2016 ) are not applicable to our method . They basically assume that the node attributes contain sufficient amount of information such that one can recover the adjacency matrix via processing the node attributes . 6- \u201c the fact that we start from a random first index means ... could this cause undesirable variance at test time ? \u201d : Since we use farthest data point sampling , finally the sampled embedding vectors cover the spatial distribution of all the embedding vectors . In addition , one can start from a deterministic embedding vector . For instance , we can start from the embedding vector which is the closest vector to the mean of all the embedding vectors . 7- \u201c Furthermore , the downsampling from A^3 is a sure-fire way to obtain dense graphs after the first pooling -- potentially severely limiting the applicability of the method for large graphs \u201d : Adjacency matrices are mostly sparse and obtaining A^2 or A^3 is not computationally expensive . In addition , in most of the graph classification applications , the size of the graphs are small . 8- \u201c CLIQUE POOLING \u201d : Thanks for bringing this paper into our attention . We cited it . 9- \u201c I recommend ( marginal ) rejection , but am open to improving my score if .... \u201d We are delighted that you found our paper innovative and we would like to note that openreview shows \u201c clear reject \u201d not marginal reject . We hope that our answers provide a more clear picture of the contributions of the paper . 10- \u201c it might be interesting to compare with a version that features learnable queries \u201d : Both Diffpool and Rank-Pool use learnable queries . 11- \u201c In Equation ( 3 ) , should the first min actually be a max ? \u201d It should be min . The distance between each data point and its closest sampled vector matter . 12- \u201c what 's the exact GNN layer being used \u201d We used spatial convolution layer as in eq ( 1 ) . We will release the data and the code after acceptance . 13- \u201c Batch normalization \u201d : We used it to help to prevent overfitting along with dropout . 14- ( Xu et al. , ICML 2018 ) : We cited it . Thanks ."}, {"review_id": "r1eQeCEYwB-2", "review_text": "The authors propose in this paper to complement the node attributes in a graph with vectors obtained using a graph embedding algorithm. More precisely, they propose a graph neural network that apply several layers of graph convolution in parallel to the node attributes and to the embedding, then takes an average of the result, which is fed to another series of graph convolution. This is combined with some form of sampling which strongly resembles median based quantization but is solved with some basic heuristics and without acknowledging the resemblance (I might be missing something). Globally, I find this paper very unclear. The authors are mixing references to graph convolution for fixed graph structure with references about general graph neural networks. Their goal is not clearly stated from the beginning and in many places the discussion lacks of focus and is difficult to follow. In the experimental evaluation several values are very close one to another and the statistical significativity of the differences is not assessed. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the comments . 1- \u201c The authors are mixing references to graph convolution for fixed graph structure with references about general graph neural networks. \u201d We are not sure if we understood this comment well . But , the spatial graph convolution used in all the scenarios is similar . Specifically , we used spatial convolution expressed in eq ( 1 ) as our convolution function . 2- \u201c their goal is not clearly stated - I find this paper very unclear \u201d : In abstract and in section 3 , the goal of the paper is explained with illustrative examples . In addition , several novel experiments are presented which showcase the importance of the proposed approach ( Table 1 and Table 2 ) . 3- \u201c In the experimental evaluation several values are very close one to another \u201d In Table 1 and Table 2 , it is shown that the improvement in the performance of the GNN is significant when the proposed approach is utilized ."}], "0": {"review_id": "r1eQeCEYwB-0", "review_text": "In this work the authors point out an issue related to graph neural networks. Specifically, if two nodes, that may be far apart in the graph, may be represented as (almost) the same vector. This is simply because when no features/labels are associated with nodes, and the local structure around those two nodes is very similar then the local aggregation of information will result in a similar representation. Therefore the authors introduce an embedding first of the graph in the Euclidean space using DeepWalk and then use this embedding in combination with the design of a CNN. The authors propose a pooling method that outperforms several state-of-the-art pooling techniques on real data. Overall, the empirical results are supportive of the fact that the proposed method can help improve the performance of GNNs. Overall I found the results of this paper to be weak, but nonetheless the paper is well-written and contains some interesting ideas. Hence my rating. Some questions follow. - While the authors call this as an \"issue\" it is more like a feature of these methods. For instance, in \"RolX: Structural Role Extraction & Mining in Large Graphs\" by Henderson et al. this \"issue\" could turn out to be an interesting feature of the GNNs in the sense that these nodes may have a similar (structural) role. It would be nice to have a short discussion related to this line of research in social networks' analysis. - Some components of the CNN (e.g., node sampling) could be done using well-developed tools for sampling matrices from numerical linear algebra, or by introducing some randomness when sampling a node as in kmeans++. - Graph downsampling appears to be an expensive operation. Can you please comment on the running times? The issue of scalability is not discussed, and the reader cannot easily infer to what sizes this method can scale up to. - Using other graph tasks, that are classical but also more challenging (e.g., learning 2-connected components of a graph just to mention something that comes up) would be interesting to see in Section 5.2. - It would have been interesting to see the effect of the embedding step on the accuracy on the real data. E.g., using node2vec or standard spectral embeddings. [Edit: The authors have replied to my comments, and the other reviewers' comments in great detail. Therefore I upgrade my score.] ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the comments . 1- \u201c Overall I found the results of this paper to be weak \u201d : Table 1 and Table 2 indicate that the improvements in the results are significant for both real and synthetic data . 2- \u201c Some components of the CNN ( e.g. , node sampling ) could be done using well-developed tools for sampling matrices from numerical linear algebra , or by introducing some randomness when sampling a node as in kmeans++ \u201d : Thanks for this suggestion . We are well aware of column/row sampling techniques and we considered several sampling methods as our candidates including the one used in K-means++ and SRS [ arXiv:1705.03566 ] . The farthest data point sampling method used in our pooling method ( which is very similar to the sampling techniques used in K-means ++ ) ensures that the sampled embedding vectors cover the spatial distribution of all the embedding vectors and this is all we expect the sampling method to do . 3- \u201c Graph downsampling appears to be an expensive operation. \u201d The complexity of the sampling method is linear with the number of sampled embedding vectors ( O ( m * n * de ) where m is the number of sampled vectors and de is the dimension of the embedding vectors ) . In addition , different from node classification , in the graph classification task , we mostly do not have large graphs ( mostly less than 100 nodes ) . In the revised paper , we described the computation complexity of the node sampling method . 4- \u201c It would have been interesting to see the effect of the embedding step on the accuracy on the real data . E.g. , using node2vec or standard spectral embeddings. \u201d \u0651In our initial experiments , we used other embedding methods such as [ arXiv:1710.02971 ] but the results were not much different . In the final version , we will report the results with several different embedding methods . 5- \u201c While the authors call this as an `` issue '' it is more like a feature of these methods . For instance , in `` RolX : Structural Role Extraction & Mining in Large Graphs '' by Henderson et al.this `` issue '' could turn out to be an interesting feature of the GNNs in the sense that these nodes may have a similar ( structural ) role. \u201d It depends on the application that we deal with . In the graph classification problem , we clearly showed that it is not a desirable feature and this feature can make the GNN unable to extract discriminative features . Moreover , our approach addresses another problem of GNNs raised by ( Xu et al . ( ICLR 2019 ) ) : GNNs can map even different local structures to same feature vectors . Since we include the information about the location of the nodes , we help the GNN to distinguish both similar and different local structures . We cited \u201c RolX : Structural Role Extraction & Mining in Large Graphs \u201d and clarified that the point that in some applications mapping node with similar local structures to similar feature vectors can be desirable . We would like to reiterate that our paper is the first work which address this problem and the presented results ( Table 1 and Table2 ) clearly show how effective is the presented solution ."}, "1": {"review_id": "r1eQeCEYwB-1", "review_text": "============ After rebuttal ============ I thank the authors for carefully discussing the points of my review. I have upgraded the score to marginal acceptance (6). ============ Original review ============ In this paper, the authors identify a shortcoming of existing GNN architectures for graph classification tasks -- specifically, the fact that, in the featureless regime, the graph convolutional layers rely on propagating very rudimentary structural information, making it hard (or impossible) to distinguish graphs with similar local structure. To fix the problem, the authors propose to augment the input feature space with graph-structural embeddings (computed by an algorithm like DeepWalk), and processing those in parallel with any other input features available. On existing real-world datasets, as well as synthetic datasets carefully constructed to illustrate this phenomenon, the proposed pipeline matches or exceeds the version without the structural embedding inputs. Further, the authors note that the structural embeddings could be used to propose a novel graph pooling method -- one which attempts to preserve as diverse structural feature sets as possible. It is shown that this method is competitive to other differentiable pooling methods, like DiffPool and Graph U-Nets. Lastly, the authors demonstrate that the addition of pooling layers does not help baseline GNNs on the synthetically constructed tasks, as the fundamental issue of handling similar local structures is still not addressed. I believe that the paper clearly exposes and proposes a nice idea which could hold great potential, and which can be useful to graph representation learning practitioners. I am particularly happy with the design of the synthetic experiments. However, I find that, in its current form, the manuscript is narrowly below the bar for a venue like ICLR. Comments: * The observation that existing GNN layers may struggle with distinguishing featureless graphs is not particularly novel. It's largely the centerpiece of the (already cited) work of Xu et al. (ICLR 2019), and I believe that its relevance and relation to the authors' work should be better stressed in the related work section. * The usage of DeepWalk to encode structural information (and even to be used as initial features for a GNN) is, ultimately, also not necessarily a novel idea. At least, it's something that should be clear to any expert GNN practitioner already: if useful features are missing from the graph, a method like DeepWalk (if applicable; see below!) could be a go-to method for obtaining such features. In its current form, I don't see that the authors are proposing anything substantially architecturally novel, and their contribution is primarily on the data/feature engineering side. * The above point is not necessarily problematic, but if the aim is to stress the importance of the architectural novelties of the proposed GNN-ESR model more, and not just the added features, I would recommend the authors to perform a few ablations: e.g. seeing how well would processing a concatenation of E and F in the same GNN layer perform. * Many of the standard graph classification datasets are known to be noisy and unreliable (see e.g. Luzhnica, Day and Li\u00f2 (ICML GraphReasoning Workshop 2019). This means that it is a must to report error bars of the cross-validation experiments. It's hard to say that many of the improvements depicted here are statistically significant otherwise. * I have concerns about the computational complexity, or even feasibility, of using DeepWalk-like methods in the general case, e.g. for node classification. Namely, if such layers are to be applied in inductive settings (with nodes gradually added to graphs), one would require re-running DeepWalk every time a new node is added. The authors should comment on this adequately, and perhaps discuss the feasibility of other unsupervised embedding techniques for obtaining the e-vectors -- such as VGAE (Kipf and Welling, NIPS BDL 2016), GraphSAGE (Hamilton et al., NIPS 2017), Graph2Gauss (Bojchevski and G\u00fcnnemann, ICLR 2018) or DGI (Veli\u010dkovi\u0107 et al., ICLR 2019). * While I find the proposed pooling method interesting (and more grounded in the graph's structural features than other proposed works), I find that there are many potential limitations to be discussed. For example, the fact that we start from a random first index means that we cannot rely on the obtained pooling to always be the same -- could this cause undesirable variance at test time? Furthermore, the downsampling from A^3 is a sure-fire way to obtain dense graphs after the first pooling -- potentially severely limiting the applicability of the method for large graphs. In my opinion, the authors should appropriately comment on these and perform ablations against pooling with A and A^2 (as was done in Graph U-Nets). It should also be interesting to note that there exist other structurally-informed pooling methods; see e.g. the Clique pooling method from Luzhnica, Day and Li\u00f2 (ICLR RLGM 2019). Given all of the above, I recommend (marginal) rejection, but am open to improving my score if the authors appropriately address the aforementioned comments. Some minor comments and thoughts: * The paper has several typos and grammar issues, and a typo pass is highly encouraged to aid clarity; * The \"attention mechanism\" of Equation (5--6) seems to be nonparametric? If so, it might be interesting to compare with a version that features learnable queries, e.g. using the Transformer-style attention. * In Equation (3), should the first min actually be a max? As we're maximising the overall minimum distances between topological features. * I'm not sure that the paper is anywhere clear on what's the exact GNN layer being used. Could this be clarified and made more explicit? It's critical to reproducibility. * I find it curious that the authors needed to resort to using batch normalisation---I usually found it to either have no meaningful effect on the results on the graph classification benchmarks, or it made results worse. Can the authors comment on this decision? * The idea to concatenate output of all convolutional layers is heavily resembling Jumping Knowledge networks (Xu et al., ICML 2018), and I believe they should be appropriately cited.", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the comments . 1- \u201c Xu et al . ( ICLR 2019 ) \u201d : Please note that the approach presented in ( Xu et al . ( ICLR 2019 ) ) does not solve the problem that we address in our paper . The authors of ( Xu et al . ( ICLR 2019 ) ) want to make sure that if two local structures are DIFFERENT , the GNN maps them to different vectors in the feature space and they showed that the sum aggregator satisfies this requirement . The solution that we proposed prevents the GNN from mapping even SIMILAR local structures to the same feature vectors because we make the GNN aware of the location of the nodes . In addition , in all our experiments we used the sum aggregator . The presented experiments clearly show that ( Xu et al . ( ICLR 2019 ) ) can not solve the problem that we addressed in our paper . We clarified this difference in the new paper . 2- \u201c their contribution is primarily on the data/feature engineering side \u201d : We believe that it is an important contribution that our paper shows that the fact that GNN maps similar local structures to same feature vectors is problematic . In addition , we propose a solution to this important problem and our solution is simple and effective ( it also addresses the problem raised in Xu et al . ( ICLR 2019 ) in a more effective way ) . Moreover , the contributions of our paper is not limited to GNN-ESR . We present a new graph pooling method too . 3- \u201c The above point is not necessarily problematic but \u2026 seeing how well would processing a concatenation of E and F in the same GNN layer perform \u201d : We do not claim that we propose a novel architecture for GNNs ( except the new pooling layer ) . In Figure 1 , if we set k1=0 , it is equivalent to the scenario in which we E and F are concatenated . Our initial experiments with few data-sets showed that k=1 is slightly better than k=0 . We will regenerate the results with k=0 and will report them in the final version . 4- \u201c This means that it is a must to report error bars of the cross-validation experiments .. \u201d : Thanks for your suggestion . We added the error bars to the results in the new paper . 5- \u201c computational complexity , or even feasibility , of using DeepWalk-like methods in the general case \u2026 or DGI ( Veli\u010dkovi\u0107 et al. , ICLR 2019 ) . \u201d In this paper , we focus on graph classification . For online cases , there are some online embedding methods . In addition , Deepwalk is scalable to large graphs . In the paper , we mention that the motivation for using the embedding vectors is to provide a point-cloud representation of the graph to the neural networks . Thus , any embedding method which yields embedding vectors such that the distance between the embedding vectors is proportional to the distance of their corresponding nodes is applicable . In our initial experiments , we used other embedding methods such as [ arXiv:1710.02971 ] but the results were not much different . Please note that methods such as VGAE ( Kipf and Welling , NIPS BDL 2016 ) are not applicable to our method . They basically assume that the node attributes contain sufficient amount of information such that one can recover the adjacency matrix via processing the node attributes . 6- \u201c the fact that we start from a random first index means ... could this cause undesirable variance at test time ? \u201d : Since we use farthest data point sampling , finally the sampled embedding vectors cover the spatial distribution of all the embedding vectors . In addition , one can start from a deterministic embedding vector . For instance , we can start from the embedding vector which is the closest vector to the mean of all the embedding vectors . 7- \u201c Furthermore , the downsampling from A^3 is a sure-fire way to obtain dense graphs after the first pooling -- potentially severely limiting the applicability of the method for large graphs \u201d : Adjacency matrices are mostly sparse and obtaining A^2 or A^3 is not computationally expensive . In addition , in most of the graph classification applications , the size of the graphs are small . 8- \u201c CLIQUE POOLING \u201d : Thanks for bringing this paper into our attention . We cited it . 9- \u201c I recommend ( marginal ) rejection , but am open to improving my score if .... \u201d We are delighted that you found our paper innovative and we would like to note that openreview shows \u201c clear reject \u201d not marginal reject . We hope that our answers provide a more clear picture of the contributions of the paper . 10- \u201c it might be interesting to compare with a version that features learnable queries \u201d : Both Diffpool and Rank-Pool use learnable queries . 11- \u201c In Equation ( 3 ) , should the first min actually be a max ? \u201d It should be min . The distance between each data point and its closest sampled vector matter . 12- \u201c what 's the exact GNN layer being used \u201d We used spatial convolution layer as in eq ( 1 ) . We will release the data and the code after acceptance . 13- \u201c Batch normalization \u201d : We used it to help to prevent overfitting along with dropout . 14- ( Xu et al. , ICML 2018 ) : We cited it . Thanks ."}, "2": {"review_id": "r1eQeCEYwB-2", "review_text": "The authors propose in this paper to complement the node attributes in a graph with vectors obtained using a graph embedding algorithm. More precisely, they propose a graph neural network that apply several layers of graph convolution in parallel to the node attributes and to the embedding, then takes an average of the result, which is fed to another series of graph convolution. This is combined with some form of sampling which strongly resembles median based quantization but is solved with some basic heuristics and without acknowledging the resemblance (I might be missing something). Globally, I find this paper very unclear. The authors are mixing references to graph convolution for fixed graph structure with references about general graph neural networks. Their goal is not clearly stated from the beginning and in many places the discussion lacks of focus and is difficult to follow. In the experimental evaluation several values are very close one to another and the statistical significativity of the differences is not assessed. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the comments . 1- \u201c The authors are mixing references to graph convolution for fixed graph structure with references about general graph neural networks. \u201d We are not sure if we understood this comment well . But , the spatial graph convolution used in all the scenarios is similar . Specifically , we used spatial convolution expressed in eq ( 1 ) as our convolution function . 2- \u201c their goal is not clearly stated - I find this paper very unclear \u201d : In abstract and in section 3 , the goal of the paper is explained with illustrative examples . In addition , several novel experiments are presented which showcase the importance of the proposed approach ( Table 1 and Table 2 ) . 3- \u201c In the experimental evaluation several values are very close one to another \u201d In Table 1 and Table 2 , it is shown that the improvement in the performance of the GNN is significant when the proposed approach is utilized ."}}