{"year": "2021", "forum": "9D_Ovq4Mgho", "title": "Network-Agnostic Knowledge Transfer for Medical Image Segmentation", "decision": "Reject", "meta_review": "A majority of the reviewers find the paper lacks novelty and provides an insufficient discussion of the state-of-the-art in knowledge distillation and student teacher training to warrant publication.\nThe approach is quite narrow to the application domain and the paper does not provide novel insights on how to chose a good network.\nA subset of the experiments performed on an internal data set with random train-test-splits do not evaluate a realistic transfer setting.\n\n", "reviews": [{"review_id": "9D_Ovq4Mgho-0", "review_text": "The paper describes a knowledge transfer technique based on training a student network using annotation creating by a teacher network . This is actually not a summary of the method but the method itself . Most the the rest of the paper is devote do describe experiment details . The idea is well known in machine learning community see e.g.Distilling the Knowledge in a Neural Network by Hinton . where is used to transfer knowledge from a huge network to a small network . Hence , there is not much novelty in the paper . Although the method is very simple it is difficult the follow the experimental results . It is written in a very unclear way . Do you use step 3 in the experiments ? what is your conclusion regarding parameter fine tuning vs. your approach ? Over all the paper is more suitable for a medical imaging conference than fro a general deep learning conference .", "rating": "3: Clear rejection", "reply_text": "~~~ Reviewer 's comment : The paper describes a knowledge transfer technique based on training a student network using annotation creating by a teacher network . This is actually not a summary of the method but the method itself . Most the the rest of the paper is devote do describe experiment details . ~~~ We sincerely thank the reviewer for taking the time to review our paper . We have revised the paper extensively . Meanwhile , we also highlighted our major contribution as well as the novelty of this article . By the way , the reviewer describes the main idea of the algorithm correctly . Please find our detailed explanation below . ~~~ Reviewer 's comment : The idea is well known in machine learning community see e.g.Distilling the Knowledge in a Neural Network by Hinton . where is used to transfer knowledge from a huge network to a small network . Hence , there is not much novelty in the paper . ~~~ We surveyed the work on knowledge distillation ( Hinton et al. , 2015 ) and some related studies ( Yoo et al. , 2019 ; Lopes et al. , 2017 ) . The novelty of our algorithm can be seen at least from four aspects . - * * The main novelty is that our algorithm does not require any teacher-training data , metadata , or additional generative network * * . Previous studies trained student models heavily relying on the training dataset of the teacher model ( Hinton et al. , 2015 ) , metadata ( Lopes et al. , 2017 ) , or additional generative networks ( Yoo et al. , 2019 ) . As pointed out by Yoo et al . ( 2019 ) and Lopes et al . ( 2017 ) , Hinton \u2019 s knowledge distillation ( Hinton et al. , 2015 ) needed to access the teacher training dataset ( labeled or unlabeled ) . Lopes et al . ( 2017 ) required producing metadata during training and the student model had to be trained on metadata instead . Yoo et al . ( 2019 ) employed an additional generative network to generate artificial dataset for training the student model , so that the performance of the generative network also affected the training of the student network . The generative network was coupled with the teacher network , so that it is necessary to design and train a generative network for each teacher model , which will increase the computation and challenge . It would be even more challenging if there are multiple teacher models . Since the teacher-training dataset , metadata , and generative network was coupled with the teacher network , they limited the application of these algorithms or increased the computation burden to train the student network . - Our algorithm transfers knowledge between heterogeneous networks of semantic segmentation , while these well-known knowledge distillation studies focused on classification . - Our transferal dataset ( used to train the student model ) is allowed to be much different from the teacher training dataset , even of different image modalities . For example , we used PASCAL VOC2020 as transferal dataset to transfer the ultrasound segmentation knowledge , and it worked well . - Last but not least , our algorithm is really simple and easy to implement . We also highlighted the salient features of our algorithm in * * Abstract * * -- \u201c The salient features of our algorithm include : 1 ) no need for original training data or generative networks , 2 ) knowledge transfer between different architectures , 3 ) ease of implementation for downstream tasks by using the downstream task dataset as the transferal dataset , 4 ) knowledge transfer of an ensemble of models , trained independently , into one student model. \u201d ~~~ Reviewer 's comment : Although the method is very simple it is difficult the follow the experimental results . It is written in a very unclear way . Do you use step 3 in the experiments ? ~~~ To make it easy to follow , we carefully revised the paper , especially the experiment section . For the convenience of the reviewer , we would like to make a brief summary of our study : - Our main algorithm is on knowledge transfer from a teacher model to an student model that is independent in architecture ( as the reviewer described in the first comment ) , which is Algorithm 1 . - If a small dataset with ground truth is provided , the knowledge transfer algorithm can be used together with fine-tuning ( Algorithm 2 ) . - In the revision , we described the two algorithms with more details , separately . We are looking forward to more comments and suggestions from the reviewer . We will try our best to improve the paper ."}, {"review_id": "9D_Ovq4Mgho-1", "review_text": "The paper proposes to use student-teacher training as a way of knowledge transfer between neural networks with different architectures without access to the source data . Instead the authors propose to use a separate dataset to transfer the knowledge of the teacher network and a potential different dataset for fine-tuning . The paper evaluates their method with various segmentation architectures by pretraining a DeepLab v3+ on an internal breast lesion dataset and testing transfer and fine-tuning using different medical datasets . The authors find that knowledge transfer performs similar to regular transfer learning in most combinations of datasets . I believe that the paper tackles an interesting scenario of transferring knowledge from a fixed pre-trained network to a potentially different application without access to the original training data and sets up an extensive set of combinations of target tasks , student network architectures and transferral dataset ( called dataset agent in the paper ) . However , I can not recommend the paper for acceptance in its current form . Most importantly the paper seems to be lacking proper positioning within the space of knowledge distillation and student-teacher training which leads to an unclear message about the novelty of the paper . Student-teacher training for knowledge distillation is not novel and the message that it is possible to transfer knowledge using student-teacher training is unsurprising given that it has been shown before that you can transfer knowledge without any observed data ( e.g.KegNet : Knowledge Extraction with No Observable Data . Yoo et al.NeurIPS 2019 ) . Further , the experiments do not contribute any new insights about how to chose the best student network nor which transferral dataset to use even though the introduction refers to unsuitability of certain pretraining tasks for a different target task . The first example ( Table 2 ) seems to be using an internal dataset that randomly has been split into train/val/test splits and therefore resembles no real transfer learning task and it seems unsuprising that the 'direct learning ' approach yields the best results . The second example ( Table 3 ) using the Baheya breast lesion dataset seems to tackle the problem of unsupervised domain adaptation rather than transfer learning : the teacher and target dataset both tackle breast lesion segmentation on ultrasound images . Here it could be that using the target dataset as transferral dataset might help to adjust batch statistics for potential normalisation layers to improve the performance . This leaves the last two examples as only real transfer learning experiments . Lastly , the whole setup assumes that the input and output space of the student and teacher network are always the same , while it is argued that this approach allows for flexibility in difference of network architectures between the student and teacher network . However , semantic segmentation tasks in medical imaging often appear with various numbers of classes and 'input channels ' requiring more advanced knowledge distillation -- this could be an interesting problem to tackle in a later version of this work . Further comments : - Data preprocessing : What preprocessing are you using for the training of the networks ? How are you handling different shapes of the images ? Are the segmentation algorithms trained on the full images or on patches ? - Transferral datasets : Looking at the images in Fig.2 , it seems that even the same modality ultrasound images show different sorts of image artefacts - do you clean those at all ? Do you think domain shift might be something that 's interacting with your setup ? - Do you have any theoretical or intuitive justification why you would want to perform knowledge transfer using unrelated data ( skin lesion / different anatomy etc ) ? Why should this be better than using no data at all or regular computer vision datasets ? Do you think the number of training examples for transfer matters ? - What do example segmentations look like ? Are there similar shapes for different datasets ? Does the network also learn some sort of shape prior ? ( see Oktay , et al.Anatomically constrained neural networks ( ACNNs ) : application to cardiac image enhancement and segmentation . 2017 ) - Which loss did you use : CE / Dice-loss or a combination of the two ? - The term dataset agent is already used in the abstract and is not very clear - I 'd personally find something like 'transfer [ ral ] dataset ' easier to grasp . - Introduction , first paragraph : 'black- [ space ] box ' - > 'black-box ' - Introduction : 'network ( teacher ) ' - > 'network ( teacher ) ' - What 's a latent dataset ? I would rather simply refer to 'learned representations ' or 'knowledge ' - 'XXX ' is already used in the introduction and not explained - I would simply refer to 'internal / in-house datasets ' . Also , note the comment on the breast lesion dataset only being a single dataset with different splits . - I have not seen the term 'educated ' in reference to neural networks before - it would be more common to say 'trained ' . - Section 5.3.2 ) : You mention that the networks trained from scratch have poor performance because of the small tuning dataset - I guess you are referring to the small training set ? - Another potential reference for knowledge transfer for medical imaging could be Kuzina , et al.Bayesian Generative Models for Knowledge Transfer in MRI Semantic Segmentation Problems . 2019 - As this work is relatively application-specific it might be better suited for one of the more medically inclined venues like MIDL , MICCAI , ...", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer their insightful comments . Following these suggestions , we have carefully and extensively improved the paper . ~~~ Reviewer 's comment : Most importantly the paper seems to be lacking proper positioning within the space of knowledge distillation and student-teacher training which leads to an unclear message about the novelty of the paper . Student-teacher training for knowledge distillation is not novel and the message that it is possible to transfer knowledge using student-teacher training is unsurprising given that it has been shown before that you can transfer knowledge without any observed data ( e.g.KegNet : Knowledge Extraction with No Observable Data . Yoo et al.NeurIPS 2019 ) . ~~~ As part of our revision , we reviewed all reviewers ' recommended papers as well as some other related papers . Accordingly , we updated the introduction section to better position our algorithm within the space of knowledge distillation and student-teacher training . Here we introduce representative algorithms on knowledge distillation ( Hinton et al. , 2015 ; Yoo et al. , 2019 ; Lopes et al. , 2017 ) . These algorithms needed to train student models relying on the training dataset of the teacher model ( Hinton et al. , 2015 ) , metadata ( Lopes et al. , 2017 ) , or additional generative networks ( Yoo et al. , 2019 ) . As pointed out by Yoo et al . ( 2019 ) and Lopes et al . ( 2017 ) , Hinton \u2019 s knowledge distillation ( Hinton et al. , 2015 ) needs to access the teacher-training dataset ( labeled or unlabeled ) . Lopes et al . ( 2017 ) required producing metadata during training and the student model had to be trained based on this metadata . Yoo et al . ( 2019 ) employed an additional generative network to generate artificial dataset for training the student model . Therefore , it was necessary to design and train a generative network for each teacher model , which increased the computation burden and finally determined the performance of the student network . This process would be even more challenging if there were multiple teacher models . * * Since the teacher training dataset , metadata , and generative network were coupled with the teacher network , they limited the application of these algorithms . Comparatively , our algorithm is simple but effective , does not rely on additional generative networks and does not have any requirements on the teacher-training data or metadata . * * We showed that the transferal dataset was allowed to be different from the teacher training-dataset . For example , we used PASCAL VOC2020 as transferal dataset to transfer the ultrasound segmentation knowledge , and it worked well . Even though our algorithm is straightforward , it was able to solve a particularly challenging problem ( even to complex algorithms such as ( Yoo et al. , 2019 ) ) . Our revised paper presents the explanation in * * Section 1 * * -- \u201c Knowledge distillation is the process of transferring the knowledge of a large neural network or an ensemble of neural networks ( teacher ) to a smaller network ( student ) ( Hinton et al. , 2015 ) . Given a set of trained teacher models , one feeds training data to them and uses their predictions instead of the true labels to train the student model . For effective transfer of knowledge , however , it is essential that a reasonable fraction of the training examples is observable by the student ( Li et al. , 2018 ) or the metadata at each layer is provided ( Lopes et al. , 2017 ) . Yoo et al . ( 2019 ) used a generative network to extract the knowledge of a teacher network , which generated labeled artificial images to train another network . As can be seen , Yoo et al. \u2019 s method had to train an additional generative network for each teacher network. \u201d * * Reference * * * Geoffrey Hinton , Oriol Vinyals , and Jeff Dean . Distilling the knowledge in a neural network . arXiv preprint arXiv:1503.02531 , 2015 . * * Raphael Gontijo Lopes , Stefano Fenu , and Thad Starner . Data-free knowledge distillation for deep neural networks . arXiv preprint arXiv:1710.07535 , 2017 . * * Jaemin Yoo , Minyong Cho , Taebum Kim , and U Kang . Knowledge extraction with no observable data . In Advances in Neural Information Processing Systems , pp . 2705\u20132714 , 2019 . *"}, {"review_id": "9D_Ovq4Mgho-2", "review_text": "In this work the authors propose to transfer knowledge between teacher and student networks trained on separate datasets , and claim to overcome challenges in availability of data annotations for challenging semantic segmentation in medical imaging domain . Strengths The proposed model is simple to follow and is targeted towards a significant problem in medical imaging analysis domain . Comments The authors have used five segmentation networks , it is suggested that the selection of these five algorithms is further justified . One of the major concern in medical imaging domain is the black-box nature of the DL algorithms used , authors should comment on how relying on this black-box nature for knowledge transfer would effect the interpretability of these results . The method relies on three datasets and three models to come up with the final target segmentation , what are the requirements on the size of these datasets , in general the authors should discuss the effect of this on the overall performance .", "rating": "7: Good paper, accept", "reply_text": "We really appreciate the encouraging comments . To better present the study , we have revised and polished the paper extensively . ~~~ Reviewer 's comment : The authors have used five segmentation networks , it is suggested that the selection of these five algorithms is further justified . ~~~ Please find our explanation in * * Section 3 * * -- \u201c We posit that if the teacher model is ineffective , a more capable student model can easily achieve similar performance ; if the student model , however , is ineffective , the cause is not easily identified . It can be attributed to the knowledge transfer algorithm or inherent limitations of the student model itself . We experimented with five state-of-the-art deep neural networks , which have different architectures and have been shown to be the best on various segmentation tasks. \u201d Also , we would like to briefly introduce each method : - DeepLabv3+ is one of Google \u2019 s latest and best performing semantic segmentation models . - U-Net has been the most widely used network for medical image segmentation . - AttU-Net is a modification of U-Net , where attention gates help the model focus on the target . - SDU-Net has demonstrated better performance by using only 40 percent of the parameters in U-Net . - Panoptic-FPN merges semantic segmentation and object detection , which provides a more rich and complete segmentation . ~~~ Reviewer 's comment : One of the major concern in medical imaging domain is the black-box nature of the DL algorithms used , authors should comment on how relying on this black-box nature for knowledge transfer would effect the interpretability of these results . ~~~ Thank you for highlighting this important point . The main difference between our knowledge transfer algorithm and conventional deep learning training is that our transferal dataset is pseudo-annotated where the mask has no physical meaning . Interpretable machine learning techniques can be grouped into two categories : local interpretability and global interpretability . Local interpretability examines an individual prediction of a model locally , trying to figure out why the model makes the decision it makes . Global interpretability implies that the user can understand how the model works globally by inspecting the structures and parameters of a complex model . Since local interpretability does not need the original training dataset , our knowledge transfer does not complicate the interpretability . However , it may complicate the global interpretability , as the parameters of the neural network are closely related to the training dataset . It would be interesting to study how interpretability is affected by knowledge transfer . Due to page limitation , we condensed the discussion in * * Section 6 * * -- \u201c Medical imaging applications requires interpretable algorithms which generate confidence in the clinical user . Knowledge transfer employs pseudo annotation for training which has no physical meaning . It is imperative to examine and quantify the interpretability of student model before deploying models clinically. \u201d ~~~ Reviewer 's comment : The method relies on three datasets and three models to come up with the final target segmentation , what are the requirements on the size of these datasets , in general the authors should discuss the effect of this on the overall performance . ~~~ Out the three datasets , only the pseudo-annotated transferal dataset is novel in this study . So , our discussion and revision mainly focuses on the transferal dataset . Please find our explanation below : - * * Teacher-training dataset : * * the teacher-training dataset is used to train the teacher model , which is the conventional training with ground truth . Our algorithm aims to solve the issue of knowledge transfer when the teacher-training dataset is not accessible ; as such , the size of teacher-training dataset is not the primary focus of this study . - * * Transferal dataset : * * As part of our revision , we detailed our method to refine the transferal dataset by excluding images without salient targets in the pseudo mask ( in * * Section 2 * * ) . Our experiments found that smaller datasets , wherein images without salient targets in the pseudo annotation are excluded , could result in better performance . Table 2 and Table 3 demonstrate that all student models trained on pseudo-annotated Baheya Breast Lesion resulted in Dice scores similar to the teacher model , although the size of Baheya Breast Lesion is much smaller than the teacher-training dataset . We condensed the discussion in * * Section 6 * * -- \u201c Transferal dataset with a large number of images of various contents has higher possibility to capture rich targets in the pseudo masks , but it may also include many images without salient targets . Future work includes optimization of a single transferal dataset or combine multiple transferal datasets to build a better one '' . - * * Fine-tuning dataset : * * Similarly , the fine-tuning dataset is similar to that of conventional fine-tuning ."}], "0": {"review_id": "9D_Ovq4Mgho-0", "review_text": "The paper describes a knowledge transfer technique based on training a student network using annotation creating by a teacher network . This is actually not a summary of the method but the method itself . Most the the rest of the paper is devote do describe experiment details . The idea is well known in machine learning community see e.g.Distilling the Knowledge in a Neural Network by Hinton . where is used to transfer knowledge from a huge network to a small network . Hence , there is not much novelty in the paper . Although the method is very simple it is difficult the follow the experimental results . It is written in a very unclear way . Do you use step 3 in the experiments ? what is your conclusion regarding parameter fine tuning vs. your approach ? Over all the paper is more suitable for a medical imaging conference than fro a general deep learning conference .", "rating": "3: Clear rejection", "reply_text": "~~~ Reviewer 's comment : The paper describes a knowledge transfer technique based on training a student network using annotation creating by a teacher network . This is actually not a summary of the method but the method itself . Most the the rest of the paper is devote do describe experiment details . ~~~ We sincerely thank the reviewer for taking the time to review our paper . We have revised the paper extensively . Meanwhile , we also highlighted our major contribution as well as the novelty of this article . By the way , the reviewer describes the main idea of the algorithm correctly . Please find our detailed explanation below . ~~~ Reviewer 's comment : The idea is well known in machine learning community see e.g.Distilling the Knowledge in a Neural Network by Hinton . where is used to transfer knowledge from a huge network to a small network . Hence , there is not much novelty in the paper . ~~~ We surveyed the work on knowledge distillation ( Hinton et al. , 2015 ) and some related studies ( Yoo et al. , 2019 ; Lopes et al. , 2017 ) . The novelty of our algorithm can be seen at least from four aspects . - * * The main novelty is that our algorithm does not require any teacher-training data , metadata , or additional generative network * * . Previous studies trained student models heavily relying on the training dataset of the teacher model ( Hinton et al. , 2015 ) , metadata ( Lopes et al. , 2017 ) , or additional generative networks ( Yoo et al. , 2019 ) . As pointed out by Yoo et al . ( 2019 ) and Lopes et al . ( 2017 ) , Hinton \u2019 s knowledge distillation ( Hinton et al. , 2015 ) needed to access the teacher training dataset ( labeled or unlabeled ) . Lopes et al . ( 2017 ) required producing metadata during training and the student model had to be trained on metadata instead . Yoo et al . ( 2019 ) employed an additional generative network to generate artificial dataset for training the student model , so that the performance of the generative network also affected the training of the student network . The generative network was coupled with the teacher network , so that it is necessary to design and train a generative network for each teacher model , which will increase the computation and challenge . It would be even more challenging if there are multiple teacher models . Since the teacher-training dataset , metadata , and generative network was coupled with the teacher network , they limited the application of these algorithms or increased the computation burden to train the student network . - Our algorithm transfers knowledge between heterogeneous networks of semantic segmentation , while these well-known knowledge distillation studies focused on classification . - Our transferal dataset ( used to train the student model ) is allowed to be much different from the teacher training dataset , even of different image modalities . For example , we used PASCAL VOC2020 as transferal dataset to transfer the ultrasound segmentation knowledge , and it worked well . - Last but not least , our algorithm is really simple and easy to implement . We also highlighted the salient features of our algorithm in * * Abstract * * -- \u201c The salient features of our algorithm include : 1 ) no need for original training data or generative networks , 2 ) knowledge transfer between different architectures , 3 ) ease of implementation for downstream tasks by using the downstream task dataset as the transferal dataset , 4 ) knowledge transfer of an ensemble of models , trained independently , into one student model. \u201d ~~~ Reviewer 's comment : Although the method is very simple it is difficult the follow the experimental results . It is written in a very unclear way . Do you use step 3 in the experiments ? ~~~ To make it easy to follow , we carefully revised the paper , especially the experiment section . For the convenience of the reviewer , we would like to make a brief summary of our study : - Our main algorithm is on knowledge transfer from a teacher model to an student model that is independent in architecture ( as the reviewer described in the first comment ) , which is Algorithm 1 . - If a small dataset with ground truth is provided , the knowledge transfer algorithm can be used together with fine-tuning ( Algorithm 2 ) . - In the revision , we described the two algorithms with more details , separately . We are looking forward to more comments and suggestions from the reviewer . We will try our best to improve the paper ."}, "1": {"review_id": "9D_Ovq4Mgho-1", "review_text": "The paper proposes to use student-teacher training as a way of knowledge transfer between neural networks with different architectures without access to the source data . Instead the authors propose to use a separate dataset to transfer the knowledge of the teacher network and a potential different dataset for fine-tuning . The paper evaluates their method with various segmentation architectures by pretraining a DeepLab v3+ on an internal breast lesion dataset and testing transfer and fine-tuning using different medical datasets . The authors find that knowledge transfer performs similar to regular transfer learning in most combinations of datasets . I believe that the paper tackles an interesting scenario of transferring knowledge from a fixed pre-trained network to a potentially different application without access to the original training data and sets up an extensive set of combinations of target tasks , student network architectures and transferral dataset ( called dataset agent in the paper ) . However , I can not recommend the paper for acceptance in its current form . Most importantly the paper seems to be lacking proper positioning within the space of knowledge distillation and student-teacher training which leads to an unclear message about the novelty of the paper . Student-teacher training for knowledge distillation is not novel and the message that it is possible to transfer knowledge using student-teacher training is unsurprising given that it has been shown before that you can transfer knowledge without any observed data ( e.g.KegNet : Knowledge Extraction with No Observable Data . Yoo et al.NeurIPS 2019 ) . Further , the experiments do not contribute any new insights about how to chose the best student network nor which transferral dataset to use even though the introduction refers to unsuitability of certain pretraining tasks for a different target task . The first example ( Table 2 ) seems to be using an internal dataset that randomly has been split into train/val/test splits and therefore resembles no real transfer learning task and it seems unsuprising that the 'direct learning ' approach yields the best results . The second example ( Table 3 ) using the Baheya breast lesion dataset seems to tackle the problem of unsupervised domain adaptation rather than transfer learning : the teacher and target dataset both tackle breast lesion segmentation on ultrasound images . Here it could be that using the target dataset as transferral dataset might help to adjust batch statistics for potential normalisation layers to improve the performance . This leaves the last two examples as only real transfer learning experiments . Lastly , the whole setup assumes that the input and output space of the student and teacher network are always the same , while it is argued that this approach allows for flexibility in difference of network architectures between the student and teacher network . However , semantic segmentation tasks in medical imaging often appear with various numbers of classes and 'input channels ' requiring more advanced knowledge distillation -- this could be an interesting problem to tackle in a later version of this work . Further comments : - Data preprocessing : What preprocessing are you using for the training of the networks ? How are you handling different shapes of the images ? Are the segmentation algorithms trained on the full images or on patches ? - Transferral datasets : Looking at the images in Fig.2 , it seems that even the same modality ultrasound images show different sorts of image artefacts - do you clean those at all ? Do you think domain shift might be something that 's interacting with your setup ? - Do you have any theoretical or intuitive justification why you would want to perform knowledge transfer using unrelated data ( skin lesion / different anatomy etc ) ? Why should this be better than using no data at all or regular computer vision datasets ? Do you think the number of training examples for transfer matters ? - What do example segmentations look like ? Are there similar shapes for different datasets ? Does the network also learn some sort of shape prior ? ( see Oktay , et al.Anatomically constrained neural networks ( ACNNs ) : application to cardiac image enhancement and segmentation . 2017 ) - Which loss did you use : CE / Dice-loss or a combination of the two ? - The term dataset agent is already used in the abstract and is not very clear - I 'd personally find something like 'transfer [ ral ] dataset ' easier to grasp . - Introduction , first paragraph : 'black- [ space ] box ' - > 'black-box ' - Introduction : 'network ( teacher ) ' - > 'network ( teacher ) ' - What 's a latent dataset ? I would rather simply refer to 'learned representations ' or 'knowledge ' - 'XXX ' is already used in the introduction and not explained - I would simply refer to 'internal / in-house datasets ' . Also , note the comment on the breast lesion dataset only being a single dataset with different splits . - I have not seen the term 'educated ' in reference to neural networks before - it would be more common to say 'trained ' . - Section 5.3.2 ) : You mention that the networks trained from scratch have poor performance because of the small tuning dataset - I guess you are referring to the small training set ? - Another potential reference for knowledge transfer for medical imaging could be Kuzina , et al.Bayesian Generative Models for Knowledge Transfer in MRI Semantic Segmentation Problems . 2019 - As this work is relatively application-specific it might be better suited for one of the more medically inclined venues like MIDL , MICCAI , ...", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer their insightful comments . Following these suggestions , we have carefully and extensively improved the paper . ~~~ Reviewer 's comment : Most importantly the paper seems to be lacking proper positioning within the space of knowledge distillation and student-teacher training which leads to an unclear message about the novelty of the paper . Student-teacher training for knowledge distillation is not novel and the message that it is possible to transfer knowledge using student-teacher training is unsurprising given that it has been shown before that you can transfer knowledge without any observed data ( e.g.KegNet : Knowledge Extraction with No Observable Data . Yoo et al.NeurIPS 2019 ) . ~~~ As part of our revision , we reviewed all reviewers ' recommended papers as well as some other related papers . Accordingly , we updated the introduction section to better position our algorithm within the space of knowledge distillation and student-teacher training . Here we introduce representative algorithms on knowledge distillation ( Hinton et al. , 2015 ; Yoo et al. , 2019 ; Lopes et al. , 2017 ) . These algorithms needed to train student models relying on the training dataset of the teacher model ( Hinton et al. , 2015 ) , metadata ( Lopes et al. , 2017 ) , or additional generative networks ( Yoo et al. , 2019 ) . As pointed out by Yoo et al . ( 2019 ) and Lopes et al . ( 2017 ) , Hinton \u2019 s knowledge distillation ( Hinton et al. , 2015 ) needs to access the teacher-training dataset ( labeled or unlabeled ) . Lopes et al . ( 2017 ) required producing metadata during training and the student model had to be trained based on this metadata . Yoo et al . ( 2019 ) employed an additional generative network to generate artificial dataset for training the student model . Therefore , it was necessary to design and train a generative network for each teacher model , which increased the computation burden and finally determined the performance of the student network . This process would be even more challenging if there were multiple teacher models . * * Since the teacher training dataset , metadata , and generative network were coupled with the teacher network , they limited the application of these algorithms . Comparatively , our algorithm is simple but effective , does not rely on additional generative networks and does not have any requirements on the teacher-training data or metadata . * * We showed that the transferal dataset was allowed to be different from the teacher training-dataset . For example , we used PASCAL VOC2020 as transferal dataset to transfer the ultrasound segmentation knowledge , and it worked well . Even though our algorithm is straightforward , it was able to solve a particularly challenging problem ( even to complex algorithms such as ( Yoo et al. , 2019 ) ) . Our revised paper presents the explanation in * * Section 1 * * -- \u201c Knowledge distillation is the process of transferring the knowledge of a large neural network or an ensemble of neural networks ( teacher ) to a smaller network ( student ) ( Hinton et al. , 2015 ) . Given a set of trained teacher models , one feeds training data to them and uses their predictions instead of the true labels to train the student model . For effective transfer of knowledge , however , it is essential that a reasonable fraction of the training examples is observable by the student ( Li et al. , 2018 ) or the metadata at each layer is provided ( Lopes et al. , 2017 ) . Yoo et al . ( 2019 ) used a generative network to extract the knowledge of a teacher network , which generated labeled artificial images to train another network . As can be seen , Yoo et al. \u2019 s method had to train an additional generative network for each teacher network. \u201d * * Reference * * * Geoffrey Hinton , Oriol Vinyals , and Jeff Dean . Distilling the knowledge in a neural network . arXiv preprint arXiv:1503.02531 , 2015 . * * Raphael Gontijo Lopes , Stefano Fenu , and Thad Starner . Data-free knowledge distillation for deep neural networks . arXiv preprint arXiv:1710.07535 , 2017 . * * Jaemin Yoo , Minyong Cho , Taebum Kim , and U Kang . Knowledge extraction with no observable data . In Advances in Neural Information Processing Systems , pp . 2705\u20132714 , 2019 . *"}, "2": {"review_id": "9D_Ovq4Mgho-2", "review_text": "In this work the authors propose to transfer knowledge between teacher and student networks trained on separate datasets , and claim to overcome challenges in availability of data annotations for challenging semantic segmentation in medical imaging domain . Strengths The proposed model is simple to follow and is targeted towards a significant problem in medical imaging analysis domain . Comments The authors have used five segmentation networks , it is suggested that the selection of these five algorithms is further justified . One of the major concern in medical imaging domain is the black-box nature of the DL algorithms used , authors should comment on how relying on this black-box nature for knowledge transfer would effect the interpretability of these results . The method relies on three datasets and three models to come up with the final target segmentation , what are the requirements on the size of these datasets , in general the authors should discuss the effect of this on the overall performance .", "rating": "7: Good paper, accept", "reply_text": "We really appreciate the encouraging comments . To better present the study , we have revised and polished the paper extensively . ~~~ Reviewer 's comment : The authors have used five segmentation networks , it is suggested that the selection of these five algorithms is further justified . ~~~ Please find our explanation in * * Section 3 * * -- \u201c We posit that if the teacher model is ineffective , a more capable student model can easily achieve similar performance ; if the student model , however , is ineffective , the cause is not easily identified . It can be attributed to the knowledge transfer algorithm or inherent limitations of the student model itself . We experimented with five state-of-the-art deep neural networks , which have different architectures and have been shown to be the best on various segmentation tasks. \u201d Also , we would like to briefly introduce each method : - DeepLabv3+ is one of Google \u2019 s latest and best performing semantic segmentation models . - U-Net has been the most widely used network for medical image segmentation . - AttU-Net is a modification of U-Net , where attention gates help the model focus on the target . - SDU-Net has demonstrated better performance by using only 40 percent of the parameters in U-Net . - Panoptic-FPN merges semantic segmentation and object detection , which provides a more rich and complete segmentation . ~~~ Reviewer 's comment : One of the major concern in medical imaging domain is the black-box nature of the DL algorithms used , authors should comment on how relying on this black-box nature for knowledge transfer would effect the interpretability of these results . ~~~ Thank you for highlighting this important point . The main difference between our knowledge transfer algorithm and conventional deep learning training is that our transferal dataset is pseudo-annotated where the mask has no physical meaning . Interpretable machine learning techniques can be grouped into two categories : local interpretability and global interpretability . Local interpretability examines an individual prediction of a model locally , trying to figure out why the model makes the decision it makes . Global interpretability implies that the user can understand how the model works globally by inspecting the structures and parameters of a complex model . Since local interpretability does not need the original training dataset , our knowledge transfer does not complicate the interpretability . However , it may complicate the global interpretability , as the parameters of the neural network are closely related to the training dataset . It would be interesting to study how interpretability is affected by knowledge transfer . Due to page limitation , we condensed the discussion in * * Section 6 * * -- \u201c Medical imaging applications requires interpretable algorithms which generate confidence in the clinical user . Knowledge transfer employs pseudo annotation for training which has no physical meaning . It is imperative to examine and quantify the interpretability of student model before deploying models clinically. \u201d ~~~ Reviewer 's comment : The method relies on three datasets and three models to come up with the final target segmentation , what are the requirements on the size of these datasets , in general the authors should discuss the effect of this on the overall performance . ~~~ Out the three datasets , only the pseudo-annotated transferal dataset is novel in this study . So , our discussion and revision mainly focuses on the transferal dataset . Please find our explanation below : - * * Teacher-training dataset : * * the teacher-training dataset is used to train the teacher model , which is the conventional training with ground truth . Our algorithm aims to solve the issue of knowledge transfer when the teacher-training dataset is not accessible ; as such , the size of teacher-training dataset is not the primary focus of this study . - * * Transferal dataset : * * As part of our revision , we detailed our method to refine the transferal dataset by excluding images without salient targets in the pseudo mask ( in * * Section 2 * * ) . Our experiments found that smaller datasets , wherein images without salient targets in the pseudo annotation are excluded , could result in better performance . Table 2 and Table 3 demonstrate that all student models trained on pseudo-annotated Baheya Breast Lesion resulted in Dice scores similar to the teacher model , although the size of Baheya Breast Lesion is much smaller than the teacher-training dataset . We condensed the discussion in * * Section 6 * * -- \u201c Transferal dataset with a large number of images of various contents has higher possibility to capture rich targets in the pseudo masks , but it may also include many images without salient targets . Future work includes optimization of a single transferal dataset or combine multiple transferal datasets to build a better one '' . - * * Fine-tuning dataset : * * Similarly , the fine-tuning dataset is similar to that of conventional fine-tuning ."}}