{"year": "2020", "forum": "HklZUpEtvr", "title": "OPTIMAL TRANSPORT, CYCLEGAN, AND PENALIZED LS FOR UNSUPERVISED LEARNING IN INVERSE PROBLEMS", "decision": "Reject", "meta_review": "This paper provides a novel approach for addressing ill-posed inverse problems based on a formulation as a regularized estimation problem and showing that this can be optimized using the CycleGAN framework. While the paper contains interesting ideas and has been substantially improved from its original form, the paper still does not meet the quality bar of ICLR due to a critical gap between the presented theory and applications. The paper will benefit from a revision and resubmission to another venue.", "reviews": [{"review_id": "HklZUpEtvr-0", "review_text": "The paper presents an interesting connection between cycleGAN, penalized Least Squares (PLS) and optimal transport (OT). From the PLS with deep regularization formulation, the authors cast a general OT problem with a specific cost combining measurement and reconstruction errors (eq. 9). From this formulation, the problem is expressed as a combination of a cycle consistency loss and an OT loss. The formulation is more generic than the classical cycleGAN formulation. Qualitative experiments are conducted on two different scenarii: accelerated MRI and deconvolution microscopy, for which the proposed method achieves good performances. In general I like the paper and the corresponding idea. However, I had a hard time understanding some of the key elements of the proposed method. Notably, in lemma 1, it is hard to understand in which case \\mathcal{X} \\times \\mathcal{X} / A \\union B is not an empty set. It is clearly the case whenever G or H are bijections, or whenever an exact reconstruction is achievable, but I guess a more open discussions could be conducted. Also, the definition of A and B are clearly dependent of G and H and this could be highlighted in the notation. As noted in proposition 2, however, the corresponding OT loss can be computed on the entire subset X and Y (since the Kantorovich potentials match those obtained on the restricted set). As a consequence, we can see the overall training procedure when H is fixed (as it is the case in both experiments), as learning for a generator in a WGAN way, that also enforces a consistency constraint (that can be seen as a regularization of the OT problem). While I like this idea, I think there might be some better justification for it (it all starts with Eq. 9 and the choice of this model. Why enforcing this minimal cost equation ?). Note that the model imposes that p=q=1, which is somehow limited. I guess other choices of p and q could be possible, eventually using regularized version of OT to remove the hard constraints on the kantorovich potentials I am willing to revise my note positively provided that a sufficient number of convincing explanations are given on my previous remarks. Minor remark Eq. 8 should include a weighting factor between the two terms since the dimensions are not the same. In the proof of Lemma 1, since you derive an equality for T, do you have to establish the first inequality ? Prop. 1 I guess there is an error on the first line (the integral is not over \\mathcal{X} \\times \\mathcal{X} / A \\union B given the previous definition Missing related work DLOW: Domain Flow for Adaptation and Generalization Rui Gong, Wen Li, Yuhua Chen, Luc Van Gool; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 2477-2486 ", "rating": "6: Weak Accept", "reply_text": "[ Q ] The paper presents an interesting connection between cycleGAN , penalized Least Squares ( PLS ) and optimal transport ( OT ) ... The formulation is more generic than the classical cycleGAN formulation . == > Thanks for your careful reading and understanding of our contribution . [ Q ] In general I like the paper and the corresponding idea . However , I had a hard time understanding some of the key elements of the proposed method . Notably , in lemma 1 , it is hard to understand in which case \\mathcal { X } \\times \\mathcal { X } / A \\union B is not an empty set . It is clearly the case whenever G or H are bijections , or whenever an exact reconstruction is achievable , but I guess a more open discussions could be conducted . == > Thank you for your constructive comments . We would like to remind you of the nature of quantities A and B . Since both are graphs of functions , they are low-dimensional manifolds embedded in the $ X \\times Y $ ambient space . Accordingly , unlike the reviewer 's concern , the technical difficulty arises as the relative measure of $ A\\cup B $ may become zero . However , depending on the existence of the singularity in the distribution , this term can contribute a significant portion of the total T loss . In this paper , we claim that this situation leads to the architecture of cycleGAN . To clarify this , we have elaborated on our discussion on the singularity in the distribution on Page 4 and 5 . Moreover , the dependency on the singularity is also explicitly shown in $ \\ell_ { OT } $ or $ \\ell_ { GAN } $ so that how these terms behave depending on $ \\sigma $ and $ \\rho $ . [ Q ] Also , the definition of A and B are clearly dependent of G and H and this could be highlighted in the notation . As noted in proposition 2 , however , the corresponding OT loss can be computed on the entire subset X and Y ( since the Kantorovich potentials match those obtained on the restricted set ) . == > Thank you for the excellent suggestion . We have revised the notation accordingly . [ Q ] As a consequence , we can see the overall training procedure when H is fixed ( as it is the case in both experiments ) , as learning for a generator in a WGAN way , that also enforces a consistency constraint ( that can be seen as a regularization of the OT problem ) . While I like this idea , I think there might be some better justification for it ( it all starts with Eq.9 and the choice of this model . Why enforcing this minimal cost equation ? ) . == > Thank you for your constructive comments . We appreciate your comments that the consistency constraint works as a regularization term of the OT problem . Accordingly , we have employed your point in this revision . Moreover , instead of claiming that our method is a simple extension of the existing PLS with deep learning prior , we significantly expanded the discussion on the existing PLS approaches to highlight the difference of our formulation . We emphasize that the reason we enforce these minimal costs is to lead the primal solution to become a true inverse of the forward operator . In addition , we showed that the dual solution in the global optimum corresponds to the primal solution , which is not the case with the standard CycleGAN . Additionally , we show that while the existing PLS with the deep image prior can be similarly converted to the Kantorovich double formulation ( see Appendix C ) , but it does not provide a simple feedforward neural network . On the other hand , our new formulation leads to a CycleGAN architecture , which leads to a simple neural feed-forward network for the estimation of x . We highlighted this as one of the motivations for our framework . [ Q ] Note that the model imposes that p=q=1 , which is somehow limited . I guess other choices of p and q could be possible , eventually using regularized version of OT to remove the hard constraints on the kantorovich potentials . == > Thanks for your constructive comments . We have added the related discussion on page 6 of the revised paper . [ Q ] Eq.8 should include a weighting factor between the two terms since the dimensions are not the same . == > Thank you for your constructive comments . We have revised the notation in ( 9 ) . However , in all experimental scenarios in this article , the dimensions of x and y are the same . Therefore , for the sake of simplicity , the remainder of the paper is assumed $ \\lambda = 1 $ . [ Q ] In the proof of Lemma 1 , since you derive an equality for T , do you have to establish the first inequality ? == > Thanks for your comments , and we now revised the proof according to your suggestion . [ Q ] Prop.1 I guess there is an error on the first line ( the integral is not over \\mathcal { X } \\times \\mathcal { X } / A \\union B given the previous definition == > We truly appreciate your careful reading and correct all typo you pointed out . [ Q ] Missing related work == > Reference added ."}, {"review_id": "HklZUpEtvr-1", "review_text": "This paper frames and contextualizes CycleGAN as a stochastic generalization of penalized least squares for inverse problems, providing several unifying theorems, rederiving some modern CycleGAN architectures within the optimal transport framework, and also demonstrating the practical use of modified architectures derived using this framework for accelerated MRI and microscopy. While I did not check the proofs in detail, the additional generalization of a well-known, working architecture and additional variants used practically seem significant to me. Demonstrating the deep relationships in the proofs and propositions shown here, followed by two clear and concise derivations *and their practical application* is compelling, making this paper broadly applicable to both practitioners of compressed sensing and generative modeling. Frankly, it will take me some time to digest the proofs and overall relationships shown in the Propositions here, as I am not deeply familiar with optimal transport. The applied sections are direct, with Figure 5 being especially meaningful. There are a few small grammatical issues in the text - a careful re-read and edit with particular focus on grammar and style would be beneficial, though as it stands these small flaws didn't meaningfully detract from the paper. The primary thing the authors could do in order to raise my score, would be to take an additional pass at grammatical clarity for the paper. More experiments are always beneficial, and I would encourage the authors to release source code to replicate some form of their experiments if possible. Some additional references which may be useful additions, primarily for interested readers to gain further background on the use of deep models for compressed sensing: Compressed Sensing with Deep Image Prior and Learned Regularization https://arxiv.org/abs/1806.06438 Compressed Sensing using Generative Models https://arxiv.org/abs/1703.03208 Deep Compressed Sensing http://proceedings.mlr.press/v97/wu19d/wu19d.pdf", "rating": "6: Weak Accept", "reply_text": "[ Q ] the additional generalization of a well-known , working architecture and additional variants used practically seem significant to me . Demonstrating the deep relationships in the proofs and propositions shown here , followed by two clear and concise derivations * and their practical application * is compelling , making this paper broadly applicable to both practitioners of compressed sensing and generative modeling . The applied sections are direct , with Figure 5 being especially meaningful . == > Thanks for your careful reading and understanding of our contribution . [ Q ] There are a few small grammatical issues in the text - a careful re-read and edit with particular focus on grammar and style would be beneficial , though as it stands these small flaws did n't meaningfully detract from the paper . == > Thanks for the suggestion . An English native speaker has proofread the revision . [ Q ] The primary thing the authors could do in order to raise my score , would be to take an additional pass at grammatical clarity for the paper . More experiments are always beneficial , and I would encourage the authors to release source code to replicate some form of their experiments if possible . == > An English native speaker has proofread the revision . In the revised paper , we have conducted additional experiments and provided quantitative results for both accelerated MRI and deconvolution microscopy . In order to follow the policy stating that `` If you want to share your code with reviewers and ACs confidentially , please use the comment feature after the submission deadline '' , the anonymized code link is now provided to reviewers and ACs confidentially using the comment feature . Once the paper is accepted , we will make the link public . [ Q ] Some additional references which may be useful additions , primarily for interested readers to gain further background on the use of deep models for compressed sensing : == > Reference added ."}, {"review_id": "HklZUpEtvr-2", "review_text": "In this paper, the authors present two contributions: 1) The primary contribution is to show that CycleGAN can be formulated as a probabilistic version of a particular penalized-least squares problem (theory) 2) As proof of concept, they apply their version of CycleGAN to accelerated MRI and deconvolution microscopy (application) While I find the idea to be potentially interesting, the presentation of the theory is unclear and not well-motivated; after reading, I\u2019m not convinced that the connection to CycleGAN is as significant as the authors claim. The experimental results are preliminary. My decision is to reject. Below are separate critiques on the sections. Section 2-3: Hope the authors could clarify / strengthen these points in revision: - Since the discussion in Section 3 is based on the optimization problem in Equation (7), this problem should be well-motivated. Currently it is presented as a problem that has been explored previously by Zhang et al and Aggarwal et al. However, after taking a look at those papers, I don\u2019t understand where this regularization term comes from. In these papers, the regularization term (i.e. equation 2 or 3 of Zhang et al) appears independent of y. Since this term is key to the paper, it should be well explained here. E.g. at the end of section 2.2: G_\\theta(y) is a CNN pretrained on what task? - In the inverse problem, the objective is to estimate x from y. Therefore we care about \\argmin x in Equation (7). In the probabilistic setting presented in Equation (8), analogously the objective is to estimate \\pi^*, which is the solution to the primal problem. The theory shows that the primal formulation in Equation (8) is equivalent to the dual formulation in Equation (16), but does not show how the dual solution yields the primal solution, which is lacking as obtaining the primal solution seems to be the point of solving the PLS problem. (Interestingly, in Section 4, the authors are using the dual solution x = G_\\theta(y) as if it is the mapping given by pi(x|y)\u2026 this needs to be explained.) - The authors claim that Proposition 1 shows that the cyclic loss term in their dual formulation is a more general version of the cycle-consistency loss in CycleGAN. But looking closely at Proposition 1 and its proof, it seems that the equivalence holds only for specific weights, not for arbitrary weights. Additionally, the specific weights are unknown (they depend on the solution \\pi^* to the primal problem\u2026). I do not understand the claim that this is a generalization of cycle-consistency loss, nor do I see how the authors implement their version of the cyclic loss as it depends on unknown weights. - The connection to CycleGAN seems to hold only when p=q=1? - End of section 3: The authors conclude \u201cour cost formulation using (17) with (18) and (19) is more general compared to the standard CycleGAN, since a general form of measurement data generator Hx can be used\u201d. I don\u2019t see the connection between the theory and this claim. Even with CycleGAN, both generators can be arbitrary or fixed if one of them is known. - The proofs are easy to follow, though perhaps they could be moved to the Appendix in favor of providing more motivation and explanation in the main text. Section 4: - The authors motivate the problem with the PLS setup but then they use the learned regularization term x = G_\\theta(y) as if it is the mapping given by pi(x|y). I am confused by this. - Putting aside the connection to the PLS problem, my interpretation of the experimental setup is that the authors use CycleGAN with Wasserstein GAN loss instead of the classic discriminator loss, where one of the generators is known (and hence only one generator/discriminator pair is needed). I might be missing something, but I\u2019m not sure that this approach is different enough from CycleGAN. - Considering that the authors have the ground truth, they could provide quantitative evaluation of their method against other methods, rather than showing a few qualitative results where it is working. ", "rating": "1: Reject", "reply_text": "[ Q ] .. I \u2019 m not convinced that the connection to CycleGAN is as significant as the authors claim ... == > We would like to assure the reviewer that the primary motivation of this work is to provide a principled method for designing cycleGAN for various inverse problems by using the original physics as regularization for OT . This is an important advance over the existing CycleGAN , which is mainly derived from trial and error . In particular , the reason we enforce the proposed PLS cost for the OT problem is to lead the primal solution to become a true inverse of the forward operator at the global minimum . As the other reviewers pointed out , our formulation is more generic than the classical cycleGAN formulation , and demonstrating their deep relationships , followed by two clear and concise derivations and their practical application , is compelling . See General Comments . [ Q ] ... Currently it is presented as a problem that has been explored previously by Zhang et al and Aggarwal et al.H .. Since this term is key to the paper , . .. == > We found that the latex bib file incorrectly referred to different Zhang 's paper . We are quoting a correct one now . However , we understand the reviewers ' concern that the existing deep learning prior approaches are indirectly related to y . In this revision , rather than emphasizing our method as an extension of the existing PLS with deep learning prior , we have highlighted the difference in our formulation . We emphasize that the reason for using ( 9 ) is to lead the primal solution to become a true inverse of the forward operator . [ Q ] ... but does not show how the dual solution yields the primal solution .... == > Thanks to our PLS formulation ( 9 ) , we confirmed that the dual solution and the primal solution are equivalent when the global optimum is achieved with $ c ( x , y ; H , \\Theta ) = 0 $ . [ Q ] The authors claim ... the cyclic loss term in their dual formulation is a more general version of the cycle-consistency loss in CycleGAN . ... == > Kindly note that the term `` general version '' is used when , under certain conditions , the new formulation can be reduced to the standard one . Similar to the standard cycleGAN , the hyperparameters should be selected by trial and error . We agree that this is the limitation for both the standard and the proposed cycleGAN . However , our `` generalized '' formulation gives better insight into the selection of hyperparameters . In fact , the parameter is the relative match between two data distributions . If it turns out that both pairs are perfect , the parameter should be 1 . In a real training scenario , the perfect match can not be found so that the hyperparamereter should be between 0 and 1 . [ Q ] The connection to CycleGAN seems to hold only when p=q=1 ? == > We only considered $ p = q = 1 $ due to the simple c-transformation . The widely used W-GAN is derived similarly by assuming p = 1 . The use of general PLS costs is of course possible and could lead to an interesting variation of the cycleGAN architecture . [ Q ] ... Even with CycleGAN , both generators can be arbitrary or fixed if one of them is known . == > The standard CycleGAN could use a fixed generator , but there is no optimal design criterion to show why this is better . On the other hand , our formulation requires only a single deep generator if the measurement physics is given by the forward model Hx . In fact , as one of the other reviewers noted , this can be seen as a consistency term from the forward model to acts as a regularization term for OT . [ Q ] The proofs ... could be moved to the Appendix .. == > Done . [ Q ] ... but then they use the learned regularization term x = G_\\theta ( y ) as if it is the mapping given by pi ( x|y ) . == > Our novel PLS cost ( 9 ) enforces the dual solution to be the primal solution of the PLS when the global minimum is reached . Therefore , in this case , $ x = G_\\Theta ( y ) $ is actually the map given by $ \\pi ( x|y ) $ . [ Q ] ... my interpretation of the experimental setup is that the authors use CycleGAN with Wasserstein GAN loss instead of the classic discriminator loss , where one of the generators is known ... but I \u2019 m not sure that this approach is different enough from CycleGAN . == > Our main contribution is the principal derivation of the cycleGAN architecture for various inverse problems . If we use p = q = 1 , the resulting discriminator loss becomes Wasserstein GAN . However , with different p , q values and the regularized version of optimal transport , our formulation offers a new class of discriminator architecture . In addition , if the forward mapping is unknown , the framework is reduced to the standard cycleGAN with two deep generators . Therefore , our framework is much more flexible . [ Q ] .. they could provide quantitative evaluation of their method against other methods ... == > Done . In the revised paper , we have provided the quantitative results for both accelerated MRI and deconvolution microscopy . The results clearly showed the advantages of the proposed method ."}], "0": {"review_id": "HklZUpEtvr-0", "review_text": "The paper presents an interesting connection between cycleGAN, penalized Least Squares (PLS) and optimal transport (OT). From the PLS with deep regularization formulation, the authors cast a general OT problem with a specific cost combining measurement and reconstruction errors (eq. 9). From this formulation, the problem is expressed as a combination of a cycle consistency loss and an OT loss. The formulation is more generic than the classical cycleGAN formulation. Qualitative experiments are conducted on two different scenarii: accelerated MRI and deconvolution microscopy, for which the proposed method achieves good performances. In general I like the paper and the corresponding idea. However, I had a hard time understanding some of the key elements of the proposed method. Notably, in lemma 1, it is hard to understand in which case \\mathcal{X} \\times \\mathcal{X} / A \\union B is not an empty set. It is clearly the case whenever G or H are bijections, or whenever an exact reconstruction is achievable, but I guess a more open discussions could be conducted. Also, the definition of A and B are clearly dependent of G and H and this could be highlighted in the notation. As noted in proposition 2, however, the corresponding OT loss can be computed on the entire subset X and Y (since the Kantorovich potentials match those obtained on the restricted set). As a consequence, we can see the overall training procedure when H is fixed (as it is the case in both experiments), as learning for a generator in a WGAN way, that also enforces a consistency constraint (that can be seen as a regularization of the OT problem). While I like this idea, I think there might be some better justification for it (it all starts with Eq. 9 and the choice of this model. Why enforcing this minimal cost equation ?). Note that the model imposes that p=q=1, which is somehow limited. I guess other choices of p and q could be possible, eventually using regularized version of OT to remove the hard constraints on the kantorovich potentials I am willing to revise my note positively provided that a sufficient number of convincing explanations are given on my previous remarks. Minor remark Eq. 8 should include a weighting factor between the two terms since the dimensions are not the same. In the proof of Lemma 1, since you derive an equality for T, do you have to establish the first inequality ? Prop. 1 I guess there is an error on the first line (the integral is not over \\mathcal{X} \\times \\mathcal{X} / A \\union B given the previous definition Missing related work DLOW: Domain Flow for Adaptation and Generalization Rui Gong, Wen Li, Yuhua Chen, Luc Van Gool; The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 2477-2486 ", "rating": "6: Weak Accept", "reply_text": "[ Q ] The paper presents an interesting connection between cycleGAN , penalized Least Squares ( PLS ) and optimal transport ( OT ) ... The formulation is more generic than the classical cycleGAN formulation . == > Thanks for your careful reading and understanding of our contribution . [ Q ] In general I like the paper and the corresponding idea . However , I had a hard time understanding some of the key elements of the proposed method . Notably , in lemma 1 , it is hard to understand in which case \\mathcal { X } \\times \\mathcal { X } / A \\union B is not an empty set . It is clearly the case whenever G or H are bijections , or whenever an exact reconstruction is achievable , but I guess a more open discussions could be conducted . == > Thank you for your constructive comments . We would like to remind you of the nature of quantities A and B . Since both are graphs of functions , they are low-dimensional manifolds embedded in the $ X \\times Y $ ambient space . Accordingly , unlike the reviewer 's concern , the technical difficulty arises as the relative measure of $ A\\cup B $ may become zero . However , depending on the existence of the singularity in the distribution , this term can contribute a significant portion of the total T loss . In this paper , we claim that this situation leads to the architecture of cycleGAN . To clarify this , we have elaborated on our discussion on the singularity in the distribution on Page 4 and 5 . Moreover , the dependency on the singularity is also explicitly shown in $ \\ell_ { OT } $ or $ \\ell_ { GAN } $ so that how these terms behave depending on $ \\sigma $ and $ \\rho $ . [ Q ] Also , the definition of A and B are clearly dependent of G and H and this could be highlighted in the notation . As noted in proposition 2 , however , the corresponding OT loss can be computed on the entire subset X and Y ( since the Kantorovich potentials match those obtained on the restricted set ) . == > Thank you for the excellent suggestion . We have revised the notation accordingly . [ Q ] As a consequence , we can see the overall training procedure when H is fixed ( as it is the case in both experiments ) , as learning for a generator in a WGAN way , that also enforces a consistency constraint ( that can be seen as a regularization of the OT problem ) . While I like this idea , I think there might be some better justification for it ( it all starts with Eq.9 and the choice of this model . Why enforcing this minimal cost equation ? ) . == > Thank you for your constructive comments . We appreciate your comments that the consistency constraint works as a regularization term of the OT problem . Accordingly , we have employed your point in this revision . Moreover , instead of claiming that our method is a simple extension of the existing PLS with deep learning prior , we significantly expanded the discussion on the existing PLS approaches to highlight the difference of our formulation . We emphasize that the reason we enforce these minimal costs is to lead the primal solution to become a true inverse of the forward operator . In addition , we showed that the dual solution in the global optimum corresponds to the primal solution , which is not the case with the standard CycleGAN . Additionally , we show that while the existing PLS with the deep image prior can be similarly converted to the Kantorovich double formulation ( see Appendix C ) , but it does not provide a simple feedforward neural network . On the other hand , our new formulation leads to a CycleGAN architecture , which leads to a simple neural feed-forward network for the estimation of x . We highlighted this as one of the motivations for our framework . [ Q ] Note that the model imposes that p=q=1 , which is somehow limited . I guess other choices of p and q could be possible , eventually using regularized version of OT to remove the hard constraints on the kantorovich potentials . == > Thanks for your constructive comments . We have added the related discussion on page 6 of the revised paper . [ Q ] Eq.8 should include a weighting factor between the two terms since the dimensions are not the same . == > Thank you for your constructive comments . We have revised the notation in ( 9 ) . However , in all experimental scenarios in this article , the dimensions of x and y are the same . Therefore , for the sake of simplicity , the remainder of the paper is assumed $ \\lambda = 1 $ . [ Q ] In the proof of Lemma 1 , since you derive an equality for T , do you have to establish the first inequality ? == > Thanks for your comments , and we now revised the proof according to your suggestion . [ Q ] Prop.1 I guess there is an error on the first line ( the integral is not over \\mathcal { X } \\times \\mathcal { X } / A \\union B given the previous definition == > We truly appreciate your careful reading and correct all typo you pointed out . [ Q ] Missing related work == > Reference added ."}, "1": {"review_id": "HklZUpEtvr-1", "review_text": "This paper frames and contextualizes CycleGAN as a stochastic generalization of penalized least squares for inverse problems, providing several unifying theorems, rederiving some modern CycleGAN architectures within the optimal transport framework, and also demonstrating the practical use of modified architectures derived using this framework for accelerated MRI and microscopy. While I did not check the proofs in detail, the additional generalization of a well-known, working architecture and additional variants used practically seem significant to me. Demonstrating the deep relationships in the proofs and propositions shown here, followed by two clear and concise derivations *and their practical application* is compelling, making this paper broadly applicable to both practitioners of compressed sensing and generative modeling. Frankly, it will take me some time to digest the proofs and overall relationships shown in the Propositions here, as I am not deeply familiar with optimal transport. The applied sections are direct, with Figure 5 being especially meaningful. There are a few small grammatical issues in the text - a careful re-read and edit with particular focus on grammar and style would be beneficial, though as it stands these small flaws didn't meaningfully detract from the paper. The primary thing the authors could do in order to raise my score, would be to take an additional pass at grammatical clarity for the paper. More experiments are always beneficial, and I would encourage the authors to release source code to replicate some form of their experiments if possible. Some additional references which may be useful additions, primarily for interested readers to gain further background on the use of deep models for compressed sensing: Compressed Sensing with Deep Image Prior and Learned Regularization https://arxiv.org/abs/1806.06438 Compressed Sensing using Generative Models https://arxiv.org/abs/1703.03208 Deep Compressed Sensing http://proceedings.mlr.press/v97/wu19d/wu19d.pdf", "rating": "6: Weak Accept", "reply_text": "[ Q ] the additional generalization of a well-known , working architecture and additional variants used practically seem significant to me . Demonstrating the deep relationships in the proofs and propositions shown here , followed by two clear and concise derivations * and their practical application * is compelling , making this paper broadly applicable to both practitioners of compressed sensing and generative modeling . The applied sections are direct , with Figure 5 being especially meaningful . == > Thanks for your careful reading and understanding of our contribution . [ Q ] There are a few small grammatical issues in the text - a careful re-read and edit with particular focus on grammar and style would be beneficial , though as it stands these small flaws did n't meaningfully detract from the paper . == > Thanks for the suggestion . An English native speaker has proofread the revision . [ Q ] The primary thing the authors could do in order to raise my score , would be to take an additional pass at grammatical clarity for the paper . More experiments are always beneficial , and I would encourage the authors to release source code to replicate some form of their experiments if possible . == > An English native speaker has proofread the revision . In the revised paper , we have conducted additional experiments and provided quantitative results for both accelerated MRI and deconvolution microscopy . In order to follow the policy stating that `` If you want to share your code with reviewers and ACs confidentially , please use the comment feature after the submission deadline '' , the anonymized code link is now provided to reviewers and ACs confidentially using the comment feature . Once the paper is accepted , we will make the link public . [ Q ] Some additional references which may be useful additions , primarily for interested readers to gain further background on the use of deep models for compressed sensing : == > Reference added ."}, "2": {"review_id": "HklZUpEtvr-2", "review_text": "In this paper, the authors present two contributions: 1) The primary contribution is to show that CycleGAN can be formulated as a probabilistic version of a particular penalized-least squares problem (theory) 2) As proof of concept, they apply their version of CycleGAN to accelerated MRI and deconvolution microscopy (application) While I find the idea to be potentially interesting, the presentation of the theory is unclear and not well-motivated; after reading, I\u2019m not convinced that the connection to CycleGAN is as significant as the authors claim. The experimental results are preliminary. My decision is to reject. Below are separate critiques on the sections. Section 2-3: Hope the authors could clarify / strengthen these points in revision: - Since the discussion in Section 3 is based on the optimization problem in Equation (7), this problem should be well-motivated. Currently it is presented as a problem that has been explored previously by Zhang et al and Aggarwal et al. However, after taking a look at those papers, I don\u2019t understand where this regularization term comes from. In these papers, the regularization term (i.e. equation 2 or 3 of Zhang et al) appears independent of y. Since this term is key to the paper, it should be well explained here. E.g. at the end of section 2.2: G_\\theta(y) is a CNN pretrained on what task? - In the inverse problem, the objective is to estimate x from y. Therefore we care about \\argmin x in Equation (7). In the probabilistic setting presented in Equation (8), analogously the objective is to estimate \\pi^*, which is the solution to the primal problem. The theory shows that the primal formulation in Equation (8) is equivalent to the dual formulation in Equation (16), but does not show how the dual solution yields the primal solution, which is lacking as obtaining the primal solution seems to be the point of solving the PLS problem. (Interestingly, in Section 4, the authors are using the dual solution x = G_\\theta(y) as if it is the mapping given by pi(x|y)\u2026 this needs to be explained.) - The authors claim that Proposition 1 shows that the cyclic loss term in their dual formulation is a more general version of the cycle-consistency loss in CycleGAN. But looking closely at Proposition 1 and its proof, it seems that the equivalence holds only for specific weights, not for arbitrary weights. Additionally, the specific weights are unknown (they depend on the solution \\pi^* to the primal problem\u2026). I do not understand the claim that this is a generalization of cycle-consistency loss, nor do I see how the authors implement their version of the cyclic loss as it depends on unknown weights. - The connection to CycleGAN seems to hold only when p=q=1? - End of section 3: The authors conclude \u201cour cost formulation using (17) with (18) and (19) is more general compared to the standard CycleGAN, since a general form of measurement data generator Hx can be used\u201d. I don\u2019t see the connection between the theory and this claim. Even with CycleGAN, both generators can be arbitrary or fixed if one of them is known. - The proofs are easy to follow, though perhaps they could be moved to the Appendix in favor of providing more motivation and explanation in the main text. Section 4: - The authors motivate the problem with the PLS setup but then they use the learned regularization term x = G_\\theta(y) as if it is the mapping given by pi(x|y). I am confused by this. - Putting aside the connection to the PLS problem, my interpretation of the experimental setup is that the authors use CycleGAN with Wasserstein GAN loss instead of the classic discriminator loss, where one of the generators is known (and hence only one generator/discriminator pair is needed). I might be missing something, but I\u2019m not sure that this approach is different enough from CycleGAN. - Considering that the authors have the ground truth, they could provide quantitative evaluation of their method against other methods, rather than showing a few qualitative results where it is working. ", "rating": "1: Reject", "reply_text": "[ Q ] .. I \u2019 m not convinced that the connection to CycleGAN is as significant as the authors claim ... == > We would like to assure the reviewer that the primary motivation of this work is to provide a principled method for designing cycleGAN for various inverse problems by using the original physics as regularization for OT . This is an important advance over the existing CycleGAN , which is mainly derived from trial and error . In particular , the reason we enforce the proposed PLS cost for the OT problem is to lead the primal solution to become a true inverse of the forward operator at the global minimum . As the other reviewers pointed out , our formulation is more generic than the classical cycleGAN formulation , and demonstrating their deep relationships , followed by two clear and concise derivations and their practical application , is compelling . See General Comments . [ Q ] ... Currently it is presented as a problem that has been explored previously by Zhang et al and Aggarwal et al.H .. Since this term is key to the paper , . .. == > We found that the latex bib file incorrectly referred to different Zhang 's paper . We are quoting a correct one now . However , we understand the reviewers ' concern that the existing deep learning prior approaches are indirectly related to y . In this revision , rather than emphasizing our method as an extension of the existing PLS with deep learning prior , we have highlighted the difference in our formulation . We emphasize that the reason for using ( 9 ) is to lead the primal solution to become a true inverse of the forward operator . [ Q ] ... but does not show how the dual solution yields the primal solution .... == > Thanks to our PLS formulation ( 9 ) , we confirmed that the dual solution and the primal solution are equivalent when the global optimum is achieved with $ c ( x , y ; H , \\Theta ) = 0 $ . [ Q ] The authors claim ... the cyclic loss term in their dual formulation is a more general version of the cycle-consistency loss in CycleGAN . ... == > Kindly note that the term `` general version '' is used when , under certain conditions , the new formulation can be reduced to the standard one . Similar to the standard cycleGAN , the hyperparameters should be selected by trial and error . We agree that this is the limitation for both the standard and the proposed cycleGAN . However , our `` generalized '' formulation gives better insight into the selection of hyperparameters . In fact , the parameter is the relative match between two data distributions . If it turns out that both pairs are perfect , the parameter should be 1 . In a real training scenario , the perfect match can not be found so that the hyperparamereter should be between 0 and 1 . [ Q ] The connection to CycleGAN seems to hold only when p=q=1 ? == > We only considered $ p = q = 1 $ due to the simple c-transformation . The widely used W-GAN is derived similarly by assuming p = 1 . The use of general PLS costs is of course possible and could lead to an interesting variation of the cycleGAN architecture . [ Q ] ... Even with CycleGAN , both generators can be arbitrary or fixed if one of them is known . == > The standard CycleGAN could use a fixed generator , but there is no optimal design criterion to show why this is better . On the other hand , our formulation requires only a single deep generator if the measurement physics is given by the forward model Hx . In fact , as one of the other reviewers noted , this can be seen as a consistency term from the forward model to acts as a regularization term for OT . [ Q ] The proofs ... could be moved to the Appendix .. == > Done . [ Q ] ... but then they use the learned regularization term x = G_\\theta ( y ) as if it is the mapping given by pi ( x|y ) . == > Our novel PLS cost ( 9 ) enforces the dual solution to be the primal solution of the PLS when the global minimum is reached . Therefore , in this case , $ x = G_\\Theta ( y ) $ is actually the map given by $ \\pi ( x|y ) $ . [ Q ] ... my interpretation of the experimental setup is that the authors use CycleGAN with Wasserstein GAN loss instead of the classic discriminator loss , where one of the generators is known ... but I \u2019 m not sure that this approach is different enough from CycleGAN . == > Our main contribution is the principal derivation of the cycleGAN architecture for various inverse problems . If we use p = q = 1 , the resulting discriminator loss becomes Wasserstein GAN . However , with different p , q values and the regularized version of optimal transport , our formulation offers a new class of discriminator architecture . In addition , if the forward mapping is unknown , the framework is reduced to the standard cycleGAN with two deep generators . Therefore , our framework is much more flexible . [ Q ] .. they could provide quantitative evaluation of their method against other methods ... == > Done . In the revised paper , we have provided the quantitative results for both accelerated MRI and deconvolution microscopy . The results clearly showed the advantages of the proposed method ."}}