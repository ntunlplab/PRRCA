{"year": "2019", "forum": "SyxXhsAcFQ", "title": "Cohen Welling bases & SO(2)-Equivariant classifiers using Tensor nonlinearity.", "decision": "Reject", "meta_review": "This paper studies group equivariant neural network representations by building on the work by [Cohen and Welling, '14], which introduced learning of group irreducible representations, and [Kondor'18], who introduced tensor product non-linearities operating directly in the group Fourier domain. \n\nReviewers highlighted the significance of the approach, but were also unanimously concerned by the lack of clarity of the current manuscript, making its widespread impact within ICLR difficult, and the lack of a large-scale experiment that corroborates the usefulness of the approach. They were also very positive about the improvements of the paper during the author response phase. The AC completely agrees with this assessment of the paper. Therefore, the paper cannot be accepted at this time, but the AC strongly encourages the authors to resubmit their work in the next conference cycle by addressing the above remarks (improve clarity of presentation and include a large-scale experiment). ", "reviews": [{"review_id": "SyxXhsAcFQ-0", "review_text": "Review: This paper deals with the issue of learning rotation invariant autoencoders and classifiers. While this problem is well motivated, I found that this paper was fairly weak experimentally, and I also found it difficult to determine what the exact algorithm was. For example, how the optimization was done is not discussed at all. At the same time, I'm not an expert in group theory, so it's possible that the paper has technical novelty or significance which I did not appreciate. Strengths: -The challenge of learning rotation equivariant representations is well motivated and the idea of learning representations which transfer between different scales also seems useful. Weaknesses: -I had a difficult time understanding how the preliminaries (section 2) were related to the experiments (section 3). -The reference (Kondor 2018) is used a lot but could refer to three different papers that are in the references. -Only reported results are on rotated mnist, but the improvements seem reasonable, but unless I'm missing something are worse than the 1.62% error reported by harmonic nets (mentioned in the introduction of the paper). In addition to rot-mnist, harmonic nets evaluated boundary detection on the berkeley segmentation dataset. -It's interesting that the model learns to be somewhat invariant across scales, but I think that the baselines for this could be better. For example, using a convolution network with mean pooling at the end, one could estimate how well the normal classifier handles evaluation at a different scale from that used during training (I imagine the invariance would be somewhat bad but it's important to confirm). Questions: -Section 3.1 makes reference to \"learning parameters\". I assume that this is done in the usual way with backpropagation and then SGD/Adam or something? -How is it guaranteed that W is orthogonal in the learning procedure? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review comments . > > > Review : This paper deals with the issue of learning rotation invariant autoencoders and classifiers . While this problem is well motivated , I found that this paper was fairly weak experimentally , and I also found it difficult to determine what the exact algorithm was . For example , how the optimization was done is not discussed at all . At the same time , I 'm not an expert in group theory , so it 's possible that the paper has technical novelty or significance which I did not appreciate . [ Reply ] The algorithm for learning a CW basis is now stated explicitly in the appendix . We have summarised our approach in the response to AnonReviewer3 . We have given it as comments titled `` Summary 1 of 2 '' & `` Summary 2 of 2 '' . We discuss our Implementation now in Section 3.4 . The main technical novelty is that equivariance is easily learned in the CW basis . As AnonReviewer1 points out , tensor product nonlinearity is perhaps more important than the basis itself . -- -- - > > > Strengths : > > > -The challenge of learning rotation equivariant representations is well motivated and the idea of learning representations which transfer between different scales also seems useful . [ Reply ] Thanks for this encouragement . -- -- - > > > Weaknesses : > > > -I had a difficult time understanding how the preliminaries ( section 2 ) were related to the experiments ( section 3 ) . [ Reply ] Sorry about this . Perhaps a reason for confusion is that whereas we use the phrase G-morphism in Section 2 , we use the phrase SO ( 2 ) equivariant maps in Section 3 . These are the same . -- -- - > > > -The reference ( Kondor 2018 ) is used a lot but could refer to three different papers that are in the references . [ Reply ] Sorry about this . This is now corrected . -- -- - > > > -Only reported results are on rotated mnist , but the improvements seem reasonable , but unless I 'm missing something are worse than the 1.62 % error reported by harmonic nets ( mentioned in the introduction of the paper ) . In addition to rot-mnist , harmonic nets evaluated boundary detection on the berkeley segmentation dataset . [ Reply ] Yes we get about 97 % , less than what harmonic nets get but the architecture is very simple . One aspect we did not emphasise much is the last column in table 1 . It is known that Harmonic nets ( and many other equivariant networks ) need a large amount of data augmentation to perform well on MNIST-rot when trained on upright MNIST . We need no such augmentation once we have a reasonable W_ { 28 } . In that sense our network is like spherical and FFS2CNN - truly rotation equivariant . We will take a look at Berkeley segmentation data and see what harmonic nets do and see if we can conduct those experiments . Thanks for this suggestion . -- -- - > > > -It 's interesting that the model learns to be somewhat invariant across scales , but I think that the baselines for this could be better . For example , using a convolution network with mean pooling at the end , one could estimate how well the normal classifier handles evaluation at a different scale from that used during training ( I imagine the invariance would be somewhat bad but it 's important to confirm ) . [ Reply ] Thanks for this suggestion . We have run these experiments . We trained a CNN with about 489K parameters on MNIST-rot 28x28 images , getting a 95.1 percent accuracy . When this was fed 14x14 images scaled up to 28x28 , we get 90.5 percent accuracy . Should we report this in the main paper ? -- -- - > > > Questions : > > > -Section 3.1 makes reference to `` learning parameters '' . I assume that this is done in the usual way with backpropagation and then SGD/Adam or something ? [ Reply ] Yes , backpropogation using ADAM optimiser . We make this explicit in Section 3.4 -- -- - > > > -How is it guaranteed that W is orthogonal in the learning procedure ? [ Reply ] Sorry , we should have mentioned this -we do so now - we add a regularizer to the reconstruction loss . -- -- -"}, {"review_id": "SyxXhsAcFQ-1", "review_text": "This paper proposes autoencoder architectures based on Cohen-Welling bases for learning rotation-equivariant image representations. The models are evaluated by reconstruction error and classification in the space of the resulting basis on rotated-MNIST, showing performance improvements with small numbers of parameters and samples. I found most of this submission difficult to read and digest. I did not understand much of the exposition. I\u2019ll freely admit I haven\u2019t followed this line of work closely, and have little background in group theory, but I doubt I\u2019m much of an outlier among the ICLR audience in that regard. The \u201cPreliminaries\u201d section is very dense and provides little hand-holding for the reader in the form of context, intuition, or motivation for each definition and remark it enumerates. I can't tell how much of the section is connected to the proposed models. (For comparison, I skimmed the prior work that this submission primarily builds upon (Cohen & Welling, 2014) and found it relatively unintimidating. It gently introduces each concept in terms that most readers familiar with common machine learning conventions would be comfortable with. It's possible to follow the overall argument and get the \"gist\" of the paper without understanding every detail.) All that being said, I don\u2019t doubt this paper makes some interesting and important contributions -- I just don\u2019t understand what they are. Here are some specific comments and questions, mostly on the proposed approaches and experiments: * What actually is the \u201ctensor (product) nonlinearity\u201d? Given that this is in the title and is repeatedly emphasized in the text, I expected that it would be presented much more prominently. But after reading the entire paper I\u2019m still not 100% sure what \u201ctensor nonlinearity\u201d refers to. * Experiments: all models are described in long-form prose. It\u2019s very difficult to read and follow. This could be made much clearer with an algorithm box or similar. * The motivation for the \u201cCoupled Autoencoder\u201d model isn\u2019t clear. What, intuitively, is to be gained from reconstructing a high-resolution image from a low-resolution basis and vice versa? The empirical gains are marginal. * Experiments: the structure of the section is hard to follow. (1) and (2) are descriptions of two different models to do the same thing (autoencoding); then (3) (bootstrapping) is another step done on top of (1), and finally (4) is a classifier, trained on top of (1) or (2). This could benefit from restructuring. * There are long lists of integer multiplicities a_i and b_i: these seem to come out of nowhere, with no explanation of how or why they were chosen -- just that they result in \u201clearn[ing] a really sharp W_28\u201d. Why not learn them? * How are the models optimized? (Which optimizer, hyperparameters, etc.?) * The baseline methods should also be run on the smaller numbers of examples (500 or 12K) that the proposed approach is run on. * A planar CNN baseline should be considered for the autoencoder experiments. * Validating on MNIST alone (rotated, spherical, or otherwise) isn\u2019t good enough in 2018. The conclusions section mentions testing the models with deeper nets on CIFAR, but the results are not reported -- only hinting that it doesn\u2019t work well. This doesn\u2019t inspire much confidence. * Why are Spherical CNNs (Cohen et al., 2018) a good baseline for this dataset? The MNIST-rot data is not spherical. * Table 1: The method labels (Ours, 28/14 Tensor, and 28/14 Scale) are not very clear (though they are described in the text) * Table 1: Why not include the classification results for the standard AE? (They are in the Fig. 6 plot, but not the table.) * Conclusions: \u201cWe believe our classifiers built from bases learnt in a CAE architecture should be robust to noise\u201d -- Why? No reasons are given for this belief. * There are many typos and grammatical errors and odd/inconsistent formatting (e.g., underlined subsection headers) throughout the paper that should be revised.", "rating": "3: Clear rejection", "reply_text": "Thanks for the review comments . > > > I found most of this submission difficult to read and digest . I did not understand much of the exposition . ......... I don \u2019 t doubt this paper makes some interesting and important contributions -- I just don \u2019 t understand what they are . [ Reply ] We are sorry that the you found the paper difficult to read . We think one source for confusion could be that we never explicitly stated that the term G-morphism used in Section 2 is the same as equivariant map used in Section 3 . We do so now . We have a short summary of what we do in comments titled `` Summary 1 of 2 '' using a language more familiar to the ML community . Hope this helps . In `` Summary 2 of 2 '' ( again short : ) we show how we apply this . We could try and incorporate this into the main paper . -- -- - > > > Here are some specific comments and questions , mostly on the proposed approaches and experiments : > > > * What actually is the \u201c tensor ( product ) nonlinearity \u201d ? Given that this is in the title and is repeatedly emphasized in the text , I expected that it would be presented much more prominently . But after reading the entire paper I \u2019 m still not 100 % sure what \u201c tensor nonlinearity \u201d refers to . [ Reply ] We hope this is answered in the explanation given in comments titled `` Summary 1 of 2 '' . -- -- - > > > * Experiments : all models are described in long-form prose . It \u2019 s very difficult to read and follow . This could be made much clearer with an algorithm box or similar . [ Reply ] Since we were referring to diagrams to explain the algorithm we felt it was easier to follow it this way . However we have written Experiment 1 as an algorithm as suggested by you and AnonReviewer1 . Currently it is in the appendix . Please let us know if this should replace the long text . -- -- - > > > * The motivation for the \u201c Coupled Autoencoder \u201d model isn \u2019 t clear . What , intuitively , is to be gained from reconstructing a high-resolution image from a low-resolution basis and vice versa ? The empirical gains are marginal . [ Reply ] That the abstract elementary features learned from such a basis should be invariant to scale is the motivation for defining this . When we started we expected that features learned from this basis would be superior at classification , but our experiments show that is not the case . However the coupled bases could be used interchangeably as we show in Section 3.3 Results of classification , Coupling interchangeability . Our experiments also show that we can simultaneously learn Fourier bases in different scales , which can later deal with downsampled images . As an application : - Consider the problem of farmers having to deal with pests which they do not recognize , but limited by resources of bandwidth , and not having cellphones which take high resolution images . A solution would be to have a high end server deployed at a central location which is trained to recognise pests using say the basis from a Coupled autoencoder . When a farmer sees a new pest she could take a photograph of this on her cell phone and transmit this low resolution image to the server - the server can then use our model . ( This needs to be tested on real world examples , something we hope to take up ) -- -- - > > > * Experiments : the structure of the section is hard to follow . ( 1 ) and ( 2 ) are descriptions of two different models to do the same thing ( autoencoding ) ; then ( 3 ) ( bootstrapping ) is another step done on top of ( 1 ) , and finally ( 4 ) is a classifier , trained on top of ( 1 ) or ( 2 ) . This could benefit from restructuring . [ Reply ] Thanks for the suggestion - We have restructured it a little by giving subsection headings and we have rewritten some parts . -- -- - > > > * There are long lists of integer multiplicities a_i and b_i : these seem to come out of nowhere , with no explanation of how or why they were chosen -- just that they result in \u201c learn [ ing ] a really sharp W_28 \u201d . Why not learn them ? [ Reply ] These are hyperparameters fine tuned by us - how many CW basis vectors to choose which are indexed by integers 0 , 1 , ... , 24 respectively . As for learning them , yes we could try learning them and would like to do carry out experiments to see if that works . -- -- - > > > * How are the models optimized ? ( Which optimizer , hyperparameters , etc. ? ) [ Reply ] We mention this now explicitly in an Implementation Section 3.4 We use Adam optimiser and implement all of this in Tensorflow . We just used what tensor flow offers with no modification . Everywhere hyperparameters are multiplicities of irreducible representations in the domain and range of our SO ( 2 ) equivariant maps \\psi and \\phi . We do mention hyperparameters explicitly in all our experiments . -- -- -"}, {"review_id": "SyxXhsAcFQ-2", "review_text": "Recently there has been a spate of work on generalized CNNs that are equivariant to various symmetry groups, such a 2D and 3D rotations, the corresponding Euclidean groups (comprising not just rotations but also translations) and so on. The approach taken in most of the recent papers is to explicitly build in these equivariances by using the appropriate generalization of convolution. In the case of nontrivial groups this effectively means working in Fourier space, i.e., transforming to a basis that is adapted to the group action. This requires some considerations from represntation theory. Earlier, however, there was some less recognized work by Cohen and Welling on actually learning the correct basis itself from data. The present paper takes this second approach, and shows for a simple task like rotated MNIST, the basis can be learned from a remarkably small amount of data, and actually performs even better than some of the fixed basis methods. There is one major caveat: the nonlinearity itself has to be rotation-covariant, and for this purpose they use the recently introduced tensor product nonlinearities. The paper is a little rough around the edges. In the first 4 pages it launches into an exposition of ideas from representation theory which is too general for the purpose: SO(2) is a really simple commutative group, so the way that \"tensor product\" representations reduce to irreducibles could be summed up in the formula $e^{-2\\pi i k_1 x}e^{-2\\pi i k_2 x}=e^{-2\\pi i (k_1+k_2) x}$. I am not sure why the authors choose to use real representations (maybe because complex numbers are not supported in PyTorch, but this could easily be hacked) and I find that the real representations make things unnecessarily complicated. I suspect that at the end of the day the algorithm does something very simple (please clarify if working with real representations is somehow crucial). But this is exactly the beauty of the approach. The whole algorithm is very rough, there are only two layers (!), no effort to carefully implement nice exact group convolutions, and still the network is as good as the competition. Another significant point is that this network is only equivariant to rotations and not translations. Naturally, the question arises why one would want to learn the group adapted basis, when one could just compute it explicitly. There are two interesting lessons here that the authors could emphasize more: 1. Having a covariant nonlinearity is strong enough of a condition to force the network to learn a group adapted (Cohen-Welling) basis. This is interesting because Fourier space (\"tensor\") nonlinearities are a relatively new idea in the literature. This finding suggests that the nonlinearity might actually be more important than the basis. 2. The images that the authors work on are not functions on R^2, but just on a 28x28 grid. Rotating a rasterized image with eg. scikit-rotate introduces various artifacts. Similarly, going back and forth between a rasterized and polar coordinate based representation (which is effectively what would be required for \"Harmonic Networks\" and other Fourier methods) introduces messy interpolation issues. Not to mention downsampling, which is actually addressed in the paper. If a network can figure out how to best handle these issues from data, that makes things easier. The experiments are admittedly very small scale, although some of the other publications in this field also only have small experiments. At the very least it would be nice to have standard deviations on the results and some measure of statistical significance. It would be even nicer to have some visualization of the learned bases/filters, and a bare bones matrix-level very simple description of the algorith. Again, what is impressive here is that such a small network can learn to do this task reasonably well. Suggestions: 1. Also cite the Tensor Field Networks of Thomas et al in the context of tensor product nonlinearities. 2. Clean up the formatting. \"This leads us to the following\" in a line by itself looks strange. Similarly \"Classification ising the learned CW-basis\". I think something went wrong with \\itemize in Section 3.1. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for the review comments . > > > The paper is a little rough around the edges . In the first 4 pages it launches into an exposition of ideas from representation theory which is too general for the purpose : SO ( 2 ) is a really simple commutative group , so the way that `` tensor product '' representations reduce to irreducibles could be summed up in the formula $ e^ { -2\\pi i k_1 x } e^ { -2\\pi i k_2 x } =e^ { -2\\pi i ( k_1+k_2 ) x } $ . I am not sure why the authors choose to use real representations ( maybe because complex numbers are not supported in PyTorch , but this could easily be hacked ) and I find that the real representations make things unnecessarily complicated . I suspect that at the end of the day the algorithm does something very simple ( please clarify if working with real representations is somehow crucial ) . [ Reply ] Yes what you say is absolutely correct - that we need n't have presented it in this generality . But one reason to do this was to suggest that the same idea will work for other groups also if that group acts naturally on objects like we have SO ( 2 ) acting on images . All you need is to understand how tensor products of irreducibles split for that group . As we mention in the conclusion we have begun exploring with the symmetric group . We have implemented our algorithms in the complex world also and the results are almost the same . However since we are using tensor flow we decided to work with reals . And since we wished to highlight the Cohen -Welling paper as one of our inspirations , we work over reals following what Cohen and Welling do . -- -- - > > > But this is exactly the beauty of the approach . The whole algorithm is very rough , there are only two layers ( ! ) , no effort to carefully implement nice exact group convolutions , and still the network is as good as the competition . Another significant point is that this network is only equivariant to rotations and not translations . [ Reply ] Thanks for these encouraging comments . -- -- - > > > 1 . Having a covariant nonlinearity is strong enough of a condition to force the network to learn a group adapted ( Cohen-Welling ) basis . This is interesting because Fourier space ( `` tensor '' ) nonlinearities are a relatively new idea in the literature . This finding suggests that the nonlinearity might actually be more important than the basis . [ Reply ] Thanks for making this so explicit . We will include this in our paper . Please refer to the explanation given to AnonReviewer3 , where we summarise our work and point to this remark of yours . -- -- - > > > 2 . The images that the authors work on are not functions on R^2 , but just on a 28x28 grid . Rotating a rasterized image with eg . scikit-rotate introduces various artifacts . Similarly , going back and forth between a rasterized and polar coordinate based representation ( which is effectively what would be required for `` Harmonic Networks '' and other Fourier methods ) introduces messy interpolation issues . Not to mention downsampling , which is actually addressed in the paper . If a network can figure out how to best handle these issues from data , that makes things easier . [ Reply ] Again , thanks for the encouraging comments . We will emphasize these points . -- -- - > > > The experiments are admittedly very small scale , although some of the other publications in this field also only have small experiments . At the very least it would be nice to have standard deviations on the results and some measure of statistical significance . It would be even nicer to have some visualization of the learned bases/filters , and a bare bones matrix-level very simple description of the algorithm . Again , what is impressive here is that such a small network can learn to do this task reasonably well . [ Reply ] Thanks for this suggestion . We have given a separate table with some statistics of our experiments - our earlier table reported accuracies in the scale 0 to 1 but deviations are better expressed in percentage . So we have put a new table . Should we replace the earlier table with the new table ( adding the percentage accuracies of the baseline models ) ? And pictures of filters are now in the appendix . And again thanks for appreciating that a small network suffices . We have a complete description of Experiment 1 as an algorithm in the appendix now . Should we put this in place of the current text ? -- -- - > > > Suggestions : > > > 1 . Also cite the Tensor Field Networks of Thomas et al in the context of tensor product nonlinearities . [ Reply ] Thanks for pointing this out . We will make an explicit reference to this in the next revision . -- -- -- > > > 2 . Clean up the formatting . `` This leads us to the following '' in a line by itself looks strange . Similarly `` Classification ising the learned CW-basis '' . I think something went wrong with \\itemize in Section 3.1 . [ Reply ] Sure . Sorry for this . It has been cleaned up . -- -- -"}], "0": {"review_id": "SyxXhsAcFQ-0", "review_text": "Review: This paper deals with the issue of learning rotation invariant autoencoders and classifiers. While this problem is well motivated, I found that this paper was fairly weak experimentally, and I also found it difficult to determine what the exact algorithm was. For example, how the optimization was done is not discussed at all. At the same time, I'm not an expert in group theory, so it's possible that the paper has technical novelty or significance which I did not appreciate. Strengths: -The challenge of learning rotation equivariant representations is well motivated and the idea of learning representations which transfer between different scales also seems useful. Weaknesses: -I had a difficult time understanding how the preliminaries (section 2) were related to the experiments (section 3). -The reference (Kondor 2018) is used a lot but could refer to three different papers that are in the references. -Only reported results are on rotated mnist, but the improvements seem reasonable, but unless I'm missing something are worse than the 1.62% error reported by harmonic nets (mentioned in the introduction of the paper). In addition to rot-mnist, harmonic nets evaluated boundary detection on the berkeley segmentation dataset. -It's interesting that the model learns to be somewhat invariant across scales, but I think that the baselines for this could be better. For example, using a convolution network with mean pooling at the end, one could estimate how well the normal classifier handles evaluation at a different scale from that used during training (I imagine the invariance would be somewhat bad but it's important to confirm). Questions: -Section 3.1 makes reference to \"learning parameters\". I assume that this is done in the usual way with backpropagation and then SGD/Adam or something? -How is it guaranteed that W is orthogonal in the learning procedure? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the review comments . > > > Review : This paper deals with the issue of learning rotation invariant autoencoders and classifiers . While this problem is well motivated , I found that this paper was fairly weak experimentally , and I also found it difficult to determine what the exact algorithm was . For example , how the optimization was done is not discussed at all . At the same time , I 'm not an expert in group theory , so it 's possible that the paper has technical novelty or significance which I did not appreciate . [ Reply ] The algorithm for learning a CW basis is now stated explicitly in the appendix . We have summarised our approach in the response to AnonReviewer3 . We have given it as comments titled `` Summary 1 of 2 '' & `` Summary 2 of 2 '' . We discuss our Implementation now in Section 3.4 . The main technical novelty is that equivariance is easily learned in the CW basis . As AnonReviewer1 points out , tensor product nonlinearity is perhaps more important than the basis itself . -- -- - > > > Strengths : > > > -The challenge of learning rotation equivariant representations is well motivated and the idea of learning representations which transfer between different scales also seems useful . [ Reply ] Thanks for this encouragement . -- -- - > > > Weaknesses : > > > -I had a difficult time understanding how the preliminaries ( section 2 ) were related to the experiments ( section 3 ) . [ Reply ] Sorry about this . Perhaps a reason for confusion is that whereas we use the phrase G-morphism in Section 2 , we use the phrase SO ( 2 ) equivariant maps in Section 3 . These are the same . -- -- - > > > -The reference ( Kondor 2018 ) is used a lot but could refer to three different papers that are in the references . [ Reply ] Sorry about this . This is now corrected . -- -- - > > > -Only reported results are on rotated mnist , but the improvements seem reasonable , but unless I 'm missing something are worse than the 1.62 % error reported by harmonic nets ( mentioned in the introduction of the paper ) . In addition to rot-mnist , harmonic nets evaluated boundary detection on the berkeley segmentation dataset . [ Reply ] Yes we get about 97 % , less than what harmonic nets get but the architecture is very simple . One aspect we did not emphasise much is the last column in table 1 . It is known that Harmonic nets ( and many other equivariant networks ) need a large amount of data augmentation to perform well on MNIST-rot when trained on upright MNIST . We need no such augmentation once we have a reasonable W_ { 28 } . In that sense our network is like spherical and FFS2CNN - truly rotation equivariant . We will take a look at Berkeley segmentation data and see what harmonic nets do and see if we can conduct those experiments . Thanks for this suggestion . -- -- - > > > -It 's interesting that the model learns to be somewhat invariant across scales , but I think that the baselines for this could be better . For example , using a convolution network with mean pooling at the end , one could estimate how well the normal classifier handles evaluation at a different scale from that used during training ( I imagine the invariance would be somewhat bad but it 's important to confirm ) . [ Reply ] Thanks for this suggestion . We have run these experiments . We trained a CNN with about 489K parameters on MNIST-rot 28x28 images , getting a 95.1 percent accuracy . When this was fed 14x14 images scaled up to 28x28 , we get 90.5 percent accuracy . Should we report this in the main paper ? -- -- - > > > Questions : > > > -Section 3.1 makes reference to `` learning parameters '' . I assume that this is done in the usual way with backpropagation and then SGD/Adam or something ? [ Reply ] Yes , backpropogation using ADAM optimiser . We make this explicit in Section 3.4 -- -- - > > > -How is it guaranteed that W is orthogonal in the learning procedure ? [ Reply ] Sorry , we should have mentioned this -we do so now - we add a regularizer to the reconstruction loss . -- -- -"}, "1": {"review_id": "SyxXhsAcFQ-1", "review_text": "This paper proposes autoencoder architectures based on Cohen-Welling bases for learning rotation-equivariant image representations. The models are evaluated by reconstruction error and classification in the space of the resulting basis on rotated-MNIST, showing performance improvements with small numbers of parameters and samples. I found most of this submission difficult to read and digest. I did not understand much of the exposition. I\u2019ll freely admit I haven\u2019t followed this line of work closely, and have little background in group theory, but I doubt I\u2019m much of an outlier among the ICLR audience in that regard. The \u201cPreliminaries\u201d section is very dense and provides little hand-holding for the reader in the form of context, intuition, or motivation for each definition and remark it enumerates. I can't tell how much of the section is connected to the proposed models. (For comparison, I skimmed the prior work that this submission primarily builds upon (Cohen & Welling, 2014) and found it relatively unintimidating. It gently introduces each concept in terms that most readers familiar with common machine learning conventions would be comfortable with. It's possible to follow the overall argument and get the \"gist\" of the paper without understanding every detail.) All that being said, I don\u2019t doubt this paper makes some interesting and important contributions -- I just don\u2019t understand what they are. Here are some specific comments and questions, mostly on the proposed approaches and experiments: * What actually is the \u201ctensor (product) nonlinearity\u201d? Given that this is in the title and is repeatedly emphasized in the text, I expected that it would be presented much more prominently. But after reading the entire paper I\u2019m still not 100% sure what \u201ctensor nonlinearity\u201d refers to. * Experiments: all models are described in long-form prose. It\u2019s very difficult to read and follow. This could be made much clearer with an algorithm box or similar. * The motivation for the \u201cCoupled Autoencoder\u201d model isn\u2019t clear. What, intuitively, is to be gained from reconstructing a high-resolution image from a low-resolution basis and vice versa? The empirical gains are marginal. * Experiments: the structure of the section is hard to follow. (1) and (2) are descriptions of two different models to do the same thing (autoencoding); then (3) (bootstrapping) is another step done on top of (1), and finally (4) is a classifier, trained on top of (1) or (2). This could benefit from restructuring. * There are long lists of integer multiplicities a_i and b_i: these seem to come out of nowhere, with no explanation of how or why they were chosen -- just that they result in \u201clearn[ing] a really sharp W_28\u201d. Why not learn them? * How are the models optimized? (Which optimizer, hyperparameters, etc.?) * The baseline methods should also be run on the smaller numbers of examples (500 or 12K) that the proposed approach is run on. * A planar CNN baseline should be considered for the autoencoder experiments. * Validating on MNIST alone (rotated, spherical, or otherwise) isn\u2019t good enough in 2018. The conclusions section mentions testing the models with deeper nets on CIFAR, but the results are not reported -- only hinting that it doesn\u2019t work well. This doesn\u2019t inspire much confidence. * Why are Spherical CNNs (Cohen et al., 2018) a good baseline for this dataset? The MNIST-rot data is not spherical. * Table 1: The method labels (Ours, 28/14 Tensor, and 28/14 Scale) are not very clear (though they are described in the text) * Table 1: Why not include the classification results for the standard AE? (They are in the Fig. 6 plot, but not the table.) * Conclusions: \u201cWe believe our classifiers built from bases learnt in a CAE architecture should be robust to noise\u201d -- Why? No reasons are given for this belief. * There are many typos and grammatical errors and odd/inconsistent formatting (e.g., underlined subsection headers) throughout the paper that should be revised.", "rating": "3: Clear rejection", "reply_text": "Thanks for the review comments . > > > I found most of this submission difficult to read and digest . I did not understand much of the exposition . ......... I don \u2019 t doubt this paper makes some interesting and important contributions -- I just don \u2019 t understand what they are . [ Reply ] We are sorry that the you found the paper difficult to read . We think one source for confusion could be that we never explicitly stated that the term G-morphism used in Section 2 is the same as equivariant map used in Section 3 . We do so now . We have a short summary of what we do in comments titled `` Summary 1 of 2 '' using a language more familiar to the ML community . Hope this helps . In `` Summary 2 of 2 '' ( again short : ) we show how we apply this . We could try and incorporate this into the main paper . -- -- - > > > Here are some specific comments and questions , mostly on the proposed approaches and experiments : > > > * What actually is the \u201c tensor ( product ) nonlinearity \u201d ? Given that this is in the title and is repeatedly emphasized in the text , I expected that it would be presented much more prominently . But after reading the entire paper I \u2019 m still not 100 % sure what \u201c tensor nonlinearity \u201d refers to . [ Reply ] We hope this is answered in the explanation given in comments titled `` Summary 1 of 2 '' . -- -- - > > > * Experiments : all models are described in long-form prose . It \u2019 s very difficult to read and follow . This could be made much clearer with an algorithm box or similar . [ Reply ] Since we were referring to diagrams to explain the algorithm we felt it was easier to follow it this way . However we have written Experiment 1 as an algorithm as suggested by you and AnonReviewer1 . Currently it is in the appendix . Please let us know if this should replace the long text . -- -- - > > > * The motivation for the \u201c Coupled Autoencoder \u201d model isn \u2019 t clear . What , intuitively , is to be gained from reconstructing a high-resolution image from a low-resolution basis and vice versa ? The empirical gains are marginal . [ Reply ] That the abstract elementary features learned from such a basis should be invariant to scale is the motivation for defining this . When we started we expected that features learned from this basis would be superior at classification , but our experiments show that is not the case . However the coupled bases could be used interchangeably as we show in Section 3.3 Results of classification , Coupling interchangeability . Our experiments also show that we can simultaneously learn Fourier bases in different scales , which can later deal with downsampled images . As an application : - Consider the problem of farmers having to deal with pests which they do not recognize , but limited by resources of bandwidth , and not having cellphones which take high resolution images . A solution would be to have a high end server deployed at a central location which is trained to recognise pests using say the basis from a Coupled autoencoder . When a farmer sees a new pest she could take a photograph of this on her cell phone and transmit this low resolution image to the server - the server can then use our model . ( This needs to be tested on real world examples , something we hope to take up ) -- -- - > > > * Experiments : the structure of the section is hard to follow . ( 1 ) and ( 2 ) are descriptions of two different models to do the same thing ( autoencoding ) ; then ( 3 ) ( bootstrapping ) is another step done on top of ( 1 ) , and finally ( 4 ) is a classifier , trained on top of ( 1 ) or ( 2 ) . This could benefit from restructuring . [ Reply ] Thanks for the suggestion - We have restructured it a little by giving subsection headings and we have rewritten some parts . -- -- - > > > * There are long lists of integer multiplicities a_i and b_i : these seem to come out of nowhere , with no explanation of how or why they were chosen -- just that they result in \u201c learn [ ing ] a really sharp W_28 \u201d . Why not learn them ? [ Reply ] These are hyperparameters fine tuned by us - how many CW basis vectors to choose which are indexed by integers 0 , 1 , ... , 24 respectively . As for learning them , yes we could try learning them and would like to do carry out experiments to see if that works . -- -- - > > > * How are the models optimized ? ( Which optimizer , hyperparameters , etc. ? ) [ Reply ] We mention this now explicitly in an Implementation Section 3.4 We use Adam optimiser and implement all of this in Tensorflow . We just used what tensor flow offers with no modification . Everywhere hyperparameters are multiplicities of irreducible representations in the domain and range of our SO ( 2 ) equivariant maps \\psi and \\phi . We do mention hyperparameters explicitly in all our experiments . -- -- -"}, "2": {"review_id": "SyxXhsAcFQ-2", "review_text": "Recently there has been a spate of work on generalized CNNs that are equivariant to various symmetry groups, such a 2D and 3D rotations, the corresponding Euclidean groups (comprising not just rotations but also translations) and so on. The approach taken in most of the recent papers is to explicitly build in these equivariances by using the appropriate generalization of convolution. In the case of nontrivial groups this effectively means working in Fourier space, i.e., transforming to a basis that is adapted to the group action. This requires some considerations from represntation theory. Earlier, however, there was some less recognized work by Cohen and Welling on actually learning the correct basis itself from data. The present paper takes this second approach, and shows for a simple task like rotated MNIST, the basis can be learned from a remarkably small amount of data, and actually performs even better than some of the fixed basis methods. There is one major caveat: the nonlinearity itself has to be rotation-covariant, and for this purpose they use the recently introduced tensor product nonlinearities. The paper is a little rough around the edges. In the first 4 pages it launches into an exposition of ideas from representation theory which is too general for the purpose: SO(2) is a really simple commutative group, so the way that \"tensor product\" representations reduce to irreducibles could be summed up in the formula $e^{-2\\pi i k_1 x}e^{-2\\pi i k_2 x}=e^{-2\\pi i (k_1+k_2) x}$. I am not sure why the authors choose to use real representations (maybe because complex numbers are not supported in PyTorch, but this could easily be hacked) and I find that the real representations make things unnecessarily complicated. I suspect that at the end of the day the algorithm does something very simple (please clarify if working with real representations is somehow crucial). But this is exactly the beauty of the approach. The whole algorithm is very rough, there are only two layers (!), no effort to carefully implement nice exact group convolutions, and still the network is as good as the competition. Another significant point is that this network is only equivariant to rotations and not translations. Naturally, the question arises why one would want to learn the group adapted basis, when one could just compute it explicitly. There are two interesting lessons here that the authors could emphasize more: 1. Having a covariant nonlinearity is strong enough of a condition to force the network to learn a group adapted (Cohen-Welling) basis. This is interesting because Fourier space (\"tensor\") nonlinearities are a relatively new idea in the literature. This finding suggests that the nonlinearity might actually be more important than the basis. 2. The images that the authors work on are not functions on R^2, but just on a 28x28 grid. Rotating a rasterized image with eg. scikit-rotate introduces various artifacts. Similarly, going back and forth between a rasterized and polar coordinate based representation (which is effectively what would be required for \"Harmonic Networks\" and other Fourier methods) introduces messy interpolation issues. Not to mention downsampling, which is actually addressed in the paper. If a network can figure out how to best handle these issues from data, that makes things easier. The experiments are admittedly very small scale, although some of the other publications in this field also only have small experiments. At the very least it would be nice to have standard deviations on the results and some measure of statistical significance. It would be even nicer to have some visualization of the learned bases/filters, and a bare bones matrix-level very simple description of the algorith. Again, what is impressive here is that such a small network can learn to do this task reasonably well. Suggestions: 1. Also cite the Tensor Field Networks of Thomas et al in the context of tensor product nonlinearities. 2. Clean up the formatting. \"This leads us to the following\" in a line by itself looks strange. Similarly \"Classification ising the learned CW-basis\". I think something went wrong with \\itemize in Section 3.1. ", "rating": "7: Good paper, accept", "reply_text": "Thanks for the review comments . > > > The paper is a little rough around the edges . In the first 4 pages it launches into an exposition of ideas from representation theory which is too general for the purpose : SO ( 2 ) is a really simple commutative group , so the way that `` tensor product '' representations reduce to irreducibles could be summed up in the formula $ e^ { -2\\pi i k_1 x } e^ { -2\\pi i k_2 x } =e^ { -2\\pi i ( k_1+k_2 ) x } $ . I am not sure why the authors choose to use real representations ( maybe because complex numbers are not supported in PyTorch , but this could easily be hacked ) and I find that the real representations make things unnecessarily complicated . I suspect that at the end of the day the algorithm does something very simple ( please clarify if working with real representations is somehow crucial ) . [ Reply ] Yes what you say is absolutely correct - that we need n't have presented it in this generality . But one reason to do this was to suggest that the same idea will work for other groups also if that group acts naturally on objects like we have SO ( 2 ) acting on images . All you need is to understand how tensor products of irreducibles split for that group . As we mention in the conclusion we have begun exploring with the symmetric group . We have implemented our algorithms in the complex world also and the results are almost the same . However since we are using tensor flow we decided to work with reals . And since we wished to highlight the Cohen -Welling paper as one of our inspirations , we work over reals following what Cohen and Welling do . -- -- - > > > But this is exactly the beauty of the approach . The whole algorithm is very rough , there are only two layers ( ! ) , no effort to carefully implement nice exact group convolutions , and still the network is as good as the competition . Another significant point is that this network is only equivariant to rotations and not translations . [ Reply ] Thanks for these encouraging comments . -- -- - > > > 1 . Having a covariant nonlinearity is strong enough of a condition to force the network to learn a group adapted ( Cohen-Welling ) basis . This is interesting because Fourier space ( `` tensor '' ) nonlinearities are a relatively new idea in the literature . This finding suggests that the nonlinearity might actually be more important than the basis . [ Reply ] Thanks for making this so explicit . We will include this in our paper . Please refer to the explanation given to AnonReviewer3 , where we summarise our work and point to this remark of yours . -- -- - > > > 2 . The images that the authors work on are not functions on R^2 , but just on a 28x28 grid . Rotating a rasterized image with eg . scikit-rotate introduces various artifacts . Similarly , going back and forth between a rasterized and polar coordinate based representation ( which is effectively what would be required for `` Harmonic Networks '' and other Fourier methods ) introduces messy interpolation issues . Not to mention downsampling , which is actually addressed in the paper . If a network can figure out how to best handle these issues from data , that makes things easier . [ Reply ] Again , thanks for the encouraging comments . We will emphasize these points . -- -- - > > > The experiments are admittedly very small scale , although some of the other publications in this field also only have small experiments . At the very least it would be nice to have standard deviations on the results and some measure of statistical significance . It would be even nicer to have some visualization of the learned bases/filters , and a bare bones matrix-level very simple description of the algorithm . Again , what is impressive here is that such a small network can learn to do this task reasonably well . [ Reply ] Thanks for this suggestion . We have given a separate table with some statistics of our experiments - our earlier table reported accuracies in the scale 0 to 1 but deviations are better expressed in percentage . So we have put a new table . Should we replace the earlier table with the new table ( adding the percentage accuracies of the baseline models ) ? And pictures of filters are now in the appendix . And again thanks for appreciating that a small network suffices . We have a complete description of Experiment 1 as an algorithm in the appendix now . Should we put this in place of the current text ? -- -- - > > > Suggestions : > > > 1 . Also cite the Tensor Field Networks of Thomas et al in the context of tensor product nonlinearities . [ Reply ] Thanks for pointing this out . We will make an explicit reference to this in the next revision . -- -- -- > > > 2 . Clean up the formatting . `` This leads us to the following '' in a line by itself looks strange . Similarly `` Classification ising the learned CW-basis '' . I think something went wrong with \\itemize in Section 3.1 . [ Reply ] Sure . Sorry for this . It has been cleaned up . -- -- -"}}