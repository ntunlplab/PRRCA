{"year": "2019", "forum": "H1MzKs05F7", "title": "Adversarial Vulnerability of Neural Networks Increases with Input Dimension", "decision": "Reject", "meta_review": "This paper suggests that adversarial vulnerability scales with the dimension of the input of neural networks, and support this hypothesis theoretically and experimentally. \n\nThe work is well-written, and all of the reviewers appreciated the easy-to-read and clear nature of the theoretical results, including the assumptions and limitations. (The AC did not consider the criticisms raised by Reviewer 3 justified. The norm-bound perturbations considered here are a sufficiently interesting unsolved problem in the community and a clear prerequisite to solving the broader network robustness problem.) \n\nHowever, many of the reviewers also agreed that the theoretical assumptions - and, in particular, the random initialization of the weights - greatly oversimplify the problem. Reviewers point out that the lack of data dependence and only considering the norm of the gradient considerably limit the significance of the corresponding theoretical results, and also does not properly address the issue of gradient masking. ", "reviews": [{"review_id": "H1MzKs05F7-0", "review_text": "The paper studies how the vulnerability of a neural network model depends on its input dimension. The authors prove that for an *untrained* model, randomly initialized with Xavier initialization, the gradient of the loss wrt the input is essentially independent of the architecture and task. This implies that the major factor affecting the norm of that gradient is the input dimension. They then support their argument by experiments measuring the relation between adversarial vulnerability and gradient norm using various *trained* models (including adversarially regularized ones). I find the main theoretical result interesting. While this is a known fact for the simple case of linear classifiers, extending it to arbitrarily deep networks is a valuable contribution. The proof crucially relies on properties of the specific initialization scheme to show that the gradient does not change too much during backproparagation through the layers. The most significant limitation of the result (which the authors kindly acknowledge) is that this result only holds at initialization. Hence it cannot distinguish between different training methods or between how different architectures evolve during training. Since the situation in adversarial robustness is much more nuanced, I am skeptical about the significance of such statements. On the experimental side, the finding that gradient regularization improves adversarial robustness to small epsilon values has been made multiple times in the past (as the authors cite in the related work section). It is worth noting that the epsilon considered is 0.005 in L_inf (1.275/255) which is pretty small. This value corresponds to the \"small-epsilon regime\" where the behavior of the model is fairly linear around the original inputs and thus defenses such as FGSM-training and gradient regularization are effective. The authors also perform an interesting experiment where they train models on downsampled ImageNet datasets and find that indeed larger input dimension leads to more vulnerable models. While I find the results interesting, I do not see clear implications. The fact that the vulnerability of a classifier depends on the L1 norm of the input gradient is already known for any locally linear classifier (i.e. deep models too), and it is fairly clear that the L1 norm will have a dimension dependence. The fact that it does not depend on architecture or task at initialization is interesting but of limited significance in my opinion. Given that the experimental results are also not particularly novel, I recommend rejection. [UPDATE]: Given the overall discussion and paper updates, I consider the current version of the paper (marginally) crossing the ICLR bar. I update my score from a 5 to a 6. Minor comments to the authors: -- I think || x ||_* is more clear than |||x||| for the dual norm. -- Consider using lambda for the regularization, epsilon is confusing since it is overloaded.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for your careful review , and for pointing out that many people do indeed care about small worst-case l_p-perturbations , and why . - Concerning : `` I am skeptical about the significance of such statements [ at initialization ] . '' & `` While I find the results interesting , I do not see clear implications . '' Please refer to our overall thread on , why understanding the vulnerability of priors helps understanding post ( robust ) training vulnerability . See also point 4/ in our overall reply . - Concerning : `` While this is a known fact for the simple case of linear classifiers ... '' Even with only linear classifiers , previous published work has predicted a linear increase of vulnerability with input-dimension rather than sqrt ( d ) , because they did not take the dimension-dependance of the weights into account . - Concerning : `` it is fairly clear that the L1 norm will have a dimension dependence '' Maybe , but it is all about getting the numbers right . Our predictions correspond to the * exact * increase-rate measured in practice . - Concerning : `` The fact that it does not depend on architecture or task at initialization is interesting but of limited significance in my opinion . '' This independence on architecture at initialization shows that , if we want to get non-vulnerable priors , we need to re-think our initialization scheme and/or introduce a new architectural building block . As to why we would want non-vulnerable priors , again , please see our overall thread on the subject . Once again , we thank you for your review and hope that our answers may help you to re-evaluate the significance of our results ."}, {"review_id": "H1MzKs05F7-1", "review_text": "This paper argues that adversarial vulnerability of neural networks increases with input dimension. Theoretical and empirical evidence are given which connect the l_p norm of the gradient of the training objective with the existence of small-worst case l_q perturbations. This connection is made by assuming that the learned function is well approximated by a linear function local to the sampled input x. By making assumptions on the initialization scheme for some simple architectures, the authors show that the l_p norm of the gradient for randomly initialized network will be large, and provide empirical evidence that these assumptions hold after training. These assumptions imply bounds on the typical magnitude of the gradient of the loss with respect to a single input coordinate, this then implies that the overall gradient norm will depend on the input dimension. I found this paper well written. The mathematical assumptions are presented in a clear, easy to understand manner. Also high level intuition is given around their main theorems which help the reader understand the main ideas. However, I have a number of concerns about this work. The first is, I do not buy the motivation for studying the \"phenomenon\" of small worst-case l_p perturbations. I realize this statement applies to a large body of literature, but since the publication of [1] we are still lacking concrete motivating scenarios for the l_p action space. I would encourage the authors instead to ask the closely related but more general question of how we can improve model generalization outside the natural distribution of images, such as generalization in the presence of commonly occurring image corruptions [2]. It's possible that the analysis in this work could better our understanding model generalization in the presence of different image corruptions, indeed by making similar linearity assumptions as considered in this work, test error in additive Gaussian noise can be linked with distance to the decision boundary [3,4]. However, this particular question was not explored in this work. Second, the work is one of many to relate the norm of the gradient with adversarial robustness (for example, this has been proposed as a defense mechanism in [5,6]). I also suspect that the main theorem relating gradient norm to initialization should easily follow for more general settings using the mean field theory developed by [7,8] (this would be particularly useful for removing assumption H1, which assumes the ReLU activation is a random variable independent of the weights). Overall, I don't see how gradient norms explain why statistical classifiers make mistakes, particularly for more realistic attacker action spaces [9]. Even for \"small\" l_p adversarial examples there seem to be limitations as to how much gradient norms can explain the phenomenon --- for example even max margin classifiers such as SVM's have \"adversarial examples\". Furthermore, adversarial training has been shown to reach a point where the model is \"robust\" locally to training points but this robustness does not generalize to the points in the test set [10]. In fact, for the synthetic data distributions considered in [10], it's proven that no learning algorithm can achieve robustness given insufficient training data. Finally, the main conclusion of this work \"adversarial vulnerability of neural networks increases with input dimension\" is an overly general statement which needs a much more nuanced view. While experiments shown in [11] support this conclusion for naturally trained networks, it is shown that when adversarial training is applied the model is more robust when the input dimension is higher (see Figure 4 a. and b.). Perhaps the assumptions for Theorem 4 are violated for these adversarially trained models. 1. https://arxiv.org/abs/1807.06732 2. https://arxiv.org/abs/1807.01697 3. https://arxiv.org/abs/1608.08967 4. https://openreview.net/forum?id=S1xoy3CcYX&noteId=BklKxJBF57. 5. https://arxiv.org/abs/1704.08847 6. https://arxiv.org/abs/1608.07690 7. https://arxiv.org/abs/1611.01232 8. https://arxiv.org/abs/1806.05393 9. https://arxiv.org/abs/1712.09665 10. https://arxiv.org/abs/1804.11285 11. https://arxiv.org/pdf/1809.02104.pdf", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank you for your expanded reviews , comments , questions and references , which we hope to address in full . -Concerning : `` The experiments for adversarially trained models in [ 1 ] directly contradict the title of this paper '' See point 2/ of our overall reply . - Concerning : `` for some settings of the weights you can show a bound such as is discussed in the paper , but there are other settings ( perhaps even initializations ) of the weights for which the conclusion will not hold . '' The current initialization-methods are used to avoid exploding/vanishing activations at init . Any other initialization would need to solve that issue . See point 3/ of our overall reply . - Concerning : `` fixing the initialization seems unlikely to buy us much more than what adversarial training achieves , and the experiments in [ 1 ] suggest to me the conclusion of this work is limited in scope '' Please refer to our overall thread on `` why prior vulnerability matters '' and how it might help understanding and harnessing the vulnerability of ( robustly ) trained networks . - Concerning : `` we hit a limit as we increase the epsilon considered for the perturbations '' Even for small epsilons , our networks are surprisingly vulnerable . If we do n't understand the small epsilon vulnerability , then we wo n't understand big epsilons . - Concerning : `` it \u2019 s not clear to me what actionable insights we can conclude from this work , and how this can be used to improve upon the current SOTA . '' Again , please see our thread on `` why prior vulnerability matters '' and how it may help understanding and harnessing the vulnerability of ( robustly ) trained networks . - Concerning : `` it was found that adversarial training eventually gives robustness to the training set , but this robustness does not generalize to the test set [ 2 ] ... the data distribution . '' See 4/ in our overall reply . - Concerning : `` in [ 2 ] it was shown that there is no learning algorithm [ that ] can become robust to small perturbations , unless that model is trained on significantly more data . '' Please refer to our thread on `` why prior vulnerability matters '' . As we explain there , to get better generalisation you can either increase your amount of training data , or decrease the complexity of your model , i.e.choose better ( non-vulnerable ! ) priors.- Concerning assumption H1 and reference [ 3 ] : the mean field approach of [ 3 ] relies on very strong independence approximations , namely , neglecting individual effects and replacing them with overall averaged effects with similar statistics . This amounts to disregarding most correlations . We do believe a mean-field treatment of our approach is possible , but in the end , the mean field approximations are much stronger than our assumption H1 , though similar in spirit . [ 1 ] Are adversarial examples inevitable ? , 2018 [ 3 ] Deep Information Propagation , Schoenholz et al. , 2017"}, {"review_id": "H1MzKs05F7-2", "review_text": "The authors provide a compelling theoretical explanation for a large class of adversarial examples. While this explanation (rooted in the norm of gradients of neural networks being the culprit for the existence of adversarial examples) is not new, they unify several old perspectives, and convincingly argue for genuinely new scaling relationships (i.e. \\sqrt(d) versus linear in d scaling of sensitivity to adversarial perturbations versus input size). They prove a number of theorems relating these scaling relationships to a broad swathe of relevant model architectures, and provide thorough empirical evidence of their work. I can honestly find very little to complain about in this work--the prose is clear, and the proofs are correct as far as I can tell (though I found Figure 4 in the appendix (left panel) to not be hugely compelling. More data here would be great!) As much of the analysis hinges on the particularities of the weight distribution at initialization, could the authors comment on possible defenses to adversarial attack by altering this weight distribution? (By, for example, imposing that the average value must grow like 1/d)?", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank you for your review and your very positive evaluation . Concerning Fig 4 in Appendix A : Appendix A is preliminary work , whose goal is essentially to illustrate how our insights on the prior-vulnerability of neural networks can help us design more robust networks ; in this case , by preferring average-poolings over other pooling-operations . But we agree that this section only contains preliminary results , which is why it is in appendix , not main text . Concerning : `` could the authors comment on possible defences to adversarial attack by altering this weight distribution ? ( By , for example , imposing that the average value must grow like 1/d ) ? '' Please refer to point 3/ of our overall reply , which explains what problems arise if we just change the overall weight-size at init . We hope that this addresses your small concerns/questions and thank you , once again , for your evaluation ."}, {"review_id": "H1MzKs05F7-3", "review_text": "This paper analyzes the relationship between \"adversarial vulnerability\" with input dimensionality of neural network. The paper proves that, under certain assumptions, as the input dimensionality increases, neural networks exhibit increasingly large gradients thus are more adversarially vulnerable. Experiments were done on neural networks trained by penalizing input gradients and FGSM-adversarial training. Similar trends on vulnerability vs dimensionality are found. The paper is clearly written and easy to follow. I appreciate that the authors also clearly stated the limitation of the theoretical analysis. The theoretical analyses on vulnerability and dimensionality is novel and provide some insights. But it is unlikely such analysis is significant There are a few reasons: - This analysis only seems to work for \"well-behaved\" models. For models with gradient masking, obfuscated gradients or even non-differentiable models, it is not clear that how this will apply. (and I appreciate that the authors also acknowledge this in the paper.) It is unclear how this specific gradient based analysis can help the understanding of the adversarial perturbation phenomena. After all, the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem. - One very important special case of the point above: the analysis probably cannot cover the adversarially PGD trained models [MMS+17] and the certifiably robust ones. Such models may have small gradients inside the box constraint, but can have large gradients between different classes. On the empirical results, the authors made a few interesting observations, for example the close correspondence between \"Adv Train\" and \"Grad Regu\" models. My concern is that the experiments were done on a narrow range of models, which only have \"weak\" adversarial training / defenses. Adversarial robustness is hard to achieve. What matters the most is \"why the strongest model is still not robust?\" not \"why some weak models are not robust?\" It is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks, e.g. PGD TRAINED models [MMS+17] which is the SOTA on MNIST/CIFAR10 L_\\infty robustness benchmark. Without comprehensive analyses on SOTA robust models, it is hard to justify the validity of the theoretical analysis in this paper, and the conclusions made by the paper. For example, re: the last sentence in the conclusion: \"They hence suggest to tackle adversarial vulnerability by designing new architectures (or new architectural building blocks) rather than by new regularization techniques.\" The reasoning is not obvious to me given the current evidence shown in the paper. [MMS+17] Madry A, Makelov A, Schmidt L, Tsipras D, Vladu A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank you for your time , review , comments and concerns , which we hope to address in full . - Concerning : `` This analysis only seems to work for 'well-behaved ' models . For models ... apply '' Indeed , we only analyse differentiable models . First , note that our results already cover many usual networks ( not just a small subset ) . Second , we think that understanding such well-behaved models is a first step towards understanding non-differentiable ones . ( For example , some non-differentiable functions can be considered differentiable at a rougher scale.But this opens a whole new research direction , while the text is long enough ... ) - Concerning : `` the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem . '' Not all adversarial vulnerability might be first-order , but first-order vulnerability * is * an aspect of vulnerability ( not an oversimplification of a problem ) . If there is first-order vulnerability , then there is vulnerability . Moreover , our results actually suggest that first-order vulnerability and its relation to gradients explains an * essential * part of vulnerability ( see Fig 2d , and paragraph `` Validity of first order expansion '' ) . - Concerning : `` the analysis probably can not cover the adversarially PGD trained models [ MMS+17 ] and the certifiably robust ones '' & `` It is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks , e.g.PGD TRAINED models '' We added experiments with PGD training on CIFAR-10 ( see Fig 2 & 6 ) . Our conclusions stay unchanged . The new experiments support our claim that first-order vulnerability plays an essential role . - Concerning : `` What matters the most is 'why the strongest model is still not robust ? ' not 'why some weak models are not robust ? ' '' We think that understanding the vulnerability of `` weak '' models ( i.e.at initialization or with usual training ) may help understanding the vulnerability of SOTA-robustly trained nets . See our post on `` why prior vulnerability matters '' . - Concerning our last sentence : We can reformulate it to : `` Nevertheless , they show that at least this type of first-order vulnerability is present , common , and firmly rooted * in the priors * of our current network architectures . In future , we may hence want to complement our robust regularisation techniques by new architectures ( or architectural building blocks ) with less vulnerable priors . '' ( Anything in that direction would do.We are open to propositions . )"}], "0": {"review_id": "H1MzKs05F7-0", "review_text": "The paper studies how the vulnerability of a neural network model depends on its input dimension. The authors prove that for an *untrained* model, randomly initialized with Xavier initialization, the gradient of the loss wrt the input is essentially independent of the architecture and task. This implies that the major factor affecting the norm of that gradient is the input dimension. They then support their argument by experiments measuring the relation between adversarial vulnerability and gradient norm using various *trained* models (including adversarially regularized ones). I find the main theoretical result interesting. While this is a known fact for the simple case of linear classifiers, extending it to arbitrarily deep networks is a valuable contribution. The proof crucially relies on properties of the specific initialization scheme to show that the gradient does not change too much during backproparagation through the layers. The most significant limitation of the result (which the authors kindly acknowledge) is that this result only holds at initialization. Hence it cannot distinguish between different training methods or between how different architectures evolve during training. Since the situation in adversarial robustness is much more nuanced, I am skeptical about the significance of such statements. On the experimental side, the finding that gradient regularization improves adversarial robustness to small epsilon values has been made multiple times in the past (as the authors cite in the related work section). It is worth noting that the epsilon considered is 0.005 in L_inf (1.275/255) which is pretty small. This value corresponds to the \"small-epsilon regime\" where the behavior of the model is fairly linear around the original inputs and thus defenses such as FGSM-training and gradient regularization are effective. The authors also perform an interesting experiment where they train models on downsampled ImageNet datasets and find that indeed larger input dimension leads to more vulnerable models. While I find the results interesting, I do not see clear implications. The fact that the vulnerability of a classifier depends on the L1 norm of the input gradient is already known for any locally linear classifier (i.e. deep models too), and it is fairly clear that the L1 norm will have a dimension dependence. The fact that it does not depend on architecture or task at initialization is interesting but of limited significance in my opinion. Given that the experimental results are also not particularly novel, I recommend rejection. [UPDATE]: Given the overall discussion and paper updates, I consider the current version of the paper (marginally) crossing the ICLR bar. I update my score from a 5 to a 6. Minor comments to the authors: -- I think || x ||_* is more clear than |||x||| for the dual norm. -- Consider using lambda for the regularization, epsilon is confusing since it is overloaded.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for your careful review , and for pointing out that many people do indeed care about small worst-case l_p-perturbations , and why . - Concerning : `` I am skeptical about the significance of such statements [ at initialization ] . '' & `` While I find the results interesting , I do not see clear implications . '' Please refer to our overall thread on , why understanding the vulnerability of priors helps understanding post ( robust ) training vulnerability . See also point 4/ in our overall reply . - Concerning : `` While this is a known fact for the simple case of linear classifiers ... '' Even with only linear classifiers , previous published work has predicted a linear increase of vulnerability with input-dimension rather than sqrt ( d ) , because they did not take the dimension-dependance of the weights into account . - Concerning : `` it is fairly clear that the L1 norm will have a dimension dependence '' Maybe , but it is all about getting the numbers right . Our predictions correspond to the * exact * increase-rate measured in practice . - Concerning : `` The fact that it does not depend on architecture or task at initialization is interesting but of limited significance in my opinion . '' This independence on architecture at initialization shows that , if we want to get non-vulnerable priors , we need to re-think our initialization scheme and/or introduce a new architectural building block . As to why we would want non-vulnerable priors , again , please see our overall thread on the subject . Once again , we thank you for your review and hope that our answers may help you to re-evaluate the significance of our results ."}, "1": {"review_id": "H1MzKs05F7-1", "review_text": "This paper argues that adversarial vulnerability of neural networks increases with input dimension. Theoretical and empirical evidence are given which connect the l_p norm of the gradient of the training objective with the existence of small-worst case l_q perturbations. This connection is made by assuming that the learned function is well approximated by a linear function local to the sampled input x. By making assumptions on the initialization scheme for some simple architectures, the authors show that the l_p norm of the gradient for randomly initialized network will be large, and provide empirical evidence that these assumptions hold after training. These assumptions imply bounds on the typical magnitude of the gradient of the loss with respect to a single input coordinate, this then implies that the overall gradient norm will depend on the input dimension. I found this paper well written. The mathematical assumptions are presented in a clear, easy to understand manner. Also high level intuition is given around their main theorems which help the reader understand the main ideas. However, I have a number of concerns about this work. The first is, I do not buy the motivation for studying the \"phenomenon\" of small worst-case l_p perturbations. I realize this statement applies to a large body of literature, but since the publication of [1] we are still lacking concrete motivating scenarios for the l_p action space. I would encourage the authors instead to ask the closely related but more general question of how we can improve model generalization outside the natural distribution of images, such as generalization in the presence of commonly occurring image corruptions [2]. It's possible that the analysis in this work could better our understanding model generalization in the presence of different image corruptions, indeed by making similar linearity assumptions as considered in this work, test error in additive Gaussian noise can be linked with distance to the decision boundary [3,4]. However, this particular question was not explored in this work. Second, the work is one of many to relate the norm of the gradient with adversarial robustness (for example, this has been proposed as a defense mechanism in [5,6]). I also suspect that the main theorem relating gradient norm to initialization should easily follow for more general settings using the mean field theory developed by [7,8] (this would be particularly useful for removing assumption H1, which assumes the ReLU activation is a random variable independent of the weights). Overall, I don't see how gradient norms explain why statistical classifiers make mistakes, particularly for more realistic attacker action spaces [9]. Even for \"small\" l_p adversarial examples there seem to be limitations as to how much gradient norms can explain the phenomenon --- for example even max margin classifiers such as SVM's have \"adversarial examples\". Furthermore, adversarial training has been shown to reach a point where the model is \"robust\" locally to training points but this robustness does not generalize to the points in the test set [10]. In fact, for the synthetic data distributions considered in [10], it's proven that no learning algorithm can achieve robustness given insufficient training data. Finally, the main conclusion of this work \"adversarial vulnerability of neural networks increases with input dimension\" is an overly general statement which needs a much more nuanced view. While experiments shown in [11] support this conclusion for naturally trained networks, it is shown that when adversarial training is applied the model is more robust when the input dimension is higher (see Figure 4 a. and b.). Perhaps the assumptions for Theorem 4 are violated for these adversarially trained models. 1. https://arxiv.org/abs/1807.06732 2. https://arxiv.org/abs/1807.01697 3. https://arxiv.org/abs/1608.08967 4. https://openreview.net/forum?id=S1xoy3CcYX&noteId=BklKxJBF57. 5. https://arxiv.org/abs/1704.08847 6. https://arxiv.org/abs/1608.07690 7. https://arxiv.org/abs/1611.01232 8. https://arxiv.org/abs/1806.05393 9. https://arxiv.org/abs/1712.09665 10. https://arxiv.org/abs/1804.11285 11. https://arxiv.org/pdf/1809.02104.pdf", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank you for your expanded reviews , comments , questions and references , which we hope to address in full . -Concerning : `` The experiments for adversarially trained models in [ 1 ] directly contradict the title of this paper '' See point 2/ of our overall reply . - Concerning : `` for some settings of the weights you can show a bound such as is discussed in the paper , but there are other settings ( perhaps even initializations ) of the weights for which the conclusion will not hold . '' The current initialization-methods are used to avoid exploding/vanishing activations at init . Any other initialization would need to solve that issue . See point 3/ of our overall reply . - Concerning : `` fixing the initialization seems unlikely to buy us much more than what adversarial training achieves , and the experiments in [ 1 ] suggest to me the conclusion of this work is limited in scope '' Please refer to our overall thread on `` why prior vulnerability matters '' and how it might help understanding and harnessing the vulnerability of ( robustly ) trained networks . - Concerning : `` we hit a limit as we increase the epsilon considered for the perturbations '' Even for small epsilons , our networks are surprisingly vulnerable . If we do n't understand the small epsilon vulnerability , then we wo n't understand big epsilons . - Concerning : `` it \u2019 s not clear to me what actionable insights we can conclude from this work , and how this can be used to improve upon the current SOTA . '' Again , please see our thread on `` why prior vulnerability matters '' and how it may help understanding and harnessing the vulnerability of ( robustly ) trained networks . - Concerning : `` it was found that adversarial training eventually gives robustness to the training set , but this robustness does not generalize to the test set [ 2 ] ... the data distribution . '' See 4/ in our overall reply . - Concerning : `` in [ 2 ] it was shown that there is no learning algorithm [ that ] can become robust to small perturbations , unless that model is trained on significantly more data . '' Please refer to our thread on `` why prior vulnerability matters '' . As we explain there , to get better generalisation you can either increase your amount of training data , or decrease the complexity of your model , i.e.choose better ( non-vulnerable ! ) priors.- Concerning assumption H1 and reference [ 3 ] : the mean field approach of [ 3 ] relies on very strong independence approximations , namely , neglecting individual effects and replacing them with overall averaged effects with similar statistics . This amounts to disregarding most correlations . We do believe a mean-field treatment of our approach is possible , but in the end , the mean field approximations are much stronger than our assumption H1 , though similar in spirit . [ 1 ] Are adversarial examples inevitable ? , 2018 [ 3 ] Deep Information Propagation , Schoenholz et al. , 2017"}, "2": {"review_id": "H1MzKs05F7-2", "review_text": "The authors provide a compelling theoretical explanation for a large class of adversarial examples. While this explanation (rooted in the norm of gradients of neural networks being the culprit for the existence of adversarial examples) is not new, they unify several old perspectives, and convincingly argue for genuinely new scaling relationships (i.e. \\sqrt(d) versus linear in d scaling of sensitivity to adversarial perturbations versus input size). They prove a number of theorems relating these scaling relationships to a broad swathe of relevant model architectures, and provide thorough empirical evidence of their work. I can honestly find very little to complain about in this work--the prose is clear, and the proofs are correct as far as I can tell (though I found Figure 4 in the appendix (left panel) to not be hugely compelling. More data here would be great!) As much of the analysis hinges on the particularities of the weight distribution at initialization, could the authors comment on possible defenses to adversarial attack by altering this weight distribution? (By, for example, imposing that the average value must grow like 1/d)?", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank you for your review and your very positive evaluation . Concerning Fig 4 in Appendix A : Appendix A is preliminary work , whose goal is essentially to illustrate how our insights on the prior-vulnerability of neural networks can help us design more robust networks ; in this case , by preferring average-poolings over other pooling-operations . But we agree that this section only contains preliminary results , which is why it is in appendix , not main text . Concerning : `` could the authors comment on possible defences to adversarial attack by altering this weight distribution ? ( By , for example , imposing that the average value must grow like 1/d ) ? '' Please refer to point 3/ of our overall reply , which explains what problems arise if we just change the overall weight-size at init . We hope that this addresses your small concerns/questions and thank you , once again , for your evaluation ."}, "3": {"review_id": "H1MzKs05F7-3", "review_text": "This paper analyzes the relationship between \"adversarial vulnerability\" with input dimensionality of neural network. The paper proves that, under certain assumptions, as the input dimensionality increases, neural networks exhibit increasingly large gradients thus are more adversarially vulnerable. Experiments were done on neural networks trained by penalizing input gradients and FGSM-adversarial training. Similar trends on vulnerability vs dimensionality are found. The paper is clearly written and easy to follow. I appreciate that the authors also clearly stated the limitation of the theoretical analysis. The theoretical analyses on vulnerability and dimensionality is novel and provide some insights. But it is unlikely such analysis is significant There are a few reasons: - This analysis only seems to work for \"well-behaved\" models. For models with gradient masking, obfuscated gradients or even non-differentiable models, it is not clear that how this will apply. (and I appreciate that the authors also acknowledge this in the paper.) It is unclear how this specific gradient based analysis can help the understanding of the adversarial perturbation phenomena. After all, the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem. - One very important special case of the point above: the analysis probably cannot cover the adversarially PGD trained models [MMS+17] and the certifiably robust ones. Such models may have small gradients inside the box constraint, but can have large gradients between different classes. On the empirical results, the authors made a few interesting observations, for example the close correspondence between \"Adv Train\" and \"Grad Regu\" models. My concern is that the experiments were done on a narrow range of models, which only have \"weak\" adversarial training / defenses. Adversarial robustness is hard to achieve. What matters the most is \"why the strongest model is still not robust?\" not \"why some weak models are not robust?\" It is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks, e.g. PGD TRAINED models [MMS+17] which is the SOTA on MNIST/CIFAR10 L_\\infty robustness benchmark. Without comprehensive analyses on SOTA robust models, it is hard to justify the validity of the theoretical analysis in this paper, and the conclusions made by the paper. For example, re: the last sentence in the conclusion: \"They hence suggest to tackle adversarial vulnerability by designing new architectures (or new architectural building blocks) rather than by new regularization techniques.\" The reasoning is not obvious to me given the current evidence shown in the paper. [MMS+17] Madry A, Makelov A, Schmidt L, Tsipras D, Vladu A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank you for your time , review , comments and concerns , which we hope to address in full . - Concerning : `` This analysis only seems to work for 'well-behaved ' models . For models ... apply '' Indeed , we only analyse differentiable models . First , note that our results already cover many usual networks ( not just a small subset ) . Second , we think that understanding such well-behaved models is a first step towards understanding non-differentiable ones . ( For example , some non-differentiable functions can be considered differentiable at a rougher scale.But this opens a whole new research direction , while the text is long enough ... ) - Concerning : `` the first order Taylor expansion argument on top of randomly initialized weights is oversimplifying the complicated problem . '' Not all adversarial vulnerability might be first-order , but first-order vulnerability * is * an aspect of vulnerability ( not an oversimplification of a problem ) . If there is first-order vulnerability , then there is vulnerability . Moreover , our results actually suggest that first-order vulnerability and its relation to gradients explains an * essential * part of vulnerability ( see Fig 2d , and paragraph `` Validity of first order expansion '' ) . - Concerning : `` the analysis probably can not cover the adversarially PGD trained models [ MMS+17 ] and the certifiably robust ones '' & `` It is especially worrisome to me that the paper does not cover the adversarially-augmented training based iterative attacks , e.g.PGD TRAINED models '' We added experiments with PGD training on CIFAR-10 ( see Fig 2 & 6 ) . Our conclusions stay unchanged . The new experiments support our claim that first-order vulnerability plays an essential role . - Concerning : `` What matters the most is 'why the strongest model is still not robust ? ' not 'why some weak models are not robust ? ' '' We think that understanding the vulnerability of `` weak '' models ( i.e.at initialization or with usual training ) may help understanding the vulnerability of SOTA-robustly trained nets . See our post on `` why prior vulnerability matters '' . - Concerning our last sentence : We can reformulate it to : `` Nevertheless , they show that at least this type of first-order vulnerability is present , common , and firmly rooted * in the priors * of our current network architectures . In future , we may hence want to complement our robust regularisation techniques by new architectures ( or architectural building blocks ) with less vulnerable priors . '' ( Anything in that direction would do.We are open to propositions . )"}}