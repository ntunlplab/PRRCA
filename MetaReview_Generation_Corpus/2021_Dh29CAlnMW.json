{"year": "2021", "forum": "Dh29CAlnMW", "title": "Parsed Categoric Encodings with Automunge", "decision": "Reject", "meta_review": "\nThis paper presents \"Automunge\" a python library for pre-processing tabular data. \nThe authors develop a useful library that can be used by practicioners for data engineering in NNs applications. \nThe reviewers raised a common concern regarding the lack of focus on the actual usefulness of the librabry in improving the \nperformance of the models that is applied on. A common concern was the lack of performance plots compared to other alternatives. \nIn the response the authors have done a rather thorough job of addressing the reviewers comments and\nadding material in the supplementary. However, given the current presentation, the manuscript needs a considerable amount of  rewriting to incorporate the suggested changes into the main paper. As it is, I don't think ICLR is the right venue for the manuscript.  It might reach its audience better in venues like SysMl or PyCon also suggested by a reviewer. \n", "reviews": [{"review_id": "Dh29CAlnMW-0", "review_text": "The submitted paper describes a very nice featurization library , AutoMunge , that converts NLP into features suitable for NNs . It 's clear that the authors of the library have put a lot of thought into its construction , and it looks very useful . However , ICLR is about /learning/ representations , not about feature engineering . So this is off-topic for the conference . To make it on-topic , the authors could , e.g. , compare using standard word representation techniques vs AutoMunge on a set of NLP tasks using some popular modern NLP architecture ( perhaps BERT ? ) . That would be a really interesting paper .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you reviewer for your interpretation of this work . I am reading the primary consideration of your review to be based on the premise that Automunge is off-topic for the ICLR conference . I will present here a short summary of why I do not believe that to be the case . One of the papers we cited was \u201c Efficient Estimation of Word Representations in Vector Space \u201d by Mikolov et al from ICLR 2013 , which rolled out the Word2Vec method for vocabulary vectorization , a precurser to the types of contextual vocabulary embeddings applied in NLP applications like GPT-3 . Quoting Gary Marcus in his recent book Rebooting AI , \u201c Rumors of the replacement of feature engineering have been somewhat exaggerated ; the hard work that goes into crafting representations like Word2Vec still counts as feature engineering , just of a different sort\u2026 \u201d . Even if string parsing is a type of feature engineering , so is Word2Vec . More particularly , we noted in our paper that vocabulary vectorization of the like of Word2Vec is inaccessible for certain types of categoric features that may be found in a tabular data set . We gave two examples of addresses and serial numbers in which language models trained on public text corpus may be insufficient for application to esoteric domains , especially considering that in tabular data sets categoric feature sets lack the context of a surrounding text corpus that may enable a fine-tuning of a language model . These type of scenarios are not uncommon in real-world tabular data sets . The premise of our work is that in a tabular data application , models will often benefit by improving the information retention of categoric encodings by extracting grammatical structure shared between categoric entries as opposed to coarse-grained encodings of mainstream methods . We believe that the string parsing operation to be beneficial both as a general purpose supplement to categoric encodings to less advanced users , but also useful for sophisticated users who might otherwise consider applying advanced NLP methods like BERT to tabular applications who recognize that there are some types of esoteric domains where BERT may not be as viable . Even for next generation technologies we are seeing from the likes of GPT-3 , having a formal framework for tabular data processing will still be necessary - if we have a NLP model that can write software in python , that doesn \u2019 t mean we don \u2019 t need python . Thus Automunge is infrastructure that could be built on top of with NLP applications . A foundation . I appreciate your review of this work . I hope you might consider reading my responses to the other reviewers as well before making your final decision . Best regards ."}, {"review_id": "Dh29CAlnMW-1", "review_text": "This package aims at automating some repetitive tasks that data analysts ( especially within the NLP domain ) deal with . I confirm that tasks like feature selection and string encoding are beneficial for applied researchers with textual data . I can see a wide interest in the community . On the negative side , the paper lacks a literature review and comparison with any existing and relevant work . I expected to see performance plots for different tasks ( whether just for this package or with comparison to some baseline alternatives.As a minor comment , I do n't think having `` String theory '' in the title is a good idea because this keyword is already taken to refer to another scientific topic ( a sub-field of physics ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you reviewer for your comments on this work . I appreciate your recognition of the benefit of automating repetitive tasks that data analysts , especially in NLP domain , deal with . Thank you for confirming that feature selection and string encoding will be beneficial for applied researchers with textual data . I hope you are right about potential for a wide interest in the community , one of the consistent challenges for the library developers has been getting the word out , as Automunge is an open source library without resources for \u201c public relations \u201d or advertising . I believe that this library could be of broad benefit to both machine learning researchers and practitioners , as tabular data preprocessing conventions have yet to hone in on a single mainstream standard . With regards to a literature review and comparison to existing and relevant work , this is partly a result of research being conducted by invention and building things from scratch ( primarily on top of the Pandas library ) . The seed of this project originated from some beginner tabular data competitions on Kaggle , and the developers basically took the approach of building what was seen as an unmet need for a tabular data standard from the ground up . The development process has been incremental and evolutionary , and along the way I believe some material improvements have been incorporated over what is otherwise available in mainstream practice for tabular data preprocessing . With respect to performance plots for different tasks , I am taking this as a good idea for further research . One of the drivers for various design decisions has been speed of application , particularly for subsequent data processed in the postmunge ( . ) function.Part of the challenge for benchmarking purposes is budgetary constraints associated with expensive licenses for commercial alternatives . We did include some benchmarking for speed in the Jupyter notebooks uploaded with the supplemental material ( see for example the uploaded notebook \u201c efficiency_tests_061020.ipynb \u201d ) . I believe one of the key advantage of this framework over a mainstream option like scikit-learn to originate from the simplicity of populating a single dictionary \u201c fit \u201d to properties of a training set which can be shared and published by researchers for fully consistent processing of additional data - as may benefit reproducibility of benchmarks and experiments . Other aspects of novelty such as these string parsing methods and automated ML for missing data infill are also a material improvement . With regards to the use of the term \u201c string theory \u201d , I hope you will grant this author a small indulgence . It \u2019 s use is admittedly a little on the humorous side , we justified that based on an assumption that there was not likely to be any confusion about the overlap between these very very different domains , especially in the context of the phrase \u201c Parsed Categoric Encodings \u201d . If you consider it beneficial for moving forward we would certainly be willing to strike those two words from the title . Again I certainly appreciate your review and recognition of the potential benefit of this library to applied research . I have also publicly responded to the other reviewers , and have taken an action to upload an additional validation demonstration notebook which should follow in the next few days . Best regards ."}, {"review_id": "Dh29CAlnMW-2", "review_text": "This paper introduces a library that preprocesses tabular data called Automunge . For software packages I feel that one of two criteria must be met : The software implements a scientifically novel algorithm , framework , model , etc . ; or the software package is so complex that a well-designed implementation in itself is of scientific significance . Whereas Automunge seems like a useful library , I am not convinced that it falls in either of these two categories . As such , I am not sure it justifies a publication at a machine learning conference . I suggest the authors target a different venue ( e.g. , PyCon , SysML , etc . ) or elaborate on the scientific impact of their software ( e.g. , show experimentally that this framework allows practitioners to train better performing models ) . Pros * The library seems useful Cons * No significant novelty * Little relevance to the scientific machine learning community * Not very clearly written", "rating": "4: Ok but not good enough - rejection", "reply_text": "I appreciate that you offered two specific criteria for software packages , I believe this software has met both of these criteria as follows : Criteria one : \u201c The software implements a scientifically novel algorithm , framework , model , etc. \u201d I believe the family tree primitives as described in Figure 6 meet this criteria , for the reason that they have formalized a fundamental aspect of processing tabular data , as enabling a simple means for command line specification of multi-transform sets that may include generations and branches of derivations . I believe the family tree primitives to be somewhat fundamental , and any data transformation sets as may be applied to a single feature set of origination ( as would be found in a \u201c tidy data \u201d set ) can be universally expressed by way of these simple and novel primitives applied by way of recursion . More particularly for string parsing considerations , the paper does not just introduce string parsing , it offers a comprehensive overview of the various permutations that may be applied for this purpose . We sought for this treatment of string parsing methods to be exhaustive . We believe that string parsing is appropriate for a machine learning conference because of just how fundamental is the application for tabular data applications of machine learning , which in practice is generally comprised of just two broad categories of feature set types - numeric and categoric . We have introduced a novel automated approach for encoding tabular categoric features . Although the benefit of string parsing is expected to vary based on esoteric characteristics of target feature sets , the paper operated on the premise of a self-evident benefit to ML for improved information retention of extracting grammatical structure that may be shared between categoric entries for presentation to a training operation in comparison to coarse-grained representations . That being said I am working now on an additional demonstration jupyter notebook to be uploaded to the supplemental material and will advise when it is ready in which I intend to experimentally demonstrate the benefit as you suggested . Criteria two : \u201c the software package is so complex that a well-designed implementation in itself is of scientific significance. \u201d I believe the simplicity of the package is deceptive for the amount of complexity that is abstracted away . I recently attended a data science conference at a high profile university where the keynote speaker described a project to apply machine learning to predict missing data infill for a specific tabular data application in industry . Automunge offers a generalized solution and abstracts away all of the complexities for any tabular data application . It is a push-button autoML solution for missing data infill , and all of the string parsing methods demonstrated have built in support . One of the most useful abstractions for purposes of hiding complexity is the manner in which the application of automunge ( . ) populates a python dictionary \u201c fit \u201d to properties of the train set , capturing all of the steps and parameters of transformations , such that for subsequent data , including streams of data for inference , consistent preparations may be applied quickly and efficiently in the postmunge ( . ) function with only the prerequisite of passing this dictionary . This practice of basing properties of transformations explicitly on properties from a designated train set is an improvement on what is still common in mainstream practice to normalize train/test/validation sets separately - which introduces issues of potential stochastic inconsistency and data leakage . We noted too in the Broader Impacts appendix that the ability of researchers to publish these populated dictionaries could benefit reproducibility of benchmarks and experiments . Thank you for the recognition that you believe this library would be useful . That is our goal . With respect the the \u201c cons \u201d that you noted : Regarding novelty : we believe the push button automation of string parsing operations to be novel . We believe the integration of command line specification for multi-transform sets , autoML missing data infill , and various other features of the library to be novel . We believe the family tree primitives to be novel and a particularly useful fundamental reframing of specifying transformation sets via recursion . I hope you will forgive my writing style , you noted that it was not clearly written . Partly this was associated with trying to cover a lot of ground I suspect . I believe the family tree primitives description benefits by taking into account the demonstrations of Figures 4 , 5 , 6 , and 7 which illustrate their application in practice for the given \u2018 or19 \u2019 root category example . Thank you again for your review . Happy to answer further questions . I hope you might reconsider your rating based on this feedback . If you are unsure please consider reviewing my response to the other two reviewers for context . Best regards ."}], "0": {"review_id": "Dh29CAlnMW-0", "review_text": "The submitted paper describes a very nice featurization library , AutoMunge , that converts NLP into features suitable for NNs . It 's clear that the authors of the library have put a lot of thought into its construction , and it looks very useful . However , ICLR is about /learning/ representations , not about feature engineering . So this is off-topic for the conference . To make it on-topic , the authors could , e.g. , compare using standard word representation techniques vs AutoMunge on a set of NLP tasks using some popular modern NLP architecture ( perhaps BERT ? ) . That would be a really interesting paper .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you reviewer for your interpretation of this work . I am reading the primary consideration of your review to be based on the premise that Automunge is off-topic for the ICLR conference . I will present here a short summary of why I do not believe that to be the case . One of the papers we cited was \u201c Efficient Estimation of Word Representations in Vector Space \u201d by Mikolov et al from ICLR 2013 , which rolled out the Word2Vec method for vocabulary vectorization , a precurser to the types of contextual vocabulary embeddings applied in NLP applications like GPT-3 . Quoting Gary Marcus in his recent book Rebooting AI , \u201c Rumors of the replacement of feature engineering have been somewhat exaggerated ; the hard work that goes into crafting representations like Word2Vec still counts as feature engineering , just of a different sort\u2026 \u201d . Even if string parsing is a type of feature engineering , so is Word2Vec . More particularly , we noted in our paper that vocabulary vectorization of the like of Word2Vec is inaccessible for certain types of categoric features that may be found in a tabular data set . We gave two examples of addresses and serial numbers in which language models trained on public text corpus may be insufficient for application to esoteric domains , especially considering that in tabular data sets categoric feature sets lack the context of a surrounding text corpus that may enable a fine-tuning of a language model . These type of scenarios are not uncommon in real-world tabular data sets . The premise of our work is that in a tabular data application , models will often benefit by improving the information retention of categoric encodings by extracting grammatical structure shared between categoric entries as opposed to coarse-grained encodings of mainstream methods . We believe that the string parsing operation to be beneficial both as a general purpose supplement to categoric encodings to less advanced users , but also useful for sophisticated users who might otherwise consider applying advanced NLP methods like BERT to tabular applications who recognize that there are some types of esoteric domains where BERT may not be as viable . Even for next generation technologies we are seeing from the likes of GPT-3 , having a formal framework for tabular data processing will still be necessary - if we have a NLP model that can write software in python , that doesn \u2019 t mean we don \u2019 t need python . Thus Automunge is infrastructure that could be built on top of with NLP applications . A foundation . I appreciate your review of this work . I hope you might consider reading my responses to the other reviewers as well before making your final decision . Best regards ."}, "1": {"review_id": "Dh29CAlnMW-1", "review_text": "This package aims at automating some repetitive tasks that data analysts ( especially within the NLP domain ) deal with . I confirm that tasks like feature selection and string encoding are beneficial for applied researchers with textual data . I can see a wide interest in the community . On the negative side , the paper lacks a literature review and comparison with any existing and relevant work . I expected to see performance plots for different tasks ( whether just for this package or with comparison to some baseline alternatives.As a minor comment , I do n't think having `` String theory '' in the title is a good idea because this keyword is already taken to refer to another scientific topic ( a sub-field of physics ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you reviewer for your comments on this work . I appreciate your recognition of the benefit of automating repetitive tasks that data analysts , especially in NLP domain , deal with . Thank you for confirming that feature selection and string encoding will be beneficial for applied researchers with textual data . I hope you are right about potential for a wide interest in the community , one of the consistent challenges for the library developers has been getting the word out , as Automunge is an open source library without resources for \u201c public relations \u201d or advertising . I believe that this library could be of broad benefit to both machine learning researchers and practitioners , as tabular data preprocessing conventions have yet to hone in on a single mainstream standard . With regards to a literature review and comparison to existing and relevant work , this is partly a result of research being conducted by invention and building things from scratch ( primarily on top of the Pandas library ) . The seed of this project originated from some beginner tabular data competitions on Kaggle , and the developers basically took the approach of building what was seen as an unmet need for a tabular data standard from the ground up . The development process has been incremental and evolutionary , and along the way I believe some material improvements have been incorporated over what is otherwise available in mainstream practice for tabular data preprocessing . With respect to performance plots for different tasks , I am taking this as a good idea for further research . One of the drivers for various design decisions has been speed of application , particularly for subsequent data processed in the postmunge ( . ) function.Part of the challenge for benchmarking purposes is budgetary constraints associated with expensive licenses for commercial alternatives . We did include some benchmarking for speed in the Jupyter notebooks uploaded with the supplemental material ( see for example the uploaded notebook \u201c efficiency_tests_061020.ipynb \u201d ) . I believe one of the key advantage of this framework over a mainstream option like scikit-learn to originate from the simplicity of populating a single dictionary \u201c fit \u201d to properties of a training set which can be shared and published by researchers for fully consistent processing of additional data - as may benefit reproducibility of benchmarks and experiments . Other aspects of novelty such as these string parsing methods and automated ML for missing data infill are also a material improvement . With regards to the use of the term \u201c string theory \u201d , I hope you will grant this author a small indulgence . It \u2019 s use is admittedly a little on the humorous side , we justified that based on an assumption that there was not likely to be any confusion about the overlap between these very very different domains , especially in the context of the phrase \u201c Parsed Categoric Encodings \u201d . If you consider it beneficial for moving forward we would certainly be willing to strike those two words from the title . Again I certainly appreciate your review and recognition of the potential benefit of this library to applied research . I have also publicly responded to the other reviewers , and have taken an action to upload an additional validation demonstration notebook which should follow in the next few days . Best regards ."}, "2": {"review_id": "Dh29CAlnMW-2", "review_text": "This paper introduces a library that preprocesses tabular data called Automunge . For software packages I feel that one of two criteria must be met : The software implements a scientifically novel algorithm , framework , model , etc . ; or the software package is so complex that a well-designed implementation in itself is of scientific significance . Whereas Automunge seems like a useful library , I am not convinced that it falls in either of these two categories . As such , I am not sure it justifies a publication at a machine learning conference . I suggest the authors target a different venue ( e.g. , PyCon , SysML , etc . ) or elaborate on the scientific impact of their software ( e.g. , show experimentally that this framework allows practitioners to train better performing models ) . Pros * The library seems useful Cons * No significant novelty * Little relevance to the scientific machine learning community * Not very clearly written", "rating": "4: Ok but not good enough - rejection", "reply_text": "I appreciate that you offered two specific criteria for software packages , I believe this software has met both of these criteria as follows : Criteria one : \u201c The software implements a scientifically novel algorithm , framework , model , etc. \u201d I believe the family tree primitives as described in Figure 6 meet this criteria , for the reason that they have formalized a fundamental aspect of processing tabular data , as enabling a simple means for command line specification of multi-transform sets that may include generations and branches of derivations . I believe the family tree primitives to be somewhat fundamental , and any data transformation sets as may be applied to a single feature set of origination ( as would be found in a \u201c tidy data \u201d set ) can be universally expressed by way of these simple and novel primitives applied by way of recursion . More particularly for string parsing considerations , the paper does not just introduce string parsing , it offers a comprehensive overview of the various permutations that may be applied for this purpose . We sought for this treatment of string parsing methods to be exhaustive . We believe that string parsing is appropriate for a machine learning conference because of just how fundamental is the application for tabular data applications of machine learning , which in practice is generally comprised of just two broad categories of feature set types - numeric and categoric . We have introduced a novel automated approach for encoding tabular categoric features . Although the benefit of string parsing is expected to vary based on esoteric characteristics of target feature sets , the paper operated on the premise of a self-evident benefit to ML for improved information retention of extracting grammatical structure that may be shared between categoric entries for presentation to a training operation in comparison to coarse-grained representations . That being said I am working now on an additional demonstration jupyter notebook to be uploaded to the supplemental material and will advise when it is ready in which I intend to experimentally demonstrate the benefit as you suggested . Criteria two : \u201c the software package is so complex that a well-designed implementation in itself is of scientific significance. \u201d I believe the simplicity of the package is deceptive for the amount of complexity that is abstracted away . I recently attended a data science conference at a high profile university where the keynote speaker described a project to apply machine learning to predict missing data infill for a specific tabular data application in industry . Automunge offers a generalized solution and abstracts away all of the complexities for any tabular data application . It is a push-button autoML solution for missing data infill , and all of the string parsing methods demonstrated have built in support . One of the most useful abstractions for purposes of hiding complexity is the manner in which the application of automunge ( . ) populates a python dictionary \u201c fit \u201d to properties of the train set , capturing all of the steps and parameters of transformations , such that for subsequent data , including streams of data for inference , consistent preparations may be applied quickly and efficiently in the postmunge ( . ) function with only the prerequisite of passing this dictionary . This practice of basing properties of transformations explicitly on properties from a designated train set is an improvement on what is still common in mainstream practice to normalize train/test/validation sets separately - which introduces issues of potential stochastic inconsistency and data leakage . We noted too in the Broader Impacts appendix that the ability of researchers to publish these populated dictionaries could benefit reproducibility of benchmarks and experiments . Thank you for the recognition that you believe this library would be useful . That is our goal . With respect the the \u201c cons \u201d that you noted : Regarding novelty : we believe the push button automation of string parsing operations to be novel . We believe the integration of command line specification for multi-transform sets , autoML missing data infill , and various other features of the library to be novel . We believe the family tree primitives to be novel and a particularly useful fundamental reframing of specifying transformation sets via recursion . I hope you will forgive my writing style , you noted that it was not clearly written . Partly this was associated with trying to cover a lot of ground I suspect . I believe the family tree primitives description benefits by taking into account the demonstrations of Figures 4 , 5 , 6 , and 7 which illustrate their application in practice for the given \u2018 or19 \u2019 root category example . Thank you again for your review . Happy to answer further questions . I hope you might reconsider your rating based on this feedback . If you are unsure please consider reviewing my response to the other two reviewers for context . Best regards ."}}