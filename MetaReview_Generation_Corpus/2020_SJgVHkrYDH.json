{"year": "2020", "forum": "SJgVHkrYDH", "title": "Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering", "decision": "Accept (Poster)", "meta_review": "The paper proposed a multi-hop machine reading method for hotpotqa and squad-open datasets. The reviewers agreed that it is very interesting to learn to retrieve, and the paper presents an interesting solution. Some additional experiments as suggested by the reviewers will help improve the paper further. ", "reviews": [{"review_id": "SJgVHkrYDH-0", "review_text": "Summary ======== This paper introduces a graph-based recurrent retrieval model for retrieving evidence documents in a multi-hop reasoning question answering task. The main idea is that (1) the graph formed by Wikipedia links between passages can be used as constraint for constructing reasoning chains, and (2) the joint encoding of the question and current passage can be used to retrieve a subsequent passage in the reasoning chain. The paper describes a model for implementing the above retrieval system, and how they jointly train with a reading comprehension model. They demonstrate the effectiveness of the system on HotPotQA, showing improvements over previously published models, and SQuaD-Open, showing competitive results. Overall Comments =============== The paper is an interesting, but incremental, improvement to the area of question answering. Overall, there are two main concerns about this work. First, while the results are somewhat strong, the ideas presented are small variations on existing systems. For example, Godbole et al 2019 and Ding et al. 2019 both explore using graphical structural to constraint iterative, multi-hop, retrieval. Also, Feldman et al 2019, describe an encoder based approach to encode question and paragraph context for iterative retrieval. Asides from smaller modeling differences (choice of RNN, training regime, BERT reader, etc.) to account for the difference in results, the main difference seems to be the joint training of the retrieval system with the reader. Secondly, the paper lacks clarity on some formal definitions and definition of the graph, making it hard to understand the content precisely. Detailed Comments ================ Below are some detailed comments about specific parts of the paper, in order of importance: 1. One important limitation of this technique is the reliance on a linked documents for constructing the retrieval system. It is not clear from the paper how much of the results are obtained from constraining the set of retrieved passages (after the initial retrieval) to Wikipedia links. And whether, for example, substituting Wikipedia links with links derived from an off-the-shelf entity linking system would suffice. 2. Given that the retrieval model is restricted to link structure in Wikipedia that induces the proposed retrieval graph, I assume that there are \u201creasoning paths\u201d that do not exist in the graph, given Wikipedia\u2019s policy of avoiding adding redundant links within a Wikipedia page. It would have been informative to conduct an \u201cOracle\u201d experiment: that is, given the initial set of retrieved nodes and the graph structure, are there *any* paths that provide the correct answer and reasoning chain? That is to say, what is the upper-bound performance on the proposed system given the currently induced Wikipedia graph? 3. In Section 3, and even later on in the paper, it was not clear what \u201cE\u201d denotes. It never seems to be defined, and is used interchangeably with \u201cgraph node\u201d, \u201cwikipedia page\u201d, \u201cwikipedia paragraph\u201d and \u201creasoning path\u201d. Are these the same thing? It would be much clearer to define what E means, and perhaps separate the different concepts (node, passage, reasoning path) properly. 4. In Section 3, it seems that \u2018q\u2019 is not defined. Is it the question? 5. In Section 3.1, it is not clear what the graph actually contains. Does it contain all the paragraphs from Wikipedia? Just the paragraphs with links? The first paragraph of every Wikipedia page? What granularity of the wikipedia page becomes an individual node in the graph? 6. In Section 3.1.1., the representation of the starting retrieval (i.e., time-step = 0), h_0, is not defined. Later in the section, the paper mentions the use of TF-IDF for the initial set of nodes, instead of the learned retrieval model. This seems a bit unusual design decision without further explanation. Particularly when taking the results in Table 4, showing TF-IDF based retrieval performs worse that the learned retrieval system from the proposed model. 7. In Section 3, C_{t} (the candidate set of paragraphs) is not defined. This is an important set to define. Is it the set of paragraphs derived from Wikipedia links, starting from the current node? 8. In Section 3.1.2, \u201cLoss function\u201d, the term g_{r} is not defined. 9. In Section 4.4 \u201cAnalysis on reasoning path length\u201d, it would have been useful to see the performance of the model with different path lengths. This analysis is somewhat common on multi-hop reasoning tasks, and should be included. 10. Typo in Section 4.4: \u201c..., and out model is likely too terminate \u2026\u201d should be \u201c likely to terminate \u201c ", "rating": "6: Weak Accept", "reply_text": "We thank you for your helpful feedback . We have substantially updated our manuscript to address all the concerns you kindly raised as much as we can . First of all , we would like to address the two weaknesses you mention in your overall comments . Originality . # On the difference with other graph-based approaches Our work has several significant originalities in its system design , training and inference time strategies from Ding et al . ( 2019 ) and Godbole et al. , ( 2019 ) , which leads to more than 20 point improvements over these previous approaches on HotpotQA full wiki . 1 ) System design : We formulate the retrieval as reasoning path search over the Wikipedia graph , instead of dynamically constructing an entity-graph for each question based on compiled document lists as in the previous work ; the recurrent module dynamically updates and expands candidate paragraphs from the initial TF-IDF-based candidates at each time step . In addition , our work also studies the interplay between our retrieval model and the reader model ( See the response of \u201c Reasoning path retrieval and the interplay between our reader and retriever \u201d below for details ) . 2 ) Training strategy : To train our recurrent module to learn to retrieve the path , leveraging the graph structure , we train our model with negative sampling and multiple reference paths ( See Section 3.1.2 and the summary by review # 1 ) . 3 ) Inference strategy : We introduce beam-search based decoding to make the framework more scalable ( See Section 3.1.1 ; also summarized by review # 1 and # 2 ) , and the beam search with our reasoning path re-ranking is more effective than a greedy search . As in Table 6 , replacing beam search with greedy search deteriorates F1 by 3.7 . Also , our method does not need to encode all possible nodes like the previous studies , and instead each path only encodes its corresponding paragraphs . The HotpotQA dataset used in the previous work is based on introductory paragraphs only . By contrast , our method is applied not only to HotpotQA but also to the Natural Questions ( See Update 1 ) and SQuAD Open datasets . These two datasets are not restricted to the introductory paragraphs , and our method achieves state-of-the-art results . This is made possible by our search-based decoding strategy . One interesting observation on our Natural Questions experiments is that our model learns to retrieve multi-hop reasoning paths with our training strategy , even without multi-hop gold path annotations as in HotpotQA ( See Appendix C.5 and Table 13 ) . This demonstrates the robustness and scalability of our approach . # On the difference with other multi-step retrieval approaches Previous multi-step approaches such as Das et al . ( 2019 ) , Qi et al . ( 2019 ) , Godbole et al . ( 2019 ) and Feldman and El-Yaniv ( 2019 ) do not accommodate arbitrary steps of reasoning . As review # 1 and review # 3 summarize , our RNN approach uses the EOE symbol to produce reasoning paths with different lengths . This allows our model to be easily applicable to both multi-hop and single-hop questions without specifically changing the model architecture . Table 8 demonstrates the effectiveness of this adaptive retrieval process . In practice , it is not obvious if a question requires single-hop or multi-hop retrieval ( e.g. , some of the Natural Questions Open are clearly answerable based on single paragraph , while in some questions , multi-hop reasoning helps . ) , and thus this flexibility is another significant advantage . # Reasoning path retrieval and the interplay between our reader and retriever Our framework benefits from the interplay between our retriever and reader . Our retriever encodes the candidate paragraphs independently for scalability , and iteratively selects a paragraph at each time step conditioned by the prediction history . Each of the resulting K reasoning paths ( K=beam size ) includes one or more paragraphs . Our reader encodes the paragraphs in the paths jointly and predicts probabilities of each reasoning path E containing an answer span . By encoding the paragraphs jointly , our reader model fully leverages the self-attention mechanism across the concatenated paragraphs in the retrieved reasoning paths ; this is especially crucial for multi-hop reasoning as discussed in recent work ( Wang et al. , 2019a ) . The additional reasoning path re-ranking makes our overall framework robust , leading to large performance improvement ( See Section 4.4 and Table 8,9 ) . This reasoning path re-ranking is one of the novel points in our work . These significant differences together demonstrate state-of-the-art performance on the four experimental settings in the three datasets , HotpotQA ( full wiki , distractor ) , SQuAD Open and Natural Questions Open ( Update 1 ) . Notably , our method outperforms all the previous graph-based or multi-step retrieval methods by more than 20 points on HotpotQA full wiki and 15 points on SQuAD Open ( See Table 1,2,3 ) . We added discussions in Section 2 ( Related Work ) to clarify these points ."}, {"review_id": "SJgVHkrYDH-1", "review_text": "This paper proposes a method to find a sequence of reasoning paragraphs in Wikipedia to answer queries requiring multi-hop reasoning. They make the key observation that answering multi-hop queries might require retrieving evidence that have very less lexical overlap with the question. Given a query, the proposed method starts from a set of initial paragraphs retrieved by a tf-idf retriever and uses the outgoing Wikipedia anchor link to hop to the next evidence. They propose a simple recurrent neural network that takes in the current paragraph (and the hidden state) and decide which paragraph to hop to in the next step. Because of the available supervision for the paragraphs (in HotpotQA), they can train a supervised path selector. They also add a special EoE token that denotes the end of the reasoning path, thereby having the ability to produce reasoning paths of different lengths. After training the retriever a beam of reasoning paths is sent to the reader module. The reader module re-ranks the reasoning paths again and then use a standard BERTQA model and the top re-ranked chain of paragraphs to find the evidence. Overall, the paper presents a well-designed system for handling multi-hop queries and the explicit recurrent state is a nice contribution and addition to the IR model proposed in Godbole et al., 2019. The paper is clearly written for the most part. ===Update (11/12/2019)=== The authors have addressed all my comments and have improved the results since. I am recommending acceptance. Nice work. Strengths: \u2014 The proposed method has demonstrated strong results on 2 datasets in challenging open-domain settings. The ablation results are helpful. \u2014 The paper is clearly written and was straightforward to follow Weaknesses: 1. The paper mentions that it studies the interplay between the retriever and reader. It is unclear how it is doing so, since the retriever and the reader are not explicitly interacting with each other. Cant the retriever and the reader be trained separately? 2. It is unclear / not motivated, why there is an extra step of re-ranking required in the reading stage? In other words, what kinds of extra inductive bias is this additional step of re-ranking providing since the same kind of supervision was used while training the retriever model. I do note that the ablation study is helpful and it is clear that it is effective, but it would be nice to see a discussion regarding why this second step of re-ranking helps. 3. Since the reader model (BERT reader) takes the top scoring chain of paragraphs concatenated together, that would imply that it is currently limited by the number of positional embeddings in the BERT model (512 tokens). I think this limitation should be explicitly mentioned and possible remedies discussed. 4. The current approach is heavily dependent on Wikipedia graph and will not work if the hyperlink graph is not provided. It would have been nice to have an entity linker component that could also create the graph structure. I believe concurrent work such as Godbole et al., 2019 has addressed this and the paper should mention this while contrasting with their work. 5. From figure 2, I got an impression that since the reader scored the span in \"Top 2 reasoning path\u201d higher, that was selected. But after section 3.2, I was left confused because it looks like the reader model consumes the top scoring chain after the second stage of re-ranking. This is not clear from the figure and should be fixed. 6. Discussion on scalability: Although the retriever is clearly very effective for such questions, the running time would be prohibitive (for open domain QA) as at test time, query dependent context representations is constructed for each of the paragraph in the reasoning chain. I would like to see a discussion / some running time comparison where query independent paragraph representations are constructed and the network just encodes the query independently at test time. Minor: Typo liked -> linked (Sec 4.3, line 5)", "rating": "8: Accept", "reply_text": "We really appreciate your supportive comments on our paper and your detailed feedback . Below , we address the weakness . # On the training of reader and retriever ( re : Weaknesses 1 ) Our reader and retriever are separately trained , and this paper does not explore joint learning . We used the term \u201c interplay \u201d to represent our reasoning path re-ranking framework where our reader verifies the retrieved reasoning paths produced by the beam search , instead of finalizing the path selection only by the retriever . Our training strategy for our reader uses not only ground-truth paragraphs but also negative examples to simulate irrelevant paths produced by our retriever . Joint learning is interesting future work ; nevertheless , such a two-stage training strategy is worth investigating , considering our strong empirical results . In practice , another advantage is that the framework is flexible ; for example , when better reader models are made available , we can easily leverage the advance , without re-training the retriever model . For this revision , we re-train our reader models for SQuAD Open and HotpotQA and leveraging these new models further advances the state-of-the-art results on the two datasets . # On the inductive bias and the differences in supervision ( re : Weaknesses 1 , 2 ) There are practical differences in training our retriever and reader models . The first difference is in paragraph interactions . Our retriever learns to capture the paragraph interactions through the BERT \u2019 s [ CLS ] representations , after independently encoding the paragraphs along with the question ; this makes our sequential retrieval scalable to the open-domain scenario . By contrast , our reader model fully leverages the self-attention mechanism across the concatenated paragraphs in the retrieved reasoning paths ; this is especially crucial for multi-hop reasoning as discussed in recent work ( Wang et al. , 2019a ) . The second difference is in supervision signals . Our retriever is trained to predict plausibility of the reasoning paths , without learning to answer the question . Our reader model also learns to predict the plausibility with the stronger paragraph interactions , and jointly learns to answer the question . In summary , our retriever is scalable , but the top-1 prediction is not always enough to fully capture multi-hop reasoning to answer the question . Therefore , we use our reader model for the additional re-ranking process to mitigate the uncertainty and make our framework robust . Table 9 shows the statistics of the re-ranking results , and one interesting observation is that our reader model prefers longer paths . We added an example in Figure 4 ( and also in Table 12 in Appendix ) where the re-ranking finds more convincing reasoning paths . # The token length limitations by BERT ( re : Weaknesses 3 ) We investigated the statistics of the token length of the concatenated paragraphs in the selected reasoning path for HotpotQA full wiki . In summary , only 0.2 % of the examples exceed 512 tokens ( based on the BERT tokenization ) , and thus we expect that the influence of BERT \u2019 s maximum length limitation is marginal . We believe this is another benefit of our framework . By selecting the reasoning path , our model can effectively avoid handling many paragraphs in the encoding steps . # Reliance on hyperlink information and experiments with off-the-shelf entity linking components ( re : Weaknesses 4 , Update 3 ) Our updated manuscript presents a comparison of our framework with and without the given hyperlinks . In place of the hyperlinks , we used an off-the-shelf entity linking system . Please refer to the details of the experiments in Section 4.4 \u201c The performance with an off-the-shelf entity linking. \u201d Table 7 shows marginal performance drop even without the hyperlinks , still achieving the state of the art on HotpotQA full wiki . We would also like to mention that the existence of hyperlinks is common , especially when using documents on the Web , and our results suggest that using hyperlinks as well as entity links is promising . The most recent work ( Anonymous , 2019 ; Nie et al. , 2019 ) also relies on the hyperlinks , while our results perform better . [ ... continued in next post ]"}, {"review_id": "SJgVHkrYDH-2", "review_text": "The paper is proposing a multi-hop machine reading method tested on hotpotqa in the Full Wikipedia setting and squad-open datasets. For hotpotqa, It could also have been interesting to evaluate the method of the distractor ones. First, the proposed method constructs a graph over the Wikipedia pages represented by their respective summary paragraphs. In this representation, the hyperlinks among pages represent the edges. Then, the authors trained a normalized RNN model to retrieve the candidate reasoning paths from the question. The model is bootstrap using TF-IDF page retrieval techniques. Then, a Beam-search decoding strategy is used to retrieve \"reasoning path\" which is then pass through a BertQA model using a simple question-reasoning-path concatenation technique. One originality of the method is the negative sampling strategy that includes negative TF-IDF retrieval as starting points to robustify the sequential extraction process. The detailed experiments and ablation tests give to illustrate the experimental relevance of the proposed method.", "rating": "6: Weak Accept", "reply_text": "Thank you for reading our paper thoroughly and providing encouraging feedback . # The results on HotpotQA distractor setting Regarding the Hotpot distractor setting , we have evaluated our method on the settings , and the scores on the development set are reported in the first version of our manuscript ( See Table 1 , columns 6-9 ) ; due to the time constraints , we did not submit our model to the leaderboard . The results show that our method achieves state-of-the-art scores on the distractor setting , outperforming the previous best-published model by more than 10 points . Our work is the first to demonstrate the state-of-the-art performance on both the distractor and full wiki settings of HotpotQA . We revised our manuscript to make the distractor evaluation clear ( See Update 7 and Section 4.1 \u201c HotpotQA \u201d and Section 4.2 in our updated manuscript ) . We have a qualitative example in Appendix C.6 and Table 14 , which shows how our sequential reasoning path process also helps in distractor setting . We added new experimental results on Natural Questions Open ( Table 4 ) and additional analysis on selected reasoning paths ( Section 4.4 ) in our updated version . We hope it will be helpful in evaluating the effectiveness of our method ."}], "0": {"review_id": "SJgVHkrYDH-0", "review_text": "Summary ======== This paper introduces a graph-based recurrent retrieval model for retrieving evidence documents in a multi-hop reasoning question answering task. The main idea is that (1) the graph formed by Wikipedia links between passages can be used as constraint for constructing reasoning chains, and (2) the joint encoding of the question and current passage can be used to retrieve a subsequent passage in the reasoning chain. The paper describes a model for implementing the above retrieval system, and how they jointly train with a reading comprehension model. They demonstrate the effectiveness of the system on HotPotQA, showing improvements over previously published models, and SQuaD-Open, showing competitive results. Overall Comments =============== The paper is an interesting, but incremental, improvement to the area of question answering. Overall, there are two main concerns about this work. First, while the results are somewhat strong, the ideas presented are small variations on existing systems. For example, Godbole et al 2019 and Ding et al. 2019 both explore using graphical structural to constraint iterative, multi-hop, retrieval. Also, Feldman et al 2019, describe an encoder based approach to encode question and paragraph context for iterative retrieval. Asides from smaller modeling differences (choice of RNN, training regime, BERT reader, etc.) to account for the difference in results, the main difference seems to be the joint training of the retrieval system with the reader. Secondly, the paper lacks clarity on some formal definitions and definition of the graph, making it hard to understand the content precisely. Detailed Comments ================ Below are some detailed comments about specific parts of the paper, in order of importance: 1. One important limitation of this technique is the reliance on a linked documents for constructing the retrieval system. It is not clear from the paper how much of the results are obtained from constraining the set of retrieved passages (after the initial retrieval) to Wikipedia links. And whether, for example, substituting Wikipedia links with links derived from an off-the-shelf entity linking system would suffice. 2. Given that the retrieval model is restricted to link structure in Wikipedia that induces the proposed retrieval graph, I assume that there are \u201creasoning paths\u201d that do not exist in the graph, given Wikipedia\u2019s policy of avoiding adding redundant links within a Wikipedia page. It would have been informative to conduct an \u201cOracle\u201d experiment: that is, given the initial set of retrieved nodes and the graph structure, are there *any* paths that provide the correct answer and reasoning chain? That is to say, what is the upper-bound performance on the proposed system given the currently induced Wikipedia graph? 3. In Section 3, and even later on in the paper, it was not clear what \u201cE\u201d denotes. It never seems to be defined, and is used interchangeably with \u201cgraph node\u201d, \u201cwikipedia page\u201d, \u201cwikipedia paragraph\u201d and \u201creasoning path\u201d. Are these the same thing? It would be much clearer to define what E means, and perhaps separate the different concepts (node, passage, reasoning path) properly. 4. In Section 3, it seems that \u2018q\u2019 is not defined. Is it the question? 5. In Section 3.1, it is not clear what the graph actually contains. Does it contain all the paragraphs from Wikipedia? Just the paragraphs with links? The first paragraph of every Wikipedia page? What granularity of the wikipedia page becomes an individual node in the graph? 6. In Section 3.1.1., the representation of the starting retrieval (i.e., time-step = 0), h_0, is not defined. Later in the section, the paper mentions the use of TF-IDF for the initial set of nodes, instead of the learned retrieval model. This seems a bit unusual design decision without further explanation. Particularly when taking the results in Table 4, showing TF-IDF based retrieval performs worse that the learned retrieval system from the proposed model. 7. In Section 3, C_{t} (the candidate set of paragraphs) is not defined. This is an important set to define. Is it the set of paragraphs derived from Wikipedia links, starting from the current node? 8. In Section 3.1.2, \u201cLoss function\u201d, the term g_{r} is not defined. 9. In Section 4.4 \u201cAnalysis on reasoning path length\u201d, it would have been useful to see the performance of the model with different path lengths. This analysis is somewhat common on multi-hop reasoning tasks, and should be included. 10. Typo in Section 4.4: \u201c..., and out model is likely too terminate \u2026\u201d should be \u201c likely to terminate \u201c ", "rating": "6: Weak Accept", "reply_text": "We thank you for your helpful feedback . We have substantially updated our manuscript to address all the concerns you kindly raised as much as we can . First of all , we would like to address the two weaknesses you mention in your overall comments . Originality . # On the difference with other graph-based approaches Our work has several significant originalities in its system design , training and inference time strategies from Ding et al . ( 2019 ) and Godbole et al. , ( 2019 ) , which leads to more than 20 point improvements over these previous approaches on HotpotQA full wiki . 1 ) System design : We formulate the retrieval as reasoning path search over the Wikipedia graph , instead of dynamically constructing an entity-graph for each question based on compiled document lists as in the previous work ; the recurrent module dynamically updates and expands candidate paragraphs from the initial TF-IDF-based candidates at each time step . In addition , our work also studies the interplay between our retrieval model and the reader model ( See the response of \u201c Reasoning path retrieval and the interplay between our reader and retriever \u201d below for details ) . 2 ) Training strategy : To train our recurrent module to learn to retrieve the path , leveraging the graph structure , we train our model with negative sampling and multiple reference paths ( See Section 3.1.2 and the summary by review # 1 ) . 3 ) Inference strategy : We introduce beam-search based decoding to make the framework more scalable ( See Section 3.1.1 ; also summarized by review # 1 and # 2 ) , and the beam search with our reasoning path re-ranking is more effective than a greedy search . As in Table 6 , replacing beam search with greedy search deteriorates F1 by 3.7 . Also , our method does not need to encode all possible nodes like the previous studies , and instead each path only encodes its corresponding paragraphs . The HotpotQA dataset used in the previous work is based on introductory paragraphs only . By contrast , our method is applied not only to HotpotQA but also to the Natural Questions ( See Update 1 ) and SQuAD Open datasets . These two datasets are not restricted to the introductory paragraphs , and our method achieves state-of-the-art results . This is made possible by our search-based decoding strategy . One interesting observation on our Natural Questions experiments is that our model learns to retrieve multi-hop reasoning paths with our training strategy , even without multi-hop gold path annotations as in HotpotQA ( See Appendix C.5 and Table 13 ) . This demonstrates the robustness and scalability of our approach . # On the difference with other multi-step retrieval approaches Previous multi-step approaches such as Das et al . ( 2019 ) , Qi et al . ( 2019 ) , Godbole et al . ( 2019 ) and Feldman and El-Yaniv ( 2019 ) do not accommodate arbitrary steps of reasoning . As review # 1 and review # 3 summarize , our RNN approach uses the EOE symbol to produce reasoning paths with different lengths . This allows our model to be easily applicable to both multi-hop and single-hop questions without specifically changing the model architecture . Table 8 demonstrates the effectiveness of this adaptive retrieval process . In practice , it is not obvious if a question requires single-hop or multi-hop retrieval ( e.g. , some of the Natural Questions Open are clearly answerable based on single paragraph , while in some questions , multi-hop reasoning helps . ) , and thus this flexibility is another significant advantage . # Reasoning path retrieval and the interplay between our reader and retriever Our framework benefits from the interplay between our retriever and reader . Our retriever encodes the candidate paragraphs independently for scalability , and iteratively selects a paragraph at each time step conditioned by the prediction history . Each of the resulting K reasoning paths ( K=beam size ) includes one or more paragraphs . Our reader encodes the paragraphs in the paths jointly and predicts probabilities of each reasoning path E containing an answer span . By encoding the paragraphs jointly , our reader model fully leverages the self-attention mechanism across the concatenated paragraphs in the retrieved reasoning paths ; this is especially crucial for multi-hop reasoning as discussed in recent work ( Wang et al. , 2019a ) . The additional reasoning path re-ranking makes our overall framework robust , leading to large performance improvement ( See Section 4.4 and Table 8,9 ) . This reasoning path re-ranking is one of the novel points in our work . These significant differences together demonstrate state-of-the-art performance on the four experimental settings in the three datasets , HotpotQA ( full wiki , distractor ) , SQuAD Open and Natural Questions Open ( Update 1 ) . Notably , our method outperforms all the previous graph-based or multi-step retrieval methods by more than 20 points on HotpotQA full wiki and 15 points on SQuAD Open ( See Table 1,2,3 ) . We added discussions in Section 2 ( Related Work ) to clarify these points ."}, "1": {"review_id": "SJgVHkrYDH-1", "review_text": "This paper proposes a method to find a sequence of reasoning paragraphs in Wikipedia to answer queries requiring multi-hop reasoning. They make the key observation that answering multi-hop queries might require retrieving evidence that have very less lexical overlap with the question. Given a query, the proposed method starts from a set of initial paragraphs retrieved by a tf-idf retriever and uses the outgoing Wikipedia anchor link to hop to the next evidence. They propose a simple recurrent neural network that takes in the current paragraph (and the hidden state) and decide which paragraph to hop to in the next step. Because of the available supervision for the paragraphs (in HotpotQA), they can train a supervised path selector. They also add a special EoE token that denotes the end of the reasoning path, thereby having the ability to produce reasoning paths of different lengths. After training the retriever a beam of reasoning paths is sent to the reader module. The reader module re-ranks the reasoning paths again and then use a standard BERTQA model and the top re-ranked chain of paragraphs to find the evidence. Overall, the paper presents a well-designed system for handling multi-hop queries and the explicit recurrent state is a nice contribution and addition to the IR model proposed in Godbole et al., 2019. The paper is clearly written for the most part. ===Update (11/12/2019)=== The authors have addressed all my comments and have improved the results since. I am recommending acceptance. Nice work. Strengths: \u2014 The proposed method has demonstrated strong results on 2 datasets in challenging open-domain settings. The ablation results are helpful. \u2014 The paper is clearly written and was straightforward to follow Weaknesses: 1. The paper mentions that it studies the interplay between the retriever and reader. It is unclear how it is doing so, since the retriever and the reader are not explicitly interacting with each other. Cant the retriever and the reader be trained separately? 2. It is unclear / not motivated, why there is an extra step of re-ranking required in the reading stage? In other words, what kinds of extra inductive bias is this additional step of re-ranking providing since the same kind of supervision was used while training the retriever model. I do note that the ablation study is helpful and it is clear that it is effective, but it would be nice to see a discussion regarding why this second step of re-ranking helps. 3. Since the reader model (BERT reader) takes the top scoring chain of paragraphs concatenated together, that would imply that it is currently limited by the number of positional embeddings in the BERT model (512 tokens). I think this limitation should be explicitly mentioned and possible remedies discussed. 4. The current approach is heavily dependent on Wikipedia graph and will not work if the hyperlink graph is not provided. It would have been nice to have an entity linker component that could also create the graph structure. I believe concurrent work such as Godbole et al., 2019 has addressed this and the paper should mention this while contrasting with their work. 5. From figure 2, I got an impression that since the reader scored the span in \"Top 2 reasoning path\u201d higher, that was selected. But after section 3.2, I was left confused because it looks like the reader model consumes the top scoring chain after the second stage of re-ranking. This is not clear from the figure and should be fixed. 6. Discussion on scalability: Although the retriever is clearly very effective for such questions, the running time would be prohibitive (for open domain QA) as at test time, query dependent context representations is constructed for each of the paragraph in the reasoning chain. I would like to see a discussion / some running time comparison where query independent paragraph representations are constructed and the network just encodes the query independently at test time. Minor: Typo liked -> linked (Sec 4.3, line 5)", "rating": "8: Accept", "reply_text": "We really appreciate your supportive comments on our paper and your detailed feedback . Below , we address the weakness . # On the training of reader and retriever ( re : Weaknesses 1 ) Our reader and retriever are separately trained , and this paper does not explore joint learning . We used the term \u201c interplay \u201d to represent our reasoning path re-ranking framework where our reader verifies the retrieved reasoning paths produced by the beam search , instead of finalizing the path selection only by the retriever . Our training strategy for our reader uses not only ground-truth paragraphs but also negative examples to simulate irrelevant paths produced by our retriever . Joint learning is interesting future work ; nevertheless , such a two-stage training strategy is worth investigating , considering our strong empirical results . In practice , another advantage is that the framework is flexible ; for example , when better reader models are made available , we can easily leverage the advance , without re-training the retriever model . For this revision , we re-train our reader models for SQuAD Open and HotpotQA and leveraging these new models further advances the state-of-the-art results on the two datasets . # On the inductive bias and the differences in supervision ( re : Weaknesses 1 , 2 ) There are practical differences in training our retriever and reader models . The first difference is in paragraph interactions . Our retriever learns to capture the paragraph interactions through the BERT \u2019 s [ CLS ] representations , after independently encoding the paragraphs along with the question ; this makes our sequential retrieval scalable to the open-domain scenario . By contrast , our reader model fully leverages the self-attention mechanism across the concatenated paragraphs in the retrieved reasoning paths ; this is especially crucial for multi-hop reasoning as discussed in recent work ( Wang et al. , 2019a ) . The second difference is in supervision signals . Our retriever is trained to predict plausibility of the reasoning paths , without learning to answer the question . Our reader model also learns to predict the plausibility with the stronger paragraph interactions , and jointly learns to answer the question . In summary , our retriever is scalable , but the top-1 prediction is not always enough to fully capture multi-hop reasoning to answer the question . Therefore , we use our reader model for the additional re-ranking process to mitigate the uncertainty and make our framework robust . Table 9 shows the statistics of the re-ranking results , and one interesting observation is that our reader model prefers longer paths . We added an example in Figure 4 ( and also in Table 12 in Appendix ) where the re-ranking finds more convincing reasoning paths . # The token length limitations by BERT ( re : Weaknesses 3 ) We investigated the statistics of the token length of the concatenated paragraphs in the selected reasoning path for HotpotQA full wiki . In summary , only 0.2 % of the examples exceed 512 tokens ( based on the BERT tokenization ) , and thus we expect that the influence of BERT \u2019 s maximum length limitation is marginal . We believe this is another benefit of our framework . By selecting the reasoning path , our model can effectively avoid handling many paragraphs in the encoding steps . # Reliance on hyperlink information and experiments with off-the-shelf entity linking components ( re : Weaknesses 4 , Update 3 ) Our updated manuscript presents a comparison of our framework with and without the given hyperlinks . In place of the hyperlinks , we used an off-the-shelf entity linking system . Please refer to the details of the experiments in Section 4.4 \u201c The performance with an off-the-shelf entity linking. \u201d Table 7 shows marginal performance drop even without the hyperlinks , still achieving the state of the art on HotpotQA full wiki . We would also like to mention that the existence of hyperlinks is common , especially when using documents on the Web , and our results suggest that using hyperlinks as well as entity links is promising . The most recent work ( Anonymous , 2019 ; Nie et al. , 2019 ) also relies on the hyperlinks , while our results perform better . [ ... continued in next post ]"}, "2": {"review_id": "SJgVHkrYDH-2", "review_text": "The paper is proposing a multi-hop machine reading method tested on hotpotqa in the Full Wikipedia setting and squad-open datasets. For hotpotqa, It could also have been interesting to evaluate the method of the distractor ones. First, the proposed method constructs a graph over the Wikipedia pages represented by their respective summary paragraphs. In this representation, the hyperlinks among pages represent the edges. Then, the authors trained a normalized RNN model to retrieve the candidate reasoning paths from the question. The model is bootstrap using TF-IDF page retrieval techniques. Then, a Beam-search decoding strategy is used to retrieve \"reasoning path\" which is then pass through a BertQA model using a simple question-reasoning-path concatenation technique. One originality of the method is the negative sampling strategy that includes negative TF-IDF retrieval as starting points to robustify the sequential extraction process. The detailed experiments and ablation tests give to illustrate the experimental relevance of the proposed method.", "rating": "6: Weak Accept", "reply_text": "Thank you for reading our paper thoroughly and providing encouraging feedback . # The results on HotpotQA distractor setting Regarding the Hotpot distractor setting , we have evaluated our method on the settings , and the scores on the development set are reported in the first version of our manuscript ( See Table 1 , columns 6-9 ) ; due to the time constraints , we did not submit our model to the leaderboard . The results show that our method achieves state-of-the-art scores on the distractor setting , outperforming the previous best-published model by more than 10 points . Our work is the first to demonstrate the state-of-the-art performance on both the distractor and full wiki settings of HotpotQA . We revised our manuscript to make the distractor evaluation clear ( See Update 7 and Section 4.1 \u201c HotpotQA \u201d and Section 4.2 in our updated manuscript ) . We have a qualitative example in Appendix C.6 and Table 14 , which shows how our sequential reasoning path process also helps in distractor setting . We added new experimental results on Natural Questions Open ( Table 4 ) and additional analysis on selected reasoning paths ( Section 4.4 ) in our updated version . We hope it will be helpful in evaluating the effectiveness of our method ."}}