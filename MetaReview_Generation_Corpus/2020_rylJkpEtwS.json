{"year": "2020", "forum": "rylJkpEtwS", "title": "Learning the Arrow of Time for Problems in Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "This paper develops the notion of the arrow of time in MDPs and explores how this might be useful in RL. All the reviewers found the paper thought provoking, well-written, and they believe the work could have significant impact. The paper does not fit the typical mold: it presents some ideas and uses illustrative experiments to suggest the potential utility of the arrow without nailing down a final algorithm or make a precise performance claim. Overall it is a solid paper, and the reviewers all agreed on acceptance.\n\nThere are certainly weaknesses in the work, and there is a bit of work to do to get this paper ready. R2 had a nice suggestion of a baseline based on simply learning a transition model (its described in the updated review)---please include it. The description of the experimental methodology is a bit of a mess. Most of the experiments in the paper do not clearly indicate how many runs were conducted or how errorbars where computed or what they represent.  It is likely that only a handful of runs were used, which is surprising given the size of some of the domains used. In many cases the figure caption does not even indicate which domain the data came from. All of this is dangerously close to criteria for rejection; please do better.\n\nReadability is also known as empowerment and it would be good to discuss this connection. In general the paper was a bit light on connections outlining how information theory has been used in RL. I suggest you start here (http://www2.hawaii.edu/~sstill/StillPrecup2011.pdf) to improve this aspect. Finally, the paper has a very large appendix (~14 oages) with many many more experiments and theory. I am still not convinced that the balance is quite right. This is probably a journal or long arxiv paper. Maybe this paper should be thought of as a nectar version of a longer standalone arxiv paper.\n\nFinally, relying on effectiveness of random exploration is no small thing and there is a long history in RL of ideas that would work well, given it is easy to gather data that accurately summarizes the dynamics of the world (e.g. proto-value, funcs). Many ideas are effective given this assumption. The paper should clearly and honestly discuss this assumption, and provide some arguments why there is hope.", "reviews": [{"review_id": "rylJkpEtwS-0", "review_text": "The paper draws on a wide range of ideas, and proposes novel perspectives on how these ideas might apply in RL. In particular, the concept of reachability, reversability and dissipation are explored, with respect to properties of the underlying MDP that can be exploited. I found many of the ideas thought-provoking. The paper is also well written and a pleasure to read. But unfortunately the work falls short of its objective in the experiment section. Not a single baseline is included. I would expect to see comparison to a simple model-based method for all the experiments on p.7. The main result for 7x7 2D world seems to be that the agent has learned to quantify irreversibility. I would expect a simple statistical estimator over state transitions (using the same samples as the h-potential method) to be able to capture this as well. The main result for Sokoban seems to be that the h-potential has detected side-effects of actions; again, why can\u2019t a model estimator learn this? Similarly in Mountain car, it seems possible to directly estimate the terrain from the data, without the h-potential. As a more minor concern, the fact that the method uses a batch of uniformly random state transitions (as per Sec.4), rather than randomly sampled trajectories is a definite concerned with respect to real-world application. Minor comments: - Top of p.3: Can you give some intuition for h(), e.g. relation to entropy over trajectory. - Bottom of p.3: you use a random policy to sample trajectories. Is this simple to implement? Can you just sample random actions at each state, or do you need to sample over the space of all trajectories? - Footnote 2, p.4. It would be interesting to expand on this point. ============== Update post-rebuttal: Indeed, my concern is that a simple T(s,a,s') estimator, using the same samples, could infer the same characteristics in each of the experiments (number of broken vases; effect of box pushing; terrain), and could then be used for model-based planning. But I do appreciate the insights provided by connecting these through the broader notion of arrow of time developed in this paper, and its connection to reachability and safety. Therefore I am raising my score to weak accept.", "rating": "6: Weak Accept", "reply_text": "Thank you for your review - we are glad that you enjoyed reading our paper and find many of our ideas thought provoking . > I would expect to see comparison to a simple model-based method [ ... ] To recapitulate , we propose a method to quantify irreversibility in MDPs without having to learn the full dynamics model of the said MDP . To that end , we obtain a function $ \\eta $ of two states $ s , s ' $ such that $ \\eta ( s , s ' ) = h ( s ' ) - h ( s ) $ is a scalar that quantifies the reversibility of the transition $ s \\to s ' $ ( cf.Sec 3.1 ) .In contrast , a forward model is typically a function of a state $ s $ and an action that outputs ( a distribution over ) the next state $ s ' $ , i.e. $ p ( s ' | s , a ) $ . While we do not see a straightforward way of extracting a baseline measure of reversibility from the environment model , we would be very receptive to your suggestions . It is also worth noting that $ \\eta ( s , s ' ) $ has the functional constraint that it must be expressible as the difference $ h ( s ' ) - h ( s ) $ of h-potentials ( akin to a siamese neural network ) . This constraint functions as an inductive bias and endows it with the ability to generalise beyond the transitions that are observed ( cf.Sec 3.1 , especially p. 5 `` Third , $ \\eta $ allows for a soft measure of reachability [ ... ] '' ) . In general , this can not be expected of an environment model . > [ ... ] for 7x7 2D world [ ... ] I would expect a simple statistical estimator over state transitions ( using the same samples as the h-potential method ) to be able to capture this as well . It would certainly be possible to hand-craft an estimator that captures irreversibility of state transitions ; this might be a challenging endeavor ( or not ) , depending on the environment . However , our goal is to _learn_ a model that can capture irreversibility . The experiment on 2D World with Vases ( Fig 2 and App C.1.1 ) therefore serves the following purposes . ( 1 ) It reassures us and our audience that the h-potential indeed learns to count the number of broken vases ( which is precisely what one would consider when designing a statistical estimator ) . ( 2 ) It serves as a sandbox to understand the strengths and limitations of our method -- for instance , we find that our method is fairly robust against random uncorrelated noise , but can be distracted by time-correlated noise . ( 3 ) It helps expose the the expected trade-off between safety and efficiency ( Fig 12 ) -- while the baseline DDQN agent trained without the h-potential out-performs the safe agent trained with the h-potential as far as the probability of reaching the goal is concerned , the baseline agent breaks a larger number of vases ( i.e.is less safe ) than the safe agent to acheive its goal . > Sokoban seems to be that the h-potential has detected side-effects of actions ; again , why can \u2019 t a model estimator learn this ? We would appreciate it if you could clarify what `` learning a model estimator '' might mean in this context . Please correct us if we are wrong , but we interpret your question as meaning either `` Why not learn a model to directly predict side-effects ? '' or `` Why not directly predict whether a transition is irreversible ? `` . For the former : we often do not have access to ground-truth labels for when a transition induces side-effects . But if we did , it might also make sense to use it directly . For the latter : our method is indeed based on predicting whether a transition is irreversible , but with a crucial inductive bias : the corresponding predictor must be a difference of two functions ( akin to a siamese neural network ) . Please refer to p. 4 ( `` Instead , our proposal is to derive [ ... ] '' et seq . ) for a discussion . > As a more minor concern , the fact that the method uses a batch of uniformly random state transitions ( as per Sec.4 ) , rather than randomly sampled trajectories is a definite concerned with respect to real-world application . This might be a misunderstanding -- by uniform random policy , we mean that the actions are sampled randomly , with no action preferred over the other . This is straightforward to implement for the environments we consider . We will clarify this in the next update . ( Continued in Part 2 . )"}, {"review_id": "rylJkpEtwS-1", "review_text": "This work proposes the h-potential, which is a solution to an objective that measures state-transition asymmetry in an MDP. Roughly speaking, in many situations some state transitions (s-->s\u2019) are more probable than their converse (s\u2019-->s), and if we have a function that assigns a higher value to a more probable transition (compared to its converse), then we can use it as a measure of the \u201creversibility\u201d of that transition. This function can then be used, for example, as an intrinsic reward signal; indeed, there may be cases where state transitions should be avoided if they are not reversible. The authors tie these ideas into the notion of the arrow of time. They go on to explore the various nuances and subtleties of this measure, and demonstrate its behaviour empirically on a number of environments. This paper was an absolute pleasure to read. The prose was clear, interesting, and nuanced. The authors anticipate many questions and do well to explain the various subtleties of their method. Overall the experiments are a nice demonstration of the presented ideas. I am inclined to give this paper a high rating, as there was very little that I felt was \u201cwrong\u201d or inaccurate. But I must also admit that some of the theoretical components are beyond my expertise, and so I can only be moderately confident. I will defer to other reviewers and any online discussion on these more technical matters. I have a few questions that I hope the authors can address. 1) The method depends on a random policy (or, more accurately, was empirically validated mainly using a random policy, aside from some very simple environments as far as I can see). Can the authors comment on the usefulness of this method for a non-random policy in more complicated environments? Do they have any experimental results showing the effect of incorporating this into an agent also receiving (and learning from) exogenous rewards in an environment such as Sokoban (as it\u2019s used here already) or Atari? 2) Related to the first point, how does this method scale to environments whose state-space can only be sparsely covered with a random policy? It seems in this case a task-relevant policy would be needed to explore more of the state-space, which would place pressure on the h-potential function approximator as it has to learn with sequentially correlated inputs. You can imagine something like an experience replay buffer being needed, but in any case, there are definitely unique challenges here not explored in the paper. 3) Can the authors comment on the notion of a function being statistically monotonic vs. deterministically so, and whether the arrow of time is classically considered the former or the latter? My reasoning for this question comes from a place where I\u2019m questioning whether the motivation of the work can be simplified. The appeal to the arrow of time is nice and reads well, but it\u2019s also the case that this work can simply be interpreted as \u201clearned state transition reversibility\u201d, with the links to the arrow of time being more of a point of discussion. ", "rating": "8: Accept", "reply_text": "Thank you for your positive review -- we are elated that you enjoyed reading our work ! > I have a few questions that I hope the authors can address . Those are all good questions . Regarding ( 1 ) and ( 2 ) : the issue of non-random policy for gathering trajectories is perhaps the most important issue we do not address in this work , for doing it full justice would detract significantly from the original goal of the paper . Nevertheless , one approach that we see being fruitful is that of off-policy learning [ 1 ] , which applies to cases where the behaviour policy used to gather trajectories differs from the evaluation policy of interest . In our case , the behaviour policy could be any combination of an exploratory policy and a learning agent , whereas the evaluation policy is random ( in order to not bias the h-potential ) . Importance sampling is one example of this scheme , but more sophisticated methods exist . As such , your assessment that `` there are unique challenges here not explored in the paper '' is spot on . Moreover , it might also be worthwhile to remark that the adoption of random rollouts is rather widespread in model-based ( related ) literature [ 3-8 ] . Regarding question ( 3 ) : Consider a stochastic process $ X_t $ ( i.e.a ( time- ) series of random variables ) . We say that a ( deterministic ) function $ h $ is statistically monotonic increasing if the function $ H ( t ) = \\mathbb { E } [ h ( X_t ) | h ( X_ { t - 1 } ) , ... , h ( X_1 ) ] $ is ( deterministically ) monotonic increasing with t. In other words , only the expectation of $ h $ ( w.r.t.its argument random variable ) must increase with time ( but not $ h $ itself ) . In technical jargon , $ h ( X_t ) $ is sometimes called a submartingale [ 2 ] . All that said , we agree with your interpretation of our work as `` learning the state transition reversibility '' , but with a crucial detail -- namely that the learner has a specific architecture resembling a siamese network , i.e.its output is a difference of a function applied to its two inputs . This function turns out to be the h-potential : cf . p. 4 of our manuscript : `` Instead , our proposal is to derive [ ... ] '' ( et seq . ) for a discussion about what we may gain by using this specific architecture . We hope to have answered your questions -- please let us know if not ! [ 1 ] Munos et al.2016 , `` Safe and Efficient Off-Policy Reinforcement Learning . '' https : //arxiv.org/abs/1606.02647 [ 2 ] https : //en.wikipedia.org/wiki/Martingale_ ( probability_theory ) # Submartingales , _supermartingales , _and_relationship_to_harmonic_functions [ 3 ] Savinov et al.2018 , `` Semi-parametric Topological Memory for Navigation . '' https : //arxiv.org/abs/1803.00653 [ 4 ] Ha & Schmidhuber 2018 , `` World Models . '' https : //arxiv.org/abs/1803.10122 [ 5 ] Savinov et al.2018 , `` Episodic Curiosity Through Reachability . '' https : //arxiv.org/abs/1810.02274 [ 6 ] Nagabandi et al.2017 , `` Learning Image-Conditioned Dynamics Models for Control of Under-actuated Legged Millirobots . '' https : //arxiv.org/abs/1711.05253 [ 7 ] Kulkarni et al.2019 , `` Unsupervised Learning of Object Keypoints for Perception and Control . '' https : //arxiv.org/abs/1906.11883 [ 8 ] Anand et al.2019 , `` Unsupervised State Representation Learning in Atari . '' https : //arxiv.org/abs/1906.08226"}, {"review_id": "rylJkpEtwS-2", "review_text": "This paper proposes that we learn the \u201carrow of time\u201d for an MDP: that is, a function (called the h-potential) that tends to increase as the MDP steps forward. Such an arrow should automatically capture notions such as irreversibility, and so can be used to define a measure of reachability, which previous work has shown can be used to penalize the agent for causing negative side effects. In addition, it can be used as intrinsic motivation for the agent: in particular, the agent can be rewarded for trajectories that decrease the h-potential (i.e. are \u201clike\u201d going backwards in time, or reducing entropy), which is hard to do and should lead to interesting skills. They propose that we learn the arrow of time by optimizing a function to grow over time along trajectories take from a random policy. Experiments demonstrate that in simple environments the learned function has the properties we would expect it to given results from physics. I am conflicted on this paper. I like the novelty of the suggestion; it is not something I would have expected to see in an ML paper. The discussion and experiments have convinced me that the idea is worth investigating: they show that the learned arrow of time approximately satisfies the properties we would expect, and demonstrate their two use cases in two simple environments: the intrinsic reward is used in a tomato-watering environment, while the side effect avoidance is shown in Sokoban (though the experiment only shows that the h-potential increases with irreversible actions -- it doesn\u2019t actually use the h-potential to create an agent that reliably avoids irreversibly pushing boxes into corners). However, it\u2019s not clear to me whether or not the method would scale to more complex environments, the current experiments are more like demonstrations (there are no baselines), and the paper is hard to understand without a background in physics. Overall, given the novelty of the suggestion, I lean towards a weak accept. ---- In your objective, you use an expectation over the timestep in the trajectory. Why not instead take an average over all the timesteps in the trajectory? This should be equivalent. (If trajectories are all the same length, you could also take a sum over the timesteps.) Similarly, in the algorithm, why do you sample from the dataset? It would likely be better to randomly shuffle the dataset (at the timestep level) once, and then iterate through the dataset computing gradient updates. (This is standard practice in supervised learning.) ---- When scaling the algorithm up to larger environments, you will likely run into the problem that uniform policies are often very bad at exploring the state space. Consider for example the Overcooked environment (https://bair.berkeley.edu/blog/2019/10/21/coordination/ ). The environment is not dissipative, so you\u2019d expect the h-potential to be zero. However, uniform policies are extremely unlikely to ever deliver a soup (which requires several hierarchical actions), but sometimes will pick up an onion. If you don\u2019t know about soup delivery, then picking up an onion looks irreversible, because there\u2019s no way to get rid of it, and the h-potential will rise. I think it would significantly improve this paper to demonstrate a solution to this problem. One possibility is to redefine the arrow of time to be with respect to some distribution over states: E_{s ~ p(s)} E_{a ~ Uniform(A)} E_{s\u2019 ~ p(s\u2019 | s, a)} [ h(s\u2019) - h(s) | s ]. Initially, your distribution over states can be the initial state distribution. But then you can use the learned arrow of time as an intrinsic reward to find a policy that finds \u201cinteresting states\u201d. In Overcooked, we would hope that this policy learns to pick up onions. Then, your new distribution over states can be the states reached in 0-20 timesteps when following the new policy, that is, you collect your dataset by running the \u201cinteresting\u201d policy for some time, and then switch back to the uniformly random policy, and only use the states / actions collected during the uniform random policy as part of your dataset. Hopefully, this would include states where the onion is placed in a pot, and the h-potential would learn that placing an onion in a pot is \u201cirreversible\u201d. Then, another round of using the h-potential would lead to a policy that places onions in pots. Collecting data could then discover delivering soups, and so on. ---- In addition to experiments on larger environments, I would like to see better experiments for the two intended use cases. For example, can you use the learned arrow of time to solve the environments in (Krakovna et al), and how does it compare to relative reachability and its many variants? Similarly, how does your intrinsic reward compare to existing exploration methods (of which there are many, but consider count-based methods, curiosity (Pathak et al), random network distillation (Burda et al))? ---- (This section did not affect my assessment of the paper) Have you considered finding theoretically what it means to take the difference between the h-potential of two states (i.e. your reachability measure)? I could imagine that the answer is something like \u201creachability(s, s\u2019) is proportional to the log probability of reaching s\u2019 from s when acting according to a uniformly random policy\u201d. Perhaps this has to be normalized against the log probability of reaching other states from s. This would be very interesting as a potential definition of reachability. ---- There is a lot of jargon from physics that will not be familiar to the typical audience at ICLR (e.g. Hamiltonian, Liouville\u2019s theorem, Maxwell\u2019s demon, free energy functionals, etc.) I would recommend improving the clarity of the paper on this axis. There\u2019s no particular need to name Maxwell\u2019s demon prominently -- the exposition is sufficient by itself, perhaps Maxwell\u2019s demon can be mentioned in a footnote. Consider adding a discussion of ergodicity (as applied to Markov chains / MDPs), which will be more familiar to the audience and serves a similar purpose as the discussion on Hamiltonian systems. Perhaps move the experiment with the free energy functional to the appendix, and move the experimental details for the other experiments from the appendix to the main paper.", "rating": "6: Weak Accept", "reply_text": "Thank you for the time you 've invested in reviewing our work -- we are glad that you find our suggestion novel , and are very grateful for your suggestions . The following is a first response to your comments ; our manuscript will be updated in the coming days to reflect your suggestions and we will notify you once the changes are in place . > the paper is hard to understand without a background in physics [ ... ] There is a lot of jargon from physics that will not be familiar to the typical audience at ICLR In hindsight , we agree with your assessment that some aspects of the paper might be difficult to understand without a background in physics , and part of this indeed has to do with the jargon . We will try to address this where possible . That said , we view our work as an attempt to view problems in RL from the lens of ideas and concepts that are already well understood in statistical physics . For instance , the connections between Variational Fokker-Planck and Reinforcement Learning is less explored ( the only relevant reference we found was [ 1 ] ) . Likewise , the notion of dissipativity has received little attention in modern reinforcement learning ( but has been well studied in the context of dynamical systems ) . We hope that our empirical results ( especially `` Comparison with the Free-Energy Functional '' , p. 8 and `` [ ... ] the Importance of Dissipativity '' , p. 7 ) inspires more theoretical research in this direction . Indeed , the fields of unsupervised learning and deep learning theory has greatly benifited from such interdisciplinary endeavors ( e.g . [ 2 ] , Energy Based Models , [ 3 ] , and more ) , and we wish the same for reinforcement learning . > In your objective , you use an expectation over the timestep in the trajectory . Why not instead take an average over all the timesteps in the trajectory ? This should be equivalent . They are indeed equivalent ( and a matter of notation ) . > Similarly , in the algorithm , why do you sample from the dataset ? It would likely be better to randomly shuffle the dataset ( at the timestep level ) once , and then iterate through the dataset computing gradient updates . It 's certainly possible to partition the training in epochs , as you suggest . We will investigate if this leads to gains in performance . > When scaling the algorithm up to larger environments , you will likely run into the problem that uniform policies are often very bad at exploring the state space . The issue you mention is exceedingly important , and doing it justice will require us to deviate significantly from the primary objective of this work . We hope to have been upfront about it ( cf.top of p. 4 : `` The price we pay is the lack of adequate exploration in complex enough environments [ ... ] '' and footnote 3 ) and intend to pursue this in future work . We think one promising approach could be based on off-policy methods [ 10 ] , which applies to cases where the behaviour policy differs from the ( evaluation ) policy of interest ( importance sampling is one simple example ) . In our case , the evaluation policy can still be random , whereas the behaviour policy is exploratory . Moreover , it is worth noting that the choice of an offline `` base policy '' is a recurring theme concerning model-based ( and related ) methods [ 9 ] , and a random policy is a widespread choice [ 4-6 , 11-13 ] . Some works use a mixture of a pre-trained and a random policy [ 7 , 8 ] , whereas some other works [ 5 ] attempt to expose and tackle the issue explicitly . ( Continued in next post . ) [ 1 ] Richemond & Maginnis 2017 , `` On Wasserstein Reinforcement Learning and the Fokker Planck equation . '' https : //arxiv.org/abs/1712.07185 [ 2 ] Sohl-Dickstein et al.2015 , `` Deep Unsupervised Learning using Nonequilibrium Thermodynamics . '' https : //arxiv.org/abs/1503.03585 [ 3 ] Poole et al.2016 , `` Exponential expressivity in deep neural networks through transient chaos . '' https : //arxiv.org/abs/1606.05340 [ 4 ] Savinov et al.2018 , `` Semi-parametric Topological Memory for Navigation . '' https : //arxiv.org/abs/1803.00653 [ 5 ] Ha & Schmidhuber 2018 , `` World Models . '' https : //arxiv.org/abs/1803.10122 [ 6 ] Savinov et al.2018 , `` Episodic Curiosity Through Reachability . '' https : //arxiv.org/abs/1810.02274 [ 7 ] Oh et al.2015 , `` Action-Conditional Video Prediction using Deep Networks in Atari Games . '' https : //arxiv.org/pdf/1507.08750.pdf [ 8 ] Chiappa et al.2017 , `` Recurrent Environment Simulators . '' https : //arxiv.org/abs/1704.02254 [ 9 ] http : //rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_9_model_based_rl.pdf [ 10 ] Munos et al.2016 , `` Safe and Efficient Off-Policy Reinforcement Learning . '' https : //arxiv.org/abs/1606.02647 [ 11 ] Nagabandi et al.2017 , `` Learning Image-Conditioned Dynamics Models for Control of Under-actuated Legged Millirobots . '' https : //arxiv.org/abs/1711.05253 [ 12 ] Kulkarni et al.2019 , `` Unsupervised Learning of Object Keypoints for Perception and Control . '' https : //arxiv.org/abs/1906.11883 [ 13 ] Anand et al.2019 , `` Unsupervised State Representation Learning in Atari . '' https : //arxiv.org/abs/1906.08226"}], "0": {"review_id": "rylJkpEtwS-0", "review_text": "The paper draws on a wide range of ideas, and proposes novel perspectives on how these ideas might apply in RL. In particular, the concept of reachability, reversability and dissipation are explored, with respect to properties of the underlying MDP that can be exploited. I found many of the ideas thought-provoking. The paper is also well written and a pleasure to read. But unfortunately the work falls short of its objective in the experiment section. Not a single baseline is included. I would expect to see comparison to a simple model-based method for all the experiments on p.7. The main result for 7x7 2D world seems to be that the agent has learned to quantify irreversibility. I would expect a simple statistical estimator over state transitions (using the same samples as the h-potential method) to be able to capture this as well. The main result for Sokoban seems to be that the h-potential has detected side-effects of actions; again, why can\u2019t a model estimator learn this? Similarly in Mountain car, it seems possible to directly estimate the terrain from the data, without the h-potential. As a more minor concern, the fact that the method uses a batch of uniformly random state transitions (as per Sec.4), rather than randomly sampled trajectories is a definite concerned with respect to real-world application. Minor comments: - Top of p.3: Can you give some intuition for h(), e.g. relation to entropy over trajectory. - Bottom of p.3: you use a random policy to sample trajectories. Is this simple to implement? Can you just sample random actions at each state, or do you need to sample over the space of all trajectories? - Footnote 2, p.4. It would be interesting to expand on this point. ============== Update post-rebuttal: Indeed, my concern is that a simple T(s,a,s') estimator, using the same samples, could infer the same characteristics in each of the experiments (number of broken vases; effect of box pushing; terrain), and could then be used for model-based planning. But I do appreciate the insights provided by connecting these through the broader notion of arrow of time developed in this paper, and its connection to reachability and safety. Therefore I am raising my score to weak accept.", "rating": "6: Weak Accept", "reply_text": "Thank you for your review - we are glad that you enjoyed reading our paper and find many of our ideas thought provoking . > I would expect to see comparison to a simple model-based method [ ... ] To recapitulate , we propose a method to quantify irreversibility in MDPs without having to learn the full dynamics model of the said MDP . To that end , we obtain a function $ \\eta $ of two states $ s , s ' $ such that $ \\eta ( s , s ' ) = h ( s ' ) - h ( s ) $ is a scalar that quantifies the reversibility of the transition $ s \\to s ' $ ( cf.Sec 3.1 ) .In contrast , a forward model is typically a function of a state $ s $ and an action that outputs ( a distribution over ) the next state $ s ' $ , i.e. $ p ( s ' | s , a ) $ . While we do not see a straightforward way of extracting a baseline measure of reversibility from the environment model , we would be very receptive to your suggestions . It is also worth noting that $ \\eta ( s , s ' ) $ has the functional constraint that it must be expressible as the difference $ h ( s ' ) - h ( s ) $ of h-potentials ( akin to a siamese neural network ) . This constraint functions as an inductive bias and endows it with the ability to generalise beyond the transitions that are observed ( cf.Sec 3.1 , especially p. 5 `` Third , $ \\eta $ allows for a soft measure of reachability [ ... ] '' ) . In general , this can not be expected of an environment model . > [ ... ] for 7x7 2D world [ ... ] I would expect a simple statistical estimator over state transitions ( using the same samples as the h-potential method ) to be able to capture this as well . It would certainly be possible to hand-craft an estimator that captures irreversibility of state transitions ; this might be a challenging endeavor ( or not ) , depending on the environment . However , our goal is to _learn_ a model that can capture irreversibility . The experiment on 2D World with Vases ( Fig 2 and App C.1.1 ) therefore serves the following purposes . ( 1 ) It reassures us and our audience that the h-potential indeed learns to count the number of broken vases ( which is precisely what one would consider when designing a statistical estimator ) . ( 2 ) It serves as a sandbox to understand the strengths and limitations of our method -- for instance , we find that our method is fairly robust against random uncorrelated noise , but can be distracted by time-correlated noise . ( 3 ) It helps expose the the expected trade-off between safety and efficiency ( Fig 12 ) -- while the baseline DDQN agent trained without the h-potential out-performs the safe agent trained with the h-potential as far as the probability of reaching the goal is concerned , the baseline agent breaks a larger number of vases ( i.e.is less safe ) than the safe agent to acheive its goal . > Sokoban seems to be that the h-potential has detected side-effects of actions ; again , why can \u2019 t a model estimator learn this ? We would appreciate it if you could clarify what `` learning a model estimator '' might mean in this context . Please correct us if we are wrong , but we interpret your question as meaning either `` Why not learn a model to directly predict side-effects ? '' or `` Why not directly predict whether a transition is irreversible ? `` . For the former : we often do not have access to ground-truth labels for when a transition induces side-effects . But if we did , it might also make sense to use it directly . For the latter : our method is indeed based on predicting whether a transition is irreversible , but with a crucial inductive bias : the corresponding predictor must be a difference of two functions ( akin to a siamese neural network ) . Please refer to p. 4 ( `` Instead , our proposal is to derive [ ... ] '' et seq . ) for a discussion . > As a more minor concern , the fact that the method uses a batch of uniformly random state transitions ( as per Sec.4 ) , rather than randomly sampled trajectories is a definite concerned with respect to real-world application . This might be a misunderstanding -- by uniform random policy , we mean that the actions are sampled randomly , with no action preferred over the other . This is straightforward to implement for the environments we consider . We will clarify this in the next update . ( Continued in Part 2 . )"}, "1": {"review_id": "rylJkpEtwS-1", "review_text": "This work proposes the h-potential, which is a solution to an objective that measures state-transition asymmetry in an MDP. Roughly speaking, in many situations some state transitions (s-->s\u2019) are more probable than their converse (s\u2019-->s), and if we have a function that assigns a higher value to a more probable transition (compared to its converse), then we can use it as a measure of the \u201creversibility\u201d of that transition. This function can then be used, for example, as an intrinsic reward signal; indeed, there may be cases where state transitions should be avoided if they are not reversible. The authors tie these ideas into the notion of the arrow of time. They go on to explore the various nuances and subtleties of this measure, and demonstrate its behaviour empirically on a number of environments. This paper was an absolute pleasure to read. The prose was clear, interesting, and nuanced. The authors anticipate many questions and do well to explain the various subtleties of their method. Overall the experiments are a nice demonstration of the presented ideas. I am inclined to give this paper a high rating, as there was very little that I felt was \u201cwrong\u201d or inaccurate. But I must also admit that some of the theoretical components are beyond my expertise, and so I can only be moderately confident. I will defer to other reviewers and any online discussion on these more technical matters. I have a few questions that I hope the authors can address. 1) The method depends on a random policy (or, more accurately, was empirically validated mainly using a random policy, aside from some very simple environments as far as I can see). Can the authors comment on the usefulness of this method for a non-random policy in more complicated environments? Do they have any experimental results showing the effect of incorporating this into an agent also receiving (and learning from) exogenous rewards in an environment such as Sokoban (as it\u2019s used here already) or Atari? 2) Related to the first point, how does this method scale to environments whose state-space can only be sparsely covered with a random policy? It seems in this case a task-relevant policy would be needed to explore more of the state-space, which would place pressure on the h-potential function approximator as it has to learn with sequentially correlated inputs. You can imagine something like an experience replay buffer being needed, but in any case, there are definitely unique challenges here not explored in the paper. 3) Can the authors comment on the notion of a function being statistically monotonic vs. deterministically so, and whether the arrow of time is classically considered the former or the latter? My reasoning for this question comes from a place where I\u2019m questioning whether the motivation of the work can be simplified. The appeal to the arrow of time is nice and reads well, but it\u2019s also the case that this work can simply be interpreted as \u201clearned state transition reversibility\u201d, with the links to the arrow of time being more of a point of discussion. ", "rating": "8: Accept", "reply_text": "Thank you for your positive review -- we are elated that you enjoyed reading our work ! > I have a few questions that I hope the authors can address . Those are all good questions . Regarding ( 1 ) and ( 2 ) : the issue of non-random policy for gathering trajectories is perhaps the most important issue we do not address in this work , for doing it full justice would detract significantly from the original goal of the paper . Nevertheless , one approach that we see being fruitful is that of off-policy learning [ 1 ] , which applies to cases where the behaviour policy used to gather trajectories differs from the evaluation policy of interest . In our case , the behaviour policy could be any combination of an exploratory policy and a learning agent , whereas the evaluation policy is random ( in order to not bias the h-potential ) . Importance sampling is one example of this scheme , but more sophisticated methods exist . As such , your assessment that `` there are unique challenges here not explored in the paper '' is spot on . Moreover , it might also be worthwhile to remark that the adoption of random rollouts is rather widespread in model-based ( related ) literature [ 3-8 ] . Regarding question ( 3 ) : Consider a stochastic process $ X_t $ ( i.e.a ( time- ) series of random variables ) . We say that a ( deterministic ) function $ h $ is statistically monotonic increasing if the function $ H ( t ) = \\mathbb { E } [ h ( X_t ) | h ( X_ { t - 1 } ) , ... , h ( X_1 ) ] $ is ( deterministically ) monotonic increasing with t. In other words , only the expectation of $ h $ ( w.r.t.its argument random variable ) must increase with time ( but not $ h $ itself ) . In technical jargon , $ h ( X_t ) $ is sometimes called a submartingale [ 2 ] . All that said , we agree with your interpretation of our work as `` learning the state transition reversibility '' , but with a crucial detail -- namely that the learner has a specific architecture resembling a siamese network , i.e.its output is a difference of a function applied to its two inputs . This function turns out to be the h-potential : cf . p. 4 of our manuscript : `` Instead , our proposal is to derive [ ... ] '' ( et seq . ) for a discussion about what we may gain by using this specific architecture . We hope to have answered your questions -- please let us know if not ! [ 1 ] Munos et al.2016 , `` Safe and Efficient Off-Policy Reinforcement Learning . '' https : //arxiv.org/abs/1606.02647 [ 2 ] https : //en.wikipedia.org/wiki/Martingale_ ( probability_theory ) # Submartingales , _supermartingales , _and_relationship_to_harmonic_functions [ 3 ] Savinov et al.2018 , `` Semi-parametric Topological Memory for Navigation . '' https : //arxiv.org/abs/1803.00653 [ 4 ] Ha & Schmidhuber 2018 , `` World Models . '' https : //arxiv.org/abs/1803.10122 [ 5 ] Savinov et al.2018 , `` Episodic Curiosity Through Reachability . '' https : //arxiv.org/abs/1810.02274 [ 6 ] Nagabandi et al.2017 , `` Learning Image-Conditioned Dynamics Models for Control of Under-actuated Legged Millirobots . '' https : //arxiv.org/abs/1711.05253 [ 7 ] Kulkarni et al.2019 , `` Unsupervised Learning of Object Keypoints for Perception and Control . '' https : //arxiv.org/abs/1906.11883 [ 8 ] Anand et al.2019 , `` Unsupervised State Representation Learning in Atari . '' https : //arxiv.org/abs/1906.08226"}, "2": {"review_id": "rylJkpEtwS-2", "review_text": "This paper proposes that we learn the \u201carrow of time\u201d for an MDP: that is, a function (called the h-potential) that tends to increase as the MDP steps forward. Such an arrow should automatically capture notions such as irreversibility, and so can be used to define a measure of reachability, which previous work has shown can be used to penalize the agent for causing negative side effects. In addition, it can be used as intrinsic motivation for the agent: in particular, the agent can be rewarded for trajectories that decrease the h-potential (i.e. are \u201clike\u201d going backwards in time, or reducing entropy), which is hard to do and should lead to interesting skills. They propose that we learn the arrow of time by optimizing a function to grow over time along trajectories take from a random policy. Experiments demonstrate that in simple environments the learned function has the properties we would expect it to given results from physics. I am conflicted on this paper. I like the novelty of the suggestion; it is not something I would have expected to see in an ML paper. The discussion and experiments have convinced me that the idea is worth investigating: they show that the learned arrow of time approximately satisfies the properties we would expect, and demonstrate their two use cases in two simple environments: the intrinsic reward is used in a tomato-watering environment, while the side effect avoidance is shown in Sokoban (though the experiment only shows that the h-potential increases with irreversible actions -- it doesn\u2019t actually use the h-potential to create an agent that reliably avoids irreversibly pushing boxes into corners). However, it\u2019s not clear to me whether or not the method would scale to more complex environments, the current experiments are more like demonstrations (there are no baselines), and the paper is hard to understand without a background in physics. Overall, given the novelty of the suggestion, I lean towards a weak accept. ---- In your objective, you use an expectation over the timestep in the trajectory. Why not instead take an average over all the timesteps in the trajectory? This should be equivalent. (If trajectories are all the same length, you could also take a sum over the timesteps.) Similarly, in the algorithm, why do you sample from the dataset? It would likely be better to randomly shuffle the dataset (at the timestep level) once, and then iterate through the dataset computing gradient updates. (This is standard practice in supervised learning.) ---- When scaling the algorithm up to larger environments, you will likely run into the problem that uniform policies are often very bad at exploring the state space. Consider for example the Overcooked environment (https://bair.berkeley.edu/blog/2019/10/21/coordination/ ). The environment is not dissipative, so you\u2019d expect the h-potential to be zero. However, uniform policies are extremely unlikely to ever deliver a soup (which requires several hierarchical actions), but sometimes will pick up an onion. If you don\u2019t know about soup delivery, then picking up an onion looks irreversible, because there\u2019s no way to get rid of it, and the h-potential will rise. I think it would significantly improve this paper to demonstrate a solution to this problem. One possibility is to redefine the arrow of time to be with respect to some distribution over states: E_{s ~ p(s)} E_{a ~ Uniform(A)} E_{s\u2019 ~ p(s\u2019 | s, a)} [ h(s\u2019) - h(s) | s ]. Initially, your distribution over states can be the initial state distribution. But then you can use the learned arrow of time as an intrinsic reward to find a policy that finds \u201cinteresting states\u201d. In Overcooked, we would hope that this policy learns to pick up onions. Then, your new distribution over states can be the states reached in 0-20 timesteps when following the new policy, that is, you collect your dataset by running the \u201cinteresting\u201d policy for some time, and then switch back to the uniformly random policy, and only use the states / actions collected during the uniform random policy as part of your dataset. Hopefully, this would include states where the onion is placed in a pot, and the h-potential would learn that placing an onion in a pot is \u201cirreversible\u201d. Then, another round of using the h-potential would lead to a policy that places onions in pots. Collecting data could then discover delivering soups, and so on. ---- In addition to experiments on larger environments, I would like to see better experiments for the two intended use cases. For example, can you use the learned arrow of time to solve the environments in (Krakovna et al), and how does it compare to relative reachability and its many variants? Similarly, how does your intrinsic reward compare to existing exploration methods (of which there are many, but consider count-based methods, curiosity (Pathak et al), random network distillation (Burda et al))? ---- (This section did not affect my assessment of the paper) Have you considered finding theoretically what it means to take the difference between the h-potential of two states (i.e. your reachability measure)? I could imagine that the answer is something like \u201creachability(s, s\u2019) is proportional to the log probability of reaching s\u2019 from s when acting according to a uniformly random policy\u201d. Perhaps this has to be normalized against the log probability of reaching other states from s. This would be very interesting as a potential definition of reachability. ---- There is a lot of jargon from physics that will not be familiar to the typical audience at ICLR (e.g. Hamiltonian, Liouville\u2019s theorem, Maxwell\u2019s demon, free energy functionals, etc.) I would recommend improving the clarity of the paper on this axis. There\u2019s no particular need to name Maxwell\u2019s demon prominently -- the exposition is sufficient by itself, perhaps Maxwell\u2019s demon can be mentioned in a footnote. Consider adding a discussion of ergodicity (as applied to Markov chains / MDPs), which will be more familiar to the audience and serves a similar purpose as the discussion on Hamiltonian systems. Perhaps move the experiment with the free energy functional to the appendix, and move the experimental details for the other experiments from the appendix to the main paper.", "rating": "6: Weak Accept", "reply_text": "Thank you for the time you 've invested in reviewing our work -- we are glad that you find our suggestion novel , and are very grateful for your suggestions . The following is a first response to your comments ; our manuscript will be updated in the coming days to reflect your suggestions and we will notify you once the changes are in place . > the paper is hard to understand without a background in physics [ ... ] There is a lot of jargon from physics that will not be familiar to the typical audience at ICLR In hindsight , we agree with your assessment that some aspects of the paper might be difficult to understand without a background in physics , and part of this indeed has to do with the jargon . We will try to address this where possible . That said , we view our work as an attempt to view problems in RL from the lens of ideas and concepts that are already well understood in statistical physics . For instance , the connections between Variational Fokker-Planck and Reinforcement Learning is less explored ( the only relevant reference we found was [ 1 ] ) . Likewise , the notion of dissipativity has received little attention in modern reinforcement learning ( but has been well studied in the context of dynamical systems ) . We hope that our empirical results ( especially `` Comparison with the Free-Energy Functional '' , p. 8 and `` [ ... ] the Importance of Dissipativity '' , p. 7 ) inspires more theoretical research in this direction . Indeed , the fields of unsupervised learning and deep learning theory has greatly benifited from such interdisciplinary endeavors ( e.g . [ 2 ] , Energy Based Models , [ 3 ] , and more ) , and we wish the same for reinforcement learning . > In your objective , you use an expectation over the timestep in the trajectory . Why not instead take an average over all the timesteps in the trajectory ? This should be equivalent . They are indeed equivalent ( and a matter of notation ) . > Similarly , in the algorithm , why do you sample from the dataset ? It would likely be better to randomly shuffle the dataset ( at the timestep level ) once , and then iterate through the dataset computing gradient updates . It 's certainly possible to partition the training in epochs , as you suggest . We will investigate if this leads to gains in performance . > When scaling the algorithm up to larger environments , you will likely run into the problem that uniform policies are often very bad at exploring the state space . The issue you mention is exceedingly important , and doing it justice will require us to deviate significantly from the primary objective of this work . We hope to have been upfront about it ( cf.top of p. 4 : `` The price we pay is the lack of adequate exploration in complex enough environments [ ... ] '' and footnote 3 ) and intend to pursue this in future work . We think one promising approach could be based on off-policy methods [ 10 ] , which applies to cases where the behaviour policy differs from the ( evaluation ) policy of interest ( importance sampling is one simple example ) . In our case , the evaluation policy can still be random , whereas the behaviour policy is exploratory . Moreover , it is worth noting that the choice of an offline `` base policy '' is a recurring theme concerning model-based ( and related ) methods [ 9 ] , and a random policy is a widespread choice [ 4-6 , 11-13 ] . Some works use a mixture of a pre-trained and a random policy [ 7 , 8 ] , whereas some other works [ 5 ] attempt to expose and tackle the issue explicitly . ( Continued in next post . ) [ 1 ] Richemond & Maginnis 2017 , `` On Wasserstein Reinforcement Learning and the Fokker Planck equation . '' https : //arxiv.org/abs/1712.07185 [ 2 ] Sohl-Dickstein et al.2015 , `` Deep Unsupervised Learning using Nonequilibrium Thermodynamics . '' https : //arxiv.org/abs/1503.03585 [ 3 ] Poole et al.2016 , `` Exponential expressivity in deep neural networks through transient chaos . '' https : //arxiv.org/abs/1606.05340 [ 4 ] Savinov et al.2018 , `` Semi-parametric Topological Memory for Navigation . '' https : //arxiv.org/abs/1803.00653 [ 5 ] Ha & Schmidhuber 2018 , `` World Models . '' https : //arxiv.org/abs/1803.10122 [ 6 ] Savinov et al.2018 , `` Episodic Curiosity Through Reachability . '' https : //arxiv.org/abs/1810.02274 [ 7 ] Oh et al.2015 , `` Action-Conditional Video Prediction using Deep Networks in Atari Games . '' https : //arxiv.org/pdf/1507.08750.pdf [ 8 ] Chiappa et al.2017 , `` Recurrent Environment Simulators . '' https : //arxiv.org/abs/1704.02254 [ 9 ] http : //rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_9_model_based_rl.pdf [ 10 ] Munos et al.2016 , `` Safe and Efficient Off-Policy Reinforcement Learning . '' https : //arxiv.org/abs/1606.02647 [ 11 ] Nagabandi et al.2017 , `` Learning Image-Conditioned Dynamics Models for Control of Under-actuated Legged Millirobots . '' https : //arxiv.org/abs/1711.05253 [ 12 ] Kulkarni et al.2019 , `` Unsupervised Learning of Object Keypoints for Perception and Control . '' https : //arxiv.org/abs/1906.11883 [ 13 ] Anand et al.2019 , `` Unsupervised State Representation Learning in Atari . '' https : //arxiv.org/abs/1906.08226"}}