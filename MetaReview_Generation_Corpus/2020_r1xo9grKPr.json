{"year": "2020", "forum": "r1xo9grKPr", "title": "Flexible and Efficient Long-Range Planning Through Curious Exploration", "decision": "Reject", "meta_review": "The authors consider planning problems with sparse rewards.                                                                        \nThey propose an algorithm that performs planning based on an auxiliary reward                                                      \ngiven by a curiosity score.                                                                                                        \nThey test they approach on a range of tasks in simulated robotics environments                                                     \nand compare to model-free baselines.                                                                                               \n                                                                                                                                   \nThe reviewers mainly criticize the lack of competitive baselines; it comes as now                                                  \nsurprise that the baselines presented in the paper do not perform well, as they                                                    \nmake use of strictly less information of the problem.                                                                              \nThe authors were very active in the rebuttal period, however eventually did not                                                    \nfully manage to address the points raised by the reviewers.                                                                        \n                                                                                                                                   \nAlthough the paper proposes an interesting approach, I think this paper is below                                                   \nacceptance threshold.                                                                                                              \nThe experimental results lack baselines,                                                                                           \nFurthermore, critical details of the algorithm are missing / hard to find.", "reviews": [{"review_id": "r1xo9grKPr-0", "review_text": "===== Summary ===== The paper introduces Curious Sample Planner (CSP) a long-horizon motion planning method that combines task and motion planning with deep reinforcement learning in order to solve simulated robotic tasks with sparse rewards. The CSP algorithm considers two different hierarchies of actions: primitive actions, which control the rotation of several joints in a robotic arm, and macro-actions corresponding to complex behaviours such as moving from one position to another or linking two objects together. Macro-actions are selected using the actor-critic architecture PPO and then turned into primitive actions using geometric motion planning and inverse kinematics. Specifically, RRT-Connect is used for motion planning with the recursive Newton-Euler algorithm for inverse kinematics on a perfect model of the environment to determine the specific sequence of primitive actions necessary to execute the macro-action. As CSP is interacting with the environment, it also builds a tree of states in the environment connected by the macro-actions leading to each of them. Each vertex of the tree is assigned a curiosity score, which is used as an exploration bonus for PPO and to determine the probability with which each vertex is sampled from the tree for future exploration. The whole process is repeated until a feasible path from the initial state to the goal state is found. The paper provides empirical evaluations in four different tasks where it compares the performance of CSP with three different curiosity measures to the performance of PPO and A2C. The results show that CSP accomplishes each task while using significantly less samples. Moreover, a second set of experiments is presented that show the potential for transfer learning across tasks using CSP. Contributions: 1. The paper introduces CSP, a successful combination of task and motion planning and deep reinforcement learning that can discover temporally extended plans. 2. The paper demonstrates a statistically significant improvement in performance over PPO and A2C in the four robotic tasks that the paper studies. 3. The paper shows evidence that CSP might facilitate transfer learning across similar tasks. ===== Decision ===== The paper represents a significant contribution to the reinforcement learning and task and motion planning literature. The main algorithm is well motivated from previous literature and demonstrate a significant improvement over previously proposed deep reinforcement learning methods. Moreover, the ideas are presented clearly and logically throughout the paper and the empirical evaluations clearly support the claims about the performance of CSP. However, I have concerns about the reproducibility of the results because of the little amount of details provided about the hyper-parameter selection and settings and about the network architectures and loss functions. Thus, I consider that the paper should be rejected, but I am willing to increase my score if my comments are properly addressed. ===== Questions and Comments ===== 1. Although the ideas in the paper are presented clearly, the algorithms and methods are presented mostly at a very high level. There are no details about the losses used in lines 11 and 12 of Algorithm 1 and there are no specifications about the network architectures and the hyperparameter settings and selection for each algorithm. This raises two concerns. First, this hinders reproducibility and future work by other authors that might be interested in building upon the ideas presented in the paper; this would also decrease the impact of the paper. Two, it is difficult to determine if the comparisons against A2C and PPO were fair without any information about the hyper-parameter selection. Thus, I consider these details should be included and I would consider increasing my score to accept if this was properly addressed. Specifically, I think the paper should include this: - The hyper-parameter settings for each different algorithm and an explanation about how they were selected. - A detailed description of the network architectures used in the experiments. - A definition for the loss functions used for the policy network, the value network, and the curiosity module. 2. As mentioned in the Decision section above, the paper clearly demonstrates an improvement over previously proposed deep reinforcement learning algorithm. However, there are no comparisons to any previously proposed Task and Motion Planning Methods. What motivated this decision? 3. An alternative to using separate networks for the policy and the value function, it could be possible to use a two-headed network with one head for the policy and another one for the value. What was the reason for using two separate networks over this alternative? 4. Were any other alternatives tested for the activation functions of the network? 5. CSP was tested with three different curiosity and novelty metrics, none of which dominated over all the other ones. However, PPO was only tested with one of the measures and A2C had no curiosity measure added to it. Where there any preliminary results that justified this decision? In terms of computation and time, how difficult would it be to include this in the paper? 6. The paper already hinted at this, but macro-actions could be framed within the option framework from Sutton, Precup, & Singh (1999). This would open up the opportunity to apply some of the already proposed methods for option discovery such as the option-critic architecture from Bacon, Harb, & Precup (2016) or the Laplacian framework for option discovery from Machado, Bellemare, & Bowling (2017), which is cited on the paper. Could the authors provide more comments about this line of future work? ===== References ===== Bacon, P., Harb, J., & Precup, D. (2016). The Option-Critic Architecture. Retrieved 17 October 2019, from https://arxiv.org/abs/1609.05140 Marlos C. Machado, Marc G. Bellemare, and Michael H. Bowling. A laplacian framework for option discovery in reinforcement learning. CoRR, abs/1703.00956, 2017. URL http://arxiv.org/abs/1703.00956. Sutton, R., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1-2), 181-211. doi: 10.1016/s0004-3702(99)00052-1", "rating": "3: Weak Reject", "reply_text": "Hi Reviewer 3 ! Thanks also for the constructive comments . See below for inline responses to each issue / question . `` The paper represents a significant contribution ... '' Thanks ! `` However , I have concerns about the reproducibility of the results ... but I am willing to increase my score if my comments are properly addressed . '' Got it.We were n't that sure how much detail to put into the paper , but we see this is a real area for improvement . ( Rev 1 also is concerned about this issue . ) As we explained to Rev . 1 , our plan for addressing this is as follows : 1 . Revise the main text of the paper to clarify several main important issues . 2.Create a detailed additional supplementary document . 3.Post an anonymous public github repo to which we will commit all the project code . Our goal is to have this ready for your review by 11/11 or 11/12 . == > Question : does this plan work for you ? Other suggestions welcomed ! `` ... the paper should include [ LIST OF SPECIFICS ] '' Happy to add these things . Some we 'll put into the main paper and some into the supplement . When done we 'll write a comment pointing you to the edits . `` ... There are no comparisons to any previously proposed Task and Motion Planning Methods . What motivated this decision ? '' Great question . Will address in a separate post due to character limitation . `` ... An alternative to using separate networks for the policy and the value function , it could be possible to use a two-headed network with one head for the policy and another one for the value . What was the reason for using two separate networks over this alternative ? '' That is indeed an alternative to using separate networks for the policy and value function . The short answer is that we adopted our architecture from a widely used reinforcement learning codebase . https : //github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c_ppo_acktr/model.py # L208 `` ... Were any other alternatives tested for the activation functions of the network ? '' No , we have n't done this . Do you think this is very important for us to try ? It 's sort of not been a major area of exploration in the literature we 've looked at , so we have the vague impression that activation function variants are n't too likely to make a huge different . But we 'd be happy to try something specific out if you point us to it , it would be great to see if there could be improvements made in such a simple way . `` CSP was tested with three different curiosity and novelty metrics , none of which dominated over all the other ones . However , PPO was only tested with one of the measures and A2C had no curiosity measure added to it . Where there any preliminary results that justified this decision ? In terms of computation and time , how difficult would it be to include this in the paper ? '' Fair point . The reason for what we did is this : once we found PPO to be better than A2C in our context and that RND was if not always the best , at least overall the best curiosity metric , we figured that PPO+RND was thus the overall strongest control of this type . And that very likely the other combinations would just be less powerful . However , it is easy to include several other combinations , and we will be happy to do this in the revised paper . `` The paper already hinted at this , but macro-actions could be framed within the option framework from Sutton , Precup , & Singh ( 1999 ) . .... Could the authors provide more comments about this line of future work ? '' Yeah , this definitely seems like a natural direction for further improvement . The key goal is really to get rid of having to build in macro actions . It seems like CSP might be a natural way to do that , and plugging it into the options framework is one clear way forward toward that . The overall concept of hierarchical planning is something would generally be relevant for us to try to connect to , and it seems natural to think that a hierarchical , and possibly curricularized , version of CSP could be substantially more powerful that the current `` single-level '' version ."}, {"review_id": "r1xo9grKPr-1", "review_text": "The idea of the paper is to augment a planner with a curiosity module to reduce the number of traversed paths, resulting in a speedup. The paper presents experiments where it is shown that deep RL methods are outperformed in that question. I recommend to reject the paper. The reason for this are threefold. - The paper is crowded with text and ends up to be hard to follow. The actual contribution is hard to distill. - The method compares to deep RL baselines. But these are *not* planning algorithms, instead these are RL methods. The paper does not compare to planners, which are tailored to solve the problem the paper adresses. - The reader is left alone to place the work within the literature. The abstract and introduction do not have a single cite; terms like TAMP and multi-step planning are mentioned and certain properties of them are stated without resorting to where a reader could look those up. It is not the readers task to use a search engine to reverse engineer the authors writing! I think that the authors need to reformulate what the contribution of their work is. That needs to be presented in a more abstract way, without focusing on the experimental setting prematurely. The experimental setup is ok if the method is restricted to robotic tasks, but too thin for the general setting of efficient planning with sparse costs. Planning algorithms that are tailored towards huge spaces shou;d be used as base lines instead of deep RL methods.", "rating": "1: Reject", "reply_text": "Hi Reviewer 2 ! We will get into details of replying to all your comments very soon . But , one thing we \u2019 re hoping to get your thoughts on as soon as possible is about the comment in the last paragraph of your review . You say : \u201c Planning algorithms that are tailored towards huge spaces should be used as base lines instead of deep RL methods. \u201d Could you make a suggestion of what specific comparisons you think we should be using as baselines ? Here \u2019 s the main cause of uncertainty we have in addressing your comment : Are you asking us to compare to standard TAMP solutions ? There \u2019 s a fundamental reason we didn \u2019 t do this . We didn \u2019 t end up making any comparisons with TAMP algorithms since all the ones we know about require the user to pre-define macro-action effects , preconditions , and predicates . This makes them quite challenging to apply in the context of the highly flexible setup we are working in . That 's because to figure out the effects and preconditions in a way that can be characterized with formal logic ( a basic requirement of TAMP ) , you kind of have to do it laboriously and specifically for each new robotic setup and macro-action set , and tailor it to the specific objects / structures in the environment . This lack of flexibility in TAMP is the whole raison d \u2019 etre of our paper . ( That \u2019 s why we call it \u201c flexible \u201d . ) If it were easily possible to apply TAMP directly as a baseline comparison , that would have obviated the point of our work . It was essentially that we couldn \u2019 t really flexibly do this that inspired our work . We \u2019 re definitely open to suggestions about how to make this direct comparison , but just not sure how to make sense of it at the moment , given the limitations of existing TAMP solutions that we know about . Maybe part of the problem here is that we were n't successful in communicating to you our main contribution . The main technical problem motivating our work is that standard TAMP ( and other planning ) solutions are efficient , but hard to apply easily in a flexible context , because they typically require the pre-specification of conditions and actions . You can view our main contribution as using a specific targeted approach to intrinsically-motived learning to find an efficient but still flexible route around this problem . Our work shows how one can * still * benefit from the advantages of ideas from planning to achieve efficiency ( in a way that standard Deep RL does not ) . An alternative possibility : are you referring to other planning solutions like RRT * or KPIECE that do n't require the pre-definition of action effects and preconditions ? There are many motion planning algorithms like these , but these algorithms have been shown to utterly fail in real-world environments which have many complex differential constraints . ( This is why most TAMP papers do n't even compare to pure motion planning anymore.So we did n't think rehashing this well-known point was super-helpful . ) Bottom line though is that we \u2019 d be very happy try something out as a new baseline if you make a specific suggestion , especially if the system you suggest has available code we could run in our setting -- but we figure we \u2019 d better ask right now to make sure we have the time to do this . Or if what you 're saying is we need to explain better the thoughts above in the paper , we 'd be happy to do that . What do you think ?"}, {"review_id": "r1xo9grKPr-2", "review_text": "This paper tackles the problem of enabling robots to learn long-horizon, sparse-reward tasks. The proposed approach, the Curious Sample Planner (CSP), builds on insights in task and motion planning (TAMP), which is a standard approach for tackling these kinds of tasks. TAMP constructs a plan in the space of macro-actions (e.g., move object 1 to location (x,y)), and uses a motion planner to execute each macro-action. However, TAMP typically requires being able to describe macro-action effects and preconditions with logical predicates, which can be impossible in real-world environments, due to complex dynamics and interactions. CSP overcomes this limitation by planning in the space of macro-actions in a way that is biased toward novelty. The core approach of CSP is to train a (macro-)action selection network to generate macro-actions that are both feasible and novel. The curiosity module is used in two ways: (1) to give reward to the action selection network for producing novel macro-actions, and (2) to expand states that are considered novel. Three state-of-the-art ways of computing novelty are compared -- state estimation (SE), forward dynamics (FD), and random network distillation (RND). CSP is evaluated on a suite of simulated robotics tasks that require the robot to build simple machines from the objects in its environment, in order to achieve the specified objective. The experiments compare agents trained with CSP versus with deep RL (specifically, A2C, PPO, and PPO + RND). There is an ablation study, that compares against planning with uniform selection of macro-actions and uniform selection of states to expand. Overall, this paper makes a significant contribution to improving robot learning of long-horizon, sparse reward tasks. The paper is clearly written and well-motivated, and the evaluations are thorough. The major downside is that CSP inherently requires knowing the dynamics of the environment (i.e., having a simulator), which means it cannot be directly run applied to real-world robotic systems. But it is a step in the right direction, and clearly outperforms vanilla deep RL. I'm leaning toward accept, but I have a few concerns / questions about the paper. First, there are not enough details included for reproducibility (see list below). With regard to evaluation, I think another ablation should be run, where the action selection network is trained for only feasibility. This would be similar to CSP-No Curiosity, but with a reward of 0 for infeasible actions, and a small fixed positive reward for feasible ones. This would more clearly answer the question of how much it matters to include the curiosity module. Finally, I'm not convinced that this approach works well for transfer, and the evaluations seem inconclusive as well. I'm surprised that even for inter-task transfer, agents trained with action selection transfer and full transfer don't just learn to solve the task immediately. Am I missing something here about how the task is instantiated? Reproducibility questions: - In Algorithm 1, what are the inputs to the novelty metric that is used to compute L_\\phi? Is it the batch of next states, S'? - What is the form of the output of the action-selection network? And what exactly is the space of macro-actions? For instance, the number of possible RemoveConstraint macro-actions depends on how many objects are connected in the environment. But the dimension of the action-selection network's output must be fixed. - What does the state vector input for FD and RND contain? Along these lines, why not also use image inputs for FD and RND, as is done for SE? - What are the learning hyperparamters used to train the networks? (e.g., learning rate) - How many perspectives (i.e., n_p) are used for the SE curiosity module? Minor comments / typos: - Avoid using the same variable with different meanings, e.g. using \\phi to indicate both the curiosity module and the parameters of the value network. - Page 3: \"flexible\", \"a flexible\" - Page 5: \"learnabe\" --> \"learnable\" - Page 8: \"in which\" --> \"in which the\" - Page 9: \"illustrate\" --> \"illustrated\"", "rating": "6: Weak Accept", "reply_text": "Hi Reviewer 1 ! Thanks for your constructive feedback . See below for inline replies to each major item . ... `` Overall , this paper makes a significant contribution to improving robot learning of long-horizon , sparse reward tasks . The paper is clearly written and well-motivated , and the evaluations are thorough .... It is a step in the right direction , and clearly outperforms vanilla deep RL . '' Thanks ! `` ... The major downside is that CSP inherently requires knowing the dynamics of the environment ( i.e. , having a simulator ) , which means it can not be directly run applied to real-world robotic systems . '' Absolutely . As you probably realized , in this world we 've taken a step-by-step strategy : first try to get the planning working * assuming * there is good forward predictor , so that in the next phase of the work we can relax that condition . Our next major step is to apply the method in the context of a non-deterministic learned forward predictor ( and in fact we hope to show that having a good planning algorithm in place makes the learning of the forward predictor substantially more efficient ) . As you note , doing this is really important for actually being able to apply to our work in the real world . `` ... First , there are not enough details included for reproducibility ( see list below ) . '' Yes , both you and Reviewer 3 had this concern , and its totally reasonable . We 'll reply to the details of your concerns below , but here are the three main high-level things we 're going to do in response : 1 . Revise the main text of the paper clarifying the several main important issues for which you and Rev 3 have asked . 2.Create an additional supplementary document with copious detailed information about each aspect of the project . 3.Make a public github account/repo to which we will commit all the project code , so that you ( and others ) can have access to it during ( and after ) this review process . We should have done this before but somehow got worried about breaking the double-blindness of the review . However , upon thinking about it , we realize it should be easy enough to create an anonymous Github user account to post the code to for review purposes . Our plan is to have this ready for your review by 11/11 or 11/12 . That will give you a few days to ask for further clarifications . == > Question : does this plan work for you ? Do you think it 's enough time to address the issues ? `` ... With regard to evaluation , I think another ablation should be run , where the action selection network is trained for only feasibility . This would be similar to CSP-No Curiosity , but with a reward of 0 for infeasible actions , and a small fixed positive reward for feasible ones . This would more clearly answer the question of how much it matters to include the curiosity module . '' Just a point of clarification , removing the curiosity feedback into the action selection networks is not the same as CSP-No Curiosity . In fact , most of the benefit of curiosity comes from selecting which nodes to add to the search tree and the frequency with which to sample those nodes . This operation is independent of the action-selection networks . A more informative ablation might be to run CSP without feasibility feedback . Comparing this result with CSP-No Curiosity would isolate the contribution of the curiosity feedback signal independent of feasibility . Is this an ablation you would be interested in seeing ? `` ... Finally , I 'm not convinced that this approach works well for transfer , and the evaluations seem inconclusive as well . I 'm surprised that even for inter-task transfer , agents trained with action selection transfer and full transfer do n't just learn to solve the task immediately . Am I missing something here about how the task is instantiated ? '' Transfer is tested on different initial instantiations of the general problem statement . So for example in 3-stack , the blocks are placed in different starting positions . It 's important to note that we are not training a policy to solve these tasks , since a valid and general policy may not be learnable from a single example . The reason for showing between-task transfer was only to show that there is some increase in efficiency from having solved the tasks before . I 'm also curious to know how much the log scaling on the y axis contributed to your interpretation of the results as being inconclusive . ( In some cases the efficient gain was substantial even when the log-scale made it look small . ) `` Reproducibility questions : [ LIST OF SPECIFICS ] '' Ok , we 'll make sure to add information about all these things in either the revised main text or the supplement . Once we post these , we 'll write another comment pointing you to where the answers have been added . `` Minor comments / typos : ... [ LIST OF SPECIFICS ] '' Thanks for catching these . All typos have now been fixed and will be uploaded along with other revisions ."}], "0": {"review_id": "r1xo9grKPr-0", "review_text": "===== Summary ===== The paper introduces Curious Sample Planner (CSP) a long-horizon motion planning method that combines task and motion planning with deep reinforcement learning in order to solve simulated robotic tasks with sparse rewards. The CSP algorithm considers two different hierarchies of actions: primitive actions, which control the rotation of several joints in a robotic arm, and macro-actions corresponding to complex behaviours such as moving from one position to another or linking two objects together. Macro-actions are selected using the actor-critic architecture PPO and then turned into primitive actions using geometric motion planning and inverse kinematics. Specifically, RRT-Connect is used for motion planning with the recursive Newton-Euler algorithm for inverse kinematics on a perfect model of the environment to determine the specific sequence of primitive actions necessary to execute the macro-action. As CSP is interacting with the environment, it also builds a tree of states in the environment connected by the macro-actions leading to each of them. Each vertex of the tree is assigned a curiosity score, which is used as an exploration bonus for PPO and to determine the probability with which each vertex is sampled from the tree for future exploration. The whole process is repeated until a feasible path from the initial state to the goal state is found. The paper provides empirical evaluations in four different tasks where it compares the performance of CSP with three different curiosity measures to the performance of PPO and A2C. The results show that CSP accomplishes each task while using significantly less samples. Moreover, a second set of experiments is presented that show the potential for transfer learning across tasks using CSP. Contributions: 1. The paper introduces CSP, a successful combination of task and motion planning and deep reinforcement learning that can discover temporally extended plans. 2. The paper demonstrates a statistically significant improvement in performance over PPO and A2C in the four robotic tasks that the paper studies. 3. The paper shows evidence that CSP might facilitate transfer learning across similar tasks. ===== Decision ===== The paper represents a significant contribution to the reinforcement learning and task and motion planning literature. The main algorithm is well motivated from previous literature and demonstrate a significant improvement over previously proposed deep reinforcement learning methods. Moreover, the ideas are presented clearly and logically throughout the paper and the empirical evaluations clearly support the claims about the performance of CSP. However, I have concerns about the reproducibility of the results because of the little amount of details provided about the hyper-parameter selection and settings and about the network architectures and loss functions. Thus, I consider that the paper should be rejected, but I am willing to increase my score if my comments are properly addressed. ===== Questions and Comments ===== 1. Although the ideas in the paper are presented clearly, the algorithms and methods are presented mostly at a very high level. There are no details about the losses used in lines 11 and 12 of Algorithm 1 and there are no specifications about the network architectures and the hyperparameter settings and selection for each algorithm. This raises two concerns. First, this hinders reproducibility and future work by other authors that might be interested in building upon the ideas presented in the paper; this would also decrease the impact of the paper. Two, it is difficult to determine if the comparisons against A2C and PPO were fair without any information about the hyper-parameter selection. Thus, I consider these details should be included and I would consider increasing my score to accept if this was properly addressed. Specifically, I think the paper should include this: - The hyper-parameter settings for each different algorithm and an explanation about how they were selected. - A detailed description of the network architectures used in the experiments. - A definition for the loss functions used for the policy network, the value network, and the curiosity module. 2. As mentioned in the Decision section above, the paper clearly demonstrates an improvement over previously proposed deep reinforcement learning algorithm. However, there are no comparisons to any previously proposed Task and Motion Planning Methods. What motivated this decision? 3. An alternative to using separate networks for the policy and the value function, it could be possible to use a two-headed network with one head for the policy and another one for the value. What was the reason for using two separate networks over this alternative? 4. Were any other alternatives tested for the activation functions of the network? 5. CSP was tested with three different curiosity and novelty metrics, none of which dominated over all the other ones. However, PPO was only tested with one of the measures and A2C had no curiosity measure added to it. Where there any preliminary results that justified this decision? In terms of computation and time, how difficult would it be to include this in the paper? 6. The paper already hinted at this, but macro-actions could be framed within the option framework from Sutton, Precup, & Singh (1999). This would open up the opportunity to apply some of the already proposed methods for option discovery such as the option-critic architecture from Bacon, Harb, & Precup (2016) or the Laplacian framework for option discovery from Machado, Bellemare, & Bowling (2017), which is cited on the paper. Could the authors provide more comments about this line of future work? ===== References ===== Bacon, P., Harb, J., & Precup, D. (2016). The Option-Critic Architecture. Retrieved 17 October 2019, from https://arxiv.org/abs/1609.05140 Marlos C. Machado, Marc G. Bellemare, and Michael H. Bowling. A laplacian framework for option discovery in reinforcement learning. CoRR, abs/1703.00956, 2017. URL http://arxiv.org/abs/1703.00956. Sutton, R., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1-2), 181-211. doi: 10.1016/s0004-3702(99)00052-1", "rating": "3: Weak Reject", "reply_text": "Hi Reviewer 3 ! Thanks also for the constructive comments . See below for inline responses to each issue / question . `` The paper represents a significant contribution ... '' Thanks ! `` However , I have concerns about the reproducibility of the results ... but I am willing to increase my score if my comments are properly addressed . '' Got it.We were n't that sure how much detail to put into the paper , but we see this is a real area for improvement . ( Rev 1 also is concerned about this issue . ) As we explained to Rev . 1 , our plan for addressing this is as follows : 1 . Revise the main text of the paper to clarify several main important issues . 2.Create a detailed additional supplementary document . 3.Post an anonymous public github repo to which we will commit all the project code . Our goal is to have this ready for your review by 11/11 or 11/12 . == > Question : does this plan work for you ? Other suggestions welcomed ! `` ... the paper should include [ LIST OF SPECIFICS ] '' Happy to add these things . Some we 'll put into the main paper and some into the supplement . When done we 'll write a comment pointing you to the edits . `` ... There are no comparisons to any previously proposed Task and Motion Planning Methods . What motivated this decision ? '' Great question . Will address in a separate post due to character limitation . `` ... An alternative to using separate networks for the policy and the value function , it could be possible to use a two-headed network with one head for the policy and another one for the value . What was the reason for using two separate networks over this alternative ? '' That is indeed an alternative to using separate networks for the policy and value function . The short answer is that we adopted our architecture from a widely used reinforcement learning codebase . https : //github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c_ppo_acktr/model.py # L208 `` ... Were any other alternatives tested for the activation functions of the network ? '' No , we have n't done this . Do you think this is very important for us to try ? It 's sort of not been a major area of exploration in the literature we 've looked at , so we have the vague impression that activation function variants are n't too likely to make a huge different . But we 'd be happy to try something specific out if you point us to it , it would be great to see if there could be improvements made in such a simple way . `` CSP was tested with three different curiosity and novelty metrics , none of which dominated over all the other ones . However , PPO was only tested with one of the measures and A2C had no curiosity measure added to it . Where there any preliminary results that justified this decision ? In terms of computation and time , how difficult would it be to include this in the paper ? '' Fair point . The reason for what we did is this : once we found PPO to be better than A2C in our context and that RND was if not always the best , at least overall the best curiosity metric , we figured that PPO+RND was thus the overall strongest control of this type . And that very likely the other combinations would just be less powerful . However , it is easy to include several other combinations , and we will be happy to do this in the revised paper . `` The paper already hinted at this , but macro-actions could be framed within the option framework from Sutton , Precup , & Singh ( 1999 ) . .... Could the authors provide more comments about this line of future work ? '' Yeah , this definitely seems like a natural direction for further improvement . The key goal is really to get rid of having to build in macro actions . It seems like CSP might be a natural way to do that , and plugging it into the options framework is one clear way forward toward that . The overall concept of hierarchical planning is something would generally be relevant for us to try to connect to , and it seems natural to think that a hierarchical , and possibly curricularized , version of CSP could be substantially more powerful that the current `` single-level '' version ."}, "1": {"review_id": "r1xo9grKPr-1", "review_text": "The idea of the paper is to augment a planner with a curiosity module to reduce the number of traversed paths, resulting in a speedup. The paper presents experiments where it is shown that deep RL methods are outperformed in that question. I recommend to reject the paper. The reason for this are threefold. - The paper is crowded with text and ends up to be hard to follow. The actual contribution is hard to distill. - The method compares to deep RL baselines. But these are *not* planning algorithms, instead these are RL methods. The paper does not compare to planners, which are tailored to solve the problem the paper adresses. - The reader is left alone to place the work within the literature. The abstract and introduction do not have a single cite; terms like TAMP and multi-step planning are mentioned and certain properties of them are stated without resorting to where a reader could look those up. It is not the readers task to use a search engine to reverse engineer the authors writing! I think that the authors need to reformulate what the contribution of their work is. That needs to be presented in a more abstract way, without focusing on the experimental setting prematurely. The experimental setup is ok if the method is restricted to robotic tasks, but too thin for the general setting of efficient planning with sparse costs. Planning algorithms that are tailored towards huge spaces shou;d be used as base lines instead of deep RL methods.", "rating": "1: Reject", "reply_text": "Hi Reviewer 2 ! We will get into details of replying to all your comments very soon . But , one thing we \u2019 re hoping to get your thoughts on as soon as possible is about the comment in the last paragraph of your review . You say : \u201c Planning algorithms that are tailored towards huge spaces should be used as base lines instead of deep RL methods. \u201d Could you make a suggestion of what specific comparisons you think we should be using as baselines ? Here \u2019 s the main cause of uncertainty we have in addressing your comment : Are you asking us to compare to standard TAMP solutions ? There \u2019 s a fundamental reason we didn \u2019 t do this . We didn \u2019 t end up making any comparisons with TAMP algorithms since all the ones we know about require the user to pre-define macro-action effects , preconditions , and predicates . This makes them quite challenging to apply in the context of the highly flexible setup we are working in . That 's because to figure out the effects and preconditions in a way that can be characterized with formal logic ( a basic requirement of TAMP ) , you kind of have to do it laboriously and specifically for each new robotic setup and macro-action set , and tailor it to the specific objects / structures in the environment . This lack of flexibility in TAMP is the whole raison d \u2019 etre of our paper . ( That \u2019 s why we call it \u201c flexible \u201d . ) If it were easily possible to apply TAMP directly as a baseline comparison , that would have obviated the point of our work . It was essentially that we couldn \u2019 t really flexibly do this that inspired our work . We \u2019 re definitely open to suggestions about how to make this direct comparison , but just not sure how to make sense of it at the moment , given the limitations of existing TAMP solutions that we know about . Maybe part of the problem here is that we were n't successful in communicating to you our main contribution . The main technical problem motivating our work is that standard TAMP ( and other planning ) solutions are efficient , but hard to apply easily in a flexible context , because they typically require the pre-specification of conditions and actions . You can view our main contribution as using a specific targeted approach to intrinsically-motived learning to find an efficient but still flexible route around this problem . Our work shows how one can * still * benefit from the advantages of ideas from planning to achieve efficiency ( in a way that standard Deep RL does not ) . An alternative possibility : are you referring to other planning solutions like RRT * or KPIECE that do n't require the pre-definition of action effects and preconditions ? There are many motion planning algorithms like these , but these algorithms have been shown to utterly fail in real-world environments which have many complex differential constraints . ( This is why most TAMP papers do n't even compare to pure motion planning anymore.So we did n't think rehashing this well-known point was super-helpful . ) Bottom line though is that we \u2019 d be very happy try something out as a new baseline if you make a specific suggestion , especially if the system you suggest has available code we could run in our setting -- but we figure we \u2019 d better ask right now to make sure we have the time to do this . Or if what you 're saying is we need to explain better the thoughts above in the paper , we 'd be happy to do that . What do you think ?"}, "2": {"review_id": "r1xo9grKPr-2", "review_text": "This paper tackles the problem of enabling robots to learn long-horizon, sparse-reward tasks. The proposed approach, the Curious Sample Planner (CSP), builds on insights in task and motion planning (TAMP), which is a standard approach for tackling these kinds of tasks. TAMP constructs a plan in the space of macro-actions (e.g., move object 1 to location (x,y)), and uses a motion planner to execute each macro-action. However, TAMP typically requires being able to describe macro-action effects and preconditions with logical predicates, which can be impossible in real-world environments, due to complex dynamics and interactions. CSP overcomes this limitation by planning in the space of macro-actions in a way that is biased toward novelty. The core approach of CSP is to train a (macro-)action selection network to generate macro-actions that are both feasible and novel. The curiosity module is used in two ways: (1) to give reward to the action selection network for producing novel macro-actions, and (2) to expand states that are considered novel. Three state-of-the-art ways of computing novelty are compared -- state estimation (SE), forward dynamics (FD), and random network distillation (RND). CSP is evaluated on a suite of simulated robotics tasks that require the robot to build simple machines from the objects in its environment, in order to achieve the specified objective. The experiments compare agents trained with CSP versus with deep RL (specifically, A2C, PPO, and PPO + RND). There is an ablation study, that compares against planning with uniform selection of macro-actions and uniform selection of states to expand. Overall, this paper makes a significant contribution to improving robot learning of long-horizon, sparse reward tasks. The paper is clearly written and well-motivated, and the evaluations are thorough. The major downside is that CSP inherently requires knowing the dynamics of the environment (i.e., having a simulator), which means it cannot be directly run applied to real-world robotic systems. But it is a step in the right direction, and clearly outperforms vanilla deep RL. I'm leaning toward accept, but I have a few concerns / questions about the paper. First, there are not enough details included for reproducibility (see list below). With regard to evaluation, I think another ablation should be run, where the action selection network is trained for only feasibility. This would be similar to CSP-No Curiosity, but with a reward of 0 for infeasible actions, and a small fixed positive reward for feasible ones. This would more clearly answer the question of how much it matters to include the curiosity module. Finally, I'm not convinced that this approach works well for transfer, and the evaluations seem inconclusive as well. I'm surprised that even for inter-task transfer, agents trained with action selection transfer and full transfer don't just learn to solve the task immediately. Am I missing something here about how the task is instantiated? Reproducibility questions: - In Algorithm 1, what are the inputs to the novelty metric that is used to compute L_\\phi? Is it the batch of next states, S'? - What is the form of the output of the action-selection network? And what exactly is the space of macro-actions? For instance, the number of possible RemoveConstraint macro-actions depends on how many objects are connected in the environment. But the dimension of the action-selection network's output must be fixed. - What does the state vector input for FD and RND contain? Along these lines, why not also use image inputs for FD and RND, as is done for SE? - What are the learning hyperparamters used to train the networks? (e.g., learning rate) - How many perspectives (i.e., n_p) are used for the SE curiosity module? Minor comments / typos: - Avoid using the same variable with different meanings, e.g. using \\phi to indicate both the curiosity module and the parameters of the value network. - Page 3: \"flexible\", \"a flexible\" - Page 5: \"learnabe\" --> \"learnable\" - Page 8: \"in which\" --> \"in which the\" - Page 9: \"illustrate\" --> \"illustrated\"", "rating": "6: Weak Accept", "reply_text": "Hi Reviewer 1 ! Thanks for your constructive feedback . See below for inline replies to each major item . ... `` Overall , this paper makes a significant contribution to improving robot learning of long-horizon , sparse reward tasks . The paper is clearly written and well-motivated , and the evaluations are thorough .... It is a step in the right direction , and clearly outperforms vanilla deep RL . '' Thanks ! `` ... The major downside is that CSP inherently requires knowing the dynamics of the environment ( i.e. , having a simulator ) , which means it can not be directly run applied to real-world robotic systems . '' Absolutely . As you probably realized , in this world we 've taken a step-by-step strategy : first try to get the planning working * assuming * there is good forward predictor , so that in the next phase of the work we can relax that condition . Our next major step is to apply the method in the context of a non-deterministic learned forward predictor ( and in fact we hope to show that having a good planning algorithm in place makes the learning of the forward predictor substantially more efficient ) . As you note , doing this is really important for actually being able to apply to our work in the real world . `` ... First , there are not enough details included for reproducibility ( see list below ) . '' Yes , both you and Reviewer 3 had this concern , and its totally reasonable . We 'll reply to the details of your concerns below , but here are the three main high-level things we 're going to do in response : 1 . Revise the main text of the paper clarifying the several main important issues for which you and Rev 3 have asked . 2.Create an additional supplementary document with copious detailed information about each aspect of the project . 3.Make a public github account/repo to which we will commit all the project code , so that you ( and others ) can have access to it during ( and after ) this review process . We should have done this before but somehow got worried about breaking the double-blindness of the review . However , upon thinking about it , we realize it should be easy enough to create an anonymous Github user account to post the code to for review purposes . Our plan is to have this ready for your review by 11/11 or 11/12 . That will give you a few days to ask for further clarifications . == > Question : does this plan work for you ? Do you think it 's enough time to address the issues ? `` ... With regard to evaluation , I think another ablation should be run , where the action selection network is trained for only feasibility . This would be similar to CSP-No Curiosity , but with a reward of 0 for infeasible actions , and a small fixed positive reward for feasible ones . This would more clearly answer the question of how much it matters to include the curiosity module . '' Just a point of clarification , removing the curiosity feedback into the action selection networks is not the same as CSP-No Curiosity . In fact , most of the benefit of curiosity comes from selecting which nodes to add to the search tree and the frequency with which to sample those nodes . This operation is independent of the action-selection networks . A more informative ablation might be to run CSP without feasibility feedback . Comparing this result with CSP-No Curiosity would isolate the contribution of the curiosity feedback signal independent of feasibility . Is this an ablation you would be interested in seeing ? `` ... Finally , I 'm not convinced that this approach works well for transfer , and the evaluations seem inconclusive as well . I 'm surprised that even for inter-task transfer , agents trained with action selection transfer and full transfer do n't just learn to solve the task immediately . Am I missing something here about how the task is instantiated ? '' Transfer is tested on different initial instantiations of the general problem statement . So for example in 3-stack , the blocks are placed in different starting positions . It 's important to note that we are not training a policy to solve these tasks , since a valid and general policy may not be learnable from a single example . The reason for showing between-task transfer was only to show that there is some increase in efficiency from having solved the tasks before . I 'm also curious to know how much the log scaling on the y axis contributed to your interpretation of the results as being inconclusive . ( In some cases the efficient gain was substantial even when the log-scale made it look small . ) `` Reproducibility questions : [ LIST OF SPECIFICS ] '' Ok , we 'll make sure to add information about all these things in either the revised main text or the supplement . Once we post these , we 'll write another comment pointing you to where the answers have been added . `` Minor comments / typos : ... [ LIST OF SPECIFICS ] '' Thanks for catching these . All typos have now been fixed and will be uploaded along with other revisions ."}}