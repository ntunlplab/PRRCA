{"year": "2017", "forum": "BymIbLKgl", "title": "Learning Invariant Representations Of Planar Curves ", "decision": "Accept (Poster)", "meta_review": "This work proposes learning of local representations of planar curves using convolutional neural networks.\n Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral). \n \n The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects.", "reviews": [{"review_id": "BymIbLKgl-0", "review_text": "Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples. The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper. Furthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts). In general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference. Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and detailed comments . Our response is as follows : About the negative examples : We like to highlight that analogous to Euclidian curvature , our goal in this paper is to train for a local descriptor of the curve ( a feature for every point of the contour ) and NOT a global shape descriptor ( a feature for the entire shape as a whole ) usually used for classifying shapes . This implies that a negative example is something that has to be different locally , for all points on the curve . A negative example is obtained by pairing a curve with its much smoothed or evolved counterpart ( after euclidian transformations of course ) , because , if we observe the curve locally at any point , the analogous point in the smooth curve would have destroyed important features of the contour and this is NOT something we want our learned descriptor to have . As you have rightly pointed out : \u201c using too easy negatives leads to inferior results \u201c . What we wish to highlight is that the theorems by Hamilton and Grayson ( Section 4 , second paragraph ) define a geometric notion of this \u201c easiness '' . It says that any closed curve can be abstracted to various levels using mean curvature smoothing . The highest level is a circle , which has a constant curvature for every point . This is exactly in sync with the contrastive loss function of the siamese network . If we do not provide any negative example ( the easiest training possible ) , we expect a constant function as the network output , analogous to a constant curvature . As we begin to provide negative examples with increasing richness profiles as in Table 1 , we make it less and less easier for training , thereby discovering a scale-space of signatures which is \u201c learned '' rather than axiomatically constructed as demonstrated in Figure 6 . The novelty in our paper is first , to show this connection between classical theorems in differential geometry and modern deep-learning systems , and second , to highlight that these learned signatures have better numerical properties as compared to their axiomatic counterparts . Although we agree that a larger evaluation set would be useful , we do not believe it would change the core results presented here . Different datasets are currently under investigation for future work ."}, {"review_id": "BymIbLKgl-1", "review_text": "I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that \"if it's not worth doing, it's not worth doing well.\" There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of \"why use this representation\" with the authors and they said their \"main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network.\" Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it's sensible to revisit this problem using convnets. I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and assessment . We appreciate your comments and agree with most parts of it . However , we do feel that it is definitely interesting and worthwhile to use learnable architectures to model geometric representations , thereby re-discovering traditional representations from a learning viewpoint . We believe that the marriage of modern ML tools with classical geometric frameworks and approaches is in fact becoming popular . See for example [ Bronstein , et al.Geometric deep learning : going beyond Euclidean data ] . We believe our work plays a role in supporting this direction and therefore our approach in this paper was to ask : \u201c Can it be done ? \u201c and \u201c Can it be done better ? \u201c .We are happy to present positive outcomes on both counts ."}, {"review_id": "BymIbLKgl-2", "review_text": "Pros : - New representation with nice properties that are derived and compared with a mathematical baseline and background - A simple algorithm to obtain the representation Cons : - The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank you for your positive reviews and appreciate the constructive suggestions . About your queries : We have updated the paper and incorporated your suggestion of analysing the first layer . We have put in a figure in the appendix section showing some of the filters from the first layer and compared their shapes to standard gaussian filters and their derivatives to obtain a rough perspective on their action . We can see that barring random sign flips ( which are possibly corrected for in the later stages of the network ) , some of the filters reasonably co-relate with standard 1D derivatives . About your question on Wavelets , I think you might be interested in the following : Tieng , Quang Minh , and W. W. Boles . `` Recognition of 2D object contours using the wavelet transform zero-crossing representation . '' IEEE Transactions on Pattern Analysis and Machine Intelligence 19.8 ( 1997 ) : 910-916 ."}], "0": {"review_id": "BymIbLKgl-0", "review_text": "Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples. The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper. Furthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts). In general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference. Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and detailed comments . Our response is as follows : About the negative examples : We like to highlight that analogous to Euclidian curvature , our goal in this paper is to train for a local descriptor of the curve ( a feature for every point of the contour ) and NOT a global shape descriptor ( a feature for the entire shape as a whole ) usually used for classifying shapes . This implies that a negative example is something that has to be different locally , for all points on the curve . A negative example is obtained by pairing a curve with its much smoothed or evolved counterpart ( after euclidian transformations of course ) , because , if we observe the curve locally at any point , the analogous point in the smooth curve would have destroyed important features of the contour and this is NOT something we want our learned descriptor to have . As you have rightly pointed out : \u201c using too easy negatives leads to inferior results \u201c . What we wish to highlight is that the theorems by Hamilton and Grayson ( Section 4 , second paragraph ) define a geometric notion of this \u201c easiness '' . It says that any closed curve can be abstracted to various levels using mean curvature smoothing . The highest level is a circle , which has a constant curvature for every point . This is exactly in sync with the contrastive loss function of the siamese network . If we do not provide any negative example ( the easiest training possible ) , we expect a constant function as the network output , analogous to a constant curvature . As we begin to provide negative examples with increasing richness profiles as in Table 1 , we make it less and less easier for training , thereby discovering a scale-space of signatures which is \u201c learned '' rather than axiomatically constructed as demonstrated in Figure 6 . The novelty in our paper is first , to show this connection between classical theorems in differential geometry and modern deep-learning systems , and second , to highlight that these learned signatures have better numerical properties as compared to their axiomatic counterparts . Although we agree that a larger evaluation set would be useful , we do not believe it would change the core results presented here . Different datasets are currently under investigation for future work ."}, "1": {"review_id": "BymIbLKgl-1", "review_text": "I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that \"if it's not worth doing, it's not worth doing well.\" There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of \"why use this representation\" with the authors and they said their \"main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network.\" Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it's sensible to revisit this problem using convnets. I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and assessment . We appreciate your comments and agree with most parts of it . However , we do feel that it is definitely interesting and worthwhile to use learnable architectures to model geometric representations , thereby re-discovering traditional representations from a learning viewpoint . We believe that the marriage of modern ML tools with classical geometric frameworks and approaches is in fact becoming popular . See for example [ Bronstein , et al.Geometric deep learning : going beyond Euclidean data ] . We believe our work plays a role in supporting this direction and therefore our approach in this paper was to ask : \u201c Can it be done ? \u201c and \u201c Can it be done better ? \u201c .We are happy to present positive outcomes on both counts ."}, "2": {"review_id": "BymIbLKgl-2", "review_text": "Pros : - New representation with nice properties that are derived and compared with a mathematical baseline and background - A simple algorithm to obtain the representation Cons : - The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank you for your positive reviews and appreciate the constructive suggestions . About your queries : We have updated the paper and incorporated your suggestion of analysing the first layer . We have put in a figure in the appendix section showing some of the filters from the first layer and compared their shapes to standard gaussian filters and their derivatives to obtain a rough perspective on their action . We can see that barring random sign flips ( which are possibly corrected for in the later stages of the network ) , some of the filters reasonably co-relate with standard 1D derivatives . About your question on Wavelets , I think you might be interested in the following : Tieng , Quang Minh , and W. W. Boles . `` Recognition of 2D object contours using the wavelet transform zero-crossing representation . '' IEEE Transactions on Pattern Analysis and Machine Intelligence 19.8 ( 1997 ) : 910-916 ."}}