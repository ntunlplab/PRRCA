{"year": "2021", "forum": "EGVxmJKLC2L", "title": "Learning not to learn: Nature versus nurture in silico", "decision": "Reject", "meta_review": "There was fairly detailed discussion among three of the four reviewers. The fundamental concern of the reviewers is regarding the contribution of the paper. During the rebuttal, the authors clarified the following:\n\n> while the effects of varying uncertainty / horizon lengths is well-understood for Bayes-optimal policies, it is not understood for existing meta-RL approaches, which is the topic of this paper\n\nThat is, the contribution of the paper is to understand the effects of varying uncertainty/horizon lengths for meta-RL approaches. However, it is known in prior work that meta-RL algorithms such as RL^2 can implement Bayes-optimal policies in principle. As a result, it's not clear whether this contribution is significant relative to prior knowledge, and this paper does not seem to bring any new insights.\n\nAn alternative framing of the paper would be to consider the question of how meta-RL solutions compare to Bayes-adaptive optimal policies. While this framing would be interesting and novel, the current version of the paper does not sufficiently answer this question, since the only experiments include RL^2 (and such a study would require experimenting with more sophisticated meta-RL algorithms beyond RL^2).\n\nAs such, this paper isn't suitable for publication at ICLR in its current form.", "reviews": [{"review_id": "EGVxmJKLC2L-0", "review_text": "Summary : This paper explores the effect of time horizon on meta-reinforcement learning agents . Using a recurrent meta-learner , it demonstrates that different strategies are learned based on the time horizon during meta-training . Pros : Creative proposal to study when incorporating new data to adapt to a task is warranted versus executing an existing behavior . Results are very nicely presented and writing is good Cons : The analogy to \u201c nature versus nurture \u201d seems tenuous Results seem somewhat obvious ( see comments below ) Detailed Comments : While I appreciate the inspiration of the biological connections argued in this work , I am not convinced that the analogy of learning versus adapting in meta-learning maps neatly onto the concepts of nature versus nurture in biology . If biology terminology is going to be used , it needs to be carefully defined for a machine learning audience , and the limitations of the analogy discussed . Presently , I find that the use of this terminology subtracts from the clarity of the work . My main criticism of the work is that the results seem fairly obvious . In the finite horizon case , the meta-learner will necessarily learn a strategy optimal for the given horizon because that \u2019 s exactly what it \u2019 s optimized for . It seems akin to me to training an RNN policy in an MDP and then noticing that the RNN is capable of * not * persisting information across timesteps since it isn \u2019 t needed . Since feedforward models are a subset of recurrent models , this seems obvious . It also seems strange to me that the agent is only tested in 1 episode . It seems like if the agent has learned a new skill , it should be able to repeat that skill if given the chance ( e.g. , with a reset ) , and that that is a hallmark of meta-learning . What is happening in this paper seems more like \u201c horizon-dependent RL. \u201d I don \u2019 t really see how the behavior in the long-time horizon setting can be called \u201c learning \u201d while the short time-horizon behavior is not . Both seem like the optimal policy for the given MDP . In Section 4 , for example , wouldn \u2019 t \u201c learning \u201d be defined as figuring out where the colored squares are ? Yet in the paper it seems to be defined as executing a policy that moves farther away from the start . It seems to me like exploration and learning are being conflated here . Several times throughout the paper it is claimed that the work \u201c investigates the interplay of three considerations when designing meta-task distributions : The diversity of the task distribution , task complexity and training lifetime. \u201d I don \u2019 t see how the first two are analyzed in this paper . Perhaps in the bandit example , but there it seems to be always entangled with the training lifetime . Besides the comments about the biological terminology mentioned earlier , the paper is clear and easy to read , with the other exception of the description of the 2-arm Gaussian bandit in Section 3 . This reviewer had to read that first paragraph about 5 times to understand the setup . The figures are well-done and clearly present the results . Recommendation : 5 . While experiments are thorough and nicely presented , I presently don \u2019 t see what insights are gained from the analysis . The claim of analyzing \u201c diversity of the task distribution , task complexity and training lifetime \u201d seems like an over-claim to me . It \u2019 s not clear to me that the agent \u201c learns \u201d a new skill , since there is only exploration , and no exploitation . -Update- After reading all the other reviews and ensuing discussions , I maintain my original score . If the research question is `` How does the optimal policy depend on task parameters such as uncertainty and horizon ? '' I believe Bayes-adaptive work answers that question . If the question is `` How do policies learned by meta-RL algorithms compare to Bayes-optimal policies ? '' then I think more empiricism is needed ( since RL2 in principle can represent the Bayes-optimal policy ) , or a comparison of multiple methods .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed and constructive comments . We adapted the manuscript and hope that they are addressed appropriately : * * '' [ ... ] I am not convinced that the analogy of learning versus adapting in meta-learning maps neatly onto the concepts of nature versus nurture in biology . `` * * We do not aim to capture every dimension of the biological \u2018 nature versus nurture \u2019 debate and believe that this would require an unpractical amount of complexity . We have added a discussion section to clarify our claims and to relate our results to the related work in computational ethology : * '' [ ... ] The observed sharp transition between exploratory learning behavior ( s ) and hard-coded , non-learning strategies can be seen as a proof-of-concept example for a `` nature-nurture '' trade-off that adds new aspects to earlier work in theoretical ecology ( Stephens , 1991 ) . From this perspective of animal behavior , meta-learning with a finite time horizon could provide an inroad into understanding the benefits and interactions of instinctive and adaptive behaviors . Potential applications could be the meta-learning of motor skills in biologically inspired agents ( Merel et al. , 2019 ) or instinctive avoidance reactions to colours or movements . The degree of biological realism that can be reached will be limited by computational resources , but qualitative insights could be gained , e.g. , for simple instinctive behaviors . '' ( p. 9 ) * * * '' [ ... ] My main criticism of the work is that the results seem fairly obvious . `` * * We agree that post-hoc , the key results feel somewhat obvious , but nevertheless believe that they have consequences not only by providing a potential new workhorse for theoretical biology , but also for a broad range of meta-learning problems . The highly nonlinear , discontinuous dependence of the optimal solution on task parameters and lifetime -- i.e. , the amount of data available to the optimized learner on the job -- is a consequence of local maxima in the reward landscape . We now investigate this in more depth in figure 2 , which investigates the sensitivity of the bimodal solution space directly at the edge between \u201c learning \u201d and \u201c not learning \u201d regimes . We summarized the results of this new analysis in the following paragraph in the discussion : * '' [ ... ] A key take-home from our results is the highly nonlinear and potentially discontinuous dependence of the meta-learned strategy on the parameters of the task ensemble . For certain parameter ranges , the reward landscape of the meta-learning problem features several local maxima that correspond to different learning strategies . The relative propensity of these strategies to emerge over the course of meta-learning depends on the task parameters and on the initialization of the agent . Generally , this supports the notion that there is not a single inductive bias for a given task distribution . Rather , there could be a whole spectrum of inductive biases that are appropriate for different amounts of training data . Even for the same task setting , different training runs can result in qualitatively different solutions , providing a note of caution for interpretations drawn by pooling over ensembles of trained networks . '' ( p. 8 ) * * * '' [ ... ] Wouldn \u2019 t \u201c learning \u201d be defined as figuring out where the colored squares are ? Yet in the paper it seems to be defined as executing a policy that moves farther away from the start . : * * We believe that there may be a misunderstanding : The object locations vary between episodes/ \u2019 lives \u2019 and the optimized agent indeed first explores the environment & identifies a reward location . It then repeatedly goes to that object ( if this is optimal given the remaining lifetime ) . The rollouts in figure 7 only visualizes a single sampled MDP for three different agents with different training lifetimes . We provide additional episode rollout visualizations in an anonymous [ Google document ] ( https : //docs.google.com/document/d/1bnmIykdsOase4QPgh4LkH-LtuDczgRPbiKRSM3M42jc/edit ? usp=sharing ) . The agents use different strategies depending on their training lifetime and acquired inner-loop learning algorithm . * * '' [ ... ] This reviewer had to read that first paragraph about 5 times to understand the setup . `` * * We excuse the inconvenience of the convoluted bandit task description and have restructured the section to provide a \u2018 smoother \u2019 reader experience . Here is the updated and hopefully better to read excerpt : * '' [ ... ] To keep it simple , one of the two arms is deterministic and always returns a reward of 0 . The task distribution is represented by the variable expected reward of the other arm , which is sampled at the beginning of an episode , from a Gaussian distribution with mean -1 and standard deviation \u03c3_p , i.e.\u00b5 ~ N ( -1 , \u03c3_p^2 ) . The standard deviation \u03c3_p controls the uncertainty of the ecological niche . For \u03c3_p < < 1 , the deterministic arm is almost always the better option . For \u03c3_p > > 1 , the chances of either arm being the best in the given episode is largely even . [ ... ] '' ( p. 3 ) *"}, {"review_id": "EGVxmJKLC2L-1", "review_text": "This paper provides an analysis of RNN-based meta learning approaches . In particular , it investigates the strategies learned via meta-learning , contrasting strategies involving task-dependent learning vs heuristic or hard-coded solutions . Empirical evidence in two sets of experiments , on a 2-armed bandit toy task and a grid-world navigation task , show that hard-coded strategies can be a function of training task distribution and task complexity as well as task horizon . While the experiments are simplistic , they provide a clear and thorough comparison of different agent behaviours across different training regimes . I enjoyed reading the paper and although the results are intuitive and unsurprising , they nicely emphasize the importance of environment and task design choices in strategies learned via memory-based meta-learning . Comments/questions : 1 . I would encourage the authors to use \u201c memory-based/RNN-based meta-learning \u201d instead of \u201c meta-learning \u201d to avoid confusion as these results might not apply more widely across different meta-learning approaches ( e.g.gradient based ) . 2.While the pattern of behaviour looks qualitatively similar across analytical and empirical results reported in Figure 1 , I wonder if it would be possible to quantitatively assess where they differ . 3.It would be nice to see error bars for the 5 independent training runs in Figure 5 . 4.In the Appendix A.2.3 it is mentioned that the discount factor is annealed from 0.8 to 1. within the first 800k episodes . Could you please expand if this was crucial to achieve your results or just an experimental choice ? Overall , I think this is an interesting contribution of perhaps limited scope but still valuable and could encourage interesting future research directions in memory-based meta-learning .", "rating": "7: Good paper, accept", "reply_text": "Thank you very much . We have modified the manuscript and hope your questions and suggestions are now addressed . Please see below for detailed replies : 1 . * * '' [ ... ] I would encourage the authors to use \u201c memory-based/RNN-based meta-learning \u201d instead of \u201c meta-learning \u201d to avoid confusion as these results might not apply more widely across different meta-learning approaches ( e.g.gradient based ) . `` * * Yes , we fully agree . The previously used meta-learning terminology may be confusing for the reader and we have replaced the wording to be more precise in the current version of the submission . ( See revised manuscript . ) 2. * * '' [ ... ] While the pattern of behaviour looks qualitatively similar across analytical and empirical results reported in Figure 1 , I wonder if it would be possible to quantitatively assess where they differ . `` * * We believe that this is a very important question and have further investigated the differences between analytical and empirical solutions . Given that those occur primarily on the edge between the learning and the no-learning regimes , we did a deeper analysis of this parameter regime . We now show that in that regime , the reward landscape has local maxima into which the gradient-based optimization falls . How often which local maximum is reached depends on the distance of the parameters from the edge . The results can be found in a new figure 2 . We also added a new paragraph discussing our findings : * '' [ ... ] In the Bayesian model , the edge between the two regimes is located at parameter values where the learning strategy and the non-learning strategy perform equally well . Because these two strategies are very distinct , we wondered whether the reward landscape for the memory-based meta-learner has two local maxima corresponding to the two strategies ( figure 2 ) . To test this , we trained $ N=1000 $ networks with different initial conditions , for task parameters close to the edge . We then evaluated for each network the number of explorative pulls of the stochastic arm , averaged across 100 episodes . The distribution of the number of explorative pulls across the 1000 networks shows i ) a peak at zero exploration and ii ) a broad tail of mean explorative pulls ( figure 2 ) , suggesting that there are indeed two classes of networks . One class never pulls the stochastic arm , i.e. , those networks adopt a non-learning strategy . The other class learns . For task parameters further away from the edge , this bimodality disappears . '' ( p. 5 ) * 3 . * * '' [ ... ] It would be nice to see error bars for the 5 independent training runs in Figure 5 . `` * * We added median and percentile bands to both figure 5 and 6 . The results remain robust and significant . ( See revised figures in the manuscript . ) 4. * * '' [ ... ] In the Appendix A.2.3 it is mentioned that the discount factor is annealed from 0.8 to 1. within the first 800k episodes . Could you please expand if this was crucial to achieve your results or just an experimental choice ? `` * * Indeed , the discount factor schedule is an important hyperparameter . While our theoretical result relies on a finite horizon and discount factor 1 , this setting is initially very challenging for the meta-learner , the main reason being the long time-scale of backpropagation through time when training the RNN . Instead , we chose to gradually extend the window of integration by increasing the discount factor . Thereby , we essentially treat the discount as a curriculum parameter ( Prokhorov and Wunsch , 1997 ) ."}, {"review_id": "EGVxmJKLC2L-2", "review_text": "Summary : This paper observes that in meta-RL ( and evolutionary biology ) , sometimes it is advantageous to learn behaviors that adapt to the particular task , while other times not adapting to the task , and instead relying on a task-agnostic \u201c hard-coded \u201d behavior is sufficient . While much meta-RL research typically focuses on the former setting , this paper studies when it is not necessary to learn adaptive behaviors . Specifically , this paper presents three main findings : ( i ) whether or not it is optimal to learn adaptive behavior strongly depends on the horizon of the task and complexity of learning such adaptive behaviors \u2014 if the horizon is too short , or if the adaptive behavior requires complex exploration , then exploring the new task to learn adaptive behaviors may not be worth it ; ( ii ) existing meta-RL agents are capable of choosing not to learn adaptive behaviors , when it is optimal to do so ; ( iii ) existing meta-RL agents generalize poorly to tasks with varying horizon-lengths . Strengths : - Novelty . Most meta-RL papers study the case where adapting to a new task requires learning new behaviors . Instead , this paper studies when such adaptation is not necessary , which is an area that has not been well-studied . Therefore , it is interesting and novel to bring attention to the fact that both regimes exist . - Clarity and Execution . This paper is generally well-executed and clear . The bandit example in Section 3 clearly illustrates the two regimes that this paper studies : when it is necessary to learn adaptive behaviors vs. not , and the results are presented in figures that impressively , clearly illustrate the points made in the text . This section also convincingly shows that existing meta-RL agents roughly learn the Bayes-optimal policy in this case . Similarly , the grid world tasks in Section 4 also clearly illustrate how the behavior of meta-RL agents changes with the horizon of the task . The paper does a good job at thoroughly studying what happens when each parameter ( e.g. , horizon-length , aleatoric uncertainty , and epistemic uncertainty ) varies . Weaknesses : - Significance . My primary concern with this work is significance . I believe that the main insights of this paper can be interpreted through the lens of Bayes-adaptive policies [ 1 ] . The optimal Bayes-adaptive policy explains when it is optimal to explore a new task and learn adaptive behaviors , depending on the horizon length and amount of exploration needed , and therefore , the main claim of the paper can be framed as observing that existing meta-RL agents can learn the Bayes-adaptive optimal policy in simple tasks . This is an interesting observation , but prior work [ 2 ] already shows that meta-RL is equivalent to learning the Bayes-adaptive optimal policy . Furthermore , for harder tasks , this is likely to be confounded with the complexity of learning efficient exploration behaviors that allow learning adaptive behaviors , as shown by prior work [ 2 , 3 , 4 ] . Overall , it is therefore unclear to me what significant takeaways the meta-RL community can gain from this work . One potential takeaway is the observation that meta-RL agents generalize poorly to tasks with varying horizon-lengths , but it \u2019 s unclear whether this setting occurs in real tasks , and even if this _is_ necessary , then simply adding the horizon to the observation ( state ) and varying the horizon during meta-training seems sufficient . Overall , I found this paper to be a well-written , well-executed , illustration of when it is optimal to learn task-specific adaptive behaviors . However , due to its limited significance , I am unfortunately unable to recommend acceptance . Additional Questions / Comments : - In Figure 2 , what does it mean with \u201c 100 Suboptimal pulls ? \u201d Is this over the course of all training ? Also , why is the initial policy entropy at 0 ? I would expect that upon initialization , the policy entropy is not 0 . - I believe that the Bayes-adaptive optimal policy can also be analytically computed for the grid world tasks . It would be nice to report the optimal returns in Figure 5 . - The interplay between the term \u201c lifetime \u201d and episodes was not explicitly defined . It was possible to infer the paper \u2019 s intent from reading the experiments , but more carefully defining this could help . - Framing the exploration trade-off in terms of evolutionary biology is interesting . References : [ 1 ] Optimal Learning : Computational procedures for Bayes-adaptive Markov decision processes . Michael O \u2019 Gordon Duff . February 2002. https : //www.gatsby.ucl.ac.uk/~yael/Okinawa/DuffThesis.pdf [ 2 ] VariBAD : A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning . Luisa Zintgraf , Kyriacos Shiarlis , Maximilian Igl , Sebastian Schulze , Yarin Gal , Katja Hofmann , Shimon Whiteson . October 2019. https : //arxiv.org/abs/1910.08348 [ 3 ] Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables . Kate Rakelly , Aurick Zhou , Deirdre Quillen , Chelsea Finn , Sergey Levine . March 2019. https : //arxiv.org/abs/1903.08254 [ 4 ] Explore then Execute : Adapting without Rewards via Factorized Meta-Reinforcement Learning . Evan Zheran Liu , Aditi Raghunathan , Percy Liang , Chelsea Finn . June 2020. https : //openreview.net/forum ? id=La1QuucFt8- * * Edit : Score raised from 4 -- > 5 following discussion below . * *", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your clear comments . We would like to first take the opportunity to address the key question of significance . * * [ ... therefore , the main claim of the paper can be framed as observing that existing meta-RL agents can learn the Bayes-adaptive optimal policy in simple tasks . This is an interesting observation , but prior work [ 2 ] already shows that meta-RL is equivalent to learning the Bayes-adaptive optimal policy . ] `` Overall , it is therefore unclear to me what significant takeaways the meta-RL community can gain from this work . One potential takeaway is the observation that meta-RL agents generalize poorly to tasks with varying horizon-lengths ... '' * * We agree that the meta-learner effectively learns the Bayes-adaptive optimal policy , but we do not think that this is our main take-home message . On a general level , meta-learning can probably always be phrased as an algorithmic way of inferring a Bayes-optimal policy . Aside from suggesting meta-learning approaches as a principled approach to nature-nurture problems in evolutionary biology , this work aims at mapping out the dependence of the optimal policy on the task parameters . We believe that the meta-learning community has paid little attention to the effects of the meta-training distribution and the amount of the inner loop adaptation . The main take home is that these effects can be nonlinear to the point of discontinuity : The switch from the learning to the non-learning regime is a sharp one . The underlying reason is local maxima in the reward landscape ( evidence for that in Fig.2 of the revised manuscript ) . These types of discontinuity ( in seemingly unimportant parameters like lifetime ) is interesting not only from the perspective of evolutionary biology -- as sharp nature-nurture transitions -- but may well occur in a range of other meta-learning tasks and could have a strong impact on gradient-based meta-learning approaches . We try to better highlight these points in the revised version . To this , we added an additional analysis close to the transition edge to illustrate the presence and impact of these local maxima ( Fig.2 ) .We also added a paragraph to the discussion : * '' [ ... ] A key take-home from our results is the highly nonlinear and potentially discontinuous dependence of the meta-learned strategy on the parameters of the task ensemble . For certain parameter ranges , the reward landscape of the meta-learning problem features several local maxima that correspond to different learning strategies . The relative propensity of these strategies to emerge over the course of meta-learning depends on the task parameters and on the initialization of the agent . Generally , this supports the notion that there is not a single inductive bias for a given task distribution . Rather , there could be a whole spectrum of inductive biases that are appropriate for different amounts of training data . Even for the same task setting , different training runs can result in qualitatively different solutions , providing a note of caution for interpretations drawn by pooling over ensembles of trained networks . `` * * * '' Furthermore , for harder tasks , this is likely to be confounded with the complexity of learning efficient exploration behaviors that allow learning adaptive behaviors , as shown by prior work [ 2 , 3 , 4 ] '' * * Yes , limited generalization can of course occur for a variety of reasons , but is n't it interesting to know that there could be a deeper underlying reason why things do n't work as well as hoped for ? We found it interesting , for example , that the washed-out edge between learning and non-learning in the simulated RL2 agents is in fact not due to a gradual change in the policy , but due to changing probabilities of falling into totally different policies ( see newly added figure 2 ) . * * '' [ ... ] even if this is necessary , then simply adding the horizon to the observation ( state ) and varying the horizon during meta-training seems sufficient . `` * * That feels like a natural extension , but given that potentially very nonlinear dependencies of the optimal policy on the horizon , it may not be so easy to train . It would certainly be worth investigating such a horizon-dependent meta-learner , and be it to allow the use of the horizon as a gradually increasing curricular parameter and thereby provide a meta-learner that adapts very quickly on the job , without hampering its precision as more training data comes in ."}, {"review_id": "EGVxmJKLC2L-3", "review_text": "* * Summary . * * The authors investigate the question of when the optimal behavior for an agent is to learn from experience versus when the optimal behavior is to apply the same ( memorized ) policy in every scenario . They begin by introducing a simple bandits environment wherein they derive the optimal policy and identify regimes in which it involves memorization vs. learning . Then they train an RL^2 agent and verify that it behaves as expected in these regimes . Next , they expand their approach to a slightly more complicated gridworld environment which does not have an analytic solution to the question . The agent behaves as expected in the gridworld environment . * * Strong points . * * This paper tackles a novel question which is fundamental to the field of metalearning . By carefully analyzing the two regimes of learning and memorization in the context of metalearning , this paper will increase awareness about the fact that the two regimes exist . The paper is clearly written and does an excellent job of putting experiments in the context of past ML research . The experimental setup is simple but goes straight to the heart of the issue . Figures and text do a good job of analyzing results and communicating them to the reader . Overall , this paper was an interesting read . * * Weak points . * * The idea that \u201c sometimes memorization is best and other times learning is best \u201d does border on the obvious . Indeed , as soon as the authors derive their analytical solution , it becomes clear that we can expect the RL^2 agent to learn the same behavior . For me , there were no surprises in the experimental sections . To the authors : was there anything that was surprising or not obvious to you ? What additional information can the experiments tell us , apart from confirming theoretical predictions ? Having said that , I also believe that very simple , well-executed research ideas sometimes make the best papers . This paper appears to be one of those cases . And even though the ideas are simple , they are significant and they are not a major part of the dialogue in the meta-learning community yet . So even if the ideas seem obvious , I think there is value in communicating them well . I have one concern about the bandit task setup : the authors adjust $ \\sigma_l $ , the width of the Gaussian from which they are sampling the reward , as a proxy for aleatoric uncertainty and hence task complexity . In doing so , they essentially equate \u201c stochasticity of the environment \u201d with \u201c task complexity. \u201d And yet , there are many other ways in which a task can be complex . Sometimes , all the information needed to perform a task is present and yet the task is difficult to solve because one needs to interpret/integrate the information in a particular way . This is why , for example , puzzles are considered difficult tasks . It is also why simulating the 3-body problem is a complex task . To the authors : can you clarify what you mean by \u201c task complexity \u201d ? In the closing paragraph of the paper , the authors claim that their approach \u201c allows us to study the emergence of inductive biases in biological systems \u201d but this claim is not supported by the rest of the paper , which makes almost no connections to biological systems . There are certainly ways in which these results are relevant to learning in biological systems , but the authors did not explore them in this paper , and so this claim is not well supported . In the same paragraph , they bring in contrasting notions of Darwinian and Lamarkian inheritance . Since they do this in one sentence -- the last sentence -- it is hard to understand what their claim is . And it was not clear that this was one of the main takeaways of the paper , as these concepts do not appear anywhere else in the paper . If the authors want to draw these conclusions , then they should add additional discussion on these topics . Otherwise , they risk misleading readers . One additional minor suggestion would be to invert the color scale of Figure 6 , as \u201c white - > red \u201d signifies values of increasing size in all preceding plots , but in Figure 6 it currently signifies values of decreasing size . Minor grammatical suggestions -- \u201c the question which aspects of behavior \u201c - > \u201c the question of which aspects of behavior \u201c -- When typing quotes in LaTex , use `` and \u2018 \u2019 instead of \u201c \u201d so as to make them open & close correctly -- \u201c interplay of the agent \u2019 s lifetime , \u201d - > \u201c interplay between the agent \u2019 s lifetime , \u201d -- \u201c We numerically show \u201d - > \u201c We show numerically \u201d -- \u201c as well as explicit models of memory \u201d - > \u201d and explicit models of memory \u201d ( same issue occurs later ) -- \u201c the agents does not have \u201d - > \u201c the agent does not have \u201d * * Recommendation . * * 6 : Marginally above acceptance threshold * * Reasoning . * * This paper is well written and the experimental setup is simple , well-executed , and produces results that are relevant to the main question of the paper . The main question of the paper -- when does it make more sense to learn vs. memorize a behavior -- is significant to ICLR and to the field of machine learning . There are a number of relatively minor weaknesses ( as described above ) but this is overall a nice paper and would be a good contribution to ICLR 2021 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks , we very much appreciate your thoughtful considerations as well as detailed corrections . We want to highlight some new analyses and comments to your points of concern : * * \u201c [ ... ] For me , there were no surprises in the experimental sections . To the authors : was there anything that was surprising or not obvious to you ? \u201d * * Honestly , we were initially surprised by the fact that the transition to the non-learning parameter regime is an abrupt one , and we believe that this has consequences for meta-learning on a more general level . The underlying reason for the sharp transition is a bimodality in the reward landscape . The transition occurs where the global optimum switches from one of those local maxima to another , as for most phase transitions . As a result , the meta-learned strategy has a highly nonlinear and discontinuous parameter dependence and for gradient-based approaches , local minima are obviously an issue . To highlight this point , we now provide a further in-depth analysis of the transition regime between learning and non-learning . We show that in this regime , different initializations lead to fundamentally different solutions . We have added a completely new figure 2 including a discussion ( please see reply to comment 2 of R2 ) , as well as a discussion paragraph : * '' [ ... ] A key take-home from our results is the highly nonlinear and potentially discontinuous dependence of the meta-learned strategy on the parameters of the task ensemble . For certain parameter ranges , the reward landscape of the meta-learning problem features several local maxima that correspond to different learning strategies . The relative propensity of these strategies to emerge over the course of meta-learning depends on the task parameters and on the initialization of the agent . Generally , this supports the notion that there is not a single inductive bias for a given task distribution . Rather , there could be a whole spectrum of inductive biases that are appropriate for different amounts of training data . Even for the same task setting , different training runs can result in qualitatively different solutions , providing a note of caution for interpretations drawn by pooling over ensembles of trained networks . '' ( p. 8 ) * * * '' [ ... ] There are many other ways in which a task can be complex . Sometimes , all the information needed to perform a task is present and yet the task is difficult to solve because one needs to interpret/integrate the information in a particular way . `` * * We agree , and tried to be a bit more precise in our definition of task complexity . An exploration of different types of task complexity is beyond the present paper : * '' Task complexity : How long does it take to learn the optimal strategy for the task at hand ? Note that this could be different from the time it takes to execute the optimal strategy . '' ( p. 2 ) * We have also changed the description of the bandit task to better explain our choices and clean up the admittedly confusing terminology in the earlier version of manuscript . Here the relevant excerpt for task complexity : * '' While the mean \u00b5 remains constant for the lifetime T of the agent , the reward obtained in a given trial is stochastic and is sampled from a second Gaussian , r ~ N ( \u00b5 , \u03c3_l ) . [ ... ] The standard deviation \u03c3_l hence controls how quickly the agent can learn the optimal policy . We therefore use it as a proxy for task complexity. \u201d ( p. 3 ) * * * \u201c [ ... ] There are certainly ways in which these results are relevant to learning in biological systems , but the authors did not explore them in this paper , and so this claim is not well supported. \u201d * * We also agree that the claim of studying inductive biases in real biological systems is too vague and have significantly adapted the manuscript . Nonetheless , we believe that future in-silico meta-learning approaches can provide testable predictions for experimental evolution . This includes the interesting line of work on experimental evolution such as simulating up to 50 generations of drosophila evolution ( e.g.Dunlap and Stephens , 2016 ; Marcus et al. , 2018 ) . We added a discussion paragraph and scaled down our claims : * '' [ ... ] From this perspective of animal behavior , meta-learning with a finite time horizon could provide an inroad into understanding the benefits and interactions of instinctive and adaptive behaviors . Potential applications could be the meta-learning of motor skills in biologically inspired agents ( Merel et al. , 2019 ) or instinctive avoidance reactions to colours or movements . The degree of biological realism that can be reached will be limited by computational resources , but qualitative insights could be gained , e.g. , for simple instinctive behaviors . '' ( p. 9 ) * Finally , we thank the reviewer for the detailed corrections . We apologize for the grammatical faux pas and have corrected these as well as adapted the color scheme of figure 6 in the current version of the submission manuscript ."}], "0": {"review_id": "EGVxmJKLC2L-0", "review_text": "Summary : This paper explores the effect of time horizon on meta-reinforcement learning agents . Using a recurrent meta-learner , it demonstrates that different strategies are learned based on the time horizon during meta-training . Pros : Creative proposal to study when incorporating new data to adapt to a task is warranted versus executing an existing behavior . Results are very nicely presented and writing is good Cons : The analogy to \u201c nature versus nurture \u201d seems tenuous Results seem somewhat obvious ( see comments below ) Detailed Comments : While I appreciate the inspiration of the biological connections argued in this work , I am not convinced that the analogy of learning versus adapting in meta-learning maps neatly onto the concepts of nature versus nurture in biology . If biology terminology is going to be used , it needs to be carefully defined for a machine learning audience , and the limitations of the analogy discussed . Presently , I find that the use of this terminology subtracts from the clarity of the work . My main criticism of the work is that the results seem fairly obvious . In the finite horizon case , the meta-learner will necessarily learn a strategy optimal for the given horizon because that \u2019 s exactly what it \u2019 s optimized for . It seems akin to me to training an RNN policy in an MDP and then noticing that the RNN is capable of * not * persisting information across timesteps since it isn \u2019 t needed . Since feedforward models are a subset of recurrent models , this seems obvious . It also seems strange to me that the agent is only tested in 1 episode . It seems like if the agent has learned a new skill , it should be able to repeat that skill if given the chance ( e.g. , with a reset ) , and that that is a hallmark of meta-learning . What is happening in this paper seems more like \u201c horizon-dependent RL. \u201d I don \u2019 t really see how the behavior in the long-time horizon setting can be called \u201c learning \u201d while the short time-horizon behavior is not . Both seem like the optimal policy for the given MDP . In Section 4 , for example , wouldn \u2019 t \u201c learning \u201d be defined as figuring out where the colored squares are ? Yet in the paper it seems to be defined as executing a policy that moves farther away from the start . It seems to me like exploration and learning are being conflated here . Several times throughout the paper it is claimed that the work \u201c investigates the interplay of three considerations when designing meta-task distributions : The diversity of the task distribution , task complexity and training lifetime. \u201d I don \u2019 t see how the first two are analyzed in this paper . Perhaps in the bandit example , but there it seems to be always entangled with the training lifetime . Besides the comments about the biological terminology mentioned earlier , the paper is clear and easy to read , with the other exception of the description of the 2-arm Gaussian bandit in Section 3 . This reviewer had to read that first paragraph about 5 times to understand the setup . The figures are well-done and clearly present the results . Recommendation : 5 . While experiments are thorough and nicely presented , I presently don \u2019 t see what insights are gained from the analysis . The claim of analyzing \u201c diversity of the task distribution , task complexity and training lifetime \u201d seems like an over-claim to me . It \u2019 s not clear to me that the agent \u201c learns \u201d a new skill , since there is only exploration , and no exploitation . -Update- After reading all the other reviews and ensuing discussions , I maintain my original score . If the research question is `` How does the optimal policy depend on task parameters such as uncertainty and horizon ? '' I believe Bayes-adaptive work answers that question . If the question is `` How do policies learned by meta-RL algorithms compare to Bayes-optimal policies ? '' then I think more empiricism is needed ( since RL2 in principle can represent the Bayes-optimal policy ) , or a comparison of multiple methods .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed and constructive comments . We adapted the manuscript and hope that they are addressed appropriately : * * '' [ ... ] I am not convinced that the analogy of learning versus adapting in meta-learning maps neatly onto the concepts of nature versus nurture in biology . `` * * We do not aim to capture every dimension of the biological \u2018 nature versus nurture \u2019 debate and believe that this would require an unpractical amount of complexity . We have added a discussion section to clarify our claims and to relate our results to the related work in computational ethology : * '' [ ... ] The observed sharp transition between exploratory learning behavior ( s ) and hard-coded , non-learning strategies can be seen as a proof-of-concept example for a `` nature-nurture '' trade-off that adds new aspects to earlier work in theoretical ecology ( Stephens , 1991 ) . From this perspective of animal behavior , meta-learning with a finite time horizon could provide an inroad into understanding the benefits and interactions of instinctive and adaptive behaviors . Potential applications could be the meta-learning of motor skills in biologically inspired agents ( Merel et al. , 2019 ) or instinctive avoidance reactions to colours or movements . The degree of biological realism that can be reached will be limited by computational resources , but qualitative insights could be gained , e.g. , for simple instinctive behaviors . '' ( p. 9 ) * * * '' [ ... ] My main criticism of the work is that the results seem fairly obvious . `` * * We agree that post-hoc , the key results feel somewhat obvious , but nevertheless believe that they have consequences not only by providing a potential new workhorse for theoretical biology , but also for a broad range of meta-learning problems . The highly nonlinear , discontinuous dependence of the optimal solution on task parameters and lifetime -- i.e. , the amount of data available to the optimized learner on the job -- is a consequence of local maxima in the reward landscape . We now investigate this in more depth in figure 2 , which investigates the sensitivity of the bimodal solution space directly at the edge between \u201c learning \u201d and \u201c not learning \u201d regimes . We summarized the results of this new analysis in the following paragraph in the discussion : * '' [ ... ] A key take-home from our results is the highly nonlinear and potentially discontinuous dependence of the meta-learned strategy on the parameters of the task ensemble . For certain parameter ranges , the reward landscape of the meta-learning problem features several local maxima that correspond to different learning strategies . The relative propensity of these strategies to emerge over the course of meta-learning depends on the task parameters and on the initialization of the agent . Generally , this supports the notion that there is not a single inductive bias for a given task distribution . Rather , there could be a whole spectrum of inductive biases that are appropriate for different amounts of training data . Even for the same task setting , different training runs can result in qualitatively different solutions , providing a note of caution for interpretations drawn by pooling over ensembles of trained networks . '' ( p. 8 ) * * * '' [ ... ] Wouldn \u2019 t \u201c learning \u201d be defined as figuring out where the colored squares are ? Yet in the paper it seems to be defined as executing a policy that moves farther away from the start . : * * We believe that there may be a misunderstanding : The object locations vary between episodes/ \u2019 lives \u2019 and the optimized agent indeed first explores the environment & identifies a reward location . It then repeatedly goes to that object ( if this is optimal given the remaining lifetime ) . The rollouts in figure 7 only visualizes a single sampled MDP for three different agents with different training lifetimes . We provide additional episode rollout visualizations in an anonymous [ Google document ] ( https : //docs.google.com/document/d/1bnmIykdsOase4QPgh4LkH-LtuDczgRPbiKRSM3M42jc/edit ? usp=sharing ) . The agents use different strategies depending on their training lifetime and acquired inner-loop learning algorithm . * * '' [ ... ] This reviewer had to read that first paragraph about 5 times to understand the setup . `` * * We excuse the inconvenience of the convoluted bandit task description and have restructured the section to provide a \u2018 smoother \u2019 reader experience . Here is the updated and hopefully better to read excerpt : * '' [ ... ] To keep it simple , one of the two arms is deterministic and always returns a reward of 0 . The task distribution is represented by the variable expected reward of the other arm , which is sampled at the beginning of an episode , from a Gaussian distribution with mean -1 and standard deviation \u03c3_p , i.e.\u00b5 ~ N ( -1 , \u03c3_p^2 ) . The standard deviation \u03c3_p controls the uncertainty of the ecological niche . For \u03c3_p < < 1 , the deterministic arm is almost always the better option . For \u03c3_p > > 1 , the chances of either arm being the best in the given episode is largely even . [ ... ] '' ( p. 3 ) *"}, "1": {"review_id": "EGVxmJKLC2L-1", "review_text": "This paper provides an analysis of RNN-based meta learning approaches . In particular , it investigates the strategies learned via meta-learning , contrasting strategies involving task-dependent learning vs heuristic or hard-coded solutions . Empirical evidence in two sets of experiments , on a 2-armed bandit toy task and a grid-world navigation task , show that hard-coded strategies can be a function of training task distribution and task complexity as well as task horizon . While the experiments are simplistic , they provide a clear and thorough comparison of different agent behaviours across different training regimes . I enjoyed reading the paper and although the results are intuitive and unsurprising , they nicely emphasize the importance of environment and task design choices in strategies learned via memory-based meta-learning . Comments/questions : 1 . I would encourage the authors to use \u201c memory-based/RNN-based meta-learning \u201d instead of \u201c meta-learning \u201d to avoid confusion as these results might not apply more widely across different meta-learning approaches ( e.g.gradient based ) . 2.While the pattern of behaviour looks qualitatively similar across analytical and empirical results reported in Figure 1 , I wonder if it would be possible to quantitatively assess where they differ . 3.It would be nice to see error bars for the 5 independent training runs in Figure 5 . 4.In the Appendix A.2.3 it is mentioned that the discount factor is annealed from 0.8 to 1. within the first 800k episodes . Could you please expand if this was crucial to achieve your results or just an experimental choice ? Overall , I think this is an interesting contribution of perhaps limited scope but still valuable and could encourage interesting future research directions in memory-based meta-learning .", "rating": "7: Good paper, accept", "reply_text": "Thank you very much . We have modified the manuscript and hope your questions and suggestions are now addressed . Please see below for detailed replies : 1 . * * '' [ ... ] I would encourage the authors to use \u201c memory-based/RNN-based meta-learning \u201d instead of \u201c meta-learning \u201d to avoid confusion as these results might not apply more widely across different meta-learning approaches ( e.g.gradient based ) . `` * * Yes , we fully agree . The previously used meta-learning terminology may be confusing for the reader and we have replaced the wording to be more precise in the current version of the submission . ( See revised manuscript . ) 2. * * '' [ ... ] While the pattern of behaviour looks qualitatively similar across analytical and empirical results reported in Figure 1 , I wonder if it would be possible to quantitatively assess where they differ . `` * * We believe that this is a very important question and have further investigated the differences between analytical and empirical solutions . Given that those occur primarily on the edge between the learning and the no-learning regimes , we did a deeper analysis of this parameter regime . We now show that in that regime , the reward landscape has local maxima into which the gradient-based optimization falls . How often which local maximum is reached depends on the distance of the parameters from the edge . The results can be found in a new figure 2 . We also added a new paragraph discussing our findings : * '' [ ... ] In the Bayesian model , the edge between the two regimes is located at parameter values where the learning strategy and the non-learning strategy perform equally well . Because these two strategies are very distinct , we wondered whether the reward landscape for the memory-based meta-learner has two local maxima corresponding to the two strategies ( figure 2 ) . To test this , we trained $ N=1000 $ networks with different initial conditions , for task parameters close to the edge . We then evaluated for each network the number of explorative pulls of the stochastic arm , averaged across 100 episodes . The distribution of the number of explorative pulls across the 1000 networks shows i ) a peak at zero exploration and ii ) a broad tail of mean explorative pulls ( figure 2 ) , suggesting that there are indeed two classes of networks . One class never pulls the stochastic arm , i.e. , those networks adopt a non-learning strategy . The other class learns . For task parameters further away from the edge , this bimodality disappears . '' ( p. 5 ) * 3 . * * '' [ ... ] It would be nice to see error bars for the 5 independent training runs in Figure 5 . `` * * We added median and percentile bands to both figure 5 and 6 . The results remain robust and significant . ( See revised figures in the manuscript . ) 4. * * '' [ ... ] In the Appendix A.2.3 it is mentioned that the discount factor is annealed from 0.8 to 1. within the first 800k episodes . Could you please expand if this was crucial to achieve your results or just an experimental choice ? `` * * Indeed , the discount factor schedule is an important hyperparameter . While our theoretical result relies on a finite horizon and discount factor 1 , this setting is initially very challenging for the meta-learner , the main reason being the long time-scale of backpropagation through time when training the RNN . Instead , we chose to gradually extend the window of integration by increasing the discount factor . Thereby , we essentially treat the discount as a curriculum parameter ( Prokhorov and Wunsch , 1997 ) ."}, "2": {"review_id": "EGVxmJKLC2L-2", "review_text": "Summary : This paper observes that in meta-RL ( and evolutionary biology ) , sometimes it is advantageous to learn behaviors that adapt to the particular task , while other times not adapting to the task , and instead relying on a task-agnostic \u201c hard-coded \u201d behavior is sufficient . While much meta-RL research typically focuses on the former setting , this paper studies when it is not necessary to learn adaptive behaviors . Specifically , this paper presents three main findings : ( i ) whether or not it is optimal to learn adaptive behavior strongly depends on the horizon of the task and complexity of learning such adaptive behaviors \u2014 if the horizon is too short , or if the adaptive behavior requires complex exploration , then exploring the new task to learn adaptive behaviors may not be worth it ; ( ii ) existing meta-RL agents are capable of choosing not to learn adaptive behaviors , when it is optimal to do so ; ( iii ) existing meta-RL agents generalize poorly to tasks with varying horizon-lengths . Strengths : - Novelty . Most meta-RL papers study the case where adapting to a new task requires learning new behaviors . Instead , this paper studies when such adaptation is not necessary , which is an area that has not been well-studied . Therefore , it is interesting and novel to bring attention to the fact that both regimes exist . - Clarity and Execution . This paper is generally well-executed and clear . The bandit example in Section 3 clearly illustrates the two regimes that this paper studies : when it is necessary to learn adaptive behaviors vs. not , and the results are presented in figures that impressively , clearly illustrate the points made in the text . This section also convincingly shows that existing meta-RL agents roughly learn the Bayes-optimal policy in this case . Similarly , the grid world tasks in Section 4 also clearly illustrate how the behavior of meta-RL agents changes with the horizon of the task . The paper does a good job at thoroughly studying what happens when each parameter ( e.g. , horizon-length , aleatoric uncertainty , and epistemic uncertainty ) varies . Weaknesses : - Significance . My primary concern with this work is significance . I believe that the main insights of this paper can be interpreted through the lens of Bayes-adaptive policies [ 1 ] . The optimal Bayes-adaptive policy explains when it is optimal to explore a new task and learn adaptive behaviors , depending on the horizon length and amount of exploration needed , and therefore , the main claim of the paper can be framed as observing that existing meta-RL agents can learn the Bayes-adaptive optimal policy in simple tasks . This is an interesting observation , but prior work [ 2 ] already shows that meta-RL is equivalent to learning the Bayes-adaptive optimal policy . Furthermore , for harder tasks , this is likely to be confounded with the complexity of learning efficient exploration behaviors that allow learning adaptive behaviors , as shown by prior work [ 2 , 3 , 4 ] . Overall , it is therefore unclear to me what significant takeaways the meta-RL community can gain from this work . One potential takeaway is the observation that meta-RL agents generalize poorly to tasks with varying horizon-lengths , but it \u2019 s unclear whether this setting occurs in real tasks , and even if this _is_ necessary , then simply adding the horizon to the observation ( state ) and varying the horizon during meta-training seems sufficient . Overall , I found this paper to be a well-written , well-executed , illustration of when it is optimal to learn task-specific adaptive behaviors . However , due to its limited significance , I am unfortunately unable to recommend acceptance . Additional Questions / Comments : - In Figure 2 , what does it mean with \u201c 100 Suboptimal pulls ? \u201d Is this over the course of all training ? Also , why is the initial policy entropy at 0 ? I would expect that upon initialization , the policy entropy is not 0 . - I believe that the Bayes-adaptive optimal policy can also be analytically computed for the grid world tasks . It would be nice to report the optimal returns in Figure 5 . - The interplay between the term \u201c lifetime \u201d and episodes was not explicitly defined . It was possible to infer the paper \u2019 s intent from reading the experiments , but more carefully defining this could help . - Framing the exploration trade-off in terms of evolutionary biology is interesting . References : [ 1 ] Optimal Learning : Computational procedures for Bayes-adaptive Markov decision processes . Michael O \u2019 Gordon Duff . February 2002. https : //www.gatsby.ucl.ac.uk/~yael/Okinawa/DuffThesis.pdf [ 2 ] VariBAD : A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning . Luisa Zintgraf , Kyriacos Shiarlis , Maximilian Igl , Sebastian Schulze , Yarin Gal , Katja Hofmann , Shimon Whiteson . October 2019. https : //arxiv.org/abs/1910.08348 [ 3 ] Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables . Kate Rakelly , Aurick Zhou , Deirdre Quillen , Chelsea Finn , Sergey Levine . March 2019. https : //arxiv.org/abs/1903.08254 [ 4 ] Explore then Execute : Adapting without Rewards via Factorized Meta-Reinforcement Learning . Evan Zheran Liu , Aditi Raghunathan , Percy Liang , Chelsea Finn . June 2020. https : //openreview.net/forum ? id=La1QuucFt8- * * Edit : Score raised from 4 -- > 5 following discussion below . * *", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your clear comments . We would like to first take the opportunity to address the key question of significance . * * [ ... therefore , the main claim of the paper can be framed as observing that existing meta-RL agents can learn the Bayes-adaptive optimal policy in simple tasks . This is an interesting observation , but prior work [ 2 ] already shows that meta-RL is equivalent to learning the Bayes-adaptive optimal policy . ] `` Overall , it is therefore unclear to me what significant takeaways the meta-RL community can gain from this work . One potential takeaway is the observation that meta-RL agents generalize poorly to tasks with varying horizon-lengths ... '' * * We agree that the meta-learner effectively learns the Bayes-adaptive optimal policy , but we do not think that this is our main take-home message . On a general level , meta-learning can probably always be phrased as an algorithmic way of inferring a Bayes-optimal policy . Aside from suggesting meta-learning approaches as a principled approach to nature-nurture problems in evolutionary biology , this work aims at mapping out the dependence of the optimal policy on the task parameters . We believe that the meta-learning community has paid little attention to the effects of the meta-training distribution and the amount of the inner loop adaptation . The main take home is that these effects can be nonlinear to the point of discontinuity : The switch from the learning to the non-learning regime is a sharp one . The underlying reason is local maxima in the reward landscape ( evidence for that in Fig.2 of the revised manuscript ) . These types of discontinuity ( in seemingly unimportant parameters like lifetime ) is interesting not only from the perspective of evolutionary biology -- as sharp nature-nurture transitions -- but may well occur in a range of other meta-learning tasks and could have a strong impact on gradient-based meta-learning approaches . We try to better highlight these points in the revised version . To this , we added an additional analysis close to the transition edge to illustrate the presence and impact of these local maxima ( Fig.2 ) .We also added a paragraph to the discussion : * '' [ ... ] A key take-home from our results is the highly nonlinear and potentially discontinuous dependence of the meta-learned strategy on the parameters of the task ensemble . For certain parameter ranges , the reward landscape of the meta-learning problem features several local maxima that correspond to different learning strategies . The relative propensity of these strategies to emerge over the course of meta-learning depends on the task parameters and on the initialization of the agent . Generally , this supports the notion that there is not a single inductive bias for a given task distribution . Rather , there could be a whole spectrum of inductive biases that are appropriate for different amounts of training data . Even for the same task setting , different training runs can result in qualitatively different solutions , providing a note of caution for interpretations drawn by pooling over ensembles of trained networks . `` * * * '' Furthermore , for harder tasks , this is likely to be confounded with the complexity of learning efficient exploration behaviors that allow learning adaptive behaviors , as shown by prior work [ 2 , 3 , 4 ] '' * * Yes , limited generalization can of course occur for a variety of reasons , but is n't it interesting to know that there could be a deeper underlying reason why things do n't work as well as hoped for ? We found it interesting , for example , that the washed-out edge between learning and non-learning in the simulated RL2 agents is in fact not due to a gradual change in the policy , but due to changing probabilities of falling into totally different policies ( see newly added figure 2 ) . * * '' [ ... ] even if this is necessary , then simply adding the horizon to the observation ( state ) and varying the horizon during meta-training seems sufficient . `` * * That feels like a natural extension , but given that potentially very nonlinear dependencies of the optimal policy on the horizon , it may not be so easy to train . It would certainly be worth investigating such a horizon-dependent meta-learner , and be it to allow the use of the horizon as a gradually increasing curricular parameter and thereby provide a meta-learner that adapts very quickly on the job , without hampering its precision as more training data comes in ."}, "3": {"review_id": "EGVxmJKLC2L-3", "review_text": "* * Summary . * * The authors investigate the question of when the optimal behavior for an agent is to learn from experience versus when the optimal behavior is to apply the same ( memorized ) policy in every scenario . They begin by introducing a simple bandits environment wherein they derive the optimal policy and identify regimes in which it involves memorization vs. learning . Then they train an RL^2 agent and verify that it behaves as expected in these regimes . Next , they expand their approach to a slightly more complicated gridworld environment which does not have an analytic solution to the question . The agent behaves as expected in the gridworld environment . * * Strong points . * * This paper tackles a novel question which is fundamental to the field of metalearning . By carefully analyzing the two regimes of learning and memorization in the context of metalearning , this paper will increase awareness about the fact that the two regimes exist . The paper is clearly written and does an excellent job of putting experiments in the context of past ML research . The experimental setup is simple but goes straight to the heart of the issue . Figures and text do a good job of analyzing results and communicating them to the reader . Overall , this paper was an interesting read . * * Weak points . * * The idea that \u201c sometimes memorization is best and other times learning is best \u201d does border on the obvious . Indeed , as soon as the authors derive their analytical solution , it becomes clear that we can expect the RL^2 agent to learn the same behavior . For me , there were no surprises in the experimental sections . To the authors : was there anything that was surprising or not obvious to you ? What additional information can the experiments tell us , apart from confirming theoretical predictions ? Having said that , I also believe that very simple , well-executed research ideas sometimes make the best papers . This paper appears to be one of those cases . And even though the ideas are simple , they are significant and they are not a major part of the dialogue in the meta-learning community yet . So even if the ideas seem obvious , I think there is value in communicating them well . I have one concern about the bandit task setup : the authors adjust $ \\sigma_l $ , the width of the Gaussian from which they are sampling the reward , as a proxy for aleatoric uncertainty and hence task complexity . In doing so , they essentially equate \u201c stochasticity of the environment \u201d with \u201c task complexity. \u201d And yet , there are many other ways in which a task can be complex . Sometimes , all the information needed to perform a task is present and yet the task is difficult to solve because one needs to interpret/integrate the information in a particular way . This is why , for example , puzzles are considered difficult tasks . It is also why simulating the 3-body problem is a complex task . To the authors : can you clarify what you mean by \u201c task complexity \u201d ? In the closing paragraph of the paper , the authors claim that their approach \u201c allows us to study the emergence of inductive biases in biological systems \u201d but this claim is not supported by the rest of the paper , which makes almost no connections to biological systems . There are certainly ways in which these results are relevant to learning in biological systems , but the authors did not explore them in this paper , and so this claim is not well supported . In the same paragraph , they bring in contrasting notions of Darwinian and Lamarkian inheritance . Since they do this in one sentence -- the last sentence -- it is hard to understand what their claim is . And it was not clear that this was one of the main takeaways of the paper , as these concepts do not appear anywhere else in the paper . If the authors want to draw these conclusions , then they should add additional discussion on these topics . Otherwise , they risk misleading readers . One additional minor suggestion would be to invert the color scale of Figure 6 , as \u201c white - > red \u201d signifies values of increasing size in all preceding plots , but in Figure 6 it currently signifies values of decreasing size . Minor grammatical suggestions -- \u201c the question which aspects of behavior \u201c - > \u201c the question of which aspects of behavior \u201c -- When typing quotes in LaTex , use `` and \u2018 \u2019 instead of \u201c \u201d so as to make them open & close correctly -- \u201c interplay of the agent \u2019 s lifetime , \u201d - > \u201c interplay between the agent \u2019 s lifetime , \u201d -- \u201c We numerically show \u201d - > \u201c We show numerically \u201d -- \u201c as well as explicit models of memory \u201d - > \u201d and explicit models of memory \u201d ( same issue occurs later ) -- \u201c the agents does not have \u201d - > \u201c the agent does not have \u201d * * Recommendation . * * 6 : Marginally above acceptance threshold * * Reasoning . * * This paper is well written and the experimental setup is simple , well-executed , and produces results that are relevant to the main question of the paper . The main question of the paper -- when does it make more sense to learn vs. memorize a behavior -- is significant to ICLR and to the field of machine learning . There are a number of relatively minor weaknesses ( as described above ) but this is overall a nice paper and would be a good contribution to ICLR 2021 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks , we very much appreciate your thoughtful considerations as well as detailed corrections . We want to highlight some new analyses and comments to your points of concern : * * \u201c [ ... ] For me , there were no surprises in the experimental sections . To the authors : was there anything that was surprising or not obvious to you ? \u201d * * Honestly , we were initially surprised by the fact that the transition to the non-learning parameter regime is an abrupt one , and we believe that this has consequences for meta-learning on a more general level . The underlying reason for the sharp transition is a bimodality in the reward landscape . The transition occurs where the global optimum switches from one of those local maxima to another , as for most phase transitions . As a result , the meta-learned strategy has a highly nonlinear and discontinuous parameter dependence and for gradient-based approaches , local minima are obviously an issue . To highlight this point , we now provide a further in-depth analysis of the transition regime between learning and non-learning . We show that in this regime , different initializations lead to fundamentally different solutions . We have added a completely new figure 2 including a discussion ( please see reply to comment 2 of R2 ) , as well as a discussion paragraph : * '' [ ... ] A key take-home from our results is the highly nonlinear and potentially discontinuous dependence of the meta-learned strategy on the parameters of the task ensemble . For certain parameter ranges , the reward landscape of the meta-learning problem features several local maxima that correspond to different learning strategies . The relative propensity of these strategies to emerge over the course of meta-learning depends on the task parameters and on the initialization of the agent . Generally , this supports the notion that there is not a single inductive bias for a given task distribution . Rather , there could be a whole spectrum of inductive biases that are appropriate for different amounts of training data . Even for the same task setting , different training runs can result in qualitatively different solutions , providing a note of caution for interpretations drawn by pooling over ensembles of trained networks . '' ( p. 8 ) * * * '' [ ... ] There are many other ways in which a task can be complex . Sometimes , all the information needed to perform a task is present and yet the task is difficult to solve because one needs to interpret/integrate the information in a particular way . `` * * We agree , and tried to be a bit more precise in our definition of task complexity . An exploration of different types of task complexity is beyond the present paper : * '' Task complexity : How long does it take to learn the optimal strategy for the task at hand ? Note that this could be different from the time it takes to execute the optimal strategy . '' ( p. 2 ) * We have also changed the description of the bandit task to better explain our choices and clean up the admittedly confusing terminology in the earlier version of manuscript . Here the relevant excerpt for task complexity : * '' While the mean \u00b5 remains constant for the lifetime T of the agent , the reward obtained in a given trial is stochastic and is sampled from a second Gaussian , r ~ N ( \u00b5 , \u03c3_l ) . [ ... ] The standard deviation \u03c3_l hence controls how quickly the agent can learn the optimal policy . We therefore use it as a proxy for task complexity. \u201d ( p. 3 ) * * * \u201c [ ... ] There are certainly ways in which these results are relevant to learning in biological systems , but the authors did not explore them in this paper , and so this claim is not well supported. \u201d * * We also agree that the claim of studying inductive biases in real biological systems is too vague and have significantly adapted the manuscript . Nonetheless , we believe that future in-silico meta-learning approaches can provide testable predictions for experimental evolution . This includes the interesting line of work on experimental evolution such as simulating up to 50 generations of drosophila evolution ( e.g.Dunlap and Stephens , 2016 ; Marcus et al. , 2018 ) . We added a discussion paragraph and scaled down our claims : * '' [ ... ] From this perspective of animal behavior , meta-learning with a finite time horizon could provide an inroad into understanding the benefits and interactions of instinctive and adaptive behaviors . Potential applications could be the meta-learning of motor skills in biologically inspired agents ( Merel et al. , 2019 ) or instinctive avoidance reactions to colours or movements . The degree of biological realism that can be reached will be limited by computational resources , but qualitative insights could be gained , e.g. , for simple instinctive behaviors . '' ( p. 9 ) * Finally , we thank the reviewer for the detailed corrections . We apologize for the grammatical faux pas and have corrected these as well as adapted the color scheme of figure 6 in the current version of the submission manuscript ."}}