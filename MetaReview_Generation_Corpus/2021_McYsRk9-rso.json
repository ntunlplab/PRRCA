{"year": "2021", "forum": "McYsRk9-rso", "title": "Reducing Implicit Bias in Latent Domain Learning", "decision": "Reject", "meta_review": "While all reviewers agree that the topic is interesting and the work has merit, several issues have been pointed out, especially by R1 and R3, that indicate that the work is not  ready for acceptance at this stage. the authors are strongly encouraged to continue to work on this topic, taking into account the feedback received.", "reviews": [{"review_id": "McYsRk9-rso-0", "review_text": "Update after discussion with authors I would like to thanks the author for their efforts by adding additional experiments , which surely enhances the significance of the proposed approach . Based on these , I increased my score to 5 . I have re-checked the final revised version , I think the current version still * requires proper organizations and justifications * . For example , the added experiments still talked about the accuracy , the in-depth analysis seems lacking . I think a substantial revision of the paper in terms of structure , idea presentation , and analysis is still needed . Based on this , my final score is 5 . -- Summary : This paper studied how to learn a neural network with multiple domains without knowing the exact domain label ( by merging all the domains as a large domain ) . Then they proposed dynamic residual adapters and weighted domain transfer to address this issue . The empirical results showed its practical benefits . Overall review Pros : [ 1 ] This paper is well-motivated . I like the analyzed scenario and I think it can have a strong practical utility . [ 2 ] The high level of proposed ideas is technically sound . Cons : [ 1 ] The submitted version seems to be a preliminary version with many missing and unclear elements . [ 2 ] As an * * empirical * * paper , the experimental results are not sufficient for ICLR . [ 3 ] Some technical details need better justifications and discussions . Based on these , I recommend a rejection at this time but encourage a major revision for resubmission . - Detailed explanations [ 1 ] Missing elements [ a ] I am rather confused and unclear about the whole learning procedure . It seems the author used DRA in the residual module . However , the role of WDT is unclear . What is the global training loss in the proposed approach ? WDT is a part of the loss or used for analyzing the problem ? I would like to see a pseudocode/protocol for the whole algorithm or a clear network structure for illustrating the idea in Sec 3.3-3.4 . [ b ] The mathematical notations defined in this paper are presented oddly ( particularly in sec 3 ) for example : [ b1 ] Equation ( 1 ) , $ \\alpha $ and $ \\theta $ are scalars or vectors ? what is meaning for $ |\\alpha|\\ll |\\theta| $ ? I guess it is $ \\text { dim } ( \\alpha ) $ but it makes me rather confused . [ b2 ] The same problem for eq ( 2 ) and $ \\epsilon $ [ b3 ] In WDT , the same problem for $ \\delta $ , $ \\delta_i $ and $ \\delta_j $ These confusions make it more difficult to understand the approach . [ 2 ] The empirical results The current empirical results only compare MLFN , which is not sufficient . I noticed the author claimed , \u201c Note the goal here is not to compare to the performance of existing multi-domain approaches that Visual Decathlon was designed for , but to show that deep networks struggle with learning small latent domains when no domain annotations are provided. \u201d I agree with this opinion if the paper aims to only analyze this scenario ( generally from a theoretical perspective ) . These kinds of experiments are sufficient . By contrast , the current version aims at * * proposing a new empirical approach for the real-world practice * * , which is not sufficient . I would like to see a * * strong practical result * * either outperforming the recent baselines or applying in many real-world problems . [ 3 ] Technical details [ a ] I suggest not using the term \u201c domain labels \u201d since it can be confusing to label $ y $ information in the unsupervised domain adaptation . I think \u201c domain index \u201d or \u201c task index \u201d are better choices . [ b ] The visualization of $ \\delta $ sounds interesting but I can not understand the meaning . A better explanation is expected . [ c ] Fig ( 1 ) , ( 2 ) why PCA visualization ? Why not Tsne ? [ d ] The benefits of self-attention are unclear . More analysis ( not numerical accuracy ) is expected . -- Suggestions I suggest a major revision on the proposed approach , empirical results , and more analysis ( not accuracy ) on the benefits of the idea .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear reviewer , many thanks for your review . We go through your comments point by point . * \u201c I am rather confused and unclear about the whole learning procedure . [ .. ] WDT is a part of the loss or used for analyzing the problem ? \u201d * We have pointed this out at various points throughout the paper \u2013 * including * the abstract : WDT is an augmentation strategy . Just like other strategies ( grayscaling , flipping , or MixUp ) this does not explicitly appear in the loss function . WDT is needed in latent domain learning because strategies that work well for standard classification problems ( such as MixUp ) do not work here ( see Table 5 ) . * \u201c The mathematical notations defined in this paper are presented oddly [ .. ] \u201d * We follow standard ML notation for all parts of our paper , distances \u03b4 for example are always scalar by their definition ; \u03f5 is a channel-sized vector ( as we state on page 4 ) . At your request , we did go ahead and clarified notation further in our revision , e.g.replaced | * * x * * | with dim ( * * x * * ) , and have introduced bold symbols for vector-valued objects such as * * x * * . * \u201c The current empirical results only compare MLFN , which is not sufficient [ .. ] I would like to see a strong practical result [ .. ] outperforming recent baselines or applying in many real-world problems. \u201d * As we state on page 7 paragraph 4 , there currently exist no customized end-to-end solutions for latent domain learning in the literature . If there is any approach you feel we \u2019 ve overlooked here , please point them out . Moreover , the goal of this paper is very clear : to * * make a * first * step toward models that learn reliably in the presence of latent domains * * , by preserving small latent domains in the dataset . Contrary to other work that expands on previous learning settings ( multi-domain , domain adaptation , etc . ) , this requires a significant extra effort in terms of motivating the problem . We do this here through an extended introduction , and furthermore present multiple qualitative insights . It is * * standard protocol * * in the multi-domain literature ( compare Rebuffi et al . ( 2017 , 2018 ) , Guo et al . ( 2019 ) , Liu et al . ( 2019 ) , etc . ) to use datasets like PACS or Visual Decathlon as a proxy for the real-world . As such , the criticism that there is no \u201c real world experiments \u201d has to be put into perspective \u2013 we are not using toy data after all , and e.g.Visual Decathlon is a * * very * * complex dataset . * \u201c The visualization of \u03b4 sounds interesting but I can not understand the meaning . A better explanation is expected. \u201d * WDT is a pairwise augmentation . This figure visualizes how much exchange occurs on average by each latent domain . In other words : how much parameter sharing happens between some latent domain ( e.g.VD-Omniglot ) and the other ones . For example : some domains ( e.g.PACS-sketch ) are quite isolated in feature space ( see Fig.3 left ) , and we show that this directly translates to a * * lower average \u03b4 in WDT = less parameter sharing with other PACS domains * * . These visualizations confirm that WDT does what it was designed for . * \u201c Fig ( 1 ) , ( 2 ) why PCA visualization ? Why not Tsne ? \u201d * Because when features are separable by the most simple , * linear * unsupervised mechanism ( which is PCA , not the * nonlinear * t-SNE ) , then this is a much stronger statement that there is * meaningful separation * within DRA . * \u201c The benefits of self-attention are unclear . More analysis ( not numerical accuracy ) is expected. \u201d * We firmly establish the benefits of self-attention . We analyze alternatives to self-attention in Table 5 , and in particular find that Gumbel-softmax negatively impacts performance . We also compare to K=1 , i.e.having no self-attention . We hope we were able to remedy your concerns in our updated revision . If there \u2019 s anything else you would like us to revisit , please let us know ."}, {"review_id": "McYsRk9-rso-1", "review_text": "1.Summarize what the paper claims to contribute . Be positive and generous . The paper claims to contribute a new method * dynamic residual adapters ( DRA ) * coupled with * weighted domain transfer ( WDT ) * to tackle * latent domain learning . * The proposed method improves the model performance against small domain datasets without hurting the model performance against large domain datasets in two different settings : ( 1 ) multi-domain setting ( 10 different tasks with 10 different domain per task ) ( 2 ) multi-style setting ( 1 task with 4 different styles ) Impressive empirical results ! I especially enjoyed reading PCA graphs and its qualitative analyses . 2.List strong and weak points of the paper . Be as comprehensive as possible . ( 1 ) Strengths a. Exhaustive empirical analyses . ( ablation tests and tests with varying K value ) b. Qualitative analyses backed by graphs like PCA and example images . c. Impressive accuracy improvements . d. Intuitive theoretical explanation . Cool idea to use a gate to make RA dynamic . ( 2 ) Weaknesses a . Skipped the math that yields the equation ( 3 ) in the section 3 . Would be nice if the steps are attached as an Appendix . b.In Table 1 , there is a data domain size metric : $ \\pi_ { d } $ . Can you add how this is computed ? Also , how small is the smallest data in sheer number of examples ? c. Table 1 has RestNet56 , but Table 2 does n't . Why did you make this choice of experiment design ? d. With PACS dataset , you have experimented with the model ( k-means+RA ) . I did n't quite understand the model setup and the motivation . In my understanding , the model learned the latent domain labels via k-means . And , then , based on this pseudo domain labels , the model is fine-tuned with Residual Adapter applied . Did I understand the model correctly ? What is the motivation of doing this ? I am not sure if this is a fair comparison between the RA and the proposed DRA+WDT methods . It seems rather a comparison between DRA+WDT and K-Means . e. In Figure 3 , the paper says `` Middle : WDT exchange or different domains , sketch is particularly inactive . '' I had a bit of difficulty parsing what `` inactive '' means here because the `` Middle '' figure is about `` WDT Exchange Strength '' and because `` sketch '' has the largest strength . Can you explain what `` inactive '' here means ? f. Based on Table 3 , the positive effect of WDT is not strong . I wonder if WDT is necessary . g. It seems to me that the strength of the proposed method is much more evident in the second problem type ( PACS ) where the task is the same across different domains . In the first problem type ( Visual Decathlon ) , the DRA+WDT 's performance boost is not consistent across different domains . I see that DRA+WDT hurts the performance compared to the baseline ResNet26/56 on a few different domains , such as Daim. , Gtsrb , Omn. , and Svhn . $ \\pi_ { d } $ values of the domains of PACS are greater than those of Visual Decathlon , excluding svhn . Why do you think that is ? When should one use or not use DRA+WDT in order to avoid hurting the model performance ? h. Minor formatting issues . See 6 below . 3.Clearly state your recommendation ( accept or reject ) with one or two key reasons for this choice . Accept because of the impressive performance of the proposed method against PACS ( single task , multi domain ) with clear visual analyses . Meantime , 2 . ( 2 ) .g requires more explanation to make the paper 's claim stronger . 4.Provide supporting arguments for your recommendation . See 2 . ( 1 ) Strengths , 3 , and 2 . ( 2 ) .g above . 5.Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment . See 2 . ( 2 ) Weaknesses above . 6.Provide additional feedback with the aim to improve the paper . Make it clear that these points are here to help , and not necessarily part of your decision assessment . ( 1 ) The indentation of Table 5 seems to be inconsistent with the rest of the tables in the paper . ( 2 ) In Figure 3 , the paper mentions $ P_ { k } $ without the denotation explained explicitly in any of the main body of the text . I had to re-read the paper to find a footnote 5 to finally understand what this denotation meant . It would be good to briefly explain this denotation in the same description of Figure 3 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer , many thanks for your suggestions . We address them point-by-point below . * '' Skipped the math that yields the equation ( 3 ) in the section 3 . `` * As requested , we have reorganized this section to clarify those points . Please let us know in case any concerns remain ! * '' In Table 1 , there is a data domain size metric : $ \\pi_d $ . Can you add how this is computed ? Also , how small is the smallest data in sheer number of examples ? `` * $ \\pi_d $ indicates the relative share of each ( hidden ) domain , i.e.we compute it as $ N_d/N $ , where $ N_d $ denotes the number of examples belonging to latent domain $ d $ , and $ N $ is the sum over all domains ( we also mention this in the first paragraph of Section 3.1 ) . Note that we only compute $ \\pi_d $ for the analysis \u2013 the model has absolutely no knowledge of which domains are large/small , nor how many there are . The smallest domains are : 1.6k ( PACS-photo ) and 2k ( Vgg-Flowers ) . * '' Can you explain what `` inactive '' here means ? `` * WDT is a pairwise augmentation , and the exchange strength is a measure of how similar each latent domain is . In other words : how much parameter sharing occurs between some latent domain ( e.g.VD-Omniglot ) and the other ones . For example : PACS-sketch is relatively isolated in feature space ( see Fig.3 left ) , and `` inactive '' here means the average exchange strength is relatively low , i.e . * * a lower average \u03b4 in WDT = fewer parameters shared with other PACS domains = an inactive domain . * * * '' Table 1 has RestNet56 , but Table 2 does n't . Why did you make this choice of experiment design ? `` * That \u2019 s a great suggestion , we have added results for ResNet56 to Table 2 . However , the improvement over ResNet26 is only marginal ( 92.70 - > 93.35 ) and significantly smaller than the improvement via DRA ( 92.70 - > 94.76 ) . Note DRA also uses around 40 % less parameters than ResNet56 . This result once again confirms that * * performance on small latent domains is suppressed * * , and * * adding more layers * * \u2013 the preferred option for standard classification \u2013 * * doesn \u2019 t solve this problem * * . Please let us know of any concerns that remain , so that we may address them ."}, {"review_id": "McYsRk9-rso-2", "review_text": "Summary : The authors propose a method for latent domain learning , where input data come from different domains and the domain labels are unknown . The proposed method consists of two parts : dynamic residual adapter and weighted domain transfer . The dynamic residual adapter acts as a mixture of expert layer . And the weighted domain transfer which augments the dataset by interpolating between different input pairs . Empirical results show that when combined together , the proposed method perform better than training a regular model . Pros : 1.Latent domain discovery is a very interesting topic . 2.Empirical results show that the method brings improvement to minority domains . Cons : 1.Maybe I missed something , but I do n't there are new insights in the paper . The proposed dynamic residual adapter is just an instance of MoE [ 1 ] with adapters , which I think should be a baseline in the experiments . 2 . `` Section 3.4 Weighted Domain Transfer '' is not well-motivated and very confusing . Here you want to interpolate between x_i and x_j . But why do you compute the difference between the input x_i and the feature \\mu_i in equation 3 ? Are they comparable with each other ? What is the goal that you want to achieve here ? Other comments : 1 . I think the introduction describes the problem too much , leaving it no space to expand your idea and intuition . For example , you start describing your idea at the very last paragraph . 2.When creating the augmented examples , can you leverage the gate information that you produced from the MoE ? 3.It will be more helpful to understand the DRA component if you can provide PCA over the original activations . [ 1 ] OUTRAGEOUSLY LARGE NEURAL NETWORKS : THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear Reviewer , thank you for your comments , which we address point by point below . * \u201c Maybe I missed something , but I do n't there are new insights in the paper . The proposed dynamic residual adapter is just an instance of MoE [ 1 ] with adapters , which I think should be a baseline in the experiments. \u201d * We make several novel contributions in this paper . 1.The most important insight is the * * identification of a new problem * * that is pervasive yet not generally known by the community . Specifically , we present clear empirical evidence that * * standard deep models underperform when latent domains are present due to suppressing small domains * * . This is a * fundamental problem with the generalization of deep architectures * , and we have included results ( page 9 of our revision ) that demonstrate it even extends to standard classification problems , e.g.on CIFAR-10 ( see Table 4 ) . 2.We find that standard augmentation strategies do not remedy this problem . Because of this , * * we propose WDT , a new augmentation strategy that reduces bias against small latent domains * * . 3.As per MoE : this is simply an elegant way of extending multi-domain approaches to latent domains , where we can not use off-the-shelf multi-domain methods ( since there are no domain labels to speak of , see page 4 paragraph 3 ) . Because there is no domain information , we * * propose DRA , an alternative way to target corrections purpose-built for latent domain learning * * . Please note DRA is not necessarily an instance of ( I ) MoE \u2013 we explored several alternatives , such as ( II ) Gumbel-softmax mechanisms that we compare to in Table 5 , or ( III ) fixed assignments ( e.g.after applying latent domain discovery e.g.through k-means , see Table 2 ) . * '' Here you want to interpolate between $ x_i $ and $ x_j $ . But why do you compute the difference between the input $ x_i $ and the feature $ \\mu_i $ in equation 3 ? `` * We discuss the motivation around WDT on page 5 paragraph 2 : for latent domains , we do not want to exchange between samples that are visually too similar . This has a straightforward motivation : exchanging too much information between samples * can * reduce the number of discriminative features for samples from the same class . What we want instead is to encourage information exchange between different ( hidden ) domains , so as to end up with a model that learns concepts of objects ( cats , dogs , etc . ) that are invariant to the ( latent ) domain . Please note the difference $ x_i-\\mu_i $ is further scaled by the standard deviation after which $ \\mu_j $ is added . This process is crucial and exactly responsible for the * transfer of statistical information from $ x_j\\to x_i $ * . * '' I think the introduction describes the problem too much , leaving it no space to expand your idea and intuition . For example , you start describing your idea at the very last paragraph . `` * We maintain a longer than usual introduction is needed , because an important aspect in this paper is to * * establish latent domain learning as a novel learning setting * * . This requires we explain the latent domain scenario in sufficient detail , in large part so that future work won \u2019 t have to . Feel free to check out the long introduction in the initial work on multi-task learning by e.g.Caruana ( 1997 ) . While it might be a bit obvious nowadays , it is still recommended literature for anyone that is starting out in the field . * '' When creating the augmented examples , can you leverage the gate information that you produced from the MoE ? `` * We agree this is an interesting idea , as having access to richer conditional information within DRA could potentially enhance the internal clustering of latent domains . However , in doing so the method would no longer be end-to-end , as we would require a full pass through the network to collect gate activations . The goal of our manuscript \u2013 as stated on page 2 paragraph 1 \u2013 is to contribute a principled end-to-end mechanism that doesn \u2019 t have a large negative impact on runtimes ( e.g.we want to avoid 9x models ) . Requiring two passes would have moved the paper in a very different direction . * '' It will be more helpful to understand the DRA component if you can provide PCA over the original activations . `` * Please note in Figure 2 we do not display PCA over activations . We collect * only the gate activations * across the ResNet26 , and then reduce this to two dimensions with a linear mechanism ( i.e.PCA ) .This shows : * individual residual adaptations * can * be attributed to latent domains . * * * DRA shares between similar domains * * , which provides an * * explanation * * of its much better performance than RA etc . We went ahead and added some clarification around this in the final paragraphs of page 6 . Please let us know in case there \u2019 s anything else you would like us to revisit ."}, {"review_id": "McYsRk9-rso-3", "review_text": "The paper describes dynamic residual adapters designed to adaptively account for latent domains , and weighted domain transfer . This framework injects adaptivity into networks , preventing them from overfitting to the largest domains in distributions , a failure mode of traditional models that are exposed in latent domain learning . The approach closes a large amount of the performance gap to domain-supervised solutions . This paper is well written . The motivation of this paper is clear and the proposed framework does break the limitation of the existing deep learning methods . Below I present some suggestions , which hopefully can help the authors improve their study : The authors do not describe any processing they have done of the data . This should be clearly included in the methods section . More experimental details and insightful discussions should be provided . I suggest the authors repeat the benchmarking using a selection of datasets more similar to those used in current studies . The benchmarking results are insufficiently described in the text and can only really be seen in the tables/figures . The authors should explain the results in more detail in the text and include their interpretation of the results . Section 3 . In this part , the authors introduce the algorithm . Since there are many formulas in this section , additional explanations on the learning procedure would help understand the proposed method .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your constructive feedback . We address each point below . * \u201c The authors do not describe any processing they have done of the data . This should be clearly included in the methods section. \u201d * We do discuss this on page 6 paragraph 3 . As we mention there , we only use standard augmentations ( flipping , normalization ) in * all * of our experiments , and otherwise make no changes to the benchmark data . * '' The authors should explain the results in more detail in the text and include their interpretation of the results . `` * We extensively discuss our results , and provide a large set of experiments for two latent domain settings , with ( I ) joint and ( II ) disjoint label spaces . Moreover , for both settings , we provide ( III ) qualitative insights , as well as ( IV ) thorough discussions of the computational requirements . Please note the 8 page limit prevented us from including additional analysis in the main paper , but we included an extensive study of DRA around ( V ) single dataset performance on CIFAR-10 and CIFAR-100 ( Table 3 ) , ( VI ) robustness to class imbalance ( Table 4 ) , as well as ( VII ) ablations ( Tables 5-8 ) in the appended pages . We have taken advantage of the 9 page limit for the revision , and following your suggestion included additional analysis ( V ) + ( VI ) ( which were previously in the appendix ) in the main parts of the paper . On page 9 we demonstrate that the benefits of DRA even extend to standard classification tasks , and DRA brings * significant * performance advantages when some classes are underrepresented in the data . The last case addresses a particularly severe issue for standard deep learning models , as we display in Table 4 for ResNet26+56 . * '' In this part , the authors introduce the algorithm . Since there are many formulas in this section , additional explanations on the learning procedure would help understand the proposed method . `` * We have reorganized the section , and added additional discussions around WDT in our revision . Thank you again for your constructive feedback . Please point out any remaining concerns so that we may address them ."}], "0": {"review_id": "McYsRk9-rso-0", "review_text": "Update after discussion with authors I would like to thanks the author for their efforts by adding additional experiments , which surely enhances the significance of the proposed approach . Based on these , I increased my score to 5 . I have re-checked the final revised version , I think the current version still * requires proper organizations and justifications * . For example , the added experiments still talked about the accuracy , the in-depth analysis seems lacking . I think a substantial revision of the paper in terms of structure , idea presentation , and analysis is still needed . Based on this , my final score is 5 . -- Summary : This paper studied how to learn a neural network with multiple domains without knowing the exact domain label ( by merging all the domains as a large domain ) . Then they proposed dynamic residual adapters and weighted domain transfer to address this issue . The empirical results showed its practical benefits . Overall review Pros : [ 1 ] This paper is well-motivated . I like the analyzed scenario and I think it can have a strong practical utility . [ 2 ] The high level of proposed ideas is technically sound . Cons : [ 1 ] The submitted version seems to be a preliminary version with many missing and unclear elements . [ 2 ] As an * * empirical * * paper , the experimental results are not sufficient for ICLR . [ 3 ] Some technical details need better justifications and discussions . Based on these , I recommend a rejection at this time but encourage a major revision for resubmission . - Detailed explanations [ 1 ] Missing elements [ a ] I am rather confused and unclear about the whole learning procedure . It seems the author used DRA in the residual module . However , the role of WDT is unclear . What is the global training loss in the proposed approach ? WDT is a part of the loss or used for analyzing the problem ? I would like to see a pseudocode/protocol for the whole algorithm or a clear network structure for illustrating the idea in Sec 3.3-3.4 . [ b ] The mathematical notations defined in this paper are presented oddly ( particularly in sec 3 ) for example : [ b1 ] Equation ( 1 ) , $ \\alpha $ and $ \\theta $ are scalars or vectors ? what is meaning for $ |\\alpha|\\ll |\\theta| $ ? I guess it is $ \\text { dim } ( \\alpha ) $ but it makes me rather confused . [ b2 ] The same problem for eq ( 2 ) and $ \\epsilon $ [ b3 ] In WDT , the same problem for $ \\delta $ , $ \\delta_i $ and $ \\delta_j $ These confusions make it more difficult to understand the approach . [ 2 ] The empirical results The current empirical results only compare MLFN , which is not sufficient . I noticed the author claimed , \u201c Note the goal here is not to compare to the performance of existing multi-domain approaches that Visual Decathlon was designed for , but to show that deep networks struggle with learning small latent domains when no domain annotations are provided. \u201d I agree with this opinion if the paper aims to only analyze this scenario ( generally from a theoretical perspective ) . These kinds of experiments are sufficient . By contrast , the current version aims at * * proposing a new empirical approach for the real-world practice * * , which is not sufficient . I would like to see a * * strong practical result * * either outperforming the recent baselines or applying in many real-world problems . [ 3 ] Technical details [ a ] I suggest not using the term \u201c domain labels \u201d since it can be confusing to label $ y $ information in the unsupervised domain adaptation . I think \u201c domain index \u201d or \u201c task index \u201d are better choices . [ b ] The visualization of $ \\delta $ sounds interesting but I can not understand the meaning . A better explanation is expected . [ c ] Fig ( 1 ) , ( 2 ) why PCA visualization ? Why not Tsne ? [ d ] The benefits of self-attention are unclear . More analysis ( not numerical accuracy ) is expected . -- Suggestions I suggest a major revision on the proposed approach , empirical results , and more analysis ( not accuracy ) on the benefits of the idea .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear reviewer , many thanks for your review . We go through your comments point by point . * \u201c I am rather confused and unclear about the whole learning procedure . [ .. ] WDT is a part of the loss or used for analyzing the problem ? \u201d * We have pointed this out at various points throughout the paper \u2013 * including * the abstract : WDT is an augmentation strategy . Just like other strategies ( grayscaling , flipping , or MixUp ) this does not explicitly appear in the loss function . WDT is needed in latent domain learning because strategies that work well for standard classification problems ( such as MixUp ) do not work here ( see Table 5 ) . * \u201c The mathematical notations defined in this paper are presented oddly [ .. ] \u201d * We follow standard ML notation for all parts of our paper , distances \u03b4 for example are always scalar by their definition ; \u03f5 is a channel-sized vector ( as we state on page 4 ) . At your request , we did go ahead and clarified notation further in our revision , e.g.replaced | * * x * * | with dim ( * * x * * ) , and have introduced bold symbols for vector-valued objects such as * * x * * . * \u201c The current empirical results only compare MLFN , which is not sufficient [ .. ] I would like to see a strong practical result [ .. ] outperforming recent baselines or applying in many real-world problems. \u201d * As we state on page 7 paragraph 4 , there currently exist no customized end-to-end solutions for latent domain learning in the literature . If there is any approach you feel we \u2019 ve overlooked here , please point them out . Moreover , the goal of this paper is very clear : to * * make a * first * step toward models that learn reliably in the presence of latent domains * * , by preserving small latent domains in the dataset . Contrary to other work that expands on previous learning settings ( multi-domain , domain adaptation , etc . ) , this requires a significant extra effort in terms of motivating the problem . We do this here through an extended introduction , and furthermore present multiple qualitative insights . It is * * standard protocol * * in the multi-domain literature ( compare Rebuffi et al . ( 2017 , 2018 ) , Guo et al . ( 2019 ) , Liu et al . ( 2019 ) , etc . ) to use datasets like PACS or Visual Decathlon as a proxy for the real-world . As such , the criticism that there is no \u201c real world experiments \u201d has to be put into perspective \u2013 we are not using toy data after all , and e.g.Visual Decathlon is a * * very * * complex dataset . * \u201c The visualization of \u03b4 sounds interesting but I can not understand the meaning . A better explanation is expected. \u201d * WDT is a pairwise augmentation . This figure visualizes how much exchange occurs on average by each latent domain . In other words : how much parameter sharing happens between some latent domain ( e.g.VD-Omniglot ) and the other ones . For example : some domains ( e.g.PACS-sketch ) are quite isolated in feature space ( see Fig.3 left ) , and we show that this directly translates to a * * lower average \u03b4 in WDT = less parameter sharing with other PACS domains * * . These visualizations confirm that WDT does what it was designed for . * \u201c Fig ( 1 ) , ( 2 ) why PCA visualization ? Why not Tsne ? \u201d * Because when features are separable by the most simple , * linear * unsupervised mechanism ( which is PCA , not the * nonlinear * t-SNE ) , then this is a much stronger statement that there is * meaningful separation * within DRA . * \u201c The benefits of self-attention are unclear . More analysis ( not numerical accuracy ) is expected. \u201d * We firmly establish the benefits of self-attention . We analyze alternatives to self-attention in Table 5 , and in particular find that Gumbel-softmax negatively impacts performance . We also compare to K=1 , i.e.having no self-attention . We hope we were able to remedy your concerns in our updated revision . If there \u2019 s anything else you would like us to revisit , please let us know ."}, "1": {"review_id": "McYsRk9-rso-1", "review_text": "1.Summarize what the paper claims to contribute . Be positive and generous . The paper claims to contribute a new method * dynamic residual adapters ( DRA ) * coupled with * weighted domain transfer ( WDT ) * to tackle * latent domain learning . * The proposed method improves the model performance against small domain datasets without hurting the model performance against large domain datasets in two different settings : ( 1 ) multi-domain setting ( 10 different tasks with 10 different domain per task ) ( 2 ) multi-style setting ( 1 task with 4 different styles ) Impressive empirical results ! I especially enjoyed reading PCA graphs and its qualitative analyses . 2.List strong and weak points of the paper . Be as comprehensive as possible . ( 1 ) Strengths a. Exhaustive empirical analyses . ( ablation tests and tests with varying K value ) b. Qualitative analyses backed by graphs like PCA and example images . c. Impressive accuracy improvements . d. Intuitive theoretical explanation . Cool idea to use a gate to make RA dynamic . ( 2 ) Weaknesses a . Skipped the math that yields the equation ( 3 ) in the section 3 . Would be nice if the steps are attached as an Appendix . b.In Table 1 , there is a data domain size metric : $ \\pi_ { d } $ . Can you add how this is computed ? Also , how small is the smallest data in sheer number of examples ? c. Table 1 has RestNet56 , but Table 2 does n't . Why did you make this choice of experiment design ? d. With PACS dataset , you have experimented with the model ( k-means+RA ) . I did n't quite understand the model setup and the motivation . In my understanding , the model learned the latent domain labels via k-means . And , then , based on this pseudo domain labels , the model is fine-tuned with Residual Adapter applied . Did I understand the model correctly ? What is the motivation of doing this ? I am not sure if this is a fair comparison between the RA and the proposed DRA+WDT methods . It seems rather a comparison between DRA+WDT and K-Means . e. In Figure 3 , the paper says `` Middle : WDT exchange or different domains , sketch is particularly inactive . '' I had a bit of difficulty parsing what `` inactive '' means here because the `` Middle '' figure is about `` WDT Exchange Strength '' and because `` sketch '' has the largest strength . Can you explain what `` inactive '' here means ? f. Based on Table 3 , the positive effect of WDT is not strong . I wonder if WDT is necessary . g. It seems to me that the strength of the proposed method is much more evident in the second problem type ( PACS ) where the task is the same across different domains . In the first problem type ( Visual Decathlon ) , the DRA+WDT 's performance boost is not consistent across different domains . I see that DRA+WDT hurts the performance compared to the baseline ResNet26/56 on a few different domains , such as Daim. , Gtsrb , Omn. , and Svhn . $ \\pi_ { d } $ values of the domains of PACS are greater than those of Visual Decathlon , excluding svhn . Why do you think that is ? When should one use or not use DRA+WDT in order to avoid hurting the model performance ? h. Minor formatting issues . See 6 below . 3.Clearly state your recommendation ( accept or reject ) with one or two key reasons for this choice . Accept because of the impressive performance of the proposed method against PACS ( single task , multi domain ) with clear visual analyses . Meantime , 2 . ( 2 ) .g requires more explanation to make the paper 's claim stronger . 4.Provide supporting arguments for your recommendation . See 2 . ( 1 ) Strengths , 3 , and 2 . ( 2 ) .g above . 5.Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment . See 2 . ( 2 ) Weaknesses above . 6.Provide additional feedback with the aim to improve the paper . Make it clear that these points are here to help , and not necessarily part of your decision assessment . ( 1 ) The indentation of Table 5 seems to be inconsistent with the rest of the tables in the paper . ( 2 ) In Figure 3 , the paper mentions $ P_ { k } $ without the denotation explained explicitly in any of the main body of the text . I had to re-read the paper to find a footnote 5 to finally understand what this denotation meant . It would be good to briefly explain this denotation in the same description of Figure 3 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer , many thanks for your suggestions . We address them point-by-point below . * '' Skipped the math that yields the equation ( 3 ) in the section 3 . `` * As requested , we have reorganized this section to clarify those points . Please let us know in case any concerns remain ! * '' In Table 1 , there is a data domain size metric : $ \\pi_d $ . Can you add how this is computed ? Also , how small is the smallest data in sheer number of examples ? `` * $ \\pi_d $ indicates the relative share of each ( hidden ) domain , i.e.we compute it as $ N_d/N $ , where $ N_d $ denotes the number of examples belonging to latent domain $ d $ , and $ N $ is the sum over all domains ( we also mention this in the first paragraph of Section 3.1 ) . Note that we only compute $ \\pi_d $ for the analysis \u2013 the model has absolutely no knowledge of which domains are large/small , nor how many there are . The smallest domains are : 1.6k ( PACS-photo ) and 2k ( Vgg-Flowers ) . * '' Can you explain what `` inactive '' here means ? `` * WDT is a pairwise augmentation , and the exchange strength is a measure of how similar each latent domain is . In other words : how much parameter sharing occurs between some latent domain ( e.g.VD-Omniglot ) and the other ones . For example : PACS-sketch is relatively isolated in feature space ( see Fig.3 left ) , and `` inactive '' here means the average exchange strength is relatively low , i.e . * * a lower average \u03b4 in WDT = fewer parameters shared with other PACS domains = an inactive domain . * * * '' Table 1 has RestNet56 , but Table 2 does n't . Why did you make this choice of experiment design ? `` * That \u2019 s a great suggestion , we have added results for ResNet56 to Table 2 . However , the improvement over ResNet26 is only marginal ( 92.70 - > 93.35 ) and significantly smaller than the improvement via DRA ( 92.70 - > 94.76 ) . Note DRA also uses around 40 % less parameters than ResNet56 . This result once again confirms that * * performance on small latent domains is suppressed * * , and * * adding more layers * * \u2013 the preferred option for standard classification \u2013 * * doesn \u2019 t solve this problem * * . Please let us know of any concerns that remain , so that we may address them ."}, "2": {"review_id": "McYsRk9-rso-2", "review_text": "Summary : The authors propose a method for latent domain learning , where input data come from different domains and the domain labels are unknown . The proposed method consists of two parts : dynamic residual adapter and weighted domain transfer . The dynamic residual adapter acts as a mixture of expert layer . And the weighted domain transfer which augments the dataset by interpolating between different input pairs . Empirical results show that when combined together , the proposed method perform better than training a regular model . Pros : 1.Latent domain discovery is a very interesting topic . 2.Empirical results show that the method brings improvement to minority domains . Cons : 1.Maybe I missed something , but I do n't there are new insights in the paper . The proposed dynamic residual adapter is just an instance of MoE [ 1 ] with adapters , which I think should be a baseline in the experiments . 2 . `` Section 3.4 Weighted Domain Transfer '' is not well-motivated and very confusing . Here you want to interpolate between x_i and x_j . But why do you compute the difference between the input x_i and the feature \\mu_i in equation 3 ? Are they comparable with each other ? What is the goal that you want to achieve here ? Other comments : 1 . I think the introduction describes the problem too much , leaving it no space to expand your idea and intuition . For example , you start describing your idea at the very last paragraph . 2.When creating the augmented examples , can you leverage the gate information that you produced from the MoE ? 3.It will be more helpful to understand the DRA component if you can provide PCA over the original activations . [ 1 ] OUTRAGEOUSLY LARGE NEURAL NETWORKS : THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear Reviewer , thank you for your comments , which we address point by point below . * \u201c Maybe I missed something , but I do n't there are new insights in the paper . The proposed dynamic residual adapter is just an instance of MoE [ 1 ] with adapters , which I think should be a baseline in the experiments. \u201d * We make several novel contributions in this paper . 1.The most important insight is the * * identification of a new problem * * that is pervasive yet not generally known by the community . Specifically , we present clear empirical evidence that * * standard deep models underperform when latent domains are present due to suppressing small domains * * . This is a * fundamental problem with the generalization of deep architectures * , and we have included results ( page 9 of our revision ) that demonstrate it even extends to standard classification problems , e.g.on CIFAR-10 ( see Table 4 ) . 2.We find that standard augmentation strategies do not remedy this problem . Because of this , * * we propose WDT , a new augmentation strategy that reduces bias against small latent domains * * . 3.As per MoE : this is simply an elegant way of extending multi-domain approaches to latent domains , where we can not use off-the-shelf multi-domain methods ( since there are no domain labels to speak of , see page 4 paragraph 3 ) . Because there is no domain information , we * * propose DRA , an alternative way to target corrections purpose-built for latent domain learning * * . Please note DRA is not necessarily an instance of ( I ) MoE \u2013 we explored several alternatives , such as ( II ) Gumbel-softmax mechanisms that we compare to in Table 5 , or ( III ) fixed assignments ( e.g.after applying latent domain discovery e.g.through k-means , see Table 2 ) . * '' Here you want to interpolate between $ x_i $ and $ x_j $ . But why do you compute the difference between the input $ x_i $ and the feature $ \\mu_i $ in equation 3 ? `` * We discuss the motivation around WDT on page 5 paragraph 2 : for latent domains , we do not want to exchange between samples that are visually too similar . This has a straightforward motivation : exchanging too much information between samples * can * reduce the number of discriminative features for samples from the same class . What we want instead is to encourage information exchange between different ( hidden ) domains , so as to end up with a model that learns concepts of objects ( cats , dogs , etc . ) that are invariant to the ( latent ) domain . Please note the difference $ x_i-\\mu_i $ is further scaled by the standard deviation after which $ \\mu_j $ is added . This process is crucial and exactly responsible for the * transfer of statistical information from $ x_j\\to x_i $ * . * '' I think the introduction describes the problem too much , leaving it no space to expand your idea and intuition . For example , you start describing your idea at the very last paragraph . `` * We maintain a longer than usual introduction is needed , because an important aspect in this paper is to * * establish latent domain learning as a novel learning setting * * . This requires we explain the latent domain scenario in sufficient detail , in large part so that future work won \u2019 t have to . Feel free to check out the long introduction in the initial work on multi-task learning by e.g.Caruana ( 1997 ) . While it might be a bit obvious nowadays , it is still recommended literature for anyone that is starting out in the field . * '' When creating the augmented examples , can you leverage the gate information that you produced from the MoE ? `` * We agree this is an interesting idea , as having access to richer conditional information within DRA could potentially enhance the internal clustering of latent domains . However , in doing so the method would no longer be end-to-end , as we would require a full pass through the network to collect gate activations . The goal of our manuscript \u2013 as stated on page 2 paragraph 1 \u2013 is to contribute a principled end-to-end mechanism that doesn \u2019 t have a large negative impact on runtimes ( e.g.we want to avoid 9x models ) . Requiring two passes would have moved the paper in a very different direction . * '' It will be more helpful to understand the DRA component if you can provide PCA over the original activations . `` * Please note in Figure 2 we do not display PCA over activations . We collect * only the gate activations * across the ResNet26 , and then reduce this to two dimensions with a linear mechanism ( i.e.PCA ) .This shows : * individual residual adaptations * can * be attributed to latent domains . * * * DRA shares between similar domains * * , which provides an * * explanation * * of its much better performance than RA etc . We went ahead and added some clarification around this in the final paragraphs of page 6 . Please let us know in case there \u2019 s anything else you would like us to revisit ."}, "3": {"review_id": "McYsRk9-rso-3", "review_text": "The paper describes dynamic residual adapters designed to adaptively account for latent domains , and weighted domain transfer . This framework injects adaptivity into networks , preventing them from overfitting to the largest domains in distributions , a failure mode of traditional models that are exposed in latent domain learning . The approach closes a large amount of the performance gap to domain-supervised solutions . This paper is well written . The motivation of this paper is clear and the proposed framework does break the limitation of the existing deep learning methods . Below I present some suggestions , which hopefully can help the authors improve their study : The authors do not describe any processing they have done of the data . This should be clearly included in the methods section . More experimental details and insightful discussions should be provided . I suggest the authors repeat the benchmarking using a selection of datasets more similar to those used in current studies . The benchmarking results are insufficiently described in the text and can only really be seen in the tables/figures . The authors should explain the results in more detail in the text and include their interpretation of the results . Section 3 . In this part , the authors introduce the algorithm . Since there are many formulas in this section , additional explanations on the learning procedure would help understand the proposed method .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your constructive feedback . We address each point below . * \u201c The authors do not describe any processing they have done of the data . This should be clearly included in the methods section. \u201d * We do discuss this on page 6 paragraph 3 . As we mention there , we only use standard augmentations ( flipping , normalization ) in * all * of our experiments , and otherwise make no changes to the benchmark data . * '' The authors should explain the results in more detail in the text and include their interpretation of the results . `` * We extensively discuss our results , and provide a large set of experiments for two latent domain settings , with ( I ) joint and ( II ) disjoint label spaces . Moreover , for both settings , we provide ( III ) qualitative insights , as well as ( IV ) thorough discussions of the computational requirements . Please note the 8 page limit prevented us from including additional analysis in the main paper , but we included an extensive study of DRA around ( V ) single dataset performance on CIFAR-10 and CIFAR-100 ( Table 3 ) , ( VI ) robustness to class imbalance ( Table 4 ) , as well as ( VII ) ablations ( Tables 5-8 ) in the appended pages . We have taken advantage of the 9 page limit for the revision , and following your suggestion included additional analysis ( V ) + ( VI ) ( which were previously in the appendix ) in the main parts of the paper . On page 9 we demonstrate that the benefits of DRA even extend to standard classification tasks , and DRA brings * significant * performance advantages when some classes are underrepresented in the data . The last case addresses a particularly severe issue for standard deep learning models , as we display in Table 4 for ResNet26+56 . * '' In this part , the authors introduce the algorithm . Since there are many formulas in this section , additional explanations on the learning procedure would help understand the proposed method . `` * We have reorganized the section , and added additional discussions around WDT in our revision . Thank you again for your constructive feedback . Please point out any remaining concerns so that we may address them ."}}