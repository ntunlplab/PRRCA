{"year": "2021", "forum": "qOCdZn3lQIJ", "title": "Compressing gradients in distributed SGD by exploiting their temporal correlation", "decision": "Reject", "meta_review": "The paper introduces a new scheme for compressing gradients in distributed learning which is argued to exploit temporal correlation.\n\nThe paper received very detailed reviews and generated a lot of discussions (thank you to the reviewers for the amazing job).  Many reviewers acknowledge that this is interesting work, a simple and potentially useful algorithm but pointed out many problems with discussion, theoretical analysis, and experiments (e.g., it was not clear to R4 and R3 that these are temporal correlations which are beneficial rather 'lossiness'). Some of these issues were addressed and the current version is currently much stronger than the initial submission (and stronger than the low average scores suggest). Still, the reviewers do not believe that the paper is ready for publication and I share this sentiment. I would strongly encourage the authors to invest more effort in addressing the reviewers' comments and resubmit the work to one of the upcoming top conferences.", "reviews": [{"review_id": "qOCdZn3lQIJ-0", "review_text": "The authors present a new scheme for compressing gradients for use in distributed training . In addition to the previously proposed techniques of sending the sign of the gradient components along with the scale , and the use of error feedback ( each sender tracks the error introduced by quantization , and adjusts future gradient updates using it ) , the authors also propose to exploit the temporal correlation of gradient values ( i.e. , over successive steps ) . They do so by computing the delta between two steps , and then use a hyperparameter $ \\alpha $ to keep only a fraction of the deltas and that is sent losslessly . The idea is an interesting ( even if a fairly simple one ) and leads to a greater than 50 % savings . However , I am confused about a basic issue . From the authors \u2019 own data ( Figure 2 and text ) and from other research on alignment of per-example gradients ( e.g.https : //arxiv.org/abs/1901.09491 , https : //arxiv.org/abs/2008.01217 ) , for the bulk of training , actual temporal correlation between gradients is quite low . So how can delta compression help ? ( As an aside , the correlation is likely to be quite affected by batch size as per the second reference above , so some exploration/data around that would also be useful in the context of this proposal . ) This leads me to believe that the compression benefit they are seeing comes from higher values of $ \\alpha $ , i.e. , by throwing away information . So a natural question is : What happens if you simply do lossy compression on gradient signs ? You would have to go to 3 values ( +1 , 0 and -1 ) , and there would likely be some natural sparsity , and $ \\alpha $ could be used to enhance that . That would appear to be an interesting baseline to see how much benefit comes from the temporal aspect v/s the lossiness induced by $ \\alpha $ . These two appear to be currently confounded . ( Related : how does $ \\alpha $ = 0 look in Figure 1 ? Pretty bad from a compression point of view presumably . If so , then the benefit really comes from lossiness rather than temporal correlation ? ) # # # After Rebuttal Increasing rating based on the authors ' clarifications on the source of the gains . Open to further changes based on further review and discussions with other reviewers", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * * Lossy-compression-only baseline * * * If we decrease lossyness ( by using a smaller $ \\alpha $ ) we indirectly reduce the correlation ( $ q $ gets closer to $ 0.5 $ ) . Therefore , it 's not straightforward to decouple the compression savings to be able to quantify them and attribute them to ( i ) lossy compression or ( ii ) leveraging temporal correlation . We try to understand the additional contribution due to exploitation of temporal correlation in two steps . First , we follow the reviewer \u2019 s suggestion to establish a lossy-compression-only baseline , then we consider our scheme . + Following the reviewer 's suggestion to design a baseline , we have come up with the following comparison . Let us consider a new encoder/decoder pair . The encoder computes $ \\text { sgn } ( \\hat { g } _k [ j ] ) $ , for which , according to the left-hand subfigure of Figure 2 , the fractions of $ +1 $ and $ -1 $ 's are equal . The encoder then randomly flips the sign of $ +1 $ 's with probability $ \\alpha $ . The resulting vector has a fraction of $ +1 $ \u2019 s equal to $ 0.5 ( 1-\\alpha ) $ and therefore can be compressed . Picking $ \\alpha = 0.7 $ , we would compute $ H ( 0.5 ( 1-0.7 ) ) = H ( 0.15 ) = 0.61 $ , i.e. , compression of 39 % . Not bad . ( Note that $ H ( \\cdot ) $ is the binary entropy function of a Bernoulli- $ \\gamma $ source where $ 0 \\leq \\gamma \\leq 1 $ , defined as $ H ( \\gamma ) = -\\gamma \\log_2 ( \\gamma ) \u2013 ( 1-\\gamma ) \\log_2 ( 1-\\gamma ) $ . ) + Next , we consider our scheme . Instead of compressing $ \\hat { g } $ we apply the same lossy compression as the baseline , but across time to $ \\hat { g } $ and $ \\bar { g } $ . However , we do not start with an unbiased source . Rather , due to the use of distortion in previous steps we start with a * * biased * * source where the bias is $ q \\leq 0.5 $ . So , now our compression rate is $ H ( q ( 1-\\alpha ) ) < H ( 0.5 ( 1-\\alpha ) ) $ . For the same $ \\alpha = 0.7 $ we find from the central figure of Figure 2 that $ q = 0.2 $ ( actually somewhat less ) yielding a $ p = q ( 1-\\alpha ) = 0.2 ( 0.3 ) = 0.06 $ ( cf.right-hand plot Figure 2 ) . $ H ( 0.06 ) = 0.33 $ , i.e. , 67 % compression . So , by designing our scheme to induce temporal correlation we increased the compression rate from 39 % to 67 % , almost doubling the compressive gains and effecting an additional reduction in bit-rate of $ 0.28 $ bits per coordinate . * * * Exploring impact of the batch size on correlation * * * We thank the reviewer for suggesting assessing the impact of batch size on the correlation . Following the reviewer \u2019 s lead , we have done this and included the results in our updated submission Appendix section A.4 ( indicated in blue color ) . In summary , we observe that $ q $ slightly increases with the batch size as the stochastic gradients become more and more positively correlated . However , as per our numerical results , this change remains minimal for typical batch sizes we use in training ."}, {"review_id": "qOCdZn3lQIJ-1", "review_text": "1.The main problem I have with this paper is that this paper idolizes the paper [ distributed EF-SGD by Zheng et al.NeuRIPS 2019 ] . However , the main result , that is , Theorem 1 in dist-EF-SGD is mathematically problematic or simply wrong . The proof of Theorem 1 as given in [ distributed EF-SGD by Zheng et al.NeuRIPS 2019 ] does not hold good when the learning rate sequence $ \\eta_t > 0 $ is decreasing . Therefore , the authors \u2019 claim in the Abstract and several parts of the present paper \u201c We strengthen their analysis to show that the rate of convergence of two-way compression with error- feedback asymptotically is the same as that of SGD \u201d eventually invalid . I suggest the authors please read the distributed-EF-SGD paper carefully , understand it better , write the proofs on their own before making these types of strong claims . Please study [ 1 ] and work on your proofs . 2.Page 2 : \u201c However , Karimireddy et al . ( 2019 ) theoretically show that SGD with compression does not converge in general. \u201d This is a very strong statement which I do not agree with . Karimireddy et al . ( 2019 ) showed how error feedback can fix the convergence issue of sign-based quantization as in signSGD . Moreover , they showed how any compressor ( biased/unbiased ) can be converted to a $ \\delta $ -compressor . But that does not mean the authors \u2019 statement in this manuscript is correct . As authors claimed \u201c error feedback-based algorithms circumvent the convergence issues for SGD with compression \u201d is not right . Error feedback is known to work well for sparsification , where a subset of gradient components are sent or for extreme quantization ( such as sign-based compressions ) . However , regular random dithering-based quantization techniques such as QSGD , natural compression , etc . converge just fine without error feedback . Actually , error feedback may degrade their performance . I would like to request the authors to first understand these works in detail before writing these types of strong statements on their paper . 3.Another vague statement is : \u201c Therefore , these two classes can be respectively thought of as approaches that reduce the quantity versus the quality of the gradient. \u201d Based on what you claimed this ? 4. \u201c Sign-based compression schemes such as Scaled-sign , SignSGD and Signum ( Bernstein et al. , 2018 ) \u2026 \u201d SIGNUM is not a sign-based compressor . It is the momentum version of signSGD , nothing novel . 5.You may want to talk about the most relevant and recent work on compression known as SketchML [ J. Jiang , F. Fu , T. Yang , and B. Cui , \u201c SketchML : Accelerating Distributed Machine Learning with Data Sketches , \u201d SIGMOD , 2018 ] while talking about delta encoding first paragraph in Section 3 . This recent paper on gradient compression uses delta encoding . 6.I failed to understand the benefit of Generalized dist-EF-SGD algorithm in Section 3 ? What are the main differences telling me ? 7. \u201c In our case , while the length of the parameter vector d is well over a million for models of practical interest , the entries of b are not necessarily i.i.d .. \u201d Can you make an assumption of the independence of the gradient components ? If you make that assumption you may elevate the issue . Also , the assumption is not a strong assumption and generally made for stochastic gradients . Please See [ Huffman Coding Based Techniques for Fast Distributed Deep Learning , Gajjala et al. , CoNext DistML workshop , 2020 ] 8 . Uniform upper bound of the stochastic gradients g_i is an obsolete concept . The authors may argue that `` The classical theoretical analysis of SGD assumes that the stochastic gradients are uniformly bounded '' . But one can even strongly argue that this bound is actually $ \\infty $ . Moreover , an even a stronger argument can be made that the above assumption is in contrast with strong convexity . Please see [ `` SGD and Hogwild ! Convergence Without the Bounded Gradients Assumption '' by Nguyen et al . ] as one of the instances . Please understand there are relaxed assumptions such as Strong growth condition on a stochastic gradient as in Assumption 4 of [ 2 ] . 9.You said : \u201c Since the communications component in Horovod is designed for a master-less setup , we simulate a master-worker environment in our implementation. \u201d But I was wondering why do you need this ? In any case , if you use all-reduce collective for aggregation even for P2P architecture the aggregation will be similar . Please correct me if I am wrong . 10.While ImageNet accuracy is similar to why test accuracies of the models on CIFAR-100 and ImageNet-32 are below 60 % ? 11.In terms of experimental results , instead of Figure 1 , the authors may use relative data-volume vs. test accuracy . Especially , when the accuracy figures are really cluttered . To do proper experiments by using compression techniques , the authors can check a very elaborative work and codebase by [ Hang Xu et . al , Compressed Communication for Distributed Deep Learning : Survey and Quantitative Evaluation . ] In that case , I would encourage the authors to plot relative data-volume vs. test accuracy similar to Figures 6 and 7 therein . I am sorry but in the present papers , the experiments and their presentations are substandard . 12.Why did not you compare with sign-sgd algorithm ? Also , sign-based algorithms are notorious in their convergence . SignSGD uses a special stepsize . So I was wondering what is your step-size schedule ? You never mentioned this in your paper . You may did it in the Appendix and I did not check the Appendix . So , please indicate if you did . Minor Comments : 1 . These two sentences in my understanding are claiming the same thing ? \u201c We strengthen their analysis to show that the rate of convergence of two-way compression with error feedback asymptotically is the same as that of SGD . As a corollary , we prove that two-way SignXOR compression with error-feedback achieves the same asymptotic rate of convergence as SGD. \u201d 2 . \u201c Novel approaches such as federated learning\u2026 \u201d Why is it novel ? Unnecessary hyperbole is not part of technical writing . 3. \u201c In this section , we prove that the combination of Algorithm 1 and Algorithm 2 converges , and the convergence rate is asymptotically the same as that of SGD. \u201d Why Algorithm 2 when Algorithm 1 implicitly implies the inclusion of Algorithm 2 ? 4.Please correct the typos and please write as it is done in a technical paper , not in a sci-fi novel . [ 1 ] Communication-Efficient Distributed SGD with Error-Feedback , Revisited , Tran Thi Phuong , Le Trieu Phong , 2020 . [ 2 ] Dutta et al.AAAI 2020 , On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning", "rating": "2: Strong rejection", "reply_text": "9.In Algorithm 1 we assume a master-worker setup with two-way compression . To be consistent with Algorithm 1 , we simulate a master-worker setup in experiments even though the underlying Horovod implementation is based on allreduce . If we use the allreduce aggregation as the reviewer suggests , workers will compress before calling allreduce . Allreduce computes the average of compressed gradients , which will be used to update the parameter vector . This is equivalent to one-way compression in a master-worker setup . However , we are considering two-way compression . 10.The reduction of test accuracy in ImageNet-32 is to be expected as it is a down-sampled version of ImageNet , therefore , comprising of less informative images . Chrabaszcz et al. , 2017 provide a baseline for ImageNet-32 . As can be observed in their Figure 2 , the final test accuracy varies between 70 % and 60 % for different learning rates . In comparison , we achieve a 60 % accuracy with SGD while they achieve a higher accuracy as they use SGD with momentum . For CIFAR-100 we use Zheng et . al.2019 as the baseline . The difference between their CIFAR-100 results and ours is due to two reasons : ( i ) we use resnet-18 and they use resnet-20 , and ( ii ) we use a weight regularizer of $ 10^ { -3 } $ whereas they use $ 5\\times10^ { -4 } $ . In any case , we note that our goal is to compare the relative performances of the algorithms under * same experimental settings * . Therefore , we do not perform an exhaustive search over hyperparameters that achieve state-of-the-art error rates . 11.We do not see how the reviewer \u2019 s suggestion of plotting data-volume vs. test accuracy provides much more information than what can already be extracted from the existing figures . The test accuracy and loss plots are clear enough to see the general trend . The $ B_X/B_S $ plots show versus epoch the relative data usage of the proposed method ( relative to Scaled-Sign ) . We chose the presentation style in Figure 1 because it provides information about loss/test accuracy with respect to both relative data usage as well as the epoch . If , as in some of the literature , a much larger set of algorithms were being considered then we agree with the reviewer that scatter plots of data-volume vs. final test accuracy can quite nicely summarize where algorithms stand in relation to each other . To make the important points that we want to make in the current submission we believe the current plots serve our purposes well . As one important example of how the plots serve our purposes , we direct the reviewer to our response to the AnonReviewer4 ( please see * Part 1/2 * therein ) . In response that that reviewer \u2019 s question we used our Figure 1 and particularly our Figure 2 to illustrate the combined effects of exploiting ( i ) the lossy compression approach we designed and ( ii ) the resulting temporal correlation induced to reduce the rate of communication . This is a key idea that we wanted to use the experiments to illustrate . We feel the experiments we provided accomplish that \u2013 though of course we thank the reviewers for asking the questions that helped us better to explain the ideas we wanted the experimental results to illustrate . We also note that our experiments and their presentation is similar to a number of other results in the field , e.g. , those by Bernstein et al. , 2018 ( Figure 3 ) , Karimireddy et al. , 2019 ( Figure 4 ) , and Zheng et . al.2019 ( Figure 2,3 ) . 12.Please refer to the first paragraph in Section 5.1 in the main text were have described the learning rate schedules for all experiments . We chose not to compare with SignSGD as ( 1 ) the communication usage ( per iteration ) of SignSGD is same as that of its error-feedback counterpart ( EF-SIGNSGD by Karimireddy et al. , 2019 ) , and ( 2 ) EF-SIGNSGD performs better than SignSGD as demonstrated by Karimireddy et al. , 2019 . We believe it suffices to compare with the better algorithm out of the two . This decision also kept our figures and discussion less cluttered . * * * Minor comments * * * 1 . The first sentence is about the convergence rate of error-feedback with * any * $ \\delta $ -compressor . The second sentence indicates that SignXOR with error-feedback converges as it is a $ \\delta $ -compressor . 2.We are sorry the reviewer \u2019 s taste was not always well served by our writing style . We appreciate that there are variations in aesthetics and also appreciate that some of the other reviewers commented favorably on the style of the paper . 3.We agree with the reviewer that phrasing in this sentence may have been confusing . Our message is that * SignXOR compression ( Algorithm 2 ) with generalized-EF-SGD ( Algorithm 1 ) converges * . We have refactored the paragraph in the latest manuscript . The updated text is indicated in blue color . 4.We will correct all the typos indicated by all reviewers ( thank you very much ! ) and will of course do another round of close reading if the paper is accepted for final publication ."}, {"review_id": "qOCdZn3lQIJ-2", "review_text": "This paper proposed an extension of blockwise scaled sign compressor in Zheng et al . ( 2019 ) .The proposed method exploits the temporal correlation between two consecutive gradients . The authors show that one can have a higher compression rate by inserting distortion to the compressed gradient . A tighten bound is provided such that the asymptotic rate ( including constant ) is exactly the same as the full-precision counterpart . The experiments show that the proposed compressor can achieve additional 40 % -50 % reduction on communication compared to the scaled sign . Overall , the reviewer thinks the idea is interesting . The reviewer has a few comments : 1 . The proposed method considers randomly flipping the direction for elements that have the same sign as the averaged gradient in the last step . In this way , the sign is always correct for the elements that have opposite direction from the last gradient . I wonder will the results change if we consider flipping the sign of the elements that have opposite direction ? 2.Since alpha has a very small upper bound , it is hard to see any theoretical improvement over scaled sign . 3.Theorem 4.2 does not show that one can achieve a linear speedup , i.e. , O ( 1/\\sqrt { nT } ) rate . 4.For the distributed training with high speed network , the extra overhead incurred by compression is not trivial and can not be overlooked . As there is no results against CPU wall clock time , it is not clear if the proposed method is really faster than the scaled sign in terms of elapsed time . 5.Can you show the final test accuracies on ImageNet achieved by each algorithm ? It seems that scaled sign has slightly higher accuracy .", "rating": "6: Marginally above acceptance threshold", "reply_text": "In the following we address all comments/questions in the same order . 1.There are two reasons for our particular choice of inverting the signs . We will first summarize the general idea , and then give some specifics in relation to the numerical results provided . The high-level reason for choosing the quantizer we do is that ( cf. , middle plot Figure 2 ) even without quantization $ q\\approx0.45 < 0.5 $ . Since the binary entropy function is symmetric around $ q=0.5 $ and is concave , for a fixed amount of distortion we get more rate savings by designing a quantizer that drives $ q $ towards 0 rather than toward 1 . If we drive $ q $ towards 1 we would need to get over the binary entropy \u201c hump \u201d at 0.5 and to 0.55 before we would start to incur rate savings . By that point we would already have incurred non-negligible distortion . For this reason , we design the quantizer to push $ q $ towards zero . ( i ) In the third sub-figure in Figure 2 we plot $ p $ , the fraction of $ 1 $ 's in vector $ b $ for ImageNet32 experiments . Recall that $ \\alpha=0 $ case recovers the vanilla scaled-sign compression . As per the third sub-figure , $ p $ for $ \\alpha=0 $ stays at around $ 0.45 $ . We observed that $ p $ stays close to but slightly lower than $ 0.5 $ for CIFAR100 experiments as well . We gain compression savings if the binary entropy $ H ( p ) $ is closer to $ 0 $ . The binary entropy is symmetric around $ 0.5 $ . Therefore , we have to choose between two strategies : ( A ) push $ p $ towards $ 0 $ , or , ( B ) push $ p $ towards $ 1 $ by introducing errors to vector $ b $ . The former ( A ) corresponds forcing some of the components with same sign to be different , and the latter ( B ) corresponds to forcing some of the components with different signs to be the same . Since for $ \\alpha=0 $ ( no errors ) we already have $ p $ less than $ 0.5 $ ( say $ 0.45 $ ) we make the choice to push $ p $ towards $ 0 $ by applying ( A ) . Let us assume we were to use ( B ) instead . This means that we will have to introduce errors just to get $ p > 0.5 $ . For example , if we were to end up with $ p=0.55 $ , we will have obtained no gain in terms of compression ( since $ H ( 0.45 ) =H ( 0.55 ) $ ) , although we would already have introduced some errors to $ b $ vector . ( ii ) We also note that , to confirm our design idea , during our research we also ran experiments that pushed q toward 1 . While we did not include those results in the final paper , those systems worked * less * well due to our reasoning in ( i ) . 2.We agree with the reviewer that the upper bound is small . This concern was also raised by the third reviewer ( AnonReviewer3 ) and we responded at length to that reviewer . We ask this reviewer to refer to that response ( please see under the * Addressing the comment about Theorem 1 * heading ) which we are not reproducing here to keep our response more compact . 3.This is due to our choice of the step size . The choice of step size by Zheng et al . ( 2019 ) yields two bounds for dist-EF-SGD and SGD as are presented in Corollary 1 . We observe that although they achieve a linear speedup , both first and the second terms in the dist-EF-SGD bound are larger by constant factors compared to those of the SGD bound ( first two terms in dist-EF-SGD have 4 and 1 , whereas first two terms in SGD have $ \\frac { 8 } { 3 } $ and $ \\frac { 2 } { 3 } $ ) . In comparison , our choice of step size gives us identical first terms for the proposed method and SGD ( our Theorem 2 ) . Not achieving a linear speedup is the price we pay for getting the first terms to match . 4.We agree that the compression overhead may be non-trivial . However , our primary goal is to showcase the reduction of the communication payload . All the steps in in Algorithm 2 can be efficiently implemented using vectorized operations except lossless compression ( and decompression ) of vector $ b $ . The lossless compression overhead heavily depends on the algorithm and the implementation of the algorithm . We employ the Python implementation of the LZMA algorithm in our experiments . Since our setup does not use the most computationally efficient implementation of compression , we do not think it is fair to compare the compression overheads involved and therefore did not include such a comparison . 5.In our original experiments we ran SignXOR only for a slightly greater than 10 epochs due to resource limitations . As per the reviewer \u2019 s request , we have rerun SignXOR $ \\alpha=0.6 $ for a longer time and included an updated figure in Appendix ( Figure 5 on last page in the updated manuscript ) . Currently it shows SignXOR $ \\alpha=0.6 $ only up to the 12th epoch ( still running past the 12th epoch as of the time of this response ) . We hope to update the final test accuracies prior to the end of the rebuttal period ."}, {"review_id": "qOCdZn3lQIJ-3", "review_text": "This paper proposes a gradient compression approach to remove the communication bottleneck in distributed stochastic gradient descent . I think the key attributes of their algorithm are as follows : - It uses error feedback ( Stich et al. , 2018 ; Karimireddy et al. , 2019 ) and operates in a parameter server model based on ( Zheng et al.2019 ) + The compressor sends the sign of each gradient coordinate and a scale factor per 'block ' of coordinates ( like Zheng et al.2019 ) + The messages are compressed with lossless entropy encoding . + Specifically , they send and encode the difference between the current sign vector and the previously transmitted one . This is meant to lower the entropy of the vectors ( make the distribution of -1 's and 1 's less even ) + Because the distribution of -1 's and 1 's is still roughly 50/50 after delta-coding , the authors introduce ( lossy ) noise in the compressor . They randomly flip some instances of 'same sign as before ' to 'different sign than before ' . This reduces the entropy of the 'difference vector ' so it can be compressed more . I believe the main contributions of the paper are : - The introduction and evaluation of lossless compression on top of sign-based gradient compression - A theoretical improvement of the constants in the rates from ( Zheng et al.2019 ) - A proof that SignSGD with delta-coding and a bias towards changing signs can still be a $ \\delta $ -compressor , as long as the bias is extremely small ( not covering the experiments presented in the paper ) I find the ideas presented in this paper interesting and novel and the experiments well executed . The writing is of good quality , and I find it easy to follow . I do , however , have two concerns : - The method is said to exploit temporal correlation in the gradients by using delta coding . To me , this seems misleading . The authors show that , without the introduced bias , there is not much gain from delta coding ( `` In our experiments presented in Section 5.2 we observe that when \u03b1 = 0 , p remains close to but slightly lower than 0.5.This implies low correlation between [ .. ] . Our solution is to make $ \\alpha > 0 $ . `` ) . With the proposed solution , the method exploits patterns created artificially by lossy compression , rather than temporal correlation in the gradients . Given that p < 0.5 , this is actually anti-correlation rather than correlation . The signs are more likely to flip due to previously introduced errors . - If the delta-coding scheme indeed fails to leverage temporal correlation in the gradients without artificially introducing extra errors , the proposed scheme does n't really do what it seems to be designed for . This makes it less elegant/complicated . This lack of simplicity could be compensated by convincing experimental results , but it seems that many gradient compression schemes achieve similar results to the proposed scheme at similar compression rates ( see Xu et al.https : //repository.kaust.edu.sa/bitstream/handle/10754/662495/gradient-compression-survey.pdf ) for an overview ) . I am not convinced by the benefits of the proposed scheme over others .", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * * Addressing the temporal correlation-related comments * * * We thank the reviewer for the comments . The issue raised by the reviewer regarding temporal correlation is very important one that was also shared by the first reviewer ( AnonReviewer4 ) and we responded at length to that reviewer . We ask this reviewer to refer to that response ( please see * Part 1/2 : Addressing concerns regarding the source of rate savings * ) which we are not reproducing here to keep the response more compact . A summary of that response is as follows . We demonstrate using our experimental results that the total compression gain is due to ( i ) lossy compression and ( ii ) temporal correlation . We argue that the temporal correlation is not because of the lossyness ( or distortion ) introduced in the current step , rather due to the introduction of distortion in previous steps . This yields the temporal correlation with the next iteration that we then leverage . In addition to this temporal correlation we add distortion to the current step which gives us more compression . As discussed in the response to AnonReviewer4 , the issues of lossy compression and leveraging temporal correlation are intertwined in our system . * * * Addressing the comment about Theorem 1 * * * We would also like to respond to the reviewer \u2019 s comment about the upper bound for bias ( $ \\alpha $ ) . We agree with the reviewer that the upper bound is small . The reason for this is the extremely small $ \\delta $ that corresponds to the scaled-sign compression . We attribute this to the mismatch one often observes between theory and experiments where the assumptions that need to be made to make the theory work are often more conservative than those that are often observed to work in practice . Next we reason why this is the case in our proposed scheme . Recall that scaled-sign is a $ \\frac { 1 } { d } $ -compressor where $ d $ is the dimension of the input vector . Let us consider the EF-SignSGD algorithm by Karimireddy et al . ( 2019 ) .The convergence bound of EF-SignSGD is given in their Remark 4 . In summary , the difference between the bounds of EF-SignSGD and vanilla SGD is that the former has an additional term with $ 1/\\delta^2 $ . Since $ d $ is in the millions for models of practical interest , we expect the $ 1/\\delta^2 $ term to be extremely large and to have a large impact on the convergence . However , Karimireddy et al . ( 2019 ) show in their experiments that EF-SignSGD performs very close to vanilla SGD from the beginning of the training . In summary , although one expects the convergence to slow down due to the small $ \\delta $ of scaled-sign , it is not quite the case in practice . This is similar to what we observe in our scheme . The proof of our Theorem 1 relies on the fact that scaled-sign is a $ \\frac { 1 } { d } $ -compressor . This constrains the wiggle room for the proposed system to be a $ \\delta $ -compressor ( since we are building on scaled-sign ) , which in turn limits the bound on $ \\alpha $ . However , our experiments demonstrate that contrary to the theoretical upper bound , one can set $ \\alpha $ to be larger and the algorithm can still converge ."}], "0": {"review_id": "qOCdZn3lQIJ-0", "review_text": "The authors present a new scheme for compressing gradients for use in distributed training . In addition to the previously proposed techniques of sending the sign of the gradient components along with the scale , and the use of error feedback ( each sender tracks the error introduced by quantization , and adjusts future gradient updates using it ) , the authors also propose to exploit the temporal correlation of gradient values ( i.e. , over successive steps ) . They do so by computing the delta between two steps , and then use a hyperparameter $ \\alpha $ to keep only a fraction of the deltas and that is sent losslessly . The idea is an interesting ( even if a fairly simple one ) and leads to a greater than 50 % savings . However , I am confused about a basic issue . From the authors \u2019 own data ( Figure 2 and text ) and from other research on alignment of per-example gradients ( e.g.https : //arxiv.org/abs/1901.09491 , https : //arxiv.org/abs/2008.01217 ) , for the bulk of training , actual temporal correlation between gradients is quite low . So how can delta compression help ? ( As an aside , the correlation is likely to be quite affected by batch size as per the second reference above , so some exploration/data around that would also be useful in the context of this proposal . ) This leads me to believe that the compression benefit they are seeing comes from higher values of $ \\alpha $ , i.e. , by throwing away information . So a natural question is : What happens if you simply do lossy compression on gradient signs ? You would have to go to 3 values ( +1 , 0 and -1 ) , and there would likely be some natural sparsity , and $ \\alpha $ could be used to enhance that . That would appear to be an interesting baseline to see how much benefit comes from the temporal aspect v/s the lossiness induced by $ \\alpha $ . These two appear to be currently confounded . ( Related : how does $ \\alpha $ = 0 look in Figure 1 ? Pretty bad from a compression point of view presumably . If so , then the benefit really comes from lossiness rather than temporal correlation ? ) # # # After Rebuttal Increasing rating based on the authors ' clarifications on the source of the gains . Open to further changes based on further review and discussions with other reviewers", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * * Lossy-compression-only baseline * * * If we decrease lossyness ( by using a smaller $ \\alpha $ ) we indirectly reduce the correlation ( $ q $ gets closer to $ 0.5 $ ) . Therefore , it 's not straightforward to decouple the compression savings to be able to quantify them and attribute them to ( i ) lossy compression or ( ii ) leveraging temporal correlation . We try to understand the additional contribution due to exploitation of temporal correlation in two steps . First , we follow the reviewer \u2019 s suggestion to establish a lossy-compression-only baseline , then we consider our scheme . + Following the reviewer 's suggestion to design a baseline , we have come up with the following comparison . Let us consider a new encoder/decoder pair . The encoder computes $ \\text { sgn } ( \\hat { g } _k [ j ] ) $ , for which , according to the left-hand subfigure of Figure 2 , the fractions of $ +1 $ and $ -1 $ 's are equal . The encoder then randomly flips the sign of $ +1 $ 's with probability $ \\alpha $ . The resulting vector has a fraction of $ +1 $ \u2019 s equal to $ 0.5 ( 1-\\alpha ) $ and therefore can be compressed . Picking $ \\alpha = 0.7 $ , we would compute $ H ( 0.5 ( 1-0.7 ) ) = H ( 0.15 ) = 0.61 $ , i.e. , compression of 39 % . Not bad . ( Note that $ H ( \\cdot ) $ is the binary entropy function of a Bernoulli- $ \\gamma $ source where $ 0 \\leq \\gamma \\leq 1 $ , defined as $ H ( \\gamma ) = -\\gamma \\log_2 ( \\gamma ) \u2013 ( 1-\\gamma ) \\log_2 ( 1-\\gamma ) $ . ) + Next , we consider our scheme . Instead of compressing $ \\hat { g } $ we apply the same lossy compression as the baseline , but across time to $ \\hat { g } $ and $ \\bar { g } $ . However , we do not start with an unbiased source . Rather , due to the use of distortion in previous steps we start with a * * biased * * source where the bias is $ q \\leq 0.5 $ . So , now our compression rate is $ H ( q ( 1-\\alpha ) ) < H ( 0.5 ( 1-\\alpha ) ) $ . For the same $ \\alpha = 0.7 $ we find from the central figure of Figure 2 that $ q = 0.2 $ ( actually somewhat less ) yielding a $ p = q ( 1-\\alpha ) = 0.2 ( 0.3 ) = 0.06 $ ( cf.right-hand plot Figure 2 ) . $ H ( 0.06 ) = 0.33 $ , i.e. , 67 % compression . So , by designing our scheme to induce temporal correlation we increased the compression rate from 39 % to 67 % , almost doubling the compressive gains and effecting an additional reduction in bit-rate of $ 0.28 $ bits per coordinate . * * * Exploring impact of the batch size on correlation * * * We thank the reviewer for suggesting assessing the impact of batch size on the correlation . Following the reviewer \u2019 s lead , we have done this and included the results in our updated submission Appendix section A.4 ( indicated in blue color ) . In summary , we observe that $ q $ slightly increases with the batch size as the stochastic gradients become more and more positively correlated . However , as per our numerical results , this change remains minimal for typical batch sizes we use in training ."}, "1": {"review_id": "qOCdZn3lQIJ-1", "review_text": "1.The main problem I have with this paper is that this paper idolizes the paper [ distributed EF-SGD by Zheng et al.NeuRIPS 2019 ] . However , the main result , that is , Theorem 1 in dist-EF-SGD is mathematically problematic or simply wrong . The proof of Theorem 1 as given in [ distributed EF-SGD by Zheng et al.NeuRIPS 2019 ] does not hold good when the learning rate sequence $ \\eta_t > 0 $ is decreasing . Therefore , the authors \u2019 claim in the Abstract and several parts of the present paper \u201c We strengthen their analysis to show that the rate of convergence of two-way compression with error- feedback asymptotically is the same as that of SGD \u201d eventually invalid . I suggest the authors please read the distributed-EF-SGD paper carefully , understand it better , write the proofs on their own before making these types of strong claims . Please study [ 1 ] and work on your proofs . 2.Page 2 : \u201c However , Karimireddy et al . ( 2019 ) theoretically show that SGD with compression does not converge in general. \u201d This is a very strong statement which I do not agree with . Karimireddy et al . ( 2019 ) showed how error feedback can fix the convergence issue of sign-based quantization as in signSGD . Moreover , they showed how any compressor ( biased/unbiased ) can be converted to a $ \\delta $ -compressor . But that does not mean the authors \u2019 statement in this manuscript is correct . As authors claimed \u201c error feedback-based algorithms circumvent the convergence issues for SGD with compression \u201d is not right . Error feedback is known to work well for sparsification , where a subset of gradient components are sent or for extreme quantization ( such as sign-based compressions ) . However , regular random dithering-based quantization techniques such as QSGD , natural compression , etc . converge just fine without error feedback . Actually , error feedback may degrade their performance . I would like to request the authors to first understand these works in detail before writing these types of strong statements on their paper . 3.Another vague statement is : \u201c Therefore , these two classes can be respectively thought of as approaches that reduce the quantity versus the quality of the gradient. \u201d Based on what you claimed this ? 4. \u201c Sign-based compression schemes such as Scaled-sign , SignSGD and Signum ( Bernstein et al. , 2018 ) \u2026 \u201d SIGNUM is not a sign-based compressor . It is the momentum version of signSGD , nothing novel . 5.You may want to talk about the most relevant and recent work on compression known as SketchML [ J. Jiang , F. Fu , T. Yang , and B. Cui , \u201c SketchML : Accelerating Distributed Machine Learning with Data Sketches , \u201d SIGMOD , 2018 ] while talking about delta encoding first paragraph in Section 3 . This recent paper on gradient compression uses delta encoding . 6.I failed to understand the benefit of Generalized dist-EF-SGD algorithm in Section 3 ? What are the main differences telling me ? 7. \u201c In our case , while the length of the parameter vector d is well over a million for models of practical interest , the entries of b are not necessarily i.i.d .. \u201d Can you make an assumption of the independence of the gradient components ? If you make that assumption you may elevate the issue . Also , the assumption is not a strong assumption and generally made for stochastic gradients . Please See [ Huffman Coding Based Techniques for Fast Distributed Deep Learning , Gajjala et al. , CoNext DistML workshop , 2020 ] 8 . Uniform upper bound of the stochastic gradients g_i is an obsolete concept . The authors may argue that `` The classical theoretical analysis of SGD assumes that the stochastic gradients are uniformly bounded '' . But one can even strongly argue that this bound is actually $ \\infty $ . Moreover , an even a stronger argument can be made that the above assumption is in contrast with strong convexity . Please see [ `` SGD and Hogwild ! Convergence Without the Bounded Gradients Assumption '' by Nguyen et al . ] as one of the instances . Please understand there are relaxed assumptions such as Strong growth condition on a stochastic gradient as in Assumption 4 of [ 2 ] . 9.You said : \u201c Since the communications component in Horovod is designed for a master-less setup , we simulate a master-worker environment in our implementation. \u201d But I was wondering why do you need this ? In any case , if you use all-reduce collective for aggregation even for P2P architecture the aggregation will be similar . Please correct me if I am wrong . 10.While ImageNet accuracy is similar to why test accuracies of the models on CIFAR-100 and ImageNet-32 are below 60 % ? 11.In terms of experimental results , instead of Figure 1 , the authors may use relative data-volume vs. test accuracy . Especially , when the accuracy figures are really cluttered . To do proper experiments by using compression techniques , the authors can check a very elaborative work and codebase by [ Hang Xu et . al , Compressed Communication for Distributed Deep Learning : Survey and Quantitative Evaluation . ] In that case , I would encourage the authors to plot relative data-volume vs. test accuracy similar to Figures 6 and 7 therein . I am sorry but in the present papers , the experiments and their presentations are substandard . 12.Why did not you compare with sign-sgd algorithm ? Also , sign-based algorithms are notorious in their convergence . SignSGD uses a special stepsize . So I was wondering what is your step-size schedule ? You never mentioned this in your paper . You may did it in the Appendix and I did not check the Appendix . So , please indicate if you did . Minor Comments : 1 . These two sentences in my understanding are claiming the same thing ? \u201c We strengthen their analysis to show that the rate of convergence of two-way compression with error feedback asymptotically is the same as that of SGD . As a corollary , we prove that two-way SignXOR compression with error-feedback achieves the same asymptotic rate of convergence as SGD. \u201d 2 . \u201c Novel approaches such as federated learning\u2026 \u201d Why is it novel ? Unnecessary hyperbole is not part of technical writing . 3. \u201c In this section , we prove that the combination of Algorithm 1 and Algorithm 2 converges , and the convergence rate is asymptotically the same as that of SGD. \u201d Why Algorithm 2 when Algorithm 1 implicitly implies the inclusion of Algorithm 2 ? 4.Please correct the typos and please write as it is done in a technical paper , not in a sci-fi novel . [ 1 ] Communication-Efficient Distributed SGD with Error-Feedback , Revisited , Tran Thi Phuong , Le Trieu Phong , 2020 . [ 2 ] Dutta et al.AAAI 2020 , On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning", "rating": "2: Strong rejection", "reply_text": "9.In Algorithm 1 we assume a master-worker setup with two-way compression . To be consistent with Algorithm 1 , we simulate a master-worker setup in experiments even though the underlying Horovod implementation is based on allreduce . If we use the allreduce aggregation as the reviewer suggests , workers will compress before calling allreduce . Allreduce computes the average of compressed gradients , which will be used to update the parameter vector . This is equivalent to one-way compression in a master-worker setup . However , we are considering two-way compression . 10.The reduction of test accuracy in ImageNet-32 is to be expected as it is a down-sampled version of ImageNet , therefore , comprising of less informative images . Chrabaszcz et al. , 2017 provide a baseline for ImageNet-32 . As can be observed in their Figure 2 , the final test accuracy varies between 70 % and 60 % for different learning rates . In comparison , we achieve a 60 % accuracy with SGD while they achieve a higher accuracy as they use SGD with momentum . For CIFAR-100 we use Zheng et . al.2019 as the baseline . The difference between their CIFAR-100 results and ours is due to two reasons : ( i ) we use resnet-18 and they use resnet-20 , and ( ii ) we use a weight regularizer of $ 10^ { -3 } $ whereas they use $ 5\\times10^ { -4 } $ . In any case , we note that our goal is to compare the relative performances of the algorithms under * same experimental settings * . Therefore , we do not perform an exhaustive search over hyperparameters that achieve state-of-the-art error rates . 11.We do not see how the reviewer \u2019 s suggestion of plotting data-volume vs. test accuracy provides much more information than what can already be extracted from the existing figures . The test accuracy and loss plots are clear enough to see the general trend . The $ B_X/B_S $ plots show versus epoch the relative data usage of the proposed method ( relative to Scaled-Sign ) . We chose the presentation style in Figure 1 because it provides information about loss/test accuracy with respect to both relative data usage as well as the epoch . If , as in some of the literature , a much larger set of algorithms were being considered then we agree with the reviewer that scatter plots of data-volume vs. final test accuracy can quite nicely summarize where algorithms stand in relation to each other . To make the important points that we want to make in the current submission we believe the current plots serve our purposes well . As one important example of how the plots serve our purposes , we direct the reviewer to our response to the AnonReviewer4 ( please see * Part 1/2 * therein ) . In response that that reviewer \u2019 s question we used our Figure 1 and particularly our Figure 2 to illustrate the combined effects of exploiting ( i ) the lossy compression approach we designed and ( ii ) the resulting temporal correlation induced to reduce the rate of communication . This is a key idea that we wanted to use the experiments to illustrate . We feel the experiments we provided accomplish that \u2013 though of course we thank the reviewers for asking the questions that helped us better to explain the ideas we wanted the experimental results to illustrate . We also note that our experiments and their presentation is similar to a number of other results in the field , e.g. , those by Bernstein et al. , 2018 ( Figure 3 ) , Karimireddy et al. , 2019 ( Figure 4 ) , and Zheng et . al.2019 ( Figure 2,3 ) . 12.Please refer to the first paragraph in Section 5.1 in the main text were have described the learning rate schedules for all experiments . We chose not to compare with SignSGD as ( 1 ) the communication usage ( per iteration ) of SignSGD is same as that of its error-feedback counterpart ( EF-SIGNSGD by Karimireddy et al. , 2019 ) , and ( 2 ) EF-SIGNSGD performs better than SignSGD as demonstrated by Karimireddy et al. , 2019 . We believe it suffices to compare with the better algorithm out of the two . This decision also kept our figures and discussion less cluttered . * * * Minor comments * * * 1 . The first sentence is about the convergence rate of error-feedback with * any * $ \\delta $ -compressor . The second sentence indicates that SignXOR with error-feedback converges as it is a $ \\delta $ -compressor . 2.We are sorry the reviewer \u2019 s taste was not always well served by our writing style . We appreciate that there are variations in aesthetics and also appreciate that some of the other reviewers commented favorably on the style of the paper . 3.We agree with the reviewer that phrasing in this sentence may have been confusing . Our message is that * SignXOR compression ( Algorithm 2 ) with generalized-EF-SGD ( Algorithm 1 ) converges * . We have refactored the paragraph in the latest manuscript . The updated text is indicated in blue color . 4.We will correct all the typos indicated by all reviewers ( thank you very much ! ) and will of course do another round of close reading if the paper is accepted for final publication ."}, "2": {"review_id": "qOCdZn3lQIJ-2", "review_text": "This paper proposed an extension of blockwise scaled sign compressor in Zheng et al . ( 2019 ) .The proposed method exploits the temporal correlation between two consecutive gradients . The authors show that one can have a higher compression rate by inserting distortion to the compressed gradient . A tighten bound is provided such that the asymptotic rate ( including constant ) is exactly the same as the full-precision counterpart . The experiments show that the proposed compressor can achieve additional 40 % -50 % reduction on communication compared to the scaled sign . Overall , the reviewer thinks the idea is interesting . The reviewer has a few comments : 1 . The proposed method considers randomly flipping the direction for elements that have the same sign as the averaged gradient in the last step . In this way , the sign is always correct for the elements that have opposite direction from the last gradient . I wonder will the results change if we consider flipping the sign of the elements that have opposite direction ? 2.Since alpha has a very small upper bound , it is hard to see any theoretical improvement over scaled sign . 3.Theorem 4.2 does not show that one can achieve a linear speedup , i.e. , O ( 1/\\sqrt { nT } ) rate . 4.For the distributed training with high speed network , the extra overhead incurred by compression is not trivial and can not be overlooked . As there is no results against CPU wall clock time , it is not clear if the proposed method is really faster than the scaled sign in terms of elapsed time . 5.Can you show the final test accuracies on ImageNet achieved by each algorithm ? It seems that scaled sign has slightly higher accuracy .", "rating": "6: Marginally above acceptance threshold", "reply_text": "In the following we address all comments/questions in the same order . 1.There are two reasons for our particular choice of inverting the signs . We will first summarize the general idea , and then give some specifics in relation to the numerical results provided . The high-level reason for choosing the quantizer we do is that ( cf. , middle plot Figure 2 ) even without quantization $ q\\approx0.45 < 0.5 $ . Since the binary entropy function is symmetric around $ q=0.5 $ and is concave , for a fixed amount of distortion we get more rate savings by designing a quantizer that drives $ q $ towards 0 rather than toward 1 . If we drive $ q $ towards 1 we would need to get over the binary entropy \u201c hump \u201d at 0.5 and to 0.55 before we would start to incur rate savings . By that point we would already have incurred non-negligible distortion . For this reason , we design the quantizer to push $ q $ towards zero . ( i ) In the third sub-figure in Figure 2 we plot $ p $ , the fraction of $ 1 $ 's in vector $ b $ for ImageNet32 experiments . Recall that $ \\alpha=0 $ case recovers the vanilla scaled-sign compression . As per the third sub-figure , $ p $ for $ \\alpha=0 $ stays at around $ 0.45 $ . We observed that $ p $ stays close to but slightly lower than $ 0.5 $ for CIFAR100 experiments as well . We gain compression savings if the binary entropy $ H ( p ) $ is closer to $ 0 $ . The binary entropy is symmetric around $ 0.5 $ . Therefore , we have to choose between two strategies : ( A ) push $ p $ towards $ 0 $ , or , ( B ) push $ p $ towards $ 1 $ by introducing errors to vector $ b $ . The former ( A ) corresponds forcing some of the components with same sign to be different , and the latter ( B ) corresponds to forcing some of the components with different signs to be the same . Since for $ \\alpha=0 $ ( no errors ) we already have $ p $ less than $ 0.5 $ ( say $ 0.45 $ ) we make the choice to push $ p $ towards $ 0 $ by applying ( A ) . Let us assume we were to use ( B ) instead . This means that we will have to introduce errors just to get $ p > 0.5 $ . For example , if we were to end up with $ p=0.55 $ , we will have obtained no gain in terms of compression ( since $ H ( 0.45 ) =H ( 0.55 ) $ ) , although we would already have introduced some errors to $ b $ vector . ( ii ) We also note that , to confirm our design idea , during our research we also ran experiments that pushed q toward 1 . While we did not include those results in the final paper , those systems worked * less * well due to our reasoning in ( i ) . 2.We agree with the reviewer that the upper bound is small . This concern was also raised by the third reviewer ( AnonReviewer3 ) and we responded at length to that reviewer . We ask this reviewer to refer to that response ( please see under the * Addressing the comment about Theorem 1 * heading ) which we are not reproducing here to keep our response more compact . 3.This is due to our choice of the step size . The choice of step size by Zheng et al . ( 2019 ) yields two bounds for dist-EF-SGD and SGD as are presented in Corollary 1 . We observe that although they achieve a linear speedup , both first and the second terms in the dist-EF-SGD bound are larger by constant factors compared to those of the SGD bound ( first two terms in dist-EF-SGD have 4 and 1 , whereas first two terms in SGD have $ \\frac { 8 } { 3 } $ and $ \\frac { 2 } { 3 } $ ) . In comparison , our choice of step size gives us identical first terms for the proposed method and SGD ( our Theorem 2 ) . Not achieving a linear speedup is the price we pay for getting the first terms to match . 4.We agree that the compression overhead may be non-trivial . However , our primary goal is to showcase the reduction of the communication payload . All the steps in in Algorithm 2 can be efficiently implemented using vectorized operations except lossless compression ( and decompression ) of vector $ b $ . The lossless compression overhead heavily depends on the algorithm and the implementation of the algorithm . We employ the Python implementation of the LZMA algorithm in our experiments . Since our setup does not use the most computationally efficient implementation of compression , we do not think it is fair to compare the compression overheads involved and therefore did not include such a comparison . 5.In our original experiments we ran SignXOR only for a slightly greater than 10 epochs due to resource limitations . As per the reviewer \u2019 s request , we have rerun SignXOR $ \\alpha=0.6 $ for a longer time and included an updated figure in Appendix ( Figure 5 on last page in the updated manuscript ) . Currently it shows SignXOR $ \\alpha=0.6 $ only up to the 12th epoch ( still running past the 12th epoch as of the time of this response ) . We hope to update the final test accuracies prior to the end of the rebuttal period ."}, "3": {"review_id": "qOCdZn3lQIJ-3", "review_text": "This paper proposes a gradient compression approach to remove the communication bottleneck in distributed stochastic gradient descent . I think the key attributes of their algorithm are as follows : - It uses error feedback ( Stich et al. , 2018 ; Karimireddy et al. , 2019 ) and operates in a parameter server model based on ( Zheng et al.2019 ) + The compressor sends the sign of each gradient coordinate and a scale factor per 'block ' of coordinates ( like Zheng et al.2019 ) + The messages are compressed with lossless entropy encoding . + Specifically , they send and encode the difference between the current sign vector and the previously transmitted one . This is meant to lower the entropy of the vectors ( make the distribution of -1 's and 1 's less even ) + Because the distribution of -1 's and 1 's is still roughly 50/50 after delta-coding , the authors introduce ( lossy ) noise in the compressor . They randomly flip some instances of 'same sign as before ' to 'different sign than before ' . This reduces the entropy of the 'difference vector ' so it can be compressed more . I believe the main contributions of the paper are : - The introduction and evaluation of lossless compression on top of sign-based gradient compression - A theoretical improvement of the constants in the rates from ( Zheng et al.2019 ) - A proof that SignSGD with delta-coding and a bias towards changing signs can still be a $ \\delta $ -compressor , as long as the bias is extremely small ( not covering the experiments presented in the paper ) I find the ideas presented in this paper interesting and novel and the experiments well executed . The writing is of good quality , and I find it easy to follow . I do , however , have two concerns : - The method is said to exploit temporal correlation in the gradients by using delta coding . To me , this seems misleading . The authors show that , without the introduced bias , there is not much gain from delta coding ( `` In our experiments presented in Section 5.2 we observe that when \u03b1 = 0 , p remains close to but slightly lower than 0.5.This implies low correlation between [ .. ] . Our solution is to make $ \\alpha > 0 $ . `` ) . With the proposed solution , the method exploits patterns created artificially by lossy compression , rather than temporal correlation in the gradients . Given that p < 0.5 , this is actually anti-correlation rather than correlation . The signs are more likely to flip due to previously introduced errors . - If the delta-coding scheme indeed fails to leverage temporal correlation in the gradients without artificially introducing extra errors , the proposed scheme does n't really do what it seems to be designed for . This makes it less elegant/complicated . This lack of simplicity could be compensated by convincing experimental results , but it seems that many gradient compression schemes achieve similar results to the proposed scheme at similar compression rates ( see Xu et al.https : //repository.kaust.edu.sa/bitstream/handle/10754/662495/gradient-compression-survey.pdf ) for an overview ) . I am not convinced by the benefits of the proposed scheme over others .", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * * Addressing the temporal correlation-related comments * * * We thank the reviewer for the comments . The issue raised by the reviewer regarding temporal correlation is very important one that was also shared by the first reviewer ( AnonReviewer4 ) and we responded at length to that reviewer . We ask this reviewer to refer to that response ( please see * Part 1/2 : Addressing concerns regarding the source of rate savings * ) which we are not reproducing here to keep the response more compact . A summary of that response is as follows . We demonstrate using our experimental results that the total compression gain is due to ( i ) lossy compression and ( ii ) temporal correlation . We argue that the temporal correlation is not because of the lossyness ( or distortion ) introduced in the current step , rather due to the introduction of distortion in previous steps . This yields the temporal correlation with the next iteration that we then leverage . In addition to this temporal correlation we add distortion to the current step which gives us more compression . As discussed in the response to AnonReviewer4 , the issues of lossy compression and leveraging temporal correlation are intertwined in our system . * * * Addressing the comment about Theorem 1 * * * We would also like to respond to the reviewer \u2019 s comment about the upper bound for bias ( $ \\alpha $ ) . We agree with the reviewer that the upper bound is small . The reason for this is the extremely small $ \\delta $ that corresponds to the scaled-sign compression . We attribute this to the mismatch one often observes between theory and experiments where the assumptions that need to be made to make the theory work are often more conservative than those that are often observed to work in practice . Next we reason why this is the case in our proposed scheme . Recall that scaled-sign is a $ \\frac { 1 } { d } $ -compressor where $ d $ is the dimension of the input vector . Let us consider the EF-SignSGD algorithm by Karimireddy et al . ( 2019 ) .The convergence bound of EF-SignSGD is given in their Remark 4 . In summary , the difference between the bounds of EF-SignSGD and vanilla SGD is that the former has an additional term with $ 1/\\delta^2 $ . Since $ d $ is in the millions for models of practical interest , we expect the $ 1/\\delta^2 $ term to be extremely large and to have a large impact on the convergence . However , Karimireddy et al . ( 2019 ) show in their experiments that EF-SignSGD performs very close to vanilla SGD from the beginning of the training . In summary , although one expects the convergence to slow down due to the small $ \\delta $ of scaled-sign , it is not quite the case in practice . This is similar to what we observe in our scheme . The proof of our Theorem 1 relies on the fact that scaled-sign is a $ \\frac { 1 } { d } $ -compressor . This constrains the wiggle room for the proposed system to be a $ \\delta $ -compressor ( since we are building on scaled-sign ) , which in turn limits the bound on $ \\alpha $ . However , our experiments demonstrate that contrary to the theoretical upper bound , one can set $ \\alpha $ to be larger and the algorithm can still converge ."}}