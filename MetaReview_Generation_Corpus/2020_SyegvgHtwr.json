{"year": "2020", "forum": "SyegvgHtwr", "title": "Localised Generative Flows", "decision": "Reject", "meta_review": "This paper proposes to overcome some fundamental limitations of normalizing flows by introducing auxiliary continuous latent variables. While the problem this paper is trying to address is mathematically legitimate, there is no strong evidence that this is a relevant problem in practice. Moreover, the proposed solution is not entirely novel, converting the flow in a latent-variable model. Overall, I believe this paper will be of minor relevance to the ICLR community.", "reviews": [{"review_id": "SyegvgHtwr-0", "review_text": "The paper introduces a straight-forward way to expand the flow models by considering mixture of flow distributions. The idea is not very novel since several previous work have tried the mixture of flow such as the mentioned RAD and Deep Mixture. The paper studies some further improvements such as using the continuous auxiliary variable and stacking multiple mixture layers. The major concerns are the following: 1) The paper tries to solve a \u201cproblem\u201d build upon intuition. The paper explains as \u201cthe normalizing flow places global constraint on the bijection\u201d, \u201cit need to match the topology of X to the topology of Z \u201d, \u201ccontinuous functions necessarily preserve topology\u201d. What kind of topological properties are referred to here? Are all topological properties preserved under continuous function? It needs to be more accurate when using such terminologies. The intuition of the paper is weak and heuristic. The example in Figure 1 can potentially be easily solved with a two component Gaussian mixture of input Z to a vanilla flow model. 2) For the mixture p(X), will the proposed method generate samples concentrated on one or some of the components? Why or why not? 3) Considering there are a plenty of improvements of flow models, it is neccesary for the proposed method to compare with, at least for some methods explained in the Related Work section. 4) Since the proposed methods inevitably lose the advantage of analytic density property of flow methods, it is better to show some advantage over implicit or semi-implicit methods. For example, (https://arxiv.org/abs/1805.11183) also uses a hierarchical model with continuous auxiliary variables and a marginalization similar to Eq.(4) in this paper. How does the proposed method related to or compared to these methods? 5) In experiment section, in Table 1, the proposed method is worse than MAF for 2 datasets out of 4? In Table 2, what are the numbers refer to? 6) On page 4, it should be p(U|Z) not p(U|X)? In sum, I think though the paper makes contribution on exploring better flow models but the novelty is relatively weak, the discussion and comparison of related work is insufficient and the experiments are not convincing or have mistakes. I think a modification is necessary before publishing. ################ I have read the author's feedback. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your review . We first address the claim that LGFs lack novelty . Please see above our discussion about RAD . In particular , our use of continuous mixing variables is not simply an incremental design choice , but provides a method for circumventing the computational cost which is exponentially increasing with the depth of a standard deep mixture model ( as mentioned in the Appendix and related work section ) _without_ introducing discontinuities into the loss function or explicit partitioning schemes . Moreover , continuous mixing variables require a significantly different approach to training ( namely a variational scheme ) which further distinguishes LGFs from these other methods . Please see below our responses to your other points : 1 ) Regarding the claim that our paper is pure intuition , please see the discussion above . As we discuss there , although we did not previously formulate our discussion in terms of concrete mathematical statements , we did take care to be precise where this was possible . In particular , note that when we talk about `` preserving topology '' , we mean so in the well-defined mathematical sense that the topology ( i.e.the open sets ) of a space is preserved under continuous mappings that have continuous inverses . Such functions indeed preserve _every_ topological characteristic , including properties such as connectedness , compactness , genus ( i.e.the number of `` holes '' in the space ) , etc . These are standard mathematical concepts , and we refer you to https : //en.wikipedia.org/wiki/Homeomorphism for more information and references . Regarding the comment that the example in Figure 1 would be fixed by a simple mixture model - we acknowledge this directly in the 4th paragraph of section 2 . However , as we argue there , this sort of approach does not scale to complicated datasets ( like CIFAR10 ) where the topology of the target is completely opaque . In these instances we would like a method that can somehow learn the topology of the target on its own . 2 ) As we discuss in section 2 , the maximum likelihood objective corresponds to a mode-covering KL objective . In other words , the loss function is encouraged to ensure the support of our model covers all the modes of the target . This is a standard feature of likelihood-based training and is not specific to our model . 3 ) Refer to the general comment above for a discussion on testing the improvement LGFs make on other flow models . As for other variational improvements on flow models , those are discussed in the related work section . These methods are either outside of the scope of LGFs ( e.g.Das et al. , ( 2019 ) , Gritsenko et al . ( 2019 ) ) , orthogonal improvements that could be combined with LGFs ( e.g.Ho et al . ( 2019 ) ) , or RAD ( Dinh at al. , 2019 ) which is discussed above . 4 ) The linked work ( Semi-Implicit Variational Inference ( SIVI ) ) may indeed seem superficially similar , but it is notably different in both its motivation and application . In particular , SIVI is a method which looks to improve the approximate variational distribution in variational inference ; LGF is a method which looks to improve the bijection approach in density estimation using normalizing flows . LGFs require a variational scheme to optimize their parameters ( for which methods like SIVI could potentially be useful ) , but this is different from considering LGFs to be a variational method on their own . Plus SIVI has no notion of stacking models to obtain even greater expressiveness - there is only one hierarchical layer . GANs are another instance of an implicit model , but they provide no reliable method to approximate log-likelihoods . 5 ) It is not clear what is referred to here . Note that we report the average test set log-likelihood , for which higher is better . In all cases in Table 1 , the log-likelihood is higher for LGF-MAF than for MAF . 6 ) Thank you for pointing this out . We have corrected this typo . Finally , we wonder whether you can comment further on the mistakes you perceive in our experiments ? Please note point 5 ) above ."}, {"review_id": "SyegvgHtwr-1", "review_text": "The authors propose to extend flow-based density models by replacing a single bijection with a hierarchical mixture of bijections. Each component in the mixture is then required to only push the prior onto a local region; this helps improve the coverage of $\\mathcal{X}$. This is motivated by the conjecture that in many cases the topology of $\\mathcal{X}$ might be overly complicated to be effectively captured by a single bijection. Formally, this is achieved by introducing a conditional random variable $U|Z$. In doing however, the log-likelihood is rendered intractable and a variational approximation must instead be resorted to. A recursive formula for computing the ELBO is introduced in this vain. Overall, I think this is a generally interesting contribution to the normalizing-flow literature that I expect to spark further research. However, there are some rough edges to this paper. The initial motivation is well-presented and relatively easy to follow, though a diagram would serve to cement the intuition regarding the support mismatch. The issue mentioned in footnote 2 deserves further discussion. At the same time, while well-reasoned, their justifications are nonetheless largely conjectural and further theoretical or empirical evidence would be welcome, both for characterising the pathologies they aim to redress and their proposed solution. For the experiment section, I would have liked to have seen comparisons not only to the simplest baseline but also to some of the other methods mentioned in related works. In general, the experiment section is quite short and I didn't get a very good sense of how well this method performs. The following should be addressed: - provide more evidence for the conjectures surrounding the motivation and derivation - supply more varied baselines (e.g. RAD model) Minor comments: - at the top of page 4, you refer to $p_{U|X}$ several times, but I think you mean $p_{U|Z}$ - in the paragraph after equation (2), the $\\theta$ superscript seems to be missing from the $p_X^\\theta$ - in the first sentence of section 3.1, you refer to \"the single $g$ used in equation 2\", but equation 2 mentions no $g$ - in the first line on page 3, you talk about some region of supp $p_X^\\theta$ being pushed out of supp $p_X^*$, shouldn't this be the other way round since the KL is infinite only if the the support of $p_x^*$ is not contained within the support of $p_X^\\theta$?", "rating": "3: Weak Reject", "reply_text": "Thank you for your review . Regarding your points about the theoretical underpinnings and empirical evaluation of our method , please see our general replies in the Common thread above . Thank you very much for your other feedback also . We agree a diagram could help to convey intuition . We have also fixed the typos and clarity issues that you have pointed out in the uploaded version of the paper ."}, {"review_id": "SyegvgHtwr-2", "review_text": "Summary: This paper conjectures that normalizing flows are fundamentally limited due to the architecture assumption that the generative function g is continuous in x. It is argued that this constraint makes maximum likelihood estimation difficult in general. Localised generative flows are proposed as a solution and consist in modeling the generative model as a continuous mixture of bijections. Experiments suggest an improvement over MAF. Decision: The observation that continuity imposes a hard constraint on the network is sound, and the proposed solution appears to show some improvement. However, in its current state, this work appears to be quite fragile both from a theoretical and experimental point of view. First, it is only conjectured that this constraint poses actual problems. Second, the experimental evaluation is weak and insufficient. It omits comparisons with more recent generative flows that have shown to be able to model discontinuous densities. For this reason, I do not recommend the paper for acceptance. Further arguments: - The whole paper rests on intuition without strong theoretical backup. - The experiments are quite poor and results frankly oversold. It is said the method \"improves performance across a variety of common density benchmarks\". While we see improvements in Table 1 over MAF, the comparison omits all recent architectures based on Normalizing Flows, such as TAN (Olivia et al, 2018), NAF (Huang et al, 2018), B-NAF (De Cao et al, 2019) or SOS (Jaini et al, 2019). All of those methods have reported better results than those provided in Table 1. They have also been shown empirically to work for discontinuous densities. While I understand that LGF can be combined with any flow architecture, the question remains whether using a continuous mixture translates into significant improvements for those baselines as well. The experimental benchmarks also omit datasets such as BSDS300, for which the higher dimensionality is usually challenging. The same goes for Table 2 which omits recent and better results, such as Glow or FFJORD. - Closer to LGF, a proper experimental comparison to RAD (Dinh et al, 2019) would be appreciated. - The proposed architecture supposedly enables better generative models. However, this comes at the price that the density can no longer be evaluated exactly and analytically. Since normalizing flows are also typically slow for sampling, this makes the benefits of the proposed architecture quite limited. In particular, it is not clear why generative models that are good at sampling only (e.g., GANs) should not then be preferred? - As a result of the point above, the experimental results are reported only in terms of approximated negative log-likelihood. I do not think this is fair, since models like MAF do provide exact values. It also makes the comparison with previous methods more difficult. Further feedback: - As per ICLR policy, higher standards should be applied to papers with 9 or more pages. I am confident the paper could be written within 8 pages only. ", "rating": "1: Reject", "reply_text": "Thank you for your review . First and foremost , we wish to state our objection to certain uncharitable comments made within this review . Regarding your claim that our experiments are oversold -- we respectfully disagree with this . The statement of ours that you quote describes improved performance on a variety of _tasks_ . We stand by this claim : compared with the standard flow-based baselines that we consider , we found that LGFs do improve density estimation for 2D densities , real-world tabular data , and high-dimensional image data , all of which have very distinct structures and dimensionalities . Please see our general reply above for further discussion of this point , as well as further empirical results . In a similar vein , when describing our contribution , you state that our `` ( e ) xperiments suggest an improvement over MAF '' . We again refer you to our discussion above , and emphasise that , for the models considered , our results demonstrate that our method yields significant and unambiguous benefit . Moreover , in addition to MAF , which we apply to tabular data , we also consider a large-scale RealNVP model that makes use of fully convolutional networks and a multi-scale architecture . We believe these changes yield a density model with a significantly different structure and characteristics to MAF , and it bears emphasising that our method provides benefit within this quite distinct context also . In response to your claim that our paper is purely based on intuition , we refer you to our general reply above . Regarding BSDS300 - although we omitted this benchmark , we did consider Fashion-MNIST and CIFAR10 , both of which have far higher dimensionality than this dataset ( 784 and 3072 as opposed to 63 dimensions ) . Even with this increase in dimension , LGFs still yielded a performance benefit over the baselines we considered . Please also see above for our results using Glow that we have obtained subsequently . Regarding RAD - please see above . Finally , regarding your points about the inexact log-likelihoods : as we argue in section 3.3.1 of the paper , this does not pose a major limitation for our method . When log-likelihoods are required at evaluation time , it is straightforward to obtain an estimate using importance sampling as described . This estimate is consistent in the sense that it is possible to achieve as much accuracy as desired simply by taking more importance samples . This approach is standard , for example , within the VAE literature -- and indeed your comment would seem to apply equally to all of these models as well as ours . For implicit models like GANs , no straightforward estimate of the likelihood is available at all . In this setting it is also typically impractical to estimate the latent distribution $ p ( z|x ) $ for a given $ x $ , which can be useful for downstream tasks . We also mention that various normalising flow models exist that do provide fast sampling as well as density estimation -- for instance , RealNVP , which we also consider in this paper . Finally , we have uploaded a revision of the paper that is not longer than 8 pages ."}], "0": {"review_id": "SyegvgHtwr-0", "review_text": "The paper introduces a straight-forward way to expand the flow models by considering mixture of flow distributions. The idea is not very novel since several previous work have tried the mixture of flow such as the mentioned RAD and Deep Mixture. The paper studies some further improvements such as using the continuous auxiliary variable and stacking multiple mixture layers. The major concerns are the following: 1) The paper tries to solve a \u201cproblem\u201d build upon intuition. The paper explains as \u201cthe normalizing flow places global constraint on the bijection\u201d, \u201cit need to match the topology of X to the topology of Z \u201d, \u201ccontinuous functions necessarily preserve topology\u201d. What kind of topological properties are referred to here? Are all topological properties preserved under continuous function? It needs to be more accurate when using such terminologies. The intuition of the paper is weak and heuristic. The example in Figure 1 can potentially be easily solved with a two component Gaussian mixture of input Z to a vanilla flow model. 2) For the mixture p(X), will the proposed method generate samples concentrated on one or some of the components? Why or why not? 3) Considering there are a plenty of improvements of flow models, it is neccesary for the proposed method to compare with, at least for some methods explained in the Related Work section. 4) Since the proposed methods inevitably lose the advantage of analytic density property of flow methods, it is better to show some advantage over implicit or semi-implicit methods. For example, (https://arxiv.org/abs/1805.11183) also uses a hierarchical model with continuous auxiliary variables and a marginalization similar to Eq.(4) in this paper. How does the proposed method related to or compared to these methods? 5) In experiment section, in Table 1, the proposed method is worse than MAF for 2 datasets out of 4? In Table 2, what are the numbers refer to? 6) On page 4, it should be p(U|Z) not p(U|X)? In sum, I think though the paper makes contribution on exploring better flow models but the novelty is relatively weak, the discussion and comparison of related work is insufficient and the experiments are not convincing or have mistakes. I think a modification is necessary before publishing. ################ I have read the author's feedback. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your review . We first address the claim that LGFs lack novelty . Please see above our discussion about RAD . In particular , our use of continuous mixing variables is not simply an incremental design choice , but provides a method for circumventing the computational cost which is exponentially increasing with the depth of a standard deep mixture model ( as mentioned in the Appendix and related work section ) _without_ introducing discontinuities into the loss function or explicit partitioning schemes . Moreover , continuous mixing variables require a significantly different approach to training ( namely a variational scheme ) which further distinguishes LGFs from these other methods . Please see below our responses to your other points : 1 ) Regarding the claim that our paper is pure intuition , please see the discussion above . As we discuss there , although we did not previously formulate our discussion in terms of concrete mathematical statements , we did take care to be precise where this was possible . In particular , note that when we talk about `` preserving topology '' , we mean so in the well-defined mathematical sense that the topology ( i.e.the open sets ) of a space is preserved under continuous mappings that have continuous inverses . Such functions indeed preserve _every_ topological characteristic , including properties such as connectedness , compactness , genus ( i.e.the number of `` holes '' in the space ) , etc . These are standard mathematical concepts , and we refer you to https : //en.wikipedia.org/wiki/Homeomorphism for more information and references . Regarding the comment that the example in Figure 1 would be fixed by a simple mixture model - we acknowledge this directly in the 4th paragraph of section 2 . However , as we argue there , this sort of approach does not scale to complicated datasets ( like CIFAR10 ) where the topology of the target is completely opaque . In these instances we would like a method that can somehow learn the topology of the target on its own . 2 ) As we discuss in section 2 , the maximum likelihood objective corresponds to a mode-covering KL objective . In other words , the loss function is encouraged to ensure the support of our model covers all the modes of the target . This is a standard feature of likelihood-based training and is not specific to our model . 3 ) Refer to the general comment above for a discussion on testing the improvement LGFs make on other flow models . As for other variational improvements on flow models , those are discussed in the related work section . These methods are either outside of the scope of LGFs ( e.g.Das et al. , ( 2019 ) , Gritsenko et al . ( 2019 ) ) , orthogonal improvements that could be combined with LGFs ( e.g.Ho et al . ( 2019 ) ) , or RAD ( Dinh at al. , 2019 ) which is discussed above . 4 ) The linked work ( Semi-Implicit Variational Inference ( SIVI ) ) may indeed seem superficially similar , but it is notably different in both its motivation and application . In particular , SIVI is a method which looks to improve the approximate variational distribution in variational inference ; LGF is a method which looks to improve the bijection approach in density estimation using normalizing flows . LGFs require a variational scheme to optimize their parameters ( for which methods like SIVI could potentially be useful ) , but this is different from considering LGFs to be a variational method on their own . Plus SIVI has no notion of stacking models to obtain even greater expressiveness - there is only one hierarchical layer . GANs are another instance of an implicit model , but they provide no reliable method to approximate log-likelihoods . 5 ) It is not clear what is referred to here . Note that we report the average test set log-likelihood , for which higher is better . In all cases in Table 1 , the log-likelihood is higher for LGF-MAF than for MAF . 6 ) Thank you for pointing this out . We have corrected this typo . Finally , we wonder whether you can comment further on the mistakes you perceive in our experiments ? Please note point 5 ) above ."}, "1": {"review_id": "SyegvgHtwr-1", "review_text": "The authors propose to extend flow-based density models by replacing a single bijection with a hierarchical mixture of bijections. Each component in the mixture is then required to only push the prior onto a local region; this helps improve the coverage of $\\mathcal{X}$. This is motivated by the conjecture that in many cases the topology of $\\mathcal{X}$ might be overly complicated to be effectively captured by a single bijection. Formally, this is achieved by introducing a conditional random variable $U|Z$. In doing however, the log-likelihood is rendered intractable and a variational approximation must instead be resorted to. A recursive formula for computing the ELBO is introduced in this vain. Overall, I think this is a generally interesting contribution to the normalizing-flow literature that I expect to spark further research. However, there are some rough edges to this paper. The initial motivation is well-presented and relatively easy to follow, though a diagram would serve to cement the intuition regarding the support mismatch. The issue mentioned in footnote 2 deserves further discussion. At the same time, while well-reasoned, their justifications are nonetheless largely conjectural and further theoretical or empirical evidence would be welcome, both for characterising the pathologies they aim to redress and their proposed solution. For the experiment section, I would have liked to have seen comparisons not only to the simplest baseline but also to some of the other methods mentioned in related works. In general, the experiment section is quite short and I didn't get a very good sense of how well this method performs. The following should be addressed: - provide more evidence for the conjectures surrounding the motivation and derivation - supply more varied baselines (e.g. RAD model) Minor comments: - at the top of page 4, you refer to $p_{U|X}$ several times, but I think you mean $p_{U|Z}$ - in the paragraph after equation (2), the $\\theta$ superscript seems to be missing from the $p_X^\\theta$ - in the first sentence of section 3.1, you refer to \"the single $g$ used in equation 2\", but equation 2 mentions no $g$ - in the first line on page 3, you talk about some region of supp $p_X^\\theta$ being pushed out of supp $p_X^*$, shouldn't this be the other way round since the KL is infinite only if the the support of $p_x^*$ is not contained within the support of $p_X^\\theta$?", "rating": "3: Weak Reject", "reply_text": "Thank you for your review . Regarding your points about the theoretical underpinnings and empirical evaluation of our method , please see our general replies in the Common thread above . Thank you very much for your other feedback also . We agree a diagram could help to convey intuition . We have also fixed the typos and clarity issues that you have pointed out in the uploaded version of the paper ."}, "2": {"review_id": "SyegvgHtwr-2", "review_text": "Summary: This paper conjectures that normalizing flows are fundamentally limited due to the architecture assumption that the generative function g is continuous in x. It is argued that this constraint makes maximum likelihood estimation difficult in general. Localised generative flows are proposed as a solution and consist in modeling the generative model as a continuous mixture of bijections. Experiments suggest an improvement over MAF. Decision: The observation that continuity imposes a hard constraint on the network is sound, and the proposed solution appears to show some improvement. However, in its current state, this work appears to be quite fragile both from a theoretical and experimental point of view. First, it is only conjectured that this constraint poses actual problems. Second, the experimental evaluation is weak and insufficient. It omits comparisons with more recent generative flows that have shown to be able to model discontinuous densities. For this reason, I do not recommend the paper for acceptance. Further arguments: - The whole paper rests on intuition without strong theoretical backup. - The experiments are quite poor and results frankly oversold. It is said the method \"improves performance across a variety of common density benchmarks\". While we see improvements in Table 1 over MAF, the comparison omits all recent architectures based on Normalizing Flows, such as TAN (Olivia et al, 2018), NAF (Huang et al, 2018), B-NAF (De Cao et al, 2019) or SOS (Jaini et al, 2019). All of those methods have reported better results than those provided in Table 1. They have also been shown empirically to work for discontinuous densities. While I understand that LGF can be combined with any flow architecture, the question remains whether using a continuous mixture translates into significant improvements for those baselines as well. The experimental benchmarks also omit datasets such as BSDS300, for which the higher dimensionality is usually challenging. The same goes for Table 2 which omits recent and better results, such as Glow or FFJORD. - Closer to LGF, a proper experimental comparison to RAD (Dinh et al, 2019) would be appreciated. - The proposed architecture supposedly enables better generative models. However, this comes at the price that the density can no longer be evaluated exactly and analytically. Since normalizing flows are also typically slow for sampling, this makes the benefits of the proposed architecture quite limited. In particular, it is not clear why generative models that are good at sampling only (e.g., GANs) should not then be preferred? - As a result of the point above, the experimental results are reported only in terms of approximated negative log-likelihood. I do not think this is fair, since models like MAF do provide exact values. It also makes the comparison with previous methods more difficult. Further feedback: - As per ICLR policy, higher standards should be applied to papers with 9 or more pages. I am confident the paper could be written within 8 pages only. ", "rating": "1: Reject", "reply_text": "Thank you for your review . First and foremost , we wish to state our objection to certain uncharitable comments made within this review . Regarding your claim that our experiments are oversold -- we respectfully disagree with this . The statement of ours that you quote describes improved performance on a variety of _tasks_ . We stand by this claim : compared with the standard flow-based baselines that we consider , we found that LGFs do improve density estimation for 2D densities , real-world tabular data , and high-dimensional image data , all of which have very distinct structures and dimensionalities . Please see our general reply above for further discussion of this point , as well as further empirical results . In a similar vein , when describing our contribution , you state that our `` ( e ) xperiments suggest an improvement over MAF '' . We again refer you to our discussion above , and emphasise that , for the models considered , our results demonstrate that our method yields significant and unambiguous benefit . Moreover , in addition to MAF , which we apply to tabular data , we also consider a large-scale RealNVP model that makes use of fully convolutional networks and a multi-scale architecture . We believe these changes yield a density model with a significantly different structure and characteristics to MAF , and it bears emphasising that our method provides benefit within this quite distinct context also . In response to your claim that our paper is purely based on intuition , we refer you to our general reply above . Regarding BSDS300 - although we omitted this benchmark , we did consider Fashion-MNIST and CIFAR10 , both of which have far higher dimensionality than this dataset ( 784 and 3072 as opposed to 63 dimensions ) . Even with this increase in dimension , LGFs still yielded a performance benefit over the baselines we considered . Please also see above for our results using Glow that we have obtained subsequently . Regarding RAD - please see above . Finally , regarding your points about the inexact log-likelihoods : as we argue in section 3.3.1 of the paper , this does not pose a major limitation for our method . When log-likelihoods are required at evaluation time , it is straightforward to obtain an estimate using importance sampling as described . This estimate is consistent in the sense that it is possible to achieve as much accuracy as desired simply by taking more importance samples . This approach is standard , for example , within the VAE literature -- and indeed your comment would seem to apply equally to all of these models as well as ours . For implicit models like GANs , no straightforward estimate of the likelihood is available at all . In this setting it is also typically impractical to estimate the latent distribution $ p ( z|x ) $ for a given $ x $ , which can be useful for downstream tasks . We also mention that various normalising flow models exist that do provide fast sampling as well as density estimation -- for instance , RealNVP , which we also consider in this paper . Finally , we have uploaded a revision of the paper that is not longer than 8 pages ."}}