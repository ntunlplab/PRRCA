{"year": "2017", "forum": "Hkz6aNqle", "title": "Deep Error-Correcting Output Codes", "decision": "Reject", "meta_review": "The reviewers unanimously recommend rejecting this paper.", "reviews": [{"review_id": "Hkz6aNqle-0", "review_text": "The paper proposes a greedy supervised layer-wise initialization strategy for (deep) multi-layer perceptrons. Layer weights are initialized by training linear SVMs for binary classification where the binary targets are constructed as error correcting codes (ECOC, including one-vs-all, one-vs-one and others). The thus pertained model (together with a softmax output layer) is then globally fine-tuned by backdrop with dropout. Note that as a heuristic greedy supervised layer-wise initialization strategy this work is very similar to the author\u2019s other ICLR submission: \u00ab Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications \u00bb. The two works differ in the supervised initialization strategy employed. While layer-wise initialization strategies are worthy of further exploration, the paper doesn\u2019t convey any insight as to what makes a better strategy. Experimental results are not sufficiently convincing by themselves alone to support a mostly incremental work; in particular I remain unconvinced that competing methods received full proper hyper-parameter tuning of their own. Results showing accuracies as bar graphs make it hard to read-off precise accuracies, and one cannot easily compare with known state-of-the-art performance references on benchmark problems (such as MNIST). CIFAR10 performance seem far from state-of-the-art. Explanations are unnecessarily detailed for standard algorithms (e.g. SVMs) and not sufficiently for aspects specific to the approach such as lesser known ECOC schemes. One important aspect remains unclear regarding the use of SVMs. Did you use linear SVMs as stated in section 3.1 (\u00ab In order to take the probabilistic outputs of the base classifiers as new representations of data, we adopt linear support vector machines (linear SVMs) as the binary classifiers \u00bb) or kernel SVMs as mentioned later \u00ab For all DeepECOC models, we used support vector machines (SVMs) with RBF kernel function\u00bb. In the latter case, the paper lacks a description of how learned RBF-kernel SVMs are transferred to a deep network layer (does it yield 2 layers the firrst of which would be a large RBF neural layer?) Also in this case of kernel SVMs the computational cost is likely to skyrocket and the method will have scaling issues. Is this the reason why the method is too expensive to use on CIFAR10 from scratch, and prompts doing LBP first? If you used linear SVMs, did you use an efficient implementation specific to linear SVMS (as opposed to generic kernel SVM code with a linear kernel?). Finally for image datasets, a visual comparison of learned filters could help provide some qualitative insight. ", "rating": "3: Clear rejection", "reply_text": "Many thanks for your review ! Firstly , this paper and the other ICLR submission about MDA are quite different , in the aspect of motivation and experiments . Secondly , in this paper , we extend traditional ECOC to a deep model and solve the problem that previous deep learning methods only initialized randomly or using unsupervised pre-training . This is the main contribution of our work . We think the reviewer may pay more attention to the experiments but neglect our main contribution . Thirdly , although we used CIFAR-10 for comparison , please note that the experimental settings are different from the regular ones . Here , we apply the compared methods on the LBP features of the images . Finally , sorry for the typos in Section 3.1 . In fact , we use RBF SVMs in all the experiments . We extract the LBP features of images is because some deep learning methods , such as autoencoders , denoising autoencoders and DeepECOC , can only be applied on vector inputs . For the efficient learning of SVMs , it 's not in the scope of this paper ."}, {"review_id": "Hkz6aNqle-1", "review_text": "Error correcting output coding is well established supervised learning approach. Stacking several layers of ECOC seems a natural way of extending the framework to deep learning. The main motivation of the authors here is to reduce the sample complexity of internal binary classifiers by using ternary coding and to learn discriminative representations at every layer thanks to the local supervision. The main idea of this work is interesting. However, the presentation can be improved and several sections (about SVMs for example) can be shortened. I am not convinced by the advantage of this approach compared to end-to-end training of neural networks of other layer-wise training of deep architecture. In particular, when the codes at every layer can are randomly generated the induced binary problems can be arbitrarily difficult and this could lead to learnability issues hence affecting the quality of the features. While this problem is counterbalanced by the decoding scheme (and error-correction) at the prediction layer, it is not clear how it should be dealt with in the intermediate layers. In addition, other approaches such as stacked RBMs or Autoencoders learn regularities capturing information about the distribution of the data at every layer. In comparison, it is not clear what the learned representation by an ECOC layer is unless it is supervised (One versus Rest). Overall, though interesting, I am not convinced by the proposed approach and the experimental section does not help mitigate the impression. More insights on the learned internal representations, and experiments using standard datasets could help. ", "rating": "3: Clear rejection", "reply_text": "Many thanks for your review ! Firstly , thank you for pointing out the writing problems . We will improve our paper accordingly . Secondly , please just consider ECOC as a building block in DeepECOC , like RBMs in deep autoencoders . The advantages using ECOC are : 1 ) its learning is supervised ( we proposed a supervised layer-wise pre-training method ) ; 2 ) for the pre-training of some weights , according to the ECOC coding , some classes of data may not be used ; 3 ) we can use random dense or random sparse ECOC coding design , which is equivalent to random initialization of the network structure ( the weights are still learned based on the binary classsifiers in the ECOC module ) . So sorry for the misleading ! It 's really difficult to clarify so many concepts in one paper ."}, {"review_id": "Hkz6aNqle-2", "review_text": "The paper proposes a strategy to pre-train the successive layers of a multi-layer perceptron for classification tasks. It consists in training successively each layer to predict an ECOC corresponding to the classification problem. The first layer predicts this ECOC from the input data and each successive layer takes as output its preceding layer to predict another ECOC. Different regularization strategies are used during training (input noise, Dropout). The MLP is then fine-tuned using SGD. Comparisons are performed with baselines on different datasets. This is a preliminary work. The idea might be valuable but it should be pushed further. Concerning the writing, sections concerning well known notions (e.g. SVM) could be suppressed. Since ECOC are central to this contribution, a more detailed description of the different ECOC strategies would be helpful. The experiments do not compare the proposed model with state of the art classifiers. The experimental conditions should be made more precise, e.g., it is not clear whether the regularization strategies have been used for all the NN architectures or only for the ECOC based ones. Finally note that there are several papers that try to learn intermediate representations for classification problems, see e.g. Samy Bengio, Jason Weston, David Grangier: Label Embedding Trees for Large Multi-Class Tasks. NIPS 2010: 163-171 ", "rating": "3: Clear rejection", "reply_text": "Many thanks for your comments ! Firstly , there are very few papers to consider supervised layer-wise pre-traing and deep ensemble learning . Hence , from the perspective of machine learning , this paper is not only a preliminary work . Secondly , thanks for pointing out the writing problems . We 'd like to revise this paper accordingly . Thirdly , we have compared the proposed method , DeepECOC , with deep autoencoder ( AE ) , stacked denoising autoencoder ( DAE ) , LeNet and PCANet . Since DeepECOC is generally applied to inputs of the vector form , we think the comparison results are sufficient to show its advantages . Fourthly , the experimental conditions are applied to all the compared methods . Finally , thanks for the suggested NIPS paper . In fact , we have more interests to learn end-to-end deep architectures ."}], "0": {"review_id": "Hkz6aNqle-0", "review_text": "The paper proposes a greedy supervised layer-wise initialization strategy for (deep) multi-layer perceptrons. Layer weights are initialized by training linear SVMs for binary classification where the binary targets are constructed as error correcting codes (ECOC, including one-vs-all, one-vs-one and others). The thus pertained model (together with a softmax output layer) is then globally fine-tuned by backdrop with dropout. Note that as a heuristic greedy supervised layer-wise initialization strategy this work is very similar to the author\u2019s other ICLR submission: \u00ab Marginal Deep Architectures: Deep learning for Small and Middle Scale Applications \u00bb. The two works differ in the supervised initialization strategy employed. While layer-wise initialization strategies are worthy of further exploration, the paper doesn\u2019t convey any insight as to what makes a better strategy. Experimental results are not sufficiently convincing by themselves alone to support a mostly incremental work; in particular I remain unconvinced that competing methods received full proper hyper-parameter tuning of their own. Results showing accuracies as bar graphs make it hard to read-off precise accuracies, and one cannot easily compare with known state-of-the-art performance references on benchmark problems (such as MNIST). CIFAR10 performance seem far from state-of-the-art. Explanations are unnecessarily detailed for standard algorithms (e.g. SVMs) and not sufficiently for aspects specific to the approach such as lesser known ECOC schemes. One important aspect remains unclear regarding the use of SVMs. Did you use linear SVMs as stated in section 3.1 (\u00ab In order to take the probabilistic outputs of the base classifiers as new representations of data, we adopt linear support vector machines (linear SVMs) as the binary classifiers \u00bb) or kernel SVMs as mentioned later \u00ab For all DeepECOC models, we used support vector machines (SVMs) with RBF kernel function\u00bb. In the latter case, the paper lacks a description of how learned RBF-kernel SVMs are transferred to a deep network layer (does it yield 2 layers the firrst of which would be a large RBF neural layer?) Also in this case of kernel SVMs the computational cost is likely to skyrocket and the method will have scaling issues. Is this the reason why the method is too expensive to use on CIFAR10 from scratch, and prompts doing LBP first? If you used linear SVMs, did you use an efficient implementation specific to linear SVMS (as opposed to generic kernel SVM code with a linear kernel?). Finally for image datasets, a visual comparison of learned filters could help provide some qualitative insight. ", "rating": "3: Clear rejection", "reply_text": "Many thanks for your review ! Firstly , this paper and the other ICLR submission about MDA are quite different , in the aspect of motivation and experiments . Secondly , in this paper , we extend traditional ECOC to a deep model and solve the problem that previous deep learning methods only initialized randomly or using unsupervised pre-training . This is the main contribution of our work . We think the reviewer may pay more attention to the experiments but neglect our main contribution . Thirdly , although we used CIFAR-10 for comparison , please note that the experimental settings are different from the regular ones . Here , we apply the compared methods on the LBP features of the images . Finally , sorry for the typos in Section 3.1 . In fact , we use RBF SVMs in all the experiments . We extract the LBP features of images is because some deep learning methods , such as autoencoders , denoising autoencoders and DeepECOC , can only be applied on vector inputs . For the efficient learning of SVMs , it 's not in the scope of this paper ."}, "1": {"review_id": "Hkz6aNqle-1", "review_text": "Error correcting output coding is well established supervised learning approach. Stacking several layers of ECOC seems a natural way of extending the framework to deep learning. The main motivation of the authors here is to reduce the sample complexity of internal binary classifiers by using ternary coding and to learn discriminative representations at every layer thanks to the local supervision. The main idea of this work is interesting. However, the presentation can be improved and several sections (about SVMs for example) can be shortened. I am not convinced by the advantage of this approach compared to end-to-end training of neural networks of other layer-wise training of deep architecture. In particular, when the codes at every layer can are randomly generated the induced binary problems can be arbitrarily difficult and this could lead to learnability issues hence affecting the quality of the features. While this problem is counterbalanced by the decoding scheme (and error-correction) at the prediction layer, it is not clear how it should be dealt with in the intermediate layers. In addition, other approaches such as stacked RBMs or Autoencoders learn regularities capturing information about the distribution of the data at every layer. In comparison, it is not clear what the learned representation by an ECOC layer is unless it is supervised (One versus Rest). Overall, though interesting, I am not convinced by the proposed approach and the experimental section does not help mitigate the impression. More insights on the learned internal representations, and experiments using standard datasets could help. ", "rating": "3: Clear rejection", "reply_text": "Many thanks for your review ! Firstly , thank you for pointing out the writing problems . We will improve our paper accordingly . Secondly , please just consider ECOC as a building block in DeepECOC , like RBMs in deep autoencoders . The advantages using ECOC are : 1 ) its learning is supervised ( we proposed a supervised layer-wise pre-training method ) ; 2 ) for the pre-training of some weights , according to the ECOC coding , some classes of data may not be used ; 3 ) we can use random dense or random sparse ECOC coding design , which is equivalent to random initialization of the network structure ( the weights are still learned based on the binary classsifiers in the ECOC module ) . So sorry for the misleading ! It 's really difficult to clarify so many concepts in one paper ."}, "2": {"review_id": "Hkz6aNqle-2", "review_text": "The paper proposes a strategy to pre-train the successive layers of a multi-layer perceptron for classification tasks. It consists in training successively each layer to predict an ECOC corresponding to the classification problem. The first layer predicts this ECOC from the input data and each successive layer takes as output its preceding layer to predict another ECOC. Different regularization strategies are used during training (input noise, Dropout). The MLP is then fine-tuned using SGD. Comparisons are performed with baselines on different datasets. This is a preliminary work. The idea might be valuable but it should be pushed further. Concerning the writing, sections concerning well known notions (e.g. SVM) could be suppressed. Since ECOC are central to this contribution, a more detailed description of the different ECOC strategies would be helpful. The experiments do not compare the proposed model with state of the art classifiers. The experimental conditions should be made more precise, e.g., it is not clear whether the regularization strategies have been used for all the NN architectures or only for the ECOC based ones. Finally note that there are several papers that try to learn intermediate representations for classification problems, see e.g. Samy Bengio, Jason Weston, David Grangier: Label Embedding Trees for Large Multi-Class Tasks. NIPS 2010: 163-171 ", "rating": "3: Clear rejection", "reply_text": "Many thanks for your comments ! Firstly , there are very few papers to consider supervised layer-wise pre-traing and deep ensemble learning . Hence , from the perspective of machine learning , this paper is not only a preliminary work . Secondly , thanks for pointing out the writing problems . We 'd like to revise this paper accordingly . Thirdly , we have compared the proposed method , DeepECOC , with deep autoencoder ( AE ) , stacked denoising autoencoder ( DAE ) , LeNet and PCANet . Since DeepECOC is generally applied to inputs of the vector form , we think the comparison results are sufficient to show its advantages . Fourthly , the experimental conditions are applied to all the compared methods . Finally , thanks for the suggested NIPS paper . In fact , we have more interests to learn end-to-end deep architectures ."}}