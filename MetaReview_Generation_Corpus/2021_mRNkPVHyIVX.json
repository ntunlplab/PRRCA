{"year": "2021", "forum": "mRNkPVHyIVX", "title": "Exploiting Safe Spots in Neural Networks for Preemptive Robustness and Out-of-Distribution Detection", "decision": "Reject", "meta_review": "The reviewers recognized that the proposed method is interesting and seems to be useful in some cases, and the authors provided sufficient empirical results to support their claim.\nIn addition, some comments have already been clarified.\nHowever, some reviewers still concerned that the proposed defence method will be defeated under some conditions, and still have the major concern regarding the issue of adopting some attack strategies to find adversarial examples near the safe spots, even though the authors clarified some critical points of the proposed method. \nThese drawbacks led to the decision to not accept. However, this paper has some merit and can be made into a stronger contribution in the future.\n", "reviews": [{"review_id": "mRNkPVHyIVX-0", "review_text": "Summary : This paper aims to improve adversarial robustness of the classifiers in a different perspective than the existing works . Usually , the networks are trained using adversarial examples to improve robustness ( adversarial training ) . This work extend this line of thought and make an input robust to adversarial attacks . Instead of updating the network , they make updates to the input to gain robustness . In other words , this work explore the existence of safe spots near the input samples that are robust against adversarial attacks . Results on CIFAR-10 and ImageNet reveals that there exists such safe spots which are resistant to adversarial perturbations and improve adversarial robustness when combined with adversarial training ( the authors term it as safe-spot aware adversarial training ) . Based on this approach , the authors also propose out-of-distribution detection method that outperforms previous works . Strengths : + Motivation is clear . + The proposed approach is interesting and different from existing works . The practical application of the proposed framework is elaborated clearly . + Technical details and formulations are clear . + Results show that the proposed approach improves adversarial robustness and clean data performance on both CIFAR-10 and ImageNet . Furthermore , the proposed approach greatly improves the robustness when evaluated with randomized smoothing . + The design of the approach enables out-of-distribution detection that outperforms previous works . Weaknesses : - The major concern lies in the evaluation of the proposed technique . Here , the authors find safe spots and also propose safe-spot aware adversarial training but evaluate on PGD based adversarial attack in a standard manner . It is important to address the possibility of safe spot aware adversarial attack on the proposed defense and its success rate . In case such attack is infeasible , please provide the rationale behind that . - Clarify the difference between S-Full and S-PGD from Experiments section . Since S-Full also uses T-step PGD , how it is different than S-PGD ? - Though the out-of-distribution detection results slightly outperforms previous works under FPR95 metric , the performance gains are very minimal and not very significant than the baseline OE ( Hendrycks et al. , 2019b ) under two metrics AUROC and AUPR . Final thoughts : The proposed method is clearly motivated . Although the performance gains on adversarial robustness is significant , there are critical points yet to be addressed . Therefore , I marginally accept this paper . Post rebuttal : The authors have addressed my concerns in the rebuttal . However , I also agree with the other critical points raised by other reviewers ( particularly Reviewer 4 ) that are of major concern . Hence , I retain my initial score and marginally accept the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the encouraging comments ( \u201c Motivation is clear \u201d , \u201c interesting and different from existing works \u201d , \u201c technical details and formulations are clear \u201d ) and constructive feedback . We would like to address the reviewer \u2019 s concerns below . * * Q : \u201c Address the possibility of safe spot aware adversarial attack. \u201d * * A : Most of the defense methods that are vulnerable against defense-aware adversarial attacks heavily rely on gradient masking [ 1 , 2 ] . Defense methods with gradient masking typically add randomized or non-differentiable operations into the model inference or increase the model complexity significantly , which makes it hard for the adversary to generate adversarial examples by backpropagation [ 3 , 4 , 5 ] . Recently , it has been shown that another line of defenses that replaces the cross-entropy loss with a new loss function during training also exhibits some gradient masking [ 6 , 7 ] . While these methods are resistant against gradient-based attacks with the cross-entropy loss , they fail against alternative gradient-based attacks using a different loss function . Unlike these methods above , our safe spot-aware training does not add any additional complex operations , change the network structure , or replace the cross-entropy loss for training with another loss function . Also , our safe spot algorithm only manipulates images before the adversary \u2019 s incursion and does not intervene during the attack process . Therefore , under our defense framework , the exact calculation of the loss gradient is straightforward , which means that standard gradient-based attacks can be a reliable measure to evaluate the robustness of our method . That said , we additionally evaluated the robustness of our defense framework using auto-attack [ 8 ] , which contains an attack method that does not rely on gradient information . We conducted the evaluation on CIFAR-10 $ \\ell_2 $ threat model ( $ \\delta=0.5 $ , $ \\epsilon=0.5 $ ) and provide the adversarial accuracies below : Method | Clean | PGD | AutoAttack ||| Madry [ 9 ] | 90.26 | 68.16 | 67.86 Ours | * * 90.92 * * | * * 90.60 * * | * * 89.57 * * Here , \u2018 Madry \u2019 refers to measuring the original images \u2019 robustness on a PGD adversarially trained model , and \u2018 Ours \u2019 refers to measuring the robustness of our safe spot solutions from S-Full on the safe spot-aware adversarially trained model ( S-FGSM+ADV ) . The results show that our safe spot solutions still remain robust against the stronger adversary , maintaining the adversarial accuracy gap above 20\\ % p . * * Q : \u201c Clarify the difference between S-Full and S-PGD. \u201d * * A : Concretely , S-PGD can be seen as solving the following optimization problem : $ $ \\\\mathop { \\\\rm minimize } _ { x_s } \\\\ : \\\\ell ( x_s , c ( x_o ) ) \\\\ : \\\\ : \\\\mathrm { subject~to } \\\\ : \\\\ : \\\\| x_s - x_o \\\\| \\\\le \\\\delta . $ $ In other words , S-PGD replaces the inner maximization $ \\sup_ { x_a } \\ell ( x_a , c ( x_o ) ) $ in our safe spot objective , by $ \\ell ( x_s , c ( x_o ) ) $ . To solve this simplified version , PGD is applied in the direction of decreasing the loss $ \\ell ( x_s , c ( x_o ) ) $ . We have revised our paper to clarify this difference and the reviewer can find the detailed description in the last paragraph of Section 3.5 . ( continued )"}, {"review_id": "mRNkPVHyIVX-1", "review_text": "This paper proposes a new adversarial framework where the defender could preemptively modify classifier inputs to find safe spots that are robust to adversarial attacks . They then introduce a novel bi-level optimization algorithm that can find safe spots on over 90 % of the correctly classified images for adversarially trained classifiers on CIFAR-10 and ImageNet datasets and show that they can be used to improve both the empirical and certified robustness on smoothed classifiers . Besides , they propose a new training scheme based on their conjecture about safe spots for out-of-distribution detection which achieves state-of-the-art results on near-distribution outliers . Overall , the writing is clear and the idea is interesting . I think they have the following contributions : 1 . Propose a novel adversarial framework and motivate it by a real-world application ( search engine example ) ; 2 . Propose an effective algorithm to find safe spots ; 3 . Show the usefulness of safe spots for adversarial robustness and out-of-distribution detection . However , I have the following concerns : 1 . I am wondering whether the safe spots actually exist . Based on the definition , the classifier should have the same predictions on all data points in the $ \\epsilon $ -ball around a safe spot . So the classifier has certified robustness on the safe spots . But it might be hard to show that the classifier has certified robustness on the safe spots . Although they have shown that the attacks could not successfully find adversarial examples for the safe spots , it might be due to the \u201c gradient masking \u201d issues . Could the authors try the auto-attack proposed in [ 1 ] to see whether the classifier is actually robust on the safe spots ? 2.From their results , we can see that the safe spots don \u2019 t exist for naturally trained models . We need to use adversarial training to produce safe spots . But it is not surprising that adversarial training could produce safe spots . In fact , if we could solve the standard adversarial training objective optimally , then any natural images from the training distribution should be safe spots . Could the authors explain why we need to find safe spots other than the natural images in this case ? 3.If the classifier is not robust on the natural image $ x_o $ , and the defender finds a safe spot $ x_s $ around $ x_o $ , then from the attacker perspective , why he could not first perturb $ x_s $ to be $ x_o $ , and then perturb $ x_o $ to find adversarial examples ? If the attacker could not find adversarial examples for $ x_s $ , then he may try other attack strategies like using larger $ \\epsilon $ or other perturbation types . In such cases , the proposed defense framework may not work . Could the authors explain it ? 4.For safe spot-aware adversarial training , they mention that the training procedure is more computationally demanding than PGD adversarial training . Then they use targeted FGSM or k-step PGD towards the ground-truth label as a proxy to safe spot search . It is hard for me to understand what they exactly do in this part . Could the authors describe it in detail ? Also , why would the safe spot-aware adversarial training be better than the standard adversarial training ? I think standard adversarial training can also produce safe spots . Is it because the safe spot-aware adversarial training search for $ x_a $ in a larger ball around $ x_o $ ( $ B_ { \\delta+\\epsilon } ( x_o ) $ ) than the standard adversarial training ? Could the authors try standard adversarial training with a perturbation budget of $ \\delta+\\epsilon $ to see if this is the case ? 5.For out-of-distribution detection , they conjecture that the samples from the learned distribution will have a higher probability of having safe spots compared to the out-of-distribution samples . But I don \u2019 t think their results could support this conjecture . In their training objective , they explicitly minimize the probability of safe spot existence of the outlier samples . So they try to train a model such that their conjecture holds . It would be better if they could show whether their conjecture holds for naturally trained models or the models trained using outlier exposure . I suggest they perform an ablation study for objective ( 4 ) . Also , I think they miss some OOD detection baselines , such as [ 2 ] and [ 3 ] . Could the authors compare their method to them ? [ 1 ] Croce , Francesco , and Matthias Hein . `` Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks . '' arXiv preprint arXiv:2003.01690 ( 2020 ) . [ 2 ] Mohseni , Sina , et al . `` Self-Supervised Learning for Generalizable Out-of-Distribution Detection . '' AAAI.2020 . [ 3 ] Liu , Weitang , et al . `` Energy-based Out-of-distribution Detection . '' arXiv preprint arXiv:2010.03759 ( 2020 ) . -- AFTER DISCUSSION WITH AUTHORS Thanks for the clarification . Some of my concerns have been addressed and I have raised my score . But I keep the concern that the proposed defense framework may be easily broken in practice given that the attacker can have unlimited power .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the encouraging comments ( \u201c the writing is clear \u201d , \u201c the idea is interesting \u201d , \u201c a novel adversarial framework \u201d , \u201c show the usefulness of safe spots \u201d ) and constructive feedback . We would like to address the reviewer \u2019 s questions below . * * Q : \u201c I am wondering whether the safe spots actually exist . Could the authors try the auto-attack proposed in [ 1 ] \u201d * * A : We evaluated our method against auto-attack using the standard setting on CIFAR-10 $ \\ell_2 $ threat model ( $ \\delta=0.5 $ , $ \\epsilon=0.5 $ ) , and provide the adversarial accuracies below : Method | clean | PGD | AutoAttack ||| Madry [ 4 ] | 90.26 | 68.16 | 67.86 Ours | * * 90.92 * * | * * 90.60 * * | * * 89.57 * * Here , \u2018 Madry \u2019 refers to measuring the original image \u2019 s robustness on a PGD adversarially trained model , and \u2018 Ours \u2019 refers to measuring the robustness of our safe spot solutions from S-Full on the safe spot-aware adversarially trained model ( S-FGSM+ADV ) . The results show that our safe spot solutions still remain robust against the stronger adversary , maintaining the adversarial accuracy gap above 20\\ % p . Also , as mentioned in Section 4.3 and Supplementary D.2 , we applied our method to randomized smoothing , which provides a theoretical guarantee for $ \\ell_2 $ robustness . We found that safe spots actually exist for 92 % of the correctly classified images on a smoothed Gaussian model in CIFAR-10 test set . * * Q : \u201c if we could solve the standard adversarial training objective optimally , then any natural images from the training distribution should be safe spots. \u201d * * A : We agree that if there is an optimal classifier that is accurate and robust , we do not need safe spots . However , recent works show that in reality , there exists an inherent trade-off between adversarial robustness and standard generalization [ 5 , 6 ] . Therefore , achieving both high robustness and high clean accuracy on natural images is extremely difficult . However , our safe spot-aware adversarial training , combined with our safe spot algorithm , can achieve high robustness with minimal clean accuracy drop . For example , in Table 2 ( left ) of the main paper , Fast adversarial training , which is a variant of standard adversarial training , only achieves an adversarial accuracy of 30 % while the clean accuracy drops by 13 % p compared to the natural classifier . Our safe spot-aware classifier , S-FGSM+FAST , can achieve an adversarial accuracy of 62 % , which is more than twice compared to Fast adversarial training , with only a 6 % p drop in clean accuracy . ( continued )"}, {"review_id": "mRNkPVHyIVX-2", "review_text": "Thank you for your answers . The paper proposes a new method for making adversarial attacks more difficult . In their method , the defender ( not the attacker ) modifies the original input $ x_o $ to $ x_s $ which is guaranteed to be safe in the sense that an attacker modifying $ x_s $ will not manage to change the predicted class until large changes to the sample are performed . The defender 's budget for modifying the sample is denoted $ \\delta $ , whereas the attackers budget is $ \\epsilon $ . After some relaxations , they arrive at the optimization problem stated in Equation ( 2 ) : Find the modification $ x_s $ ( subject to budget $ \\delta $ ) which minimizes the risk ( measured by cross-entropy ) that any modification of $ x_s $ by an attacker ( subject to budget $ \\epsilon $ ) is misclassified . They also extend the idea to out-of distribution ( OOD ) detection , though the main contribution seems to be in mitigating adversarial attacks during testing . Strong Points : - the basic idea and derivation of the optimization problem is clearly written . - the idea of modifying an input image before classification is interesting , apparently new , and effective in mitigating the impact of adverserial attacks ( according to the experiments in 4.1 , 4.2 , 4.3 ) . Weak/Unclear Points : - Section 3.5 `` SAFE SPOT-AWARE ADVERSARIAL TRAINING '' is a little bit unclear to me . Is it that now the training data , not the test data is modified ? But then it is not so clear to me , whether the final training objective is still well defined . It might just have a similar effect as adding noise to training samples . Furthermore , it appears that the whole thing becomes difficult to train , since during training it necessary to iterate between ( A ) ordinary model training and ( B ) modification of training samples . - Section 3.6 `` OUT-OF-DISTRIBUTION DETECTION '' is not convincing : Apparently the proposed method is not only hard to train , but also has three important hyper-parameters $ \\gamma $ , $ \\lambda $ , and $ \\mu $ , which need to be carefully tuned . Therefore , even though the authors report improvements over previous methods in Section 4 , I am not convinced that this is a practical approach to OOD . - In Section 4.1 , I am not sure what the authors mean with `` our methods can find safe spots on over 85 % of the test set images '' . My understanding is that , if the class label could not be changed by an attacker , then the method was successful , even if the original sample was misclassified . However , Table 1 reports only classification accuracy .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the encouraging comments ( \u201c the basic idea and derivation ... is clearly written \u201d , \u201c interesting , apparently new , and effective \u201d ) and constructive feedback . We would like to address the reviewer \u2019 s concerns below . * * Q : \u201c Section 3.5 `` SAFE SPOT-AWARE ADVERSARIAL TRAINING '' is a little bit unclear to me. \u201d * * A : Safe spot-aware adversarial training directly aims to improve preemptive robustness by simulating the safe spot generation and the following adversarial attack on the training set . The final training objective directly relates to Equation 1 , while ground-truth labels are used instead as we know them during training . The mentioned iteration between ordinary model training and modification of training samples is a feature that also exists in standard adversarial training . The modification procedure does get a bit more complicated due to the safe spot search . However , as we approximate it with targeted FGSM or k-step PGD , the overall complexity does not increase as much , and can be adjusted to fit the needs of the practitioner . We have revised Section 3.5 of our paper to make this more clearer . * * Q : \u201c Section 3.6 `` OUT-OF-DISTRIBUTION DETECTION '' is not convincing : Apparently the proposed method is not only hard to train , but also has three important hyper-parameters $ \\gamma $ , $ \\lambda $ , and $ \\mu $ , which need to be carefully tuned. \u201d * * A : As we describe in Supplementary E.2 , the hyperparameters for OE fine-tuning , $ \\gamma $ and $ \\lambda $ , do not need to be carefully tuned . We conducted a hyperparameter search for $ \\gamma $ and $ \\lambda $ on a coarse $ 3 \\times 3 $ grid . Also , the hyperparameter for the score function , $ \\mu $ , can be easily tuned since it does not require network training . * * Q : \u201c In Section 4.1 , I am not sure what the authors mean with `` our methods can find safe spots on over 85 % of the test set images \u201d * * A : Thank you for your comment . We revised our paper to make the term \u201c safe spot \u201d more consistent throughout the paper . In the revised version , a safe spot also has to preserve the ground-truth label of the original image . With this definition , reporting classification accuracy ( under adversarial attack ) is an appropriate measure of performance ."}, {"review_id": "mRNkPVHyIVX-3", "review_text": "The authors argue that there are some safe `` spots '' in the data space that are less prone to adversarial attacks . The authors propose a technique to identify such `` safe spots '' . They then leverage them for robust training and observe higher robust accuracy than baseline . Finally , they leverage this observation to identify out of distribution data . The application is important and the results look promising . However , I have the following concern : - The authors propose a new threat model where the adversary may have access to the labeled data . They motivated such a setting with an example of Google image search . However , such a setting is quite limited . There are also existing methods that use supervised learning setting with incorrect labeling . The paper should discuss how they differ from such a line of work . - The search algorithm requires that a correct predicted label is available . This setting is not quite realistic . How can we find a safe spot when the label is unknown . - Some of the findings are not quite surprising . For example , a safe spot is more in a robust model with small epsilon .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the encouraging comments ( \u201c The application is important \u201d , \u201c the results look promising \u201d ) and constructive feedback . We would like to address the reviewer \u2019 s concerns below . * * Q : \u201c The authors propose a new threat model where the adversary may have access to the labeled data . However , such a setting is quite limited. \u201d * * A : Concretely , there are three adversaries in our framework . The first is the virtual adversary ( A ) used by the defender while training a safe-spot aware classifier . The second is the virtual adversary ( B ) used by the defender while generating safe spots after the model training . The last adversary ( C ) is the real adversary that attacks the generated safe spots . In our setting , A has access to the labeled data , assuming ground-truth labels are available during training . However , B does not have access to the labeled data , because as the reviewer mentioned , random images in the real-world usually do not have annotated labels . Lastly , the real adversary C has access to the labels , which is common in adversarial attack literature , including those in real-world settings [ 1 , 2 , 3 , 4 ] . * * Q : \u201c There are also existing methods that use supervised learning setting with incorrect labeling . The paper should discuss how they differ from such a line of work. \u201d * * A : Our defense framework consists of two main parts . The first part is to train a safe spot-aware classifier , given a correctly labeled dataset . The second part is to find a safe spot from an unlabeled natural image , given a pre-trained classifier . Therefore , to be exact , our work is not closely related to supervised learning with incorrect labeling . However , to make our framework more applicable , we could consider a situation where the training data in the first part is wrongly labeled . Developing a safe spot-aware classifier that considers such label corruption scenario will be a good direction for future research . * * Q : \u201c The search algorithm requires that a correct predicted label is available . How can we find a safe spot when the label is unknown. \u201d * * A : If the reviewer meant the ground-truth label is unknown , then our search algorithm already assumes the situation where the ground-truth label is unknown . We have revised our paper to describe our safe spot searching algorithm more clearly in the second paragraph of Section 3.2 . On the other hand , if the reviewer meant the predicted label may be incorrect , we note that it is not possible to find a safe spot $ x_s $ for an incorrectly classified image $ x_o $ if the modification budgets of the defender and the adversary are the same , * i.e . * , $ \\epsilon = \\delta $ . To see why , suppose there exists a safe spot $ x_s $ of $ x_o $ . Since $ x_o \\in B_ { \\delta } ( x_s ) = B_ { \\epsilon } ( x_s ) $ , it satisfies that $ y_o = c ( x_s ) = c ( x_o ) $ , which is directly from the definition of safe spot . However , it contradicts the fact that $ x_o $ is misclassified , * i.e . * , $ y_o \\neq c ( x_o ) $ . * * Q : \u201c Some of the findings are not quite surprising . For example , a safe spot is more in a robust model with small epsilon. \u201d * * A : It is natural to guess that more robust classifiers will also have more safe spots near the natural images , thus being more preemptively robust . However , our experiments on ImageNet in Section 4.2 show that the robust model trained with smaller $ \\epsilon_ { \\text { train } } $ , which is obviously less robust than its larger $ \\epsilon_ { \\text { train } } $ counterpart , is actually more preemptively robust . This shows that the conventional notion of robustness does not necessarily translate to preemptive robustness . [ 1 ] Szegedy , et al. , Intriguing properties of neural networks , arXiv:1312.6199 . [ 2 ] Goodfellow , et al. , Explaining and harnessing adversarial examples , ICLR 2015 . [ 3 ] Kurakin , et al. , Adversarial examples in the physical-world , ICLR workshop 2017 . [ 4 ] Eykholt , et al. , Robust Physical-World Attacks on Deep Learning Models , CVPR 2018 ."}], "0": {"review_id": "mRNkPVHyIVX-0", "review_text": "Summary : This paper aims to improve adversarial robustness of the classifiers in a different perspective than the existing works . Usually , the networks are trained using adversarial examples to improve robustness ( adversarial training ) . This work extend this line of thought and make an input robust to adversarial attacks . Instead of updating the network , they make updates to the input to gain robustness . In other words , this work explore the existence of safe spots near the input samples that are robust against adversarial attacks . Results on CIFAR-10 and ImageNet reveals that there exists such safe spots which are resistant to adversarial perturbations and improve adversarial robustness when combined with adversarial training ( the authors term it as safe-spot aware adversarial training ) . Based on this approach , the authors also propose out-of-distribution detection method that outperforms previous works . Strengths : + Motivation is clear . + The proposed approach is interesting and different from existing works . The practical application of the proposed framework is elaborated clearly . + Technical details and formulations are clear . + Results show that the proposed approach improves adversarial robustness and clean data performance on both CIFAR-10 and ImageNet . Furthermore , the proposed approach greatly improves the robustness when evaluated with randomized smoothing . + The design of the approach enables out-of-distribution detection that outperforms previous works . Weaknesses : - The major concern lies in the evaluation of the proposed technique . Here , the authors find safe spots and also propose safe-spot aware adversarial training but evaluate on PGD based adversarial attack in a standard manner . It is important to address the possibility of safe spot aware adversarial attack on the proposed defense and its success rate . In case such attack is infeasible , please provide the rationale behind that . - Clarify the difference between S-Full and S-PGD from Experiments section . Since S-Full also uses T-step PGD , how it is different than S-PGD ? - Though the out-of-distribution detection results slightly outperforms previous works under FPR95 metric , the performance gains are very minimal and not very significant than the baseline OE ( Hendrycks et al. , 2019b ) under two metrics AUROC and AUPR . Final thoughts : The proposed method is clearly motivated . Although the performance gains on adversarial robustness is significant , there are critical points yet to be addressed . Therefore , I marginally accept this paper . Post rebuttal : The authors have addressed my concerns in the rebuttal . However , I also agree with the other critical points raised by other reviewers ( particularly Reviewer 4 ) that are of major concern . Hence , I retain my initial score and marginally accept the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the encouraging comments ( \u201c Motivation is clear \u201d , \u201c interesting and different from existing works \u201d , \u201c technical details and formulations are clear \u201d ) and constructive feedback . We would like to address the reviewer \u2019 s concerns below . * * Q : \u201c Address the possibility of safe spot aware adversarial attack. \u201d * * A : Most of the defense methods that are vulnerable against defense-aware adversarial attacks heavily rely on gradient masking [ 1 , 2 ] . Defense methods with gradient masking typically add randomized or non-differentiable operations into the model inference or increase the model complexity significantly , which makes it hard for the adversary to generate adversarial examples by backpropagation [ 3 , 4 , 5 ] . Recently , it has been shown that another line of defenses that replaces the cross-entropy loss with a new loss function during training also exhibits some gradient masking [ 6 , 7 ] . While these methods are resistant against gradient-based attacks with the cross-entropy loss , they fail against alternative gradient-based attacks using a different loss function . Unlike these methods above , our safe spot-aware training does not add any additional complex operations , change the network structure , or replace the cross-entropy loss for training with another loss function . Also , our safe spot algorithm only manipulates images before the adversary \u2019 s incursion and does not intervene during the attack process . Therefore , under our defense framework , the exact calculation of the loss gradient is straightforward , which means that standard gradient-based attacks can be a reliable measure to evaluate the robustness of our method . That said , we additionally evaluated the robustness of our defense framework using auto-attack [ 8 ] , which contains an attack method that does not rely on gradient information . We conducted the evaluation on CIFAR-10 $ \\ell_2 $ threat model ( $ \\delta=0.5 $ , $ \\epsilon=0.5 $ ) and provide the adversarial accuracies below : Method | Clean | PGD | AutoAttack ||| Madry [ 9 ] | 90.26 | 68.16 | 67.86 Ours | * * 90.92 * * | * * 90.60 * * | * * 89.57 * * Here , \u2018 Madry \u2019 refers to measuring the original images \u2019 robustness on a PGD adversarially trained model , and \u2018 Ours \u2019 refers to measuring the robustness of our safe spot solutions from S-Full on the safe spot-aware adversarially trained model ( S-FGSM+ADV ) . The results show that our safe spot solutions still remain robust against the stronger adversary , maintaining the adversarial accuracy gap above 20\\ % p . * * Q : \u201c Clarify the difference between S-Full and S-PGD. \u201d * * A : Concretely , S-PGD can be seen as solving the following optimization problem : $ $ \\\\mathop { \\\\rm minimize } _ { x_s } \\\\ : \\\\ell ( x_s , c ( x_o ) ) \\\\ : \\\\ : \\\\mathrm { subject~to } \\\\ : \\\\ : \\\\| x_s - x_o \\\\| \\\\le \\\\delta . $ $ In other words , S-PGD replaces the inner maximization $ \\sup_ { x_a } \\ell ( x_a , c ( x_o ) ) $ in our safe spot objective , by $ \\ell ( x_s , c ( x_o ) ) $ . To solve this simplified version , PGD is applied in the direction of decreasing the loss $ \\ell ( x_s , c ( x_o ) ) $ . We have revised our paper to clarify this difference and the reviewer can find the detailed description in the last paragraph of Section 3.5 . ( continued )"}, "1": {"review_id": "mRNkPVHyIVX-1", "review_text": "This paper proposes a new adversarial framework where the defender could preemptively modify classifier inputs to find safe spots that are robust to adversarial attacks . They then introduce a novel bi-level optimization algorithm that can find safe spots on over 90 % of the correctly classified images for adversarially trained classifiers on CIFAR-10 and ImageNet datasets and show that they can be used to improve both the empirical and certified robustness on smoothed classifiers . Besides , they propose a new training scheme based on their conjecture about safe spots for out-of-distribution detection which achieves state-of-the-art results on near-distribution outliers . Overall , the writing is clear and the idea is interesting . I think they have the following contributions : 1 . Propose a novel adversarial framework and motivate it by a real-world application ( search engine example ) ; 2 . Propose an effective algorithm to find safe spots ; 3 . Show the usefulness of safe spots for adversarial robustness and out-of-distribution detection . However , I have the following concerns : 1 . I am wondering whether the safe spots actually exist . Based on the definition , the classifier should have the same predictions on all data points in the $ \\epsilon $ -ball around a safe spot . So the classifier has certified robustness on the safe spots . But it might be hard to show that the classifier has certified robustness on the safe spots . Although they have shown that the attacks could not successfully find adversarial examples for the safe spots , it might be due to the \u201c gradient masking \u201d issues . Could the authors try the auto-attack proposed in [ 1 ] to see whether the classifier is actually robust on the safe spots ? 2.From their results , we can see that the safe spots don \u2019 t exist for naturally trained models . We need to use adversarial training to produce safe spots . But it is not surprising that adversarial training could produce safe spots . In fact , if we could solve the standard adversarial training objective optimally , then any natural images from the training distribution should be safe spots . Could the authors explain why we need to find safe spots other than the natural images in this case ? 3.If the classifier is not robust on the natural image $ x_o $ , and the defender finds a safe spot $ x_s $ around $ x_o $ , then from the attacker perspective , why he could not first perturb $ x_s $ to be $ x_o $ , and then perturb $ x_o $ to find adversarial examples ? If the attacker could not find adversarial examples for $ x_s $ , then he may try other attack strategies like using larger $ \\epsilon $ or other perturbation types . In such cases , the proposed defense framework may not work . Could the authors explain it ? 4.For safe spot-aware adversarial training , they mention that the training procedure is more computationally demanding than PGD adversarial training . Then they use targeted FGSM or k-step PGD towards the ground-truth label as a proxy to safe spot search . It is hard for me to understand what they exactly do in this part . Could the authors describe it in detail ? Also , why would the safe spot-aware adversarial training be better than the standard adversarial training ? I think standard adversarial training can also produce safe spots . Is it because the safe spot-aware adversarial training search for $ x_a $ in a larger ball around $ x_o $ ( $ B_ { \\delta+\\epsilon } ( x_o ) $ ) than the standard adversarial training ? Could the authors try standard adversarial training with a perturbation budget of $ \\delta+\\epsilon $ to see if this is the case ? 5.For out-of-distribution detection , they conjecture that the samples from the learned distribution will have a higher probability of having safe spots compared to the out-of-distribution samples . But I don \u2019 t think their results could support this conjecture . In their training objective , they explicitly minimize the probability of safe spot existence of the outlier samples . So they try to train a model such that their conjecture holds . It would be better if they could show whether their conjecture holds for naturally trained models or the models trained using outlier exposure . I suggest they perform an ablation study for objective ( 4 ) . Also , I think they miss some OOD detection baselines , such as [ 2 ] and [ 3 ] . Could the authors compare their method to them ? [ 1 ] Croce , Francesco , and Matthias Hein . `` Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks . '' arXiv preprint arXiv:2003.01690 ( 2020 ) . [ 2 ] Mohseni , Sina , et al . `` Self-Supervised Learning for Generalizable Out-of-Distribution Detection . '' AAAI.2020 . [ 3 ] Liu , Weitang , et al . `` Energy-based Out-of-distribution Detection . '' arXiv preprint arXiv:2010.03759 ( 2020 ) . -- AFTER DISCUSSION WITH AUTHORS Thanks for the clarification . Some of my concerns have been addressed and I have raised my score . But I keep the concern that the proposed defense framework may be easily broken in practice given that the attacker can have unlimited power .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the encouraging comments ( \u201c the writing is clear \u201d , \u201c the idea is interesting \u201d , \u201c a novel adversarial framework \u201d , \u201c show the usefulness of safe spots \u201d ) and constructive feedback . We would like to address the reviewer \u2019 s questions below . * * Q : \u201c I am wondering whether the safe spots actually exist . Could the authors try the auto-attack proposed in [ 1 ] \u201d * * A : We evaluated our method against auto-attack using the standard setting on CIFAR-10 $ \\ell_2 $ threat model ( $ \\delta=0.5 $ , $ \\epsilon=0.5 $ ) , and provide the adversarial accuracies below : Method | clean | PGD | AutoAttack ||| Madry [ 4 ] | 90.26 | 68.16 | 67.86 Ours | * * 90.92 * * | * * 90.60 * * | * * 89.57 * * Here , \u2018 Madry \u2019 refers to measuring the original image \u2019 s robustness on a PGD adversarially trained model , and \u2018 Ours \u2019 refers to measuring the robustness of our safe spot solutions from S-Full on the safe spot-aware adversarially trained model ( S-FGSM+ADV ) . The results show that our safe spot solutions still remain robust against the stronger adversary , maintaining the adversarial accuracy gap above 20\\ % p . Also , as mentioned in Section 4.3 and Supplementary D.2 , we applied our method to randomized smoothing , which provides a theoretical guarantee for $ \\ell_2 $ robustness . We found that safe spots actually exist for 92 % of the correctly classified images on a smoothed Gaussian model in CIFAR-10 test set . * * Q : \u201c if we could solve the standard adversarial training objective optimally , then any natural images from the training distribution should be safe spots. \u201d * * A : We agree that if there is an optimal classifier that is accurate and robust , we do not need safe spots . However , recent works show that in reality , there exists an inherent trade-off between adversarial robustness and standard generalization [ 5 , 6 ] . Therefore , achieving both high robustness and high clean accuracy on natural images is extremely difficult . However , our safe spot-aware adversarial training , combined with our safe spot algorithm , can achieve high robustness with minimal clean accuracy drop . For example , in Table 2 ( left ) of the main paper , Fast adversarial training , which is a variant of standard adversarial training , only achieves an adversarial accuracy of 30 % while the clean accuracy drops by 13 % p compared to the natural classifier . Our safe spot-aware classifier , S-FGSM+FAST , can achieve an adversarial accuracy of 62 % , which is more than twice compared to Fast adversarial training , with only a 6 % p drop in clean accuracy . ( continued )"}, "2": {"review_id": "mRNkPVHyIVX-2", "review_text": "Thank you for your answers . The paper proposes a new method for making adversarial attacks more difficult . In their method , the defender ( not the attacker ) modifies the original input $ x_o $ to $ x_s $ which is guaranteed to be safe in the sense that an attacker modifying $ x_s $ will not manage to change the predicted class until large changes to the sample are performed . The defender 's budget for modifying the sample is denoted $ \\delta $ , whereas the attackers budget is $ \\epsilon $ . After some relaxations , they arrive at the optimization problem stated in Equation ( 2 ) : Find the modification $ x_s $ ( subject to budget $ \\delta $ ) which minimizes the risk ( measured by cross-entropy ) that any modification of $ x_s $ by an attacker ( subject to budget $ \\epsilon $ ) is misclassified . They also extend the idea to out-of distribution ( OOD ) detection , though the main contribution seems to be in mitigating adversarial attacks during testing . Strong Points : - the basic idea and derivation of the optimization problem is clearly written . - the idea of modifying an input image before classification is interesting , apparently new , and effective in mitigating the impact of adverserial attacks ( according to the experiments in 4.1 , 4.2 , 4.3 ) . Weak/Unclear Points : - Section 3.5 `` SAFE SPOT-AWARE ADVERSARIAL TRAINING '' is a little bit unclear to me . Is it that now the training data , not the test data is modified ? But then it is not so clear to me , whether the final training objective is still well defined . It might just have a similar effect as adding noise to training samples . Furthermore , it appears that the whole thing becomes difficult to train , since during training it necessary to iterate between ( A ) ordinary model training and ( B ) modification of training samples . - Section 3.6 `` OUT-OF-DISTRIBUTION DETECTION '' is not convincing : Apparently the proposed method is not only hard to train , but also has three important hyper-parameters $ \\gamma $ , $ \\lambda $ , and $ \\mu $ , which need to be carefully tuned . Therefore , even though the authors report improvements over previous methods in Section 4 , I am not convinced that this is a practical approach to OOD . - In Section 4.1 , I am not sure what the authors mean with `` our methods can find safe spots on over 85 % of the test set images '' . My understanding is that , if the class label could not be changed by an attacker , then the method was successful , even if the original sample was misclassified . However , Table 1 reports only classification accuracy .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the encouraging comments ( \u201c the basic idea and derivation ... is clearly written \u201d , \u201c interesting , apparently new , and effective \u201d ) and constructive feedback . We would like to address the reviewer \u2019 s concerns below . * * Q : \u201c Section 3.5 `` SAFE SPOT-AWARE ADVERSARIAL TRAINING '' is a little bit unclear to me. \u201d * * A : Safe spot-aware adversarial training directly aims to improve preemptive robustness by simulating the safe spot generation and the following adversarial attack on the training set . The final training objective directly relates to Equation 1 , while ground-truth labels are used instead as we know them during training . The mentioned iteration between ordinary model training and modification of training samples is a feature that also exists in standard adversarial training . The modification procedure does get a bit more complicated due to the safe spot search . However , as we approximate it with targeted FGSM or k-step PGD , the overall complexity does not increase as much , and can be adjusted to fit the needs of the practitioner . We have revised Section 3.5 of our paper to make this more clearer . * * Q : \u201c Section 3.6 `` OUT-OF-DISTRIBUTION DETECTION '' is not convincing : Apparently the proposed method is not only hard to train , but also has three important hyper-parameters $ \\gamma $ , $ \\lambda $ , and $ \\mu $ , which need to be carefully tuned. \u201d * * A : As we describe in Supplementary E.2 , the hyperparameters for OE fine-tuning , $ \\gamma $ and $ \\lambda $ , do not need to be carefully tuned . We conducted a hyperparameter search for $ \\gamma $ and $ \\lambda $ on a coarse $ 3 \\times 3 $ grid . Also , the hyperparameter for the score function , $ \\mu $ , can be easily tuned since it does not require network training . * * Q : \u201c In Section 4.1 , I am not sure what the authors mean with `` our methods can find safe spots on over 85 % of the test set images \u201d * * A : Thank you for your comment . We revised our paper to make the term \u201c safe spot \u201d more consistent throughout the paper . In the revised version , a safe spot also has to preserve the ground-truth label of the original image . With this definition , reporting classification accuracy ( under adversarial attack ) is an appropriate measure of performance ."}, "3": {"review_id": "mRNkPVHyIVX-3", "review_text": "The authors argue that there are some safe `` spots '' in the data space that are less prone to adversarial attacks . The authors propose a technique to identify such `` safe spots '' . They then leverage them for robust training and observe higher robust accuracy than baseline . Finally , they leverage this observation to identify out of distribution data . The application is important and the results look promising . However , I have the following concern : - The authors propose a new threat model where the adversary may have access to the labeled data . They motivated such a setting with an example of Google image search . However , such a setting is quite limited . There are also existing methods that use supervised learning setting with incorrect labeling . The paper should discuss how they differ from such a line of work . - The search algorithm requires that a correct predicted label is available . This setting is not quite realistic . How can we find a safe spot when the label is unknown . - Some of the findings are not quite surprising . For example , a safe spot is more in a robust model with small epsilon .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the encouraging comments ( \u201c The application is important \u201d , \u201c the results look promising \u201d ) and constructive feedback . We would like to address the reviewer \u2019 s concerns below . * * Q : \u201c The authors propose a new threat model where the adversary may have access to the labeled data . However , such a setting is quite limited. \u201d * * A : Concretely , there are three adversaries in our framework . The first is the virtual adversary ( A ) used by the defender while training a safe-spot aware classifier . The second is the virtual adversary ( B ) used by the defender while generating safe spots after the model training . The last adversary ( C ) is the real adversary that attacks the generated safe spots . In our setting , A has access to the labeled data , assuming ground-truth labels are available during training . However , B does not have access to the labeled data , because as the reviewer mentioned , random images in the real-world usually do not have annotated labels . Lastly , the real adversary C has access to the labels , which is common in adversarial attack literature , including those in real-world settings [ 1 , 2 , 3 , 4 ] . * * Q : \u201c There are also existing methods that use supervised learning setting with incorrect labeling . The paper should discuss how they differ from such a line of work. \u201d * * A : Our defense framework consists of two main parts . The first part is to train a safe spot-aware classifier , given a correctly labeled dataset . The second part is to find a safe spot from an unlabeled natural image , given a pre-trained classifier . Therefore , to be exact , our work is not closely related to supervised learning with incorrect labeling . However , to make our framework more applicable , we could consider a situation where the training data in the first part is wrongly labeled . Developing a safe spot-aware classifier that considers such label corruption scenario will be a good direction for future research . * * Q : \u201c The search algorithm requires that a correct predicted label is available . How can we find a safe spot when the label is unknown. \u201d * * A : If the reviewer meant the ground-truth label is unknown , then our search algorithm already assumes the situation where the ground-truth label is unknown . We have revised our paper to describe our safe spot searching algorithm more clearly in the second paragraph of Section 3.2 . On the other hand , if the reviewer meant the predicted label may be incorrect , we note that it is not possible to find a safe spot $ x_s $ for an incorrectly classified image $ x_o $ if the modification budgets of the defender and the adversary are the same , * i.e . * , $ \\epsilon = \\delta $ . To see why , suppose there exists a safe spot $ x_s $ of $ x_o $ . Since $ x_o \\in B_ { \\delta } ( x_s ) = B_ { \\epsilon } ( x_s ) $ , it satisfies that $ y_o = c ( x_s ) = c ( x_o ) $ , which is directly from the definition of safe spot . However , it contradicts the fact that $ x_o $ is misclassified , * i.e . * , $ y_o \\neq c ( x_o ) $ . * * Q : \u201c Some of the findings are not quite surprising . For example , a safe spot is more in a robust model with small epsilon. \u201d * * A : It is natural to guess that more robust classifiers will also have more safe spots near the natural images , thus being more preemptively robust . However , our experiments on ImageNet in Section 4.2 show that the robust model trained with smaller $ \\epsilon_ { \\text { train } } $ , which is obviously less robust than its larger $ \\epsilon_ { \\text { train } } $ counterpart , is actually more preemptively robust . This shows that the conventional notion of robustness does not necessarily translate to preemptive robustness . [ 1 ] Szegedy , et al. , Intriguing properties of neural networks , arXiv:1312.6199 . [ 2 ] Goodfellow , et al. , Explaining and harnessing adversarial examples , ICLR 2015 . [ 3 ] Kurakin , et al. , Adversarial examples in the physical-world , ICLR workshop 2017 . [ 4 ] Eykholt , et al. , Robust Physical-World Attacks on Deep Learning Models , CVPR 2018 ."}}