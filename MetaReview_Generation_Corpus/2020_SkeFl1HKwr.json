{"year": "2020", "forum": "SkeFl1HKwr", "title": "Empirical Studies on the Properties of Linear Regions in Deep Neural Networks", "decision": "Accept (Poster)", "meta_review": "This paper studies the properties of regions where a DNN with piecewise linear activations behaves linearly. They develop a variety of techniques to chracterize properties and show how these properties correlate with various parameters of the network architecture and training method.\n\nThe reviewers were in consensus on the quality of the paper: The paper is well written and contains a number of insights that would be of broad interest to the deep learning community.\n\nI therefore recommend acceptance.", "reviews": [{"review_id": "SkeFl1HKwr-0", "review_text": "First, I believe that the acknowledgements in the manuscript give identifying information which could stand in conflict with a double blind review process. I\u2019ll leave it to the area chairs/program chairs to make a decision on this. The following review will be contingent on the fact that the authors did not break the submission rules. This paper aims to give new insights into deep neural networks by presenting a number of approaches to analyse the linear regions in such networks. The authors define a linear region around a point x* as the intersection between a number of half spaces that are defined through linear approximations of a DNN (tangents) around that point x*. The authors show that points within these regions can be found using convex optimization with a number of linear constraints that are equal or less than the number of nodes in a DNN. In experiments with a fully connected network the authors analyse different properties of these linear regions: (1) How big is the biggest sphere that we can fit in a linear region? (2) How much do the hyperplanes that define a region correlate with each other? (3) How reliably does a linear region represent a single class? And (4), How does a linear region interact with neighbouring regions? In their presentation, the authors focus on comparing these properties between models that were either trained without regularisation, with batch normalisation, or with dropout and with different learning rates. This allows them to draw insightful conclusions about the difference between linear regions in these models. The authors hope that their work will enable new ways of analysing DDNs that will inspire new architectures and optimization techniques. I vote to accept this paper. The authors present a large array of methods to analyse the linear regions of DNNs. Their insights into the differences of BN and Dropout are useful (figure 1 & 2) and sensible (figure 3). The implications of linear regions on adversarial robustness can have an impact in the future. Because the paper relies on geometrical reasoning, I wished there would be more visualisations that guide the reader. Here are a number of comments and questions that I have on the manuscript: - Figure 1 Top: What do the different colour represent in the linear regions plot? - Section 2.1: maybe add a toy graph that visualises the depth-wise \u2018exclusion\u2019 process of feasible \u201cneighbours\u201d of x*? - Eq. (2) & (3): Explain where these equations come from. - Sec 3.2, first sentence. The authors claim that inspheres of linear regions are highly relate to the expressivity of DNN. Can they elaborate on that claim? Is this claim a result of their experiments? - What is the relationship between the number of constraints in eq. (5) and the radius of an insphere? Does the insphere size decrease with more constraints? What implications would that have on deeper networks than the one that was presented? - Why should distortion be a good measure of the size of a linear region? - The authors claim that it is expensive to run their approach, and that they will aim to improve speed in the future. Can the authors give a more concrete example of runtimes in their current approach? ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the constructive comments , which helped improve the paper . We also apologize for our mistake for including the acknowledgement . It has been removed in our revision . Thank you for pointing this out ! We address your detailed comments below . - Figure 1 Top : What do the different colour represent in the linear regions plot ? The color represents the ratio of the activated nodes in a linear region . We added this in the caption of Fig.1 in the revision . Albeit that different colors were used to separate linear regions in the previous paper , we believe our plot can provide more information since the gray lines have already illustrated in different regions . - Section 2.1 : maybe add a toy graph that visualises the depth-wise \u2018 exclusion \u2019 process of feasible \u201c neighbours \u201d of x * ? Thanks for your suggestion ! We added Fig.2 to illustrate this more clearly . Please check Section 2.1 in our revision . - Eq . ( 2 ) & ( 3 ) : Explain where these equations come from . As mentioned in Section 2.1 , the first $ l-1 $ hidden layers serve as an affine transformation of $ \\mathbf { x } \\in S_ { l-1 } $ . Besides , the pre-activation outputs of the $ l $ -th hidden layer $ \\mathbf { h } ^l ( \\mathbf { x } ) $ are also an affine transformation of the activation outputs of the $ ( l-1 ) $ -th layer , hence $ \\mathbf { h } ^l ( \\mathbf { x } ) $ is a linear function of $ \\mathbf { x } \\in S_ { l-1 } $ , which means $ \\mathbf { h } ^l_n ( \\mathbf { x } ) =\\mathbf { w } _n^T\\mathbf { x } +b_n $ , where $ n $ denotes a node of the $ l $ -th layer . For a linear function $ y=\\mathbf { w } ^T\\mathbf { x } +b $ , the $ \\mathbf { w } $ can be directly calculated by $ \\mathbf { w } =\\nabla_ { \\mathbf { x } } y $ , whereas $ b=y-\\mathbf { w } ^T\\mathbf { x } $ . Here $ \\mathbf { x } $ and $ y $ can be replaced by $ \\mathbf { x } ^ * $ and $ \\mathbf { h } _n^l ( \\mathbf { x } ^ * ) $ because $ \\mathbf { x } ^ * $ shares the same linear function as other $ \\mathbf { x } \\in S_ { l-1 } $ . Last , the parameters are multiplied by $ \\mbox { sgn } ( \\mathbf { h } _n^l ( \\mathbf { x } ^ * ) ) $ to make sure that the inequalities , which indicate the activation states of the $ l $ -th layer , are all in the $ \\geq $ form . A formal deduction was added in Appendix B . - Sec 3.2 , first sentence . The authors claim that inspheres of linear regions are highly relate to the expressivity of DNN . Can they elaborate on that claim ? Is this claim a result of their experiments ? It is believed that a DNN with more linear regions has a larger potential to fit complex functions [ 1 ] [ 2 ] . For example , a regular hexagon is a better approximation of a circle than a square . Small inspheres do demonstrate the narrowness of the linear regions , resulting in a large number of regions . [ 1 ] Poole et al.Exponential expressivity in deep neural networks through transient chaos . NIPS , 2016 . [ 2 ] Pascanu et al.On the number of response regions of deep feed forward networks with piece-wise linear activations . https : //arxiv.org/abs/1312.6098 . 2014 - What is the relationship between the number of constraints in eq . ( 5 ) and the radius of an insphere ? Does the insphere size decrease with more constraints ? What implications would that have on deeper networks than the one that was presented ? Yes , the radius does decrease with more constraints , or more precisely , irredundant constraints ( which means the constraints can not be implied by others ) . A smaller inradius usually results in a larger number of linear regions , hence deeper networks usually have higher fitting ability . Regarding the number of linear regions , i.e.the complexity of DNNs , a well-known question is that why deeper networks have better generalization , instead of overfitting ? We believe it comes from the relevance among the linear regions . A node has a set of fixed weights , creating different constraints for different activation states of the preceding layers , which can be regarded as that part of the weights are picked to construct a constraint . However , different parts of the weights are chosen from the same set , resulting in this relevance . Maybe we are a little off the topic here , but it is really an interesting research direction . Another interesting direction is to show that depth provides irredundant constraints more efficiently than width , which is still our work in progress ."}, {"review_id": "SkeFl1HKwr-1", "review_text": "This work presents an array of analytical tools to characterize linear regions of deep neural networks (DNNs). Using the tools the work analyzes the effect of dropout and batch normalization (BN) on the linear regions of trained DNNs; namely, by assessing the properties such as inspheres, orientation of hyperplanes, decision boundaries and relevance of surrounding regions, the authors highlight the differences and similarities of linear regions induced by vanilla SGD as compared to SGD with dropout or BN. The paper is clearly written and is easy to follow for the most part. The paper indeed presents a number properties for analyzing the nature of linear regions in DNNs; however it falls short of connecting them with an improvement in the optimization or interpretability of DNNs. Even with respect to providing support for general applicability, the work does not go very far: without enough variation in data (not just image benchmarks), tasks and architecture, it is hard to determine if the analysis tools presented in the paper generalize beyond the chosen setup. For instance just the optimization techniques compared in the paper have their own hyperparameters and it is not clear how the results might vary with them. I am not sure what to take away from figure 1 since it's only a two-dimensional slice of very high-dimensional input space. Maybe the authors could instead choose an example with a low-dimensional input space for illustration purposes. Also, how much can be perceived from distributions shown in figure 2, since inradius (Eq. 5) may turnout to be a very coarse representation of linear regions, especially for deeper networks. Can the authors clarify this? Moreover, how would the figures look if we were using a different objective, dataset or architecture? In figure 3, is it not possible to show the average results instead of just one example? I would further like to know how the authors would deal with scalability issues if their analysis were to applied to more realistic (i.e. large) network architectures.", "rating": "3: Weak Reject", "reply_text": "Thank you for the valuable feedback ! We respond to the weakness in the following and hope we have addressed all the concerns . - The paper is clearly written and is easy to follow for the most part . The paper indeed presents a number properties for analyzing the nature of linear regions in DNNs ; however it falls short of connecting them with an improvement in the optimization or interpretability of DNNs . Even with respect to providing support for general applicability , the work does not go very far : without enough variation in data ( not just image benchmarks ) , tasks and architecture , it is hard to determine if the analysis tools presented in the paper generalize beyond the chosen setup . For instance just the optimization techniques compared in the paper have their own hyperparameters and it is not clear how the results might vary with them . The main purpose of our paper is to provide a new geometric perspective to study the linear region instead of just counting them . The number of the linear regions represents the expressivity of a DNN , but fails to indicate the influence of region \u2019 s geometric properties on some local behaviors of DNN , such as robustness . We think our research may give some new inspirations for studying linear regions . There are a number of studies on linear regions , and our experimental setting mostly followed a previous paper [ 1 ] . In addition to the fully-connected DNN , we also presented the results of a simple CNN on the CIFAR-10 dataset , which showed similar patterns . We also performed the experiment on a toy dataset as you suggested , whose results demonstrated similar properties of linear regions . As there are so many choices for training a DNN , we can not fit all of them in a single paper ; so , we put the emphasis on BN and dropout while keeping other hyper-parameters by default . However , some of our findings were also observed in other studies [ 2 ] [ 4 ] , which shows the generalization of our results beyond the chosen setup . - I am not sure what to take away from figure 1 since it 's only a two-dimensional slice of very high-dimensional input space . Maybe the authors could instead choose an example with a low-dimensional input space for illustration purposes . The figure showing a two-dimensional slice of the input space was widely used in other papers to show some intuitive properties of the linear regions [ 1 ] [ 2 ] [ 3 ] , and we thought it is suitable for illustration purposes . However , we do believe it is important to precisely illustrate the properties ; so , according to your suggestion , we added another experiment on a toy 2D dataset in the revision . Please see Appendix A for more details ."}, {"review_id": "SkeFl1HKwr-2", "review_text": "This paper addresses the following: how do batch normalization and dropout affect the number of linear regions present in a deep network? It does so by devising a search procedure for enumerating a set of linear inequalities that define the linear region around a particular input. The linear region is defined as the region of input space that activates the same units/nodes in the network. The authors compute these linear regions for three different types of fully connected networks trained with: vanilla (nothing added), batch norm, and dropout. Given these linear regions, the authors studied a number of their properties, such as the radii of inscribed spheres, angles between hyperplanes, and number of unique surrounding regions. Comments - This paper enumerates a number of interesting findings, all of which seem to raise intriguing questions about the properties of trained networks. However, after reading the paper, I am left a little unsure of what to make of the results. However, I do not think this is a fault of the paper, instead I enjoyed that this paper raises so many interesting questions. Still, some more discussion and interpretation of the results is perhaps warranted. - I especially enjoyed the writing, the problem statement and exposition were clear and easy to follow. - Perhaps the authors could comment, in the discussion, if they think their methods could be extended to networks with smooth nonlinearities (such as tanh), or what aspects of their results are also apply to networks with different nonlinearities. - I was also curious if the authors could comment on similarities and differences between their findings and this relevant paper (https://arxiv.org/abs/1802.08760) by Novak et al. that empirically computes linear regions for 2D slices through input space. Minor edits - After introducing the definition of the insphere (eq 5), it would be helpful to remind the reader that this is for a particular region defined by the set of inequalities C^\\*. - Typo in footnote 2 on page 5: partitioned", "rating": "8: Accept", "reply_text": "Thank you for the positive comments ! Our detailed replies are given below . - This paper enumerates a number of interesting findings , all of which seem to raise intriguing questions about the properties of trained networks . However , after reading the paper , I am left a little unsure of what to make of the results . However , I do not think this is a fault of the paper , instead I enjoyed that this paper raises so many interesting questions . Still , some more discussion and interpretation of the results is perhaps warranted . I especially enjoyed the writing , the problem statement and exposition were clear and easy to follow . Thanks for your interest ! We expanded our discussion according to your suggestions . Please see our revision for more details . - Perhaps the authors could comment , in the discussion , if they think their methods could be extended to networks with smooth nonlinearities ( such as tanh ) , or what aspects of their results are also apply to networks with different nonlinearities . \u2018 Hard \u2019 linear regions can only be defined when the activation is piecewise linear . However , we believe our findings can be extended to networks with smooth nonlinear activation , because a smooth nonlinear activation , like tanh , can be approximated by piecewise linear functions . So far there is no precise definition of \u2018 soft \u2019 linear regions for DNNs with smooth nonlinearities , but we may provide some preliminary ideas to find these \u2018 soft \u2019 linear regions . First , we need a local linearity measure , such as Eq . ( 5 ) in https : //arxiv.org/abs/1907.02610 ; and then set a threshold of nonlinearity of every neuron , resulting in a set of inequalities to describe a \u2018 soft \u2019 linear region . Unfortunately , our methods can not be directly applied to analyzing these \u2018 soft \u2019 linear regions since their convexity is not guaranteed . It is still an open problem to precisely analyze these \u2018 soft \u2019 linear regions . A similar discussion was added in Section 4 . Please check our revision for more information . - I was also curious if the authors could comment on similarities and differences between their findings and this relevant paper ( https : //arxiv.org/abs/1802.08760 ) by Novak et al.that empirically computes linear regions for 2D slices through input space . This is a highly related work to ours . Thanks for pointing this out ! Fig.3 in Novak et al.illustrates that the on-manifold regions are usually larger than the off-manifold regions after training , which is also implied by our Fig.3 : the manifold regions are usually larger than the decision regions ( see the blue lines of the first two columns ) . Besides , our paper also shows some other properties of the linear regions , and compares the influences introduced by different optimization techniques . We updated Section 3.2 in our paper to include this discussion . - Minor edits We revised our manuscript according to your suggestions . Thanks again !"}], "0": {"review_id": "SkeFl1HKwr-0", "review_text": "First, I believe that the acknowledgements in the manuscript give identifying information which could stand in conflict with a double blind review process. I\u2019ll leave it to the area chairs/program chairs to make a decision on this. The following review will be contingent on the fact that the authors did not break the submission rules. This paper aims to give new insights into deep neural networks by presenting a number of approaches to analyse the linear regions in such networks. The authors define a linear region around a point x* as the intersection between a number of half spaces that are defined through linear approximations of a DNN (tangents) around that point x*. The authors show that points within these regions can be found using convex optimization with a number of linear constraints that are equal or less than the number of nodes in a DNN. In experiments with a fully connected network the authors analyse different properties of these linear regions: (1) How big is the biggest sphere that we can fit in a linear region? (2) How much do the hyperplanes that define a region correlate with each other? (3) How reliably does a linear region represent a single class? And (4), How does a linear region interact with neighbouring regions? In their presentation, the authors focus on comparing these properties between models that were either trained without regularisation, with batch normalisation, or with dropout and with different learning rates. This allows them to draw insightful conclusions about the difference between linear regions in these models. The authors hope that their work will enable new ways of analysing DDNs that will inspire new architectures and optimization techniques. I vote to accept this paper. The authors present a large array of methods to analyse the linear regions of DNNs. Their insights into the differences of BN and Dropout are useful (figure 1 & 2) and sensible (figure 3). The implications of linear regions on adversarial robustness can have an impact in the future. Because the paper relies on geometrical reasoning, I wished there would be more visualisations that guide the reader. Here are a number of comments and questions that I have on the manuscript: - Figure 1 Top: What do the different colour represent in the linear regions plot? - Section 2.1: maybe add a toy graph that visualises the depth-wise \u2018exclusion\u2019 process of feasible \u201cneighbours\u201d of x*? - Eq. (2) & (3): Explain where these equations come from. - Sec 3.2, first sentence. The authors claim that inspheres of linear regions are highly relate to the expressivity of DNN. Can they elaborate on that claim? Is this claim a result of their experiments? - What is the relationship between the number of constraints in eq. (5) and the radius of an insphere? Does the insphere size decrease with more constraints? What implications would that have on deeper networks than the one that was presented? - Why should distortion be a good measure of the size of a linear region? - The authors claim that it is expensive to run their approach, and that they will aim to improve speed in the future. Can the authors give a more concrete example of runtimes in their current approach? ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for the constructive comments , which helped improve the paper . We also apologize for our mistake for including the acknowledgement . It has been removed in our revision . Thank you for pointing this out ! We address your detailed comments below . - Figure 1 Top : What do the different colour represent in the linear regions plot ? The color represents the ratio of the activated nodes in a linear region . We added this in the caption of Fig.1 in the revision . Albeit that different colors were used to separate linear regions in the previous paper , we believe our plot can provide more information since the gray lines have already illustrated in different regions . - Section 2.1 : maybe add a toy graph that visualises the depth-wise \u2018 exclusion \u2019 process of feasible \u201c neighbours \u201d of x * ? Thanks for your suggestion ! We added Fig.2 to illustrate this more clearly . Please check Section 2.1 in our revision . - Eq . ( 2 ) & ( 3 ) : Explain where these equations come from . As mentioned in Section 2.1 , the first $ l-1 $ hidden layers serve as an affine transformation of $ \\mathbf { x } \\in S_ { l-1 } $ . Besides , the pre-activation outputs of the $ l $ -th hidden layer $ \\mathbf { h } ^l ( \\mathbf { x } ) $ are also an affine transformation of the activation outputs of the $ ( l-1 ) $ -th layer , hence $ \\mathbf { h } ^l ( \\mathbf { x } ) $ is a linear function of $ \\mathbf { x } \\in S_ { l-1 } $ , which means $ \\mathbf { h } ^l_n ( \\mathbf { x } ) =\\mathbf { w } _n^T\\mathbf { x } +b_n $ , where $ n $ denotes a node of the $ l $ -th layer . For a linear function $ y=\\mathbf { w } ^T\\mathbf { x } +b $ , the $ \\mathbf { w } $ can be directly calculated by $ \\mathbf { w } =\\nabla_ { \\mathbf { x } } y $ , whereas $ b=y-\\mathbf { w } ^T\\mathbf { x } $ . Here $ \\mathbf { x } $ and $ y $ can be replaced by $ \\mathbf { x } ^ * $ and $ \\mathbf { h } _n^l ( \\mathbf { x } ^ * ) $ because $ \\mathbf { x } ^ * $ shares the same linear function as other $ \\mathbf { x } \\in S_ { l-1 } $ . Last , the parameters are multiplied by $ \\mbox { sgn } ( \\mathbf { h } _n^l ( \\mathbf { x } ^ * ) ) $ to make sure that the inequalities , which indicate the activation states of the $ l $ -th layer , are all in the $ \\geq $ form . A formal deduction was added in Appendix B . - Sec 3.2 , first sentence . The authors claim that inspheres of linear regions are highly relate to the expressivity of DNN . Can they elaborate on that claim ? Is this claim a result of their experiments ? It is believed that a DNN with more linear regions has a larger potential to fit complex functions [ 1 ] [ 2 ] . For example , a regular hexagon is a better approximation of a circle than a square . Small inspheres do demonstrate the narrowness of the linear regions , resulting in a large number of regions . [ 1 ] Poole et al.Exponential expressivity in deep neural networks through transient chaos . NIPS , 2016 . [ 2 ] Pascanu et al.On the number of response regions of deep feed forward networks with piece-wise linear activations . https : //arxiv.org/abs/1312.6098 . 2014 - What is the relationship between the number of constraints in eq . ( 5 ) and the radius of an insphere ? Does the insphere size decrease with more constraints ? What implications would that have on deeper networks than the one that was presented ? Yes , the radius does decrease with more constraints , or more precisely , irredundant constraints ( which means the constraints can not be implied by others ) . A smaller inradius usually results in a larger number of linear regions , hence deeper networks usually have higher fitting ability . Regarding the number of linear regions , i.e.the complexity of DNNs , a well-known question is that why deeper networks have better generalization , instead of overfitting ? We believe it comes from the relevance among the linear regions . A node has a set of fixed weights , creating different constraints for different activation states of the preceding layers , which can be regarded as that part of the weights are picked to construct a constraint . However , different parts of the weights are chosen from the same set , resulting in this relevance . Maybe we are a little off the topic here , but it is really an interesting research direction . Another interesting direction is to show that depth provides irredundant constraints more efficiently than width , which is still our work in progress ."}, "1": {"review_id": "SkeFl1HKwr-1", "review_text": "This work presents an array of analytical tools to characterize linear regions of deep neural networks (DNNs). Using the tools the work analyzes the effect of dropout and batch normalization (BN) on the linear regions of trained DNNs; namely, by assessing the properties such as inspheres, orientation of hyperplanes, decision boundaries and relevance of surrounding regions, the authors highlight the differences and similarities of linear regions induced by vanilla SGD as compared to SGD with dropout or BN. The paper is clearly written and is easy to follow for the most part. The paper indeed presents a number properties for analyzing the nature of linear regions in DNNs; however it falls short of connecting them with an improvement in the optimization or interpretability of DNNs. Even with respect to providing support for general applicability, the work does not go very far: without enough variation in data (not just image benchmarks), tasks and architecture, it is hard to determine if the analysis tools presented in the paper generalize beyond the chosen setup. For instance just the optimization techniques compared in the paper have their own hyperparameters and it is not clear how the results might vary with them. I am not sure what to take away from figure 1 since it's only a two-dimensional slice of very high-dimensional input space. Maybe the authors could instead choose an example with a low-dimensional input space for illustration purposes. Also, how much can be perceived from distributions shown in figure 2, since inradius (Eq. 5) may turnout to be a very coarse representation of linear regions, especially for deeper networks. Can the authors clarify this? Moreover, how would the figures look if we were using a different objective, dataset or architecture? In figure 3, is it not possible to show the average results instead of just one example? I would further like to know how the authors would deal with scalability issues if their analysis were to applied to more realistic (i.e. large) network architectures.", "rating": "3: Weak Reject", "reply_text": "Thank you for the valuable feedback ! We respond to the weakness in the following and hope we have addressed all the concerns . - The paper is clearly written and is easy to follow for the most part . The paper indeed presents a number properties for analyzing the nature of linear regions in DNNs ; however it falls short of connecting them with an improvement in the optimization or interpretability of DNNs . Even with respect to providing support for general applicability , the work does not go very far : without enough variation in data ( not just image benchmarks ) , tasks and architecture , it is hard to determine if the analysis tools presented in the paper generalize beyond the chosen setup . For instance just the optimization techniques compared in the paper have their own hyperparameters and it is not clear how the results might vary with them . The main purpose of our paper is to provide a new geometric perspective to study the linear region instead of just counting them . The number of the linear regions represents the expressivity of a DNN , but fails to indicate the influence of region \u2019 s geometric properties on some local behaviors of DNN , such as robustness . We think our research may give some new inspirations for studying linear regions . There are a number of studies on linear regions , and our experimental setting mostly followed a previous paper [ 1 ] . In addition to the fully-connected DNN , we also presented the results of a simple CNN on the CIFAR-10 dataset , which showed similar patterns . We also performed the experiment on a toy dataset as you suggested , whose results demonstrated similar properties of linear regions . As there are so many choices for training a DNN , we can not fit all of them in a single paper ; so , we put the emphasis on BN and dropout while keeping other hyper-parameters by default . However , some of our findings were also observed in other studies [ 2 ] [ 4 ] , which shows the generalization of our results beyond the chosen setup . - I am not sure what to take away from figure 1 since it 's only a two-dimensional slice of very high-dimensional input space . Maybe the authors could instead choose an example with a low-dimensional input space for illustration purposes . The figure showing a two-dimensional slice of the input space was widely used in other papers to show some intuitive properties of the linear regions [ 1 ] [ 2 ] [ 3 ] , and we thought it is suitable for illustration purposes . However , we do believe it is important to precisely illustrate the properties ; so , according to your suggestion , we added another experiment on a toy 2D dataset in the revision . Please see Appendix A for more details ."}, "2": {"review_id": "SkeFl1HKwr-2", "review_text": "This paper addresses the following: how do batch normalization and dropout affect the number of linear regions present in a deep network? It does so by devising a search procedure for enumerating a set of linear inequalities that define the linear region around a particular input. The linear region is defined as the region of input space that activates the same units/nodes in the network. The authors compute these linear regions for three different types of fully connected networks trained with: vanilla (nothing added), batch norm, and dropout. Given these linear regions, the authors studied a number of their properties, such as the radii of inscribed spheres, angles between hyperplanes, and number of unique surrounding regions. Comments - This paper enumerates a number of interesting findings, all of which seem to raise intriguing questions about the properties of trained networks. However, after reading the paper, I am left a little unsure of what to make of the results. However, I do not think this is a fault of the paper, instead I enjoyed that this paper raises so many interesting questions. Still, some more discussion and interpretation of the results is perhaps warranted. - I especially enjoyed the writing, the problem statement and exposition were clear and easy to follow. - Perhaps the authors could comment, in the discussion, if they think their methods could be extended to networks with smooth nonlinearities (such as tanh), or what aspects of their results are also apply to networks with different nonlinearities. - I was also curious if the authors could comment on similarities and differences between their findings and this relevant paper (https://arxiv.org/abs/1802.08760) by Novak et al. that empirically computes linear regions for 2D slices through input space. Minor edits - After introducing the definition of the insphere (eq 5), it would be helpful to remind the reader that this is for a particular region defined by the set of inequalities C^\\*. - Typo in footnote 2 on page 5: partitioned", "rating": "8: Accept", "reply_text": "Thank you for the positive comments ! Our detailed replies are given below . - This paper enumerates a number of interesting findings , all of which seem to raise intriguing questions about the properties of trained networks . However , after reading the paper , I am left a little unsure of what to make of the results . However , I do not think this is a fault of the paper , instead I enjoyed that this paper raises so many interesting questions . Still , some more discussion and interpretation of the results is perhaps warranted . I especially enjoyed the writing , the problem statement and exposition were clear and easy to follow . Thanks for your interest ! We expanded our discussion according to your suggestions . Please see our revision for more details . - Perhaps the authors could comment , in the discussion , if they think their methods could be extended to networks with smooth nonlinearities ( such as tanh ) , or what aspects of their results are also apply to networks with different nonlinearities . \u2018 Hard \u2019 linear regions can only be defined when the activation is piecewise linear . However , we believe our findings can be extended to networks with smooth nonlinear activation , because a smooth nonlinear activation , like tanh , can be approximated by piecewise linear functions . So far there is no precise definition of \u2018 soft \u2019 linear regions for DNNs with smooth nonlinearities , but we may provide some preliminary ideas to find these \u2018 soft \u2019 linear regions . First , we need a local linearity measure , such as Eq . ( 5 ) in https : //arxiv.org/abs/1907.02610 ; and then set a threshold of nonlinearity of every neuron , resulting in a set of inequalities to describe a \u2018 soft \u2019 linear region . Unfortunately , our methods can not be directly applied to analyzing these \u2018 soft \u2019 linear regions since their convexity is not guaranteed . It is still an open problem to precisely analyze these \u2018 soft \u2019 linear regions . A similar discussion was added in Section 4 . Please check our revision for more information . - I was also curious if the authors could comment on similarities and differences between their findings and this relevant paper ( https : //arxiv.org/abs/1802.08760 ) by Novak et al.that empirically computes linear regions for 2D slices through input space . This is a highly related work to ours . Thanks for pointing this out ! Fig.3 in Novak et al.illustrates that the on-manifold regions are usually larger than the off-manifold regions after training , which is also implied by our Fig.3 : the manifold regions are usually larger than the decision regions ( see the blue lines of the first two columns ) . Besides , our paper also shows some other properties of the linear regions , and compares the influences introduced by different optimization techniques . We updated Section 3.2 in our paper to include this discussion . - Minor edits We revised our manuscript according to your suggestions . Thanks again !"}}