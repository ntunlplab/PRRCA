{"year": "2019", "forum": "HJxpDiC5tX", "title": " Large-Scale Visual Speech Recognition", "decision": "Reject", "meta_review": "This paper describes the development of a large-scale continuous visual speech recognition (lipreading) system, including an audiovisual processing pipeline that is used to extract stabilized videos of lips and corresponding phone sequences from YouTube videos, a deep network architecture trained with CTC loss that maps video sequences to sequences of distributions over phones, and an FST-based decoder that produces word sequences from the phone score sequences. A performance evaluation shows that the proposed system outperforms other models described in the literature, as well as professional lipreaders. A number of ablation experiments compare the performance of the proposed architecture to the previously proposed LipNet and \"Watch, Attend, and Spell\" architectures, explore the performance differences caused by using phone- or character-based CTC models, and some variations on the proposed architecture. This paper was extremely controversial and received a robust discussion between the authors and reviewers, with the primary point of contention being the suitability of the paper for ICLR. All reviewers agree that the quality of the work in the paper is excellent and that the reported results are impressive, but there was strong disagreement on whether or not this was sufficient for an ICLR paper. One reviewer thought so, while the other two reviewers argued that this is insufficient, and that to appear in ICLR the paper either (1) should have focused more on the preparation of the dataset, included public release of the data so other researchers could build on the work, and put forth the V2P model as a (very) strong baseline for the task; or (2) done a more in-depth exploration of the representation learning aspects of the work by comparing phoneme and viseme units and providing more (admittedly costly) ablation experiments to shed more light on what aspects of the V2P architecture lead to the reported improvements in performance. The AC finds the arguments of the two negative reviewers to be persuasive. It is quite clear at this point that many supervised classification tasks (even structured classification tasks like lipreading) can be effectively tackled by a combination of a sufficiently flexible learning architecture and collection of a massive, annotated dataset, and the modeling techniques used in this paper are not new, per se, even if their application to lipreading is. Moreover, if the dataset is not publicly available, it is impossible for anyone else to build on this work. The paper, as it currently stands, would be appropriate in a more applications-oriented venue.", "reviews": [{"review_id": "HJxpDiC5tX-0", "review_text": "This is a good paper. First of all, it presents a large-scale corpus for visual speech recognition. Second, it demonstrates a visual speech recognition system based on open-vocabulary that gives the state-of-the-art recognition accuracy. The paper is very well written and all the technical details are clearly laid out. I, for one, would like to thank the authors for this meticulous work to the community. This is by far the largest dataset and the most impressive performance for VSR I have even seen in the ASR/VSR community. I enjoyed reading this paper. I extend this review based on the replies. One of the arguments is that the work presented in this paper is a great success in engineering but it lacks technical novelty and therefore can not be accepted by the conference, which I think otherwise. First of all, the authors put together a very detailed and carefully designed technical pipeline for creating a very large visual speech recognition dataset, which is a valuable contribution to be community. (I assumed that the databset will become available to the community when reviewing the paper, which turned out not to be totally accurate. My apologies. I do hope the dataset will be made public. This is a major reason I gave a high score.) Second, the authors have built systems that give the state-of-the-art performance on visual speech recognition. Although the models and architectures are already out there, the impressive performance itself is an impact to the field. This is not simply achieved by piling in a large amount of data (although it does play a role). This is a system paper but its impact and its performance should at least get it in to the conference. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for your encouraging feedback ."}, {"review_id": "HJxpDiC5tX-1", "review_text": "The paper presents a non-trivial data processing pipeline, a large data set, and a system based on CTC and FSTs for automatic lipreading from videos. The review of the previous work is comprehensive. The authors are also awared of the state of the art in speech recognition, a highly related task. The collection of the data set is definitely a contribution, but other than that, the technical novelty is scarce, since all of the techniques have been proposed either in lipreading from video or in speech recognition. The numbers in Table 1 are impressive, but it is hard to tell where the improvement is coming from. It is worth running a few more experiments a) with the label set fixed while changing the network architecture b) with the network architecture fixed while changing the label set c) with the network and the label set fixed while changing dropout or group normalization. seq2seq is an odd child in this case, because you cannot really compare it to other settings. The result in Table 2 is also impressive, but it would be nice to have the proposed system trained on LRS3-TED and compare against TM-seq2seq. It is generally a consensus that a large model paired with a large amount of data gives you improvement, and this type of improvement is not considered a contribution. It is then the authors' responsibility to have a comprehensive experiments showing that the improvement is not just due to having a larger model and more data. Here are some minor details: p.6. note that there must be a blank between the 'e' characters to avoid collapsing ... --> this is actually not true, at least not in the original CTC formulation, where removing the duplicates and blanks have to be done in that order. To explain why modeling characters with CTC is problematic, ... --> this argument is not theoretically sound, so the question is does this happen in practice? the loss only measures at the independence level, but this doesn't prohibit the network to learn dependencies before the loss.", "rating": "4: Ok but not good enough - rejection", "reply_text": "> The result in Table 2 is also impressive , but it would be nice to have the proposed system trained on LRS3-TED and compare against TM-seq2seq. > It is generally a consensus that a large model paired with a large amount of data gives you improvement , and this type of improvement is not considered a contribution . It is then the authors ' responsibility to have a comprehensive experiments showing that the improvement is not just due to having a larger model and more data . Thank you . As our model takes about a month to train even with 64 GPUs , and the LRS3-TED dataset was released less than a month before the paper submission deadline , this was deemed infeasible . We think our ablations ( Table 1 ) make it very clear that a large model is not enough -- - we explicitly controlled for this , and found that one needs a well designed large model . The point of testing on the smaller LRS3-TED dataset was solely to illustrate that our model beats the state-of-the-art there even without being trained there -- - that is , the objective of the experiment was to show that our model generalizes well to other data . > note that there must be a blank between the ' e ' characters to avoid collapsing ... -- > this is actually not true , at least not in the original CTC formulation , where removing the duplicates and blanks have to be done in that order . In the original CTC formulation , indeed removal of duplicates happens before the removal of blanks . So in our example and the original CTC paper 's ' B ' function , B ( be # ee ) = bee , where # denotes blank . We will reword that sentence for clarity in the next update . > To explain why modeling characters with CTC is problematic , ... -- > this argument is not theoretically sound , so the question is does this happen in practice ? the loss only measures at the independence level , but this does n't prohibit the network to learn dependencies before the loss . This argument is indeed backed up by our empirical findings . While the network could learn dependencies before the loss , this would hamper its ability to represent the uncertainty that is inherent in visual speech recognition due to phonemes that appear visually similar ( visemes ) . In the case of audio ASR on clean audio , this argument is indeed less applicable ."}, {"review_id": "HJxpDiC5tX-2", "review_text": "The paper presents a large-scale lipreading system - no surprises there. This is good work and probably the strongest general purpose lip-reading system out there at this time, but i don't see both the work and the paper as a good fit for ICLR. The authors take a large corpus of YouTube videos (on which Google has already trained direct acoustics-to-word speech recognizers, and which is manually transcribed), filter it, and extract regions that can be used for lipreading. They then describe a scalable preprocessing, and train a phone-based acoustic model using CTC. They seem to be using the (Miao et al., 2015) and Google WFST based decoding framework, and achieve a word error rate of ca 40%. That is impressive, but I don't see any novelty here, and the paper is full of contradictions, and leaves some important open questions: - the authors argue for \"phonemes and ctc\", and no speech person would disagree with them; in fact (Miao et al., 2015) and many other papers show that the WERs with a good phoneme based dictionary in English are lower than with a character based model. it's just easier if one does not need a dictionary. - why are the authors not using a viseme dictionary, or map their phoneme dictionary to a viseme dictionary. In visual space, their own \"homonym\" argument applies, too, and \"mop\" (or \"mom\") and \"pop\" should be mapped to the same \"viseme\" sequence - and the resulting uncertainty should be handled by the decoder, and not the classifier. - how did the authors generate the one million word phoneme vocabulary? even google used around 100,000k words in their whole-word experiments, if i remember correctly? what happens if the authros reduce the vocabulary? could you provide some error analysis or at least deletions/ insertions/ substitutiosn, and compare them against an audio system? - LipNet and the proposed architecture seem to be very similar - maybe you could provide some insight into which changes made the biggest difference? - is the data going to be available? - what is a \"production-level speech decoder\"? how come your model \"is the first to combine a deep learning-based phoneme recognition model with production-grade word-level decoding techniques\" if Google does essentially the same (\"in production\")? - in Section 1, you say that \"by design, the trained model only performs well when videos are shot at specific angles when a subject is facing the camera, [...] It does not perform well in other contexts\". in Section 5, you demonstrate the \"generalization power of our V2P approach\"and find that it \"is able to generalize well\" - please clarify - \"speech impaired patients\" often have non-canonical articulation, the proposed system may not work well for them - it would be interesting to also know the absolute levels of insertions/ deletions/ substitutions for words and/ or phonemes, and for the audio only and visual systems, to be able to diagnose what the problems are. - finally, Figure 10 is really hard to view - i'd be happy to be shown fewer faces, the main message is that the quality of the face detection is really good?", "rating": "3: Clear rejection", "reply_text": "> could you provide some error analysis or at least deletions/ insertions/ substitutiosn , and compare them against an audio system ? We provide heatmaps with deletions / insertions / substitutions in Figures 4 and 7 for V2P , and we could easily provide similar results for our audio baseline . Thanks for the suggestion . > LipNet and the proposed architecture seem to be very similar - maybe you could provide some insight into which changes made the biggest difference ? Overall we found that 1 ) introducing stabilization in the processing pipeline , changing the 2 ) network size , 3 ) depth , 4 ) replacing dropout with group normalization , and 5 ) working in phoneme level with a decoder pipeline were the key components of our performance , an ablation for each is shown in Table 1 . We will ensure the differences are better emphasized in the main text . > is the data going to be available ? We are very interested in publishing our dataset / video timestamps and we are investigating possible ways to do so . We will only do so provided there are no privacy or security concerns . The massive scale and importance of this dataset demands responsible use . > what is a `` production-level speech decoder '' ? how come your model `` is the first to combine a deep learning-based phoneme recognition model with production-grade word-level decoding techniques '' if Google does essentially the same ( `` in production '' ) ? We mean specifically in the context of visual speech recognition . That is , the phoneme recognition model takes videos as input , not audio . Thank you , we will clarify it in our next update . > in Section 1 , you say that `` by design , the trained model only performs well when videos are shot at specific angles when a subject is facing the camera , [ ... ] It does not perform well in other contexts '' . in Section 5 , you demonstrate the `` generalization power of our V2P approach '' and find that it `` is able to generalize well '' - please clarify We apologize for the confusion . We will be more precise in the next version . The precise facts are as reported : In the TED experiments we examined the performance of speaker angles outside our training set and , as shown in Table 2 , outside this range the performance dropped by 8 WER , but indeed our model still outperforms TM-seq2seq.Thus , there is a drop in performance and this performance will eventually drop to zero if the lips stop appearing , but our approach is better at generalizing than other approaches . > `` speech impaired patients '' often have non-canonical articulation , the proposed system may not work well for them Correct , hyperarticulation is a case where our proposed system would not work well . We are working closely with specialists for identifying the cases we can help . Our preliminary investigation can be found in Appendix A . We expect that patients who spoke normally for their whole lives but only recently lost the ability to produce sound will retain mostly normal articulation . We have conducted some tests and the results are positive , and as soon as we obtain proper approval , we will release these . > it would be interesting to also know the absolute levels of insertions/ deletions/ substitutions for words and/ or phonemes , and for the audio only and visual systems , to be able to diagnose what the problems are . A table with absolute values would n't easily fit in the PDF , but we can include these as additional supplementary material . > finally , Figure 10 is really hard to view - i 'd be happy to be shown fewer faces , the main message is that the quality of the face detection is really good ? The point of this picture was to show the diversity of the LSVSR dataset . Thanks , we will add a note to this effect . References : [ 1 ] Chung , Joon Son , et al . `` Lip Reading Sentences in the Wild . '' CVPR.2017 . [ 2 ] Bear , Helen L. , and Richard Harvey . `` Phoneme-to-viseme mappings : the good , the bad , and the ugly . '' Speech Communication 95 ( 2017 ) : 40-67 . [ 3 ] Fisher , Cletus G. `` Confusions among visually perceived consonants . '' Journal of Speech , Language , and Hearing Research 11.4 ( 1968 ) : 796-804 . [ 4 ] Hazen , Timothy J. , et al . `` A segment-based audio-visual speech recognizer : Data collection , development , and initial experiments . '' Proceedings of the 6th international conference on Multimodal interfaces . ACM , 2004 ."}], "0": {"review_id": "HJxpDiC5tX-0", "review_text": "This is a good paper. First of all, it presents a large-scale corpus for visual speech recognition. Second, it demonstrates a visual speech recognition system based on open-vocabulary that gives the state-of-the-art recognition accuracy. The paper is very well written and all the technical details are clearly laid out. I, for one, would like to thank the authors for this meticulous work to the community. This is by far the largest dataset and the most impressive performance for VSR I have even seen in the ASR/VSR community. I enjoyed reading this paper. I extend this review based on the replies. One of the arguments is that the work presented in this paper is a great success in engineering but it lacks technical novelty and therefore can not be accepted by the conference, which I think otherwise. First of all, the authors put together a very detailed and carefully designed technical pipeline for creating a very large visual speech recognition dataset, which is a valuable contribution to be community. (I assumed that the databset will become available to the community when reviewing the paper, which turned out not to be totally accurate. My apologies. I do hope the dataset will be made public. This is a major reason I gave a high score.) Second, the authors have built systems that give the state-of-the-art performance on visual speech recognition. Although the models and architectures are already out there, the impressive performance itself is an impact to the field. This is not simply achieved by piling in a large amount of data (although it does play a role). This is a system paper but its impact and its performance should at least get it in to the conference. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for your encouraging feedback ."}, "1": {"review_id": "HJxpDiC5tX-1", "review_text": "The paper presents a non-trivial data processing pipeline, a large data set, and a system based on CTC and FSTs for automatic lipreading from videos. The review of the previous work is comprehensive. The authors are also awared of the state of the art in speech recognition, a highly related task. The collection of the data set is definitely a contribution, but other than that, the technical novelty is scarce, since all of the techniques have been proposed either in lipreading from video or in speech recognition. The numbers in Table 1 are impressive, but it is hard to tell where the improvement is coming from. It is worth running a few more experiments a) with the label set fixed while changing the network architecture b) with the network architecture fixed while changing the label set c) with the network and the label set fixed while changing dropout or group normalization. seq2seq is an odd child in this case, because you cannot really compare it to other settings. The result in Table 2 is also impressive, but it would be nice to have the proposed system trained on LRS3-TED and compare against TM-seq2seq. It is generally a consensus that a large model paired with a large amount of data gives you improvement, and this type of improvement is not considered a contribution. It is then the authors' responsibility to have a comprehensive experiments showing that the improvement is not just due to having a larger model and more data. Here are some minor details: p.6. note that there must be a blank between the 'e' characters to avoid collapsing ... --> this is actually not true, at least not in the original CTC formulation, where removing the duplicates and blanks have to be done in that order. To explain why modeling characters with CTC is problematic, ... --> this argument is not theoretically sound, so the question is does this happen in practice? the loss only measures at the independence level, but this doesn't prohibit the network to learn dependencies before the loss.", "rating": "4: Ok but not good enough - rejection", "reply_text": "> The result in Table 2 is also impressive , but it would be nice to have the proposed system trained on LRS3-TED and compare against TM-seq2seq. > It is generally a consensus that a large model paired with a large amount of data gives you improvement , and this type of improvement is not considered a contribution . It is then the authors ' responsibility to have a comprehensive experiments showing that the improvement is not just due to having a larger model and more data . Thank you . As our model takes about a month to train even with 64 GPUs , and the LRS3-TED dataset was released less than a month before the paper submission deadline , this was deemed infeasible . We think our ablations ( Table 1 ) make it very clear that a large model is not enough -- - we explicitly controlled for this , and found that one needs a well designed large model . The point of testing on the smaller LRS3-TED dataset was solely to illustrate that our model beats the state-of-the-art there even without being trained there -- - that is , the objective of the experiment was to show that our model generalizes well to other data . > note that there must be a blank between the ' e ' characters to avoid collapsing ... -- > this is actually not true , at least not in the original CTC formulation , where removing the duplicates and blanks have to be done in that order . In the original CTC formulation , indeed removal of duplicates happens before the removal of blanks . So in our example and the original CTC paper 's ' B ' function , B ( be # ee ) = bee , where # denotes blank . We will reword that sentence for clarity in the next update . > To explain why modeling characters with CTC is problematic , ... -- > this argument is not theoretically sound , so the question is does this happen in practice ? the loss only measures at the independence level , but this does n't prohibit the network to learn dependencies before the loss . This argument is indeed backed up by our empirical findings . While the network could learn dependencies before the loss , this would hamper its ability to represent the uncertainty that is inherent in visual speech recognition due to phonemes that appear visually similar ( visemes ) . In the case of audio ASR on clean audio , this argument is indeed less applicable ."}, "2": {"review_id": "HJxpDiC5tX-2", "review_text": "The paper presents a large-scale lipreading system - no surprises there. This is good work and probably the strongest general purpose lip-reading system out there at this time, but i don't see both the work and the paper as a good fit for ICLR. The authors take a large corpus of YouTube videos (on which Google has already trained direct acoustics-to-word speech recognizers, and which is manually transcribed), filter it, and extract regions that can be used for lipreading. They then describe a scalable preprocessing, and train a phone-based acoustic model using CTC. They seem to be using the (Miao et al., 2015) and Google WFST based decoding framework, and achieve a word error rate of ca 40%. That is impressive, but I don't see any novelty here, and the paper is full of contradictions, and leaves some important open questions: - the authors argue for \"phonemes and ctc\", and no speech person would disagree with them; in fact (Miao et al., 2015) and many other papers show that the WERs with a good phoneme based dictionary in English are lower than with a character based model. it's just easier if one does not need a dictionary. - why are the authors not using a viseme dictionary, or map their phoneme dictionary to a viseme dictionary. In visual space, their own \"homonym\" argument applies, too, and \"mop\" (or \"mom\") and \"pop\" should be mapped to the same \"viseme\" sequence - and the resulting uncertainty should be handled by the decoder, and not the classifier. - how did the authors generate the one million word phoneme vocabulary? even google used around 100,000k words in their whole-word experiments, if i remember correctly? what happens if the authros reduce the vocabulary? could you provide some error analysis or at least deletions/ insertions/ substitutiosn, and compare them against an audio system? - LipNet and the proposed architecture seem to be very similar - maybe you could provide some insight into which changes made the biggest difference? - is the data going to be available? - what is a \"production-level speech decoder\"? how come your model \"is the first to combine a deep learning-based phoneme recognition model with production-grade word-level decoding techniques\" if Google does essentially the same (\"in production\")? - in Section 1, you say that \"by design, the trained model only performs well when videos are shot at specific angles when a subject is facing the camera, [...] It does not perform well in other contexts\". in Section 5, you demonstrate the \"generalization power of our V2P approach\"and find that it \"is able to generalize well\" - please clarify - \"speech impaired patients\" often have non-canonical articulation, the proposed system may not work well for them - it would be interesting to also know the absolute levels of insertions/ deletions/ substitutions for words and/ or phonemes, and for the audio only and visual systems, to be able to diagnose what the problems are. - finally, Figure 10 is really hard to view - i'd be happy to be shown fewer faces, the main message is that the quality of the face detection is really good?", "rating": "3: Clear rejection", "reply_text": "> could you provide some error analysis or at least deletions/ insertions/ substitutiosn , and compare them against an audio system ? We provide heatmaps with deletions / insertions / substitutions in Figures 4 and 7 for V2P , and we could easily provide similar results for our audio baseline . Thanks for the suggestion . > LipNet and the proposed architecture seem to be very similar - maybe you could provide some insight into which changes made the biggest difference ? Overall we found that 1 ) introducing stabilization in the processing pipeline , changing the 2 ) network size , 3 ) depth , 4 ) replacing dropout with group normalization , and 5 ) working in phoneme level with a decoder pipeline were the key components of our performance , an ablation for each is shown in Table 1 . We will ensure the differences are better emphasized in the main text . > is the data going to be available ? We are very interested in publishing our dataset / video timestamps and we are investigating possible ways to do so . We will only do so provided there are no privacy or security concerns . The massive scale and importance of this dataset demands responsible use . > what is a `` production-level speech decoder '' ? how come your model `` is the first to combine a deep learning-based phoneme recognition model with production-grade word-level decoding techniques '' if Google does essentially the same ( `` in production '' ) ? We mean specifically in the context of visual speech recognition . That is , the phoneme recognition model takes videos as input , not audio . Thank you , we will clarify it in our next update . > in Section 1 , you say that `` by design , the trained model only performs well when videos are shot at specific angles when a subject is facing the camera , [ ... ] It does not perform well in other contexts '' . in Section 5 , you demonstrate the `` generalization power of our V2P approach '' and find that it `` is able to generalize well '' - please clarify We apologize for the confusion . We will be more precise in the next version . The precise facts are as reported : In the TED experiments we examined the performance of speaker angles outside our training set and , as shown in Table 2 , outside this range the performance dropped by 8 WER , but indeed our model still outperforms TM-seq2seq.Thus , there is a drop in performance and this performance will eventually drop to zero if the lips stop appearing , but our approach is better at generalizing than other approaches . > `` speech impaired patients '' often have non-canonical articulation , the proposed system may not work well for them Correct , hyperarticulation is a case where our proposed system would not work well . We are working closely with specialists for identifying the cases we can help . Our preliminary investigation can be found in Appendix A . We expect that patients who spoke normally for their whole lives but only recently lost the ability to produce sound will retain mostly normal articulation . We have conducted some tests and the results are positive , and as soon as we obtain proper approval , we will release these . > it would be interesting to also know the absolute levels of insertions/ deletions/ substitutions for words and/ or phonemes , and for the audio only and visual systems , to be able to diagnose what the problems are . A table with absolute values would n't easily fit in the PDF , but we can include these as additional supplementary material . > finally , Figure 10 is really hard to view - i 'd be happy to be shown fewer faces , the main message is that the quality of the face detection is really good ? The point of this picture was to show the diversity of the LSVSR dataset . Thanks , we will add a note to this effect . References : [ 1 ] Chung , Joon Son , et al . `` Lip Reading Sentences in the Wild . '' CVPR.2017 . [ 2 ] Bear , Helen L. , and Richard Harvey . `` Phoneme-to-viseme mappings : the good , the bad , and the ugly . '' Speech Communication 95 ( 2017 ) : 40-67 . [ 3 ] Fisher , Cletus G. `` Confusions among visually perceived consonants . '' Journal of Speech , Language , and Hearing Research 11.4 ( 1968 ) : 796-804 . [ 4 ] Hazen , Timothy J. , et al . `` A segment-based audio-visual speech recognizer : Data collection , development , and initial experiments . '' Proceedings of the 6th international conference on Multimodal interfaces . ACM , 2004 ."}}