{"year": "2017", "forum": "BJmCKBqgl", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "decision": "Reject", "meta_review": "The Area Chair recommends to reject this paper given the reviewers concern about the limited significance of this work and the lack of comparisons. We encourage the authors to take into account the reviewers feedback and resubmit.", "reviews": [{"review_id": "BJmCKBqgl-0", "review_text": "Dyvedeep presents three approximation techniques for deep vision models aimed at improving inference speed. The techniques are novel as far as I know. The paper is clear, the results are plausible. The evaluation of the proposed techniques is does not make a compelling case that someone interested in faster inference would ultimately be well-served by a solution involving the proposed methods. The authors delineate \"static\" acceleration techniques (e.g. reduced bit-width, weight pruning) from \"dynamic\" acceleration techniques which are changes to the inference algorithm itself. The delineation would be fine if the use of each family of techniques were independent of the other, but this is not the case. For example, the use of SPET would, I think, conflict with the use of factored weight matrices (I recall this from http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning.pdf, but I suspect there may be more recent work). For this reason, a comparison between SPET and factored weight matrices would strengthen the case that SPET is a relevant innovation. In favor of the factored-matrix approach, there would I think be fewer hyperparameters and the computations would make more-efficient use of blocked linear algebra routines--the case for the superiority of SPET might be difficult to make. The authors also do not address their choice of the Xeon for benchmarking, when the use cases they identify in the introduction include \"low power\" and \"deeply embedded\" applications. In these sorts of applications, a mobile GPU would be used, not a Xeon. A GPU implementation of a convnet works differently than a CPU implementation in ways that might reduce or eliminate the advantage of the acceleration techniques put forward in this paper. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Use of Xeon : We thank the reviewer for the insightful comment . The primary goal of DyVEDeep was to reduce the average number of compute operations per input , which is platform independent . However , as the reviewer correctly points out , how the reduction in operations translates to benefits in performance depends on the implementation platform . We chose the Xeon to prototype DyVEDeep , but believe that the benefits on low-power CPUs with small numbers of cores such as Intel Atom or ARM would be quite similar . With regard to GPU implementations , the improvements are less obvious and would need experimental evaluation ( we are currently pursuing this ) . However , as described in the response to reviewer 2 ( AnonReviewer2 ) , our approximations are coarse-grained and largely preserve regularity . Hence , we believe they would be useful on GPUs ."}, {"review_id": "BJmCKBqgl-1", "review_text": "This work proposes a number of approximations for speeding up feed-forward network computations at inference time. Unlike much of the previous work in this area which tries to compress a large network, the authors propose algorithms that decide whether to approximate computations for each particular input example. Speeding up inference is an important problem and this work takes a novel approach. The presentation is exceptionally clear, the diagrams are very beautiful, the ideas are interesting, and the experiments are good. This is a high-quality paper. I especially enjoyed the description of the different methods proposed (SPET, SDSS, SFMA) to exploit patterns in the classifer. My main concern is that the significance of this work is limited because of the additional complexity and computational costs of using these approximations. In the experiments, the DyVEDeep approach was compared to serial implementations of four large classification models --- inference in these models is order of magnitudes faster on systems that support parallelization. I assume that DyVEDeep has little-to-no performance advantage on a system that allows parallelization, and so anyone looking to speed up their inference on a serial system would want to see a comparison between this approach and the model-compression approaches. Thus, I am not sure how much of an impact this approach can have in it's current state. Suggestions: -I wondered what (if any) bounds could be made on the approximation errors of the proposed methods?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the comments . In the context of sequential implementations , we have revised the paper to include a quantitative comparison between DyVEDeep and weight compression techniques . Specifically , we applied DyVEDeep to a compressed AlexNet model available at https : //github.com/songhan/Deep-Compression-AlexNet ( Han et al. , 2015 ) . We achieved 1.8X improvement in performance over the model-compressed AlexNet implementation . Qualitatively , DyVEDeep is complementary to model compression techniques , as they target different opportunities . Model compression techniques prune/quantize weights of small magnitude , whereas DyVEDeep leverages other properties such as the saturating nature of neurons ( SPET ) and the correlation between neuron activations ( SSDS and SFMA ) . Moreover , model compression is static whereas DyveDeep is based on dynamic techniques . Finally , the benefits of model compression are primarily seen in fully connected layers whereas the DyVEDeep primarily benefits convolutional layers . Thus DyVEDeep achieves substantial performance improvement on top of model compression approaches . In the case of parallel implementations , the reviewer is right to point out that DyVEDeep introduces dynamism in the workload , which can have implications on efficiency . We are yet to prototype DyVEDeep on parallel systems , and are thus unable to show quantitative results on this front . However , we believe that benefits from DyVEDeep can be leveraged in the context of parallel implementations , as the approximations are introduced in a coarse-grained manner and largely preserve the regularity of computation . Consider a typical parallelization strategy , wherein each core in the system is assigned a group of neurons in a layer . In the case of SPET , since all neurons need to process the first 50 % of their inputs , the workload is balanced during this phase . When the remaining inputs are evaluated for selected neurons , the workload is still roughly balanced if , on an average , equal number of neurons need to be processed on each core . However , if an imbalance exists , work ( neurons ) from one core needs to be dynamically migrated to another . Dynamic scheduling strategies such as work stealing , supported by many popular parallelization frameworks ( e.g.TBB , Cilk etc . ) , could be leveraged to achieve this . Since neurons in deep networks have large numbers of inputs , the quantum of work is substantial enough to outweigh any overheads . In the context of the SDSS technique , work is balanced in the initial phase as the features are uniformly sampled . The process of identifying which of unsampled neurons need to be evaluated also involves equal amount of work to be executed on all cores . Similar to the SPET technique , any imbalance in the process of evaluating the neurons selected can be addressed through dynamic load balancing . The final technique , SFMA , is the most coarse-grained approximation , wherein the entire 2D convolution is replaced by a scalar multiplication . The process of identifying which input features to approximate for each output feature can be uniformly partitioned across all cores . Also , since all neurons in the feature are uniformly approximated , the workload is largely uniform across all the cores . In summary , since the approximations introduced by the proposed techniques are coarse-grained and preserve regularity , we believe the benefits from DyVEDeep can be realized in the context of parallel implementations ."}, {"review_id": "BJmCKBqgl-2", "review_text": "The authors describe a series of techniques which can be used to reduce the total amount of computation that needs to be performed in Deep Neural Networks. The authors propose to selectively identify how important a certain set of computations is to the final DNN output, and to use this information to selectively skip certain computations in the network. As deep learning technologies become increasingly widespread on mobile devices, techniques which enable efficient inference on such devices are becoming increasingly important for practical applications. The paper is generally well-written and clear to follow. I had two main comments that concern the experimental design, and the relationship to previous work: 1. In the context of deployment on mobile devices, computational costs in terms of both system memory as well as processing are important consideration. While the proposed techniques do improve computational costs, they don\u2019t reduce model size in terms of total number of parameters. Also, the gains obtained using the proposed method appear to be similar to other works that do allow for improvements in terms of both memory and computation (see, e.g., (Han et al., 2015)). It would have been interesting if the authors had reported results when the proposed techniques were applied to models that have been compressed in size as well. S. Han, H. Mao, and W. J. Dally. \"Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding.\" arXiv prepring arXiv:1510.00149 (2015). 2. The SDSS technique in the paper appears to be very similar to the \u201cPerforated CNN\u201d technique proposed by Figurnov et al. (2015). In that work, as in the authors work, CNN activations are approximated by interpolating responses from neighbors. The authors should comment on the similarity and differences between the proposed method and the referenced work. Figurnov, Michael, Dmitry Vetrov, and Pushmeet Kohli. \"Perforatedcnns: Acceleration through elimination of redundant convolutions.\" arXiv preprint arXiv:1504.08362 (2015). Other minor comments appear below: 3. A clarification question: In comparing the proposed methods to the baseline, in Section 4, the authors mention that they used their own custom implementation. However, do the baselines use the same custom implementation, or do they used the optimized BLAS libraries? 4. The authors should also consider citing the following additional references: * S. Tan and K. C. Sim, \"Towards implicit complexity control using variable-depth deep neural networks for automatic speech recognition,\" 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, 2016, pp. 5965-5969. * Graves, Alex. \"Adaptive Computation Time for Recurrent Neural Networks.\" arXiv preprint arXiv:1603.08983 (2016). 5. Please explain what the Y-axis in Figure 7 represents in the text. 6. Typographical Error: Last paragraph of Section 2: \u201c... are qualitatively different the aforementioned ...\u201d \u2192 \u201c... are qualitatively different from the aforementioned ...\u201d", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the comments . Please find our responses to individual questions below . 1.Comparison to Deep compression : We agree with the reviewers comment that DyVEDeep targets reducing the number of computations performed , and does not reduce the model size . Based on the reviewer \u2019 s suggestion , we applied the techniques proposed in DyVEDeep to the compressed AlexNet model avaliable at https : //github.com/songhan/Deep-Compression-AlexNet ( Han et al. , 2015 ) . We achieved 1.8X improvement in runtime for < 0.5 % loss in accuracy on top of the compressed AlexNet model . For quick turn-around , we used the same value of DyVEDeep hyper-parameters for this experiment that we obtained for the original AlexNet DNN . We believe that the speedup can be further improved with hyper-parameter tuning ; nevertheless , this result establishes that our proposal can achieve considerable improvements over and beyond Deep Compression . We also want to emphasize that DyVEDeep is qualitatively very different from weight compression techniques such as Deep compression . Weight compression techniques typically approximate DNNs by pruning/quantizing connections whose weight magnitudes are close to zero . On the contrary , the techniques proposed in DyVEDeep target other opportunities viz . the saturating nature of neurons , spatial correlation between neurons in a feature etc. , which are not exploited by weight compression . It is also worth noting that leveraging the above opportunities requires DyVEDeep to dynamically evaluate the significance of computations . In contrast , weight compression techniques are static i.e.the same compressed model is applied to all inputs . Further , specifically in the case of Deep compression , improvement in performance is reported only for the fully connected layers , which exhibit the highest compression ratio . The authors explicitly attribute the performance improvement to the fact that the fully-connected layer weights fit in the the L3 cache of their hardware platform . Although DyVEDeep is applicable to fully-connected layers , almost all of our benefits stem from speeding up the convolutional layers of our benchmarks . In summary , we believe DyVEDeep is complementary to Deep compression and other weight compression techniques , and can achieve benefits over and beyond them . 2.Difference between SDSS and Perforated CNN : We thank the reviewer for pointing out this related work . We have added a reference to Perforated CNNs in the modified version of the paper . At a high level , the SDSS technique is related to Perforated CNNs in that it approximates the activation of a neuron as a function of its neighbours . However , the key difference is that , in SDSS , the neurons that are approximated are selected at runtime based on the magnitude and variance between activations of its neighbors . On the other hand , in Perforated CNNs , the approximated neurons are statically determined during training time . Thus , SDSS can dynamically focus approximations on regions wherein neuron activations exhibit the highest spatial correlation , potentially yielding a superior speedup vs. accuracy trade-off . It is also worth noting that the other techniques proposed in DyVEDeep viz . SPET and SFMA are completely different from Perforated CNNs . 3.Baseline implementation : We used our custom implementation to realize both the baseline and DyVEDeep versions of all the benchmarks . 4.Additional References : We have modified the paper to include the above references . 5.Fig.7 Y-axis : The Y-axis in Figure 7 is a normalized scale to represent the improvement in both the scalar operations and runtime . We have included this explanation in the paper . 6.Typographical error : We have corrected the typographical error in the revised version of the paper ."}], "0": {"review_id": "BJmCKBqgl-0", "review_text": "Dyvedeep presents three approximation techniques for deep vision models aimed at improving inference speed. The techniques are novel as far as I know. The paper is clear, the results are plausible. The evaluation of the proposed techniques is does not make a compelling case that someone interested in faster inference would ultimately be well-served by a solution involving the proposed methods. The authors delineate \"static\" acceleration techniques (e.g. reduced bit-width, weight pruning) from \"dynamic\" acceleration techniques which are changes to the inference algorithm itself. The delineation would be fine if the use of each family of techniques were independent of the other, but this is not the case. For example, the use of SPET would, I think, conflict with the use of factored weight matrices (I recall this from http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning.pdf, but I suspect there may be more recent work). For this reason, a comparison between SPET and factored weight matrices would strengthen the case that SPET is a relevant innovation. In favor of the factored-matrix approach, there would I think be fewer hyperparameters and the computations would make more-efficient use of blocked linear algebra routines--the case for the superiority of SPET might be difficult to make. The authors also do not address their choice of the Xeon for benchmarking, when the use cases they identify in the introduction include \"low power\" and \"deeply embedded\" applications. In these sorts of applications, a mobile GPU would be used, not a Xeon. A GPU implementation of a convnet works differently than a CPU implementation in ways that might reduce or eliminate the advantage of the acceleration techniques put forward in this paper. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Use of Xeon : We thank the reviewer for the insightful comment . The primary goal of DyVEDeep was to reduce the average number of compute operations per input , which is platform independent . However , as the reviewer correctly points out , how the reduction in operations translates to benefits in performance depends on the implementation platform . We chose the Xeon to prototype DyVEDeep , but believe that the benefits on low-power CPUs with small numbers of cores such as Intel Atom or ARM would be quite similar . With regard to GPU implementations , the improvements are less obvious and would need experimental evaluation ( we are currently pursuing this ) . However , as described in the response to reviewer 2 ( AnonReviewer2 ) , our approximations are coarse-grained and largely preserve regularity . Hence , we believe they would be useful on GPUs ."}, "1": {"review_id": "BJmCKBqgl-1", "review_text": "This work proposes a number of approximations for speeding up feed-forward network computations at inference time. Unlike much of the previous work in this area which tries to compress a large network, the authors propose algorithms that decide whether to approximate computations for each particular input example. Speeding up inference is an important problem and this work takes a novel approach. The presentation is exceptionally clear, the diagrams are very beautiful, the ideas are interesting, and the experiments are good. This is a high-quality paper. I especially enjoyed the description of the different methods proposed (SPET, SDSS, SFMA) to exploit patterns in the classifer. My main concern is that the significance of this work is limited because of the additional complexity and computational costs of using these approximations. In the experiments, the DyVEDeep approach was compared to serial implementations of four large classification models --- inference in these models is order of magnitudes faster on systems that support parallelization. I assume that DyVEDeep has little-to-no performance advantage on a system that allows parallelization, and so anyone looking to speed up their inference on a serial system would want to see a comparison between this approach and the model-compression approaches. Thus, I am not sure how much of an impact this approach can have in it's current state. Suggestions: -I wondered what (if any) bounds could be made on the approximation errors of the proposed methods?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the comments . In the context of sequential implementations , we have revised the paper to include a quantitative comparison between DyVEDeep and weight compression techniques . Specifically , we applied DyVEDeep to a compressed AlexNet model available at https : //github.com/songhan/Deep-Compression-AlexNet ( Han et al. , 2015 ) . We achieved 1.8X improvement in performance over the model-compressed AlexNet implementation . Qualitatively , DyVEDeep is complementary to model compression techniques , as they target different opportunities . Model compression techniques prune/quantize weights of small magnitude , whereas DyVEDeep leverages other properties such as the saturating nature of neurons ( SPET ) and the correlation between neuron activations ( SSDS and SFMA ) . Moreover , model compression is static whereas DyveDeep is based on dynamic techniques . Finally , the benefits of model compression are primarily seen in fully connected layers whereas the DyVEDeep primarily benefits convolutional layers . Thus DyVEDeep achieves substantial performance improvement on top of model compression approaches . In the case of parallel implementations , the reviewer is right to point out that DyVEDeep introduces dynamism in the workload , which can have implications on efficiency . We are yet to prototype DyVEDeep on parallel systems , and are thus unable to show quantitative results on this front . However , we believe that benefits from DyVEDeep can be leveraged in the context of parallel implementations , as the approximations are introduced in a coarse-grained manner and largely preserve the regularity of computation . Consider a typical parallelization strategy , wherein each core in the system is assigned a group of neurons in a layer . In the case of SPET , since all neurons need to process the first 50 % of their inputs , the workload is balanced during this phase . When the remaining inputs are evaluated for selected neurons , the workload is still roughly balanced if , on an average , equal number of neurons need to be processed on each core . However , if an imbalance exists , work ( neurons ) from one core needs to be dynamically migrated to another . Dynamic scheduling strategies such as work stealing , supported by many popular parallelization frameworks ( e.g.TBB , Cilk etc . ) , could be leveraged to achieve this . Since neurons in deep networks have large numbers of inputs , the quantum of work is substantial enough to outweigh any overheads . In the context of the SDSS technique , work is balanced in the initial phase as the features are uniformly sampled . The process of identifying which of unsampled neurons need to be evaluated also involves equal amount of work to be executed on all cores . Similar to the SPET technique , any imbalance in the process of evaluating the neurons selected can be addressed through dynamic load balancing . The final technique , SFMA , is the most coarse-grained approximation , wherein the entire 2D convolution is replaced by a scalar multiplication . The process of identifying which input features to approximate for each output feature can be uniformly partitioned across all cores . Also , since all neurons in the feature are uniformly approximated , the workload is largely uniform across all the cores . In summary , since the approximations introduced by the proposed techniques are coarse-grained and preserve regularity , we believe the benefits from DyVEDeep can be realized in the context of parallel implementations ."}, "2": {"review_id": "BJmCKBqgl-2", "review_text": "The authors describe a series of techniques which can be used to reduce the total amount of computation that needs to be performed in Deep Neural Networks. The authors propose to selectively identify how important a certain set of computations is to the final DNN output, and to use this information to selectively skip certain computations in the network. As deep learning technologies become increasingly widespread on mobile devices, techniques which enable efficient inference on such devices are becoming increasingly important for practical applications. The paper is generally well-written and clear to follow. I had two main comments that concern the experimental design, and the relationship to previous work: 1. In the context of deployment on mobile devices, computational costs in terms of both system memory as well as processing are important consideration. While the proposed techniques do improve computational costs, they don\u2019t reduce model size in terms of total number of parameters. Also, the gains obtained using the proposed method appear to be similar to other works that do allow for improvements in terms of both memory and computation (see, e.g., (Han et al., 2015)). It would have been interesting if the authors had reported results when the proposed techniques were applied to models that have been compressed in size as well. S. Han, H. Mao, and W. J. Dally. \"Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding.\" arXiv prepring arXiv:1510.00149 (2015). 2. The SDSS technique in the paper appears to be very similar to the \u201cPerforated CNN\u201d technique proposed by Figurnov et al. (2015). In that work, as in the authors work, CNN activations are approximated by interpolating responses from neighbors. The authors should comment on the similarity and differences between the proposed method and the referenced work. Figurnov, Michael, Dmitry Vetrov, and Pushmeet Kohli. \"Perforatedcnns: Acceleration through elimination of redundant convolutions.\" arXiv preprint arXiv:1504.08362 (2015). Other minor comments appear below: 3. A clarification question: In comparing the proposed methods to the baseline, in Section 4, the authors mention that they used their own custom implementation. However, do the baselines use the same custom implementation, or do they used the optimized BLAS libraries? 4. The authors should also consider citing the following additional references: * S. Tan and K. C. Sim, \"Towards implicit complexity control using variable-depth deep neural networks for automatic speech recognition,\" 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, 2016, pp. 5965-5969. * Graves, Alex. \"Adaptive Computation Time for Recurrent Neural Networks.\" arXiv preprint arXiv:1603.08983 (2016). 5. Please explain what the Y-axis in Figure 7 represents in the text. 6. Typographical Error: Last paragraph of Section 2: \u201c... are qualitatively different the aforementioned ...\u201d \u2192 \u201c... are qualitatively different from the aforementioned ...\u201d", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the comments . Please find our responses to individual questions below . 1.Comparison to Deep compression : We agree with the reviewers comment that DyVEDeep targets reducing the number of computations performed , and does not reduce the model size . Based on the reviewer \u2019 s suggestion , we applied the techniques proposed in DyVEDeep to the compressed AlexNet model avaliable at https : //github.com/songhan/Deep-Compression-AlexNet ( Han et al. , 2015 ) . We achieved 1.8X improvement in runtime for < 0.5 % loss in accuracy on top of the compressed AlexNet model . For quick turn-around , we used the same value of DyVEDeep hyper-parameters for this experiment that we obtained for the original AlexNet DNN . We believe that the speedup can be further improved with hyper-parameter tuning ; nevertheless , this result establishes that our proposal can achieve considerable improvements over and beyond Deep Compression . We also want to emphasize that DyVEDeep is qualitatively very different from weight compression techniques such as Deep compression . Weight compression techniques typically approximate DNNs by pruning/quantizing connections whose weight magnitudes are close to zero . On the contrary , the techniques proposed in DyVEDeep target other opportunities viz . the saturating nature of neurons , spatial correlation between neurons in a feature etc. , which are not exploited by weight compression . It is also worth noting that leveraging the above opportunities requires DyVEDeep to dynamically evaluate the significance of computations . In contrast , weight compression techniques are static i.e.the same compressed model is applied to all inputs . Further , specifically in the case of Deep compression , improvement in performance is reported only for the fully connected layers , which exhibit the highest compression ratio . The authors explicitly attribute the performance improvement to the fact that the fully-connected layer weights fit in the the L3 cache of their hardware platform . Although DyVEDeep is applicable to fully-connected layers , almost all of our benefits stem from speeding up the convolutional layers of our benchmarks . In summary , we believe DyVEDeep is complementary to Deep compression and other weight compression techniques , and can achieve benefits over and beyond them . 2.Difference between SDSS and Perforated CNN : We thank the reviewer for pointing out this related work . We have added a reference to Perforated CNNs in the modified version of the paper . At a high level , the SDSS technique is related to Perforated CNNs in that it approximates the activation of a neuron as a function of its neighbours . However , the key difference is that , in SDSS , the neurons that are approximated are selected at runtime based on the magnitude and variance between activations of its neighbors . On the other hand , in Perforated CNNs , the approximated neurons are statically determined during training time . Thus , SDSS can dynamically focus approximations on regions wherein neuron activations exhibit the highest spatial correlation , potentially yielding a superior speedup vs. accuracy trade-off . It is also worth noting that the other techniques proposed in DyVEDeep viz . SPET and SFMA are completely different from Perforated CNNs . 3.Baseline implementation : We used our custom implementation to realize both the baseline and DyVEDeep versions of all the benchmarks . 4.Additional References : We have modified the paper to include the above references . 5.Fig.7 Y-axis : The Y-axis in Figure 7 is a normalized scale to represent the improvement in both the scalar operations and runtime . We have included this explanation in the paper . 6.Typographical error : We have corrected the typographical error in the revised version of the paper ."}}