{"year": "2020", "forum": "Hyg53gSYPB", "title": "Defense against Adversarial Examples by Encoder-Assisted Search in the Latent Coding Space", "decision": "Reject", "meta_review": "The paper proposes a defense for adversarial attacks based on autoencoders that tries to find the closest point to the natural image in the output span of the decoder and \"purify\" the adversarial example. There were concerns about the work being too incremental over DefenseGAN and about empirical evaluation of the defense. It is crucial to test the defense methods against best available attacks to establish the effectiveness. Authors should also discuss and consider evaluating their method against the attack proposed in https://arxiv.org/pdf/1712.09196.pdf that claims to greatly reduce the defense accuracy of DefenseGAN. ", "reviews": [{"review_id": "Hyg53gSYPB-0", "review_text": "Summary: This paper proposes AE-GAN+sr, an auto-encoder based GAN for equipping neural networks with better defenses against adversarial attacks. The authors evaluate their method on black-box attacks, white-box attacks, and gray-box attacks on MNIST and Fashion-MNIST, and show decent empirical results when compared to baselines. Decision: Reject. The writing was hard to follow and the experimental evaluations could have been stronger. Supporting Arguments/Feedback: - There are a lot of phrases which should be reworded in the text (e.g. \u201cit requires attention to\u201d in paragraph 1, \u201cdetect if sample is from a normal distribution\u201d at the end of Section 1, etc.) for better clarity. The writing was pretty hard to follow and the paper would really benefit overall after it has been improved. - It was hard for me to tell from the text how exactly the models were implemented and trained (see Question #1). Making this point more clear, in either the main text or the Appendix, would be really helpful. - The performance of AE-GAN+sr as evaluated on MNIST and FashionMNIST did not provide convincing evidence that it outperformed baselines such as Defense-GAN or adversarial training, though I did appreciate the authors\u2019 additional analysis on the tradeoffs between computational cost and performance for the Defense-GAN. The results would also have been more compelling had the authors evaluated their method on more complex datasets such as CIFAR-10. Questions: - It wasn\u2019t clear to me how the encoder-assisted search process (Section 3.4) works in practice. When you use gradient descent to find the best encoding for a corrupted input, how many steps do you need to take? Does this step happen while the autoencoder and GAN are being trained, or are those pre-trained and you just take additional gradient steps in the latent space? - Given that the BiGAN also incorporates an encoder (and they were also benchmarked against in the experiments), where do the advantages of the AE-GAN+sr come from? I think making this point in the text would also be helpful as well. ", "rating": "3: Weak Reject", "reply_text": "Thanks for giving us valuable advice . We would like to address your points one by one as follows . Q1.There are a lot of phrases that should be reworded in the text for better clarity . The writing was pretty hard to follow and the paper would really benefit overall after it has been improved . A1.We admit that there are confusing phrases in the original version . We will address your concern by revising the paper thoroughly . Q2.It was hard for me to tell from the text how exactly the models were implemented and trained . Making this point more clear , in either the main text or the Appendix , would be really helpful . A2.Thanks for the suggestion . We would like to provide pseudo-codes for the training and testing stage in Appendix C in our revised version . The testing stage was implemented differently from the training stage . In Sec.3.3 of the paper , the method for training , denoted as AE-GAN+r , does not include the searching process . When defending against adversarial samples in the inference process , the trained AE-GAN+r becomes AE-GAN+rs ( see Sec 3.2 and Sec 3.4 ) by including the searching process which is denoted as { +s } ( see Sec 3.4 ) . More details are given below : ( 1 ) For the training stage , AE-GAN+r has no searching process and is trained with legitimate samples . As a modified auto-encoder , AE-GAN+r adds a discriminator to AE and is equipped with an adversarial loss such that the decoder produces realistic images . AE-GAN+r is trained to optimize the min-max loss in Eq . ( 4 ) in the manuscript . The training strategy is the same as GANs [ 1 ] , where the parameters in Encoder-Decoder and the parameters in discriminator are updated alternatively to minimize \u201c Generative Loss + Reconstruction Loss \u201d and \u201c D_Loss \u201d respectively . ( 2 ) In the inference stage , the AE-GAN+r has been trained and we want to use it to defend against adversarial samples . To get better reconstruction , the searching process is activated , i.e. , we use gradient descent to find the best encoding in the latent space for a corrupted input . This defense strategy is denoted as AE-GAN+rs . In practice , we take 15 steps for GD iterations in the searching process , and the learning rate is set to 0.01 ( multiplied by 0.5 every 5 steps ) . Q3.The performance of AE-GAN+rs as evaluated on MNIST and FashionMNIST did not provide convincing evidence that it outperformed baselines such as Defense-GAN or adversarial training . The results would also have been more compelling had the authors evaluated their method on more complex datasets such as CIFAR-10 . A3.Thanks for your suggestion . Based on the suggestion , we have added additional results on the Large-scale CelebFaces Attributes ( CelebA ) dataset in the appendix of the paper . Since it takes more time to prepare the results on CIFAR-10 , we will include the results on CIFAR-10 later . The results on CelebA show that AE-GAN+rs is still effective on more complex datasets with large RGB images . For further details , please refer to Appendix G in the revised version . Here we list some results on FGSM ( $ \\epsilon=0.3 $ ) white-box attacks , gray-box attacks as well as black-box attacks . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * No Defense Adv.tr . Defense-GAN Our White-box 0.0416 0.6119 0.8879 0.9364 Gary-box 0.0416 -- - 0.8559 0.8897 Black-box 0.0404 0.6121 0.8517 0.8839 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Q4 . It wasn \u2019 t clear to me how the encoder-assisted search process ( Section 3.4 ) works in practice . When you use gradient descent to find the best encoding for a corrupted input , how many steps do you need to take ? Does this step happen while the auto-encoder and GAN are being trained , or are those pre-trained and you just take additional gradient steps in the latent space ? A4.Please see the above A2 for the details . References : [ 1 ] Goodfellow , I. , Pouget-Abadie , J. , Mirza , M. , Xu , B. , Warde-Farley , D. , Ozair , S. , ... & Bengio , Y . ( 2014 ) .Generative adversarial nets . In Advances in neural information processing systems ( pp.2672-2680 ) ."}, {"review_id": "Hyg53gSYPB-1", "review_text": "The paper combines an auto-encoder (AE) based approach to correct the adversarially perturbed samples with a GAN based approach for the same task. More specifically, the encoder of the AE is used to provide a better initialization for the latent space optimization employed in the Defense-GAN approach. The authors propose a two-stage inference process. First, the autoencoder is used to detect if an input sample is from the natural image distribution or adversarial distribution. In the latter case, Defense-GAN is employed that uses gradient descent in the latent space, with encoder output as the initialization, to find a natural or non-adversarial counterpart of the input sample. Results are reported for MNIST and F-MNIST that show that the proposed method is computationally cheaper than Defence-GAN. Questions / Concerns: - The methods seem to heavily rely on the autoencoder's ability to detect adversarial samples. The detection performance can be still susceptible to white-box attacks. The methods also relies on the capacity of the auto-encoder to learn good representations/reconstructions. It will be useful to show the performance on more complicated datasets was learning a good AE model is more challenging. - The autoencoder is also equipped with an adversarial loss such that the decoder produces realistic images. Is there any assurance that the latent distribution F(x) from the data and the prior p(z) will be similar. It might be useful to explicitly ensure this. The discriminator currently never sees images that re reconstructions of the true data samples. So the decoder behavior could be different for different regions of the input space. - It is not clear why the proposed method performs better on stringer white-box attacks compared to much weaker black-box attacks. ", "rating": "3: Weak Reject", "reply_text": "We thank you for the comments and suggestions , and answer your questions as follows : Q1 . The methods seem to heavily rely on the auto-encoder 's ability to detect adversarial samples . The detection performance can be still susceptible to white-box attacks . A1.To defend an adversarial example , our method can always activate the searching process by AE-GAN+rs , i.e. , use the code determined by the searching process to reconstruct the example , and then feed the reconstruction into the classifier for the final classification result . To be more efficient , we also design a detection mechanism ( in Sec.4.4 ) to avoid the unnecessary searching process for the clean examples or the adversarial examples that are affected by small perturbations . If the detection mechanism is used , the adversarial example may cheat the detection step and attack the classifier by the reconstruction from AE-GAN+r . Moreover , as you mentioned in the above comments , the detection mechanism can be susceptible to white-box attacks , which design adversarial examples by attacking the stacked model , i.e. , concatenating the classifier with AE-GAN+r as a new classifier . In the following , we conduct experiments to demonstrate that the proposed method is still effective against the above mentioned white-box adversarial attacks . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Using MSE as the indicator with threshold=0.01 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * AE-GAN+r AE-GAN+rs AE-GAN+rs ( with detection ) Detection FGSM 3.0 : 0.691 0.8526 0.8565 100 % FGSM 1.0 : 0.8943 0.9721 0.9721 99.62 % Cw : 0.1182 0.7346 0.7272 99.83 % * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Using L1 loss as the indicator with threshold=0.025 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * AE-GAN+r AE-GAN+rs AE-GAN+rs ( with detection ) Detection FGSM 3.0 : 0.691 0.8526 0.8565 100 % FGSM 1.0 : 0.8943 0.9721 0.9723 100 % Cw : 0.1182 0.7346 That is , no searching process is activated for any input . The second column denotes the classification accuracy when using AE-GAN+rs to purify the adversarial examples , i.e. , the searching process is activated for every input . The third column denotes the classification accuracy when incorporating the detection mechanism to determine whether the searching process is activated for the input . Moreover , among the examples that successfully attack the stacked model ( i.e. , concatenating the classifier with AE-GAN+r ) , we calculate the percentage of detected ones , as given in the fourth column . By comparing the first column and the third column , we can see that the detection mechanism is still effective in detecting the adversarial examples generated by the white-box attacks on the stacked model ( i.e. , concatenating the classifier with AE-GAN+r ) . Take the first row for example , when the AE-GAN+r can not diminish the adversarial perturbations for the 1-0.691=30.9 % examples , these 30.9 % examples can be 100 % detected by the detection mechanism ( as shown in the last column ) . Then , they will trigger the searching process and be purified by AE-GAN+rs for higher accuracy . Thus , the final performance of the proposed method ( AE-GAN+rs incorporated with detection ) on the generated adversarial examples achieves 85.65 % , close to AE-GAN+rs ( without detection ) . Therefore , the above white-box attacks did n't affect the performance of the proposed method . By comparing the first column and the second column , we can see that the searching process can achieve a higher accuracy on the generated adversarial examples , which demonstrates the effectiveness of the searching process . When the detection mechanism is incorporated , there is no decline in performance . In other words , when the detection helps to save time , it didn \u2019 t affect the performance , even under the strongest white-box attacks . We have conducted more experiments to investigate the performance of the detection stage in the appendix in our revised version . For more details , please refer to Appendix F in the revised paper ."}, {"review_id": "Hyg53gSYPB-2", "review_text": "The paper aims to refine DefenseGAN (ICLR 18), where an autoencoder is used to initialize the search for projecting an adversarial examples to the manifold of real examples. The main contribution is to reduce the computational cost of DefenseGAN, the claim is \"by an order of magnitude\". Though the idea is good, I found the contribution to be too incremental for the paper to be accepted: * The comparison should include the state of the art adversarial training defenses, PGD Adversarial Training (Madry et al; you might want to cite the ICLR 18 paper) and TRADES (Zhang et al., Interpreting adversarially trained convolutional NN, ICML 2019); * I would consider the Szegedy baseline as obsolete; * Fig. 4 is unclear: the percentage of adversarial examples detected by the detector, but quid of the false alarms; There are quite some typos (nosie; iterarion; tabel; unsafety) and missing words. The paper has been hastily written; and it includes one page more than allowed. ", "rating": "3: Weak Reject", "reply_text": "Thanks for reading our paper thoroughly and the helpful comments . We would like to address your concerns one by one . Q1.The comparison should include the state of the art adversarial training defenses , PGD Adversarial Training ( Madry et al ; you might want to cite the ICLR 18 paper ) and TRADES ( Zhang et al. , Interpreting adversarially trained convolutional NN , ICML 2019 ) A1 . Thanks for providing the information about the two papers . According to your suggestion , we take PGD adversarial training [ 1 ] into comparisons under the same experimental settings . Here we list the comparisons with PGD adversarial training . Due to the time limit , we have not finished the comparisons with TRADES . We will include the comparisons with TRADES later . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * On MNIST * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * None PGD adv.tr Our White-box FGSM : 0.144 0.949 0.984 PGD : 0.007 0.920 0.982 CW : 0.008 0.773 0.979 Black-box : A/B 0.701 0.971 0.935 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * On FashionMNIST * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * None PGD adv.tr Our White-box FGSM : 0.073 0.739 0.804 PGD : 0.028 0.717 0.816 CW : 0.062 0.224 0.767 Black-box : A/B 0.227 0.784 0.603 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * The results in the above tables indicate that our method can consistently provide effective defense on various attacks , while PGD adversarial training can not generalize to CW attacks . However , the performances of defenses on the F-MNIST dataset shows noticeably lower than that on MNIST , this is due to the large $ \\epsilon =0.3 $ in the FGSM attack . The qualitative examples in Appendix H can show that $ \\epsilon=0.3 $ represents large perturbations . Q2.Fig.4 is unclear : the percentage of adversarial examples detected by the detector , but quid of the false alarms . A2.According to your suggestion , we revised Figure 4 by adding Receiver Operating Characteristic ( ROC ) curves as well as the Area Under the Curve ( AUC ) metric . Also , we conduct more experiments on the detection performance and visualize how False Negative Rate ( FNR , the percentage of missed diagnosis ) and False Positive Rate ( FPR , the percentage of false alarms ) changes with the threshold increases . The added visualizations on MSNIT are shown in Figure 3 in the revised version , and the results on FashionMNIST are included in Appendix F. Here we give some examples . For more details , please refer to Figure 3 and Appendix F in our revised version . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Using MSE as the indicator * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * FPR FNR $ \\epsilon=0.1 $ $ \\epsilon=0.3 $ Threshold=0.01 73.37 % 4.34 % 0 % Threshold=0.015 44.01 % 30.18 % 0 % Threshold=0.02 23.25 % 59.81 % 0 % * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Using L1 distance as the indicator , $ \\epsilon=0.1 $ * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * FPR FNR Threshold=0.02 20.15 % 0 % Threshold=0.025 9.65 % 0 % Threshold=0.03 4.35 % 0 % * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Notation : $ \\epsilon $ is the strength of attacks . Q3.There are quite some typos ( nosie ; iterarion ; tabel ; unsafety ) and missing words . The paper has been hastily written ; and it includes one page more than allowed . A3.Thanks for carefully checking our presentation and pointing out our typos . We have carefully revised our paper and improved the quality and readability . To meet the page limit , we tried to revise the paper accordingly but the paper is still a little more than the recommended paper length , but we do not exceed the strict upper limit of the length ( 10 pages ) . References : [ 1 ] Madry , A. , Makelov , A. , Schmidt , L. , Tsipras , D. , & Vladu , A . ( 2017 ) .Towards deep learning models resistant to adversarial attacks . arXiv preprint arXiv:1706.06083 ."}, {"review_id": "Hyg53gSYPB-3", "review_text": "Authors propose to use a modified autoencoder at the input of a network to ward off adversarial samples by purifying them. The encoder-decoder is trained with a discriminator which can identify whether the input is malicious or normal. A malicious input image is purified with the help of a search procedure in the latent space of the auto-encoder. The gradient based search obtains a latent code that corresponds to the reconstructed image that is closest to the input. The algorithm uses output of the decoder as a starting point in the search. Experiments show a reasonable performance against adversarial attacks on MNIST and F-MNIST images. The proposal seems to be an alternative to the Defence-GAN method of (Samangouei et. al. 2018) where search is computationally expensive - due to multiple starting points and many iterations. The ease of optimization in the proposed method comes at the cost of accuracy (Table 1). However, for a single starting point during search and fewer iterations, it outperforms Defense-GAN by a significant margin (Table 2). ", "rating": "6: Weak Accept", "reply_text": "We thank reviewer # 1 very much for their positive and encouraging comments . You are right that the proposed method can be regarded as an alternative to the Defence-GAN method of ( Samangouei et.al.2018 ) which employed a computational expensive search due to multiple starting points and many iterations . Our method is much faster with comparable performance , and it significantly outperforms Defense-GAN for a single starting point during search and fewer iterations . We have also carefully revised our paper and improved the quality of the paper ."}], "0": {"review_id": "Hyg53gSYPB-0", "review_text": "Summary: This paper proposes AE-GAN+sr, an auto-encoder based GAN for equipping neural networks with better defenses against adversarial attacks. The authors evaluate their method on black-box attacks, white-box attacks, and gray-box attacks on MNIST and Fashion-MNIST, and show decent empirical results when compared to baselines. Decision: Reject. The writing was hard to follow and the experimental evaluations could have been stronger. Supporting Arguments/Feedback: - There are a lot of phrases which should be reworded in the text (e.g. \u201cit requires attention to\u201d in paragraph 1, \u201cdetect if sample is from a normal distribution\u201d at the end of Section 1, etc.) for better clarity. The writing was pretty hard to follow and the paper would really benefit overall after it has been improved. - It was hard for me to tell from the text how exactly the models were implemented and trained (see Question #1). Making this point more clear, in either the main text or the Appendix, would be really helpful. - The performance of AE-GAN+sr as evaluated on MNIST and FashionMNIST did not provide convincing evidence that it outperformed baselines such as Defense-GAN or adversarial training, though I did appreciate the authors\u2019 additional analysis on the tradeoffs between computational cost and performance for the Defense-GAN. The results would also have been more compelling had the authors evaluated their method on more complex datasets such as CIFAR-10. Questions: - It wasn\u2019t clear to me how the encoder-assisted search process (Section 3.4) works in practice. When you use gradient descent to find the best encoding for a corrupted input, how many steps do you need to take? Does this step happen while the autoencoder and GAN are being trained, or are those pre-trained and you just take additional gradient steps in the latent space? - Given that the BiGAN also incorporates an encoder (and they were also benchmarked against in the experiments), where do the advantages of the AE-GAN+sr come from? I think making this point in the text would also be helpful as well. ", "rating": "3: Weak Reject", "reply_text": "Thanks for giving us valuable advice . We would like to address your points one by one as follows . Q1.There are a lot of phrases that should be reworded in the text for better clarity . The writing was pretty hard to follow and the paper would really benefit overall after it has been improved . A1.We admit that there are confusing phrases in the original version . We will address your concern by revising the paper thoroughly . Q2.It was hard for me to tell from the text how exactly the models were implemented and trained . Making this point more clear , in either the main text or the Appendix , would be really helpful . A2.Thanks for the suggestion . We would like to provide pseudo-codes for the training and testing stage in Appendix C in our revised version . The testing stage was implemented differently from the training stage . In Sec.3.3 of the paper , the method for training , denoted as AE-GAN+r , does not include the searching process . When defending against adversarial samples in the inference process , the trained AE-GAN+r becomes AE-GAN+rs ( see Sec 3.2 and Sec 3.4 ) by including the searching process which is denoted as { +s } ( see Sec 3.4 ) . More details are given below : ( 1 ) For the training stage , AE-GAN+r has no searching process and is trained with legitimate samples . As a modified auto-encoder , AE-GAN+r adds a discriminator to AE and is equipped with an adversarial loss such that the decoder produces realistic images . AE-GAN+r is trained to optimize the min-max loss in Eq . ( 4 ) in the manuscript . The training strategy is the same as GANs [ 1 ] , where the parameters in Encoder-Decoder and the parameters in discriminator are updated alternatively to minimize \u201c Generative Loss + Reconstruction Loss \u201d and \u201c D_Loss \u201d respectively . ( 2 ) In the inference stage , the AE-GAN+r has been trained and we want to use it to defend against adversarial samples . To get better reconstruction , the searching process is activated , i.e. , we use gradient descent to find the best encoding in the latent space for a corrupted input . This defense strategy is denoted as AE-GAN+rs . In practice , we take 15 steps for GD iterations in the searching process , and the learning rate is set to 0.01 ( multiplied by 0.5 every 5 steps ) . Q3.The performance of AE-GAN+rs as evaluated on MNIST and FashionMNIST did not provide convincing evidence that it outperformed baselines such as Defense-GAN or adversarial training . The results would also have been more compelling had the authors evaluated their method on more complex datasets such as CIFAR-10 . A3.Thanks for your suggestion . Based on the suggestion , we have added additional results on the Large-scale CelebFaces Attributes ( CelebA ) dataset in the appendix of the paper . Since it takes more time to prepare the results on CIFAR-10 , we will include the results on CIFAR-10 later . The results on CelebA show that AE-GAN+rs is still effective on more complex datasets with large RGB images . For further details , please refer to Appendix G in the revised version . Here we list some results on FGSM ( $ \\epsilon=0.3 $ ) white-box attacks , gray-box attacks as well as black-box attacks . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * No Defense Adv.tr . Defense-GAN Our White-box 0.0416 0.6119 0.8879 0.9364 Gary-box 0.0416 -- - 0.8559 0.8897 Black-box 0.0404 0.6121 0.8517 0.8839 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Q4 . It wasn \u2019 t clear to me how the encoder-assisted search process ( Section 3.4 ) works in practice . When you use gradient descent to find the best encoding for a corrupted input , how many steps do you need to take ? Does this step happen while the auto-encoder and GAN are being trained , or are those pre-trained and you just take additional gradient steps in the latent space ? A4.Please see the above A2 for the details . References : [ 1 ] Goodfellow , I. , Pouget-Abadie , J. , Mirza , M. , Xu , B. , Warde-Farley , D. , Ozair , S. , ... & Bengio , Y . ( 2014 ) .Generative adversarial nets . In Advances in neural information processing systems ( pp.2672-2680 ) ."}, "1": {"review_id": "Hyg53gSYPB-1", "review_text": "The paper combines an auto-encoder (AE) based approach to correct the adversarially perturbed samples with a GAN based approach for the same task. More specifically, the encoder of the AE is used to provide a better initialization for the latent space optimization employed in the Defense-GAN approach. The authors propose a two-stage inference process. First, the autoencoder is used to detect if an input sample is from the natural image distribution or adversarial distribution. In the latter case, Defense-GAN is employed that uses gradient descent in the latent space, with encoder output as the initialization, to find a natural or non-adversarial counterpart of the input sample. Results are reported for MNIST and F-MNIST that show that the proposed method is computationally cheaper than Defence-GAN. Questions / Concerns: - The methods seem to heavily rely on the autoencoder's ability to detect adversarial samples. The detection performance can be still susceptible to white-box attacks. The methods also relies on the capacity of the auto-encoder to learn good representations/reconstructions. It will be useful to show the performance on more complicated datasets was learning a good AE model is more challenging. - The autoencoder is also equipped with an adversarial loss such that the decoder produces realistic images. Is there any assurance that the latent distribution F(x) from the data and the prior p(z) will be similar. It might be useful to explicitly ensure this. The discriminator currently never sees images that re reconstructions of the true data samples. So the decoder behavior could be different for different regions of the input space. - It is not clear why the proposed method performs better on stringer white-box attacks compared to much weaker black-box attacks. ", "rating": "3: Weak Reject", "reply_text": "We thank you for the comments and suggestions , and answer your questions as follows : Q1 . The methods seem to heavily rely on the auto-encoder 's ability to detect adversarial samples . The detection performance can be still susceptible to white-box attacks . A1.To defend an adversarial example , our method can always activate the searching process by AE-GAN+rs , i.e. , use the code determined by the searching process to reconstruct the example , and then feed the reconstruction into the classifier for the final classification result . To be more efficient , we also design a detection mechanism ( in Sec.4.4 ) to avoid the unnecessary searching process for the clean examples or the adversarial examples that are affected by small perturbations . If the detection mechanism is used , the adversarial example may cheat the detection step and attack the classifier by the reconstruction from AE-GAN+r . Moreover , as you mentioned in the above comments , the detection mechanism can be susceptible to white-box attacks , which design adversarial examples by attacking the stacked model , i.e. , concatenating the classifier with AE-GAN+r as a new classifier . In the following , we conduct experiments to demonstrate that the proposed method is still effective against the above mentioned white-box adversarial attacks . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Using MSE as the indicator with threshold=0.01 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * AE-GAN+r AE-GAN+rs AE-GAN+rs ( with detection ) Detection FGSM 3.0 : 0.691 0.8526 0.8565 100 % FGSM 1.0 : 0.8943 0.9721 0.9721 99.62 % Cw : 0.1182 0.7346 0.7272 99.83 % * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Using L1 loss as the indicator with threshold=0.025 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * AE-GAN+r AE-GAN+rs AE-GAN+rs ( with detection ) Detection FGSM 3.0 : 0.691 0.8526 0.8565 100 % FGSM 1.0 : 0.8943 0.9721 0.9723 100 % Cw : 0.1182 0.7346 That is , no searching process is activated for any input . The second column denotes the classification accuracy when using AE-GAN+rs to purify the adversarial examples , i.e. , the searching process is activated for every input . The third column denotes the classification accuracy when incorporating the detection mechanism to determine whether the searching process is activated for the input . Moreover , among the examples that successfully attack the stacked model ( i.e. , concatenating the classifier with AE-GAN+r ) , we calculate the percentage of detected ones , as given in the fourth column . By comparing the first column and the third column , we can see that the detection mechanism is still effective in detecting the adversarial examples generated by the white-box attacks on the stacked model ( i.e. , concatenating the classifier with AE-GAN+r ) . Take the first row for example , when the AE-GAN+r can not diminish the adversarial perturbations for the 1-0.691=30.9 % examples , these 30.9 % examples can be 100 % detected by the detection mechanism ( as shown in the last column ) . Then , they will trigger the searching process and be purified by AE-GAN+rs for higher accuracy . Thus , the final performance of the proposed method ( AE-GAN+rs incorporated with detection ) on the generated adversarial examples achieves 85.65 % , close to AE-GAN+rs ( without detection ) . Therefore , the above white-box attacks did n't affect the performance of the proposed method . By comparing the first column and the second column , we can see that the searching process can achieve a higher accuracy on the generated adversarial examples , which demonstrates the effectiveness of the searching process . When the detection mechanism is incorporated , there is no decline in performance . In other words , when the detection helps to save time , it didn \u2019 t affect the performance , even under the strongest white-box attacks . We have conducted more experiments to investigate the performance of the detection stage in the appendix in our revised version . For more details , please refer to Appendix F in the revised paper ."}, "2": {"review_id": "Hyg53gSYPB-2", "review_text": "The paper aims to refine DefenseGAN (ICLR 18), where an autoencoder is used to initialize the search for projecting an adversarial examples to the manifold of real examples. The main contribution is to reduce the computational cost of DefenseGAN, the claim is \"by an order of magnitude\". Though the idea is good, I found the contribution to be too incremental for the paper to be accepted: * The comparison should include the state of the art adversarial training defenses, PGD Adversarial Training (Madry et al; you might want to cite the ICLR 18 paper) and TRADES (Zhang et al., Interpreting adversarially trained convolutional NN, ICML 2019); * I would consider the Szegedy baseline as obsolete; * Fig. 4 is unclear: the percentage of adversarial examples detected by the detector, but quid of the false alarms; There are quite some typos (nosie; iterarion; tabel; unsafety) and missing words. The paper has been hastily written; and it includes one page more than allowed. ", "rating": "3: Weak Reject", "reply_text": "Thanks for reading our paper thoroughly and the helpful comments . We would like to address your concerns one by one . Q1.The comparison should include the state of the art adversarial training defenses , PGD Adversarial Training ( Madry et al ; you might want to cite the ICLR 18 paper ) and TRADES ( Zhang et al. , Interpreting adversarially trained convolutional NN , ICML 2019 ) A1 . Thanks for providing the information about the two papers . According to your suggestion , we take PGD adversarial training [ 1 ] into comparisons under the same experimental settings . Here we list the comparisons with PGD adversarial training . Due to the time limit , we have not finished the comparisons with TRADES . We will include the comparisons with TRADES later . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * On MNIST * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * None PGD adv.tr Our White-box FGSM : 0.144 0.949 0.984 PGD : 0.007 0.920 0.982 CW : 0.008 0.773 0.979 Black-box : A/B 0.701 0.971 0.935 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * On FashionMNIST * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * None PGD adv.tr Our White-box FGSM : 0.073 0.739 0.804 PGD : 0.028 0.717 0.816 CW : 0.062 0.224 0.767 Black-box : A/B 0.227 0.784 0.603 * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * The results in the above tables indicate that our method can consistently provide effective defense on various attacks , while PGD adversarial training can not generalize to CW attacks . However , the performances of defenses on the F-MNIST dataset shows noticeably lower than that on MNIST , this is due to the large $ \\epsilon =0.3 $ in the FGSM attack . The qualitative examples in Appendix H can show that $ \\epsilon=0.3 $ represents large perturbations . Q2.Fig.4 is unclear : the percentage of adversarial examples detected by the detector , but quid of the false alarms . A2.According to your suggestion , we revised Figure 4 by adding Receiver Operating Characteristic ( ROC ) curves as well as the Area Under the Curve ( AUC ) metric . Also , we conduct more experiments on the detection performance and visualize how False Negative Rate ( FNR , the percentage of missed diagnosis ) and False Positive Rate ( FPR , the percentage of false alarms ) changes with the threshold increases . The added visualizations on MSNIT are shown in Figure 3 in the revised version , and the results on FashionMNIST are included in Appendix F. Here we give some examples . For more details , please refer to Figure 3 and Appendix F in our revised version . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Using MSE as the indicator * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * FPR FNR $ \\epsilon=0.1 $ $ \\epsilon=0.3 $ Threshold=0.01 73.37 % 4.34 % 0 % Threshold=0.015 44.01 % 30.18 % 0 % Threshold=0.02 23.25 % 59.81 % 0 % * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Using L1 distance as the indicator , $ \\epsilon=0.1 $ * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * FPR FNR Threshold=0.02 20.15 % 0 % Threshold=0.025 9.65 % 0 % Threshold=0.03 4.35 % 0 % * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Notation : $ \\epsilon $ is the strength of attacks . Q3.There are quite some typos ( nosie ; iterarion ; tabel ; unsafety ) and missing words . The paper has been hastily written ; and it includes one page more than allowed . A3.Thanks for carefully checking our presentation and pointing out our typos . We have carefully revised our paper and improved the quality and readability . To meet the page limit , we tried to revise the paper accordingly but the paper is still a little more than the recommended paper length , but we do not exceed the strict upper limit of the length ( 10 pages ) . References : [ 1 ] Madry , A. , Makelov , A. , Schmidt , L. , Tsipras , D. , & Vladu , A . ( 2017 ) .Towards deep learning models resistant to adversarial attacks . arXiv preprint arXiv:1706.06083 ."}, "3": {"review_id": "Hyg53gSYPB-3", "review_text": "Authors propose to use a modified autoencoder at the input of a network to ward off adversarial samples by purifying them. The encoder-decoder is trained with a discriminator which can identify whether the input is malicious or normal. A malicious input image is purified with the help of a search procedure in the latent space of the auto-encoder. The gradient based search obtains a latent code that corresponds to the reconstructed image that is closest to the input. The algorithm uses output of the decoder as a starting point in the search. Experiments show a reasonable performance against adversarial attacks on MNIST and F-MNIST images. The proposal seems to be an alternative to the Defence-GAN method of (Samangouei et. al. 2018) where search is computationally expensive - due to multiple starting points and many iterations. The ease of optimization in the proposed method comes at the cost of accuracy (Table 1). However, for a single starting point during search and fewer iterations, it outperforms Defense-GAN by a significant margin (Table 2). ", "rating": "6: Weak Accept", "reply_text": "We thank reviewer # 1 very much for their positive and encouraging comments . You are right that the proposed method can be regarded as an alternative to the Defence-GAN method of ( Samangouei et.al.2018 ) which employed a computational expensive search due to multiple starting points and many iterations . Our method is much faster with comparable performance , and it significantly outperforms Defense-GAN for a single starting point during search and fewer iterations . We have also carefully revised our paper and improved the quality of the paper ."}}