{"year": "2021", "forum": "cy0jU8F60Hy", "title": "ACT: Asymptotic Conditional Transport", "decision": "Reject", "meta_review": "The paper proposes a new measure of difference between two distributions using conditional transport. The paper considers an important problem.  However, some major concerns remain after the discussion among the reviewers. In particular, the paper focuses on the evaluation on a toy dataset. It is unclear whether the claim carries over to large real datasets. The presentation of the paper also needs substantial improvement.\n", "reviews": [{"review_id": "cy0jU8F60Hy-0", "review_text": "The paper proposes a new transport-based divergence between distributions ( CT ) and a variant for empirical distributions ( ACT ) . The new divergence is claimed to be more suitable for learning deep generative models than existing divergences like KL , JS ( as in the vanilla GAN ) and Wasserstein ( as used in WGAN and its variants ) . The proposed divergence mostly resembles , in my opinion , the Wasserstein divergence variant that uses the Kantorovich\u2013Rubinstein dual definition ( which requires the learned function to be 1-Lipschitz ) . It seems that the main advantages of ACT over Wasserstein is that there is no constraint on the Lipschitz smoothness ( which has to be enforced in WGAN by means of e.g.gradient clipping or gradient penalty ) , and the fact that ACT provides unbiased gradients that do not require the critic to reach an optimal point ( as required in theory in GAN or WGAN ) . The paper presents a potentially interesting and significant method , however , it was a bit hard to follow , making it difficult for me to asses the actual significance of the contribution . Specifically , the distinction between $ d ( x , y ) $ and $ c ( x , y ) $ was not 100 % clear to me . $ d ( x , y ) $ seems to be part of the `` navigator '' $ \\pi ( x | y ) $ - an energy-based conditional probability and $ c ( x , y ) $ is the point-to-point transport cost . For the CT to be low , it looks like $ c ( x , y ) $ and $ d ( x , y ) $ should be positively correlated . It was not clear , however ( at least from the introduction ) , how $ d $ and $ c $ fit the generator-critic GAN scheme . Are they both part of the critic ? Do the claims that the critic does not have to reach optimality for the gradients to be unbiased refer just to $ c $ or to $ d $ as well ? I believe this is partially explained , but only towards the end of the paper ( eq.16 ) .Perhaps a figure showing the trained elements in ACT compared to the trained elements in WGAN could help clarify the method . Specific questions : - Why are both the forward and backward CT needed ? - Are the parameters of the forward and backward navigators shared ? From eq.16 it looks like they are . Does it make sense ? - In section 2.3 , the authors claim that L2 distance in the original image domain is known not to work well for high-dimensional data that resided on a lower dimensional manifold , but what about L2 on some pre-trained feature-space e.g.perceptual distance [ 1 ] ? - The difference in principle between the proposed ACT and OT-GAN and MMD-GAN should be described . To summarize : pros : - Interesting method , seems to be novel - Potential significance ( as an alternative loss for training deep generative models ) cons : - Writing hard to follow - Some open questions * * update : the authors have answered most of my questions * * [ 1 ] Zhang , Richard , et al . `` The unreasonable effectiveness of deep features as a perceptual metric . '' Proceedings of the IEEE conference on computer vision and pattern recognition . 2018 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your constructive comments and insightful questions . Below please find our point-by-point response . > Q1 : \u201c Why are both the forward and backward CT needed ? \u201d Response : We have added an analysis to compare the forward and backward transport properties to help the readers to understand why ACT can well fit the distribution and not miss modes ( see newly added paragraph in Page 8 and Figs.5 & 8 ) .As a result , we show that the forward conditional transport cost encourages the generator to cover all modes , while the backward conditional transport cost encourages it to seek a mode . To this end , the navigators , although introduced as additional components , do not make the training harder , but help to make the training more stable and converge to better results . If only using forward CT/ACT , we observe mode covering behavior , while if only using backward CT/ACT , we observe mode seeking behavior . Combining both of them strikes a good balance between mode covering and seeking , providing strong resistance to mode collapse . > Q2 : \u201c Superficially , the distinction between d ( x , y ) and c ( x , y ) was not 100 % clear to me . d ( x , y ) seems to be part of the `` navigator '' \\pi ( x | y ) - an energy-based conditional probability and c ( x , y ) is the point-to-point transport cost . For the CT to be low , it looks like c ( x , y ) and d ( x , y ) should be positively correlated. \u201d Response : We \u2019 d like to clarify that they are not necessarily positively correlated when the navigator parameter is different from the critic parameter , i.e. , when $ \\phi \\neq \\eta $ . A good example is shown in Figure 14 of Appendix B.3 , where we show the generated data and corresponding logits . In this 2D toy data case , $ c ( x , y ) $ is the Euclidean distance between point $ x $ and $ y $ ; while $ d ( x , y ) $ is the Euclidean distance between $ T_\\phi ( x ) $ and $ T_\\phi ( y ) $ . Let \u2019 s use the third row ( 8-Gaussian mixture ) as an example , in the second and third panels , we use different colors to indicate the correspondence between each point $ x $ and its $ T_\\phi ( x ) $ . For the points from the two modes that are marked in pink , we can observe in the data space , they have the largest distance , while in the navigator space , they are close to each other . The navigator space can be viewed as a 45-degree zero-crossing diagonal line in the figure , and hence the distances between the points in the navigator space are determined by their projections to this diagonal line , which also justifies that $ c ( x , y ) $ and $ d ( x , y ) $ are not necessarily positively correlated . > Q3 : \u201c Are the parameters of the forward and backward navigators shared ? From eq.16 it looks like they are . Does it make sense ? \u201d Response : Yes , as defined in Eq.2 and Eq.3 , they are shared . While in the proposed construction , both conditional transport plans are defined as conditional distributions parameterized by the same $ \\phi $ , this is not mandatory as long as the conditional distributions are valid and can be optimized . There could exist various alternative ways . For example , one may use one neural network for the forward navigator and another different one for the backward , which could help further increase flexibility and hence the performance , but at the expense of doubling the navigator-related memory and computation cost . We will further investigate this in our future work . > Q4 : \u201c In section 2.3 , the authors claim that L2 distance in the original image domain is known not to work well for high-dimensional data that resided on a lower-dimensional manifold , but what about L2 on some pre-trained feature-space e.g.perceptual distance. \u201d Response : We have added a paragraph in Pages 22-23 and Fig.18 to help answer this question . To summarize , our experiments do not show promising results for high-dimensional natural images , when fixing the pre-trained feature extractor and letting it play the role of the critic in ACT . Specifically , we apply the pre-trained feature extractor from Zhang et al . ( 2018 ) to define the critic of the ACT , but the results are not promising ( Fig.18 ) unless we allow the feature-extract network to be fine-tuned under the ACT loss shown in Eq.16.We have also tested with other generative models such as energy distance , and none of them show good results either when the pre-trained feature extractor is used to define the critic and fixed during the training ."}, {"review_id": "cy0jU8F60Hy-1", "review_text": "Summary : The paper introduces an asymptotic conditional transport divergence to measure the discrepancy between two probability distributions . The new divergence leads to a new adversarial game of generative adversarial networks , which aims at minimizing a distribution-to-distribution transport cost by optimizing the generator distribution and the conditional transport-path distributions of navigators . Strength : Introducing the asymptotic conditional transport divergence to generative modeling looks novel and interesting to me . Both the motivation and the technical details seem sound . The evaluation shows the effectiveness of the proposed method on both toy datasets and some popular image generation datasets . Weakness : Fig.3 has shown the superiority of the proposed divergence against the existing divergence ( JS , Wasserstein distance , and Sliced Wasserstein distance ) to fit the toy data distribution . But readers might doubt whether the comparison is fair or not . Do they use the same network backbone ( e.g. , DCGAN , WGAN-GP ) ? As there are many improved variants of SWD like ( Deshpande et al. , 2018 , Deshpande et al. , 2019 , Wu et al. , 2019 ) , I am also wondering which SWD is used . The problem might get more serious for general cases . To address the problem , the paper is suggested to additionally present a clear theoretical study on the comparison , as readers really want to know why the proposed divergence works better than others , while some explanations have been presented in the introduction part . For instance , it is not clear to me why the sample estimate of the ACT divergence and its gradient stay unbiased . As the new divergence requests two additional navigators for bidirectional distribution-to-distribution transport , the adversarial game gets more complex in theory . It is known that GAN models generally suffer from unstable training and mode collapse issues . I am afraid the higher complexity might make such issues more serious . While Fig.1 , Fig.2 somehow shows the behaviors of the navigators , I suggest to further study whether the proposed method can overcome or avoid such issues .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your valuable comments and suggestions . Below please find our point-by-point response . > Q1 : \u201c Fig.3 has shown the superiority of the proposed divergence against the existing divergence ( JS , Wasserstein distance , and Sliced Wasserstein distance ) to fit the toy data distribution . But readers might doubt whether the comparison is fair or not . Do they use the same network backbone ( e.g. , DCGAN , WGAN-GP ) ? \u201d \u201c Which SWD is used ? \u201d Response : Yes , to ensure a fair comparison , on all toy data , we used the same architecture shown in Table 3 of Appendix B.6 . Specifically , we applied the architecture in Table 3 ( a ) for the generator of all the compared models , and applied the architecture in Table 3 ( b ) for the discriminator of both GAN and WGAN-GP as well as the navigator of ACT , which means these models only differ in the training loss . As described in the \u201c ACT for 2D toy data \u201d paragraph in Page 7 , the SWD we used for toy data is from Deshpande et al. , 2018 . In Table 1 , we included several additional SWD variants for comparison . > Q2 : \u201c The paper is suggested to additionally present a clear theoretical study on the comparison , as readers really want to know why the proposed divergence works better than others. \u201d We have added theoretical analysis and corresponding experiment results ( see Fig.3 in the revised paper ) to show the advantage of ACT over the Wasserstein distance on mini-batch SGD based optimization . From our experiments , we can observe ACT consistently show good fitting results , which indicates ACT does not suffer the bias issue caused by the usage of mini-batches as the sample Wasserstein distance does . Theoretically , 1 ) if the objective is $ \\mathcal W ( \\mu , \\nu ) $ , then the sample Wasserstein defined as $ \\mathcal W ( \\hat\\mu_N , \\hat\\nu_M ) $ is a biased estimator and its gradient is certainly biased . 2 ) If the objective is the expected sample Wasserstein defined as $ E_ { \\hat\\mu_N , \\hat\\nu_M } [ \\mathcal W ( \\hat\\mu_N , \\hat\\nu_M ) ] $ , then the sample Wasserstein $ \\mathcal W ( \\hat\\mu_N , \\hat\\nu_M ) $ is an unbiased estimator , computing which , however , requires solving a separate combinatorial problem for each mini-batch . 3 ) If the objective is $ \\mathcal W ( \\mu , \\nu ) $ and we estimate it using the dual form , then the gradient is unbiased only if the critic has reached its optimum . By contrast , our objective is the ACT divergence . As shown in Lemma 4 , Eq ( 13 ) is an unbiased estimator and its gradient is unbiased . The usage of the navigators , whose parameter is optimized with SGD across mini-batches , amortizes the computation of the transport plans on each mini-batch . > Q3 : \u201c It is known that GAN models generally suffer from unstable training and mode collapse issues . I am afraid the higher complexity might make such issues more serious . While Fig.1 , Fig.2 somehow shows the behaviors of the navigators , I suggest to further study whether the proposed method can overcome or avoid such issues. \u201d We have followed your suggestion to conduct additional experiments to study whether ACT can effectively overcome these well-known issues of GANs . In Fig.4 of the revised paper , we have added two experiments to show how ACT resists the mode collapse problem . We have also added an analysis of the amortization property compared to optimal transport ( please refer to our response to Q1 of Reviewer3 ) . Moreover , we have provided more analysis on the distinction between the forward and backward transport ( please refer to our response to Q1 of Reviewer4 ) to further justify how ACT resists mode collapse issues ."}, {"review_id": "cy0jU8F60Hy-2", "review_text": "The paper proposes conditional transport as a new divergence to measure the difference between two distributions . The idea is to learn the conditional transport plan of transporting one point in one distribution to the other marginal distribution . This conditional transport plan is modeled using a neural network . The resulting model is then applied to optimal transport formulation . Experiments are shown on image-based generative modeling dataset . The main idea is to decompose the joint transportation plan $ \\pi ( x , y ) $ using conditionals as $ p ( x ) \\pi ( y|x ) $ and $ p ( y ) \\pi ( x|y ) $ . The conditional transportation plan $ \\pi ( x|y ) $ and $ \\pi ( y|x ) $ are then modeled using neural contrastive losses , with the idea being similar points in two distributions are mapped closer . This decomposition is then applied in optimal transport formulation , leading to a new objective for optimizing OT . The authors also derive an empirical version of this objective , that makes it amenable for stochastic mini-batch optimization . While the idea of this decomposition is interesting , I see the following issues with the formulation . ( 1 ) From the forms of conditional transportation plans $ \\pi ( x|y ) $ and $ \\pi ( y|x ) $ in Eq 2 and 3 , $ p ( x ) \\pi ( y|x ) \\neq p ( y ) \\pi ( x|y ) $ . It would be good to have models that satisfy this equality . ( 2 ) I am not sure if the contrastive model assumed in Eq 2 and 3 is expressive enough to model the entire space of marginal constraints in OT . That is , I don \u2019 t think $ p ( x ) \\pi ( y|x ) $ will cover $ \\Pi ( \\mu , \\nu ) $ . Lemma 1 just shows that under some conditions , $ p ( x ) \\pi ( y|x ) $ lies inside $ \\Pi ( \\mu , \\nu ) $ , but it would be interesting to see if then entire space of $ \\Pi ( \\mu , \\nu ) $ is covered by the neural contrastive model . ( 3 ) When critic is used in ground cost function $ c ( x , y ) $ , the resulting optimization ( Eq 16 ) has one additional network compared to standard GANs . Also , the adversarial game still exists . So , it looks like this optimization is more harder ( or at least equally harder ) compared to standard GANs . Is this true ? Do you see any optimization benefits compared to standard GANs ? ( 4 ) The results of image-based generative modeling is not that impressive . On CIFAR and LSUN datasets , the performance is similar / slightly better than the compared GAN models . Also , many SOTA GAN models are not compared . So , it is very hard to say if the proposed model advances SOTA results . ( 5 ) I would have liked to see more interesting results . The formulation of authors gives an estimate of transportation plan $ \\pi ( x , y ) $ in addition to the generative model itself , which is not possible to estimate in dual-based OT GAN optimization . The transportation plan can be used in interesting applications . One possibility is to estimate likelihoods and possibly use in OOD detection . Take a look at Balaji et al. , \u201c Entropic GANs meet VAEs : A statistical approach to compute sample likelihoods in GANs \u201d for this . This is just one idea . Other interesting applications could have been demonstrated as well . Overall , while the idea is interesting , more results could have positioned the paper better . Just selling it as a paper that improves image-based generative modeling is not that great in my opinion . I would encourage authors to think more interesting experiments .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear AnonReviewer3 , we \u2019 d like to first provide a response to your first two concerns , clarifying which is important to help understand how the proposed conditional transport ( CT ) differs from optimal transport . Concern ( 1 ) : From the forms of conditional transportation plans $ \\pi ( x|y ) $ and and $ \\pi ( y|x ) $ in Eq 2 and 3 , $ p ( x ) \\pi ( y|x ) \\neq p ( y ) \\pi ( x|y ) $ . It would be good to have models that satisfy this equality . Our response : It is actually our intention to not imposing this equality . An important feature of the proposed CT is to allow $ p ( x ) \\pi ( y|x ) \\neq p ( y ) \\pi ( x|y ) $ , which makes it differ from optimal transport in imposing less constraint on the joint distribution of $ x $ and $ y $ . To be more specific , denoting $ \\pi ( x , y ) =p ( x ) \\pi ( y|x ) $ , if $ p ( x ) \\pi ( y|x ) = p ( y ) \\pi ( x|y ) $ is enforced , then $ $ \\int \\pi ( x , y ) dy = \\int p ( x ) \\pi ( y|x ) dy = p ( x ) \\int \\pi ( y|x ) dy=p ( x ) $ $ $ $ \\int \\pi ( x , y ) dx = \\int p ( y ) \\pi ( x|y ) dx =p ( y ) \\int \\pi ( x|y ) dx = p ( y ) , $ $ which means that the joint probability measure $ \\pi \\in \\Pi ( \\mu , \\nu ) $ . Thus we will have $ $ E_ { x\\sim p ( x ) } E_ { y\\sim \\pi ( y|x ) } [ c ( x , y ) ] = E_ { y\\sim p ( y ) } E_ { x\\sim \\pi ( x|y ) } [ c ( x , y ) ] =E_ { ( x , y ) \\sim \\pi ( x , y ) } [ c ( x , y ) ] $ $ and hence finding the best $ \\pi ( y|x ) $ ( or $ \\pi ( x|y ) $ ) that minimizes the above expectation , subject to the constraint $ p ( x ) \\pi ( y|x ) = p ( y ) \\pi ( x|y ) $ , is the same as the optimal transport problem that finds the best $ \\pi ( x , y ) $ to minimize the above expectation subject to the constraint $ \\pi \\in \\Pi ( \\mu , \\nu ) $ . In the proposed CT , we do not impose $ p ( x ) \\pi ( y|x ) = p ( y ) \\pi ( x|y ) $ . The cost of the forward CT is defined as $ E_ { x\\sim p ( x ) } E_ { y\\sim \\pi ( y|x ) } [ c ( x , y ) ] $ , where the joint distribution of $ x $ and $ y $ , denoted as $ \\pi_ { forward } ( x , y ) =p ( x ) \\pi ( y|x ) $ , satisfies $ \\int \\pi_ { forward } ( x , y ) dy=p ( x ) $ but is not constrained to satisfy $ \\int \\pi_ { forward } ( x , y ) dx=p ( y ) $ . Similarly , the joint distribution in the backward CT , denoted as $ \\pi_ { backward } ( x , y ) =p ( y ) \\pi ( x|y ) $ , satisfies $ \\int \\pi_ { backward } ( x , y ) dx=p ( y ) $ but is not constrained to satisfy $ \\int \\pi_ { forward } ( x , y ) dy=p ( x ) $ . Therefore , in theory we have $ $ \\min_ { \\pi ( y|x ) } E_ { ( x , y ) \\sim\\pi_ { forward } ( x , y ) } [ c ( x , y ) ] \\le \\min_ { \\pi\\in \\Pi ( \\mu , \\nu ) } E_ { ( x , y ) \\sim\\pi ( x , y ) } [ c ( x , y ) ] $ $ and $ $ \\min_ { \\pi ( x|y ) } E_ { ( x , y ) \\sim\\pi_ { backward } ( x , y ) } [ c ( x , y ) ] \\le \\min_ { \\pi\\in \\Pi ( \\mu , \\nu ) } E_ { ( x , y ) \\sim\\pi ( x , y ) } [ c ( x , y ) ] $ $ which means the smallest possible cost of the ( forward/backward ) CT is smaller than or equal to the Wasserstein distance . Concern ( 2 ) : I am not sure if the contrastive model assumed in Eq 2 and 3 is expressive enough to model the entire space of marginal constraints in OT . That is , I don \u2019 t think $ p ( x ) \\pi ( y|x ) $ will cover $ \\Pi ( \\mu , \\nu ) $ . Lemma 1 just shows that under some conditions , $ p ( x ) \\pi ( y|x ) $ lies inside $ \\Pi ( \\mu , \\nu ) $ , but it would be interesting to see if then entire space of $ \\Pi ( \\mu , \\nu ) $ is covered by the neural contrastive model . Our response : Related to our response to Concern ( 1 ) , for $ \\pi_ { forward } ( x , y ) =p ( x ) \\pi ( y|x ) $ , we have $ \\int\\pi_ { forward } ( x , y ) dy=p ( x ) $ but allow either $ \\int\\pi_ { forward } ( x , y ) dx = p ( y ) $ or $ \\int\\pi_ { forward } ( x , y ) dx \\neq p ( y ) $ , whereas for $ \\pi\\in\\Pi ( \\mu , \\nu ) $ , we have both $ \\int\\pi ( x , y ) dy=p ( x ) $ and $ \\int \\pi ( x , y ) dx= p ( y ) $ . Therefore , the set of all possible joint probability measures , which $ \\pi_ { forward } $ belongs to , covers $ \\Pi ( \\mu , \\nu ) $ , in other words , the entire space of $ \\Pi ( \\mu , \\nu ) $ is covered by the space of the joint probability measure of the forward CT. A similar conclusion can also be made for $ \\pi_ { backward } ( x , y ) =p ( y ) \\pi ( x|y ) $ . We note Lemma 1 adds these conditions , including that $ p ( x ) $ is equal to $ p ( y ) $ in distribution ( not required by a regular $ \\Pi ( \\mu , \\nu ) $ ) , to make $ \\int\\pi_ { forward } ( x , y ) dx = p ( y ) $ and $ \\int\\pi_ { backward } ( x , y ) dx = p ( x ) $ ."}], "0": {"review_id": "cy0jU8F60Hy-0", "review_text": "The paper proposes a new transport-based divergence between distributions ( CT ) and a variant for empirical distributions ( ACT ) . The new divergence is claimed to be more suitable for learning deep generative models than existing divergences like KL , JS ( as in the vanilla GAN ) and Wasserstein ( as used in WGAN and its variants ) . The proposed divergence mostly resembles , in my opinion , the Wasserstein divergence variant that uses the Kantorovich\u2013Rubinstein dual definition ( which requires the learned function to be 1-Lipschitz ) . It seems that the main advantages of ACT over Wasserstein is that there is no constraint on the Lipschitz smoothness ( which has to be enforced in WGAN by means of e.g.gradient clipping or gradient penalty ) , and the fact that ACT provides unbiased gradients that do not require the critic to reach an optimal point ( as required in theory in GAN or WGAN ) . The paper presents a potentially interesting and significant method , however , it was a bit hard to follow , making it difficult for me to asses the actual significance of the contribution . Specifically , the distinction between $ d ( x , y ) $ and $ c ( x , y ) $ was not 100 % clear to me . $ d ( x , y ) $ seems to be part of the `` navigator '' $ \\pi ( x | y ) $ - an energy-based conditional probability and $ c ( x , y ) $ is the point-to-point transport cost . For the CT to be low , it looks like $ c ( x , y ) $ and $ d ( x , y ) $ should be positively correlated . It was not clear , however ( at least from the introduction ) , how $ d $ and $ c $ fit the generator-critic GAN scheme . Are they both part of the critic ? Do the claims that the critic does not have to reach optimality for the gradients to be unbiased refer just to $ c $ or to $ d $ as well ? I believe this is partially explained , but only towards the end of the paper ( eq.16 ) .Perhaps a figure showing the trained elements in ACT compared to the trained elements in WGAN could help clarify the method . Specific questions : - Why are both the forward and backward CT needed ? - Are the parameters of the forward and backward navigators shared ? From eq.16 it looks like they are . Does it make sense ? - In section 2.3 , the authors claim that L2 distance in the original image domain is known not to work well for high-dimensional data that resided on a lower dimensional manifold , but what about L2 on some pre-trained feature-space e.g.perceptual distance [ 1 ] ? - The difference in principle between the proposed ACT and OT-GAN and MMD-GAN should be described . To summarize : pros : - Interesting method , seems to be novel - Potential significance ( as an alternative loss for training deep generative models ) cons : - Writing hard to follow - Some open questions * * update : the authors have answered most of my questions * * [ 1 ] Zhang , Richard , et al . `` The unreasonable effectiveness of deep features as a perceptual metric . '' Proceedings of the IEEE conference on computer vision and pattern recognition . 2018 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your constructive comments and insightful questions . Below please find our point-by-point response . > Q1 : \u201c Why are both the forward and backward CT needed ? \u201d Response : We have added an analysis to compare the forward and backward transport properties to help the readers to understand why ACT can well fit the distribution and not miss modes ( see newly added paragraph in Page 8 and Figs.5 & 8 ) .As a result , we show that the forward conditional transport cost encourages the generator to cover all modes , while the backward conditional transport cost encourages it to seek a mode . To this end , the navigators , although introduced as additional components , do not make the training harder , but help to make the training more stable and converge to better results . If only using forward CT/ACT , we observe mode covering behavior , while if only using backward CT/ACT , we observe mode seeking behavior . Combining both of them strikes a good balance between mode covering and seeking , providing strong resistance to mode collapse . > Q2 : \u201c Superficially , the distinction between d ( x , y ) and c ( x , y ) was not 100 % clear to me . d ( x , y ) seems to be part of the `` navigator '' \\pi ( x | y ) - an energy-based conditional probability and c ( x , y ) is the point-to-point transport cost . For the CT to be low , it looks like c ( x , y ) and d ( x , y ) should be positively correlated. \u201d Response : We \u2019 d like to clarify that they are not necessarily positively correlated when the navigator parameter is different from the critic parameter , i.e. , when $ \\phi \\neq \\eta $ . A good example is shown in Figure 14 of Appendix B.3 , where we show the generated data and corresponding logits . In this 2D toy data case , $ c ( x , y ) $ is the Euclidean distance between point $ x $ and $ y $ ; while $ d ( x , y ) $ is the Euclidean distance between $ T_\\phi ( x ) $ and $ T_\\phi ( y ) $ . Let \u2019 s use the third row ( 8-Gaussian mixture ) as an example , in the second and third panels , we use different colors to indicate the correspondence between each point $ x $ and its $ T_\\phi ( x ) $ . For the points from the two modes that are marked in pink , we can observe in the data space , they have the largest distance , while in the navigator space , they are close to each other . The navigator space can be viewed as a 45-degree zero-crossing diagonal line in the figure , and hence the distances between the points in the navigator space are determined by their projections to this diagonal line , which also justifies that $ c ( x , y ) $ and $ d ( x , y ) $ are not necessarily positively correlated . > Q3 : \u201c Are the parameters of the forward and backward navigators shared ? From eq.16 it looks like they are . Does it make sense ? \u201d Response : Yes , as defined in Eq.2 and Eq.3 , they are shared . While in the proposed construction , both conditional transport plans are defined as conditional distributions parameterized by the same $ \\phi $ , this is not mandatory as long as the conditional distributions are valid and can be optimized . There could exist various alternative ways . For example , one may use one neural network for the forward navigator and another different one for the backward , which could help further increase flexibility and hence the performance , but at the expense of doubling the navigator-related memory and computation cost . We will further investigate this in our future work . > Q4 : \u201c In section 2.3 , the authors claim that L2 distance in the original image domain is known not to work well for high-dimensional data that resided on a lower-dimensional manifold , but what about L2 on some pre-trained feature-space e.g.perceptual distance. \u201d Response : We have added a paragraph in Pages 22-23 and Fig.18 to help answer this question . To summarize , our experiments do not show promising results for high-dimensional natural images , when fixing the pre-trained feature extractor and letting it play the role of the critic in ACT . Specifically , we apply the pre-trained feature extractor from Zhang et al . ( 2018 ) to define the critic of the ACT , but the results are not promising ( Fig.18 ) unless we allow the feature-extract network to be fine-tuned under the ACT loss shown in Eq.16.We have also tested with other generative models such as energy distance , and none of them show good results either when the pre-trained feature extractor is used to define the critic and fixed during the training ."}, "1": {"review_id": "cy0jU8F60Hy-1", "review_text": "Summary : The paper introduces an asymptotic conditional transport divergence to measure the discrepancy between two probability distributions . The new divergence leads to a new adversarial game of generative adversarial networks , which aims at minimizing a distribution-to-distribution transport cost by optimizing the generator distribution and the conditional transport-path distributions of navigators . Strength : Introducing the asymptotic conditional transport divergence to generative modeling looks novel and interesting to me . Both the motivation and the technical details seem sound . The evaluation shows the effectiveness of the proposed method on both toy datasets and some popular image generation datasets . Weakness : Fig.3 has shown the superiority of the proposed divergence against the existing divergence ( JS , Wasserstein distance , and Sliced Wasserstein distance ) to fit the toy data distribution . But readers might doubt whether the comparison is fair or not . Do they use the same network backbone ( e.g. , DCGAN , WGAN-GP ) ? As there are many improved variants of SWD like ( Deshpande et al. , 2018 , Deshpande et al. , 2019 , Wu et al. , 2019 ) , I am also wondering which SWD is used . The problem might get more serious for general cases . To address the problem , the paper is suggested to additionally present a clear theoretical study on the comparison , as readers really want to know why the proposed divergence works better than others , while some explanations have been presented in the introduction part . For instance , it is not clear to me why the sample estimate of the ACT divergence and its gradient stay unbiased . As the new divergence requests two additional navigators for bidirectional distribution-to-distribution transport , the adversarial game gets more complex in theory . It is known that GAN models generally suffer from unstable training and mode collapse issues . I am afraid the higher complexity might make such issues more serious . While Fig.1 , Fig.2 somehow shows the behaviors of the navigators , I suggest to further study whether the proposed method can overcome or avoid such issues .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your valuable comments and suggestions . Below please find our point-by-point response . > Q1 : \u201c Fig.3 has shown the superiority of the proposed divergence against the existing divergence ( JS , Wasserstein distance , and Sliced Wasserstein distance ) to fit the toy data distribution . But readers might doubt whether the comparison is fair or not . Do they use the same network backbone ( e.g. , DCGAN , WGAN-GP ) ? \u201d \u201c Which SWD is used ? \u201d Response : Yes , to ensure a fair comparison , on all toy data , we used the same architecture shown in Table 3 of Appendix B.6 . Specifically , we applied the architecture in Table 3 ( a ) for the generator of all the compared models , and applied the architecture in Table 3 ( b ) for the discriminator of both GAN and WGAN-GP as well as the navigator of ACT , which means these models only differ in the training loss . As described in the \u201c ACT for 2D toy data \u201d paragraph in Page 7 , the SWD we used for toy data is from Deshpande et al. , 2018 . In Table 1 , we included several additional SWD variants for comparison . > Q2 : \u201c The paper is suggested to additionally present a clear theoretical study on the comparison , as readers really want to know why the proposed divergence works better than others. \u201d We have added theoretical analysis and corresponding experiment results ( see Fig.3 in the revised paper ) to show the advantage of ACT over the Wasserstein distance on mini-batch SGD based optimization . From our experiments , we can observe ACT consistently show good fitting results , which indicates ACT does not suffer the bias issue caused by the usage of mini-batches as the sample Wasserstein distance does . Theoretically , 1 ) if the objective is $ \\mathcal W ( \\mu , \\nu ) $ , then the sample Wasserstein defined as $ \\mathcal W ( \\hat\\mu_N , \\hat\\nu_M ) $ is a biased estimator and its gradient is certainly biased . 2 ) If the objective is the expected sample Wasserstein defined as $ E_ { \\hat\\mu_N , \\hat\\nu_M } [ \\mathcal W ( \\hat\\mu_N , \\hat\\nu_M ) ] $ , then the sample Wasserstein $ \\mathcal W ( \\hat\\mu_N , \\hat\\nu_M ) $ is an unbiased estimator , computing which , however , requires solving a separate combinatorial problem for each mini-batch . 3 ) If the objective is $ \\mathcal W ( \\mu , \\nu ) $ and we estimate it using the dual form , then the gradient is unbiased only if the critic has reached its optimum . By contrast , our objective is the ACT divergence . As shown in Lemma 4 , Eq ( 13 ) is an unbiased estimator and its gradient is unbiased . The usage of the navigators , whose parameter is optimized with SGD across mini-batches , amortizes the computation of the transport plans on each mini-batch . > Q3 : \u201c It is known that GAN models generally suffer from unstable training and mode collapse issues . I am afraid the higher complexity might make such issues more serious . While Fig.1 , Fig.2 somehow shows the behaviors of the navigators , I suggest to further study whether the proposed method can overcome or avoid such issues. \u201d We have followed your suggestion to conduct additional experiments to study whether ACT can effectively overcome these well-known issues of GANs . In Fig.4 of the revised paper , we have added two experiments to show how ACT resists the mode collapse problem . We have also added an analysis of the amortization property compared to optimal transport ( please refer to our response to Q1 of Reviewer3 ) . Moreover , we have provided more analysis on the distinction between the forward and backward transport ( please refer to our response to Q1 of Reviewer4 ) to further justify how ACT resists mode collapse issues ."}, "2": {"review_id": "cy0jU8F60Hy-2", "review_text": "The paper proposes conditional transport as a new divergence to measure the difference between two distributions . The idea is to learn the conditional transport plan of transporting one point in one distribution to the other marginal distribution . This conditional transport plan is modeled using a neural network . The resulting model is then applied to optimal transport formulation . Experiments are shown on image-based generative modeling dataset . The main idea is to decompose the joint transportation plan $ \\pi ( x , y ) $ using conditionals as $ p ( x ) \\pi ( y|x ) $ and $ p ( y ) \\pi ( x|y ) $ . The conditional transportation plan $ \\pi ( x|y ) $ and $ \\pi ( y|x ) $ are then modeled using neural contrastive losses , with the idea being similar points in two distributions are mapped closer . This decomposition is then applied in optimal transport formulation , leading to a new objective for optimizing OT . The authors also derive an empirical version of this objective , that makes it amenable for stochastic mini-batch optimization . While the idea of this decomposition is interesting , I see the following issues with the formulation . ( 1 ) From the forms of conditional transportation plans $ \\pi ( x|y ) $ and $ \\pi ( y|x ) $ in Eq 2 and 3 , $ p ( x ) \\pi ( y|x ) \\neq p ( y ) \\pi ( x|y ) $ . It would be good to have models that satisfy this equality . ( 2 ) I am not sure if the contrastive model assumed in Eq 2 and 3 is expressive enough to model the entire space of marginal constraints in OT . That is , I don \u2019 t think $ p ( x ) \\pi ( y|x ) $ will cover $ \\Pi ( \\mu , \\nu ) $ . Lemma 1 just shows that under some conditions , $ p ( x ) \\pi ( y|x ) $ lies inside $ \\Pi ( \\mu , \\nu ) $ , but it would be interesting to see if then entire space of $ \\Pi ( \\mu , \\nu ) $ is covered by the neural contrastive model . ( 3 ) When critic is used in ground cost function $ c ( x , y ) $ , the resulting optimization ( Eq 16 ) has one additional network compared to standard GANs . Also , the adversarial game still exists . So , it looks like this optimization is more harder ( or at least equally harder ) compared to standard GANs . Is this true ? Do you see any optimization benefits compared to standard GANs ? ( 4 ) The results of image-based generative modeling is not that impressive . On CIFAR and LSUN datasets , the performance is similar / slightly better than the compared GAN models . Also , many SOTA GAN models are not compared . So , it is very hard to say if the proposed model advances SOTA results . ( 5 ) I would have liked to see more interesting results . The formulation of authors gives an estimate of transportation plan $ \\pi ( x , y ) $ in addition to the generative model itself , which is not possible to estimate in dual-based OT GAN optimization . The transportation plan can be used in interesting applications . One possibility is to estimate likelihoods and possibly use in OOD detection . Take a look at Balaji et al. , \u201c Entropic GANs meet VAEs : A statistical approach to compute sample likelihoods in GANs \u201d for this . This is just one idea . Other interesting applications could have been demonstrated as well . Overall , while the idea is interesting , more results could have positioned the paper better . Just selling it as a paper that improves image-based generative modeling is not that great in my opinion . I would encourage authors to think more interesting experiments .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear AnonReviewer3 , we \u2019 d like to first provide a response to your first two concerns , clarifying which is important to help understand how the proposed conditional transport ( CT ) differs from optimal transport . Concern ( 1 ) : From the forms of conditional transportation plans $ \\pi ( x|y ) $ and and $ \\pi ( y|x ) $ in Eq 2 and 3 , $ p ( x ) \\pi ( y|x ) \\neq p ( y ) \\pi ( x|y ) $ . It would be good to have models that satisfy this equality . Our response : It is actually our intention to not imposing this equality . An important feature of the proposed CT is to allow $ p ( x ) \\pi ( y|x ) \\neq p ( y ) \\pi ( x|y ) $ , which makes it differ from optimal transport in imposing less constraint on the joint distribution of $ x $ and $ y $ . To be more specific , denoting $ \\pi ( x , y ) =p ( x ) \\pi ( y|x ) $ , if $ p ( x ) \\pi ( y|x ) = p ( y ) \\pi ( x|y ) $ is enforced , then $ $ \\int \\pi ( x , y ) dy = \\int p ( x ) \\pi ( y|x ) dy = p ( x ) \\int \\pi ( y|x ) dy=p ( x ) $ $ $ $ \\int \\pi ( x , y ) dx = \\int p ( y ) \\pi ( x|y ) dx =p ( y ) \\int \\pi ( x|y ) dx = p ( y ) , $ $ which means that the joint probability measure $ \\pi \\in \\Pi ( \\mu , \\nu ) $ . Thus we will have $ $ E_ { x\\sim p ( x ) } E_ { y\\sim \\pi ( y|x ) } [ c ( x , y ) ] = E_ { y\\sim p ( y ) } E_ { x\\sim \\pi ( x|y ) } [ c ( x , y ) ] =E_ { ( x , y ) \\sim \\pi ( x , y ) } [ c ( x , y ) ] $ $ and hence finding the best $ \\pi ( y|x ) $ ( or $ \\pi ( x|y ) $ ) that minimizes the above expectation , subject to the constraint $ p ( x ) \\pi ( y|x ) = p ( y ) \\pi ( x|y ) $ , is the same as the optimal transport problem that finds the best $ \\pi ( x , y ) $ to minimize the above expectation subject to the constraint $ \\pi \\in \\Pi ( \\mu , \\nu ) $ . In the proposed CT , we do not impose $ p ( x ) \\pi ( y|x ) = p ( y ) \\pi ( x|y ) $ . The cost of the forward CT is defined as $ E_ { x\\sim p ( x ) } E_ { y\\sim \\pi ( y|x ) } [ c ( x , y ) ] $ , where the joint distribution of $ x $ and $ y $ , denoted as $ \\pi_ { forward } ( x , y ) =p ( x ) \\pi ( y|x ) $ , satisfies $ \\int \\pi_ { forward } ( x , y ) dy=p ( x ) $ but is not constrained to satisfy $ \\int \\pi_ { forward } ( x , y ) dx=p ( y ) $ . Similarly , the joint distribution in the backward CT , denoted as $ \\pi_ { backward } ( x , y ) =p ( y ) \\pi ( x|y ) $ , satisfies $ \\int \\pi_ { backward } ( x , y ) dx=p ( y ) $ but is not constrained to satisfy $ \\int \\pi_ { forward } ( x , y ) dy=p ( x ) $ . Therefore , in theory we have $ $ \\min_ { \\pi ( y|x ) } E_ { ( x , y ) \\sim\\pi_ { forward } ( x , y ) } [ c ( x , y ) ] \\le \\min_ { \\pi\\in \\Pi ( \\mu , \\nu ) } E_ { ( x , y ) \\sim\\pi ( x , y ) } [ c ( x , y ) ] $ $ and $ $ \\min_ { \\pi ( x|y ) } E_ { ( x , y ) \\sim\\pi_ { backward } ( x , y ) } [ c ( x , y ) ] \\le \\min_ { \\pi\\in \\Pi ( \\mu , \\nu ) } E_ { ( x , y ) \\sim\\pi ( x , y ) } [ c ( x , y ) ] $ $ which means the smallest possible cost of the ( forward/backward ) CT is smaller than or equal to the Wasserstein distance . Concern ( 2 ) : I am not sure if the contrastive model assumed in Eq 2 and 3 is expressive enough to model the entire space of marginal constraints in OT . That is , I don \u2019 t think $ p ( x ) \\pi ( y|x ) $ will cover $ \\Pi ( \\mu , \\nu ) $ . Lemma 1 just shows that under some conditions , $ p ( x ) \\pi ( y|x ) $ lies inside $ \\Pi ( \\mu , \\nu ) $ , but it would be interesting to see if then entire space of $ \\Pi ( \\mu , \\nu ) $ is covered by the neural contrastive model . Our response : Related to our response to Concern ( 1 ) , for $ \\pi_ { forward } ( x , y ) =p ( x ) \\pi ( y|x ) $ , we have $ \\int\\pi_ { forward } ( x , y ) dy=p ( x ) $ but allow either $ \\int\\pi_ { forward } ( x , y ) dx = p ( y ) $ or $ \\int\\pi_ { forward } ( x , y ) dx \\neq p ( y ) $ , whereas for $ \\pi\\in\\Pi ( \\mu , \\nu ) $ , we have both $ \\int\\pi ( x , y ) dy=p ( x ) $ and $ \\int \\pi ( x , y ) dx= p ( y ) $ . Therefore , the set of all possible joint probability measures , which $ \\pi_ { forward } $ belongs to , covers $ \\Pi ( \\mu , \\nu ) $ , in other words , the entire space of $ \\Pi ( \\mu , \\nu ) $ is covered by the space of the joint probability measure of the forward CT. A similar conclusion can also be made for $ \\pi_ { backward } ( x , y ) =p ( y ) \\pi ( x|y ) $ . We note Lemma 1 adds these conditions , including that $ p ( x ) $ is equal to $ p ( y ) $ in distribution ( not required by a regular $ \\Pi ( \\mu , \\nu ) $ ) , to make $ \\int\\pi_ { forward } ( x , y ) dx = p ( y ) $ and $ \\int\\pi_ { backward } ( x , y ) dx = p ( x ) $ ."}}