{"year": "2017", "forum": "r1fYuytex", "title": "Sparsely-Connected Neural Networks: Towards Efficient VLSI Implementation of Deep Neural Networks", "decision": "Accept (Poster)", "meta_review": "After discussion, the reviewers unanimously recommend accepting the paper.", "reviews": [{"review_id": "r1fYuytex-0", "review_text": "The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. The paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets. It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements. The paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs). It led me to think that LFSRs reduced the connections by keeping some of the information in the registers. However, LFSR is only used as a random binary generator. Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation. This explanation would be clearer to me: In this paper, we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks. Random connection masks are generated by LFSR, which is also used in the VLSI implementation to disable the connections. Algorithm 1 is basically training a network with back-propogation where each layer has a binary mask that disables some of the connections. This explanation can be added to the text. Using random connections is not a new idea in CNNs. It was used between CNN layers in a 1998 paper by Yann LeCun and others: http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf It was not used in fully connected layer before. The sparsity in fully connected layer decreases the computational burden but it is difficult to speed up. Also the author's VLSI implementation does not speed up the network inference. How are the results of this work compared to Network in Network (NiN)? https://arxiv.org/abs/1312.4400 In NiN, the authors removed the fully connected layers completely and used a cheap pooling operation and also got improved performance. Are the results presented here better? It would be more convincing to see this method tested on ImageNet, which actually uses a big fully connected layer. Increased my rating from 5-6 after rebuttal. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Reviewer comment : The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90 % of memory compared to the conventional implementations of fully connected neural networks . The paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets . It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements . Response : We sincerely thank the reviewer for the comments . -- -- -- -- -- -- -- -- -- -- -- -- -- Reviewer comment : The paper was hard for me to understand because of this misleading statement : In this paper , we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers ( LFSRs ) . It led me to think that LFSRs reduced the connections by keeping some of the information in the registers . However , LFSR is only used as a random binary generator . Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation . This explanation would be clearer to me : In this paper , we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks . Random connection masks are generated by LFSR , which is also used in the VLSI implementation to disable the connections . Response : We thank the reviewer for pointing out this unclear statement . We have modified the explanation according to your comment . -- -- -- -- -- -- -- -- -- -- -- -- -- Reviewer comment : Algorithm 1 is basically training a network with back-propagation where each layer has a binary mask that disables some of the connections . This explanation can be added to the text . Response : We thank the reviewer for the comment . We agree with the reviewer that Alg . 1 is the back-propagation algorithm with a binary mask . We believe that summarizing the proposed method as an algorithm would be clearer to the public reviewer . As suggested , we added the explanation to the text . -- -- -- -- -- -- -- -- -- -- -- -- -- Reviewer comment : Using random connections is not a new idea in CNNs . It was used between CNN layers in a 1998 paper by Yann LeCun and others : http : //yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf It was not used in fully connected layer before . Response : We thank the reviewer for pointing this out . Regarding the 1998 paper , we agree with the reviewer that a pruning technique was used . However , we believe that the presented pruning technique follows a pattern provided in Table I of the 1998 paper : it is not declared that the pattern was generated randomly , and no explanation is provided on this matter . Moreover , this pruning technique was only used for one convolutional layer ( from S2 to C3 ) with 37 % less connections . Besides , the method presented in the 1998 paper only applies a high level pruning technique in a \u201c binary \u201d way : it either eliminates or maintains each complete 5 * 5 filter , and it does not drop connections within the filter . Also , in the aforementioned paper no explanation is provided on the effect of the presented pruning technique in terms of both misclassification rate ( MCR ) and hardware implementation . -- -- -- -- -- -- -- -- -- -- -- -- -- Reviewer comment : The sparsity in fully connected layer decreases the computational burden but it is difficult to speed up . Also the author 's VLSI implementation does not speed up the network inference . Response : We thank the reviewer for the comment . The current VLSI implementations of fully-connected layers mainly suffer from copious power/energy consumption due to the energy/power-hungry memory accesses [ * ] , [ * * ] . That is why many works recently have focused on pruning techniques to reduce the memory accesses and consequently power/energy consumption . We agree with the reviewer that VLSI implementation of our proposed pruning method does not speed up the process compared to conventional VLSI implementations . However , the proposed VLSI implementation provides other advantages . First , we show that the proposed pruning technique can be simply performed using the conventional back-propagation algorithm where each layer has a binary mask , as opposed to other pruning techniques that usually require an additional training stage . Secondly , we show that the proposed technique can drop up to 90 % of connections while improving MCR . More importantly , the proposed technique can be easily implemented using LFSR units as shown in Fig.3 while highly reducing power/energy consumption and required memory size , and thus making fully connected network implementable in practice . VLSI implementations of fully-connected layers generally speed up the computations compared to GPUs/CPUs as shown in [ * ] . [ * ] S. Han , X. Liu , H. Mao , J. Pu , A. Pedram , M. A. Horowitz , and W. J. Dally . EIE : Efficient inference engine on compressed deep neural network . In 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture ( ISCA ) , pp . 243\u2013254 , June 2016. doi : 10.1109/ISCA.2016.30 . [ * * ] Matthieu Courbariaux and Yoshua Bengio . BinaryNet : Training deep neural networks with weights and activations constrained to +1 or -1 . CoRR , abs/1602.02830 , 2016 . -- -- -- -- -- -- -- -- -- -- -- -- -- Reviewer comment : How are the results of this work compared to Network in Network ( NiN ) ? https : //arxiv.org/abs/1312.4400 In NiN , the authors removed the fully connected layers completely and used a cheap pooling operation and also got improved performance . Are the results presented here better ? It would be more convincing to see this method tested on ImageNet , which actually uses a big fully connected layer . Response : We thank the reviewer for the comment . The MCRs of the NiN paper are summarized in Table 5 of our paper . The simulation results show that the proposed technique has a better MCR compared the NiN paper on CIFAR10 and SVHN datasets . Regarding the ImageNet dataset , we are currently working on the extension on this paper : it will include tests results obtained on ImageNet ."}, {"review_id": "r1fYuytex-1", "review_text": "Experimental results look reasonable, validated on 3 tasks. References could be improved, for example I would rather see Rumelhart's paper cited for back-propagation than the Deep Learning book. ", "rating": "7: Good paper, accept", "reply_text": "We sincerely thank the reviewer for pointing this out . The mentioned paper is now added to the manuscript . We will also double check the references to make sure that the original papers are cited ."}, {"review_id": "r1fYuytex-2", "review_text": "From my original comments: The results looks good but the baselines proposed are quite bad. For instance in the table 2 \"Misclassification rate for a 784-1024-1024-1024-10 \" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see \"significant\" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers... In CIFAR-10 experiments, i do not understand why \"Sparsely-Connected 90% + Single-Precision Floating-Point\" is worse than \"Sparsely-Connected 90% + BinaryConnect\". So it is better to use binary than float. Again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision. In fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines. ---- The authors reply still does not convince me. I still think that the same technique should be applied on more challenging scenarios. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely thank the reviewer for the comments . We are currently running simulations on more challenging scenarios , and we will do our best to upload the results before the submission deadline ."}], "0": {"review_id": "r1fYuytex-0", "review_text": "The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. The paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets. It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements. The paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs). It led me to think that LFSRs reduced the connections by keeping some of the information in the registers. However, LFSR is only used as a random binary generator. Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation. This explanation would be clearer to me: In this paper, we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks. Random connection masks are generated by LFSR, which is also used in the VLSI implementation to disable the connections. Algorithm 1 is basically training a network with back-propogation where each layer has a binary mask that disables some of the connections. This explanation can be added to the text. Using random connections is not a new idea in CNNs. It was used between CNN layers in a 1998 paper by Yann LeCun and others: http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf It was not used in fully connected layer before. The sparsity in fully connected layer decreases the computational burden but it is difficult to speed up. Also the author's VLSI implementation does not speed up the network inference. How are the results of this work compared to Network in Network (NiN)? https://arxiv.org/abs/1312.4400 In NiN, the authors removed the fully connected layers completely and used a cheap pooling operation and also got improved performance. Are the results presented here better? It would be more convincing to see this method tested on ImageNet, which actually uses a big fully connected layer. Increased my rating from 5-6 after rebuttal. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Reviewer comment : The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90 % of memory compared to the conventional implementations of fully connected neural networks . The paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets . It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements . Response : We sincerely thank the reviewer for the comments . -- -- -- -- -- -- -- -- -- -- -- -- -- Reviewer comment : The paper was hard for me to understand because of this misleading statement : In this paper , we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers ( LFSRs ) . It led me to think that LFSRs reduced the connections by keeping some of the information in the registers . However , LFSR is only used as a random binary generator . Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation . This explanation would be clearer to me : In this paper , we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks . Random connection masks are generated by LFSR , which is also used in the VLSI implementation to disable the connections . Response : We thank the reviewer for pointing out this unclear statement . We have modified the explanation according to your comment . -- -- -- -- -- -- -- -- -- -- -- -- -- Reviewer comment : Algorithm 1 is basically training a network with back-propagation where each layer has a binary mask that disables some of the connections . This explanation can be added to the text . Response : We thank the reviewer for the comment . We agree with the reviewer that Alg . 1 is the back-propagation algorithm with a binary mask . We believe that summarizing the proposed method as an algorithm would be clearer to the public reviewer . As suggested , we added the explanation to the text . -- -- -- -- -- -- -- -- -- -- -- -- -- Reviewer comment : Using random connections is not a new idea in CNNs . It was used between CNN layers in a 1998 paper by Yann LeCun and others : http : //yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf It was not used in fully connected layer before . Response : We thank the reviewer for pointing this out . Regarding the 1998 paper , we agree with the reviewer that a pruning technique was used . However , we believe that the presented pruning technique follows a pattern provided in Table I of the 1998 paper : it is not declared that the pattern was generated randomly , and no explanation is provided on this matter . Moreover , this pruning technique was only used for one convolutional layer ( from S2 to C3 ) with 37 % less connections . Besides , the method presented in the 1998 paper only applies a high level pruning technique in a \u201c binary \u201d way : it either eliminates or maintains each complete 5 * 5 filter , and it does not drop connections within the filter . Also , in the aforementioned paper no explanation is provided on the effect of the presented pruning technique in terms of both misclassification rate ( MCR ) and hardware implementation . -- -- -- -- -- -- -- -- -- -- -- -- -- Reviewer comment : The sparsity in fully connected layer decreases the computational burden but it is difficult to speed up . Also the author 's VLSI implementation does not speed up the network inference . Response : We thank the reviewer for the comment . The current VLSI implementations of fully-connected layers mainly suffer from copious power/energy consumption due to the energy/power-hungry memory accesses [ * ] , [ * * ] . That is why many works recently have focused on pruning techniques to reduce the memory accesses and consequently power/energy consumption . We agree with the reviewer that VLSI implementation of our proposed pruning method does not speed up the process compared to conventional VLSI implementations . However , the proposed VLSI implementation provides other advantages . First , we show that the proposed pruning technique can be simply performed using the conventional back-propagation algorithm where each layer has a binary mask , as opposed to other pruning techniques that usually require an additional training stage . Secondly , we show that the proposed technique can drop up to 90 % of connections while improving MCR . More importantly , the proposed technique can be easily implemented using LFSR units as shown in Fig.3 while highly reducing power/energy consumption and required memory size , and thus making fully connected network implementable in practice . VLSI implementations of fully-connected layers generally speed up the computations compared to GPUs/CPUs as shown in [ * ] . [ * ] S. Han , X. Liu , H. Mao , J. Pu , A. Pedram , M. A. Horowitz , and W. J. Dally . EIE : Efficient inference engine on compressed deep neural network . In 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture ( ISCA ) , pp . 243\u2013254 , June 2016. doi : 10.1109/ISCA.2016.30 . [ * * ] Matthieu Courbariaux and Yoshua Bengio . BinaryNet : Training deep neural networks with weights and activations constrained to +1 or -1 . CoRR , abs/1602.02830 , 2016 . -- -- -- -- -- -- -- -- -- -- -- -- -- Reviewer comment : How are the results of this work compared to Network in Network ( NiN ) ? https : //arxiv.org/abs/1312.4400 In NiN , the authors removed the fully connected layers completely and used a cheap pooling operation and also got improved performance . Are the results presented here better ? It would be more convincing to see this method tested on ImageNet , which actually uses a big fully connected layer . Response : We thank the reviewer for the comment . The MCRs of the NiN paper are summarized in Table 5 of our paper . The simulation results show that the proposed technique has a better MCR compared the NiN paper on CIFAR10 and SVHN datasets . Regarding the ImageNet dataset , we are currently working on the extension on this paper : it will include tests results obtained on ImageNet ."}, "1": {"review_id": "r1fYuytex-1", "review_text": "Experimental results look reasonable, validated on 3 tasks. References could be improved, for example I would rather see Rumelhart's paper cited for back-propagation than the Deep Learning book. ", "rating": "7: Good paper, accept", "reply_text": "We sincerely thank the reviewer for pointing this out . The mentioned paper is now added to the manuscript . We will also double check the references to make sure that the original papers are cited ."}, "2": {"review_id": "r1fYuytex-2", "review_text": "From my original comments: The results looks good but the baselines proposed are quite bad. For instance in the table 2 \"Misclassification rate for a 784-1024-1024-1024-10 \" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see \"significant\" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers... In CIFAR-10 experiments, i do not understand why \"Sparsely-Connected 90% + Single-Precision Floating-Point\" is worse than \"Sparsely-Connected 90% + BinaryConnect\". So it is better to use binary than float. Again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision. In fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines. ---- The authors reply still does not convince me. I still think that the same technique should be applied on more challenging scenarios. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We sincerely thank the reviewer for the comments . We are currently running simulations on more challenging scenarios , and we will do our best to upload the results before the submission deadline ."}}