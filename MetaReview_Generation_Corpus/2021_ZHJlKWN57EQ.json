{"year": "2021", "forum": "ZHJlKWN57EQ", "title": "Revisiting BFfloat16 Training", "decision": "Reject", "meta_review": "After reading the paper, reviews and authors\u2019 feedback. The meta-reviewer agrees with reviewers that the paper has limited novelty and could be more clear about mix precision training. Therefore this paper is rejected.\n\nThank you for submitting the paper to ICLR. \n", "reviews": [{"review_id": "ZHJlKWN57EQ-0", "review_text": "During the rebuttal , I concluded that this submission is highly confusing , rather misleading . I was led to believe the authors are in fact talking 'pure 16b MAC ' - meaning 16b FP multiplies and 16b accumulate . After reading their responses to R4 , I now learnt that they in fact are using 32b accumulates as is already standard practice in BF16 - If so , they have little or nothing new to offer . Rounding discussions the paper focuses on become highly secondary . BF16 is already well-understood and accepted . Their writeup was highly misleading to say the least . I change my rating based on this . They use a representative set of benchmarks which include , CNN-based Resnet , recommendation proxy DLRM , and NLP proxy BERT . Novelty is limited , but critical : They reiterate prior observations that accurate weight updates are more critical for maintaining accuracy than forward-backward gradient updates . Former is what suffers when round-to-nearest cancels out small updates . Stochastic rounding has also been published before and shown to still miss the accuracy mark with > 0.,1 % accuracy gap for some important benchmarks . Novel part of this work is suggesting a Kahan summation based software fix on top of stochastic rounding to overcome the accuracy loss . Primary issue : This is a very confusing and misleading writeup . Authors should have clearly spelt out that they are in fact talking about BF16 - which is well-publicized for years now as 16b mule and 32b accumulate - and not labelled it as 'pure 16b MAC ' - which is already known to exist as well , and proven to be not sufficient for DL training . In light of this , this work has very little value-add . I change my rating now to 'clear reject ' for their misleading writing style .", "rating": "3: Clear rejection", "reply_text": "We thank R3 for the favorable consideration on the importance of insights from our study . We refer to the general reply for questions shared across reviewers and resolve the remaining comments in the below . We agree with R3 that FP32 support is needed for general purpose processors . However , we believe eliminating the requirement of 32-bit compute units can be of practical importance for accelerators specialized for training generic deep learning models . E.g.as discussed in the general reply for common questions , both 32-bit training and mixed precision training require compute units with 32-bit multiply for operations inside optimizers . In contrast , the 16-bit training algorithms we study only require faster and more energy efficient units with 16-bit multiply ; this benefit is especially pronounced for training large SOTA models using optimizers with sophisticated multiplications such Adam . As accelerators for DL training are active research topics and open a large industrial market for both startups and hardware giants , we are very excited to continuously explore the feasibility of accelerators only using < 32 bit compute units . To clarify the above , we will add discussions in the preliminary section ."}, {"review_id": "ZHJlKWN57EQ-1", "review_text": "# # # Summary This work reinvigorates half precision training as an alternative to either full single precision or mixed half and single precision . The authors demonstrate that the nearest rounding is the culprit for the worse performance of half precision training compared to single precision , due to cancelling small updates . They then propose two known techniques that can mitigate this effect , stochastic rounding and Kahan summation . Empirically , they demonstrate on an extensive suite of tasks that the proposed adjustments lead to half precision performance that is almost on par to single precision . # # # Pros - Solid experimental evaluation and results on a large variety of tasks - Both modifications that need to done are simple and straightforward # # # Cons - Limited novelty # # # Recommendation Overall , even though the paper does not have a lot of novel content , the experimental evaluation is thorough and demonstrates that the improvements close the gap to single precision training . Therefore I am keen on accepting this work , although admittedly not by much . # # # Detailed feedback The paper is relatively well written and easy to follow . The authors nicely motivate their modifications which clearly show that the gap between half and single precision training is almost closed . My main point of criticism is that the novelty of this paper is relatively small , as the techniques proposed for performing the quantized weight update are , firstly , not new and , secondly , have been used in previous works for similar reasons . More specifically , performing stochastic rounding for the weight update in order to make progress when the magnitude of the update is small ( thus rounded to zero ) was also proposed at [ 1 ] , which also showed successful 16 bit training ( albeit for outdated tasks ) . As for Kahan summation ; that has been proposed before for training quantised neural networks at [ 2 ] ( again as a means to avoid making no progress when the parameter update is small ) . As a result , the true contributions of this work lie on the extensive empirical evaluation along with the theoretical analysis of rounding in a simple linear model . As for other feedback and questions - At section 3.1 you first assumption A1 seems peculiar , as you state that you work on a least squares regression problem , but you assume that the model is overparametrized . How can this be the case ? In a linear model , the amount of parameters is bounded by the dimensionality of the input ( and it doesn \u2019 t seem that you perform any feature expansion for x ) . I believe that a more reasonable statement would be to assume that the actual data are generated by a linear model , hence there exists a true solution w^ * . - Stochastic rounding requires the generation of random numbers ; can this step also be done accurately in bfloat16 ? - From the evaluation it seems that Kahan summation performs better but it increases the memory size by a factor of 2 ; how does this fare memory wise to having the weights in single precision and is it a worthy tradeoff ? - I believe that comparisons against other methods for quantised training that use fixed point instead of floating point would be interesting , e.g. , [ 3 , 4 ] . This would highlight the differences between these two formats and show whether the more expensive floating point operations are necessary . [ 1 ] Deep Learning with Limited Numerical Precision , S. Gupta , A. Agrawal , K. Gopalakrishnan , P. Narayanan , 2015 [ 2 ] Training Deep Neural Networks in Limited Precision , H. Park , J. H. Lee , Y. Oh , S. Ha , S. Lee , 2018 [ 3 ] Training and Inference with Integers in Deep Neural Networks , S. Wu , G. Li , F. Chen , L. She , 2018 [ 4 ] Per-Tensor Fixed-Point Quantization of the Backpropagation Algorithm , C. Sakr , N. Shanbhag , 2018", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank R1 for the thoughtful comments and refer to the general reply for shared questions ; we resolve the remaining comments in the below . R1 raises the question when a least square regression will be overparameterized and suggests alternative elaboration on the overparameterized assumption . Our overparameterized assumption refers to the underdetermined least-squares regression models . In such cases , the model dimensionality ( input feature dimension ) is fixed but can be a very large value ( relative to the number of samples ) . We consider this setting to reflect the overparameterized nature of deep neural networks . We agree with R1 that alternatively it is also very intuitive to elaborate the same technical condition as an assumption where the training data is generated from an underlying linear model . We will better elaborate on the overparameterization assumption in the beginning paragraph in Section 3.1 . We agree with R1 that it is intrinsically interesting to compare floating point low precision training algorithms to fixed point ones . Though the focus of our paper is on floating point training algorithms which are widely adopted in emerging model-agnostic accelerators , we are very eager to explore this comparison in future work to reveal more insights for accelerator designers in an even further horizon ."}, {"review_id": "ZHJlKWN57EQ-2", "review_text": "This paper explores the possibilities of reducing the precision of the weight update operation ( i.e.AXPY ops ) from 32 bit to 16 bit in today \u2019 s BFloat16 training framework . To enable 16 bit update , the authors proposed two techniques , i.e.stochastic rounding and Kahan summation . The authors use a simple least-squares regression model to theoretically explain the model accuracy degradation due to nearest rounding , then experimentally demonstrate the effectiveness of two techniques on a range of deep learning models and dataset . Strong points : This paper is very well written . The problem is well addressed , and the solutions are well explained . The authors also provided both theoretical analysis and comprehensive experimental results . Week points : 1 ) . Novelty : Both the problem addressed , and the solutions proposed in this paper have been reported in recent publications . In particular , both ( https : //papers.nips.cc/paper/7994-training-deep-neural-networks-with-8-bit-floating-point-numbers ) and ( https : //papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks ) investigated the reduced precision of weight update from 32 bit to 16 bit or less . Moreover , the former introduced stochastic rounding and the later introduced round-off residue which basically the same as the Kahan accumulation technique discussed in this paper . Although , both papers discussed this topic in the FP8 training frameworks , while this paper in the BFloat16 framework , the basic concepts are the same . The authors of this paper summarized the techniques nicely , however , the novelty limited . 2 ) .On the same note , theoretical analysis on the impact of rounding mode on quantized weight update were also discussed in recent publications , such as ( https : //arxiv.org/abs/1706.02379 and https : //openreview.net/forum ? id=ryM_IoAqYX ) . It would have been nice to include these discussions as background knowledge and to distinguish this work from others . To clarify : 1 ) . The proposed Kahan Summation method created a second tensor to store/accumulate the quantization error . Both weight and rounding error tensors are in 16 bit which in total , effectively , is 32 bit . Since AXPY is a very fast operation , this method does not seem to save much in terms of memory or speed . 2 ) .Today , SGD is often used with momentum , could the authors comment on the precision of momentum accumulation . And how about other popular optimizers , such as Adam ? 3 ) .The analysis , i.e.Theroem 1 and 2 , is based on a simple least-square regression model . Can this theory generalize to deep learning models ? 4 ) .On Table 2 , last two rows of last column , the value seems to be inconsistent with other data in the same row .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank R2 for the detailed feedback on both the theory and empirical aspects . We refer to the general reply for questions in common and discuss the remaining questions in the below . R2 asked what is the precision of optimizer states , such as momentum in SGD . In our experiments , we also use BFloat16 precision for momentum in the SGD optimizer , and for first / second moments in the Adam optimizer . This ensures that all the optimizer operations can directly use 16-bit input and generate 16-bit output using 16-bit compute units . To make this more clear , we will increment the discussion on the optimizer implementation using stochastic rounding or Kahan summation for model weight updates in Appendix B . Regarding the generalization of our analysis on least-squares models , the insights that nearest rounding for model weight updates degrades convergence due to cancellation of updates empirically generalize to deep learning models . E.g.in Appendix D.3 , we show that up to 80 % of model updates can get cancelled due to BFloat16 nearest rounding for weight updates in a DLRM recommender model ; this leads to ~3 % AUC degradation compared to 32-bit training . We are also excited to study rigorous theory generalization to deep learning models feasible for analysis as future works . R4 also very sharply noticed that in Table 2 , standard 16-bit training demonstrates one magnitude worse validation perplexity on Wiki103 datasets than 32-bit training . This large discrepancy is valid . It is because that validation perplexity value is exponential with respect to the validation loss . Thus the difference of validation loss can be significantly magnified by validation perplexity which is the standard metric for language modeling tasks ."}, {"review_id": "ZHJlKWN57EQ-3", "review_text": "I think the use of QPyTorch for the experiments here invalidates the results since the intermediate matrix multiplies are done in single precision ( FP32 ) , and so are more optimistic than a pure 16-bit implementation . ( This is both according to the authors Sec 4 , experiment setup ; and according to the QPyTorch paper arxiv:1910.04540 , Sec 3 intro . ) For these kinds of experiments to be meaningful , they have to be done on native 16-bit hardware which luckily is becoming more common , e.g. , Google 's TPUs or the newer NVIDIA GPUs . There are two other problems . First , it is not clear how stochastic rounding would be implemented in hardware . Doing it for every MAC operation could likely be even more expensive than just doing 32-bit MAC operations , since it involves the generation of random numbers , division , etc . Second , Kahan summation takes up twice the weight storage , so a more detailed calculation is needed to compare any hardware/energy savings to use that instead of just 32-bit . As an aside , it may be interesting in Figure 1 to zoom in on the initial part of training to understand where the difference between 32-bit and standard 16-bit comes from in early training since at that point , the gradients are generally larger than later on in training .", "rating": "3: Clear rejection", "reply_text": "We thank R4 for the detailed feedback and discussion . We refer to the general reply on common questions and resolve the other comments in the following . R4 suggested that we should run experiments with TPUs or recent GPUs to achieve evaluation when the accumulation operation in MAC is also in 16-bit precision . Unfortunately these hardware accelerators ( like all of the existing ones that we know of ) use higher precision accumulators in MAC units . In more details , the BFloat16 unit we considered takes 16-bit input for multiplication and uses 32-bit higher precision for the accumulation in MAC operations . Such higher precision accumulation is inexpensive compared to multiplication but important to the numerical accuracy of operations such as matrix multiplication and convolutions . Thus it is the standard design practice in today \u2019 s TPUs and GPUs [ 6 , 8 ] . Because there is minimal reward from a hardware design perspective in eliminating higher precision accumulators , studying 16-bit precision accumulation in MAC operation is beyond the scope of this paper ( albeit still very interesting conceptually ) . Therefore , QPyTorch can support exact compute simulation for the BFloat16 units [ 9 ] in our study that exist ( and will continue to exist ) in modern hardware ; we use it to flexibly evaluate numerical techniques . Using TPUs or new GPUs would still result in the same numerical output for BFloat16 units as in the QPyTorch because these accelerators are also using 32-bit instead of 16-bit MAC accumulation [ 6 , 8 ] . We thank R4 for bringing up this point as we very much agree that it needs to be addressed more clearly and explicitly in our draft and we will modify the introduction to address this point . R4 also noticed that the training accuracies are different at the early stage for 32-bit and standard 16-bit training in Figure 1 . This is because we use strong smoothing for the curves for clear visualization . Both training algorithms start from the same initialization and the training accuracy gradually diverges . We will include a less smoothed curve in the appendix D.1 to clarify this observation and link from the main paper ."}], "0": {"review_id": "ZHJlKWN57EQ-0", "review_text": "During the rebuttal , I concluded that this submission is highly confusing , rather misleading . I was led to believe the authors are in fact talking 'pure 16b MAC ' - meaning 16b FP multiplies and 16b accumulate . After reading their responses to R4 , I now learnt that they in fact are using 32b accumulates as is already standard practice in BF16 - If so , they have little or nothing new to offer . Rounding discussions the paper focuses on become highly secondary . BF16 is already well-understood and accepted . Their writeup was highly misleading to say the least . I change my rating based on this . They use a representative set of benchmarks which include , CNN-based Resnet , recommendation proxy DLRM , and NLP proxy BERT . Novelty is limited , but critical : They reiterate prior observations that accurate weight updates are more critical for maintaining accuracy than forward-backward gradient updates . Former is what suffers when round-to-nearest cancels out small updates . Stochastic rounding has also been published before and shown to still miss the accuracy mark with > 0.,1 % accuracy gap for some important benchmarks . Novel part of this work is suggesting a Kahan summation based software fix on top of stochastic rounding to overcome the accuracy loss . Primary issue : This is a very confusing and misleading writeup . Authors should have clearly spelt out that they are in fact talking about BF16 - which is well-publicized for years now as 16b mule and 32b accumulate - and not labelled it as 'pure 16b MAC ' - which is already known to exist as well , and proven to be not sufficient for DL training . In light of this , this work has very little value-add . I change my rating now to 'clear reject ' for their misleading writing style .", "rating": "3: Clear rejection", "reply_text": "We thank R3 for the favorable consideration on the importance of insights from our study . We refer to the general reply for questions shared across reviewers and resolve the remaining comments in the below . We agree with R3 that FP32 support is needed for general purpose processors . However , we believe eliminating the requirement of 32-bit compute units can be of practical importance for accelerators specialized for training generic deep learning models . E.g.as discussed in the general reply for common questions , both 32-bit training and mixed precision training require compute units with 32-bit multiply for operations inside optimizers . In contrast , the 16-bit training algorithms we study only require faster and more energy efficient units with 16-bit multiply ; this benefit is especially pronounced for training large SOTA models using optimizers with sophisticated multiplications such Adam . As accelerators for DL training are active research topics and open a large industrial market for both startups and hardware giants , we are very excited to continuously explore the feasibility of accelerators only using < 32 bit compute units . To clarify the above , we will add discussions in the preliminary section ."}, "1": {"review_id": "ZHJlKWN57EQ-1", "review_text": "# # # Summary This work reinvigorates half precision training as an alternative to either full single precision or mixed half and single precision . The authors demonstrate that the nearest rounding is the culprit for the worse performance of half precision training compared to single precision , due to cancelling small updates . They then propose two known techniques that can mitigate this effect , stochastic rounding and Kahan summation . Empirically , they demonstrate on an extensive suite of tasks that the proposed adjustments lead to half precision performance that is almost on par to single precision . # # # Pros - Solid experimental evaluation and results on a large variety of tasks - Both modifications that need to done are simple and straightforward # # # Cons - Limited novelty # # # Recommendation Overall , even though the paper does not have a lot of novel content , the experimental evaluation is thorough and demonstrates that the improvements close the gap to single precision training . Therefore I am keen on accepting this work , although admittedly not by much . # # # Detailed feedback The paper is relatively well written and easy to follow . The authors nicely motivate their modifications which clearly show that the gap between half and single precision training is almost closed . My main point of criticism is that the novelty of this paper is relatively small , as the techniques proposed for performing the quantized weight update are , firstly , not new and , secondly , have been used in previous works for similar reasons . More specifically , performing stochastic rounding for the weight update in order to make progress when the magnitude of the update is small ( thus rounded to zero ) was also proposed at [ 1 ] , which also showed successful 16 bit training ( albeit for outdated tasks ) . As for Kahan summation ; that has been proposed before for training quantised neural networks at [ 2 ] ( again as a means to avoid making no progress when the parameter update is small ) . As a result , the true contributions of this work lie on the extensive empirical evaluation along with the theoretical analysis of rounding in a simple linear model . As for other feedback and questions - At section 3.1 you first assumption A1 seems peculiar , as you state that you work on a least squares regression problem , but you assume that the model is overparametrized . How can this be the case ? In a linear model , the amount of parameters is bounded by the dimensionality of the input ( and it doesn \u2019 t seem that you perform any feature expansion for x ) . I believe that a more reasonable statement would be to assume that the actual data are generated by a linear model , hence there exists a true solution w^ * . - Stochastic rounding requires the generation of random numbers ; can this step also be done accurately in bfloat16 ? - From the evaluation it seems that Kahan summation performs better but it increases the memory size by a factor of 2 ; how does this fare memory wise to having the weights in single precision and is it a worthy tradeoff ? - I believe that comparisons against other methods for quantised training that use fixed point instead of floating point would be interesting , e.g. , [ 3 , 4 ] . This would highlight the differences between these two formats and show whether the more expensive floating point operations are necessary . [ 1 ] Deep Learning with Limited Numerical Precision , S. Gupta , A. Agrawal , K. Gopalakrishnan , P. Narayanan , 2015 [ 2 ] Training Deep Neural Networks in Limited Precision , H. Park , J. H. Lee , Y. Oh , S. Ha , S. Lee , 2018 [ 3 ] Training and Inference with Integers in Deep Neural Networks , S. Wu , G. Li , F. Chen , L. She , 2018 [ 4 ] Per-Tensor Fixed-Point Quantization of the Backpropagation Algorithm , C. Sakr , N. Shanbhag , 2018", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank R1 for the thoughtful comments and refer to the general reply for shared questions ; we resolve the remaining comments in the below . R1 raises the question when a least square regression will be overparameterized and suggests alternative elaboration on the overparameterized assumption . Our overparameterized assumption refers to the underdetermined least-squares regression models . In such cases , the model dimensionality ( input feature dimension ) is fixed but can be a very large value ( relative to the number of samples ) . We consider this setting to reflect the overparameterized nature of deep neural networks . We agree with R1 that alternatively it is also very intuitive to elaborate the same technical condition as an assumption where the training data is generated from an underlying linear model . We will better elaborate on the overparameterization assumption in the beginning paragraph in Section 3.1 . We agree with R1 that it is intrinsically interesting to compare floating point low precision training algorithms to fixed point ones . Though the focus of our paper is on floating point training algorithms which are widely adopted in emerging model-agnostic accelerators , we are very eager to explore this comparison in future work to reveal more insights for accelerator designers in an even further horizon ."}, "2": {"review_id": "ZHJlKWN57EQ-2", "review_text": "This paper explores the possibilities of reducing the precision of the weight update operation ( i.e.AXPY ops ) from 32 bit to 16 bit in today \u2019 s BFloat16 training framework . To enable 16 bit update , the authors proposed two techniques , i.e.stochastic rounding and Kahan summation . The authors use a simple least-squares regression model to theoretically explain the model accuracy degradation due to nearest rounding , then experimentally demonstrate the effectiveness of two techniques on a range of deep learning models and dataset . Strong points : This paper is very well written . The problem is well addressed , and the solutions are well explained . The authors also provided both theoretical analysis and comprehensive experimental results . Week points : 1 ) . Novelty : Both the problem addressed , and the solutions proposed in this paper have been reported in recent publications . In particular , both ( https : //papers.nips.cc/paper/7994-training-deep-neural-networks-with-8-bit-floating-point-numbers ) and ( https : //papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks ) investigated the reduced precision of weight update from 32 bit to 16 bit or less . Moreover , the former introduced stochastic rounding and the later introduced round-off residue which basically the same as the Kahan accumulation technique discussed in this paper . Although , both papers discussed this topic in the FP8 training frameworks , while this paper in the BFloat16 framework , the basic concepts are the same . The authors of this paper summarized the techniques nicely , however , the novelty limited . 2 ) .On the same note , theoretical analysis on the impact of rounding mode on quantized weight update were also discussed in recent publications , such as ( https : //arxiv.org/abs/1706.02379 and https : //openreview.net/forum ? id=ryM_IoAqYX ) . It would have been nice to include these discussions as background knowledge and to distinguish this work from others . To clarify : 1 ) . The proposed Kahan Summation method created a second tensor to store/accumulate the quantization error . Both weight and rounding error tensors are in 16 bit which in total , effectively , is 32 bit . Since AXPY is a very fast operation , this method does not seem to save much in terms of memory or speed . 2 ) .Today , SGD is often used with momentum , could the authors comment on the precision of momentum accumulation . And how about other popular optimizers , such as Adam ? 3 ) .The analysis , i.e.Theroem 1 and 2 , is based on a simple least-square regression model . Can this theory generalize to deep learning models ? 4 ) .On Table 2 , last two rows of last column , the value seems to be inconsistent with other data in the same row .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank R2 for the detailed feedback on both the theory and empirical aspects . We refer to the general reply for questions in common and discuss the remaining questions in the below . R2 asked what is the precision of optimizer states , such as momentum in SGD . In our experiments , we also use BFloat16 precision for momentum in the SGD optimizer , and for first / second moments in the Adam optimizer . This ensures that all the optimizer operations can directly use 16-bit input and generate 16-bit output using 16-bit compute units . To make this more clear , we will increment the discussion on the optimizer implementation using stochastic rounding or Kahan summation for model weight updates in Appendix B . Regarding the generalization of our analysis on least-squares models , the insights that nearest rounding for model weight updates degrades convergence due to cancellation of updates empirically generalize to deep learning models . E.g.in Appendix D.3 , we show that up to 80 % of model updates can get cancelled due to BFloat16 nearest rounding for weight updates in a DLRM recommender model ; this leads to ~3 % AUC degradation compared to 32-bit training . We are also excited to study rigorous theory generalization to deep learning models feasible for analysis as future works . R4 also very sharply noticed that in Table 2 , standard 16-bit training demonstrates one magnitude worse validation perplexity on Wiki103 datasets than 32-bit training . This large discrepancy is valid . It is because that validation perplexity value is exponential with respect to the validation loss . Thus the difference of validation loss can be significantly magnified by validation perplexity which is the standard metric for language modeling tasks ."}, "3": {"review_id": "ZHJlKWN57EQ-3", "review_text": "I think the use of QPyTorch for the experiments here invalidates the results since the intermediate matrix multiplies are done in single precision ( FP32 ) , and so are more optimistic than a pure 16-bit implementation . ( This is both according to the authors Sec 4 , experiment setup ; and according to the QPyTorch paper arxiv:1910.04540 , Sec 3 intro . ) For these kinds of experiments to be meaningful , they have to be done on native 16-bit hardware which luckily is becoming more common , e.g. , Google 's TPUs or the newer NVIDIA GPUs . There are two other problems . First , it is not clear how stochastic rounding would be implemented in hardware . Doing it for every MAC operation could likely be even more expensive than just doing 32-bit MAC operations , since it involves the generation of random numbers , division , etc . Second , Kahan summation takes up twice the weight storage , so a more detailed calculation is needed to compare any hardware/energy savings to use that instead of just 32-bit . As an aside , it may be interesting in Figure 1 to zoom in on the initial part of training to understand where the difference between 32-bit and standard 16-bit comes from in early training since at that point , the gradients are generally larger than later on in training .", "rating": "3: Clear rejection", "reply_text": "We thank R4 for the detailed feedback and discussion . We refer to the general reply on common questions and resolve the other comments in the following . R4 suggested that we should run experiments with TPUs or recent GPUs to achieve evaluation when the accumulation operation in MAC is also in 16-bit precision . Unfortunately these hardware accelerators ( like all of the existing ones that we know of ) use higher precision accumulators in MAC units . In more details , the BFloat16 unit we considered takes 16-bit input for multiplication and uses 32-bit higher precision for the accumulation in MAC operations . Such higher precision accumulation is inexpensive compared to multiplication but important to the numerical accuracy of operations such as matrix multiplication and convolutions . Thus it is the standard design practice in today \u2019 s TPUs and GPUs [ 6 , 8 ] . Because there is minimal reward from a hardware design perspective in eliminating higher precision accumulators , studying 16-bit precision accumulation in MAC operation is beyond the scope of this paper ( albeit still very interesting conceptually ) . Therefore , QPyTorch can support exact compute simulation for the BFloat16 units [ 9 ] in our study that exist ( and will continue to exist ) in modern hardware ; we use it to flexibly evaluate numerical techniques . Using TPUs or new GPUs would still result in the same numerical output for BFloat16 units as in the QPyTorch because these accelerators are also using 32-bit instead of 16-bit MAC accumulation [ 6 , 8 ] . We thank R4 for bringing up this point as we very much agree that it needs to be addressed more clearly and explicitly in our draft and we will modify the introduction to address this point . R4 also noticed that the training accuracies are different at the early stage for 32-bit and standard 16-bit training in Figure 1 . This is because we use strong smoothing for the curves for clear visualization . Both training algorithms start from the same initialization and the training accuracy gradually diverges . We will include a less smoothed curve in the appendix D.1 to clarify this observation and link from the main paper ."}}