{"year": "2017", "forum": "rJeKjwvclx", "title": "Dynamic Coattention Networks For Question Answering", "decision": "Accept (Poster)", "meta_review": "The program committee appreciates the authors' response to concerns raised in the reviews. All reviewers agree that this is a good piece of work that should be accepted to ICLR. Authors are encouraged to incorporate reviewer feedback to further strengthen the paper.", "reviews": [{"review_id": "rJeKjwvclx-0", "review_text": "Summary: The paper proposes a novel deep neural network architecture for the task of question answering on the SQuAD dataset. The model consists of two main components -- coattention encoder and dynamic pointer decoder. The encoder produces attention over the question as well as over the document in parallel and thus learns co-dependent representations of the question and the document. The decoder predicts the starting and the end token of the answer iteratively, with the motivation that multiple iterations will help the model escape local maxima and thus will reduce the errors made by the model. The proposed model achieved state-of-art result on SQuAD dataset at the time of writing the paper. The paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc. The paper also performs some ablation studies such as performing only single round of iteration on decoder, etc. Strengths: 1. The paper is well-motivated with two main motivations -- co-attending to the document and the question, and iteratively producing the answer. 2. The proposed model architecture is novel and the design choices made seem reasonable. 3. The experiments show that the proposed model outperforms the existing model (at the time of writing the paper) on the SQuAD dataset by significant margin. 4. The analyses of the results and the ablation studies performed (as per someone's request) provide insights into the various modelling design choices made. Weaknesses/Questions/Suggestions: 1. In order to gain insights into how much each additional iteration in the decoder help, I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration. 2. Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations. Could authors please report how often this happens? 3. In order to estimate how much modelling of attention in the encoder helps, it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder (neither over question, nor over document). 4. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required. 5. In Wang and Jiang (2016), the attention is predicted over question for each word in the document. But in table 2, when performing ablation study to make the proposed model similar to Wang and Jiang, C^D is set to C^Q. But isn\u2019t C^Q attention over document for each word in the question? So, how is this similar to Wang and Jiang\u2019s attention? I think QA^D will be similar to Wang and Jiang's attention since QA^D is attention over question for each word in the document. Please clarify. 6. In section 2.1, \u201cn\u201d and \u201cm\u201d are swapped when explaining the Document and Question encoding matrix. Please fix it. Review Summary: The paper presents a novel and interesting model for the task of question answering on SQuAD dataset and shows that the model outperforms existing models. However, to gain more insights into the functioning of the model, I would like see more analyses of the results and one more ablation study (see weaknesses section above). ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for your thoughtful reviews . > > > > 1.In order to gain insights into how much each additional iteration in the decoder help , I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration . We do not have strong support ( number of examples ) for high iteration ( e.g.3-4 iterations ) examples . We did find in our ablation study that allowing the model multiple iterations , as opposed to forcing it to a single iteration , improves performance . Below , we show the 4-iteration model \u2019 s performance across iterations needed to converge . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - iterations mean f1 frequency 1 0.7660381405761181 9935 2 0.5698158862824868 527 3 0.5464594744971136 50 4 0.6213130048723088 58 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - > > > > 2 . Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations . Could authors please report how often this happens ? This occurs very rarely . For example , on the development , only 58 examples reach the maximum of 4 iterations . > > > > 3.In order to estimate how much modeling of attention in the encoder helps , it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder ( neither over question , nor over document ) . We attempted a model without attention , in which the encoder first ingests the question then the document . The decoder then points over the document encoding as usual . The best validation performance we obtained for this model is 41.9 % F1 and 33.3 % exact match . > > > > 4.I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning ( table 3 in SQuAD paper ) . This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required . I emphasize with your position that we analyze the model with respect to the reasoning required to gain more insights into what it is doing . In particular , I would be very interested in seeing how each top model submitted to SQuAD differs in this regard . That said , we contacted Rajpurkar et al , the original authors of the SQuAD paper , who declined to share the 200 sampled used to estimate the statistics in Table 3 of the SQuAD paper . In particular , Pranav offered that releasing the 192 examples would have very little value of information of tuning and comparing models . That said , they are considering including in the next iteration of the dataset a more thorough analysis for the purposes of analyzing the strengths and weaknesses of each model . We will update our results when the dataset is updated . > > > > 5.In Wang and Jiang ( 2016 ) , the attention is predicted over question for each word in the document . But in table 2 , when performing ablation study to make the proposed model similar to Wang and Jiang , C^D is set to C^Q . But isn \u2019 t C^Q attention over document for each word in the question ? So , how is this similar to Wang and Jiang \u2019 s attention ? I think QA^D will be similar to Wang and Jiang 's attention since QA^D is attention over question for each word in the document . Please clarify . You are correct , this is indeed a typo ( it \u2019 s suppose to be QA^D as opposed to C^Q ) . > > > > 6.In section 2.1 , \u201c n \u201d and \u201c m \u201d are swapped when explaining the Document and Question encoding matrix . Please fix it . Thanks ! We will fix it !"}, {"review_id": "rJeKjwvclx-1", "review_text": " Paper Summary: The paper introduces a question answering model called Dynamic Coattention Network (DCN). It extracts co-dependent representations of the document and question, and then uses an iterative dynamic pointing decoder to predict an answer span. The proposed model achieves state-of-the-art performance, outperforming all published models. Paper Strengths: -- The proposed model introduces two new concepts to QA models -- 1) using attention in both directions, and 2) a dynamic decoder which iterates over multiple answer spans until convergence or maximum number of iterations. -- The paper also presents ablation study of the proposed model which shows the importance of their design choices. -- It is interesting to see the same idea of co-attention performing well in 2 different domains -- Visual Question Answering and machine reading comprehension. -- The performance breakdown over document and question lengths (Figure 6) strengthens the importance of attention for QA task. -- The proposed model achieves state-of-the-art result on SQuAD dataset. -- The model architecture has been clearly described. Paper Weaknesses / Future Thoughts: -- The paper provides model's performance when the maximum number of iterations is 1 and 4. I would like to see how the performance of the model changes with the number of iterations, i.e., the model performance when that number is 2 and 3. Is there a clear trend? What type of questions is the model able to get correct with more iterations? -- As with many deep learning approaches, the overall architecture seems quite complex, and the design choices seem to be driven by performance numbers. As future work, authors might try to analyze qualitative advantages of different choices in the proposed model. What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction, when using Maxout Highway Network instead of a simple MLP, etc? Preliminary Evaluation: Novel and state-of-the-art question answering approach. Model is clearly described in detail. In my thoughts, a clear accept.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for your thoughtful review . > > > > I would like to see how the performance of the model changes with the number of iterations , i.e. , the model performance when that number is 2 and 3 . Is there a clear trend ? Yes , the model consistently performs better with with more iterations , with diminishing returns as the number of iteration increases . With more iterations , the model performs better on examples in which there exists multiple potential answers ( e.g.multiple people for a `` who question '' ) . > > > > As future work , authors might try to analyze qualitative advantages of different choices in the proposed model . What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction , when using Maxout Highway Network instead of a simple MLP , etc ? We agree , we will analyze the advantages of different choices in the proposed model across different question types . Also as suggested by Reviewer 1 , we think it would be interesting to analyze the model ( e.g.across encoder types , decoder types , and iteration counts ) with respect to the type of reasoning involved once the dataset is updated with such information ."}, {"review_id": "rJeKjwvclx-2", "review_text": "This paper proposed a dynamic coattention network for the question answering task with long contextual documents. The model is able to encode co-dependent representations of the question and the document, and a dynamic decoder iteratively pointing the potential answer spans to locate the final answer. Overall, this is a well-written paper. Although the model is a bit complicated (coattention encoder, iterative dynamic pointering decoder and highway maxout network), the intuitions behind and the details of the model are clearly presented. Also the performance on the SQuAD dataset is good. I would recommend this paper to be accepted. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for your thoughtful review ."}], "0": {"review_id": "rJeKjwvclx-0", "review_text": "Summary: The paper proposes a novel deep neural network architecture for the task of question answering on the SQuAD dataset. The model consists of two main components -- coattention encoder and dynamic pointer decoder. The encoder produces attention over the question as well as over the document in parallel and thus learns co-dependent representations of the question and the document. The decoder predicts the starting and the end token of the answer iteratively, with the motivation that multiple iterations will help the model escape local maxima and thus will reduce the errors made by the model. The proposed model achieved state-of-art result on SQuAD dataset at the time of writing the paper. The paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc. The paper also performs some ablation studies such as performing only single round of iteration on decoder, etc. Strengths: 1. The paper is well-motivated with two main motivations -- co-attending to the document and the question, and iteratively producing the answer. 2. The proposed model architecture is novel and the design choices made seem reasonable. 3. The experiments show that the proposed model outperforms the existing model (at the time of writing the paper) on the SQuAD dataset by significant margin. 4. The analyses of the results and the ablation studies performed (as per someone's request) provide insights into the various modelling design choices made. Weaknesses/Questions/Suggestions: 1. In order to gain insights into how much each additional iteration in the decoder help, I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration. 2. Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations. Could authors please report how often this happens? 3. In order to estimate how much modelling of attention in the encoder helps, it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder (neither over question, nor over document). 4. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required. 5. In Wang and Jiang (2016), the attention is predicted over question for each word in the document. But in table 2, when performing ablation study to make the proposed model similar to Wang and Jiang, C^D is set to C^Q. But isn\u2019t C^Q attention over document for each word in the question? So, how is this similar to Wang and Jiang\u2019s attention? I think QA^D will be similar to Wang and Jiang's attention since QA^D is attention over question for each word in the document. Please clarify. 6. In section 2.1, \u201cn\u201d and \u201cm\u201d are swapped when explaining the Document and Question encoding matrix. Please fix it. Review Summary: The paper presents a novel and interesting model for the task of question answering on SQuAD dataset and shows that the model outperforms existing models. However, to gain more insights into the functioning of the model, I would like see more analyses of the results and one more ablation study (see weaknesses section above). ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for your thoughtful reviews . > > > > 1.In order to gain insights into how much each additional iteration in the decoder help , I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration . We do not have strong support ( number of examples ) for high iteration ( e.g.3-4 iterations ) examples . We did find in our ablation study that allowing the model multiple iterations , as opposed to forcing it to a single iteration , improves performance . Below , we show the 4-iteration model \u2019 s performance across iterations needed to converge . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - iterations mean f1 frequency 1 0.7660381405761181 9935 2 0.5698158862824868 527 3 0.5464594744971136 50 4 0.6213130048723088 58 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - > > > > 2 . Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations . Could authors please report how often this happens ? This occurs very rarely . For example , on the development , only 58 examples reach the maximum of 4 iterations . > > > > 3.In order to estimate how much modeling of attention in the encoder helps , it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder ( neither over question , nor over document ) . We attempted a model without attention , in which the encoder first ingests the question then the document . The decoder then points over the document encoding as usual . The best validation performance we obtained for this model is 41.9 % F1 and 33.3 % exact match . > > > > 4.I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning ( table 3 in SQuAD paper ) . This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required . I emphasize with your position that we analyze the model with respect to the reasoning required to gain more insights into what it is doing . In particular , I would be very interested in seeing how each top model submitted to SQuAD differs in this regard . That said , we contacted Rajpurkar et al , the original authors of the SQuAD paper , who declined to share the 200 sampled used to estimate the statistics in Table 3 of the SQuAD paper . In particular , Pranav offered that releasing the 192 examples would have very little value of information of tuning and comparing models . That said , they are considering including in the next iteration of the dataset a more thorough analysis for the purposes of analyzing the strengths and weaknesses of each model . We will update our results when the dataset is updated . > > > > 5.In Wang and Jiang ( 2016 ) , the attention is predicted over question for each word in the document . But in table 2 , when performing ablation study to make the proposed model similar to Wang and Jiang , C^D is set to C^Q . But isn \u2019 t C^Q attention over document for each word in the question ? So , how is this similar to Wang and Jiang \u2019 s attention ? I think QA^D will be similar to Wang and Jiang 's attention since QA^D is attention over question for each word in the document . Please clarify . You are correct , this is indeed a typo ( it \u2019 s suppose to be QA^D as opposed to C^Q ) . > > > > 6.In section 2.1 , \u201c n \u201d and \u201c m \u201d are swapped when explaining the Document and Question encoding matrix . Please fix it . Thanks ! We will fix it !"}, "1": {"review_id": "rJeKjwvclx-1", "review_text": " Paper Summary: The paper introduces a question answering model called Dynamic Coattention Network (DCN). It extracts co-dependent representations of the document and question, and then uses an iterative dynamic pointing decoder to predict an answer span. The proposed model achieves state-of-the-art performance, outperforming all published models. Paper Strengths: -- The proposed model introduces two new concepts to QA models -- 1) using attention in both directions, and 2) a dynamic decoder which iterates over multiple answer spans until convergence or maximum number of iterations. -- The paper also presents ablation study of the proposed model which shows the importance of their design choices. -- It is interesting to see the same idea of co-attention performing well in 2 different domains -- Visual Question Answering and machine reading comprehension. -- The performance breakdown over document and question lengths (Figure 6) strengthens the importance of attention for QA task. -- The proposed model achieves state-of-the-art result on SQuAD dataset. -- The model architecture has been clearly described. Paper Weaknesses / Future Thoughts: -- The paper provides model's performance when the maximum number of iterations is 1 and 4. I would like to see how the performance of the model changes with the number of iterations, i.e., the model performance when that number is 2 and 3. Is there a clear trend? What type of questions is the model able to get correct with more iterations? -- As with many deep learning approaches, the overall architecture seems quite complex, and the design choices seem to be driven by performance numbers. As future work, authors might try to analyze qualitative advantages of different choices in the proposed model. What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction, when using Maxout Highway Network instead of a simple MLP, etc? Preliminary Evaluation: Novel and state-of-the-art question answering approach. Model is clearly described in detail. In my thoughts, a clear accept.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for your thoughtful review . > > > > I would like to see how the performance of the model changes with the number of iterations , i.e. , the model performance when that number is 2 and 3 . Is there a clear trend ? Yes , the model consistently performs better with with more iterations , with diminishing returns as the number of iteration increases . With more iterations , the model performs better on examples in which there exists multiple potential answers ( e.g.multiple people for a `` who question '' ) . > > > > As future work , authors might try to analyze qualitative advantages of different choices in the proposed model . What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction , when using Maxout Highway Network instead of a simple MLP , etc ? We agree , we will analyze the advantages of different choices in the proposed model across different question types . Also as suggested by Reviewer 1 , we think it would be interesting to analyze the model ( e.g.across encoder types , decoder types , and iteration counts ) with respect to the type of reasoning involved once the dataset is updated with such information ."}, "2": {"review_id": "rJeKjwvclx-2", "review_text": "This paper proposed a dynamic coattention network for the question answering task with long contextual documents. The model is able to encode co-dependent representations of the question and the document, and a dynamic decoder iteratively pointing the potential answer spans to locate the final answer. Overall, this is a well-written paper. Although the model is a bit complicated (coattention encoder, iterative dynamic pointering decoder and highway maxout network), the intuitions behind and the details of the model are clearly presented. Also the performance on the SQuAD dataset is good. I would recommend this paper to be accepted. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for your thoughtful review ."}}