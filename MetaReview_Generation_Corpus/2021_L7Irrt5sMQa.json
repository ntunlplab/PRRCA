{"year": "2021", "forum": "L7Irrt5sMQa", "title": "The Surprising Power of Graph Neural Networks with Random Node Initialization", "decision": "Reject", "meta_review": "In this paper, the authors show the effect of RNI on the expressive power of GNN for the first time, where the RNI was initially proposed in Sato et al. 2020. Overall, I like the idea of random node initialization because it is simple, effective, and theoretically well-founded. The key concern was that the novelty over the Sato's paper and the reviewers were still not convinced by the response. Therefore, the paper is still below the acceptance threshold.  I strongly encourage authors to revise the paper based on the reviewer's comments and resubmit it to a future venue. \n", "reviews": [{"review_id": "L7Irrt5sMQa-0", "review_text": "* * Post Rebuttal * * I thank the authors for the quick replies and updates to the paper . I keep my positive score . * * Summary of Contributions * * The paper analyzes the model of Random features in GNNs as suggested by Sato et.al. , in the paper called RNI . A result proving the universality of the RNI framework is introduced , a first of its kind in low tensor degree GNNs . To evaluate the expressiveness of RNI and other more expressive GNNs , the authors design two datasets wich require 2-WL distinguishing power ( which is higher than the ones MPNNs have ) * * Strengths * * - Novelty - The universality result on RNI is novel and further extends the hints of improved expressiveness explored by Sato et.al . - Dataset design - the design of new datasets for expressiveness discrimination are an important contribution to the community . * * Weaknesses * * - Regarding invariance of RNI - a more rigorous explanation would be fit there . Why does RNI preserve invariance ? - The main theoretical result of the paper is just appearing in the paper without details and intuitions towards the proof , a proof sketch or some discussion of that flavor would make the result more clear . - The experimental setting is not clear enough as presented in the main body of the paper . A lot of important details has to be dug our from the appendix which is tedious for the reader . For example : 1 . The partial RNI is not well explained in the paper , and it was not clear whether the partial applies to the feature dimensions or the nodes . 2.The input features in the designed datasets are not mentioned in the body of the paper . 3.An elaborate description of the 3-GCN variant is missing . - * Partial RNI * - can the authors provide an intuition as to why it works ? In a way , the universality comes from the network not taking into account the node features due to the full randomness but more of a statistical behavior and the fact that the nodes are completely distinguishable . So why does partial randomness work ? * * Recommendation * * The paper states an important and surprising result which can contribute greatly to the graph learning community . A good paper , Accept .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the constructive feedback , and respond to the comments below : - * * Invariance of RNI : * * RNI introduces continuous node features which are re-sampled at every iteration of training or evaluation . These features , by construction , vary around a mean value and , in expectation , this mean value is what the GNN will rely on for predictions . However , the variability between different samples , and in particular that of a random sample relative to its mean , enables graph discrimination and improves expressiveness . Hence , in expectation , RNI preserves invariance , as all samples ultimately converge on average to a single value that can be sampled , whereas variance is leveraged for increased expressiveness . We clarified this further in the highlighted part of the paragraph preceding Theorem 4.1 ( Page 4 ) . - * * Proof Intuitions : * * We have provided additional intuitions for the proof in the main body of the paper with a new paragraph in Page 5 after Theorem 4.1 . Our result extends Barcelo et al . 's result , showing that the class of functions captured by MPNNs ( or ACR-GNNs ) are precisely those in $ \\text { C } ^2 $ . The intuition behind our proof is to use RNI in order to create an implicit total order over graph nodes , and make use of the fact that such ordered graphs can always be distinguished . The ordered representation is produced with a probability $ ( 1 - \\delta ) $ , where $ \\delta $ affects embedding width , and based on this representation , we show that any function over graphs can be learned , by proving the result for the class of Boolean functions , and then lifting them to the general case . - * * Experimental Setup : * * Thanks for pointing this out : ( 1 ) Partial RNI : We acknowledge that this is not clearly stated in the paper , and have explicitly and unambiguously mentioned this in the paper , particularly in the experimental setup section presenting GCN-xRNI ( Page 6 ) . The partial RNI indeed applies to feature dimensions , i.e. , a 50-dimensional node embedding with 50 % RNI has 25 random dimensions , and 25 deterministic dimensions , as has also been done in Sato et al . ( 2 ) Features : We also explained the features in the experimental setup ( Page 6 ) , as part of the GCN-xRNI description . ( 3 ) 3-GCN description : This was shortened in the submitted version due to space constraints . We have now extended the model description in the experimental setup . - * * Partial RNI Intuition : * * Let us start by noting that our universality result also works with partial RNI . In fact , it already holds with only * one * randomized dimension ( now mentioned in Page 5 ) . That is , additional deterministic features do not affect the universality result as long as there is a single randomized dimension , since the deterministic dimensions can be simply concatenated without changing any of the derived probabilities in Theorem 4.1 . So , it is theoretically quite plausible for partial RNI to do well . The fact that it tends to do better in practice is discussed in the experimental section the paper : Partial RNI somehow achieves a 'best-of-both-worlds ' scenario , where the GNN has access to both deterministic node features ( with better inductive bias ) which are informative for the prediction task , and random features to improve its expressiveness . Practically , partial RNI achieves the same benefits as full RNI , in that it enables node distinguishability , and this highlights that little randomization is practically needed to improve MPNNs . Hence , partial RNI supplements the power of random features with informative deterministic features , which lends itself to better prediction performance , particularly on CEXP . [ 1 ] Barcelo ' et al . `` The Logical Expressiveness of Graph Neural Networks '' , ICLR 2020 . [ 2 ] Sato et al . `` Random Features Strengthen Graph Neural Networks . `` , arXiv 2002.03155"}, {"review_id": "L7Irrt5sMQa-1", "review_text": "The paper studies the how random initialization of node states can improve the expressivity of message passing graph neural networks . Theoretically the paper shows that RNI makes MPNNs universal approximators for invariant functions over graphs . To supplement this claim , the authors evaluate GNNs with RNI and higher-order GNNs over a carefully constructed synthetic dataset and show that RNI ( even if only a fraction of the nodes are randomly initialized ) are as expressive as higher order GNNs . They also highlight some drawbacks of this approach , such as slower training and sensitivity to hyperparameter tuning . Overall , I find the topic and the findings of the paper quite interesting . However , explanations as to why or how randomness helps is missing in the paper . There is also room for improvement in the presentation and writing . The statement of Theorem 4.1 is a little vague ; it would help if its made more precise ( e.g. , how much randomness is required , what is the depth of the MPNN required , the state dimensions etc . ) . The notion of invariance and equivariance can also be defined much earlier in the paper . While it \u2019 s interesting that randomness can yield MPNNs to be universal approximators , could you please give some intuition as to what makes randomness so essential ? In the proof of Theorem 4.1 ( in Appendix A.2 ) , the random initialization together with the linearized sigmoid activation function is used to show that the vectors $ x_i^ ( 1 ) $ are mutually distinct { 0 , 1 } vectors with high probability . To my understanding , this is the only place where the randomness of the initialization is used . If that is the case , why can \u2019 t any deterministic initialization that ensures mutually distinct { 0 , 1 } vectors for $ x_i^ ( 1 ) $ suffice ? The datasets EXP and CEXP seem to be motivated as graph encoding of SAT problems . Are they bipartite graphs ? Could you give a sense for how the graphs look like ( number of nodes , edges etc . ) ? In the partial random node initialization ( GCN-x % RNI ) , are you initializing some nodes randomly and other deterministically ? Or are you initializing some entries in the state vector randomly and remaining entries deterministically ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the constructive feedback , and address the concerns below : - * * Theorem : * * We now explicitly mention the required theoretical width , shown in the appendix , in the main body of the paper ( Page 5 ) . In terms of GNN depth , our result has the same required depth as that of Barcelo et al.We also now provide intuitions , and a proof sketch to better clarify the role that RNI plays after the theorem statement ( Page 5 ) . The intuition behind our proof is that RNI can implicitly induce a total order over graph nodes , which in turn , makes these graphs distinguishable . Since this representation is produced with a probability $ ( 1 - \\delta ) $ , we obtain the approximability result given in the paper . Please also refer to our answer to Reviewers 4 and 2 . - * * Why not deterministic extra features ? * * The fundamental motivation for randomization over deterministic extra features is generalizability . More specifically , adding deterministic distinguishing features introduces redundant information which plays no part in prediction , and this itself leads to overfitting when training a GNN . Indeed , we have experimented with such a deterministic setup , where we set the same added features for every graph ( these were generated randomly , but were preserved for every run ) , but this yielded very poor performance , albeit marginally better than 50 % in terms of accuracy . By contrast , the results that we report with repeated randomization show near-perfect performance , and we believe this is due to the GNN not fitting to the randomized dimensions due to their variability , and thus developing a robustness to these added features , which is not possible with deterministic features . - * * The properties of the datasets EXP/CEXP : * * As both datasets consist of SAT formulas , they consist of nodes representing literals and disjunctions , such that literals that are negations of one another are connected , and disjunction nodes are connected to literal nodes for all literals that appear in the disjunction . Hence , although CEXP/EXP are planar , they are not bipartite , due the inter-literal edges . In terms of # nodes , # edges , the instance sizes are small and typically do not exceed 70 nodes , and the number of edges would be roughly the number of instance variables ( 12-16 ) plus disjunction connections ( at most 5 per disjunction by construction ) . Further details of these datasets , including the generation process , are provided in the appendix of the paper . - * * Partial RNI : * * Our partial randomization is over node features , and not nodes themselves , so the latter option . That is , partial randomization implies that every node only has a fraction of its dimensions randomized ( as is the case in Sato et al . ) , and not that a subset of nodes is fully randomized . We acknowledge that this was ambiguously written in the paper , and have clarified this in the updated version . [ 1 ] Sato et al . `` Random Features Strengthen Graph Neural Networks . '' arXiv 2002.03155 [ 2 ] Barcelo ' et al . `` The Logical Expressiveness of Graph Neural Networks '' , ICLR 2020 ."}, {"review_id": "L7Irrt5sMQa-2", "review_text": "This paper studies the power of message passing neural networks ( MPNNs ) with random node initialization ( RNI ) . Although the power of standard MPNNs is limited to 1-WL , the main result of the paper is to prove that RNI makes MPNNs universal . The paper also introduces two graph classification datasets where each graph is a SAT problem and the label is the satisfiability/unsatisfiability . The datasets have been created in a way that their graphs are 1-WL indistinguishable to serve as a test-bed for graph neural networks with power beyond 1-WL . The results on these two datasets show the merit of adding RNI to MPNNs . My biggest reservation with this work is that the theoretical and empirical results do not seem to offer much more than what has already been provided in [ 1 ] . In fact , if [ 1 ] did n't exist , I would have given a strong acceptance to this paper . I \u2019 ll elaborate on each of these aspects below : Theoretical result : While Theorem 4.1 is quite strong as it imposes no restrictions on the function f , two things are not clear to me : 1- What classes of functions does the Theorem 1 of [ 1 ] not cover that Theorem 4.1 of this paper does ? , 2- Is the Theorem 4.1 of this paper a straightforward extension of the Theorem 1 in [ 1 ] ? Empirical results : While I appreciate the two datasets developed in this work and I believe they can be useful for future research , I have a hard time understanding what intuition/insights the results provide that has not been already provided in [ 1 ] . It has been already shown in [ 1 ] that RNI increases the power of MPNNs and enables them to do well on graphs that are 1-WL indistinguishable , where existing MPNNs fail . If Theorem 4.1 covers a larger class of models than Theorem 1 of [ 1 ] , then I would expect at least some experiments covering those classes of models . Right now , the only added insight of the empirical results seems to be that randomizing a subset of the nodes is better than randomizing all of them . All mentioned about the connection between this work and [ 1 ] is \u201c Indeed , RNI has enabled MPNNs to distinguish instances that 1-WL can not distinguish , and is proven to enable better approximation of a class of combinatorial problems ( Sato et al. , 2020 ) . However , the effect of RNI on the expressive power of GNNs has not yet been comprehensively studied , and its impact on the inductive capacity and learning ability of GNNs remains unclear. \u201d , which does not seem satisfactory . Other comments/questions : 1- Some experiments on standard benchmarks can strengthen the paper . 2- For GCN-x % RNI , how do you initialize the other ( 1-x ) % of the nodes ? Do you initialize them as all 0s ? 3- In [ 1 ] , a new random feature is assigned every time the procedure is called . Is this what you do as well ? 4- Any reason why the results of 1-GCN and GCN-x % RNI are not reported in Table 1 ? [ 1 ] Sato , Ryoma , Makoto Yamada , and Hisashi Kashima . `` Random Features Strengthen Graph Neural Networks . '' arXiv preprint arXiv:2002.03155 ( 2020 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments , and address the main points below : * * Contribution relative to Sato et al . : * * Please note that the question of theoretically quantifying the impact of RNI on GNN expressiveness remained open ; quoting from Sato et al . : ( i ) `` Although this heuristic seems to work well , it is not trivial whether adding a random feature can theoretically strengthen the capability of GNNs '' , and ( ii ) `` We prove that the addition of random features indeed improves the theoretical capability of GNNs * in terms of the approximation ratios * '' . Sato et al.only addresses the theoretical power of RNI in the limited context of approximation ratios for some combinatorial problems , based on analogies with existing algorithms , which have known approximations , showing that RNI can help detect local structures in an input graph with high probability . This clearly leaves several questions open , including all functions beyond substructure detection and all functions which do n't have an approximation . Our result shows the universality of GNNs with RNI in the general case , and is hence a strict generalisation of Sato et al . * * Responses to the specific questions : * * ( 1 ) Sato et al.only addresses the theoretical power of RNI for some combinatorial problems , and anything beyond is not captured . These results are algorithmic ( in the sense of algorithmic alignment ) , and hence limited to functions which have a known approximation . The problem used in our experiments , i.e. , SAT solving is one example that is not captured by Sato et al.Indeed , SAT is NP-complete even on our planar instances , is hard to approximate , and does not rely on fixed local structures . Essentially , our result addresses any real-valued function over graphs , and shows it to be theoretically learnable , and is much broader , and provides a more complete picture on the role of RNI . ( 2 ) Our result is by no means a straightforward extension to Theorem 1 of Sato et al. , as it proves a much more general statement . We prove a universality result , based on a logical characterization of the power of GNNs with RNI with a descriptive complexity argument . By contrast , Sato et al.builds on specific problem settings , and aligns its GNN models with the specific problem setting to yield its results , so their approach , can not be extended to yield universality , as several problems , either 1 ) do not admit approximation results to be adapted into the settings , or 2 ) do not rely on fixed local substructures , or both . * * Empirical results compared to Sato et al . : * * Our datasets are based on the SAT problem , where neither local structures are useful nor approximation schemes are available . Hence , our experiments evaluate GNNs with RNI on a significantly more challenging problem setting . We also address MPNNs , but we also go beyond this and compare with more powerful models , namely 3-GCNs , to obtain better insights on the practical gains stemming from RNI . Hence , we provide several empirical insights in addition to the insights given by partial randomization that , which we summarize below : ( a ) Going beyond Sato et al , we show that RNI is also beneficial in a challenging setting not amenable to approximation or local structures . ( b ) We compare MPNNs with RNI with higher-order GNNs to better quantify the impact of RNI , and show that MPNNs with RNI are competitive with these-order models . That is , we do n't only show that MPNNs with RNI outperform MPNNs , but also that MPNNs with RNI can even surpass higher-order GNN models . We are not aware of any work that quantifies the performance of higher-order GNN models on datasets specifically dedicated to test expressivity . ( c ) We conduct a careful study based on convergence of different models , and illustrate that randomization slows down convergence . Additionally , we give concrete evidence that a partial randomization regime can alleviate this problem . We made all these points clearer in the related work section ( Page 4 ) , to better highlight the stronger nature of our results . * * Other comments : * * ( 1 ) Sato et al.perform such a study , and show the benefit of randomization , using an analogous randomization regime as our own , on real-world datasets . ( 2 ) Partial RNI is at node feature level , as in Sato et al. , and not at node level . We have addressed this ambiguity in the updated version of the paper . ( 3 ) Yes , we follow the same randomization strategy . ( 4 ) The result of 1-GCN is 50 % , as it can not distinguish between EXP graph pairs , whereas the results of partial RNI systems are highly similar , and even slightly better , than those of full RNI , and are plotted in Figure 2 ."}, {"review_id": "L7Irrt5sMQa-3", "review_text": "The paper study the effects of adding random features ( RF ) to graph neural networks ( GNN ) . First , it is shown that , quite surprisingly , adding random features makes GNN universal approximators of invariant functions . Next , a novel dataset is defined that is aimed at evaluating the performance of models that have high expressive power . Finally , several experiments show that adding RF performs well on the proposed dataset . I think the paper is well written and well organized . The theoretical aspect seems novel and quite surprising , especially since it shows that adding RF makes GNN more expressive than k-GCN ( or k-IGN ) for any k. The new proposed dataset is interesting , and the experimental result looks promising , especially since RF performs quite well compared to 3-GCN while having much fewer parameters . Also , the observation that only partial randomness can already be beneficial is interesting . I have a couple of concerns regarding the paper for which I would be happy to see the authors \u2019 comment : 1 ) The phrasing of Theorem 4.1 is a bit vague because it is stated with an \\epsilon , \\delta approximation while there is no clear explanation of how these parameters affect the theorem . I suppose that smaller \\epsilon , \\delta would mean a wider network , but this dependence should be shown explicitly . For \\delta , this dependence is somewhat shown in the proof of Lemma A.5 , but for \\epsilon it is very unclear and is probably related to Lemma A.2 which cited from another paper . 2 ) If I understand the proof correctly , the width of the network in Lemma A.1 ( and also in Theorem 4.1 ) should be super-exponential , no matter what is the target function . That is because in Lemma A.4 the sentence takes into account every graph in G_ { n , k } , the number of such graphs is super-exponential and I suppose that the width of the GNN that realizes the sentence depends on the length of the sentence ( or at least the number of literals ) . If this is true , then I think it is important to point that out in the main text . 3 ) Continuing the previous point , the paragraph after Remark 1 isn \u2019 t clear : why the construction is adaptive to the descriptive complexity of the target function if for any target function the construction requires memorization of all graphs with n nodes ? 4 ) Regarding the Experimental results , I think it is important to also test RF on real datasets . The reason is that without this experiment it is not clear if adding RF doesn \u2019 t actually harm the performance of GNN and thus are actually very impractical . As the authors stated , I don \u2019 t expect RF to significantly improve performance on real datasets because high expressivity might not be required , but I am concerned that it will hurt the performance . I also highly suggest uploading the EXP and CEXP datasets as supplementary material , since these are newly generated datasets , and this way other people could also experiment on them and compare performance . To conclude , I think this is a very nice and well-written paper that adds a novel view on adding RF to GNN , both theoretically and empirically . With that said , there are some issues with the vague form in which the main theorem is stated , and experiments on real datasets would help clarify whether adding RF could actually prove helpful ( or at least not harmful ) for practical uses .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the insightful feedback and points . We are very pleased that you find our paper interesting : Our results indeed support the viability of RF as an efficient alternative to higher-order models . We address your concerns below : 1 . Lemma A.1 ( Boolean functions ) and Theorem 4.1 ( general functions ) have the following implications : ( i ) $ \\delta $ : The confidence parameter $ \\delta $ affects embedding dimensionality ( i.e. , GNN width ) : In the proof of Lemma A.1 , we show that the needed dimensionality is $ O ( n^2 \\delta^ { -1 } ) $ , where n is the maximal number of graph nodes . Therefore , it is correct that lower $ \\delta $ implies higher GNN width . ( ii ) $ \\epsilon $ : The error $ \\epsilon $ is only used in defining a universal readout function , and is needed in our ( and , in Barcelo et al . 's ) construction solely for this reason . Specifically , it thresholds the error of the GNN when making approximations of the target function , defining a universal readout function ( e.g. , MLP ) following the GNN to enable $ \\epsilon $ error relative to the target . Thus , $ \\epsilon $ has no direct effect on the width or depth of the GNN . ( iii ) GNN depth & width : Barcelo et al.show that the depth and width required to learn a function over graphs depends on the quantifier depth of the target logical sentence . The depth is the same in our case , and as for the width , we need to choose `` the dimension of the state vectors in such a way that it is greater than $ c \u22c5 n^2 $ and is at least as large as the dimension of the state vectors of $ \\mathcal { N } _f $ '' ( Proof of Lemma A.1 ) . 2.As stated in point 1 , the GNN width and depth required for learning a function rely on the complexity of the target sentence and hence the target function . We distinguish Lemma A.1 ( Boolean functions ) and Theorem 4.1 ( general functions ) in our response : * * Lemma A.1 . * * In Lemma A.4 , we show that any Boolean function over the set of graphs can be represented as a formula in $ \\ \\text { C } ^2 $ , and thus that MPNNs with RF can learn any Boolean function over graphs . As the reviewer correctly points , the construction can potentially yield an exponentially large sentence , but this is not necessarily the case , since this depends tightly on the target function h. There are two main reasons for this : ( a ) the target function h may require only a sentence that uses at most polynomially many disjuncts in our construction ( i.e. , the set H may be of polynomial size ) , and ( b ) the target function h can be captured by a logically equivalent ( but much shorter ) sentence than the one given in our construction . That is , our proof relies on the existence of a $ \\text { C } ^2 $ formula , but does not make any specific assumptions based on the formula derived in Lemma A.4 . Hence , our construction is very adaptive and tightly linked to the descriptive complexity of the function we want to approximate , and it is easy to see that much more efficient constructions can be given for a more restricted class of functions . As we state in Remark 1 , this deserves a more thorough investigation , and an important direction for future work is to understand , e.g. , the class of functions that can be approximated via poly sized GNN . * * Theorem 4.1 . * * While lifting the result from Boolean functions to the general case , we rely on exponentially many dimensions , and this blow-up seems unavoidable in the general case , where we put no restrictions on the class of functions . The adaptive nature of our constructions also has implications on Theorem 4.1 , yielding better bounds . For instance , if the target function has a compact range , then we can cover the domain with a bounded number of points , which only depends on the desired error epsilon . We have now explicitly introduced the bound on width , and provided explanations of all these points , following Theorem 4.1 ( Page 5 ) . 3.As described in point 2 , the construction aims to show the existence of a $ \\text { C } ^2 $ sentence capturing the target function . It is possible , and quite plausible , to characterize the target function in terms of more succinct sentences , once we restrict our attention to a less general class of functions . This subtlety is the reason why the overall idea is not based on memorization of individual graphs , but rather on the complexity of the sentences being learned . 4.Sato et al.investigate partial RF on real-world datasets , and show that GNNs supplemented with partial RFs perform comparably , if not marginally better , on standard datasets MUTAG , NCI1 , and PROTEINS . This means that partial RF does not seem to hurt performance on real-world datasets . This confirms the intuition about the impact of RF we present in the paper , and further justifies the need for datasets EXP and CEXP . * * CEXP/EXP * * : We share the datasets in the supplementary material . [ 1 ] Barcelo ' et al . `` The Logical Expressiveness of Graph Neural Networks '' , ICLR 2020 . [ 2 ] Sato et al . `` Random Features Strengthen Graph Neural Networks . `` , arXiv 2002.03155"}], "0": {"review_id": "L7Irrt5sMQa-0", "review_text": "* * Post Rebuttal * * I thank the authors for the quick replies and updates to the paper . I keep my positive score . * * Summary of Contributions * * The paper analyzes the model of Random features in GNNs as suggested by Sato et.al. , in the paper called RNI . A result proving the universality of the RNI framework is introduced , a first of its kind in low tensor degree GNNs . To evaluate the expressiveness of RNI and other more expressive GNNs , the authors design two datasets wich require 2-WL distinguishing power ( which is higher than the ones MPNNs have ) * * Strengths * * - Novelty - The universality result on RNI is novel and further extends the hints of improved expressiveness explored by Sato et.al . - Dataset design - the design of new datasets for expressiveness discrimination are an important contribution to the community . * * Weaknesses * * - Regarding invariance of RNI - a more rigorous explanation would be fit there . Why does RNI preserve invariance ? - The main theoretical result of the paper is just appearing in the paper without details and intuitions towards the proof , a proof sketch or some discussion of that flavor would make the result more clear . - The experimental setting is not clear enough as presented in the main body of the paper . A lot of important details has to be dug our from the appendix which is tedious for the reader . For example : 1 . The partial RNI is not well explained in the paper , and it was not clear whether the partial applies to the feature dimensions or the nodes . 2.The input features in the designed datasets are not mentioned in the body of the paper . 3.An elaborate description of the 3-GCN variant is missing . - * Partial RNI * - can the authors provide an intuition as to why it works ? In a way , the universality comes from the network not taking into account the node features due to the full randomness but more of a statistical behavior and the fact that the nodes are completely distinguishable . So why does partial randomness work ? * * Recommendation * * The paper states an important and surprising result which can contribute greatly to the graph learning community . A good paper , Accept .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the constructive feedback , and respond to the comments below : - * * Invariance of RNI : * * RNI introduces continuous node features which are re-sampled at every iteration of training or evaluation . These features , by construction , vary around a mean value and , in expectation , this mean value is what the GNN will rely on for predictions . However , the variability between different samples , and in particular that of a random sample relative to its mean , enables graph discrimination and improves expressiveness . Hence , in expectation , RNI preserves invariance , as all samples ultimately converge on average to a single value that can be sampled , whereas variance is leveraged for increased expressiveness . We clarified this further in the highlighted part of the paragraph preceding Theorem 4.1 ( Page 4 ) . - * * Proof Intuitions : * * We have provided additional intuitions for the proof in the main body of the paper with a new paragraph in Page 5 after Theorem 4.1 . Our result extends Barcelo et al . 's result , showing that the class of functions captured by MPNNs ( or ACR-GNNs ) are precisely those in $ \\text { C } ^2 $ . The intuition behind our proof is to use RNI in order to create an implicit total order over graph nodes , and make use of the fact that such ordered graphs can always be distinguished . The ordered representation is produced with a probability $ ( 1 - \\delta ) $ , where $ \\delta $ affects embedding width , and based on this representation , we show that any function over graphs can be learned , by proving the result for the class of Boolean functions , and then lifting them to the general case . - * * Experimental Setup : * * Thanks for pointing this out : ( 1 ) Partial RNI : We acknowledge that this is not clearly stated in the paper , and have explicitly and unambiguously mentioned this in the paper , particularly in the experimental setup section presenting GCN-xRNI ( Page 6 ) . The partial RNI indeed applies to feature dimensions , i.e. , a 50-dimensional node embedding with 50 % RNI has 25 random dimensions , and 25 deterministic dimensions , as has also been done in Sato et al . ( 2 ) Features : We also explained the features in the experimental setup ( Page 6 ) , as part of the GCN-xRNI description . ( 3 ) 3-GCN description : This was shortened in the submitted version due to space constraints . We have now extended the model description in the experimental setup . - * * Partial RNI Intuition : * * Let us start by noting that our universality result also works with partial RNI . In fact , it already holds with only * one * randomized dimension ( now mentioned in Page 5 ) . That is , additional deterministic features do not affect the universality result as long as there is a single randomized dimension , since the deterministic dimensions can be simply concatenated without changing any of the derived probabilities in Theorem 4.1 . So , it is theoretically quite plausible for partial RNI to do well . The fact that it tends to do better in practice is discussed in the experimental section the paper : Partial RNI somehow achieves a 'best-of-both-worlds ' scenario , where the GNN has access to both deterministic node features ( with better inductive bias ) which are informative for the prediction task , and random features to improve its expressiveness . Practically , partial RNI achieves the same benefits as full RNI , in that it enables node distinguishability , and this highlights that little randomization is practically needed to improve MPNNs . Hence , partial RNI supplements the power of random features with informative deterministic features , which lends itself to better prediction performance , particularly on CEXP . [ 1 ] Barcelo ' et al . `` The Logical Expressiveness of Graph Neural Networks '' , ICLR 2020 . [ 2 ] Sato et al . `` Random Features Strengthen Graph Neural Networks . `` , arXiv 2002.03155"}, "1": {"review_id": "L7Irrt5sMQa-1", "review_text": "The paper studies the how random initialization of node states can improve the expressivity of message passing graph neural networks . Theoretically the paper shows that RNI makes MPNNs universal approximators for invariant functions over graphs . To supplement this claim , the authors evaluate GNNs with RNI and higher-order GNNs over a carefully constructed synthetic dataset and show that RNI ( even if only a fraction of the nodes are randomly initialized ) are as expressive as higher order GNNs . They also highlight some drawbacks of this approach , such as slower training and sensitivity to hyperparameter tuning . Overall , I find the topic and the findings of the paper quite interesting . However , explanations as to why or how randomness helps is missing in the paper . There is also room for improvement in the presentation and writing . The statement of Theorem 4.1 is a little vague ; it would help if its made more precise ( e.g. , how much randomness is required , what is the depth of the MPNN required , the state dimensions etc . ) . The notion of invariance and equivariance can also be defined much earlier in the paper . While it \u2019 s interesting that randomness can yield MPNNs to be universal approximators , could you please give some intuition as to what makes randomness so essential ? In the proof of Theorem 4.1 ( in Appendix A.2 ) , the random initialization together with the linearized sigmoid activation function is used to show that the vectors $ x_i^ ( 1 ) $ are mutually distinct { 0 , 1 } vectors with high probability . To my understanding , this is the only place where the randomness of the initialization is used . If that is the case , why can \u2019 t any deterministic initialization that ensures mutually distinct { 0 , 1 } vectors for $ x_i^ ( 1 ) $ suffice ? The datasets EXP and CEXP seem to be motivated as graph encoding of SAT problems . Are they bipartite graphs ? Could you give a sense for how the graphs look like ( number of nodes , edges etc . ) ? In the partial random node initialization ( GCN-x % RNI ) , are you initializing some nodes randomly and other deterministically ? Or are you initializing some entries in the state vector randomly and remaining entries deterministically ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the constructive feedback , and address the concerns below : - * * Theorem : * * We now explicitly mention the required theoretical width , shown in the appendix , in the main body of the paper ( Page 5 ) . In terms of GNN depth , our result has the same required depth as that of Barcelo et al.We also now provide intuitions , and a proof sketch to better clarify the role that RNI plays after the theorem statement ( Page 5 ) . The intuition behind our proof is that RNI can implicitly induce a total order over graph nodes , which in turn , makes these graphs distinguishable . Since this representation is produced with a probability $ ( 1 - \\delta ) $ , we obtain the approximability result given in the paper . Please also refer to our answer to Reviewers 4 and 2 . - * * Why not deterministic extra features ? * * The fundamental motivation for randomization over deterministic extra features is generalizability . More specifically , adding deterministic distinguishing features introduces redundant information which plays no part in prediction , and this itself leads to overfitting when training a GNN . Indeed , we have experimented with such a deterministic setup , where we set the same added features for every graph ( these were generated randomly , but were preserved for every run ) , but this yielded very poor performance , albeit marginally better than 50 % in terms of accuracy . By contrast , the results that we report with repeated randomization show near-perfect performance , and we believe this is due to the GNN not fitting to the randomized dimensions due to their variability , and thus developing a robustness to these added features , which is not possible with deterministic features . - * * The properties of the datasets EXP/CEXP : * * As both datasets consist of SAT formulas , they consist of nodes representing literals and disjunctions , such that literals that are negations of one another are connected , and disjunction nodes are connected to literal nodes for all literals that appear in the disjunction . Hence , although CEXP/EXP are planar , they are not bipartite , due the inter-literal edges . In terms of # nodes , # edges , the instance sizes are small and typically do not exceed 70 nodes , and the number of edges would be roughly the number of instance variables ( 12-16 ) plus disjunction connections ( at most 5 per disjunction by construction ) . Further details of these datasets , including the generation process , are provided in the appendix of the paper . - * * Partial RNI : * * Our partial randomization is over node features , and not nodes themselves , so the latter option . That is , partial randomization implies that every node only has a fraction of its dimensions randomized ( as is the case in Sato et al . ) , and not that a subset of nodes is fully randomized . We acknowledge that this was ambiguously written in the paper , and have clarified this in the updated version . [ 1 ] Sato et al . `` Random Features Strengthen Graph Neural Networks . '' arXiv 2002.03155 [ 2 ] Barcelo ' et al . `` The Logical Expressiveness of Graph Neural Networks '' , ICLR 2020 ."}, "2": {"review_id": "L7Irrt5sMQa-2", "review_text": "This paper studies the power of message passing neural networks ( MPNNs ) with random node initialization ( RNI ) . Although the power of standard MPNNs is limited to 1-WL , the main result of the paper is to prove that RNI makes MPNNs universal . The paper also introduces two graph classification datasets where each graph is a SAT problem and the label is the satisfiability/unsatisfiability . The datasets have been created in a way that their graphs are 1-WL indistinguishable to serve as a test-bed for graph neural networks with power beyond 1-WL . The results on these two datasets show the merit of adding RNI to MPNNs . My biggest reservation with this work is that the theoretical and empirical results do not seem to offer much more than what has already been provided in [ 1 ] . In fact , if [ 1 ] did n't exist , I would have given a strong acceptance to this paper . I \u2019 ll elaborate on each of these aspects below : Theoretical result : While Theorem 4.1 is quite strong as it imposes no restrictions on the function f , two things are not clear to me : 1- What classes of functions does the Theorem 1 of [ 1 ] not cover that Theorem 4.1 of this paper does ? , 2- Is the Theorem 4.1 of this paper a straightforward extension of the Theorem 1 in [ 1 ] ? Empirical results : While I appreciate the two datasets developed in this work and I believe they can be useful for future research , I have a hard time understanding what intuition/insights the results provide that has not been already provided in [ 1 ] . It has been already shown in [ 1 ] that RNI increases the power of MPNNs and enables them to do well on graphs that are 1-WL indistinguishable , where existing MPNNs fail . If Theorem 4.1 covers a larger class of models than Theorem 1 of [ 1 ] , then I would expect at least some experiments covering those classes of models . Right now , the only added insight of the empirical results seems to be that randomizing a subset of the nodes is better than randomizing all of them . All mentioned about the connection between this work and [ 1 ] is \u201c Indeed , RNI has enabled MPNNs to distinguish instances that 1-WL can not distinguish , and is proven to enable better approximation of a class of combinatorial problems ( Sato et al. , 2020 ) . However , the effect of RNI on the expressive power of GNNs has not yet been comprehensively studied , and its impact on the inductive capacity and learning ability of GNNs remains unclear. \u201d , which does not seem satisfactory . Other comments/questions : 1- Some experiments on standard benchmarks can strengthen the paper . 2- For GCN-x % RNI , how do you initialize the other ( 1-x ) % of the nodes ? Do you initialize them as all 0s ? 3- In [ 1 ] , a new random feature is assigned every time the procedure is called . Is this what you do as well ? 4- Any reason why the results of 1-GCN and GCN-x % RNI are not reported in Table 1 ? [ 1 ] Sato , Ryoma , Makoto Yamada , and Hisashi Kashima . `` Random Features Strengthen Graph Neural Networks . '' arXiv preprint arXiv:2002.03155 ( 2020 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the comments , and address the main points below : * * Contribution relative to Sato et al . : * * Please note that the question of theoretically quantifying the impact of RNI on GNN expressiveness remained open ; quoting from Sato et al . : ( i ) `` Although this heuristic seems to work well , it is not trivial whether adding a random feature can theoretically strengthen the capability of GNNs '' , and ( ii ) `` We prove that the addition of random features indeed improves the theoretical capability of GNNs * in terms of the approximation ratios * '' . Sato et al.only addresses the theoretical power of RNI in the limited context of approximation ratios for some combinatorial problems , based on analogies with existing algorithms , which have known approximations , showing that RNI can help detect local structures in an input graph with high probability . This clearly leaves several questions open , including all functions beyond substructure detection and all functions which do n't have an approximation . Our result shows the universality of GNNs with RNI in the general case , and is hence a strict generalisation of Sato et al . * * Responses to the specific questions : * * ( 1 ) Sato et al.only addresses the theoretical power of RNI for some combinatorial problems , and anything beyond is not captured . These results are algorithmic ( in the sense of algorithmic alignment ) , and hence limited to functions which have a known approximation . The problem used in our experiments , i.e. , SAT solving is one example that is not captured by Sato et al.Indeed , SAT is NP-complete even on our planar instances , is hard to approximate , and does not rely on fixed local structures . Essentially , our result addresses any real-valued function over graphs , and shows it to be theoretically learnable , and is much broader , and provides a more complete picture on the role of RNI . ( 2 ) Our result is by no means a straightforward extension to Theorem 1 of Sato et al. , as it proves a much more general statement . We prove a universality result , based on a logical characterization of the power of GNNs with RNI with a descriptive complexity argument . By contrast , Sato et al.builds on specific problem settings , and aligns its GNN models with the specific problem setting to yield its results , so their approach , can not be extended to yield universality , as several problems , either 1 ) do not admit approximation results to be adapted into the settings , or 2 ) do not rely on fixed local substructures , or both . * * Empirical results compared to Sato et al . : * * Our datasets are based on the SAT problem , where neither local structures are useful nor approximation schemes are available . Hence , our experiments evaluate GNNs with RNI on a significantly more challenging problem setting . We also address MPNNs , but we also go beyond this and compare with more powerful models , namely 3-GCNs , to obtain better insights on the practical gains stemming from RNI . Hence , we provide several empirical insights in addition to the insights given by partial randomization that , which we summarize below : ( a ) Going beyond Sato et al , we show that RNI is also beneficial in a challenging setting not amenable to approximation or local structures . ( b ) We compare MPNNs with RNI with higher-order GNNs to better quantify the impact of RNI , and show that MPNNs with RNI are competitive with these-order models . That is , we do n't only show that MPNNs with RNI outperform MPNNs , but also that MPNNs with RNI can even surpass higher-order GNN models . We are not aware of any work that quantifies the performance of higher-order GNN models on datasets specifically dedicated to test expressivity . ( c ) We conduct a careful study based on convergence of different models , and illustrate that randomization slows down convergence . Additionally , we give concrete evidence that a partial randomization regime can alleviate this problem . We made all these points clearer in the related work section ( Page 4 ) , to better highlight the stronger nature of our results . * * Other comments : * * ( 1 ) Sato et al.perform such a study , and show the benefit of randomization , using an analogous randomization regime as our own , on real-world datasets . ( 2 ) Partial RNI is at node feature level , as in Sato et al. , and not at node level . We have addressed this ambiguity in the updated version of the paper . ( 3 ) Yes , we follow the same randomization strategy . ( 4 ) The result of 1-GCN is 50 % , as it can not distinguish between EXP graph pairs , whereas the results of partial RNI systems are highly similar , and even slightly better , than those of full RNI , and are plotted in Figure 2 ."}, "3": {"review_id": "L7Irrt5sMQa-3", "review_text": "The paper study the effects of adding random features ( RF ) to graph neural networks ( GNN ) . First , it is shown that , quite surprisingly , adding random features makes GNN universal approximators of invariant functions . Next , a novel dataset is defined that is aimed at evaluating the performance of models that have high expressive power . Finally , several experiments show that adding RF performs well on the proposed dataset . I think the paper is well written and well organized . The theoretical aspect seems novel and quite surprising , especially since it shows that adding RF makes GNN more expressive than k-GCN ( or k-IGN ) for any k. The new proposed dataset is interesting , and the experimental result looks promising , especially since RF performs quite well compared to 3-GCN while having much fewer parameters . Also , the observation that only partial randomness can already be beneficial is interesting . I have a couple of concerns regarding the paper for which I would be happy to see the authors \u2019 comment : 1 ) The phrasing of Theorem 4.1 is a bit vague because it is stated with an \\epsilon , \\delta approximation while there is no clear explanation of how these parameters affect the theorem . I suppose that smaller \\epsilon , \\delta would mean a wider network , but this dependence should be shown explicitly . For \\delta , this dependence is somewhat shown in the proof of Lemma A.5 , but for \\epsilon it is very unclear and is probably related to Lemma A.2 which cited from another paper . 2 ) If I understand the proof correctly , the width of the network in Lemma A.1 ( and also in Theorem 4.1 ) should be super-exponential , no matter what is the target function . That is because in Lemma A.4 the sentence takes into account every graph in G_ { n , k } , the number of such graphs is super-exponential and I suppose that the width of the GNN that realizes the sentence depends on the length of the sentence ( or at least the number of literals ) . If this is true , then I think it is important to point that out in the main text . 3 ) Continuing the previous point , the paragraph after Remark 1 isn \u2019 t clear : why the construction is adaptive to the descriptive complexity of the target function if for any target function the construction requires memorization of all graphs with n nodes ? 4 ) Regarding the Experimental results , I think it is important to also test RF on real datasets . The reason is that without this experiment it is not clear if adding RF doesn \u2019 t actually harm the performance of GNN and thus are actually very impractical . As the authors stated , I don \u2019 t expect RF to significantly improve performance on real datasets because high expressivity might not be required , but I am concerned that it will hurt the performance . I also highly suggest uploading the EXP and CEXP datasets as supplementary material , since these are newly generated datasets , and this way other people could also experiment on them and compare performance . To conclude , I think this is a very nice and well-written paper that adds a novel view on adding RF to GNN , both theoretically and empirically . With that said , there are some issues with the vague form in which the main theorem is stated , and experiments on real datasets would help clarify whether adding RF could actually prove helpful ( or at least not harmful ) for practical uses .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the insightful feedback and points . We are very pleased that you find our paper interesting : Our results indeed support the viability of RF as an efficient alternative to higher-order models . We address your concerns below : 1 . Lemma A.1 ( Boolean functions ) and Theorem 4.1 ( general functions ) have the following implications : ( i ) $ \\delta $ : The confidence parameter $ \\delta $ affects embedding dimensionality ( i.e. , GNN width ) : In the proof of Lemma A.1 , we show that the needed dimensionality is $ O ( n^2 \\delta^ { -1 } ) $ , where n is the maximal number of graph nodes . Therefore , it is correct that lower $ \\delta $ implies higher GNN width . ( ii ) $ \\epsilon $ : The error $ \\epsilon $ is only used in defining a universal readout function , and is needed in our ( and , in Barcelo et al . 's ) construction solely for this reason . Specifically , it thresholds the error of the GNN when making approximations of the target function , defining a universal readout function ( e.g. , MLP ) following the GNN to enable $ \\epsilon $ error relative to the target . Thus , $ \\epsilon $ has no direct effect on the width or depth of the GNN . ( iii ) GNN depth & width : Barcelo et al.show that the depth and width required to learn a function over graphs depends on the quantifier depth of the target logical sentence . The depth is the same in our case , and as for the width , we need to choose `` the dimension of the state vectors in such a way that it is greater than $ c \u22c5 n^2 $ and is at least as large as the dimension of the state vectors of $ \\mathcal { N } _f $ '' ( Proof of Lemma A.1 ) . 2.As stated in point 1 , the GNN width and depth required for learning a function rely on the complexity of the target sentence and hence the target function . We distinguish Lemma A.1 ( Boolean functions ) and Theorem 4.1 ( general functions ) in our response : * * Lemma A.1 . * * In Lemma A.4 , we show that any Boolean function over the set of graphs can be represented as a formula in $ \\ \\text { C } ^2 $ , and thus that MPNNs with RF can learn any Boolean function over graphs . As the reviewer correctly points , the construction can potentially yield an exponentially large sentence , but this is not necessarily the case , since this depends tightly on the target function h. There are two main reasons for this : ( a ) the target function h may require only a sentence that uses at most polynomially many disjuncts in our construction ( i.e. , the set H may be of polynomial size ) , and ( b ) the target function h can be captured by a logically equivalent ( but much shorter ) sentence than the one given in our construction . That is , our proof relies on the existence of a $ \\text { C } ^2 $ formula , but does not make any specific assumptions based on the formula derived in Lemma A.4 . Hence , our construction is very adaptive and tightly linked to the descriptive complexity of the function we want to approximate , and it is easy to see that much more efficient constructions can be given for a more restricted class of functions . As we state in Remark 1 , this deserves a more thorough investigation , and an important direction for future work is to understand , e.g. , the class of functions that can be approximated via poly sized GNN . * * Theorem 4.1 . * * While lifting the result from Boolean functions to the general case , we rely on exponentially many dimensions , and this blow-up seems unavoidable in the general case , where we put no restrictions on the class of functions . The adaptive nature of our constructions also has implications on Theorem 4.1 , yielding better bounds . For instance , if the target function has a compact range , then we can cover the domain with a bounded number of points , which only depends on the desired error epsilon . We have now explicitly introduced the bound on width , and provided explanations of all these points , following Theorem 4.1 ( Page 5 ) . 3.As described in point 2 , the construction aims to show the existence of a $ \\text { C } ^2 $ sentence capturing the target function . It is possible , and quite plausible , to characterize the target function in terms of more succinct sentences , once we restrict our attention to a less general class of functions . This subtlety is the reason why the overall idea is not based on memorization of individual graphs , but rather on the complexity of the sentences being learned . 4.Sato et al.investigate partial RF on real-world datasets , and show that GNNs supplemented with partial RFs perform comparably , if not marginally better , on standard datasets MUTAG , NCI1 , and PROTEINS . This means that partial RF does not seem to hurt performance on real-world datasets . This confirms the intuition about the impact of RF we present in the paper , and further justifies the need for datasets EXP and CEXP . * * CEXP/EXP * * : We share the datasets in the supplementary material . [ 1 ] Barcelo ' et al . `` The Logical Expressiveness of Graph Neural Networks '' , ICLR 2020 . [ 2 ] Sato et al . `` Random Features Strengthen Graph Neural Networks . `` , arXiv 2002.03155"}}