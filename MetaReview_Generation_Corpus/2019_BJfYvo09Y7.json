{"year": "2019", "forum": "BJfYvo09Y7", "title": "Hierarchical Visuomotor Control of Humanoids", "decision": "Accept (Poster)", "meta_review": "A hierarchical method is presented for developing humanoid motion control,\nusing low-level control fragments, egocentric visual input, recurrent high-level control.\nIt is likely the first demonstration of 3D humanoids learning to do memory-enabled tasks using only\nproprioceptive and head-based ego-centric vision. The use of control fragments as opposed\nto mocapclip-based skills allows for finer-grained repurposing of pieces of motion, while\nstill allowing for mocap-based learning\n\nWeaknesses: It is largely a mashup up of previously known results (R2).  Caveat: this can be said for all research\nat some sufficient level of abstraction. The motions are jerky when transitions happen between control fragments (R2,R3).\nThere are some concerns as to whether the method compares against other methods; the authors note\nthat they are either not directly comparable, i.e., solving a different problem, or are implicitly\ncontained in some of the comparisons that are performed in the paper.\n\nOverall, the reviewers and AC are in broad agreement regarding the strengths and weaknesses of the paper.\n\nThe AC believes that the work will be of broad interest. Demonstrating memory-enabled, vision-driven,\nmocap-imitating skills is a broad step forward. The paper also provides a further datapoint as \nto which combinations of method work well, and some of the specific features required to make them work.\n\nThe paper could acknowledge motion quality artifacts, as noted by the reviewers and \nin the online discussion.  Suggest to include  [Peng et al 2017] as some of the most relevant related HRL humanoid control work, as per the reviews & discussion.\n\n", "reviews": [{"review_id": "BJfYvo09Y7-0", "review_text": "1) Summary The authors propose an interesting hierarchical reinforcement learning method that makes use of visual inputs as well as proprioception for locomotion of humanoid agents. The low-level controllers make use of \u201cmotion capture\u201d data and are expected to form a set of movement primitives that can be used by a higher-level controller that has vision and memory. Their method is tested on a variety of tasks and different choices of low-level controller are explored. 2) Pros + Combining vision, memory, and motor control + Allows the high-level controller to operate at a coarser time scale + The set of low-level movement primitives can be extended by using more mocap data 3) Cons - No comparison to earlier work - Highly unnatural motions even though it makes use of mocap data - Sample inefficient: more than 1 billion time-steps to train the high-level controller 4) Comments Showing that the agent can provide suitable solutions for these tasks using raw vision input is indeed interesting, however it is not clear what the main contribution of the paper is as the authors fail to compare their results with earlier work. It would be useful if the authors could cover the related work in more depth in order to motivate their method and contrast it with the existing solutions. As an example, DeepLoco (Peng et al. 2017) solves a similar problem in which they use an egocentric heightmap instead of direct visual input, hence a formal consideration of the trade-offs would be informative. In addition, the appeal of using hierarchical reinforcement learning is to divide up the task into easier chunks that can be solved easier, however it is not obvious how well this method succeeds at this task, keeping in mind that the high-level controller takes in the order of 1 billion time-steps to learn most tasks (5 billion in the case of \u201cHeterogeneous Forage\u201d). In the end, an ablation study could be useful since the authors make plenty of novel design decision, yet their effect on the final performance is not clear. 6) Questions - Is is possible to entirely remove proprioception from the input to the high-level controller or at least use just a small portion of it? How do the results compare in this case? - How robust is \u201ccold-switching\u201d between control fragments? Is it possible to transition between most fragments without losing balance or does the high-level controller have to be extremely careful as to which combination it should use? The former case would suggest that this method is indeed useful as a hierarchical method. However the latter case might imply that the hierarchical method is failing and the higher level controller\u2019s task has not been made much easier than the original problem itself. - Table 2 describes the mocap clips used to train the low-level controllers in each task. What is the effect of choosing different sets of motions? Specifically how well does the steerable controller work if walking motions were used for the \u201cGo To Target\u201d and \u201cWalls\u201d tasks rather than running motions? Presumably, this can result in a more flexible controller which allows sharper turns without loosing balance. - The network in Figure A.1 gets the last action as an input. Why is this required? Especially since the LSTM unit can learn to remember any information related to the previous actions. - How does the supervised pre-training described in section 2.1 effect the training of low-level controllers? Is it used as a speed-up mechanism or a way of escaping local minima? - In section 2.1 the authors mention that the episodes are \u201cterminated when the pose deviates too far from the trajectory\u201d. I believe this termination criteria was not present in the earlier works (Peng et al. 2018), then what is the effect of adding such a criterion? Can this make the learned agent less robust as it will not learn to recover from larger perturbations? 7) Conclusion The method and the results are interesting but further comparison with existing work is required.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for a careful reading and for these questions . We will first address the three \u201c cons \u201d described above . Comparison to earlier work : We have provided a thorough set of comparisons and investigation of the components of the system where possible . In particular , our low-level controllers are built using a variety of techniques , encompassing the techniques in Liu et al. , 2017 , and Peng et al. , 2018 . Therefore , at the low-level policy level , we are indeed primarily benchmarking and scaling up existing techniques . At the composite system level , however , we feel that the question is ill-posed . Consider , for example , the DeepLoco paper ( Peng et al. , 2017 ) : the motion capture data used in this work were manually selected and preprocessed . While undoubtedly excellent work , this step of manual curation makes it difficult to understand what an objective algorithm comparison would mean . Our work aimed to minimize this manual curation . As we demonstrated , the control fragments approach scaled well with the inclusion of redundant or irrelevant motion capture data . We examined the application of existing techniques requiring little manual curation to the problem of hierarchical , memory-based visuomotor control of humanoids . Jerkiness : The movements were admittedly slightly jerky in the control fragment model due to switching among the fragments . In the appendix , we also demonstrate the trade-off between longer fragments , which would result in smoother motion , and task performance . For the steering and graph switching approaches , any jerkiness was merely a consequence of the artistry with which mocap is curated : we generally preferred methods that enable scaling to large numbers of clips and the solution of new high-level tasks , instead of manual curation of motion capture data . Sample efficiency : While we agree that 1 billion timesteps seems large , we would emphasize that this is standard at present ( e.g.Peng et al.2018 uses within 1 order of magnitude of that number for learning skill-selection behavior even without vision-based control ) . Additionally , for each higher-level task , we provided a comparison with simple rolling-ball body . This body was used to demonstrate the difficulty of the task independent of the control problem . We would like to emphasize that there is limited learning-based work on humanoids in simulation reusing motor skills to solve new tasks , and this work is novel in this regard : certainly , the use of an egocentric camera to guide visuomotor behavior of humanoids is little studied . Other work on simulated humanoid control has primarily used state features designed to provide input to the agent about terrain ( Peng et al.2017 , Heess et al.2017 , Peng et al.2018 ) .A contribution of the present work was to move past hand-designed features towards a more ecological observation setting . A scientific contribution of this work was also to show that hierarchical motor skill reuse enabled tasks that were unsolvable with flat policy learning to be solved . We clearly demonstrated this . For the walls and go-to-target tasks , learning from scratch was slower and produced less robust behavior ; for the forage tasks , learning from scratch failed completely . Since control fragments were the most compelling LL control approach among those we considered , we did a thorough study of the effect of fragment length , number of fragments , as well as introduction of redundant clips ( see appendix B ) ."}, {"review_id": "BJfYvo09Y7-1", "review_text": "The paper proposes a control architecture for learning task oriented whole body behaviors in simulated humanoid robots bootstrapped with motion capture data. The authors use a hierarchical approach, where the low-level controllers are trained to follow motion captured data whereas the high-level control combines them. The topic of the paper is interesting and the language is understandable. The paper discusses and compares different ways to achieve such a higher level control. It probably won\u2019t be useful for real robots, but will be possibly useful for computer graphics. I suspect that code will not be published anytime soon, and I am afraid it will be hard to reproduce without. There is a solid software engineering involved and the system has many parameters. The related work section (or lack thereof) can be improved. What is the advantage of this work over the multi-skill integration in Peng et al 2018? Please explain explicitly in the paper. The end-to-end approach seems a bit too weak to me. The video shows more artifacts than other similar papers, (cf. Heess et al. 2017). What\u2019s the detail of the training for the end-to-end baseline? Are the environments randomized in each rollout? If not then this would need an ablation study which ablates memory/vision to prove its claim of integrating vision and memory. How much is the memory used in the tasks where nothing needs to be memorized? Is there any noise in the simulations? One weakness is that the low-level controllers are not adapted any further. That is probably why the fragments outperformed the transition policies etc., because the higher level policy has more flexibility. Overall, from the perspective of deep learning, I think the paper is novel and provides some insights into different approaches to the problem. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for their feedback . We share your interest in reproducibility . The body we have used is only a slight variation on the DM control suite CMU humanoid ( https : //github.com/deepmind/dm_control ; Tassa et al.2018 ) ; the motion capture data are already available , as is code in the suite to register mocap data to the body . We intend to release the updated body and task environments used in this work . Unfortunately , because of the complexity and inter-linkedness of the agent-level code , along with the requirements for cluster-specific , high-performance compute , we can not at the present easily release it , but elements of the algorithms are available ( e.g. , the high-level controllers were trained with the same algorithm as in Espeholt et al. , 2018 ; http : //github.com/deepmind/scalable_agent ) . Peng et al.2018 described a few approaches for multi-skill integration : The multi-clip reward and skill selector approaches are similar to our conditional tracking along a graph as it involves training a low-level controller based on a few clips . The details differ slightly , but these approaches are similar in spirit to our baseline of training a low-level controller to switch based on a parametric input . We will clarify this relationship in the text . The composite policy approach in Peng et al.2018 is more specific and involves selecting subsequent skills based on the relative value function for transitioning to subsequent behaviors -- it is essentially autonomous , so it is less immediately amenable to re-purposing in the context of new tasks and less relevant in the context of the present work . Overall though , we emphasize that the Peng et al.2018 style approaches require manual curation of the mocap data ( similar to our transition-graph and steerable tracking approaches ) , and we aimed with this work as well to explore approaches ( such as control fragments ) that require very little/no manual curation . The comparison with Heess et al.2017 is a good question . Sensible looking locomotion behavior can be achieved in a variety of ways ( environmental constraints , appropriate shaping rewards , task setup including multiple tasks or curricula ) . Heess et al.2017 uses a simpler reward that encourages a constant forward velocity together with environmental variations , on a simpler humanoid model . In our own end-to-end experiments , especially slow movements and transitions between standing and moving have proven relatively difficult . The comparison here is mostly meant to demonstrate that the particular go-to-target task setup does not easily give rise to naturalistic looking behavior when trained end-to-end . However , we can expand on the details of the end-to-end training in the text . In each episode of a task , the environment is randomized . In the Walls and Gaps task the layout of the track is randomized , requiring the agent to use vision to navigate . In the Forage task , the maze layout and placement of rewards is randomized . Finally , in the heterogeneous forage task , each orb color is assigned a random positive or negative reward . In this case the agent must match color to reward in the episode to only eat the \u201c good \u201d orbs . We can clarify in the text that not all tasks require memory . More broadly , the use of memory is a little subtle insofar as the high-level agent sees information at each low-level timestep , but only acts to switch among the low-level controllers ( for control fragments , e.g.every 3 timesteps ; it at least makes sense that some state information seen by the high-level controller between selection timesteps could inform subsequent selection , but we agree we don \u2019 t assess this explicitly ) . We agree that adapting low-level skills is important and this is a clear direction for future research ."}, {"review_id": "BJfYvo09Y7-2", "review_text": "1) Summary This paper proposes a hierarchical reinforcement learning (HRL) method for visual motor control of humanoid agents. The method is decomposed into a high-level controller that takes in visual input and proprioceptive information, and a low-level controller (they compare may ways of doing this) that takes care of the agent\u2019s motor control. In experiments, the proposed method is tested on a variety of RL tasks where the many low-level controllers presented in the paper are compared against each other. 2) Pros: + Novel high-level controller that takes in front-view visual information + Novel multi-policy low level controller + Interesting experimental section 3) Cons: Numerical comparison to previous methods: - The only issue I found with this paper is that there is no comparison with other methods. Even if the other methods do not take in front-view visual input, it would be nice to compare with them. Maybe visual inputs results in better high-level controller? Or even show that performance is similar would be an interesting result. 4) Comments: Jerky transitions in switching controller: - Due to the fact that one policy takes over after each other based on the high-level controller choice, there is a jerk artifact that shows when the policies are being changed/executed. Did you guys try to add a connection in feature space between policies rather than only passing the state of the agent? This may be able to help with that artifact that sampling noise adds to the actions. Can the authors comment on this? Steerable controller limited rotation: - From observing the steerable controller policy in action, it seems the policy learned a steering that is somewhat independent of what the limbs are doing. Maybe adding a mechanism where the leg motion intensity depends more on the direction of movement could be a way to fix the issue where this policy moves to fast for the turning it tries to do. Maybe an energy based objective to minimize the torques or something in that line. 4) Conclusion: To the best of my knowledge, this paper proposes a novel interesting method for modeling humanoid motor skills with front-view visual input. However, as mentioned above, the paper lacks of numerical comparisons with other methods, and only compares against its own variations which is more of an ablation study. I am willing to increase my review score if the authors successfully address the concerns mentioned above", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for appreciating the difficulty of the problem and the novelty of the setting , including the use of egocentric vision . The main concern of this reviewer seems to be about comparison to other methods . Our core intent with this paper has indeed been a comparison of different methods , adapted from the literature for reusing low-level motor skills . Previous work ( e.g.Merel et al 2017 , Peng et al 2018 , and others ) have attempted to build low-level controllers that incorporate transitions and are conditioned on a pre-specified input parameterization ( such as heading direction ) . This style of approach is represented in our comparisons by the steering and graph-transitioning approaches . We also explored the use of control fragments for skill reuse ( proposed by Liu et al.2017 ) -- we presented results using a default version of this as well as a novel variant . As far as we are aware , these approaches amount to the current most competitive approaches for motor skill reuse . Our work goes beyond this by demonstrating that these forms of motor reuse enable visuomotor control and we can , in some cases , be used to solve more complex whole-body tasks than previously done . Overall , we view this current paper as an integrative work that helps establish and clarify the state of the art involving motor reuse for generic humanoid movement tasks . Concerning jerky transitions and other visual idiosyncrasies -- we agree that transitions between sub-behaviors are jerky here . One could attempt to enforce smoothness , however , without adjusting the low level controllers , this would merely amount to a prior or constraint on which transitions would occur and would reduce task performance . We also agree that there are numerous ways to train the low-level controllers to be a bit smoother -- this is important for computer graphics and would make movements more visually pleasing , but does not necessarily affect task performance . Importantly , these manual adjustments can require considerable human effort to tune and for this paper we wanted to focus on general techniques for reuse that require as little hand-tuning as possible as we believe this will be critical for scaling to large skill repertoires ."}], "0": {"review_id": "BJfYvo09Y7-0", "review_text": "1) Summary The authors propose an interesting hierarchical reinforcement learning method that makes use of visual inputs as well as proprioception for locomotion of humanoid agents. The low-level controllers make use of \u201cmotion capture\u201d data and are expected to form a set of movement primitives that can be used by a higher-level controller that has vision and memory. Their method is tested on a variety of tasks and different choices of low-level controller are explored. 2) Pros + Combining vision, memory, and motor control + Allows the high-level controller to operate at a coarser time scale + The set of low-level movement primitives can be extended by using more mocap data 3) Cons - No comparison to earlier work - Highly unnatural motions even though it makes use of mocap data - Sample inefficient: more than 1 billion time-steps to train the high-level controller 4) Comments Showing that the agent can provide suitable solutions for these tasks using raw vision input is indeed interesting, however it is not clear what the main contribution of the paper is as the authors fail to compare their results with earlier work. It would be useful if the authors could cover the related work in more depth in order to motivate their method and contrast it with the existing solutions. As an example, DeepLoco (Peng et al. 2017) solves a similar problem in which they use an egocentric heightmap instead of direct visual input, hence a formal consideration of the trade-offs would be informative. In addition, the appeal of using hierarchical reinforcement learning is to divide up the task into easier chunks that can be solved easier, however it is not obvious how well this method succeeds at this task, keeping in mind that the high-level controller takes in the order of 1 billion time-steps to learn most tasks (5 billion in the case of \u201cHeterogeneous Forage\u201d). In the end, an ablation study could be useful since the authors make plenty of novel design decision, yet their effect on the final performance is not clear. 6) Questions - Is is possible to entirely remove proprioception from the input to the high-level controller or at least use just a small portion of it? How do the results compare in this case? - How robust is \u201ccold-switching\u201d between control fragments? Is it possible to transition between most fragments without losing balance or does the high-level controller have to be extremely careful as to which combination it should use? The former case would suggest that this method is indeed useful as a hierarchical method. However the latter case might imply that the hierarchical method is failing and the higher level controller\u2019s task has not been made much easier than the original problem itself. - Table 2 describes the mocap clips used to train the low-level controllers in each task. What is the effect of choosing different sets of motions? Specifically how well does the steerable controller work if walking motions were used for the \u201cGo To Target\u201d and \u201cWalls\u201d tasks rather than running motions? Presumably, this can result in a more flexible controller which allows sharper turns without loosing balance. - The network in Figure A.1 gets the last action as an input. Why is this required? Especially since the LSTM unit can learn to remember any information related to the previous actions. - How does the supervised pre-training described in section 2.1 effect the training of low-level controllers? Is it used as a speed-up mechanism or a way of escaping local minima? - In section 2.1 the authors mention that the episodes are \u201cterminated when the pose deviates too far from the trajectory\u201d. I believe this termination criteria was not present in the earlier works (Peng et al. 2018), then what is the effect of adding such a criterion? Can this make the learned agent less robust as it will not learn to recover from larger perturbations? 7) Conclusion The method and the results are interesting but further comparison with existing work is required.", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for a careful reading and for these questions . We will first address the three \u201c cons \u201d described above . Comparison to earlier work : We have provided a thorough set of comparisons and investigation of the components of the system where possible . In particular , our low-level controllers are built using a variety of techniques , encompassing the techniques in Liu et al. , 2017 , and Peng et al. , 2018 . Therefore , at the low-level policy level , we are indeed primarily benchmarking and scaling up existing techniques . At the composite system level , however , we feel that the question is ill-posed . Consider , for example , the DeepLoco paper ( Peng et al. , 2017 ) : the motion capture data used in this work were manually selected and preprocessed . While undoubtedly excellent work , this step of manual curation makes it difficult to understand what an objective algorithm comparison would mean . Our work aimed to minimize this manual curation . As we demonstrated , the control fragments approach scaled well with the inclusion of redundant or irrelevant motion capture data . We examined the application of existing techniques requiring little manual curation to the problem of hierarchical , memory-based visuomotor control of humanoids . Jerkiness : The movements were admittedly slightly jerky in the control fragment model due to switching among the fragments . In the appendix , we also demonstrate the trade-off between longer fragments , which would result in smoother motion , and task performance . For the steering and graph switching approaches , any jerkiness was merely a consequence of the artistry with which mocap is curated : we generally preferred methods that enable scaling to large numbers of clips and the solution of new high-level tasks , instead of manual curation of motion capture data . Sample efficiency : While we agree that 1 billion timesteps seems large , we would emphasize that this is standard at present ( e.g.Peng et al.2018 uses within 1 order of magnitude of that number for learning skill-selection behavior even without vision-based control ) . Additionally , for each higher-level task , we provided a comparison with simple rolling-ball body . This body was used to demonstrate the difficulty of the task independent of the control problem . We would like to emphasize that there is limited learning-based work on humanoids in simulation reusing motor skills to solve new tasks , and this work is novel in this regard : certainly , the use of an egocentric camera to guide visuomotor behavior of humanoids is little studied . Other work on simulated humanoid control has primarily used state features designed to provide input to the agent about terrain ( Peng et al.2017 , Heess et al.2017 , Peng et al.2018 ) .A contribution of the present work was to move past hand-designed features towards a more ecological observation setting . A scientific contribution of this work was also to show that hierarchical motor skill reuse enabled tasks that were unsolvable with flat policy learning to be solved . We clearly demonstrated this . For the walls and go-to-target tasks , learning from scratch was slower and produced less robust behavior ; for the forage tasks , learning from scratch failed completely . Since control fragments were the most compelling LL control approach among those we considered , we did a thorough study of the effect of fragment length , number of fragments , as well as introduction of redundant clips ( see appendix B ) ."}, "1": {"review_id": "BJfYvo09Y7-1", "review_text": "The paper proposes a control architecture for learning task oriented whole body behaviors in simulated humanoid robots bootstrapped with motion capture data. The authors use a hierarchical approach, where the low-level controllers are trained to follow motion captured data whereas the high-level control combines them. The topic of the paper is interesting and the language is understandable. The paper discusses and compares different ways to achieve such a higher level control. It probably won\u2019t be useful for real robots, but will be possibly useful for computer graphics. I suspect that code will not be published anytime soon, and I am afraid it will be hard to reproduce without. There is a solid software engineering involved and the system has many parameters. The related work section (or lack thereof) can be improved. What is the advantage of this work over the multi-skill integration in Peng et al 2018? Please explain explicitly in the paper. The end-to-end approach seems a bit too weak to me. The video shows more artifacts than other similar papers, (cf. Heess et al. 2017). What\u2019s the detail of the training for the end-to-end baseline? Are the environments randomized in each rollout? If not then this would need an ablation study which ablates memory/vision to prove its claim of integrating vision and memory. How much is the memory used in the tasks where nothing needs to be memorized? Is there any noise in the simulations? One weakness is that the low-level controllers are not adapted any further. That is probably why the fragments outperformed the transition policies etc., because the higher level policy has more flexibility. Overall, from the perspective of deep learning, I think the paper is novel and provides some insights into different approaches to the problem. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for their feedback . We share your interest in reproducibility . The body we have used is only a slight variation on the DM control suite CMU humanoid ( https : //github.com/deepmind/dm_control ; Tassa et al.2018 ) ; the motion capture data are already available , as is code in the suite to register mocap data to the body . We intend to release the updated body and task environments used in this work . Unfortunately , because of the complexity and inter-linkedness of the agent-level code , along with the requirements for cluster-specific , high-performance compute , we can not at the present easily release it , but elements of the algorithms are available ( e.g. , the high-level controllers were trained with the same algorithm as in Espeholt et al. , 2018 ; http : //github.com/deepmind/scalable_agent ) . Peng et al.2018 described a few approaches for multi-skill integration : The multi-clip reward and skill selector approaches are similar to our conditional tracking along a graph as it involves training a low-level controller based on a few clips . The details differ slightly , but these approaches are similar in spirit to our baseline of training a low-level controller to switch based on a parametric input . We will clarify this relationship in the text . The composite policy approach in Peng et al.2018 is more specific and involves selecting subsequent skills based on the relative value function for transitioning to subsequent behaviors -- it is essentially autonomous , so it is less immediately amenable to re-purposing in the context of new tasks and less relevant in the context of the present work . Overall though , we emphasize that the Peng et al.2018 style approaches require manual curation of the mocap data ( similar to our transition-graph and steerable tracking approaches ) , and we aimed with this work as well to explore approaches ( such as control fragments ) that require very little/no manual curation . The comparison with Heess et al.2017 is a good question . Sensible looking locomotion behavior can be achieved in a variety of ways ( environmental constraints , appropriate shaping rewards , task setup including multiple tasks or curricula ) . Heess et al.2017 uses a simpler reward that encourages a constant forward velocity together with environmental variations , on a simpler humanoid model . In our own end-to-end experiments , especially slow movements and transitions between standing and moving have proven relatively difficult . The comparison here is mostly meant to demonstrate that the particular go-to-target task setup does not easily give rise to naturalistic looking behavior when trained end-to-end . However , we can expand on the details of the end-to-end training in the text . In each episode of a task , the environment is randomized . In the Walls and Gaps task the layout of the track is randomized , requiring the agent to use vision to navigate . In the Forage task , the maze layout and placement of rewards is randomized . Finally , in the heterogeneous forage task , each orb color is assigned a random positive or negative reward . In this case the agent must match color to reward in the episode to only eat the \u201c good \u201d orbs . We can clarify in the text that not all tasks require memory . More broadly , the use of memory is a little subtle insofar as the high-level agent sees information at each low-level timestep , but only acts to switch among the low-level controllers ( for control fragments , e.g.every 3 timesteps ; it at least makes sense that some state information seen by the high-level controller between selection timesteps could inform subsequent selection , but we agree we don \u2019 t assess this explicitly ) . We agree that adapting low-level skills is important and this is a clear direction for future research ."}, "2": {"review_id": "BJfYvo09Y7-2", "review_text": "1) Summary This paper proposes a hierarchical reinforcement learning (HRL) method for visual motor control of humanoid agents. The method is decomposed into a high-level controller that takes in visual input and proprioceptive information, and a low-level controller (they compare may ways of doing this) that takes care of the agent\u2019s motor control. In experiments, the proposed method is tested on a variety of RL tasks where the many low-level controllers presented in the paper are compared against each other. 2) Pros: + Novel high-level controller that takes in front-view visual information + Novel multi-policy low level controller + Interesting experimental section 3) Cons: Numerical comparison to previous methods: - The only issue I found with this paper is that there is no comparison with other methods. Even if the other methods do not take in front-view visual input, it would be nice to compare with them. Maybe visual inputs results in better high-level controller? Or even show that performance is similar would be an interesting result. 4) Comments: Jerky transitions in switching controller: - Due to the fact that one policy takes over after each other based on the high-level controller choice, there is a jerk artifact that shows when the policies are being changed/executed. Did you guys try to add a connection in feature space between policies rather than only passing the state of the agent? This may be able to help with that artifact that sampling noise adds to the actions. Can the authors comment on this? Steerable controller limited rotation: - From observing the steerable controller policy in action, it seems the policy learned a steering that is somewhat independent of what the limbs are doing. Maybe adding a mechanism where the leg motion intensity depends more on the direction of movement could be a way to fix the issue where this policy moves to fast for the turning it tries to do. Maybe an energy based objective to minimize the torques or something in that line. 4) Conclusion: To the best of my knowledge, this paper proposes a novel interesting method for modeling humanoid motor skills with front-view visual input. However, as mentioned above, the paper lacks of numerical comparisons with other methods, and only compares against its own variations which is more of an ablation study. I am willing to increase my review score if the authors successfully address the concerns mentioned above", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for appreciating the difficulty of the problem and the novelty of the setting , including the use of egocentric vision . The main concern of this reviewer seems to be about comparison to other methods . Our core intent with this paper has indeed been a comparison of different methods , adapted from the literature for reusing low-level motor skills . Previous work ( e.g.Merel et al 2017 , Peng et al 2018 , and others ) have attempted to build low-level controllers that incorporate transitions and are conditioned on a pre-specified input parameterization ( such as heading direction ) . This style of approach is represented in our comparisons by the steering and graph-transitioning approaches . We also explored the use of control fragments for skill reuse ( proposed by Liu et al.2017 ) -- we presented results using a default version of this as well as a novel variant . As far as we are aware , these approaches amount to the current most competitive approaches for motor skill reuse . Our work goes beyond this by demonstrating that these forms of motor reuse enable visuomotor control and we can , in some cases , be used to solve more complex whole-body tasks than previously done . Overall , we view this current paper as an integrative work that helps establish and clarify the state of the art involving motor reuse for generic humanoid movement tasks . Concerning jerky transitions and other visual idiosyncrasies -- we agree that transitions between sub-behaviors are jerky here . One could attempt to enforce smoothness , however , without adjusting the low level controllers , this would merely amount to a prior or constraint on which transitions would occur and would reduce task performance . We also agree that there are numerous ways to train the low-level controllers to be a bit smoother -- this is important for computer graphics and would make movements more visually pleasing , but does not necessarily affect task performance . Importantly , these manual adjustments can require considerable human effort to tune and for this paper we wanted to focus on general techniques for reuse that require as little hand-tuning as possible as we believe this will be critical for scaling to large skill repertoires ."}}