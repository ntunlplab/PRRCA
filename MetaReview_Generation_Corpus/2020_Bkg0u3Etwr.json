{"year": "2020", "forum": "Bkg0u3Etwr", "title": "Maxmin Q-learning: Controlling the Estimation Bias of Q-learning", "decision": "Accept (Poster)", "meta_review": "The authors propose the use of an ensembling scheme to remove over-estimation bias in Q-Learning. The idea is simple but well-founded on theory and backed by experimental evidence. The authors also extensively clarified distinctions between their idea and similar ideas in the reinforcement learning literature in response to reviewer concerns.", "reviews": [{"review_id": "Bkg0u3Etwr-0", "review_text": "This paper proposes a new Q learning algorithm framework: maxmin Q-learning, to address the overestimation bias issue of Q learning. The main contributions of this paper are three folds: 1) It provides an inspiring example on overestimation/underestimation of Q learning. 2) Generalize Q learning by a new maxmin Q-learning by maintaining independent Q estimator and interact them in a max-min way for the update. 3) Provide both theoretical and empirical analyses of their algorithm. I have two main concerns for this paper: 1) When is your algorithm useful? What's your criterion of picking the hyper-parameters (e.g. number of Q functions you want to learn). 2) Comparison to more intriguing way for jointly update of multiple Q functions, like soft Q learning. For the first concern, the paper has shown an interesting example in Figure 2. However, it seems that we cannot decide whether overestimation or underestimation will help the exploration, since the reward function is often unknown in real world. And in both cases, maxmin Q learning is not the best algorithm than either Q learning and double q learning. On the other hand, if we use a softmax policy for Q function, e.g. $\\pi(a|s) \\propto \\exp(\\alpha Q(s,a))$, a drift for Q learning(e.g. Q(s,a) = Q*(s,a) + c) has no effect on our policy. I believe in this case we should more focusing on the inner difference between different value of Q function, rather than comparing our estimate Q function with the true Q*. For the second concern, we can view the framework of maxmin q learning as a joint update scheme for different Q function. In experimental part, the comparison is not fair since the paper use multiple Q function to compare with single or double Q function. One reasonable baseline is to update N different Q function, and take the minimum of the final Q function as our decision policy, compare with maxmin Q learning with N different Q function. Another baseline the paper should consider is soft Q learning, where it maintain multiple Q function and jointly update Q different function to maximize the entropy while moving towards an improvement Q. Overall, I believe the idea of the paper is novel and interesting, but further improvements should be added in order to improve the score the paper.", "rating": "6: Weak Accept", "reply_text": "We appreciate your feedback . For the first concern , you are right , we can not know for an unknown environment whether overestimation or underestimation will help . This is exactly what we show in section 3 -- the optimal bias is environment dependent . So the optimal N is also environment dependent . N is a parameter that can be tuned , to specialize to each different problem . However , the theory does provide some guidance , namely by setting $ t_ { MN } =1/2 $ to remove bias . For example , when M=4 , a choice of N=3 reduces the bias to near 0 ( more results are shown in Figure 5 ) . This may not be the right choice , though , if the noise does not satisfy the assumptions . In our experiments , we found that the optimal N was usually between 2 to 9 , and that performance was usually improved with relatively small N. But , more work is needed to better understand more generally how to select N automatically . The MDP we present in section 3 is designed as a motivating example to show that overestimation and underestimation bias can both help and hurt . Note that we fixed the step-size and other hyperparameters for all algorithms ( instead of tuning them in order to achieve the best performance ) since this MDP was not used as a benchmark for performance comparison . It is just an illustrative example . We are not exactly sure what you mean by your comment that `` a drift for Q learning ( e.g. $ Q ( s , a ) = Q^ { * } ( s , a ) + c $ ) has no effect on our policy '' . What is c ? If it is a constant for all actions , absolutely it has no effect . But , the problem we are solving is that , due to stochasticity , a different constant could be added $ Q ( s , a ) $ for each $ a $ . This will effect the policy . If c is random , could you clarify further what you mean here ? You are right that our Maxmin Q-learning is a joint update scheme for different Q functions , and one of our contributions is that we provide a convergence proof for such a framework under reasonable assumptions . However , we politely disagree with your claim that our empirical comparison is unfair . Note that on Mountain Car ( Figure 3 ) , we compare Double Q-learning , Averaged Q-learning ( N=2 ) , and Maxmin Q-learning ( N=2 ) . Here , all three algorithms have two Q functions and Maxmin Q-learning shows significant robustness and achieves better performance . Similarly , in the other seven more complex environments , both Maxmin DQN and Averaged DQN learn multiple Q functions . And the number of Q functions is tuned in the same scope . Again , Maxmin DQN outperforms Averaged DQN . So we believe the comparison is fair . We are also unsure about what you mean by `` one reasonable baseline is to update N different Q function , and take the minimum of the final Q function as our decision policy . '' This could mean the following . N Q functions are learned with Q-learning ( rather than say with the Maxmin update ) . On each step , the agent selects actions using the max action from the minimum of the Q functions . ( If this is not what you intended , please do clarify ) . Theoretically , this strategy also incurs overestimation bias since it uses the same target action-value as Q-learning to update . Further it does not reduce estimation variance because it uses only one Q function to update . For Soft Q-learning ( SQL ) , we assume you mean the algorithm from the paper Reinforcement Learning with Deep Energy-Based Policies by Haarnoja et al. , ( again , please correct us if we are wrong ) . Maxmin Q-learning is a value-based method for discrete control to flexibly control bias and reduce variance . In contrast , SQL is a policy-based method for continuous control , with just one action-value function ( the policy is a Gibbs distribution on Q function ) . It is not clear why we would compare to SQL , since it only uses one Q function and tackles a different problem . If you can further clarify why we should compare to SQL , we would be happy to respond further ."}, {"review_id": "Bkg0u3Etwr-1", "review_text": "The paper tackles the problem of bias in target Q-values when performing Q-learning. The paper proposes a technique for computing target Q-values, by first taking the min over an ensemble of learned Q-values and then taking the max over actions. The paper provides some theoretical properties of this technique: (1) the bias of the estimator can be somewhat controlled by the size of the ensemble; (2) performing Q-learning with these target values is convergent. Experimental results show that the proposed technique can provide performance improvement on a number of tasks. Overall, this paper is a modest contribution to the field, since variants of this technique are known and the theoretical arguments are derivatives of known arguments, which places it roughly borderline for an ICLR conference paper. My comments: -- The paper is very well-written. Thank you for putting the effort to provide clear writing. -- The idea of computing a target value as the minimum of an ensemble is well-known in continuous control. See https://arxiv.org/abs/1802.09477 as well as a number of works which follow it. -- The method is motivated as a way to control over/under-estimation. However, the theoretical arguments show that this depends on N, M, and tau (unknown). Are there any ways to choose N other than hyperparameter tuning?", "rating": "3: Weak Reject", "reply_text": "Thank you for your valuable comments . We will address your concerns point by point . First , thank you for pointing us to this control work , which we were not aware of . However , we would like to point out the significant difference between Maxmin Q-learning and TD3 in the paper you mentioned when computing the target value . For Maxmin Q-learning , we first take the minimum of action-values among all N estimators and then choose the action that maximizes these minimum action-values , i.e. $ Y^ { MQ } = r + \\gamma \\max_ { a \\in A } \\min_ { i=1 , \\cdots , N } Q^ { i } ( s , a ) $ . However , in TD3 , the action is chosen first by some policy $ \\pi $ , and then the minimum action-value is selected as the target value , i.e. $ Y^ { TD3 } = r + \\gamma \\min_ { i=1,2 } Q^ { i } ( s , \\pi ( s ) ) $ . The policy $ \\pi $ is expected to converge to the optimal policy , so $ Q^ { i } ( s , \\pi ( s ) ) \\approx \\max_ { a \\in A } Q^ { i } ( s , a ) $ . Thus , we have $ Y^ { TD3 } \\approx r + \\gamma \\min_ { i=1,2 } \\max_ { a \\in A } Q^ { i } ( s , a ) $ . Note that the order of taking minimum and maximum is different . By the max\u2013min inequality , it is easy to get $ Y^ { TD3 } > Y^ { MQ } $ . Furthermore , using the same method as in our paper , we can get $ E [ Y^ { TD3 } ] > E [ Y^ { True } ] $ where $ Y^ { True } = r + \\gamma \\max_ { a \\in A } Q^ { * } ( s , a ) $ . Thus TD3 still suffers from the overestimation bias , while we can adjust $ N $ in Maxmin Q-learning to reduce the bias from positive to negative . In conclusion , these two techniques may seem to be similar at first glance , but they are actually quite different and lead to different properties . We will cite the TD3 paper and add one more paragraph to discuss the difference in our revised paper . Also , there is no theoretical analysis for applying the minimum operator in TD3 paper , whereas we present a theoretical analysis not only for bias control but also for variance reduction . Actually , when we first tried to solve the overestimation bias problem , we also considered a similar approach to TD3 ( we called it Minmax Q-learning as a counterpart ) . However , after some derivatives and analyses , we found that Maxmin Q-learning would be better than Minmax Q-learning theoretically , in terms of reducing overestimation bias . These theoretical analyses guided our design of the Maxmin Q-learning algorithm . They are important and non-trivial . You are right that `` the theoretical arguments are derivatives of known arguments '' , but this is the case for many theoretical arguments . We have not come up with a new proof technique , but do have a novel theoretical result characterizing a new algorithm . Choosing the optimal N is not straightforward . It is a parameter that can be tuned , to specialize for each different problem setting . The theory does provide some guidance , namely by setting $ t_ { MN } =1/2 $ to remove bias . For example , when M=4 , a choice of N=3 reduces the bias to near 0 ( more results are shown in Figure 5 ) . This may not be the right choice , though , if the noise does not satisfy the assumptions . In our experiments , we found that the optimal N was usually between 2 to 9 , and that performance was usually improved with relatively small N. You are correct that future work should investigate how best to select N ."}, {"review_id": "Bkg0u3Etwr-2", "review_text": "This paper proposes a novel variant of Q-learning, called Maxmin Q-learning, to address the issue of overestimation bias Q-learning suffers from (variance of the reward of best action leading to overestimated reward). The idea is to keep a number of estimators each estimated using a different sub-sample, and taking the minimum of the (maximum) reward value of each. The paper gives theoretical analyses, in terms of the reduction in the overestimation bias, as well as the convergence of a class of generalized Q-learning methods including Maxmin Q-learning. The experiment section presents a thorough evaluation of the proposed method, including how the obtained rewards vary as a function of the variance of the reward function and as a function of learning steps, as compared to a number of existing methods such as the Double Q-learning method and its variants. The experimental results are quite convincing, and the theoretical analyses seem solid. Overall this is a well balanced paper which proposes a reasonable new idea, simple but effective, backed by sound theoretical analysis and well executed experimental evaluation. ", "rating": "8: Accept", "reply_text": "Thank you for your positive comments !"}], "0": {"review_id": "Bkg0u3Etwr-0", "review_text": "This paper proposes a new Q learning algorithm framework: maxmin Q-learning, to address the overestimation bias issue of Q learning. The main contributions of this paper are three folds: 1) It provides an inspiring example on overestimation/underestimation of Q learning. 2) Generalize Q learning by a new maxmin Q-learning by maintaining independent Q estimator and interact them in a max-min way for the update. 3) Provide both theoretical and empirical analyses of their algorithm. I have two main concerns for this paper: 1) When is your algorithm useful? What's your criterion of picking the hyper-parameters (e.g. number of Q functions you want to learn). 2) Comparison to more intriguing way for jointly update of multiple Q functions, like soft Q learning. For the first concern, the paper has shown an interesting example in Figure 2. However, it seems that we cannot decide whether overestimation or underestimation will help the exploration, since the reward function is often unknown in real world. And in both cases, maxmin Q learning is not the best algorithm than either Q learning and double q learning. On the other hand, if we use a softmax policy for Q function, e.g. $\\pi(a|s) \\propto \\exp(\\alpha Q(s,a))$, a drift for Q learning(e.g. Q(s,a) = Q*(s,a) + c) has no effect on our policy. I believe in this case we should more focusing on the inner difference between different value of Q function, rather than comparing our estimate Q function with the true Q*. For the second concern, we can view the framework of maxmin q learning as a joint update scheme for different Q function. In experimental part, the comparison is not fair since the paper use multiple Q function to compare with single or double Q function. One reasonable baseline is to update N different Q function, and take the minimum of the final Q function as our decision policy, compare with maxmin Q learning with N different Q function. Another baseline the paper should consider is soft Q learning, where it maintain multiple Q function and jointly update Q different function to maximize the entropy while moving towards an improvement Q. Overall, I believe the idea of the paper is novel and interesting, but further improvements should be added in order to improve the score the paper.", "rating": "6: Weak Accept", "reply_text": "We appreciate your feedback . For the first concern , you are right , we can not know for an unknown environment whether overestimation or underestimation will help . This is exactly what we show in section 3 -- the optimal bias is environment dependent . So the optimal N is also environment dependent . N is a parameter that can be tuned , to specialize to each different problem . However , the theory does provide some guidance , namely by setting $ t_ { MN } =1/2 $ to remove bias . For example , when M=4 , a choice of N=3 reduces the bias to near 0 ( more results are shown in Figure 5 ) . This may not be the right choice , though , if the noise does not satisfy the assumptions . In our experiments , we found that the optimal N was usually between 2 to 9 , and that performance was usually improved with relatively small N. But , more work is needed to better understand more generally how to select N automatically . The MDP we present in section 3 is designed as a motivating example to show that overestimation and underestimation bias can both help and hurt . Note that we fixed the step-size and other hyperparameters for all algorithms ( instead of tuning them in order to achieve the best performance ) since this MDP was not used as a benchmark for performance comparison . It is just an illustrative example . We are not exactly sure what you mean by your comment that `` a drift for Q learning ( e.g. $ Q ( s , a ) = Q^ { * } ( s , a ) + c $ ) has no effect on our policy '' . What is c ? If it is a constant for all actions , absolutely it has no effect . But , the problem we are solving is that , due to stochasticity , a different constant could be added $ Q ( s , a ) $ for each $ a $ . This will effect the policy . If c is random , could you clarify further what you mean here ? You are right that our Maxmin Q-learning is a joint update scheme for different Q functions , and one of our contributions is that we provide a convergence proof for such a framework under reasonable assumptions . However , we politely disagree with your claim that our empirical comparison is unfair . Note that on Mountain Car ( Figure 3 ) , we compare Double Q-learning , Averaged Q-learning ( N=2 ) , and Maxmin Q-learning ( N=2 ) . Here , all three algorithms have two Q functions and Maxmin Q-learning shows significant robustness and achieves better performance . Similarly , in the other seven more complex environments , both Maxmin DQN and Averaged DQN learn multiple Q functions . And the number of Q functions is tuned in the same scope . Again , Maxmin DQN outperforms Averaged DQN . So we believe the comparison is fair . We are also unsure about what you mean by `` one reasonable baseline is to update N different Q function , and take the minimum of the final Q function as our decision policy . '' This could mean the following . N Q functions are learned with Q-learning ( rather than say with the Maxmin update ) . On each step , the agent selects actions using the max action from the minimum of the Q functions . ( If this is not what you intended , please do clarify ) . Theoretically , this strategy also incurs overestimation bias since it uses the same target action-value as Q-learning to update . Further it does not reduce estimation variance because it uses only one Q function to update . For Soft Q-learning ( SQL ) , we assume you mean the algorithm from the paper Reinforcement Learning with Deep Energy-Based Policies by Haarnoja et al. , ( again , please correct us if we are wrong ) . Maxmin Q-learning is a value-based method for discrete control to flexibly control bias and reduce variance . In contrast , SQL is a policy-based method for continuous control , with just one action-value function ( the policy is a Gibbs distribution on Q function ) . It is not clear why we would compare to SQL , since it only uses one Q function and tackles a different problem . If you can further clarify why we should compare to SQL , we would be happy to respond further ."}, "1": {"review_id": "Bkg0u3Etwr-1", "review_text": "The paper tackles the problem of bias in target Q-values when performing Q-learning. The paper proposes a technique for computing target Q-values, by first taking the min over an ensemble of learned Q-values and then taking the max over actions. The paper provides some theoretical properties of this technique: (1) the bias of the estimator can be somewhat controlled by the size of the ensemble; (2) performing Q-learning with these target values is convergent. Experimental results show that the proposed technique can provide performance improvement on a number of tasks. Overall, this paper is a modest contribution to the field, since variants of this technique are known and the theoretical arguments are derivatives of known arguments, which places it roughly borderline for an ICLR conference paper. My comments: -- The paper is very well-written. Thank you for putting the effort to provide clear writing. -- The idea of computing a target value as the minimum of an ensemble is well-known in continuous control. See https://arxiv.org/abs/1802.09477 as well as a number of works which follow it. -- The method is motivated as a way to control over/under-estimation. However, the theoretical arguments show that this depends on N, M, and tau (unknown). Are there any ways to choose N other than hyperparameter tuning?", "rating": "3: Weak Reject", "reply_text": "Thank you for your valuable comments . We will address your concerns point by point . First , thank you for pointing us to this control work , which we were not aware of . However , we would like to point out the significant difference between Maxmin Q-learning and TD3 in the paper you mentioned when computing the target value . For Maxmin Q-learning , we first take the minimum of action-values among all N estimators and then choose the action that maximizes these minimum action-values , i.e. $ Y^ { MQ } = r + \\gamma \\max_ { a \\in A } \\min_ { i=1 , \\cdots , N } Q^ { i } ( s , a ) $ . However , in TD3 , the action is chosen first by some policy $ \\pi $ , and then the minimum action-value is selected as the target value , i.e. $ Y^ { TD3 } = r + \\gamma \\min_ { i=1,2 } Q^ { i } ( s , \\pi ( s ) ) $ . The policy $ \\pi $ is expected to converge to the optimal policy , so $ Q^ { i } ( s , \\pi ( s ) ) \\approx \\max_ { a \\in A } Q^ { i } ( s , a ) $ . Thus , we have $ Y^ { TD3 } \\approx r + \\gamma \\min_ { i=1,2 } \\max_ { a \\in A } Q^ { i } ( s , a ) $ . Note that the order of taking minimum and maximum is different . By the max\u2013min inequality , it is easy to get $ Y^ { TD3 } > Y^ { MQ } $ . Furthermore , using the same method as in our paper , we can get $ E [ Y^ { TD3 } ] > E [ Y^ { True } ] $ where $ Y^ { True } = r + \\gamma \\max_ { a \\in A } Q^ { * } ( s , a ) $ . Thus TD3 still suffers from the overestimation bias , while we can adjust $ N $ in Maxmin Q-learning to reduce the bias from positive to negative . In conclusion , these two techniques may seem to be similar at first glance , but they are actually quite different and lead to different properties . We will cite the TD3 paper and add one more paragraph to discuss the difference in our revised paper . Also , there is no theoretical analysis for applying the minimum operator in TD3 paper , whereas we present a theoretical analysis not only for bias control but also for variance reduction . Actually , when we first tried to solve the overestimation bias problem , we also considered a similar approach to TD3 ( we called it Minmax Q-learning as a counterpart ) . However , after some derivatives and analyses , we found that Maxmin Q-learning would be better than Minmax Q-learning theoretically , in terms of reducing overestimation bias . These theoretical analyses guided our design of the Maxmin Q-learning algorithm . They are important and non-trivial . You are right that `` the theoretical arguments are derivatives of known arguments '' , but this is the case for many theoretical arguments . We have not come up with a new proof technique , but do have a novel theoretical result characterizing a new algorithm . Choosing the optimal N is not straightforward . It is a parameter that can be tuned , to specialize for each different problem setting . The theory does provide some guidance , namely by setting $ t_ { MN } =1/2 $ to remove bias . For example , when M=4 , a choice of N=3 reduces the bias to near 0 ( more results are shown in Figure 5 ) . This may not be the right choice , though , if the noise does not satisfy the assumptions . In our experiments , we found that the optimal N was usually between 2 to 9 , and that performance was usually improved with relatively small N. You are correct that future work should investigate how best to select N ."}, "2": {"review_id": "Bkg0u3Etwr-2", "review_text": "This paper proposes a novel variant of Q-learning, called Maxmin Q-learning, to address the issue of overestimation bias Q-learning suffers from (variance of the reward of best action leading to overestimated reward). The idea is to keep a number of estimators each estimated using a different sub-sample, and taking the minimum of the (maximum) reward value of each. The paper gives theoretical analyses, in terms of the reduction in the overestimation bias, as well as the convergence of a class of generalized Q-learning methods including Maxmin Q-learning. The experiment section presents a thorough evaluation of the proposed method, including how the obtained rewards vary as a function of the variance of the reward function and as a function of learning steps, as compared to a number of existing methods such as the Double Q-learning method and its variants. The experimental results are quite convincing, and the theoretical analyses seem solid. Overall this is a well balanced paper which proposes a reasonable new idea, simple but effective, backed by sound theoretical analysis and well executed experimental evaluation. ", "rating": "8: Accept", "reply_text": "Thank you for your positive comments !"}}