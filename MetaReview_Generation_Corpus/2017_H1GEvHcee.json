{"year": "2017", "forum": "H1GEvHcee", "title": "Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM", "decision": "Reject", "meta_review": "This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS.\n \n This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here.", "reviews": [{"review_id": "H1GEvHcee-0", "review_text": "The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM. A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported. It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work. 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed. 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly. 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.", "rating": "5: Marginally below acceptance threshold", "reply_text": "1.WRT the correctness and the computational cost : For the correctness , please see Appendix E , where we discuss the connection between annealing the leakiness and energy . We show that annealing the leakiness is the special case of annealing the energy by properly choosing a special distributions as the initial distribution . The theory of general AIS has been studied by several literatures ( e.g.Neal ( 1996 ) ) . Therefore , those theoretical guarantees without strong assumptions on the initial distributions are all applicable to sampling by annealing leakiness . For the computational concern , the inner loop is the same as standard CD except for one more flip-flops to decrease leakiness parameter in every iteration , which can be ignored compared with the projection step . The computational time of inner loop and the projection step is shown in Appendix F.2 . 2.WRT the GRBM : In Section 4.2 , we mention our implementation can reproduce the exactly same results as the code provided by http : //www.cs.toronto.edu/~tang/code/GaussianRBM.m Although we tried to grid learning rate between 1e-1~1e-6 with hand-tuned learning rate decay schedule and the momentum parameter between 0.9~0.1 , we can not reproduce the result from the same group ( e.g.Tang et. , al ( 2012 ) ) . As we mention in the paper , we conjecture the unsatisfactory results are caused by the parameter tuning and well-designed learning weight decay . We would appreciate if anyone can provide the right parameters to us and we can rerun the experiments.This also shows the difficulty of training Gaussian RBM with Birnary units ."}, {"review_id": "H1GEvHcee-1", "review_text": " Based on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning. Pro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function. However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier. Con: Because of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute. On the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens. This would demonstrate the superiority of the leaky ReLU hidden units. In addition, there are more results on binary MNIST modeling with which the authors can compare the results to. While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD. This paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "1.WRT binary RBM result : we put the result and the discussion in the revision . See Appendix F.3 . We study both toy datasets and two benchmarks , including MNIST and Caltech 101 . 2.WRT Baseline comparison ( Nair & Hinton , and spike-and-slab RBM ) : Based on Ravanbakhsh et al. \u2019 16 , the standard ReLU does not result in valid exponential family RBM and therefore even if it defines a valid distribution we do not know a close form for its joint distribution p ( v , h ) and marginals p ( v ) . This also means we can not evaluate the likelihood of standard ReLU RBM as defined in Nair & Hinton \u2019 10 . The heuristic sampling technique used in that paper samples using max ( 0 , x+N ( 0 , \u03c3 ( x ) ) . In expectation , this corresponds to a monotonic activation function f ( x ) = E [ max ( 0 , x+N ( 0 , \u03c3 ( x ) ) ] , which does not have a simple analytic form . Therefore we do not know the energy function associated with their sampling heuristic , which in turn prevents evaluation of the likelihood in that case . See also answer to a similar comment from reviewer 1 . The other extension , such as spike-and-slab RBM , is based on Nair & Hinton \u2019 10 . With the elementary calculation , the marginal distribution has one more integration without closed form than leaky RBM and it seems non-trivial to estimate it directly . Therefore , the likelihood evaluation is also unknown to the community . In the original papers of both works , they all consider the classification results only . We are the first work to show the baseline performance of RBM with the commonly-used ( leaky ) ReLU activation function and demonstrate that it could outperform Boolean hidden units . 3.WRT to the novelty of this paper : The analysis of this paper is based on Ravanbakhsh et. , al ( 2016 ) and Yang et. , al ( 2012 ) . The first contribution of this paper is connecting the leaky ReLU RBM and the notion of \u201c union \u201d of truncated Gaussian distribution ( Section 3.1 ) . We then propose the novel sampling strategy by utilizing the property of Gaussian distribution which benefits the partition function estimation ( Section 4 ) . The next contribution is we are able to give the baseline performance of RBM with the commonly-used ( leaky ) ReLU activation function in terms of the likelihood.We further show that the proposed sampling is better than the conventional Gibbs sampling by studying the optimization performance ( Section 5 ) ."}, {"review_id": "H1GEvHcee-2", "review_text": "The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN. This is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. Unfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper. PROS: Introduces an energy function having the leaky-relu as an activation function Introduces a novel sampling procedure based on annealing the leakiness parameter Similar sampling scheme shown to outperform AIS CONS: Results are somewhat out of date Missing experiments on binary datasets (more comparable to prior RBM work) Missing PCD baseline Cost of projection method ", "rating": "6: Marginally above acceptance threshold", "reply_text": "1.WRT binary dataset : we put the result and the discussion in the revision . See Appendix F.3 . We study both toy datasets and two benchmarks , including MNIST and Caltech 101 . 2.WRT PCD : We put the result of PCD in the Appendix F.3 . It shows PCD does not perform well as CD in the ReLU-RBM case though it has better performance in binary RBM case . Note that we show that the proposed \u201c annealing leakiness \u201d idea can also be used for learning . We observe that it has a better mixing than both CD and PCD , with no additional computation cost . Note that the additional cost of our training algorithm is due to projection ( i.e.the model ) rather than the new sampling procedure . 3.WRT to the projection cost : see Appendix F.2 ."}], "0": {"review_id": "H1GEvHcee-0", "review_text": "The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM. A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported. It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work. 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed. 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly. 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.", "rating": "5: Marginally below acceptance threshold", "reply_text": "1.WRT the correctness and the computational cost : For the correctness , please see Appendix E , where we discuss the connection between annealing the leakiness and energy . We show that annealing the leakiness is the special case of annealing the energy by properly choosing a special distributions as the initial distribution . The theory of general AIS has been studied by several literatures ( e.g.Neal ( 1996 ) ) . Therefore , those theoretical guarantees without strong assumptions on the initial distributions are all applicable to sampling by annealing leakiness . For the computational concern , the inner loop is the same as standard CD except for one more flip-flops to decrease leakiness parameter in every iteration , which can be ignored compared with the projection step . The computational time of inner loop and the projection step is shown in Appendix F.2 . 2.WRT the GRBM : In Section 4.2 , we mention our implementation can reproduce the exactly same results as the code provided by http : //www.cs.toronto.edu/~tang/code/GaussianRBM.m Although we tried to grid learning rate between 1e-1~1e-6 with hand-tuned learning rate decay schedule and the momentum parameter between 0.9~0.1 , we can not reproduce the result from the same group ( e.g.Tang et. , al ( 2012 ) ) . As we mention in the paper , we conjecture the unsatisfactory results are caused by the parameter tuning and well-designed learning weight decay . We would appreciate if anyone can provide the right parameters to us and we can rerun the experiments.This also shows the difficulty of training Gaussian RBM with Birnary units ."}, "1": {"review_id": "H1GEvHcee-1", "review_text": " Based on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning. Pro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function. However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier. Con: Because of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute. On the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens. This would demonstrate the superiority of the leaky ReLU hidden units. In addition, there are more results on binary MNIST modeling with which the authors can compare the results to. While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD. This paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "1.WRT binary RBM result : we put the result and the discussion in the revision . See Appendix F.3 . We study both toy datasets and two benchmarks , including MNIST and Caltech 101 . 2.WRT Baseline comparison ( Nair & Hinton , and spike-and-slab RBM ) : Based on Ravanbakhsh et al. \u2019 16 , the standard ReLU does not result in valid exponential family RBM and therefore even if it defines a valid distribution we do not know a close form for its joint distribution p ( v , h ) and marginals p ( v ) . This also means we can not evaluate the likelihood of standard ReLU RBM as defined in Nair & Hinton \u2019 10 . The heuristic sampling technique used in that paper samples using max ( 0 , x+N ( 0 , \u03c3 ( x ) ) . In expectation , this corresponds to a monotonic activation function f ( x ) = E [ max ( 0 , x+N ( 0 , \u03c3 ( x ) ) ] , which does not have a simple analytic form . Therefore we do not know the energy function associated with their sampling heuristic , which in turn prevents evaluation of the likelihood in that case . See also answer to a similar comment from reviewer 1 . The other extension , such as spike-and-slab RBM , is based on Nair & Hinton \u2019 10 . With the elementary calculation , the marginal distribution has one more integration without closed form than leaky RBM and it seems non-trivial to estimate it directly . Therefore , the likelihood evaluation is also unknown to the community . In the original papers of both works , they all consider the classification results only . We are the first work to show the baseline performance of RBM with the commonly-used ( leaky ) ReLU activation function and demonstrate that it could outperform Boolean hidden units . 3.WRT to the novelty of this paper : The analysis of this paper is based on Ravanbakhsh et. , al ( 2016 ) and Yang et. , al ( 2012 ) . The first contribution of this paper is connecting the leaky ReLU RBM and the notion of \u201c union \u201d of truncated Gaussian distribution ( Section 3.1 ) . We then propose the novel sampling strategy by utilizing the property of Gaussian distribution which benefits the partition function estimation ( Section 4 ) . The next contribution is we are able to give the baseline performance of RBM with the commonly-used ( leaky ) ReLU activation function in terms of the likelihood.We further show that the proposed sampling is better than the conventional Gibbs sampling by studying the optimization performance ( Section 5 ) ."}, "2": {"review_id": "H1GEvHcee-2", "review_text": "The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN. This is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. Unfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper. PROS: Introduces an energy function having the leaky-relu as an activation function Introduces a novel sampling procedure based on annealing the leakiness parameter Similar sampling scheme shown to outperform AIS CONS: Results are somewhat out of date Missing experiments on binary datasets (more comparable to prior RBM work) Missing PCD baseline Cost of projection method ", "rating": "6: Marginally above acceptance threshold", "reply_text": "1.WRT binary dataset : we put the result and the discussion in the revision . See Appendix F.3 . We study both toy datasets and two benchmarks , including MNIST and Caltech 101 . 2.WRT PCD : We put the result of PCD in the Appendix F.3 . It shows PCD does not perform well as CD in the ReLU-RBM case though it has better performance in binary RBM case . Note that we show that the proposed \u201c annealing leakiness \u201d idea can also be used for learning . We observe that it has a better mixing than both CD and PCD , with no additional computation cost . Note that the additional cost of our training algorithm is due to projection ( i.e.the model ) rather than the new sampling procedure . 3.WRT to the projection cost : see Appendix F.2 ."}}