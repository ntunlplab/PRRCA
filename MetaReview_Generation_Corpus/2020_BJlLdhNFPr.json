{"year": "2020", "forum": "BJlLdhNFPr", "title": "Explaining A Black-box By Using A Deep Variational Information Bottleneck Approach", "decision": "Reject", "meta_review": "The authors present a system-agnostic interpretable method based on the idea of that provides a brief (=compressed) but comprehensive (=informative) explanation. Their system is build upon the idea of VIB. The authors compare against 3 state-of-the-art interpretable machine learning methods and the evaluation is terms of interpretability (=human understandable) and fidelity (=accuracy of approximating black-box model). Overall, all reviewers agreed that the topic of model interpretability is an important one and the novel connection between IB and interpretable data-summaries is a very natural one.  \n\nThis manuscript has generated a lot of discussion among the reviewers during the rebuttal and there are a number of concerns that are currently preventing me from recommending this paper for acceptance. The first concern relates to the lack of comparison against attention methods (I agree with the authors that this is a model-specific solution whereas they propose a model-agnostic one), however attention is currently the elephant in room and the first thing someone thinks of when thinking of interpretability. As such, the authors should have presented such a comparison. The second concern relates to the human evaluation protocol which could be significantly improved  (Why 100 samples from all models but 200 for VIBI? Given the small set of results, are these model differences significant? Similarly, assuming that we have multiple annotations per sample, what is the variance in the annotations?).\n\nThis paper is currently borderline and given reviewers' concerns and the limited space in the conference program I cannot recommend acceptance of this paper. ", "reviews": [{"review_id": "BJlLdhNFPr-0", "review_text": "[Due to the rebuttal, my score was raised from a weak reject to a weak accept] Summary The paper addresses the problem of interpreting predictions/decisions of a black-box classifier/regressor by masking the parts of the input that were most relevant. The proposed approach consists of, first, manually designing \u201ccognitive chunks\u201d of input data, e.g. individual words for sentiment classification or fixed-size image-patches for image-classification. Then, a variational IB framework is used to infer which of these chunks are relevant for the classifier\u2019s decision. Additionally, there is a (hard) constraint, making sure that only a fixed (small) number of chunks is used. The bottleneck variable, in this case, is a sparse-chunk representation of the data. The latter is obviously a more compressed representation of the data, but importantly it is a more compressed representation that contains the largest possible amount of relevant information about the decision (because of properties of the information-bottleneck objective). Both factors together, according to the paper, constitute a \u201cgood\u201d (i.e. brief but comprehensive) explanation which allows for interpretability and attribution of the black-box system\u2019s decision. The method is evaluated on three tasks (sentiment prediction, image classification, TCR to epitope binding prediction) and performance is reported to be on-par or better than state-of-the-art methods. Contributions -) Application of the IB-method for generating summaries of decision-relevant input-data, which are good candidates for interpretability. The theoretical properties of the IB objective are appealing for producing interpretable data-summaries. -) Adaptation of the variational IB framework, using bits and pieces reported in the literature such that the bottleneck variable is a sparse, binary vector over \u201ccognitive chunks\u201d. -) Experimental evaluation, where human judges rate the \u201cinterpretability\u201c of various state-of-the-art attribution methods. Quality, Clarity, Novelty, Impact The paper addresses a timely and important problem, particularly the IB framework could add some solid theoretical footing (the \u201ctheory of relevant information\u201d) to the field of interpetability methods. The paper is well written (though it needs another pass for typos, etc.), related methods and literature are discussed and compared against, and the specific variational IB objective is introduced nicely. Large parts of the method (deep variational IB, VI with categorical variables) have been published before, but these parts are combined in a novel and original way. My main issues with the current paper are (I) interpretability and comprehensiveness are not necessarily the same as maximum compression of maximally relevant information, (II) the method (in theory) depends strongly on the quality of the approximator, this is currently not mentioned and not explored, (III) the experimental section is currently not very strong, in particular the MNIST experiment. See more details for the main issues below. Overall, I personally think that the main idea of the paper is interesting, mature and fleshed out enough for a publication, however, the experimental section is somewhat lacking and (II) is missing from the current manuscript. While the method has theoretical advantages, empirically it seems to perform more or less equal to L2X (but the chunks produced seem qualitatively different which is interesting). I am therefore slightly leaning towards suggesting a major revision of the paper, but I am happy to be convinced otherwise by the other reviewers and the authors during discussion/rebuttal. (I would rate the paper as \"borderline\", but it seems that this year's review system only allows for \"weak reject\" or \"weak accept\", so I'll go for \"weak reject\" for now). Improvements / major issues (I) Good compression of highly relevant information is not (always) the same as good interpretability/comprehensiveness. In the limit, the bottleneck variable captures a minimal sufficient statistic, i.e. a maximally compressed version of all relevant information - for finite beta, the bottleneck approximates such a minimal sufficient statistic. From a theoretical point of view this is very appealing, since it is guaranteed to cover a maximal amount of information (given a certain level of compression). But the way this information is represented matters a lot for interpretability - any reversible mapping of the bottleneck variable does not change its information content but can have substantial effects on interpretability, e.g. consider encrypting or randomly perturbing elements of the explanation (i.e. the selected cognitive chunks). This is a major open problem, and some theoretical grounding in the IB framework helps by talking about this problem in very concrete terms. While I would not expect the paper to solve the problem in full generality, some discussion, and perhaps adding a \u201cshortcomings\u201d section would be nice. (II) The relevance of information is measured via I(t;y), which ultimately boils down to the approximation q(y|t). The quality of this approximation is crucial, which can of course be seen by how it influences the tightness of the bound. While I appreciate that the paper investigates the quality of the approximation to some degree (by inspecting the approximator fidelity), I would highly appreciate a thorough discussion of this issue (because ultimately the method will produce cognitive chunks that are relevant for q(y|t), not p(y|x) - the interpretations can be trusted only if q matches fairly well). It would be very interesting to see how quickly interpretability degrades with lower-quality q(y|t) - the latter would of course require more experiments with human \u201cinterpreters\u201d which I would not expect to be easily feasible within the rebuttal period. Another interesting experiment to test the match between q(y|t) and p(y|x) would be to \u201cminimally intervene\u201d on the input-chunks suggested by the method and see whether that actually affects the predictions of the black-box models. E.g. do small random perturbations to the selected cognitive chunks in the MNIST digits change the prediction of the black-box classifier? Compare this against small random perturbations in arbitrary chunks of the input. (III) Experimental section: I\u2019m fairly happy with the IMDB experiment, and the TCR to Epitope binding is a nice non-standard application but I find the quality and significance of the results a bit hard to judge. My main concern though is the MNIST experiment: what I would have expected was the following: cognitive chunks are shown to participants and they need to guess the correct number (just like in the IMDB experiment). In the experiment reported in the paper, I\u2019m afraid that there\u2019s a certain bias for judges favoring explanations that lie on the digits rather than off digits. It remains unclear whether they simply prefer the chunks selected by VIBI over other methods, or whether they have actually gained more understanding of how the black-box makes decisions. (IV) Table 2 can easily be misleading because entries with highest mean-accuracy are marked in bold, regardless of whether confidence intervals overlap with other entries or not. Please fix this by either only marking entries in bold where the error bars don\u2019t overlap with an entry in the same row, or marking all entries in bold that lie within the error bars of the best-performing entry. Particularly for \u201cApproximate Fidelity\u201d VIBI often does not perform significantly better than L2X but performs roughly equally well. Of course it would also help to run more repetitions to potentially shrink confidence intervals. (V) Please state the (parametric form) of the prior r(z*) used for the experiments. Also state the analytical expression for the KL-term in the final objective that this prior leads to. Minor comments a) Please add some discussion on how the method depends on hand-crafting cognitive chunks, and how hard/easy this might be for different domains. b) Rather than fixing r(z), other papers have proposed to optimize the prior as well (typically in the context of VAEs / VIB) which is well justified from an IB perspective. It might be interesting to explore these possibilities for VIBI as well in the future. [1] Fixing a broken ELBO. Alemi et al. 2017 [2] The beta-VAE\u2019s Implicit Prior. Hoffman et al. 2017 c) Instead of fixing the number of cognitive chunks in advance, it could also be interesting to infer that number as well (as a future extension of the method). This could either be achieved via a sparsity-inducing prior r(z), or perhaps by borrowing some ideas from the Deterministic IB [3], and its variational version. [3] The deterministic information bottleneck. Strouse and Schwab. 2016", "rating": "6: Weak Accept", "reply_text": "Thank you for providing constructive comments and critiques . We believe this input has improved the quality of our work . # # Major : I ) Thank you for the insightful comment about possible shortcomings . We will add this to the revised manuscript . The VIBI 's explainer always returns a certain form of output ( a $ k $ -hot vector assigned to each chunk ) and the IB layer will always have a certain form ( a masked input ) . If we understand your comment correctly , this can help to address such concerns to some extent because the explanations have a form that are at least recognizable to humans . However , the hand-crafted chunking strategies change the way the information is represented . We think the minor comment a ) you made below is in line with this issue . We would appreciate it if you can make a further discussion about this issue . II ) Please see our next comment . III ) Thank you for carefully expressing the concerns about the experiment . The two evaluation strategies we used in the IMDB and MNIST analysis have pros and cons . The first strategy ( i.e. , asking participants on MTurk to infer the black-box output , like in the IMDB experiment ) is less subjective and has less variation among individual participants . However , it is a proxy approach to evaluate how ` good ' the explanations are : the measurements could be considered as human-evaluated fidelity than interpretability . For using this strategy , we need to assume that a ` good ' explanation allows humans to better infer the black-box output , which is not always true . For example , inferring the black-box output based on the ` good ' off-digit explanations is harder than the same ` good ' on-digit explanations . Also , think about the case when the consistency of the explanations between similar inputs is a matter for evaluating the goodness of explanations . The second strategy ( i.e.asking participants to directly score conciseness of the explanation , like in the MNIST experiment ) is a direct way to evaluate how ` good ' the explanations are , which can evaluate the ` goodness ' of interpretation based on a whole aspect of goodness to HUMAN , not just based on its fidelity . But it requires qualified survey participants who can evaluate the explanations based on whether they could gain a ` good ' understanding of how the black-box makes decisions . We strongly believe that the participants can evaluate based on whether they could gain more understanding of how the black-box makes decisions . We first would like to note that most of them have taken three or more graduate-level ML/DL classes and have two or more papers published in ML/DL or related fields . ( Please understand that we are afraid to reveal many details about them , which can be a clue to infer the author 's affiliation . ) Regarding the concerns that the participants may prefer explanations that lie on digits than off digits , we informed the participants that the ` good ' explanation does not need to be on the digits . Whether the chunks lie on or off digits , the participants actually gave higher scores if the chunks caught all necessary key characteristics of digits . Regarding the concerns that it is unclear whether they simply prefer the chunks over others or they have actually gained more understanding , we asked the participants to use the absolute scoring ( NOT the relative scoring ) based on the criteria in Figure 7 , which can help to reduce such concerns . We hope this helps to address your concerns about the experiments . IV ) We have fixed Table 2 , as suggested . Regarding the approximator fidelity of VIBI and L2X , we think it gives an insight into the benefit of using the compressiveness term ( i.e. , I ( x , t ) ) . Note that L2X can be viewed as a special case of VIBI that has no compressiveness term ( i.e. , $ beta = 0 $ ) . Since the Gumble-softmax trick uses the continuously-relaxed masked $ \\mathbf { x } $ as an input to the approximator for learning the models , the information may go through the non-selected chunks . Our experiment reported VIBI and L2X have similar approximator fidelity , which is the prediction performance of the approximator taking continuous-relaxed masked $ \\mathbf { x } $ as an input . However , VIBI outperforms L2X in the rationale fidelity , which is the prediction performance of the approximator taking hard-masked $ \\mathbf { x } $ as an input . This tells that the compressiveness term can help to prevent information leakage through the non-selected chunks . In other words , it helps the information pass mostly through the selected chunks . V ) We have stated the form of the prior and KL-term in the Supplementary Material B ( colored by red ) . # # Minor : a ) Please see our response to the major comment I. b - c ) Thank you for the constructive suggestions for our future work . We think the suggestions are very helpful . We also plan to extend this work by using better IB models ."}, {"review_id": "BJlLdhNFPr-1", "review_text": "The paper proposes a method to learn an explanation of black-box systems from its outputs. The method is based on the information bottleneck as the objective function is designed to measure mutual information between input x, system output y, and narrowed information of input t. t is constructed by filtering x with maintaining interpretability of y, so that it is finally assumed as the explanation of the system extracted by the proposed method. The paper is well motivated and well written. Enough experiments were conducted to assess the advantage of the proposed method in the classification tasks. It looks a good paper. Maybe the paper is focused on only tasks that the predictor does not generate much information, such as classification. It is still unclear how the proposed method work when it is applied to the output-rich models, i.e., the model should keep as much information as inputs. The proposed method automatically selects some important chunks from inputs, but the chunks still rely on some task-specific hand-crafted chunking strategies. The paper also conducted some experiments by changing the strategy, but it is still unclear what is the important criteria. It is also good to show how actually the thickness of the bottleneck (controlled by k) works in actual cases, e.g., showing results for the same example with moving k. Trivial comments: * The example in 3.2 \"great, great\" and \"great, thought provoking\" looks still ambiguous to explain what the section want to say. * \"x_i \\times z_j\" in p.iv looks ambiguous. * z_j^* in 3rd eqn. of p.v should take l: z_j^{*(l)} * the max operator over l in 3rd eqn. of p.v looks to hide other values than the highest one (specifically, the L1 norm of z_j^* does not become k by this eqn. as the k-hot vector does). Summation looks intuitively better than max. Could you explain how this eqn. was constructed? * f(.) in the 4th eqn. of p.v may be undefined in the main text. * \\beta_1 of Adam looks to be set to not a standard value (0.9). Is there any reason?", "rating": "6: Weak Accept", "reply_text": "Thank you for the detailed comments , and we hope that our revisions address your concerns . Comment1 : Maybe the paper is focused on only tasks that the predictor does not generate much information , such as classification . It is still unclear how the proposed method work when it is applied to the output-rich models , i.e. , the model should keep as much information as inputs . Answer 1 : ( Note - It is hard for us to understand what it means by 'the output-rich models . 'If you can give some examples of the output-rich models , it will be helpful . ) If we understand your comment correctly , your concern is that ( due to the compression procedure ) our model may not able to pass enough information to the outputs in the output-rich models . But this is not the case . The IB framework ( Tishby 2015 , 2000 ) provides a theoretical background that IB captures a minimal sufficient statistic , i.e.the most compressed representation that captures all the possible ( i.e.sufficient ) amount of information about output . Our IB approximates such a minimal sufficient statistic , so it approximately covers a sufficient amount of information about ( both rich and non-rich ) outputs . Comment 2 : The proposed method automatically selects some important chunks from inputs , but the chunks still rely on some task-specific hand-crafted chunking strategies . The paper also conducted some experiments by changing the strategy , but it is still unclear what is the important criteria . Answer 2 : Please see our next comment . Comment 3 : It is also good to show how actually the thickness of the bottleneck ( controlled by k ) works in actual cases , e.g. , showing results for the same example with moving k. Answer 3 : Thank you for the suggestion . We have added several qualitative examples with moving $ k = 4 , 6 , 10 , 20 $ to the revised manuscript ( Figure 8 in the supplementary material ) . Obviously , the compressiveness of explanations depends on the sparsity $ k $ ( i.e. , the number of cognitive chunks to be selected ) . A larger $ k $ allows the information bottleneck to convey more information about output , but it gives less compressive explanations than a smaller $ k $ . For deciding $ k $ , we recommend choosing the minimum possible $ k $ that achieves a target fidelity because an unnecessarily large $ k $ can make redundant explanations ( i.e. , chunks ) . Figure 8 shows how our method works under different sparsity . When we increase $ k $ , VIBI tends to select chunks that are the same or nearby the previously selected chunks and additionally select patches that catch new characteristics of digits . The most important explanations tend to be selected again at a larger $ k $ . We will add the above discussion on how our method works under different sparsity and the choice of $ k $ to the revised manuscript . If you have any , we are happy to have a further discussion about this . Trivial comments : 1 ) There were typographical errors . We 've revised `` great , thought provoking '' to `` great , thought-provoking '' . Also , we 've revised the whole paragraph as follows : For example , consider a movie review where `` great '' occurs a lot and two explanations in judging the sentiment of the review : `` great , great '' and `` great , thought-provoking '' . They have the same level of sparsity ( $ k = 2 $ ) , but the former has semantic redundancy . In this case , MI helps to choose a better explanation . The first explanation has a larger MI with the input document . The second explanation has smaller MI and hence is more brief and preferable . 2 ) Intuitively , $ \\mathbf { t } $ is the masked input of $ \\mathbf { x } $ having the masking indicator $ \\mathbf { z } $ . We hope Figure 1B help you to understand this formula . For example , the j-th ( $ j = 2 $ ) word `` cough '' is represented by three features $ [ x_4 , x_5 , x_6 ] $ and selected by the indicator $ z_2 = 1 $ . Now , its masked input is $ [ t_4 , t_5 , t_6 ] = [ x_4 \\times z_2 , x_5 \\times z_2 , x_6 \\times z_2 ] = [ x_4 , x_5 , x_6 ] $ . Similarly , the j-th ( $ j = 3 $ ) word `` a '' is represented by $ [ x_7 , x_8 , x_9 ] $ but not selected ( i.e. $ z_3 = 0 $ ) . Now , its masked input is $ [ t_7 , t_8 , t_9 ] = [ x_7 \\times z_3 , x_8 \\times z_3 , x_9 \\times z_3 ] = [ 0 , 0 , 0 ] $ . 3 ) $ z_j^ { * } $ should NOT take $ l $ . The $ l $ should be removed , since $ z_j^ { * } $ is the maximum value $ c_j^ { ( l ) } $ over all $ l $ 's . 4 ) Note that $ \\mathbf { z } ^ { * } $ is a continuous-version of the k-hot vector $ \\mathbf { z } $ . For example , we can have $ \\mathbf { z } ^ { * } = [ 0.99 , 0.01 , 0.08 , 0.90 ] $ as a continuous-version of the k-hot ( $ k = 2 $ ) vector $ \\mathbf { z } = [ 1 , 0 , 0 , 1 ] $ . Here , $ z_1^ { * } = 0.99 , z_2^ { * } = 0.01 , z_3^ { * } = 0.08 , z_4^ { * } = 0.90 $ and the L1 norm of $ \\mathbf { z } ^ { * } $ ( NOT $ \\mathbf { z } _j^ { * } $ ) will be approximately $ k ( = 2 ) $ which is the L1 norm of $ \\mathbf { z } $ . The L1 norm of $ \\mathbf { z } _j^ { * } $ should be approximately 1 , hence `` max '' is the right choice . 5 ) Thank you for pointing out . We 've added the definition of $ f ( . ) $ . Also , we 've revised the whole formula to make it more clear . 6 ) $ \\beta_1 $ of Adam is the default value provided by PyTorch ."}, {"review_id": "BJlLdhNFPr-2", "review_text": "\"an information theoretic principle, information bottleneck principle\" in the abstract is quite redundant with the use of 'principle' twice '\"great, great\" and \"great, thought provoking\". They have the same level of sparsity.' What kind of sparsity are you referring to with this example? Why can't sparsity reduce semantic redundancy? Please explain further. \"However, the first explanation has a large MI with the input document where \"great\" occurs a lot.\" What example input document are you referring to? You should save the explanation of how your method in Equation 2 differs from the original information bottleneck of Equation 1 until after you have actually written out Equation 2. As it is now, you are referencing Equation 2 before it has been seen. I find Equation 2 confusing. Is it possible to make the dependence of the expression on z more explicit. It isn't clear from the equation itself how p(z|x) influences either quantity in Equation 2. Perhaps you should wait to introduce this equation until you have first explained how z relates to x and t. As it is, z is not clearly defined. I can gather information about it from the figure, from how you describe the difference in your method from the original information bottleneck, but the relationship is not clear enough by reading only the text of the paper before Equation 2 is presented. Can you explain briefly how your \"hierarchical LSTM\" works in the main text of the paper? Its an unusual enough term that I would want to see a citation or brief explanation right away rather than having it deferred to the Appendix. Why not use a state-of-the-art model for IMDb? Are you not using the standard splits for IMDb? the In Appendix B.1 \"output vector is averaged and followed by log-softmax calculation. The final layer is formed to return a log-probability indicating which cognitive chunks should be taken as an input to the approximator\" The single output vector of the biLSTM is averaged and followed by log-softmax? the final layer is formed? What does this mean? I find the phrasing of \"Negative Sentiment if any negative words\" and the corresponding title for positive in Fig 2 confusing. What do you mean by \"if any\"? The phrasing makes it sound like the prediction of the model somehow depends on a logical step based on whether there are any negative/positive words found. I find the lack of a comparison to some kind of attentional method somewhat glaring in the IMDb example, since I would expect that many classifiers with attention would simply attend to the same words. What does your method give us that attention would not? The same can be said for the MNIST example regarding an attention map. \"by the human intelligences\" sounds quite robotic Can you provide some sense of inter annotator agreement for labeling the images and sentences? It does seem that there is key information like the definition of approximator fidelity in the appendices which is crucial to actually understanding the paper.", "rating": "3: Weak Reject", "reply_text": "Thank you for the detailed comments , and we hope that our revisions address your concerns . Here , we first address your 1 - 5th comments . We will address the 6 - 11th in the next post . Comment 1 and 2 ) ' '' great , great '' and `` great , thought provoking '' . They have the same level of sparsity . ' What kind of sparsity are you referring to with this example ? Why ca n't sparsity reduce semantic redundancy ? Please explain further . `` However , the first explanation has a large MI with the input document where `` great '' occurs a lot . '' What example input document are you referring to ? Answer 1 and 2 ) There were typographical errors . We 've revised `` great , thought provoking '' to `` great , thought-provoking '' . ( Note that `` thought-provoking '' is a word not two words ( https : //www.merriam-webster.com/dictionary/thought-provoking ) ) . Here , the sparsity indicates the number of words ( $ k = 2 $ ) . For better understanding , we have revised the section as follow : For example , consider a movie review where `` great '' occurs a lot and two explanations in judging the sentiment of the review : `` great , great '' and `` great , thought-provoking '' . They have the same level of sparsity ( $ k = 2 $ ) , but the former has semantic redundancy . In this case , MI helps to choose a better explanation . The first explanation has a larger MI with the input document . The second explanation has smaller MI and hence is more brief and preferable . Comment 3 ) You should save the explanation ... Answer 3 ) Thank for pointing out . We have moved the explanation to after Equation 2 . Comment 4 ) I find Equation 2 confusing . Is it possible to make the dependence of the expression on z more explicit . It is n't clear from the equation itself how p ( z|x ) influences either quantity in Equation 2 . Perhaps you should wait to introduce this equation until you have first explained how z relates to x and t. As it is , z is not clearly defined . I can gather information about it from the figure , from how you describe the difference in your method from the original information bottleneck , but the relationship is not clear enough by reading only the text of the paper before Equation 2 is presented . Answer 4 ) Thank you point out . In the revised manuscript , we have added the definition of $ z $ and moved the paragraph that explains $ z $ and its relationship $ x $ and $ t $ right after the equation ( 2 ) 's paragraph . Comment 5 ) Can you explain briefly how your `` hierarchical LSTM '' works in the main text of the paper ? Its an unusual enough term that I would want to see a citation or brief explanation right away rather than having it deferred to the Appendix . Why not use a state-of-the-art model for IMDb ? Are you not using the standard splits for IMDb ? the Answer 5 ) ( 1 ) We have added a sentence to it . We did not include much information about the black-box models because black-box model itself is NOT essential and how good the black-box model is NOT of interest . This is the reason that we do not use the state-of-the-art model for IMDB . What we are interested in this paper is to EXPLAIN black-box models no matter how good the black-box models are . For example , in our TCR experiment , we explained a black-box model having a BAD performance and showed that VIBI 's explanation could be used to improve the performance . In the IMDB and MNIST experiment , we used CNN for MNIST and LSTM for IMDB because they are well-known so that people already have some idea how the models work , which can help to assess the quality of explanations provided by VIBI and others themselves . ( 2 ) It is not clear what `` splits '' you are asking in the last sentence . We are guessing you are talking about dataset splits for training/validation/test sets . ( Please clarify if this is not what you intended . ) The original split provided by PyTorch is 25,000 for training and 25,000 for testing , which does not have samples for validation . Therefore , we split the 25,000 test set into 12,500 validation and 12,500 test sets . Similarly , we also did it for the MNIST ."}], "0": {"review_id": "BJlLdhNFPr-0", "review_text": "[Due to the rebuttal, my score was raised from a weak reject to a weak accept] Summary The paper addresses the problem of interpreting predictions/decisions of a black-box classifier/regressor by masking the parts of the input that were most relevant. The proposed approach consists of, first, manually designing \u201ccognitive chunks\u201d of input data, e.g. individual words for sentiment classification or fixed-size image-patches for image-classification. Then, a variational IB framework is used to infer which of these chunks are relevant for the classifier\u2019s decision. Additionally, there is a (hard) constraint, making sure that only a fixed (small) number of chunks is used. The bottleneck variable, in this case, is a sparse-chunk representation of the data. The latter is obviously a more compressed representation of the data, but importantly it is a more compressed representation that contains the largest possible amount of relevant information about the decision (because of properties of the information-bottleneck objective). Both factors together, according to the paper, constitute a \u201cgood\u201d (i.e. brief but comprehensive) explanation which allows for interpretability and attribution of the black-box system\u2019s decision. The method is evaluated on three tasks (sentiment prediction, image classification, TCR to epitope binding prediction) and performance is reported to be on-par or better than state-of-the-art methods. Contributions -) Application of the IB-method for generating summaries of decision-relevant input-data, which are good candidates for interpretability. The theoretical properties of the IB objective are appealing for producing interpretable data-summaries. -) Adaptation of the variational IB framework, using bits and pieces reported in the literature such that the bottleneck variable is a sparse, binary vector over \u201ccognitive chunks\u201d. -) Experimental evaluation, where human judges rate the \u201cinterpretability\u201c of various state-of-the-art attribution methods. Quality, Clarity, Novelty, Impact The paper addresses a timely and important problem, particularly the IB framework could add some solid theoretical footing (the \u201ctheory of relevant information\u201d) to the field of interpetability methods. The paper is well written (though it needs another pass for typos, etc.), related methods and literature are discussed and compared against, and the specific variational IB objective is introduced nicely. Large parts of the method (deep variational IB, VI with categorical variables) have been published before, but these parts are combined in a novel and original way. My main issues with the current paper are (I) interpretability and comprehensiveness are not necessarily the same as maximum compression of maximally relevant information, (II) the method (in theory) depends strongly on the quality of the approximator, this is currently not mentioned and not explored, (III) the experimental section is currently not very strong, in particular the MNIST experiment. See more details for the main issues below. Overall, I personally think that the main idea of the paper is interesting, mature and fleshed out enough for a publication, however, the experimental section is somewhat lacking and (II) is missing from the current manuscript. While the method has theoretical advantages, empirically it seems to perform more or less equal to L2X (but the chunks produced seem qualitatively different which is interesting). I am therefore slightly leaning towards suggesting a major revision of the paper, but I am happy to be convinced otherwise by the other reviewers and the authors during discussion/rebuttal. (I would rate the paper as \"borderline\", but it seems that this year's review system only allows for \"weak reject\" or \"weak accept\", so I'll go for \"weak reject\" for now). Improvements / major issues (I) Good compression of highly relevant information is not (always) the same as good interpretability/comprehensiveness. In the limit, the bottleneck variable captures a minimal sufficient statistic, i.e. a maximally compressed version of all relevant information - for finite beta, the bottleneck approximates such a minimal sufficient statistic. From a theoretical point of view this is very appealing, since it is guaranteed to cover a maximal amount of information (given a certain level of compression). But the way this information is represented matters a lot for interpretability - any reversible mapping of the bottleneck variable does not change its information content but can have substantial effects on interpretability, e.g. consider encrypting or randomly perturbing elements of the explanation (i.e. the selected cognitive chunks). This is a major open problem, and some theoretical grounding in the IB framework helps by talking about this problem in very concrete terms. While I would not expect the paper to solve the problem in full generality, some discussion, and perhaps adding a \u201cshortcomings\u201d section would be nice. (II) The relevance of information is measured via I(t;y), which ultimately boils down to the approximation q(y|t). The quality of this approximation is crucial, which can of course be seen by how it influences the tightness of the bound. While I appreciate that the paper investigates the quality of the approximation to some degree (by inspecting the approximator fidelity), I would highly appreciate a thorough discussion of this issue (because ultimately the method will produce cognitive chunks that are relevant for q(y|t), not p(y|x) - the interpretations can be trusted only if q matches fairly well). It would be very interesting to see how quickly interpretability degrades with lower-quality q(y|t) - the latter would of course require more experiments with human \u201cinterpreters\u201d which I would not expect to be easily feasible within the rebuttal period. Another interesting experiment to test the match between q(y|t) and p(y|x) would be to \u201cminimally intervene\u201d on the input-chunks suggested by the method and see whether that actually affects the predictions of the black-box models. E.g. do small random perturbations to the selected cognitive chunks in the MNIST digits change the prediction of the black-box classifier? Compare this against small random perturbations in arbitrary chunks of the input. (III) Experimental section: I\u2019m fairly happy with the IMDB experiment, and the TCR to Epitope binding is a nice non-standard application but I find the quality and significance of the results a bit hard to judge. My main concern though is the MNIST experiment: what I would have expected was the following: cognitive chunks are shown to participants and they need to guess the correct number (just like in the IMDB experiment). In the experiment reported in the paper, I\u2019m afraid that there\u2019s a certain bias for judges favoring explanations that lie on the digits rather than off digits. It remains unclear whether they simply prefer the chunks selected by VIBI over other methods, or whether they have actually gained more understanding of how the black-box makes decisions. (IV) Table 2 can easily be misleading because entries with highest mean-accuracy are marked in bold, regardless of whether confidence intervals overlap with other entries or not. Please fix this by either only marking entries in bold where the error bars don\u2019t overlap with an entry in the same row, or marking all entries in bold that lie within the error bars of the best-performing entry. Particularly for \u201cApproximate Fidelity\u201d VIBI often does not perform significantly better than L2X but performs roughly equally well. Of course it would also help to run more repetitions to potentially shrink confidence intervals. (V) Please state the (parametric form) of the prior r(z*) used for the experiments. Also state the analytical expression for the KL-term in the final objective that this prior leads to. Minor comments a) Please add some discussion on how the method depends on hand-crafting cognitive chunks, and how hard/easy this might be for different domains. b) Rather than fixing r(z), other papers have proposed to optimize the prior as well (typically in the context of VAEs / VIB) which is well justified from an IB perspective. It might be interesting to explore these possibilities for VIBI as well in the future. [1] Fixing a broken ELBO. Alemi et al. 2017 [2] The beta-VAE\u2019s Implicit Prior. Hoffman et al. 2017 c) Instead of fixing the number of cognitive chunks in advance, it could also be interesting to infer that number as well (as a future extension of the method). This could either be achieved via a sparsity-inducing prior r(z), or perhaps by borrowing some ideas from the Deterministic IB [3], and its variational version. [3] The deterministic information bottleneck. Strouse and Schwab. 2016", "rating": "6: Weak Accept", "reply_text": "Thank you for providing constructive comments and critiques . We believe this input has improved the quality of our work . # # Major : I ) Thank you for the insightful comment about possible shortcomings . We will add this to the revised manuscript . The VIBI 's explainer always returns a certain form of output ( a $ k $ -hot vector assigned to each chunk ) and the IB layer will always have a certain form ( a masked input ) . If we understand your comment correctly , this can help to address such concerns to some extent because the explanations have a form that are at least recognizable to humans . However , the hand-crafted chunking strategies change the way the information is represented . We think the minor comment a ) you made below is in line with this issue . We would appreciate it if you can make a further discussion about this issue . II ) Please see our next comment . III ) Thank you for carefully expressing the concerns about the experiment . The two evaluation strategies we used in the IMDB and MNIST analysis have pros and cons . The first strategy ( i.e. , asking participants on MTurk to infer the black-box output , like in the IMDB experiment ) is less subjective and has less variation among individual participants . However , it is a proxy approach to evaluate how ` good ' the explanations are : the measurements could be considered as human-evaluated fidelity than interpretability . For using this strategy , we need to assume that a ` good ' explanation allows humans to better infer the black-box output , which is not always true . For example , inferring the black-box output based on the ` good ' off-digit explanations is harder than the same ` good ' on-digit explanations . Also , think about the case when the consistency of the explanations between similar inputs is a matter for evaluating the goodness of explanations . The second strategy ( i.e.asking participants to directly score conciseness of the explanation , like in the MNIST experiment ) is a direct way to evaluate how ` good ' the explanations are , which can evaluate the ` goodness ' of interpretation based on a whole aspect of goodness to HUMAN , not just based on its fidelity . But it requires qualified survey participants who can evaluate the explanations based on whether they could gain a ` good ' understanding of how the black-box makes decisions . We strongly believe that the participants can evaluate based on whether they could gain more understanding of how the black-box makes decisions . We first would like to note that most of them have taken three or more graduate-level ML/DL classes and have two or more papers published in ML/DL or related fields . ( Please understand that we are afraid to reveal many details about them , which can be a clue to infer the author 's affiliation . ) Regarding the concerns that the participants may prefer explanations that lie on digits than off digits , we informed the participants that the ` good ' explanation does not need to be on the digits . Whether the chunks lie on or off digits , the participants actually gave higher scores if the chunks caught all necessary key characteristics of digits . Regarding the concerns that it is unclear whether they simply prefer the chunks over others or they have actually gained more understanding , we asked the participants to use the absolute scoring ( NOT the relative scoring ) based on the criteria in Figure 7 , which can help to reduce such concerns . We hope this helps to address your concerns about the experiments . IV ) We have fixed Table 2 , as suggested . Regarding the approximator fidelity of VIBI and L2X , we think it gives an insight into the benefit of using the compressiveness term ( i.e. , I ( x , t ) ) . Note that L2X can be viewed as a special case of VIBI that has no compressiveness term ( i.e. , $ beta = 0 $ ) . Since the Gumble-softmax trick uses the continuously-relaxed masked $ \\mathbf { x } $ as an input to the approximator for learning the models , the information may go through the non-selected chunks . Our experiment reported VIBI and L2X have similar approximator fidelity , which is the prediction performance of the approximator taking continuous-relaxed masked $ \\mathbf { x } $ as an input . However , VIBI outperforms L2X in the rationale fidelity , which is the prediction performance of the approximator taking hard-masked $ \\mathbf { x } $ as an input . This tells that the compressiveness term can help to prevent information leakage through the non-selected chunks . In other words , it helps the information pass mostly through the selected chunks . V ) We have stated the form of the prior and KL-term in the Supplementary Material B ( colored by red ) . # # Minor : a ) Please see our response to the major comment I. b - c ) Thank you for the constructive suggestions for our future work . We think the suggestions are very helpful . We also plan to extend this work by using better IB models ."}, "1": {"review_id": "BJlLdhNFPr-1", "review_text": "The paper proposes a method to learn an explanation of black-box systems from its outputs. The method is based on the information bottleneck as the objective function is designed to measure mutual information between input x, system output y, and narrowed information of input t. t is constructed by filtering x with maintaining interpretability of y, so that it is finally assumed as the explanation of the system extracted by the proposed method. The paper is well motivated and well written. Enough experiments were conducted to assess the advantage of the proposed method in the classification tasks. It looks a good paper. Maybe the paper is focused on only tasks that the predictor does not generate much information, such as classification. It is still unclear how the proposed method work when it is applied to the output-rich models, i.e., the model should keep as much information as inputs. The proposed method automatically selects some important chunks from inputs, but the chunks still rely on some task-specific hand-crafted chunking strategies. The paper also conducted some experiments by changing the strategy, but it is still unclear what is the important criteria. It is also good to show how actually the thickness of the bottleneck (controlled by k) works in actual cases, e.g., showing results for the same example with moving k. Trivial comments: * The example in 3.2 \"great, great\" and \"great, thought provoking\" looks still ambiguous to explain what the section want to say. * \"x_i \\times z_j\" in p.iv looks ambiguous. * z_j^* in 3rd eqn. of p.v should take l: z_j^{*(l)} * the max operator over l in 3rd eqn. of p.v looks to hide other values than the highest one (specifically, the L1 norm of z_j^* does not become k by this eqn. as the k-hot vector does). Summation looks intuitively better than max. Could you explain how this eqn. was constructed? * f(.) in the 4th eqn. of p.v may be undefined in the main text. * \\beta_1 of Adam looks to be set to not a standard value (0.9). Is there any reason?", "rating": "6: Weak Accept", "reply_text": "Thank you for the detailed comments , and we hope that our revisions address your concerns . Comment1 : Maybe the paper is focused on only tasks that the predictor does not generate much information , such as classification . It is still unclear how the proposed method work when it is applied to the output-rich models , i.e. , the model should keep as much information as inputs . Answer 1 : ( Note - It is hard for us to understand what it means by 'the output-rich models . 'If you can give some examples of the output-rich models , it will be helpful . ) If we understand your comment correctly , your concern is that ( due to the compression procedure ) our model may not able to pass enough information to the outputs in the output-rich models . But this is not the case . The IB framework ( Tishby 2015 , 2000 ) provides a theoretical background that IB captures a minimal sufficient statistic , i.e.the most compressed representation that captures all the possible ( i.e.sufficient ) amount of information about output . Our IB approximates such a minimal sufficient statistic , so it approximately covers a sufficient amount of information about ( both rich and non-rich ) outputs . Comment 2 : The proposed method automatically selects some important chunks from inputs , but the chunks still rely on some task-specific hand-crafted chunking strategies . The paper also conducted some experiments by changing the strategy , but it is still unclear what is the important criteria . Answer 2 : Please see our next comment . Comment 3 : It is also good to show how actually the thickness of the bottleneck ( controlled by k ) works in actual cases , e.g. , showing results for the same example with moving k. Answer 3 : Thank you for the suggestion . We have added several qualitative examples with moving $ k = 4 , 6 , 10 , 20 $ to the revised manuscript ( Figure 8 in the supplementary material ) . Obviously , the compressiveness of explanations depends on the sparsity $ k $ ( i.e. , the number of cognitive chunks to be selected ) . A larger $ k $ allows the information bottleneck to convey more information about output , but it gives less compressive explanations than a smaller $ k $ . For deciding $ k $ , we recommend choosing the minimum possible $ k $ that achieves a target fidelity because an unnecessarily large $ k $ can make redundant explanations ( i.e. , chunks ) . Figure 8 shows how our method works under different sparsity . When we increase $ k $ , VIBI tends to select chunks that are the same or nearby the previously selected chunks and additionally select patches that catch new characteristics of digits . The most important explanations tend to be selected again at a larger $ k $ . We will add the above discussion on how our method works under different sparsity and the choice of $ k $ to the revised manuscript . If you have any , we are happy to have a further discussion about this . Trivial comments : 1 ) There were typographical errors . We 've revised `` great , thought provoking '' to `` great , thought-provoking '' . Also , we 've revised the whole paragraph as follows : For example , consider a movie review where `` great '' occurs a lot and two explanations in judging the sentiment of the review : `` great , great '' and `` great , thought-provoking '' . They have the same level of sparsity ( $ k = 2 $ ) , but the former has semantic redundancy . In this case , MI helps to choose a better explanation . The first explanation has a larger MI with the input document . The second explanation has smaller MI and hence is more brief and preferable . 2 ) Intuitively , $ \\mathbf { t } $ is the masked input of $ \\mathbf { x } $ having the masking indicator $ \\mathbf { z } $ . We hope Figure 1B help you to understand this formula . For example , the j-th ( $ j = 2 $ ) word `` cough '' is represented by three features $ [ x_4 , x_5 , x_6 ] $ and selected by the indicator $ z_2 = 1 $ . Now , its masked input is $ [ t_4 , t_5 , t_6 ] = [ x_4 \\times z_2 , x_5 \\times z_2 , x_6 \\times z_2 ] = [ x_4 , x_5 , x_6 ] $ . Similarly , the j-th ( $ j = 3 $ ) word `` a '' is represented by $ [ x_7 , x_8 , x_9 ] $ but not selected ( i.e. $ z_3 = 0 $ ) . Now , its masked input is $ [ t_7 , t_8 , t_9 ] = [ x_7 \\times z_3 , x_8 \\times z_3 , x_9 \\times z_3 ] = [ 0 , 0 , 0 ] $ . 3 ) $ z_j^ { * } $ should NOT take $ l $ . The $ l $ should be removed , since $ z_j^ { * } $ is the maximum value $ c_j^ { ( l ) } $ over all $ l $ 's . 4 ) Note that $ \\mathbf { z } ^ { * } $ is a continuous-version of the k-hot vector $ \\mathbf { z } $ . For example , we can have $ \\mathbf { z } ^ { * } = [ 0.99 , 0.01 , 0.08 , 0.90 ] $ as a continuous-version of the k-hot ( $ k = 2 $ ) vector $ \\mathbf { z } = [ 1 , 0 , 0 , 1 ] $ . Here , $ z_1^ { * } = 0.99 , z_2^ { * } = 0.01 , z_3^ { * } = 0.08 , z_4^ { * } = 0.90 $ and the L1 norm of $ \\mathbf { z } ^ { * } $ ( NOT $ \\mathbf { z } _j^ { * } $ ) will be approximately $ k ( = 2 ) $ which is the L1 norm of $ \\mathbf { z } $ . The L1 norm of $ \\mathbf { z } _j^ { * } $ should be approximately 1 , hence `` max '' is the right choice . 5 ) Thank you for pointing out . We 've added the definition of $ f ( . ) $ . Also , we 've revised the whole formula to make it more clear . 6 ) $ \\beta_1 $ of Adam is the default value provided by PyTorch ."}, "2": {"review_id": "BJlLdhNFPr-2", "review_text": "\"an information theoretic principle, information bottleneck principle\" in the abstract is quite redundant with the use of 'principle' twice '\"great, great\" and \"great, thought provoking\". They have the same level of sparsity.' What kind of sparsity are you referring to with this example? Why can't sparsity reduce semantic redundancy? Please explain further. \"However, the first explanation has a large MI with the input document where \"great\" occurs a lot.\" What example input document are you referring to? You should save the explanation of how your method in Equation 2 differs from the original information bottleneck of Equation 1 until after you have actually written out Equation 2. As it is now, you are referencing Equation 2 before it has been seen. I find Equation 2 confusing. Is it possible to make the dependence of the expression on z more explicit. It isn't clear from the equation itself how p(z|x) influences either quantity in Equation 2. Perhaps you should wait to introduce this equation until you have first explained how z relates to x and t. As it is, z is not clearly defined. I can gather information about it from the figure, from how you describe the difference in your method from the original information bottleneck, but the relationship is not clear enough by reading only the text of the paper before Equation 2 is presented. Can you explain briefly how your \"hierarchical LSTM\" works in the main text of the paper? Its an unusual enough term that I would want to see a citation or brief explanation right away rather than having it deferred to the Appendix. Why not use a state-of-the-art model for IMDb? Are you not using the standard splits for IMDb? the In Appendix B.1 \"output vector is averaged and followed by log-softmax calculation. The final layer is formed to return a log-probability indicating which cognitive chunks should be taken as an input to the approximator\" The single output vector of the biLSTM is averaged and followed by log-softmax? the final layer is formed? What does this mean? I find the phrasing of \"Negative Sentiment if any negative words\" and the corresponding title for positive in Fig 2 confusing. What do you mean by \"if any\"? The phrasing makes it sound like the prediction of the model somehow depends on a logical step based on whether there are any negative/positive words found. I find the lack of a comparison to some kind of attentional method somewhat glaring in the IMDb example, since I would expect that many classifiers with attention would simply attend to the same words. What does your method give us that attention would not? The same can be said for the MNIST example regarding an attention map. \"by the human intelligences\" sounds quite robotic Can you provide some sense of inter annotator agreement for labeling the images and sentences? It does seem that there is key information like the definition of approximator fidelity in the appendices which is crucial to actually understanding the paper.", "rating": "3: Weak Reject", "reply_text": "Thank you for the detailed comments , and we hope that our revisions address your concerns . Here , we first address your 1 - 5th comments . We will address the 6 - 11th in the next post . Comment 1 and 2 ) ' '' great , great '' and `` great , thought provoking '' . They have the same level of sparsity . ' What kind of sparsity are you referring to with this example ? Why ca n't sparsity reduce semantic redundancy ? Please explain further . `` However , the first explanation has a large MI with the input document where `` great '' occurs a lot . '' What example input document are you referring to ? Answer 1 and 2 ) There were typographical errors . We 've revised `` great , thought provoking '' to `` great , thought-provoking '' . ( Note that `` thought-provoking '' is a word not two words ( https : //www.merriam-webster.com/dictionary/thought-provoking ) ) . Here , the sparsity indicates the number of words ( $ k = 2 $ ) . For better understanding , we have revised the section as follow : For example , consider a movie review where `` great '' occurs a lot and two explanations in judging the sentiment of the review : `` great , great '' and `` great , thought-provoking '' . They have the same level of sparsity ( $ k = 2 $ ) , but the former has semantic redundancy . In this case , MI helps to choose a better explanation . The first explanation has a larger MI with the input document . The second explanation has smaller MI and hence is more brief and preferable . Comment 3 ) You should save the explanation ... Answer 3 ) Thank for pointing out . We have moved the explanation to after Equation 2 . Comment 4 ) I find Equation 2 confusing . Is it possible to make the dependence of the expression on z more explicit . It is n't clear from the equation itself how p ( z|x ) influences either quantity in Equation 2 . Perhaps you should wait to introduce this equation until you have first explained how z relates to x and t. As it is , z is not clearly defined . I can gather information about it from the figure , from how you describe the difference in your method from the original information bottleneck , but the relationship is not clear enough by reading only the text of the paper before Equation 2 is presented . Answer 4 ) Thank you point out . In the revised manuscript , we have added the definition of $ z $ and moved the paragraph that explains $ z $ and its relationship $ x $ and $ t $ right after the equation ( 2 ) 's paragraph . Comment 5 ) Can you explain briefly how your `` hierarchical LSTM '' works in the main text of the paper ? Its an unusual enough term that I would want to see a citation or brief explanation right away rather than having it deferred to the Appendix . Why not use a state-of-the-art model for IMDb ? Are you not using the standard splits for IMDb ? the Answer 5 ) ( 1 ) We have added a sentence to it . We did not include much information about the black-box models because black-box model itself is NOT essential and how good the black-box model is NOT of interest . This is the reason that we do not use the state-of-the-art model for IMDB . What we are interested in this paper is to EXPLAIN black-box models no matter how good the black-box models are . For example , in our TCR experiment , we explained a black-box model having a BAD performance and showed that VIBI 's explanation could be used to improve the performance . In the IMDB and MNIST experiment , we used CNN for MNIST and LSTM for IMDB because they are well-known so that people already have some idea how the models work , which can help to assess the quality of explanations provided by VIBI and others themselves . ( 2 ) It is not clear what `` splits '' you are asking in the last sentence . We are guessing you are talking about dataset splits for training/validation/test sets . ( Please clarify if this is not what you intended . ) The original split provided by PyTorch is 25,000 for training and 25,000 for testing , which does not have samples for validation . Therefore , we split the 25,000 test set into 12,500 validation and 12,500 test sets . Similarly , we also did it for the MNIST ."}}