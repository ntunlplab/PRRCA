{"year": "2019", "forum": "BygANhA9tQ", "title": "Cost-Sensitive Robustness against Adversarial Examples", "decision": "Accept (Poster)", "meta_review": "This paper studies the notion of certified cost-sensitive robustness against adversarial examples, by building from the recent [Wong & Koller'18]. Its main contribution is to adapt the robust classification objective to a 'cost-sensitive' objective, that weights labelling errors according to their potential damage. \nThis paper received mixed reviews, with a clear champion and two skeptical reviewers. On the one hand, they all highlighted the clarity of the presentation and the relevance of the topic as strengths; on the other hand, they noted the relatively little novelty of the paper relative [W & K'18]. Reviewers also acknowledged the diligence of authors during the response phase. The AC mostly agrees with these assessments, and taking them all into consideration, he/she concludes that the potential practical benefits of cost-sensitive certified robustness outweight the limited scientific novelty. Therefore, he recommends acceptance as a poster. ", "reviews": [{"review_id": "BygANhA9tQ-0", "review_text": "The authors define the notion of cost-sensitive robustness, which measures the seriousness of adversarial attack with a cost matrix. The authors then plug the costs of adversarial attack into the objective of optimization to get a model that is (cost-sensitively) robust against adversarial attacks. The initiative is novel and interesting. Considering the long history of cost-sensitive learning, the proposed model is rather ad-hoc for two reasons: (1) It is not clear why the objective should take the form of (3.1). In particular, if using the logistic function as a surrogate for 0-1 loss, shouldn't the sum of cost be in front of \"log\"? If using the probability estimated from the network in a Meta-Cost guided sense, shouldn't the cost be multiplied by the probability estimate (like 1/(1+exp(...))) instead of the exp itself? The mysterious design of (3.1) makes no physical sense to me, or at least other designs used in previous cost-sensitive neural network models like Chung et al., Cost-aware pre-training for multiclass cost-sensitive deep learning, IJCAI 2016 Zhou and Liu, Training cost-sensitive neural networks with methods addressing the class imbalance problem, TKDE 2006 (which is cited by the authors) are not discussed nor compared. Update: I thank the authors for providing updated information in the Appendix discussing about other alternatives. While I still think it worth comparing with other approaches (as it is still not clear whether Khan's approach is regarded as state-of-the-art for *general* cost-sensitive deep learning), I think the authors have sufficiently justified their choice. (2) It is not clear why the perturbed example should take the cost-sensitive form, while the original examples shouldn't (as the original examples follow the original loss). Or alternatively, if we optimize the original examples by the cost-sensitive loss, would it naturally achieve some cost-sensitive robustness (as the model would naturally make it harder to make high-cost mistakes)? Those issues are yet to be studied. Update: I thank the authors for providing additional experiments on this part. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . Your comments about the model being ad hoc stem from a few misunderstandings , which we hope to clarify : 1 . Justification of training objective ( 3.1 ) The design of ( 3.1 ) is not ad hoc , but follows from previous cost-sensitive learning work such as MetaCost , and is inspired by the cost-sensitive CE loss ( see equation ( 10 ) of [ 1 ] for a detailed definition ) . To be specific , class probabilities for cost-sensitive CE loss are computed by multiplying the corresponding cost and then normalizing the result vector . As a result , transformations that induce larger cost will receive larger penalization by minimizing the cost-sensitive CE loss . We neglected to include this explanation in the paper , and will revise it to make this clear . For the first question , moving the sum of cost in front of \u201c log \u201d is unreasonable because the loss for each seed example will not be a negative log-likelihood term as in the case of cross-entropy . We can check the sanity of the objective by examining whether it reduces to standard CE loss if we set C = 1 * 1^\\top-I . For the second question , we indeed multiply the probability estimates by the cost , but the result vector has to be normalized before plugging into the cross entropy loss . Thus , the sum of cost will appear in front of the \u201c exp \u201d term . 2.Comparison with other alternative designs The cost-sensitive neural network models you mentioned are only demonstrated to be effective in the non-adversarial settings , whereas we show that our proposed classifier is effective in the adversarial setting . Thus , comparing our method with theirs is not appropriate , since it is unclear whether such alternative cost-sensitive models can be adapted and remain effective in the adversarial setting . Even if they can be adapted , it is still not the main focus of our paper , as our main goal is to show that our proposed classifier achieves significant improvements in cost-sensitive robustness in comparison with models trained for overall robustness . 3.Why are the original examples are not in cost-sensitive form ? The training objective ( 3.1 ) is constructed for maximizing both cost-sensitive robustness and standard classification accuracy , and allows us to use the alpha hyperparameter to control the weighting between these goals . Thus , the first term in ( 3.1 ) doesn \u2019 t involve cost-sensitivity . We regard the standard classification accuracy as an important criteria for measuring classifier performance . Besides , the cost matrix for misclassification of original examples might be different from the cost matrix of adversarial transformations . For instance , misclassifying a benign program as malicious may still induce some cost in the non-adversarial setting , whereas the adversary may only benefit from transforming a malicious program into a benign one . In a scenario where the model is cost-sensitive regardless of adversaries , it could make sense to incorporate a cost-sensitive loss function as the first term also , but we have not explored this and are focused on the adversarial setting where cost-sensitivity is with respect to adversarial goals . 4.What if we only optimize the original examples by cost-sensitive loss Given the vulnerability of deep learning classifiers against adversarial examples , we highly doubt that if we only optimize the original training by the cost-sensitive loss it would achieve significant cost-sensitive robustness ( this expectation is based on how poorly models trained with the goal of overall accuracy do at achieving overall robustness ) . To be more convincing , we are running an experiment to test the robustness of a standard cost-sensitive classifier and will post the results soon . Reference [ 1 ] . Khan , et al. , Cost-Sensitive Learning of Deep Feature Representations from Imbalanced Data . https : //arxiv.org/abs/1508.03422"}, {"review_id": "BygANhA9tQ-1", "review_text": "** review score incremented following discussion below ** Strengths: Well written and clear paper Intuition is strong: not all source-target class pairs are as beneficial to find adversarial examples for Weaknesses: Cost matrices choices feel a bit arbitrary in experiments CIFAR experiments still use very small norm-balls The submission builds on seminal work by Dalvi et al. (2004), which studied cost-sensitive adversaries in the context of spam detection. In particular, it extends the approach to certifiable robustness introduced by Wong and Kolter with a cost matrix that specifies for each pair of source-target classes whether the model should be robust to adversarial examples that are able to take an input from the source class to the target (or conversely whether these adversarial examples are of interest to an adversary). While the presentation of the paper is overall of great quality, some elements from the certified robustness literature could be reminded in order to ensure that the paper is self-contained. For instance, it is unclear how the guaranteed lower bound is derived without reading prior work. Adding this information in the present submission would make it easier for the reader to follow not only Sections 3.1 and 3.2 but also the computations behind Figure 1.b. The experiments results are clearly presented but some of the details of the experimental setup are not always justified. If you are able to clarify the following choices in your rebuttal, this would help revise my review. First, the choice of cost matrices feels a bit arbitrary and somewhat cyclical. For instance, binary cost matrices for MNIST are chosen according to results found in Figure 1.b, but then later the same bounds are used to evaluate the performance of the approach. Yet, adversarial incentives may not be directly correlated with the \u201chardness\u201d of a source-target class pair as measured in Figure 1.b. The real-valued cost matrices are better justified in that respect. Second, would you be able to provide additional justification or analysis of the choice of the epsilon parameter for CIFAR-10? For MNIST, you were able to improve the epsilon parameter from epsilon=0.1 to epsilon=0.2 but for CIFAR-10 the epsilon parameter is identical to Wong et al. Does that indicate that the results presented in this paper do not scale beyond simple datasets like MNIST? Minor comments: P2: The definition of adversarial examples given in Section 2.2 is a bit too restrictive, and in particular only applies to the vision domain. Adversarial examples are usually described as any test input manipulated by an adversary to force a model to mispredict. P3: typo in \u201coptimzation\u201d P5: trade off -> trade-off P8: the font used in Figure 2 is small and hard to read when printed. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We hope the following explanations address your questions : 1 . Regarding the choice of the cost matrices Our goal in the experiments was to evaluate how well a variety of different types of cost matrices can be supported . MNIST and CIFAR-10 are toy datasets , thus defining cost matrices corresponding to meaningful security applications for these datasets is difficult . Instead , we selected representative tasks and designed cost matrices to capture them . Our experimental results show the promise of the cost-sensitive training method works across a variety of different types of cost matrices , so we believe it can be generalized to other cost matrix scenarios that would be found in realistic applications . It is a good point that the cost matrices that were selected based on the robust error rates in Fig 1B are somewhat cyclical , but it does not invalidate our evaluation . We use the \u201c hardness \u201d of adversarial transformation between classes only for choosing representative cost matrices , and the robust error results on the overall-robustness trained model as a measure for transformation hardness . Further , the transformation hardness implied by the robust error heatmap is generally consistent with intuitions about the MNIST digit classes ( e.g. , \u201c 9 \u201d and \u201c 4 \u201d look similar so are harder to make robust to transformation ) , as well as with the visualization results produced by dimensional reduction techniques , such as t-SNE [ 1 ] . 2.Regarding the choice of epsilon for CIFAR-10 In our CIFAR-10 experiments , we set epsilon=2/255 , the same experimental setup as in [ 2 ] . Our proposed cost-sensitive robust classifier can be applied to larger epsilon for CIFAR-10 dataset , and similar improvements have been observed for different epsilon settings . In particular , we have run experiments on CIFAR-10 with epsilon varying from { 2/255 , 4/255 , 6/255 } for the single seed task . The comparison results are reported in Figure 5 ( b ) , added to the revised PDF . These results support the generalizability of our method to larger epsilon settings . [ 1 ] Maaten and Hinton , Visualizing Data using t-SNE . http : //www.jmlr.org/papers/v9/vandermaaten08a.html [ 2 ] Wong , et al. , Scaling Provable Adversarial Defenses . https : //arxiv.org/abs/1805.12514"}, {"review_id": "BygANhA9tQ-2", "review_text": "The paper introduces a new concept of certified cost-sensitive robustness against adversarial attacks. A cost-sensitive robust optimization formulation is then proposed for deep adversarial learning. Experimental results on two benchmark datasets (MNIST, CIFAR-10) are reported to show the superiority of the proposed method to overall robustness method, both with binary and real-value cost matrices. The idea of cost-sensitive adversarial deep learning is well motivated. The proposed method is clearly presented and the results are easy to access. My main concern is about the novelty of the approach which looks mostly incremental as a rather direct extension of the robust model (Wong & Kolter 2018) to cost-sensitive setting. Particularly, the duality lower-bound based loss function and its related training procedure are almost identical to those from (Wong & Kolter 2018), up to certain trivial modification to respect the pre-specified misclassification costs. The numerical results show some promise. However, as a practical paper, the current empirical study appears limited in data scale: I believe additional evaluation on more challenging data sets can be useful to better support the importance of approach. Pros: - The concept of certified cost-sensitive robustness is well motivated and clearly presented. Cons: - The novelty of method is mostly incremental given the prior work of (Wong & Kolter 2018). - Numerical results show some promise of cost-sensitive adversarial learning in the considered settings, but still not supportive enough to the importance of approach. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . Please see our responses below . 1.Concern regarding the novelty The review correctly notes that the method we use to achieve cost-sensitive robustness is a straightforward extension to the training procedure in Wong & Kolter ( 2018 ) . The novelty of our paper lies in the introduction of cost-sensitive robustness as a more appropriate criteria to measure classifier \u2019 s performance , and in showing experimentally that the cost-sensitive robust training procedure is effective . Previous robustness training methods were designed for overall robustness , which does not capture well the goals of adversaries in most realistic scenarios . We consider it an advantage that our method enables cost-sensitive robustness to be achieved with straightforward modifications to overall robustness training . 2.Limitation in data scale We agree with the reviewer that certified robustness methods , including our work , are a long way from scaling to interesting models . All previous work on certified adversarial defenses has been limited to simple models on small or medium sized datasets ( e.g. , [ 1-3 ] below ) , but there is growing awareness that non-certified defenses are unlikely to resist adaptive adversaries and strong interest in scaling these methods . The method we propose and evaluate for incorporating cost-sensitivity in robustness training is generic enough that we expect it will also work with most improvements to certifiable robustness training . So , even though our implementation is not immediately practical today , we believe our results are of scientific interest , and the methods we propose are likely to become practical as rapid progress continues in scaling certifiable defenses . [ 1 ] Wong and Kolter , Provable defenses against adversarial examples via the convex outer adversarial polytope . https : //arxiv.org/abs/1711.00851 [ 2 ] Raghunathan , et al. , Certified Defenses against Adversarial Examples . https : //arxiv.org/abs/1801.09344 [ 3 ] Wong , et al. , Scaling Provable Adversarial Defenses . https : //arxiv.org/abs/1805.12514"}], "0": {"review_id": "BygANhA9tQ-0", "review_text": "The authors define the notion of cost-sensitive robustness, which measures the seriousness of adversarial attack with a cost matrix. The authors then plug the costs of adversarial attack into the objective of optimization to get a model that is (cost-sensitively) robust against adversarial attacks. The initiative is novel and interesting. Considering the long history of cost-sensitive learning, the proposed model is rather ad-hoc for two reasons: (1) It is not clear why the objective should take the form of (3.1). In particular, if using the logistic function as a surrogate for 0-1 loss, shouldn't the sum of cost be in front of \"log\"? If using the probability estimated from the network in a Meta-Cost guided sense, shouldn't the cost be multiplied by the probability estimate (like 1/(1+exp(...))) instead of the exp itself? The mysterious design of (3.1) makes no physical sense to me, or at least other designs used in previous cost-sensitive neural network models like Chung et al., Cost-aware pre-training for multiclass cost-sensitive deep learning, IJCAI 2016 Zhou and Liu, Training cost-sensitive neural networks with methods addressing the class imbalance problem, TKDE 2006 (which is cited by the authors) are not discussed nor compared. Update: I thank the authors for providing updated information in the Appendix discussing about other alternatives. While I still think it worth comparing with other approaches (as it is still not clear whether Khan's approach is regarded as state-of-the-art for *general* cost-sensitive deep learning), I think the authors have sufficiently justified their choice. (2) It is not clear why the perturbed example should take the cost-sensitive form, while the original examples shouldn't (as the original examples follow the original loss). Or alternatively, if we optimize the original examples by the cost-sensitive loss, would it naturally achieve some cost-sensitive robustness (as the model would naturally make it harder to make high-cost mistakes)? Those issues are yet to be studied. Update: I thank the authors for providing additional experiments on this part. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . Your comments about the model being ad hoc stem from a few misunderstandings , which we hope to clarify : 1 . Justification of training objective ( 3.1 ) The design of ( 3.1 ) is not ad hoc , but follows from previous cost-sensitive learning work such as MetaCost , and is inspired by the cost-sensitive CE loss ( see equation ( 10 ) of [ 1 ] for a detailed definition ) . To be specific , class probabilities for cost-sensitive CE loss are computed by multiplying the corresponding cost and then normalizing the result vector . As a result , transformations that induce larger cost will receive larger penalization by minimizing the cost-sensitive CE loss . We neglected to include this explanation in the paper , and will revise it to make this clear . For the first question , moving the sum of cost in front of \u201c log \u201d is unreasonable because the loss for each seed example will not be a negative log-likelihood term as in the case of cross-entropy . We can check the sanity of the objective by examining whether it reduces to standard CE loss if we set C = 1 * 1^\\top-I . For the second question , we indeed multiply the probability estimates by the cost , but the result vector has to be normalized before plugging into the cross entropy loss . Thus , the sum of cost will appear in front of the \u201c exp \u201d term . 2.Comparison with other alternative designs The cost-sensitive neural network models you mentioned are only demonstrated to be effective in the non-adversarial settings , whereas we show that our proposed classifier is effective in the adversarial setting . Thus , comparing our method with theirs is not appropriate , since it is unclear whether such alternative cost-sensitive models can be adapted and remain effective in the adversarial setting . Even if they can be adapted , it is still not the main focus of our paper , as our main goal is to show that our proposed classifier achieves significant improvements in cost-sensitive robustness in comparison with models trained for overall robustness . 3.Why are the original examples are not in cost-sensitive form ? The training objective ( 3.1 ) is constructed for maximizing both cost-sensitive robustness and standard classification accuracy , and allows us to use the alpha hyperparameter to control the weighting between these goals . Thus , the first term in ( 3.1 ) doesn \u2019 t involve cost-sensitivity . We regard the standard classification accuracy as an important criteria for measuring classifier performance . Besides , the cost matrix for misclassification of original examples might be different from the cost matrix of adversarial transformations . For instance , misclassifying a benign program as malicious may still induce some cost in the non-adversarial setting , whereas the adversary may only benefit from transforming a malicious program into a benign one . In a scenario where the model is cost-sensitive regardless of adversaries , it could make sense to incorporate a cost-sensitive loss function as the first term also , but we have not explored this and are focused on the adversarial setting where cost-sensitivity is with respect to adversarial goals . 4.What if we only optimize the original examples by cost-sensitive loss Given the vulnerability of deep learning classifiers against adversarial examples , we highly doubt that if we only optimize the original training by the cost-sensitive loss it would achieve significant cost-sensitive robustness ( this expectation is based on how poorly models trained with the goal of overall accuracy do at achieving overall robustness ) . To be more convincing , we are running an experiment to test the robustness of a standard cost-sensitive classifier and will post the results soon . Reference [ 1 ] . Khan , et al. , Cost-Sensitive Learning of Deep Feature Representations from Imbalanced Data . https : //arxiv.org/abs/1508.03422"}, "1": {"review_id": "BygANhA9tQ-1", "review_text": "** review score incremented following discussion below ** Strengths: Well written and clear paper Intuition is strong: not all source-target class pairs are as beneficial to find adversarial examples for Weaknesses: Cost matrices choices feel a bit arbitrary in experiments CIFAR experiments still use very small norm-balls The submission builds on seminal work by Dalvi et al. (2004), which studied cost-sensitive adversaries in the context of spam detection. In particular, it extends the approach to certifiable robustness introduced by Wong and Kolter with a cost matrix that specifies for each pair of source-target classes whether the model should be robust to adversarial examples that are able to take an input from the source class to the target (or conversely whether these adversarial examples are of interest to an adversary). While the presentation of the paper is overall of great quality, some elements from the certified robustness literature could be reminded in order to ensure that the paper is self-contained. For instance, it is unclear how the guaranteed lower bound is derived without reading prior work. Adding this information in the present submission would make it easier for the reader to follow not only Sections 3.1 and 3.2 but also the computations behind Figure 1.b. The experiments results are clearly presented but some of the details of the experimental setup are not always justified. If you are able to clarify the following choices in your rebuttal, this would help revise my review. First, the choice of cost matrices feels a bit arbitrary and somewhat cyclical. For instance, binary cost matrices for MNIST are chosen according to results found in Figure 1.b, but then later the same bounds are used to evaluate the performance of the approach. Yet, adversarial incentives may not be directly correlated with the \u201chardness\u201d of a source-target class pair as measured in Figure 1.b. The real-valued cost matrices are better justified in that respect. Second, would you be able to provide additional justification or analysis of the choice of the epsilon parameter for CIFAR-10? For MNIST, you were able to improve the epsilon parameter from epsilon=0.1 to epsilon=0.2 but for CIFAR-10 the epsilon parameter is identical to Wong et al. Does that indicate that the results presented in this paper do not scale beyond simple datasets like MNIST? Minor comments: P2: The definition of adversarial examples given in Section 2.2 is a bit too restrictive, and in particular only applies to the vision domain. Adversarial examples are usually described as any test input manipulated by an adversary to force a model to mispredict. P3: typo in \u201coptimzation\u201d P5: trade off -> trade-off P8: the font used in Figure 2 is small and hard to read when printed. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We hope the following explanations address your questions : 1 . Regarding the choice of the cost matrices Our goal in the experiments was to evaluate how well a variety of different types of cost matrices can be supported . MNIST and CIFAR-10 are toy datasets , thus defining cost matrices corresponding to meaningful security applications for these datasets is difficult . Instead , we selected representative tasks and designed cost matrices to capture them . Our experimental results show the promise of the cost-sensitive training method works across a variety of different types of cost matrices , so we believe it can be generalized to other cost matrix scenarios that would be found in realistic applications . It is a good point that the cost matrices that were selected based on the robust error rates in Fig 1B are somewhat cyclical , but it does not invalidate our evaluation . We use the \u201c hardness \u201d of adversarial transformation between classes only for choosing representative cost matrices , and the robust error results on the overall-robustness trained model as a measure for transformation hardness . Further , the transformation hardness implied by the robust error heatmap is generally consistent with intuitions about the MNIST digit classes ( e.g. , \u201c 9 \u201d and \u201c 4 \u201d look similar so are harder to make robust to transformation ) , as well as with the visualization results produced by dimensional reduction techniques , such as t-SNE [ 1 ] . 2.Regarding the choice of epsilon for CIFAR-10 In our CIFAR-10 experiments , we set epsilon=2/255 , the same experimental setup as in [ 2 ] . Our proposed cost-sensitive robust classifier can be applied to larger epsilon for CIFAR-10 dataset , and similar improvements have been observed for different epsilon settings . In particular , we have run experiments on CIFAR-10 with epsilon varying from { 2/255 , 4/255 , 6/255 } for the single seed task . The comparison results are reported in Figure 5 ( b ) , added to the revised PDF . These results support the generalizability of our method to larger epsilon settings . [ 1 ] Maaten and Hinton , Visualizing Data using t-SNE . http : //www.jmlr.org/papers/v9/vandermaaten08a.html [ 2 ] Wong , et al. , Scaling Provable Adversarial Defenses . https : //arxiv.org/abs/1805.12514"}, "2": {"review_id": "BygANhA9tQ-2", "review_text": "The paper introduces a new concept of certified cost-sensitive robustness against adversarial attacks. A cost-sensitive robust optimization formulation is then proposed for deep adversarial learning. Experimental results on two benchmark datasets (MNIST, CIFAR-10) are reported to show the superiority of the proposed method to overall robustness method, both with binary and real-value cost matrices. The idea of cost-sensitive adversarial deep learning is well motivated. The proposed method is clearly presented and the results are easy to access. My main concern is about the novelty of the approach which looks mostly incremental as a rather direct extension of the robust model (Wong & Kolter 2018) to cost-sensitive setting. Particularly, the duality lower-bound based loss function and its related training procedure are almost identical to those from (Wong & Kolter 2018), up to certain trivial modification to respect the pre-specified misclassification costs. The numerical results show some promise. However, as a practical paper, the current empirical study appears limited in data scale: I believe additional evaluation on more challenging data sets can be useful to better support the importance of approach. Pros: - The concept of certified cost-sensitive robustness is well motivated and clearly presented. Cons: - The novelty of method is mostly incremental given the prior work of (Wong & Kolter 2018). - Numerical results show some promise of cost-sensitive adversarial learning in the considered settings, but still not supportive enough to the importance of approach. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . Please see our responses below . 1.Concern regarding the novelty The review correctly notes that the method we use to achieve cost-sensitive robustness is a straightforward extension to the training procedure in Wong & Kolter ( 2018 ) . The novelty of our paper lies in the introduction of cost-sensitive robustness as a more appropriate criteria to measure classifier \u2019 s performance , and in showing experimentally that the cost-sensitive robust training procedure is effective . Previous robustness training methods were designed for overall robustness , which does not capture well the goals of adversaries in most realistic scenarios . We consider it an advantage that our method enables cost-sensitive robustness to be achieved with straightforward modifications to overall robustness training . 2.Limitation in data scale We agree with the reviewer that certified robustness methods , including our work , are a long way from scaling to interesting models . All previous work on certified adversarial defenses has been limited to simple models on small or medium sized datasets ( e.g. , [ 1-3 ] below ) , but there is growing awareness that non-certified defenses are unlikely to resist adaptive adversaries and strong interest in scaling these methods . The method we propose and evaluate for incorporating cost-sensitivity in robustness training is generic enough that we expect it will also work with most improvements to certifiable robustness training . So , even though our implementation is not immediately practical today , we believe our results are of scientific interest , and the methods we propose are likely to become practical as rapid progress continues in scaling certifiable defenses . [ 1 ] Wong and Kolter , Provable defenses against adversarial examples via the convex outer adversarial polytope . https : //arxiv.org/abs/1711.00851 [ 2 ] Raghunathan , et al. , Certified Defenses against Adversarial Examples . https : //arxiv.org/abs/1801.09344 [ 3 ] Wong , et al. , Scaling Provable Adversarial Defenses . https : //arxiv.org/abs/1805.12514"}}