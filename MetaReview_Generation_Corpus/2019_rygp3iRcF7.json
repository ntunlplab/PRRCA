{"year": "2019", "forum": "rygp3iRcF7", "title": "Area Attention", "decision": "Reject", "meta_review": "although the idea is a straightforward extension of the usual (flat) attention mechanism (which is positive), it does show some improvement in a series of experiments done in this submission. the reviewers however found the experimental results to be rather weak and believe that there may be other problems in which the proposed attention mechanism could be better utilized, despite the authors' effort at improving the result further during the rebuttal period. this may be due to a less-than-desirable form the initial submission was in, and when the new version with perhaps a new set of more convincing experiments is reviewed elsewhere, it may be received with a more positive attitude from the reviewers.", "reviews": [{"review_id": "rygp3iRcF7-0", "review_text": " I have several concerns about this paper. [originality] Some important related studies are missing. # Related studies about the perspective of \u201carea\u201d. The consecutive position in sequence is often referred to as \u201cspan\u201d in NLP filed, which is identical to what the authors call \u201carea\u201d in this paper. Then, the idea of utilizing spans currently becomes a very popular in NLP field. We can find several papers, e.g., Wenhui Wang, Baobao Chang, \u201cGraph-based dependency parsing with bidirectional lstm\u201d, ACL-2016. Mitchell Stern, Jacob Andreas, Dan Klein, \u201cA Minimal Span-Based Neural Constituency Parser\u201d, ACL-2017. Kenton Lee, Luheng He, Mike Lewis, Luke Zettlemoyer, \u201cEnd-to-end Neural Coreference Resolution\u201d, EMNLP-2017. Nikita Kitaev, Dan Klein, \u201cConstituency Parsing with a Self-Attentive Encoder\u201d, ACL-2018. Similarly, there are several related studies in image processing field, e,g., Marco Pedersoli, Thomas Lucas, Cordelia Schmid, Jakob Verbeek, \u201cAreas of Attention for image captioning\u201d, ICCV-2017 Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo, \u201cImage Captioning with Semantic Attention\u201d, CVPR-2016. # Related studies about the perspective of \u201cstructured attention\u201d. Several papers about structured attention have already been proposed, e.g., Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush. \u201cStructured Attention Networks\u201d, ICLR-2017. Vlad Niculae, Mathieu Blondel. \u201cA Regularized Framework for Sparse and Structured Neural Attention\u201d, NIPS-2017. I think the authors should explain the relations between their method and the methods proposed in the above listed papers. [significance] # Concern about experimental settings The experimental setting for NMT looks unnormal in the community. Currently, most of papers use sentences split in subword units rather than character units. I cannot find a reason to select the character units. I think the authors should report the effectiveness of the proposed method on the widely-used settings. # computational cost The authors should report the actual calculation speed by comparing with the baseline method and the proposed method. In Sec. 2.2, the authors provided the computational cost. I feel that the cost of O(|M|A) is still enough large and that can unacceptably damage the actual calculation speed of the proposed method. Overall, the proposed method itself seems to be novel and interesting. However, in my opinion, writing and organization of this paper should be much improved as a conference paper. I feel like the current status of this paper is still ongoing to write. Thus, it is a bit hard for me to strongly recommend this paper to be accepted. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you so much for the thorough feedback that really strengthens the paper . We have conducted additional experiments and revised the paper to address these points . We here respond each point in your review . Re : originality Thanks for bringing up a great collection of previous works , which helped us better position our work in the literature . We have added the Related Work section to clarify the relationship of area attention with each previous work you suggested . Re : Concern about experimental settings We added token-level translation experiments as requested . The results are summarized in Table 1 & Table 2 . Area attention consistently outperformed regular attention on token-level translation on all the conditions as well . There is particularly a significant accuracy gain on EN-DE tasks . Re : computational cost We reported the actual calculation speed of area attention in comparison to regular attention for two major model configurations ( Transformer Base and Big ) in section 4.1.1 . Briefly , for Transformer Base model , on 8 NVIDIA P100 GPUs , each training step took 0.4 seconds for Regular Attention , 0.5 seconds for the basic form of Area Attention ( Eq.3 & Eq.4 ) , 0.8 seconds for Area Attention using multiple features ( Eq.9 & Eq.4 ) . We have made a number of other improvements to the paper . Please see the summary of changes and the revised paper for details ."}, {"review_id": "rygp3iRcF7-1", "review_text": "[Summary] Paper \u201cAREA ATTENTION\u201d extends the current attention models from word level to \u201carea level\u201d, i.e., the combination of adjacent words. Specifically, every $r_i$ adjacent words are first merged into a new item; next a key and the value for this item is calculated based on Eqn.(3 or 7) and Eqn. (4), and then the conventional attention models are applied to these new items. The authors work on (char level) NMT and image captioning to verify the algorithm. [Details] 1. In the abstract, \u201c\u2026 Using an area of items, instead of a single, we hope attention mechanisms can better capture the nature of the task \u2026\u201d, can you provide an example to show why \u201can area of items\u201d can \u201cbetter capture the nature of the task\u201d? In particular, you need to show why the conventional attention mechanism fails. 2. In this new proposed framework, how should we define the query for each area including multiple items like words? For example, in Figure 1, what is the query for $n$-item areas where $n=1,2,3$. 3. Two different kinds of keys are proposed in Eqn. (3) and Eqn. (7). Any comparison between them? 4. I am not convinced by the experimental results. (4a) On WMT\u201914 En-to-Fr and En-to-De, we know that \u201ctransformer_big\u201d can achieve better results than the three settings shown in Table 1 & 2. The results of using transformer_big are not reported. Besides, it is not necessary to use the \u201ctiny\u201d setting for En-to-{De, Fr} translation considering the data size. (4b) It is widely adopted to use token-level neural machine translation. It is not convincing to work on char-level NMT only. Also, please provide the results using transformer_big setting. (4c) There are no BLEU scores for the LSTM setting. Note that comment (4b) and (4c) are also pointed by anonymous readers. (4d) It is really strange for me to \u201ctrained on COCO and tested on Flickr\u201d (See the title of Table 4). It is not a common practice in image captioning literature. Even if in (Soricut et al., 2018), the authors report the results of training of COCO and test on COCO (the Table 5). Therefore, the results are not convincing. You should train on COCO and test on COCO too. e. What if we use different area size? I do not find the study in this paper. [Pros & Cons] (+) A new attempt of the attention model that tries to build the attention beyond unigrams. (-) Experiments are not convincing. (-) The motivation is not strong. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed comments . We have carefully addressed all your questions in the revision . Please see the Summary of Changes and the revised paper for details . We here respond to each of your questions . Re : `` 1. can you provide an example to show why \u201c an area of items \u201d can \u201c better capture the nature of the task \u201d ? In particular , you need to show why the conventional attention mechanism fails . '' We miscommunicated our motivation in the original paper . Our motivation is to allow a model to attend to information with varying granularity . Conventional attention mechanisms attend to items with a predefined and fixed granularity , e.g. , a word piece of a character in translation or a cell in a fixed grid for image captioning . With area attention , the unit of attention can be a combination of a group of adjacent items . For example , multiple cells in an image grid can make a car that can be more efficient for the word \u201c car \u201d in the caption to attend to . We have clarified our motivation throughout the paper . Re : `` 2.In this new proposed framework , how should we define the query for each area including multiple items like words ? For example , in Figure 1 , what is the query for $ n $ -item areas where $ n=1,2,3 $ . '' There is no special requirement on queries . A query can be the same as in a regular attention . For example , a word in a target sentence can be a translation of a multi-word phrase ( an area ) in a source sentence . Re : `` 3.Two different kinds of keys are proposed in Eqn . ( 3 ) and Eqn . ( 7 ) .Any comparison between them ? '' We added a comparison of these two forms of keys in Token-Level Translation Tasks ( see Table 1 & 2 ) . Overall , keys with feature combination ( Eqn.5-9 ) generally performs better than the basic form Eqn.3 , although the basic form already outperforms the baselines in most conditions . Re : `` ( 4a ) ( 4b ) '' We added Token-Level Translation tasks in the revised paper ( see Section 4.1 ) . We also added a comparison with \u201c Transformer_Big \u201d . Again , area attention consistently improves these models over all the conditions . In particular , area attention achieved BLEU 29.68 on EN-DE that improved upon the previous result of BLEU 28.4 reported for Transformer Big in the original Transformer paper . For EN-FR , area attention also outperformed the baseline , although it didn \u2019 t match the previous result on EN-FR . We think it \u2019 s due to the use of a different batch size and training steps . Re : `` ( 4c ) There are no BLEU scores for the LSTM setting . Note that comment ( 4b ) and ( 4c ) are also pointed by anonymous readers . '' We reported the BLEU scores for LSTM as well . All the models are tested for both token and character-level translation tasks . Re : `` ( 4d ) It is really strange for me to \u201c trained on COCO and tested on Flickr \u201d ( See the title of Table 4 ) . It is not a common practice in image captioning literature . Even if in ( Soricut et al. , 2018 ) , the authors report the results of training of COCO and test on COCO ( the Table 5 ) . Therefore , the results are not convincing . You should train on COCO and test on COCO too . '' We added COCO40 test results ( from the official COCO challenge website ) in the revised paper ( see Table 5 ) . Area attention significantly improved the original model on both CIDEr and ROUGE-L metrics . Re : `` What if we use different area size ? I do not find the study in this paper . '' This is a great question . We reported the effect of different area sizes for image captioning tasks , e.g. , 2x2 versus 3x3 . We explored which layers in Transformer can benefit from area attention and found area attention helps more when it \u2019 s used in lower layers . We speculate that this is because each position is well equipped with contextual information due to self attention in latent layers and area attention might not be able to help much . In contrast , the lower layers can benefit a lot from area attention . We also found a large area size does not necessarily improve accuracy . It is worth tuning the max area size as a hyper parameter to achieve even better results for specific problems ."}, {"review_id": "rygp3iRcF7-2", "review_text": "I prefer the idea of using some statistics (such as variances) of multiple items for attention. This direction may lead to better attention units for future works. I do not fully understand the argument, \"Attention mechanisms are designed to focus on a single item in the entire memory\". In my understanding, the attention formulation has no mathematical bias to focus on a single item. I have been working on the enterprise NMT for years, and observed many cases where the attention weights concentrate in a few (not a single), tokens. Do you have any comments? Could you show some concise examples that we really need to attend multiple (adjacent) items to boost the performance? For example, in char-based machine translation case, we can mimic the area attention with the wordpiece + token-wise NMT. For the image case, the adjacent area looks like a \"super pixel\". It is unfortunate to observe that the gains on BLEU and perplexity are limited. Since the authors do not provide any statistical tests, or a confidence interval of the scores, I cannot be sure these gains are truly significant. From my experiences +1.0 BLEU score is often insignificant in NMT experiments (BLEU variance is high in general). Summary + A new variant of attention, allowing attention to asses statistics of multiple items (such as variances) is interesting - Claims are not so much convincing for the need of attending multiple adjacent items. - Gains in experiments are limited. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your feedback . We have revised the paper substantially and here address each of your concerns . Re : \u201c Attention mechanisms are designed to focus on a single item \u201d This is indeed miscommunication in our original submission . What we intend to convey is that regular attention mechanisms are designed to focus on individual items , i.e. , the granularity of attention is each item . Such granularity is predetermined and fixed , e.g. , a character or a word-piece . In contrast , area attention allows a model to attend to information with varying granularity by dynamically grouping adjacent items . For example , it can be a group of adjacent regions on an image that forms an object or a `` super pixel '' , or multiple word pieces that form a phrase . The difference with regular attention is that we do not have to decide what the proper unit is . Rather , we let the model pick on the right level of aggregation of items or raw features . Such granularity or aggregation is acquired through learning . We have clarified this point throughout the paper . # the gains on BLEU and perplexity are limited . We reported BLEU scores on all the translation tasks including those previously with perplexity . We also added COCO40 Official test results for the image captioning tasks . Overall , for translation tasks , while the margin is not as large as we hoped , the accuracy gain with area attention is quite consistent throughout most conditions . Particularly on Token-level EN-DE translation , area attention achieved BLEU 29.68 that improved upon the state of the art results with a big margin . For image captioning , the accuracy gain is significant for both in-domain and out-of-domain tests . We have improved the paper in many ways . Please see the complete list of changes we made in the Summary of Changes and the revised paper for details ."}], "0": {"review_id": "rygp3iRcF7-0", "review_text": " I have several concerns about this paper. [originality] Some important related studies are missing. # Related studies about the perspective of \u201carea\u201d. The consecutive position in sequence is often referred to as \u201cspan\u201d in NLP filed, which is identical to what the authors call \u201carea\u201d in this paper. Then, the idea of utilizing spans currently becomes a very popular in NLP field. We can find several papers, e.g., Wenhui Wang, Baobao Chang, \u201cGraph-based dependency parsing with bidirectional lstm\u201d, ACL-2016. Mitchell Stern, Jacob Andreas, Dan Klein, \u201cA Minimal Span-Based Neural Constituency Parser\u201d, ACL-2017. Kenton Lee, Luheng He, Mike Lewis, Luke Zettlemoyer, \u201cEnd-to-end Neural Coreference Resolution\u201d, EMNLP-2017. Nikita Kitaev, Dan Klein, \u201cConstituency Parsing with a Self-Attentive Encoder\u201d, ACL-2018. Similarly, there are several related studies in image processing field, e,g., Marco Pedersoli, Thomas Lucas, Cordelia Schmid, Jakob Verbeek, \u201cAreas of Attention for image captioning\u201d, ICCV-2017 Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo, \u201cImage Captioning with Semantic Attention\u201d, CVPR-2016. # Related studies about the perspective of \u201cstructured attention\u201d. Several papers about structured attention have already been proposed, e.g., Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush. \u201cStructured Attention Networks\u201d, ICLR-2017. Vlad Niculae, Mathieu Blondel. \u201cA Regularized Framework for Sparse and Structured Neural Attention\u201d, NIPS-2017. I think the authors should explain the relations between their method and the methods proposed in the above listed papers. [significance] # Concern about experimental settings The experimental setting for NMT looks unnormal in the community. Currently, most of papers use sentences split in subword units rather than character units. I cannot find a reason to select the character units. I think the authors should report the effectiveness of the proposed method on the widely-used settings. # computational cost The authors should report the actual calculation speed by comparing with the baseline method and the proposed method. In Sec. 2.2, the authors provided the computational cost. I feel that the cost of O(|M|A) is still enough large and that can unacceptably damage the actual calculation speed of the proposed method. Overall, the proposed method itself seems to be novel and interesting. However, in my opinion, writing and organization of this paper should be much improved as a conference paper. I feel like the current status of this paper is still ongoing to write. Thus, it is a bit hard for me to strongly recommend this paper to be accepted. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you so much for the thorough feedback that really strengthens the paper . We have conducted additional experiments and revised the paper to address these points . We here respond each point in your review . Re : originality Thanks for bringing up a great collection of previous works , which helped us better position our work in the literature . We have added the Related Work section to clarify the relationship of area attention with each previous work you suggested . Re : Concern about experimental settings We added token-level translation experiments as requested . The results are summarized in Table 1 & Table 2 . Area attention consistently outperformed regular attention on token-level translation on all the conditions as well . There is particularly a significant accuracy gain on EN-DE tasks . Re : computational cost We reported the actual calculation speed of area attention in comparison to regular attention for two major model configurations ( Transformer Base and Big ) in section 4.1.1 . Briefly , for Transformer Base model , on 8 NVIDIA P100 GPUs , each training step took 0.4 seconds for Regular Attention , 0.5 seconds for the basic form of Area Attention ( Eq.3 & Eq.4 ) , 0.8 seconds for Area Attention using multiple features ( Eq.9 & Eq.4 ) . We have made a number of other improvements to the paper . Please see the summary of changes and the revised paper for details ."}, "1": {"review_id": "rygp3iRcF7-1", "review_text": "[Summary] Paper \u201cAREA ATTENTION\u201d extends the current attention models from word level to \u201carea level\u201d, i.e., the combination of adjacent words. Specifically, every $r_i$ adjacent words are first merged into a new item; next a key and the value for this item is calculated based on Eqn.(3 or 7) and Eqn. (4), and then the conventional attention models are applied to these new items. The authors work on (char level) NMT and image captioning to verify the algorithm. [Details] 1. In the abstract, \u201c\u2026 Using an area of items, instead of a single, we hope attention mechanisms can better capture the nature of the task \u2026\u201d, can you provide an example to show why \u201can area of items\u201d can \u201cbetter capture the nature of the task\u201d? In particular, you need to show why the conventional attention mechanism fails. 2. In this new proposed framework, how should we define the query for each area including multiple items like words? For example, in Figure 1, what is the query for $n$-item areas where $n=1,2,3$. 3. Two different kinds of keys are proposed in Eqn. (3) and Eqn. (7). Any comparison between them? 4. I am not convinced by the experimental results. (4a) On WMT\u201914 En-to-Fr and En-to-De, we know that \u201ctransformer_big\u201d can achieve better results than the three settings shown in Table 1 & 2. The results of using transformer_big are not reported. Besides, it is not necessary to use the \u201ctiny\u201d setting for En-to-{De, Fr} translation considering the data size. (4b) It is widely adopted to use token-level neural machine translation. It is not convincing to work on char-level NMT only. Also, please provide the results using transformer_big setting. (4c) There are no BLEU scores for the LSTM setting. Note that comment (4b) and (4c) are also pointed by anonymous readers. (4d) It is really strange for me to \u201ctrained on COCO and tested on Flickr\u201d (See the title of Table 4). It is not a common practice in image captioning literature. Even if in (Soricut et al., 2018), the authors report the results of training of COCO and test on COCO (the Table 5). Therefore, the results are not convincing. You should train on COCO and test on COCO too. e. What if we use different area size? I do not find the study in this paper. [Pros & Cons] (+) A new attempt of the attention model that tries to build the attention beyond unigrams. (-) Experiments are not convincing. (-) The motivation is not strong. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed comments . We have carefully addressed all your questions in the revision . Please see the Summary of Changes and the revised paper for details . We here respond to each of your questions . Re : `` 1. can you provide an example to show why \u201c an area of items \u201d can \u201c better capture the nature of the task \u201d ? In particular , you need to show why the conventional attention mechanism fails . '' We miscommunicated our motivation in the original paper . Our motivation is to allow a model to attend to information with varying granularity . Conventional attention mechanisms attend to items with a predefined and fixed granularity , e.g. , a word piece of a character in translation or a cell in a fixed grid for image captioning . With area attention , the unit of attention can be a combination of a group of adjacent items . For example , multiple cells in an image grid can make a car that can be more efficient for the word \u201c car \u201d in the caption to attend to . We have clarified our motivation throughout the paper . Re : `` 2.In this new proposed framework , how should we define the query for each area including multiple items like words ? For example , in Figure 1 , what is the query for $ n $ -item areas where $ n=1,2,3 $ . '' There is no special requirement on queries . A query can be the same as in a regular attention . For example , a word in a target sentence can be a translation of a multi-word phrase ( an area ) in a source sentence . Re : `` 3.Two different kinds of keys are proposed in Eqn . ( 3 ) and Eqn . ( 7 ) .Any comparison between them ? '' We added a comparison of these two forms of keys in Token-Level Translation Tasks ( see Table 1 & 2 ) . Overall , keys with feature combination ( Eqn.5-9 ) generally performs better than the basic form Eqn.3 , although the basic form already outperforms the baselines in most conditions . Re : `` ( 4a ) ( 4b ) '' We added Token-Level Translation tasks in the revised paper ( see Section 4.1 ) . We also added a comparison with \u201c Transformer_Big \u201d . Again , area attention consistently improves these models over all the conditions . In particular , area attention achieved BLEU 29.68 on EN-DE that improved upon the previous result of BLEU 28.4 reported for Transformer Big in the original Transformer paper . For EN-FR , area attention also outperformed the baseline , although it didn \u2019 t match the previous result on EN-FR . We think it \u2019 s due to the use of a different batch size and training steps . Re : `` ( 4c ) There are no BLEU scores for the LSTM setting . Note that comment ( 4b ) and ( 4c ) are also pointed by anonymous readers . '' We reported the BLEU scores for LSTM as well . All the models are tested for both token and character-level translation tasks . Re : `` ( 4d ) It is really strange for me to \u201c trained on COCO and tested on Flickr \u201d ( See the title of Table 4 ) . It is not a common practice in image captioning literature . Even if in ( Soricut et al. , 2018 ) , the authors report the results of training of COCO and test on COCO ( the Table 5 ) . Therefore , the results are not convincing . You should train on COCO and test on COCO too . '' We added COCO40 test results ( from the official COCO challenge website ) in the revised paper ( see Table 5 ) . Area attention significantly improved the original model on both CIDEr and ROUGE-L metrics . Re : `` What if we use different area size ? I do not find the study in this paper . '' This is a great question . We reported the effect of different area sizes for image captioning tasks , e.g. , 2x2 versus 3x3 . We explored which layers in Transformer can benefit from area attention and found area attention helps more when it \u2019 s used in lower layers . We speculate that this is because each position is well equipped with contextual information due to self attention in latent layers and area attention might not be able to help much . In contrast , the lower layers can benefit a lot from area attention . We also found a large area size does not necessarily improve accuracy . It is worth tuning the max area size as a hyper parameter to achieve even better results for specific problems ."}, "2": {"review_id": "rygp3iRcF7-2", "review_text": "I prefer the idea of using some statistics (such as variances) of multiple items for attention. This direction may lead to better attention units for future works. I do not fully understand the argument, \"Attention mechanisms are designed to focus on a single item in the entire memory\". In my understanding, the attention formulation has no mathematical bias to focus on a single item. I have been working on the enterprise NMT for years, and observed many cases where the attention weights concentrate in a few (not a single), tokens. Do you have any comments? Could you show some concise examples that we really need to attend multiple (adjacent) items to boost the performance? For example, in char-based machine translation case, we can mimic the area attention with the wordpiece + token-wise NMT. For the image case, the adjacent area looks like a \"super pixel\". It is unfortunate to observe that the gains on BLEU and perplexity are limited. Since the authors do not provide any statistical tests, or a confidence interval of the scores, I cannot be sure these gains are truly significant. From my experiences +1.0 BLEU score is often insignificant in NMT experiments (BLEU variance is high in general). Summary + A new variant of attention, allowing attention to asses statistics of multiple items (such as variances) is interesting - Claims are not so much convincing for the need of attending multiple adjacent items. - Gains in experiments are limited. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your feedback . We have revised the paper substantially and here address each of your concerns . Re : \u201c Attention mechanisms are designed to focus on a single item \u201d This is indeed miscommunication in our original submission . What we intend to convey is that regular attention mechanisms are designed to focus on individual items , i.e. , the granularity of attention is each item . Such granularity is predetermined and fixed , e.g. , a character or a word-piece . In contrast , area attention allows a model to attend to information with varying granularity by dynamically grouping adjacent items . For example , it can be a group of adjacent regions on an image that forms an object or a `` super pixel '' , or multiple word pieces that form a phrase . The difference with regular attention is that we do not have to decide what the proper unit is . Rather , we let the model pick on the right level of aggregation of items or raw features . Such granularity or aggregation is acquired through learning . We have clarified this point throughout the paper . # the gains on BLEU and perplexity are limited . We reported BLEU scores on all the translation tasks including those previously with perplexity . We also added COCO40 Official test results for the image captioning tasks . Overall , for translation tasks , while the margin is not as large as we hoped , the accuracy gain with area attention is quite consistent throughout most conditions . Particularly on Token-level EN-DE translation , area attention achieved BLEU 29.68 that improved upon the state of the art results with a big margin . For image captioning , the accuracy gain is significant for both in-domain and out-of-domain tests . We have improved the paper in many ways . Please see the complete list of changes we made in the Summary of Changes and the revised paper for details ."}}