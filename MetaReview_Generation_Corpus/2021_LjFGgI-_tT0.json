{"year": "2021", "forum": "LjFGgI-_tT0", "title": "BayesAdapter: Being Bayesian, Inexpensively and Robustly, via Bayesian Fine-tuning", "decision": "Reject", "meta_review": "This paper aims at improving the adoption of Bayesian NNs by providing a practical and user friendly variational inference method. The main ideas consist of two parts:\n1. Warm-start the variational inference from a pre-trained deterministic NN. It takes advantage of existing deep learning library features for easy implementation including weight decay, batch matrix multiplication, etc.\n2. Calibrating uncertainty estimation for out-of-domain detection using adversarial examples.\n\nPros:\n1. A practical way of implementing DNN variational inference with reduced variance, without sacrificing classification accuracy of the pretrained NN model.\n2. Significantly better OOD detection accuracy compared to other BNN approaches without taking OOD into account explicitly.\n\nCons:\n1. During discussion, it becomes clear that most of the techniques have been proposed similarly in the literature. Krishnan, 2020 applied BNN starting from MAP of NN, Flipout (Wen et. al., 2018) applies instance-wise sampling, Hendrycks et. al., 2018 and Hafner et. al., 2018 improves detection accuracy by training on OOD examples. The novelty of the proposed method is therefore limited.\n2. There's not much benefit on the classification performance compared to the initial MAP and is inferior to MCMC-based SOTA BNNs. One of the reviewers considers the SGLD-type approach may be more appealing to ML practitioners with the overhead of VI in training additional variance parameters.\n3. The authors argue MCMC-based BNN methods cannot achieve good performance without temperature scaling. But the main performance improvement of the paper is in the OOD detection with uncertainty regularization that modifies the posterior as well. The method of training with OOD samples is orthogonal to applying Bayesian inference to NNs, and the detection performance is limited to the distribution close to examples during training.\n\nThis paper falls on the borderline for acceptance. With the goal of improving adoption of BNN in practice, it is not convincing yet making mean field VI easier to implement could realize it without achieving competitive performance.\n", "reviews": [{"review_id": "LjFGgI-_tT0-0", "review_text": "* * Contributions * * This paper proposes a post-hoc approach to obtain model uncertainty estimates from vanilla pre-trained NNs through MFVI fine-tuning . Namely , 1 ) the authors re-cast the KL divergence in the VI objective as weight decay applied to the variational parameters 2 ) the authors propose a variance reduction technique for the reparametrisation trick 3 ) the authors explicitly train their model to produce large model uncertainty on Out of Distribution ( OOD ) inputs . Empirically , the proposed methods seems to retain the strong performance , and much of the simplicity , of point-estimate NNs while providing enhanced robustness in terms of uncertainty estimation . * * originality and significance * * To my knowledge , most of the proposed techniques ( or variants of them ) have appeared before in the literature or are simple extensions of existing approaches : Re-casting MFVI as SGD [ Khan et . al. , 2018 ] , Decorrelation of reparametrisation gradients across batch elements [ Wen et . al. , 2018 ] , Training on OOD measurement points to produce large uncertainty [ Hendrycks et . al. , 2018 and Hafner et . al. , 2018 ] . However , this work refines these ideas and puts them into a single framework which seems to produce strong results . I view this as a noteworthy contribution which might bring Bayesian Deep Learning closer to real world deployments . * * clarity * * Most ideas are presented clearly . The paper is well structured and easy to follow . Some passages are slightly ungrammatical but never does this impede the transmission of ideas . * * pros * * * Presents useful practices to make BNNs more mainstream with strong empirical performance . * Authors provide code for an efficient implementation of exemplar reparametrisation . * The proposed technique for OOD detection bypasses typical pathologies of MFVI [ Ovadia et . al. , 2019 ] by explicitly optimising variational parameters to produce large model uncertainties OOD . * * cons * * * Exemplar reparametrisation is very similar to Flipout [ Wen et . al. , 2018 ] . A comparison of the two would be appreciated . * The proposed technique for OOD detection is not very principled and has provides no guarantees . It seems empirically successful however . * The experimental setup is not very clear , even when reading the supplementary sections concerning experimental setup . Some questions I was left with : * There are many hyperparameters , how did you find all of them ? \u2014Especially the weight decay coefficients . Are the standard deviations implied by these priors interesting / meaningful in any way ? * A single Mutual Information threshold is provided . Is this one used for uncertainty calibration training on all tasks ? Is it also used when classifying inputs as in-distribution or OOD ? If this is the case , it is possible that models without uncertainty calibration training would benefit from using a different threshold . A better metric might be ROC-AUC , as it is threshold agnostic . * The only baselines provided are other VI approaches . SWAG is known to be a decently strong baseline but it is not evaluated for OOD detection peroformance . The current state of the art baseline for uncertainty quantification is deep ensembles [ Ovadia et . al. , 2019 ] . These are much more expensive . However , it might be interesting to see how they compare . * The authors repeat all experiments 3 times but only provide mean results . In some cases , like tables 1 and 2 , the values presented are similar across methods . I think that errorbars ( standard deviation across 3 runs ) would be very informative to the reader . * * Other comments and questions : * * * Typo in title : Bayesian , not Bayeisian * The maximum predictive entropy ( and thus mutual information ) will depend on dimensionality of output space ( number of classes ) . In your experimental section , you say you set a single threshold for all models . Could you further comment on this ? * * References * * [ Wen et . al. , 2018 ] https : //arxiv.org/pdf/1803.04386.pdf [ Khan et . al. , 2018 ] http : //proceedings.mlr.press/v80/khan18a/khan18a.pdf [ Ovadia et . al. , 2019 ] https : //papers.nips.cc/paper/9547-can-you-trust-your-models-uncertainty-evaluating-predictive-uncertainty-under-dataset-shift.pdf [ Hendrycks et . al. , 2018 ] https : //openreview.net/forum ? id=HyxCxhRcY7 [ Hafner et . al. , 2018 ] https : //arxiv.org/abs/1807.09289", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # # Q5 : Regarding the error bars : A : We add the variance of the results into the paper . Here is a copy : | Metric | Acc . ( % ) | NLL | AP ( PGD ) | AP ( fake ) | |-| : - : ||-|-| | CIFAR-10 | 96.82\u00b10.07 | 0.1004\u00b10.0026 | 0.993\u00b10.003 | 0.994\u00b10.001 | | ImageNet | top1 : 76.26\u00b10.06 top5 : 92.96\u00b10.03 | 0.9428\u00b10.0020 | 0.964\u00b10.009 | 0.848\u00b10.037 | We can see that the results of BayesAdapter exhibit less variance . # # # # Q6 : Other questions A : We have revised the typo . As we clarified , we only use a shared threshold $ \\gamma=0.75 $ for training across all the settings . And the learned model is robust against the choice of the $ \\gamma $ , testified by an ablation study on $ \\gamma $ ( on CIFAR-10 ) : | $ \\gamma $ | 0.25 | 0.50 | 0.75 | 1.0 | 1.50 | | -- | -- | -- | -- | -- | -- | | Acc . | 96.93 % | 96.70 % | 96.82 % | 96.74 % | 96.79 % | | AP ( PGD ) | 0.915 | 0.948 | 0.993 | 0.991 | 0.944 | | AP ( fake ) | 0.910 | 0.981 | 0.994 | 0.994 | 0.988 | Different dimensionality of output space would result in different scales of uncertainty , and this is also proved by the sub-figure ( b ) and ( d ) in Fig.3.But we can also note that though with different uncertainty scale , most of the normal examples have uncertainty no more than 0.75 ( on both CIFAR-10 and ImageNet ) , thus once we punish the model to assign no less than 0.75 uncertainty for the OOD data , the model naturally acquires the ability to detect OOD data . Per your suggestion , we might achieve better OOD detection by tuning this threshold according to the data and task , though we skipped at the time of submission ."}, {"review_id": "LjFGgI-_tT0-1", "review_text": "This paper proposed one simple and effective way to trainBayesian neural networks ( BNN ) . Pros : 1.The proposed method is quite simple and cheap to realize , compared to previous Bayesian methods . 2.Extensive experiments on a diverse set of challenging benchmarks have been conducted , which shows several promising results of the proposed method . 3.The proposed idea is novel which distinguishes from most of previous efforts , which try to train BNN from scratch using Bayesian methods . As described in this paper , most of previous methods , though paying much additional efforts than deterministic ones , do not lead to expected results , even with non-diagonal covariance matrices . The BayesAdapter , however , pays little efforts and obtains improvements even with diagonal covariance matrices . From this perspective , this is an encoraging result . Cons : 1.The results of comparison in Table 1 only repot the average result in 3 runs ( 3 is kind of small ) . However , it is better to show the std metric of the result to make the comparison more convincing because the improvement of BayesAdapter in average value is in fact not very apparent , especially compared with MAP . If the variance of the result is large , then there will be large overlap between different methods and thus it is not reasonable to claim that there is an apparent advantage over previous methods . 2.In evaluating the result of BayesAdapter , MC samples are used . What if only using the mean value of the posterior ? Compared to deterministic methods like MAP , inference using MC is more costly . In addition , it is suggested to provide some visualizations of the posterior distribution after BayesAdapter . 3.Based on results in Table 3 , BayesAdapter- performs similar as baselines , which indicates that the improvement comes from calibrating the uncertainty estimation . This leads to another question : what if we also use such calibration for the baseline methods . It would be interesting to make such a comparison .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the positive review . In this following , we address the detailed comments . # # # # Q1 : The std metric Sorry for missing the std . Here we provide it on CIFAR-10 and ImageNet benchmarks . | Metric | Acc . ( % ) | NLL | AP ( PGD ) | AP ( fake ) | |-| : - : ||-|-| | CIFAR-10 | 96.82\u00b10.07 | 0.1004\u00b10.0026 | 0.993\u00b10.003 | 0.994\u00b10.001 | | ImageNet | top1 : 76.26\u00b10.06 top5 : 92.96\u00b10.03 | 0.9428\u00b10.0020 | 0.964\u00b10.009 | 0.848\u00b10.037 | We can see that the results of BayesAdapter are relatively stable , outperforming BNN and MAP with statistical evidence on ImageNet . Following a similar suggestion by R1 , we have revised the paper to make clear the major goal of this work , which is to quickly and cheaply adapt a pre-trained DNN to be Bayesian without compromising performance when facing new tasks , instead of delivering a mechanism for learning better BNNs . # # # # Q2 : MC samples and posterior appearance A : We kindly point out that the results of deterministic inference with only the posterior mean of BayesAdapter are provided in the ablation study \u201c The impacts of ensemble number \u201d and Figure 4 . It is clear that with more than around 20 MC samples , Bayes ensemble ( the green line ) can achieve better prediction results than the deterministic inference ( the yellow line ) . This reflects that the learned posterior does not suffer too much from mode collapse , which is popularly witnessed on mean-field variational inference by the community . As suggested , we plot the parameter posterior of the first convolutional kernel in ResNet-50 architecture learned by BayesAdapter on ImageNet . The results are depicted in Appendix E. The learned variance seems to be disordered , unlike the mean . We leave more explanations as future work . # # # # Q3 : Apply uncertainty regularization to other models A : Yes , we totally agree with this point . We conducted such an experiment , and observed improved uncertainty estimation in the initial phase when applying this technique to other BNN methods , including the BNN baseline . These further confirm the effectiveness and universality of the proposed uncertainty regularization technique . We will try to add complete results in the final version since that training BNNs from scratch is time-consuming ."}, {"review_id": "LjFGgI-_tT0-2", "review_text": "The paper explores the variational training of Bayesian neural networks . It proposes to improve the quality of the inferred variational posterior and computational efficiency of the procedure by ( i ) better initialization ( mean parameters are initialized at the MAP ) ( ii ) reducing variance in the Monte Carlo approximated evidence lower bound by increasing the number of weight samples ( one per datapoint in a batch ) ( iii ) a posterior regularization encouraging higher uncertainty on adversarially generated or other \u201c near OOD \u201d data . The authors use the term BayesAdapter to refer to the process of running black-box variational inference from a fully factorized variational approximation with mean initialized at the MAP estimate and randomly initialized variances . The fact that variational inference ( especially those employing fully factorized approximations ) are susceptible to poor local optima and that better initializations can help navigate these local optima is widely known . The fact that better initializations can lead to somewhat improved posterior approximations is not surprising . Such initializations are also standard practice when employing stochastic gradient MCMC techniques and Laplace approximations ( where it is a requirement ) . Reducing variance by increasing the number of weight samples to one per datapoint in a batch is another straightforward idea , and it is unclear whether it can be claimed as a contribution of the current paper . Kingma et al. , in their local re-parameterization considered a variant with per data samples as well . The uncertainty regularization is indeed novel and appears effective ( but the experiments illustrating its benefits need to be better explained ) . Given the modest methods contributions , the empirical section needs to be particularly strong to demonstrate that the combination of these incremental improvements provides meaningful empirical advantages . To their credit , the authors demonstrate their approach on several large datasets and do provide experiments for vetting different aspects of the proposed extensions to variational BNN training . However , many experiments are missing details and some are lacking key comparisons . Overall this section could be significantly strengthened . * Tables 1 and 2 need to include comparisons against deep ensembles and multi-SWAG ( https : //arxiv.org/pdf/2002.08791.pdf ) . If the goal of this paper is to claim that variationally trained BNNs ( with the proposed improvements ) are useful in practice , a natural question to ask is whether they are competitive with far simpler ensembling approaches that are able to account for the multimodality of the posterior surface , unlike variational BNNs . * How was the calibration threshold $ \\gamma $ chosen for these experiments ? How sensitive is the performance to this choice ? Ideally , the authors would include results with different settings of $ \\gamma $ . How were the prior precisions selected ( which determine $ \\lambda $ set ? My main worry is that the marginal improvements provided by Bayesadapter variants over BNNs disappear when making slightly different parameter choices . It would be great if the authors can demonstrate that this isn \u2019 t the case . * Section 4.2 needs more details about the experimental setup . Did the 1000 / 10000 OOD training/test examples include both images created via PGD and SNGAN ( for CIFAR 10 ) and PGD and BigGAN ( for imagenet ) ? If so , how many from each source ? If not , it would be interesting to see cross performance \u2014 using PGD images for training and SNGAN images for testing . * In Table 4 , it doesn \u2019 t make sense to include ECE numbers from a different architecture trained via SWAG . These numbers are not comparable . Also , interestingly , both BayesAdapter variants have lower ECE scores than vanilla BNN on CIFAR , suggesting poorer calibration . Do the authors have an explanation for this ? Based on concerns about both novelty and experiments I am currently leaning towards a reject , but could be convinced otherwise based on the authors \u2019 response and additional comparisons .", "rating": "5: Marginally below acceptance threshold", "reply_text": "# # # # Q4 : Regarding setup We are sorry for causing the misunderstanding . To clarify : For the training , we use only 1000 fake examples ( e.g. , those from SNGAN on CIFAR-10 and from BigGAN on ImageNet ) and all the _uniformly perturbed training examples_ for optimizing uncertainty regularization ( see the last part of Section 3 ) . We did not include PGD perturbed examples into training because that resembles adversarial training and is time-consuming . For evaluation , we estimate the uncertainty on a held-out set of fake examples and _PGD perturbed validation examples_ and report the results . We want to point out that the idea to \u201c see cross performance \u201d might not be helpful with the uncertainty estimation trained on adversarially perturbed data , we can not expect it to successfully generalize to the examples produced by GAN which have significantly different fingerprints ( see Figure 6 in Appendix ) . We have offered a study on if the learned uncertainty could reasonably generalize in part 2 of Section 4.3 . The results give a positive answer . # # # # Q5 : Regarding ECE Thanks for the advice . We clarify that on CIFAR-10 , SWAG also uses wide-ResNet-28-10 , and shows weaker calibration than BayesAdapter with a substantial margin . The comparison to SWAG with a different architecture on ImageNet may indeed be less meaningful , and we have revised this . Regarding the second point , we emphasize the core notion of Bayesian deep learning : the predictive confidence is usually unreliable , thus we need a better measure like predictive uncertainty . With the superiority of the uncertainty estimation of BayesAdapter validated by Table 3 and Table 5 , its weaker ECE does not substantially undermine its practical value . As shown in the ablation study \u201c Uncertainty-based rejective decision \u201d and Figure 5 , we can leverage the predictive uncertainty to achieve robust rejective decision making instead of using predictive confidence . Anyway , returning to this phenomenon , we speculate this is because the fine-tuning start point _MAP_ has too bad ECE and the fine-tuning rounds are few ."}, {"review_id": "LjFGgI-_tT0-3", "review_text": "This paper introduces a fast way to get Bayesian posterior by using a pretrained deterministic model . Specifically , the authors first train a standard DNN model and then use it to initialize the variational parameters . Finally the variational parameters are optimized through standard variational inference ( VI ) training . To further improve uncertainty estimate , the authors propose an uncertainty regularization which maximizes the prediction inconsistency on out-of-distribution ( OOD ) data . Experiments including image classification and uncertainty estimates are conducted to demonstrate the proposed method . The idea of this paper is quite simple : initialize the mean in variational parameters by a pretrained DNN . Thus the method is cheap and simple enough to use broadly in practice . The authors did reasonable empirical tests . I especially appreciate the ablation study which helps understand the method a lot . The paper is well-written and easy to follow . I mainly have the following concerns about the paper . - One of the main motivations to use a pretrained DNN is that BNN learned from scratch is worse than its corresponding DNN . I feel this claim is misleading . Many papers have shown that BNNs trained from scratch outperform DNNs . Particularly the paper [ Wenzel et.al . 2020 ] which the authors cited to support their claim clearly shows that BNN ( with reasonable temperature ) is significantly better than DNN in predictive performance . But I do agree that the proposed method is cheaper than training BNNs from scratch . - The experimental results verify the effectiveness of the proposed method . But the results of the proposed method seem to be worse than BNNs training from scratch ( e.g.the ImageNet results in [ Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning , ICLR 2020 ] are much better ) . This also supports my first point , that the main benefit of the proposed method is to get the Bayesian posterior fast and cheaply , instead of to improve BNNs \u2019 performance . I think the authors should revise the claim to be more precise . - The proposed method is closely related to [ Specifying Weight Priors in Bayesian Deep Neural Networks with Empirical Bayes , AAAI 2020 ] which also uses a pretrained DNN as the initialization of variational parameters . How does the proposed method compare to it theoretically and empirically ? The main idea seems quite similar ; the only difference is that the proposed method does not use the pretrained model as prior in VI . Due to the similarity , I think a comparison is necessary . - The authors argue that BNN training suffers from suboptimal local optima . Could the authors provide evidence/citations to support this claim ? I do not think it is true . Perhaps it is true only for a few BNN methods such as BNN using naive VI . - As the proposed method is essentially a VI method , it would be interesting to see comparisons to SOTA VI methods . Overall I 'm positive about this paper and would be happy to increase my score if my concerns are addressed .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the positive feedback . We are encouraged by the acknowledgment of the practicability of the proposed approach and the thoroughness of the experiments . We address specific comments below and have updated the paper accordingly . # # # # Q1 : Concern on \u201c BNN learned from scratch is worse than its corresponding DNN \u201d . To clarify , \u201c BNNs learned from scratch \u201d here refer to those without explicitly sharpened posterior ( i.e. , cold posterior ) . As evidenced in [ Wenzel et . al 2020 ] , they typically demonstrate worse performance than the corresponding DNNs . Sharpening the posterior w.r.t.some validation metrics might improve performance but will cause the learned posterior to aggressively deviate from the Bayesian paradigm [ Wenzel et al. , 2020 ] , possibly compromising the major benefits of BNNs such as calibrated uncertainty estimation . # # # # Q2 : Concern on \u201c improved performance \u201d We agree with the reviewer \u2019 s point , and make more clarifications : the improved performance we claimed corresponds to the comparison between our approach and baselines trained from scratch , e.g. , between BayesAdapter/BayesAdapter- and BNN on ImageNet classification ( Table 1 ) and face recognition ( Table 2 ) . Instead of delivering a new mechanism for learning better BNNs , the major goal of this work is to quickly and cheaply adapt a pre-trained DNN to be Bayesian without compromising performance when facing new tasks . Such a goal is practical and useful as it enables one to first employ advanced techniques to train a DNN with strong performance and then cheaply adapt it to be a BNN . We have revised the paper to make the claim more appropriate . # # # # Q3 : Comparison to [ Krishnan , 2020 ] BayesAdapter connects to [ Krishnan , 2020 ] in that the variational configurations of BayesAdapter and [ Krishnan , 2020 ] are both based on MAP . With the prior specified as MAP mean and unit variance , the primary objectives of [ Krishnan , 2020 ] are also to speed up the learning and to bypass the potential local optima of the posterior ( see Fig.1 of [ Krishnan , 2020 ] ) . Yet , beyond these , BayesAdapter is further designed to achieve good user-friendliness , improved learning stability , and trustable uncertainty estimation , by virtue of optimizers with built-in weight decay , exemplar reparameterization , and uncertainty regularization , respectively . These designs significantly boost the practicability of the proposed method , especially in real-world/large-scale settings . We \u2019 ve added these discussions and comparisons to the revision ( see related work section ) . # # # # Q4 : Regarding \u201c BNN trained from scratch suffers from suboptimal local optima \u201d Yes , for mean-field variational BNNs , this claim is supported by the extensive results in [ Krishnan , 2020 ] and the comparisons between BayesAdapter/BayesAdapter- and BNN in our experiment section . As mentioned by R2 , stochastic gradient MCMC also benefits a lot from a good initialization . For variational BNNs with more complicated posterior , this claim is not rigorous owing to the mismatch between the posterior ( e.g. , a FLOW ) and the pre-trained parameters . We have revised this claim . # # # # Q5 : Comparisons to SoTA VI methods Thanks for the advice . As stated by R4 , the SWAG baseline we compared to is a decently strong baseline . We tried some advanced VI methods like VOGN and noisy-KFAC , but encountered difficulties to scale them up to models with more parameters ( e.g. , wide-ResNet-28-10 ) and large datasets ( e.g. , ImageNet ) , or compatibility issues with practical data augmentation and batch normalization techniques . Nevertheless , we will try to reproduce some of them and include the results in the final version . [ 1 ] * Specifying Weight Priors in Bayesian Deep Neural Networks with Empirical Bayes , Krishnan et al. , AAAI 2020 * [ 2 ] * How Good is the Bayes Posterior in Deep Neural Networks Really ? Wenzel et al. , ICML 2020 *"}], "0": {"review_id": "LjFGgI-_tT0-0", "review_text": "* * Contributions * * This paper proposes a post-hoc approach to obtain model uncertainty estimates from vanilla pre-trained NNs through MFVI fine-tuning . Namely , 1 ) the authors re-cast the KL divergence in the VI objective as weight decay applied to the variational parameters 2 ) the authors propose a variance reduction technique for the reparametrisation trick 3 ) the authors explicitly train their model to produce large model uncertainty on Out of Distribution ( OOD ) inputs . Empirically , the proposed methods seems to retain the strong performance , and much of the simplicity , of point-estimate NNs while providing enhanced robustness in terms of uncertainty estimation . * * originality and significance * * To my knowledge , most of the proposed techniques ( or variants of them ) have appeared before in the literature or are simple extensions of existing approaches : Re-casting MFVI as SGD [ Khan et . al. , 2018 ] , Decorrelation of reparametrisation gradients across batch elements [ Wen et . al. , 2018 ] , Training on OOD measurement points to produce large uncertainty [ Hendrycks et . al. , 2018 and Hafner et . al. , 2018 ] . However , this work refines these ideas and puts them into a single framework which seems to produce strong results . I view this as a noteworthy contribution which might bring Bayesian Deep Learning closer to real world deployments . * * clarity * * Most ideas are presented clearly . The paper is well structured and easy to follow . Some passages are slightly ungrammatical but never does this impede the transmission of ideas . * * pros * * * Presents useful practices to make BNNs more mainstream with strong empirical performance . * Authors provide code for an efficient implementation of exemplar reparametrisation . * The proposed technique for OOD detection bypasses typical pathologies of MFVI [ Ovadia et . al. , 2019 ] by explicitly optimising variational parameters to produce large model uncertainties OOD . * * cons * * * Exemplar reparametrisation is very similar to Flipout [ Wen et . al. , 2018 ] . A comparison of the two would be appreciated . * The proposed technique for OOD detection is not very principled and has provides no guarantees . It seems empirically successful however . * The experimental setup is not very clear , even when reading the supplementary sections concerning experimental setup . Some questions I was left with : * There are many hyperparameters , how did you find all of them ? \u2014Especially the weight decay coefficients . Are the standard deviations implied by these priors interesting / meaningful in any way ? * A single Mutual Information threshold is provided . Is this one used for uncertainty calibration training on all tasks ? Is it also used when classifying inputs as in-distribution or OOD ? If this is the case , it is possible that models without uncertainty calibration training would benefit from using a different threshold . A better metric might be ROC-AUC , as it is threshold agnostic . * The only baselines provided are other VI approaches . SWAG is known to be a decently strong baseline but it is not evaluated for OOD detection peroformance . The current state of the art baseline for uncertainty quantification is deep ensembles [ Ovadia et . al. , 2019 ] . These are much more expensive . However , it might be interesting to see how they compare . * The authors repeat all experiments 3 times but only provide mean results . In some cases , like tables 1 and 2 , the values presented are similar across methods . I think that errorbars ( standard deviation across 3 runs ) would be very informative to the reader . * * Other comments and questions : * * * Typo in title : Bayesian , not Bayeisian * The maximum predictive entropy ( and thus mutual information ) will depend on dimensionality of output space ( number of classes ) . In your experimental section , you say you set a single threshold for all models . Could you further comment on this ? * * References * * [ Wen et . al. , 2018 ] https : //arxiv.org/pdf/1803.04386.pdf [ Khan et . al. , 2018 ] http : //proceedings.mlr.press/v80/khan18a/khan18a.pdf [ Ovadia et . al. , 2019 ] https : //papers.nips.cc/paper/9547-can-you-trust-your-models-uncertainty-evaluating-predictive-uncertainty-under-dataset-shift.pdf [ Hendrycks et . al. , 2018 ] https : //openreview.net/forum ? id=HyxCxhRcY7 [ Hafner et . al. , 2018 ] https : //arxiv.org/abs/1807.09289", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # # Q5 : Regarding the error bars : A : We add the variance of the results into the paper . Here is a copy : | Metric | Acc . ( % ) | NLL | AP ( PGD ) | AP ( fake ) | |-| : - : ||-|-| | CIFAR-10 | 96.82\u00b10.07 | 0.1004\u00b10.0026 | 0.993\u00b10.003 | 0.994\u00b10.001 | | ImageNet | top1 : 76.26\u00b10.06 top5 : 92.96\u00b10.03 | 0.9428\u00b10.0020 | 0.964\u00b10.009 | 0.848\u00b10.037 | We can see that the results of BayesAdapter exhibit less variance . # # # # Q6 : Other questions A : We have revised the typo . As we clarified , we only use a shared threshold $ \\gamma=0.75 $ for training across all the settings . And the learned model is robust against the choice of the $ \\gamma $ , testified by an ablation study on $ \\gamma $ ( on CIFAR-10 ) : | $ \\gamma $ | 0.25 | 0.50 | 0.75 | 1.0 | 1.50 | | -- | -- | -- | -- | -- | -- | | Acc . | 96.93 % | 96.70 % | 96.82 % | 96.74 % | 96.79 % | | AP ( PGD ) | 0.915 | 0.948 | 0.993 | 0.991 | 0.944 | | AP ( fake ) | 0.910 | 0.981 | 0.994 | 0.994 | 0.988 | Different dimensionality of output space would result in different scales of uncertainty , and this is also proved by the sub-figure ( b ) and ( d ) in Fig.3.But we can also note that though with different uncertainty scale , most of the normal examples have uncertainty no more than 0.75 ( on both CIFAR-10 and ImageNet ) , thus once we punish the model to assign no less than 0.75 uncertainty for the OOD data , the model naturally acquires the ability to detect OOD data . Per your suggestion , we might achieve better OOD detection by tuning this threshold according to the data and task , though we skipped at the time of submission ."}, "1": {"review_id": "LjFGgI-_tT0-1", "review_text": "This paper proposed one simple and effective way to trainBayesian neural networks ( BNN ) . Pros : 1.The proposed method is quite simple and cheap to realize , compared to previous Bayesian methods . 2.Extensive experiments on a diverse set of challenging benchmarks have been conducted , which shows several promising results of the proposed method . 3.The proposed idea is novel which distinguishes from most of previous efforts , which try to train BNN from scratch using Bayesian methods . As described in this paper , most of previous methods , though paying much additional efforts than deterministic ones , do not lead to expected results , even with non-diagonal covariance matrices . The BayesAdapter , however , pays little efforts and obtains improvements even with diagonal covariance matrices . From this perspective , this is an encoraging result . Cons : 1.The results of comparison in Table 1 only repot the average result in 3 runs ( 3 is kind of small ) . However , it is better to show the std metric of the result to make the comparison more convincing because the improvement of BayesAdapter in average value is in fact not very apparent , especially compared with MAP . If the variance of the result is large , then there will be large overlap between different methods and thus it is not reasonable to claim that there is an apparent advantage over previous methods . 2.In evaluating the result of BayesAdapter , MC samples are used . What if only using the mean value of the posterior ? Compared to deterministic methods like MAP , inference using MC is more costly . In addition , it is suggested to provide some visualizations of the posterior distribution after BayesAdapter . 3.Based on results in Table 3 , BayesAdapter- performs similar as baselines , which indicates that the improvement comes from calibrating the uncertainty estimation . This leads to another question : what if we also use such calibration for the baseline methods . It would be interesting to make such a comparison .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the positive review . In this following , we address the detailed comments . # # # # Q1 : The std metric Sorry for missing the std . Here we provide it on CIFAR-10 and ImageNet benchmarks . | Metric | Acc . ( % ) | NLL | AP ( PGD ) | AP ( fake ) | |-| : - : ||-|-| | CIFAR-10 | 96.82\u00b10.07 | 0.1004\u00b10.0026 | 0.993\u00b10.003 | 0.994\u00b10.001 | | ImageNet | top1 : 76.26\u00b10.06 top5 : 92.96\u00b10.03 | 0.9428\u00b10.0020 | 0.964\u00b10.009 | 0.848\u00b10.037 | We can see that the results of BayesAdapter are relatively stable , outperforming BNN and MAP with statistical evidence on ImageNet . Following a similar suggestion by R1 , we have revised the paper to make clear the major goal of this work , which is to quickly and cheaply adapt a pre-trained DNN to be Bayesian without compromising performance when facing new tasks , instead of delivering a mechanism for learning better BNNs . # # # # Q2 : MC samples and posterior appearance A : We kindly point out that the results of deterministic inference with only the posterior mean of BayesAdapter are provided in the ablation study \u201c The impacts of ensemble number \u201d and Figure 4 . It is clear that with more than around 20 MC samples , Bayes ensemble ( the green line ) can achieve better prediction results than the deterministic inference ( the yellow line ) . This reflects that the learned posterior does not suffer too much from mode collapse , which is popularly witnessed on mean-field variational inference by the community . As suggested , we plot the parameter posterior of the first convolutional kernel in ResNet-50 architecture learned by BayesAdapter on ImageNet . The results are depicted in Appendix E. The learned variance seems to be disordered , unlike the mean . We leave more explanations as future work . # # # # Q3 : Apply uncertainty regularization to other models A : Yes , we totally agree with this point . We conducted such an experiment , and observed improved uncertainty estimation in the initial phase when applying this technique to other BNN methods , including the BNN baseline . These further confirm the effectiveness and universality of the proposed uncertainty regularization technique . We will try to add complete results in the final version since that training BNNs from scratch is time-consuming ."}, "2": {"review_id": "LjFGgI-_tT0-2", "review_text": "The paper explores the variational training of Bayesian neural networks . It proposes to improve the quality of the inferred variational posterior and computational efficiency of the procedure by ( i ) better initialization ( mean parameters are initialized at the MAP ) ( ii ) reducing variance in the Monte Carlo approximated evidence lower bound by increasing the number of weight samples ( one per datapoint in a batch ) ( iii ) a posterior regularization encouraging higher uncertainty on adversarially generated or other \u201c near OOD \u201d data . The authors use the term BayesAdapter to refer to the process of running black-box variational inference from a fully factorized variational approximation with mean initialized at the MAP estimate and randomly initialized variances . The fact that variational inference ( especially those employing fully factorized approximations ) are susceptible to poor local optima and that better initializations can help navigate these local optima is widely known . The fact that better initializations can lead to somewhat improved posterior approximations is not surprising . Such initializations are also standard practice when employing stochastic gradient MCMC techniques and Laplace approximations ( where it is a requirement ) . Reducing variance by increasing the number of weight samples to one per datapoint in a batch is another straightforward idea , and it is unclear whether it can be claimed as a contribution of the current paper . Kingma et al. , in their local re-parameterization considered a variant with per data samples as well . The uncertainty regularization is indeed novel and appears effective ( but the experiments illustrating its benefits need to be better explained ) . Given the modest methods contributions , the empirical section needs to be particularly strong to demonstrate that the combination of these incremental improvements provides meaningful empirical advantages . To their credit , the authors demonstrate their approach on several large datasets and do provide experiments for vetting different aspects of the proposed extensions to variational BNN training . However , many experiments are missing details and some are lacking key comparisons . Overall this section could be significantly strengthened . * Tables 1 and 2 need to include comparisons against deep ensembles and multi-SWAG ( https : //arxiv.org/pdf/2002.08791.pdf ) . If the goal of this paper is to claim that variationally trained BNNs ( with the proposed improvements ) are useful in practice , a natural question to ask is whether they are competitive with far simpler ensembling approaches that are able to account for the multimodality of the posterior surface , unlike variational BNNs . * How was the calibration threshold $ \\gamma $ chosen for these experiments ? How sensitive is the performance to this choice ? Ideally , the authors would include results with different settings of $ \\gamma $ . How were the prior precisions selected ( which determine $ \\lambda $ set ? My main worry is that the marginal improvements provided by Bayesadapter variants over BNNs disappear when making slightly different parameter choices . It would be great if the authors can demonstrate that this isn \u2019 t the case . * Section 4.2 needs more details about the experimental setup . Did the 1000 / 10000 OOD training/test examples include both images created via PGD and SNGAN ( for CIFAR 10 ) and PGD and BigGAN ( for imagenet ) ? If so , how many from each source ? If not , it would be interesting to see cross performance \u2014 using PGD images for training and SNGAN images for testing . * In Table 4 , it doesn \u2019 t make sense to include ECE numbers from a different architecture trained via SWAG . These numbers are not comparable . Also , interestingly , both BayesAdapter variants have lower ECE scores than vanilla BNN on CIFAR , suggesting poorer calibration . Do the authors have an explanation for this ? Based on concerns about both novelty and experiments I am currently leaning towards a reject , but could be convinced otherwise based on the authors \u2019 response and additional comparisons .", "rating": "5: Marginally below acceptance threshold", "reply_text": "# # # # Q4 : Regarding setup We are sorry for causing the misunderstanding . To clarify : For the training , we use only 1000 fake examples ( e.g. , those from SNGAN on CIFAR-10 and from BigGAN on ImageNet ) and all the _uniformly perturbed training examples_ for optimizing uncertainty regularization ( see the last part of Section 3 ) . We did not include PGD perturbed examples into training because that resembles adversarial training and is time-consuming . For evaluation , we estimate the uncertainty on a held-out set of fake examples and _PGD perturbed validation examples_ and report the results . We want to point out that the idea to \u201c see cross performance \u201d might not be helpful with the uncertainty estimation trained on adversarially perturbed data , we can not expect it to successfully generalize to the examples produced by GAN which have significantly different fingerprints ( see Figure 6 in Appendix ) . We have offered a study on if the learned uncertainty could reasonably generalize in part 2 of Section 4.3 . The results give a positive answer . # # # # Q5 : Regarding ECE Thanks for the advice . We clarify that on CIFAR-10 , SWAG also uses wide-ResNet-28-10 , and shows weaker calibration than BayesAdapter with a substantial margin . The comparison to SWAG with a different architecture on ImageNet may indeed be less meaningful , and we have revised this . Regarding the second point , we emphasize the core notion of Bayesian deep learning : the predictive confidence is usually unreliable , thus we need a better measure like predictive uncertainty . With the superiority of the uncertainty estimation of BayesAdapter validated by Table 3 and Table 5 , its weaker ECE does not substantially undermine its practical value . As shown in the ablation study \u201c Uncertainty-based rejective decision \u201d and Figure 5 , we can leverage the predictive uncertainty to achieve robust rejective decision making instead of using predictive confidence . Anyway , returning to this phenomenon , we speculate this is because the fine-tuning start point _MAP_ has too bad ECE and the fine-tuning rounds are few ."}, "3": {"review_id": "LjFGgI-_tT0-3", "review_text": "This paper introduces a fast way to get Bayesian posterior by using a pretrained deterministic model . Specifically , the authors first train a standard DNN model and then use it to initialize the variational parameters . Finally the variational parameters are optimized through standard variational inference ( VI ) training . To further improve uncertainty estimate , the authors propose an uncertainty regularization which maximizes the prediction inconsistency on out-of-distribution ( OOD ) data . Experiments including image classification and uncertainty estimates are conducted to demonstrate the proposed method . The idea of this paper is quite simple : initialize the mean in variational parameters by a pretrained DNN . Thus the method is cheap and simple enough to use broadly in practice . The authors did reasonable empirical tests . I especially appreciate the ablation study which helps understand the method a lot . The paper is well-written and easy to follow . I mainly have the following concerns about the paper . - One of the main motivations to use a pretrained DNN is that BNN learned from scratch is worse than its corresponding DNN . I feel this claim is misleading . Many papers have shown that BNNs trained from scratch outperform DNNs . Particularly the paper [ Wenzel et.al . 2020 ] which the authors cited to support their claim clearly shows that BNN ( with reasonable temperature ) is significantly better than DNN in predictive performance . But I do agree that the proposed method is cheaper than training BNNs from scratch . - The experimental results verify the effectiveness of the proposed method . But the results of the proposed method seem to be worse than BNNs training from scratch ( e.g.the ImageNet results in [ Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning , ICLR 2020 ] are much better ) . This also supports my first point , that the main benefit of the proposed method is to get the Bayesian posterior fast and cheaply , instead of to improve BNNs \u2019 performance . I think the authors should revise the claim to be more precise . - The proposed method is closely related to [ Specifying Weight Priors in Bayesian Deep Neural Networks with Empirical Bayes , AAAI 2020 ] which also uses a pretrained DNN as the initialization of variational parameters . How does the proposed method compare to it theoretically and empirically ? The main idea seems quite similar ; the only difference is that the proposed method does not use the pretrained model as prior in VI . Due to the similarity , I think a comparison is necessary . - The authors argue that BNN training suffers from suboptimal local optima . Could the authors provide evidence/citations to support this claim ? I do not think it is true . Perhaps it is true only for a few BNN methods such as BNN using naive VI . - As the proposed method is essentially a VI method , it would be interesting to see comparisons to SOTA VI methods . Overall I 'm positive about this paper and would be happy to increase my score if my concerns are addressed .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the positive feedback . We are encouraged by the acknowledgment of the practicability of the proposed approach and the thoroughness of the experiments . We address specific comments below and have updated the paper accordingly . # # # # Q1 : Concern on \u201c BNN learned from scratch is worse than its corresponding DNN \u201d . To clarify , \u201c BNNs learned from scratch \u201d here refer to those without explicitly sharpened posterior ( i.e. , cold posterior ) . As evidenced in [ Wenzel et . al 2020 ] , they typically demonstrate worse performance than the corresponding DNNs . Sharpening the posterior w.r.t.some validation metrics might improve performance but will cause the learned posterior to aggressively deviate from the Bayesian paradigm [ Wenzel et al. , 2020 ] , possibly compromising the major benefits of BNNs such as calibrated uncertainty estimation . # # # # Q2 : Concern on \u201c improved performance \u201d We agree with the reviewer \u2019 s point , and make more clarifications : the improved performance we claimed corresponds to the comparison between our approach and baselines trained from scratch , e.g. , between BayesAdapter/BayesAdapter- and BNN on ImageNet classification ( Table 1 ) and face recognition ( Table 2 ) . Instead of delivering a new mechanism for learning better BNNs , the major goal of this work is to quickly and cheaply adapt a pre-trained DNN to be Bayesian without compromising performance when facing new tasks . Such a goal is practical and useful as it enables one to first employ advanced techniques to train a DNN with strong performance and then cheaply adapt it to be a BNN . We have revised the paper to make the claim more appropriate . # # # # Q3 : Comparison to [ Krishnan , 2020 ] BayesAdapter connects to [ Krishnan , 2020 ] in that the variational configurations of BayesAdapter and [ Krishnan , 2020 ] are both based on MAP . With the prior specified as MAP mean and unit variance , the primary objectives of [ Krishnan , 2020 ] are also to speed up the learning and to bypass the potential local optima of the posterior ( see Fig.1 of [ Krishnan , 2020 ] ) . Yet , beyond these , BayesAdapter is further designed to achieve good user-friendliness , improved learning stability , and trustable uncertainty estimation , by virtue of optimizers with built-in weight decay , exemplar reparameterization , and uncertainty regularization , respectively . These designs significantly boost the practicability of the proposed method , especially in real-world/large-scale settings . We \u2019 ve added these discussions and comparisons to the revision ( see related work section ) . # # # # Q4 : Regarding \u201c BNN trained from scratch suffers from suboptimal local optima \u201d Yes , for mean-field variational BNNs , this claim is supported by the extensive results in [ Krishnan , 2020 ] and the comparisons between BayesAdapter/BayesAdapter- and BNN in our experiment section . As mentioned by R2 , stochastic gradient MCMC also benefits a lot from a good initialization . For variational BNNs with more complicated posterior , this claim is not rigorous owing to the mismatch between the posterior ( e.g. , a FLOW ) and the pre-trained parameters . We have revised this claim . # # # # Q5 : Comparisons to SoTA VI methods Thanks for the advice . As stated by R4 , the SWAG baseline we compared to is a decently strong baseline . We tried some advanced VI methods like VOGN and noisy-KFAC , but encountered difficulties to scale them up to models with more parameters ( e.g. , wide-ResNet-28-10 ) and large datasets ( e.g. , ImageNet ) , or compatibility issues with practical data augmentation and batch normalization techniques . Nevertheless , we will try to reproduce some of them and include the results in the final version . [ 1 ] * Specifying Weight Priors in Bayesian Deep Neural Networks with Empirical Bayes , Krishnan et al. , AAAI 2020 * [ 2 ] * How Good is the Bayes Posterior in Deep Neural Networks Really ? Wenzel et al. , ICML 2020 *"}}