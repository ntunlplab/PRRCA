{"year": "2021", "forum": "jMc7DlflrMC", "title": "Density Constrained Reinforcement Learning", "decision": "Reject", "meta_review": "The reviewers raised several theoretical and empirical questions about the paper. During the rebuttals, the authors seem to  successfully address the experimental issues, in particular those raised by Reviewers 1 and 2. However, the theoretical concerns have mainly remained unanswered. Reviewer 2 has a major concern about the convergence of Algorithm 1, both Reviewers 3 and 4 (in particular Reviewer 3) have several questions about the theoretical analysis and see the technical contribution of the work relatively low. These are not minor issues and require a major revision to be properly addressed. So, I suggest that the authors take the reviewers comments into account, revise their work and properly address the issues raised by the reviewers, and prepare their work for an upcoming conference. ", "reviews": [{"review_id": "jMc7DlflrMC-0", "review_text": "Summary : This paper proposes to solve constrained RL through the lens of setting constraints on state density , rather than setting constraints on value functions . The main contributions are ( 1 ) a proof of the duality between the density function and Q-value function in constrained RL , and ( 2 ) a primal-dual algorithm , called DCRL , for solving the constrained density problem . Recommendation : This paper tackles an important problem , and presents a theoretically-grounded novel perspective on constrained RL . I 'm leaning toward accept , but have several suggestions regarding the empirical evaluation , that are detailed below . Pros : * The paper is clearly written and well-motivated . * It seems easier to express constraints in terms of the state density function rather than Q-values . * The empirical evaluation contains a comparison to several state-of-the-art approaches for constrained RL . * DCRL shows modest improvements in the experiments , compared to the baselines . Cons : * I would like to see a comparison against the simple yet effective baseline of PPO combined with Lagrangian relaxation , with respect to constraints on Q-values . * I would like to see results on more challenging constrained RL tasks , where the state density function is harder to estimate , for instance the Safety Gym task suite [ 1 ] . These tasks are particularly interesting because CPO ( surprisingly ) performs worse on them than the simpler PPO-Lagrangian approach . I 'm curious whether DCRL would struggle as well . Comments/questions regarding notation : * Using $ P_a ( s , s ' ) $ and $ R_a ( s , s ' ) $ in the definition of an MDP ( Section 2 ) is odd ; I recommend using the typical notation of $ P ( s , a , s ' ) $ and $ R ( s , a , s ' ) $ . * In the definition of MDPs , $ V^\\pi ( s ) $ is incorrectly referred to as _reward_ / _total reward_ , but it should instead be referred to as the _return_ or _expected cumulative discounted reward_ . * It 's not clear that the $ [ 0 , \\infty ] $ for the density function $ \\rho $ corresponds to $ t $ ( in the first sentence of the Density Functions section ) . * Why are there two $ s $ 's in $ \\rho_s^\\pi ( s ) $ ( e.g. , in Equations 1 and 2 ) . Should the subscript be a $ \\gamma $ instead ? * What does it mean to `` linearly interpolate '' $ \\sigma_+ $ and $ \\sigma_- $ ( at the end of Section 3.3 ) ? [ 1 ] Ray et al.Benchmarking Safe Exploration in Deep Reinforcement Learning . 2019 .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your detailed comments . In the following , we will address your major concerns . We have added your requested comparison experiments and updated our draft accordingly with updates in blue color . We hope that you find the updated information erasing your concerns . * * Q1 : Add comparison to PPO with Lagrangian relaxation . Add experiment on Safe Gym . * * A1 : We have added the experiments in * * Appendix B.5 * * on the Safe Gym and compared to PPO with Lagrangian relaxation ( PPO-L ) . Three tasks are considered , including PointGoal , PointButton and PointPush . It is shown that DCRL has better performance than PPO-L on improving the reward while respecting the constraints . In fact , RL with constrained value functions is a special case of our proposed density constrained RL since we can define the density constraint in the form of a value function . * * Q2 : Some notations are old . $ V^ { \\pi } ( s ) $ is sometimes incorrectly referred to as reward or total reward . * * A2 : We have changed the notations and refer to $ V^ { \\pi } ( s ) $ as expected cumulative discounted reward . The changes can be found in the updated draft . * * Q3 : It is not clear that $ [ 0 , \\infty ] $ corresponds to the time $ t $ in the definition of $ \\rho $ . * * A3 : It is true that $ [ 0 , \\infty ] $ corresponds to $ t $ . But in our paper , we mainly consider the stationary density $ \\rho_s ( s ) $ that does not depend on $ t $ . To avoid confusion , we have removed the $ [ 0 , \\infty ] $ in our definition of density . * * Q4 : Why are there 2 $ s $ \u2019 s in density $ \\rho_s^ { \\pi } ( s ) $ ? * * A4 : The subscript s means the density is stationary . * * Q5 : What does it mean to linearly interpolate $ \\bar { \\sigma } _ { + } $ * * * * and * * $ \\bar { \\sigma } _ { - } $ ? A5 : $ \\bar { \\sigma } $ only contains the values of Lagrangian multipliers on finite state samples . When evaluating $ \\bar { \\sigma } $ on continuous states , e.g. , state $ s $ , we perform linear interpolation on the Lagrangian multipliers on samples near s so that the value of $ \\bar { \\sigma } $ at $ s $ can be evaluated ."}, {"review_id": "jMc7DlflrMC-1", "review_text": "The submission introduces the dual formulation of a Constrained MDP optimization . While their algorithm does not guarantee that the constraints are satisfied during learning , the experiments show that they are reasonably respected , and yield high returns . Despite the typos and approximations I could identify , I found the exposition of the submission very clear , the arguments are convincing , and the experiments are compelling . For all these reasons , I recommend to accept it . I rate my confidence low because I have some doubts about the novelty of Theorems 1 and 2 . My main regret is the lack of discussion of the results , both on the theoretical and empirical side . Indeed : - Theorems 1 and 2 : is n't it immediate since J_d=J_p ? Is it really novel ? - Proposition 1 : I 'm surprised the failure modes of Proposition 1 are not more discussed . How can it not converge ? Ca n't we guarantee monotonic improvement ? Likewise , how can it converge to an unfeasible solution ? Ca n't we enforce the feasibility during the iteration ? Does it only happen when no feasible solution is ever encountered ? - Experiments : what explains the apparent superiority of the primal-dual formulation as opposed to the other algorithms ? What are their failure modes ? Too conservative ? Not safe enough ? Does it depend on the setting of some hyperparameters ? Is n't it simply because the constraint is defined on the density level and that Algorithm 1 is the only one designed to solve this ? Typos and minor comments : - P_a and R_a are SxS- > _ - I feel that straightening the d , like with \\dd from physics package would make the integrals more readable . - why is the density function defined as [ 0 , \\infty ) xS- > R_ > 0 , is n't [ 0 , \\infty ) =R_ > 0 ? What is it used for ? Should n't it be simply S- > R_ > 0 ? - I would advise to rename \\rho_s to \\rho , because s is used as a variable everywhere , for instance in Eq.2 \\rho_s ( s ' ) looks like it depends on s. - equation 3 \\tho_s^\\pi should be just \\rho .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your positive feedback . In the following , we will address your major concerns . We have updated our draft according to suggestions by other reviewers with updates in blue color . We hope that you find our answers informative and we look forward to more discussions . * * Q1 : Are Theorems 1 and 2 novel ? * * A1 : Although the duality between value function and density function is already known to the community , Theorem 1 is the first to rigorously prove that the duality also exists between Q function and density function , so it can be used on model-free RL . Theorem 2 proves that the duality still exists under constraints on state density , which is the key enabler of the proposed density constrained RL . Our work is the first to formulate density constrained RL as a more general safety constrained RL problem , on both discrete and continuous space , and use duality to solve the density constrained RL problem . Please also see our response to Reviewer 3 . * * Q2 : How can Algorithm 1 not converge and the assumption of Proposition 1 be not satisfied ? * * A2 : In our experiments ( Section 4 and Appendix B ) , the algorithm could always converge to feasible solutions . The only failure mode we observed is that when the maximum and minimum density thresholds ( $ \\rho_ { max } $ and $ \\rho_ { min } $ ) are extreme values that are impossible to satisfy . In this case , the perturbation terms would be updated in every iteration , yet the constraints remain unsatisfied . When setting the density thresholds , we can check whether the problem itself is feasible under such constraints and avoid infeasible settings . * * Q3 : What explains the superior performance of DCRL over other algorithms ? * * A3 : Proposition 1 has proved the optimality of the proposed DCRL algorithm , which means DCRL can achieve the highest cumulative return under the constraints . Other methods would be more conservative in exploration based on the empirical observation that they did not achieve higher return than DCRL in the experiment . The constraints in Section 4.1 are all defined on value functions rather than state density . We converted the value constraints to density constraints using the duality between value function and density function . Thus , under the same constraints on value functions , the comparison is fair ."}, {"review_id": "jMc7DlflrMC-2", "review_text": "This paper studied density constrained reinforcement learning ( DCRL ) , which compared with standard RL has constraints on the stationary state distribution . It proposes a primal-dual optimization method and proves its optimality . The theoretical analysis in this paper assumes $ P_a ( s^\\prime , s ) $ is perfectly known whenever needed . In fact , in this case formulations ( 3 ) , ( 4 ) , ( 5 ) , ( 6 ) are simple deterministic convex ( linear ) optimization ( regardless if density constraints are introduced or not ) . The duality $ J_d $ and $ J_p $ duality follows directly from duality for convex programs . The developed algorithm , Algorithm 1 , is also simple application of ( primal ) -dual subgradient method for convex programs . If the D_s update and \\rho_s^\\pi update are done exactly , i.e. , assuming perfect knowledge of P_a ( s^\\prime , s ) , then the theorems in this paper is just simple consequence of well-established convex optimization results . ( I looked into the proof in the appendix.It seems the analysis indeed assumes perfect $ P_a ( s^\\prime , s ) $ in the analysis . ) Overall , I think the technical contribution and novelty of this are marginal .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for raising your concerns on the novelty of our paper . We would like to point out a couple of misunderstandings which may lead to your concern of novelty . We have also updated our draft to make both the theoretical guarantee and experiments results stronger as suggested by other reviewers . * * First * * , we would like to clarify that we don \u2019 t assume the transition probabilities to be known . Instead , both the primal update and dual update are totally model-free . The primal update uses standard RL algorithms by taking samples from the environment ; the dual update uses the samples from the primal step to approximate the density function ( see our answers to Reviewer 2 on how to approximate the density function ) and update the perturbation term . All calculation is done model-free , relying only on the samples . We do not assume the perfect knowledge of $ P_a ( s \u2019 , s ) $ . * * Second * * , although duality is a standard approach in convex programs , we argue that our work is non-trivial and important , as also agreed by other reviewers . The fact that duality is a standard technique in convex programs does not mean that it is obviously known how to use it to solve density/safety constrained RL . The duality relationship has inspired many important works over decades . We feel it is unfair to use the fact that such duality is a standard technique to conclude that our work is incremental . * * To be more concrete about our contributions : * * 1 ) Our work is the first to formulate density constrained RL as a more general safety constrained RL problem , on both discrete and continuous space . We are also the first to rigorously prove and use the duality between density functions and Q functions over continuous state space to solve density constrained RL . Our algorithm can automatically solve density constrained RL with guarantees on satisfying the density constraints , unlike the \u201c reward-based \u201d RL which needs fine-tuning and trial-and-error for each use case . 2 ) Any nonnegative dual variable satisfying the conservation law ( Liouville PDE in the continuous case ) is a valid dual . However , among all valid dual variables , the state density we defined is associated with a clear physical interpretation as the concentration of states , and we are able to directly apply constraints on the density in RL . 3 ) we accomplish the non-trivial work of using the algorithm on an extensive set of case studies and show its advantage over other state-of-the-art safety constrained RL methods . Regarding concerns on the robustness of the approach using $ P_a $ from samples but not perfectly known , there is plenty of literature on this topic yet the robustness relies on the implementation . For example , a discrete MDP is much easier to analyze than a continuous MDP where function approximation is used . We plan to give a rigorous robustness analysis in our future work ."}, {"review_id": "jMc7DlflrMC-3", "review_text": "In this paper , the authors explored the duality between density function and value function in the setting of density constrained RL . Based on this , the author proposed a new safe constrained policy optimization algorithm by jointing optimizing policy and lagrangian multipliers of the density constraints , with an extra effort to estimate the stationary state density of current policy $ \\pi $ using kernel density estimation . The empirical results demonstrate that the proposed algorithm is effective in several mujoco benchmark and autonomous electric vehicle controlling . I believe exploring the duality between density function and value function in the setting in density constrained reinforcement learning is new , and may be helpful in safe reinforcement learning . Followings are my detailed questions and comments : - I have a major concern about estimating the marginal ( or stationary ) state function of the policy $ \\pi $ . The author use kernel density estimation to estimate $ \\rho ( s ) $ , which I think deserve more discussion in this paper , and at least some empirical experiments should be conducted to verify its accuracy . Moreover , since the estimation requires to use kernels , it would be good to see the empirical results of choices of kernels and bandwidth of the kernel . - All the derivation and the stop criterion is based on infinite number of samples , which will not guarantee that the constrain is satisfied using finite mc samples to estimate ( since you do n't have the transition , you can only use samples to estimate ) . Will there be any guarantee that the exact estimation of density and constrain is upper bounded by some empirical estimation , such that we can use mc sampling to estimate the density or constraints ? - The duality between the density function and value function is well-known in the community , and has drawn great attention recently in the policy evaluation community [ 1 , 2 , 3 , 4 ] , which I think the authors should have some discussion on this in the related work , and to see if the techniques can be used to estimate the density functions . Overall I think the paper proposed a new perspective for density constrained rl , which I think is interesting and is publishable if my above concerns are well addressed . -- I will update my scores if my questions are addressed : ) [ 1 ] Nachum , Ofir , et al . `` Dualdice : Behavior-agnostic estimation of discounted stationary distribution corrections . '' Advances in Neural Information Processing Systems . 2019 . [ 2 ] Tang , Ziyang , et al . `` Doubly robust bias reduction in infinite horizon off-policy estimation . `` , ICLR 2020 . [ 3 ] Uehara , Masatoshi , Jiawei Huang , and Nan Jiang . `` Minimax weight and q-function learning for off-policy evaluation . '' arXiv preprint arXiv:1910.12809 ( 2019 ) . [ 4 ] Nachum , Ofir , and Bo Dai . `` Reinforcement learning via fenchel-rockafellar duality . '' arXiv preprint arXiv:2001.01866 ( 2020 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your detailed comments . In the following , we will address your major concerns . We have updated our draft accordingly with updates in blue color . We hope that you find the updated information erasing your concerns . * * Q1 : More discussion on the kernel density estimation . * * A1 : We added * * Appendix C * * , in which we have added both the theoretical upper bounds and the empirical results of the estimation error of using kernel density estimation . The theoretical bounds are from well-known results . In the experiments , we chose the Epanechnikov kernel and linear kernel with 4 different bandwidth ( 8 configurations in total ) , and evaluated their accuracy on estimating the state density of the Point-Circle task . It is shown that all the 8 configurations can result in a good estimation of the state density . * * Q2 : Guarantees of the algorithm and theory using a finite number of samples to estimate the density . * * A2 : We have also addressed this in our newly added Appendix C. In short , the guarantee can be easily computed using the theoretical kernel density estimation accuracy found and then added as a buffer to the density constraint to be robust against the errors of density estimation . We have provided Algorithm 2 in Appendix C as an updated version of Algorithm 1 , to demonstrate how to consider the error in density estimation when using our proposed DCRL approach . * * Q3 : Related works on the duality between the density function and value function . * * A3 : Thank you for pointing out the references . We agree that these are very relevant papers and we have added a discussion on the related work , see the highlighted part of Section 1 ."}], "0": {"review_id": "jMc7DlflrMC-0", "review_text": "Summary : This paper proposes to solve constrained RL through the lens of setting constraints on state density , rather than setting constraints on value functions . The main contributions are ( 1 ) a proof of the duality between the density function and Q-value function in constrained RL , and ( 2 ) a primal-dual algorithm , called DCRL , for solving the constrained density problem . Recommendation : This paper tackles an important problem , and presents a theoretically-grounded novel perspective on constrained RL . I 'm leaning toward accept , but have several suggestions regarding the empirical evaluation , that are detailed below . Pros : * The paper is clearly written and well-motivated . * It seems easier to express constraints in terms of the state density function rather than Q-values . * The empirical evaluation contains a comparison to several state-of-the-art approaches for constrained RL . * DCRL shows modest improvements in the experiments , compared to the baselines . Cons : * I would like to see a comparison against the simple yet effective baseline of PPO combined with Lagrangian relaxation , with respect to constraints on Q-values . * I would like to see results on more challenging constrained RL tasks , where the state density function is harder to estimate , for instance the Safety Gym task suite [ 1 ] . These tasks are particularly interesting because CPO ( surprisingly ) performs worse on them than the simpler PPO-Lagrangian approach . I 'm curious whether DCRL would struggle as well . Comments/questions regarding notation : * Using $ P_a ( s , s ' ) $ and $ R_a ( s , s ' ) $ in the definition of an MDP ( Section 2 ) is odd ; I recommend using the typical notation of $ P ( s , a , s ' ) $ and $ R ( s , a , s ' ) $ . * In the definition of MDPs , $ V^\\pi ( s ) $ is incorrectly referred to as _reward_ / _total reward_ , but it should instead be referred to as the _return_ or _expected cumulative discounted reward_ . * It 's not clear that the $ [ 0 , \\infty ] $ for the density function $ \\rho $ corresponds to $ t $ ( in the first sentence of the Density Functions section ) . * Why are there two $ s $ 's in $ \\rho_s^\\pi ( s ) $ ( e.g. , in Equations 1 and 2 ) . Should the subscript be a $ \\gamma $ instead ? * What does it mean to `` linearly interpolate '' $ \\sigma_+ $ and $ \\sigma_- $ ( at the end of Section 3.3 ) ? [ 1 ] Ray et al.Benchmarking Safe Exploration in Deep Reinforcement Learning . 2019 .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your detailed comments . In the following , we will address your major concerns . We have added your requested comparison experiments and updated our draft accordingly with updates in blue color . We hope that you find the updated information erasing your concerns . * * Q1 : Add comparison to PPO with Lagrangian relaxation . Add experiment on Safe Gym . * * A1 : We have added the experiments in * * Appendix B.5 * * on the Safe Gym and compared to PPO with Lagrangian relaxation ( PPO-L ) . Three tasks are considered , including PointGoal , PointButton and PointPush . It is shown that DCRL has better performance than PPO-L on improving the reward while respecting the constraints . In fact , RL with constrained value functions is a special case of our proposed density constrained RL since we can define the density constraint in the form of a value function . * * Q2 : Some notations are old . $ V^ { \\pi } ( s ) $ is sometimes incorrectly referred to as reward or total reward . * * A2 : We have changed the notations and refer to $ V^ { \\pi } ( s ) $ as expected cumulative discounted reward . The changes can be found in the updated draft . * * Q3 : It is not clear that $ [ 0 , \\infty ] $ corresponds to the time $ t $ in the definition of $ \\rho $ . * * A3 : It is true that $ [ 0 , \\infty ] $ corresponds to $ t $ . But in our paper , we mainly consider the stationary density $ \\rho_s ( s ) $ that does not depend on $ t $ . To avoid confusion , we have removed the $ [ 0 , \\infty ] $ in our definition of density . * * Q4 : Why are there 2 $ s $ \u2019 s in density $ \\rho_s^ { \\pi } ( s ) $ ? * * A4 : The subscript s means the density is stationary . * * Q5 : What does it mean to linearly interpolate $ \\bar { \\sigma } _ { + } $ * * * * and * * $ \\bar { \\sigma } _ { - } $ ? A5 : $ \\bar { \\sigma } $ only contains the values of Lagrangian multipliers on finite state samples . When evaluating $ \\bar { \\sigma } $ on continuous states , e.g. , state $ s $ , we perform linear interpolation on the Lagrangian multipliers on samples near s so that the value of $ \\bar { \\sigma } $ at $ s $ can be evaluated ."}, "1": {"review_id": "jMc7DlflrMC-1", "review_text": "The submission introduces the dual formulation of a Constrained MDP optimization . While their algorithm does not guarantee that the constraints are satisfied during learning , the experiments show that they are reasonably respected , and yield high returns . Despite the typos and approximations I could identify , I found the exposition of the submission very clear , the arguments are convincing , and the experiments are compelling . For all these reasons , I recommend to accept it . I rate my confidence low because I have some doubts about the novelty of Theorems 1 and 2 . My main regret is the lack of discussion of the results , both on the theoretical and empirical side . Indeed : - Theorems 1 and 2 : is n't it immediate since J_d=J_p ? Is it really novel ? - Proposition 1 : I 'm surprised the failure modes of Proposition 1 are not more discussed . How can it not converge ? Ca n't we guarantee monotonic improvement ? Likewise , how can it converge to an unfeasible solution ? Ca n't we enforce the feasibility during the iteration ? Does it only happen when no feasible solution is ever encountered ? - Experiments : what explains the apparent superiority of the primal-dual formulation as opposed to the other algorithms ? What are their failure modes ? Too conservative ? Not safe enough ? Does it depend on the setting of some hyperparameters ? Is n't it simply because the constraint is defined on the density level and that Algorithm 1 is the only one designed to solve this ? Typos and minor comments : - P_a and R_a are SxS- > _ - I feel that straightening the d , like with \\dd from physics package would make the integrals more readable . - why is the density function defined as [ 0 , \\infty ) xS- > R_ > 0 , is n't [ 0 , \\infty ) =R_ > 0 ? What is it used for ? Should n't it be simply S- > R_ > 0 ? - I would advise to rename \\rho_s to \\rho , because s is used as a variable everywhere , for instance in Eq.2 \\rho_s ( s ' ) looks like it depends on s. - equation 3 \\tho_s^\\pi should be just \\rho .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your positive feedback . In the following , we will address your major concerns . We have updated our draft according to suggestions by other reviewers with updates in blue color . We hope that you find our answers informative and we look forward to more discussions . * * Q1 : Are Theorems 1 and 2 novel ? * * A1 : Although the duality between value function and density function is already known to the community , Theorem 1 is the first to rigorously prove that the duality also exists between Q function and density function , so it can be used on model-free RL . Theorem 2 proves that the duality still exists under constraints on state density , which is the key enabler of the proposed density constrained RL . Our work is the first to formulate density constrained RL as a more general safety constrained RL problem , on both discrete and continuous space , and use duality to solve the density constrained RL problem . Please also see our response to Reviewer 3 . * * Q2 : How can Algorithm 1 not converge and the assumption of Proposition 1 be not satisfied ? * * A2 : In our experiments ( Section 4 and Appendix B ) , the algorithm could always converge to feasible solutions . The only failure mode we observed is that when the maximum and minimum density thresholds ( $ \\rho_ { max } $ and $ \\rho_ { min } $ ) are extreme values that are impossible to satisfy . In this case , the perturbation terms would be updated in every iteration , yet the constraints remain unsatisfied . When setting the density thresholds , we can check whether the problem itself is feasible under such constraints and avoid infeasible settings . * * Q3 : What explains the superior performance of DCRL over other algorithms ? * * A3 : Proposition 1 has proved the optimality of the proposed DCRL algorithm , which means DCRL can achieve the highest cumulative return under the constraints . Other methods would be more conservative in exploration based on the empirical observation that they did not achieve higher return than DCRL in the experiment . The constraints in Section 4.1 are all defined on value functions rather than state density . We converted the value constraints to density constraints using the duality between value function and density function . Thus , under the same constraints on value functions , the comparison is fair ."}, "2": {"review_id": "jMc7DlflrMC-2", "review_text": "This paper studied density constrained reinforcement learning ( DCRL ) , which compared with standard RL has constraints on the stationary state distribution . It proposes a primal-dual optimization method and proves its optimality . The theoretical analysis in this paper assumes $ P_a ( s^\\prime , s ) $ is perfectly known whenever needed . In fact , in this case formulations ( 3 ) , ( 4 ) , ( 5 ) , ( 6 ) are simple deterministic convex ( linear ) optimization ( regardless if density constraints are introduced or not ) . The duality $ J_d $ and $ J_p $ duality follows directly from duality for convex programs . The developed algorithm , Algorithm 1 , is also simple application of ( primal ) -dual subgradient method for convex programs . If the D_s update and \\rho_s^\\pi update are done exactly , i.e. , assuming perfect knowledge of P_a ( s^\\prime , s ) , then the theorems in this paper is just simple consequence of well-established convex optimization results . ( I looked into the proof in the appendix.It seems the analysis indeed assumes perfect $ P_a ( s^\\prime , s ) $ in the analysis . ) Overall , I think the technical contribution and novelty of this are marginal .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for raising your concerns on the novelty of our paper . We would like to point out a couple of misunderstandings which may lead to your concern of novelty . We have also updated our draft to make both the theoretical guarantee and experiments results stronger as suggested by other reviewers . * * First * * , we would like to clarify that we don \u2019 t assume the transition probabilities to be known . Instead , both the primal update and dual update are totally model-free . The primal update uses standard RL algorithms by taking samples from the environment ; the dual update uses the samples from the primal step to approximate the density function ( see our answers to Reviewer 2 on how to approximate the density function ) and update the perturbation term . All calculation is done model-free , relying only on the samples . We do not assume the perfect knowledge of $ P_a ( s \u2019 , s ) $ . * * Second * * , although duality is a standard approach in convex programs , we argue that our work is non-trivial and important , as also agreed by other reviewers . The fact that duality is a standard technique in convex programs does not mean that it is obviously known how to use it to solve density/safety constrained RL . The duality relationship has inspired many important works over decades . We feel it is unfair to use the fact that such duality is a standard technique to conclude that our work is incremental . * * To be more concrete about our contributions : * * 1 ) Our work is the first to formulate density constrained RL as a more general safety constrained RL problem , on both discrete and continuous space . We are also the first to rigorously prove and use the duality between density functions and Q functions over continuous state space to solve density constrained RL . Our algorithm can automatically solve density constrained RL with guarantees on satisfying the density constraints , unlike the \u201c reward-based \u201d RL which needs fine-tuning and trial-and-error for each use case . 2 ) Any nonnegative dual variable satisfying the conservation law ( Liouville PDE in the continuous case ) is a valid dual . However , among all valid dual variables , the state density we defined is associated with a clear physical interpretation as the concentration of states , and we are able to directly apply constraints on the density in RL . 3 ) we accomplish the non-trivial work of using the algorithm on an extensive set of case studies and show its advantage over other state-of-the-art safety constrained RL methods . Regarding concerns on the robustness of the approach using $ P_a $ from samples but not perfectly known , there is plenty of literature on this topic yet the robustness relies on the implementation . For example , a discrete MDP is much easier to analyze than a continuous MDP where function approximation is used . We plan to give a rigorous robustness analysis in our future work ."}, "3": {"review_id": "jMc7DlflrMC-3", "review_text": "In this paper , the authors explored the duality between density function and value function in the setting of density constrained RL . Based on this , the author proposed a new safe constrained policy optimization algorithm by jointing optimizing policy and lagrangian multipliers of the density constraints , with an extra effort to estimate the stationary state density of current policy $ \\pi $ using kernel density estimation . The empirical results demonstrate that the proposed algorithm is effective in several mujoco benchmark and autonomous electric vehicle controlling . I believe exploring the duality between density function and value function in the setting in density constrained reinforcement learning is new , and may be helpful in safe reinforcement learning . Followings are my detailed questions and comments : - I have a major concern about estimating the marginal ( or stationary ) state function of the policy $ \\pi $ . The author use kernel density estimation to estimate $ \\rho ( s ) $ , which I think deserve more discussion in this paper , and at least some empirical experiments should be conducted to verify its accuracy . Moreover , since the estimation requires to use kernels , it would be good to see the empirical results of choices of kernels and bandwidth of the kernel . - All the derivation and the stop criterion is based on infinite number of samples , which will not guarantee that the constrain is satisfied using finite mc samples to estimate ( since you do n't have the transition , you can only use samples to estimate ) . Will there be any guarantee that the exact estimation of density and constrain is upper bounded by some empirical estimation , such that we can use mc sampling to estimate the density or constraints ? - The duality between the density function and value function is well-known in the community , and has drawn great attention recently in the policy evaluation community [ 1 , 2 , 3 , 4 ] , which I think the authors should have some discussion on this in the related work , and to see if the techniques can be used to estimate the density functions . Overall I think the paper proposed a new perspective for density constrained rl , which I think is interesting and is publishable if my above concerns are well addressed . -- I will update my scores if my questions are addressed : ) [ 1 ] Nachum , Ofir , et al . `` Dualdice : Behavior-agnostic estimation of discounted stationary distribution corrections . '' Advances in Neural Information Processing Systems . 2019 . [ 2 ] Tang , Ziyang , et al . `` Doubly robust bias reduction in infinite horizon off-policy estimation . `` , ICLR 2020 . [ 3 ] Uehara , Masatoshi , Jiawei Huang , and Nan Jiang . `` Minimax weight and q-function learning for off-policy evaluation . '' arXiv preprint arXiv:1910.12809 ( 2019 ) . [ 4 ] Nachum , Ofir , and Bo Dai . `` Reinforcement learning via fenchel-rockafellar duality . '' arXiv preprint arXiv:2001.01866 ( 2020 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your detailed comments . In the following , we will address your major concerns . We have updated our draft accordingly with updates in blue color . We hope that you find the updated information erasing your concerns . * * Q1 : More discussion on the kernel density estimation . * * A1 : We added * * Appendix C * * , in which we have added both the theoretical upper bounds and the empirical results of the estimation error of using kernel density estimation . The theoretical bounds are from well-known results . In the experiments , we chose the Epanechnikov kernel and linear kernel with 4 different bandwidth ( 8 configurations in total ) , and evaluated their accuracy on estimating the state density of the Point-Circle task . It is shown that all the 8 configurations can result in a good estimation of the state density . * * Q2 : Guarantees of the algorithm and theory using a finite number of samples to estimate the density . * * A2 : We have also addressed this in our newly added Appendix C. In short , the guarantee can be easily computed using the theoretical kernel density estimation accuracy found and then added as a buffer to the density constraint to be robust against the errors of density estimation . We have provided Algorithm 2 in Appendix C as an updated version of Algorithm 1 , to demonstrate how to consider the error in density estimation when using our proposed DCRL approach . * * Q3 : Related works on the duality between the density function and value function . * * A3 : Thank you for pointing out the references . We agree that these are very relevant papers and we have added a discussion on the related work , see the highlighted part of Section 1 ."}}