{"year": "2020", "forum": "SJlbGJrtDB", "title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers", "decision": "Accept (Poster)", "meta_review": "The paper lies on the borderline. An accept is suggested based on majority reviews and authors' response.", "reviews": [{"review_id": "SJlbGJrtDB-0", "review_text": "## Update after the rebuttal I appreciate the author's clarification in the rebuttal and the additional result on ImageNet, which addressed some of my concerns. # Summary This paper proposes a trainable mask layer in neural networks for compressing neural networks end-to-end. The main idea is to apply a differentiable mask to individual weights such that the mask itself is also trained through backpropagation. They also propose to add a regularization term that encourages weights are masked out as much as possible. The result on MNIST and CIFAR show that their method can achieve the highest (weight) compression rate and the lowest accuracy reduction compared to baselines. # Originality - The idea of applying trainable mask to weights and regularizing toward masking out is quite interesting and new to my knowledge. # Quality - The performance seems to be good, though it would be more convincing if the paper showed results on larger datasets like ImageNet. - The analysis is interesting, but I am not fully convinced by the \"strong evidence to the efficiency and effectiveness of our algorithm\". For example, the final layer's remaining ratio is constantly 1 in Figure 3, while it starts from nearly 0 in Figure 4. The paper also argues that the final layer was not that important in Figure 4 because the lower layers have not learned useful features. This seems not only contradictory to the result of Figure 3 but also inconsistent of the accuracy being quickly increasing up to near 90% while the remaining ratio is nearly 0 in Figure 4. - If the motivation of the sparse training is to reduce memory consumption AND computation, showing some results on the reduction of the computation cost after sparse training would important to complete the story. # Clarity - The description of the main idea is not clear. - What are \"structure gradient\" and \"performance gradient\"? They are not mathematically defined in the paper. - I do not understand how the proposed method can \"recover\" from pruned connection, although it seems to be indeed happening in the experiment. The paper claims that the use of long-tailed higher-order estimator H(x) makes it possible to recover. However, H(x) still seems to have flat lines where the derivative is 0. Is H(x) in Equation 3 and Figure 2d are showing \"derivative\" or step function itself? In any cases, I do not see how the gradient flows once a weight is masked out. # Significance - This paper proposes an interesting idea (trainable mask), though I did not fully get how the mask is defined/trained and has a potential to recover after pruning. The analysis of the compression rate throughout training is interesting but does not seem to be fully convincing. It would be stronger if the paper 1) included more results on bigger datasets like ImageNet, 2) described the main idea more clearly, and 3) provided more convincing evidence why the proposed method is effective. ", "rating": "6: Weak Accept", "reply_text": "Thank you so much for the detailed reviews and valuable remarks . I am sorry for the unclarity caused by the lack of information and improper usage of terms . Here I will use the trainable masked fully connected layer as an example to explain your concerns about Clarity . Consider a trainable masked fully connected layer with parameter $ W\\in R^ { m\\times n } $ and trainable threshold vector $ t\\in R^m $ . This means that this layer get $ n $ input neurons and $ m $ output neurons . A neuron-wise threshold $ t_i $ is defined for the $ i $ th output neuron . 1 ) How the mask $ M\\in R^ { m\\times n } $ is generated and used in the feed forward process $ M_ { ij } = S ( |W_ { ij } |-t_i ) $ for $ 1\\leq i \\leq m $ , $ 1\\leq j \\leq n $ , where $ S ( x ) $ is the unit step function . For each connection connects to output neuron $ i $ , the magnitude of the corresponding weight $ W_ { ij } $ will be compared with the neuron-wise threshold $ t_i $ . Instead of directly setting $ W_ { ij } $ to 0 like traditional pruning algorithms , the value of $ W_ { ij } $ is preserved in our method . The information about whether to prune this connection is stored in $ M_ { ij } $ , where 0 means pruned ( masked ) and 1 means unpruned ( unmasked ) . We denote $ P = W\\odot M $ . Instead of the original parameter $ W $ , $ P $ will be used in the matrix-vector multiplication . Meanwhile $ Q\\in R^ { m\\times n } $ is just a intermediate variable , where $ Q_ { ij } = |W_ { ij } |-t_i $ for $ 1\\leq i \\leq m $ , $ 1\\leq j \\leq n $ 2 ) What are `` structure gradient '' and `` performance gradient '' mathematically Refer to Figure 1 , in the back-propagation process , $ P $ will receive a gradient and we denote it as $ dP $ . Let 's consider the gradients that flow from right to left . The performance gradient is $ dP \\odot M $ The gradient received by $ M $ is $ dP\\odot W $ The gradient received by $ Q $ is $ dP\\odot W\\odot H ( Q ) $ , where $ H ( x ) $ is the long-tail derivative estimation for $ S ( x ) $ and $ H ( Q ) $ is the result of $ H ( x ) $ applied to $ Q $ elementwisely . The structure gradient is $ dP\\odot W\\odot H ( Q ) \\odot sgn ( W ) $ , where $ sgn ( W ) $ is the result of sign function applied to $ W $ elementwisely . The gradient received by the vector threshold $ t $ is $ dt\\in R^m $ . We denote $ dT = -dP\\odot W\\odot H ( Q ) $ , then $ dT\\in R^ { m\\times n } $ . And we will have $ dt_i = \\sum_ { j=1 } ^nT_ { ij } $ for $ 1\\leq i \\leq m $ . 3 ) How the gradient flow to pruned ( masked ) weights The gradient received by the parameter $ W $ is $ dW = dP\\odot M + dP\\odot W\\odot H ( Q ) \\odot sgn ( W ) $ Since we add $ \\ell_2 $ regularization in the training process , all the elements in $ W $ are distributed within $ [ -1 , 1 ] $ . Meanwhile , almost all the elements in the vector threshold are distributed within $ [ 0 , 1 ] $ . The exceptions are the situation as shown in Figure 3 ( a ) and Figure 4 ( a ) where the last layer get no weight pruned ( masked ) . Regarding the process of getting $ Q $ , all the elements in $ Q $ are within $ [ -1 , 1 ] $ . Therefore $ H ( Q ) $ is a dense matrix . Then $ W $ , $ H ( Q ) $ and $ sgn ( W ) $ are all dense matrices and the pruned ( masked ) weights can receive the structure gradient $ dP\\odot W\\odot H ( Q ) \\odot sgn ( W ) $ 4 ) How the pruned ( masked ) connection get recovered The masked weights , the unmasked weights and the vector threshold can all receive gradients and be updated constantly during the training process . A connection with corresponding weight $ W_ { ij } $ and threshold $ t_i $ may be pruned ( masked ) if $ |W_ { ij } | < t_i $ at certain time point in the training process . Meanwhile , it can be easily recovered ( unmasked ) if $ |W_ { ij } | > t_i $ during the later training process . 5 ) Question regarding Equation 3 and Figure 2 ( d ) The Equation 3 and Figure 2 ( d ) are both present the long-tail derivative estimation . The Figure 2 ( a ) present the unit step function ."}, {"review_id": "SJlbGJrtDB-1", "review_text": "This paper presents a novel network pruning algorithm -- Dynamic Sparse Training. It aims at jointly finding the optimal network parameters and sparse network structure in a unified optimization process with trainable pruning thresholds. The experiments on MNIST, and cifar-10 show that proposed model can find sparse neural network models, but unfortunately with little performance loss. The key limitation of the proposed model come from the experiments. (1) Nowadays, the nature and important question is that, one can not tolerate the degraded performance, even with sparse neural network. Thus, it is important to show that the proposed model can find sparse neural network models, and with increased performance. (2) Another weakness is that proposed model has to be tested on large scale dataset, e.g. ImageNet-2012.current two datasets are too small to support the conclusive results of this proposed model. (3) As for the model itself, I donot find very significant novelty. For example, Sec. 3.3 (TRAINABLE MASKED LAYERS) in general is quite following previous works\u2019 designing principle. Thus, the novelty should be summarized, and highlighted in the paper. ", "rating": "3: Weak Reject", "reply_text": "Thank you so much for the constructive comments . We find these comments really helpful . Following are the explanations about the limitations : 1 ) The Performance of sparse models . Our method indeed gets sparse models with increased performance than dense models . As present in section 4.4 , our method can get sparse models with better performance when the sparsity of models is less than 90 % . It is only when we want to get modes with high sparsity ( > 90 % ) that there will be a noticeable performance loss . We think this is a common case for network pruning that there will be a loss of performance when over 90 % of parameters are removed . 2 ) The lack of result on ImageNet The results are updated in the revision . 3 ) Summary of Novelty It is our negligence that does not present the novelty clearly . The followings present the novelty of our method : 1 . Directly get sparse models in the training process The typical pruning process is a three-stage pipeline , i.e. , training , pruning and fine-tuning . In our method , no further pruning and fine-tuning are needed . We can directly get sparse models in the training process . There exist some similar works but we get better performance compared with the existing methods as present in Section 4.1 . 2.Trainable fine-grained pruning thresholds Most of the previous pruning algorithm adopt a single pruning threshold for each layer or the whole architecture . In our method , since a threshold vector $ t\\in R^m $ is used for each layer with parameter $ W\\in R^ { m\\times n } $ , we have neuron-wise pruning thresholds for fully connected and recurrent layer and filter-wise pruning thresholds for convolutional layer . Meanwhile , all these fine-grained pruning thresholds can be updated automatically via back-propagation as present in Section 3 . We are not aware of any other method that achieves this . 3.The ability to properly recover the previously pruned weights . Most of the current pruning and all the sparse training methods conduct hard pruning with following properties : - Pruned weights will be directly set to 0 - No further update via back-propagation for pruned weights Directly setting pruned weight to 0 causes the loss of historical parameter importance , which make it hard to determine : - When and which pruned weights should be recovered . - What value should we assigned to the recovered weights . Therefore , current sparse training methods that allow the recovery of pruned weights randomly choose a predefined portion of pruned weights to recover and these recover weights are randomly initialized . Our method has following properties that properly solve these problems : - Pruned weights will not be directly set to 0 . Instead , the mask $ W $ will store the information about which weights are pruned . - Pruned weights can still be updated via back-propagation . - The corresponding pruning thresholds will also be updated via back-propagation Therefore , it is the dynamic change of both pruned weights and the corresponding pruning thresholds that determine when and which pruned weights should be recovered . Meanwhile , the recovered weight has the value that it learns via back-propagation instead of a randomly assigned value . 4.Continuous fine-grained pruning and recovering over the whole training process A typical pruning process is conducted after a certain training epoch as following : - Determine current importance of weight - Prune certain percentage of weight . Usually , a training epoch will have tens of thousands of training steps , which is the feed-forward and back-propagation pass for a single mini-batch . In our method , since both the network weights and the pruning thresholds will be updated vis back-propagation at each training step . Our method can have continuous fine-grained pruning and recovering at each training step . We are not aware of any other method that can achieves this . 5.Automatic and dynamic layer-wise pruning rates adjustment over the network There are two critical problems in network pruning : - How many weights should be pruned in each pruning step - What is the proper pruning rates for each layer over the network Usually , the pruning is conduct by some predefined pruning schedule like pruning 5 % at each step with totally 10 pruning steps . Meanwhile , it is quite hard to properly determine the pruning rates for each layer . Current methods either use a single global pruning threshold for the whole model or layer-by-layer greedy pruning . We illustrate their limitation on Page 1 . In our method , with the property present above , the portion of weights to be pruned at each step and the proper pruning rates for each layer are automatically determined by the dynamic update of parameter $ W $ and threshold $ t $ . Meanwhile , as present in section 4.2 and 4.3 , the swift adjustment of pruning rates and consistent sparse patterns prove the effectiveness of our method in proper adjustment of layer-wise pruning rates ."}, {"review_id": "SJlbGJrtDB-2", "review_text": "This paper proposes an algorithm for training networks with sparse parameter tensors. This involves achieving sparsity by application of a binary mask, where the mask is determined by current parameter values and a learned threshold. It also involves the addition of a specific regularizer which encourages the thresholds used for the mask to be large. Gradients with respect to both masked-out parameters, and with respect to mask thresholds, are computed using a \"long tailed\" variant of the straight-through-estimator. The algorithm proposed in this paper seems sensible, but rather ad hoc. It is not motivated by theory or careful experiments. As such, the value of the paper will be determined largely by the strength of the experimental results. I believe the experimental results to be strong, though I am not familiar enough with this subfield to be confident there are not missing baselines. There are many minor English language problems (e.g. with articles, prepositions, plural vs. singular forms, and verb tense), though these don't significantly interfere with understanding. Rounding up to weak accept, though my confidence is low because I am basing this positive assessment on experimental results for tasks on which I am not well calibrated. more detailed comments: \"using the same training epochs\" -> \"using the same number of training epochs\" \"achieves prior art performance\" -> \"achieves state of the art performance\" \"the inference of deep neural network\" -> \"inference in deep neural networks\" This paper considered only sparsity of weights -- it might have been nice to also discuss/run experiments exploring sparsity of activations. eq. 4 -- Can you say more about why this particular form is the right one for the regularizer? It seems rather arbitrary. (it will tend to be dominated by the smallest thresholds, and so would seem to encourage a minimum degree of sparsity in every layer) I appreciate the analysis in section 4.3.", "rating": "6: Weak Accept", "reply_text": "Thank you so much for your time and constructive comments . It is our negligence that does not present our motivation and contributions of our work clearly in the early manuscript . We are revising it to clearly present our motivations , methods and contributions . We really appreciate your positive assessment of our experimental results . We are running experiments on the more complex dataset ( ImageNet 2012 ) to make our results more convinced . Thank you so much for pointing out the language problems in our manuscript . We are polishing the writing continuously and will update the revised manuscript soon . Following are the responses to the concerns : 1 . The sparsity of activations . Usually , only the sparsity of weights is considered for the evaluation of network pruning methods . Your suggestion that we should evaluate the sparsity of activations provides a new point of view . We are conducting related experiments and will add this in the revised manuscript . 2.The choice of sparse regularizer . The sparse regularizer is used to penalize low threshold values to increase the degree of sparsity . So the basic requirement is that the value of the regularizer function $ f ( x ) $ should decrease as $ x $ increases . We actually tested several options like $ \\exp ( -x ) $ , $ \\frac { 1 } { x } $ , and $ \\log ( { \\frac { 1 } { x } } ) $ . It seems that other choices except $ \\exp ( -x ) $ penalize too much . Therefore the training loss is dominated by the sparse regularizer term $ L_s $ , which tends to mask out all the weights easily . Due to our experiments , $ \\exp ( -x ) $ is the best choice among all these options that support wider range choice of $ \\alpha $ and get higher degree of sparsity . So $ \\exp ( -x ) $ is adpoted for the sparse regularizer . We are still searching for the better sparse regularizer . 3.Sparse regularizer dominated by the smallest thresholds Yes , as you point out , the sparse regularizer will be dominated by some small thresholds . Although we discuss the layer-wise sparsity in the paper , the thresholds are actually neuron-wise or filter-wise . Considering a masked fully connected layer with parameter $ W\\in R^ { m\\times n } $ and threshold vector $ t\\in R^m $ , this layer will have $ m $ output neurons . For each output neuron $ i $ , our method assigns a neuron-wise threshold $ t_i $ . The elements in $ t $ are all initialized to $ 0 $ , which means that we assume the neurons and weights in this layer have the same importance before the training . And the same penalties for small value are added for all these thresholds . At the end of the training process , if some thresholds still have small values , it only indicates that these neurons are more important than other neurons so that the weights corresponding to these neurons should have a small degree of sparsity . Usually , there are hundreds of neurons in each layer . So the layer-wise sparsity will still be high even if there are few small neuron-wise thresholds . 4.The analysis in 4.3 Thank you for your positive feedback about the analysis in section 4.3 . As we present in section 4.3 , our method can generate consistent sparse pattern that indicates the degree of redundancy for each layer . Besides , our method can distinguish neuron-wise or filter-wise importance with fine-grained neuron-wise and layer-wise thresholds as we present above . Currently , we are not aware of any other method that can have similar effects ."}], "0": {"review_id": "SJlbGJrtDB-0", "review_text": "## Update after the rebuttal I appreciate the author's clarification in the rebuttal and the additional result on ImageNet, which addressed some of my concerns. # Summary This paper proposes a trainable mask layer in neural networks for compressing neural networks end-to-end. The main idea is to apply a differentiable mask to individual weights such that the mask itself is also trained through backpropagation. They also propose to add a regularization term that encourages weights are masked out as much as possible. The result on MNIST and CIFAR show that their method can achieve the highest (weight) compression rate and the lowest accuracy reduction compared to baselines. # Originality - The idea of applying trainable mask to weights and regularizing toward masking out is quite interesting and new to my knowledge. # Quality - The performance seems to be good, though it would be more convincing if the paper showed results on larger datasets like ImageNet. - The analysis is interesting, but I am not fully convinced by the \"strong evidence to the efficiency and effectiveness of our algorithm\". For example, the final layer's remaining ratio is constantly 1 in Figure 3, while it starts from nearly 0 in Figure 4. The paper also argues that the final layer was not that important in Figure 4 because the lower layers have not learned useful features. This seems not only contradictory to the result of Figure 3 but also inconsistent of the accuracy being quickly increasing up to near 90% while the remaining ratio is nearly 0 in Figure 4. - If the motivation of the sparse training is to reduce memory consumption AND computation, showing some results on the reduction of the computation cost after sparse training would important to complete the story. # Clarity - The description of the main idea is not clear. - What are \"structure gradient\" and \"performance gradient\"? They are not mathematically defined in the paper. - I do not understand how the proposed method can \"recover\" from pruned connection, although it seems to be indeed happening in the experiment. The paper claims that the use of long-tailed higher-order estimator H(x) makes it possible to recover. However, H(x) still seems to have flat lines where the derivative is 0. Is H(x) in Equation 3 and Figure 2d are showing \"derivative\" or step function itself? In any cases, I do not see how the gradient flows once a weight is masked out. # Significance - This paper proposes an interesting idea (trainable mask), though I did not fully get how the mask is defined/trained and has a potential to recover after pruning. The analysis of the compression rate throughout training is interesting but does not seem to be fully convincing. It would be stronger if the paper 1) included more results on bigger datasets like ImageNet, 2) described the main idea more clearly, and 3) provided more convincing evidence why the proposed method is effective. ", "rating": "6: Weak Accept", "reply_text": "Thank you so much for the detailed reviews and valuable remarks . I am sorry for the unclarity caused by the lack of information and improper usage of terms . Here I will use the trainable masked fully connected layer as an example to explain your concerns about Clarity . Consider a trainable masked fully connected layer with parameter $ W\\in R^ { m\\times n } $ and trainable threshold vector $ t\\in R^m $ . This means that this layer get $ n $ input neurons and $ m $ output neurons . A neuron-wise threshold $ t_i $ is defined for the $ i $ th output neuron . 1 ) How the mask $ M\\in R^ { m\\times n } $ is generated and used in the feed forward process $ M_ { ij } = S ( |W_ { ij } |-t_i ) $ for $ 1\\leq i \\leq m $ , $ 1\\leq j \\leq n $ , where $ S ( x ) $ is the unit step function . For each connection connects to output neuron $ i $ , the magnitude of the corresponding weight $ W_ { ij } $ will be compared with the neuron-wise threshold $ t_i $ . Instead of directly setting $ W_ { ij } $ to 0 like traditional pruning algorithms , the value of $ W_ { ij } $ is preserved in our method . The information about whether to prune this connection is stored in $ M_ { ij } $ , where 0 means pruned ( masked ) and 1 means unpruned ( unmasked ) . We denote $ P = W\\odot M $ . Instead of the original parameter $ W $ , $ P $ will be used in the matrix-vector multiplication . Meanwhile $ Q\\in R^ { m\\times n } $ is just a intermediate variable , where $ Q_ { ij } = |W_ { ij } |-t_i $ for $ 1\\leq i \\leq m $ , $ 1\\leq j \\leq n $ 2 ) What are `` structure gradient '' and `` performance gradient '' mathematically Refer to Figure 1 , in the back-propagation process , $ P $ will receive a gradient and we denote it as $ dP $ . Let 's consider the gradients that flow from right to left . The performance gradient is $ dP \\odot M $ The gradient received by $ M $ is $ dP\\odot W $ The gradient received by $ Q $ is $ dP\\odot W\\odot H ( Q ) $ , where $ H ( x ) $ is the long-tail derivative estimation for $ S ( x ) $ and $ H ( Q ) $ is the result of $ H ( x ) $ applied to $ Q $ elementwisely . The structure gradient is $ dP\\odot W\\odot H ( Q ) \\odot sgn ( W ) $ , where $ sgn ( W ) $ is the result of sign function applied to $ W $ elementwisely . The gradient received by the vector threshold $ t $ is $ dt\\in R^m $ . We denote $ dT = -dP\\odot W\\odot H ( Q ) $ , then $ dT\\in R^ { m\\times n } $ . And we will have $ dt_i = \\sum_ { j=1 } ^nT_ { ij } $ for $ 1\\leq i \\leq m $ . 3 ) How the gradient flow to pruned ( masked ) weights The gradient received by the parameter $ W $ is $ dW = dP\\odot M + dP\\odot W\\odot H ( Q ) \\odot sgn ( W ) $ Since we add $ \\ell_2 $ regularization in the training process , all the elements in $ W $ are distributed within $ [ -1 , 1 ] $ . Meanwhile , almost all the elements in the vector threshold are distributed within $ [ 0 , 1 ] $ . The exceptions are the situation as shown in Figure 3 ( a ) and Figure 4 ( a ) where the last layer get no weight pruned ( masked ) . Regarding the process of getting $ Q $ , all the elements in $ Q $ are within $ [ -1 , 1 ] $ . Therefore $ H ( Q ) $ is a dense matrix . Then $ W $ , $ H ( Q ) $ and $ sgn ( W ) $ are all dense matrices and the pruned ( masked ) weights can receive the structure gradient $ dP\\odot W\\odot H ( Q ) \\odot sgn ( W ) $ 4 ) How the pruned ( masked ) connection get recovered The masked weights , the unmasked weights and the vector threshold can all receive gradients and be updated constantly during the training process . A connection with corresponding weight $ W_ { ij } $ and threshold $ t_i $ may be pruned ( masked ) if $ |W_ { ij } | < t_i $ at certain time point in the training process . Meanwhile , it can be easily recovered ( unmasked ) if $ |W_ { ij } | > t_i $ during the later training process . 5 ) Question regarding Equation 3 and Figure 2 ( d ) The Equation 3 and Figure 2 ( d ) are both present the long-tail derivative estimation . The Figure 2 ( a ) present the unit step function ."}, "1": {"review_id": "SJlbGJrtDB-1", "review_text": "This paper presents a novel network pruning algorithm -- Dynamic Sparse Training. It aims at jointly finding the optimal network parameters and sparse network structure in a unified optimization process with trainable pruning thresholds. The experiments on MNIST, and cifar-10 show that proposed model can find sparse neural network models, but unfortunately with little performance loss. The key limitation of the proposed model come from the experiments. (1) Nowadays, the nature and important question is that, one can not tolerate the degraded performance, even with sparse neural network. Thus, it is important to show that the proposed model can find sparse neural network models, and with increased performance. (2) Another weakness is that proposed model has to be tested on large scale dataset, e.g. ImageNet-2012.current two datasets are too small to support the conclusive results of this proposed model. (3) As for the model itself, I donot find very significant novelty. For example, Sec. 3.3 (TRAINABLE MASKED LAYERS) in general is quite following previous works\u2019 designing principle. Thus, the novelty should be summarized, and highlighted in the paper. ", "rating": "3: Weak Reject", "reply_text": "Thank you so much for the constructive comments . We find these comments really helpful . Following are the explanations about the limitations : 1 ) The Performance of sparse models . Our method indeed gets sparse models with increased performance than dense models . As present in section 4.4 , our method can get sparse models with better performance when the sparsity of models is less than 90 % . It is only when we want to get modes with high sparsity ( > 90 % ) that there will be a noticeable performance loss . We think this is a common case for network pruning that there will be a loss of performance when over 90 % of parameters are removed . 2 ) The lack of result on ImageNet The results are updated in the revision . 3 ) Summary of Novelty It is our negligence that does not present the novelty clearly . The followings present the novelty of our method : 1 . Directly get sparse models in the training process The typical pruning process is a three-stage pipeline , i.e. , training , pruning and fine-tuning . In our method , no further pruning and fine-tuning are needed . We can directly get sparse models in the training process . There exist some similar works but we get better performance compared with the existing methods as present in Section 4.1 . 2.Trainable fine-grained pruning thresholds Most of the previous pruning algorithm adopt a single pruning threshold for each layer or the whole architecture . In our method , since a threshold vector $ t\\in R^m $ is used for each layer with parameter $ W\\in R^ { m\\times n } $ , we have neuron-wise pruning thresholds for fully connected and recurrent layer and filter-wise pruning thresholds for convolutional layer . Meanwhile , all these fine-grained pruning thresholds can be updated automatically via back-propagation as present in Section 3 . We are not aware of any other method that achieves this . 3.The ability to properly recover the previously pruned weights . Most of the current pruning and all the sparse training methods conduct hard pruning with following properties : - Pruned weights will be directly set to 0 - No further update via back-propagation for pruned weights Directly setting pruned weight to 0 causes the loss of historical parameter importance , which make it hard to determine : - When and which pruned weights should be recovered . - What value should we assigned to the recovered weights . Therefore , current sparse training methods that allow the recovery of pruned weights randomly choose a predefined portion of pruned weights to recover and these recover weights are randomly initialized . Our method has following properties that properly solve these problems : - Pruned weights will not be directly set to 0 . Instead , the mask $ W $ will store the information about which weights are pruned . - Pruned weights can still be updated via back-propagation . - The corresponding pruning thresholds will also be updated via back-propagation Therefore , it is the dynamic change of both pruned weights and the corresponding pruning thresholds that determine when and which pruned weights should be recovered . Meanwhile , the recovered weight has the value that it learns via back-propagation instead of a randomly assigned value . 4.Continuous fine-grained pruning and recovering over the whole training process A typical pruning process is conducted after a certain training epoch as following : - Determine current importance of weight - Prune certain percentage of weight . Usually , a training epoch will have tens of thousands of training steps , which is the feed-forward and back-propagation pass for a single mini-batch . In our method , since both the network weights and the pruning thresholds will be updated vis back-propagation at each training step . Our method can have continuous fine-grained pruning and recovering at each training step . We are not aware of any other method that can achieves this . 5.Automatic and dynamic layer-wise pruning rates adjustment over the network There are two critical problems in network pruning : - How many weights should be pruned in each pruning step - What is the proper pruning rates for each layer over the network Usually , the pruning is conduct by some predefined pruning schedule like pruning 5 % at each step with totally 10 pruning steps . Meanwhile , it is quite hard to properly determine the pruning rates for each layer . Current methods either use a single global pruning threshold for the whole model or layer-by-layer greedy pruning . We illustrate their limitation on Page 1 . In our method , with the property present above , the portion of weights to be pruned at each step and the proper pruning rates for each layer are automatically determined by the dynamic update of parameter $ W $ and threshold $ t $ . Meanwhile , as present in section 4.2 and 4.3 , the swift adjustment of pruning rates and consistent sparse patterns prove the effectiveness of our method in proper adjustment of layer-wise pruning rates ."}, "2": {"review_id": "SJlbGJrtDB-2", "review_text": "This paper proposes an algorithm for training networks with sparse parameter tensors. This involves achieving sparsity by application of a binary mask, where the mask is determined by current parameter values and a learned threshold. It also involves the addition of a specific regularizer which encourages the thresholds used for the mask to be large. Gradients with respect to both masked-out parameters, and with respect to mask thresholds, are computed using a \"long tailed\" variant of the straight-through-estimator. The algorithm proposed in this paper seems sensible, but rather ad hoc. It is not motivated by theory or careful experiments. As such, the value of the paper will be determined largely by the strength of the experimental results. I believe the experimental results to be strong, though I am not familiar enough with this subfield to be confident there are not missing baselines. There are many minor English language problems (e.g. with articles, prepositions, plural vs. singular forms, and verb tense), though these don't significantly interfere with understanding. Rounding up to weak accept, though my confidence is low because I am basing this positive assessment on experimental results for tasks on which I am not well calibrated. more detailed comments: \"using the same training epochs\" -> \"using the same number of training epochs\" \"achieves prior art performance\" -> \"achieves state of the art performance\" \"the inference of deep neural network\" -> \"inference in deep neural networks\" This paper considered only sparsity of weights -- it might have been nice to also discuss/run experiments exploring sparsity of activations. eq. 4 -- Can you say more about why this particular form is the right one for the regularizer? It seems rather arbitrary. (it will tend to be dominated by the smallest thresholds, and so would seem to encourage a minimum degree of sparsity in every layer) I appreciate the analysis in section 4.3.", "rating": "6: Weak Accept", "reply_text": "Thank you so much for your time and constructive comments . It is our negligence that does not present our motivation and contributions of our work clearly in the early manuscript . We are revising it to clearly present our motivations , methods and contributions . We really appreciate your positive assessment of our experimental results . We are running experiments on the more complex dataset ( ImageNet 2012 ) to make our results more convinced . Thank you so much for pointing out the language problems in our manuscript . We are polishing the writing continuously and will update the revised manuscript soon . Following are the responses to the concerns : 1 . The sparsity of activations . Usually , only the sparsity of weights is considered for the evaluation of network pruning methods . Your suggestion that we should evaluate the sparsity of activations provides a new point of view . We are conducting related experiments and will add this in the revised manuscript . 2.The choice of sparse regularizer . The sparse regularizer is used to penalize low threshold values to increase the degree of sparsity . So the basic requirement is that the value of the regularizer function $ f ( x ) $ should decrease as $ x $ increases . We actually tested several options like $ \\exp ( -x ) $ , $ \\frac { 1 } { x } $ , and $ \\log ( { \\frac { 1 } { x } } ) $ . It seems that other choices except $ \\exp ( -x ) $ penalize too much . Therefore the training loss is dominated by the sparse regularizer term $ L_s $ , which tends to mask out all the weights easily . Due to our experiments , $ \\exp ( -x ) $ is the best choice among all these options that support wider range choice of $ \\alpha $ and get higher degree of sparsity . So $ \\exp ( -x ) $ is adpoted for the sparse regularizer . We are still searching for the better sparse regularizer . 3.Sparse regularizer dominated by the smallest thresholds Yes , as you point out , the sparse regularizer will be dominated by some small thresholds . Although we discuss the layer-wise sparsity in the paper , the thresholds are actually neuron-wise or filter-wise . Considering a masked fully connected layer with parameter $ W\\in R^ { m\\times n } $ and threshold vector $ t\\in R^m $ , this layer will have $ m $ output neurons . For each output neuron $ i $ , our method assigns a neuron-wise threshold $ t_i $ . The elements in $ t $ are all initialized to $ 0 $ , which means that we assume the neurons and weights in this layer have the same importance before the training . And the same penalties for small value are added for all these thresholds . At the end of the training process , if some thresholds still have small values , it only indicates that these neurons are more important than other neurons so that the weights corresponding to these neurons should have a small degree of sparsity . Usually , there are hundreds of neurons in each layer . So the layer-wise sparsity will still be high even if there are few small neuron-wise thresholds . 4.The analysis in 4.3 Thank you for your positive feedback about the analysis in section 4.3 . As we present in section 4.3 , our method can generate consistent sparse pattern that indicates the degree of redundancy for each layer . Besides , our method can distinguish neuron-wise or filter-wise importance with fine-grained neuron-wise and layer-wise thresholds as we present above . Currently , we are not aware of any other method that can have similar effects ."}}