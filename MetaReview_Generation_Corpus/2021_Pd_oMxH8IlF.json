{"year": "2021", "forum": "Pd_oMxH8IlF", "title": "Iterated learning for emergent systematicity in VQA", "decision": "Accept (Oral)", "meta_review": "This paper presents an original perspective on how to learn layouts and modules of neural module networks jointly, in a way that encourages the emergence of compositional solutions. In particular, layouts are treated as messages from an emergent language, and iterated learning is used to encourage the emergence of structure. The paper shows good performance in inducing compositional structure in two datasets.\n\nSummarizing the reviewers' doubts, one is that the idea is tested on relatively toyish data sets, and it is not clear how it would scale up. The second, coming from one reviewer, concerns a lack of originality that, honestly, I do not see. If anything, this is probably the most original paper in my pool.\n\nConcerning the first point, that is a fair objection, but I think that getting good results on program learning on datasets such as CLEVER is more than encouraging for a paper that is introducing quite a novel idea for the first time.\n\nFinally, the authors added new text and new experiments strenghtening their conclusion during the discussion.\n\nI am strongly in favour of accepting this paper.\n", "reviews": [{"review_id": "Pd_oMxH8IlF-0", "review_text": "Review : The authors address methods to encourage the emergence of the layout expression structures on the frameworks of neural module networks ( NMN ) for the visual QA problems . The methods are motivated from the works on language emergence for communication between multi-agents and the language acquisition of new-born babies from parents , which achieved with limited data . The methods , \u2018 iterative learning \u2019 ( IL ) are designed as forming two agents ( program generators and execution engines ) to play VQA games . Basic architectures and learning methods seem to be very similar to the approach of semi-supervised learning introduced in [ ICCV17 ] . This paper deals with one of very interesting topics , the language emergence among cooperative multi-agent environments and the compositionality of human language as recent related studies are well-surveyed in related work . In particular , the main idea of problem formation , layout expressions in NMN as emergent languages is very fresh and interesting . The main claims are as follows : ( 1 ) the proposed approach of IL improves generalization performance for visual QA , and it is shown experimentally by comparing the ablation results of IL . ( 2 ) the language structures in the ground-truth data are recovered with only limited supervisions and the superiority is validated on two datasets \u2013 SHAPES-SyGeT and CLOSURE . However , I believe that the evidences for their claims are insufficient . Specifically , the authors do not provide enough information of language structure such as the superiority compared to other methods and the structure similarity of recovery levels . I recommend 'ok , but not good enough \u2013 reject \u2019 for this paper . Pros : - The authors propose novel interesting problem and their solutions . Arguably , it seems potentially to be on one of important research flows to make influence to lots of works for academia in the future . - They find and report good performance for out-of-distribution accuracies for visual QA datasets . Concerns : - It is not clear which parts in the proposed methods are novel with respect to previous works . Those make vague which parts are the authors \u2019 contributions . - I think IL should be clarified from semi-supervised learning approaches on them for visual QA . Also , for reproducibility , it should be specified which parts are with/without IL . What is \u2018 learning bottleneck \u2019 on this approach ? Also , it is not enough how program generators and execution engines are specified , even though some explanations are in appendix . I think it needs more links for reference or explanation . - As mentioned above , the supports for main claims are not appropriate or unclear . It needs to theoretically or experimentally show the results of comparison with other methods and the similarity of the recovery level of language structures . - Table 1 reports the result of comparative methods such as MAC and FiLM without program supervision . How are they configured in the experiments ? - I think that it would be better understandable to show usability and superiority with the experimental results on realistic visual QA such as VQA and GQA . Minors : - In Section 3.1 , \u03b3 . - > \u03b3 , \u03b2.- > \u03b2 - It needs the description for operators and symbols for the formulae in Section 3.1 [ ICCV17 ] Johnson et al. , Inferring and executing programs for visual reasoning , ICCV 2017 . After rebuttal From the revised version of this manuscript , the authors resolve my major concerns such as clarity/reproducibility of the method , differentiation from the previous works including semi-supervised learning , and scalability . So , I 've raised my score to 6 . Thank you for the contributions !", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their insightful comments . - Contributions and note on other datasets : We have provided a clear set of contributions of this paper , as well as a note on datasets , as a standalone comment . We kindly ask the reviewer to please refer to that comment first . - Relation to semi-supervised approaches : We see iterated learning ( IL ) as more of a regularization technique than a semi-supervised learning method . IL in general can also be used in unsupervised settings . We happen to apply it in a semi-supervised learning setting for VQA , diverging from its previous scope of language emergence in the machine learning literature . That said , IL is indeed related to semi-supervised learning . Where semi-supervised learning deals with grounding a subset of its predictions in ground-truth labels , IL is supervised using a subset of the previous generation 's utterances . The crucial difference is that in IL , the supervision set evolves , changing the language acquired through the learning bottleneck in every new generation until convergence . - Learning bottleneck : In our work , the learning bottleneck primarily arises from limiting the length of the learning phase . In the learning phase , a new program generator ( PG ) is trained on the previous PG 's utterances for a limited number of gradient updates . This is related to early stopping , which is an effective strategy for regularization in machine learning . The central hypothesis for the success of this bottleneck is that structured language is easier to fit for neural networks ( [ 4 ] , [ 5 ] ) . Thus , easy-to-learn structure is acquired quickly by training the new PG for a limited number of steps . In contrast , idiomatic concepts that are harder to acquire ( requiring additional learning time or resources ) are not transmitted . Consequently , a student PG can learn a more structured language than its teacher . The repeated application of this bottleneck can push the language of programs towards one that is more structured . The ability to solve the task using programs exhibiting structure promotes systematic generalization . If the learning phase 's length is too small or too large , the effectiveness of the learning bottleneck can decrease due to underfitting or overfitting the teacher . We will clarify this intuition in the paper . - Specifics of architecture : Since our work 's focus is on the IL algorithm , much of our architectural setup is based on previous works ( [ 3 ] for Tensor-NMN , [ 2 ] for Vector-NMN and the PG ) , and we provide details for our Tensor-FiLM-NMN module architecture in Section 3.1 . We will expand Section 3.1 to explain the notations and operational details better , as well as add more details for reproducibility in the appendix . - Clarity of which parts are with/without IL : The details of which parts of the architecture are modified when running experiments with and without IL are specified in Section 3.2 . However , for clarity and ease of reference , we will add the algorithm ( abstract pseudocode format ) describing all the steps of our approach more precisely in the appendix . - Comparison to other methods and FiLM/MAC setting : Since this work aims to demonstrate the effectiveness of IL for encouraging systematic generalization in NMNs , the comparisons of interest are the ones between NMNs with IL and without IL , which we have presented in Section 5 . We present the results on FiLM and MAC , which achieve near-perfect accuracy on large datasets like CLEVR ( see [ 6 ] , [ 7 ] ) , not to claim state-of-the-art performance , but to provide a better understanding for readers as to where models without program supervision stand . We know from [ 1 ] and [ 2 ] that NMNs can systematically generalize better than models with generic deep architectures . However , there are stringent supervision requirements for this to happen in practice , which we address in this work . As with all our models , both FiLM and MAC are configured by finding the hyperparameters that perform the best on Val-IID . [ continued in the next comment ]"}, {"review_id": "Pd_oMxH8IlF-1", "review_text": "The authors apply iterated learning - a procedure originating in CogSci analyses of how human languages might develop - to the training of neural module networks . The goal is for iterated learning to encourage these networks to develop compositional structures that support systematic generalization without requiring explicit pressures for compositional structures ( in past work , such explicit pressures have generally been necessary ) . The proposed approach brings substantial improvements in systematic generalization across two datasets , SHAPES and CLEVR . Strengths : 1 . The approach is well-motivated , including impressive coverage of prior literature in both ML and CogSci . 2.The approach brings impressive gains in an area that is one of the major weaknesses of current ML systems , namely systematic generalization . 3.In addition to the gains in accuracy , one particularly impressive benefit of this approach is the decreased amount of supervision that it requires compared to past approaches . 4.The paper is generally well-written and easy to follow . Weaknesses : 1 . One of the motivations is to expand the use of iterated learning beyond toy datasets . While SHAPES and CLEVR may be not as toy-ish as datasets used in the past , they still are pretty toy-ish , so I \u2019 m not sure if this paper can reasonably claim that one of its contributions is to expand iterated learning to realistic domains . 2.Though the paper in general was very clear , I found Section 3.2 to be a bit hard to follow , and that section is important as it is the part that describes the structure of the iterated learning framing . I think this section would benefit from starting each subpart with a more high-level , intuitive description of what that stage accomplished , before diving into the details . Minor comments : 1 . Fodor et al only has 2 authors - Fodor and Pylyshyn 2 . Page 3 : \u201c Although , the Gumbel straight-through estimator \u201d : this use of \u201c although \u201d is usually frowned upon - better to use \u201c However \u201d 3 . Page 5 : typo : \u201c minimzing \u201d 4 . In general , for the bibliography , check to see if a paper has been published at a conference or journal ; if so , cite that version instead of the arXiv version . E.g. , \u201c Neural machine translation by jointly learning to align and translate. \u201d was published in ICLR 2015 , and \u201c Systematic generalization : what is required and can it be learned ? \u201d was published at ICLR 2019 .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for taking the time to read our paper and for their valuable suggestions . We have incorporated the reviewer \u2019 s minor comments and will make Section 3.2 easier to follow in the paper . While we agree that SHAPES is fairly toy-ish ( 244 unique questions in 12 question templates with 30x30 images laid out in a 3x3 grid ) , CLEVR is substantially more complex in comparison ( 700,000 unique questions in 90 question templates , with 3D-rendered 224x224 images ) . As a crude point of comparison , it takes about 30 minutes to train a SHAPES-SyGeT model to achieve decent generalization on a modern GPU , whereas it takes over two days to do the same on CLEVR . That said , CLEVR is indeed a synthetic dataset , although this has the advantage of allowing us to clearly demonstrate the systematic generalization gains of our method by removing other sources of error . As we mention in the separate standalone comment , we understand the need to test methods such as ours on realistic data and are thus experimenting with the GQA dataset . These results are not necessary for the goal of our paper , and we will be unable to explicitly evaluate systematic generalization as we do in the case of SHAPES-SyGeT and CLEVR/CLOSURE . Thus , we plan to include the validation performances in our appendix rather than the paper \u2019 s main body , which we will do before the camera-ready deadline . Due to the subjective definition of \u201c toy \u201d tasks , we can modify that claim in the paper to specify that we open up IL to broader machine learning applications beyond the previously-explored scope of language emergence and preservation ."}, {"review_id": "Pd_oMxH8IlF-2", "review_text": "This paper proposes to combine iterated learning ( the process of repeated language transmission from a \u2018 parent \u2019 agent to a \u2018 child \u2019 agent ) with neural module networks ( NMNs ) , in order to emerge NMN layouts that perform better at systematic generalization . The paper evaluates on their new variant of the SHAPES dataset that tests systematic generalization , and on CLEVR / CLOSURE , showing improved systematic generalization performance while requiring a small amount of ground-truth layout supervision . Pros : - I think the idea of combining IL with NMNs is really clever ( heh ) . Treating the program generator and execution engine as two agents that need to coordinate through a shared language ( NMN layouts ) is really interesting . If it were to work without layout supervision , it could open up new doors to applying IL + NMNs to many other tasks - The paper is quite well-written , and easy to follow - The related work section is thorough - The experiments on SHAPES-SyGeT , show that IL helps significantly for generalization of NMN models - I appreciate the ablations in the Appendix . Cons : - One of the main questions I have about this approach is whether it will provide any benefit on more complex problems ( e.g.large-scale VQA ) . There are a few reasons to think it might not be able to do so : 1 ) As alluded to in the paper , the IL procedure requires a lot of compute , which could be used to train larger models on more pre-training data 2 ) The improvement in validation performance on the more complex CLEVR dataset is fairly modest ( though the program accuracy increase is large ) . While CLEVR is more complex than SHAPES , it is still a fairly artificial dataset targeted \u2018 compositional \u2019 in nature , compared to general VQA . This suggests that it might be hard to get IL+NMN to work well on harder problems . 3 ) The method still requires ( a small amount of ) ground-truth layout supervision , which is not obtainable in general VQA or most other tasks . - I would also like to have seen a bit more analysis / description of why the method currently fails without any ground-truth layout supervision . I think this would improve the paper a lot , as it would help other researchers improve upon the method to address this problem . Overall : I think the ideas in this paper are interesting enough , and the execution good enough , to warrant acceptance . While I have some concerns about whether this approach will scale , these questions will have to be answered in subsequent works and with further research . Small typos : \u201c each new-born child need \u201d - > needs \u201c Recently machine learning community also show \u201d - > the machine learning community also shows \u201c Minimzing \u201d - > minimizing", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for taking the time to read our paper and for their comments . - Compute requirements of iterated learning ( IL ) : The compute required for IL depends on the compute required to train the program generator ( PG ) and the execution engine ( EE ) independently and the frequency of re-initialization of these components . For CLEVR , the EE takes about a day to achieve high validation performance on a modern GPU , even with the ground-truth layouts . Thus , re-initializing the EE for every iteration becomes computationally intractable . Fortunately , the seq2seq PG is quick to train , and thus not reinitializing the EE drastically reduces the computational overhead of IL . With this setup for CLEVR , we find little difference in the compute between IL and non-IL models . This is because IL model training is dominated by the interacting phase , while non-IL models are essentially trained through one long interacting phase . However , if the question and program languages are more complex and the PG is a larger model such as a transformer , re-training the PG can become a bottleneck . Making IL work with a harder-to-train PG without completely re-initializing it can be an interesting research direction . Seeded IL ( [ 1 ] ) is one possible strategy to avoid learning the PG from scratch at every iteration . Another idea could be using pruning or strong weight decay on the previous PG to produce an initialization point for the new PG . - Results for CLEVR/CLOSURE : As noted in a separate standalone comment , we have updated our CLEVR/CLOSURE results indicating stronger systematic generalization performance . Regarding in-distribution performance , the improvement on the CLEVR validation set is substantial for Tensor-NMN , which is the widely used NMN architecture from [ 2 ] . There is also a modest improvement in the case of Vector-NMN , but baseline performance on CLEVR validation here is already very high . - Artificial nature of SHAPES-SyGeT and CLEVR/CLOSURE : Please see the standalone comment . - Requirement of ground-truth programs : Our method 's requirement for ground-truth programs primarily arises from the optimization difficulties in jointly training the PG and the EE from scratch . The EE modules can specialize in performing specific roles only if the programs reuse the modules in appropriate ways . On the other hand , a good quality reinforcement signal from the EE to the PG relies on some amount of specialization of the modules . However , we note that only 100 programs is sufficient for us to achieve CLEVR validation performance close to that of previous NMN papers , which used 18000 ( [ 2 ] ) or 1000 ( [ 3 ] ) ground-truth programs . With a reduced need for supervision , it might become tractable for practitioners to achieve good systematic generalization in realistic VQA tasks by labeling programs for only a small set of questions . References : [ 1 ] Yuchen Lu , Soumye Singhal , Florian Strub , Olivier Pietquin , and Aaron Courville . Countering language drift with seeded iterated learning . arXiv preprint arXiv:2003.12694 , 2020 . [ 2 ] Justin Johnson , Bharath Hariharan , Laurens Van Der Maaten , Judy Hoffman , Li Fei-Fei , C Lawrence Zitnick , and Ross Girshick . Inferring and executing programs for visual reasoning . In Proceedings of the IEEE International Conference on Computer Vision , pp . 2989\u20132998 , 2017 . [ 3 ] Ramakrishna Vedantam , Karan Desai , Stefan Lee , Marcus Rohrbach , Dhruv Batra , and Devi Parikh . Probabilistic neural-symbolic models for interpretable visual question answering . arXiv preprint arXiv:1902.07864 , 2019 ."}], "0": {"review_id": "Pd_oMxH8IlF-0", "review_text": "Review : The authors address methods to encourage the emergence of the layout expression structures on the frameworks of neural module networks ( NMN ) for the visual QA problems . The methods are motivated from the works on language emergence for communication between multi-agents and the language acquisition of new-born babies from parents , which achieved with limited data . The methods , \u2018 iterative learning \u2019 ( IL ) are designed as forming two agents ( program generators and execution engines ) to play VQA games . Basic architectures and learning methods seem to be very similar to the approach of semi-supervised learning introduced in [ ICCV17 ] . This paper deals with one of very interesting topics , the language emergence among cooperative multi-agent environments and the compositionality of human language as recent related studies are well-surveyed in related work . In particular , the main idea of problem formation , layout expressions in NMN as emergent languages is very fresh and interesting . The main claims are as follows : ( 1 ) the proposed approach of IL improves generalization performance for visual QA , and it is shown experimentally by comparing the ablation results of IL . ( 2 ) the language structures in the ground-truth data are recovered with only limited supervisions and the superiority is validated on two datasets \u2013 SHAPES-SyGeT and CLOSURE . However , I believe that the evidences for their claims are insufficient . Specifically , the authors do not provide enough information of language structure such as the superiority compared to other methods and the structure similarity of recovery levels . I recommend 'ok , but not good enough \u2013 reject \u2019 for this paper . Pros : - The authors propose novel interesting problem and their solutions . Arguably , it seems potentially to be on one of important research flows to make influence to lots of works for academia in the future . - They find and report good performance for out-of-distribution accuracies for visual QA datasets . Concerns : - It is not clear which parts in the proposed methods are novel with respect to previous works . Those make vague which parts are the authors \u2019 contributions . - I think IL should be clarified from semi-supervised learning approaches on them for visual QA . Also , for reproducibility , it should be specified which parts are with/without IL . What is \u2018 learning bottleneck \u2019 on this approach ? Also , it is not enough how program generators and execution engines are specified , even though some explanations are in appendix . I think it needs more links for reference or explanation . - As mentioned above , the supports for main claims are not appropriate or unclear . It needs to theoretically or experimentally show the results of comparison with other methods and the similarity of the recovery level of language structures . - Table 1 reports the result of comparative methods such as MAC and FiLM without program supervision . How are they configured in the experiments ? - I think that it would be better understandable to show usability and superiority with the experimental results on realistic visual QA such as VQA and GQA . Minors : - In Section 3.1 , \u03b3 . - > \u03b3 , \u03b2.- > \u03b2 - It needs the description for operators and symbols for the formulae in Section 3.1 [ ICCV17 ] Johnson et al. , Inferring and executing programs for visual reasoning , ICCV 2017 . After rebuttal From the revised version of this manuscript , the authors resolve my major concerns such as clarity/reproducibility of the method , differentiation from the previous works including semi-supervised learning , and scalability . So , I 've raised my score to 6 . Thank you for the contributions !", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their insightful comments . - Contributions and note on other datasets : We have provided a clear set of contributions of this paper , as well as a note on datasets , as a standalone comment . We kindly ask the reviewer to please refer to that comment first . - Relation to semi-supervised approaches : We see iterated learning ( IL ) as more of a regularization technique than a semi-supervised learning method . IL in general can also be used in unsupervised settings . We happen to apply it in a semi-supervised learning setting for VQA , diverging from its previous scope of language emergence in the machine learning literature . That said , IL is indeed related to semi-supervised learning . Where semi-supervised learning deals with grounding a subset of its predictions in ground-truth labels , IL is supervised using a subset of the previous generation 's utterances . The crucial difference is that in IL , the supervision set evolves , changing the language acquired through the learning bottleneck in every new generation until convergence . - Learning bottleneck : In our work , the learning bottleneck primarily arises from limiting the length of the learning phase . In the learning phase , a new program generator ( PG ) is trained on the previous PG 's utterances for a limited number of gradient updates . This is related to early stopping , which is an effective strategy for regularization in machine learning . The central hypothesis for the success of this bottleneck is that structured language is easier to fit for neural networks ( [ 4 ] , [ 5 ] ) . Thus , easy-to-learn structure is acquired quickly by training the new PG for a limited number of steps . In contrast , idiomatic concepts that are harder to acquire ( requiring additional learning time or resources ) are not transmitted . Consequently , a student PG can learn a more structured language than its teacher . The repeated application of this bottleneck can push the language of programs towards one that is more structured . The ability to solve the task using programs exhibiting structure promotes systematic generalization . If the learning phase 's length is too small or too large , the effectiveness of the learning bottleneck can decrease due to underfitting or overfitting the teacher . We will clarify this intuition in the paper . - Specifics of architecture : Since our work 's focus is on the IL algorithm , much of our architectural setup is based on previous works ( [ 3 ] for Tensor-NMN , [ 2 ] for Vector-NMN and the PG ) , and we provide details for our Tensor-FiLM-NMN module architecture in Section 3.1 . We will expand Section 3.1 to explain the notations and operational details better , as well as add more details for reproducibility in the appendix . - Clarity of which parts are with/without IL : The details of which parts of the architecture are modified when running experiments with and without IL are specified in Section 3.2 . However , for clarity and ease of reference , we will add the algorithm ( abstract pseudocode format ) describing all the steps of our approach more precisely in the appendix . - Comparison to other methods and FiLM/MAC setting : Since this work aims to demonstrate the effectiveness of IL for encouraging systematic generalization in NMNs , the comparisons of interest are the ones between NMNs with IL and without IL , which we have presented in Section 5 . We present the results on FiLM and MAC , which achieve near-perfect accuracy on large datasets like CLEVR ( see [ 6 ] , [ 7 ] ) , not to claim state-of-the-art performance , but to provide a better understanding for readers as to where models without program supervision stand . We know from [ 1 ] and [ 2 ] that NMNs can systematically generalize better than models with generic deep architectures . However , there are stringent supervision requirements for this to happen in practice , which we address in this work . As with all our models , both FiLM and MAC are configured by finding the hyperparameters that perform the best on Val-IID . [ continued in the next comment ]"}, "1": {"review_id": "Pd_oMxH8IlF-1", "review_text": "The authors apply iterated learning - a procedure originating in CogSci analyses of how human languages might develop - to the training of neural module networks . The goal is for iterated learning to encourage these networks to develop compositional structures that support systematic generalization without requiring explicit pressures for compositional structures ( in past work , such explicit pressures have generally been necessary ) . The proposed approach brings substantial improvements in systematic generalization across two datasets , SHAPES and CLEVR . Strengths : 1 . The approach is well-motivated , including impressive coverage of prior literature in both ML and CogSci . 2.The approach brings impressive gains in an area that is one of the major weaknesses of current ML systems , namely systematic generalization . 3.In addition to the gains in accuracy , one particularly impressive benefit of this approach is the decreased amount of supervision that it requires compared to past approaches . 4.The paper is generally well-written and easy to follow . Weaknesses : 1 . One of the motivations is to expand the use of iterated learning beyond toy datasets . While SHAPES and CLEVR may be not as toy-ish as datasets used in the past , they still are pretty toy-ish , so I \u2019 m not sure if this paper can reasonably claim that one of its contributions is to expand iterated learning to realistic domains . 2.Though the paper in general was very clear , I found Section 3.2 to be a bit hard to follow , and that section is important as it is the part that describes the structure of the iterated learning framing . I think this section would benefit from starting each subpart with a more high-level , intuitive description of what that stage accomplished , before diving into the details . Minor comments : 1 . Fodor et al only has 2 authors - Fodor and Pylyshyn 2 . Page 3 : \u201c Although , the Gumbel straight-through estimator \u201d : this use of \u201c although \u201d is usually frowned upon - better to use \u201c However \u201d 3 . Page 5 : typo : \u201c minimzing \u201d 4 . In general , for the bibliography , check to see if a paper has been published at a conference or journal ; if so , cite that version instead of the arXiv version . E.g. , \u201c Neural machine translation by jointly learning to align and translate. \u201d was published in ICLR 2015 , and \u201c Systematic generalization : what is required and can it be learned ? \u201d was published at ICLR 2019 .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for taking the time to read our paper and for their valuable suggestions . We have incorporated the reviewer \u2019 s minor comments and will make Section 3.2 easier to follow in the paper . While we agree that SHAPES is fairly toy-ish ( 244 unique questions in 12 question templates with 30x30 images laid out in a 3x3 grid ) , CLEVR is substantially more complex in comparison ( 700,000 unique questions in 90 question templates , with 3D-rendered 224x224 images ) . As a crude point of comparison , it takes about 30 minutes to train a SHAPES-SyGeT model to achieve decent generalization on a modern GPU , whereas it takes over two days to do the same on CLEVR . That said , CLEVR is indeed a synthetic dataset , although this has the advantage of allowing us to clearly demonstrate the systematic generalization gains of our method by removing other sources of error . As we mention in the separate standalone comment , we understand the need to test methods such as ours on realistic data and are thus experimenting with the GQA dataset . These results are not necessary for the goal of our paper , and we will be unable to explicitly evaluate systematic generalization as we do in the case of SHAPES-SyGeT and CLEVR/CLOSURE . Thus , we plan to include the validation performances in our appendix rather than the paper \u2019 s main body , which we will do before the camera-ready deadline . Due to the subjective definition of \u201c toy \u201d tasks , we can modify that claim in the paper to specify that we open up IL to broader machine learning applications beyond the previously-explored scope of language emergence and preservation ."}, "2": {"review_id": "Pd_oMxH8IlF-2", "review_text": "This paper proposes to combine iterated learning ( the process of repeated language transmission from a \u2018 parent \u2019 agent to a \u2018 child \u2019 agent ) with neural module networks ( NMNs ) , in order to emerge NMN layouts that perform better at systematic generalization . The paper evaluates on their new variant of the SHAPES dataset that tests systematic generalization , and on CLEVR / CLOSURE , showing improved systematic generalization performance while requiring a small amount of ground-truth layout supervision . Pros : - I think the idea of combining IL with NMNs is really clever ( heh ) . Treating the program generator and execution engine as two agents that need to coordinate through a shared language ( NMN layouts ) is really interesting . If it were to work without layout supervision , it could open up new doors to applying IL + NMNs to many other tasks - The paper is quite well-written , and easy to follow - The related work section is thorough - The experiments on SHAPES-SyGeT , show that IL helps significantly for generalization of NMN models - I appreciate the ablations in the Appendix . Cons : - One of the main questions I have about this approach is whether it will provide any benefit on more complex problems ( e.g.large-scale VQA ) . There are a few reasons to think it might not be able to do so : 1 ) As alluded to in the paper , the IL procedure requires a lot of compute , which could be used to train larger models on more pre-training data 2 ) The improvement in validation performance on the more complex CLEVR dataset is fairly modest ( though the program accuracy increase is large ) . While CLEVR is more complex than SHAPES , it is still a fairly artificial dataset targeted \u2018 compositional \u2019 in nature , compared to general VQA . This suggests that it might be hard to get IL+NMN to work well on harder problems . 3 ) The method still requires ( a small amount of ) ground-truth layout supervision , which is not obtainable in general VQA or most other tasks . - I would also like to have seen a bit more analysis / description of why the method currently fails without any ground-truth layout supervision . I think this would improve the paper a lot , as it would help other researchers improve upon the method to address this problem . Overall : I think the ideas in this paper are interesting enough , and the execution good enough , to warrant acceptance . While I have some concerns about whether this approach will scale , these questions will have to be answered in subsequent works and with further research . Small typos : \u201c each new-born child need \u201d - > needs \u201c Recently machine learning community also show \u201d - > the machine learning community also shows \u201c Minimzing \u201d - > minimizing", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for taking the time to read our paper and for their comments . - Compute requirements of iterated learning ( IL ) : The compute required for IL depends on the compute required to train the program generator ( PG ) and the execution engine ( EE ) independently and the frequency of re-initialization of these components . For CLEVR , the EE takes about a day to achieve high validation performance on a modern GPU , even with the ground-truth layouts . Thus , re-initializing the EE for every iteration becomes computationally intractable . Fortunately , the seq2seq PG is quick to train , and thus not reinitializing the EE drastically reduces the computational overhead of IL . With this setup for CLEVR , we find little difference in the compute between IL and non-IL models . This is because IL model training is dominated by the interacting phase , while non-IL models are essentially trained through one long interacting phase . However , if the question and program languages are more complex and the PG is a larger model such as a transformer , re-training the PG can become a bottleneck . Making IL work with a harder-to-train PG without completely re-initializing it can be an interesting research direction . Seeded IL ( [ 1 ] ) is one possible strategy to avoid learning the PG from scratch at every iteration . Another idea could be using pruning or strong weight decay on the previous PG to produce an initialization point for the new PG . - Results for CLEVR/CLOSURE : As noted in a separate standalone comment , we have updated our CLEVR/CLOSURE results indicating stronger systematic generalization performance . Regarding in-distribution performance , the improvement on the CLEVR validation set is substantial for Tensor-NMN , which is the widely used NMN architecture from [ 2 ] . There is also a modest improvement in the case of Vector-NMN , but baseline performance on CLEVR validation here is already very high . - Artificial nature of SHAPES-SyGeT and CLEVR/CLOSURE : Please see the standalone comment . - Requirement of ground-truth programs : Our method 's requirement for ground-truth programs primarily arises from the optimization difficulties in jointly training the PG and the EE from scratch . The EE modules can specialize in performing specific roles only if the programs reuse the modules in appropriate ways . On the other hand , a good quality reinforcement signal from the EE to the PG relies on some amount of specialization of the modules . However , we note that only 100 programs is sufficient for us to achieve CLEVR validation performance close to that of previous NMN papers , which used 18000 ( [ 2 ] ) or 1000 ( [ 3 ] ) ground-truth programs . With a reduced need for supervision , it might become tractable for practitioners to achieve good systematic generalization in realistic VQA tasks by labeling programs for only a small set of questions . References : [ 1 ] Yuchen Lu , Soumye Singhal , Florian Strub , Olivier Pietquin , and Aaron Courville . Countering language drift with seeded iterated learning . arXiv preprint arXiv:2003.12694 , 2020 . [ 2 ] Justin Johnson , Bharath Hariharan , Laurens Van Der Maaten , Judy Hoffman , Li Fei-Fei , C Lawrence Zitnick , and Ross Girshick . Inferring and executing programs for visual reasoning . In Proceedings of the IEEE International Conference on Computer Vision , pp . 2989\u20132998 , 2017 . [ 3 ] Ramakrishna Vedantam , Karan Desai , Stefan Lee , Marcus Rohrbach , Dhruv Batra , and Devi Parikh . Probabilistic neural-symbolic models for interpretable visual question answering . arXiv preprint arXiv:1902.07864 , 2019 ."}}