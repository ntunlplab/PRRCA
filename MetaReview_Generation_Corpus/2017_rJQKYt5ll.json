{"year": "2017", "forum": "rJQKYt5ll", "title": "Steerable CNNs", "decision": "Accept (Poster)", "meta_review": "The AC fully agrees with reviewer #4 that the paper contains a bit of an overkill in formalism: A lot of maths whose justification is not, in the end, very clear. The paper probably has an important contribution, but the AC would suggest reorganizing and restructuring, lessening the excess in formalism. \n\nAs for the PCs, while we believe extending the experiments would further support the claims made in the paper, overall we still believe this paper deserves to appear at the conference as a poster.", "reviews": [{"review_id": "rJQKYt5ll-0", "review_text": "This paper presents a theoretical treatment of transformation groups applied to convnets, and presents some empirical results showing more efficient usage of network parameters. The basic idea of steerability makes huge sense and seems like a very important idea to develop. It is also a very old idea in image processing and goes back to Simoncelli, Freeman, Adelson, as well as Perona/Greenspan and others in the early 1990s. This paper approaches it through a formal treatment of group theory. But at the end of the day the idea seems pretty simple - the feature representation of a transformed image should be equivalent to a transformed feature representation of the original image. Given that the authors are limiting their analysis to discrete groups - for example rotations of 0, 90, 180, and 270 deg. - the formalities brought in from the group theoretic analysis seem a bit overkill. I'm not sure what this buys us in the end. it seems the real challenge lies in implementing continuous transformations, so if the theory could guide us in that direction it would be immensely helpful. Also the description of the experiments is fairly opaque. I would have a hard time replicating what exactly the authors did here in terms of implementing capsules or transformation groups. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and useful suggestions . We are glad to hear that you agree that steerability is a very important topic of research in deep learning . We agree that the pioneering work on steerable filters should be recognized , so we have added references to early works by Simoncelli , Freeman , Adelson , Perona and Greenspan . You are right that many of the ideas presented in the paper can be understood without higher mathematics . Still , we believe a mathematical treatment brings a lot of value , because it increases precision and generality , and establishes a bridge between fields . The value of generality is exemplified by the fact that the mathematical theory carries over almost without change to the continuous setting . By highlighting connections to advanced mathematical concepts and explaining them in a simple manner in the context of CNNs , we hope to foster future cross-fertilization between mathematics and machine learning . On the one hand , our work may provide an entry point for mathematicians and physicists who want to contribute to machine learning . On the other hand , machine learning researchers may benefit from the knowledge in the mathematical literature ( as we have ) , but this requires knowing where to look . For this reason it is useful to know , for instance , that this structure we 're dealing with in steerable CNNs is known as the `` induced representation '' by mathematicians , even if you were not familiar with the concept before , and could understand the basic idea without knowing its mathematical name . Regarding the description of the experiments : we will be releasing our code , which should answer any potential question about the experiments . In the mean time , if there are specific things you think are missing , we will add them ."}, {"review_id": "rJQKYt5ll-1", "review_text": "The authors propose a parameterization of CNNs that guarantees equivariance wrt a large family of geometric transformations. The mathematical analysis is rigorous and the material is very interesting and novel. The paper overall reads well; there is a real effort to explain the math accessibly, though some small improvements could be made. The theory is general enough to include continuous transformations, although the experiments are restricted to discrete ones. While this could be seen as a negative point, it is justified by the experiments, which show that this set of transformations is powerful enough to yield very good results on CIFAR. Another form of intertwiner has been studied recently by Lenc & Vedaldi [1]; they have studied equivariance empirically in CNNs, which offers an orthogonal view. In addition to the recent references on scale/rotation deep networks suggested below, geometric equivariance has been studied extensively in the 2000's; mentioning at least one work would be appropriate. The one that probably comes closest to the proposed method is the work by Reisert [2], who studied steerable filters for invariance and equivariance, using Lie group theory. The difference, of course, is that the focus at the time was on kernel machines rather than CNNs, but many of the tools and theorems are relatable. Some of the notation could be simplified, to make the formulas easier to grasp on a first read: Working over a lattice Z^d is unnecessarily abstract -- since the inputs are always images, Z^2 would make much of the later math easier to parse. Generalization is straightforward, so I don't think the results lose anything by it; and the authors go back to 2D latices later anyway. It could be more natural to do away with the layer index l which appears throughout the paper, and have notation for current/next layer instead (e.g. pi and pi'; K and D instead of K_{l+1} and K_l). In any case I leave it up to the authors to decide whether to include these suggestions on notation, but I urge them to consider them (or other ways to unburden notation). A few minor issues: Some statements would be better supported with an accompanying reference (e.g. \"Explicit formulas exist\" on page 5, the introduction of intertwiners on page 3). Finally, there is a tiny mistake in the Balduzzi & Ghifary reference (some extra information was included as an author name). [1] Lenc & Vedaldi, \"Understanding image representations by measuring their equivariance and equivalence\", 2015 [2] Reisert, \"Group integration techniques in pattern analysis: a kernel view\", 2008 ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and helpful suggestions . We have incorporated the proposed notational changes : we changed Z^d to Z^2 and now use a prime instead of layer index . We believe that this does indeed make the paper quite a bit easier to read . Other changes : - We have added several references to kernel-era work on equivariance . - Added a reference to Lenc & Vedaldi . - Fixed Balduzzi & Ghifary reference Thanks again for carefully reading our manuscript , and the constructive feedback ."}, {"review_id": "rJQKYt5ll-2", "review_text": "This paper essentially presents a new inductive bias in the architecture of (convolutional) neural networks (CNN). The mathematical motivations/derivations of the proposed architecture are detailed and rigorous. The proposed architecture promises to produce equivariant representations with steerable features using fewer parameters than traditional CNNs, which is particularly useful in small data regimes. Interesting and novel connections are presented between steerable filters and so called \u201csteerable fibers\u201d. The architecture is strongly inspired by the author\u2019s previous work, as well as that of \u201ccapsules\u201d (Hinton, 2011). The proposed architecture is compared on CIFAR10 against state-of-the-art inspired architectures (ResNets), and is shown to be superior particularly in the small data regime. The lack of empirical comparison on large scale dataset, such as ImageNet or COCO makes this largely a theoretical contribution. I would have also liked to see more empirical evaluation of the equivariance properties. It is not intuitively clear exactly why this architecture performs better on CIFAR10 as it is not clear that capturing equivariances helps to classify different instances of object categories. Wouldn\u2019t action-recognition in videos, for example, not be a better illustrative dataset? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review . While state of the art results on CIFAR should not be undervalued ( it is the most competitive dataset for comparison of CNN architectures ) , we agree that more large-scale validation would have been nice . We also believe that , as you say , the method may perform even better on problems where geometry plays a dominant role ( action recognition , motion estimation , continuous control , etc . ) and tasks where there is a clear symmetry ( such as astrophysical data , histopathology slides , and so on ) . CIFAR has approximate rotation symmetry at small scales , but lacks full global symmetry . We see the fact that our method works very well despite this as an encouraging signal ."}], "0": {"review_id": "rJQKYt5ll-0", "review_text": "This paper presents a theoretical treatment of transformation groups applied to convnets, and presents some empirical results showing more efficient usage of network parameters. The basic idea of steerability makes huge sense and seems like a very important idea to develop. It is also a very old idea in image processing and goes back to Simoncelli, Freeman, Adelson, as well as Perona/Greenspan and others in the early 1990s. This paper approaches it through a formal treatment of group theory. But at the end of the day the idea seems pretty simple - the feature representation of a transformed image should be equivalent to a transformed feature representation of the original image. Given that the authors are limiting their analysis to discrete groups - for example rotations of 0, 90, 180, and 270 deg. - the formalities brought in from the group theoretic analysis seem a bit overkill. I'm not sure what this buys us in the end. it seems the real challenge lies in implementing continuous transformations, so if the theory could guide us in that direction it would be immensely helpful. Also the description of the experiments is fairly opaque. I would have a hard time replicating what exactly the authors did here in terms of implementing capsules or transformation groups. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and useful suggestions . We are glad to hear that you agree that steerability is a very important topic of research in deep learning . We agree that the pioneering work on steerable filters should be recognized , so we have added references to early works by Simoncelli , Freeman , Adelson , Perona and Greenspan . You are right that many of the ideas presented in the paper can be understood without higher mathematics . Still , we believe a mathematical treatment brings a lot of value , because it increases precision and generality , and establishes a bridge between fields . The value of generality is exemplified by the fact that the mathematical theory carries over almost without change to the continuous setting . By highlighting connections to advanced mathematical concepts and explaining them in a simple manner in the context of CNNs , we hope to foster future cross-fertilization between mathematics and machine learning . On the one hand , our work may provide an entry point for mathematicians and physicists who want to contribute to machine learning . On the other hand , machine learning researchers may benefit from the knowledge in the mathematical literature ( as we have ) , but this requires knowing where to look . For this reason it is useful to know , for instance , that this structure we 're dealing with in steerable CNNs is known as the `` induced representation '' by mathematicians , even if you were not familiar with the concept before , and could understand the basic idea without knowing its mathematical name . Regarding the description of the experiments : we will be releasing our code , which should answer any potential question about the experiments . In the mean time , if there are specific things you think are missing , we will add them ."}, "1": {"review_id": "rJQKYt5ll-1", "review_text": "The authors propose a parameterization of CNNs that guarantees equivariance wrt a large family of geometric transformations. The mathematical analysis is rigorous and the material is very interesting and novel. The paper overall reads well; there is a real effort to explain the math accessibly, though some small improvements could be made. The theory is general enough to include continuous transformations, although the experiments are restricted to discrete ones. While this could be seen as a negative point, it is justified by the experiments, which show that this set of transformations is powerful enough to yield very good results on CIFAR. Another form of intertwiner has been studied recently by Lenc & Vedaldi [1]; they have studied equivariance empirically in CNNs, which offers an orthogonal view. In addition to the recent references on scale/rotation deep networks suggested below, geometric equivariance has been studied extensively in the 2000's; mentioning at least one work would be appropriate. The one that probably comes closest to the proposed method is the work by Reisert [2], who studied steerable filters for invariance and equivariance, using Lie group theory. The difference, of course, is that the focus at the time was on kernel machines rather than CNNs, but many of the tools and theorems are relatable. Some of the notation could be simplified, to make the formulas easier to grasp on a first read: Working over a lattice Z^d is unnecessarily abstract -- since the inputs are always images, Z^2 would make much of the later math easier to parse. Generalization is straightforward, so I don't think the results lose anything by it; and the authors go back to 2D latices later anyway. It could be more natural to do away with the layer index l which appears throughout the paper, and have notation for current/next layer instead (e.g. pi and pi'; K and D instead of K_{l+1} and K_l). In any case I leave it up to the authors to decide whether to include these suggestions on notation, but I urge them to consider them (or other ways to unburden notation). A few minor issues: Some statements would be better supported with an accompanying reference (e.g. \"Explicit formulas exist\" on page 5, the introduction of intertwiners on page 3). Finally, there is a tiny mistake in the Balduzzi & Ghifary reference (some extra information was included as an author name). [1] Lenc & Vedaldi, \"Understanding image representations by measuring their equivariance and equivalence\", 2015 [2] Reisert, \"Group integration techniques in pattern analysis: a kernel view\", 2008 ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review and helpful suggestions . We have incorporated the proposed notational changes : we changed Z^d to Z^2 and now use a prime instead of layer index . We believe that this does indeed make the paper quite a bit easier to read . Other changes : - We have added several references to kernel-era work on equivariance . - Added a reference to Lenc & Vedaldi . - Fixed Balduzzi & Ghifary reference Thanks again for carefully reading our manuscript , and the constructive feedback ."}, "2": {"review_id": "rJQKYt5ll-2", "review_text": "This paper essentially presents a new inductive bias in the architecture of (convolutional) neural networks (CNN). The mathematical motivations/derivations of the proposed architecture are detailed and rigorous. The proposed architecture promises to produce equivariant representations with steerable features using fewer parameters than traditional CNNs, which is particularly useful in small data regimes. Interesting and novel connections are presented between steerable filters and so called \u201csteerable fibers\u201d. The architecture is strongly inspired by the author\u2019s previous work, as well as that of \u201ccapsules\u201d (Hinton, 2011). The proposed architecture is compared on CIFAR10 against state-of-the-art inspired architectures (ResNets), and is shown to be superior particularly in the small data regime. The lack of empirical comparison on large scale dataset, such as ImageNet or COCO makes this largely a theoretical contribution. I would have also liked to see more empirical evaluation of the equivariance properties. It is not intuitively clear exactly why this architecture performs better on CIFAR10 as it is not clear that capturing equivariances helps to classify different instances of object categories. Wouldn\u2019t action-recognition in videos, for example, not be a better illustrative dataset? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review . While state of the art results on CIFAR should not be undervalued ( it is the most competitive dataset for comparison of CNN architectures ) , we agree that more large-scale validation would have been nice . We also believe that , as you say , the method may perform even better on problems where geometry plays a dominant role ( action recognition , motion estimation , continuous control , etc . ) and tasks where there is a clear symmetry ( such as astrophysical data , histopathology slides , and so on ) . CIFAR has approximate rotation symmetry at small scales , but lacks full global symmetry . We see the fact that our method works very well despite this as an encouraging signal ."}}