{"year": "2019", "forum": "B1lnzn0ctQ", "title": "ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA", "decision": "Accept (Poster)", "meta_review": "This is a well executed paper that makes clear contributions to the understanding of unrolled iterative optimization and soft thresholding for sparse signal recovery with neural networks.", "reviews": [{"review_id": "B1lnzn0ctQ-0", "review_text": "The paper describes ALISTA, a version of LISTA that uses the dictionary only for one of its roles (synthesis) in ISTA and learns a matrix to play the other role (analysis), as seen in equations (3) and (6). The number of matrices to learn is reduced by tying the different layers of LISTA together. The motivation for this paper is a little confusing. ISTA, FISTA, etc. are algorithms for sparse recovery that do not require training. LISTA modified ISTA to allow for training of the \"dictionary matrix\" used in each iteration of ISTA, assuming that it is unknown, and offering a deep-learning-based alternative to dictionary learning. ALISTA shows that the dictionary does not need to change, and fewer parameters are used than in LISTA, but it still requires learning matrices of the same dimensionality as LISTA (i.e., the reduction is in the constant, not the order). If the argument that fewer parameters are needed is impactful, then the paper should discuss the computational complexity (and computing times) for training ALISTA vs. the competing approaches. There are approaches to sparse modeling that assume separate analysis and synthesis dictionaries (e.g., Rubinstein and Elad, \"Dictionary Learning for Analysis-Synthesis Thresholding\"). A discussion of these would be relevant in this paper. * The intuition and feasibility of identifying \"good\" matrices (Defs. 1 and 2) should be detailed. For example, how do we know that an arbitrary starting W belongs in the set (12) so that (14) applies? * Can you comment on the difference between the maximum entry \"norm\" used in Def. 1 and the Frobenius norm used in (17)? * Definition 3: No dependence on theta(k) appears in (13), thus it is not clear how \"as long as theta(k) is large enough\" is obtained. * How is gamma learned (Section 2.3)? * The notation in Section 3 is a bit confusing - lowercase letters b, d, x refer to matrices instead of vectors. In (20), Dconv,m(.) is undefined; later Wconv is undefined. * For the convolutional formulation of Section 3, it is not clear why some transposes from (6) disappear in (21). * In Section 3.1, \"an efficient approximated way\" is an incomplete sentence - perhaps you mean \"an efficient approximation\"?. Before (25), Dconv should be Dcir? The dependence on d should be more explicitly stated. * Page 8 typo \"Figure 1 (a) (a)\". * Figure 2(a): the legend is better used as the label for the y axis. * I do not think Figure 2(b) verifies Theorem 1; rather, it verifies that your learning scheme gives parameter values that allow for Theorem 1 to apply (which is true by design). * Figure 3: isn't it easier to use metrics from support detection (false alarm/missed detection proportions given by the ALISTA output)?", "rating": "7: Good paper, accept", "reply_text": "Answers to individual comments : - Q1 ( Intuition and feasibility of identifying `` good '' matrices ; Definition 1 ) : Definition 1 describes a property of good matrices : small coherence with respect to D. This is inspired by Donoho & Elad , 2003 ; Elad , 2007 ; Lu et al , 2018 . Our Theorem 1 validates this point : a small mutual coherence leads to a large c and faster convergence . Feasibility is proved in ( Chen et al. , 2018 ) . We have added these clarifications in our update . - Q1 ( Clarification of Definition 2 ) : Because W and D are both \u201c fat \u201d matrices , the product W \u2019 D , and such products of their submatrices consisting of two or more their corresponding columns , generally can not be very close to the identity matrix . For a given D , Definition 2 let sigma_min represent the minimal \u201c distance \u201d and define the set of corresponding W matrices . A larger sigma_min implies slower convergence in Theorem 2 . We have added numerical validations of ( 11 ) to the appendix in the update . ( The original definition ( 12 ) is ( 11 ) in the updated version . ) - Q2 ( Difference between the maximum entry `` norm '' and the Frobenius norm ) : We use a Frobenius norm in ( 16 ) instead of a sup-norm in Def . 1 ( 8 ) for computational efficiency . Directly minimizing the sup norm leads to a large-scale linear program . The sizes of the matrices W and D that we used in our numerical experiments are 250 by 500 . We implemented an LP solver for the sup-norm minimization ( 8 ) based on Gurobi , which requires more than 8GB of memory and may be intractable on a typical PC . However , solving ( 16 ) in MATLAB needs only around 10MB of memory and a few seconds . Besides the Frobenius norm , we also tried to minimize the L_ { 1,1 } norm but found no advantages . ( The original formula ( 17 ) is ( 16 ) in the update . ) - Q3 ( Definition 3 ) : By ( 6 ) , x^k depends on thresholding parameters theta^0 , theta^1 , ... , theta^ { k-1 } . When these theta parameters are large enough , x^k can be sufficiently sparse . Theorem 1 implies we can ensure \u201c support ( x^k ) belongs to S \u201d for all k by properly choosing the theta^k sequence . - Q4 ( How is gamma learned ) : The step sizes gamma^k and thresholds theta^k ( for all k ) are updated to minimize the empirical recovery loss in ( 5 ) , using the standard training method based on backpropagation and the Adam method . For ALISTA , the big Theta in ( 5 ) , which is the set of parameters subject to learning , consists of only gammas and thetas . The matrix W is pre-computed by analytic optimization and , therefore , is fixed during training . - Q5 ( The notation in Section 3 ) : The lowercase letters are always vectors . The matrices D_ { conv , m } are defined so that ( 18 ) , which is precise but complicated , is equivalent to ( 19 ) , which is simple and compact . The full definition of D_ { conv , m } is given in Appendix C.2 . The matrices W_ { conv , m } are defined for a similar purpose before ( 21 ) . We have added these clarifications in the updated version . ( The original formula ( 20 ) is ( 19 ) in the current version . ) - Q6 ( Transpose in convolution ) : Transposing a circulant matrix is equivalent to applying the convolution with rotated filters ( Equation ( 6 ) and Footnote 2 in Chalasani et al. , 2013 ) . We have made clarifications in the update . - Q7 & Q8 & Q9 ( Typos and figure suggestions ) : Thanks for finding the typos and making suggestions for figures . We have fixed the typos and will carefully proofread our paper . - Q10 ( \u201c I do not think Figure 2 ( b ) verifies Theorem 1 \u201d ) : We agree that we incorrectly used the words `` verify '' and `` validation . '' Rather , the numerical observations in Figure 2 ( b ) justify our choices of parameters in Theorem 1 . We have made this correction . - Q11 ( Figure 3 ) : We agree that the number and proportion of false alarms are a more straightforward performance metric . However , they are sensitive to the threshold . We found that , although using a smaller threshold leads to more false alarms , the final recovery quality is better and those false alarms have small magnitudes and are easy to remove by thresholding during post-processing . That 's why we chose to show their magnitudes , implying that we get easy-to-remove false alarms . We have added this reasoning to the final version ."}, {"review_id": "B1lnzn0ctQ-1", "review_text": "The papers studies neural network-based sparse signal recovery, and derives many new theoretical insights into the classical LISTA model. The authors proposed Analytic LISTA (ALISTA), where the weight matrix in LISTA is pre-computed with a data-free coherence minimization, followed by a separate data-driven learning step for merely (a very small number of) step-size and threshold parameters. Their theory is extensible to convolutional cases. The two-stage decomposed pipeline was shown to keep the optimal linear convergence proved in (Chen et al., 2018). Experiments observe that ALISTA has almost no performance loss compared to the much heavier parameterized LISTA, in contrast to the common wisdom that (brutal-force) \u201cend-to-end\u201d always outperforms stage-wise training. Their contributions thus manifest in both novel theory results, and the practical impacts of simplifying/accelerating LISTA training. Besides, they also proposed an interesting new strategy called Robust ALISTA to overcome the small perturbations on the encoding basis, which also benefits from this decomposed problems structure. The proofs and conclusions are mathematically correct to my best knowledge. I personally worked on similar sparse unfolding problems before so this work looks particularly novel and interesting to me. My intuition then was that, it should not be really necessary to use heavily parameterized networks to approximate a simple linear sparse coding form (LISTA idea). Similar accelerations could have been achieved with line search for something similar to steepest descent (also computational expensive, but need learn step-sizes only, and agnostic to input distribution). Correspondingly, there should exist a more elegant network solution with very light learnable weights. This work perfectly coincides with the intuition, providing very solid guidance on how a LISTA model could be built right. Given in recent three years, many application works rely on unfold-truncating techniques (compressive sensing, reconstruction, super resolution, image restoration, clustering\u2026), I envision this paper to generate important impacts for practitioners pursuing those ideas. Additionally, I like Theorem 3 in Section 3.1, on the provable efficient approximation of general convolution using circular convolution. It could be useful for many other problems such as filter response matching. I therefore hold a very positive attitude towards this paper and support for its acceptance. Some questions I would like the authors to clarify & improve in revision: 1. Eqn (7) assumes noise-free case. The author stated \u201cThe zero-noise assumption is for simplicity of the proofs.\u201d Could the authors elaborate which part of current theory/proof will fail in noisy case? If so, can it be overcome (even by less \u201csimpler\u201d way)? How about convolutional case, the same? Could the authors at least provide some empirical results for ALISTA\u2019s performance under noise? 2. Section 5.3. It is unclear to me why Robust ALISTA has to work better than the data augmented ALISTA. Is it potentially because that in the data augmentation baseline, the training data volume is much amplified, and one ALISTA model might become underfitting? It would be interesting to create a larger-capacity ALISTA model (e.g., by increasing unfolded layer numbers), train it on the augmented data, and see if it can compare more favorably against Robust ALISTA? 3. The writeup is overall very good, mature, and easy to follow. But still, typos occur from time to time, showing a bit rush. For example, Section 5.1, \u201cthe x-axes denotes is the indices of layers\u201d should remove \u201cis\u201d. Please make sure more proofreading will be done. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for your careful reading and comments ! - Q1 : In our proofs , we take b as b=Ax * . If we add noise to the measurements , almost all the inequalities in the proof need to be modified . We will end up getting \u201c convergence \u201d to a neighbor of x * with a size depending on the noise level . Such modifications also apply to the analysis for convolutional dictionaries . Numerically , figures 1 ( b ) , 1 ( c ) and 1 ( d ) depict the results of ALISTA under SNRs = 40dB , 30dB and 20dB , respectively . - Q2 : We basically agree with your comment on why data augmented TiLISTA and ALISTA are not performing as well as robust ALISTA . We are conducting the experiments that you have suggested and will update the results in comments once they become available , and also add them to the paper \u2019 s next update . - Q3 : Thanks for kindly pointing out our writing issues . We will carefully fix typos and use more proofreading ."}, {"review_id": "B1lnzn0ctQ-2", "review_text": "The paper raises many important questions about unrolled iterative optimization algorithms, and answers many questions for the case of iterative soft thresholding algorithm (ISTA, and learned variant LISTA). The authors demonstrate that a major simplification is available for the learned network: instead of learning a matrix for each layer, or even a single (potentially large) matrix, one may obtain the matrix analytically and learn only a series of scalars. These simplifications are not only practically useful but allow for theoretical analysis in the context of optimization theory. On top of this seminal contribution, the results are extended to the convolutional-LISTA setting. Finally, yet another fascinating result is presented, namely that the analytic weights can be determined from a Gaussian-perturbed version of the dictionary. Experimental validation of all results is presented. My only constructive criticism of this paper are a few grammatical typos, but specifically the 2nd to last sentence before Sec 2.1 states the wrong thing \"In this way, the LISTA model could be further significantly simplified, without little performance loss\" ... it should be \"with little\". ", "rating": "10: Top 5% of accepted papers, seminal paper", "reply_text": "Thank you for your careful reading and kindly identifying the typos in our paper ! We will fix these typos and meticulously proofread our article ."}], "0": {"review_id": "B1lnzn0ctQ-0", "review_text": "The paper describes ALISTA, a version of LISTA that uses the dictionary only for one of its roles (synthesis) in ISTA and learns a matrix to play the other role (analysis), as seen in equations (3) and (6). The number of matrices to learn is reduced by tying the different layers of LISTA together. The motivation for this paper is a little confusing. ISTA, FISTA, etc. are algorithms for sparse recovery that do not require training. LISTA modified ISTA to allow for training of the \"dictionary matrix\" used in each iteration of ISTA, assuming that it is unknown, and offering a deep-learning-based alternative to dictionary learning. ALISTA shows that the dictionary does not need to change, and fewer parameters are used than in LISTA, but it still requires learning matrices of the same dimensionality as LISTA (i.e., the reduction is in the constant, not the order). If the argument that fewer parameters are needed is impactful, then the paper should discuss the computational complexity (and computing times) for training ALISTA vs. the competing approaches. There are approaches to sparse modeling that assume separate analysis and synthesis dictionaries (e.g., Rubinstein and Elad, \"Dictionary Learning for Analysis-Synthesis Thresholding\"). A discussion of these would be relevant in this paper. * The intuition and feasibility of identifying \"good\" matrices (Defs. 1 and 2) should be detailed. For example, how do we know that an arbitrary starting W belongs in the set (12) so that (14) applies? * Can you comment on the difference between the maximum entry \"norm\" used in Def. 1 and the Frobenius norm used in (17)? * Definition 3: No dependence on theta(k) appears in (13), thus it is not clear how \"as long as theta(k) is large enough\" is obtained. * How is gamma learned (Section 2.3)? * The notation in Section 3 is a bit confusing - lowercase letters b, d, x refer to matrices instead of vectors. In (20), Dconv,m(.) is undefined; later Wconv is undefined. * For the convolutional formulation of Section 3, it is not clear why some transposes from (6) disappear in (21). * In Section 3.1, \"an efficient approximated way\" is an incomplete sentence - perhaps you mean \"an efficient approximation\"?. Before (25), Dconv should be Dcir? The dependence on d should be more explicitly stated. * Page 8 typo \"Figure 1 (a) (a)\". * Figure 2(a): the legend is better used as the label for the y axis. * I do not think Figure 2(b) verifies Theorem 1; rather, it verifies that your learning scheme gives parameter values that allow for Theorem 1 to apply (which is true by design). * Figure 3: isn't it easier to use metrics from support detection (false alarm/missed detection proportions given by the ALISTA output)?", "rating": "7: Good paper, accept", "reply_text": "Answers to individual comments : - Q1 ( Intuition and feasibility of identifying `` good '' matrices ; Definition 1 ) : Definition 1 describes a property of good matrices : small coherence with respect to D. This is inspired by Donoho & Elad , 2003 ; Elad , 2007 ; Lu et al , 2018 . Our Theorem 1 validates this point : a small mutual coherence leads to a large c and faster convergence . Feasibility is proved in ( Chen et al. , 2018 ) . We have added these clarifications in our update . - Q1 ( Clarification of Definition 2 ) : Because W and D are both \u201c fat \u201d matrices , the product W \u2019 D , and such products of their submatrices consisting of two or more their corresponding columns , generally can not be very close to the identity matrix . For a given D , Definition 2 let sigma_min represent the minimal \u201c distance \u201d and define the set of corresponding W matrices . A larger sigma_min implies slower convergence in Theorem 2 . We have added numerical validations of ( 11 ) to the appendix in the update . ( The original definition ( 12 ) is ( 11 ) in the updated version . ) - Q2 ( Difference between the maximum entry `` norm '' and the Frobenius norm ) : We use a Frobenius norm in ( 16 ) instead of a sup-norm in Def . 1 ( 8 ) for computational efficiency . Directly minimizing the sup norm leads to a large-scale linear program . The sizes of the matrices W and D that we used in our numerical experiments are 250 by 500 . We implemented an LP solver for the sup-norm minimization ( 8 ) based on Gurobi , which requires more than 8GB of memory and may be intractable on a typical PC . However , solving ( 16 ) in MATLAB needs only around 10MB of memory and a few seconds . Besides the Frobenius norm , we also tried to minimize the L_ { 1,1 } norm but found no advantages . ( The original formula ( 17 ) is ( 16 ) in the update . ) - Q3 ( Definition 3 ) : By ( 6 ) , x^k depends on thresholding parameters theta^0 , theta^1 , ... , theta^ { k-1 } . When these theta parameters are large enough , x^k can be sufficiently sparse . Theorem 1 implies we can ensure \u201c support ( x^k ) belongs to S \u201d for all k by properly choosing the theta^k sequence . - Q4 ( How is gamma learned ) : The step sizes gamma^k and thresholds theta^k ( for all k ) are updated to minimize the empirical recovery loss in ( 5 ) , using the standard training method based on backpropagation and the Adam method . For ALISTA , the big Theta in ( 5 ) , which is the set of parameters subject to learning , consists of only gammas and thetas . The matrix W is pre-computed by analytic optimization and , therefore , is fixed during training . - Q5 ( The notation in Section 3 ) : The lowercase letters are always vectors . The matrices D_ { conv , m } are defined so that ( 18 ) , which is precise but complicated , is equivalent to ( 19 ) , which is simple and compact . The full definition of D_ { conv , m } is given in Appendix C.2 . The matrices W_ { conv , m } are defined for a similar purpose before ( 21 ) . We have added these clarifications in the updated version . ( The original formula ( 20 ) is ( 19 ) in the current version . ) - Q6 ( Transpose in convolution ) : Transposing a circulant matrix is equivalent to applying the convolution with rotated filters ( Equation ( 6 ) and Footnote 2 in Chalasani et al. , 2013 ) . We have made clarifications in the update . - Q7 & Q8 & Q9 ( Typos and figure suggestions ) : Thanks for finding the typos and making suggestions for figures . We have fixed the typos and will carefully proofread our paper . - Q10 ( \u201c I do not think Figure 2 ( b ) verifies Theorem 1 \u201d ) : We agree that we incorrectly used the words `` verify '' and `` validation . '' Rather , the numerical observations in Figure 2 ( b ) justify our choices of parameters in Theorem 1 . We have made this correction . - Q11 ( Figure 3 ) : We agree that the number and proportion of false alarms are a more straightforward performance metric . However , they are sensitive to the threshold . We found that , although using a smaller threshold leads to more false alarms , the final recovery quality is better and those false alarms have small magnitudes and are easy to remove by thresholding during post-processing . That 's why we chose to show their magnitudes , implying that we get easy-to-remove false alarms . We have added this reasoning to the final version ."}, "1": {"review_id": "B1lnzn0ctQ-1", "review_text": "The papers studies neural network-based sparse signal recovery, and derives many new theoretical insights into the classical LISTA model. The authors proposed Analytic LISTA (ALISTA), where the weight matrix in LISTA is pre-computed with a data-free coherence minimization, followed by a separate data-driven learning step for merely (a very small number of) step-size and threshold parameters. Their theory is extensible to convolutional cases. The two-stage decomposed pipeline was shown to keep the optimal linear convergence proved in (Chen et al., 2018). Experiments observe that ALISTA has almost no performance loss compared to the much heavier parameterized LISTA, in contrast to the common wisdom that (brutal-force) \u201cend-to-end\u201d always outperforms stage-wise training. Their contributions thus manifest in both novel theory results, and the practical impacts of simplifying/accelerating LISTA training. Besides, they also proposed an interesting new strategy called Robust ALISTA to overcome the small perturbations on the encoding basis, which also benefits from this decomposed problems structure. The proofs and conclusions are mathematically correct to my best knowledge. I personally worked on similar sparse unfolding problems before so this work looks particularly novel and interesting to me. My intuition then was that, it should not be really necessary to use heavily parameterized networks to approximate a simple linear sparse coding form (LISTA idea). Similar accelerations could have been achieved with line search for something similar to steepest descent (also computational expensive, but need learn step-sizes only, and agnostic to input distribution). Correspondingly, there should exist a more elegant network solution with very light learnable weights. This work perfectly coincides with the intuition, providing very solid guidance on how a LISTA model could be built right. Given in recent three years, many application works rely on unfold-truncating techniques (compressive sensing, reconstruction, super resolution, image restoration, clustering\u2026), I envision this paper to generate important impacts for practitioners pursuing those ideas. Additionally, I like Theorem 3 in Section 3.1, on the provable efficient approximation of general convolution using circular convolution. It could be useful for many other problems such as filter response matching. I therefore hold a very positive attitude towards this paper and support for its acceptance. Some questions I would like the authors to clarify & improve in revision: 1. Eqn (7) assumes noise-free case. The author stated \u201cThe zero-noise assumption is for simplicity of the proofs.\u201d Could the authors elaborate which part of current theory/proof will fail in noisy case? If so, can it be overcome (even by less \u201csimpler\u201d way)? How about convolutional case, the same? Could the authors at least provide some empirical results for ALISTA\u2019s performance under noise? 2. Section 5.3. It is unclear to me why Robust ALISTA has to work better than the data augmented ALISTA. Is it potentially because that in the data augmentation baseline, the training data volume is much amplified, and one ALISTA model might become underfitting? It would be interesting to create a larger-capacity ALISTA model (e.g., by increasing unfolded layer numbers), train it on the augmented data, and see if it can compare more favorably against Robust ALISTA? 3. The writeup is overall very good, mature, and easy to follow. But still, typos occur from time to time, showing a bit rush. For example, Section 5.1, \u201cthe x-axes denotes is the indices of layers\u201d should remove \u201cis\u201d. Please make sure more proofreading will be done. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for your careful reading and comments ! - Q1 : In our proofs , we take b as b=Ax * . If we add noise to the measurements , almost all the inequalities in the proof need to be modified . We will end up getting \u201c convergence \u201d to a neighbor of x * with a size depending on the noise level . Such modifications also apply to the analysis for convolutional dictionaries . Numerically , figures 1 ( b ) , 1 ( c ) and 1 ( d ) depict the results of ALISTA under SNRs = 40dB , 30dB and 20dB , respectively . - Q2 : We basically agree with your comment on why data augmented TiLISTA and ALISTA are not performing as well as robust ALISTA . We are conducting the experiments that you have suggested and will update the results in comments once they become available , and also add them to the paper \u2019 s next update . - Q3 : Thanks for kindly pointing out our writing issues . We will carefully fix typos and use more proofreading ."}, "2": {"review_id": "B1lnzn0ctQ-2", "review_text": "The paper raises many important questions about unrolled iterative optimization algorithms, and answers many questions for the case of iterative soft thresholding algorithm (ISTA, and learned variant LISTA). The authors demonstrate that a major simplification is available for the learned network: instead of learning a matrix for each layer, or even a single (potentially large) matrix, one may obtain the matrix analytically and learn only a series of scalars. These simplifications are not only practically useful but allow for theoretical analysis in the context of optimization theory. On top of this seminal contribution, the results are extended to the convolutional-LISTA setting. Finally, yet another fascinating result is presented, namely that the analytic weights can be determined from a Gaussian-perturbed version of the dictionary. Experimental validation of all results is presented. My only constructive criticism of this paper are a few grammatical typos, but specifically the 2nd to last sentence before Sec 2.1 states the wrong thing \"In this way, the LISTA model could be further significantly simplified, without little performance loss\" ... it should be \"with little\". ", "rating": "10: Top 5% of accepted papers, seminal paper", "reply_text": "Thank you for your careful reading and kindly identifying the typos in our paper ! We will fix these typos and meticulously proofread our article ."}}