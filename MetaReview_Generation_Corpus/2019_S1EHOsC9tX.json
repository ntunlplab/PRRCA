{"year": "2019", "forum": "S1EHOsC9tX", "title": "Towards the first adversarially robust neural network model on MNIST", "decision": "Accept (Poster)", "meta_review": "The paper presents a technique of training robust classification models that uses the input distribution within each class to achieve high accuracy and robustness against adversarial perturbations. \n\nStrengths:\n\n- The resulting model offers good robustness guarantees for a wide range of norm-bounded perturbations\n\n- The authors put a lot of care into the robustness evaluation\n\nWeaknesses: \n\n- Some of the \"shortcomings\" attributed to the previous work seem confusing, as the reported vulnerability corresponds to threat models that the previous work did not made claims about\n\nOverall, this looks like a valuable and interesting contribution.\n", "reviews": [{"review_id": "S1EHOsC9tX-0", "review_text": "This paper shows that the problem of defending MNIST is still unsuccessful. It hereby proposes a model that is robust by design specifically for the MNIST classification task. Unlike conventional classifiers, the proposal learns a class-dependent data distribution using VAEs, and conducts variational inference by optimizing over the latent space to estimate the classification logits. Some extensive experiments verify the model robustness with respect to different distance measure, with most state-of-the-art attacking schemes, and compared against several baselines. The added experiments with rotation and translation further consolidate the value of the work. Overall I think this is a nice paper. Although being lack of some good intuition, the proposed model indeed show superior robustness to previous defending approaches. Also, the model has some other benefits that are shown in Figure 3 and 4. The results show that the model has indeed learned the data distribution rather than roughly determining the decision boundary of the input space as most existing models do. However, I have the following comments that might help to improve the paper: 1. It would be more interesting to add more intuition on why the proposed model is already robust by design. 2. Although the paper is designed for MNIST specifically, the proposed scheme should apply to other classification tasks. Have you tried the models on other datasets like CIFAR10/100? It would be interesting to see whether the proposal would work for more complicated tasks. When the training data for each label is unbalanced, namely, some class has very few samples, would you expect the model to fail? 3. Equation (8) is complicated and still model-dependent. Without further relaxation and simplification, it\u2019s not easy to see if this value is small or large, or to understand what kind of message this section is trying to pass. 4. Although the main contribution of the paper is to propose a model that is robust without further defending, the proposed model could still benefit from adversarial training. Have you tried to retrain your model using the adversarial examples you have got and see if it helps? ", "rating": "7: Good paper, accept", "reply_text": "`` Although the paper is designed for MNIST specifically , the proposed scheme should apply to other classification tasks . Have you tried the models on other datasets like CIFAR10/100 ? It would be interesting to see whether the proposal would work for more complicated tasks . '' First experiments suggest that our robustness is not limited to MNIST . To show this , we trained the proposed ABS model and a vanilla CNN on two class CIFAR and achieve a robustness ~3x larger than a CNN . Robustness results on 2 class CIFAR : model accuracy | L2 robustness CNN 97.1 % | 0.8 ( estimated with BIM ) ABS 89.7 % | 2.5 ( estimated with LatentDescent attack ) To tackle the reduced accuracy of ABS on CIFAR-10 and other datasets , we are currently working on extensions of our architecture and the training procedure . First experiments show that this can improve the accuracy substantially over baseline ABS and still comes with the same robustness to adversarial perturbations ( but this is beyond the scope of this paper ) . `` When the training data for each label is unbalanced , namely , some class has very few samples , would you expect the model to fail ? '' In contrast to purely discriminative models that require manual rebalancing of the training data , our generative architecture can cope well with unbalanced datasets out of the box . To demonstrate this experimentally , we have trained a two-class MNIST classifier ( ones vs. sevens ) both on a balanced dataset , an unbalanced datasets ( 10 times as many sevens than ones during training ) and a highly unbalanced dataset ( 100 times as many ones as sevens during training ) . They all perform similarly well : accuracy | L_2 median perturbation size with Latent Descent attack balanced ABS 99.6 +- 0.1 % | 3.5 +- 0.1 10 :1 unbalanced ABS 99.3 +- 0.2 % | 3.4 +- 0.2 100:1 unbalanced ABS 98.5 +- 0.2 % | 3.2 +- 0.2 `` It would be more interesting to add more intuition on why the proposed model is already robust by design . '' Adversarial training is used to prevent small changes in the input to make large changes in the model decision . In the ABS model , the Gaussian posterior in the reconstruction term ensures that small changes in the input can only entail small changes to the posterior likelihood and thus to the model decision . In other words , small changes in the input can only lead to small changes in the reconstruction error and so the logits ( = reconstruction error + KL divergence ) can only change slowly with varying inputs . `` Equation ( 8 ) is complicated and still model-dependent . Without further relaxation and simplification , it \u2019 s not easy to see if this value is small or large , or to understand what kind of message this section is trying to pass . '' We provide quantitative values in the results section `` Lower bounds on Robustness '' ( we 'll add a pointer ) . For ABS , the mean L2 perturbation ( i.e.the mean of epsilon in eq.8 across samples ) is 0.69 . For comparison , Hein et al . [ 1 ] reaches 0.48 . [ 1 ] Matthias Hein and Maksym Andriushchenko . Formal guarantees on the robustness of a classifier against adversarial manipulation . In Advances in Neural Information Processing Systems 30 , pp . 2266\u20132276 . Curran Associates , Inc. , 2017 . `` Although the main contribution of the paper is to propose a model that is robust without further defending , the proposed model could still benefit from adversarial training . Have you tried to retrain your model using the adversarial examples you have got and see if it helps ? '' It 's an interesting question as to whether a combination of analysis by synthesis and adversarial training can yield even better results . One potential problem could be that adversarial training makes little sense if adversarials are already at the perceptual boundary between two classes . This would need to be evaluated carefully and we feel that such an analysis goes beyond the scope of this paper . We will , however , release the code and the pretrained model for the community to play around with such ideas . Thanks for the suggestion !"}, {"review_id": "S1EHOsC9tX-1", "review_text": "In this paper, the authors argued that the current approaches are not robust to adversarial attacks, even for MNIST. They proposed a generative approach for classification, which uses variational autoencoder (VAE) to estimate the class specific feature distribution. Robustness guarantees are derived for their model. Through numeric studies, they demonstrated the performance of their proposal (ABS). They also demonstrated that many of the adversarial examples for their ABS model are actually meaningful to humans, which are different from existing approaches, such as SOTA. Overall this is a well written paper. The presentation of their methodology is clear, so are the numerical studies. Some comments: 1) it was not very clear to me that the authors were estimating the p(x) for each y. The transition from p(x|y) to p(x) at the end of page 3 was astute and confused me. The authors should make it more clear. 2) it would be beneficial if the authors could comment on the how strict/loose the lower bound of (2) is, as it is critical in estimating the class specific density.", "rating": "7: Good paper, accept", "reply_text": "`` it was not very clear to me that the authors were estimating the p ( x ) for each y . The transition from p ( x|y ) to p ( x ) at the end of page 3 was astute and confused me . The authors should make it more clear . '' We agree , thank you for pointing this out . We changed p ( x ) - > p ( x|y ) in Equation ( 2 ) and the text . `` it would be beneficial if the authors could comment on the how strict/loose the lower bound of ( 2 ) is , as it is critical in estimating the class specific density . '' For a standard VAE trained on MNIST , the estimate of log p ( x ) is around -93 while true log-likeilhood is at around -87 ( see https : //openreview.net/pdf ? id=HyZoi-WRb , Figure 3 ) . Hence , the bound is neither extremely loose , nor extremely tight . In any case , one should keep in mind that the goal of the model is not optimal density estimation but accuracy and model robustness , so we can accept to be non-optimal . You may be right , however , that tighter bounds might also increase accuracy and robustness , which is an exciting question to be answered in future work ."}, {"review_id": "S1EHOsC9tX-2", "review_text": "Paper summary: The paper presents a robust Analysis by Synthesis classification model that uses the input distribution within each class to achieve high accuracy and robustness against adversarial perturbations. The architecture involves training VAEs for each class to learn p(x|y) and performing exact inference during evaluation. The authors show that ABS and binary ABS outperform other models in terms of robustness for L2, Linf and L0 attacks respectively. The paper in general is well written and clear, and the approach of using generative methods such as VAE for better robustness is good. Pros: Using VAEs for modeling class conditional distributions for data is an exhaustive approach. The authors show in Fig 4 that ABS generates adversarials that are semantically meaningful for humans, which is not achieved by Madry et al and other models. Cons: 1) The main concern with this work is that it is heavily tailored towards MNIST and the authors do mention this. Scaling this to other datasets does not seem easy. 2) Using VAEs to model the conditional class distributions is a nice idea, but how does this scale for datasets with large number of classes like imagenet? This would result in having 1000s of VAEs. 3) It would be nice to see this model behaves for skewed datasets. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "`` The main concern with this work is that it is heavily tailored towards MNIST and the authors do mention this . Scaling this to other datasets does not seem easy. `` `` Using VAEs to model the conditional class distributions is a nice idea , but how does this scale for datasets with large number of classes like imagenet ? This would result in having 1000s of VAEs . '' First experiments suggest that our robustness is not limited to MNIST . To show this , we trained the proposed ABS model and a vanilla CNN on two class CIFAR and achieve a robustness ~3x larger than a CNN . Robustness results on 2 class CIFAR : model accuracy | L2 robustness CNN 97.1 % | 0.8 ( estimated with BIM ) ABS 89.7 % | 2.5 ( estimated with LatentDescent attack ) To tackle the reduced accuracy of ABS on CIFAR-10 and other datasets , we are currently working on extensions of our architecture and the training procedure . First experiments show that this can improve the accuracy substantially over baseline ABS and still comes with the same robustness to adversarial perturbations ( but this is beyond the scope of this paper ) . `` It would be nice to see this model behaves for skewed datasets . '' In contrast to purely discriminative models that require manual rebalancing of the training data , our generative architecture can cope well with unbalanced datasets out of the box . To demonstrate this experimentally , we have trained a two-class MNIST classifier ( ones vs. sevens ) both on a balanced dataset , an unbalanced datasets ( 10 times as many sevens than ones during training ) and a highly unbalanced dataset ( 100 times as many ones as sevens during training ) . They all perform similarly well : accuracy | L_2 median perturbation size with Latent Descent attack balanced ABS 99.6 +- 0.1 % | 3.5 +- 0.1 10 :1 unbalanced ABS 99.3 +- 0.2 % | 3.4 +- 0.2 100:1 unbalanced ABS 98.5 +- 0.2 % | 3.2 +- 0.2"}], "0": {"review_id": "S1EHOsC9tX-0", "review_text": "This paper shows that the problem of defending MNIST is still unsuccessful. It hereby proposes a model that is robust by design specifically for the MNIST classification task. Unlike conventional classifiers, the proposal learns a class-dependent data distribution using VAEs, and conducts variational inference by optimizing over the latent space to estimate the classification logits. Some extensive experiments verify the model robustness with respect to different distance measure, with most state-of-the-art attacking schemes, and compared against several baselines. The added experiments with rotation and translation further consolidate the value of the work. Overall I think this is a nice paper. Although being lack of some good intuition, the proposed model indeed show superior robustness to previous defending approaches. Also, the model has some other benefits that are shown in Figure 3 and 4. The results show that the model has indeed learned the data distribution rather than roughly determining the decision boundary of the input space as most existing models do. However, I have the following comments that might help to improve the paper: 1. It would be more interesting to add more intuition on why the proposed model is already robust by design. 2. Although the paper is designed for MNIST specifically, the proposed scheme should apply to other classification tasks. Have you tried the models on other datasets like CIFAR10/100? It would be interesting to see whether the proposal would work for more complicated tasks. When the training data for each label is unbalanced, namely, some class has very few samples, would you expect the model to fail? 3. Equation (8) is complicated and still model-dependent. Without further relaxation and simplification, it\u2019s not easy to see if this value is small or large, or to understand what kind of message this section is trying to pass. 4. Although the main contribution of the paper is to propose a model that is robust without further defending, the proposed model could still benefit from adversarial training. Have you tried to retrain your model using the adversarial examples you have got and see if it helps? ", "rating": "7: Good paper, accept", "reply_text": "`` Although the paper is designed for MNIST specifically , the proposed scheme should apply to other classification tasks . Have you tried the models on other datasets like CIFAR10/100 ? It would be interesting to see whether the proposal would work for more complicated tasks . '' First experiments suggest that our robustness is not limited to MNIST . To show this , we trained the proposed ABS model and a vanilla CNN on two class CIFAR and achieve a robustness ~3x larger than a CNN . Robustness results on 2 class CIFAR : model accuracy | L2 robustness CNN 97.1 % | 0.8 ( estimated with BIM ) ABS 89.7 % | 2.5 ( estimated with LatentDescent attack ) To tackle the reduced accuracy of ABS on CIFAR-10 and other datasets , we are currently working on extensions of our architecture and the training procedure . First experiments show that this can improve the accuracy substantially over baseline ABS and still comes with the same robustness to adversarial perturbations ( but this is beyond the scope of this paper ) . `` When the training data for each label is unbalanced , namely , some class has very few samples , would you expect the model to fail ? '' In contrast to purely discriminative models that require manual rebalancing of the training data , our generative architecture can cope well with unbalanced datasets out of the box . To demonstrate this experimentally , we have trained a two-class MNIST classifier ( ones vs. sevens ) both on a balanced dataset , an unbalanced datasets ( 10 times as many sevens than ones during training ) and a highly unbalanced dataset ( 100 times as many ones as sevens during training ) . They all perform similarly well : accuracy | L_2 median perturbation size with Latent Descent attack balanced ABS 99.6 +- 0.1 % | 3.5 +- 0.1 10 :1 unbalanced ABS 99.3 +- 0.2 % | 3.4 +- 0.2 100:1 unbalanced ABS 98.5 +- 0.2 % | 3.2 +- 0.2 `` It would be more interesting to add more intuition on why the proposed model is already robust by design . '' Adversarial training is used to prevent small changes in the input to make large changes in the model decision . In the ABS model , the Gaussian posterior in the reconstruction term ensures that small changes in the input can only entail small changes to the posterior likelihood and thus to the model decision . In other words , small changes in the input can only lead to small changes in the reconstruction error and so the logits ( = reconstruction error + KL divergence ) can only change slowly with varying inputs . `` Equation ( 8 ) is complicated and still model-dependent . Without further relaxation and simplification , it \u2019 s not easy to see if this value is small or large , or to understand what kind of message this section is trying to pass . '' We provide quantitative values in the results section `` Lower bounds on Robustness '' ( we 'll add a pointer ) . For ABS , the mean L2 perturbation ( i.e.the mean of epsilon in eq.8 across samples ) is 0.69 . For comparison , Hein et al . [ 1 ] reaches 0.48 . [ 1 ] Matthias Hein and Maksym Andriushchenko . Formal guarantees on the robustness of a classifier against adversarial manipulation . In Advances in Neural Information Processing Systems 30 , pp . 2266\u20132276 . Curran Associates , Inc. , 2017 . `` Although the main contribution of the paper is to propose a model that is robust without further defending , the proposed model could still benefit from adversarial training . Have you tried to retrain your model using the adversarial examples you have got and see if it helps ? '' It 's an interesting question as to whether a combination of analysis by synthesis and adversarial training can yield even better results . One potential problem could be that adversarial training makes little sense if adversarials are already at the perceptual boundary between two classes . This would need to be evaluated carefully and we feel that such an analysis goes beyond the scope of this paper . We will , however , release the code and the pretrained model for the community to play around with such ideas . Thanks for the suggestion !"}, "1": {"review_id": "S1EHOsC9tX-1", "review_text": "In this paper, the authors argued that the current approaches are not robust to adversarial attacks, even for MNIST. They proposed a generative approach for classification, which uses variational autoencoder (VAE) to estimate the class specific feature distribution. Robustness guarantees are derived for their model. Through numeric studies, they demonstrated the performance of their proposal (ABS). They also demonstrated that many of the adversarial examples for their ABS model are actually meaningful to humans, which are different from existing approaches, such as SOTA. Overall this is a well written paper. The presentation of their methodology is clear, so are the numerical studies. Some comments: 1) it was not very clear to me that the authors were estimating the p(x) for each y. The transition from p(x|y) to p(x) at the end of page 3 was astute and confused me. The authors should make it more clear. 2) it would be beneficial if the authors could comment on the how strict/loose the lower bound of (2) is, as it is critical in estimating the class specific density.", "rating": "7: Good paper, accept", "reply_text": "`` it was not very clear to me that the authors were estimating the p ( x ) for each y . The transition from p ( x|y ) to p ( x ) at the end of page 3 was astute and confused me . The authors should make it more clear . '' We agree , thank you for pointing this out . We changed p ( x ) - > p ( x|y ) in Equation ( 2 ) and the text . `` it would be beneficial if the authors could comment on the how strict/loose the lower bound of ( 2 ) is , as it is critical in estimating the class specific density . '' For a standard VAE trained on MNIST , the estimate of log p ( x ) is around -93 while true log-likeilhood is at around -87 ( see https : //openreview.net/pdf ? id=HyZoi-WRb , Figure 3 ) . Hence , the bound is neither extremely loose , nor extremely tight . In any case , one should keep in mind that the goal of the model is not optimal density estimation but accuracy and model robustness , so we can accept to be non-optimal . You may be right , however , that tighter bounds might also increase accuracy and robustness , which is an exciting question to be answered in future work ."}, "2": {"review_id": "S1EHOsC9tX-2", "review_text": "Paper summary: The paper presents a robust Analysis by Synthesis classification model that uses the input distribution within each class to achieve high accuracy and robustness against adversarial perturbations. The architecture involves training VAEs for each class to learn p(x|y) and performing exact inference during evaluation. The authors show that ABS and binary ABS outperform other models in terms of robustness for L2, Linf and L0 attacks respectively. The paper in general is well written and clear, and the approach of using generative methods such as VAE for better robustness is good. Pros: Using VAEs for modeling class conditional distributions for data is an exhaustive approach. The authors show in Fig 4 that ABS generates adversarials that are semantically meaningful for humans, which is not achieved by Madry et al and other models. Cons: 1) The main concern with this work is that it is heavily tailored towards MNIST and the authors do mention this. Scaling this to other datasets does not seem easy. 2) Using VAEs to model the conditional class distributions is a nice idea, but how does this scale for datasets with large number of classes like imagenet? This would result in having 1000s of VAEs. 3) It would be nice to see this model behaves for skewed datasets. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "`` The main concern with this work is that it is heavily tailored towards MNIST and the authors do mention this . Scaling this to other datasets does not seem easy. `` `` Using VAEs to model the conditional class distributions is a nice idea , but how does this scale for datasets with large number of classes like imagenet ? This would result in having 1000s of VAEs . '' First experiments suggest that our robustness is not limited to MNIST . To show this , we trained the proposed ABS model and a vanilla CNN on two class CIFAR and achieve a robustness ~3x larger than a CNN . Robustness results on 2 class CIFAR : model accuracy | L2 robustness CNN 97.1 % | 0.8 ( estimated with BIM ) ABS 89.7 % | 2.5 ( estimated with LatentDescent attack ) To tackle the reduced accuracy of ABS on CIFAR-10 and other datasets , we are currently working on extensions of our architecture and the training procedure . First experiments show that this can improve the accuracy substantially over baseline ABS and still comes with the same robustness to adversarial perturbations ( but this is beyond the scope of this paper ) . `` It would be nice to see this model behaves for skewed datasets . '' In contrast to purely discriminative models that require manual rebalancing of the training data , our generative architecture can cope well with unbalanced datasets out of the box . To demonstrate this experimentally , we have trained a two-class MNIST classifier ( ones vs. sevens ) both on a balanced dataset , an unbalanced datasets ( 10 times as many sevens than ones during training ) and a highly unbalanced dataset ( 100 times as many ones as sevens during training ) . They all perform similarly well : accuracy | L_2 median perturbation size with Latent Descent attack balanced ABS 99.6 +- 0.1 % | 3.5 +- 0.1 10 :1 unbalanced ABS 99.3 +- 0.2 % | 3.4 +- 0.2 100:1 unbalanced ABS 98.5 +- 0.2 % | 3.2 +- 0.2"}}