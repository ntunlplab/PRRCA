{"year": "2021", "forum": "arNvQ7QRyVb", "title": "Sharing Less is More: Lifelong Learning in Deep Networks with Selective Layer Transfer", "decision": "Reject", "meta_review": "The reviewers enjoyed reading about an interesting take on lifelong learning, encapsulating an EM methodology for selecting a transfer configuration and then optimizing the parameters. R3 made valid concerns regarding comparison with previous, recent work. R2 also would prefer to see more thorough experiments (ideally in settings where multiple tasks exist, as also commented by R4). During the rebuttal phase the authors made a good effort to run additional experiments which cover the related work aspect better. These experiments and the overall paper were discussed extensively among reviewers after the rebuttal phase.\n\nIn the discussions, the reviewers agreed that an interesting idea can be publishable even if it does not achieve SOTA results in all scenarios, as long as it brings new perspectives and shows at least comparable results. However, in the particular case of this paper, there exist remaining concerns regarding the usefulness and applicability of the method. Specifically, the paper could benefit from a more convincing demonstration about how the method can scale (e.g. R3 and R4\u2019s comments), especially since training time and model capacity are important factors to consider for practical continual learning scenarios. Furthermore, it is not clear how the proposed method can be used in combination with other machine learning tools within a continual learning application, for example by leveraging modern deep architectures or by complementing existing adaptive knowledge approaches (as discussed by R3). \n\nAlthough the opinions of the reviewers are not fully aligned, this borderline paper seemed to lack an enthusiastic endorsement by a reviewer to compensate for the concerns discussed above and the relatively weak experimental results. Therefore I recommend rejection. \n", "reviews": [{"review_id": "arNvQ7QRyVb-0", "review_text": "-Summary- The paper proposes a method for selective weight sharing per layer during continual learning . The authors show observations that sharing all layers can not be optimal for lifelong learning . Hence , they adopt a layerwise transfer configuration vector which decides activated layer-sharing at specific tasks . The problem is solved by EM algorithm-based approach . -Pros- - Observations are reasonable and give many inspirations for solving continual learning issues and developing the existing methods . - The paper is well written and easy to follow . - The point of view connected to neural architecture search is understandable . - The model outperforms old baselines . -Cons- - The problem is task-incremental which clearly gives task oracle during training and inference . Recent continual learning works obviously tackle class-incremental learning problems that are more challenging and applicable to a broader area [ 1 ] . - Baselines are too weak . If the paper targets task-incremental learning problems , the authors should compare their methods with recent works , rather than with 3-5 past years ' works . I recommend to include further strong baselines like [ 2,3,4 ] . - The strengths of the methods may not be impressive on modern deeper networks , like ResNet-50 . Since it requires massive computation time . The paper did n't show the results/analysis of modern deep architectures . - The model inevitably requires much capacity for not-shared task-specific layers . But the authors did n't include it . -Comments- - Why do the authors only use a fraction of datasets ? - [ 1 ] van de Ven , Gido M. , and Andreas S. Tolias . `` Three scenarios for continual learning . '' arXiv preprint arXiv:1904.07734 ( 2019 ) . - [ 2 ] Titsias , Michalis K. , et al . `` Functional Regularisation for Continual Learning with Gaussian Processes . `` , ICLR 2020 . - [ 3 ] Yoon , Jaehong , et al . `` Scalable and Order-robust Continual Learning with Additive Parameter Decomposition . `` , ICLR 2020 . - [ 4 ] Davide Abati , et al . `` Conditional Channel Gated Networks for Task-Aware Continual Learning . `` , CVPR 2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank the reviewer for the feedback . We are encouraged that the most of our paper is clear and intuitive to the reviewer . We answer the reviewer \u2019 s questions . * * Con1.Class-incremental learning problem is more challenging than task-incremental problem : * * We agree that lifelong learning problems without task indices are more challenging , but task-incremental learning is far from solved . There are many applications which require task-incremental learning , and so work in that topic is certainly relevant and still current . In the future , we do aim to extend this work to handle learning scenarios even when task information is not available to the learner . * * Con2.Baselines : * * We are implementing the suggested baselines and running experiments . We will incorporate the comparison with the baselines during revision . * * Con3.Application to deeper networks : * * This work proposes the method to find useful layers to be shared between tasks , so we \u2019 re dependent on the network architectures used by existing lifelong learning methods . Deeper networks like ResNet-50 show great results on visual domain problems , but these architectures are not designed for knowledge transfer between tasks . If one ResNet-50 with multiple output heads , for instance , is used in a lifelong learning scenario , it is conceptually the same as hard-parameter sharing architecture because of the explicit re-use of parameters . It is an astute observation that the reviewer pointed out the difficulty of applying neural architecture search ( NAS ) to a deeper network . It is a common issue in the NAS literature , and previous works remedy this issue by applying NAS methods over groups of layers instead of individual layers . We used this solution to apply LASEM to the network with 9 convolutional layers to share , and the details and experimental results are described in Appendix E. Lifelong learning experiments ( repeated many times to get proper statistics ) take a while , and so we simply don \u2019 t have the computational resources to try it over larger networks such as ResNet-50 . * * Con4.Unspecified capacity for non-shared task-specific layers : * * We \u2019 ve tried to understand your concern , but it is unclear . Would you mind restating it in more detail ? Appendix A specifies the sizes of both shared and non-shared layers , and we theoretically analyzed the memory usage of the method with respect to the number of tasks ( including the task-specific layers ) in Appendix F. The concern about the non-shared layer storage seems more an aspect of the base lifelong learner than our LASEM algorithm that operates on that base learner . Since this paper focuses on a multi-model formulation of lifelong learning , the total number of additional parameters ( for the shared layers and the transfer configuration storage ) is a constant as the number of tasks grows . The storage for non-shared layers does grow proportionally to the number of tasks , but such per-task parameters are typical for such multi-model lifelong learners . Please see the references for the base lifelong learning algorithms used in the paper . * * Question . Reason for using a fraction of datasets : * * We followed previous work on this ( see experiment details in Section 2 and Appendix A ) , using a fraction of the datasets to evaluate lifelong learning performance in a low-data regime . In the low-data regime , it is important to transfer useful knowledge of previously learned tasks due to the lack of training data . Therefore , using a fraction of datasets makes comparison between knowledge transfer methods clear , and allows direct comparison to previous work ."}, {"review_id": "arNvQ7QRyVb-1", "review_text": "This paper introduces a lifelong learning algorithm across multiple tasks that automatically learns which layers need to be optimized using an EM learning strategy for each task . In the expectation step , the algorithm updates the next best configuration and in the M step , it optimizes the model parameters . Some details of the algorithm are not explained well . It could help a lot if the authors could provide the objective function they are trying to optimize . As examples : ( 1 ) The motivation behind Equation 3 is not clear at all . Based on Equation 3 , It seems that the choice of different configurations is mini-batch dependent and not task dependent . I have a difficult time to understand why this is the case . I might have misunderstood the algorithm but providing the objective function helps a lot . ( 2 ) It is not clear why n_ { c_i } in Equation 1 is the number of previous mini-batches for which C_ { ( t ) } is the most probable configuration . It makes sense to use a soft version of this ( sum of probabilities ) to have it more compatible with the rest of the algorithm . Again , understanding what objective function the authors are targeting to optimize helps to understand this better too . Even though , some aspect of the proposed algorithm is not clear , the algorithm works in practice and outperform multiple suggested baselines . Minor type : Page 5 : 5 % percent \uf0e0 5 %", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the feedback . We are encouraged that most of our paper is clear and intuitive to the reviewers . We answer the questions regarding the algorithm of our proposed method . * * Q1.Clarification of equation 3 : * * The choice of configurations is task-dependent . Although the selection may be updated by each mini-batch , the selection is based on the aggregate updates over all mini-batches . The selection of transfer configurations is based on their relevant weight , which is computed by the posterior ( equation 2 ) combining information from the current mini-batch ( likelihood ) and task-wise history ( prior , equation 1 ) . The objective function of the optimization is log-likelihood $ P ( y_ { new } |X_ { new } , c_ { i } ) $ . Equation 3 is simply the gradient updates of the objective for all configurations weighted by the estimated posterior of those configurations . So , you can think of it as the expected gradient update based on the distribution over the configurations . Consequently , Equation 3 is the M-step of Algorithm 1 ( line 11 ) . Please note that the two equations in Equation 3 are almost identical except that one is for the parameters of the shared layers and the other is for the parameters of task-specific layers . * * Q2.Soft version of counting configurations for equation 1 : * * When developing this approach we tried two formulations for the prior probability of the transfer configuration : ( 1 ) the method described in the paper using the number of mini-batches ( Equation 1 ) , and ( 2 ) the sum of probabilities of the soft version of Equation 1 , as you suggested . These two methods were not different statistically in an empirical evaluation on CIFAR100 and Office-Home , so we chose the simpler of the two methods to include in the paper . We \u2019 ll make a note of this in the revision . Thanks for finding the typo !"}, {"review_id": "arNvQ7QRyVb-2", "review_text": "Summary : authors look into life long learning setting ( when task arrive one after another ) and try to understand which layers of the source model need to be reused ( transfered ) and which should be re-learnt . Authors argue that this decision should be task specific , and come up with an algorithm ( EM like ) that for each task derives indices of the layers to reuse and also updates reusable and non reusable layers . Overall , this paper is really well executed . Easy to follow , enough of background and motivation is provided , and the algorithm for the most part is clear and intuitive . My main complaint is that it is somewhat thin on the experiments . Additional questions/comments : 1 ) How is update 13 from Algo 1 happening ? ( how P ( C|data ) is updated ? is it working by recalculating the counts from step ( 1 ) ( so it is not a smooth function that is differentiable and updated via optimization ) 2 ) In Algo 1 : while IsMoreTrainingDataAvailable is not clear to me . I assume that you do several epochs over task specific data , do n't you ? Also do you monitor anyhow EM convergence ? 3 ) Experiments : One baseline would be useful to just randomly select c_t for each task and see how it does . EM is pretty expensive 4 ) Finally , I feel that related work should really go after the into", "rating": "7: Good paper, accept", "reply_text": "We would like to thank the reviewer for the feedback . We are encouraged that the most of our paper is clear and intuitive to the reviewer . We answer questions that the reviewer specified below . * * Q1.Details of update on line 13 of Algorithm 1 : * * Equation 1 is the detail of the function PriorUpdater ( ) in line 13 of Algorithm 1 . It counts the number of mini-batches for the current task ( line 2-14 of Algorithm 1 ) . Only the M-step , line 11 , requires gradient computation , so the prior is not a smooth function . * * Q2.Details of IsMoreTrainingDataAvailable and convergence : * * Yes , we do several epochs over data of a specific task . Algorithm 1 considers the general setting in which data of any task can be given to the learner at each timestep . The idea is that at each timestep , the learner gets whatever data is available regardless of tasks , so the function getNextTrainingData ( ) returns a task index t as well as training data $ X_ { new } $ and $ y_ { new } $ for the learner to know what the current task is . As long as more data is available , the function isMoreTrainingDataAvailable ( ) will be true , and the learner can then incorporate available data . In the experiment of the paper , we imposed additional constraints on the setting so that tasks remain the same along the specific amount of time or epochs . We considered lifelong learning scenarios in which a learner has no control over tasks , so the current task can be switched to another one without convergence of the learning algorithm . Because of this setting , we don \u2019 t require EM convergence , but it is possible to monitor the convergence by checking the probability weight over transfer configurations . * * Q3 Additional Ablative Baseline : * * That \u2019 s an interesting suggestion , and what essentially occurs at the start of LASEM for a new task ( when the transfer configuration is essentially random ) . But , it \u2019 s likely to yield performance somewhere around the means of the black boxes of Figure 3 , which is clearly exceeded by just a bit of search over the configurations . Additionally , during the early stage of LASEM , both the transfer configuration and the layer parameters are not converged , so converging the transfer configuration immediately via random sampling as you suggest in the ablative baseline wouldn \u2019 t likely accelerate convergence . * * Q4 Related Work Placement : * * Thanks for your suggestion on the paper structure . As you can tell , the paper structure is a bit atypical ( but earlier feedback said that it worked well for this paper ) , so it \u2019 s good to have another viewpoint ."}, {"review_id": "arNvQ7QRyVb-3", "review_text": "This paper studies the problem of lifelong learning of a sequence of tasks by selectively transferring knowledge of some layers across tasks . The proposed approach , LASEM , uses EM to dynamically adjust transfer configuration between tasks by performing architecture search . The authors present results in benchmark datasets for continual learning . As strengths of the paper I would remark : - The paper is well-motivated and demonstrates experimentally why simply transferring all layers does not lead to the best transfer configuration . - The proposed approach in section 3 is clearly explained and the main design choices are well argued . In general , the paper is well-written and easy to follow . - The paper includes thorough results of the computational complexity of the proposed method , which could be a concern given the proposed approach of maintaining both shared and specific sets of parameters for each task . The weaknesses I observe in this paper are : - I am concerned on the extensibility of the proposed method to a large number of tasks , which I would think would pose a challenge in the proposed setting similar to how it does in multitask learning . What would be the impact of a large number of tasks , specifically on the set of shared parameters ? The experiments provide results for a limited number of tasks ( up to 20 ) . Questions for authors : - Please address the question regarding the effect of number of tasks on the proposed approach .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the feedback . We are encouraged that the reviewer found our paper well-motivated . The reviewer raised a question about the extensibility of the proposed method to a large number of tasks . We \u2019 re following experimental protocols ( including datasets and tasks ) used by previous work , and you \u2019 re certainly correct that the number of tasks is limited . This is largely to permit numerous evaluations for statistical purposes within a reasonable time . However , our approach should scale to numerous tasks . Our algorithm aggregates only the information of the current task and resets when the task is changed . Therefore , the computational requirement of LASEM remains the same regardless of the number of tasks . The memory requirement of task models according to the number of tasks is directly related to the lifelong learning architecture that LASEM is applied to . As the number of tasks grows larger , so too often does the task distribution broaden , and so the capacity of the shared tensors would likely need to be set correspondingly larger to handle the increased breadth of the task models . This observation is less relevant to our proposed LASEM algorithm , and more relevant to the base lifelong learners that it operates upon . Note that the extensibility of LASEM is highly correlated to the depth of the network as other neural architecture search methods do . We adopted a solution of previous works of neural architecture search for networks with many layers , and the result is described in Appendix E ."}], "0": {"review_id": "arNvQ7QRyVb-0", "review_text": "-Summary- The paper proposes a method for selective weight sharing per layer during continual learning . The authors show observations that sharing all layers can not be optimal for lifelong learning . Hence , they adopt a layerwise transfer configuration vector which decides activated layer-sharing at specific tasks . The problem is solved by EM algorithm-based approach . -Pros- - Observations are reasonable and give many inspirations for solving continual learning issues and developing the existing methods . - The paper is well written and easy to follow . - The point of view connected to neural architecture search is understandable . - The model outperforms old baselines . -Cons- - The problem is task-incremental which clearly gives task oracle during training and inference . Recent continual learning works obviously tackle class-incremental learning problems that are more challenging and applicable to a broader area [ 1 ] . - Baselines are too weak . If the paper targets task-incremental learning problems , the authors should compare their methods with recent works , rather than with 3-5 past years ' works . I recommend to include further strong baselines like [ 2,3,4 ] . - The strengths of the methods may not be impressive on modern deeper networks , like ResNet-50 . Since it requires massive computation time . The paper did n't show the results/analysis of modern deep architectures . - The model inevitably requires much capacity for not-shared task-specific layers . But the authors did n't include it . -Comments- - Why do the authors only use a fraction of datasets ? - [ 1 ] van de Ven , Gido M. , and Andreas S. Tolias . `` Three scenarios for continual learning . '' arXiv preprint arXiv:1904.07734 ( 2019 ) . - [ 2 ] Titsias , Michalis K. , et al . `` Functional Regularisation for Continual Learning with Gaussian Processes . `` , ICLR 2020 . - [ 3 ] Yoon , Jaehong , et al . `` Scalable and Order-robust Continual Learning with Additive Parameter Decomposition . `` , ICLR 2020 . - [ 4 ] Davide Abati , et al . `` Conditional Channel Gated Networks for Task-Aware Continual Learning . `` , CVPR 2020 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We would like to thank the reviewer for the feedback . We are encouraged that the most of our paper is clear and intuitive to the reviewer . We answer the reviewer \u2019 s questions . * * Con1.Class-incremental learning problem is more challenging than task-incremental problem : * * We agree that lifelong learning problems without task indices are more challenging , but task-incremental learning is far from solved . There are many applications which require task-incremental learning , and so work in that topic is certainly relevant and still current . In the future , we do aim to extend this work to handle learning scenarios even when task information is not available to the learner . * * Con2.Baselines : * * We are implementing the suggested baselines and running experiments . We will incorporate the comparison with the baselines during revision . * * Con3.Application to deeper networks : * * This work proposes the method to find useful layers to be shared between tasks , so we \u2019 re dependent on the network architectures used by existing lifelong learning methods . Deeper networks like ResNet-50 show great results on visual domain problems , but these architectures are not designed for knowledge transfer between tasks . If one ResNet-50 with multiple output heads , for instance , is used in a lifelong learning scenario , it is conceptually the same as hard-parameter sharing architecture because of the explicit re-use of parameters . It is an astute observation that the reviewer pointed out the difficulty of applying neural architecture search ( NAS ) to a deeper network . It is a common issue in the NAS literature , and previous works remedy this issue by applying NAS methods over groups of layers instead of individual layers . We used this solution to apply LASEM to the network with 9 convolutional layers to share , and the details and experimental results are described in Appendix E. Lifelong learning experiments ( repeated many times to get proper statistics ) take a while , and so we simply don \u2019 t have the computational resources to try it over larger networks such as ResNet-50 . * * Con4.Unspecified capacity for non-shared task-specific layers : * * We \u2019 ve tried to understand your concern , but it is unclear . Would you mind restating it in more detail ? Appendix A specifies the sizes of both shared and non-shared layers , and we theoretically analyzed the memory usage of the method with respect to the number of tasks ( including the task-specific layers ) in Appendix F. The concern about the non-shared layer storage seems more an aspect of the base lifelong learner than our LASEM algorithm that operates on that base learner . Since this paper focuses on a multi-model formulation of lifelong learning , the total number of additional parameters ( for the shared layers and the transfer configuration storage ) is a constant as the number of tasks grows . The storage for non-shared layers does grow proportionally to the number of tasks , but such per-task parameters are typical for such multi-model lifelong learners . Please see the references for the base lifelong learning algorithms used in the paper . * * Question . Reason for using a fraction of datasets : * * We followed previous work on this ( see experiment details in Section 2 and Appendix A ) , using a fraction of the datasets to evaluate lifelong learning performance in a low-data regime . In the low-data regime , it is important to transfer useful knowledge of previously learned tasks due to the lack of training data . Therefore , using a fraction of datasets makes comparison between knowledge transfer methods clear , and allows direct comparison to previous work ."}, "1": {"review_id": "arNvQ7QRyVb-1", "review_text": "This paper introduces a lifelong learning algorithm across multiple tasks that automatically learns which layers need to be optimized using an EM learning strategy for each task . In the expectation step , the algorithm updates the next best configuration and in the M step , it optimizes the model parameters . Some details of the algorithm are not explained well . It could help a lot if the authors could provide the objective function they are trying to optimize . As examples : ( 1 ) The motivation behind Equation 3 is not clear at all . Based on Equation 3 , It seems that the choice of different configurations is mini-batch dependent and not task dependent . I have a difficult time to understand why this is the case . I might have misunderstood the algorithm but providing the objective function helps a lot . ( 2 ) It is not clear why n_ { c_i } in Equation 1 is the number of previous mini-batches for which C_ { ( t ) } is the most probable configuration . It makes sense to use a soft version of this ( sum of probabilities ) to have it more compatible with the rest of the algorithm . Again , understanding what objective function the authors are targeting to optimize helps to understand this better too . Even though , some aspect of the proposed algorithm is not clear , the algorithm works in practice and outperform multiple suggested baselines . Minor type : Page 5 : 5 % percent \uf0e0 5 %", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the feedback . We are encouraged that most of our paper is clear and intuitive to the reviewers . We answer the questions regarding the algorithm of our proposed method . * * Q1.Clarification of equation 3 : * * The choice of configurations is task-dependent . Although the selection may be updated by each mini-batch , the selection is based on the aggregate updates over all mini-batches . The selection of transfer configurations is based on their relevant weight , which is computed by the posterior ( equation 2 ) combining information from the current mini-batch ( likelihood ) and task-wise history ( prior , equation 1 ) . The objective function of the optimization is log-likelihood $ P ( y_ { new } |X_ { new } , c_ { i } ) $ . Equation 3 is simply the gradient updates of the objective for all configurations weighted by the estimated posterior of those configurations . So , you can think of it as the expected gradient update based on the distribution over the configurations . Consequently , Equation 3 is the M-step of Algorithm 1 ( line 11 ) . Please note that the two equations in Equation 3 are almost identical except that one is for the parameters of the shared layers and the other is for the parameters of task-specific layers . * * Q2.Soft version of counting configurations for equation 1 : * * When developing this approach we tried two formulations for the prior probability of the transfer configuration : ( 1 ) the method described in the paper using the number of mini-batches ( Equation 1 ) , and ( 2 ) the sum of probabilities of the soft version of Equation 1 , as you suggested . These two methods were not different statistically in an empirical evaluation on CIFAR100 and Office-Home , so we chose the simpler of the two methods to include in the paper . We \u2019 ll make a note of this in the revision . Thanks for finding the typo !"}, "2": {"review_id": "arNvQ7QRyVb-2", "review_text": "Summary : authors look into life long learning setting ( when task arrive one after another ) and try to understand which layers of the source model need to be reused ( transfered ) and which should be re-learnt . Authors argue that this decision should be task specific , and come up with an algorithm ( EM like ) that for each task derives indices of the layers to reuse and also updates reusable and non reusable layers . Overall , this paper is really well executed . Easy to follow , enough of background and motivation is provided , and the algorithm for the most part is clear and intuitive . My main complaint is that it is somewhat thin on the experiments . Additional questions/comments : 1 ) How is update 13 from Algo 1 happening ? ( how P ( C|data ) is updated ? is it working by recalculating the counts from step ( 1 ) ( so it is not a smooth function that is differentiable and updated via optimization ) 2 ) In Algo 1 : while IsMoreTrainingDataAvailable is not clear to me . I assume that you do several epochs over task specific data , do n't you ? Also do you monitor anyhow EM convergence ? 3 ) Experiments : One baseline would be useful to just randomly select c_t for each task and see how it does . EM is pretty expensive 4 ) Finally , I feel that related work should really go after the into", "rating": "7: Good paper, accept", "reply_text": "We would like to thank the reviewer for the feedback . We are encouraged that the most of our paper is clear and intuitive to the reviewer . We answer questions that the reviewer specified below . * * Q1.Details of update on line 13 of Algorithm 1 : * * Equation 1 is the detail of the function PriorUpdater ( ) in line 13 of Algorithm 1 . It counts the number of mini-batches for the current task ( line 2-14 of Algorithm 1 ) . Only the M-step , line 11 , requires gradient computation , so the prior is not a smooth function . * * Q2.Details of IsMoreTrainingDataAvailable and convergence : * * Yes , we do several epochs over data of a specific task . Algorithm 1 considers the general setting in which data of any task can be given to the learner at each timestep . The idea is that at each timestep , the learner gets whatever data is available regardless of tasks , so the function getNextTrainingData ( ) returns a task index t as well as training data $ X_ { new } $ and $ y_ { new } $ for the learner to know what the current task is . As long as more data is available , the function isMoreTrainingDataAvailable ( ) will be true , and the learner can then incorporate available data . In the experiment of the paper , we imposed additional constraints on the setting so that tasks remain the same along the specific amount of time or epochs . We considered lifelong learning scenarios in which a learner has no control over tasks , so the current task can be switched to another one without convergence of the learning algorithm . Because of this setting , we don \u2019 t require EM convergence , but it is possible to monitor the convergence by checking the probability weight over transfer configurations . * * Q3 Additional Ablative Baseline : * * That \u2019 s an interesting suggestion , and what essentially occurs at the start of LASEM for a new task ( when the transfer configuration is essentially random ) . But , it \u2019 s likely to yield performance somewhere around the means of the black boxes of Figure 3 , which is clearly exceeded by just a bit of search over the configurations . Additionally , during the early stage of LASEM , both the transfer configuration and the layer parameters are not converged , so converging the transfer configuration immediately via random sampling as you suggest in the ablative baseline wouldn \u2019 t likely accelerate convergence . * * Q4 Related Work Placement : * * Thanks for your suggestion on the paper structure . As you can tell , the paper structure is a bit atypical ( but earlier feedback said that it worked well for this paper ) , so it \u2019 s good to have another viewpoint ."}, "3": {"review_id": "arNvQ7QRyVb-3", "review_text": "This paper studies the problem of lifelong learning of a sequence of tasks by selectively transferring knowledge of some layers across tasks . The proposed approach , LASEM , uses EM to dynamically adjust transfer configuration between tasks by performing architecture search . The authors present results in benchmark datasets for continual learning . As strengths of the paper I would remark : - The paper is well-motivated and demonstrates experimentally why simply transferring all layers does not lead to the best transfer configuration . - The proposed approach in section 3 is clearly explained and the main design choices are well argued . In general , the paper is well-written and easy to follow . - The paper includes thorough results of the computational complexity of the proposed method , which could be a concern given the proposed approach of maintaining both shared and specific sets of parameters for each task . The weaknesses I observe in this paper are : - I am concerned on the extensibility of the proposed method to a large number of tasks , which I would think would pose a challenge in the proposed setting similar to how it does in multitask learning . What would be the impact of a large number of tasks , specifically on the set of shared parameters ? The experiments provide results for a limited number of tasks ( up to 20 ) . Questions for authors : - Please address the question regarding the effect of number of tasks on the proposed approach .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank the reviewer for the feedback . We are encouraged that the reviewer found our paper well-motivated . The reviewer raised a question about the extensibility of the proposed method to a large number of tasks . We \u2019 re following experimental protocols ( including datasets and tasks ) used by previous work , and you \u2019 re certainly correct that the number of tasks is limited . This is largely to permit numerous evaluations for statistical purposes within a reasonable time . However , our approach should scale to numerous tasks . Our algorithm aggregates only the information of the current task and resets when the task is changed . Therefore , the computational requirement of LASEM remains the same regardless of the number of tasks . The memory requirement of task models according to the number of tasks is directly related to the lifelong learning architecture that LASEM is applied to . As the number of tasks grows larger , so too often does the task distribution broaden , and so the capacity of the shared tensors would likely need to be set correspondingly larger to handle the increased breadth of the task models . This observation is less relevant to our proposed LASEM algorithm , and more relevant to the base lifelong learners that it operates upon . Note that the extensibility of LASEM is highly correlated to the depth of the network as other neural architecture search methods do . We adopted a solution of previous works of neural architecture search for networks with many layers , and the result is described in Appendix E ."}}