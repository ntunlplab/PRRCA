{"year": "2019", "forum": "S1M6Z2Cctm", "title": "Harmonic Unpaired Image-to-image Translation", "decision": "Accept (Poster)", "meta_review": "The proposed method introduces a method for unsupervised image-to-image mapping, using a new term into the objective function that enforces consistency in similarity between image patches across domains. Reviewers left constructive and detailed comments, which, the authors have made substantial efforts to address.\n\nReviewers have ranked paper as borderline, and in Area Chair's opinion, most major issued have been addressed:\n\n- R3&R2: Novelty compared to DistanceGAN/CRF limited: authors have clarified contributions in reference to DistanceGAN/CRF and demonstrated improved performance relative to several datasets.\n- R3&R1: Evaluation on additional datasets required: authors added evaluation on 4 more tasks\n- R3&R1: Details missing: authors added details. \n\n", "reviews": [{"review_id": "S1M6Z2Cctm-0", "review_text": "This paper proposes a method called HarmonicGAN for unpaired image-to-image translation. The key idea is to introduce a regularization term on the basis of CycleGAN, which encourages similar image patches to acquire similar transformations. Two feature domains are explored for evaluating the patch-level similarity, including soft RGB histogram and semantic features based on VGGNet. In fact, the key idea is very similar to that of DistanceGAN. The proposed method can be regarded as a combination of the advantages of DistanceGAN and CycleGAN. Thus, the technical novelty is very limited in my opinion. Some experimental results are provided to demonstrate the superiority of the proposed method over CycleGAN, DistanceGAN and UNIT. Given the limited novelty and the inadequate number of experiments, I am leaning to reject this submission. Major questions: 1. Lots of method details are missing. In Section 3.3.2, what layers are chosen for computing the semantic features? What exactly is the metric for computing the distance between semantic features. 2. The qualitative results on the task, Horse2Zebra and Zebra2Horse, are not impressive. Obvious artifacts can be observed in the results. Although the paper claims that the proposed method does not change the background and performs more complete transformations, the background is changed in the result for the Horse2Zebra case in Fig. 5. More qualitative results are needed to demonstrate the effectiveness of the proposed method. 3. To demonstrate the effectiveness of a general unpaired image-to-image translation method, the proposed method is needed to be testified on more tasks. 4. Implementation details are missing. I am not able to judge whether the comparisons are fair enough. [New comment:] I have read the authors' explanations and clarifications that make me increase my rating. Regarding the technical novelty, I still don't think this paper bears sufficient stuff. If there is extra quota, I would recommend Accept. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q1 : The key idea of this paper is very similar to that of DistanceGAN . The proposed method can be regarded as a combination of the advantages of DistanceGAN and CycleGAN . A1 : There is a large difference between DistanceGAN and the proposed HarmonicGAN . First , DistanceGAN already included the CycleGAN loss . Second , DistanceGAN is about preserving the AVERAGED distance between the sample pairs from the source to the target domain , which is not sufficient to retain the underlying integrity and manifold structure . Next , we elaborate the key difference between DistanceGAN and HarmonicGAN . DistanceGAN encourages the distance of samples to be close to an ABSOLUTE MEAN during translation . In contrast , HarmonicGAN enforces a smoothness term naturally under the graph Laplacian , making the motivations of DistanceGAN and HarmonicGAN quite different . In more detail , the distance constraint in DistanceGAN uses the expectation of the absolute differences between the distances in each domain , formulated as : L_ { distance } ( G , X ) = E_ { x_i , x_j \\in X } \\left| ( || x_i - x_j || - \\mu_X ) / \\sigma_X + ( || G ( x_i ) - G ( x_j ) || - \\mu_Y ) / sigma_Y \\right| , where \\mu_X , \\mu_Y ( \\sigma_X , \\sigma_Y ) are the precomputed means ( standard deviations ) of pairwise distances in the training sets from domain X and Y . This distance preserving is interesting but not strong enough to preserve the manifold structure . We suspect that it is probably the reason for DistanceGAN not performing well , as seen in the qualitative and quantitative measures . Differently , HarmonicGAN introduces a smoothness constraint to provide similarity-consistency between image patches during the translation . The smoothness term defines a graph Laplacian with the minimal value achieved as a harmonic function . We define the set consisting of individual image patches as the nodes of the graph , and define the affinity measure ( similarity ) computed on image patches as the edges of the graph . The smoothness term acts as a graph Laplacian imposed on all pairs of image patches . For the translation from X to Y , the smoothness constraint is formulated as : L_ { smooth } ( G , X , Y ) = E_ { { \\bf x } \\in X } \\big [ \\sum_ { i , j } w_ { ij } ( X ) \\times Dist [ G ( \\vec { x } ) ( i ) , G ( \\vec { x } ) ( j ) ] + \\sum_ { i , j } w_ { ij } ( G ( X ) ) \\times Dist [ F ( G ( \\vec { x } ) ) ( i ) , F ( G ( \\vec { x } ) ) ( j ) ] } \\big ] where w_ { ij } ( X ) = \\exp_ { - Dist [ \\vec { x } ( i ) , \\vec { x } ( j ) ] / \\sigma^2 } defines the affinity between the two patches \\vec { x } ( i ) and \\vec { x } ( j ) . Additionally , the similarity of a pair of patches is measured on the features of each patch , e.g.Histogram or CNN features . Comparing the distance constraint in DistanceGAN and the smoothness constraint in HarmonicGAN , we can conclude the following main three differences between them : ( 1 ) They show different motivations and formulations . Most importantly , the loss term in DistanceGAN essentially matches the distance of sample pairs in the source domain to the AVERAGED distance in the target domain ; it is not about preserving the distance of the individual sample pairs . From a manifold learning point of view , preserving the averaged distance is not sufficient for preserving the underlying manifold structure . In contrast , the smoothness constraint in our method is designed from a graph Laplacian to build the similarity-consistency between image patches . Thus , the smoothness constraint uses the affinity between two patches as weight to measure the similarity-consistency between two domains . Our approach is in the vein of manifold learning . The smoothness term defines a Laplacian \\Delta = D - W , where W is our weight matrix and D is a diagonal matrix with D_ { i } = \\sum_j w_ { ij } , thus , the smoothness term defines a graph Laplacian with the minimal value achieved as a harmonic function . ( 2 ) They are different in implementation . The smoothness term in HarmonicGAN is computed on image patches while the distance term in DistanceGAN is computed for the average . Therefore , the smoothness constraint is more fine-grained compared to the distance preserving term in DistanceGAN . Moreover , the distances in DistanceGAN is directly computed from the samples in each domain . They scale the distances with the precomputed means and stds of two domains to reduce the effect of gap between two domains . Differently , the smoothness constraint in HarmonicGAN is measured on the features ( Histogram or CNN features ) of each patches , which maps samples in two domains into the same feature space and removes the gap between two domains . ( continued below )"}, {"review_id": "S1M6Z2Cctm-1", "review_text": "This paper adds a spatial regularization loss to the well-known CycleGAN loss for unpaired image-to-image translation (Zhu et al., ICCV17). Essentially, the regularization loss (Eq. 6) is similar to imposing a CRF (Conditional Random Field) term on the network outputs, encouraging spatial consistency between patches within each generated image. The paper is clear and well written. Unpaired Image-to-Image translation is an important problem. The way the smoothness loss (Eq. 6) is presented gives readers the impression that spatial pairwise regularization is new, ignoring its long history (e.g., CRFs) in computer vision (not a single classical paper on CRFs is cited). Putting aside classical spatial regularization works, imposing pairwise regularization on the outputs of modern deep networks has been investigated in a very large number of works recently, particularly in the context of weakly-supervised semantic CNN segmentation, e.g., [Tang et al., On Regularized Losses for Weakly-supervised CNN Segmentation, ECCV 18 ], [Lin et al. : Scribblesup: Scribble-supervised convolutional networks for semantic segmentation, CVPR 2016], among many other works. Very similar in spirit to this ICLR submission, these works impose within-image pairwise regularization (e.g., CRF) on the latent outputs of deep networks, with the main difference that these works use CNN semantic segmentation classifiers whereas here we have a CycleGAN for image generation. Also, in the context of supervised CNN segmentation, CRFs have made a significant impact when used as post-processing step, e.g., very well known works such as [DeepLab by Chen et al. ICLR15] and [CRFs as recurrent Neural Networks by Zheng et al., ICCV 2015]. It might be a valid contribution to evaluate spatial regularization (e.g., CRFs losses) on image generation tasks (such as CycleGAN), but the paper really needs to acknowledge very related prior works on regularization (at least in the context of deep networks). There are also related pioneering semi-supervised deep learning works based on graph Laplacian regularization, e.g., [Westen et al., Deep Learning via Semi-supervised embedding, ICML 2008], which the paper does not acknowledge/discuss. The manifold regularization terminology is misleading. The regularization is not over the feature space of image samples. It is within the spatial domain of each generated image (patch or pixel level); so, in my opinion, CRF (or spatial) regularization (instead of manifold regularization) is a much more appropriate terminology. Also, I would not call this approach HarmonicGan. I would call it CRF-GAN or Spatially-Regularized GAN. The computation of harmonic functions is just one way, among many other (potentially better) ways to optimize pairwise smoothness terms (including the case of the used smoothness loss). And, by the way, I did not get how the loss in (9) gives a harmonic function. Could you please clarify and give more details? In my understanding, the harmonic solution in [ Zhu and Ghahramani, ICML 2013] comes directly as a solution of the graph Laplacian (and it assumes some labeled points, i.e., a semi-supervised setting). Even, if the solution is correct (which I do not see how), I do not think it is an efficient way to handle pairwise-regularization problems in image processing, particularly when matrix W = [w_{ij}] is dense (which might be the case here, unless you are truncating the Gaussian kernel with some heuristics). In this case, back-propagating the proposed loss would be of quadratic complexity w.r.t the number of image patches. Again, there is a long tradition in optimizing efficiently pairwise regularizers in vision/learning (even in the case of dense affinity matrices), and one very well-known work, which is currently being used a lot in the context imposing CRF structure on the outputs of deep networks, is [Krahenbuhl and Koltun, Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials], NIPS 2011. This highly related and widely used inference work for dense pairwise regulation is not cited/discussed neither. The Gaussian filtering ideas of the work of Krahenbuhl and Koltun, which ease optimizing dense pairwise terms (from quadratic to linear) are applicable here (as a Gaussian kernel is used), and are widely used in computer vision, including closely related works imposing spatial regularization losses on the outputs of deep networks, e.g., [Tang et al., On Regularized Losses for Weakly-supervised CNN Segmentation, ECCV 18], among many others. When using feature from pre-training (VGG) in the CRF loss, the comparison with unsupervised CycleGAN is not fair. In Table 2 (Label translation on Cityscapes), CycleGAN outperforms the proposed method in all metrics when only unsupervised histogram features are used, which makes me doubt about the practical value of the proposed regularization in the context of image-translation tasks. Having said that, the histogram-based regularization is helping in the medical-imaging application (Table 1). By the way, the use of histograms (of patches or super-pixels) as unsupervised features in pairwise regularization is not new neither; see for instance [Lin et al.: Scribblesup: Scribble-supervised convolutional networks for semantic segmentation, CVPR 2016]. Also, it might be better to use super-pixels instead of patches. So, in summary, the technical contribution is minor, in my opinion (imposing pairwise regularization on the outputs of deep networks has been done in many works, but not for CycleGAN); optimization of the proposed loss as a harmonic function is not clear to me; using VGG in the comparisons with CycleGAN is not fair; and the long history of closely-related spatial regularization terms (e.g., CRFs) in computer vision is completely ignored. Minor: please use \u2018term\u2019 instead of \u2018constraint\u2019. These are unconstrained optimization problems and there are no equality or inequality constraints here. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q1 : This paper adds a spatial regularization loss to the well-known CycleGAN loss for unpaired image-to-image translation ( Zhu et al. , ICCV17 ) . Essentially , the regularization loss ( Eq.6 ) is similar to imposing a CRF ( Conditional Random Field ) term on the network outputs , encouraging spatial consistency between patches within each generated image . Imposing pairwise regularization on the outputs of modern deep networks has been investigated in a very large number of works recently , particularly in the context of weakly-supervised and supervised CNN segmentation , e.g. , Tang et al. , ECCV 18 , Lin et al.CVPR 2016 , Chen et al.ICLR 2015 and Zheng et al. , ICCV 2015 . Very similar in spirit to this ICLR submission , these works impose within-image pairwise regularization ( e.g. , CRF ) on the latent outputs of deep networks , with the main difference that these works use CNN semantic segmentation classifiers whereas here we have a CycleGAN for image generation . The manifold regularization terminology is misleading . The regularization is not over the feature space of image samples . It is within the spatial domain of each generated image ( patch or pixel level ) ; so , in my opinion , CRF ( or spatial ) regularization ( instead of manifold regularization ) is a much more appropriate terminology . A1 : There are some fundamental differences between the CRF literature and our work . They differ in output space , mathematical formulation , application domain , effectiveness , and the role in the overall algorithm . The similarity between CRF and HarmonicGAN lies the adoption of a regularization term : a binary term in the CRF case and a Laplacian term in HarmonicGAN . The differences are detailed below : 1 . Label space vs. feature space The key difference is the explicit graph Laplacian adopted in HarmonicGAN on vectorized representation on all pairs vs. a binary term for the neighboring labels on the scalar representation . HarmonicGAN is indeed formulated in the feature space , not just limited to patches within the single image . The CycleGAN implementation by Zhu et al.happens to include one image only in a batch for computational reason . We follow the standard pipeline of CycleGAN in HarmonicGAN and might have created a confusion here . The description has been clarified in the revised text and we have added citations to the mentioned papers . 2.Mathematical formulation When learning a CRF model , the objective function often combines a unary term and binary term to minimize \\arg \\min_ { w , a } - \\sum_ { i } \\log p ( y_i|X_i ; w ) + \\sum_ { ( i , j ) \\in Neighborhood } a \\log p ( y_i , y_j|X_i , X_j ; w ) where w and a are the parameters in CRF to be learned , and y_i and y_i are SCALAR \\in { 1 , ... , k } for k-class labels . For HarmonicGAN , the objective function includes bidirectional translation having the unary term ( CycleGAN loss ) and binary term . For simplicity we can look at one direction only : \\arg \\min_ { G , F } \\sum_ { i } |F ( G ( X ) ) _i , x_i| + \\sum_ { i , j \\in ImageLattice } w_ { ij } Dist [ F ( y ) ( i ) , F ( y ) ( j ) ] where w_ { i , j } defines the similarity measure and F ( y ) ( i ) computes a feature VECTOR center at i . The key difference lies in the explicit graph Laplacian defined with w_ { ij } for Dist [ F ( y ) ( i ) , F ( y ) ( j ) ] for all pairs whereas p ( y_i , y_j|X_i , X_j ; w ) is a joint probability for the neighboring pixels i and j . In both supervised CRF or weakly-supervised CRF , y_i and y_j are scalars , which are not applicable to the general image translation task for non-labeling tasks since the feature vector space is too high for CRF to model . In addition , the graph Laplacian term in HarmonicGAN is explicitly modeled , which is very different from a joint probability model on the labels ( scalar ) for the neighboring pixels . It is true that HarmonicGAN adopts a smoothness term but so do semi-supervised learning , manifold learning , Markov Random Fields , spectral clustering and normalized cuts , and Laplacian eigenmaps . 3.Application domain CFR models are used in supervised and weakly-supervised image labeling task but HarmonicGAN , like CycleGAN , is applied to the generic image translation tasks where the output is beyond image labels . The reason we show the result on Cityscapes here is twofold : ( 1 ) it is shown in the original CyceleGAN paper and we want to have a direct comparison with , and ( 2 ) the labeling result can have a quantitative measures since the ground-truth labels are available . The family of unpaired image translation tasks can be quite broad , as seen in a number of applications following CycleGAN . ( continued below )"}, {"review_id": "S1M6Z2Cctm-2", "review_text": "Summary: The paper proposes a new smoothness constraint in the original cycle-gan formulation. The cycle-gan formulation minimizes reconstruction error on the input, and there is no criterion other than the adversarial loss function to ensure that it produce a good output (this is in sync with the observations from Gokaslan et al. ECCV'18 and Bansal et al. ECCV'18). A smoothness constraint is defined across random patches in input image and corresponding patches in transformed image. This enables the translation network to preserve edge discontinuities and variation in the output, and leads to better outputs for medical imaging, image to labels task, and horse to zebra and vice versa. Pros: 1. Additional smoothness constraints help in improving the performance over multiple tasks. This constraint is intuitive. 2. Impressive human studies for medical imaging. 3. Improvement in the qualitative results for the shown examples in paper and appendix. Things not clear from the submission: 1. The paper is lacking in technical details: a. what is the patch-size used for RGB-histogram? b. what features or conv-layers are used to get the features from VGG (19?) net? c. other than medical imaging where there isn't a variation in colors of the two domains, it is not clear why RGB-histogram would work? d. the current formulation can be thought as a variant of perceptual loss from Johnson et al. ECCV'16 (applied for the patches, or including pair of patches). In my opinion, implementing via perceptual loss formulation would have made the formulation cleaner and simpler? The authors might want to clarify as how it is different from adding perceptual loss over the pair of patches along with the adversarial loss. One would hope that a perceptual loss would help improve the performance. Also see, Chen and Koltun, ICCV'17. 2. The proposed approach is highly constrained to the settings where structure in input-output does not change. I am not sure how would this approach work if the settings from Gokaslan et al. ECCV'18 were considered (like cats to dogs where the structure changes while going from input to output)? 3. Does the proposed approach also provide temporal smoothness in the output? E.g. Figure-6 shows an example of man on horse being zebrafied. My guess is that input is a small video sequence, and I am wondering if it provides temporal smoothness in the output? The failure on human body makes me wonder that smoothness constraints are helping learn the edge discontinuities. What if the edges of the input (using an edge detection algorithm such as HED from Xie and Tu, ICCV'15) were concatenated to the input and used in formulation? This would be similar in spirit to the formulation of deep cascaded bi-networks from Zhu et al . ECCV'16.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q1 : The paper is lacking in technical details : a. what is the patch-size used for RGB-histogram ? b. what features or conv-layers are used to get the features from VGG ( 19 ? ) net ? A1 : For the RGB histogram , we set the patch size to 8 \\times 8 . For the CNN features , we select the layer 4_3 after ReLU from VGG-16 network . Considering the limited space of ICLR submission , we put the demonstration of implementation details in the appendix ; given that multiple reviewers pointed this out , we 've moved the implementation details to the main paper and expanded the paper to 9 pages . Q2 : Other than medical imaging where there is n't a variation in colors of the two domains , it is not clear why RGB-histogram would work ? A2 : The RGB-histogram for non-medical image cases is still useful as it captures the `` textureness '' of an image patch although it might not be a very rich representation . Based on our experiments , our framework learns translations of changing colors and textures . E.g.for the task of Horse2Zebra ( or Zebra2Horse ) , regions of horse are brown and are expected to be translated to zebra-like texture with black and white stripes . At the same time , the background often shows different appearance for the horse or zebra . Therefore , two patches which are both from the horse or both from the background will have small distance in the RGB histogram , while two patches from horse and background respectively will have larger distance in the RGB histogram . This makes the RGB histogram useful for building a smoothness constraint in the proposed method to improve the translation results . In the task of Label2City , labels are shown with different colormaps , so here again it is reasonable to employ RGB histograms to represent the label patches . However , for the Photos2City , there are some categories which have variable colors and patterns and are not suitable to be represented by a RGB histogram , such as cars and humans . Therefore , using the RGB histogram may be damaging for the diversity of these categories , and this is why the RGB histogram shows a little lower performance than standard CycleGAN in Table 2 . Q3 : the current formulation can be thought as a variant of perceptual loss from Johnson et al.ECCV'16 ( applied for the patches , or including pair of patches ) . In my opinion , implementing via perceptual loss formulation would have made the formulation cleaner and simpler ? The authors might want to clarify as how it is different from adding perceptual loss over the pair of patches along with the adversarial loss . One would hope that a perceptual loss would help improve the performance . Also see , Chen and Koltun , ICCV'17 . A3 : The proposed smoothness term has a great difference compared with perceptual loss . A key and one-sentence summary would be : the perceptual loss preserves the ABSOLUTE high-level feature values for A pattern before and after the translation ( therefore effective in style transfer to preserve the content part ) whereas HarmonicGAN preserves the DIFFERENCE/DISTANCE of a PAIR of patterns before and after the translation . Perceptual loss is proposed for the style transfer task . It forces the result to maintain the content of the content target and preserve the style of the style target . Perceptual loss includes two parts , for content and style respectively , formulated as : content perceptual loss : L_ { content } ( x , y ) = ||\\phi_j ( y ) - \\phi_j ( x ) ||^2_2 / ( C_j H_j W_j ) , style perceptual loss : L_ { style } ( x , y ) = || G_j ( y ) - G_j ( x ) ||^2_F , where \\phi_j represents the activations of the jth layer in a pre-trained network ( e.g.VGG-Net ) , and C_j , H_j , W_j are the channel , height , width of jth layer , G_j represents the Gram matrix computed on the jth layer . Therefore , perceptual loss enforces the output y to reconstruct the feature of the Gram matrix of the input x . In contrast , the proposed smoothness term in HarmonicGAN aims to provide similarity-consistency between image patches during the translation , formulated in Eq.6 , 7 , 8.The smoothness term is designed to build a graph Laplacian on all pairs of image patches , and the smoothness constraint preserves the overall integrity of the translation from the manifold learning perspective , rather than reconstructing the input sample directly . In addition , although the smoothness constraint in HarmonicGAN is measured on the features of each patches , including a RGB histogram or CNN features , it is not suitable to treat the smoothness constraint as a variant of perceptual loss : the CNN feature is only a kind of representation of image patches , not a major design part of the smoothness constraint . Other methods of representing image patches could also be employed in the smoothness constraint , such as RGB histogram . ( continued below )"}], "0": {"review_id": "S1M6Z2Cctm-0", "review_text": "This paper proposes a method called HarmonicGAN for unpaired image-to-image translation. The key idea is to introduce a regularization term on the basis of CycleGAN, which encourages similar image patches to acquire similar transformations. Two feature domains are explored for evaluating the patch-level similarity, including soft RGB histogram and semantic features based on VGGNet. In fact, the key idea is very similar to that of DistanceGAN. The proposed method can be regarded as a combination of the advantages of DistanceGAN and CycleGAN. Thus, the technical novelty is very limited in my opinion. Some experimental results are provided to demonstrate the superiority of the proposed method over CycleGAN, DistanceGAN and UNIT. Given the limited novelty and the inadequate number of experiments, I am leaning to reject this submission. Major questions: 1. Lots of method details are missing. In Section 3.3.2, what layers are chosen for computing the semantic features? What exactly is the metric for computing the distance between semantic features. 2. The qualitative results on the task, Horse2Zebra and Zebra2Horse, are not impressive. Obvious artifacts can be observed in the results. Although the paper claims that the proposed method does not change the background and performs more complete transformations, the background is changed in the result for the Horse2Zebra case in Fig. 5. More qualitative results are needed to demonstrate the effectiveness of the proposed method. 3. To demonstrate the effectiveness of a general unpaired image-to-image translation method, the proposed method is needed to be testified on more tasks. 4. Implementation details are missing. I am not able to judge whether the comparisons are fair enough. [New comment:] I have read the authors' explanations and clarifications that make me increase my rating. Regarding the technical novelty, I still don't think this paper bears sufficient stuff. If there is extra quota, I would recommend Accept. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q1 : The key idea of this paper is very similar to that of DistanceGAN . The proposed method can be regarded as a combination of the advantages of DistanceGAN and CycleGAN . A1 : There is a large difference between DistanceGAN and the proposed HarmonicGAN . First , DistanceGAN already included the CycleGAN loss . Second , DistanceGAN is about preserving the AVERAGED distance between the sample pairs from the source to the target domain , which is not sufficient to retain the underlying integrity and manifold structure . Next , we elaborate the key difference between DistanceGAN and HarmonicGAN . DistanceGAN encourages the distance of samples to be close to an ABSOLUTE MEAN during translation . In contrast , HarmonicGAN enforces a smoothness term naturally under the graph Laplacian , making the motivations of DistanceGAN and HarmonicGAN quite different . In more detail , the distance constraint in DistanceGAN uses the expectation of the absolute differences between the distances in each domain , formulated as : L_ { distance } ( G , X ) = E_ { x_i , x_j \\in X } \\left| ( || x_i - x_j || - \\mu_X ) / \\sigma_X + ( || G ( x_i ) - G ( x_j ) || - \\mu_Y ) / sigma_Y \\right| , where \\mu_X , \\mu_Y ( \\sigma_X , \\sigma_Y ) are the precomputed means ( standard deviations ) of pairwise distances in the training sets from domain X and Y . This distance preserving is interesting but not strong enough to preserve the manifold structure . We suspect that it is probably the reason for DistanceGAN not performing well , as seen in the qualitative and quantitative measures . Differently , HarmonicGAN introduces a smoothness constraint to provide similarity-consistency between image patches during the translation . The smoothness term defines a graph Laplacian with the minimal value achieved as a harmonic function . We define the set consisting of individual image patches as the nodes of the graph , and define the affinity measure ( similarity ) computed on image patches as the edges of the graph . The smoothness term acts as a graph Laplacian imposed on all pairs of image patches . For the translation from X to Y , the smoothness constraint is formulated as : L_ { smooth } ( G , X , Y ) = E_ { { \\bf x } \\in X } \\big [ \\sum_ { i , j } w_ { ij } ( X ) \\times Dist [ G ( \\vec { x } ) ( i ) , G ( \\vec { x } ) ( j ) ] + \\sum_ { i , j } w_ { ij } ( G ( X ) ) \\times Dist [ F ( G ( \\vec { x } ) ) ( i ) , F ( G ( \\vec { x } ) ) ( j ) ] } \\big ] where w_ { ij } ( X ) = \\exp_ { - Dist [ \\vec { x } ( i ) , \\vec { x } ( j ) ] / \\sigma^2 } defines the affinity between the two patches \\vec { x } ( i ) and \\vec { x } ( j ) . Additionally , the similarity of a pair of patches is measured on the features of each patch , e.g.Histogram or CNN features . Comparing the distance constraint in DistanceGAN and the smoothness constraint in HarmonicGAN , we can conclude the following main three differences between them : ( 1 ) They show different motivations and formulations . Most importantly , the loss term in DistanceGAN essentially matches the distance of sample pairs in the source domain to the AVERAGED distance in the target domain ; it is not about preserving the distance of the individual sample pairs . From a manifold learning point of view , preserving the averaged distance is not sufficient for preserving the underlying manifold structure . In contrast , the smoothness constraint in our method is designed from a graph Laplacian to build the similarity-consistency between image patches . Thus , the smoothness constraint uses the affinity between two patches as weight to measure the similarity-consistency between two domains . Our approach is in the vein of manifold learning . The smoothness term defines a Laplacian \\Delta = D - W , where W is our weight matrix and D is a diagonal matrix with D_ { i } = \\sum_j w_ { ij } , thus , the smoothness term defines a graph Laplacian with the minimal value achieved as a harmonic function . ( 2 ) They are different in implementation . The smoothness term in HarmonicGAN is computed on image patches while the distance term in DistanceGAN is computed for the average . Therefore , the smoothness constraint is more fine-grained compared to the distance preserving term in DistanceGAN . Moreover , the distances in DistanceGAN is directly computed from the samples in each domain . They scale the distances with the precomputed means and stds of two domains to reduce the effect of gap between two domains . Differently , the smoothness constraint in HarmonicGAN is measured on the features ( Histogram or CNN features ) of each patches , which maps samples in two domains into the same feature space and removes the gap between two domains . ( continued below )"}, "1": {"review_id": "S1M6Z2Cctm-1", "review_text": "This paper adds a spatial regularization loss to the well-known CycleGAN loss for unpaired image-to-image translation (Zhu et al., ICCV17). Essentially, the regularization loss (Eq. 6) is similar to imposing a CRF (Conditional Random Field) term on the network outputs, encouraging spatial consistency between patches within each generated image. The paper is clear and well written. Unpaired Image-to-Image translation is an important problem. The way the smoothness loss (Eq. 6) is presented gives readers the impression that spatial pairwise regularization is new, ignoring its long history (e.g., CRFs) in computer vision (not a single classical paper on CRFs is cited). Putting aside classical spatial regularization works, imposing pairwise regularization on the outputs of modern deep networks has been investigated in a very large number of works recently, particularly in the context of weakly-supervised semantic CNN segmentation, e.g., [Tang et al., On Regularized Losses for Weakly-supervised CNN Segmentation, ECCV 18 ], [Lin et al. : Scribblesup: Scribble-supervised convolutional networks for semantic segmentation, CVPR 2016], among many other works. Very similar in spirit to this ICLR submission, these works impose within-image pairwise regularization (e.g., CRF) on the latent outputs of deep networks, with the main difference that these works use CNN semantic segmentation classifiers whereas here we have a CycleGAN for image generation. Also, in the context of supervised CNN segmentation, CRFs have made a significant impact when used as post-processing step, e.g., very well known works such as [DeepLab by Chen et al. ICLR15] and [CRFs as recurrent Neural Networks by Zheng et al., ICCV 2015]. It might be a valid contribution to evaluate spatial regularization (e.g., CRFs losses) on image generation tasks (such as CycleGAN), but the paper really needs to acknowledge very related prior works on regularization (at least in the context of deep networks). There are also related pioneering semi-supervised deep learning works based on graph Laplacian regularization, e.g., [Westen et al., Deep Learning via Semi-supervised embedding, ICML 2008], which the paper does not acknowledge/discuss. The manifold regularization terminology is misleading. The regularization is not over the feature space of image samples. It is within the spatial domain of each generated image (patch or pixel level); so, in my opinion, CRF (or spatial) regularization (instead of manifold regularization) is a much more appropriate terminology. Also, I would not call this approach HarmonicGan. I would call it CRF-GAN or Spatially-Regularized GAN. The computation of harmonic functions is just one way, among many other (potentially better) ways to optimize pairwise smoothness terms (including the case of the used smoothness loss). And, by the way, I did not get how the loss in (9) gives a harmonic function. Could you please clarify and give more details? In my understanding, the harmonic solution in [ Zhu and Ghahramani, ICML 2013] comes directly as a solution of the graph Laplacian (and it assumes some labeled points, i.e., a semi-supervised setting). Even, if the solution is correct (which I do not see how), I do not think it is an efficient way to handle pairwise-regularization problems in image processing, particularly when matrix W = [w_{ij}] is dense (which might be the case here, unless you are truncating the Gaussian kernel with some heuristics). In this case, back-propagating the proposed loss would be of quadratic complexity w.r.t the number of image patches. Again, there is a long tradition in optimizing efficiently pairwise regularizers in vision/learning (even in the case of dense affinity matrices), and one very well-known work, which is currently being used a lot in the context imposing CRF structure on the outputs of deep networks, is [Krahenbuhl and Koltun, Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials], NIPS 2011. This highly related and widely used inference work for dense pairwise regulation is not cited/discussed neither. The Gaussian filtering ideas of the work of Krahenbuhl and Koltun, which ease optimizing dense pairwise terms (from quadratic to linear) are applicable here (as a Gaussian kernel is used), and are widely used in computer vision, including closely related works imposing spatial regularization losses on the outputs of deep networks, e.g., [Tang et al., On Regularized Losses for Weakly-supervised CNN Segmentation, ECCV 18], among many others. When using feature from pre-training (VGG) in the CRF loss, the comparison with unsupervised CycleGAN is not fair. In Table 2 (Label translation on Cityscapes), CycleGAN outperforms the proposed method in all metrics when only unsupervised histogram features are used, which makes me doubt about the practical value of the proposed regularization in the context of image-translation tasks. Having said that, the histogram-based regularization is helping in the medical-imaging application (Table 1). By the way, the use of histograms (of patches or super-pixels) as unsupervised features in pairwise regularization is not new neither; see for instance [Lin et al.: Scribblesup: Scribble-supervised convolutional networks for semantic segmentation, CVPR 2016]. Also, it might be better to use super-pixels instead of patches. So, in summary, the technical contribution is minor, in my opinion (imposing pairwise regularization on the outputs of deep networks has been done in many works, but not for CycleGAN); optimization of the proposed loss as a harmonic function is not clear to me; using VGG in the comparisons with CycleGAN is not fair; and the long history of closely-related spatial regularization terms (e.g., CRFs) in computer vision is completely ignored. Minor: please use \u2018term\u2019 instead of \u2018constraint\u2019. These are unconstrained optimization problems and there are no equality or inequality constraints here. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q1 : This paper adds a spatial regularization loss to the well-known CycleGAN loss for unpaired image-to-image translation ( Zhu et al. , ICCV17 ) . Essentially , the regularization loss ( Eq.6 ) is similar to imposing a CRF ( Conditional Random Field ) term on the network outputs , encouraging spatial consistency between patches within each generated image . Imposing pairwise regularization on the outputs of modern deep networks has been investigated in a very large number of works recently , particularly in the context of weakly-supervised and supervised CNN segmentation , e.g. , Tang et al. , ECCV 18 , Lin et al.CVPR 2016 , Chen et al.ICLR 2015 and Zheng et al. , ICCV 2015 . Very similar in spirit to this ICLR submission , these works impose within-image pairwise regularization ( e.g. , CRF ) on the latent outputs of deep networks , with the main difference that these works use CNN semantic segmentation classifiers whereas here we have a CycleGAN for image generation . The manifold regularization terminology is misleading . The regularization is not over the feature space of image samples . It is within the spatial domain of each generated image ( patch or pixel level ) ; so , in my opinion , CRF ( or spatial ) regularization ( instead of manifold regularization ) is a much more appropriate terminology . A1 : There are some fundamental differences between the CRF literature and our work . They differ in output space , mathematical formulation , application domain , effectiveness , and the role in the overall algorithm . The similarity between CRF and HarmonicGAN lies the adoption of a regularization term : a binary term in the CRF case and a Laplacian term in HarmonicGAN . The differences are detailed below : 1 . Label space vs. feature space The key difference is the explicit graph Laplacian adopted in HarmonicGAN on vectorized representation on all pairs vs. a binary term for the neighboring labels on the scalar representation . HarmonicGAN is indeed formulated in the feature space , not just limited to patches within the single image . The CycleGAN implementation by Zhu et al.happens to include one image only in a batch for computational reason . We follow the standard pipeline of CycleGAN in HarmonicGAN and might have created a confusion here . The description has been clarified in the revised text and we have added citations to the mentioned papers . 2.Mathematical formulation When learning a CRF model , the objective function often combines a unary term and binary term to minimize \\arg \\min_ { w , a } - \\sum_ { i } \\log p ( y_i|X_i ; w ) + \\sum_ { ( i , j ) \\in Neighborhood } a \\log p ( y_i , y_j|X_i , X_j ; w ) where w and a are the parameters in CRF to be learned , and y_i and y_i are SCALAR \\in { 1 , ... , k } for k-class labels . For HarmonicGAN , the objective function includes bidirectional translation having the unary term ( CycleGAN loss ) and binary term . For simplicity we can look at one direction only : \\arg \\min_ { G , F } \\sum_ { i } |F ( G ( X ) ) _i , x_i| + \\sum_ { i , j \\in ImageLattice } w_ { ij } Dist [ F ( y ) ( i ) , F ( y ) ( j ) ] where w_ { i , j } defines the similarity measure and F ( y ) ( i ) computes a feature VECTOR center at i . The key difference lies in the explicit graph Laplacian defined with w_ { ij } for Dist [ F ( y ) ( i ) , F ( y ) ( j ) ] for all pairs whereas p ( y_i , y_j|X_i , X_j ; w ) is a joint probability for the neighboring pixels i and j . In both supervised CRF or weakly-supervised CRF , y_i and y_j are scalars , which are not applicable to the general image translation task for non-labeling tasks since the feature vector space is too high for CRF to model . In addition , the graph Laplacian term in HarmonicGAN is explicitly modeled , which is very different from a joint probability model on the labels ( scalar ) for the neighboring pixels . It is true that HarmonicGAN adopts a smoothness term but so do semi-supervised learning , manifold learning , Markov Random Fields , spectral clustering and normalized cuts , and Laplacian eigenmaps . 3.Application domain CFR models are used in supervised and weakly-supervised image labeling task but HarmonicGAN , like CycleGAN , is applied to the generic image translation tasks where the output is beyond image labels . The reason we show the result on Cityscapes here is twofold : ( 1 ) it is shown in the original CyceleGAN paper and we want to have a direct comparison with , and ( 2 ) the labeling result can have a quantitative measures since the ground-truth labels are available . The family of unpaired image translation tasks can be quite broad , as seen in a number of applications following CycleGAN . ( continued below )"}, "2": {"review_id": "S1M6Z2Cctm-2", "review_text": "Summary: The paper proposes a new smoothness constraint in the original cycle-gan formulation. The cycle-gan formulation minimizes reconstruction error on the input, and there is no criterion other than the adversarial loss function to ensure that it produce a good output (this is in sync with the observations from Gokaslan et al. ECCV'18 and Bansal et al. ECCV'18). A smoothness constraint is defined across random patches in input image and corresponding patches in transformed image. This enables the translation network to preserve edge discontinuities and variation in the output, and leads to better outputs for medical imaging, image to labels task, and horse to zebra and vice versa. Pros: 1. Additional smoothness constraints help in improving the performance over multiple tasks. This constraint is intuitive. 2. Impressive human studies for medical imaging. 3. Improvement in the qualitative results for the shown examples in paper and appendix. Things not clear from the submission: 1. The paper is lacking in technical details: a. what is the patch-size used for RGB-histogram? b. what features or conv-layers are used to get the features from VGG (19?) net? c. other than medical imaging where there isn't a variation in colors of the two domains, it is not clear why RGB-histogram would work? d. the current formulation can be thought as a variant of perceptual loss from Johnson et al. ECCV'16 (applied for the patches, or including pair of patches). In my opinion, implementing via perceptual loss formulation would have made the formulation cleaner and simpler? The authors might want to clarify as how it is different from adding perceptual loss over the pair of patches along with the adversarial loss. One would hope that a perceptual loss would help improve the performance. Also see, Chen and Koltun, ICCV'17. 2. The proposed approach is highly constrained to the settings where structure in input-output does not change. I am not sure how would this approach work if the settings from Gokaslan et al. ECCV'18 were considered (like cats to dogs where the structure changes while going from input to output)? 3. Does the proposed approach also provide temporal smoothness in the output? E.g. Figure-6 shows an example of man on horse being zebrafied. My guess is that input is a small video sequence, and I am wondering if it provides temporal smoothness in the output? The failure on human body makes me wonder that smoothness constraints are helping learn the edge discontinuities. What if the edges of the input (using an edge detection algorithm such as HED from Xie and Tu, ICCV'15) were concatenated to the input and used in formulation? This would be similar in spirit to the formulation of deep cascaded bi-networks from Zhu et al . ECCV'16.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q1 : The paper is lacking in technical details : a. what is the patch-size used for RGB-histogram ? b. what features or conv-layers are used to get the features from VGG ( 19 ? ) net ? A1 : For the RGB histogram , we set the patch size to 8 \\times 8 . For the CNN features , we select the layer 4_3 after ReLU from VGG-16 network . Considering the limited space of ICLR submission , we put the demonstration of implementation details in the appendix ; given that multiple reviewers pointed this out , we 've moved the implementation details to the main paper and expanded the paper to 9 pages . Q2 : Other than medical imaging where there is n't a variation in colors of the two domains , it is not clear why RGB-histogram would work ? A2 : The RGB-histogram for non-medical image cases is still useful as it captures the `` textureness '' of an image patch although it might not be a very rich representation . Based on our experiments , our framework learns translations of changing colors and textures . E.g.for the task of Horse2Zebra ( or Zebra2Horse ) , regions of horse are brown and are expected to be translated to zebra-like texture with black and white stripes . At the same time , the background often shows different appearance for the horse or zebra . Therefore , two patches which are both from the horse or both from the background will have small distance in the RGB histogram , while two patches from horse and background respectively will have larger distance in the RGB histogram . This makes the RGB histogram useful for building a smoothness constraint in the proposed method to improve the translation results . In the task of Label2City , labels are shown with different colormaps , so here again it is reasonable to employ RGB histograms to represent the label patches . However , for the Photos2City , there are some categories which have variable colors and patterns and are not suitable to be represented by a RGB histogram , such as cars and humans . Therefore , using the RGB histogram may be damaging for the diversity of these categories , and this is why the RGB histogram shows a little lower performance than standard CycleGAN in Table 2 . Q3 : the current formulation can be thought as a variant of perceptual loss from Johnson et al.ECCV'16 ( applied for the patches , or including pair of patches ) . In my opinion , implementing via perceptual loss formulation would have made the formulation cleaner and simpler ? The authors might want to clarify as how it is different from adding perceptual loss over the pair of patches along with the adversarial loss . One would hope that a perceptual loss would help improve the performance . Also see , Chen and Koltun , ICCV'17 . A3 : The proposed smoothness term has a great difference compared with perceptual loss . A key and one-sentence summary would be : the perceptual loss preserves the ABSOLUTE high-level feature values for A pattern before and after the translation ( therefore effective in style transfer to preserve the content part ) whereas HarmonicGAN preserves the DIFFERENCE/DISTANCE of a PAIR of patterns before and after the translation . Perceptual loss is proposed for the style transfer task . It forces the result to maintain the content of the content target and preserve the style of the style target . Perceptual loss includes two parts , for content and style respectively , formulated as : content perceptual loss : L_ { content } ( x , y ) = ||\\phi_j ( y ) - \\phi_j ( x ) ||^2_2 / ( C_j H_j W_j ) , style perceptual loss : L_ { style } ( x , y ) = || G_j ( y ) - G_j ( x ) ||^2_F , where \\phi_j represents the activations of the jth layer in a pre-trained network ( e.g.VGG-Net ) , and C_j , H_j , W_j are the channel , height , width of jth layer , G_j represents the Gram matrix computed on the jth layer . Therefore , perceptual loss enforces the output y to reconstruct the feature of the Gram matrix of the input x . In contrast , the proposed smoothness term in HarmonicGAN aims to provide similarity-consistency between image patches during the translation , formulated in Eq.6 , 7 , 8.The smoothness term is designed to build a graph Laplacian on all pairs of image patches , and the smoothness constraint preserves the overall integrity of the translation from the manifold learning perspective , rather than reconstructing the input sample directly . In addition , although the smoothness constraint in HarmonicGAN is measured on the features of each patches , including a RGB histogram or CNN features , it is not suitable to treat the smoothness constraint as a variant of perceptual loss : the CNN feature is only a kind of representation of image patches , not a major design part of the smoothness constraint . Other methods of representing image patches could also be employed in the smoothness constraint , such as RGB histogram . ( continued below )"}}