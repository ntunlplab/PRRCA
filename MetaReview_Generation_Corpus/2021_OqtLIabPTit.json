{"year": "2021", "forum": "OqtLIabPTit", "title": "Exploring Balanced Feature Spaces for Representation Learning", "decision": "Accept (Poster)", "meta_review": "\n This paper studies the difference between cross-entropy and contrastive learning losses in the feature representations that they learn, specifically looking at class-imbalanced datasets. The authors show that contrastive losses result in a more \"balanced\" representation, as measured by the balance of accuracy across the classes when a linear classifier is learned mapping from the feature representation to the class labels. They also show that empirically this tends to result in better generalization to downstream tasks. Inspired by this, they devise a simple modification of the prior supervised contrastive loss method and show that it can improve performance on ImageNet-LT and even generalization performance when trained on balanced datasets and applied to downstream tasks. \n\n  The reviewers identified several weaknesses, including some clarity issues (R1), limitations of how balancedness is measured and lack of theoretical/statistical rigor in terms of the resulting claims (R2), and differences with respect to concurrent work (R4). A lengthy discussion occurred between reviewers and authors, as well as input from a co-author of the concurrent work. In the end, the reviewers were not fully satisfied both in terms of the balancedness measure and relationship to the concurrent work. \n\n  Overall, despite this and the valid limitations of the work, I recommend accepting this paper as I believe the contributions outweigh the limitations, and that the findings would be interesting to the community. First, the paper provides some interesting analysis of balancedness and differences across these two loss functions, as well as connections to generalization, which even the concurrent work does not provide. The resulting method, while being a simple modification of the supervised contrastive loss work, is effective both for long-tailed datasets and generalization to downstream tasks (even when trained in a balanced manner) which is nice. In the end, we should not use [3] to reject this paper since it was accepted right before the ICLR deadline. \n\n  However, I **strongly** recommend that the paper address the valid limitations mentioned in the discussions. Specifically: \n  1) While I agree that [3] is concurrent work, this paper should none-the-less tone down its claims of being the first in exploring balance for the camera-ready version and clearly address differences between this paper and that one (even if mentioned as concurrent work). It is important to give credit when it is due, and while I think [3] is a different perspective it should be mentioned. Further, the claim that their methodology is not correct is highly arguable, so this should not be mentioned; rather the differences in perspectives and what each paper shows should be emphasized. Even without [3], self-supervised pre-training (initialization) should arguably be included as a baseline given that it is the logical first choice for incorporating self-supervised learning.\n\n  2) Like R2, I do not believe the balancedness metric shows uniformity of the feature space. This would have to be shown through methods such as t-SNE or in some other way. Being linearly separable in a balanced way across classes (which is what you showed) is not sufficient to show that feature space \"uniformity\". One can draw many feature space distributions that do not have the intuitive meaning of this (which isn't precisely defined by the authors) but still be linearly separable. I recommend authors remove this type of characterization (unless they can define/show it) and instead include a discussion of the limitations of the current methodology for measuring balancedness. Figure 1 should also emphasize that it is notional (not from real data). ", "reviews": [{"review_id": "OqtLIabPTit-0", "review_text": "Summary : - This paper made a key observation that the self-supervised contrastive learning methods perform stably well even when the datasets are heavily imbalanced . As for the method this paper proposes a classed-balanced version of supervised contrastive loss ( Khosla et al. , 2020 ) . Extensive experiments demonstrate its superiority on multiple recognition tasks . Pros : - The paper is well written with nice figures . - The paper conducted experiments on extensions other than image classification , which provides some useful insights . Cons : - The advantage of using self-supervised learning to learn a representation to combat label bias issue in imbalanced problems has been discovered and validated by a previous work [ 1 ] . I understand that [ 1 ] was accepted just a few days before the DDL of ICLR . Unfortunately I still have to devalue the contribution of this paper as it seems to me that the major contribution of this paper relies on the finding that SSL can help to alleviate the issue of label bias . In terms of large-scale experiments on ImageNet-LT and iNatualist , it seems that this paper has no advantage over [ 1 ] . - It seems the runtime of the proposed method is much worse than supervised based methods . Additional Questions and Concerns : - Why does the author use cosine lr schedular ? - How to interpret accuracys on imagenet in fig . 4 ? Why is there only 10 classes ? [ 1 ] Yang , Yuzhe , and Zhi Xu . `` Rethinking the Value of Labels for Improving Class-Imbalanced Learning . '' NeurIPS 2020 - post-rebuttal update I appreciate the discussions between the authors . I plan to keep my original score , for the reason that , at least in my point of view , the difference of the two methods is subtle and it is not clear whether the subtle difference results in drastic improvement .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * A1 . * * * [ The main discovery has been discovered and validated by a previous work . ] * - First , our findings are different from the ones in [ 3 ] . The discovery made in [ 3 ] is \u201c the advantage of using self-supervised learning to learn a representation to combat label bias issue in imbalanced problems. \u201d However , our finding is : * * unsupervised contrastive loss is able to learn balanced feature spaces , which is not only a property that benefits long-tailed recognition but also gives better generalization ability for normal representation learning models . * * As evidenced in Sec.4.3 , when testing on fully balanced datasets ( e.g.ImageNet ) , a more balanced feature produced by our proposed KCL method beats supervised learning on downstream tasks . None of these have been discussed in [ 3 ] . - Second , we would like to provide a more detailed discussion of [ 3 ] . [ 3 ] draws the conclusion that SSL pretraining is able to overcome label bias issues mainly by the following contrast experiments . They pretrain a model using SSL then further train it using supervised learning , resulting in a final model ( refered as ssl model in the following ) . Then the ssl model is directly compared to a baseline model , which is directly trained with supervised learning . The ssl model is trained with longer time ( 200 epochs with ssl loss , and 90 epochs with CE loss ) , than the baseline model ( 90 epochs with CE ) . Therefore , * * the performance improvement observed by [ 3 ] may come from longer training , since training longer is shown to be very effective for long-tailed recognition in [ 4 ] . * * To exclude such a factor and make a fair comparison , we conduct experiments with two additional baselines : the \u201c ssl model \u201d is firstly trained with supervised learning for 90 epochs , then further trained with supervised learning for another 90 epochs . ; another baseline model is simply trained for a longer time ( 200 epochs ) . The results are given as follows , |Method | Many | Medium | Few | All | | - | - | -- | -- | -| |Baseline model ( 90 epochs ) | 59.735 | 45.113 | 27.082 | 48.110 | |Baseline model ( 200 epochs ) | 61.382 | 48.486 | 32.890 | 51.174 | | ssl model ( 200+90 epochs ) | 60.670 | 48.392 | 31.493 | 50.652 | |sl model ( 90+90 epochs ) | 60.078 | 47.684 | 31.082 | 50.032 | Note that all results are given using tau-normalization proposed in [ 4 ] . * * The results clearly show that ssl pretraining gives no additional benefits compared to supervised learning . * * Moreover , we also tried to train a model by optimizing supervised loss and SSL loss together . The results show that the learned model is negatively affected as explained in answer 4 to reviewer # 1 . * * All these results together prove that our proposed KCL loss is the first successful practice to combine the benefits of supervised learning and self-supervised learning . * * - Third , besides the above discovery , we also make the following contributions which should not be neglected : ( 1 ) We are the first to show the performance of SSL is robust to dataset imbalance by studying SSL on various imbalanced datasets . ( 2 ) We propose a new concept of \u201c balancedness \u201d for feature space and demonstrate such property is important for both classification performance and generalization to different distributions and tasks . ( 3 ) The proposed method , KCL , is novel . It learns balanced and discriminative features , and outperforms CL and SL on both long-tailed datasets ( Sec.4.2 ) and balanced datasets ( Sec.4.3 ) .We sincerely hope our contributions can be fairly treated . [ 3 ] Yang , Yuzhe , and Zhi Xu . `` Rethinking the Value of Labels for Improving Class-Imbalanced Learning . '' NeurIPS 2020 [ 4 ] Kang , Bingyi , et al . `` Decoupling representation and classifier for long-tailed recognition . '' ICLR 2020 . * * A2 . * * * [ The runtime of the proposed method is much worse than supervised based methods . ] * The proposed method needs longer training epochs because the contrastive loss takes much longer to converge , as explained in Appendix A . The same criticism can be directed to all the SSL methods in the literature , including [ 3 ] . However , our method enjoys the advantage of learning balanced and discriminative features , which is demonstrated to be crucial for imbalanced classification problems . * * A3 . * * * [ Why cosine lr schedule ? ] * For MoCo , we use the original step decay strategy . For our proposed KCL method , we adopt the same lr scheduler used in supervised learning for a fair comparison , as explained in Appendix A . * * A4 . * * * [ How to interpret accuracies on imagenet in fig . 4 ? Why are there only 10 classes ? ] * We will add y-axis in the next version for Fig.4 . Actually , there are 1000 classes . For clearer visualization , we divide them into 10 bins according to their training instance numbers . Each dot in the figure represents the average accuracy of the corresponding bin ."}, {"review_id": "OqtLIabPTit-1", "review_text": "* * Overview : * * The paper presents experiments showing that the contrastive learning losses produce better embeddings or feature spaces than those produced by using binary cross-entropy losses . The experiments show that embeddings learned using contrastive learning losses seem to favor long-tailed learning tasks , out-of-distribution tasks , and object detection . The paper also presents an extension of the contrastive loss to improve the embeddings . The experiments in the paper use common and recent long-tail datasets as well as datasets for object detection and out-of-distribution tasks . * * Pros * * : * Interesting problem and approach * . I think the paper tackles a hard and important problem , i.e. , learning from a long-tailed dataset . Overall , I think that learning a feature space improving the learning from these imbalanced datasets is an interesting idea . * Clarity of the paper * . Overall the clarity of the paper is good . The motivation is clear and the narrative is clear overall . However , I think the clarity in the experiments is insufficient , see below . * * Cons * * : * Insufficient clarity in the experiments * . I have several concerns with the experiments : 1 . The * balancedness * metric in Eq . ( 3 ) may not be a robust metric for measure performance . The reason I am not convinced about this metric is that if the accuracies of the classifier are low but equal , then the metric will say that the * balancedness * is good . I think a good metric for a classifier learning from an imbalanced dataset is one that indicates if the overall accuracy is high , maintains the many-shot the classification accuracy high , and increases the accuracy of the classes in the tail . I think this metric does not indicate if the overall accuracy is high . 2.I am not convinced about how classifiers are trained in experiments in Sec.3.2.The paper trains and tests using a balanced set after learning a feature space . To my understanding , the challenge of learning from a long-tailed dataset is to test whether a classifier can generalize well for classes with few training samples while maintaining a good performance on classes with more training examples . Thus , by training a linear classifier with a balanced set using the learned feature space does not really comply with learning from an imbalanced dataset . I think if the experiments would 've been stronger if they included results of a trained linear classifier on the learned embedding and still showing good results , then I would be more convinced about the impact of a contrastive loss . From the practical point of view , what matters is the classifier performance . In practice , it is challenging to have a balanced dataset as the paper used . The main question is about the performance when training a linear classifier from a long-tailed dataset using the learned representation . 3.Datasets derived from ImageNet-LT . While I value the goal of using different datasets varying the imbalance in a dataset , I am not convinced that ImageNet-LT is the dataset to use . The reason is that ImageNet-LT is a synthetic long-tailed dataset . In fact , while the dataset shows imbalance , it does not necessarily follow a power-law distribution . I think the generation of these datasets in the experiments should be done using a power-law distribution . From the text , it is unclear how the datasets from ImageNet-LT were generated for the experiments . Minor concerns : * Plots lack information * . What is the y-scale in Fig.2 ? The figure is missing y-scale information and it is hard to interpret the gap in accuracy between CE and CL in the left plot . Same comment for Fig 4 , what is the scale in y-axis ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We clarify our experiment settings and results here and welcome further discussions . The reviewer might misunderstand one of our core contributions , the \u201c balancedness \u201d of a feature space . * * Balancedness defined in Eqn . ( 3 ) is not used for evaluating the performance of a classifier but for measuring the quality of a feature space , i.e. , whether it is skewed to some dominant classes and leads to biased classification accuracy . * * This property is found by our extensive empirical studies on feature representation learning in various practical settings . When two feature spaces share similar discriminativeness , the more balanced one gives better generalization ability . This conclusion not only holds for long-tailed recognition ( Sec.4.2 ) , but is also applicable to the normal balanced datasets ( Sec.4.3 ) .So instead of only focusing on learning \u201c discriminative \u201d feature space as in previous works , we propose \u201c balancedness \u201d as another important property for a good feature space and we suggest learning both discriminative and balanced feature spaces . * * A1 . * * * [ The balancedness metric may not be a robust metric for measuring performance . ] * The reviewer might misunderstand the purpose of developing the balancedness metric as explained above . We would like to emphasize that the balancedness metric is only used to measure feature spaces from a new perspective , which is a complement to usual performance measures ( e.g. , accuracy ) , but not a replacement . We will explicitly clarify it in the revision . * * A2 . * * * [ I am not convinced about how classifiers are trained in experiments in Sec.3.2 . ] * We agree with the reviewer on the evaluation of long-tailed classifiers and we have presented the experiments in Sec.4.2.But * * the reviewer misunderstands the study in Sec 3.2 , whose purpose is to investigate how different loss functions ( e.g. , CE and CL ) affects the balancedness of learned embeddings , not to study the long-tailed recognition performance * * , as explained multiple times in the introduction and Sec.3.2 , * * < Experimental setting is Sec.3.2 is reasonable > * * To evaluate feature spaces from different losses , we first train several representation models with the six imbalanced datasets . However , the properties ( balancedness and discriminativeness ) of a feature space relies on the classification accuracies of learned classifiers . \u201c Since linear classifiers are easily biased by skewed training dataset distribution , it is necessary to eliminate the imblancedness of the evaluation datasets for reliable representation evaluation . Thus , we use the ( balanced ) training and test sets of ImageNet to learn classifiers and evaluate their classification accuracy. \u201d We have explained this in the first paragraph of Sec.3.2 . * * < Long-tailed classifiers are evaluated by exactly following the setting you explained here > * * For detailed settings and results for long-tailed recognition , please refer to Sec.4.2 . It clearly shows that our proposed KCL loss substantially outperforms existing SOTAs , e.g. , bringing 4.2 absolute points improvement on ImageNet-LT . * * A3 . * * * [ Datasets do not follow a power-law distribution . ] * The ImageNet-LT dataset is generated following a power-law distribution as explained in [ 2 ] . Therefore , based on the equation generating our datasets ( Eqn . ( 5 ) ) , it is clear that our datasets also follow power-law distributions . We also mention the datasets are generated by power law in the supplementary material . [ 2 ] Liu , Ziwei , et al . `` Large-scale long-tailed recognition in an open world . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2019. * * A4 . * * * [ missing y-axis in Fig.2 and Fig.4 ] * The y-axis is removed for clarity as we care more about the relative value . The precise values are all provided in tables . We will add a y-axis in the revision ."}, {"review_id": "OqtLIabPTit-2", "review_text": "In this paper , the authors propose a new loss function to learn feature representations for image datasets that are class-imbalanced . The loss function is a simple yet effective tweak on an existing supervised contrastive loss work . A number of empirical tests are performed on long-tailed datasets showing the benefits of the proposed loss in beating state of the art methods . Some specific questions are listed below : 1 . Is Figure 1 hypothetical ( `` desired outcome '' ) or real ( based on actual observations ) ? 2.Minor style comment : please do n't call your own contributions `` important '' : - ) - that is for others to decide . 3.Eqn . ( 3 ) is not clear at all - please provide an intuitive explanation and motivation . It seems to appear out of thin air . 4.Is there a tradeoff between accuracy and balancedness ? Figure 2 seems to suggest so . For example , if we did not train CE loss to maximal accuracy , would be automatically get the balancedness property ? Was this tested ? How about a combined loss CE + CL to get the best of both worlds ? And similarly , how about CE + KCL ? 5.Table 1 the delta for VOC seems to be computed wrongly . 6.Figure 7 : it 's strange that there are no numbers on the y-axis . 7.What is the ImageNet accuracy of KCL ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * A1 . * * Figure 1 is a hypothetical illustration of the feature spaces learned with different loss functions . However , this illustration is strongly evidenced by the experimental results on real large-scale datasets . As shown in Fig.4 ( right ) : - SL ( using the CE loss ) gives promising overall performance but heavily skewed accuracy curve , indicating the learned feature space is discriminative for different classes but biased to the head-classes ; - CL ( MoCo ) gives the poorest overall performance but a balanced accuracy curve , i.e. , the learned feature space is well balanced but not so discriminative . - Our proposed KCL performs the best , presenting the strengths of both CE and CL . This implies its learned feature space is both balanced and discriminative . * * A2 . * * Noted with thanks . We will revise the paper accordingly . * * A3 . * * Eqn . ( 3 ) defines a measure of the balancedness of a feature space based on the following intuition . As explained in the context of Eqn . ( 3 ) , a feature space is balanced if the linear classifier trained on its sample features does not give biased performance to any classes , i.e. , the classifier gives balanced ( similar ) accuracy on all the classes ( \u201c similar degrees of linear separability \u201d ) . Since balancedness ( similarity ) of a set of values can be computed by their pairwise Gaussian distance [ 1 ] , we use Eqn . ( 3 ) to compute the balancedness of the class-wise accuracy of a linear classifier and take it to measure the feature space balancedness . We will make this clear and add a more intuitive explanation in the revision . [ 1 ] Wang , Tongzhou , and Phillip Isola . `` Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere . '' arXiv preprint arXiv:2005.10242 ( 2020 ) . * * A4 . * * * [ Is there a tradeoff between accuracy and balancedness ? ] * We would like to clarify that we are proposing balancedness as a complementary property to discriminativeness ( or classification accuracy ) for representation learning . There is no trade-off between them . More detailed explanations are given below . * [ More explanation for Fig.2 . ] * For Fig.2 , by comparing the performance ( left ) and balancedness ( right ) curves given by CE loss ( green curves ) , we can observe that the performance increases as balancedness improves , which demonstrates a positive correlation , instead of trade-off , between accuracy and balancedness . When comparing CE loss with CL loss , CE gives high accuracy but low balancedness , while CL gives poor accuracy but strong balancedness . But this does not imply there is a trade-off between accuracy and balancedness because these two loss functions focus on different aspects of feature learning ( one for discriminativeness , the other one for balancedness ) . Therefore , it is possible to optimize discriminativeness and balancedness simultaneously , which motivates us to propose the KCL loss . * [ Would a random model automatically get the balancedness property ? ] * Yes , it is possible that a random classification model gives optimal balancedness property . Based on Eqn . ( 3 ) , all zero accuracies give the largest balancedness score . However , please note that balancedness is a complementary property to discriminativeness . * [ How about a combined loss CE + CL ? ] * We have tried to train a model with a combined loss CE + $ \\lambda $ CL with $ \\lambda $ as the weighting factor . * * Our experiments reveal that directly optimizing these two losses gives sub-optimal results . * * When the CL term weighting factor increases ( learned more balanced representations ) , accuracy decreases . The experimental results on ImageNet-LT are given as follows | $ \\lambda $ | Many | Medium | Few | All | | | -- | | -- | - | | 0.0 | 59.735 | 45.113 | 27.082 | 48.110 | | 0.3 | 59.326 | 44.142 | 28.408 | 47.744 | | 1.0 | 58.169 | 42.151 | 24.462 | 45.744 | However , this does not mean that balancedness and good performance can not be achieved at the same time . Actually , KCL is motivated by achieving these two properties simultaneously . * * The above results show this target is non-trivial , further validating the novelty and necessity of our KCL loss . * * * [ How about a combined loss CE + KCL ? ] * The results can be inferred from the CE and KCL results . ( 1 ) KCL is designed to combine the strengths of CE and CL , which gives both good balancedness and accuracy . ( 2 ) KCL + CE loss will achieve the performance lying between CE and KCL results . * * A5 . * * Thank you for pointing it out . We will address it in the revision . * * A6 . * * There is no Fig.7 in our paper . The reviewer may refer to Fig.2 and Fig.4 . We remove the y-axis value for more clearly showing the relative difference of different methods . The actual values of Fig.4 are all provided in Table 2 . We will add y-axis values in the revision . * * A7 . * * KCL achieves an accuracy of 76.814 on ImageNet , as given in Table 5 , which is comparable with the result of CE ( 76.616 ) ."}], "0": {"review_id": "OqtLIabPTit-0", "review_text": "Summary : - This paper made a key observation that the self-supervised contrastive learning methods perform stably well even when the datasets are heavily imbalanced . As for the method this paper proposes a classed-balanced version of supervised contrastive loss ( Khosla et al. , 2020 ) . Extensive experiments demonstrate its superiority on multiple recognition tasks . Pros : - The paper is well written with nice figures . - The paper conducted experiments on extensions other than image classification , which provides some useful insights . Cons : - The advantage of using self-supervised learning to learn a representation to combat label bias issue in imbalanced problems has been discovered and validated by a previous work [ 1 ] . I understand that [ 1 ] was accepted just a few days before the DDL of ICLR . Unfortunately I still have to devalue the contribution of this paper as it seems to me that the major contribution of this paper relies on the finding that SSL can help to alleviate the issue of label bias . In terms of large-scale experiments on ImageNet-LT and iNatualist , it seems that this paper has no advantage over [ 1 ] . - It seems the runtime of the proposed method is much worse than supervised based methods . Additional Questions and Concerns : - Why does the author use cosine lr schedular ? - How to interpret accuracys on imagenet in fig . 4 ? Why is there only 10 classes ? [ 1 ] Yang , Yuzhe , and Zhi Xu . `` Rethinking the Value of Labels for Improving Class-Imbalanced Learning . '' NeurIPS 2020 - post-rebuttal update I appreciate the discussions between the authors . I plan to keep my original score , for the reason that , at least in my point of view , the difference of the two methods is subtle and it is not clear whether the subtle difference results in drastic improvement .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * A1 . * * * [ The main discovery has been discovered and validated by a previous work . ] * - First , our findings are different from the ones in [ 3 ] . The discovery made in [ 3 ] is \u201c the advantage of using self-supervised learning to learn a representation to combat label bias issue in imbalanced problems. \u201d However , our finding is : * * unsupervised contrastive loss is able to learn balanced feature spaces , which is not only a property that benefits long-tailed recognition but also gives better generalization ability for normal representation learning models . * * As evidenced in Sec.4.3 , when testing on fully balanced datasets ( e.g.ImageNet ) , a more balanced feature produced by our proposed KCL method beats supervised learning on downstream tasks . None of these have been discussed in [ 3 ] . - Second , we would like to provide a more detailed discussion of [ 3 ] . [ 3 ] draws the conclusion that SSL pretraining is able to overcome label bias issues mainly by the following contrast experiments . They pretrain a model using SSL then further train it using supervised learning , resulting in a final model ( refered as ssl model in the following ) . Then the ssl model is directly compared to a baseline model , which is directly trained with supervised learning . The ssl model is trained with longer time ( 200 epochs with ssl loss , and 90 epochs with CE loss ) , than the baseline model ( 90 epochs with CE ) . Therefore , * * the performance improvement observed by [ 3 ] may come from longer training , since training longer is shown to be very effective for long-tailed recognition in [ 4 ] . * * To exclude such a factor and make a fair comparison , we conduct experiments with two additional baselines : the \u201c ssl model \u201d is firstly trained with supervised learning for 90 epochs , then further trained with supervised learning for another 90 epochs . ; another baseline model is simply trained for a longer time ( 200 epochs ) . The results are given as follows , |Method | Many | Medium | Few | All | | - | - | -- | -- | -| |Baseline model ( 90 epochs ) | 59.735 | 45.113 | 27.082 | 48.110 | |Baseline model ( 200 epochs ) | 61.382 | 48.486 | 32.890 | 51.174 | | ssl model ( 200+90 epochs ) | 60.670 | 48.392 | 31.493 | 50.652 | |sl model ( 90+90 epochs ) | 60.078 | 47.684 | 31.082 | 50.032 | Note that all results are given using tau-normalization proposed in [ 4 ] . * * The results clearly show that ssl pretraining gives no additional benefits compared to supervised learning . * * Moreover , we also tried to train a model by optimizing supervised loss and SSL loss together . The results show that the learned model is negatively affected as explained in answer 4 to reviewer # 1 . * * All these results together prove that our proposed KCL loss is the first successful practice to combine the benefits of supervised learning and self-supervised learning . * * - Third , besides the above discovery , we also make the following contributions which should not be neglected : ( 1 ) We are the first to show the performance of SSL is robust to dataset imbalance by studying SSL on various imbalanced datasets . ( 2 ) We propose a new concept of \u201c balancedness \u201d for feature space and demonstrate such property is important for both classification performance and generalization to different distributions and tasks . ( 3 ) The proposed method , KCL , is novel . It learns balanced and discriminative features , and outperforms CL and SL on both long-tailed datasets ( Sec.4.2 ) and balanced datasets ( Sec.4.3 ) .We sincerely hope our contributions can be fairly treated . [ 3 ] Yang , Yuzhe , and Zhi Xu . `` Rethinking the Value of Labels for Improving Class-Imbalanced Learning . '' NeurIPS 2020 [ 4 ] Kang , Bingyi , et al . `` Decoupling representation and classifier for long-tailed recognition . '' ICLR 2020 . * * A2 . * * * [ The runtime of the proposed method is much worse than supervised based methods . ] * The proposed method needs longer training epochs because the contrastive loss takes much longer to converge , as explained in Appendix A . The same criticism can be directed to all the SSL methods in the literature , including [ 3 ] . However , our method enjoys the advantage of learning balanced and discriminative features , which is demonstrated to be crucial for imbalanced classification problems . * * A3 . * * * [ Why cosine lr schedule ? ] * For MoCo , we use the original step decay strategy . For our proposed KCL method , we adopt the same lr scheduler used in supervised learning for a fair comparison , as explained in Appendix A . * * A4 . * * * [ How to interpret accuracies on imagenet in fig . 4 ? Why are there only 10 classes ? ] * We will add y-axis in the next version for Fig.4 . Actually , there are 1000 classes . For clearer visualization , we divide them into 10 bins according to their training instance numbers . Each dot in the figure represents the average accuracy of the corresponding bin ."}, "1": {"review_id": "OqtLIabPTit-1", "review_text": "* * Overview : * * The paper presents experiments showing that the contrastive learning losses produce better embeddings or feature spaces than those produced by using binary cross-entropy losses . The experiments show that embeddings learned using contrastive learning losses seem to favor long-tailed learning tasks , out-of-distribution tasks , and object detection . The paper also presents an extension of the contrastive loss to improve the embeddings . The experiments in the paper use common and recent long-tail datasets as well as datasets for object detection and out-of-distribution tasks . * * Pros * * : * Interesting problem and approach * . I think the paper tackles a hard and important problem , i.e. , learning from a long-tailed dataset . Overall , I think that learning a feature space improving the learning from these imbalanced datasets is an interesting idea . * Clarity of the paper * . Overall the clarity of the paper is good . The motivation is clear and the narrative is clear overall . However , I think the clarity in the experiments is insufficient , see below . * * Cons * * : * Insufficient clarity in the experiments * . I have several concerns with the experiments : 1 . The * balancedness * metric in Eq . ( 3 ) may not be a robust metric for measure performance . The reason I am not convinced about this metric is that if the accuracies of the classifier are low but equal , then the metric will say that the * balancedness * is good . I think a good metric for a classifier learning from an imbalanced dataset is one that indicates if the overall accuracy is high , maintains the many-shot the classification accuracy high , and increases the accuracy of the classes in the tail . I think this metric does not indicate if the overall accuracy is high . 2.I am not convinced about how classifiers are trained in experiments in Sec.3.2.The paper trains and tests using a balanced set after learning a feature space . To my understanding , the challenge of learning from a long-tailed dataset is to test whether a classifier can generalize well for classes with few training samples while maintaining a good performance on classes with more training examples . Thus , by training a linear classifier with a balanced set using the learned feature space does not really comply with learning from an imbalanced dataset . I think if the experiments would 've been stronger if they included results of a trained linear classifier on the learned embedding and still showing good results , then I would be more convinced about the impact of a contrastive loss . From the practical point of view , what matters is the classifier performance . In practice , it is challenging to have a balanced dataset as the paper used . The main question is about the performance when training a linear classifier from a long-tailed dataset using the learned representation . 3.Datasets derived from ImageNet-LT . While I value the goal of using different datasets varying the imbalance in a dataset , I am not convinced that ImageNet-LT is the dataset to use . The reason is that ImageNet-LT is a synthetic long-tailed dataset . In fact , while the dataset shows imbalance , it does not necessarily follow a power-law distribution . I think the generation of these datasets in the experiments should be done using a power-law distribution . From the text , it is unclear how the datasets from ImageNet-LT were generated for the experiments . Minor concerns : * Plots lack information * . What is the y-scale in Fig.2 ? The figure is missing y-scale information and it is hard to interpret the gap in accuracy between CE and CL in the left plot . Same comment for Fig 4 , what is the scale in y-axis ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We clarify our experiment settings and results here and welcome further discussions . The reviewer might misunderstand one of our core contributions , the \u201c balancedness \u201d of a feature space . * * Balancedness defined in Eqn . ( 3 ) is not used for evaluating the performance of a classifier but for measuring the quality of a feature space , i.e. , whether it is skewed to some dominant classes and leads to biased classification accuracy . * * This property is found by our extensive empirical studies on feature representation learning in various practical settings . When two feature spaces share similar discriminativeness , the more balanced one gives better generalization ability . This conclusion not only holds for long-tailed recognition ( Sec.4.2 ) , but is also applicable to the normal balanced datasets ( Sec.4.3 ) .So instead of only focusing on learning \u201c discriminative \u201d feature space as in previous works , we propose \u201c balancedness \u201d as another important property for a good feature space and we suggest learning both discriminative and balanced feature spaces . * * A1 . * * * [ The balancedness metric may not be a robust metric for measuring performance . ] * The reviewer might misunderstand the purpose of developing the balancedness metric as explained above . We would like to emphasize that the balancedness metric is only used to measure feature spaces from a new perspective , which is a complement to usual performance measures ( e.g. , accuracy ) , but not a replacement . We will explicitly clarify it in the revision . * * A2 . * * * [ I am not convinced about how classifiers are trained in experiments in Sec.3.2 . ] * We agree with the reviewer on the evaluation of long-tailed classifiers and we have presented the experiments in Sec.4.2.But * * the reviewer misunderstands the study in Sec 3.2 , whose purpose is to investigate how different loss functions ( e.g. , CE and CL ) affects the balancedness of learned embeddings , not to study the long-tailed recognition performance * * , as explained multiple times in the introduction and Sec.3.2 , * * < Experimental setting is Sec.3.2 is reasonable > * * To evaluate feature spaces from different losses , we first train several representation models with the six imbalanced datasets . However , the properties ( balancedness and discriminativeness ) of a feature space relies on the classification accuracies of learned classifiers . \u201c Since linear classifiers are easily biased by skewed training dataset distribution , it is necessary to eliminate the imblancedness of the evaluation datasets for reliable representation evaluation . Thus , we use the ( balanced ) training and test sets of ImageNet to learn classifiers and evaluate their classification accuracy. \u201d We have explained this in the first paragraph of Sec.3.2 . * * < Long-tailed classifiers are evaluated by exactly following the setting you explained here > * * For detailed settings and results for long-tailed recognition , please refer to Sec.4.2 . It clearly shows that our proposed KCL loss substantially outperforms existing SOTAs , e.g. , bringing 4.2 absolute points improvement on ImageNet-LT . * * A3 . * * * [ Datasets do not follow a power-law distribution . ] * The ImageNet-LT dataset is generated following a power-law distribution as explained in [ 2 ] . Therefore , based on the equation generating our datasets ( Eqn . ( 5 ) ) , it is clear that our datasets also follow power-law distributions . We also mention the datasets are generated by power law in the supplementary material . [ 2 ] Liu , Ziwei , et al . `` Large-scale long-tailed recognition in an open world . '' Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2019. * * A4 . * * * [ missing y-axis in Fig.2 and Fig.4 ] * The y-axis is removed for clarity as we care more about the relative value . The precise values are all provided in tables . We will add a y-axis in the revision ."}, "2": {"review_id": "OqtLIabPTit-2", "review_text": "In this paper , the authors propose a new loss function to learn feature representations for image datasets that are class-imbalanced . The loss function is a simple yet effective tweak on an existing supervised contrastive loss work . A number of empirical tests are performed on long-tailed datasets showing the benefits of the proposed loss in beating state of the art methods . Some specific questions are listed below : 1 . Is Figure 1 hypothetical ( `` desired outcome '' ) or real ( based on actual observations ) ? 2.Minor style comment : please do n't call your own contributions `` important '' : - ) - that is for others to decide . 3.Eqn . ( 3 ) is not clear at all - please provide an intuitive explanation and motivation . It seems to appear out of thin air . 4.Is there a tradeoff between accuracy and balancedness ? Figure 2 seems to suggest so . For example , if we did not train CE loss to maximal accuracy , would be automatically get the balancedness property ? Was this tested ? How about a combined loss CE + CL to get the best of both worlds ? And similarly , how about CE + KCL ? 5.Table 1 the delta for VOC seems to be computed wrongly . 6.Figure 7 : it 's strange that there are no numbers on the y-axis . 7.What is the ImageNet accuracy of KCL ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * A1 . * * Figure 1 is a hypothetical illustration of the feature spaces learned with different loss functions . However , this illustration is strongly evidenced by the experimental results on real large-scale datasets . As shown in Fig.4 ( right ) : - SL ( using the CE loss ) gives promising overall performance but heavily skewed accuracy curve , indicating the learned feature space is discriminative for different classes but biased to the head-classes ; - CL ( MoCo ) gives the poorest overall performance but a balanced accuracy curve , i.e. , the learned feature space is well balanced but not so discriminative . - Our proposed KCL performs the best , presenting the strengths of both CE and CL . This implies its learned feature space is both balanced and discriminative . * * A2 . * * Noted with thanks . We will revise the paper accordingly . * * A3 . * * Eqn . ( 3 ) defines a measure of the balancedness of a feature space based on the following intuition . As explained in the context of Eqn . ( 3 ) , a feature space is balanced if the linear classifier trained on its sample features does not give biased performance to any classes , i.e. , the classifier gives balanced ( similar ) accuracy on all the classes ( \u201c similar degrees of linear separability \u201d ) . Since balancedness ( similarity ) of a set of values can be computed by their pairwise Gaussian distance [ 1 ] , we use Eqn . ( 3 ) to compute the balancedness of the class-wise accuracy of a linear classifier and take it to measure the feature space balancedness . We will make this clear and add a more intuitive explanation in the revision . [ 1 ] Wang , Tongzhou , and Phillip Isola . `` Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere . '' arXiv preprint arXiv:2005.10242 ( 2020 ) . * * A4 . * * * [ Is there a tradeoff between accuracy and balancedness ? ] * We would like to clarify that we are proposing balancedness as a complementary property to discriminativeness ( or classification accuracy ) for representation learning . There is no trade-off between them . More detailed explanations are given below . * [ More explanation for Fig.2 . ] * For Fig.2 , by comparing the performance ( left ) and balancedness ( right ) curves given by CE loss ( green curves ) , we can observe that the performance increases as balancedness improves , which demonstrates a positive correlation , instead of trade-off , between accuracy and balancedness . When comparing CE loss with CL loss , CE gives high accuracy but low balancedness , while CL gives poor accuracy but strong balancedness . But this does not imply there is a trade-off between accuracy and balancedness because these two loss functions focus on different aspects of feature learning ( one for discriminativeness , the other one for balancedness ) . Therefore , it is possible to optimize discriminativeness and balancedness simultaneously , which motivates us to propose the KCL loss . * [ Would a random model automatically get the balancedness property ? ] * Yes , it is possible that a random classification model gives optimal balancedness property . Based on Eqn . ( 3 ) , all zero accuracies give the largest balancedness score . However , please note that balancedness is a complementary property to discriminativeness . * [ How about a combined loss CE + CL ? ] * We have tried to train a model with a combined loss CE + $ \\lambda $ CL with $ \\lambda $ as the weighting factor . * * Our experiments reveal that directly optimizing these two losses gives sub-optimal results . * * When the CL term weighting factor increases ( learned more balanced representations ) , accuracy decreases . The experimental results on ImageNet-LT are given as follows | $ \\lambda $ | Many | Medium | Few | All | | | -- | | -- | - | | 0.0 | 59.735 | 45.113 | 27.082 | 48.110 | | 0.3 | 59.326 | 44.142 | 28.408 | 47.744 | | 1.0 | 58.169 | 42.151 | 24.462 | 45.744 | However , this does not mean that balancedness and good performance can not be achieved at the same time . Actually , KCL is motivated by achieving these two properties simultaneously . * * The above results show this target is non-trivial , further validating the novelty and necessity of our KCL loss . * * * [ How about a combined loss CE + KCL ? ] * The results can be inferred from the CE and KCL results . ( 1 ) KCL is designed to combine the strengths of CE and CL , which gives both good balancedness and accuracy . ( 2 ) KCL + CE loss will achieve the performance lying between CE and KCL results . * * A5 . * * Thank you for pointing it out . We will address it in the revision . * * A6 . * * There is no Fig.7 in our paper . The reviewer may refer to Fig.2 and Fig.4 . We remove the y-axis value for more clearly showing the relative difference of different methods . The actual values of Fig.4 are all provided in Table 2 . We will add y-axis values in the revision . * * A7 . * * KCL achieves an accuracy of 76.814 on ImageNet , as given in Table 5 , which is comparable with the result of CE ( 76.616 ) ."}}