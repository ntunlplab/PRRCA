{"year": "2018", "forum": "ByS1VpgRZ", "title": "cGANs with Projection Discriminator", "decision": "Accept (Poster)", "meta_review": "The paper proposes a simple modification to conditional GANs, where the discriminator involves an inner product term between the condition vector y and the feature vector of x. This formulation is reasonable and well motivated from popular models (e.g., log-linear, Gaussians). Experimentally, the proposed method is evaluated on conditional image generation and super-resolution tasks, demonstrating improved qualitative and qualitative performance over the existing state-of-the-art (AC-GAN).\n", "reviews": [{"review_id": "ByS1VpgRZ-0", "review_text": " I thank the authors for the thoughtful response and updated manuscript. After reading through both, my review score remains unchanged. ================= The authors describe a new variant of a generative adversarial network (GAN) for generating images. This model employs a 'projection discriminator' in order to incorporate image labels and demonstrate that the resulting model outperforms state-of-the-art GAN models. Major comments: 1) Spatial resolution. What spatial resolution is the model generating images at? The AC-GAN work performed an analysis to assess how information is being introduced at each spatial resolution by assessing the gains in the Inception score versus naively resizing the image. It is not clear how much the gains of this model is due to generating better lower resolution images and performing simple upscaling. It would be great to see the authors address this issue in a serious manner. 2) FID in real data. The numbers in Table 1 appear favorable to the projection model. Please add error bars (based on Figure 4, I would imagine they are quite large). Additionally, would it be possible to compute this statistic for *real* images? I would be curious to know what the FID looks like as a 'gold standard'. 3) Conditional batch normalization. I am not clear how much of the gains arose from employing conditional batch normalization versus the proposed method for incorporating the projection based discriminator. The former has been seen to be quite powerful in accomodating multi-modal tasks (e.g. https://arxiv.org/abs/1709.07871, https://arxiv.org/abs/1610.07629 ). If the authors could provide some evidence highlighting the marginal gains of one technique, that would be extremely helpful. Minor comments: - I believe you have the incorrect reference for conditional batch normalization on Page 5. A Learned Representation For Artistic Style Dumoulin, Shlens and Kudlur (2017) https://arxiv.org/abs/1610.07629 - Please enlarge images in Figure 5-8. Hard to see the detail of 128x128 images. - Please add citations for Figures 1a-1b. Do these correspond with some known models? Depending on how the authors respond to the reviews, I would consider upgrading the score of my review.", "rating": "6: Marginally above acceptance threshold", "reply_text": "> 1 ) Spatial resolution . What spatial resolution is the model generating images at ? We are sorry for the lack of the spatial resolution information . The model generates at `` 128x128 '' spatial resolution . > 1 ) Spatial resolution , 3 ) Conditional batch normalization , The goal of our paper is to show the efficacy of our `` projection '' model for the discriminator , so all our experiments use same architecture for the generator . In all our experiments , we are equipping the generator with conditional BN . This includes our experiments with `` AC-GANs '' , as well as `` concat '' model and `` projection '' model . We can indeed explore the same result with generators equipped with a different way to introduce the conditional information ( such as label concatenation ) ; however , we intend not to make this ( generator structure ) the focus of our paper . On the base of our theoretical motivations , ( Section 3 ) we also believe that our way will perform well even with a different way of label conditionalization of the generator . We would also like to emphasize that our projection model is prevailing on the super-resolution task as well , suggesting that our success is not the model and task specific . Lastly , as interesting as it is , we would not find a way to include the dependence of the performance on the image resolution into the scope of our paper . > 2 ) > FID in real data . The numbers in Table 1 appear favorable to the projection model . Please add error bars ( based on Figure 4 , I would imagine they are quite large ) . We are sorry , but we are little confused about this suggestion . First of all , we are dealing with images from different classes in our experiments . The difficulty of image generation differs across each class , and the intra FID shall depend on the dataset of each class . We , therefore , found no particular need for showing the size of its variance ( error bar ) in this experiment . The goal of our Figure 4 here is to simply show that our projection method outperforms `` concat '' and `` AC-GANs '' on `` most of the classes '' , and we felt it more appropriate to visualize our claim with scatter plot . > Additionally , would it be possible to compute this statistic for * real * images ? I would be curious to know what the FID looks like as a 'gold standard . ' Please take a look at the definition of FID ( p5 , ( Heusel et al. , 2017 ) ) . FID is a measure of a difference between two distributions . If there are infinitely many 'real ' images , the FID between 'real ' images against 'real ' images is trivially 0 . In our paper , we are comparing the empirical distribution of generated samples over 5000 samples against the that of the training 'real ' images . If we compute the empirical distribution of 'real ' images against another empirical distribution of the 'real ' images , we are bound to observe some nonzero FID value . However , we find no particular importance in computing such value . > Minor comments : > - I believe you have the incorrect reference for conditional batch normalization on Page 5 . > - Please enlarge images in Figure 5-8 . Hard to see the detail of 128x128 images . > - Please add citations for Figures 1a-1b . Do these correspond with some known models ? Thanks for pointing out the incorrect references ! We would revise the designated citations accordingly . We would also like to modify the figure images to improve the visuality ."}, {"review_id": "ByS1VpgRZ-1", "review_text": "The paper proposes a simple modification to conditional GANs, obtaining impressive results on both the quality and diversity of samples on ImageNet dataset. Instead of concatenating the condition vector y to the input image x or hidden layers of the discriminator D as in the literature, the authors propose to project the condition y onto a penultimate feature space V of D (by simply taking an inner product between y and V) . This implementation basically restricts the conditional distribution p(y|x) to be really simple and seems to be posing a good prior leading to great empirical results. + Quality: - Simple method leading to great results on ImageNet! - While the paper admittedly leaves theoretical work for future work, the paper would be much stronger if the authors could perform an ablation study to provide readers with more intuition on why this work. One experiment could be: sticking y to every hidden layer of D before the current projection layer, and removing these y's increasingly and seeing how performance changes. - Appropriate comparison with existing conditional models: AC-GANs and PPGNs. - Appropriate (extensive) metrics were used (Inception score/accuracy, MS-SSIM, FID) + Clarity: - Should explicitly define p, q, r upfront before Equation 1 (or between Eq1 and Eq2). - PPG should be PPGNs. + Originality: This work proposes a simple method that is original compared existing GANs. + Significance: While the contribution is significant, more experiments providing more intuition into why this projection works so well would make the paper much stronger. Overall, I really enjoy reading this paper and recommend for acceptance! ", "rating": "7: Good paper, accept", "reply_text": "We are very glad to hear that you enjoy our manuscript ! > While the paper admittedly leaves theoretical work for future work , the paper would be much stronger if the authors could perform an ablation study to provide readers with more intuition on why this work . One experiment could be : sticking y to every hidden layer of D before the current projection layer , and removing these y 's increasingly and seeing how performance changes . > While the contribution is significant , more experiments providing more intuition into why this projection works so well would make the paper much stronger . Ablation study was in fact a vexing issue in our paper , and we are still unsure of a way to theoretically back up our results . We may attempt your suggestion , and meanwhile continue looking for still other convincing experiments . > Should explicitly define p , q , r upfront before Equation 1 ( or between Eq1 and Eq2 ) . > PPG should be PPGNs . Thanks for pointing out the mistakes ! We will make changes accordingly in the revised version ."}, {"review_id": "ByS1VpgRZ-2", "review_text": "This manuscript makes the case for a particular parameterization of conditional GANs, specifically how to add conditioning information into the network. It motivates the method by examining the form of the log density ratio in the continuous and discrete cases. This paper's empirical work is quite strong, bringing to bare nearly all of the established tools we currently have for evaluating implicit image models (MS-SSIM, FID, Inception scores). What bothers me is mostly that, while hyperparameters are stated (and thank you for that), they seem to be optimized for the candidate method rather than the baseline. In particular, Beta1 = 0 for the Adam momentum coefficient seems like a bold choice based on my experience. It would be an easier sell if hyperparameter search details were included and a separate hyperparameter search were conducted for the candidate and control, allowing the baseline to put its best foot forward. The sentence containing \"assume that the network model can be shared\" had me puzzled for a few minutes. I think what is meant here is just that we can parameterize the log density ratio directly (including some terms that belong to the data distribution to which we do not have explicit access). This could be clearer.", "rating": "6: Marginally above acceptance threshold", "reply_text": "> What bothers me is mostly that , while hyperparameters are stated ( and thank you for that ) , they seem to be optimized for the candidate method rather than the baseline . In particular , Beta1 = 0 for the Adam momentum coefficient seems like a bold choice based on my experience . We did not perform any hyper-parameter optimization for the Adam optimizer and the number of critic updates , etc\u2026 We just used the same hyper-parameters used in Gulrajani et al . ( 2017 , https : //github.com/igul222/improved_wgan_training/blob/master/gan_cifar_resnet.py ) , because we adopted the practically the same architecture used in the very paper . We must admit that we simply could not spare enough time for the parameter search for the ImageNet experiments . However , we plan to do the search for ( beta1 , alpha of Adam ) on CIFAR 10 or CIFAR 100 and compare the performance against `` AC-GANs '' , `` concat '' and `` projection '' . > The sentence containing `` assume that the network model can be shared '' had me puzzled for a few minutes . I think what is meant here is just that we can parameterize the log density ratio directly ( including some terms that belong to the data distribution to which we do not have explicit access ) . This could be clearer . Thank you very much ! We concur with you in your views and we will reflect the suggestion on this part of the revision ."}], "0": {"review_id": "ByS1VpgRZ-0", "review_text": " I thank the authors for the thoughtful response and updated manuscript. After reading through both, my review score remains unchanged. ================= The authors describe a new variant of a generative adversarial network (GAN) for generating images. This model employs a 'projection discriminator' in order to incorporate image labels and demonstrate that the resulting model outperforms state-of-the-art GAN models. Major comments: 1) Spatial resolution. What spatial resolution is the model generating images at? The AC-GAN work performed an analysis to assess how information is being introduced at each spatial resolution by assessing the gains in the Inception score versus naively resizing the image. It is not clear how much the gains of this model is due to generating better lower resolution images and performing simple upscaling. It would be great to see the authors address this issue in a serious manner. 2) FID in real data. The numbers in Table 1 appear favorable to the projection model. Please add error bars (based on Figure 4, I would imagine they are quite large). Additionally, would it be possible to compute this statistic for *real* images? I would be curious to know what the FID looks like as a 'gold standard'. 3) Conditional batch normalization. I am not clear how much of the gains arose from employing conditional batch normalization versus the proposed method for incorporating the projection based discriminator. The former has been seen to be quite powerful in accomodating multi-modal tasks (e.g. https://arxiv.org/abs/1709.07871, https://arxiv.org/abs/1610.07629 ). If the authors could provide some evidence highlighting the marginal gains of one technique, that would be extremely helpful. Minor comments: - I believe you have the incorrect reference for conditional batch normalization on Page 5. A Learned Representation For Artistic Style Dumoulin, Shlens and Kudlur (2017) https://arxiv.org/abs/1610.07629 - Please enlarge images in Figure 5-8. Hard to see the detail of 128x128 images. - Please add citations for Figures 1a-1b. Do these correspond with some known models? Depending on how the authors respond to the reviews, I would consider upgrading the score of my review.", "rating": "6: Marginally above acceptance threshold", "reply_text": "> 1 ) Spatial resolution . What spatial resolution is the model generating images at ? We are sorry for the lack of the spatial resolution information . The model generates at `` 128x128 '' spatial resolution . > 1 ) Spatial resolution , 3 ) Conditional batch normalization , The goal of our paper is to show the efficacy of our `` projection '' model for the discriminator , so all our experiments use same architecture for the generator . In all our experiments , we are equipping the generator with conditional BN . This includes our experiments with `` AC-GANs '' , as well as `` concat '' model and `` projection '' model . We can indeed explore the same result with generators equipped with a different way to introduce the conditional information ( such as label concatenation ) ; however , we intend not to make this ( generator structure ) the focus of our paper . On the base of our theoretical motivations , ( Section 3 ) we also believe that our way will perform well even with a different way of label conditionalization of the generator . We would also like to emphasize that our projection model is prevailing on the super-resolution task as well , suggesting that our success is not the model and task specific . Lastly , as interesting as it is , we would not find a way to include the dependence of the performance on the image resolution into the scope of our paper . > 2 ) > FID in real data . The numbers in Table 1 appear favorable to the projection model . Please add error bars ( based on Figure 4 , I would imagine they are quite large ) . We are sorry , but we are little confused about this suggestion . First of all , we are dealing with images from different classes in our experiments . The difficulty of image generation differs across each class , and the intra FID shall depend on the dataset of each class . We , therefore , found no particular need for showing the size of its variance ( error bar ) in this experiment . The goal of our Figure 4 here is to simply show that our projection method outperforms `` concat '' and `` AC-GANs '' on `` most of the classes '' , and we felt it more appropriate to visualize our claim with scatter plot . > Additionally , would it be possible to compute this statistic for * real * images ? I would be curious to know what the FID looks like as a 'gold standard . ' Please take a look at the definition of FID ( p5 , ( Heusel et al. , 2017 ) ) . FID is a measure of a difference between two distributions . If there are infinitely many 'real ' images , the FID between 'real ' images against 'real ' images is trivially 0 . In our paper , we are comparing the empirical distribution of generated samples over 5000 samples against the that of the training 'real ' images . If we compute the empirical distribution of 'real ' images against another empirical distribution of the 'real ' images , we are bound to observe some nonzero FID value . However , we find no particular importance in computing such value . > Minor comments : > - I believe you have the incorrect reference for conditional batch normalization on Page 5 . > - Please enlarge images in Figure 5-8 . Hard to see the detail of 128x128 images . > - Please add citations for Figures 1a-1b . Do these correspond with some known models ? Thanks for pointing out the incorrect references ! We would revise the designated citations accordingly . We would also like to modify the figure images to improve the visuality ."}, "1": {"review_id": "ByS1VpgRZ-1", "review_text": "The paper proposes a simple modification to conditional GANs, obtaining impressive results on both the quality and diversity of samples on ImageNet dataset. Instead of concatenating the condition vector y to the input image x or hidden layers of the discriminator D as in the literature, the authors propose to project the condition y onto a penultimate feature space V of D (by simply taking an inner product between y and V) . This implementation basically restricts the conditional distribution p(y|x) to be really simple and seems to be posing a good prior leading to great empirical results. + Quality: - Simple method leading to great results on ImageNet! - While the paper admittedly leaves theoretical work for future work, the paper would be much stronger if the authors could perform an ablation study to provide readers with more intuition on why this work. One experiment could be: sticking y to every hidden layer of D before the current projection layer, and removing these y's increasingly and seeing how performance changes. - Appropriate comparison with existing conditional models: AC-GANs and PPGNs. - Appropriate (extensive) metrics were used (Inception score/accuracy, MS-SSIM, FID) + Clarity: - Should explicitly define p, q, r upfront before Equation 1 (or between Eq1 and Eq2). - PPG should be PPGNs. + Originality: This work proposes a simple method that is original compared existing GANs. + Significance: While the contribution is significant, more experiments providing more intuition into why this projection works so well would make the paper much stronger. Overall, I really enjoy reading this paper and recommend for acceptance! ", "rating": "7: Good paper, accept", "reply_text": "We are very glad to hear that you enjoy our manuscript ! > While the paper admittedly leaves theoretical work for future work , the paper would be much stronger if the authors could perform an ablation study to provide readers with more intuition on why this work . One experiment could be : sticking y to every hidden layer of D before the current projection layer , and removing these y 's increasingly and seeing how performance changes . > While the contribution is significant , more experiments providing more intuition into why this projection works so well would make the paper much stronger . Ablation study was in fact a vexing issue in our paper , and we are still unsure of a way to theoretically back up our results . We may attempt your suggestion , and meanwhile continue looking for still other convincing experiments . > Should explicitly define p , q , r upfront before Equation 1 ( or between Eq1 and Eq2 ) . > PPG should be PPGNs . Thanks for pointing out the mistakes ! We will make changes accordingly in the revised version ."}, "2": {"review_id": "ByS1VpgRZ-2", "review_text": "This manuscript makes the case for a particular parameterization of conditional GANs, specifically how to add conditioning information into the network. It motivates the method by examining the form of the log density ratio in the continuous and discrete cases. This paper's empirical work is quite strong, bringing to bare nearly all of the established tools we currently have for evaluating implicit image models (MS-SSIM, FID, Inception scores). What bothers me is mostly that, while hyperparameters are stated (and thank you for that), they seem to be optimized for the candidate method rather than the baseline. In particular, Beta1 = 0 for the Adam momentum coefficient seems like a bold choice based on my experience. It would be an easier sell if hyperparameter search details were included and a separate hyperparameter search were conducted for the candidate and control, allowing the baseline to put its best foot forward. The sentence containing \"assume that the network model can be shared\" had me puzzled for a few minutes. I think what is meant here is just that we can parameterize the log density ratio directly (including some terms that belong to the data distribution to which we do not have explicit access). This could be clearer.", "rating": "6: Marginally above acceptance threshold", "reply_text": "> What bothers me is mostly that , while hyperparameters are stated ( and thank you for that ) , they seem to be optimized for the candidate method rather than the baseline . In particular , Beta1 = 0 for the Adam momentum coefficient seems like a bold choice based on my experience . We did not perform any hyper-parameter optimization for the Adam optimizer and the number of critic updates , etc\u2026 We just used the same hyper-parameters used in Gulrajani et al . ( 2017 , https : //github.com/igul222/improved_wgan_training/blob/master/gan_cifar_resnet.py ) , because we adopted the practically the same architecture used in the very paper . We must admit that we simply could not spare enough time for the parameter search for the ImageNet experiments . However , we plan to do the search for ( beta1 , alpha of Adam ) on CIFAR 10 or CIFAR 100 and compare the performance against `` AC-GANs '' , `` concat '' and `` projection '' . > The sentence containing `` assume that the network model can be shared '' had me puzzled for a few minutes . I think what is meant here is just that we can parameterize the log density ratio directly ( including some terms that belong to the data distribution to which we do not have explicit access ) . This could be clearer . Thank you very much ! We concur with you in your views and we will reflect the suggestion on this part of the revision ."}}