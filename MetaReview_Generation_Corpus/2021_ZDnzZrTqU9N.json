{"year": "2021", "forum": "ZDnzZrTqU9N", "title": "Modeling the Second Player in Distributionally Robust Optimization", "decision": "Accept (Poster)", "meta_review": "# Paper Summary\n\nThis paper considers the problem of distributionally robust optimization (DRO), in which one is attempting to minimize a loss on the worst of all distributions that are some distance (here, measured in terms of KL divergence) from the training set. The main novelty here is that this adversarial distribution is represented as a model, with parameters that are learned jointly with the primary model.\n\nThis is an intuitive idea, but as the authors explain, attempting to implement it leads to a number of complications. One of these is that it is challenging to constrain the adversarial distribution model to be a certain KL divergence away from the training set. To address this, they write down the Lagrangian, but do not actually optimize over the Lagrange multiplier resulting from this constraint: instead, they keep it at a fixed constant value (a hyperparameter). A second, and potentially more worrisome, issue is that it is difficult to optimize the KL divergence as written--instead, they swap the two parameters, which is of course incorrect but they claim leads to much nicer convergence behavior.\n\nThey also propose a stopping condition, which terminates optimization once the robust validation loss (i.e. the validation loss w.r.t. the worst permissible distribution) stops decreasing. Normally, this would require a search for the worst such distribution at every iteration, which would be prohibitively expensive, so they propose instead only checking the distributions that have been found by the adversary during the course of optimization.\n\nThey close with a set of experiments that is nicely designed to narrow in on and explore particular details of their approach (e.g. they have an experiment that validates their stopping criterion), and have a realistic experiment on two NLP datasets.\n\n# Pros\n\n1. Reviewers agreed that it was very well-written, well-organized, and comprehensive\n1. Good discussion of background material. The paper is very accessible\n1. Intuitive idea, although the details of the approach become somewhat complex\n1. Aside from the \"realistic\" experiment, each is designed to explore a particular facet of their approach\n\n# Cons\n\n1. Some reviewers were concerned that the baselines were insufficient. In response the authors added the new Hu et al. baseline (NonParam), which seemed to be satisfactory\n1. While the approach is more general, one reviewer noted that the experiments only consider NLP problems. This is a minor negative point, in my view\n1. One reviewer was concerned that the results were \"too good\", and encouraged the authors to double-check their results. My belief is that, at least on the non-\"realistic\" experiments (which were mostly intended to drill down into specific attributes of their approach, rather than demonstrate its overall performance), this is because the problem was constructed to perform especially poorly with a non-DRO approach\n1. One reviewer was unsatisfied with the idea of swapping the parameters to the KL divergence (I share this concern). The authors clarified, both in the response and in the paper, that swapping the parameters is indeed incorrect, and may in fact be a very bad approximation to the true quantity of interest, but that the performance difference was so dramatic that it couldn't be undone. This seemed to partially satisfy the reviewer\n\n# Conclusion\n\nAll four reviewers ultimately recommended acceptance. The major concerns were (i) that the baselines weren't good enough (which the authors addressed by adding a new baseline), and (ii) that swapping the parameters to the KL divergence results in a very poor approximation to the original KL divergence (which the authors now explicitly acknowledge in the paper, with an explanation for why they feel it is necessary). Overall, this is a nice idea, and while bringing it into practice may require more hand-waving than would be ideal (which is the main reason I suggested a poster acceptance instead of a spotlight or oral), it seems to work well experimentally, and the experiments are overall very careful and well thought-out. Additionally, the writing quality is excellent, as is the organization and presentation of background material.", "reviews": [{"review_id": "ZDnzZrTqU9N-0", "review_text": "This paper considers distributionally robust optimization ( DRO ) and uses the neural generative models to characterize the uncertainty sets . To tackle the optimization challenges , several implementation tricks are incorporated to solve the minimax problem . The proposed robust method is validated on NLP tasks . This paper is well-written and of a good structure . Although the main idea is simple , the authors make several modifications to the algorithm to make it tractable and with performance guaranteed heuristically . To summarize , the main contribution of this paper is a new algorithm that combines standard techniques , such as Lagrangian relaxation and KL reverse , into the DRO problem with KL uncertainty sets . And this algorithm was shown to perform well under synthetic and real-data NLP tasks . Since there is no novel techniques proposed in this paper and there is no performance guarantee for the proposed framework , overall , I think this is a borderline paper due to its limitations in theoretical development and technical novelty . Moreover , if the main focus of this paper is on developing a new computational framework that can lead to more robust results , then the authors should compare with more benchmark methods , while I only see the comparison with ERM , Topic-CVaR , etc . For example , I am wondering is it applicable to compare with Wasserstein DRO or Huber 's classical work of Total variation based DRO , or some other DRO works in the literature , so that it will be more convincing on the performance of the proposed method . A minor typo in the paper : in section 6 , there is a duplicated `` produce '' in the sentence : `` In such cases where good quality generative models are unavailable , or such model can not produce produce densities efficiently '' .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their encouraging comments and helpful feedback . We address their specific concerns below , and we are happy to continue discussing any of these points or answer follow-up questions . \\ > Since there is no novel techniques proposed in this paper and there is no performance guarantee for the proposed framework , overall , I think this is a borderline paper due to its limitations in theoretical development and technical novelty We do agree with the reviewer that the paper does not make a strong theoretical contribution , and our approach is very much motivated by proposing a method that works in a practical scenario rather than what is most satisfying theoretically . The main novelty of the paper is the use of a parametric family as the uncertainty set in DRO . However , our contribution goes beyond the brute-force approach of simply plugging parametric models into the classical DRO min-max ( which doesn \u2019 t work , as demonstrated in our toy experiments in Section 4.4 ) . In particular while a number of the adjustments detailed in Section 3 are not novel by themselves ( lagrangian relaxation , KL reversal ... ) , the fact that they can be combined and applied to the problem of parametric DRO is ( in the authors \u2019 opinion ) far from being a given . \\ > More baselines The reviewer \u2019 s point regarding more baselines is well taken . First , we would like to point out that Wasserstein DRO presumes the existence of a canonical metric on the input space , of which there is none for discrete sequential inputs such as natural language sentences . Adaptation of Wasserstein DRO to NLP is an interesting direction , but it is far from straightforward , and would warrant a more thorough investigation of its own . Huber \u2019 s work on robust statistics solves a related but different setting : ensuring that models are robust to an adversary who modifies the training data . Our paper considers the problem where the training data is fixed , and the test distribution is potentially different . To our knowledge , Huber \u2019 s robust statistics approaches do not directly address KL-robust DRO problems . That being said , we did run the additional baseline of non-parametric KL-constrained DRO , inspired by the formulation of Hu et al . ( 2016 ) ( https : //arxiv.org/abs/1611.02041 ) . We refer to our general response to all reviewers for more details and initial results . We are currently working towards adding these additional baseline results throughout the paper ."}, {"review_id": "ZDnzZrTqU9N-1", "review_text": "Good points - - The objective of the paper is sound : fight distributional shift in systems whose predictions might have life-changing consequences ( e.g data bias toxicity prediction models , etc . ) . - The paper is well-written and easy to follow . Bad points - - I do n't see just how this model is `` parametric '' . In statistics , `` parametric '' the adversarial distribution is modeled as a gaussian , etc . with sought-for parameters ( mean , covariance , etc . ) . In the absence of that , I would have expected `` parametric '' to mean parametrizing the adversarial distribution as the ( softmax ) output of a neural network . Neither of the above is the case in this paper . So , what are the `` parameters '' in the proposed DRO adversary ? All I can see is that the authors do a full search over all distributions , subject to a KL constraint ( see sections 2 and 3.2 ) . There is nothing `` parametric '' about this . - The authors say `` In particular , direct gradient descent on the uncertainty set suffers from instability due to the large variance of the gradients ( Greensmith et al. , 2004 ) , and hyper-parameter selection is not straightforward . '' I 'm not sure about this claim ( which is one of the main premises of the manuscript.What do the authors make of this paper for example Faury et al . ( AAAI 2020 ) `` Distributionally Robust Counterfactual Risk Minimization '' ? The authors of that paper demonstrate how to efficiently formulate and solve KL-based DRO problems . That paper also contains both theoretical and practical insights . - The technical contribution of the paper is negligible ( if any ) . - The arguments in the paper very heuristic . - Since the paper is supposed to be empirical ( see previous points ) , I would have expected experiments on real datasets . Errors - Change `` solve the inner-max efficient '' to `` solve the inner maximization problem efficiently '' - Change `` $ x , y ~ $ `` to `` $ ( x , y ) ~ $ '' all through the manuscript - Eqn ( 5 ) : why not take $ p $ and $ q_ { \\psi_0 } $ to equal the empirical distribution ( as is usually done ) in DRO ? - In eqn defining $ q_ { \\psi_0 } $ , replace $ \\arg\\max_ { q_\\psi } $ with $ \\arg\\max_\\psi $", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their detailed feedback . We address their specific concerns below , and we are happy to continue discussing any of these points or answer follow-up questions . \\ > I do n't see just how this model is `` parametric '' The proposed approach is parametric in the sense that the confusion set is represented by a parametric family of models . To take the reviewer \u2019 s example , in one of our ablation experiments ( Section 4.4 ) , the adversary is indeed a gaussian , and its sought-for parameters are the mean . In our other experiments , the uncertainty set is composed of transformer-based language models . This setting is also parametric in the sense that each possible test distribution in the uncertainty set is associated with a set of parameters for a transformer model , and these parameters are optimized jointly with the classification model following the procedure described in Section 3 . To clarify this point , we have added results for a non-parametric KL-constrained baseline . Please refer to our general response to all reviewers for more details . \\ > Comparison to Faury et al . ( AAAI 2020 ) `` Distributionally Robust Counterfactual Risk Minimization '' We thank the reviewer for pointing out this relevant citation , which we \u2019 ll include into the next revision . As far as we can tell , this paper proposes a non-parametric KL-constrained formulation of DRO which is very similar to that of Hu & Hong ( 2013 ) or Hu et al . ( 2016 ) , but applied to CRM . In fact , Faury et al.corroborate our point : they state ( eg.Section 2.2 ) that \u201c We are interested in DRO instances that are amenable to direct optimization . To this end , we focus here only on uncertainty sets U based on information divergence \u201d . In our work , we consider the challenges that occur when moving outside this tractable set of uncertainty sets , and consider intersections of the KL uncertainty set with parametric models where the inner-maximization problem becomes intractable ( hence the need for the approximations described in Section 3 ) . Again , we refer to our general response for more details on an additional , relevant baseline . \\ > The technical contribution of the paper is negligible ( if any ) To the best of our knowledge , we are the first to investigate DRO with neural-network based parametric confusions sets and to address the associated challenges ( intractability of the inner-max , difficulty of enforcing the KL constraint ... ) . We believe these are all technical contributions that are not attested to by previous research , and all required a significant amount of thought , design , implementation , and empirical validation . \\ > Since the paper is supposed to be empirical ( see previous points ) , I would have expected experiments on real datasets . We would like to point out that the experiments in the final section of the paper are performed on two toxicity detection datasets , which are well established datasets addressing an important real problem and widely used in the community : Davidson et al . ( 2017 ) and Founta et al . ( 2018 ) ( 760 and 109 citations respectively according to google scholar ) . \\ > In Eq.why not take q_psi_0 and p to equal the empirical distribution ( as is usually done ) in DRO ? Due to its parametric nature , the support of the adversary q_\\psi is larger than that of the empirical distribution , therefore , we need to use the true data distribution p ( which we assume has the same support as q_psi ) in the denominator . In practice , since p is unavailable , we resort to the MLE q_\\psi_0 , which also has the same support ."}, {"review_id": "ZDnzZrTqU9N-2", "review_text": "TL ; DR : The paper makes an interesting contribution from a practical point of view , but two important theoretical concerns need to be addressed in the rebuttal for acceptance . The paper proposes the use of ideas taken from the literature on distributionally robust optimization within a parametric framework . More precisely , the main idea is to consider only a ( parameterised ) subset of the traditional KLD-uncertainty sets . As this avoids the need for elegant analytic solutions ( at the expense of a more brute force computation ) , it has the flavour of a more \u2018 black box \u2019 approach towards the deployment of DRO . Overall , I really enjoyed the way this paper was written . Purpose and use of the contributions are clear throughout , and the reader is drawn in . I also liked the contribution and believe that the paper demonstrated its ideas to be useful . There are however two points of major concern from a more theoretical side . In my mind , these are rather substantial , and I will list them below . To recommend that the paper be accepted , these points will have to be addressed in a future version of the paper : ( 1 ) How do you ensure that the KLD between $ q_ { \\psi } $ and $ p $ is finite ? p clearly is the empirical measure ( as is emphasised e.g.just above eq . ( 5 ) ) , but $ q_ { \\psi } $ will be continuous . This means that the KLD between the two distributions is not defined/infinity for any value of \\psi ( Mismatch of support problem ) . These kind of problems are the precise reasons why other quasi-distances ( like the Wasserstein distances ) have become increasingly interesting for ML . As far as I can tell , this problem is not elaborated upon anywhere in the paper . ( 2 ) It is totally unclear to me why it should be viable to suddenly flip the direction of the KLD . The KLD is not symmetric and in general will not even have the same minimum . In fact , generally speaking the only time the minimum will be the same in either direction is when the KLD \u2019 s global minimum is such that $ q_ { \\psi } = q_ { \\tau , \\theta } $ ( i.e.we can drop the KLD term for the loss in ( 7 ) completely , so that it simply equals $ C $ ) . Given the definition of $ q_ { \\tau , \\theta } $ , it is unreasonable to assume that this global minimum is attained . This makes the flipping of the KLD \u2019 s direction questionable at best . Calling the outcome an \u2018 approximation \u2019 is then grossly inaccurate . ( See e.g.the visualisations here : https : //wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/ ) Lastly , since the chief concern of the paper is the construction of new uncertainty sets , I would have liked to see two additional recent references discussed which have produced uncertainty sets purely based on moments ( https : //arxiv.org/abs/2007.04458 , ICML 2020 ) as well as on general IPMs ( https : //arxiv.org/abs/2006.04349 , NeurIPs 2020 ) . Both these types of uncertainty sets do * not * suffer from the mismatch of support problem , and\u2014like the famous f-divergence based uncertainty sets\u2014have elegant dual forms . POST-DISCUSSION : The authors promised to clarify the two issues I pointed out in ways that are satisfactory for a paper whose main concern is practicality ( as opposed to theoretical rigour ) . I will thus raise my score to a weak accept .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate the reviewer \u2019 s enthusiasm for our approach , and are grateful for the insightful feedback . We address their specific concerns below , and we are happy to continue discussing any of these points or answer follow-up questions . \\ > How do we ensure that the KL divergence between q_\\psi and the empirical distribution p is finite or even well-defined ? The reviewer makes an keen observation that the empirical distribution , being of finite support , does not have a finite KL divergence with q_\\psi . In truth , we are only interested in the KL divergence between q_psi and the true underlying data distribution , which we can reasonably assume is finite . We agree that the phrasing of the paper is confusing in this regard , as we interchangeably refer to both the \u201c true \u201d data distribution and the empirical distribution ( of finite support over the training data ) as p. We will edit the paper to make this clearer . \\ > Why is flipping the KL viable ? The reviewer is correct that the KL is not symmetric , and as such the reversed loss L_rev is not equivalent to the original \u201c forward \u201d KL minimization problem . First , we would like to clarify that we did try to optimize the forward-KL constrained objective and found in our toy experiments ( Section 4.4 ) that this generally failed . This failure is echoed by a variety of previous work ( eg.RAML ( Norouzi et al. , 2016 ) , but also in the RL literature ) . We do agree that flipping the KL divergence is an unsatisfactory approximation ( and we will update the paper to further emphasize this point ) , however as shown empirically in previous work , it seems to be effective in practice . Ultimately , we choose to make concessions to the optimization concerns , to the expense of theoretical exactness . We do believe that attempting to directly minimize the forward KL is a promising future direction . However , the current version of the paper demonstrates empirically that the KL reversal is not only viable , but is also sufficient for P-DRO to yield more robust models . If performing P-DRO with the forward KL results in superior results than the results that we have obtained with reverse KL , then we argue that it would only further improve the utility of our already-promising approach . \\ > Missing references ( Husain , 2020 and Nguyen et al.2020 ) We thank the reviewer for pointing out these recent relevant references , which we \u2019 ll include in the upcoming revised version of the paper ."}, {"review_id": "ZDnzZrTqU9N-3", "review_text": "The paper proposes to define the uncertainty set in the DRO problem as a family of parametric generative models , which is to allow more flexibility in the choice of the uncertainty set architecture . To realize this idea , the paper first proposes a new relaxation of the DRO game 's inner maximization problem ( with KL constraints ) so as to improve the training stability . It then develops a principled approach to select the hyper-parameters of the proposed method . Strengths : + The paper is well-written . + The proposed method is novel and important for the DRO community . + Experiments with real-world problems are conducted to evaluate the effectiveness of the proposed method . I particularly like the experimental analysis the authors conducted to understand the behavior of their proposed method . Weaknesses : - The experiments are only on NLP tasks . I have few questions to the authors : 1 ) How good the adversary model needs to be for the proposed method to perform well ? In the experiments , an auto-regressive transformer model based on the GPT-2 language model is employed . What is the accuracy of this model on the train dataset of the DRO problem ? Will the proposed method performance be too sensitive to the accuracy of the adversary model ? 2 ) In the experiment ( last paragraph of Section 5.1 ) , the temperature \\tau and the normalizing window k are fixed whilst the adversary learning rate \\lambda is searched by grid-search . So how \\tau and k are selected in practice ? What is the performance of the proposed method when \\tau and k vary ?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their encouraging feedback . We address their specific concerns below , and we are happy to continue discussing any of these points or answer follow-up questions . \\ > The experiments are only on NLP tasks While in general our proposed approach can be applied to any modality , the reviewer is correct that we only experiment on NLP datasets ( except in our toy experiment in Section 4.4 ) . As mentioned in the paper , this is motivated by the widely recognized success of language models , which make them a prime candidate for testing P-DRO . For other modalities where generative models are either not readily available or can \u2019 t provide normalized probabilities efficiently , such as GANs , an alternative solution might be to model the likelihood ratio q_psi/p directly , however this poses a variety of other challenges , which we defer to future work . \\ > How good of a generative model is the adversary , and how important is its performance ? As pointed out by the reviewer , in most of our experiments , the adversary is a transformer model based on the GPT-2 architecture ( albeit with fewer parameters than the actual GPT-2 model ) . On the BiasedSST dataset , this model attains a \u201c perplexity \u201d of 49.84 ( note : this model predicts both label and text , as such the perplexity is not directly comparable to regular language models ) . Measuring the effect of the adversary \u2019 s performance on the effectiveness of P-DRO is an interesting ablation study . Should time and computing resources permit , we will make our best efforts to obtain additional results with smaller adversaries during the rest of the rebuttal period . \\ > How are \\tau and k chosen in practice ? As shown in Section 4 , \\tau and k can be chosen via grid-search using the Minmax criterion described in Section 3 . For the experiments in Section 5 specifically , we fixed \\tau and k in order to reduce the search space and make grid search more manageable . Possibly , better results could be obtained in Section 5 by searching for better \\tau and k. We will edit the paper to clarify this . As to the effect of the choice of k and \\tau on performance , we performed an ablation study on BiasedSST . We start from the configuration \\lambda=10^-4 , k=5 and \\tau=0.01 and vary either k or \\tau . We report two numbers for each configuration : robust accuracy of the best model using Greedy-Minmax stopping and using Oracle stopping . The latter is useful to disentangle the effect of the stopping criterion . ||Robust Accuracy ( Minmax stopping ) |Robust Accuracy ( Oracle stopping ) | |-|-|-| |k=0 | 41.98 \u00b1 4.48 | 49.60 \u00b1 5.39 | |k=5 | 44.74 \u00b1 3.24 | 50.43 \u00b1 5.05 | |k=10 | 32.17 \u00b1 11.20 | 50.95 \u00b1 5.01 | |-|-|-| |\\tau=0.1 | 39.72 \u00b1 5.55 | 50.00 \u00b1 4.98 | |\\tau=0.01 | 44.74 \u00b1 3.24 | 50.43 \u00b1 5.05 | |\\tau=0.001 | 44.74 \u00b1 3.24 | 50.87 \u00b1 5.09 | Interestingly , neither k nor \\tau have a strong effect on robust performance when using Oracle stopping . We will add these ablation studies to the updated manuscript ."}], "0": {"review_id": "ZDnzZrTqU9N-0", "review_text": "This paper considers distributionally robust optimization ( DRO ) and uses the neural generative models to characterize the uncertainty sets . To tackle the optimization challenges , several implementation tricks are incorporated to solve the minimax problem . The proposed robust method is validated on NLP tasks . This paper is well-written and of a good structure . Although the main idea is simple , the authors make several modifications to the algorithm to make it tractable and with performance guaranteed heuristically . To summarize , the main contribution of this paper is a new algorithm that combines standard techniques , such as Lagrangian relaxation and KL reverse , into the DRO problem with KL uncertainty sets . And this algorithm was shown to perform well under synthetic and real-data NLP tasks . Since there is no novel techniques proposed in this paper and there is no performance guarantee for the proposed framework , overall , I think this is a borderline paper due to its limitations in theoretical development and technical novelty . Moreover , if the main focus of this paper is on developing a new computational framework that can lead to more robust results , then the authors should compare with more benchmark methods , while I only see the comparison with ERM , Topic-CVaR , etc . For example , I am wondering is it applicable to compare with Wasserstein DRO or Huber 's classical work of Total variation based DRO , or some other DRO works in the literature , so that it will be more convincing on the performance of the proposed method . A minor typo in the paper : in section 6 , there is a duplicated `` produce '' in the sentence : `` In such cases where good quality generative models are unavailable , or such model can not produce produce densities efficiently '' .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their encouraging comments and helpful feedback . We address their specific concerns below , and we are happy to continue discussing any of these points or answer follow-up questions . \\ > Since there is no novel techniques proposed in this paper and there is no performance guarantee for the proposed framework , overall , I think this is a borderline paper due to its limitations in theoretical development and technical novelty We do agree with the reviewer that the paper does not make a strong theoretical contribution , and our approach is very much motivated by proposing a method that works in a practical scenario rather than what is most satisfying theoretically . The main novelty of the paper is the use of a parametric family as the uncertainty set in DRO . However , our contribution goes beyond the brute-force approach of simply plugging parametric models into the classical DRO min-max ( which doesn \u2019 t work , as demonstrated in our toy experiments in Section 4.4 ) . In particular while a number of the adjustments detailed in Section 3 are not novel by themselves ( lagrangian relaxation , KL reversal ... ) , the fact that they can be combined and applied to the problem of parametric DRO is ( in the authors \u2019 opinion ) far from being a given . \\ > More baselines The reviewer \u2019 s point regarding more baselines is well taken . First , we would like to point out that Wasserstein DRO presumes the existence of a canonical metric on the input space , of which there is none for discrete sequential inputs such as natural language sentences . Adaptation of Wasserstein DRO to NLP is an interesting direction , but it is far from straightforward , and would warrant a more thorough investigation of its own . Huber \u2019 s work on robust statistics solves a related but different setting : ensuring that models are robust to an adversary who modifies the training data . Our paper considers the problem where the training data is fixed , and the test distribution is potentially different . To our knowledge , Huber \u2019 s robust statistics approaches do not directly address KL-robust DRO problems . That being said , we did run the additional baseline of non-parametric KL-constrained DRO , inspired by the formulation of Hu et al . ( 2016 ) ( https : //arxiv.org/abs/1611.02041 ) . We refer to our general response to all reviewers for more details and initial results . We are currently working towards adding these additional baseline results throughout the paper ."}, "1": {"review_id": "ZDnzZrTqU9N-1", "review_text": "Good points - - The objective of the paper is sound : fight distributional shift in systems whose predictions might have life-changing consequences ( e.g data bias toxicity prediction models , etc . ) . - The paper is well-written and easy to follow . Bad points - - I do n't see just how this model is `` parametric '' . In statistics , `` parametric '' the adversarial distribution is modeled as a gaussian , etc . with sought-for parameters ( mean , covariance , etc . ) . In the absence of that , I would have expected `` parametric '' to mean parametrizing the adversarial distribution as the ( softmax ) output of a neural network . Neither of the above is the case in this paper . So , what are the `` parameters '' in the proposed DRO adversary ? All I can see is that the authors do a full search over all distributions , subject to a KL constraint ( see sections 2 and 3.2 ) . There is nothing `` parametric '' about this . - The authors say `` In particular , direct gradient descent on the uncertainty set suffers from instability due to the large variance of the gradients ( Greensmith et al. , 2004 ) , and hyper-parameter selection is not straightforward . '' I 'm not sure about this claim ( which is one of the main premises of the manuscript.What do the authors make of this paper for example Faury et al . ( AAAI 2020 ) `` Distributionally Robust Counterfactual Risk Minimization '' ? The authors of that paper demonstrate how to efficiently formulate and solve KL-based DRO problems . That paper also contains both theoretical and practical insights . - The technical contribution of the paper is negligible ( if any ) . - The arguments in the paper very heuristic . - Since the paper is supposed to be empirical ( see previous points ) , I would have expected experiments on real datasets . Errors - Change `` solve the inner-max efficient '' to `` solve the inner maximization problem efficiently '' - Change `` $ x , y ~ $ `` to `` $ ( x , y ) ~ $ '' all through the manuscript - Eqn ( 5 ) : why not take $ p $ and $ q_ { \\psi_0 } $ to equal the empirical distribution ( as is usually done ) in DRO ? - In eqn defining $ q_ { \\psi_0 } $ , replace $ \\arg\\max_ { q_\\psi } $ with $ \\arg\\max_\\psi $", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their detailed feedback . We address their specific concerns below , and we are happy to continue discussing any of these points or answer follow-up questions . \\ > I do n't see just how this model is `` parametric '' The proposed approach is parametric in the sense that the confusion set is represented by a parametric family of models . To take the reviewer \u2019 s example , in one of our ablation experiments ( Section 4.4 ) , the adversary is indeed a gaussian , and its sought-for parameters are the mean . In our other experiments , the uncertainty set is composed of transformer-based language models . This setting is also parametric in the sense that each possible test distribution in the uncertainty set is associated with a set of parameters for a transformer model , and these parameters are optimized jointly with the classification model following the procedure described in Section 3 . To clarify this point , we have added results for a non-parametric KL-constrained baseline . Please refer to our general response to all reviewers for more details . \\ > Comparison to Faury et al . ( AAAI 2020 ) `` Distributionally Robust Counterfactual Risk Minimization '' We thank the reviewer for pointing out this relevant citation , which we \u2019 ll include into the next revision . As far as we can tell , this paper proposes a non-parametric KL-constrained formulation of DRO which is very similar to that of Hu & Hong ( 2013 ) or Hu et al . ( 2016 ) , but applied to CRM . In fact , Faury et al.corroborate our point : they state ( eg.Section 2.2 ) that \u201c We are interested in DRO instances that are amenable to direct optimization . To this end , we focus here only on uncertainty sets U based on information divergence \u201d . In our work , we consider the challenges that occur when moving outside this tractable set of uncertainty sets , and consider intersections of the KL uncertainty set with parametric models where the inner-maximization problem becomes intractable ( hence the need for the approximations described in Section 3 ) . Again , we refer to our general response for more details on an additional , relevant baseline . \\ > The technical contribution of the paper is negligible ( if any ) To the best of our knowledge , we are the first to investigate DRO with neural-network based parametric confusions sets and to address the associated challenges ( intractability of the inner-max , difficulty of enforcing the KL constraint ... ) . We believe these are all technical contributions that are not attested to by previous research , and all required a significant amount of thought , design , implementation , and empirical validation . \\ > Since the paper is supposed to be empirical ( see previous points ) , I would have expected experiments on real datasets . We would like to point out that the experiments in the final section of the paper are performed on two toxicity detection datasets , which are well established datasets addressing an important real problem and widely used in the community : Davidson et al . ( 2017 ) and Founta et al . ( 2018 ) ( 760 and 109 citations respectively according to google scholar ) . \\ > In Eq.why not take q_psi_0 and p to equal the empirical distribution ( as is usually done ) in DRO ? Due to its parametric nature , the support of the adversary q_\\psi is larger than that of the empirical distribution , therefore , we need to use the true data distribution p ( which we assume has the same support as q_psi ) in the denominator . In practice , since p is unavailable , we resort to the MLE q_\\psi_0 , which also has the same support ."}, "2": {"review_id": "ZDnzZrTqU9N-2", "review_text": "TL ; DR : The paper makes an interesting contribution from a practical point of view , but two important theoretical concerns need to be addressed in the rebuttal for acceptance . The paper proposes the use of ideas taken from the literature on distributionally robust optimization within a parametric framework . More precisely , the main idea is to consider only a ( parameterised ) subset of the traditional KLD-uncertainty sets . As this avoids the need for elegant analytic solutions ( at the expense of a more brute force computation ) , it has the flavour of a more \u2018 black box \u2019 approach towards the deployment of DRO . Overall , I really enjoyed the way this paper was written . Purpose and use of the contributions are clear throughout , and the reader is drawn in . I also liked the contribution and believe that the paper demonstrated its ideas to be useful . There are however two points of major concern from a more theoretical side . In my mind , these are rather substantial , and I will list them below . To recommend that the paper be accepted , these points will have to be addressed in a future version of the paper : ( 1 ) How do you ensure that the KLD between $ q_ { \\psi } $ and $ p $ is finite ? p clearly is the empirical measure ( as is emphasised e.g.just above eq . ( 5 ) ) , but $ q_ { \\psi } $ will be continuous . This means that the KLD between the two distributions is not defined/infinity for any value of \\psi ( Mismatch of support problem ) . These kind of problems are the precise reasons why other quasi-distances ( like the Wasserstein distances ) have become increasingly interesting for ML . As far as I can tell , this problem is not elaborated upon anywhere in the paper . ( 2 ) It is totally unclear to me why it should be viable to suddenly flip the direction of the KLD . The KLD is not symmetric and in general will not even have the same minimum . In fact , generally speaking the only time the minimum will be the same in either direction is when the KLD \u2019 s global minimum is such that $ q_ { \\psi } = q_ { \\tau , \\theta } $ ( i.e.we can drop the KLD term for the loss in ( 7 ) completely , so that it simply equals $ C $ ) . Given the definition of $ q_ { \\tau , \\theta } $ , it is unreasonable to assume that this global minimum is attained . This makes the flipping of the KLD \u2019 s direction questionable at best . Calling the outcome an \u2018 approximation \u2019 is then grossly inaccurate . ( See e.g.the visualisations here : https : //wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/ ) Lastly , since the chief concern of the paper is the construction of new uncertainty sets , I would have liked to see two additional recent references discussed which have produced uncertainty sets purely based on moments ( https : //arxiv.org/abs/2007.04458 , ICML 2020 ) as well as on general IPMs ( https : //arxiv.org/abs/2006.04349 , NeurIPs 2020 ) . Both these types of uncertainty sets do * not * suffer from the mismatch of support problem , and\u2014like the famous f-divergence based uncertainty sets\u2014have elegant dual forms . POST-DISCUSSION : The authors promised to clarify the two issues I pointed out in ways that are satisfactory for a paper whose main concern is practicality ( as opposed to theoretical rigour ) . I will thus raise my score to a weak accept .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate the reviewer \u2019 s enthusiasm for our approach , and are grateful for the insightful feedback . We address their specific concerns below , and we are happy to continue discussing any of these points or answer follow-up questions . \\ > How do we ensure that the KL divergence between q_\\psi and the empirical distribution p is finite or even well-defined ? The reviewer makes an keen observation that the empirical distribution , being of finite support , does not have a finite KL divergence with q_\\psi . In truth , we are only interested in the KL divergence between q_psi and the true underlying data distribution , which we can reasonably assume is finite . We agree that the phrasing of the paper is confusing in this regard , as we interchangeably refer to both the \u201c true \u201d data distribution and the empirical distribution ( of finite support over the training data ) as p. We will edit the paper to make this clearer . \\ > Why is flipping the KL viable ? The reviewer is correct that the KL is not symmetric , and as such the reversed loss L_rev is not equivalent to the original \u201c forward \u201d KL minimization problem . First , we would like to clarify that we did try to optimize the forward-KL constrained objective and found in our toy experiments ( Section 4.4 ) that this generally failed . This failure is echoed by a variety of previous work ( eg.RAML ( Norouzi et al. , 2016 ) , but also in the RL literature ) . We do agree that flipping the KL divergence is an unsatisfactory approximation ( and we will update the paper to further emphasize this point ) , however as shown empirically in previous work , it seems to be effective in practice . Ultimately , we choose to make concessions to the optimization concerns , to the expense of theoretical exactness . We do believe that attempting to directly minimize the forward KL is a promising future direction . However , the current version of the paper demonstrates empirically that the KL reversal is not only viable , but is also sufficient for P-DRO to yield more robust models . If performing P-DRO with the forward KL results in superior results than the results that we have obtained with reverse KL , then we argue that it would only further improve the utility of our already-promising approach . \\ > Missing references ( Husain , 2020 and Nguyen et al.2020 ) We thank the reviewer for pointing out these recent relevant references , which we \u2019 ll include in the upcoming revised version of the paper ."}, "3": {"review_id": "ZDnzZrTqU9N-3", "review_text": "The paper proposes to define the uncertainty set in the DRO problem as a family of parametric generative models , which is to allow more flexibility in the choice of the uncertainty set architecture . To realize this idea , the paper first proposes a new relaxation of the DRO game 's inner maximization problem ( with KL constraints ) so as to improve the training stability . It then develops a principled approach to select the hyper-parameters of the proposed method . Strengths : + The paper is well-written . + The proposed method is novel and important for the DRO community . + Experiments with real-world problems are conducted to evaluate the effectiveness of the proposed method . I particularly like the experimental analysis the authors conducted to understand the behavior of their proposed method . Weaknesses : - The experiments are only on NLP tasks . I have few questions to the authors : 1 ) How good the adversary model needs to be for the proposed method to perform well ? In the experiments , an auto-regressive transformer model based on the GPT-2 language model is employed . What is the accuracy of this model on the train dataset of the DRO problem ? Will the proposed method performance be too sensitive to the accuracy of the adversary model ? 2 ) In the experiment ( last paragraph of Section 5.1 ) , the temperature \\tau and the normalizing window k are fixed whilst the adversary learning rate \\lambda is searched by grid-search . So how \\tau and k are selected in practice ? What is the performance of the proposed method when \\tau and k vary ?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their encouraging feedback . We address their specific concerns below , and we are happy to continue discussing any of these points or answer follow-up questions . \\ > The experiments are only on NLP tasks While in general our proposed approach can be applied to any modality , the reviewer is correct that we only experiment on NLP datasets ( except in our toy experiment in Section 4.4 ) . As mentioned in the paper , this is motivated by the widely recognized success of language models , which make them a prime candidate for testing P-DRO . For other modalities where generative models are either not readily available or can \u2019 t provide normalized probabilities efficiently , such as GANs , an alternative solution might be to model the likelihood ratio q_psi/p directly , however this poses a variety of other challenges , which we defer to future work . \\ > How good of a generative model is the adversary , and how important is its performance ? As pointed out by the reviewer , in most of our experiments , the adversary is a transformer model based on the GPT-2 architecture ( albeit with fewer parameters than the actual GPT-2 model ) . On the BiasedSST dataset , this model attains a \u201c perplexity \u201d of 49.84 ( note : this model predicts both label and text , as such the perplexity is not directly comparable to regular language models ) . Measuring the effect of the adversary \u2019 s performance on the effectiveness of P-DRO is an interesting ablation study . Should time and computing resources permit , we will make our best efforts to obtain additional results with smaller adversaries during the rest of the rebuttal period . \\ > How are \\tau and k chosen in practice ? As shown in Section 4 , \\tau and k can be chosen via grid-search using the Minmax criterion described in Section 3 . For the experiments in Section 5 specifically , we fixed \\tau and k in order to reduce the search space and make grid search more manageable . Possibly , better results could be obtained in Section 5 by searching for better \\tau and k. We will edit the paper to clarify this . As to the effect of the choice of k and \\tau on performance , we performed an ablation study on BiasedSST . We start from the configuration \\lambda=10^-4 , k=5 and \\tau=0.01 and vary either k or \\tau . We report two numbers for each configuration : robust accuracy of the best model using Greedy-Minmax stopping and using Oracle stopping . The latter is useful to disentangle the effect of the stopping criterion . ||Robust Accuracy ( Minmax stopping ) |Robust Accuracy ( Oracle stopping ) | |-|-|-| |k=0 | 41.98 \u00b1 4.48 | 49.60 \u00b1 5.39 | |k=5 | 44.74 \u00b1 3.24 | 50.43 \u00b1 5.05 | |k=10 | 32.17 \u00b1 11.20 | 50.95 \u00b1 5.01 | |-|-|-| |\\tau=0.1 | 39.72 \u00b1 5.55 | 50.00 \u00b1 4.98 | |\\tau=0.01 | 44.74 \u00b1 3.24 | 50.43 \u00b1 5.05 | |\\tau=0.001 | 44.74 \u00b1 3.24 | 50.87 \u00b1 5.09 | Interestingly , neither k nor \\tau have a strong effect on robust performance when using Oracle stopping . We will add these ablation studies to the updated manuscript ."}}