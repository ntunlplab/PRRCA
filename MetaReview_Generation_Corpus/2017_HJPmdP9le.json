{"year": "2017", "forum": "HJPmdP9le", "title": "Efficient Summarization with Read-Again and Copy Mechanism", "decision": "Reject", "meta_review": "This work presents a method for reducing the target vocabulary for abstractive summarization by employing a read-again copy mechanism. Reviewers felt that the paper was lacking in several regards particularly focusing on issues clarity and originality. \n \n Issues raised:\n - Several reviewers focused on clarity/writing issues of the work, highlighting inconsistencies of notation, justifications, and extraneous material. They recommend a rewrite of the work. \n - The question of originality and novelty is important. The copy mechanism has now been invented many times. Reviewers argue that simply applying this to summary is not enough, or at least not the focus of ICLR. The same question is posed about self-attention.\n - In terms of quality, there are concerns about comparisons to more recent baselines and getting the experiments to run correctly on Gigaword.", "reviews": [{"review_id": "HJPmdP9le-0", "review_text": "Summary: This paper proposes a read-again attention-based representation of the document with the copy mechanism for the summarization task. The model reads each sentence in the input document twice and creates a hierarchical representation of it instead of a bidirectional RNN. During the decoding, it uses the representation of the document obtained via the read-again mechanism and points the words that are OOV in the source document. The model does abstractive summarization. The authors show improvements on DUC 2004 dataset and provide an analysis of their model with different configurations. Contributions: The main contribution of this paper is the read-again attention mechanism where the model reads the same sentence twice and obtains a better representation of the document. Writing: The text of this paper needs more work. There are several typos and the explanations of the model/architecture are not really clear, some parts of the paper feel somewhat bloated. Pros: - The proposed model is a simple extension to the model to the model proposed in [2] for summarization. - The results are better than the baselines. Cons: - The improvements are not that large. - Justifications are not strong enough. - The paper needs a better writeup. Several parts of the text are not using a clear/precise language and the paper needs a better reorganization. Some parts of the text is somewhat informal. - The paper is very application oriented. Question: - How does the training speed when compared to the regular LSTM? Some Criticisms: A similar approach to the read again mechanism which is proposed in this paper has already been explored in [1] in the context of algorithmic learning and I wouldn\u2019t consider the application of that on the summarization task a significant contribution. The justification behind the read-again mechanism proposed in this paper is very weak. It is not really clear why additional gating alpha_i is needed for the read again stage. As authors also suggest, pointer mechanism for the unknown/rare words [2] and it is adopted for the read-again attention mechanism. However, in the paper, it is not clear where the real is the gain coming from, whether from \u201cread-again\u201d mechanism or the use of \u201cpointing\u201d. The paper is very application focused, the contributions of the paper in terms of ML point of view is very weak. It is possible to try this read-again mechanism on more tasks other than summarization, such as NMT, in order to see whether if those improvements are The writing of this paper needs more work. In general, it is not very well-written. Minor comments: Some of the corrections that I would recommend fixing, On page 4: \u201c\u2026 better than a single value \u2026 \u201d \u2014> \u201c\u2026 scalar gating \u2026\u201d On page 4: \u201c\u2026 single value lacks the ability to model the variances among these dimensions.\u201d \u2014> \u201c\u2026 scalar gating couldn\u2019t capture the \u2026.\u201d On page 6: \u201c \u2026 where h_0^2 and h_0^'2 are initial zero vectors \u2026 \u201c \u2014> \u201c\u2026 h_0^2 and h_0^'2 are initialized to a zero vector in the beginning of each sequence \u2026\" There are some inconsistencies for example parts of the paper refer to Tab. 1 and some parts of the paper refer to Table 2. Better naming of the models in Table 1 is needed. The location of Table 1 is a bit off. [1] Zaremba, Wojciech, and Ilya Sutskever. \"Reinforcement learning neural Turing machines.\" arXiv preprint arXiv:1505.00521 362 (2015). [2] Gulcehre, Caglar, et al. \"Pointing the Unknown Words.\" arXiv preprint arXiv:1603.08148 (2016). ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable comments and questions . Here are some rebuttals . - How does the training speed when compared to the regular LSTM ? We compare the training time of our read-again model with our GRU baseline . A vanilla GRU encoder-decoder architecture takes 0.2s per minibatch , while our Read-Again GRU encoder-decoder takes 0.27s for the same minibatch . - Justification of Read-Again Vanilla RNN calculates the representation of each word conditioned only on the past history ( words ) . This might be sub-optimal especially when the input sequence is long and future words also bring important information . This is indeed the case of summarization where the input text is lengthy . Our Read-Again mechanism tries to address this issue by building the representation of each world once the full input text has been read . For example , the gating alpha_i depends not only in the current world but also in future words . We plan to generalize our model to perform document-level summarization . We expect and even larger improvement as the inputs are much longer . - Copy Mechanism Although our copy mechanism is inspired by Pointer Network , the main difference is that our model selects automatically whether to copy a word from the input sequence or to generate a new word from the pre-defined dictionary , while Pointer Network can only select words from the input sequences . Therefore , our copy mechanism is more natural for abstractive summarization task . Furthermore , our model can extract embeddings of rare-words in the input sequence . This help us decrease the dictionary size of the decoder as well as encoder . ( See extra experiment response ) . As for the reason of gain in the experiments , we refer the reader to Table.1 . Our Read-Again models ( Ours-GRU and Ours-LSTM ) defeat vanilla RNNs . Besides , incorporating the Copy mechanism further improves performance . - Other comments Thanks for your advice . We \u2019 re improving our writing ."}, {"review_id": "HJPmdP9le-1", "review_text": "This paper proposed two incremental ideas to extend the current state-of-the-art summarization work based on seq2seq models with attention and copy/pointer mechanisms. 1. This paper introduces 2-pass reading where the representations from the 1st-pass is used to re-wight the contribution of each word to the sequential representation of the 2nd-pass. The authors described how such a so-called read-again process applies to both GRU and LSTM. 2. On the decoder side, the authors also use the softmax to choose between generating from decoder vocabulary and copying a source position, with a new twist of representing the previous decoded word Y_{t-1} differently. This allows the author to explore a smaller decoder vocabulary hence led to faster inference time without losing summarization performance. This paper claims the new state-of-the-art on DUC2004 but the comparison on Gigaword seems to be incomplete (missing more recent results after Rush 2015 etc). While the overall work is solid, there are also other things missing scientifically. For example, - how much computational costs does the 2nd pass reading add to the end-to-end system? - How does the decoder small vocabulary trick work without 2nd-pass reading on the encoder side for both summarization performance and runtime speed? - There are other ways to improve the embedding of a sentence. How does the 2nd-pass reading compare to recent work from multiple authors on self-attention and/or LSTMN? For example, Cheng et al. 2016, Long Short-Term Memory-Networks for Machine Reading; Parikh et al. 2016, A Decomposable Attention Model for Natural Language Inference?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the valuable comments and questions . Here are some rebuttals . - Experiments on Gigaword : Previous state-of-the-art work is evaluated on an internal testing set . However , this set and the source code are not openly available at the time we submitting this paper , and thus it \u2019 s hard to directly compare with previous work on Gigaword . We have to compare our model with our self-implemented baselines on a randomly sampled testing set , and then finish the comparison with other state-of-the-art models on DUC2004 . - Computation Cost : The training time of vanilla GRU is around 2 days while our Read-Again GRU takes less than 3 days . More specifically , it takes 0.20s per minibatch for a GRU and 0.27s for a Read-Again GRU . Both models achieve the best validation perplexity after running for 10 epochs over the whole training set . - Small vocabulary : We argue that the copy mechanism is orthogonal to the read-again mechanism . We empirically observe in our experiments that our copy mechanism consistently improves the model performance with or without read-again mechanism . - Other work : Thanks for pointing out to additional papers . The main idea of the self-attention is to expand the memory capacity in vanilla LSTM , while the output of the model during each step is still only dependent on the past history . Our attention mechanism tries to address this issue , and utilize the information from both past and future words ( in a self-attention maner differed from bi-direction GRU ) ."}, {"review_id": "HJPmdP9le-2", "review_text": "This work explores the neural models for sentence summarisation by using a read-again attention model and a copy mechanism which grants the ability of direct copying word representations from the source sentences. The experiments demonstrate the model achieved better results on DUC dataset. Overall, this paper is not well-written. There are confusing points, some of the claims are lack of evidence and the experimental results are incomplete. Detailed comments: -Read-again attention. How does it work better than a vanilla attention? What would happen if you read the same sentences multiple times? Have you compared it with staked LSTM (with same number of parameters)? There is no model ablation in the experiment section. -Why do you need reading two sentences? The Gigaword dataset is a source-to-compression dataset which does not need multiple input sentences. How do you compare your model with single sent input and two sent input? -Copy mechanism. What if there are multiple same words appeared in the source sentences to be copied? According to equation (5), you only copy one vector to the decoder. However, there is no this kind of issue for a hard copy mechanism. Besides, there is no comparison between the hard copy mechanism and this vector copy mechanism in the experiment section -Vocabulary size. This part is a bit off the main track of this paper. If there is no evidence showing this is the special property of vector copy mechanism, it would be trivial in this paper. -Experiments. On the DUC dataset, it compares the model with other up-to-date models, while on the Gigaword dataset paper only compares the model with the ABS Rush et al. (2015) and the GRU (?), which are quite weak baseline models. It is irresponsible to claim this model achieved the state-of-the-art performance in the context of summarization. Typos: (1) Tab. 1. -> Table 1. (2) Fig. 3.1.2.? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable comments and questions . Here are some rebuttals . - Read-Again attention : There are two different and orthogonal attention mechanism in our model . The first one models attention between the decoder and the encoder , as commonly used in sequence to sequence models , and might be what you referred to as vanilla attention . The second attention mechanism we use is our Read-Again attention , which is orthogonal and thus can not be compared . One could use multiple passes of reading , but that would be expensive . Instead , we expect the first pass to capture enough global information about the full sentence . - Two sentences : Gigaword is actually a document to sentence summarization dataset . However , previous work was mostly concerned with compressing the first sentence in a document to a shorter summary , and thus they cut off the remaining part of document in their experiments . Following this convention , we first evaluate our models on the task of summarizing the first sentence . We also demonstrated that our model could utilize information from more than 1 sentence , achieving better results when feeding 2 sentences rather than one . - Copy mechanism : The copy anchor ( output-input copied word alignment ) is constructed if a word in the output is Out of Vocabulary ( OOV ) for the decoder but it appears in the input . When OOV words appear multiple times , we use its first appearance in the input Experiments : Previous state-of-the-art work is evaluated on an internal testing set . However , this set and the source code are not openly available at the time we submitting this paper , and thus it \u2019 s hard to directly compare with previous work on Gigaword . Therefore , we first compare our Read-Again attention with GRU baseline , which is a fair comparison with our models , to testify the effectiveness of our attention mechanism . We then conduct the comparison of our model with state-of-the-art models on the standard DUC summarization task . Typos : Thank you for pointing them out . We \u2019 ll fix them ."}], "0": {"review_id": "HJPmdP9le-0", "review_text": "Summary: This paper proposes a read-again attention-based representation of the document with the copy mechanism for the summarization task. The model reads each sentence in the input document twice and creates a hierarchical representation of it instead of a bidirectional RNN. During the decoding, it uses the representation of the document obtained via the read-again mechanism and points the words that are OOV in the source document. The model does abstractive summarization. The authors show improvements on DUC 2004 dataset and provide an analysis of their model with different configurations. Contributions: The main contribution of this paper is the read-again attention mechanism where the model reads the same sentence twice and obtains a better representation of the document. Writing: The text of this paper needs more work. There are several typos and the explanations of the model/architecture are not really clear, some parts of the paper feel somewhat bloated. Pros: - The proposed model is a simple extension to the model to the model proposed in [2] for summarization. - The results are better than the baselines. Cons: - The improvements are not that large. - Justifications are not strong enough. - The paper needs a better writeup. Several parts of the text are not using a clear/precise language and the paper needs a better reorganization. Some parts of the text is somewhat informal. - The paper is very application oriented. Question: - How does the training speed when compared to the regular LSTM? Some Criticisms: A similar approach to the read again mechanism which is proposed in this paper has already been explored in [1] in the context of algorithmic learning and I wouldn\u2019t consider the application of that on the summarization task a significant contribution. The justification behind the read-again mechanism proposed in this paper is very weak. It is not really clear why additional gating alpha_i is needed for the read again stage. As authors also suggest, pointer mechanism for the unknown/rare words [2] and it is adopted for the read-again attention mechanism. However, in the paper, it is not clear where the real is the gain coming from, whether from \u201cread-again\u201d mechanism or the use of \u201cpointing\u201d. The paper is very application focused, the contributions of the paper in terms of ML point of view is very weak. It is possible to try this read-again mechanism on more tasks other than summarization, such as NMT, in order to see whether if those improvements are The writing of this paper needs more work. In general, it is not very well-written. Minor comments: Some of the corrections that I would recommend fixing, On page 4: \u201c\u2026 better than a single value \u2026 \u201d \u2014> \u201c\u2026 scalar gating \u2026\u201d On page 4: \u201c\u2026 single value lacks the ability to model the variances among these dimensions.\u201d \u2014> \u201c\u2026 scalar gating couldn\u2019t capture the \u2026.\u201d On page 6: \u201c \u2026 where h_0^2 and h_0^'2 are initial zero vectors \u2026 \u201c \u2014> \u201c\u2026 h_0^2 and h_0^'2 are initialized to a zero vector in the beginning of each sequence \u2026\" There are some inconsistencies for example parts of the paper refer to Tab. 1 and some parts of the paper refer to Table 2. Better naming of the models in Table 1 is needed. The location of Table 1 is a bit off. [1] Zaremba, Wojciech, and Ilya Sutskever. \"Reinforcement learning neural Turing machines.\" arXiv preprint arXiv:1505.00521 362 (2015). [2] Gulcehre, Caglar, et al. \"Pointing the Unknown Words.\" arXiv preprint arXiv:1603.08148 (2016). ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable comments and questions . Here are some rebuttals . - How does the training speed when compared to the regular LSTM ? We compare the training time of our read-again model with our GRU baseline . A vanilla GRU encoder-decoder architecture takes 0.2s per minibatch , while our Read-Again GRU encoder-decoder takes 0.27s for the same minibatch . - Justification of Read-Again Vanilla RNN calculates the representation of each word conditioned only on the past history ( words ) . This might be sub-optimal especially when the input sequence is long and future words also bring important information . This is indeed the case of summarization where the input text is lengthy . Our Read-Again mechanism tries to address this issue by building the representation of each world once the full input text has been read . For example , the gating alpha_i depends not only in the current world but also in future words . We plan to generalize our model to perform document-level summarization . We expect and even larger improvement as the inputs are much longer . - Copy Mechanism Although our copy mechanism is inspired by Pointer Network , the main difference is that our model selects automatically whether to copy a word from the input sequence or to generate a new word from the pre-defined dictionary , while Pointer Network can only select words from the input sequences . Therefore , our copy mechanism is more natural for abstractive summarization task . Furthermore , our model can extract embeddings of rare-words in the input sequence . This help us decrease the dictionary size of the decoder as well as encoder . ( See extra experiment response ) . As for the reason of gain in the experiments , we refer the reader to Table.1 . Our Read-Again models ( Ours-GRU and Ours-LSTM ) defeat vanilla RNNs . Besides , incorporating the Copy mechanism further improves performance . - Other comments Thanks for your advice . We \u2019 re improving our writing ."}, "1": {"review_id": "HJPmdP9le-1", "review_text": "This paper proposed two incremental ideas to extend the current state-of-the-art summarization work based on seq2seq models with attention and copy/pointer mechanisms. 1. This paper introduces 2-pass reading where the representations from the 1st-pass is used to re-wight the contribution of each word to the sequential representation of the 2nd-pass. The authors described how such a so-called read-again process applies to both GRU and LSTM. 2. On the decoder side, the authors also use the softmax to choose between generating from decoder vocabulary and copying a source position, with a new twist of representing the previous decoded word Y_{t-1} differently. This allows the author to explore a smaller decoder vocabulary hence led to faster inference time without losing summarization performance. This paper claims the new state-of-the-art on DUC2004 but the comparison on Gigaword seems to be incomplete (missing more recent results after Rush 2015 etc). While the overall work is solid, there are also other things missing scientifically. For example, - how much computational costs does the 2nd pass reading add to the end-to-end system? - How does the decoder small vocabulary trick work without 2nd-pass reading on the encoder side for both summarization performance and runtime speed? - There are other ways to improve the embedding of a sentence. How does the 2nd-pass reading compare to recent work from multiple authors on self-attention and/or LSTMN? For example, Cheng et al. 2016, Long Short-Term Memory-Networks for Machine Reading; Parikh et al. 2016, A Decomposable Attention Model for Natural Language Inference?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the valuable comments and questions . Here are some rebuttals . - Experiments on Gigaword : Previous state-of-the-art work is evaluated on an internal testing set . However , this set and the source code are not openly available at the time we submitting this paper , and thus it \u2019 s hard to directly compare with previous work on Gigaword . We have to compare our model with our self-implemented baselines on a randomly sampled testing set , and then finish the comparison with other state-of-the-art models on DUC2004 . - Computation Cost : The training time of vanilla GRU is around 2 days while our Read-Again GRU takes less than 3 days . More specifically , it takes 0.20s per minibatch for a GRU and 0.27s for a Read-Again GRU . Both models achieve the best validation perplexity after running for 10 epochs over the whole training set . - Small vocabulary : We argue that the copy mechanism is orthogonal to the read-again mechanism . We empirically observe in our experiments that our copy mechanism consistently improves the model performance with or without read-again mechanism . - Other work : Thanks for pointing out to additional papers . The main idea of the self-attention is to expand the memory capacity in vanilla LSTM , while the output of the model during each step is still only dependent on the past history . Our attention mechanism tries to address this issue , and utilize the information from both past and future words ( in a self-attention maner differed from bi-direction GRU ) ."}, "2": {"review_id": "HJPmdP9le-2", "review_text": "This work explores the neural models for sentence summarisation by using a read-again attention model and a copy mechanism which grants the ability of direct copying word representations from the source sentences. The experiments demonstrate the model achieved better results on DUC dataset. Overall, this paper is not well-written. There are confusing points, some of the claims are lack of evidence and the experimental results are incomplete. Detailed comments: -Read-again attention. How does it work better than a vanilla attention? What would happen if you read the same sentences multiple times? Have you compared it with staked LSTM (with same number of parameters)? There is no model ablation in the experiment section. -Why do you need reading two sentences? The Gigaword dataset is a source-to-compression dataset which does not need multiple input sentences. How do you compare your model with single sent input and two sent input? -Copy mechanism. What if there are multiple same words appeared in the source sentences to be copied? According to equation (5), you only copy one vector to the decoder. However, there is no this kind of issue for a hard copy mechanism. Besides, there is no comparison between the hard copy mechanism and this vector copy mechanism in the experiment section -Vocabulary size. This part is a bit off the main track of this paper. If there is no evidence showing this is the special property of vector copy mechanism, it would be trivial in this paper. -Experiments. On the DUC dataset, it compares the model with other up-to-date models, while on the Gigaword dataset paper only compares the model with the ABS Rush et al. (2015) and the GRU (?), which are quite weak baseline models. It is irresponsible to claim this model achieved the state-of-the-art performance in the context of summarization. Typos: (1) Tab. 1. -> Table 1. (2) Fig. 3.1.2.? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the valuable comments and questions . Here are some rebuttals . - Read-Again attention : There are two different and orthogonal attention mechanism in our model . The first one models attention between the decoder and the encoder , as commonly used in sequence to sequence models , and might be what you referred to as vanilla attention . The second attention mechanism we use is our Read-Again attention , which is orthogonal and thus can not be compared . One could use multiple passes of reading , but that would be expensive . Instead , we expect the first pass to capture enough global information about the full sentence . - Two sentences : Gigaword is actually a document to sentence summarization dataset . However , previous work was mostly concerned with compressing the first sentence in a document to a shorter summary , and thus they cut off the remaining part of document in their experiments . Following this convention , we first evaluate our models on the task of summarizing the first sentence . We also demonstrated that our model could utilize information from more than 1 sentence , achieving better results when feeding 2 sentences rather than one . - Copy mechanism : The copy anchor ( output-input copied word alignment ) is constructed if a word in the output is Out of Vocabulary ( OOV ) for the decoder but it appears in the input . When OOV words appear multiple times , we use its first appearance in the input Experiments : Previous state-of-the-art work is evaluated on an internal testing set . However , this set and the source code are not openly available at the time we submitting this paper , and thus it \u2019 s hard to directly compare with previous work on Gigaword . Therefore , we first compare our Read-Again attention with GRU baseline , which is a fair comparison with our models , to testify the effectiveness of our attention mechanism . We then conduct the comparison of our model with state-of-the-art models on the standard DUC summarization task . Typos : Thank you for pointing them out . We \u2019 ll fix them ."}}