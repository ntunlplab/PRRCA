{"year": "2021", "forum": "3X64RLgzY6O", "title": "Direction Matters: On the Implicit Bias of Stochastic Gradient Descent with Moderate Learning Rate", "decision": "Accept (Poster)", "meta_review": "This work compares and contrasts the learning rate dynamics of GD and SGD and shows that under practical learning rate settings, SGD is biased to approach the minimum along the direction of steepest descent, leading to better performance. Reviewers agree that the theoretical results are significant. The authors satisfactorily responded to reviewers\u2019 questions and improved the paper\u2019s clarity during the discussion phase.", "reviews": [{"review_id": "3X64RLgzY6O-0", "review_text": "Summary : In this paper , an implicit bias of SGD and GD in terms of the direction of convergence points is studied . This study shows that , in a setting of linear regression , SGD and GD converge to different directions , which are determined by the largest/smallest eigenvectors of a data matrix when the learning rate is moderately large . Experiments using synthetic data and Fashion MNIST support the theoretical results . Detailed comments : First of all , I do n't have much experience in the analysis of SGD/GD and my assessment for technical points may miss some important points . Overall , the paper is well written . The motivation and problem setting are clearly written . Related work is sufficiently introduced . The main theoretical results ( Theorems 1 , 2 , 4 ) are interesting . As far as I know , there 's no study to reveal the implicit bias in terms of direction . However , I have several concerns . 1.In the data generation process , the noiseless output y = < w * , x > is assumed . However , the output often contains observations noise such as an additive Gaussian model as y = < w * , x > + $ \\xi $ where $ \\xi $ is small Gaussian noise . Would it be possible to show similar results in the noisy case ? 2.More importantly , the benefit of the directional bias is not clear . Theorem 4 shows SGD achieves $ \\epsilon $ -optimal solution and GD achieves $ M $ -suboptimal solution . However , we can not conclude that SGD solution is better than GD solution because the generalization performance depends on the unknown constants $ \\epsilon $ and $ M $ . So , GD may win in some cases but SGD may win in other cases , but it seems there is no way to know in what conditions $ \\epsilon $ and $ M $ satisfy some specific values so that SGD beats GD . Would it be possible to clarify the conditions ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for appreciating the novelty and writing of our work . We answer your concerns as follows : 1 . \u201c Noisy labels \u201d . - Our main focus in this work is to justify the directional bias for SGD with moderate learning rate . Therefore our results are presented for a realizable ( interpolating ) overparameterized linear model to keep the statements concise . Nevertheless , our results are applicable to noisy label settings as well . Note that given a sufficient overparameterization , a linear model can fit the noisy labels perfectly ( in other words , interpolating the training data points with noisy labels ) with high probability [ Bartlett et al. , 2020 ] . Then we only need to revise our current results by taking a conditional expectation over the i.i.d.zero mean label noise . 2. \u201c Constants in Theorem 4 \u201d . - We apologize for the imprecise descriptions on the constants $ \\epsilon $ and $ M $ in the original Theorem 4 . In fact , according to the formal version of Theorem 4 in Appendix B , we have $ \\epsilon = o ( 1 ) $ and $ M = \\gamma_1 / \\gamma_n - o ( 1 ) $ , where $ \\gamma_1 $ and $ \\gamma_n $ are the largest and smallest ( non-zero ) eigenvalues of the data covariance matrix , respectively . Since $ M - ( 1+\\epsilon ) > \\gamma_1/\\gamma_n - 1 - o ( 1 ) > 0 $ , we have a separation between the test error of SGD with moderate learning rate and the test error of GD/SGD with small learning rate . We have made the constants in Theorem 4 precise in the revision . We also add a discussion to emphasize this separation after Theorem 4 in the revision ."}, {"review_id": "3X64RLgzY6O-1", "review_text": "In the paper the authors analyzed the convergence dynamics toward a minimum of gradient descent ( GD ) and stochastic gradient descent ( SGD ) . The algorithms are considered to start in the basin of attraction of a minimum , and using discrete steps they approach the bottom . The main result of the paper concerns the fact that , with moderate learning rate , SGD approaches the minimum along the steepest direction , contrary to GD . The authors conclude that the implicit bias of SGD does not appear in small learning rate formulation , but it becomes manifest when a moderate learning rate is considered . * It seems from the analysis that SGD differs from GD because its learning rate is modified by the batch size and therefor it uses an `` effective learning rate '' . The question is : what would happen if GD is taken with a learning rate that matches the effective learning rate of SGD ? This situation is very similar to the one observed in [ Nakkiran ( 2020 ) ] ( and also close to [ Lewkowycz et al . ( 2020 ) ] ) , where they consider GD and draw similar conclusions . * Another questions is whether it is really necessary to have discrete steps to have a difference between GD and SGD in the continuous formulations . In the introduction the authors make a comparison between the implicit biases in the literature for GD and SGD claiming that they are the same because of the small learning rate . However this is not clear to me , could the authors cite papers where this is explicitly shown ? In particular there is a literature on the effect of SGD in the continuous formulation [ e.g Jastrzebski et al.2018 ( arXiv:1711.04623 ) ; Zhu et al.2019 ( ICML2019 ) ; Xie et al.2020 ( arXiv:2002.03495 ) ] using stochastic differential equations it was possible to show that SGD has an implicit temperature that adapts to the geometry of the landscape favoring , in particular , flatter solutions . In few words , do we need to rely on the discrete formulation to understand the advantages of SGD ? * The result by [ Nakkiran ( 2020 ) ] seems to heavily rely on having a data distribution that is highly homogeneous to observe the difference between the training and the generalization loss . Do we have evidence that this is relevant for practical situations ? I would happy have more numerical experiments as the one in Fig.2 . In particular showing the effect of increasing the learning rate in GD ( and SGD ) from the current `` small '' value to a value large enough so that it does not converge anymore . Both in the synthetic and real dataset . The questions above block me from giving a higher rating , but I will be happy to increase the rate given satisfactory answers .", "rating": "7: Good paper, accept", "reply_text": "Thank you for recognizing our contributions . Regarding your concerns , we address them in the following : 1 . \u201c What would happen if GD is taken with a learning rate that matches the effective learning rate of SGD ? This situation is very similar to the one observed in [ Nakkiran ( 2020 ) ] ( and also close to [ Lewkowycz et al . ( 2020 ) ] ) , where they consider GD and draw similar conclusions. \u201d - We thank you ( and R # 4 ) for raising this question . We argue that Theorem 2 precludes GD from having a legitimate large learning rate ( LR ) to converge along large eigenvalue directions . In specific , Eq . ( 6 ) gives the range of LR where GD falls into the regime of Theorem 2 , i.e. , converging along small eigenvalue directions . Note that the upper bound in Eq . ( 6 ) , $ n / ( 2\\lambda_1 ) $ , is linear in the number of training data $ n $ , which is very huge in practice ( e.g. , $ n > 10^4 $ for MNIST and CIFAR10 ) . Therefore Eq . ( 6 ) already covers the largest possible LR that one can use in practice . We have updated remark 4 to include the above discussions . - Regarding the similarity with [ Nakkiran , 2020 ] , we would like to highlight the difference between [ Nakkiran , 2020 ] and our work . Specifically , [ Nakkiran , 2020 ] showed the separation between the test error of GD with \u201c large \u201d and annealing learning rate , and test error of GD with small learning rate . However , the \u201c large \u201d learning rate for GD in their analysis is linear in the training sample size and is impractically large as we have discussed above . In contrast , we showed that under the practically used moderate learning rate , there is a separation between the test error of SGD and the test error of GD . Therefore , our result is different from that in [ Nakkiran , 2020 ] , and our contribution is unique . We have revised remark 6 to emphasize this point . 2. \u201c Another question is whether it is really necessary to have discrete steps to have a difference between GD and SGD in the continuous formulations. \u201d - Yes , it is necessary to consider discrete steps to explain the difference between the implicit bias of GD and SGD . Regarding the reference which shows the implicit biases of GD and SGD are the same for infinitesimal learning rate , please refer to Theorem 2.1 in [ Kushner & Yin , 2003 ] . It directly shows that SGD converges to the same ODE given by GD when the learning rate tends to zero ( under mild assumptions ) . Therefore , in the continuous time scenario , SGD can not be differentiated from GD . Regarding the references [ Jastrzebski et al.2018 ; Zhu et al.2019 ; Xie et al.2020 ] you mentioned and also cited in our paper , they use SDE to approximate SGD and can not deal with infinitesimal step size ( note the Brownian motion term in the SDE is scaled by $ \\sqrt { \\eta } $ , thus an infinitesimal step size will make it vanish and the SDE will degenerate to an ODE ) . Therefore , their results are orthogonal to our results and not directly comparable to ours . 3. \u201c Do we have evidence that the data is homogeneous for practical situations , as assumed by [ Nakkiran ( 2020 ) ] ? I would be happy to have more numerical experiments as the one in ( original ) Fig.2 . In particular showing the effect of increasing the learning rate in GD ( and SGD ) from the current `` small '' value to a value large enough so that it does not converge anymore . Both in the synthetic and real dataset. \u201d - The distribution of real-world dataset is too complicated to be classified as \u201c homogeneous \u201d or \u201c inhomogeneous \u201d . But we do think data preprocessing ( e.g. , whitening and normalization ) can make the data distribution more \u201c homogeneous \u201d . This is an interesting problem and deserves more study in the future work . - Thank you for your suggestion on the experiments . We have done additional experiments on neural networks to show the effect of increasing the learning rate in GD and SGD in Appendix D.3 , Figure 4 . The additional experiment results also corroborate our theory ."}, {"review_id": "3X64RLgzY6O-2", "review_text": "This paper analyzes the differences in the convergence of SGD and GD when using `` small '' and `` moderate '' learning rates to shed light on why SGD with `` moderate '' and annealing learning rates perform well in practice . Focusing on an overparametrized linear regression problem , the paper claims that SGD with a `` moderate '' learning converges differently ( along the large eigenvalue directions ) compared to SGD/GD with a `` small '' learning rate ( along the small eigenvalue directions ) . They further show analytically for this problem that there exists a learning rate schedule ( with moderate learning rates ) such that SGD will perform + generalize well , while there exist small learning rate schedules such that GD will perform + generalize poorly . This paper is clearly written and presents a clear illustration of their hypothesis for why SGD with a moderate learning rate performs well in practice through Figure 1 and a simple linear regression example in section 3 . The paper focuses most of its analysis on a toy example problem ( overparametrized linear regression ) and the empirical experiments do not clearly support the paper 's claims . This paper should be rejected as is . Although the motivating example and illustrations are interesting , the analysis for the overparameterized linear regression problem is too specific to clearly claim that this `` directional bias '' is why SGD with moderate learning rates is successful . I have two main concerns : ( 1 ) in the special case of overparametrized linear regression , Theorem 2 only shows that GD with small learning rates converge along the smallest eigenvalue direction . It does not preclude GD from having the same behavior as SGD for larger learning rates . I also do not agree with remark 4 . ( 2 ) it 's not clear how or why the results for overparameterized linear regression would extend to generic NN losses . The example experiments ( Fig 2 ) need more detail to be clear and are not sufficient to support the hypothesis in the paper . Questions : In section 3 , does n't Eq ( 3 ) show that GD will behave identically to SGD if the learning rate is doubled ? What is considered a `` moderate '' learning rate for GD + SGD does not need to be identical , right ? For the overparametrized regression , why do we not include some form of regularization on the weights ( as would commonly be done in practice ) ? Does this affect the results ? What is the minibatch size for SGD in the experiments ? How are the results ( in Figure 2a + 2b ) sensitive to random initialization ? Minor comment : Page 6 : the definition of level set probably needs a $ \\leq $ sign .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for recognizing the novelty and writing of our work . Regarding your concerns , please see below : 1 . \u201c Theorem 2 only shows that GD with small learning rates converge along the smallest eigenvalue direction . It does not preclude GD from having the same behavior as SGD for larger learning rates. \u201d - This is a huge misunderstanding . We apologize that we did not make it clear . In fact , Theorem 2 indeed precludes GD from having a large learning rate ( LR ) to converge along large eigenvalue directions . In specific , Eq . ( 6 ) gives the range of LR where GD falls into the government of Theorem 2 , i.e. , converging along small eigenvalue directions . Note that the upper bound in Eq . ( 6 ) , $ n / ( 2\\lambda_1 ) $ , is linear in the training sample size , which is very large for typical datasets ( e.g. , $ n > 10^4 $ for MNIST and CIFAR10 ) . Therefore , Eq . ( 6 ) has already covered the largest possible LR that one can use in practice . 2. \u201c I also do not agree with remark 4. \u201d - We thank R # 4 to point out the issue on remark 4 . We have updated remark 4 to include the above discussions on Theorem 2 . We hope the new remark is crystal clear that it is impossible to have a numerically stable LR such that GD converges along large eigenvalue directions . 3. \u201c It 's not clear how or why the results for overparameterized linear regression would extend to generic NN losses. \u201d - We thank the reviewer for this comment . We agree with the reviewer that proving directional bias of SGD with moderate learning rate for neural networks is still an open problem . However , even in the linear regression setting , there is no such kind of result before our work . Without a full characterization of directional bias of SGD for the simplest possible problem \u2014 the linear regression \u2014 it seems unlikely that one can prove it for neural networks . As our work is the first work on the directional bias of SGD , we believe we have taken an important step along this direction and the contribution of our work is very significant . - In our revision , we have added new experiments ( Figure 2 ( b ) ) on NN to support our theory , which also shed light on understanding the directional bias for SGD for general NNs . A brief description of the experiments is in the \u201c reply to all reviewers \u201d response . 4. \u201c The example experiments ( original Fig 2 ) need more detail to be clear and are not sufficient to support the hypothesis in the paper. \u201d - We add a new section ( Appendix D ) to describe the full details about our experiments . We have also provided the Code in the supplemental material for the reproducibility of our experiments . We argue that Figure 2 ( a ) does verify our theory in an overparameterized linear model . As for NNs , new experiments are provided in ( new ) Figure 2 ( b ) to support our theory . Should there be any missed detail , please let us know and we are happy to clarify . 5. \u201c In section 3 , does n't Eq ( 3 ) show that GD will behave identically to SGD if the learning rate is doubled ? What is considered a `` moderate '' learning rate for GD + SGD does not need to be identical , right ? \u201d - Your comment is absolutely correct for this toy example , which involves only TWO training data points and is meant for proof of concept . In general , however , according to Theorem 2 and our previous discussions , the \u201c effective \u201d LR for GD scales linearly in the training sample size $ n $ , which is quite huge in practice . So doubling the learning rate of GD can not make GD and SGD ( with moderate LR ) behave the same . 6. \u201c For the overparameterized regression , why do we not include some form of regularization on the weights ( as would commonly be done in practice ) ? Does this affect the results ? \u201d - As our work is the first to study the implicit bias ( i.e. , implicit regularization ) of SGD with moderate learning rate , we chose to exclude explicit regularizers . By adding explicit regularization , we think our implicit bias result will be affected in general , and it is an interesting future work to study the interplays between explicit regularization and implicit regularization . 7. \u201c What is the minibatch size for SGD in the experiments ? How are the results ( in Figure 2a + 2b , original ) sensitive to random initialization ? \u201d - ( 1 ) The minibatch size of SGD is 25 . Full experiment details are in Appendix D. ( 2 ) Results are robust to random initialization . In the revision , we have updated experiments for neural networks to include the standard deviation computed over 10 runs , which shows that the results are quite robust . We have also attached Code for reproducibility . 8. \u201c Page 6 : the definition of level set probably needs a < = sign. \u201d - This is a misunderstanding . Level sets are indeed defined with \u201c = \u201d . Those defined with \u201c < = \u201d are called sublevel sets . We hope your concerns have been addressed , and are looking forward to your further comments ."}], "0": {"review_id": "3X64RLgzY6O-0", "review_text": "Summary : In this paper , an implicit bias of SGD and GD in terms of the direction of convergence points is studied . This study shows that , in a setting of linear regression , SGD and GD converge to different directions , which are determined by the largest/smallest eigenvectors of a data matrix when the learning rate is moderately large . Experiments using synthetic data and Fashion MNIST support the theoretical results . Detailed comments : First of all , I do n't have much experience in the analysis of SGD/GD and my assessment for technical points may miss some important points . Overall , the paper is well written . The motivation and problem setting are clearly written . Related work is sufficiently introduced . The main theoretical results ( Theorems 1 , 2 , 4 ) are interesting . As far as I know , there 's no study to reveal the implicit bias in terms of direction . However , I have several concerns . 1.In the data generation process , the noiseless output y = < w * , x > is assumed . However , the output often contains observations noise such as an additive Gaussian model as y = < w * , x > + $ \\xi $ where $ \\xi $ is small Gaussian noise . Would it be possible to show similar results in the noisy case ? 2.More importantly , the benefit of the directional bias is not clear . Theorem 4 shows SGD achieves $ \\epsilon $ -optimal solution and GD achieves $ M $ -suboptimal solution . However , we can not conclude that SGD solution is better than GD solution because the generalization performance depends on the unknown constants $ \\epsilon $ and $ M $ . So , GD may win in some cases but SGD may win in other cases , but it seems there is no way to know in what conditions $ \\epsilon $ and $ M $ satisfy some specific values so that SGD beats GD . Would it be possible to clarify the conditions ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for appreciating the novelty and writing of our work . We answer your concerns as follows : 1 . \u201c Noisy labels \u201d . - Our main focus in this work is to justify the directional bias for SGD with moderate learning rate . Therefore our results are presented for a realizable ( interpolating ) overparameterized linear model to keep the statements concise . Nevertheless , our results are applicable to noisy label settings as well . Note that given a sufficient overparameterization , a linear model can fit the noisy labels perfectly ( in other words , interpolating the training data points with noisy labels ) with high probability [ Bartlett et al. , 2020 ] . Then we only need to revise our current results by taking a conditional expectation over the i.i.d.zero mean label noise . 2. \u201c Constants in Theorem 4 \u201d . - We apologize for the imprecise descriptions on the constants $ \\epsilon $ and $ M $ in the original Theorem 4 . In fact , according to the formal version of Theorem 4 in Appendix B , we have $ \\epsilon = o ( 1 ) $ and $ M = \\gamma_1 / \\gamma_n - o ( 1 ) $ , where $ \\gamma_1 $ and $ \\gamma_n $ are the largest and smallest ( non-zero ) eigenvalues of the data covariance matrix , respectively . Since $ M - ( 1+\\epsilon ) > \\gamma_1/\\gamma_n - 1 - o ( 1 ) > 0 $ , we have a separation between the test error of SGD with moderate learning rate and the test error of GD/SGD with small learning rate . We have made the constants in Theorem 4 precise in the revision . We also add a discussion to emphasize this separation after Theorem 4 in the revision ."}, "1": {"review_id": "3X64RLgzY6O-1", "review_text": "In the paper the authors analyzed the convergence dynamics toward a minimum of gradient descent ( GD ) and stochastic gradient descent ( SGD ) . The algorithms are considered to start in the basin of attraction of a minimum , and using discrete steps they approach the bottom . The main result of the paper concerns the fact that , with moderate learning rate , SGD approaches the minimum along the steepest direction , contrary to GD . The authors conclude that the implicit bias of SGD does not appear in small learning rate formulation , but it becomes manifest when a moderate learning rate is considered . * It seems from the analysis that SGD differs from GD because its learning rate is modified by the batch size and therefor it uses an `` effective learning rate '' . The question is : what would happen if GD is taken with a learning rate that matches the effective learning rate of SGD ? This situation is very similar to the one observed in [ Nakkiran ( 2020 ) ] ( and also close to [ Lewkowycz et al . ( 2020 ) ] ) , where they consider GD and draw similar conclusions . * Another questions is whether it is really necessary to have discrete steps to have a difference between GD and SGD in the continuous formulations . In the introduction the authors make a comparison between the implicit biases in the literature for GD and SGD claiming that they are the same because of the small learning rate . However this is not clear to me , could the authors cite papers where this is explicitly shown ? In particular there is a literature on the effect of SGD in the continuous formulation [ e.g Jastrzebski et al.2018 ( arXiv:1711.04623 ) ; Zhu et al.2019 ( ICML2019 ) ; Xie et al.2020 ( arXiv:2002.03495 ) ] using stochastic differential equations it was possible to show that SGD has an implicit temperature that adapts to the geometry of the landscape favoring , in particular , flatter solutions . In few words , do we need to rely on the discrete formulation to understand the advantages of SGD ? * The result by [ Nakkiran ( 2020 ) ] seems to heavily rely on having a data distribution that is highly homogeneous to observe the difference between the training and the generalization loss . Do we have evidence that this is relevant for practical situations ? I would happy have more numerical experiments as the one in Fig.2 . In particular showing the effect of increasing the learning rate in GD ( and SGD ) from the current `` small '' value to a value large enough so that it does not converge anymore . Both in the synthetic and real dataset . The questions above block me from giving a higher rating , but I will be happy to increase the rate given satisfactory answers .", "rating": "7: Good paper, accept", "reply_text": "Thank you for recognizing our contributions . Regarding your concerns , we address them in the following : 1 . \u201c What would happen if GD is taken with a learning rate that matches the effective learning rate of SGD ? This situation is very similar to the one observed in [ Nakkiran ( 2020 ) ] ( and also close to [ Lewkowycz et al . ( 2020 ) ] ) , where they consider GD and draw similar conclusions. \u201d - We thank you ( and R # 4 ) for raising this question . We argue that Theorem 2 precludes GD from having a legitimate large learning rate ( LR ) to converge along large eigenvalue directions . In specific , Eq . ( 6 ) gives the range of LR where GD falls into the regime of Theorem 2 , i.e. , converging along small eigenvalue directions . Note that the upper bound in Eq . ( 6 ) , $ n / ( 2\\lambda_1 ) $ , is linear in the number of training data $ n $ , which is very huge in practice ( e.g. , $ n > 10^4 $ for MNIST and CIFAR10 ) . Therefore Eq . ( 6 ) already covers the largest possible LR that one can use in practice . We have updated remark 4 to include the above discussions . - Regarding the similarity with [ Nakkiran , 2020 ] , we would like to highlight the difference between [ Nakkiran , 2020 ] and our work . Specifically , [ Nakkiran , 2020 ] showed the separation between the test error of GD with \u201c large \u201d and annealing learning rate , and test error of GD with small learning rate . However , the \u201c large \u201d learning rate for GD in their analysis is linear in the training sample size and is impractically large as we have discussed above . In contrast , we showed that under the practically used moderate learning rate , there is a separation between the test error of SGD and the test error of GD . Therefore , our result is different from that in [ Nakkiran , 2020 ] , and our contribution is unique . We have revised remark 6 to emphasize this point . 2. \u201c Another question is whether it is really necessary to have discrete steps to have a difference between GD and SGD in the continuous formulations. \u201d - Yes , it is necessary to consider discrete steps to explain the difference between the implicit bias of GD and SGD . Regarding the reference which shows the implicit biases of GD and SGD are the same for infinitesimal learning rate , please refer to Theorem 2.1 in [ Kushner & Yin , 2003 ] . It directly shows that SGD converges to the same ODE given by GD when the learning rate tends to zero ( under mild assumptions ) . Therefore , in the continuous time scenario , SGD can not be differentiated from GD . Regarding the references [ Jastrzebski et al.2018 ; Zhu et al.2019 ; Xie et al.2020 ] you mentioned and also cited in our paper , they use SDE to approximate SGD and can not deal with infinitesimal step size ( note the Brownian motion term in the SDE is scaled by $ \\sqrt { \\eta } $ , thus an infinitesimal step size will make it vanish and the SDE will degenerate to an ODE ) . Therefore , their results are orthogonal to our results and not directly comparable to ours . 3. \u201c Do we have evidence that the data is homogeneous for practical situations , as assumed by [ Nakkiran ( 2020 ) ] ? I would be happy to have more numerical experiments as the one in ( original ) Fig.2 . In particular showing the effect of increasing the learning rate in GD ( and SGD ) from the current `` small '' value to a value large enough so that it does not converge anymore . Both in the synthetic and real dataset. \u201d - The distribution of real-world dataset is too complicated to be classified as \u201c homogeneous \u201d or \u201c inhomogeneous \u201d . But we do think data preprocessing ( e.g. , whitening and normalization ) can make the data distribution more \u201c homogeneous \u201d . This is an interesting problem and deserves more study in the future work . - Thank you for your suggestion on the experiments . We have done additional experiments on neural networks to show the effect of increasing the learning rate in GD and SGD in Appendix D.3 , Figure 4 . The additional experiment results also corroborate our theory ."}, "2": {"review_id": "3X64RLgzY6O-2", "review_text": "This paper analyzes the differences in the convergence of SGD and GD when using `` small '' and `` moderate '' learning rates to shed light on why SGD with `` moderate '' and annealing learning rates perform well in practice . Focusing on an overparametrized linear regression problem , the paper claims that SGD with a `` moderate '' learning converges differently ( along the large eigenvalue directions ) compared to SGD/GD with a `` small '' learning rate ( along the small eigenvalue directions ) . They further show analytically for this problem that there exists a learning rate schedule ( with moderate learning rates ) such that SGD will perform + generalize well , while there exist small learning rate schedules such that GD will perform + generalize poorly . This paper is clearly written and presents a clear illustration of their hypothesis for why SGD with a moderate learning rate performs well in practice through Figure 1 and a simple linear regression example in section 3 . The paper focuses most of its analysis on a toy example problem ( overparametrized linear regression ) and the empirical experiments do not clearly support the paper 's claims . This paper should be rejected as is . Although the motivating example and illustrations are interesting , the analysis for the overparameterized linear regression problem is too specific to clearly claim that this `` directional bias '' is why SGD with moderate learning rates is successful . I have two main concerns : ( 1 ) in the special case of overparametrized linear regression , Theorem 2 only shows that GD with small learning rates converge along the smallest eigenvalue direction . It does not preclude GD from having the same behavior as SGD for larger learning rates . I also do not agree with remark 4 . ( 2 ) it 's not clear how or why the results for overparameterized linear regression would extend to generic NN losses . The example experiments ( Fig 2 ) need more detail to be clear and are not sufficient to support the hypothesis in the paper . Questions : In section 3 , does n't Eq ( 3 ) show that GD will behave identically to SGD if the learning rate is doubled ? What is considered a `` moderate '' learning rate for GD + SGD does not need to be identical , right ? For the overparametrized regression , why do we not include some form of regularization on the weights ( as would commonly be done in practice ) ? Does this affect the results ? What is the minibatch size for SGD in the experiments ? How are the results ( in Figure 2a + 2b ) sensitive to random initialization ? Minor comment : Page 6 : the definition of level set probably needs a $ \\leq $ sign .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for recognizing the novelty and writing of our work . Regarding your concerns , please see below : 1 . \u201c Theorem 2 only shows that GD with small learning rates converge along the smallest eigenvalue direction . It does not preclude GD from having the same behavior as SGD for larger learning rates. \u201d - This is a huge misunderstanding . We apologize that we did not make it clear . In fact , Theorem 2 indeed precludes GD from having a large learning rate ( LR ) to converge along large eigenvalue directions . In specific , Eq . ( 6 ) gives the range of LR where GD falls into the government of Theorem 2 , i.e. , converging along small eigenvalue directions . Note that the upper bound in Eq . ( 6 ) , $ n / ( 2\\lambda_1 ) $ , is linear in the training sample size , which is very large for typical datasets ( e.g. , $ n > 10^4 $ for MNIST and CIFAR10 ) . Therefore , Eq . ( 6 ) has already covered the largest possible LR that one can use in practice . 2. \u201c I also do not agree with remark 4. \u201d - We thank R # 4 to point out the issue on remark 4 . We have updated remark 4 to include the above discussions on Theorem 2 . We hope the new remark is crystal clear that it is impossible to have a numerically stable LR such that GD converges along large eigenvalue directions . 3. \u201c It 's not clear how or why the results for overparameterized linear regression would extend to generic NN losses. \u201d - We thank the reviewer for this comment . We agree with the reviewer that proving directional bias of SGD with moderate learning rate for neural networks is still an open problem . However , even in the linear regression setting , there is no such kind of result before our work . Without a full characterization of directional bias of SGD for the simplest possible problem \u2014 the linear regression \u2014 it seems unlikely that one can prove it for neural networks . As our work is the first work on the directional bias of SGD , we believe we have taken an important step along this direction and the contribution of our work is very significant . - In our revision , we have added new experiments ( Figure 2 ( b ) ) on NN to support our theory , which also shed light on understanding the directional bias for SGD for general NNs . A brief description of the experiments is in the \u201c reply to all reviewers \u201d response . 4. \u201c The example experiments ( original Fig 2 ) need more detail to be clear and are not sufficient to support the hypothesis in the paper. \u201d - We add a new section ( Appendix D ) to describe the full details about our experiments . We have also provided the Code in the supplemental material for the reproducibility of our experiments . We argue that Figure 2 ( a ) does verify our theory in an overparameterized linear model . As for NNs , new experiments are provided in ( new ) Figure 2 ( b ) to support our theory . Should there be any missed detail , please let us know and we are happy to clarify . 5. \u201c In section 3 , does n't Eq ( 3 ) show that GD will behave identically to SGD if the learning rate is doubled ? What is considered a `` moderate '' learning rate for GD + SGD does not need to be identical , right ? \u201d - Your comment is absolutely correct for this toy example , which involves only TWO training data points and is meant for proof of concept . In general , however , according to Theorem 2 and our previous discussions , the \u201c effective \u201d LR for GD scales linearly in the training sample size $ n $ , which is quite huge in practice . So doubling the learning rate of GD can not make GD and SGD ( with moderate LR ) behave the same . 6. \u201c For the overparameterized regression , why do we not include some form of regularization on the weights ( as would commonly be done in practice ) ? Does this affect the results ? \u201d - As our work is the first to study the implicit bias ( i.e. , implicit regularization ) of SGD with moderate learning rate , we chose to exclude explicit regularizers . By adding explicit regularization , we think our implicit bias result will be affected in general , and it is an interesting future work to study the interplays between explicit regularization and implicit regularization . 7. \u201c What is the minibatch size for SGD in the experiments ? How are the results ( in Figure 2a + 2b , original ) sensitive to random initialization ? \u201d - ( 1 ) The minibatch size of SGD is 25 . Full experiment details are in Appendix D. ( 2 ) Results are robust to random initialization . In the revision , we have updated experiments for neural networks to include the standard deviation computed over 10 runs , which shows that the results are quite robust . We have also attached Code for reproducibility . 8. \u201c Page 6 : the definition of level set probably needs a < = sign. \u201d - This is a misunderstanding . Level sets are indeed defined with \u201c = \u201d . Those defined with \u201c < = \u201d are called sublevel sets . We hope your concerns have been addressed , and are looking forward to your further comments ."}}