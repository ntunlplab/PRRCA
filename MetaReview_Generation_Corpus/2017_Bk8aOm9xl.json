{"year": "2017", "forum": "Bk8aOm9xl", "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning", "decision": "Invite to Workshop Track", "meta_review": "The paper proposes an intuitive method for exploration, namely to build a model of the system dynamics and explore regions where this approximation differs from the observed data (i.e., how \"surprised\" the agent was by an observation). The idea is a nice one, and part of the benefit comes from the simplicity and wide applicability of the approach.\n \n The main drawback of this paper is simply that the resulting approach doesn't substantially outperform existing approaches, at least not to a degree where it seem like the paper should should be clearly accepted to ICLR. On the continuous control tasks, the advantage over VIME seems very unclear (at best the results are mixed, showing sometimes surprisal and sometime VIME do better), and on the Atari games no comparison is made against many of the methods tuned for this setting, such as Gorila (Nair, 2015) which achieves some of the best results we are aware of on the Venture game, which is definitely the strongest result in this current paper, but not as good as this previous work. We know the settings are different, but overall it seems like the approach is largely outperformed by existing approaches, and thus the advantage mainly comes from runtime. This is certainly an interesting take, but needs to be studied a lot more thoroughly before it would make a really compelling case. We would like to recomend this paper to the workshop track. The pros/cons are as follows:\n \n Pros:\n + Simple and intuitive method for exploration\n \n Cons:\n - Doesn't seem to substantially outperform existing methods\n - No comparison to many alternative approaches for some of the \"better\" results in the paper.", "reviews": [{"review_id": "Bk8aOm9xl-0", "review_text": "The authors present a novel approach to surprise-based intrinsic motivation in deep reinforcement learning. The authors clearly explain the difference from other recent approaches to intrinsic motivation and back up their method with results from a broad class of discrete and continuous action domains. They present two tractable approximations to their framework - one which ignores the stochasticity of the true environmental dynamics, and one which approximates the rate of information gain (somewhat similar to Schmidhuber's formal theory of creativity, fun and intrinsic motivation). The results of this exploration bonus when added to TRPO are generally better than standard TRPO. However, I would have appreciated a more thorough comparison against other recent work on intrinsic motivation. For instance, Bellemare et al 2016 recently achieved significant performance gains on challenging Atari games like Montezuma's Revenge by combining DQN with an exploration bonus, however Montezuma's Revenge is not presented as an experiment here. Such comparisons would significantly improve the strength of the paper.", "rating": "6: Marginally above acceptance threshold", "reply_text": "First , thanks for your time and remarks ! Our primary goal here was to introduce , motivate , and proof-of-concept our methods , and we felt that our slate of experiments was sufficient to achieve that end . We did some experiments in Montezuma 's Revenge , but we were n't able to find a hyperparameter combination that produced good results before we decided to move on . I agree that it would be nice to compare directly against Bellemare et al 's psuedocount-based methods , but this would have required many more ( time-consuming and expensive ) experiments . I think a useful future work would focus specifically on a rigorous comparison and benchmarking between different methods for intrinsic motivation - along the lines of `` Benchmarking Deep Reinforcement Learning for Continuous Control , '' by Duan et al.- which could compare well-tuned implementations of psuedocounts , VIME , our methods , and counting with hashes ( concurrent work also submitted to ICLR17 by Tang and Houthooft et al . ) . To facilitate this possibility , we will be releasing our code at some point in the near future ."}, {"review_id": "Bk8aOm9xl-1", "review_text": "This paper provides a surprise-based intrinsic reward method for reinforcement learning, along with two practical algorithms for estimating those rewards. The ideas are similar to previous work in intrinsic motivation (including VIME and other work in intrinsic motivation). As a positive, the methods are simple to implement, and provide benefits on a number of tasks. However, they are almost always outmatched by VIME, and not one of their proposed method is consistently the best of those proposed (perhaps the most consistent is the surprisal, which is unfortunately not asymptotically equal to the true reward). The authors claim massive speed up, but the numerical measurements show that VIME is slower to initialize but not significantly slower per iteration otherwise (perhaps a big O analysis would clarify the claims). Overall it's a decent, simple technique, perhaps slightly incremental on previous state of the art.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Hi , thank you for your time and review ! With regards to the performance comparison with VIME : it 's true that VIME definitely performs better on MountainCar and CartpoleSwingup , but our methods did outperform VIME on HalfCheetah , and we believe that the comparison on SwimmerGather is more ambiguous . Although the performance of VIME for SwimmerGather appears to be stronger , this is based on a smaller sample size of experiments ; the VIME curve is a median of only two runs of the algorithm with different random seeds , while the curve for our methods is the median of ten runs . If you would like , we can show the comparison of the top two runs of our methods against the two runs for VIME ( although this is also unfair ) ; in this comparison , our method edges out VIME . However , I feel obliged to note that concurrent work by Tang and Houthooft et al , also under review in ICLR17 , reported results for SwimmerGather that clearly exceeded both VIME and our approach . Regarding the speed comparison with VIME : what follows is intended to address the concerns that both you and AnonReviewer2 expressed . If it is n't clear from the text and figure , the per-iteration speedup is a factor of three in our speed test ( that is , excluding initialization time ) . For how we arrived at this number : Time to 15 Iterations = Total Algorith Execution Time - Initialization Time Intrinsic Reward Time = Time to 15 Iterations - Baseline Time to 15 Iterations * Speedup over VIME = Our Intrinsic Reward Time / VIME Intrinsic Reward Time * The baseline is from running TRPO with no bonuses . The initialization time and time to 15 iterations for VIME , our approach , and the baseline are what we report in the table . I think you make an excellent point that a big O style analysis is useful here , because the speedup over VIME depends on a couple of factors which may vary between experiments and hyperparameters . I 'll describe the analysis here , and incorporate it into the next version of the paper . Let 's focus on the per-iteration time cost here , and treat the initialization time as negligible . ( For longer experiments , this is reasonable . ) Suppose that for both VIME and our approach , you use equivalent neural network architectures for the dynamics models . At each iteration , there is a time cost for fitting the dynamics model . Let 's say VIME requires ( fit time ) _VIME , and ours requires ( fit time ) _SURPRISE . To calculate each intrinsic reward , VIME requires both a forward and a backward pass through the network . Let 's say that the time cost of a forward pass through the network is t_forward , and the time cost for a backward pass is t_backward . Our method only requires forward passes , and even though VIME uses a more complex model ( because you sample multiple parameters from the Bayesian neural network parameter distribution to evaluate a forward pass ) , we 'll assume that the forward pass costs are equivalent . Suppose the batch size ( number of rewards per iteration that need to be computed ) is batch_size . Because VIME requires a separate * * forward/backward pass for each ( state , action , next state ) tuple to compute an intrinsic reward , we obtain the per-iteration cost of VIME as ( per iteration ) _VIME = ( fit time ) _VIME + batch_size * ( t_forward + t_backward ) / n_threads , where n_threads is how many different threads you can divide the reward computation load between . On the other hand , in our method , no backward pass is necessary . So we can take advantage of the fact that forward passes are highly parallelizable . Suppose that for a given architecture of dynamics model , as many as ' k ' inputs can be processed in a forward pass simultaneously . The per-iteration cost of our surprisal method is then : ( per iteration ) _SURPRISAL = ( fit time ) _SURPRISE + batch_size * t_forward / ( n_threads * k ) . The speedup over VIME , as the batch_size becomes larger ( so the fit times become negligible by comparison ) aproaches something that is the same order of magnitude as ' k ' . In the experiment we ran for the speed comparison , ' k ' works out to about 7 if I 'm not remembering incorrectly , so this is pretty good . And in that experiment , the batch_size was only 5000 , so we were pretty far from maxing out the speed advantage ; on experiments like SwimmerGather and in the Atari domain , I imagine the difference would be more pronounced . I will double-check this sometime wthin the next week . A speedup factor between 2 and 10 , in our view , is substantial . We think it is likely that the speedup will usually fall in this range , even though more complex dynamics models may fall on the shorter end of that . But if you think the claims in our paper are excessive , we are open to the possibility of moderating the language . * * The forward and backward passes for different ( state , action , next state ) tuples have to be separate because of limitations in existing deep learning toolkits . To the best of my knowledge , none of Tensorflow , Theano , Torch , or their derivatives have tools for taking k inputs and returning k gradients efficiently -- they only seem to take k inputs and return k forward passes efficiently , or take k inputs and return 1 gradient , or take k inputs and return k gradients slowly ( computing them separately , as in the VIME implementation ) . So , to be clear , the speedup over VIME is largely based on implementation , not theory . ( Hopefully this was well-described in the paper ; if not , I am happy to clarify . ) Overall , we agree with the critique that our work is incremental , but we think our methods represent useful tools in the RL arsenal ."}, {"review_id": "Bk8aOm9xl-2", "review_text": "This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \u201cventure\u201d, maybe). Novelty: none of the proposed types of intrinsic motivation are novel, and it\u2019s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012). Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent\u2019s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts. Computation time: I find the paper\u2019s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Hi , and thanks for your remarks ! Regarding potential : I agree with the spirit of this remark , although not the example ( Go ) . Surpise about dynamics is almost certainly not the full story for intrinsic motivation - and I think in Bellemare et al 's paper on psuedocounts , they made an argument against it ( suggesting that this was too conservative ) - but I think it 's useful because the dynamics provide a rich and `` low-hanging '' source of information to inspire an agent with . I say `` low-hanging '' because in most systems we care about , dynamics are complicated enough that they ca n't be trivially learned , so you can get a useful surprise signal from most transitions , which helps with the real problem of reward sparsity . I think competence-based models might be a little harder to define and implement satisfactorily - like , if your competence measure involves a count over possible futures your agent can reach , you need to know the dynamics to estimate that , and if you do n't know the dynamics , you have to learn them , and until you learn them well , you could have substantial errors in your competence measure . There have been some recent papers on using unsupervised auxiliary losses ( by people out of Deepmind ) in deep RL , and those looked promising . They used reward prediction , additional value replay , and pixel control as auxiliary tasks , and found that they could get great gains on sample efficiency . So , if I were to start thinking about other forms of learning progress or surprise to use as intrinsic rewards , those might be the first places I 'd look . As for the example of Go - our approach is really best-suited for environments where reward signals are so rare that randomly initialized policies will almost never find them . A game like Go has a well-defined reward signal , because you 're guaranteed to get a reward every episode from whichever of the two players won the game . I think we did a decent job of emphasizing that this approach is really meant for those kind of sparse reward environments , but if you think it needs to be a little more clear on that front , I could revise some of the language in the paper to reflect that . And the problem of too-quickly learning a trivial dynamics function : we did see that happening on MountainCar with the learning progress incentive , as we mention in the paper . Regarding computation time concerns : since you and AnonReviewer3 had similar concerns , I tried to answer both of your remarks on this in the same response . So , see above for something thorough/complete . But , in the speed test that we have in the paper , the per-step cost was 3 times better for our method than VIME . ( Initialization costs ignored . ) This is pretty good , no ?"}], "0": {"review_id": "Bk8aOm9xl-0", "review_text": "The authors present a novel approach to surprise-based intrinsic motivation in deep reinforcement learning. The authors clearly explain the difference from other recent approaches to intrinsic motivation and back up their method with results from a broad class of discrete and continuous action domains. They present two tractable approximations to their framework - one which ignores the stochasticity of the true environmental dynamics, and one which approximates the rate of information gain (somewhat similar to Schmidhuber's formal theory of creativity, fun and intrinsic motivation). The results of this exploration bonus when added to TRPO are generally better than standard TRPO. However, I would have appreciated a more thorough comparison against other recent work on intrinsic motivation. For instance, Bellemare et al 2016 recently achieved significant performance gains on challenging Atari games like Montezuma's Revenge by combining DQN with an exploration bonus, however Montezuma's Revenge is not presented as an experiment here. Such comparisons would significantly improve the strength of the paper.", "rating": "6: Marginally above acceptance threshold", "reply_text": "First , thanks for your time and remarks ! Our primary goal here was to introduce , motivate , and proof-of-concept our methods , and we felt that our slate of experiments was sufficient to achieve that end . We did some experiments in Montezuma 's Revenge , but we were n't able to find a hyperparameter combination that produced good results before we decided to move on . I agree that it would be nice to compare directly against Bellemare et al 's psuedocount-based methods , but this would have required many more ( time-consuming and expensive ) experiments . I think a useful future work would focus specifically on a rigorous comparison and benchmarking between different methods for intrinsic motivation - along the lines of `` Benchmarking Deep Reinforcement Learning for Continuous Control , '' by Duan et al.- which could compare well-tuned implementations of psuedocounts , VIME , our methods , and counting with hashes ( concurrent work also submitted to ICLR17 by Tang and Houthooft et al . ) . To facilitate this possibility , we will be releasing our code at some point in the near future ."}, "1": {"review_id": "Bk8aOm9xl-1", "review_text": "This paper provides a surprise-based intrinsic reward method for reinforcement learning, along with two practical algorithms for estimating those rewards. The ideas are similar to previous work in intrinsic motivation (including VIME and other work in intrinsic motivation). As a positive, the methods are simple to implement, and provide benefits on a number of tasks. However, they are almost always outmatched by VIME, and not one of their proposed method is consistently the best of those proposed (perhaps the most consistent is the surprisal, which is unfortunately not asymptotically equal to the true reward). The authors claim massive speed up, but the numerical measurements show that VIME is slower to initialize but not significantly slower per iteration otherwise (perhaps a big O analysis would clarify the claims). Overall it's a decent, simple technique, perhaps slightly incremental on previous state of the art.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Hi , thank you for your time and review ! With regards to the performance comparison with VIME : it 's true that VIME definitely performs better on MountainCar and CartpoleSwingup , but our methods did outperform VIME on HalfCheetah , and we believe that the comparison on SwimmerGather is more ambiguous . Although the performance of VIME for SwimmerGather appears to be stronger , this is based on a smaller sample size of experiments ; the VIME curve is a median of only two runs of the algorithm with different random seeds , while the curve for our methods is the median of ten runs . If you would like , we can show the comparison of the top two runs of our methods against the two runs for VIME ( although this is also unfair ) ; in this comparison , our method edges out VIME . However , I feel obliged to note that concurrent work by Tang and Houthooft et al , also under review in ICLR17 , reported results for SwimmerGather that clearly exceeded both VIME and our approach . Regarding the speed comparison with VIME : what follows is intended to address the concerns that both you and AnonReviewer2 expressed . If it is n't clear from the text and figure , the per-iteration speedup is a factor of three in our speed test ( that is , excluding initialization time ) . For how we arrived at this number : Time to 15 Iterations = Total Algorith Execution Time - Initialization Time Intrinsic Reward Time = Time to 15 Iterations - Baseline Time to 15 Iterations * Speedup over VIME = Our Intrinsic Reward Time / VIME Intrinsic Reward Time * The baseline is from running TRPO with no bonuses . The initialization time and time to 15 iterations for VIME , our approach , and the baseline are what we report in the table . I think you make an excellent point that a big O style analysis is useful here , because the speedup over VIME depends on a couple of factors which may vary between experiments and hyperparameters . I 'll describe the analysis here , and incorporate it into the next version of the paper . Let 's focus on the per-iteration time cost here , and treat the initialization time as negligible . ( For longer experiments , this is reasonable . ) Suppose that for both VIME and our approach , you use equivalent neural network architectures for the dynamics models . At each iteration , there is a time cost for fitting the dynamics model . Let 's say VIME requires ( fit time ) _VIME , and ours requires ( fit time ) _SURPRISE . To calculate each intrinsic reward , VIME requires both a forward and a backward pass through the network . Let 's say that the time cost of a forward pass through the network is t_forward , and the time cost for a backward pass is t_backward . Our method only requires forward passes , and even though VIME uses a more complex model ( because you sample multiple parameters from the Bayesian neural network parameter distribution to evaluate a forward pass ) , we 'll assume that the forward pass costs are equivalent . Suppose the batch size ( number of rewards per iteration that need to be computed ) is batch_size . Because VIME requires a separate * * forward/backward pass for each ( state , action , next state ) tuple to compute an intrinsic reward , we obtain the per-iteration cost of VIME as ( per iteration ) _VIME = ( fit time ) _VIME + batch_size * ( t_forward + t_backward ) / n_threads , where n_threads is how many different threads you can divide the reward computation load between . On the other hand , in our method , no backward pass is necessary . So we can take advantage of the fact that forward passes are highly parallelizable . Suppose that for a given architecture of dynamics model , as many as ' k ' inputs can be processed in a forward pass simultaneously . The per-iteration cost of our surprisal method is then : ( per iteration ) _SURPRISAL = ( fit time ) _SURPRISE + batch_size * t_forward / ( n_threads * k ) . The speedup over VIME , as the batch_size becomes larger ( so the fit times become negligible by comparison ) aproaches something that is the same order of magnitude as ' k ' . In the experiment we ran for the speed comparison , ' k ' works out to about 7 if I 'm not remembering incorrectly , so this is pretty good . And in that experiment , the batch_size was only 5000 , so we were pretty far from maxing out the speed advantage ; on experiments like SwimmerGather and in the Atari domain , I imagine the difference would be more pronounced . I will double-check this sometime wthin the next week . A speedup factor between 2 and 10 , in our view , is substantial . We think it is likely that the speedup will usually fall in this range , even though more complex dynamics models may fall on the shorter end of that . But if you think the claims in our paper are excessive , we are open to the possibility of moderating the language . * * The forward and backward passes for different ( state , action , next state ) tuples have to be separate because of limitations in existing deep learning toolkits . To the best of my knowledge , none of Tensorflow , Theano , Torch , or their derivatives have tools for taking k inputs and returning k gradients efficiently -- they only seem to take k inputs and return k forward passes efficiently , or take k inputs and return 1 gradient , or take k inputs and return k gradients slowly ( computing them separately , as in the VIME implementation ) . So , to be clear , the speedup over VIME is largely based on implementation , not theory . ( Hopefully this was well-described in the paper ; if not , I am happy to clarify . ) Overall , we agree with the critique that our work is incremental , but we think our methods represent useful tools in the RL arsenal ."}, "2": {"review_id": "Bk8aOm9xl-2", "review_text": "This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \u201cventure\u201d, maybe). Novelty: none of the proposed types of intrinsic motivation are novel, and it\u2019s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012). Potential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent\u2019s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts. Computation time: I find the paper\u2019s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Hi , and thanks for your remarks ! Regarding potential : I agree with the spirit of this remark , although not the example ( Go ) . Surpise about dynamics is almost certainly not the full story for intrinsic motivation - and I think in Bellemare et al 's paper on psuedocounts , they made an argument against it ( suggesting that this was too conservative ) - but I think it 's useful because the dynamics provide a rich and `` low-hanging '' source of information to inspire an agent with . I say `` low-hanging '' because in most systems we care about , dynamics are complicated enough that they ca n't be trivially learned , so you can get a useful surprise signal from most transitions , which helps with the real problem of reward sparsity . I think competence-based models might be a little harder to define and implement satisfactorily - like , if your competence measure involves a count over possible futures your agent can reach , you need to know the dynamics to estimate that , and if you do n't know the dynamics , you have to learn them , and until you learn them well , you could have substantial errors in your competence measure . There have been some recent papers on using unsupervised auxiliary losses ( by people out of Deepmind ) in deep RL , and those looked promising . They used reward prediction , additional value replay , and pixel control as auxiliary tasks , and found that they could get great gains on sample efficiency . So , if I were to start thinking about other forms of learning progress or surprise to use as intrinsic rewards , those might be the first places I 'd look . As for the example of Go - our approach is really best-suited for environments where reward signals are so rare that randomly initialized policies will almost never find them . A game like Go has a well-defined reward signal , because you 're guaranteed to get a reward every episode from whichever of the two players won the game . I think we did a decent job of emphasizing that this approach is really meant for those kind of sparse reward environments , but if you think it needs to be a little more clear on that front , I could revise some of the language in the paper to reflect that . And the problem of too-quickly learning a trivial dynamics function : we did see that happening on MountainCar with the learning progress incentive , as we mention in the paper . Regarding computation time concerns : since you and AnonReviewer3 had similar concerns , I tried to answer both of your remarks on this in the same response . So , see above for something thorough/complete . But , in the speed test that we have in the paper , the per-step cost was 3 times better for our method than VIME . ( Initialization costs ignored . ) This is pretty good , no ?"}}