{"year": "2019", "forum": "SkeXehR9t7", "title": "Graph2Seq: Graph to Sequence Learning with Attention-Based Neural Networks", "decision": "Reject", "meta_review": "Strengths:\nThe work proposes a novel architecture for graph to sequence learning.\nThe paper shows improved performance on synthetic transduction tasks and for graph to text generation. \n\nWeaknesses:\nMultiple reviewers felt that the experiments were insufficient to evaluate the novel aspects of the submission relative to prior work.  Newer experiments with the proposed aggregation strategy and a different graph representation were not as promising with respect to simple baselines. \n\nPoints of contention:\nThe discussion with the authors and one of the reviewers was particular contentious.\nThe title of the paper & sentences within the paper such as \"We propose a new attention-based neural networks paradigm to elegantly address graph- to-sequence learning problems\" caused significant contention, as this was perceived to discount the importance of prior work on graph-to-sequence problems which led to a perception of the paper \"overclaiming\" novelty.\n\nConsensus:\nConsensus was not reached, but both the reviewer with the lowest score and one of the reviewers giving a 6 came to the consensus that the experimental evaluation does not yet evaluate the novel aspects of the submission thoroughly enough.\n\nDue to the aggregate score, factors discussed above (and others) the AC recommends rejection; however, this work shows promise and additional experimental work should allow a new set of reviewers to better understand the behaviour and utility of the proposed method.\n\n", "reviews": [{"review_id": "SkeXehR9t7-0", "review_text": "The submission discusses a graph2seq architecture that combines a graph encoder that mixes GGNN and GCN components with an attentional sequence encoder. The resulting model is evaluated on three very simple tasks, showing small improvements over baselines. I'm not entirely sure what the contribution of this paper is supposed to be. The technical novelty seems to be limited to new notation for existing work: - (Sect. 3.1) The separation of forward/backward edges was already present in the (repeatedly cited) Li et al 2015 paper on GGNN (and in Schlichtkrull et al 2017 for GCN). The state update mechanism (a FC layer of the concatenation of old state / incoming messages) seems to be somewhere between a gated unit (as in GGNN) and the \"add self-loops to all nodes\" trick used in GCN; but no comparison is provided with these existing baselines. - (Sect 3.2) The discussed graph aggregation mechanism are those proposed in Li et al and Gilmer et al; no comparison to these baselines is provided. - (Sect. 3.3) This is a standard attention-based decoder; the fact that the memories come from a graph doesn't change anything fundamental. The experiments are not very informative, as simple baselines already reach >95% accuracy on the chosen tasks. The most notable difference between GGS-NNs and this work seems to be the attention-based decoder, but that is not evaluated explicitly. For the rebuttal phase, I would like to ask the authors to provide the following: - Experimental results for either GGS-NN with an attentional decoder, or their model without an attentional decoder, to check if the reported gains come from that. The final paragraph in Sect. 4 seems to indicate that the attention mechanism is the core enabler of the (small) experimental gains on the baselines. - The results of the GCN/GG-NN models (i.e., just as an encoder) with their decoder on the NLG task. - More precise definition of what they feel the contribution of this paper is, taking into account my comments from above. Overall, I do not think that the paper in its current state merits publication at ICLR. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We first would like to thank the referees for their very careful reading , for identifying subpar language , typos , and discrepancies in text , and for asking questions that will help us significantly improve the presentation . Q1 : The submission discusses a graph2seq architecture that combines a graph encoder that mixes GGNN and GCN components with an attentional sequence encoder . Answer : our graph encoder is mainly inspired by the GraphSAGE work ( Hamilton et al , .2017a ) , which as we discussed its connection with other graph encoders such as GCN in related work . GGNN or GGS-NN is a relevant work but we are not sure which parts of our graph encoder are related to them . A good analogy between GGS-NN and GCN is the relationship between RNN ( GRU ) and CNN . In this spirit , our Graph2Seq model can be an analogy to convolutional Seq2Seq.Q2 : The resulting model is evaluated on three very simple tasks , showing small improvements over baselines . Answer : there might be a misunderstanding about the experimental results . We respectfully argue two facts in terms of our experimental settings and results : 1 ) As we stated in the first paragraph of the Experiments , we chose the first two tasks to overlap with the tasks used by Li et al. , ( 2015 ) , which are synthetic datasets for better studying the characteristics of the proposed Graph2Seq model as well as other graph neural networks . 2 ) We chose the third dataset WikiSQL that is motivated by a real-world application - Natural Language Generation so that we can test the usefulness of our model as well as other baselines . 3 ) In Table 1 , for bAbI T19 and SP-S tasks , our Graph2Seq achieve slightly better accuracy while for SP-L , our model significantly outperformed other graph neural networks based methods . In Table 2 , for results on WikiSQL , our Graph2Seq models achieved much better results compared to other Seq2Seq and Tree2Seq models . Q3 : ( Sect.3.1 ) The separation of forward/backward edges was already present in the ( repeatedly cited ) Li et al 2015 paper on GGNN ( and in Schlichtkrull et al 2017 for GCN ) . The state update mechanism ( a FC layer of the concatenation of old state / incoming messages ) seems to be somewhere between a gated unit ( as in GGNN ) and the `` add self-loops to all nodes '' trick used in GCN ; but no comparison is provided with these existing baselines . Answer : we agree with the reviewer that there are many ways to handle directed graphs like the one used in GGS-NN ( Li et al. , 2015 ) and syntactic GCN ( Schlichtkrull et al. , 2017 ) . However , our design of combining the forward and backward representations is inspired by the bi-LSTM architecture , that is , generating the representation of a node for each direction ( forward or backward ) , and then concatenating them . Intuitively , this architecture could explicitly represent the context of a node , i.e. , the forward context representation and backward context representation . The state update mechanism of our graph encoder subsumes GCN as a special case , which is also discussed in ( Hamilton et al , .2017a ) .In addition , our bi-directional node embedding approach has been demonstrated and analyzed why it approaches the optimal performance faster than GCN and GGS-NN , as shown in Figure 4 and Table 1 . Q4 : ( Sect 3.2 ) The discussed graph aggregation mechanism are those proposed in Li et al and Gilmer et al ; no comparison to these baselines is provided . Answer : we noticed that in ( Li et al. , 2015 ) and ( Gilmer et al. , 2017 ) they employed some soft attention based weighted node embedding to compute the graph-level embedding . This is very different from two graph embeddings we exploit in Sec 3.2 . In particular , we have explored two graph embedding schemes : 1 ) Pooling-based graph embedding : we fed the node embeddings to a fully-connected neural network and then applied an pooling method ( i.g.max-pooling , min-pooling , and average pooling ) element-wise . This is different from weighted node embedding using soft attention . 2 ) Node-based graph embedding : we add on supernode into the input graph and all other nodes in the graph direct to this super node . Then the graph embedding can be obtained by aggregating the embeddings of the neighbor nodes . This approach has been discussed in the original GNN work ( Scarselli et al. , 2009 ) but with different aggregation approach ."}, {"review_id": "SkeXehR9t7-1", "review_text": "This work proposes an end-to-end graph encoder to sequence decoder model with an attention mechanism in between. Pros (+) : + Overall, the paper provides a good first step towards flexible end-to-end graph-to-seq models. + Experiments show promising results for the model to be tested in further domains. Cons (-) : - The paper would benefit more motivation and organization. Further details below (+ for pros / ~ for suggestions / - for cons): The paper could benefit a little more motivation: - Mentioning a few tasks in the introduction may not be enough. Explaining why these tasks are important may help. What is the greater problem the authors are trying to solve? - Same thing in the experiments, not well motivated, why these three? What characteristics are the authors trying to analyze with each of these tasks? Rephrase the novelty argument: - The authors argue to present a \u201cnovel attention mechanism\u201d but the attention mechanism used is not new (Bahdanau 2014 a & b). The fact that it is applied between a sequence decoder and graph node embeddings makes the paper interesting but maybe not novel. ~ The novelty added by this paper is the \u201cbi-edge-direction\u201c aggregation technique with the exploration of various pooling techniques. This could be emphasized more. Previous work: ~ The Related Work section could mention Graph Attention Networks (https://arxiv.org/abs/1710.10903) as an alternative to the node aggregation strategy. Aggregation variations: + The exploration between the three aggregator architectures is well presented and well reported in experiments. ~ The two Graph Embedding methods are also well presented, however, I didn\u2019t see them in experiments. Actually, it isn\u2019t clear at all if these are even used since the decoder is attending over node embeddings, not graph embedding\u2026 Could benefit a little more explanation Experiments: + Experiments show some improvement on the proposed tasks compared to a few baselines. - The change of baselines between table 1 for the first two tasks and table 2 for the third task is not explained and thus confusing. ~ There are multiple references to the advantage of using \u201cbi-directional\u201d node embeddings, but it is not clear from the description of each task where the edge direction comes from. A better explanation of each task could help. Results: - Page 9, the \u201cImpact of Attention Mechanism\u201d is discussed but no experimental result is shown to support these claims. Some editing notes: (1) Page 1, in the intro, when saying \u201cseq2seq are excellent for NMT, NLG, Speech Reco, and drug discovery\u201d: this last example breaks the logical structure of the sentence because it has nothing to do with NLP. (2) Page 1, in the intro, when saying that \u201c<...> a network can only be applied to sequential inputs\u201d: replace network by seq2seq models to be exact. (3) Typo on page 3, in paragraph \u201cNeural Networks on Graphs\u201d, on 8th line \u201cusig\u201d -> \u201cusing\u201d (4) Page 3, in paragraph \u201cNeural Networks on Graphs\u201d, the following sentence: \u201cAn extension of GCN can be shown to be mathematically related to one variant of our graph encoder on undirected graphs.\u201d is missing some information, like a reference, or a proof in Appendix, or something else\u2026 (5) Page 9, the last section of the \u201cImpact of Hop Size\u201d paragraph talks about the impact of the attention strategy. This should be moved to the next paragraph which discusses attention. (6) Some references are duplicates: |_ Hamilton 2017 a & c |_ Bahdanau 2014 a & b ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We first would like to thank the referees for their very careful reading , for identifying subpar language , typos , and discrepancies in text , and for asking questions that will help us significantly improve the presentation such as motivation and organization . We are very grateful for the kind comments of reviewers # 3 , in particular for your recognition of the key contributions of the paper . Q1 : The paper could benefit a little more motivation : motivated applications and creteria to select datasets Answer : we agree with the reviewer that the motivation for the proposed Graph2Seq model could be further enhanced . We try to explain the motivation clearer as discussed below : 1 ) The most important motivation is that the celebrated Seq2Seq model demands the problems inputs are represented in sequences , which may potentially hurt the capability of a neural network model can learn from data . a ) On one hand , the input data may be naturally represented in a more complex form . As we discussed in Introduction ( second paragraph ) , AMR-to-text , SQL-to-text , and path planning in the mobile robot are good examples of such problems , where the inputs are essentially in a graph format instead of a sequence . b ) On the other hand , even if the raw inputs are originally expressed in a sequence form , it can still benefit from the enhanced inputs with additional information ( to formulate graph inputs ) . For example , for semantic parsing tasks ( text-to-AMR or text-to-SQL ) , they have been shown better performance by augmenting the original sentence sequences with other structural information such as dependency parsing trees ( Pust et al. , 2015 ) . c ) For these aforementioned problems , we presented Graph2Seq model which can be also viewed as a generalized Seq2Seq model for graphs inputs . 2 ) In general , we chose these three different tasks to demonstrate the effectiveness of our end-to-end Graph2Seq model over other Seq2Seq models . We have two main criteria to choose the selected datasets ( applications ) : a ) The first main criteria is to choose the tasks that have been used in the previous literature so it is easier to make a fair comparison on well-known datasets . As we stated in the first paragraph of the Experiments section , we chose the first two tasks to overlap with the tasks used by Li et al. , ( 2015 ) . b ) The second main criteria is to choose a real-world application so that we can test the usefulness of our model as well as other baselines . The same evaluation philosophy has also used in ( Li et al. , 2015 ) . c ) We also provided the ablation study of our model such as the impacts of aggregator and hop size on Graph2Seq model in Sec 4.4 , where these important characteristics of our model were studied . d ) We agree with the reviewer \u2019 s comments that it would be more beneficial to briefly discuss the purpose or some emphasis of each task . Therefore , we have correspondingly revised the draft based on these comments . Q2 : Rephrase the novelty argument \u201c novel attention mechanism \u201d Answer : we thank the reviewer for a very careful reading of the proper wording of \u201c novel attention mechanism \u201d . Indeed , the attention mechanism has been proposed by Bahdanau et al. , ( 2014 ) . However , what \u2019 s new in our setting is that we are the first one to design an attention mechanism between a sequence decoder and the graph node embeddings . While we agree that our attention mechanism is * simple * once the construction is seen , we argue that it is not * trivial * to create this construction in the first place . If this were trivial , we might have expected earlier work such as ( Li et al. , 2015 ) and ( Schlichtkrull et al. , 2017 ) have already to use it , yet they did not . We will revise it and make it more clear in the revision . Q3 : The novelty added by this paper is the \u201c bi-edge-direction \u201c aggregation technique with the exploration of various pooling techniques . This could be emphasized more . Answer : This is an accurate summary . We are grateful for your recognition of one of the key contributions of the paper . We will emphasize these points more in the revision . Q4 : The Related Work section could mention Graph Attention Networks ( https : //arxiv.org/abs/1710.10903 ) as an alternative to the node aggregation strategy . Answer : Yes , we fully agree with you that the self-attention scheme proposed in the work of graph attention networks can be combined with our graph encoder as well . As we mentioned in the Introduction , \u201c Graph2Seq is simple yet general and is highly extensible where its two building blocks , graph encoder , and sequence decoder , can be replaced by other models \u201d ."}, {"review_id": "SkeXehR9t7-2", "review_text": "This paper proposes a graph to sequence transducer consisting of a graph encoder and a RNN with attention decoder. Strengths: - Novel architecture for graph to sequence learning. - Improved performance on synthetic transduction tasks and graph to text generation. Weaknesses: - Experiments could provide more insight into model architecture design and the strengths and weaknesses of the model on non-synthetic data. Transduction with structured inputs such as graphs is still an under-explored area, so this paper makes a valuable contribution in that direction. Previous work has mostly focused on learning graph embeddings producing outputs. This paper extends the encoder proposed by Hamilton et al (2017a) by modelling edge direction through learning \u201cforward\u201d and \u201cbackward\u201d representations of nodes. Node embeddings are pooled to a form a graph embedding to initialize the decoder, which is a standard RNN with attention over the node embeddings. The model is relatively similar to the architecture proposed by Bastings et al (2017) that uses a graph convolutional encoder, although the details of the graph node embedding computation differs. Although this model is presented in a more general framework, that model also accounted for edge directionality (as well as edge labels, which this model do not support). This paper does compare the proposed model with graph convolutional networks (GCNs) as encoder experimentally, finding that the proposed approach performs better on shortest directed path tasks. However the paper could make difference between these architectures clearer, and provide more insight into whether different graph encoder architectures might be more suited to graphs with different structural properties. The model obtains strong performance on the somewhat artificial bAbI and Shortest path tasks, while the strongest result is probably that of strong improvement over the baselines in SQL to text generation. However, very little insight is provided into this result. It would be interesting to apply this model to established NLG tasks such as AMR to text generation. Overall, this is an interesting paper, and I\u2019d be fine with it being accepted. However, the modelling contribution is relatively limited and it feels like for this to be a really strong contribution more insight into the graph encoder design, or more applications to real tasks and insight into the model\u2019s performance on these tasks is required. Editing notes: Hamilton et al 2017a and 2017c is the same paper. In some cases the citation format is used incorrectly: when the citation form part of the sentence, the citation should be inline. E.g. (p3) introduced by (Bruna et al., 2013) -> introduced by Bruna et al. (2013). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We first would like to thank the referees for their very careful reading , for identifying subpar language , typos , and discrepancies in text , and for asking questions that will help us significantly improve the presentation such as motivation and organization . Q1 : Novel architecture for graph to sequence learning\u2026 Transduction with structured inputs such as graphs is still an under-explored area , so this paper makes a valuable contribution in that direction ... We are very grateful for the kind comments of reviewers # 1 , in particular for your recognition of the key contributions of the paper . Q2 : Experiments could provide more insight into model architecture design and the strengths and weaknesses of the model on non-synthetic data . Answer : we fully agree with the reviewer \u2019 s comments ( which is also brought up by reviewer # 3 ) . According to your and reviewer # 3 \u2019 s comments , we have completely rewrite the paragraph that describes the experimental result of the SQL-to-Text task . We have also added/modified quite a few of paragraphs to better provide more insights on dataset selections , the motivation of the comparisons , and various characteristics of our Graph2Seq in terms of hop size , aggregation strategies , attention mechanism , and input graph sizes . Q3 : The model is relatively similar to the architecture proposed by Bastings et al ( 2017 ) Answer : We noticed that Bastings et al. , ( 2017 ) has utilized GCN for improving the encoder of the neural machine translation system , as we discussed in the related work . However , we would like to point out three major differences between their model and our Graph2Seq model in terms of model architecture : 1 ) Since Bastings \u2019 s encoder are built on GCN , which itself is derived from spectral graph convolutional neural networks , their model can be only used under transductive settings . In contrast , our graph encoder can be used under both transductive and inductive settings . 2 ) Although Bastings \u2019 s encoder takes into account both incoming and outgoing edges as well as the edge labels , they only compute a single node embedding using the information from both directions . This is quite different from our bidirectional aggregation strategies , which is inspired from the bi-LSTM architecture , where we generate the representation of a node for each direction ( forward or backward ) , and then concatenating them . Intuitively , this architecture could explicitly represent the context of a node , i.e. , the forward context representation and backward context representation . 3 ) Bastings et al . ( 2017 ) used the same attention based decoder of Bahdanau et al . ( 2015 ) while we design an attention-based decoder over the graph node embeddings . Therefore , our attention mechanism is independent of underlying specific tasks and generally applicable to different tasks . Q4 : However the paper could make difference between these architectures clearer , and provide more insight into whether different graph encoder architectures might be more suited to graphs with different structural properties . Answer : we thank for the reviewer \u2019 s suggestions and we have added some more discussions about the differences between different graph encoder . For different graph encoders , the most important advantages of our graph encoder lie in the fact that , when the graph size increases , all graph encoders started to perform poorer due to losing global information of a graph . However , our bi-directional node embeddings help converge rapidly to the optimal performance as shown in Figure 4 . Q5 : However , very little insight is provided into this result . It would be interesting to apply this model to established NLG tasks such as AMR to text generation . Answer : please see our previous response for providing more insights into the SQL-to-Text task . Conceptually , the AMR-to-Text task would be similar to the SQL-to-Text task , which we would be happy to explore in the near future . Q6 : Overall , this is an interesting paper , and I \u2019 d be fine with it being accepted . However , the modeling contribution is relatively limited and it feels like for this to be a really strong contribution more insight into the graph encoder design , or more applications to real tasks and insight into the model \u2019 s performance on these tasks is required . Answer : we hope that our previous responses have helped better explain the novelty of our Graph2Seq model architecture over existing works . According to your and reviewer # 3 \u2019 s comments , we have rephrased our key contributions of our model , that is , a novel graph encoder to learn a bi-directional node embedding for directed and undirected graphs with node attributes by employing various aggregation strategies , and to learn graph-level embedding by exploiting two different graph embedding techniques . In addition , to the best of knowledge , our attention mechanism to learn the alignments between nodes and sequence elements to better cope with larger graphs is also proposed for the first time ."}], "0": {"review_id": "SkeXehR9t7-0", "review_text": "The submission discusses a graph2seq architecture that combines a graph encoder that mixes GGNN and GCN components with an attentional sequence encoder. The resulting model is evaluated on three very simple tasks, showing small improvements over baselines. I'm not entirely sure what the contribution of this paper is supposed to be. The technical novelty seems to be limited to new notation for existing work: - (Sect. 3.1) The separation of forward/backward edges was already present in the (repeatedly cited) Li et al 2015 paper on GGNN (and in Schlichtkrull et al 2017 for GCN). The state update mechanism (a FC layer of the concatenation of old state / incoming messages) seems to be somewhere between a gated unit (as in GGNN) and the \"add self-loops to all nodes\" trick used in GCN; but no comparison is provided with these existing baselines. - (Sect 3.2) The discussed graph aggregation mechanism are those proposed in Li et al and Gilmer et al; no comparison to these baselines is provided. - (Sect. 3.3) This is a standard attention-based decoder; the fact that the memories come from a graph doesn't change anything fundamental. The experiments are not very informative, as simple baselines already reach >95% accuracy on the chosen tasks. The most notable difference between GGS-NNs and this work seems to be the attention-based decoder, but that is not evaluated explicitly. For the rebuttal phase, I would like to ask the authors to provide the following: - Experimental results for either GGS-NN with an attentional decoder, or their model without an attentional decoder, to check if the reported gains come from that. The final paragraph in Sect. 4 seems to indicate that the attention mechanism is the core enabler of the (small) experimental gains on the baselines. - The results of the GCN/GG-NN models (i.e., just as an encoder) with their decoder on the NLG task. - More precise definition of what they feel the contribution of this paper is, taking into account my comments from above. Overall, I do not think that the paper in its current state merits publication at ICLR. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We first would like to thank the referees for their very careful reading , for identifying subpar language , typos , and discrepancies in text , and for asking questions that will help us significantly improve the presentation . Q1 : The submission discusses a graph2seq architecture that combines a graph encoder that mixes GGNN and GCN components with an attentional sequence encoder . Answer : our graph encoder is mainly inspired by the GraphSAGE work ( Hamilton et al , .2017a ) , which as we discussed its connection with other graph encoders such as GCN in related work . GGNN or GGS-NN is a relevant work but we are not sure which parts of our graph encoder are related to them . A good analogy between GGS-NN and GCN is the relationship between RNN ( GRU ) and CNN . In this spirit , our Graph2Seq model can be an analogy to convolutional Seq2Seq.Q2 : The resulting model is evaluated on three very simple tasks , showing small improvements over baselines . Answer : there might be a misunderstanding about the experimental results . We respectfully argue two facts in terms of our experimental settings and results : 1 ) As we stated in the first paragraph of the Experiments , we chose the first two tasks to overlap with the tasks used by Li et al. , ( 2015 ) , which are synthetic datasets for better studying the characteristics of the proposed Graph2Seq model as well as other graph neural networks . 2 ) We chose the third dataset WikiSQL that is motivated by a real-world application - Natural Language Generation so that we can test the usefulness of our model as well as other baselines . 3 ) In Table 1 , for bAbI T19 and SP-S tasks , our Graph2Seq achieve slightly better accuracy while for SP-L , our model significantly outperformed other graph neural networks based methods . In Table 2 , for results on WikiSQL , our Graph2Seq models achieved much better results compared to other Seq2Seq and Tree2Seq models . Q3 : ( Sect.3.1 ) The separation of forward/backward edges was already present in the ( repeatedly cited ) Li et al 2015 paper on GGNN ( and in Schlichtkrull et al 2017 for GCN ) . The state update mechanism ( a FC layer of the concatenation of old state / incoming messages ) seems to be somewhere between a gated unit ( as in GGNN ) and the `` add self-loops to all nodes '' trick used in GCN ; but no comparison is provided with these existing baselines . Answer : we agree with the reviewer that there are many ways to handle directed graphs like the one used in GGS-NN ( Li et al. , 2015 ) and syntactic GCN ( Schlichtkrull et al. , 2017 ) . However , our design of combining the forward and backward representations is inspired by the bi-LSTM architecture , that is , generating the representation of a node for each direction ( forward or backward ) , and then concatenating them . Intuitively , this architecture could explicitly represent the context of a node , i.e. , the forward context representation and backward context representation . The state update mechanism of our graph encoder subsumes GCN as a special case , which is also discussed in ( Hamilton et al , .2017a ) .In addition , our bi-directional node embedding approach has been demonstrated and analyzed why it approaches the optimal performance faster than GCN and GGS-NN , as shown in Figure 4 and Table 1 . Q4 : ( Sect 3.2 ) The discussed graph aggregation mechanism are those proposed in Li et al and Gilmer et al ; no comparison to these baselines is provided . Answer : we noticed that in ( Li et al. , 2015 ) and ( Gilmer et al. , 2017 ) they employed some soft attention based weighted node embedding to compute the graph-level embedding . This is very different from two graph embeddings we exploit in Sec 3.2 . In particular , we have explored two graph embedding schemes : 1 ) Pooling-based graph embedding : we fed the node embeddings to a fully-connected neural network and then applied an pooling method ( i.g.max-pooling , min-pooling , and average pooling ) element-wise . This is different from weighted node embedding using soft attention . 2 ) Node-based graph embedding : we add on supernode into the input graph and all other nodes in the graph direct to this super node . Then the graph embedding can be obtained by aggregating the embeddings of the neighbor nodes . This approach has been discussed in the original GNN work ( Scarselli et al. , 2009 ) but with different aggregation approach ."}, "1": {"review_id": "SkeXehR9t7-1", "review_text": "This work proposes an end-to-end graph encoder to sequence decoder model with an attention mechanism in between. Pros (+) : + Overall, the paper provides a good first step towards flexible end-to-end graph-to-seq models. + Experiments show promising results for the model to be tested in further domains. Cons (-) : - The paper would benefit more motivation and organization. Further details below (+ for pros / ~ for suggestions / - for cons): The paper could benefit a little more motivation: - Mentioning a few tasks in the introduction may not be enough. Explaining why these tasks are important may help. What is the greater problem the authors are trying to solve? - Same thing in the experiments, not well motivated, why these three? What characteristics are the authors trying to analyze with each of these tasks? Rephrase the novelty argument: - The authors argue to present a \u201cnovel attention mechanism\u201d but the attention mechanism used is not new (Bahdanau 2014 a & b). The fact that it is applied between a sequence decoder and graph node embeddings makes the paper interesting but maybe not novel. ~ The novelty added by this paper is the \u201cbi-edge-direction\u201c aggregation technique with the exploration of various pooling techniques. This could be emphasized more. Previous work: ~ The Related Work section could mention Graph Attention Networks (https://arxiv.org/abs/1710.10903) as an alternative to the node aggregation strategy. Aggregation variations: + The exploration between the three aggregator architectures is well presented and well reported in experiments. ~ The two Graph Embedding methods are also well presented, however, I didn\u2019t see them in experiments. Actually, it isn\u2019t clear at all if these are even used since the decoder is attending over node embeddings, not graph embedding\u2026 Could benefit a little more explanation Experiments: + Experiments show some improvement on the proposed tasks compared to a few baselines. - The change of baselines between table 1 for the first two tasks and table 2 for the third task is not explained and thus confusing. ~ There are multiple references to the advantage of using \u201cbi-directional\u201d node embeddings, but it is not clear from the description of each task where the edge direction comes from. A better explanation of each task could help. Results: - Page 9, the \u201cImpact of Attention Mechanism\u201d is discussed but no experimental result is shown to support these claims. Some editing notes: (1) Page 1, in the intro, when saying \u201cseq2seq are excellent for NMT, NLG, Speech Reco, and drug discovery\u201d: this last example breaks the logical structure of the sentence because it has nothing to do with NLP. (2) Page 1, in the intro, when saying that \u201c<...> a network can only be applied to sequential inputs\u201d: replace network by seq2seq models to be exact. (3) Typo on page 3, in paragraph \u201cNeural Networks on Graphs\u201d, on 8th line \u201cusig\u201d -> \u201cusing\u201d (4) Page 3, in paragraph \u201cNeural Networks on Graphs\u201d, the following sentence: \u201cAn extension of GCN can be shown to be mathematically related to one variant of our graph encoder on undirected graphs.\u201d is missing some information, like a reference, or a proof in Appendix, or something else\u2026 (5) Page 9, the last section of the \u201cImpact of Hop Size\u201d paragraph talks about the impact of the attention strategy. This should be moved to the next paragraph which discusses attention. (6) Some references are duplicates: |_ Hamilton 2017 a & c |_ Bahdanau 2014 a & b ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We first would like to thank the referees for their very careful reading , for identifying subpar language , typos , and discrepancies in text , and for asking questions that will help us significantly improve the presentation such as motivation and organization . We are very grateful for the kind comments of reviewers # 3 , in particular for your recognition of the key contributions of the paper . Q1 : The paper could benefit a little more motivation : motivated applications and creteria to select datasets Answer : we agree with the reviewer that the motivation for the proposed Graph2Seq model could be further enhanced . We try to explain the motivation clearer as discussed below : 1 ) The most important motivation is that the celebrated Seq2Seq model demands the problems inputs are represented in sequences , which may potentially hurt the capability of a neural network model can learn from data . a ) On one hand , the input data may be naturally represented in a more complex form . As we discussed in Introduction ( second paragraph ) , AMR-to-text , SQL-to-text , and path planning in the mobile robot are good examples of such problems , where the inputs are essentially in a graph format instead of a sequence . b ) On the other hand , even if the raw inputs are originally expressed in a sequence form , it can still benefit from the enhanced inputs with additional information ( to formulate graph inputs ) . For example , for semantic parsing tasks ( text-to-AMR or text-to-SQL ) , they have been shown better performance by augmenting the original sentence sequences with other structural information such as dependency parsing trees ( Pust et al. , 2015 ) . c ) For these aforementioned problems , we presented Graph2Seq model which can be also viewed as a generalized Seq2Seq model for graphs inputs . 2 ) In general , we chose these three different tasks to demonstrate the effectiveness of our end-to-end Graph2Seq model over other Seq2Seq models . We have two main criteria to choose the selected datasets ( applications ) : a ) The first main criteria is to choose the tasks that have been used in the previous literature so it is easier to make a fair comparison on well-known datasets . As we stated in the first paragraph of the Experiments section , we chose the first two tasks to overlap with the tasks used by Li et al. , ( 2015 ) . b ) The second main criteria is to choose a real-world application so that we can test the usefulness of our model as well as other baselines . The same evaluation philosophy has also used in ( Li et al. , 2015 ) . c ) We also provided the ablation study of our model such as the impacts of aggregator and hop size on Graph2Seq model in Sec 4.4 , where these important characteristics of our model were studied . d ) We agree with the reviewer \u2019 s comments that it would be more beneficial to briefly discuss the purpose or some emphasis of each task . Therefore , we have correspondingly revised the draft based on these comments . Q2 : Rephrase the novelty argument \u201c novel attention mechanism \u201d Answer : we thank the reviewer for a very careful reading of the proper wording of \u201c novel attention mechanism \u201d . Indeed , the attention mechanism has been proposed by Bahdanau et al. , ( 2014 ) . However , what \u2019 s new in our setting is that we are the first one to design an attention mechanism between a sequence decoder and the graph node embeddings . While we agree that our attention mechanism is * simple * once the construction is seen , we argue that it is not * trivial * to create this construction in the first place . If this were trivial , we might have expected earlier work such as ( Li et al. , 2015 ) and ( Schlichtkrull et al. , 2017 ) have already to use it , yet they did not . We will revise it and make it more clear in the revision . Q3 : The novelty added by this paper is the \u201c bi-edge-direction \u201c aggregation technique with the exploration of various pooling techniques . This could be emphasized more . Answer : This is an accurate summary . We are grateful for your recognition of one of the key contributions of the paper . We will emphasize these points more in the revision . Q4 : The Related Work section could mention Graph Attention Networks ( https : //arxiv.org/abs/1710.10903 ) as an alternative to the node aggregation strategy . Answer : Yes , we fully agree with you that the self-attention scheme proposed in the work of graph attention networks can be combined with our graph encoder as well . As we mentioned in the Introduction , \u201c Graph2Seq is simple yet general and is highly extensible where its two building blocks , graph encoder , and sequence decoder , can be replaced by other models \u201d ."}, "2": {"review_id": "SkeXehR9t7-2", "review_text": "This paper proposes a graph to sequence transducer consisting of a graph encoder and a RNN with attention decoder. Strengths: - Novel architecture for graph to sequence learning. - Improved performance on synthetic transduction tasks and graph to text generation. Weaknesses: - Experiments could provide more insight into model architecture design and the strengths and weaknesses of the model on non-synthetic data. Transduction with structured inputs such as graphs is still an under-explored area, so this paper makes a valuable contribution in that direction. Previous work has mostly focused on learning graph embeddings producing outputs. This paper extends the encoder proposed by Hamilton et al (2017a) by modelling edge direction through learning \u201cforward\u201d and \u201cbackward\u201d representations of nodes. Node embeddings are pooled to a form a graph embedding to initialize the decoder, which is a standard RNN with attention over the node embeddings. The model is relatively similar to the architecture proposed by Bastings et al (2017) that uses a graph convolutional encoder, although the details of the graph node embedding computation differs. Although this model is presented in a more general framework, that model also accounted for edge directionality (as well as edge labels, which this model do not support). This paper does compare the proposed model with graph convolutional networks (GCNs) as encoder experimentally, finding that the proposed approach performs better on shortest directed path tasks. However the paper could make difference between these architectures clearer, and provide more insight into whether different graph encoder architectures might be more suited to graphs with different structural properties. The model obtains strong performance on the somewhat artificial bAbI and Shortest path tasks, while the strongest result is probably that of strong improvement over the baselines in SQL to text generation. However, very little insight is provided into this result. It would be interesting to apply this model to established NLG tasks such as AMR to text generation. Overall, this is an interesting paper, and I\u2019d be fine with it being accepted. However, the modelling contribution is relatively limited and it feels like for this to be a really strong contribution more insight into the graph encoder design, or more applications to real tasks and insight into the model\u2019s performance on these tasks is required. Editing notes: Hamilton et al 2017a and 2017c is the same paper. In some cases the citation format is used incorrectly: when the citation form part of the sentence, the citation should be inline. E.g. (p3) introduced by (Bruna et al., 2013) -> introduced by Bruna et al. (2013). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We first would like to thank the referees for their very careful reading , for identifying subpar language , typos , and discrepancies in text , and for asking questions that will help us significantly improve the presentation such as motivation and organization . Q1 : Novel architecture for graph to sequence learning\u2026 Transduction with structured inputs such as graphs is still an under-explored area , so this paper makes a valuable contribution in that direction ... We are very grateful for the kind comments of reviewers # 1 , in particular for your recognition of the key contributions of the paper . Q2 : Experiments could provide more insight into model architecture design and the strengths and weaknesses of the model on non-synthetic data . Answer : we fully agree with the reviewer \u2019 s comments ( which is also brought up by reviewer # 3 ) . According to your and reviewer # 3 \u2019 s comments , we have completely rewrite the paragraph that describes the experimental result of the SQL-to-Text task . We have also added/modified quite a few of paragraphs to better provide more insights on dataset selections , the motivation of the comparisons , and various characteristics of our Graph2Seq in terms of hop size , aggregation strategies , attention mechanism , and input graph sizes . Q3 : The model is relatively similar to the architecture proposed by Bastings et al ( 2017 ) Answer : We noticed that Bastings et al. , ( 2017 ) has utilized GCN for improving the encoder of the neural machine translation system , as we discussed in the related work . However , we would like to point out three major differences between their model and our Graph2Seq model in terms of model architecture : 1 ) Since Bastings \u2019 s encoder are built on GCN , which itself is derived from spectral graph convolutional neural networks , their model can be only used under transductive settings . In contrast , our graph encoder can be used under both transductive and inductive settings . 2 ) Although Bastings \u2019 s encoder takes into account both incoming and outgoing edges as well as the edge labels , they only compute a single node embedding using the information from both directions . This is quite different from our bidirectional aggregation strategies , which is inspired from the bi-LSTM architecture , where we generate the representation of a node for each direction ( forward or backward ) , and then concatenating them . Intuitively , this architecture could explicitly represent the context of a node , i.e. , the forward context representation and backward context representation . 3 ) Bastings et al . ( 2017 ) used the same attention based decoder of Bahdanau et al . ( 2015 ) while we design an attention-based decoder over the graph node embeddings . Therefore , our attention mechanism is independent of underlying specific tasks and generally applicable to different tasks . Q4 : However the paper could make difference between these architectures clearer , and provide more insight into whether different graph encoder architectures might be more suited to graphs with different structural properties . Answer : we thank for the reviewer \u2019 s suggestions and we have added some more discussions about the differences between different graph encoder . For different graph encoders , the most important advantages of our graph encoder lie in the fact that , when the graph size increases , all graph encoders started to perform poorer due to losing global information of a graph . However , our bi-directional node embeddings help converge rapidly to the optimal performance as shown in Figure 4 . Q5 : However , very little insight is provided into this result . It would be interesting to apply this model to established NLG tasks such as AMR to text generation . Answer : please see our previous response for providing more insights into the SQL-to-Text task . Conceptually , the AMR-to-Text task would be similar to the SQL-to-Text task , which we would be happy to explore in the near future . Q6 : Overall , this is an interesting paper , and I \u2019 d be fine with it being accepted . However , the modeling contribution is relatively limited and it feels like for this to be a really strong contribution more insight into the graph encoder design , or more applications to real tasks and insight into the model \u2019 s performance on these tasks is required . Answer : we hope that our previous responses have helped better explain the novelty of our Graph2Seq model architecture over existing works . According to your and reviewer # 3 \u2019 s comments , we have rephrased our key contributions of our model , that is , a novel graph encoder to learn a bi-directional node embedding for directed and undirected graphs with node attributes by employing various aggregation strategies , and to learn graph-level embedding by exploiting two different graph embedding techniques . In addition , to the best of knowledge , our attention mechanism to learn the alignments between nodes and sequence elements to better cope with larger graphs is also proposed for the first time ."}}