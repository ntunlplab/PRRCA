{"year": "2017", "forum": "Sks9_ajex", "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer", "decision": "Accept (Poster)", "meta_review": "Important task (attention models), interesting distillation application, well-written paper. The authors have been responsive in updating the paper, adding new experiments, and being balanced in presenting their findings. I support accepting this paper.", "reviews": [{"review_id": "Sks9_ajex-0", "review_text": "The paper presented a modified knowledge distillation framework that minimizes the difference of the sum of statistics across the a feature map between the teacher and the student network. The authors empirically demonstrated the proposed methods outperform the fitnet style distillation baseline. Pros: + The author evaluated the proposed methods on various computer vision dataset + The paper is in general well-written Cons: - The method seems to be limited to the convolutional architecture - The attention terminology is misleading in the paper. The proposed method really just try to distill the summed squared(or other statistics e.g. summed lp norm) of activations in a hidden feature map. - The gradient-based attention transfer seems out-of-place. The proposed gradient-based methods are never compared directly to nor are used jointly with the \"attention-based\" transfer. It seems like a parallel idea added to the paper that does not seem to add much value. - It is also not clear how the induced 2-norms in eq.(2) is computed. Q is a matrix \\in \\mathbb{R}^{H \\times W} whose induced 2-norm is its largest singular value. It seems computationally expensive to compute such cost function. Is it possible the authors really mean the Frobenius norm? Overall, the proposed distillation method works well in practice but the paper has some organization issues and unclear notation. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer , Below are our answers : Q : The method seems to be limited to the convolutional architecture A : Fist , we would like to note that our method can be directly applied even to non convolutional architectures ( such as MLPs ) given that our definitions of attention are also valid for such architectures . We \u2019 re also currently looking into applying attention transfer for recurrent neural networks in NLP tasks . Independently of the above , we consider that the convolutional architecture is one of the most important and widely used type of network architectures ( with a really huge impact on several fields ) . Q : The attention terminology is misleading in the paper . The proposed method really just try to distill the summed squared ( or other statistics e.g.summed lp norm ) of activations in a hidden feature map . A : We respectfully disagree with this statement . In our view a spatial attention map is supposed to indicate how important each spatial location of the input layer ( or of an intermediate layer ) is w.r.t.the output computed by the network ( in other words , roughly how much focus the network puts per spatial location ) . Here we provide two simple ways of defining such an attention map , an activation based one and a gradient based one . Q : The gradient-based attention transfer seems out-of-place . The proposed gradient-based methods are never compared directly to nor are used jointly with the `` attention-based '' transfer . It seems like a parallel idea added to the paper that does not seem to add much value . A : A main contribution of this work is the idea that attention transfer can be very useful for improving the performance of a network . We therefore wanted to show that this is indeed true even when using different ways for defining attention , which is why we included results for both activation based and gradient based attention . Nevertheless we agree that it will be useful to compare the two methods under the same conditions , and , to that end , we are going to update the paper with new experiments . Q : It is also not clear how the induced 2-norms in eq . ( 2 ) is computed . Q is a matrix \\in \\mathbb { R } ^ { H \\times W } whose induced 2-norm is its largest singular value . It seems computationally expensive to compute such cost function . Is it possible the authors really mean the Frobenius norm ? A : Sorry for the confusion , it is indeed a Frobenius norm . Essentially , in eq . ( 2 ) we consider that all attention maps are in vectorized form , in which case the standard norm notation for vectors applies . We have updated the paper respectively . Thanks ."}, {"review_id": "Sks9_ajex-1", "review_text": "The paper proposes a new way of transferring knowledge. I like the idea of transferring attention maps instead of activations. However, the experiments don\u2019t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section. I would consider updating the score if the authors extend the last section 4.2.2.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer , Below is our answer : Q : However , the experiments don \u2019 t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section . A : We \u2019 ve updated the paper with an ImageNet experiment where we used two attention losses and trained from scratch , and got 1.1 % /0.8 % top1/top5 accuracy improvement over baseline ResNet-18 , ~30 % reduction in error difference between teacher and student . We think that this is a remarkable result , considering our very limited computational resources ( 8 GPUs ) , and are sure it could be easily further improved by using more AT losses , tuning hyperparameters , or using more powerful teachers . We are currently running experiments with : 1 . All four AT losses , one for each group of residual blocks ; 2 . KD baseline ; and will update the paper as soon as we get the results . Thanks ."}, {"review_id": "Sks9_ajex-2", "review_text": "This paper proposes to investigate attention transfers between a teacher and a student network. Attention transfer is performed by minimising the l2 distance between the teacher/student attention maps at different layers, in addition to minimising the classification loss and optionally a knowledge distillation term. Authors define several activation based attentions (sum of absolute feature values raise at the power p or max of values raised at the power p). They also propose a gradient based attention (derivative of the Loss w.r.t. inputs). They evaluate their approaches on several datasets (CIFAR, Cub/Scene, Imagenet) showing that attention transfers does help improving the student network test performance. However, the student networks performs worst than the teacher, even with attention. Few remarks/questions: - in section 3 authors claim that networks with higher accuracy have a higher spatial correlation between the object and the attention map. While Figure 4 is compelling, it would be nice to have quantitative results showing that as well. - how did you choose the hyperparameter values, it would be nice to see what is the impact of $\\beta$. - it would be nice to report teacher train and validation loss in Figure 7 b) - from the experiments, it is not clear what at the pros/cons of the different attention maps - AT does not lead to better result than the teacher. However, the student networks have less parameters. It would be interesting to characterise the corresponding speed-up. If you keep the same architecture between the student and the teacher, is there any benefit to the attention transfer? In summary: Pros: - Clearly written and well motivated. - Consistent improvement of the student with attention compared to the student alone. Cons: - Students have worst performances than the teacher models. - It is not clear which attention to use in which case? - Somewhat incremental novelty relatively to Fitnet ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer , Below are our answers : Q : in section 3 authors claim that networks with higher accuracy have a higher spatial correlation between the object and the attention map . While Figure 4 is compelling , it would be nice to have quantitative results showing that as well . A : Thanks for the suggestion . We are going to try to look into how to do that for the VOC or COCO dataset . However , it should be noted that this is in fact a not so trivial task ( e.g. , due to the need of bounding box or segmentation mask annotations , due to the need to deal with overlapping objects etc . ) Q : how did you choose the hyperparameter values , it would be nice to see what is the impact of $ \\beta $ . A : We found our method to be quite stable with respect to different values of beta , i.e.in all our experiments we observed that there \u2019 s typically a wide range of values which give improvements close to optimal ; Q : it would be nice to report teacher train and validation loss in Figure 7 b ) A : Thanks , we are going to add this ; Q : from the experiments , it is not clear what at the pros/cons of the different attention maps A : We are currently running attention transfer experiments using grad-based and activation-based attention maps under the same conditions , and going to update the paper as soon as we have the results ; Q : AT does not lead to better result than the teacher . However , the student networks have less parameters . It would be interesting to characterise the corresponding speed-up . If you keep the same architecture between the student and the teacher , is there any benefit to the attention transfer ? A : The baselines we choose have very different numbers of parameters , thus making it very difficult for student to have better or even the same accuracy as teacher , as network performance largely depends on this number . This was shown in a number of recent works , including our Wide ResNet paper . It can be shown though that AT leads to drastic improvements in speed and number of parameters needed to achieve the same accuracy . For example , to achieve 7.5 % on CIFAR-10 one would need a ResNet with 300k ( e.g.WRN-16-1.3 ) parameters , whereas AT+KD achieves it with only 160k parameters , resulting in about 2x less parameters and a much more efficient network ; Q : Somewhat incremental novelty relatively to Fitnet A : We would like to note that one main goal/contribution of this work is to convey the idea that the use of attention transfer during training can be an important factor for improving a network \u2019 s performance . In a case like this , the student is forced to mimic only a small \u201c summary \u201d of the teacher \u2019 s data , as opposed to FitNets that try to mimic the full activation maps . Yet we show that our method achieves much better results , which we consider as a very interesting finding . Thanks ."}], "0": {"review_id": "Sks9_ajex-0", "review_text": "The paper presented a modified knowledge distillation framework that minimizes the difference of the sum of statistics across the a feature map between the teacher and the student network. The authors empirically demonstrated the proposed methods outperform the fitnet style distillation baseline. Pros: + The author evaluated the proposed methods on various computer vision dataset + The paper is in general well-written Cons: - The method seems to be limited to the convolutional architecture - The attention terminology is misleading in the paper. The proposed method really just try to distill the summed squared(or other statistics e.g. summed lp norm) of activations in a hidden feature map. - The gradient-based attention transfer seems out-of-place. The proposed gradient-based methods are never compared directly to nor are used jointly with the \"attention-based\" transfer. It seems like a parallel idea added to the paper that does not seem to add much value. - It is also not clear how the induced 2-norms in eq.(2) is computed. Q is a matrix \\in \\mathbb{R}^{H \\times W} whose induced 2-norm is its largest singular value. It seems computationally expensive to compute such cost function. Is it possible the authors really mean the Frobenius norm? Overall, the proposed distillation method works well in practice but the paper has some organization issues and unclear notation. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer , Below are our answers : Q : The method seems to be limited to the convolutional architecture A : Fist , we would like to note that our method can be directly applied even to non convolutional architectures ( such as MLPs ) given that our definitions of attention are also valid for such architectures . We \u2019 re also currently looking into applying attention transfer for recurrent neural networks in NLP tasks . Independently of the above , we consider that the convolutional architecture is one of the most important and widely used type of network architectures ( with a really huge impact on several fields ) . Q : The attention terminology is misleading in the paper . The proposed method really just try to distill the summed squared ( or other statistics e.g.summed lp norm ) of activations in a hidden feature map . A : We respectfully disagree with this statement . In our view a spatial attention map is supposed to indicate how important each spatial location of the input layer ( or of an intermediate layer ) is w.r.t.the output computed by the network ( in other words , roughly how much focus the network puts per spatial location ) . Here we provide two simple ways of defining such an attention map , an activation based one and a gradient based one . Q : The gradient-based attention transfer seems out-of-place . The proposed gradient-based methods are never compared directly to nor are used jointly with the `` attention-based '' transfer . It seems like a parallel idea added to the paper that does not seem to add much value . A : A main contribution of this work is the idea that attention transfer can be very useful for improving the performance of a network . We therefore wanted to show that this is indeed true even when using different ways for defining attention , which is why we included results for both activation based and gradient based attention . Nevertheless we agree that it will be useful to compare the two methods under the same conditions , and , to that end , we are going to update the paper with new experiments . Q : It is also not clear how the induced 2-norms in eq . ( 2 ) is computed . Q is a matrix \\in \\mathbb { R } ^ { H \\times W } whose induced 2-norm is its largest singular value . It seems computationally expensive to compute such cost function . Is it possible the authors really mean the Frobenius norm ? A : Sorry for the confusion , it is indeed a Frobenius norm . Essentially , in eq . ( 2 ) we consider that all attention maps are in vectorized form , in which case the standard norm notation for vectors applies . We have updated the paper respectively . Thanks ."}, "1": {"review_id": "Sks9_ajex-1", "review_text": "The paper proposes a new way of transferring knowledge. I like the idea of transferring attention maps instead of activations. However, the experiments don\u2019t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section. I would consider updating the score if the authors extend the last section 4.2.2.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer , Below is our answer : Q : However , the experiments don \u2019 t show a big improvement compared with knowledge distillation alone and I think more experiments are required in IMAGENET section . A : We \u2019 ve updated the paper with an ImageNet experiment where we used two attention losses and trained from scratch , and got 1.1 % /0.8 % top1/top5 accuracy improvement over baseline ResNet-18 , ~30 % reduction in error difference between teacher and student . We think that this is a remarkable result , considering our very limited computational resources ( 8 GPUs ) , and are sure it could be easily further improved by using more AT losses , tuning hyperparameters , or using more powerful teachers . We are currently running experiments with : 1 . All four AT losses , one for each group of residual blocks ; 2 . KD baseline ; and will update the paper as soon as we get the results . Thanks ."}, "2": {"review_id": "Sks9_ajex-2", "review_text": "This paper proposes to investigate attention transfers between a teacher and a student network. Attention transfer is performed by minimising the l2 distance between the teacher/student attention maps at different layers, in addition to minimising the classification loss and optionally a knowledge distillation term. Authors define several activation based attentions (sum of absolute feature values raise at the power p or max of values raised at the power p). They also propose a gradient based attention (derivative of the Loss w.r.t. inputs). They evaluate their approaches on several datasets (CIFAR, Cub/Scene, Imagenet) showing that attention transfers does help improving the student network test performance. However, the student networks performs worst than the teacher, even with attention. Few remarks/questions: - in section 3 authors claim that networks with higher accuracy have a higher spatial correlation between the object and the attention map. While Figure 4 is compelling, it would be nice to have quantitative results showing that as well. - how did you choose the hyperparameter values, it would be nice to see what is the impact of $\\beta$. - it would be nice to report teacher train and validation loss in Figure 7 b) - from the experiments, it is not clear what at the pros/cons of the different attention maps - AT does not lead to better result than the teacher. However, the student networks have less parameters. It would be interesting to characterise the corresponding speed-up. If you keep the same architecture between the student and the teacher, is there any benefit to the attention transfer? In summary: Pros: - Clearly written and well motivated. - Consistent improvement of the student with attention compared to the student alone. Cons: - Students have worst performances than the teacher models. - It is not clear which attention to use in which case? - Somewhat incremental novelty relatively to Fitnet ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer , Below are our answers : Q : in section 3 authors claim that networks with higher accuracy have a higher spatial correlation between the object and the attention map . While Figure 4 is compelling , it would be nice to have quantitative results showing that as well . A : Thanks for the suggestion . We are going to try to look into how to do that for the VOC or COCO dataset . However , it should be noted that this is in fact a not so trivial task ( e.g. , due to the need of bounding box or segmentation mask annotations , due to the need to deal with overlapping objects etc . ) Q : how did you choose the hyperparameter values , it would be nice to see what is the impact of $ \\beta $ . A : We found our method to be quite stable with respect to different values of beta , i.e.in all our experiments we observed that there \u2019 s typically a wide range of values which give improvements close to optimal ; Q : it would be nice to report teacher train and validation loss in Figure 7 b ) A : Thanks , we are going to add this ; Q : from the experiments , it is not clear what at the pros/cons of the different attention maps A : We are currently running attention transfer experiments using grad-based and activation-based attention maps under the same conditions , and going to update the paper as soon as we have the results ; Q : AT does not lead to better result than the teacher . However , the student networks have less parameters . It would be interesting to characterise the corresponding speed-up . If you keep the same architecture between the student and the teacher , is there any benefit to the attention transfer ? A : The baselines we choose have very different numbers of parameters , thus making it very difficult for student to have better or even the same accuracy as teacher , as network performance largely depends on this number . This was shown in a number of recent works , including our Wide ResNet paper . It can be shown though that AT leads to drastic improvements in speed and number of parameters needed to achieve the same accuracy . For example , to achieve 7.5 % on CIFAR-10 one would need a ResNet with 300k ( e.g.WRN-16-1.3 ) parameters , whereas AT+KD achieves it with only 160k parameters , resulting in about 2x less parameters and a much more efficient network ; Q : Somewhat incremental novelty relatively to Fitnet A : We would like to note that one main goal/contribution of this work is to convey the idea that the use of attention transfer during training can be an important factor for improving a network \u2019 s performance . In a case like this , the student is forced to mimic only a small \u201c summary \u201d of the teacher \u2019 s data , as opposed to FitNets that try to mimic the full activation maps . Yet we show that our method achieves much better results , which we consider as a very interesting finding . Thanks ."}}