{"year": "2017", "forum": "H1Fk2Iqex", "title": "Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on Animal calls and Speech", "decision": "Invite to Workshop Track", "meta_review": "This paper studies efficient signal representations to perform bioacoustic classification based on CNNs. Contrary to image classification, where most useful information can be extracted with spatially localized kernels, bioacoustic signatures are more localized in the frequency domain, requiring to rethink the design of convolutional architectures. The authors propose to enforce the lower layers of the architecture with chirplet transforms, which are localized in the time-frequency plane as wavelets, but with time-varying central frequency. They present preliminary numerical experiments showing promising improvements over existing baselines. \n \n The reviewers found interest in the method, but raised concerns on the relatively narrow scope of the method, as well as the clarity and rigor of the presentation. Whereas the first concern is up to debate, I agree that the paper currently suffers from poor english which affects its clarity. \n \n Despite these concerns, the AC finds the contribution useful in the broader context of inductive bias and injecting priors in neural networks. This is an example where the inductive priors that work well on images (localized convolutions rather than generic fully connected layers) are not sufficient unless given massive amounts of data. The AC thus recommends rejection, but invites the contribution to the workshop track.", "reviews": [{"review_id": "H1Fk2Iqex-0", "review_text": "While I understand the difficulty of collecting audio data from animals, I think this type of feature engineering does not go in the right direction. I would rather see a model than learns the feature representation from data. I would think it should be possible to collect a more substantial corpus in zoos / nature etc, and then train a generative model. The underlying learned feature representation could be then used to feed a classifier. I'm not familiar with the particularities of this task, it's hard to judge the improvements by using chirplets.", "rating": "6: Marginally above acceptance threshold", "reply_text": "This is indeed the direction that we discuss in this paper . We trained a CNN from raw audio ( see page12 ) to then we show faster training and better MAP with our approach . A full feature learning should be taken for large scale problems being tackled with deep learning approaches . However , in order to use fully learned models , the number of observations must be important ( at least as many as the number of free parameters in the model ) , and from clean mono species recordings , and important regularization should be applied precisely . Nowadays , bioacoustic research ( see http : //sabiod.org to get links to the largest available challenge on bird classification , including some we organized at NIPS and ICML workshops ) , the volume of available mono-species clean recordings to learn the underlying distribution per species is limited . The experiment we run in this paper is based on the largest Amazon avian dataset ( cf LifeClef 2015 , 16 , 17 ) . The selected species we train are represented by the real available files for each species . It is not so much as you can see . An alternative is thus to use our prior knowledge from advanced Q constant acoustic representation , to try to bias the network towards this direction which in our case corresponds to applying a chirplet transform to the raw waveform , and then retrain the system to adapt the chirpnet to the training data ."}, {"review_id": "H1Fk2Iqex-1", "review_text": "Pros: - Introduction of a nice filter banks and its implementation - Good numerical results - Refinement of the representation via back propagation, and a demonstration that it speeds up learning Cons: - The algorithms (section 3.1) are not necessary, and they even affect the presentation of the paper. However, a source code would be great! - The link with a scattering transform is not clear - Sometimes (as mentionned in some of my comments), the writing could be improved. From a personal point of view, I also believe the negative points I mention can be easily removed.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the comments and reviewing process . We now have new performances results on the TIMIT ( speech ) dataset . This plus a reworking of the paper had been done . All the changes are present in the updated paper version . Regards"}, {"review_id": "H1Fk2Iqex-2", "review_text": "The authors advocate use of chirplets as a basis for modeling audio signals. They introduce a fast chiplet transform for efficient computation. Also introduced is the idea of initializing (pre-training) CNN layers to mimic chirplet transform of audio signal (similar to ideas proposed by Mallet et al. on scattering transforms). The paper is fairly easy to follow but in a few places contains undefined terms (e.g. AM-FM, MAP). While the idea of using chirplet transform is interesting, my main concern is that the empirical evidence provided is in a rather narrow domain of bird call classification. Furthermore, the accuracy gains shown in that domain are relatively small (61% MAP for log-Mel features vs 61.5% for chirplet transforms). I would recommend that authors provide evidence for how this generalizes to other audio (including speech) tasks.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your questions . This idea of pre-training is not similar to what is proposed by Mallat , in fact in the scattering transform , everything is deterministic and not just `` initialized '' the only learning is done by the final classifier . Only recently they changed this to pre-training due to the success of neural networks . The used bird call classification challenge provides the largest sound corpus for this kind of bioacoustic classification tasks . The gain is not just concerning the gain in classification accuracy but also the speed of convergence of the neural network . We are currently working on the TIMIT ( speech ) dataset to present more results on the chirpnet performances , hopefully before the review deadline . Feel free to ask anymore points . Regards ."}], "0": {"review_id": "H1Fk2Iqex-0", "review_text": "While I understand the difficulty of collecting audio data from animals, I think this type of feature engineering does not go in the right direction. I would rather see a model than learns the feature representation from data. I would think it should be possible to collect a more substantial corpus in zoos / nature etc, and then train a generative model. The underlying learned feature representation could be then used to feed a classifier. I'm not familiar with the particularities of this task, it's hard to judge the improvements by using chirplets.", "rating": "6: Marginally above acceptance threshold", "reply_text": "This is indeed the direction that we discuss in this paper . We trained a CNN from raw audio ( see page12 ) to then we show faster training and better MAP with our approach . A full feature learning should be taken for large scale problems being tackled with deep learning approaches . However , in order to use fully learned models , the number of observations must be important ( at least as many as the number of free parameters in the model ) , and from clean mono species recordings , and important regularization should be applied precisely . Nowadays , bioacoustic research ( see http : //sabiod.org to get links to the largest available challenge on bird classification , including some we organized at NIPS and ICML workshops ) , the volume of available mono-species clean recordings to learn the underlying distribution per species is limited . The experiment we run in this paper is based on the largest Amazon avian dataset ( cf LifeClef 2015 , 16 , 17 ) . The selected species we train are represented by the real available files for each species . It is not so much as you can see . An alternative is thus to use our prior knowledge from advanced Q constant acoustic representation , to try to bias the network towards this direction which in our case corresponds to applying a chirplet transform to the raw waveform , and then retrain the system to adapt the chirpnet to the training data ."}, "1": {"review_id": "H1Fk2Iqex-1", "review_text": "Pros: - Introduction of a nice filter banks and its implementation - Good numerical results - Refinement of the representation via back propagation, and a demonstration that it speeds up learning Cons: - The algorithms (section 3.1) are not necessary, and they even affect the presentation of the paper. However, a source code would be great! - The link with a scattering transform is not clear - Sometimes (as mentionned in some of my comments), the writing could be improved. From a personal point of view, I also believe the negative points I mention can be easily removed.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the comments and reviewing process . We now have new performances results on the TIMIT ( speech ) dataset . This plus a reworking of the paper had been done . All the changes are present in the updated paper version . Regards"}, "2": {"review_id": "H1Fk2Iqex-2", "review_text": "The authors advocate use of chirplets as a basis for modeling audio signals. They introduce a fast chiplet transform for efficient computation. Also introduced is the idea of initializing (pre-training) CNN layers to mimic chirplet transform of audio signal (similar to ideas proposed by Mallet et al. on scattering transforms). The paper is fairly easy to follow but in a few places contains undefined terms (e.g. AM-FM, MAP). While the idea of using chirplet transform is interesting, my main concern is that the empirical evidence provided is in a rather narrow domain of bird call classification. Furthermore, the accuracy gains shown in that domain are relatively small (61% MAP for log-Mel features vs 61.5% for chirplet transforms). I would recommend that authors provide evidence for how this generalizes to other audio (including speech) tasks.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your questions . This idea of pre-training is not similar to what is proposed by Mallat , in fact in the scattering transform , everything is deterministic and not just `` initialized '' the only learning is done by the final classifier . Only recently they changed this to pre-training due to the success of neural networks . The used bird call classification challenge provides the largest sound corpus for this kind of bioacoustic classification tasks . The gain is not just concerning the gain in classification accuracy but also the speed of convergence of the neural network . We are currently working on the TIMIT ( speech ) dataset to present more results on the chirpnet performances , hopefully before the review deadline . Feel free to ask anymore points . Regards ."}}