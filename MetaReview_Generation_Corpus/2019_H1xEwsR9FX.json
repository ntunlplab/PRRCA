{"year": "2019", "forum": "H1xEwsR9FX", "title": "Convolutional CRFs for Semantic Segmentation", "decision": "Reject", "meta_review": "The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel. They show that inference is more efficient and training is easier.\n\nIn practice, the synthetic experiments seem to show a greater improvement than appears in real data.  There are concerns about the clarity, lack of theoretical proofs, and at times overstated claims that do not have sufficient support.\n\nThe ratings before the rebuttal and discussion were 7-4-6.  After, R1 adjusted their score from 6 to 4.  R2 initially gave a 7 but later said \"I think the authors missed an opportunity here. I rated it as an accept, because I saw what it could have been after a good revision. The core idea is good, but fully agree with R1 and R3 that the paper needs work (which the authors were not willing to do). I checked the latest revision (as of Monday morning). None of R3's writing/claims issues are fixed, neither were my additional experimental requests, not even R1's typos.\" There is therefore a consensus among reviewers for reject.\n", "reviews": [{"review_id": "H1xEwsR9FX-0", "review_text": "+ well written + Good idea - Technical section not fully clear - Some experimental issues The paper is well written, and clearly explains the background material and concepts. It might almost be a bit too detailed, as the main technical section (4) feels a bit rushed. (more below). From what I can judge the main idea in the paper is sound. The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel. They show that inference is more efficient and training is easier. The technical section is not very clear. For example: Are the filter weights recomputed for each spatial location, is there any acceleration that speeds this up? How large can the authors make the filter kernel, before the perhutohedral lattice is faster again? Finally, the experimental section has some room for improvement. I liked the comparison of decoupled and coupled CRF training, but I didn't get much out of the synthetic experiments. I found it particularly confusing since Table 1 doesn't mention that the experiments use ground truth (test) labels that were corrupted. Second, it would be nice to have a side-to-side comparison between ConvCRF and CRFasRNN. I'd recommend the authors to either use the CRFasRNN training setup for both methods, or spend the week or two training CRFasRNN using their training procedure. It is fine to do either of the two experiments and have four entries in that table.", "rating": "7: Good paper, accept", "reply_text": "> > From what I can judge the main idea in the paper is sound . The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel . They show that inference is more efficient and training is easier . This is exactly what the paper is about . > > Are the filter weights recomputed for each spatial location , is there any acceleration that speeds this up ? Yes , the weights of the filter matrix $ k_g $ need to be computed for each spatial location , since they are based in features unique to each location . We implement this by representing $ k_g $ as one large 5-dimensional tensor ( $ k_g [ b , dx , dy , x , y ] $ ) . For a fixed $ dx $ and $ dy $ the partial tensor $ k_g [ : ,dx , dy , : , : ] $ can be computed using matrix operations on the feature vector $ f_i $ . Those matrix operations follow the single instruction , multiple data ( SIMD ) shema and thus can be computed highly efficiently on GPUs ( using any common deep learning framework ) . We do n't apply any additional optimization . > > How large can the authors make the filter kernel , before the perhutohedral lattice is faster again ? In section 5 , Table 1 we report the speed of ConvCRF with different filter sizes . The computational time increases quadratically w.r.t.the filter size . So we can estimate that with a filter size of about $ 50 $ both filtering methods have the same speed . Unfortunately we ca n't experimentally verify this , since our GPU memory can only handle filters up to $ 31 $ . This is however not a huge drawback , since we do n't observe any improvement in performance for filter-sizes larger than $ 11 $ . > > I liked the comparison of decoupled and coupled CRF training , but I did n't get much out of the synthetic experiments . I found it particularly confusing since Table 1 does n't mention that the experiments use ground truth ( test ) labels that were corrupted . The experimental setup for table 1 is discussed in section 5.1 . However we have also augmented the caption of the table 1 to make this point clearer ( all changes will be uploaded till Monday ) . The synthetic benchmark serves multiple purposes . Label completion and denoising are tasks in which CRFs have been traditionally strong in . Thus we provide a sound 'hello world ' task which can be solved by both CRFs without any training . This also helps us understand the structural differences between the models since optimization is not anymore part of the equation . > > I 'd recommend the authors to either use the CRFasRNN training setup for both methods , or spend the week or two training CRFasRNN using their training procedure . It is fine to do either of the two experiments and have four entries in that table . This is indeed a good idea . The experiments are currently running and will be finished till Monday . We are going to amend section 5.3 with those results ."}, {"review_id": "H1xEwsR9FX-1", "review_text": "The authors propose an efficient method to perform message passing on a truncated Gaussian kernel CRF. The main contributions are the definition of a specific form of truncated Gaussian kernel that allows for fast message passing via convolutions, and the implementation of such parallelized message passing on GPU. In my opinion, the paper fails to convey the main idea in a clear and precise manner, the notation is mixed and often confusing, furthermore there are a number of sentences that should be rephrased to be less sensationalist, or removed. The experiments seem to show performance in par with the FullCRF on decoupled training, which seem in contrast with the much bigger performance gain of the first experiment on syntetic data. No discussion has been provided as to the possible reasons of this performance gap, although the experimental settings appear to be similar. Finally, in the last experiments with end-to-end training the authors report a performance improvement over CRFasRNN, a 3 years old paper that is far away in terms of performance with the current SOTA on Pascal VOC. The authors base on a different network than that of the CRFasRNN baseline (i.e., the difference is not only in the CRF implementation, but rather the whole network before the CRF in the proposed method), it is therefore difficult to say whether the performance improvement is due to the ResNet101 + FCN unary potentials, which is not a contribution of this manuscript, or to the proposed CRF. In general, I believe that the considerable speed gain of the proposed method might be enough to justify a publication, but the paper should be phrased in that sense if that was the intention of the authors. It is unclear to me whether the main contribution they claim is segmentation performance (IoU) or speed or both. The main contributions of this work should be stated clearly, and the modelling differences w.r.t. the FullCRF model that they aim to improve should be more explicit in the text rather than let to the reader to infer comparing the formulas. On these grounds, I suggest a major revision of the paper and I don't recommend publication at this stage. MAJOR 1) I firmly advocate against making strong claims, unless supported by solid proofs. I strongly recommend to rephrase, if not remove, exaggerate claims such as: a) \"[deep networks] lack the capability to utilize context information and cannot model interactions between predictions directly\". This is simply not true. Any CNN with enough layers will exploit contextual information. Furthermore, any autoregressive model will model the interaction between predictions directly. See e.g., \"RiFCN: Recurrent Network in Fully Convolutional Network for Semantic Segmentation of High Resolution Remote Sensing Images\" by Mou et Al., \"ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation\" by Visin et Al., or \"Predicting Deeper into the Future of Semantic Segmentation\" by Luc et Al. for video semantic segmentation. b) \"CRF inference is two orders of magnitude slower than CNN inference\": this, of course, depends on the kind of CRF. c) \"The long training times of the current generation of CRFs also make more in-depth research and experiments with such structured models impractical\": again, not true. While it's true that CRFs tend to be slow, research with such models is not impractical and indeed there are papers that focus exactly on that (among the others, some of the ones cited in this manuscript). d) \"we propose to add the strong and valid assumption of conditional independence\": as with every assumption, this is an approximation. I wouldn't claim it to be valid nor invalid, as it is simply a modeling choice. e) \"Predictions are pixel-wise and conditionally independendent (given the common feature base of nearby pixels). Structured knowledge and background context is ignored in these models.\". It's unclear what is meant by \"structured knowledge\", but to my best understanding this sentence is misleading or wrong. Background context is considered by CNN-based models, as well as the general structure in the image. f) \"In the context of semantic segmentation most CRF based approaches are based on the Fully Connected CRF\". CRFs have been used much earlier than 2011, before Fully Connected CRF was published. g) \"This makes the theoretical foundation of ConvCRF very promising, strong and valid assumptions are the powerhouse of machine learning modelling.\" The authors here propose a logical, but quite obvious, approximation, i.e., to constrain the CRF to model dependencies in a local neighborhood. This is the same implicit assumption of many other Gaussian kernels and algorithms for approximated inference. For how it's surely valid, and possibly strong, I don't see how it can make a \"theoretical foundation\". The self-congratulatory closure is unnecessary, and inappropriate. 2) While CRFs have been used for a long time on visual data, the citations in this work focus mostly on the last few years. I suggest to add at least one of the following: * \u201cDiscriminative fields for modeling spatial dependencies in natural images\" 2003 * \"Multiscale conditional random fields for image labelling\" 2004 * \u201cTextonboost: Joint appearance, shape and context modeling for mulit-class object recognition and segmentation,\u201d 2006 It could also be beneficial to the reader to add to the references broad overview works, such as \"An Introduction to Conditional Random Fields\" by Charles Sutton and Andrew McCallum, 2011, and/or \"Structured prediction and learning in computer vision\" by Nowozin and Lampert, 2011. 3) Line 5 of the algorithm adds the unary potentials at each iteration of message passing. Can you elaborate on the motivation behind this choice? The algorithm is already initialized with such potentials, and to my best knowledge unary potentials are not usually added in the mean field message passing loop. It would be interesting to compare the performance of the algorithm with and without this addition. 4) Sec4.1 reports that the filters are constant over the channel dimension c and that, in other words, this can be seen as applying the convolution over the dimension c. I fail to understand this sentence. My understanding from the formula is that the same kernel is applied to all the channels of the input, i.e., the channels of the input fed to the CRF are all processed in the same way. This should be explained clearly and the reasoning behind this choice should be explained as well. Furthermore, if I am not mistaken the authors learn a different filter in each position *of the input* rather than reusing the same filters at every position. This choice should be clarified and discussed. 5) Regarding the implementation, is there a reason not to apply a convolution with a flipped kernel to compute the cross-correlation? Also, IIRC if the kernel is symmetric (as should be for a Gaussian kernel) convolution and cross-correlation are the same. 6) The notation is often ambiguous and at times unnecessarely heavy. I strongly recommend to go over the manuscript and use a consistent notation, making sure that every element of the notation is introduced before or right after it's used. In particular, a) Sec3: in the text, a segmentation instance is referred to as X, while in the formula as \\hat x. \\tilde I is never introduced. k_{\\alpha} is defined but I believe never used (I suggest to drop the name if there is no reference to it). Is there a reason to drop the subscript G in the FullCRF pairwise potential? Or conversely, is there a reason to have it everywhere else? Furthermore, there doesn't seem to be a difference between k_G, k_g and g, I suggest to use only one consistent notation to refer to the Gaussian kernels throughout the paper. It's also unclear if the I superscript is needed for the feature vectors. b) Sec4.1, The shape of the Gaussian kernels is the same as that of the input. I believe that the input in this context refers to a patch and not to the whole image. If so, this should be specified, otherwise the dimensions of the kernel should be referred to with a different letter than those of the input. c) Sec4.1, I believe dx and dy refer to the in-kernel displacement. Their semantic is not clear from the text and should be defined properly. d) Sec4.1, the feature vectors are defined in the text as f_1..f_d. The formula of the kernel uses f_i^{(d)} instead. It's unclear what the superscripts stands for and whether it is actually useful or redundant. e) Sec4.1, x and y are not defined, I suspect they refer to the position of the pixel, which was previously encoded as p_i and p_j. Once again, the notation should be consistent across the manuscript. f) In the definition of the convCRFs, w is used for the width of the input, w_i for the weights of the kernels. In the FullCRF, w^{(1)} and w^{(2)} for the potentials, w^{(m)} for the sum over the kernels. k is used for the kernel dimension, k_G to refer to the kernel itself, as well as g. The notation could be made less ambiguous and consistent (superscript vs subscript semantics). g) Sec3, the number of pixel is defined as n but in Formula 2 N is used instead. g) Vectors and matrices should be bold-face. The use of capital letters for constants might also improve the readability of the manuscript. 7) The experiments with the Conv (ConvCRF?) variants of Table 2 are not discussed in the text. 8) Although Sec5.2 concludes with \"The experiments also confirm the observation of Sec5.1, that ConvCRF performs slightly better than FullCRF\", Sec5.1 reported that \"it can be seen that ConvCRFs outperform FullCRFs significantly\". The authors should decide whether the results are slightly better or outperform the baseline. In general a in-depth discussion on the performance of the algorithm is missing. 9) Sec5.3, it's unclear what this sentence means \"introduce an auxiliary unary loss to counterbalance the vanishing gradient problem\". If such a term has been added, it should be reported in a formula and it's effectiveness should be supported by experimental data. MINOR m1) In the related work, the sentence \"transposed convolution layers are applied at the end of the prediction pipeline ot produce high-resolution output\" seems to suggest these are always applied, while many recent methods rely on bilinear upsampling to recover the original resolution. Please rephrase it accordingly. m2) In Parameter learning in CRF: \"the idea utilizes, that for the message passing the identity .. is valid.\" This sentence doesn't make any sense to me. Is it possible it is a leftover? m4) In Sec3, the features vectors [...] may depend on the input image I. I am confused as to when they might be independent of the image. Can you elaborate on that? m5) In Sec3, it's unclear to me what the vertical bar in the Pot model stands for. I believe the correct formula should be 1_{[xi != xj]}. m6) In mean field inference, Algorithm 1 does not refer to FullCRFs. m7) End of page 6, \"Note that this gives FullCRFs a natural advantage. The performance of CRFs however is very robust [...]\". Why is this an advantage for FullCRFs? How does that relate to the following sentence? m8) Sec4.1, the authors claim that one of the key contribution of the paper is that exact message passing is efficient. Given the locality assumption, message passing is approximate - which is also why it's efficient. The authors could instead argue that using convolutions is faster and possibly leads to better final performance than using the permutohedral lattice approximation (although it's unclear whether this is the case from the experiments), with proper reference to compelling results in this direction. Finally, a few typos: * Abstract, space missing after GPUs * Introduction, Convolutional Neuronal -> Convolutional Neural * Introduction, order of magnitude slower then -> than * Introduction, to slow -> too slow * Parameter learning in CRF: missing space before proposed to use gradient descent * Parameter learning in CRF: gradient decent -> descent * Parameter learning in CRF: extra comma after \"another advantage of this method is\" * Sec 3: \"weighted sum of Gaussian kernels\", the apex of the second should be \"M\" I believe. * Sec 3: \"can be chosen arbitrary\" -> arbitrarily * Sec 5.2, beginning of page 8: then -> than", "rating": "4: Ok but not good enough - rejection", "reply_text": "I heavily disagree that my notation is confusion or incorrect . I am main using well established conventions and notation and I am quite thorough in defining open parameters and objects . Running indices in sums and matrices ( like $ i $ in $ sum_ { i } = i * i $ ) as well as explicit function arguments ( like $ x $ in $ f ( x ) = x * * 2 $ ) do not need to be further defined , since they are self contained in the equation . > > a ) Sec3 : in the text , a segmentation instance is referred to as X , while in the formula as \\hat x . \\tilde I is never introduced . No , $ X $ is a random variable ( as defined in the text ) and $ \\hat x $ its realization . This follows well established conventions from statistic and probability theory . The notation $ P ( X=\\hat x | \\tilde I = I ) = ... $ , is well understood across disciplines , $ \\hat x $ does not need to be further defined in this context . Analogously , since I define $ I $ in the text it is clear that $ \\tilde I $ is a random variable which produces images . There is no disambiguity here . Also $ \\tilde I $ is only used in this equation for the sake of completeness . > > b ) Sec4.1 , The shape of the Gaussian kernels is the same as that of the input . I believe that the input in this context refers to a patch and not to the whole image . If so , this should be specified , otherwise the dimensions of the kernel should be referred to with a different letter than those of the input . The input refers to the whole image . The shape of the Gaussian kernel is not discussed in the section . I think you mean the shape feature vectors $ f_1 ... f_d $ which define the kernel . Those feature vectors have the same shape as the input image ( apart from the channel dimension ) . The notation deliberately emphasizes this and the relation is correct . > > c ) Sec4.1 , I believe dx and dy refer to the in-kernel displacement . Their semantic is not clear from the text and should be defined properly . I heavily disagree with this statement . The variables $ dx $ , $ dy $ are running indices in a sum ( eq.6 ) or matrix definition ( eq.7 ) respectively . I have never seen a work where running indices are further defined in text since they are self-contained in the formula . It is true that traditionally most people would use $ i $ and $ j $ in this place . I choose $ dx $ , $ dy $ as names for the variables to give the equations more semantic and help the reader understand why the equation follows this particular formula . > > d ) Sec4.1 , the feature vectors are defined in the text as f_1 .. f_d . The formula of the kernel uses f_i^ { ( d ) } instead . It 's unclear what the superscripts stands for and whether it is actually useful or redundant . The superscript was supposed to emphasize that $ f_i $ is a vector . I do agree that this superscript might cause more harm then good , so it is removed . Vectors are now printed in bold instead . > > e ) Sec4.1 , x and y are not defined , I suspect they refer to the position of the pixel , which was previously encoded as p_i and p_j . Once again , the notation should be consistent across the manuscript . Again , $ x $ and $ y $ are running indices in a matrix definition . As such they are self contained . Also $ p_i $ and $ p_j $ are values of the smoothness kernel while $ x $ and $ y $ refer to coordinates in feature space . I think that it is useful to differentiate between these ."}, {"review_id": "H1xEwsR9FX-2", "review_text": "The paper is well written with many relevant references and easy to read. Some points that need clarification and mentioned below. The main points of this paper are the use of the convolution operator to perform the message passing mean field inference. Using this operation allows us to get away from the permutohedral lattice and yet allows speed up of 100x. This also means, that training will be able to done faster. Besides this the training parameters can also be learnt. These are the main contributions. The denoising task experiment shows positive results. The idea could be used in the future by others looking for faster model inference and training. If a Manhattan distance d is used i.e. dx,dy<k in equation (6), why is this a FullCRF? It seems like the new CRF is no longer a fully connected one. Page 5, first paragraph describing how the reorganization in the GPU is avoided is not very clear. It would be helpful to a reader to have more details and explanations about this. It is not clear from the experimental results how much improvement allowing to train the CRF parameters gets or might get. Comparing to the Deeplab results etc for the non-trained case, the non-trained model still seems to be performing competitively. Table 2 of Table 3 does not really bring out the advantage of training. The +C, +T, +CT don't seem to be hugely different in terms of validation metrics. Note that Table 3 does not mention other models that might not be trained (assuming that those results are in Table 2) but the text also mentions that the training is not completely fair. In section 5, Unary, it is mentioned that the network is not trained on larger datasets like other work, why? And under CRF, what does iterations are unrolled mean? In section 5.1, why does the random flipping help in simulating inaccuracies? Minor points: Abstract: Add space after \"GPUs.\". Would be good to define what Q, *, ' indicate in paragraph 4, page 2. \"hight\" -> \"height\" in section 4.1 ", "rating": "6: Marginally above acceptance threshold", "reply_text": "> > If a Manhattan distance d is used i.e.dx , dy < k in equation ( 6 ) , why is this a FullCRF ? It seems like the new CRF is no longer a fully connected one . Yes , this is why we call out method ConvolutionalCRF . FullCRF refers to `` Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials '' , Kr\u00e4henb\u00fchl & Vladlen Koltun ( 2012 ) . > > It is not clear from the experimental results how much improvement allowing to train the CRF parameters gets or might get . Comparing to the Deeplab results etc for the non-trained case , the non-trained model still seems to be performing competitively . Yes , the performance ( IoU ) of DeepLab CRF is very comparable to the performance of ConvCRF . This is not surprising as ConvCRF was designed to be as close as possible to FullCRF . The goal is to show that we achieve similar performance at a much larger speed . The same holds for the trainable potentials . The main goal is to show that our approach is able to train potentials . Trainable potentials open up a large design space for future research where our FullCRF based model can serve as an initial baseline . > > In section 5 , unary , it is mentioned that the network is not trained on larger datasets like other work , why ? The single digit performance gain by training on external datasets does not justify the large amount of additional GPU time required for those experiments . This also makes our experiments much easier to reproduce for individuals and people in academia . > > And under CRF , what does iterations are unrolled mean ? In our initial implementation we have implemented RNN like a five layer CNN which shares weights . This is not true in our newest ( published ) implementation , we have removed the statement . ( Both implementation behave the same , given same weights ) . > > In section 5.1 , why does the random flipping help in simulating inaccuracies ? With random flipping we meant that some predictions are randomly changed . We have changed to wording to `` random noise is added to the predictions '' as the word flipping might be misleading ."}], "0": {"review_id": "H1xEwsR9FX-0", "review_text": "+ well written + Good idea - Technical section not fully clear - Some experimental issues The paper is well written, and clearly explains the background material and concepts. It might almost be a bit too detailed, as the main technical section (4) feels a bit rushed. (more below). From what I can judge the main idea in the paper is sound. The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel. They show that inference is more efficient and training is easier. The technical section is not very clear. For example: Are the filter weights recomputed for each spatial location, is there any acceleration that speeds this up? How large can the authors make the filter kernel, before the perhutohedral lattice is faster again? Finally, the experimental section has some room for improvement. I liked the comparison of decoupled and coupled CRF training, but I didn't get much out of the synthetic experiments. I found it particularly confusing since Table 1 doesn't mention that the experiments use ground truth (test) labels that were corrupted. Second, it would be nice to have a side-to-side comparison between ConvCRF and CRFasRNN. I'd recommend the authors to either use the CRFasRNN training setup for both methods, or spend the week or two training CRFasRNN using their training procedure. It is fine to do either of the two experiments and have four entries in that table.", "rating": "7: Good paper, accept", "reply_text": "> > From what I can judge the main idea in the paper is sound . The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel . They show that inference is more efficient and training is easier . This is exactly what the paper is about . > > Are the filter weights recomputed for each spatial location , is there any acceleration that speeds this up ? Yes , the weights of the filter matrix $ k_g $ need to be computed for each spatial location , since they are based in features unique to each location . We implement this by representing $ k_g $ as one large 5-dimensional tensor ( $ k_g [ b , dx , dy , x , y ] $ ) . For a fixed $ dx $ and $ dy $ the partial tensor $ k_g [ : ,dx , dy , : , : ] $ can be computed using matrix operations on the feature vector $ f_i $ . Those matrix operations follow the single instruction , multiple data ( SIMD ) shema and thus can be computed highly efficiently on GPUs ( using any common deep learning framework ) . We do n't apply any additional optimization . > > How large can the authors make the filter kernel , before the perhutohedral lattice is faster again ? In section 5 , Table 1 we report the speed of ConvCRF with different filter sizes . The computational time increases quadratically w.r.t.the filter size . So we can estimate that with a filter size of about $ 50 $ both filtering methods have the same speed . Unfortunately we ca n't experimentally verify this , since our GPU memory can only handle filters up to $ 31 $ . This is however not a huge drawback , since we do n't observe any improvement in performance for filter-sizes larger than $ 11 $ . > > I liked the comparison of decoupled and coupled CRF training , but I did n't get much out of the synthetic experiments . I found it particularly confusing since Table 1 does n't mention that the experiments use ground truth ( test ) labels that were corrupted . The experimental setup for table 1 is discussed in section 5.1 . However we have also augmented the caption of the table 1 to make this point clearer ( all changes will be uploaded till Monday ) . The synthetic benchmark serves multiple purposes . Label completion and denoising are tasks in which CRFs have been traditionally strong in . Thus we provide a sound 'hello world ' task which can be solved by both CRFs without any training . This also helps us understand the structural differences between the models since optimization is not anymore part of the equation . > > I 'd recommend the authors to either use the CRFasRNN training setup for both methods , or spend the week or two training CRFasRNN using their training procedure . It is fine to do either of the two experiments and have four entries in that table . This is indeed a good idea . The experiments are currently running and will be finished till Monday . We are going to amend section 5.3 with those results ."}, "1": {"review_id": "H1xEwsR9FX-1", "review_text": "The authors propose an efficient method to perform message passing on a truncated Gaussian kernel CRF. The main contributions are the definition of a specific form of truncated Gaussian kernel that allows for fast message passing via convolutions, and the implementation of such parallelized message passing on GPU. In my opinion, the paper fails to convey the main idea in a clear and precise manner, the notation is mixed and often confusing, furthermore there are a number of sentences that should be rephrased to be less sensationalist, or removed. The experiments seem to show performance in par with the FullCRF on decoupled training, which seem in contrast with the much bigger performance gain of the first experiment on syntetic data. No discussion has been provided as to the possible reasons of this performance gap, although the experimental settings appear to be similar. Finally, in the last experiments with end-to-end training the authors report a performance improvement over CRFasRNN, a 3 years old paper that is far away in terms of performance with the current SOTA on Pascal VOC. The authors base on a different network than that of the CRFasRNN baseline (i.e., the difference is not only in the CRF implementation, but rather the whole network before the CRF in the proposed method), it is therefore difficult to say whether the performance improvement is due to the ResNet101 + FCN unary potentials, which is not a contribution of this manuscript, or to the proposed CRF. In general, I believe that the considerable speed gain of the proposed method might be enough to justify a publication, but the paper should be phrased in that sense if that was the intention of the authors. It is unclear to me whether the main contribution they claim is segmentation performance (IoU) or speed or both. The main contributions of this work should be stated clearly, and the modelling differences w.r.t. the FullCRF model that they aim to improve should be more explicit in the text rather than let to the reader to infer comparing the formulas. On these grounds, I suggest a major revision of the paper and I don't recommend publication at this stage. MAJOR 1) I firmly advocate against making strong claims, unless supported by solid proofs. I strongly recommend to rephrase, if not remove, exaggerate claims such as: a) \"[deep networks] lack the capability to utilize context information and cannot model interactions between predictions directly\". This is simply not true. Any CNN with enough layers will exploit contextual information. Furthermore, any autoregressive model will model the interaction between predictions directly. See e.g., \"RiFCN: Recurrent Network in Fully Convolutional Network for Semantic Segmentation of High Resolution Remote Sensing Images\" by Mou et Al., \"ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation\" by Visin et Al., or \"Predicting Deeper into the Future of Semantic Segmentation\" by Luc et Al. for video semantic segmentation. b) \"CRF inference is two orders of magnitude slower than CNN inference\": this, of course, depends on the kind of CRF. c) \"The long training times of the current generation of CRFs also make more in-depth research and experiments with such structured models impractical\": again, not true. While it's true that CRFs tend to be slow, research with such models is not impractical and indeed there are papers that focus exactly on that (among the others, some of the ones cited in this manuscript). d) \"we propose to add the strong and valid assumption of conditional independence\": as with every assumption, this is an approximation. I wouldn't claim it to be valid nor invalid, as it is simply a modeling choice. e) \"Predictions are pixel-wise and conditionally independendent (given the common feature base of nearby pixels). Structured knowledge and background context is ignored in these models.\". It's unclear what is meant by \"structured knowledge\", but to my best understanding this sentence is misleading or wrong. Background context is considered by CNN-based models, as well as the general structure in the image. f) \"In the context of semantic segmentation most CRF based approaches are based on the Fully Connected CRF\". CRFs have been used much earlier than 2011, before Fully Connected CRF was published. g) \"This makes the theoretical foundation of ConvCRF very promising, strong and valid assumptions are the powerhouse of machine learning modelling.\" The authors here propose a logical, but quite obvious, approximation, i.e., to constrain the CRF to model dependencies in a local neighborhood. This is the same implicit assumption of many other Gaussian kernels and algorithms for approximated inference. For how it's surely valid, and possibly strong, I don't see how it can make a \"theoretical foundation\". The self-congratulatory closure is unnecessary, and inappropriate. 2) While CRFs have been used for a long time on visual data, the citations in this work focus mostly on the last few years. I suggest to add at least one of the following: * \u201cDiscriminative fields for modeling spatial dependencies in natural images\" 2003 * \"Multiscale conditional random fields for image labelling\" 2004 * \u201cTextonboost: Joint appearance, shape and context modeling for mulit-class object recognition and segmentation,\u201d 2006 It could also be beneficial to the reader to add to the references broad overview works, such as \"An Introduction to Conditional Random Fields\" by Charles Sutton and Andrew McCallum, 2011, and/or \"Structured prediction and learning in computer vision\" by Nowozin and Lampert, 2011. 3) Line 5 of the algorithm adds the unary potentials at each iteration of message passing. Can you elaborate on the motivation behind this choice? The algorithm is already initialized with such potentials, and to my best knowledge unary potentials are not usually added in the mean field message passing loop. It would be interesting to compare the performance of the algorithm with and without this addition. 4) Sec4.1 reports that the filters are constant over the channel dimension c and that, in other words, this can be seen as applying the convolution over the dimension c. I fail to understand this sentence. My understanding from the formula is that the same kernel is applied to all the channels of the input, i.e., the channels of the input fed to the CRF are all processed in the same way. This should be explained clearly and the reasoning behind this choice should be explained as well. Furthermore, if I am not mistaken the authors learn a different filter in each position *of the input* rather than reusing the same filters at every position. This choice should be clarified and discussed. 5) Regarding the implementation, is there a reason not to apply a convolution with a flipped kernel to compute the cross-correlation? Also, IIRC if the kernel is symmetric (as should be for a Gaussian kernel) convolution and cross-correlation are the same. 6) The notation is often ambiguous and at times unnecessarely heavy. I strongly recommend to go over the manuscript and use a consistent notation, making sure that every element of the notation is introduced before or right after it's used. In particular, a) Sec3: in the text, a segmentation instance is referred to as X, while in the formula as \\hat x. \\tilde I is never introduced. k_{\\alpha} is defined but I believe never used (I suggest to drop the name if there is no reference to it). Is there a reason to drop the subscript G in the FullCRF pairwise potential? Or conversely, is there a reason to have it everywhere else? Furthermore, there doesn't seem to be a difference between k_G, k_g and g, I suggest to use only one consistent notation to refer to the Gaussian kernels throughout the paper. It's also unclear if the I superscript is needed for the feature vectors. b) Sec4.1, The shape of the Gaussian kernels is the same as that of the input. I believe that the input in this context refers to a patch and not to the whole image. If so, this should be specified, otherwise the dimensions of the kernel should be referred to with a different letter than those of the input. c) Sec4.1, I believe dx and dy refer to the in-kernel displacement. Their semantic is not clear from the text and should be defined properly. d) Sec4.1, the feature vectors are defined in the text as f_1..f_d. The formula of the kernel uses f_i^{(d)} instead. It's unclear what the superscripts stands for and whether it is actually useful or redundant. e) Sec4.1, x and y are not defined, I suspect they refer to the position of the pixel, which was previously encoded as p_i and p_j. Once again, the notation should be consistent across the manuscript. f) In the definition of the convCRFs, w is used for the width of the input, w_i for the weights of the kernels. In the FullCRF, w^{(1)} and w^{(2)} for the potentials, w^{(m)} for the sum over the kernels. k is used for the kernel dimension, k_G to refer to the kernel itself, as well as g. The notation could be made less ambiguous and consistent (superscript vs subscript semantics). g) Sec3, the number of pixel is defined as n but in Formula 2 N is used instead. g) Vectors and matrices should be bold-face. The use of capital letters for constants might also improve the readability of the manuscript. 7) The experiments with the Conv (ConvCRF?) variants of Table 2 are not discussed in the text. 8) Although Sec5.2 concludes with \"The experiments also confirm the observation of Sec5.1, that ConvCRF performs slightly better than FullCRF\", Sec5.1 reported that \"it can be seen that ConvCRFs outperform FullCRFs significantly\". The authors should decide whether the results are slightly better or outperform the baseline. In general a in-depth discussion on the performance of the algorithm is missing. 9) Sec5.3, it's unclear what this sentence means \"introduce an auxiliary unary loss to counterbalance the vanishing gradient problem\". If such a term has been added, it should be reported in a formula and it's effectiveness should be supported by experimental data. MINOR m1) In the related work, the sentence \"transposed convolution layers are applied at the end of the prediction pipeline ot produce high-resolution output\" seems to suggest these are always applied, while many recent methods rely on bilinear upsampling to recover the original resolution. Please rephrase it accordingly. m2) In Parameter learning in CRF: \"the idea utilizes, that for the message passing the identity .. is valid.\" This sentence doesn't make any sense to me. Is it possible it is a leftover? m4) In Sec3, the features vectors [...] may depend on the input image I. I am confused as to when they might be independent of the image. Can you elaborate on that? m5) In Sec3, it's unclear to me what the vertical bar in the Pot model stands for. I believe the correct formula should be 1_{[xi != xj]}. m6) In mean field inference, Algorithm 1 does not refer to FullCRFs. m7) End of page 6, \"Note that this gives FullCRFs a natural advantage. The performance of CRFs however is very robust [...]\". Why is this an advantage for FullCRFs? How does that relate to the following sentence? m8) Sec4.1, the authors claim that one of the key contribution of the paper is that exact message passing is efficient. Given the locality assumption, message passing is approximate - which is also why it's efficient. The authors could instead argue that using convolutions is faster and possibly leads to better final performance than using the permutohedral lattice approximation (although it's unclear whether this is the case from the experiments), with proper reference to compelling results in this direction. Finally, a few typos: * Abstract, space missing after GPUs * Introduction, Convolutional Neuronal -> Convolutional Neural * Introduction, order of magnitude slower then -> than * Introduction, to slow -> too slow * Parameter learning in CRF: missing space before proposed to use gradient descent * Parameter learning in CRF: gradient decent -> descent * Parameter learning in CRF: extra comma after \"another advantage of this method is\" * Sec 3: \"weighted sum of Gaussian kernels\", the apex of the second should be \"M\" I believe. * Sec 3: \"can be chosen arbitrary\" -> arbitrarily * Sec 5.2, beginning of page 8: then -> than", "rating": "4: Ok but not good enough - rejection", "reply_text": "I heavily disagree that my notation is confusion or incorrect . I am main using well established conventions and notation and I am quite thorough in defining open parameters and objects . Running indices in sums and matrices ( like $ i $ in $ sum_ { i } = i * i $ ) as well as explicit function arguments ( like $ x $ in $ f ( x ) = x * * 2 $ ) do not need to be further defined , since they are self contained in the equation . > > a ) Sec3 : in the text , a segmentation instance is referred to as X , while in the formula as \\hat x . \\tilde I is never introduced . No , $ X $ is a random variable ( as defined in the text ) and $ \\hat x $ its realization . This follows well established conventions from statistic and probability theory . The notation $ P ( X=\\hat x | \\tilde I = I ) = ... $ , is well understood across disciplines , $ \\hat x $ does not need to be further defined in this context . Analogously , since I define $ I $ in the text it is clear that $ \\tilde I $ is a random variable which produces images . There is no disambiguity here . Also $ \\tilde I $ is only used in this equation for the sake of completeness . > > b ) Sec4.1 , The shape of the Gaussian kernels is the same as that of the input . I believe that the input in this context refers to a patch and not to the whole image . If so , this should be specified , otherwise the dimensions of the kernel should be referred to with a different letter than those of the input . The input refers to the whole image . The shape of the Gaussian kernel is not discussed in the section . I think you mean the shape feature vectors $ f_1 ... f_d $ which define the kernel . Those feature vectors have the same shape as the input image ( apart from the channel dimension ) . The notation deliberately emphasizes this and the relation is correct . > > c ) Sec4.1 , I believe dx and dy refer to the in-kernel displacement . Their semantic is not clear from the text and should be defined properly . I heavily disagree with this statement . The variables $ dx $ , $ dy $ are running indices in a sum ( eq.6 ) or matrix definition ( eq.7 ) respectively . I have never seen a work where running indices are further defined in text since they are self-contained in the formula . It is true that traditionally most people would use $ i $ and $ j $ in this place . I choose $ dx $ , $ dy $ as names for the variables to give the equations more semantic and help the reader understand why the equation follows this particular formula . > > d ) Sec4.1 , the feature vectors are defined in the text as f_1 .. f_d . The formula of the kernel uses f_i^ { ( d ) } instead . It 's unclear what the superscripts stands for and whether it is actually useful or redundant . The superscript was supposed to emphasize that $ f_i $ is a vector . I do agree that this superscript might cause more harm then good , so it is removed . Vectors are now printed in bold instead . > > e ) Sec4.1 , x and y are not defined , I suspect they refer to the position of the pixel , which was previously encoded as p_i and p_j . Once again , the notation should be consistent across the manuscript . Again , $ x $ and $ y $ are running indices in a matrix definition . As such they are self contained . Also $ p_i $ and $ p_j $ are values of the smoothness kernel while $ x $ and $ y $ refer to coordinates in feature space . I think that it is useful to differentiate between these ."}, "2": {"review_id": "H1xEwsR9FX-2", "review_text": "The paper is well written with many relevant references and easy to read. Some points that need clarification and mentioned below. The main points of this paper are the use of the convolution operator to perform the message passing mean field inference. Using this operation allows us to get away from the permutohedral lattice and yet allows speed up of 100x. This also means, that training will be able to done faster. Besides this the training parameters can also be learnt. These are the main contributions. The denoising task experiment shows positive results. The idea could be used in the future by others looking for faster model inference and training. If a Manhattan distance d is used i.e. dx,dy<k in equation (6), why is this a FullCRF? It seems like the new CRF is no longer a fully connected one. Page 5, first paragraph describing how the reorganization in the GPU is avoided is not very clear. It would be helpful to a reader to have more details and explanations about this. It is not clear from the experimental results how much improvement allowing to train the CRF parameters gets or might get. Comparing to the Deeplab results etc for the non-trained case, the non-trained model still seems to be performing competitively. Table 2 of Table 3 does not really bring out the advantage of training. The +C, +T, +CT don't seem to be hugely different in terms of validation metrics. Note that Table 3 does not mention other models that might not be trained (assuming that those results are in Table 2) but the text also mentions that the training is not completely fair. In section 5, Unary, it is mentioned that the network is not trained on larger datasets like other work, why? And under CRF, what does iterations are unrolled mean? In section 5.1, why does the random flipping help in simulating inaccuracies? Minor points: Abstract: Add space after \"GPUs.\". Would be good to define what Q, *, ' indicate in paragraph 4, page 2. \"hight\" -> \"height\" in section 4.1 ", "rating": "6: Marginally above acceptance threshold", "reply_text": "> > If a Manhattan distance d is used i.e.dx , dy < k in equation ( 6 ) , why is this a FullCRF ? It seems like the new CRF is no longer a fully connected one . Yes , this is why we call out method ConvolutionalCRF . FullCRF refers to `` Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials '' , Kr\u00e4henb\u00fchl & Vladlen Koltun ( 2012 ) . > > It is not clear from the experimental results how much improvement allowing to train the CRF parameters gets or might get . Comparing to the Deeplab results etc for the non-trained case , the non-trained model still seems to be performing competitively . Yes , the performance ( IoU ) of DeepLab CRF is very comparable to the performance of ConvCRF . This is not surprising as ConvCRF was designed to be as close as possible to FullCRF . The goal is to show that we achieve similar performance at a much larger speed . The same holds for the trainable potentials . The main goal is to show that our approach is able to train potentials . Trainable potentials open up a large design space for future research where our FullCRF based model can serve as an initial baseline . > > In section 5 , unary , it is mentioned that the network is not trained on larger datasets like other work , why ? The single digit performance gain by training on external datasets does not justify the large amount of additional GPU time required for those experiments . This also makes our experiments much easier to reproduce for individuals and people in academia . > > And under CRF , what does iterations are unrolled mean ? In our initial implementation we have implemented RNN like a five layer CNN which shares weights . This is not true in our newest ( published ) implementation , we have removed the statement . ( Both implementation behave the same , given same weights ) . > > In section 5.1 , why does the random flipping help in simulating inaccuracies ? With random flipping we meant that some predictions are randomly changed . We have changed to wording to `` random noise is added to the predictions '' as the word flipping might be misleading ."}}