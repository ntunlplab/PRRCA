{"year": "2021", "forum": "N5Zacze7uru", "title": "Neural Lyapunov Model Predictive Control", "decision": "Reject", "meta_review": "The authors propose an MPC based approach for learning to control systems with continuous state and actions - the dynamics, control policy and a Lyapunov function are parameterized as neural networks and the authors claim to derive stability certificates based on the Lyapunov function.\n\nThe reviewers raised several serious technical issues with the paper as well as the lack of clarity in the presentation of the main technique in the initial version of the paper. While the clarity concerns were partially addressed during the rebuttal, the technical concerns (in particular those raised by reviewer 1) remain unaddressed - the stability certificate derived is questionable due to the fact that sampling based approaches to certifying that a function is a valid Lyapunov function are insufficient to derive any stability guarantee. Further, the experimental results are only demonstrated on relatively simple dynamical systems. Hence I recommend rejection. \n\nHowever, all reviewers agree that the ideas presented in the paper are potentially interesting - I would suggest that the authors consider revising the paper to address the feedback on technical issues and submit to a future venue.\n\n", "reviews": [{"review_id": "N5Zacze7uru-0", "review_text": "This paper proposes an MPC algorithm based on a learned ( neural network ) Lyapunov function . In particular , they learn both the Lyapunov function and the forward model of the dynamics , and then control the system using an MPC with respect to these models . Cons - Poorly written - Unclear connections to related work - Weak experiments It is unclear exactly what problem the authors are attempting to solve . In general , the authors introduce a large amount of notation and theory , but very little of it appears to be directly related to their algorithm . For example , they refer to the stability guarantees afforded by Lyapunov functions , but as far as I can tell , they never prove that their algorithm actually learns a Lyapunov function ( indeed , Lemma 1 starts with \u201c Assume that V ( x ) satisfies ( 5 ) [ the Lyapunov condition ] ... \u201d ) . Similarly , they allude to \u201c robustness margins to model errors \u201d , but nothing in the algorithm actually takes into account model errors . Is the point of these margins just to show that they exist ? If so , it \u2019 s not clear the results ( either theoretical or empirical ) are very meaningful , given that they depend on the unknown model error ( which they assume to be bounded ) . In addition , the different loss functions they use ( e.g. , ( 10 ) ) are poorly justified . Why is this loss the right one to use to learn a Lyapunov function ? Furthermore , the authors \u2019 approach is closely related to learning the value function and planning over some horizon using the value function as the terminal cost ( indeed , the value function is a valid Lyapunov function , but not necessarily vice versa ) . For instance ; Buckman et al. , Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion . In NeurIPS , 2018 . The most closely related work I \u2019 m aware of is the following : Deits et al. , Lvis : Learning from value function intervals for contact-aware robot controllers . In ICRA , 2019 . The authors should clarify their contributions with respect to these papers . More importantly , the authors should discuss their motivation for indirectly learning a Lyapunov function instead of simply learning the value function ( which appears to be more natural and potentially more effective ) . Next , the authors \u2019 experiments are very weak . They only consider two environments , the inverted pendulum and car , both of which are very simple . The inverted pendulum starts near the unstable equilibrium , which further trivializes the problem . In addition , they do not even appear to give the dynamics model of the car they are using ( or the state space ) . Finally , this paper is poorly written and hard to follow . They provide a lot of definitions and equations without sufficient explanation or justification , and introduce a lot of terminology without giving sufficient background . Post rebuttal : While I appreciate the authors ' comments , they do not fundamentally address my concerns that the paper is too unclear in terms of the meaning of its technical results to merit acceptance . As a concrete example , in their clarification , the authors indicate that they obtain `` probabilistic safety guarantees '' by checking the Lyapunov condition ( 5 ) using sampling . However , at best , sampling can ensure that the function is `` approximately '' Lypaunov ( e.g. , using PAC guarantees ) -- i.e. , satisfies ( 5 ) on all but 1-\\epsilon of the state space . Unfortunately , an `` approximately '' Lyapunov function ( i.e. , satisfies the Lyapunov condition ( 5 ) on 1-\\epsilon of the state space ) provides * zero * safety guarantees ( not even probabilistic safety at any confidence level ) . Intuitively , at each step , the system has a 1-\\epsilon chance of exiting a given level set of the Lyapunov function . These errors compound as time progresses ; after time horizon T , only 1 - T * \\epsilon of the state space is guaranteed to remain in the level set , so eventually the safety guarantee is entirely void . One way to remedy this is if the Lyapunov function is Lipschitz continuous . However , then , the number of samples required would still be exponential in the dimension of the state space . At this point , existing formal methods tools for verifying Lyapunov functions would perform just as well if not better , e.g. , see : Soonho Kong , Sicun Gao , Wei Chen , and Edmund Clarke . dReach : \u03b4-Reachability Analysis for Hybrid Systems . 2015.This approach was recently applied to synthesizing NN Lyapunov functions ( Chang et al.2019 ) .My point is n't that the authors ' approach is invalid , but that given the current writing it is impossible for me to understand the theoretical properties of their approach . Overall , I think the paper may have some interesting ideas , but I can not support publishing it in its current state", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for their feedback and hope that we address the concerns raised in our reply . Our work addresses the problem of designing a controller to maximize the stability of the closed-loop system . The objective in this work is to combine learning from data and control theory to train a controller that has provable safety certificates in terms of Lyapunov functions . Regarding clarity , the paper borrows terms and definitions from control theory which are instrumental to motivate the proposed algorithm . While it is not practical to write explanations to all the standard results in control systems and Lyapunov stability , we have tried our best to elucidate the relevant background in Section 2 . For more information on Lyapunov-based control methods and certification , we would like to refer the reviewer to Bobiti ( 2017 ) and Limon ( 2003 , 2009 ) which we have also cited in the paper . We are open to further suggestions by the reviewer on how to improve the readability of the work . Please find our comments on the specific questions/concerns that have been raised : # # # # 1 . Regarding the problem statement Restating the approach rationale specified on page 2 , our work aims to match or enlarge the safe region of an unknown controller . We aim to design a stabilizing controller using data collected ( one-step demonstrations ) from an unknown controller ( demonstrator ) such that the largest possible stability region is obtained . In an ideal case , the region obtained by the new controller should match or extend the one from the original unknown one ( demonstrator ) . However , due to function approximation , this may not always be possible . In this work , we offer a framework to produce a verifiably safe new controller while at the same time attempting to match the size of the stable region of the demonstrator as much as possible . # # # # 2.Regarding learning of the Lyapunov function in the algorithm As specified in Section 3.1 , we design the Lyapunov neural network ( NN ) such that it is Lipchitz and satisfies the Lyapunov conditions ( 4 ) . This is described in more detail in the work from Richards et.al ( 2018 ) . Provided that condition ( 5 ) is verified , the stability guarantees provided in Theorem 1 ( formerly called Lemma 1 ) are that for an MPC which uses the Lyapunov NN function as its terminal cost . Proving that a NN trained via SGD is \u201c exact \u201d at inference time or bounding its test error during training are open areas of research . As stated on page 3 , in our work , we use a-posteriori sampling-based verification of the neural Lyapunov function to verify ( 5 ) at each stage of our alternate learning algorithm . This provides a high probability certificate that the network is a Lyapunov function according to the conditions ( 4 ) and ( 5 ) , as described in Bobiti ( 2017 ) . Due to page constraints , a detailed description of the algorithm used for verification is specified in Appendix E. # # # # 3 . Regarding robustness margins to model errors We thank the reviewer for bringing this to our notice . In the appendix , we have shown a bound for the maximum model error for which the closed-loop system can retain a given Input-to-State Stability ( ISS ) , i.e.robustness to additive errors . Due to space limitations , we had not included this bound as part of the theorem in the main paper . However , we recognize that it would be beneficial to state this result and will do so in the updated version of the paper . The bound on the maximum model error for ISS tells us that , for a stronger contraction factor and a greater safe set size , we can proportionally tolerate more uncertainty on the model . However , the effect on the stability performance of the controller is highlighted in Theorem 2 ( formerly Lemma 2 ) , where we discuss how the model error affects the final radius of convergence of the cost ( and implicitly the control error ) . The bound presented there shows that it is beneficial to decrease the horizon length if the uncertainty in the model is large and the system to be controlled has large Lipschitz constants . In our experiments , we use this result in designing the controller by reducing its horizon length to one and still showing that ensures stability ."}, {"review_id": "N5Zacze7uru-1", "review_text": "This paper addresses the question of how to stabilize a system in a vicinity of an equilibrium . While the majority of reinforcement learning algorithms rely on trial and error , which may damage the system , the authors introduce an algorithm for safe exploration and control . A traditional approach in model-based RL is to use MPC with a surrogate forward model to minimize a planning objective comprising a sum of stage costs along with a terminal cost , often chosen as an approximated value function -i.e.the optimal expected cost-to-go- which can be learned by a Bellman equation . Instead , this work is placed in the framework of Robust MPC , where this value function is replaced by a Luyapunov function $ V $ , which is related to the notion of stability and is only constrained to decrease along trajectories . Such a Luyapunov function , when available , provides both a safe region , defined as a level-set of V , and a MPC policy for which stability analyses have been developed : the authors extend a result from Limon et al . ( 2003 ; 2009 ) to show that this MPC policy enjoys asymptotic stability in general , and input-to-state stability in the presence of small enough model errors . Accordingly , the authors propose a scheme allowing to learn a Lyapunov function $ V $ from demonstration data only , through a loss function that penalizes increments of $ V $ along one-step transitions . A regularization parameter $ \\alpha $ of this MPC , which balances stability with constraints satisfaction and stage costs , is also learned jointly by an alternative training procedure . This approach is evaluated empirically on two standard constrained non-linear continuous control tasks . Strong points : 1 . This paper is clearly written , well motivated , honest about its place in the literature , and all derivations seem technically correct . 2.By bringing together existing results and techniques ( MPC stability analyses , value-based RL analyses , LuyapunovNet ) , the authors manage to relax several assumptions of prior works ( no need for access to the perfect dynamics or a stabilizing policy , but only to a demonstration dataset ) which makes the approach more practical . 3.Empirically , the learned Luyapunov functions seem to effectively capture useful stability information , since the proposed approach outperforms a standard MPC with a longer planning horizon . Even better , this observation is theoretically justified by Lemma 2 : errors of the surrogate model compound when used in an MPC , which is detrimental for long-term planning . Conversely , if $ V $ contains long-term information , it can directly be used for short-sighted planning , similarly to being greedy with respect to a value function . Weak points : 1 . The part which was the least clear to me is the * Performance with surrogate models * paragraph , with Lemma 2 . The authors draw a parallel between Luyapunov functions in control theory and value functions in RL , but the latter are not really defined clearly in the text . The authors state in their introduction that they treat `` the learned Lyapunov NN as an estimate of the value function '' and later they mention a `` correct '' value function $ V^ * $ for optimal `` expected infinite-horizon performance '' , but this quantity is nowhere defined . I suppose $ V^ * $ is an expected infinite sum of discounted stage costs , but which costs ? The same as in equation ( 3 ) ? If so , I find it hard to believe that the assumption of Lemma 2 should be satisfied ( $ V^ * $ close to $ \\alpha V $ ) , given that the stage cost $ l $ of ( 3 ) used to define $ V^ * $ does not appear in the loss ( 10 ) used to learn the Lyapunov function $ V $ . 2.I find it difficult to assess the novelty of the theoretical results . The authors are honest in stating that they are extending/adapting known results , but it is not precisely stated what their added value is . Moreover , the abstract mentions that `` we also present theorems '' but in the article these results are presented as lemmas , which to me usually suggests that they are either instrumental to another result ( which they are not ) , or of minor importance . 3.One of the main claim of the paper is the ability of the method to expand a demonstrated safe region . However , this is not really observed consistently across tasks and iterations . For instance , it is not the case in Figure 3 . Likewise , the Table 1 states that `` With iterations , the number of points verified by the controller increases '' , but only a two iterations is provided ( i.e.a single opportunity to increase/decrease ) . This seems a bit suspicious and suggests that the ratio of verified points may actually decrease on subsequent iterations , as it does on iteration 3 of Table 3 . To conclude , I lean toward recommending acceptance , but I am ready to increase my score provided that the authors improve the clarity on both the analogy between Lyapunov and value functions , and state more clearly the novelty of their theoretical contributions . Minor remarks and typos : * I do not really see the relevance of the safety performance metric in the Inverted Pendulum experiment ( Fig.3 ) : the state constraints are loose enough that every successful trajectory is also considered safe , so this safety plot ( right ) does not really bring any new information to the table . * p2 , Equation ( 5 ) : X_s\\ * * X_T * * * p3 , Learning and Safety Ver * * i * * fication * p6 , to minimize the loss defined in * * ( 11 ) * *", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for thoroughly reading the paper and the positive and constructive feedback . We particularly appreciate that our work was considered \u201c clear \u201d , \u201c well-motivated \u201d , \u201c honest \u201d , \u201c technically correct \u201d and its relevance was recognized . We have tried to address all the concerns by the reviewer in the following but are open to further discussions . We will fix the typos and make the required additions to the revised upload of the paper . We hope we convince the reviewer to possibly further improve their score . Please find our comments on the specific questions/concerns that have been raised : # # # # 1 . Regarding performance with surrogate models Yes , by using the Lyapunov function as the terminal cost , we want to approximate the tail of an infinite-horizon control formulation that uses the stage cost defined in ( 3 ) . We do not expect the error to be small with respect to the correct value function . The error can , however , be upper-bounded since all considered functions , the system dynamics , are Lipschitz and the constraints sets are bounded . Lemma 2 aims to break down the different components that affect performance and communicate the role of horizon length in the presence of model uncertainty . It would be possible in principle to combine the approach with value estimation to reduce the error but this is beyond the scope of the paper . The reason for not using the stage cost directly is that the unknown controller might not be optimal with respect to this stage cost ( reward ) but it is generally an engineered working solution . This choice was highly beneficial for instance in the car scenario , where our initial long horizon MPC used to generate the data produced suboptimal solutions due to the optimizer tolerances , correct decrease rate can not be guaranteed , differently from an LQR . Moreover , the long horizon MPC trajectories converge only to a neighborhood of the origin for the car . Despite this , our algorithm learns a Lyapunov function that can provide the system to even outperform the initial MPC , as kindly pointed out by the reviewer , in terms of convergence to a smaller neighborhood . # # # # 2.Regarding the novelty of the theoretical results In the revised paper , we will rectify calling the theoretical results as `` lemmas '' instead of `` theorems '' . We mainly extend existing results in Theorem 1 by considering the discount factor in the formulation and the contraction factor in the Lyapunov inequality instead of the loss . For the rest of Theorem 1 , we build and proceed upon the same steps of the cited articles from Limon et al . ( 2003 , 2009 ) . As also remarked by the reviewer , we state the same on page 4 in the paragraph on \u201c stability and safety \u201d . We did not mention the contraction instead of the loss but we will make this clearer in the revised upload . Overall , Theorem 1 aims to motivate our proposed algorithm , which is our main contribution in this work . We thought it was important , however , to state the theorem to mark the relevance of learning a Lyapunov function instead of just a value function surrogate . In Theorem 2 , we instead aim to close the gap with Lowrey et al . ( 2018 ) in terms of performance evaluation . This is also useful to understand the contribution of the horizon length and model error . With a perfect model , a longer horizon is indeed beneficial as claimed by Lowrey et al . ( 2018 ) , however , this is true only for a perfect model . In this case , it is also better in terms of the size of the stable region of the MPC . However , having a Lyapunov function that induces a larger safe set allows one to employ a shorter horizon and retain the same stable region with a shorter horizon . We reinforce the statement by demonstrating that , with function approximation in the model , a shorter horizon can become much more beneficial also in terms of optimality . The bound in Theorem 2 can be used to estimate the effect on convergence by model error and horizon length . We will clarify this further in the revised upload ."}, {"review_id": "N5Zacze7uru-2", "review_text": "In this paper the author proposed an MPC algorithm in which both the dynamics function and the Lyapunov function are parameterized with neural networks .. Specifically leveraging the results of Lyapunov networks ( 2018 CORL paper : https : //arxiv.org/abs/1808.00924 ) for learning Lyapunov functions , the authors derived an MPC algorithm for quadratic cost/reward problems and also proved the stability , robustness , and sub-optimality performance . To demonstrate the effectiveness of the algorithms , the authors also evaluated this approach on the simple inverted pendulum and car kinematics tasks . In general I find this paper presents a comprehensive results of a model-based control method that is very popular in the control theory community . To justify their algorithms they also proved several standard properties ( stability , sub-optimality performance ) in control , which I appreciate their efforts . However , I do have severals questions/concerns regarding the details of their approach : 1 ) The presentation of the loss function of Lyapunov network is not easy to parse , especially there are couple terms that contain specific mathematical operators ( sign , ReLU ) . Can the authors explain each term in the loss and why such choices of loss terms are necessary . Is this loss function identical to the Lyapunov network 2018 CORL paper ? 2 ) From the main paper it is unclear how the NN-dynamics model \\hat f is learned . Does it just train based on prediction loss ? More importantly , while the MPC algorithm uses the learned model how does the dynamics model error affect the stability/robustness/performance bounds of the control algorithm ? I can not immediately find this information in lemma 1 and lemma 2 , which makes me worried about the correctness of these results . ( Unfortunately I have n't had a chance to check the appendix for proofs ) 3 ) Having sub-optimality performance for MPC algorithms is a nice result , as not many MPC algorithms have performance guarantees . However these kind of results are also not new ( for example , see https : //ieeexplore.ieee.org/document/4639448 ) . How does the MPC performance result here compared with the ones by Grune and Rantzer ? 4 ) Among various safe MPC papers , how does the proposed one in this paper compared with this safe MPC algorithm : https : //arxiv.org/pdf/1803.08287.pdf , which is also proposed by Andreas Krause 's group ( that proposed the Lyapunov network ) ? At least experimentally how does the proposed algorithm compare with other safe MPC baselines ( such as the one above ) on the standard benchmark tasks ( for example the above work also tested the algorithms on the pendulum task ) . On the overall , I find this paper 's algorithm interesting . However , there are several technical question listed above , and one high-level concern is its novelty . Without further discussions , it appears to me that the work combines several existing results on Lyapunov network and MPC , for which the contribution is rather incremental .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their constructive feedback and are pleased to hear that our proposed algorithm was found interesting . We address the raised points in the following reply and will make the necessary changes in the uploaded version as well . We hope these answer the remarks by the reviewer and convince them to improve the score . We are of course open to further iterations and amendments to the paper . Please find our comments on the specific questions/concerns that have been raised : # # # # 1 . Regarding the novelty of the work As mentioned in the paper , we extend the theoretical results from Limon et al . ( 2003 ) to the discounted setting and the use of lambda-constractive Lyapunov functions ; those from Lowrey et al . ( 2018 ) are extended to the uncertain model setup . The theorems presented in our paper aim to motivate the proposed algorithm which we believe has never been presented in the literature . # # # # 2.Regarding loss function for learning Lyapunov NN : The loss proposed in our paper is similar to the one by Richards et al . ( 2018 ) .The first part encourages the function to decrease over trajectories from within the estimated safe set . The second part is aimed at estimating the function level that defines the safe set . The main difference in this loss function from that by Richards et al . ( 2018 ) is in this second part . Instead of using stability labels obtained by performing a forward propagation of T steps and verifying convergence , our algorithm only performs a unitary time-step forward propagation . This is mentioned at the beginning of page 6 in Section 3.3 , however , we will reinforce the statement in the new upload . We use the Lyapunov network itself to generate pseudo-labels ( stability certificate via $ sign ( \\Delta V ) $ ) . This is more data-efficient as we don \u2019 t need to have all trajectories reach completion , and is less prone to error accumulation than using a long-horizon simulation with an imperfect surrogate model ( Richards et al . ( 2018 ) use the true dynamics model for forward propagation ) . Besides that , in the training procedure , we verify the Lyapunov network on a validation dataset and perform cross-validation based on the number of verified and not verified points . We also perform a posteriori formal verification ( the algorithm for this is presented in Appendix E ) . We will make these points clearer in the updated version . Notation-wise : $ ReLU ( x ) =max ( x,0 ) $ and $ sign ( \\cdot ) $ is the sign of the quantity which returns either 1 or -1 . We will clarify this in the paper as well . # # # # 3.Regarding surrogate dynamics model : We do not require the surrogate dynamics model to be trained in a particular way . Further , it is not essential that the model is parametrized by a neural network , as long as its function class has Lipschitz continuity . Since all the sets are bounded , the model \u2019 s error bound can be inferred based on the Lipschitz constants ( as discussed on page 3 ) . However , in our algorithm , we assume that the learned dynamics model is given and its error bound is known . We will add a worst-case one-step error bound for ISS of the controller in the revised version of Theorem 1 ( formerly Lemma 1 ) . This margin was previously stated in the proof of the theorem ( in Appendix A ) , however , we will move it to the main paper . Due to page constraints , we specified the details about the system models and the training of the dynamics model in Appendix D , while providing the information about the setup in the main paper ( Section 4 ) . The NN-dynamics surrogate model $ \\hat { f } $ is trained using transition tuples ( this is stated above eq . ( 8 ) on page 3 ) . The data to train the model is collected using a random policy as typically done in system identification . This is mentioned on pages 6-7 for both experiments . # # # # 4.Regarding related work on MPC suboptimality : We thank the reviewer for pointing us to this important paper . We cited a different work from these authors on the stability of optimal control with discount factors ( Gaitsgory et al. , 2015 ) which is closely related to our discounted setup . The work by Gruene and Rantzer ( 2008 ) , however , is indeed also related and offers a key set of results for MPC suboptimality under the assumption that the loss satisfies an exponential controllability condition . In our paper , we consider a discounted setting which is more common in RL than in controls ( because of stability limitations as we highlight in Theorem 1 ) . Our results are built upon those from Lowrey et al . ( 2018 ) .However , we will add the missing reference ( Gruene and Rantzer , 2008 ) to our related work section in the revised upload ."}], "0": {"review_id": "N5Zacze7uru-0", "review_text": "This paper proposes an MPC algorithm based on a learned ( neural network ) Lyapunov function . In particular , they learn both the Lyapunov function and the forward model of the dynamics , and then control the system using an MPC with respect to these models . Cons - Poorly written - Unclear connections to related work - Weak experiments It is unclear exactly what problem the authors are attempting to solve . In general , the authors introduce a large amount of notation and theory , but very little of it appears to be directly related to their algorithm . For example , they refer to the stability guarantees afforded by Lyapunov functions , but as far as I can tell , they never prove that their algorithm actually learns a Lyapunov function ( indeed , Lemma 1 starts with \u201c Assume that V ( x ) satisfies ( 5 ) [ the Lyapunov condition ] ... \u201d ) . Similarly , they allude to \u201c robustness margins to model errors \u201d , but nothing in the algorithm actually takes into account model errors . Is the point of these margins just to show that they exist ? If so , it \u2019 s not clear the results ( either theoretical or empirical ) are very meaningful , given that they depend on the unknown model error ( which they assume to be bounded ) . In addition , the different loss functions they use ( e.g. , ( 10 ) ) are poorly justified . Why is this loss the right one to use to learn a Lyapunov function ? Furthermore , the authors \u2019 approach is closely related to learning the value function and planning over some horizon using the value function as the terminal cost ( indeed , the value function is a valid Lyapunov function , but not necessarily vice versa ) . For instance ; Buckman et al. , Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion . In NeurIPS , 2018 . The most closely related work I \u2019 m aware of is the following : Deits et al. , Lvis : Learning from value function intervals for contact-aware robot controllers . In ICRA , 2019 . The authors should clarify their contributions with respect to these papers . More importantly , the authors should discuss their motivation for indirectly learning a Lyapunov function instead of simply learning the value function ( which appears to be more natural and potentially more effective ) . Next , the authors \u2019 experiments are very weak . They only consider two environments , the inverted pendulum and car , both of which are very simple . The inverted pendulum starts near the unstable equilibrium , which further trivializes the problem . In addition , they do not even appear to give the dynamics model of the car they are using ( or the state space ) . Finally , this paper is poorly written and hard to follow . They provide a lot of definitions and equations without sufficient explanation or justification , and introduce a lot of terminology without giving sufficient background . Post rebuttal : While I appreciate the authors ' comments , they do not fundamentally address my concerns that the paper is too unclear in terms of the meaning of its technical results to merit acceptance . As a concrete example , in their clarification , the authors indicate that they obtain `` probabilistic safety guarantees '' by checking the Lyapunov condition ( 5 ) using sampling . However , at best , sampling can ensure that the function is `` approximately '' Lypaunov ( e.g. , using PAC guarantees ) -- i.e. , satisfies ( 5 ) on all but 1-\\epsilon of the state space . Unfortunately , an `` approximately '' Lyapunov function ( i.e. , satisfies the Lyapunov condition ( 5 ) on 1-\\epsilon of the state space ) provides * zero * safety guarantees ( not even probabilistic safety at any confidence level ) . Intuitively , at each step , the system has a 1-\\epsilon chance of exiting a given level set of the Lyapunov function . These errors compound as time progresses ; after time horizon T , only 1 - T * \\epsilon of the state space is guaranteed to remain in the level set , so eventually the safety guarantee is entirely void . One way to remedy this is if the Lyapunov function is Lipschitz continuous . However , then , the number of samples required would still be exponential in the dimension of the state space . At this point , existing formal methods tools for verifying Lyapunov functions would perform just as well if not better , e.g. , see : Soonho Kong , Sicun Gao , Wei Chen , and Edmund Clarke . dReach : \u03b4-Reachability Analysis for Hybrid Systems . 2015.This approach was recently applied to synthesizing NN Lyapunov functions ( Chang et al.2019 ) .My point is n't that the authors ' approach is invalid , but that given the current writing it is impossible for me to understand the theoretical properties of their approach . Overall , I think the paper may have some interesting ideas , but I can not support publishing it in its current state", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for their feedback and hope that we address the concerns raised in our reply . Our work addresses the problem of designing a controller to maximize the stability of the closed-loop system . The objective in this work is to combine learning from data and control theory to train a controller that has provable safety certificates in terms of Lyapunov functions . Regarding clarity , the paper borrows terms and definitions from control theory which are instrumental to motivate the proposed algorithm . While it is not practical to write explanations to all the standard results in control systems and Lyapunov stability , we have tried our best to elucidate the relevant background in Section 2 . For more information on Lyapunov-based control methods and certification , we would like to refer the reviewer to Bobiti ( 2017 ) and Limon ( 2003 , 2009 ) which we have also cited in the paper . We are open to further suggestions by the reviewer on how to improve the readability of the work . Please find our comments on the specific questions/concerns that have been raised : # # # # 1 . Regarding the problem statement Restating the approach rationale specified on page 2 , our work aims to match or enlarge the safe region of an unknown controller . We aim to design a stabilizing controller using data collected ( one-step demonstrations ) from an unknown controller ( demonstrator ) such that the largest possible stability region is obtained . In an ideal case , the region obtained by the new controller should match or extend the one from the original unknown one ( demonstrator ) . However , due to function approximation , this may not always be possible . In this work , we offer a framework to produce a verifiably safe new controller while at the same time attempting to match the size of the stable region of the demonstrator as much as possible . # # # # 2.Regarding learning of the Lyapunov function in the algorithm As specified in Section 3.1 , we design the Lyapunov neural network ( NN ) such that it is Lipchitz and satisfies the Lyapunov conditions ( 4 ) . This is described in more detail in the work from Richards et.al ( 2018 ) . Provided that condition ( 5 ) is verified , the stability guarantees provided in Theorem 1 ( formerly called Lemma 1 ) are that for an MPC which uses the Lyapunov NN function as its terminal cost . Proving that a NN trained via SGD is \u201c exact \u201d at inference time or bounding its test error during training are open areas of research . As stated on page 3 , in our work , we use a-posteriori sampling-based verification of the neural Lyapunov function to verify ( 5 ) at each stage of our alternate learning algorithm . This provides a high probability certificate that the network is a Lyapunov function according to the conditions ( 4 ) and ( 5 ) , as described in Bobiti ( 2017 ) . Due to page constraints , a detailed description of the algorithm used for verification is specified in Appendix E. # # # # 3 . Regarding robustness margins to model errors We thank the reviewer for bringing this to our notice . In the appendix , we have shown a bound for the maximum model error for which the closed-loop system can retain a given Input-to-State Stability ( ISS ) , i.e.robustness to additive errors . Due to space limitations , we had not included this bound as part of the theorem in the main paper . However , we recognize that it would be beneficial to state this result and will do so in the updated version of the paper . The bound on the maximum model error for ISS tells us that , for a stronger contraction factor and a greater safe set size , we can proportionally tolerate more uncertainty on the model . However , the effect on the stability performance of the controller is highlighted in Theorem 2 ( formerly Lemma 2 ) , where we discuss how the model error affects the final radius of convergence of the cost ( and implicitly the control error ) . The bound presented there shows that it is beneficial to decrease the horizon length if the uncertainty in the model is large and the system to be controlled has large Lipschitz constants . In our experiments , we use this result in designing the controller by reducing its horizon length to one and still showing that ensures stability ."}, "1": {"review_id": "N5Zacze7uru-1", "review_text": "This paper addresses the question of how to stabilize a system in a vicinity of an equilibrium . While the majority of reinforcement learning algorithms rely on trial and error , which may damage the system , the authors introduce an algorithm for safe exploration and control . A traditional approach in model-based RL is to use MPC with a surrogate forward model to minimize a planning objective comprising a sum of stage costs along with a terminal cost , often chosen as an approximated value function -i.e.the optimal expected cost-to-go- which can be learned by a Bellman equation . Instead , this work is placed in the framework of Robust MPC , where this value function is replaced by a Luyapunov function $ V $ , which is related to the notion of stability and is only constrained to decrease along trajectories . Such a Luyapunov function , when available , provides both a safe region , defined as a level-set of V , and a MPC policy for which stability analyses have been developed : the authors extend a result from Limon et al . ( 2003 ; 2009 ) to show that this MPC policy enjoys asymptotic stability in general , and input-to-state stability in the presence of small enough model errors . Accordingly , the authors propose a scheme allowing to learn a Lyapunov function $ V $ from demonstration data only , through a loss function that penalizes increments of $ V $ along one-step transitions . A regularization parameter $ \\alpha $ of this MPC , which balances stability with constraints satisfaction and stage costs , is also learned jointly by an alternative training procedure . This approach is evaluated empirically on two standard constrained non-linear continuous control tasks . Strong points : 1 . This paper is clearly written , well motivated , honest about its place in the literature , and all derivations seem technically correct . 2.By bringing together existing results and techniques ( MPC stability analyses , value-based RL analyses , LuyapunovNet ) , the authors manage to relax several assumptions of prior works ( no need for access to the perfect dynamics or a stabilizing policy , but only to a demonstration dataset ) which makes the approach more practical . 3.Empirically , the learned Luyapunov functions seem to effectively capture useful stability information , since the proposed approach outperforms a standard MPC with a longer planning horizon . Even better , this observation is theoretically justified by Lemma 2 : errors of the surrogate model compound when used in an MPC , which is detrimental for long-term planning . Conversely , if $ V $ contains long-term information , it can directly be used for short-sighted planning , similarly to being greedy with respect to a value function . Weak points : 1 . The part which was the least clear to me is the * Performance with surrogate models * paragraph , with Lemma 2 . The authors draw a parallel between Luyapunov functions in control theory and value functions in RL , but the latter are not really defined clearly in the text . The authors state in their introduction that they treat `` the learned Lyapunov NN as an estimate of the value function '' and later they mention a `` correct '' value function $ V^ * $ for optimal `` expected infinite-horizon performance '' , but this quantity is nowhere defined . I suppose $ V^ * $ is an expected infinite sum of discounted stage costs , but which costs ? The same as in equation ( 3 ) ? If so , I find it hard to believe that the assumption of Lemma 2 should be satisfied ( $ V^ * $ close to $ \\alpha V $ ) , given that the stage cost $ l $ of ( 3 ) used to define $ V^ * $ does not appear in the loss ( 10 ) used to learn the Lyapunov function $ V $ . 2.I find it difficult to assess the novelty of the theoretical results . The authors are honest in stating that they are extending/adapting known results , but it is not precisely stated what their added value is . Moreover , the abstract mentions that `` we also present theorems '' but in the article these results are presented as lemmas , which to me usually suggests that they are either instrumental to another result ( which they are not ) , or of minor importance . 3.One of the main claim of the paper is the ability of the method to expand a demonstrated safe region . However , this is not really observed consistently across tasks and iterations . For instance , it is not the case in Figure 3 . Likewise , the Table 1 states that `` With iterations , the number of points verified by the controller increases '' , but only a two iterations is provided ( i.e.a single opportunity to increase/decrease ) . This seems a bit suspicious and suggests that the ratio of verified points may actually decrease on subsequent iterations , as it does on iteration 3 of Table 3 . To conclude , I lean toward recommending acceptance , but I am ready to increase my score provided that the authors improve the clarity on both the analogy between Lyapunov and value functions , and state more clearly the novelty of their theoretical contributions . Minor remarks and typos : * I do not really see the relevance of the safety performance metric in the Inverted Pendulum experiment ( Fig.3 ) : the state constraints are loose enough that every successful trajectory is also considered safe , so this safety plot ( right ) does not really bring any new information to the table . * p2 , Equation ( 5 ) : X_s\\ * * X_T * * * p3 , Learning and Safety Ver * * i * * fication * p6 , to minimize the loss defined in * * ( 11 ) * *", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for thoroughly reading the paper and the positive and constructive feedback . We particularly appreciate that our work was considered \u201c clear \u201d , \u201c well-motivated \u201d , \u201c honest \u201d , \u201c technically correct \u201d and its relevance was recognized . We have tried to address all the concerns by the reviewer in the following but are open to further discussions . We will fix the typos and make the required additions to the revised upload of the paper . We hope we convince the reviewer to possibly further improve their score . Please find our comments on the specific questions/concerns that have been raised : # # # # 1 . Regarding performance with surrogate models Yes , by using the Lyapunov function as the terminal cost , we want to approximate the tail of an infinite-horizon control formulation that uses the stage cost defined in ( 3 ) . We do not expect the error to be small with respect to the correct value function . The error can , however , be upper-bounded since all considered functions , the system dynamics , are Lipschitz and the constraints sets are bounded . Lemma 2 aims to break down the different components that affect performance and communicate the role of horizon length in the presence of model uncertainty . It would be possible in principle to combine the approach with value estimation to reduce the error but this is beyond the scope of the paper . The reason for not using the stage cost directly is that the unknown controller might not be optimal with respect to this stage cost ( reward ) but it is generally an engineered working solution . This choice was highly beneficial for instance in the car scenario , where our initial long horizon MPC used to generate the data produced suboptimal solutions due to the optimizer tolerances , correct decrease rate can not be guaranteed , differently from an LQR . Moreover , the long horizon MPC trajectories converge only to a neighborhood of the origin for the car . Despite this , our algorithm learns a Lyapunov function that can provide the system to even outperform the initial MPC , as kindly pointed out by the reviewer , in terms of convergence to a smaller neighborhood . # # # # 2.Regarding the novelty of the theoretical results In the revised paper , we will rectify calling the theoretical results as `` lemmas '' instead of `` theorems '' . We mainly extend existing results in Theorem 1 by considering the discount factor in the formulation and the contraction factor in the Lyapunov inequality instead of the loss . For the rest of Theorem 1 , we build and proceed upon the same steps of the cited articles from Limon et al . ( 2003 , 2009 ) . As also remarked by the reviewer , we state the same on page 4 in the paragraph on \u201c stability and safety \u201d . We did not mention the contraction instead of the loss but we will make this clearer in the revised upload . Overall , Theorem 1 aims to motivate our proposed algorithm , which is our main contribution in this work . We thought it was important , however , to state the theorem to mark the relevance of learning a Lyapunov function instead of just a value function surrogate . In Theorem 2 , we instead aim to close the gap with Lowrey et al . ( 2018 ) in terms of performance evaluation . This is also useful to understand the contribution of the horizon length and model error . With a perfect model , a longer horizon is indeed beneficial as claimed by Lowrey et al . ( 2018 ) , however , this is true only for a perfect model . In this case , it is also better in terms of the size of the stable region of the MPC . However , having a Lyapunov function that induces a larger safe set allows one to employ a shorter horizon and retain the same stable region with a shorter horizon . We reinforce the statement by demonstrating that , with function approximation in the model , a shorter horizon can become much more beneficial also in terms of optimality . The bound in Theorem 2 can be used to estimate the effect on convergence by model error and horizon length . We will clarify this further in the revised upload ."}, "2": {"review_id": "N5Zacze7uru-2", "review_text": "In this paper the author proposed an MPC algorithm in which both the dynamics function and the Lyapunov function are parameterized with neural networks .. Specifically leveraging the results of Lyapunov networks ( 2018 CORL paper : https : //arxiv.org/abs/1808.00924 ) for learning Lyapunov functions , the authors derived an MPC algorithm for quadratic cost/reward problems and also proved the stability , robustness , and sub-optimality performance . To demonstrate the effectiveness of the algorithms , the authors also evaluated this approach on the simple inverted pendulum and car kinematics tasks . In general I find this paper presents a comprehensive results of a model-based control method that is very popular in the control theory community . To justify their algorithms they also proved several standard properties ( stability , sub-optimality performance ) in control , which I appreciate their efforts . However , I do have severals questions/concerns regarding the details of their approach : 1 ) The presentation of the loss function of Lyapunov network is not easy to parse , especially there are couple terms that contain specific mathematical operators ( sign , ReLU ) . Can the authors explain each term in the loss and why such choices of loss terms are necessary . Is this loss function identical to the Lyapunov network 2018 CORL paper ? 2 ) From the main paper it is unclear how the NN-dynamics model \\hat f is learned . Does it just train based on prediction loss ? More importantly , while the MPC algorithm uses the learned model how does the dynamics model error affect the stability/robustness/performance bounds of the control algorithm ? I can not immediately find this information in lemma 1 and lemma 2 , which makes me worried about the correctness of these results . ( Unfortunately I have n't had a chance to check the appendix for proofs ) 3 ) Having sub-optimality performance for MPC algorithms is a nice result , as not many MPC algorithms have performance guarantees . However these kind of results are also not new ( for example , see https : //ieeexplore.ieee.org/document/4639448 ) . How does the MPC performance result here compared with the ones by Grune and Rantzer ? 4 ) Among various safe MPC papers , how does the proposed one in this paper compared with this safe MPC algorithm : https : //arxiv.org/pdf/1803.08287.pdf , which is also proposed by Andreas Krause 's group ( that proposed the Lyapunov network ) ? At least experimentally how does the proposed algorithm compare with other safe MPC baselines ( such as the one above ) on the standard benchmark tasks ( for example the above work also tested the algorithms on the pendulum task ) . On the overall , I find this paper 's algorithm interesting . However , there are several technical question listed above , and one high-level concern is its novelty . Without further discussions , it appears to me that the work combines several existing results on Lyapunov network and MPC , for which the contribution is rather incremental .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their constructive feedback and are pleased to hear that our proposed algorithm was found interesting . We address the raised points in the following reply and will make the necessary changes in the uploaded version as well . We hope these answer the remarks by the reviewer and convince them to improve the score . We are of course open to further iterations and amendments to the paper . Please find our comments on the specific questions/concerns that have been raised : # # # # 1 . Regarding the novelty of the work As mentioned in the paper , we extend the theoretical results from Limon et al . ( 2003 ) to the discounted setting and the use of lambda-constractive Lyapunov functions ; those from Lowrey et al . ( 2018 ) are extended to the uncertain model setup . The theorems presented in our paper aim to motivate the proposed algorithm which we believe has never been presented in the literature . # # # # 2.Regarding loss function for learning Lyapunov NN : The loss proposed in our paper is similar to the one by Richards et al . ( 2018 ) .The first part encourages the function to decrease over trajectories from within the estimated safe set . The second part is aimed at estimating the function level that defines the safe set . The main difference in this loss function from that by Richards et al . ( 2018 ) is in this second part . Instead of using stability labels obtained by performing a forward propagation of T steps and verifying convergence , our algorithm only performs a unitary time-step forward propagation . This is mentioned at the beginning of page 6 in Section 3.3 , however , we will reinforce the statement in the new upload . We use the Lyapunov network itself to generate pseudo-labels ( stability certificate via $ sign ( \\Delta V ) $ ) . This is more data-efficient as we don \u2019 t need to have all trajectories reach completion , and is less prone to error accumulation than using a long-horizon simulation with an imperfect surrogate model ( Richards et al . ( 2018 ) use the true dynamics model for forward propagation ) . Besides that , in the training procedure , we verify the Lyapunov network on a validation dataset and perform cross-validation based on the number of verified and not verified points . We also perform a posteriori formal verification ( the algorithm for this is presented in Appendix E ) . We will make these points clearer in the updated version . Notation-wise : $ ReLU ( x ) =max ( x,0 ) $ and $ sign ( \\cdot ) $ is the sign of the quantity which returns either 1 or -1 . We will clarify this in the paper as well . # # # # 3.Regarding surrogate dynamics model : We do not require the surrogate dynamics model to be trained in a particular way . Further , it is not essential that the model is parametrized by a neural network , as long as its function class has Lipschitz continuity . Since all the sets are bounded , the model \u2019 s error bound can be inferred based on the Lipschitz constants ( as discussed on page 3 ) . However , in our algorithm , we assume that the learned dynamics model is given and its error bound is known . We will add a worst-case one-step error bound for ISS of the controller in the revised version of Theorem 1 ( formerly Lemma 1 ) . This margin was previously stated in the proof of the theorem ( in Appendix A ) , however , we will move it to the main paper . Due to page constraints , we specified the details about the system models and the training of the dynamics model in Appendix D , while providing the information about the setup in the main paper ( Section 4 ) . The NN-dynamics surrogate model $ \\hat { f } $ is trained using transition tuples ( this is stated above eq . ( 8 ) on page 3 ) . The data to train the model is collected using a random policy as typically done in system identification . This is mentioned on pages 6-7 for both experiments . # # # # 4.Regarding related work on MPC suboptimality : We thank the reviewer for pointing us to this important paper . We cited a different work from these authors on the stability of optimal control with discount factors ( Gaitsgory et al. , 2015 ) which is closely related to our discounted setup . The work by Gruene and Rantzer ( 2008 ) , however , is indeed also related and offers a key set of results for MPC suboptimality under the assumption that the loss satisfies an exponential controllability condition . In our paper , we consider a discounted setting which is more common in RL than in controls ( because of stability limitations as we highlight in Theorem 1 ) . Our results are built upon those from Lowrey et al . ( 2018 ) .However , we will add the missing reference ( Gruene and Rantzer , 2008 ) to our related work section in the revised upload ."}}