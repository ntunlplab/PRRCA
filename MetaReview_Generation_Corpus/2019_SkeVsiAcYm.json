{"year": "2019", "forum": "SkeVsiAcYm", "title": "Generative predecessor models for sample-efficient imitation learning", "decision": "Accept (Poster)", "meta_review": "This paper proposes to estimate the predecessor state dynamics for more sample-efficient imitation learning. While backward models have been used in the past in reinforcement learning, the application to imitation learning has not been previously studied. The paper is well-written and the results are good, showing clear improvements over GAIL in the presented experiments. The primary weakness of the paper is the lack of comparisons to the baselines suggested by reviewer 1 (a jumpy forward model and a single step predecessor model) to fully evaluate the contribution, and to SAIL and AIRL. Despite these weaknesses, the paper slightly exceeds the bar for acceptance at ICLR.\nThe authors are strongly encouraged to include these comparisons in the final version.", "reviews": [{"review_id": "SkeVsiAcYm-0", "review_text": "The submission builds up on recent advances in neural density estimation to develop a new algorithm for imitation learning based on a probabilistic model for predecessor state dynamics. In particular, the method trains masked autoregressive flows as a probabilistic model for state action pairs conditioned on future states. This model is used to estimate the gradient of the stationary distribution of a policies visited states. Finally, the proposed objective uses this estimate and the gradient of the log likelihood of expert actions under the policy to maximise the similarity of the expert\u2019s and agent\u2019s stationary state-action distributions. The proposed method outperforms existing imitation learning approach (GAIL & BC) on 2 simulation-based manipulation tasks. It performs particularly well in terms of sample efficiency. The magnitude of difference between the sample efficiencies of GAIL and the proposed approach seems quite surprising and it would be beneficial if the authors could explicitly state if the measured number of samples include the ones used for training of the probabilistic model as well as the policy (apologies if I have missed a section fulfilling this purpose). While the improvements on the presented experiments are clear, the experimental section represents a small shortcoming of the submitted paper. The 2 experiments (clip and peg insertion) are quite similar in type and to not take into account other common domains e.g. locomotion tasks from the original GAIL paper. Furthermore, an additional comparison to SAIL would be recommended since the approaches are closely related as the authors acknowledge. The provided comparison with different types of available expert data is quite interesting and could possibly be extended to test other state-of-the-art methods (action-free versions of GAIL, AIRL,etc.). Nonetheless, the paper overall presents a strong submission based on novelty & relevance of the proposed method and is recommended for publication. Minor issues: - Related work: improve transitions between the section about trajectory tracking and BC. - Ablation studies with less flexible probabilistic models would strengthen the experiment section further. - Add derivation from Eq. 3 to 4 and 5 to appendix to render the paper more self-contained and easier to access. - A release of the code base would further strengthen the contributions of the submission. General recommendation: - The authors are encouraged to further investigate off-policy corrections for improved convergence. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . We would like to address some of the questions and points raised in your review : + Regarding sample efficiency , our algorithm only uses artificial samples and expert samples when updating the policy . The reported number of environment samples are the samples used to train the generative model . Note that while it is efficient compared to other algorithms of this type , the data efficiency is not extraordinary to train a network of this size on a problem of these dimensions . This indicates that the samples generated from the model are likely to be useful even as the policy changes . + Regarding the chosen domains , we believe that they highlight a type of problem that is difficult for approaches based on adversarial updates ( due to sample efficiency as well as the ability to control the state in its entirety ) while also being representative for a class of problems one might encounter in practice . While we believe our approach to be widely applicable and would like to see it applied in other domains , existing approaches are already able to achieve very high scores on domains such as the mujoco walkers . + In comparison to SAIL , we aim to address scalability w.r.t.the number of parameters of the policy . We found that larger policies are able to achieve more accurate results on our domains , yet policies of this size are out of reach for SAIL which has to predict the gradient for each parameter of the network . + We added additional steps to Appendix A to make the derivation more self-contained and changed the wording in the related works section based on your suggestion . Unfortunately , we are currently not able to release the code ."}, {"review_id": "SkeVsiAcYm-1", "review_text": "The paper proposes to use predecessor models for imitation learning to overcome the issue of only observing expert samples during imitation from expert trajectories. The paper is very well written. But the proposed method is really not novel. The idea of using predecessor models have already been explored in multiple places [1], [2] (but not in imitation learning scenario!). Hence, the novelty comes from using the predecessor models for imitation learning. The introduction of the paper should mention this to reflect the contribution. [1] Recall Traces: Efficient Backtracking models for efficient RL https://arxiv.org/abs/1804.00379 [2] Organizing Experience: A Deeper Look at Replay Mechanisms for Sample-based Planning in Continuous State Domains https://arxiv.org/abs/1806.04624 Both of these papers should be cited and discussed. Results: The proposed method outperforms GAIL and behaviour cloning in terms of sample efficiency on simulation-based manipulation tasks. Regarding experiments, I would like to see certain baselines. - What happens when you predict sequentially using predecessor models ? I understand that the sequential generation is prone to accumulating errors, but as [1] points out, using predecessor models you can sample from many states on the expert trajectory. And Hence possible to get good learning signal even while sampling shorter trajectories using predecessor models. - Comparison with Dyna based methods. For this baseline, authors would learn a forward model. And then sample from the forward model, and use the samples from the forward model for imitation learning. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . Predecessor models do indeed have a long history in reinforcement learning and recent work explores the use of deep networks in this context . While our algorithm followed directly from the derivation of the state-distribution gradient in section 3.1 , we can see that a comparison to the aforementioned works might be useful to the reader and have added this in section 1 . We hope that this addition will adequately specify the scope of our work . In particular , we claim the following two contributions : 1 . Derivation of the state-distribution gradient based on samples from a predecessor model . While such models have been used in the past , to the best of our knowledge this connection has not been pointed out before . Instead , most work focuses on the use of predecessor models as a more efficient order of bellman backups while the recent Recall Traces uses a justification based on a variational lower bound . We believe that our derivation provides further justification to the approach used in Recall Traces and may furthermore help to guide design decisions when developing such algorithms in the field of reinforcement learning . 2.Development of a novel , state-of-the-art imitation learning algorithm . To the best of our knowledge , the use of predecessor models to achieve state-action distribution matching in imitation learning is novel . We believe that predecessor models are a natural fit for imitation learning as , unlike in reinforcement learning , future observations and their accordance with demonstrations are very difficult to evaluate . We demonstrate the effectiveness of such models on traditionally difficult real world imitation learning problems in our evaluation . Regarding our choice of multi-step models and comparison to one-step models of either direction , we note that in the general case , the error in naive one-step models grows exponentially ( Venkatraman et al. , 2015 ) thus requiring careful design of such models . Recent work such as Ha and Schmidhuber , 2018 and Gregor and Besse , 2018 achieves impressive predictions on sequential rollouts indicating that it is very likely that a one-step model can be applied in our setting as well . However , these works require a significant effort on the modelling side and we thus decided to side-step the issue by modelling the desired distribution directly . As the contact dynamics in our domain can be complex , we believe that the effort required in our domain would have been significant as well . We note that the main contribution of this paper is the use of samples of a predecessor model to match state-action distributions in a principled way while the choice of model is a design choice that was made to avoid increasing complexity ."}, {"review_id": "SkeVsiAcYm-2", "review_text": "This paper studies the problem of matching the state-action distributions of agent and expert demonstrations. In order to address this problem, the authors consider a likelihood treatment comprising a conditional probability (which is estimated from demonstrations) and a state distribution (which is estimated from sampling approximations). The authors provide a descent result (i.e., equ. (7)) to estimate the gradient of the logarithmic state distribution. One problem is that it is unclear how the discount factor $\\gamma$ influence this result? In addition, in (12), two scaling factors are used, so how to balance these weights? Specifically, in (11), it seems the authors are considering the stationary joint state-action distribution, which is different from the state-action distribution generated by the agent on-line, it is suggested to clarify this issue.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . The questions you raise about the effect of the approximations made in our derivation are valid and we have added additional discussion to the paper that we hope will answer these questions satisfactorily : + The discount factor \u03b3 is not only similar to the discount factor used in reinforcement learning but can be seen as identical . This was not immediately apparent in the original submission and we have added Appendix C to the manuscript to explore the connection between our gradient estimate and policy gradients . As a result , we can draw on the understanding of the discount factor in reinforcement learning to gain insight into the behavior of \u03b3 and conclude that first , the discount factor introduces a trade-off where an agent with lower \u03b3 prefers to reach demonstrated states more quickly while agents with higher \u03b3 will aim to reproduce the state-action distribution more closely in the long-term . Second , as \u03b3 approaches 1 , the variance grows . However , in reinforcement learning , it has been empirically shown that lower discount factors can be an accurate , low-variance approximation even when the true objective is more accurately described by the average reward objective ( \u03b3 - > 1 ) . The alternative derivation introduced in Appendix C thus indicates that lower values of \u03b3 are likely to be reasonable approximations in the imitation learning setting as well . + With regards to stationarity , under the usual ergodicity assumptions the expected distribution of state-action pairs the agent will observe and the stationary distribution should be identical in the infinite horizon case ( using the modified MDP with terminal states being treated as transitions to initial states as discussed in section 2.1 ) . In general , matching the joint stationary distribution to the empirical distribution of the expert implies a form of loop in the agents behavior which may be as simple as restarting after reaching a terminal state . This is the case in many practical scenarios as well as our experiments . While handling the finite horizon case explicitly might also be interesting , we are not considering it for the purposes of this work . + The scaling factors \u03b2 were added to provide more freedom to tune the behavior of the learning algorithm but we agree that additional discussion would be useful and have added it to section 3.3 . In particular , the factors are the result of dropping the factor of 1/ ( 1-\u03b3 ) in equation 7 , this indicates that a sensible starting point would be \u03b2_\u03c0= ( 1-\u03b3 ) \u03b2_d . However , we did not find this to be optimal in all cases . In particular , if behavioral cloning is likely to overfit strongly , lower values of \u03b2_\u03c0 may be adequate while in cases where exploring to learn the generative models is more difficult , higher values of \u03b2_\u03c0 may provide more guidance . We hope that we answered your questions to your satisfaction , please let us know if you have further questions or concerns that you would like us to address ."}], "0": {"review_id": "SkeVsiAcYm-0", "review_text": "The submission builds up on recent advances in neural density estimation to develop a new algorithm for imitation learning based on a probabilistic model for predecessor state dynamics. In particular, the method trains masked autoregressive flows as a probabilistic model for state action pairs conditioned on future states. This model is used to estimate the gradient of the stationary distribution of a policies visited states. Finally, the proposed objective uses this estimate and the gradient of the log likelihood of expert actions under the policy to maximise the similarity of the expert\u2019s and agent\u2019s stationary state-action distributions. The proposed method outperforms existing imitation learning approach (GAIL & BC) on 2 simulation-based manipulation tasks. It performs particularly well in terms of sample efficiency. The magnitude of difference between the sample efficiencies of GAIL and the proposed approach seems quite surprising and it would be beneficial if the authors could explicitly state if the measured number of samples include the ones used for training of the probabilistic model as well as the policy (apologies if I have missed a section fulfilling this purpose). While the improvements on the presented experiments are clear, the experimental section represents a small shortcoming of the submitted paper. The 2 experiments (clip and peg insertion) are quite similar in type and to not take into account other common domains e.g. locomotion tasks from the original GAIL paper. Furthermore, an additional comparison to SAIL would be recommended since the approaches are closely related as the authors acknowledge. The provided comparison with different types of available expert data is quite interesting and could possibly be extended to test other state-of-the-art methods (action-free versions of GAIL, AIRL,etc.). Nonetheless, the paper overall presents a strong submission based on novelty & relevance of the proposed method and is recommended for publication. Minor issues: - Related work: improve transitions between the section about trajectory tracking and BC. - Ablation studies with less flexible probabilistic models would strengthen the experiment section further. - Add derivation from Eq. 3 to 4 and 5 to appendix to render the paper more self-contained and easier to access. - A release of the code base would further strengthen the contributions of the submission. General recommendation: - The authors are encouraged to further investigate off-policy corrections for improved convergence. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . We would like to address some of the questions and points raised in your review : + Regarding sample efficiency , our algorithm only uses artificial samples and expert samples when updating the policy . The reported number of environment samples are the samples used to train the generative model . Note that while it is efficient compared to other algorithms of this type , the data efficiency is not extraordinary to train a network of this size on a problem of these dimensions . This indicates that the samples generated from the model are likely to be useful even as the policy changes . + Regarding the chosen domains , we believe that they highlight a type of problem that is difficult for approaches based on adversarial updates ( due to sample efficiency as well as the ability to control the state in its entirety ) while also being representative for a class of problems one might encounter in practice . While we believe our approach to be widely applicable and would like to see it applied in other domains , existing approaches are already able to achieve very high scores on domains such as the mujoco walkers . + In comparison to SAIL , we aim to address scalability w.r.t.the number of parameters of the policy . We found that larger policies are able to achieve more accurate results on our domains , yet policies of this size are out of reach for SAIL which has to predict the gradient for each parameter of the network . + We added additional steps to Appendix A to make the derivation more self-contained and changed the wording in the related works section based on your suggestion . Unfortunately , we are currently not able to release the code ."}, "1": {"review_id": "SkeVsiAcYm-1", "review_text": "The paper proposes to use predecessor models for imitation learning to overcome the issue of only observing expert samples during imitation from expert trajectories. The paper is very well written. But the proposed method is really not novel. The idea of using predecessor models have already been explored in multiple places [1], [2] (but not in imitation learning scenario!). Hence, the novelty comes from using the predecessor models for imitation learning. The introduction of the paper should mention this to reflect the contribution. [1] Recall Traces: Efficient Backtracking models for efficient RL https://arxiv.org/abs/1804.00379 [2] Organizing Experience: A Deeper Look at Replay Mechanisms for Sample-based Planning in Continuous State Domains https://arxiv.org/abs/1806.04624 Both of these papers should be cited and discussed. Results: The proposed method outperforms GAIL and behaviour cloning in terms of sample efficiency on simulation-based manipulation tasks. Regarding experiments, I would like to see certain baselines. - What happens when you predict sequentially using predecessor models ? I understand that the sequential generation is prone to accumulating errors, but as [1] points out, using predecessor models you can sample from many states on the expert trajectory. And Hence possible to get good learning signal even while sampling shorter trajectories using predecessor models. - Comparison with Dyna based methods. For this baseline, authors would learn a forward model. And then sample from the forward model, and use the samples from the forward model for imitation learning. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . Predecessor models do indeed have a long history in reinforcement learning and recent work explores the use of deep networks in this context . While our algorithm followed directly from the derivation of the state-distribution gradient in section 3.1 , we can see that a comparison to the aforementioned works might be useful to the reader and have added this in section 1 . We hope that this addition will adequately specify the scope of our work . In particular , we claim the following two contributions : 1 . Derivation of the state-distribution gradient based on samples from a predecessor model . While such models have been used in the past , to the best of our knowledge this connection has not been pointed out before . Instead , most work focuses on the use of predecessor models as a more efficient order of bellman backups while the recent Recall Traces uses a justification based on a variational lower bound . We believe that our derivation provides further justification to the approach used in Recall Traces and may furthermore help to guide design decisions when developing such algorithms in the field of reinforcement learning . 2.Development of a novel , state-of-the-art imitation learning algorithm . To the best of our knowledge , the use of predecessor models to achieve state-action distribution matching in imitation learning is novel . We believe that predecessor models are a natural fit for imitation learning as , unlike in reinforcement learning , future observations and their accordance with demonstrations are very difficult to evaluate . We demonstrate the effectiveness of such models on traditionally difficult real world imitation learning problems in our evaluation . Regarding our choice of multi-step models and comparison to one-step models of either direction , we note that in the general case , the error in naive one-step models grows exponentially ( Venkatraman et al. , 2015 ) thus requiring careful design of such models . Recent work such as Ha and Schmidhuber , 2018 and Gregor and Besse , 2018 achieves impressive predictions on sequential rollouts indicating that it is very likely that a one-step model can be applied in our setting as well . However , these works require a significant effort on the modelling side and we thus decided to side-step the issue by modelling the desired distribution directly . As the contact dynamics in our domain can be complex , we believe that the effort required in our domain would have been significant as well . We note that the main contribution of this paper is the use of samples of a predecessor model to match state-action distributions in a principled way while the choice of model is a design choice that was made to avoid increasing complexity ."}, "2": {"review_id": "SkeVsiAcYm-2", "review_text": "This paper studies the problem of matching the state-action distributions of agent and expert demonstrations. In order to address this problem, the authors consider a likelihood treatment comprising a conditional probability (which is estimated from demonstrations) and a state distribution (which is estimated from sampling approximations). The authors provide a descent result (i.e., equ. (7)) to estimate the gradient of the logarithmic state distribution. One problem is that it is unclear how the discount factor $\\gamma$ influence this result? In addition, in (12), two scaling factors are used, so how to balance these weights? Specifically, in (11), it seems the authors are considering the stationary joint state-action distribution, which is different from the state-action distribution generated by the agent on-line, it is suggested to clarify this issue.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . The questions you raise about the effect of the approximations made in our derivation are valid and we have added additional discussion to the paper that we hope will answer these questions satisfactorily : + The discount factor \u03b3 is not only similar to the discount factor used in reinforcement learning but can be seen as identical . This was not immediately apparent in the original submission and we have added Appendix C to the manuscript to explore the connection between our gradient estimate and policy gradients . As a result , we can draw on the understanding of the discount factor in reinforcement learning to gain insight into the behavior of \u03b3 and conclude that first , the discount factor introduces a trade-off where an agent with lower \u03b3 prefers to reach demonstrated states more quickly while agents with higher \u03b3 will aim to reproduce the state-action distribution more closely in the long-term . Second , as \u03b3 approaches 1 , the variance grows . However , in reinforcement learning , it has been empirically shown that lower discount factors can be an accurate , low-variance approximation even when the true objective is more accurately described by the average reward objective ( \u03b3 - > 1 ) . The alternative derivation introduced in Appendix C thus indicates that lower values of \u03b3 are likely to be reasonable approximations in the imitation learning setting as well . + With regards to stationarity , under the usual ergodicity assumptions the expected distribution of state-action pairs the agent will observe and the stationary distribution should be identical in the infinite horizon case ( using the modified MDP with terminal states being treated as transitions to initial states as discussed in section 2.1 ) . In general , matching the joint stationary distribution to the empirical distribution of the expert implies a form of loop in the agents behavior which may be as simple as restarting after reaching a terminal state . This is the case in many practical scenarios as well as our experiments . While handling the finite horizon case explicitly might also be interesting , we are not considering it for the purposes of this work . + The scaling factors \u03b2 were added to provide more freedom to tune the behavior of the learning algorithm but we agree that additional discussion would be useful and have added it to section 3.3 . In particular , the factors are the result of dropping the factor of 1/ ( 1-\u03b3 ) in equation 7 , this indicates that a sensible starting point would be \u03b2_\u03c0= ( 1-\u03b3 ) \u03b2_d . However , we did not find this to be optimal in all cases . In particular , if behavioral cloning is likely to overfit strongly , lower values of \u03b2_\u03c0 may be adequate while in cases where exploring to learn the generative models is more difficult , higher values of \u03b2_\u03c0 may provide more guidance . We hope that we answered your questions to your satisfaction , please let us know if you have further questions or concerns that you would like us to address ."}}