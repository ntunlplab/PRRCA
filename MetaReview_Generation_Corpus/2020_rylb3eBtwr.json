{"year": "2020", "forum": "rylb3eBtwr", "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "decision": "Accept (Poster)", "meta_review": "Three reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission.", "reviews": [{"review_id": "rylb3eBtwr-0", "review_text": "This paper proposes to use the robust subspace recovery layer (RSR) in the autoencoder model for unsupervised anomaly detection. This paper is well written overall. Presentation is clear and it is easy to follow. The proposed approach is a simple combination of existing approaches. Although its theoretical analysis with respect to the performance of anomaly detection is limited, experiments show that the proposed method is effective and superior to the existing anomaly detection methods. I have the following comments: - Parameter sensitivity should be examined. The proposed method has the number of parameters including \\lambda_1, \\lambda_2, and parameters in neural networks. Since parameter tuning is fundamentally difficult in the unsupervised setting, the sensitivity of the proposed method with respect to changes of such parameters should be examined. - Since the efficiency is also an important issue for anomaly detection methods, runtime comparison would be interesting. - It would be also interesting whether the proposed method is also effective for non-structured data, where a dataset is given as just a set of (real-valued) feature vectors, with its comparison to the standard anomaly detection methods such as LOF and iForest. ", "rating": "8: Accept", "reply_text": "Thank you for your comments and constructive suggestions . We respond before the deadline , so we may communicate if needed . \u201c The proposed approach is a simple combination of existing approaches . \u201d Response : While AE and RSR are previous well-known approaches , RSRAE is not a simple combination of both of them . They are not two independent components that are combined together . In fact , we show that a simple combination of a least absolute deviations energy used in RSR and an AE , which we refer to AE-1 , does not improve the performance of AE for anomaly detection . The RSR layer is between the encoder and decoder of the AE and it is optimized together with the autoencoder and not separately from it . We are planning to further clarify why this layer should be effective and what makes it unique . \u201c - Parameter sensitivity should be examined . The proposed method has the number of parameters including \\lambda_1 , \\lambda_2 , and parameters in neural networks . Since parameter tuning is fundamentally difficult in the unsupervised setting , the sensitivity of the proposed method with respect to changes of such parameters should be examined . \u201d Response : We promoted the use of alternating minimization , which does not require predetermined $ \\lambda $ \u2019 s . That is , the values of $ \\lambda_1 $ and $ \\lambda_2 $ are not relevant for the algorithm we advocated . They are only relevant to RSRAE+ , but we do not advocate this algorithm due to the time it takes to test different values of lambda . Nevertheless , following this comment we will test the sensitivity of RSRAE+ to $ \\lambda_1 $ and $ \\lambda_2 $ . We also agree that it is important to test the sensitivity with respect to the learning rate and the dimension of the subspace , and we will report it in the revised version . \u201c - Since the efficiency is also an important issue for anomaly detection methods , runtime comparison would be interesting . \u201d Response : We plan to have a runtime comparison in the revised version . \u201c - It would be also interesting whether the proposed method is also effective for non-structured data , where a dataset is given as just a set of ( real-valued ) feature vectors , with its comparison to the standard anomaly detection methods such as LOF and iForest . \u201d Response : When we tested our method on the deep features of Imagenet and the features of Reuters and 20 Newsgroups , we worked with a set of feature vectors . In those examples , we didn \u2019 t treat the features like images or sequences , and the network was simply a fully-connected network instead of a convolutional one . Please let us know if we missed anything ."}, {"review_id": "rylb3eBtwr-1", "review_text": "After reading all the reviews and the comments, I feel more positive about the paper. I appreciate the feedback of the Authors and I have decided to increase the rating. ============================ The paper proposes using Robust Subspace Recovery in combination with an autoencoder (and possibly GANs) for anomaly detection. The encoder maps input data to the latent space of dimensionality D, which then is linearly projected to a subspace of dimensionality d (d < D). The projection of the latent space then goes to a decoder that reconstructs the input. A transformation matrix A is trained jointly with the autoencoder. Two additional terms are added to the loss: one to encourage the subspace of A^TA to approximate the latent space z and the second one to force it to be an orthogonal projector. The paper claims to generalize the existing RSR framework to the nonlinear case. However, the linear RSR is applied to the latent space of the autoencoder. In addition to that, all the following discussion and proofs are limited to the linear case. Since the proposed method is using RSB as it\u2019s core part, and claims to be a non-linear extension of it, it would be crucial to have a comparison with RSB, at least on those experimental setups, where high-level features are used (Tiny Imagenet with ResNET features, Reuters-21578, and 20 Newsgroups). However, there is no such comparison. Since autoencoders can potentially learn any, arbitrary entangled latent space, it is not clear why outliers should necessarily have such embedding that is outside of the learned subspace. In the case of the original RSR it happens due to the dimensionality reduction by the orthogonal projector. However, autoencoders already perform dimensionality reduction at each layer down to the bottleneck layer. The matrix A and the parameters of the AE are trained jointly. So, it can be seen that two processes can occur: - The AE in order to minimize the reconstruction error would learn such latent space z, that would fit into the subspace of A^TA, so that projection \\tilde z =Az doesn\u2019t cause data loss. - The AE in order to minimize the reconstruction error would learn such A, so that the subspace that z approximates is the best possible. It is not clear, which of the two cases would take place. If the first one would dominate, then it is not clear if such method would have any discriminating capabilities. My point is mainly that the presented work is not really a generalization of RSR as it claims to be, but rather it is just using RSR on a leaned embedding of the data. Some citations are missing, as well as it is missing a comparison to some state-of-the art methods such as OCNN \u2018Robust, Deep and Inductive Anomaly Detection\u2019 ECML 2017; \u2018Adversarially Learned One-Class Classifier for Novelty Detection\u2019 CVPR 2017; DSVDD \u2018Deep one-class classification.\u2019 ICML, 2018; ODIN \u2018Enhancing The Reliability Of Out-of-distribution Image Detection In Neural Networks\u2019 ICLR 2018; \u2018Generative Probabilistic Novelty Detection with Adversarial Autoencoders\u2019 NeurIPS 2018. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments . We respond before the deadline , so we may communicate if needed . \u201c The paper claims to generalize the existing RSR framework to the nonlinear case. \u201d Response : Our paper does not claim to generalize the RSR framework to a nonlinear one , but it combines ideas from RSR within an autoencoder in order to make the autoencoder more robust to outliers . The mere use of a \u201c robust metric \u201d ( such as the least absolute deviations ) for an autoencoder is not sufficiently robust to anomalies and this is why we find an idea like ours natural . We believe that the above wrong interpretation of our paper is due to the following two sentences in Section 5 : \u201c Goodfellow et al . ( 2016 ) exemplified how PCA can be structured as a linear autoencoder . Similarly , RSR can be directly used to form an outliers-robust linear autoencoder and our current work generalizes this basic idea to a nonlinear setting \u201d . The first claim here is that similarly to having an autoencoder for PCA ( with linear encoder and decoder and least squares minimization ) , one can have an autoencoder for RSR ( with linear encoder and decoder and absolute deviations minimization ) . Note that the difference between a PCA autoencoder and an RSR autoencoder is obtained by changing the minimization from least squares to absolute deviations ( where it is also natural to introduce a simple normalization in the implementation part ) . On the other hand , changing the metric of a general autoencoder , which corresponds to \u201c nonlinear PCA \u201d , from least squares to absolute deviations does not make it robust ( especially as ideas of simple normalizations for adversarial outliers do not work out in this case ) . We demonstrated this in our experiments ( see performance of AE-1 vs. RSRAE in Section 4.3 and Appendix F2 ) . We find that the RSR layer is a very natural idea to make an autoencoder robust . By writing \u201c generalizes this basic idea to a nonlinear setting \u201d , we meant extending the autoencoder to be robust to outliers , in analogy to making a PCA autoencoder robust by changing it into an RSR ( linear ) autoencoder , but we did not mean using the same method as in the latter one and formally generalizing RSR to the nonlinear case . The purpose of these two sentences was only to motivate the rigorous theory for WGAN . We considered them as part of a minor comment , which the expert can easily figure out . There was no claim about generalizing RSR to the nonlinear case in the introduction or the conclusion . Due to the confusion , we will extend and rewrite this part with a careful explanation how our method is expected to make an autoencoder more robust to outliers , and why the mere change of a metric is not sufficient to make the method robust . Anyway , we don \u2019 t find these new details necessary for understanding the paper , but they may avoid a similar confusion . \u201c However , the linear RSR is applied to the latent space of the autoencoder. \u201d Response : The linear layer is in the middle of an autoencoder and an RSR loss is part of the total loss function . It is not correct to think of two separate entities : an autoencoder and a linear RSR applied to the latent space of the autoencoder . That is , we do not try to first get the latent code form AE and then apply RSR to the latent code . Note that a general autoencoder tries to parametrize the structure of the data with a latent code within a low-dimensional linear space . Here we try to parametrize only the structure of the inliers , where the low-dimensional subspace lies within the ambient space of the encoder , we then map the output of the encoder onto a low-dimensional space corresponding to the latter subspace . This mapping is natural to the inliers , but not to the outliers , which are expected to have high reconstruction error ."}, {"review_id": "rylb3eBtwr-2", "review_text": "This paper adapts the concept of Robust Subspace Recovery (RSR) as a layer in an auto-encoder model for anomaly detection. A loss function is proposed that combines reconstruction error and a regularizer that enforces robustness against outliers. The reconstruction error expresses the accuracy of the nonlinear dimensionality reduction imposed by the autoencoder. The regularizer is the sum of absolute deviations from the latent subspace that represents a linear structure robust against outliers. An alternative procedure is applied where the loss terms are applied iteratively during training. Once trained, the reconstruction error is used directly for anomaly detection with a threshold. The AUC is used as a performance measure. The method is compared against 6 other methods (LOF, OCSVM, IF, DESBM, GT, DAGMM). The setting is fully unsupervised, meaning that the training data contains various amounts of anomalies, and the results are parametrized with the amount of corruption. The results show that the proposed approach outperforms the other methods in most cases, especially for larger amounts of corruption. An ablation study compares the approach with auto-encoder-only and a non-alternating gradient descent (fixed factors for each part of the loss function) and shows that the alternating method outperfroms all by a wide margin. PROS: * A novel approach to fully unsupervised anomaly detection that beats the state of the art. * The RSR layer is a simple fully connected layer and the loss function is simple to calculate, making the approach computationally efficient. * A pseudo-code algorithm is provided in the appendix, which should help reproducibility. * The paper is well written and the math is clearly laid out. * The result benchmarks are sufficiently exhaustive in both the methods that are compared and the datasets used. * The ablation study is informative and shows the effect of the regularization term of the loss function as well as the effect of alternating the gradient descent with the separate losses. CONS: * There is a serious problem in the results (Figure 1) as the AP curves show better scores for larger corruption factors. Are the AP-score graphs flipped ? Please explain. * The AUC and AP scores need to be defined. * The results should include the case where the training data is not contaminated with outliers (c=0). This would correspond to the semi-supervised scenario and it would be very interesting to see how the method compares to DAGMM and GT which are build for that scenario. * It would be interesting to see the effect of varying the subspace dimension. The authors chose 10 for all experiments, why is this number chosen, what would be the effect of choosing a smaller one ? This is a key parameter as it defines the structure of the projection subspace. Should this parameter be systematically tuned for each dataset ? Overall this is a good paper proposing a novel approach to fully unsupervised anomaly detection with state-of-the art results. ", "rating": "8: Accept", "reply_text": "Thank you for your comments and constructive suggestions . We respond before the deadline , so we may communicate if needed . \u201c * There is a serious problem in the results ( Figure 1 ) as the AP curves show better scores for larger corruption factors . Are the AP-score graphs flipped ? Please explain . * The AUC and AP scores need to be defined . \u201d Response : The AUC and AP scores are calculated using the corresponding functions in the scikit-learn package . We will include the definitions in the appendix and will refer to scikit-learn . The AP scores are correct . They increase with c because the size of the outliers ( that is , the size of the \u201c Positive \u201d class ) increases with c. To clarify this issue , let \u2019 s assume a dataset with 2n points ( where n > 1 ) with a single inlier and ( 2n-1 ) outliers and suppose an algorithm for anomaly detection that returns a similarity score ( as mentioned in line 16 of Algorithm 1 ) of 0.5 for all inliers , 0.9 for ( n-1 ) of the outliers and 0.1 for the other n outliers . Then AUC = $ n/ ( 2n-1 ) $ while AP = $ 1 - ( n-1 ) / ( 4n^2-2n ) $ . AUC decreases with n and AP increases with n. * The results should include the case where the training data is not contaminated with outliers ( c=0 ) . This would correspond to the semi-supervised scenario and it would be very interesting to see how the method compares to DAGMM and GT which are build for that scenario . \u201d Response : Our method was designed for the unsupervised setting . Indeed , RSRAE tries to extract the main structure from data in the presence of outliers without any information on the inliers . The main issue of the semi-supervised setting case is to learn the best model for the training data and use it for identifying outliers . If we use our method in a semi-supervised setting , then we only use its AE component without taking advantage of the RSR layer . This is not special to our method and similar results can be obtained by other autoencoders . Also , generative models may be advantageous for this problem . \u201c * It would be interesting to see the effect of varying the subspace dimension . The authors chose 10 for all experiments , why is this number chosen , what would be the effect of choosing a smaller one ? This is a key parameter as it defines the structure of the projection subspace . Should this parameter be systematically tuned for each dataset ? \u201d Response : We plan to add some experiments addressing the subspace dimension . We chose 10 because we were interested in a single parameter that may work for a wide range of real datasets , and we generally noticed some stability to changes of this dimension . However , we agree that we need to report the experiments that demonstrate this stability ."}], "0": {"review_id": "rylb3eBtwr-0", "review_text": "This paper proposes to use the robust subspace recovery layer (RSR) in the autoencoder model for unsupervised anomaly detection. This paper is well written overall. Presentation is clear and it is easy to follow. The proposed approach is a simple combination of existing approaches. Although its theoretical analysis with respect to the performance of anomaly detection is limited, experiments show that the proposed method is effective and superior to the existing anomaly detection methods. I have the following comments: - Parameter sensitivity should be examined. The proposed method has the number of parameters including \\lambda_1, \\lambda_2, and parameters in neural networks. Since parameter tuning is fundamentally difficult in the unsupervised setting, the sensitivity of the proposed method with respect to changes of such parameters should be examined. - Since the efficiency is also an important issue for anomaly detection methods, runtime comparison would be interesting. - It would be also interesting whether the proposed method is also effective for non-structured data, where a dataset is given as just a set of (real-valued) feature vectors, with its comparison to the standard anomaly detection methods such as LOF and iForest. ", "rating": "8: Accept", "reply_text": "Thank you for your comments and constructive suggestions . We respond before the deadline , so we may communicate if needed . \u201c The proposed approach is a simple combination of existing approaches . \u201d Response : While AE and RSR are previous well-known approaches , RSRAE is not a simple combination of both of them . They are not two independent components that are combined together . In fact , we show that a simple combination of a least absolute deviations energy used in RSR and an AE , which we refer to AE-1 , does not improve the performance of AE for anomaly detection . The RSR layer is between the encoder and decoder of the AE and it is optimized together with the autoencoder and not separately from it . We are planning to further clarify why this layer should be effective and what makes it unique . \u201c - Parameter sensitivity should be examined . The proposed method has the number of parameters including \\lambda_1 , \\lambda_2 , and parameters in neural networks . Since parameter tuning is fundamentally difficult in the unsupervised setting , the sensitivity of the proposed method with respect to changes of such parameters should be examined . \u201d Response : We promoted the use of alternating minimization , which does not require predetermined $ \\lambda $ \u2019 s . That is , the values of $ \\lambda_1 $ and $ \\lambda_2 $ are not relevant for the algorithm we advocated . They are only relevant to RSRAE+ , but we do not advocate this algorithm due to the time it takes to test different values of lambda . Nevertheless , following this comment we will test the sensitivity of RSRAE+ to $ \\lambda_1 $ and $ \\lambda_2 $ . We also agree that it is important to test the sensitivity with respect to the learning rate and the dimension of the subspace , and we will report it in the revised version . \u201c - Since the efficiency is also an important issue for anomaly detection methods , runtime comparison would be interesting . \u201d Response : We plan to have a runtime comparison in the revised version . \u201c - It would be also interesting whether the proposed method is also effective for non-structured data , where a dataset is given as just a set of ( real-valued ) feature vectors , with its comparison to the standard anomaly detection methods such as LOF and iForest . \u201d Response : When we tested our method on the deep features of Imagenet and the features of Reuters and 20 Newsgroups , we worked with a set of feature vectors . In those examples , we didn \u2019 t treat the features like images or sequences , and the network was simply a fully-connected network instead of a convolutional one . Please let us know if we missed anything ."}, "1": {"review_id": "rylb3eBtwr-1", "review_text": "After reading all the reviews and the comments, I feel more positive about the paper. I appreciate the feedback of the Authors and I have decided to increase the rating. ============================ The paper proposes using Robust Subspace Recovery in combination with an autoencoder (and possibly GANs) for anomaly detection. The encoder maps input data to the latent space of dimensionality D, which then is linearly projected to a subspace of dimensionality d (d < D). The projection of the latent space then goes to a decoder that reconstructs the input. A transformation matrix A is trained jointly with the autoencoder. Two additional terms are added to the loss: one to encourage the subspace of A^TA to approximate the latent space z and the second one to force it to be an orthogonal projector. The paper claims to generalize the existing RSR framework to the nonlinear case. However, the linear RSR is applied to the latent space of the autoencoder. In addition to that, all the following discussion and proofs are limited to the linear case. Since the proposed method is using RSB as it\u2019s core part, and claims to be a non-linear extension of it, it would be crucial to have a comparison with RSB, at least on those experimental setups, where high-level features are used (Tiny Imagenet with ResNET features, Reuters-21578, and 20 Newsgroups). However, there is no such comparison. Since autoencoders can potentially learn any, arbitrary entangled latent space, it is not clear why outliers should necessarily have such embedding that is outside of the learned subspace. In the case of the original RSR it happens due to the dimensionality reduction by the orthogonal projector. However, autoencoders already perform dimensionality reduction at each layer down to the bottleneck layer. The matrix A and the parameters of the AE are trained jointly. So, it can be seen that two processes can occur: - The AE in order to minimize the reconstruction error would learn such latent space z, that would fit into the subspace of A^TA, so that projection \\tilde z =Az doesn\u2019t cause data loss. - The AE in order to minimize the reconstruction error would learn such A, so that the subspace that z approximates is the best possible. It is not clear, which of the two cases would take place. If the first one would dominate, then it is not clear if such method would have any discriminating capabilities. My point is mainly that the presented work is not really a generalization of RSR as it claims to be, but rather it is just using RSR on a leaned embedding of the data. Some citations are missing, as well as it is missing a comparison to some state-of-the art methods such as OCNN \u2018Robust, Deep and Inductive Anomaly Detection\u2019 ECML 2017; \u2018Adversarially Learned One-Class Classifier for Novelty Detection\u2019 CVPR 2017; DSVDD \u2018Deep one-class classification.\u2019 ICML, 2018; ODIN \u2018Enhancing The Reliability Of Out-of-distribution Image Detection In Neural Networks\u2019 ICLR 2018; \u2018Generative Probabilistic Novelty Detection with Adversarial Autoencoders\u2019 NeurIPS 2018. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your comments . We respond before the deadline , so we may communicate if needed . \u201c The paper claims to generalize the existing RSR framework to the nonlinear case. \u201d Response : Our paper does not claim to generalize the RSR framework to a nonlinear one , but it combines ideas from RSR within an autoencoder in order to make the autoencoder more robust to outliers . The mere use of a \u201c robust metric \u201d ( such as the least absolute deviations ) for an autoencoder is not sufficiently robust to anomalies and this is why we find an idea like ours natural . We believe that the above wrong interpretation of our paper is due to the following two sentences in Section 5 : \u201c Goodfellow et al . ( 2016 ) exemplified how PCA can be structured as a linear autoencoder . Similarly , RSR can be directly used to form an outliers-robust linear autoencoder and our current work generalizes this basic idea to a nonlinear setting \u201d . The first claim here is that similarly to having an autoencoder for PCA ( with linear encoder and decoder and least squares minimization ) , one can have an autoencoder for RSR ( with linear encoder and decoder and absolute deviations minimization ) . Note that the difference between a PCA autoencoder and an RSR autoencoder is obtained by changing the minimization from least squares to absolute deviations ( where it is also natural to introduce a simple normalization in the implementation part ) . On the other hand , changing the metric of a general autoencoder , which corresponds to \u201c nonlinear PCA \u201d , from least squares to absolute deviations does not make it robust ( especially as ideas of simple normalizations for adversarial outliers do not work out in this case ) . We demonstrated this in our experiments ( see performance of AE-1 vs. RSRAE in Section 4.3 and Appendix F2 ) . We find that the RSR layer is a very natural idea to make an autoencoder robust . By writing \u201c generalizes this basic idea to a nonlinear setting \u201d , we meant extending the autoencoder to be robust to outliers , in analogy to making a PCA autoencoder robust by changing it into an RSR ( linear ) autoencoder , but we did not mean using the same method as in the latter one and formally generalizing RSR to the nonlinear case . The purpose of these two sentences was only to motivate the rigorous theory for WGAN . We considered them as part of a minor comment , which the expert can easily figure out . There was no claim about generalizing RSR to the nonlinear case in the introduction or the conclusion . Due to the confusion , we will extend and rewrite this part with a careful explanation how our method is expected to make an autoencoder more robust to outliers , and why the mere change of a metric is not sufficient to make the method robust . Anyway , we don \u2019 t find these new details necessary for understanding the paper , but they may avoid a similar confusion . \u201c However , the linear RSR is applied to the latent space of the autoencoder. \u201d Response : The linear layer is in the middle of an autoencoder and an RSR loss is part of the total loss function . It is not correct to think of two separate entities : an autoencoder and a linear RSR applied to the latent space of the autoencoder . That is , we do not try to first get the latent code form AE and then apply RSR to the latent code . Note that a general autoencoder tries to parametrize the structure of the data with a latent code within a low-dimensional linear space . Here we try to parametrize only the structure of the inliers , where the low-dimensional subspace lies within the ambient space of the encoder , we then map the output of the encoder onto a low-dimensional space corresponding to the latter subspace . This mapping is natural to the inliers , but not to the outliers , which are expected to have high reconstruction error ."}, "2": {"review_id": "rylb3eBtwr-2", "review_text": "This paper adapts the concept of Robust Subspace Recovery (RSR) as a layer in an auto-encoder model for anomaly detection. A loss function is proposed that combines reconstruction error and a regularizer that enforces robustness against outliers. The reconstruction error expresses the accuracy of the nonlinear dimensionality reduction imposed by the autoencoder. The regularizer is the sum of absolute deviations from the latent subspace that represents a linear structure robust against outliers. An alternative procedure is applied where the loss terms are applied iteratively during training. Once trained, the reconstruction error is used directly for anomaly detection with a threshold. The AUC is used as a performance measure. The method is compared against 6 other methods (LOF, OCSVM, IF, DESBM, GT, DAGMM). The setting is fully unsupervised, meaning that the training data contains various amounts of anomalies, and the results are parametrized with the amount of corruption. The results show that the proposed approach outperforms the other methods in most cases, especially for larger amounts of corruption. An ablation study compares the approach with auto-encoder-only and a non-alternating gradient descent (fixed factors for each part of the loss function) and shows that the alternating method outperfroms all by a wide margin. PROS: * A novel approach to fully unsupervised anomaly detection that beats the state of the art. * The RSR layer is a simple fully connected layer and the loss function is simple to calculate, making the approach computationally efficient. * A pseudo-code algorithm is provided in the appendix, which should help reproducibility. * The paper is well written and the math is clearly laid out. * The result benchmarks are sufficiently exhaustive in both the methods that are compared and the datasets used. * The ablation study is informative and shows the effect of the regularization term of the loss function as well as the effect of alternating the gradient descent with the separate losses. CONS: * There is a serious problem in the results (Figure 1) as the AP curves show better scores for larger corruption factors. Are the AP-score graphs flipped ? Please explain. * The AUC and AP scores need to be defined. * The results should include the case where the training data is not contaminated with outliers (c=0). This would correspond to the semi-supervised scenario and it would be very interesting to see how the method compares to DAGMM and GT which are build for that scenario. * It would be interesting to see the effect of varying the subspace dimension. The authors chose 10 for all experiments, why is this number chosen, what would be the effect of choosing a smaller one ? This is a key parameter as it defines the structure of the projection subspace. Should this parameter be systematically tuned for each dataset ? Overall this is a good paper proposing a novel approach to fully unsupervised anomaly detection with state-of-the art results. ", "rating": "8: Accept", "reply_text": "Thank you for your comments and constructive suggestions . We respond before the deadline , so we may communicate if needed . \u201c * There is a serious problem in the results ( Figure 1 ) as the AP curves show better scores for larger corruption factors . Are the AP-score graphs flipped ? Please explain . * The AUC and AP scores need to be defined . \u201d Response : The AUC and AP scores are calculated using the corresponding functions in the scikit-learn package . We will include the definitions in the appendix and will refer to scikit-learn . The AP scores are correct . They increase with c because the size of the outliers ( that is , the size of the \u201c Positive \u201d class ) increases with c. To clarify this issue , let \u2019 s assume a dataset with 2n points ( where n > 1 ) with a single inlier and ( 2n-1 ) outliers and suppose an algorithm for anomaly detection that returns a similarity score ( as mentioned in line 16 of Algorithm 1 ) of 0.5 for all inliers , 0.9 for ( n-1 ) of the outliers and 0.1 for the other n outliers . Then AUC = $ n/ ( 2n-1 ) $ while AP = $ 1 - ( n-1 ) / ( 4n^2-2n ) $ . AUC decreases with n and AP increases with n. * The results should include the case where the training data is not contaminated with outliers ( c=0 ) . This would correspond to the semi-supervised scenario and it would be very interesting to see how the method compares to DAGMM and GT which are build for that scenario . \u201d Response : Our method was designed for the unsupervised setting . Indeed , RSRAE tries to extract the main structure from data in the presence of outliers without any information on the inliers . The main issue of the semi-supervised setting case is to learn the best model for the training data and use it for identifying outliers . If we use our method in a semi-supervised setting , then we only use its AE component without taking advantage of the RSR layer . This is not special to our method and similar results can be obtained by other autoencoders . Also , generative models may be advantageous for this problem . \u201c * It would be interesting to see the effect of varying the subspace dimension . The authors chose 10 for all experiments , why is this number chosen , what would be the effect of choosing a smaller one ? This is a key parameter as it defines the structure of the projection subspace . Should this parameter be systematically tuned for each dataset ? \u201d Response : We plan to add some experiments addressing the subspace dimension . We chose 10 because we were interested in a single parameter that may work for a wide range of real datasets , and we generally noticed some stability to changes of this dimension . However , we agree that we need to report the experiments that demonstrate this stability ."}}