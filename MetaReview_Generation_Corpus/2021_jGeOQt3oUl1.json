{"year": "2021", "forum": "jGeOQt3oUl1", "title": "Representational aspects of depth and conditioning in normalizing flows", "decision": "Reject", "meta_review": "The paper studies three aspects of the representational capabilities of normalizing flows, with a particular focus on affine coupling layers. Normalizing flows are valuable generative-modelling tools, so advancing our understanding of their theoretical properties is an important research direction.\n\nReviewers #2 and #4 found the contribution of the paper significant without expressing major concerns, and so recommended acceptance.\n\nReviewer #3 reviewed the paper very thoroughly, and expressed some concerns mainly about the experimental evaluation. Most of their concerns were addressed in the rebuttal, so they recommended weak acceptance, recognizing the merits of the paper but also pointing out the potential for improvement.\n\nReviewer #1 was the most critical: they expressed major concerns regarding the significance of the contributions and the overall clarity of the exposition. Despite a long exchange between the reviewer and the authors, a consensus was not reached, so the concerns remain.\n\nThe discussion so far has led me to believe that there are potentially valuable theoretical contributions in the paper, however it's clear that there is significant room for improvement in getting the contributions across. Given the strong concerns expressed, the lack of consensus, and the clear potential for improvement, I'm unable to recommend acceptance of the paper in its current form. However, I do believe that the work has potential, and I hope that the discussion here will help improve the paper for a future submission.", "reviews": [{"review_id": "jGeOQt3oUl1-0", "review_text": "* * Summary * * This paper studies theoretical properties of flow models , mainly focussing on affine coupling layers . The authors investigate several questions on the representational capacity of flow models , focusing on the role of flow depth and the regularity/conditioning of the flow model . Their findings can be summarized as follows : 1 ) The authors show that an invertible flow model ( not necessarily of the affine coupling form ) that needs to match the generated data of a non-invertible generator needs to be substantially deeper than the non-invertible generator , while maintaining roughly the same amount of parameters as the non-invertible generator . 2 ) The authors show that any linear invertible map can be learned with a constant number of affine coupling layers with a fixed partitioning . Therefore , approximations for learnable permutation layers such as the 1x1 convolutions of GLOW can be replaced by increasing the size of the flow network with a constant factor . 3 ) The authors show that if the Jacobian of affine flow models can be arbitrarily close to singular , affine flow models are universal approximators for distributions with bounded support . This results requires no zero padding of the input , as was the case of previous work [ 1 ] . 4 ) Finally , the authors explore the effect of zero padding or gaussian padding on 2d toy examples , and find that gaussian padding leads to better matching of the data distribution and a better condition number . * * Pros * * * The paper presents interesting fundamental questions on the representational capacity of normalizing flows , which can help practitioners in their design choices of normalizing flows . * The authors clearly made an effort to try to make the proof sketches accessible . However , the proof sketches and the relation between theorems can be made much clearer . Improving along this axis would make the paper much more accessible to a wider audience . * The authors empirically validate some of their theoretical results . * * Cons * * * There is quite some room for improvement on the side of the empirical results . - First , empirical results are obtained by optimizing with an objective that is not maximum likelihood but a regression objective . This makes the transferability of some of the results to flows trained with maximum likelihood ( almost always the case ) questionable . For instance , will the results on the different padding strategies in section 6.1 and appendix C2 still hold for maximum likelihood trained models ? The argument for using the regression loss in 5.3 instead of maximum likelihood is also quite vague \u201c ... to minimize algorithmic ( training ) effects as the theorems are focusing on the representational aspects. \u201d Does that mean that if you train with maximum likelihood you don \u2019 t obtain the same empirical results ? - Second , the results for learning an invertible linear mapping with affine flow models is only done for a very particular construction of invertible linear mappings : the elements of the invertible matrix are sampled from a standard Gaussian . For this particular case it indeed seems that the practical number of affine coupling layers needed to approximate an invertible linear mapping is lower than the suggested upper bound ( of 47 ) . However , it is unclear if this observation extends to other invertible mappings that are not random Gaussian matrices . - Third , the empirical evaluation in section 5.3 could also be done with nonlinear affine coupling layers to show how this affects the practical results relative to the bounds that are claimed to also hold for this type of affine coupling layers . - Finally , the conclusion that Gaussian padding works better in practice than zero padding is based only on experiments with two-dimensional toy examples . In higher dimensional cases such as image datasets it is unclear if this still holds , as dimensionality can potentially play an important role here . * The exposition and conditions for some of the theorems and how they relate to one another could be made clearer . For instance , how does the universal approximation without padding theorem ( theorem 4 ) relate to the additional remark in section 5.3 , where it is described that for a distribution generated by applying an elementwise tanh on a Gaussian random variable , this distribution can not be modeled with a finite number of affine coupling layers ? I appreciate that sections 4 , 5 , and 6 try to sketch the proofs of the theorems to give more insight to the reader . However , the proof sketches are not much clearer than the actual proofs in the appendices . I imagine some of the explanations could be made more accessible with visualizations . It would also be helpful for instance to relate theorem 1 and theorem 4 and their relationship . Some of the details of the theorems are a bit confusing : * In Theorem 2 , why is only the case det ( T ) > 0 considered ? Could it also hold for invertible matrices with det ( T ) < 0 such as a permutation with determinant equal to -1 ? Should in that case the diagonal matrices B and C have entries smaller than zero ? * I don \u2019 t understand the paragraph directly below theorem 3 . Nonlinear affine coupling layers ( where the nonlinearity refers to the nonlinearity of the scale and translation networks ) are more flexible than linear affine coupling layers , so I would expect that theorem 2 can indeed also hold for nonlinear affine coupling layers , but that in theorem 3 the bound could be different because the nonlinear version is more flexible . * The counting argument for the lower bound of K > =5 in theorem 3 is presented in a confusing way . In section 5.2 , the number of parameters of a single coupling layer is indeed $ d^2 + d $ , so a product of $ K $ coupling layers ( either upper or lower triangular ) is $ K ( d^2 + d ) $ , and you would indeed expect 4 coupling layers to match the $ 4d^2 $ parameters of the linear map . But the product terms in theorem 2 always consist of 2 coupling layers ( one lower triangular $ \\in \\mathcal { A_L } $ and one upper triangular coupling layer $ \\in \\mathcal { A_U } $ ) . So there the amount of parameters for a product of K terms would be $ 2K ( d^2+d ) $ , leading to K=2 as an estimated sufficient amount based on parameter count alone , unless you assume that only one of the two coupling layers in each term is not the identity . I assume that the latter is the case as it would give you the freedom to represent the sequence of coupling layers as arbitrary combinations of upper and lower triangular coupling layers . * In Theorem 1 , if g is not invertible , how can we be sure that the distribution induced by the non-invertible mapping g is a valid distribution ? The cons currently outweigh the pros for me , leading to the rating of 5 , but I do think this paper could be a valuable contribution to the normalizing flow community . Therefore , I hope the exposition can be made a little clearer during the revision period so I can raise my score . * * Minor comments/questions * * * Normalizing flows don \u2019 t need to start from Gaussian latent distributions as stated in the first paragraph of the introduction . * Seems like $ \\sigma $ is used both for the activation function of layerwise invertible feedforward networks as well as the standard deviation for the gaussian distributions in theorem 1 . These type of double usages make the paper less readable . * Should n't the answer to question 1 be affirmative since you are confirming that such a distribution exists ? * in the paragraph below theorem 1 , should $ \\Theta ( k ) $ and $ \\Theta ( d^2 ) $ be $ O ( k ) $ and $ O ( d^2 ) $ ? * In definition 1 , the scale and translation parameters of affine coupling layers are indicated with $ g $ and $ h $ respectively , but in some parts of the text , the authors refer to $ s $ and $ t $ for these factors ( for instance just below theorem 3 , and theorem 4 ) . Please keep this consistent . * Please include masked autoregressive flows in related work on generative normalizing flows [ 2 ] . * The related work in between the sections introducing the theorems and the proof sketches interrupts the flow of reading a little . * Second paragraph page 6 : typo \u201c stanard \u201d \u2192 \u201c standard \u201d . * Just below lemma 6 , should it be $ \\mathcal { A_L A_U A_L A_U } $ instead of $ \\mathcal { A_L A_R A_L A_R } $ . * In appendix C2 , the data distribution of the swiss roll is depicted as the two moons data distribution . [ 1 ] Huang et al.2020 , augmented normalizing flows : bridging the gap between generative flows and latent variable models . https : //arxiv.org/abs/2002.07101 [ 2 ] Papmakarios et al. , masked autoregressive flow for density estimation . https : //arxiv.org/abs/1705.07057", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and comments . We hope to use them to greatly improve the final paper . We appreciate your positive feedback and will address your concerns roughly in order . Re : empirical results -- to clarify , only our linear function experiments were trained with regression . There our theorem was strictly concerned with representational power results from Theorems 2 and 3 . The regression objective is easier to optimize ( since it \u2019 s supervised ) , and isolates checking representational power ( as opposed to the influence of the extra layers on training dynamics for likelihood ) . Moreover , the likelihood objective is invariant up to a rotation when the function is linear , so it \u2019 s hard to measure the error of the learned network . In 6.1 we are in fact training by maximum likelihood . We \u2019 ve added a note to the paper clarifying this . We agree that the case of Gaussian matrices is a special case and we added a similar experiment with random Toeplitz matrices in Appendix C.1 in the updated version of our paper ( Figures 9-13 ) . Of course , whichever distribution of matrices we run experiments with -- the ultimate adjudication can only come by proving our bounds are tight ( i.e.either finding a matrix which realizes the 47-matrix bound or improving the upper bound ) . Closing this gap is an interesting problem . We also added experiments which investigate whether adding nonlinearity to the affine couplings affects the number of layers needed , as suggested by the reviewer -- the resulting plots have been attached as a supplementary zip . Re : higher dimensional datasets -- we agree that it would be desirable to investigate the performance of padding on higher-dimensional datasets . Unfortunately , models which can produce high quality samples on image data so far are very large ( see e.g.GLOW ) which makes it difficult to perform full experiments at this scale . This is the reason we , like * * many * * other papers in this area , are running low-dimensional experiments . Re : relating the theorems -- we added some additional exposition relating the theorems . In particular , our comment on tanh is that it is not exactly representable but Theorem 4 shows it is approximately representable ( with ill-conditioned models ) . If you have additional comments on what to clarify with respect to the relationships between the theorems we are happy to oblige ! Re : det ( T ) > 0 -- the comment on Theorem 2 is correct , we only consider maps with det ( T ) > 0 as affine coupling models are orientation-preserving . If we were to add a diagonal layer with negative signs allowed , we would immediately recover the whole group . Re : the relationship between nonlinear models and Theorems 2 and 3 -- we note that though nonlinear models are more expressive than linear models , they both have Jacobians which are linear and of the form given . Therefore , the theorem still constrains what functions would be possible in the nonlinear setting . Re : inconsistencies in naming/counting -- Thank you for pointing out the inconsistencies in counting and naming matrices and coupling networks . We have corrected them in the paper . We also added the citation you noted . We hope we \u2019 ve been able to address your questions and concerns here , as well as with our updates to the paper ."}, {"review_id": "jGeOQt3oUl1-1", "review_text": "The normalizing flows ( NF ) are among popular generative models , as they have the capability of converting a simple base distribution to complex distributions by successively applying change of variable formula . More importantly , NFs let us do inference by maximizing exact likelihood functions instead of other approximate functions like ELBO in VAEs . NFs have the invertibility constraints which make the calculations of their Jacobean determinants computationally prohibitive and require some tricks that make them easier to compute . All these tricks , among them affine coupling layers are of great popularity , come at the price of losing expressiveness . Therefore , we might need more layers ( deep ) to compensate for that . However , we lack a theoretical understanding of how much deep is enough to guarantee best results . This paper provides many useful and insightful theorems which sheds light on these kind of questions in using NFs . It specifically studies the affine coupling layers and provides an upper bound and a lower bound on the depth of the layers required to achieve a good performance . Then it investigates the effects of zero padding in the inputs of the NFs and shows that the reason that the NFs can not work well with zero padding is actually related to poor conditionings of Jacobeans during the trainings . Although this paper does not answers all the issues of the NFs , I think providing some interesting theorems on even simple questions , such as the effect of depth , could shed light on other questions in this field for other researchers . I have the following comments for this paper : 1- I think the formatting of this paper is wider than usual ones , which makes the reading difficult and it seems more compact . I think the authors should double check it to make it coherent with the other papers . 2- Although this is a theoretical paper , the number of experiments are limited . I believe that adding extra experiments can better reflect the value of the paper .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the positive comments ! Re : the formatting -- we apologize for the inconvenience -- we were allowed to fix the margins by the chairs so long as the length ends up < 9 pages . ( Which it does . ) Re : experiments : We did , as you noted , run some experiments verifying some of our theorems . If you have particular experiments to suggest that would help support the results and thrust of our paper , please let us know !"}, {"review_id": "jGeOQt3oUl1-2", "review_text": "The paper gives very thorough mathematical representation for two challenges related to normalization flows , namely model \u2019 s large depth and conditioning which relates to the smallest singular value of the forward map . Topic is presented in a very orderly and comprehensive manner . All variables and concepts are explained and presentation is clear . Text and appendices give proofs for everything that has been discussed and appendices extensive presentation of experiments . The cons of the paper are that it positions itself for previous research quite loosely . The paper would be a good handbook on the mathematics of the subject , but it is unclear at which level the topic has been addressed in the previous research , although the related work is presented in a short section . Also , it would be interesting to discuss how the obtained results could be considered when e.g.modelling the complex distributions .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the positive comments ! We have added a few more references to the related literature , which hopefully gives you a better sense of the position of our paper relative to them . Since space is limited , we kept our literature survey constrained to strictly relevant papers ( a lot of which are mathematical in nature , since so is our paper ) . If there are some particular paper ( s ) you think we missed , we are happy to add them/discuss the relationship to our results ."}, {"review_id": "jGeOQt3oUl1-3", "review_text": "# # # # DESCRIPTION This paper considers the representational ability of normalizing flows in terms of their overall size ( depth , no.of parameters etc ) and how they choose a partition for coupling layer transformations . # # # # DISCUSSION While I think exploring explicit limitations of the representational ability of normalizing flows imposed by the invertible constraints and bipartite partitioning of coupling layers , I 'm not sure that ( i ) the specifics discussed in this paper get to the heart of the matter , and ( ii ) the points that * are * made are hard to understand and stand behind . Specifically , I have the following main concerns : `` Empirically , these models seem to require a much larger size than other generative models ( e.g.GANs ) and most notably , a much larger depth . '' -- what does it mean for one generative model to be `` bigger '' than another , or to have `` larger depth '' ? Is it number of parameters , or memory cost ? The number of transformations in a flow and the number of layers in a GAN are not really comparable . You could also argue that normalizing flows could be adapted to have a memory cost constant in their depth , since their activations can be reconstructed on the fly for backprop . `` Question 1 '' -- the answer to this is yes , example : in R^2 , the function f ( x1 , x2 ) = ( x1 , x1 ) ( where x = ( x1 , x2 ) ~ N ( O , I ) ) induces a distribution on the line x2 = x1 which can not be represented by an invertible mapping . What you really mean to discuss is * to what extent * enforcing invertibility impacts representational ability , not * whether * enforcing invertibility impacts representational ability , because it does . I am finding it very difficult to understand what Theorem 1 is saying , other than that it 's in some sense trying to formalize the fact the invertible functions impose representational constraints . Specifically : What does it mean for a network to have size O ( k ) ? What is k ? It seems to be defined as k = o ( exp ( d ) ) ? What does that mean ? What does it mean for the depth to be k / p = o ( exp ( d ) ) / no . of parameters ? Why is the number of parameters per layer p the same for every layer ? It seems to me as if the statements here are vague and not well-defined . `` Question 2 '' -- I do n't understand the point of this question . Why is it worthwhile to have a result about how many affine coupling layers with a fixed partition are needed to compensate for the removal of a 1 x 1 convolution ? From the Glow paper , the 1 x 1 convolution added increased the parameter count of their model by 0.2 % , and the wallclock time for training increased by approximately 7 % . Neither of these are prohibitive costs which would warrant careful examination of the necessity of the 1 x 1 convolution . Like Theorem 1 , I 'm also not sure of the meaning/point of Theorem 2 . In particular , `` any applications of permutations to achieve a different partition of the inputs can in principle be represented as a composition of not-too-many affine coupling layers . '' How many is not-too-many ? Even if we could be specific , why does it matter that we can compensate for the removal of 1 x 1 convolutions with more affine coupling layers ? Sections 4 & 5 & 6 : I 'm finding it very difficult to parse what 's going on here . The sections are technically dense , draw on a wide range of formal results in passing , and it 's hard for me to understand how the material is relevant or necessary . Moreover , why are there extensive proof 'sketches ' in the main text at all ? Where are the actual proofs ? The experimental results are toy , and I 'm not entirely sure what they 're trying to show . What exactly is happening with the padding ? As in Huang et al ( 2020 ) , are the authors now doing variational inference in an extended space ? None of this is clear . It would be remiss of me not to mention the fact that the paper has altered the margins of the ICLR template to increase the amount of material . For better or worse , an 8-page conference paper is the widely used format for machine learning publications . While I 'm sympathetic that sometimes this can be an undue constraint , it provides a level playing field , and sharpening a message to fit within imposed limits is part of research . Deliberately altering the margins ( i ) makes the job of a reviewer more difficult because while reviewing they have to reason about which parts of the paper will be removed from the main text to fit in the final template , and ( ii ) removes the need for the authors to reflect on whether their message is focused enough . Content aside , the structure and layout of this paper could be significantly improved -- the abstract does not need to be multi-paragraph , the sweeping technical details are introduced quickly and poorly motivated , and I found the overall narrative confusing and difficult to follow . # # # # EXTRA NOTES `` we can efficiently evaluate the likelihood of a data point '' -- the likelihood is a function of the parameters , not data . `` which model distributions as pushforwards of a standard Gaussian '' -- the base distribution in a normalizing flow is not necessarily Gaussian . In section 2.2 , computational constraints are not necessarily the reason for not using invertible weight matrices and invertible pointwise nonlinearities -- the O ( d^3 ) determinant calculation can be sidestepped by parameterizing the weight matrix using e.g.an LU decomposition . # # # # CONCLUSION As I mentioned at the beginning , I think an examination of the representational limitations of normalizing flows in terms of their overall size and how they are affected by a chosen partitioning may be worthwhile , but I feel as if this paper does n't adequately address these questions , and what it does say is difficult to understand .", "rating": "3: Clear rejection", "reply_text": "Thank you for your review and comments on the paper . We \u2019 d like to first clarify two general points , then we will answer detailed queries . First , we structured the paper such that there is informal discussion to motivate the results presented , a summary ( sketches ) of the proof techniques used , and a full presentation of the proofs in the appendix . It seems several of your questions are comments and concerns about our informal section , treating it as if it is formal mathematics . This feels very uncharitable : we do actually present precise theorem statements and proofs to bear that scrutiny , but the informal discussion is less precise so as to be approachable , especially by readers who are not specialists in this area . Second , in theoretical study of deep learning ( and machine learning in general ) it is common to split analysis into a trichotomy of questions about representational power ( of a class of predictors , probabilistic models , etc . ) , statistical complexity ( e.g.quantities like VC dimension , Rademacher complexity , etc ) , and algorithmic complexity ( e.g.can training be done efficiently , does gradient descent get stuck in suboptimal local minima , etc . ) . We explicitly approach the first topic in this work . In works on representation it is typical to consider constraints to the class the model belongs to ( e.g.disallowing 1x1 convolutions ) and quantify how much bigger it would need to be made to account for this restriction . ( There is a long tradition of this in theoretical computer science and machine learning , specifically deep learning , e.g.how much bigger a shallow network needs to be to approximate a deeper network . ) We now proceed to answer individual questions . Re : size and depth -- in our paper , size is taken to mean the number of parameters ( i.e.trainable weights ) of a model . As we discuss in the abstract , the primary reason we care about * depth * is due to gradient vanishing / exploding ( equivalently , the conditioning of the Jacobian , which in general will also be worse for a deeper network ) . From this aspect , it \u2019 s fair to compare the number of layers in a generator with the number of affine couplings in a flow . In fact , since in Theorem 1 we are lower bounding the depth of the normalizing flow model in terms of number of transformations , each of which may further be deep , we are in fact being conservative ( the \u201c effective depth \u201d in terms of the Jacobian conditioning should be the number of transformations x depth of each of those transformations.We provide some \u201c back of the envelope \u201d calculations in the paragraph after Theorem 1 . ) We don \u2019 t consider algorithmic questions such as the memory complexity of training here . Re : Question 1 -- the question is part of the exposition , not a formal statement ( see our first general point ) . We added the phrase \u201c approximated by \u201d to avoid confusion with distributions with degenerate supports ( this is of course , a trivial obstacle for invertible architectures -- though only if we are aiming to * exactly * represent the distribution ) . The question additionally asks for a * depth * separation : namely , we want a distribution that can be written as the pushforward through a small ( but potentially non-invertible ) , shallow neural network , that can not even be approximated unless a much deeper invertible architecture is used . The questions regarding $ k $ seem to stem from the following confusion : $ k $ is a parameter . Writing $ k = \\exp ( o ( d ) ) $ just means `` for every $ k $ that isn \u2019 t too big ( i.e.k grows slowly with the exponential of d asymptotically ) , the result in Theorem 1 holds '' . It is n't a definition of k. This is standard notation , but we added a bit more exposition in the paper to this effect . The size of our networks being $ O ( k ) $ just means the number of parameters of the networks is $ O ( k ) $ . Finally , the number of parameters $ p $ per transform is an upper bound , and can be taken to be $ p = \\max_i p_i $ for layers with $ p_i $ parameters per layer . Re : Question 2 -- we note that having an additional type of layer gives another knob to tune when designing a network , so it is useful to know to what extent this knob is necessary . Our work is the first to show that these layers do not help much with respect to representational power ( see our second general point about ways to theoretically study questions in deep learning ) . Your comment seems to be focusing on the increase in train-time ( in terms of wall-clock time ) by adding some amount of 1x1 convolutions . This does not get at the issue at all of how much we are `` handicapping '' the representational power of the model when disallowing these layers . We also give a precise number ( 24 ) for \u201c not-too-many \u201d in the statement of Theorem 2 , additionally in the exposition just two lines under the \u201c not-too-many \u201d sentence . ( See our first general point about expository vs formal parts of our paper . )"}], "0": {"review_id": "jGeOQt3oUl1-0", "review_text": "* * Summary * * This paper studies theoretical properties of flow models , mainly focussing on affine coupling layers . The authors investigate several questions on the representational capacity of flow models , focusing on the role of flow depth and the regularity/conditioning of the flow model . Their findings can be summarized as follows : 1 ) The authors show that an invertible flow model ( not necessarily of the affine coupling form ) that needs to match the generated data of a non-invertible generator needs to be substantially deeper than the non-invertible generator , while maintaining roughly the same amount of parameters as the non-invertible generator . 2 ) The authors show that any linear invertible map can be learned with a constant number of affine coupling layers with a fixed partitioning . Therefore , approximations for learnable permutation layers such as the 1x1 convolutions of GLOW can be replaced by increasing the size of the flow network with a constant factor . 3 ) The authors show that if the Jacobian of affine flow models can be arbitrarily close to singular , affine flow models are universal approximators for distributions with bounded support . This results requires no zero padding of the input , as was the case of previous work [ 1 ] . 4 ) Finally , the authors explore the effect of zero padding or gaussian padding on 2d toy examples , and find that gaussian padding leads to better matching of the data distribution and a better condition number . * * Pros * * * The paper presents interesting fundamental questions on the representational capacity of normalizing flows , which can help practitioners in their design choices of normalizing flows . * The authors clearly made an effort to try to make the proof sketches accessible . However , the proof sketches and the relation between theorems can be made much clearer . Improving along this axis would make the paper much more accessible to a wider audience . * The authors empirically validate some of their theoretical results . * * Cons * * * There is quite some room for improvement on the side of the empirical results . - First , empirical results are obtained by optimizing with an objective that is not maximum likelihood but a regression objective . This makes the transferability of some of the results to flows trained with maximum likelihood ( almost always the case ) questionable . For instance , will the results on the different padding strategies in section 6.1 and appendix C2 still hold for maximum likelihood trained models ? The argument for using the regression loss in 5.3 instead of maximum likelihood is also quite vague \u201c ... to minimize algorithmic ( training ) effects as the theorems are focusing on the representational aspects. \u201d Does that mean that if you train with maximum likelihood you don \u2019 t obtain the same empirical results ? - Second , the results for learning an invertible linear mapping with affine flow models is only done for a very particular construction of invertible linear mappings : the elements of the invertible matrix are sampled from a standard Gaussian . For this particular case it indeed seems that the practical number of affine coupling layers needed to approximate an invertible linear mapping is lower than the suggested upper bound ( of 47 ) . However , it is unclear if this observation extends to other invertible mappings that are not random Gaussian matrices . - Third , the empirical evaluation in section 5.3 could also be done with nonlinear affine coupling layers to show how this affects the practical results relative to the bounds that are claimed to also hold for this type of affine coupling layers . - Finally , the conclusion that Gaussian padding works better in practice than zero padding is based only on experiments with two-dimensional toy examples . In higher dimensional cases such as image datasets it is unclear if this still holds , as dimensionality can potentially play an important role here . * The exposition and conditions for some of the theorems and how they relate to one another could be made clearer . For instance , how does the universal approximation without padding theorem ( theorem 4 ) relate to the additional remark in section 5.3 , where it is described that for a distribution generated by applying an elementwise tanh on a Gaussian random variable , this distribution can not be modeled with a finite number of affine coupling layers ? I appreciate that sections 4 , 5 , and 6 try to sketch the proofs of the theorems to give more insight to the reader . However , the proof sketches are not much clearer than the actual proofs in the appendices . I imagine some of the explanations could be made more accessible with visualizations . It would also be helpful for instance to relate theorem 1 and theorem 4 and their relationship . Some of the details of the theorems are a bit confusing : * In Theorem 2 , why is only the case det ( T ) > 0 considered ? Could it also hold for invertible matrices with det ( T ) < 0 such as a permutation with determinant equal to -1 ? Should in that case the diagonal matrices B and C have entries smaller than zero ? * I don \u2019 t understand the paragraph directly below theorem 3 . Nonlinear affine coupling layers ( where the nonlinearity refers to the nonlinearity of the scale and translation networks ) are more flexible than linear affine coupling layers , so I would expect that theorem 2 can indeed also hold for nonlinear affine coupling layers , but that in theorem 3 the bound could be different because the nonlinear version is more flexible . * The counting argument for the lower bound of K > =5 in theorem 3 is presented in a confusing way . In section 5.2 , the number of parameters of a single coupling layer is indeed $ d^2 + d $ , so a product of $ K $ coupling layers ( either upper or lower triangular ) is $ K ( d^2 + d ) $ , and you would indeed expect 4 coupling layers to match the $ 4d^2 $ parameters of the linear map . But the product terms in theorem 2 always consist of 2 coupling layers ( one lower triangular $ \\in \\mathcal { A_L } $ and one upper triangular coupling layer $ \\in \\mathcal { A_U } $ ) . So there the amount of parameters for a product of K terms would be $ 2K ( d^2+d ) $ , leading to K=2 as an estimated sufficient amount based on parameter count alone , unless you assume that only one of the two coupling layers in each term is not the identity . I assume that the latter is the case as it would give you the freedom to represent the sequence of coupling layers as arbitrary combinations of upper and lower triangular coupling layers . * In Theorem 1 , if g is not invertible , how can we be sure that the distribution induced by the non-invertible mapping g is a valid distribution ? The cons currently outweigh the pros for me , leading to the rating of 5 , but I do think this paper could be a valuable contribution to the normalizing flow community . Therefore , I hope the exposition can be made a little clearer during the revision period so I can raise my score . * * Minor comments/questions * * * Normalizing flows don \u2019 t need to start from Gaussian latent distributions as stated in the first paragraph of the introduction . * Seems like $ \\sigma $ is used both for the activation function of layerwise invertible feedforward networks as well as the standard deviation for the gaussian distributions in theorem 1 . These type of double usages make the paper less readable . * Should n't the answer to question 1 be affirmative since you are confirming that such a distribution exists ? * in the paragraph below theorem 1 , should $ \\Theta ( k ) $ and $ \\Theta ( d^2 ) $ be $ O ( k ) $ and $ O ( d^2 ) $ ? * In definition 1 , the scale and translation parameters of affine coupling layers are indicated with $ g $ and $ h $ respectively , but in some parts of the text , the authors refer to $ s $ and $ t $ for these factors ( for instance just below theorem 3 , and theorem 4 ) . Please keep this consistent . * Please include masked autoregressive flows in related work on generative normalizing flows [ 2 ] . * The related work in between the sections introducing the theorems and the proof sketches interrupts the flow of reading a little . * Second paragraph page 6 : typo \u201c stanard \u201d \u2192 \u201c standard \u201d . * Just below lemma 6 , should it be $ \\mathcal { A_L A_U A_L A_U } $ instead of $ \\mathcal { A_L A_R A_L A_R } $ . * In appendix C2 , the data distribution of the swiss roll is depicted as the two moons data distribution . [ 1 ] Huang et al.2020 , augmented normalizing flows : bridging the gap between generative flows and latent variable models . https : //arxiv.org/abs/2002.07101 [ 2 ] Papmakarios et al. , masked autoregressive flow for density estimation . https : //arxiv.org/abs/1705.07057", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review and comments . We hope to use them to greatly improve the final paper . We appreciate your positive feedback and will address your concerns roughly in order . Re : empirical results -- to clarify , only our linear function experiments were trained with regression . There our theorem was strictly concerned with representational power results from Theorems 2 and 3 . The regression objective is easier to optimize ( since it \u2019 s supervised ) , and isolates checking representational power ( as opposed to the influence of the extra layers on training dynamics for likelihood ) . Moreover , the likelihood objective is invariant up to a rotation when the function is linear , so it \u2019 s hard to measure the error of the learned network . In 6.1 we are in fact training by maximum likelihood . We \u2019 ve added a note to the paper clarifying this . We agree that the case of Gaussian matrices is a special case and we added a similar experiment with random Toeplitz matrices in Appendix C.1 in the updated version of our paper ( Figures 9-13 ) . Of course , whichever distribution of matrices we run experiments with -- the ultimate adjudication can only come by proving our bounds are tight ( i.e.either finding a matrix which realizes the 47-matrix bound or improving the upper bound ) . Closing this gap is an interesting problem . We also added experiments which investigate whether adding nonlinearity to the affine couplings affects the number of layers needed , as suggested by the reviewer -- the resulting plots have been attached as a supplementary zip . Re : higher dimensional datasets -- we agree that it would be desirable to investigate the performance of padding on higher-dimensional datasets . Unfortunately , models which can produce high quality samples on image data so far are very large ( see e.g.GLOW ) which makes it difficult to perform full experiments at this scale . This is the reason we , like * * many * * other papers in this area , are running low-dimensional experiments . Re : relating the theorems -- we added some additional exposition relating the theorems . In particular , our comment on tanh is that it is not exactly representable but Theorem 4 shows it is approximately representable ( with ill-conditioned models ) . If you have additional comments on what to clarify with respect to the relationships between the theorems we are happy to oblige ! Re : det ( T ) > 0 -- the comment on Theorem 2 is correct , we only consider maps with det ( T ) > 0 as affine coupling models are orientation-preserving . If we were to add a diagonal layer with negative signs allowed , we would immediately recover the whole group . Re : the relationship between nonlinear models and Theorems 2 and 3 -- we note that though nonlinear models are more expressive than linear models , they both have Jacobians which are linear and of the form given . Therefore , the theorem still constrains what functions would be possible in the nonlinear setting . Re : inconsistencies in naming/counting -- Thank you for pointing out the inconsistencies in counting and naming matrices and coupling networks . We have corrected them in the paper . We also added the citation you noted . We hope we \u2019 ve been able to address your questions and concerns here , as well as with our updates to the paper ."}, "1": {"review_id": "jGeOQt3oUl1-1", "review_text": "The normalizing flows ( NF ) are among popular generative models , as they have the capability of converting a simple base distribution to complex distributions by successively applying change of variable formula . More importantly , NFs let us do inference by maximizing exact likelihood functions instead of other approximate functions like ELBO in VAEs . NFs have the invertibility constraints which make the calculations of their Jacobean determinants computationally prohibitive and require some tricks that make them easier to compute . All these tricks , among them affine coupling layers are of great popularity , come at the price of losing expressiveness . Therefore , we might need more layers ( deep ) to compensate for that . However , we lack a theoretical understanding of how much deep is enough to guarantee best results . This paper provides many useful and insightful theorems which sheds light on these kind of questions in using NFs . It specifically studies the affine coupling layers and provides an upper bound and a lower bound on the depth of the layers required to achieve a good performance . Then it investigates the effects of zero padding in the inputs of the NFs and shows that the reason that the NFs can not work well with zero padding is actually related to poor conditionings of Jacobeans during the trainings . Although this paper does not answers all the issues of the NFs , I think providing some interesting theorems on even simple questions , such as the effect of depth , could shed light on other questions in this field for other researchers . I have the following comments for this paper : 1- I think the formatting of this paper is wider than usual ones , which makes the reading difficult and it seems more compact . I think the authors should double check it to make it coherent with the other papers . 2- Although this is a theoretical paper , the number of experiments are limited . I believe that adding extra experiments can better reflect the value of the paper .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the positive comments ! Re : the formatting -- we apologize for the inconvenience -- we were allowed to fix the margins by the chairs so long as the length ends up < 9 pages . ( Which it does . ) Re : experiments : We did , as you noted , run some experiments verifying some of our theorems . If you have particular experiments to suggest that would help support the results and thrust of our paper , please let us know !"}, "2": {"review_id": "jGeOQt3oUl1-2", "review_text": "The paper gives very thorough mathematical representation for two challenges related to normalization flows , namely model \u2019 s large depth and conditioning which relates to the smallest singular value of the forward map . Topic is presented in a very orderly and comprehensive manner . All variables and concepts are explained and presentation is clear . Text and appendices give proofs for everything that has been discussed and appendices extensive presentation of experiments . The cons of the paper are that it positions itself for previous research quite loosely . The paper would be a good handbook on the mathematics of the subject , but it is unclear at which level the topic has been addressed in the previous research , although the related work is presented in a short section . Also , it would be interesting to discuss how the obtained results could be considered when e.g.modelling the complex distributions .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the positive comments ! We have added a few more references to the related literature , which hopefully gives you a better sense of the position of our paper relative to them . Since space is limited , we kept our literature survey constrained to strictly relevant papers ( a lot of which are mathematical in nature , since so is our paper ) . If there are some particular paper ( s ) you think we missed , we are happy to add them/discuss the relationship to our results ."}, "3": {"review_id": "jGeOQt3oUl1-3", "review_text": "# # # # DESCRIPTION This paper considers the representational ability of normalizing flows in terms of their overall size ( depth , no.of parameters etc ) and how they choose a partition for coupling layer transformations . # # # # DISCUSSION While I think exploring explicit limitations of the representational ability of normalizing flows imposed by the invertible constraints and bipartite partitioning of coupling layers , I 'm not sure that ( i ) the specifics discussed in this paper get to the heart of the matter , and ( ii ) the points that * are * made are hard to understand and stand behind . Specifically , I have the following main concerns : `` Empirically , these models seem to require a much larger size than other generative models ( e.g.GANs ) and most notably , a much larger depth . '' -- what does it mean for one generative model to be `` bigger '' than another , or to have `` larger depth '' ? Is it number of parameters , or memory cost ? The number of transformations in a flow and the number of layers in a GAN are not really comparable . You could also argue that normalizing flows could be adapted to have a memory cost constant in their depth , since their activations can be reconstructed on the fly for backprop . `` Question 1 '' -- the answer to this is yes , example : in R^2 , the function f ( x1 , x2 ) = ( x1 , x1 ) ( where x = ( x1 , x2 ) ~ N ( O , I ) ) induces a distribution on the line x2 = x1 which can not be represented by an invertible mapping . What you really mean to discuss is * to what extent * enforcing invertibility impacts representational ability , not * whether * enforcing invertibility impacts representational ability , because it does . I am finding it very difficult to understand what Theorem 1 is saying , other than that it 's in some sense trying to formalize the fact the invertible functions impose representational constraints . Specifically : What does it mean for a network to have size O ( k ) ? What is k ? It seems to be defined as k = o ( exp ( d ) ) ? What does that mean ? What does it mean for the depth to be k / p = o ( exp ( d ) ) / no . of parameters ? Why is the number of parameters per layer p the same for every layer ? It seems to me as if the statements here are vague and not well-defined . `` Question 2 '' -- I do n't understand the point of this question . Why is it worthwhile to have a result about how many affine coupling layers with a fixed partition are needed to compensate for the removal of a 1 x 1 convolution ? From the Glow paper , the 1 x 1 convolution added increased the parameter count of their model by 0.2 % , and the wallclock time for training increased by approximately 7 % . Neither of these are prohibitive costs which would warrant careful examination of the necessity of the 1 x 1 convolution . Like Theorem 1 , I 'm also not sure of the meaning/point of Theorem 2 . In particular , `` any applications of permutations to achieve a different partition of the inputs can in principle be represented as a composition of not-too-many affine coupling layers . '' How many is not-too-many ? Even if we could be specific , why does it matter that we can compensate for the removal of 1 x 1 convolutions with more affine coupling layers ? Sections 4 & 5 & 6 : I 'm finding it very difficult to parse what 's going on here . The sections are technically dense , draw on a wide range of formal results in passing , and it 's hard for me to understand how the material is relevant or necessary . Moreover , why are there extensive proof 'sketches ' in the main text at all ? Where are the actual proofs ? The experimental results are toy , and I 'm not entirely sure what they 're trying to show . What exactly is happening with the padding ? As in Huang et al ( 2020 ) , are the authors now doing variational inference in an extended space ? None of this is clear . It would be remiss of me not to mention the fact that the paper has altered the margins of the ICLR template to increase the amount of material . For better or worse , an 8-page conference paper is the widely used format for machine learning publications . While I 'm sympathetic that sometimes this can be an undue constraint , it provides a level playing field , and sharpening a message to fit within imposed limits is part of research . Deliberately altering the margins ( i ) makes the job of a reviewer more difficult because while reviewing they have to reason about which parts of the paper will be removed from the main text to fit in the final template , and ( ii ) removes the need for the authors to reflect on whether their message is focused enough . Content aside , the structure and layout of this paper could be significantly improved -- the abstract does not need to be multi-paragraph , the sweeping technical details are introduced quickly and poorly motivated , and I found the overall narrative confusing and difficult to follow . # # # # EXTRA NOTES `` we can efficiently evaluate the likelihood of a data point '' -- the likelihood is a function of the parameters , not data . `` which model distributions as pushforwards of a standard Gaussian '' -- the base distribution in a normalizing flow is not necessarily Gaussian . In section 2.2 , computational constraints are not necessarily the reason for not using invertible weight matrices and invertible pointwise nonlinearities -- the O ( d^3 ) determinant calculation can be sidestepped by parameterizing the weight matrix using e.g.an LU decomposition . # # # # CONCLUSION As I mentioned at the beginning , I think an examination of the representational limitations of normalizing flows in terms of their overall size and how they are affected by a chosen partitioning may be worthwhile , but I feel as if this paper does n't adequately address these questions , and what it does say is difficult to understand .", "rating": "3: Clear rejection", "reply_text": "Thank you for your review and comments on the paper . We \u2019 d like to first clarify two general points , then we will answer detailed queries . First , we structured the paper such that there is informal discussion to motivate the results presented , a summary ( sketches ) of the proof techniques used , and a full presentation of the proofs in the appendix . It seems several of your questions are comments and concerns about our informal section , treating it as if it is formal mathematics . This feels very uncharitable : we do actually present precise theorem statements and proofs to bear that scrutiny , but the informal discussion is less precise so as to be approachable , especially by readers who are not specialists in this area . Second , in theoretical study of deep learning ( and machine learning in general ) it is common to split analysis into a trichotomy of questions about representational power ( of a class of predictors , probabilistic models , etc . ) , statistical complexity ( e.g.quantities like VC dimension , Rademacher complexity , etc ) , and algorithmic complexity ( e.g.can training be done efficiently , does gradient descent get stuck in suboptimal local minima , etc . ) . We explicitly approach the first topic in this work . In works on representation it is typical to consider constraints to the class the model belongs to ( e.g.disallowing 1x1 convolutions ) and quantify how much bigger it would need to be made to account for this restriction . ( There is a long tradition of this in theoretical computer science and machine learning , specifically deep learning , e.g.how much bigger a shallow network needs to be to approximate a deeper network . ) We now proceed to answer individual questions . Re : size and depth -- in our paper , size is taken to mean the number of parameters ( i.e.trainable weights ) of a model . As we discuss in the abstract , the primary reason we care about * depth * is due to gradient vanishing / exploding ( equivalently , the conditioning of the Jacobian , which in general will also be worse for a deeper network ) . From this aspect , it \u2019 s fair to compare the number of layers in a generator with the number of affine couplings in a flow . In fact , since in Theorem 1 we are lower bounding the depth of the normalizing flow model in terms of number of transformations , each of which may further be deep , we are in fact being conservative ( the \u201c effective depth \u201d in terms of the Jacobian conditioning should be the number of transformations x depth of each of those transformations.We provide some \u201c back of the envelope \u201d calculations in the paragraph after Theorem 1 . ) We don \u2019 t consider algorithmic questions such as the memory complexity of training here . Re : Question 1 -- the question is part of the exposition , not a formal statement ( see our first general point ) . We added the phrase \u201c approximated by \u201d to avoid confusion with distributions with degenerate supports ( this is of course , a trivial obstacle for invertible architectures -- though only if we are aiming to * exactly * represent the distribution ) . The question additionally asks for a * depth * separation : namely , we want a distribution that can be written as the pushforward through a small ( but potentially non-invertible ) , shallow neural network , that can not even be approximated unless a much deeper invertible architecture is used . The questions regarding $ k $ seem to stem from the following confusion : $ k $ is a parameter . Writing $ k = \\exp ( o ( d ) ) $ just means `` for every $ k $ that isn \u2019 t too big ( i.e.k grows slowly with the exponential of d asymptotically ) , the result in Theorem 1 holds '' . It is n't a definition of k. This is standard notation , but we added a bit more exposition in the paper to this effect . The size of our networks being $ O ( k ) $ just means the number of parameters of the networks is $ O ( k ) $ . Finally , the number of parameters $ p $ per transform is an upper bound , and can be taken to be $ p = \\max_i p_i $ for layers with $ p_i $ parameters per layer . Re : Question 2 -- we note that having an additional type of layer gives another knob to tune when designing a network , so it is useful to know to what extent this knob is necessary . Our work is the first to show that these layers do not help much with respect to representational power ( see our second general point about ways to theoretically study questions in deep learning ) . Your comment seems to be focusing on the increase in train-time ( in terms of wall-clock time ) by adding some amount of 1x1 convolutions . This does not get at the issue at all of how much we are `` handicapping '' the representational power of the model when disallowing these layers . We also give a precise number ( 24 ) for \u201c not-too-many \u201d in the statement of Theorem 2 , additionally in the exposition just two lines under the \u201c not-too-many \u201d sentence . ( See our first general point about expository vs formal parts of our paper . )"}}