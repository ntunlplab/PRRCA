{"year": "2017", "forum": "r1GKzP5xx", "title": "Recurrent Normalization Propagation", "decision": "Invite to Workshop Track", "meta_review": "Paper proposes a modification of batch normalization. After the revisions the paper is a much better read. However it still needs more diverse experiments to show the success of the method.\n \n Pros:\n - interesting idea with interesting analysis of the gradient norms\n - claims to need less computation\n \n Cons:\n - Experiments are not very convincing and only focus on only a small set of lm tasks.\n - The argument for computation gain is not convincing and no real experimental evidence is presented. The case is made that in speech domain, with long sequences this should help, but it is not supported.\n \n With more experimental evidence the paper should be a nice contribution.", "reviews": [{"review_id": "r1GKzP5xx-0", "review_text": "The authors show how the hidden states of an LSTM can be normalised in order to preserve means and variances. The method\u2019s gradient behaviour is analysed. Experimental results seem to indicate that the method compares well with similar approaches. Points 1) The writing is sloppy in parts. See at the end of the review for a non-exhaustive list. 2) The experimental results show marginal improvements, of which the the statistical significance is impossible to asses. (Not completely the author\u2019s fault for PTB, as they partially rely on results published by others.) Weight normalisation seems to be a viable alternative in the: the performance and runtime are similar. The implementation complexity of weight norm is, however, arguably much lower. More effort could have been put in by the authors to clear that up. In the current state, practitioners as well as researchers will have to put in more effort to judge whether the proposed method is really worth it for them to replicate. 3) Section 4 is nice, and I applaud the authors for doing such an analysis. List of typos etc. - maintain -> maintain - requisits -> requisites - a LSTM -> an LSTM - \"The gradients of ot and ft are equivalent to equation 25.\u201d Gradients cannot be equivalent to an equation. - \u201cbeacause\"-> because - One of the \u03b3x > \u03b3h at the end of page 5 is wrong. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your feedback . 1 ) We updated the paper with better English . 2 ) The main difference between the WN-LSTM and Norm-LSTM is the preservation of the variance through the recurrence . In both experiments that we conducted , we observed way better optimization ( see for example figure 2 for the training performances on the PTB experiment ) , and slightly better generalization , at an almost identical computational cost . Moreover , having normalized hidden states is also nice when dealing with stacks of LSTMs . WN doesn \u2019 t provide normalized hidden states , unless you perform a data dependent initialization ( which is arguably more complex to implement ) . We also plan to release the code soon . 3 ) Thank you !"}, {"review_id": "r1GKzP5xx-1", "review_text": "I think this build upon previous works, in the attempt of doing something similar to batch norm specific for RNNs. To me the experiments are not yet very convincing, I think is not clear this works better than e.g. Layer Norm or not significantly so. I'm not convinced on how significant the speed up is either, I can appreciate is faster, but it doesn't feel like order of magnitude faster. The theoretical analysis also doesn't provide any new insights. All in all I think is good incremental work, but maybe is not yet significant enough for ICLR.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments . About the experiments : We do agree that the experimental setup could be improved . About the speed up : Since we are still computing an LSTM , we can \u2019 t expect order of magnitude of speed up with such reparametrization , and can only wish for computation time close to the vanilla LSTM . From a computational point of view , the main difference between the BN-LSTM ( or LN-LSTM ) and the Norm-LSTM is that the BN-LSTM adds extra computations ( means and variances ) at each time step , where the Norm-LSTM only adds an overhead before the recurrence ( the normalization of the weight matrices ) . Thus for long sequences , such as in speech recognition , the Norm-LSTM has a serious advantage over the BN-LSTM : Their performances are similar , while the Norm-LSTM is almost as cheap computationally as the vanilla LSTM . About the theoretical analysis : In our opinion , the theoretical analysis provides good insights on the role of gamma_x with respect to gamma_h , showing how we can play with their initialization to bias the network into a more short or long term memory ."}, {"review_id": "r1GKzP5xx-2", "review_text": "The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well. The contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field. The contribution is not extremely novel: the change with respect to weight normalization is minor. The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better. Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments ! About data dependent initialization : As stated in the Weight Normalization paper , data dependent initialization can \u2019 t be used for RNNs , because of the recurrent nature of RNNs . Instead , they simply propose to initialize gamma to 1 , and we therefore use the same setup as in the Weight Normalization paper . About the novelty : We extended the Normalization Propagation framework to the LSTM , in order to be able to better preserve the variance through the recurrence ( which is not the case in Weight Normalization ) . We empirically showed that preserving the variance seems to help with the optimization of the Norm-LSTM compared to the WN-LSTM , leading to slightly better results on both experiments ."}], "0": {"review_id": "r1GKzP5xx-0", "review_text": "The authors show how the hidden states of an LSTM can be normalised in order to preserve means and variances. The method\u2019s gradient behaviour is analysed. Experimental results seem to indicate that the method compares well with similar approaches. Points 1) The writing is sloppy in parts. See at the end of the review for a non-exhaustive list. 2) The experimental results show marginal improvements, of which the the statistical significance is impossible to asses. (Not completely the author\u2019s fault for PTB, as they partially rely on results published by others.) Weight normalisation seems to be a viable alternative in the: the performance and runtime are similar. The implementation complexity of weight norm is, however, arguably much lower. More effort could have been put in by the authors to clear that up. In the current state, practitioners as well as researchers will have to put in more effort to judge whether the proposed method is really worth it for them to replicate. 3) Section 4 is nice, and I applaud the authors for doing such an analysis. List of typos etc. - maintain -> maintain - requisits -> requisites - a LSTM -> an LSTM - \"The gradients of ot and ft are equivalent to equation 25.\u201d Gradients cannot be equivalent to an equation. - \u201cbeacause\"-> because - One of the \u03b3x > \u03b3h at the end of page 5 is wrong. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your feedback . 1 ) We updated the paper with better English . 2 ) The main difference between the WN-LSTM and Norm-LSTM is the preservation of the variance through the recurrence . In both experiments that we conducted , we observed way better optimization ( see for example figure 2 for the training performances on the PTB experiment ) , and slightly better generalization , at an almost identical computational cost . Moreover , having normalized hidden states is also nice when dealing with stacks of LSTMs . WN doesn \u2019 t provide normalized hidden states , unless you perform a data dependent initialization ( which is arguably more complex to implement ) . We also plan to release the code soon . 3 ) Thank you !"}, "1": {"review_id": "r1GKzP5xx-1", "review_text": "I think this build upon previous works, in the attempt of doing something similar to batch norm specific for RNNs. To me the experiments are not yet very convincing, I think is not clear this works better than e.g. Layer Norm or not significantly so. I'm not convinced on how significant the speed up is either, I can appreciate is faster, but it doesn't feel like order of magnitude faster. The theoretical analysis also doesn't provide any new insights. All in all I think is good incremental work, but maybe is not yet significant enough for ICLR.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments . About the experiments : We do agree that the experimental setup could be improved . About the speed up : Since we are still computing an LSTM , we can \u2019 t expect order of magnitude of speed up with such reparametrization , and can only wish for computation time close to the vanilla LSTM . From a computational point of view , the main difference between the BN-LSTM ( or LN-LSTM ) and the Norm-LSTM is that the BN-LSTM adds extra computations ( means and variances ) at each time step , where the Norm-LSTM only adds an overhead before the recurrence ( the normalization of the weight matrices ) . Thus for long sequences , such as in speech recognition , the Norm-LSTM has a serious advantage over the BN-LSTM : Their performances are similar , while the Norm-LSTM is almost as cheap computationally as the vanilla LSTM . About the theoretical analysis : In our opinion , the theoretical analysis provides good insights on the role of gamma_x with respect to gamma_h , showing how we can play with their initialization to bias the network into a more short or long term memory ."}, "2": {"review_id": "r1GKzP5xx-2", "review_text": "The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well. The contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field. The contribution is not extremely novel: the change with respect to weight normalization is minor. The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better. Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments ! About data dependent initialization : As stated in the Weight Normalization paper , data dependent initialization can \u2019 t be used for RNNs , because of the recurrent nature of RNNs . Instead , they simply propose to initialize gamma to 1 , and we therefore use the same setup as in the Weight Normalization paper . About the novelty : We extended the Normalization Propagation framework to the LSTM , in order to be able to better preserve the variance through the recurrence ( which is not the case in Weight Normalization ) . We empirically showed that preserving the variance seems to help with the optimization of the Norm-LSTM compared to the WN-LSTM , leading to slightly better results on both experiments ."}}