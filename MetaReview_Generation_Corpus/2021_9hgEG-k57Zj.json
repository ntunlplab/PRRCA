{"year": "2021", "forum": "9hgEG-k57Zj", "title": "Addressing Distribution Shift in Online Reinforcement Learning with Offline Datasets", "decision": "Reject", "meta_review": "This paper addresses an distribution shift and biased Q-values that happens when offline agents are finetuned in an online manner. The final revision of the paper is very well written and easy to understand. The proposed method in the paper is interesting, and aiming to address an important issue in RL. The proposed method involves a combination of two well-known methods in RL to tackle the distribution shift issue, the paper first suggests to use a balanced replay mechanism a replay for online experiences and another one for the offline. The second improvement is coming from the ensemble distillation.\n\nIt seems like in the light of the reviews, the authors have improved manuscript. However, I would like to recommend the paper for rejection. I would like the authors to do further experiments on the individual components of the algorithms, for example what if we run all the experiments only with BR or only using ED how would the performance change. How much improvement is coming from each one of those individual components? As it stands, it is not clear to me right now, and the proposed solution looks a bit complicated and hacky. \n\nThe balanced replay mechanism is very similar to the replay approaches that are used for learning from demos methods like R2D3 [1] and DQfD. Also the ensemble distillation approach is very akin to RAND [2] and distillation approaches that are used in lifelong learning algorithms. It is not clear, why it is that important for offline RL. It should potentially improve online RL as well, perhaps some experiments on online RL would be interesting.\n\nNevertheless, I think the paper is very interesting and attempting to address a very important problem in RL. I would recommend the authors to resubmit the paper to a different venue after doing some small changes on it.\n\n[1] Gulcehre, C., Le Paine, T., Shahriari, B., Denil, M., Hoffman, M., Soyer, H., ... & Barth-Maron, G. (2019, September). Making Efficient Use of Demonstrations to Solve Hard Exploration Problems. In International Conference on Learning Representations.\n\n[2] Burda, Y., Edwards, H., Storkey, A., & Klimov, O. (2018). Exploration by random network distillation. arXiv preprint arXiv:1810.12894.\n\n\n\n", "reviews": [{"review_id": "9hgEG-k57Zj-0", "review_text": "This paper considers the problem of policy learning in Markov Decision Process ( MDP ) from the combination of online and offline samples . The offline samples are generated by a behavior policy in the same MDP model , i.e. , the behavior agent and the learning agent share the same state-action space . The learning procedure goes as follows . One first trains a MDP policy from the offline data ; the online samples are then used to fine-tune the learned policy . The authors propose a simple yet effective approach . First , the authors keep separate offline and online replay buffers , and carefully balance the number of samples from each buffer during updates . Then , multiple actor-critic offline RL policies are trained , and a single policy is distilled from these policies using ensemble methods . Experiment results show that the proposed method consistently outperforms state-of-art algorithms . This paper is clearly written and well organized . I am not sure about the novelty of the proposed method , since it seems to follow the line of carefully reweight online and offline samples . However , the experimental results show a significant improvement over existing methods . Question for the authors : 1 . In practice , what is a good heuristic for selecting the initial fraction p0 of online samples ? How sensitive is the learned policy w.r.t.the initial fraction p0 ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear R3 , We sincerely appreciate your valuable comments and efforts helpful for improving our manuscript . We address each comment in detail , one by one as below . * * ( Q1 ) Novelty of the proposed method . * * ( A1 ) We are aware that various replay schemes and ensemble learning schemes are widely used for RL . However , we believe that applying them in various fine-tuning setups is a novel contribution of our work . For instance , one of the first works in fine-tuning offline RL agents [ 1 ] only considers datasets of relatively small sizes ( \\~100K ) , in which case the disproportionate sizes of offline dataset and online samples is not a big issue . Accordingly , [ 1 ] uses a single replay buffer for fine-tuning . On the other hand , we also consider datasets of disproportionately large sizes ( \\~1M , \\~2M ) , in which case it is necessary to sample a balanced mix of offline and online samples . Also , most previous distillation schemes focus on multi-task setups [ 2,3,4 ] , and most prior works on ensembling in RL focus on \u2018 train-from-scratch \u2019 scenarios [ 5,6 ] . In light of this , we believe our ensemble distillation scheme is a novel contribution to the offline RL & fine-tuning literature . * * ( Q2 ) Initial fraction $ \\rho_ { 0 } ^ { \\texttt { on } } $ . * * ( A2 ) For selecting initial fraction $ \\rho_ { 0 } ^ { \\texttt { on } } $ , we conducted hyperparameter search over $ \\ { 0.25 , 0.5 , 0.75 , 0.9\\ } $ and used the best performing hyperparameter for each dataset . We additionally included the results for these in Figure 12 , Appendix G of the revised manuscript . As for sensitivity of the policy to the $ \\rho_ { 0 } ^ { \\texttt { on } } $ , we observe that fine-tuning performance is robust to initial fractions in halfcheetah , and most hopper tasks , but sometimes sensitive in walker2d tasks , which is known to be a difficult control task . Also , as R4 pointed out , automatic adjustment of sampling ratio is an interesting and promising future direction . For example , one can adjust the sampling ratio based on how distribution of online samples is different from offline data distribution using density esimtation ( see ( A3 ) and Figure 1b of the revised manuscript for more details ) . We would continue to add additional experiments under this scheme , if time permits . -- * * REFERENCES * * [ 1 ] Nair , Ashvin , et al . `` Accelerating online reinforcement learning with offline datasets . '' arXiv preprint arXiv:2006.09359 . 2020 . [ 2 ] Rusu , Andrei A. , et al . `` Policy distillation . '' International Conference on Learning Representations . 2016 . [ 3 ] Schmitt , Simon , et al . `` Kickstarting deep reinforcement learning . '' arXiv preprint arXiv:1803.03835 . 2018 . [ 4 ] Traor\u00e9 , Ren\u00e9 , et al . `` DISCORL : Continual reinforcement learning via policy distillation . '' arXiv preprint arXiv:1907.05855 ( 2019 ) . [ 5 ] Anschel , Oron , Nir Baram , and Nahum Shimkin . `` Averaged-dqn : Variance reduction and stabilization for deep reinforcement learning . '' International Conference on Machine Learning . 2017 . [ 6 ] Osband , Ian , et al . `` Deep exploration via bootstrapped DQN . '' Advances in neural information processing systems . 2016 ."}, {"review_id": "9hgEG-k57Zj-1", "review_text": "Summary : This paper proposes to deal with distribution shift problem between online and offline samples when the agent trained by offline data is fine-tuned with online interactions . Two mechanisms are introduced : ( 1 ) using two replay buffers for offline and online data respectively , and training the agent with data sampled from these two buffers with a certain ratio ( the ratio changes in a way that more online data is used in later epochs ) ; ( 2 ) learning an ensemble of independent agents in the offline phase , and distilling them into a mean policy to overcome bootstrapping error . Empirical results demonstrate that the proposed method perform well during fine-tuning when there is distribution shift . Strengths : 1 . The paper is overall well-written and easy to follow . 2.The problem studied by this paper is well-motivated , and the proposed methods are simple yet effective , making intuitive sense . 3.The experiment section shows that the proposed method ( BRED ) outperforms baselines on 3 mujoco tasks with multiple types of offline data . Further ablation study verifies the effectiveness of the two proposed mechanisms . Weakness : 1 . Although empirical results are good , my main concern is that the novelty of this paper is limited . The main contribution of this paper is combining CQL with two tricks : balanced replay and ensemble distillation . As already discussed in the related work section , these two tricks are commonly used in the literature , even though the concrete setting varies . 2.The specific way of combining the offline buffer and the online buffer ( online fraction grows linearly in timestep ) could be effective in some cases , but might not always be the best choice . I am curious whether the authors have tried other ways . And especially , can the agent chooses the ratio adaptively based on the degree of distribution shift and the learning performance ? 3.The definition of distribution shift in this paper is descriptive and informal . It will be nice to provide deeper analysis or insights w.r.t.the distribution shift problem . For example , is there a way to rigorously characterize the distribution shift ? Is there a measurement for how large the gap is ? This will be relevant to addressing point 2 above . 4.Although policy ensemble can make the learned Q-function more accurate and stable , it is not very clear to me why it tackles distribution shift . Even the distilled policy has a more accurate estimation of the Q values , the estimation is still w.r.t.the offline data distribution . It still suffers from the bootstrapping error when using out-of-distribution samples . Minor comments : 1 . Figure 1 is a good visualization . However , the figure is not very informative since details are not provided ( e.g. , what is contained in a sample , in what sense / which dimension the distribution is different ) . 2.In the experiment , I am wondering how an \u201c ensemble-only \u201d method would work and compare to baselines , i.e. , use multiple pertained CQL agents , and fine-tune the distilled policy . The \u201c Effects of ensemble distillation \u201d paragraph shows a comparison between the proposed ensemble distillation and the ensemble of independent policies . However , the result does not reflect how ensemble compares with non-ensemble . 3.The paper focuses on the distribution shift problem in offline RL . However , there could be multiple interpretations of distribution shift , e.g. , state distribution changes due to the change of behavior policy , or dynamics distribution changes due to the slight change of the environment , etc . I think this paper is mainly dealing with the state distribution change case . But it will be better if the authors can make it explicit .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear R4 , We sincerely appreciate your valuable comments and efforts helpful for improving our manuscript . We address each comment in detail , one by one as below . * * ( Q1 ) \u201c The novelty of this paper is limited. \u201d * * ( A1 ) We are aware that various replay schemes and ensembling schemes are widely used for RL . However , we believe that applying them in various fine-tuning setups is a novel contribution of our work . For instance , one of the first works in fine-tuning offline RL agents [ 1 ] only considers datasets of relatively small sizes ( \\~100K ) , in which case the disproportionate sizes of offline dataset and online samples is not a big issue . Accordingly , [ 1 ] uses a single replay buffer for fine-tuning . On the other hand , we also consider datasets of disproportionately large sizes ( \\~1M , \\~2M ) , in which case it is necessary to sample a balanced mix of offline and online samples . Also , most distillation schemes focus on multi-task setups [ 2,3,4 ] , and most ensemble schemes in RL focus on \u2018 train-from-scratch \u2019 scenarios [ 5,6 ] . In light of this , we believe our ensemble distillation scheme is a novel contribution to the offline RL & fine-tuning literature . * * ( Q2 ) Other scheduling schemes for balanced replay . * * ( A2 ) We have additionally tried various other linear schedules and exponential schedules ( see Appendix G , H , I of the revised manuscript ) . For one , we varied the initial fraction $ \\rho_ { 0 } ^ { \\texttt { on } } $ of online samples to take values from $ \\ { 0.25 , 0.5 , 0.75 , 0.9\\ } $ . Also , we performed search over the final timestep of the annealing schedule , $ t_ { \\texttt { final } } $ . Finally , we also experimented with an exponential schedule . As shown in Figure 12 , 13 , 14 , fine-tuning performance stays relatively robust to such different scheduling schemes in halfcheetah and most hopper datasets . However , performance can be sensitive for walker2d tasks , which is known to be a difficult control task . As pointed out by the R4 , automatic adjustment of sampling ratio is an interesting and promising future direction . For example , one can adjust the sampling ratio based on how distribution of online samples is different from offline data distribution using density esimtation ( see A3 and Figure 1b of the revised manuscript for more details ) . We would continue to add additional experiments under this scheme , if time permits . * * ( Q3 ) Formal description / analysis of distribution shift . * * ( A3 ) Note that offline dataset is generated according to the following distribution : $ p_ { \\texttt { data } } ( s , a ) = d^ { \\pi_ { \\beta } } ( s ) \\pi_ { \\beta } ( a|s ) , $ where $ \\pi_ { \\beta } $ denotes the behavior policy , and $ d^ { \\pi } $ the discounted marginal state distribution of $ \\pi $ . Then distribution shift refers to the difference between $ p_ { \\texttt { data } } ( s , a ) $ and $ p_ { \\theta } ( s , a ) $ , i.e. , there exists a shift in both state distribution and state-conditioned action distribution . We would like to point out that the distribution shift we tackle is different from the distribution shift usually referred to in the offline RL literature : the learned policy deviating from the behavior policy , i.e. $ D ( \\pi_ { \\theta } ( \\cdot|s ) , \\pi_ { \\beta } ( \\cdot|s ) ) $ becoming large , for a given state $ s\\sim d^ { \\pi_ { \\beta } } $ . As a measurement for the degree of distribution shift , we provide the histogram of log-likelihoods for offline and online samples ( Figure 1b of the revised manuscript ) , obtained using VAE pre-trained using offline dataset ( also see our response A5 to R2 for more details ) . Also , we added the formal description of distribution shift in Section 3 of the revised manuscript . * * ( Q4 ) How does ensemble distillation tackle distribution shift ? * * ( A4 ) Ensemble distillation reduces the policy variance , and thereby provides further robustness to Q-function approximation error . To show this , we performed an additional experiment : during training , given each observation in the sampled minibatch , we sampled 10 actions from the policy being trained , then measured the variance of Q values among these 10 actions . We averaged the variance in Q values across the minibatch samples . Figure 16 in Appendix K shows the result , where Q values for actions sampled by independent ensembles have much higher variance when compared to the ensemble distillation counterpart ."}, {"review_id": "9hgEG-k57Zj-2", "review_text": "# # # # # Summary The paper proposes a fine-tuning method for an offline RL algorithm , CQL . The method incorporates a balanced replay scheme for both online and offline samples , and an ensemble distillation to stabilize policy learning . The proposed method is evaluated on benchmark environments . The paper is clearly written and easy to read . However , the problem setting in the paper is not well-motivated and the goal is unclear . The paper makes some claims without supporting them ( details come later ) . Moreover , some important experiments details are missing which makes it hard to assess the empirical results . I think all of these issues needs to be addressed before publication . Therefore , I recommend to reject the paper . # # # # # Supporting arguments and clarification questions First of all , I think it is not motivated why online fine-tuning procedure is necessary ( see first paragraph in the introduction ) . If online simulation/interaction with the real environment is practical , why do we need to train an offline RL agent first and perform online fine-tuning ( given that the paper claims that fine-tuning is challenging ) ? If online simulation is not practical , how could we perform online fine-tuning ? It is also unclear to me what is the goal we want the proposed method to achieve ( e.g. , what are the evaluation metrics ) ? Do we want the fine-tunning method achieve the best sample efficiency compared to purely online algorithm , or achieve the best asymptotic performance compared to other fine-tuning methods ? Without a clear goal , it is hard to assess the significance or soundness of the proposed method . In section 3 , what is the reason for fine-tuning a CQL agent by SAC updates ? Have you tried fine-tuning by DQN updates or its variant ? I think SAC is learning a value function with an extra entropy term and CQL is learning the true value function ( with some regularization during the optimization procedure ) , so these algorithm are essentially learning two different targets . It makes sense that we would see instability during fine-tuning since they are optimizing towards different targets . Therefore , I don \u2019 t think the clam \u201c this instability occurs due to the shift between offline and online data distribution \u201d is supported in the experiment . It might be due to the fact that we are optimizing different objectives . In the experiment , how were the hyper-parameters selected ? The hyper-parameters should be selected based on the offline dataset , not the fine-tuning performance . I think the paper should also include a baseline , which is the performance achieved the offline RL agents ( i.e. , horizontal lines in the learning curve ) . In Figure 3 , it seems like BRED is much more stable compared to other methods in Hopper medium , so I wonder if the reason is that BRED already learns a good policy with offline dataset and it does not change much during the fine-tuning procedures . In section 5 , the paper provides a nice discussion on the related works . However , the paper claims that \u201c this method ( Nair et al. , 2020 ) relies on regression , hence the learned policy seldom surpasses the best data-generating policy \u201d . Can you elaborate more on this ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear R1 , We sincerely appreciate your valuable comments for improving our manuscript . We address each comment in detail , one by one as below . * * ( Q1 ) Necessity of online fine-tuning . * * ( A1 ) As R4 mentioned , we believe fine-tuning an offline RL agent is a well-motivated and promising research topic . In particular , fine-tuning a suboptimal offline RL agent is a better strategy than training an agent from scratch when online interaction/simulation is available , for the cost of online interaction usually decreases as the agent becomes more proficient . For one , this is typical of safety-critical tasks such as autonomous driving , where training an agent from scratch would lead to costly failures , whereas an already decent agent would experience relatively infrequent , less costly failures . Also , in robot learning , training is often done in an episodic manner , where a human has to manually reset the agent every time the agent reaches the terminal state , i.e. , fails or gets stuck . As pointed out by [ 1 ] , such manual resets are a serious bottleneck in robot learning . Warm-starting the agent via offline RL then fine-tuning it is one viable strategy to alleviate this issue , and is drawing a lot of research interest accordingly [ 2,3 ] . * * ( Q2 ) Goal of our work . * * ( A2 ) Our goal is to achieve both ( 1 ) strong initial performance as well as maintaining it during the initial training phase , and ( 2 ) better sample-efficiency during fine-tuning . We clarified our goal in Section 1 and Section 6.1 . Also , to better reflect ( 1 ) , we added the performance of initial offline RL agents in all relevant performance plots in the revised manuscript . * * ( Q3 ) Fine-tuning a CQL agent with SAC ? * * ( A3 ) Thank you for pointing this out . As we have shown in Appendix A of the original manuscript , fine-tuning with the same regularized objective as CQL results in almost no ( if at all ) improvement of the agent , due to the regularization term inducing pessimistic updates . We used SAC updates for fine-tuning a CQL agent because CQL is built upon SAC in that it learns a stochastic policy and employs an automatic entropy tuning as in SAC . And indeed , with a careful approach , i.e. , balanced replay and ensemble distillation , SAC-like updates can lead to stable fine-tuning ( Figure 3 ) . However , to resolve the reviewer \u2019 s concern , we have conducted additional experiments using ( i ) TD3 [ 4 ] , a more stable variant of DDPG [ 5 ] , and ( ii ) SAC [ 6 ] with deterministic backup , i.e. , with the same objective as CQL except for the regularization term . As shown in Appendix C of the revised manuscript , these methods still suffer from unstable fine-tuning , especially when using online samples exclusively during fine-tuning . This suggests that instability is not due to the difference in optimization objectives , but more due to the distribution shift between offline and online samples . * * ( Q4 ) Hyperparameter selection . * * ( A4 ) We selected the hyperparameters based on grid search , where the selection criteria were sample-efficiency and fine-tuning stability . We agree with the reviewer that it is ideal to select the hyperparameters before fine-tuning , based on the offline dataset and the offline agent trained on it . However , we would like to point out that the task of evaluating an offline RL agent , namely , off-policy evaluation ( OPE ) , is a challenging research question ( See [ 7 ] for more details ) . Considering this , deciding on all hyperparameters for fine-tuning a priori is beyond the scope of our work . One rule of thumb we observed to work well , however , is that reduced policy learning rates result in better fine-tuning stability . This observation was also made in a recent work on fine-tuning robotic agents [ 8 ] . In addition , we added experimental results concerning different hyperparameter selections for balanced replay scheme in Appendix G and H in the revised manuscript . We observed that BRED is robust to $ \\rho_ { 0 } ^ { \\tt { on } } $ and $ t_ { \\tt { final } } $ in halfcheetah and most hopper tasks , but sometimes sensitive to them in tasks involving walker2d , which is known to be a difficult control task ."}, {"review_id": "9hgEG-k57Zj-3", "review_text": "Summary - Offline RL allows agents to be trained on static , offline datasets . If the agent has already undergone offline training however , distribution shift makes further online training difficult . The authors propose to address this problem by keeping separate offline and online replay buffers , and sampling different proportions of each . In addition , they propose to ensemble a distillation policy . The proposed method is evaluated on logged MuJoCo datasets answering whether BRED improves over other fine-tuning methods and whether balancing or replay contribute more to BRED 's success . Decision -- BRED is an interesting approach , but the experiments do not demonstrate that it is better than the baseline . As such , my preliminary rating of this paper is rejection . I appreciate that the sampling scheme proposed is simple and well motivated to address the problem of distribution shift . It is not clear whether the sampling scheme actually addresses the problem of distribution shift however , as it gives the agent access to additional experience . The distillation scheme also does not seem to address distribution shift directly and the experiments do not demonstrate that it helps performance outside of the Walker2d-random task . Moreover , it is not clear whether the baselines are also ensembles , which would make the experiment results less compelling . Originality -- The contributions are : a sampling scheme combining offline and online samples , as well as a distillation scheme . The sampling scheme is incremental and simple , but seems like a novel approach . Distillation on the other hand , is well-explored in RL ( Czarnecki et al.2019 ) and does not seem to contribute much to the problem addressed in the paper . Taken together , the paper has limited novelty . Quality and Clarity - The paper is easy-to-follow and well-written . However , I have some issues with the title . The title suggests that your aim is to address distribution shift in online RL . However , distribution shift is a problem due to offline training . Of course , the problem you are addressing is further online training after offline training . Perhaps the title should be reworded to make this more clear . Strengths - There is a clear exposition of the problem addressed , namely distribution shift . This is well motivated by the t-SNE visualization . Section 3 in particular highlights the correlation between distribution shift and the performance of offline RL algorithms . - BRED , the proposed method , is a straightforward solution via balancing offline and online replay buffers . In addition , the schedule for updating the proportion sampled of each replay buffer is well argued and also simple . - The results are clearly motivated by questions in the beginning of the section . Sensible baselines are used to compare the proposed method ( BRED ) , and these are discussed and reasoned in detail . Furthermore , there is a good coverage of tasks ( 3 MuJoCo ) and difficulties ( random , medium , medium-replay , medium-expert ) . Weaknesses - - While fine-tuning an offline RL agent can pose challenges , similar challenges can arise in supervised learning . For example , Ash and Adams ( 2019 ) note that in different regimes , warm-starting neural network training can hamper generalization . Overall , I feel that the correlation between distribution shift and online performance after offline training is not shown to be causal , but correlative . - It is unclear why distillation is included , as it does not directly address the problem of distribution shift in offline RL , outside of the intuition that it mitigates bad $ Q $ estimates . This is not enough motivation however , and the experiment results ( Figure 5 and 6 ) does not show any significant benefit of ensemble distillation over independent ensembles ( besides Walker2d-random ) . Furthermore , this may not be a fair comparison to other baseline methods which do not seem to be ensembles . - Despite a very well-motivated experiment section , the experimental conclusions are far too strong . The results do not back this up , when there is high overlap in the confidence intervals for many Figures ( in each section ) . Furthermore , there are unexplained peculiarities in the results , such as very big drops in performance or the fact that the agent trained with the offline dataset is substantially better than the medium one . Detailed Comments -- - `` pointed out that offline RL algorithms\u2026 are not amenable to fine-tuning , due to the difficulty of modeling the dataset-generating policy in the online setup . '' This is not clear to me , how does fine-tuning connect to modelling the dataset-generating policy ? Furthermore , how does the dataset-generating policy ( I assume this is the behavior policy ) connect to online RL ? - Figure 1b : the agent is trained on the offline dataset and then that dataset is thrown away for the online agent , whereas the agent in red gets to keep using it . This does show the difficulty of fine-tuning but does not provide conclusive evidence that distributional shift is the cause . Fine-tuning can be difficult even in supervised learning ( Ash and Adams 2019 ) . - `` On the other hand , when using online samples exclusively , the agent is exposed to unseen samples only , for which Q function does not provide a reliable value estimate . This may lead to bootstrapping error , and hence a dip in performance as seen in Figure 1b . '' Ca n't this be shown experimentally in an environment to provide conclusive evidence for distribution shift being the cause of the dip in performance ? - Figure 3 : It seems that halfcheetah-random results in a better policy than halfcheetah-medium . This does not make sense to me . In addition , there is a strange dip in performance for CQL-ft in Figure 3g . Many of the figures have overlapping confidence intervals ( although the shaded region is only the standard deviation , the corresponding confidence interval would be approximately the standard deviation.Anyway these should be confidence intervals rather than standard deviation ) . While BRED does perform statistically significantly better in some tasks , I do n't think the comparison is fair due to ensembling used only for BRED . - Figure 4 : Going from figure 4a to 4b , balanced replay is shown to be more beneficial for a dataset generated by a random behavior policy . This seems to suggest that the main benefit of BRED is artificial exploration . Counter-intuitvely , this trend is reversed in Figure 4c . Moreover , the online-only curve is very different in Figure 4c than 4a and 4b , and to my understanding there should not be any difference . Due to the noisiness of the curves , its hard to make any straightforward conclusion from these results . - Figure 5 and 6 : Again , there is no statistical difference in medium and medium-replay between independent ensembles and a distillation ensemble . Minor Comments -- - Equation 3 : missing brackets on the left side . Ash , Jordan T. , and Ryan P. Adams . 2019. \u201c On Warm-Starting Neural Network Training. \u201d * arXiv:1910.08475 * . < http : //arxiv.org/abs/1910.08475v2 > . Czarnecki , Wojciech Marian , Razvan Pascanu , Simon Osindero , Siddhant M. Jayakumar , Grzegorz Swirszcz , and Max Jaderberg . 2019. \u201c Distilling Policy Distillation. \u201d * arXiv:1902.02186 * . < http : //arxiv.org/abs/1902.02186v1 > .", "rating": "3: Clear rejection", "reply_text": "Dear R2 , We sincerely appreciate your valuable and insightful comments . We found them extremely helpful for improving our manuscript . We address each comment in detail , one by one below . * * ( Q1 ) Clarification on the meaning of title . * * ( A1 ) We agree with the author that the title may be misleading , for we mainly deal with fine-tuning offline RL agents , not randomly initialized agents . Accordingly , we changed the title from \u2018 Addressing Distribution Shift in Online Reinforcement Learning with Offline Datasets \u2019 to \u2018 Addressing Distribution Shift in Offline-to-Online Reinforcement Learning \u2019 . * * ( Q2 ) Clarification on dataset-generating policy and its relationship to online RL . * * ( A2 ) Thank you for pointing out a possibly confusing point . A prior work [ 1 ] pointed out that it is not feasible to extend such offline RL methods as BCQ [ 2 ] and BEAR [ 3 ] to the fine-tuning setup . This is because these early methods require modeling the policy used to generate the offline dataset , i.e.behavior policy/dataset-generating policy , and training such density models in the online fine-tuning setup is challenging [ 1 ] . Instead , we consider building upon a more recent offline RL algorithm , conservative Q-learning ( CQL ) [ 4 ] , which does not require modeling the behavior policy . Accordingly , our method does not model the behavior policy . We clarified this in Section 1 . * * ( Q3 ) Is instability in fine-tuning due to distribution shift ? * * ( A3 ) We would like to point out that there have been several prior works where fine-tuning has been successfully applied for robot learning [ 5,6 ] . These works rely on an ample amount of diverse offline data , whereas we considered various scenarios where such large and diverse dataset may not be available , in which case distribution shift may hamper the fine-tuning process . Also , we would like to emphasize that fine-tuning a Q-learning RL agent poses a unique challenge when compared to supervised learning , for it involves bootstrapping - a process especially vulnerable to distribution shift . Indeed , a prior work [ 1 ] trained a SAC agent from scratch , but with a pre-populated replay buffer , i.e. , an agent that suffers from distribution shift , but not from the difficulty of fine-tuning a neural network . Thusly trained SAC agent is shown to perform worse than a SAC agent trained from scratch with an empty replay buffer . This observation shows that distribution shift is indeed a non-trivial challenge when training an RL agent . * * ( Q4 ) Justification of ensemble distillation . * * ( A4 ) Ensemble distillation provides further robustness to Q-function approximation error , by reducing policy variance . In order to demonstrate this , we conducted an additional experiment : for each update during training , given an observation in the sampled mini-batch , we sampled 10 actions from the policy being trained , then measured the variance of Q values among these 10 actions . Then we averaged the variance in Q values across the sampled minibatch , i.e. , $ \\mathbb { E } _ { s\\sim \\mathcal { B } , a\\sim \\pi ( \\cdot|s ) } \\big [ Q_\\theta ( s , a ) \\big ] $ . As shown in Appendix K , Figure 16 , Q values for actions thusly sampled are of much higher variance for the independent ensemble learner . This shows that by training a single distilled policy , we obtain a stability gain . * * ( Q5 ) Direct evidence of distribution shift . * * ( A5 ) In order to demonstrate the distribution shift in a more direct manner , we additionally trained a variational autoencoder ( VAE ) and measured the log-likelihoods of offline and online samples ( Figure 1b of the revised manuscript ) . We observe that offline and online samples follow different distributions , which evidences that there exists a distribution shift ( see our response A3 to R4 for more formal description of distribution shift ) . Specifically , we first split a given offline dataset into a training dataset and validation dataset . Then , we trained a VAE on the training offline dataset , and measured log-likelihood for ( 1 ) offline samples from the held out validation dataset and ( 2 ) online samples gathered by the offline agent . We included the experimental detail in Section 3 of the revised manuscript also . In order to show the harmful effects of distribution shift more directly , we conducted additional experiments with walker2d tasks . Similar to [ 7 ] , we compared the average Q value estimate over 1000 state-action pairs sampled from the replay buffer , and the average of their true Q values . As shown in Figure 17 from Appendix L of the revised manuscript , we see that in three out of four tasks ( walker2d-random , walker2d-medium-replay , and walker2d-medium-expert ) , Online-only agents suffer from severe overestimation bias and high variance , especially during the early fine-tuning stage . Experimental details are also provided in Appendix L of the revised manuscript ."}], "0": {"review_id": "9hgEG-k57Zj-0", "review_text": "This paper considers the problem of policy learning in Markov Decision Process ( MDP ) from the combination of online and offline samples . The offline samples are generated by a behavior policy in the same MDP model , i.e. , the behavior agent and the learning agent share the same state-action space . The learning procedure goes as follows . One first trains a MDP policy from the offline data ; the online samples are then used to fine-tune the learned policy . The authors propose a simple yet effective approach . First , the authors keep separate offline and online replay buffers , and carefully balance the number of samples from each buffer during updates . Then , multiple actor-critic offline RL policies are trained , and a single policy is distilled from these policies using ensemble methods . Experiment results show that the proposed method consistently outperforms state-of-art algorithms . This paper is clearly written and well organized . I am not sure about the novelty of the proposed method , since it seems to follow the line of carefully reweight online and offline samples . However , the experimental results show a significant improvement over existing methods . Question for the authors : 1 . In practice , what is a good heuristic for selecting the initial fraction p0 of online samples ? How sensitive is the learned policy w.r.t.the initial fraction p0 ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear R3 , We sincerely appreciate your valuable comments and efforts helpful for improving our manuscript . We address each comment in detail , one by one as below . * * ( Q1 ) Novelty of the proposed method . * * ( A1 ) We are aware that various replay schemes and ensemble learning schemes are widely used for RL . However , we believe that applying them in various fine-tuning setups is a novel contribution of our work . For instance , one of the first works in fine-tuning offline RL agents [ 1 ] only considers datasets of relatively small sizes ( \\~100K ) , in which case the disproportionate sizes of offline dataset and online samples is not a big issue . Accordingly , [ 1 ] uses a single replay buffer for fine-tuning . On the other hand , we also consider datasets of disproportionately large sizes ( \\~1M , \\~2M ) , in which case it is necessary to sample a balanced mix of offline and online samples . Also , most previous distillation schemes focus on multi-task setups [ 2,3,4 ] , and most prior works on ensembling in RL focus on \u2018 train-from-scratch \u2019 scenarios [ 5,6 ] . In light of this , we believe our ensemble distillation scheme is a novel contribution to the offline RL & fine-tuning literature . * * ( Q2 ) Initial fraction $ \\rho_ { 0 } ^ { \\texttt { on } } $ . * * ( A2 ) For selecting initial fraction $ \\rho_ { 0 } ^ { \\texttt { on } } $ , we conducted hyperparameter search over $ \\ { 0.25 , 0.5 , 0.75 , 0.9\\ } $ and used the best performing hyperparameter for each dataset . We additionally included the results for these in Figure 12 , Appendix G of the revised manuscript . As for sensitivity of the policy to the $ \\rho_ { 0 } ^ { \\texttt { on } } $ , we observe that fine-tuning performance is robust to initial fractions in halfcheetah , and most hopper tasks , but sometimes sensitive in walker2d tasks , which is known to be a difficult control task . Also , as R4 pointed out , automatic adjustment of sampling ratio is an interesting and promising future direction . For example , one can adjust the sampling ratio based on how distribution of online samples is different from offline data distribution using density esimtation ( see ( A3 ) and Figure 1b of the revised manuscript for more details ) . We would continue to add additional experiments under this scheme , if time permits . -- * * REFERENCES * * [ 1 ] Nair , Ashvin , et al . `` Accelerating online reinforcement learning with offline datasets . '' arXiv preprint arXiv:2006.09359 . 2020 . [ 2 ] Rusu , Andrei A. , et al . `` Policy distillation . '' International Conference on Learning Representations . 2016 . [ 3 ] Schmitt , Simon , et al . `` Kickstarting deep reinforcement learning . '' arXiv preprint arXiv:1803.03835 . 2018 . [ 4 ] Traor\u00e9 , Ren\u00e9 , et al . `` DISCORL : Continual reinforcement learning via policy distillation . '' arXiv preprint arXiv:1907.05855 ( 2019 ) . [ 5 ] Anschel , Oron , Nir Baram , and Nahum Shimkin . `` Averaged-dqn : Variance reduction and stabilization for deep reinforcement learning . '' International Conference on Machine Learning . 2017 . [ 6 ] Osband , Ian , et al . `` Deep exploration via bootstrapped DQN . '' Advances in neural information processing systems . 2016 ."}, "1": {"review_id": "9hgEG-k57Zj-1", "review_text": "Summary : This paper proposes to deal with distribution shift problem between online and offline samples when the agent trained by offline data is fine-tuned with online interactions . Two mechanisms are introduced : ( 1 ) using two replay buffers for offline and online data respectively , and training the agent with data sampled from these two buffers with a certain ratio ( the ratio changes in a way that more online data is used in later epochs ) ; ( 2 ) learning an ensemble of independent agents in the offline phase , and distilling them into a mean policy to overcome bootstrapping error . Empirical results demonstrate that the proposed method perform well during fine-tuning when there is distribution shift . Strengths : 1 . The paper is overall well-written and easy to follow . 2.The problem studied by this paper is well-motivated , and the proposed methods are simple yet effective , making intuitive sense . 3.The experiment section shows that the proposed method ( BRED ) outperforms baselines on 3 mujoco tasks with multiple types of offline data . Further ablation study verifies the effectiveness of the two proposed mechanisms . Weakness : 1 . Although empirical results are good , my main concern is that the novelty of this paper is limited . The main contribution of this paper is combining CQL with two tricks : balanced replay and ensemble distillation . As already discussed in the related work section , these two tricks are commonly used in the literature , even though the concrete setting varies . 2.The specific way of combining the offline buffer and the online buffer ( online fraction grows linearly in timestep ) could be effective in some cases , but might not always be the best choice . I am curious whether the authors have tried other ways . And especially , can the agent chooses the ratio adaptively based on the degree of distribution shift and the learning performance ? 3.The definition of distribution shift in this paper is descriptive and informal . It will be nice to provide deeper analysis or insights w.r.t.the distribution shift problem . For example , is there a way to rigorously characterize the distribution shift ? Is there a measurement for how large the gap is ? This will be relevant to addressing point 2 above . 4.Although policy ensemble can make the learned Q-function more accurate and stable , it is not very clear to me why it tackles distribution shift . Even the distilled policy has a more accurate estimation of the Q values , the estimation is still w.r.t.the offline data distribution . It still suffers from the bootstrapping error when using out-of-distribution samples . Minor comments : 1 . Figure 1 is a good visualization . However , the figure is not very informative since details are not provided ( e.g. , what is contained in a sample , in what sense / which dimension the distribution is different ) . 2.In the experiment , I am wondering how an \u201c ensemble-only \u201d method would work and compare to baselines , i.e. , use multiple pertained CQL agents , and fine-tune the distilled policy . The \u201c Effects of ensemble distillation \u201d paragraph shows a comparison between the proposed ensemble distillation and the ensemble of independent policies . However , the result does not reflect how ensemble compares with non-ensemble . 3.The paper focuses on the distribution shift problem in offline RL . However , there could be multiple interpretations of distribution shift , e.g. , state distribution changes due to the change of behavior policy , or dynamics distribution changes due to the slight change of the environment , etc . I think this paper is mainly dealing with the state distribution change case . But it will be better if the authors can make it explicit .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear R4 , We sincerely appreciate your valuable comments and efforts helpful for improving our manuscript . We address each comment in detail , one by one as below . * * ( Q1 ) \u201c The novelty of this paper is limited. \u201d * * ( A1 ) We are aware that various replay schemes and ensembling schemes are widely used for RL . However , we believe that applying them in various fine-tuning setups is a novel contribution of our work . For instance , one of the first works in fine-tuning offline RL agents [ 1 ] only considers datasets of relatively small sizes ( \\~100K ) , in which case the disproportionate sizes of offline dataset and online samples is not a big issue . Accordingly , [ 1 ] uses a single replay buffer for fine-tuning . On the other hand , we also consider datasets of disproportionately large sizes ( \\~1M , \\~2M ) , in which case it is necessary to sample a balanced mix of offline and online samples . Also , most distillation schemes focus on multi-task setups [ 2,3,4 ] , and most ensemble schemes in RL focus on \u2018 train-from-scratch \u2019 scenarios [ 5,6 ] . In light of this , we believe our ensemble distillation scheme is a novel contribution to the offline RL & fine-tuning literature . * * ( Q2 ) Other scheduling schemes for balanced replay . * * ( A2 ) We have additionally tried various other linear schedules and exponential schedules ( see Appendix G , H , I of the revised manuscript ) . For one , we varied the initial fraction $ \\rho_ { 0 } ^ { \\texttt { on } } $ of online samples to take values from $ \\ { 0.25 , 0.5 , 0.75 , 0.9\\ } $ . Also , we performed search over the final timestep of the annealing schedule , $ t_ { \\texttt { final } } $ . Finally , we also experimented with an exponential schedule . As shown in Figure 12 , 13 , 14 , fine-tuning performance stays relatively robust to such different scheduling schemes in halfcheetah and most hopper datasets . However , performance can be sensitive for walker2d tasks , which is known to be a difficult control task . As pointed out by the R4 , automatic adjustment of sampling ratio is an interesting and promising future direction . For example , one can adjust the sampling ratio based on how distribution of online samples is different from offline data distribution using density esimtation ( see A3 and Figure 1b of the revised manuscript for more details ) . We would continue to add additional experiments under this scheme , if time permits . * * ( Q3 ) Formal description / analysis of distribution shift . * * ( A3 ) Note that offline dataset is generated according to the following distribution : $ p_ { \\texttt { data } } ( s , a ) = d^ { \\pi_ { \\beta } } ( s ) \\pi_ { \\beta } ( a|s ) , $ where $ \\pi_ { \\beta } $ denotes the behavior policy , and $ d^ { \\pi } $ the discounted marginal state distribution of $ \\pi $ . Then distribution shift refers to the difference between $ p_ { \\texttt { data } } ( s , a ) $ and $ p_ { \\theta } ( s , a ) $ , i.e. , there exists a shift in both state distribution and state-conditioned action distribution . We would like to point out that the distribution shift we tackle is different from the distribution shift usually referred to in the offline RL literature : the learned policy deviating from the behavior policy , i.e. $ D ( \\pi_ { \\theta } ( \\cdot|s ) , \\pi_ { \\beta } ( \\cdot|s ) ) $ becoming large , for a given state $ s\\sim d^ { \\pi_ { \\beta } } $ . As a measurement for the degree of distribution shift , we provide the histogram of log-likelihoods for offline and online samples ( Figure 1b of the revised manuscript ) , obtained using VAE pre-trained using offline dataset ( also see our response A5 to R2 for more details ) . Also , we added the formal description of distribution shift in Section 3 of the revised manuscript . * * ( Q4 ) How does ensemble distillation tackle distribution shift ? * * ( A4 ) Ensemble distillation reduces the policy variance , and thereby provides further robustness to Q-function approximation error . To show this , we performed an additional experiment : during training , given each observation in the sampled minibatch , we sampled 10 actions from the policy being trained , then measured the variance of Q values among these 10 actions . We averaged the variance in Q values across the minibatch samples . Figure 16 in Appendix K shows the result , where Q values for actions sampled by independent ensembles have much higher variance when compared to the ensemble distillation counterpart ."}, "2": {"review_id": "9hgEG-k57Zj-2", "review_text": "# # # # # Summary The paper proposes a fine-tuning method for an offline RL algorithm , CQL . The method incorporates a balanced replay scheme for both online and offline samples , and an ensemble distillation to stabilize policy learning . The proposed method is evaluated on benchmark environments . The paper is clearly written and easy to read . However , the problem setting in the paper is not well-motivated and the goal is unclear . The paper makes some claims without supporting them ( details come later ) . Moreover , some important experiments details are missing which makes it hard to assess the empirical results . I think all of these issues needs to be addressed before publication . Therefore , I recommend to reject the paper . # # # # # Supporting arguments and clarification questions First of all , I think it is not motivated why online fine-tuning procedure is necessary ( see first paragraph in the introduction ) . If online simulation/interaction with the real environment is practical , why do we need to train an offline RL agent first and perform online fine-tuning ( given that the paper claims that fine-tuning is challenging ) ? If online simulation is not practical , how could we perform online fine-tuning ? It is also unclear to me what is the goal we want the proposed method to achieve ( e.g. , what are the evaluation metrics ) ? Do we want the fine-tunning method achieve the best sample efficiency compared to purely online algorithm , or achieve the best asymptotic performance compared to other fine-tuning methods ? Without a clear goal , it is hard to assess the significance or soundness of the proposed method . In section 3 , what is the reason for fine-tuning a CQL agent by SAC updates ? Have you tried fine-tuning by DQN updates or its variant ? I think SAC is learning a value function with an extra entropy term and CQL is learning the true value function ( with some regularization during the optimization procedure ) , so these algorithm are essentially learning two different targets . It makes sense that we would see instability during fine-tuning since they are optimizing towards different targets . Therefore , I don \u2019 t think the clam \u201c this instability occurs due to the shift between offline and online data distribution \u201d is supported in the experiment . It might be due to the fact that we are optimizing different objectives . In the experiment , how were the hyper-parameters selected ? The hyper-parameters should be selected based on the offline dataset , not the fine-tuning performance . I think the paper should also include a baseline , which is the performance achieved the offline RL agents ( i.e. , horizontal lines in the learning curve ) . In Figure 3 , it seems like BRED is much more stable compared to other methods in Hopper medium , so I wonder if the reason is that BRED already learns a good policy with offline dataset and it does not change much during the fine-tuning procedures . In section 5 , the paper provides a nice discussion on the related works . However , the paper claims that \u201c this method ( Nair et al. , 2020 ) relies on regression , hence the learned policy seldom surpasses the best data-generating policy \u201d . Can you elaborate more on this ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear R1 , We sincerely appreciate your valuable comments for improving our manuscript . We address each comment in detail , one by one as below . * * ( Q1 ) Necessity of online fine-tuning . * * ( A1 ) As R4 mentioned , we believe fine-tuning an offline RL agent is a well-motivated and promising research topic . In particular , fine-tuning a suboptimal offline RL agent is a better strategy than training an agent from scratch when online interaction/simulation is available , for the cost of online interaction usually decreases as the agent becomes more proficient . For one , this is typical of safety-critical tasks such as autonomous driving , where training an agent from scratch would lead to costly failures , whereas an already decent agent would experience relatively infrequent , less costly failures . Also , in robot learning , training is often done in an episodic manner , where a human has to manually reset the agent every time the agent reaches the terminal state , i.e. , fails or gets stuck . As pointed out by [ 1 ] , such manual resets are a serious bottleneck in robot learning . Warm-starting the agent via offline RL then fine-tuning it is one viable strategy to alleviate this issue , and is drawing a lot of research interest accordingly [ 2,3 ] . * * ( Q2 ) Goal of our work . * * ( A2 ) Our goal is to achieve both ( 1 ) strong initial performance as well as maintaining it during the initial training phase , and ( 2 ) better sample-efficiency during fine-tuning . We clarified our goal in Section 1 and Section 6.1 . Also , to better reflect ( 1 ) , we added the performance of initial offline RL agents in all relevant performance plots in the revised manuscript . * * ( Q3 ) Fine-tuning a CQL agent with SAC ? * * ( A3 ) Thank you for pointing this out . As we have shown in Appendix A of the original manuscript , fine-tuning with the same regularized objective as CQL results in almost no ( if at all ) improvement of the agent , due to the regularization term inducing pessimistic updates . We used SAC updates for fine-tuning a CQL agent because CQL is built upon SAC in that it learns a stochastic policy and employs an automatic entropy tuning as in SAC . And indeed , with a careful approach , i.e. , balanced replay and ensemble distillation , SAC-like updates can lead to stable fine-tuning ( Figure 3 ) . However , to resolve the reviewer \u2019 s concern , we have conducted additional experiments using ( i ) TD3 [ 4 ] , a more stable variant of DDPG [ 5 ] , and ( ii ) SAC [ 6 ] with deterministic backup , i.e. , with the same objective as CQL except for the regularization term . As shown in Appendix C of the revised manuscript , these methods still suffer from unstable fine-tuning , especially when using online samples exclusively during fine-tuning . This suggests that instability is not due to the difference in optimization objectives , but more due to the distribution shift between offline and online samples . * * ( Q4 ) Hyperparameter selection . * * ( A4 ) We selected the hyperparameters based on grid search , where the selection criteria were sample-efficiency and fine-tuning stability . We agree with the reviewer that it is ideal to select the hyperparameters before fine-tuning , based on the offline dataset and the offline agent trained on it . However , we would like to point out that the task of evaluating an offline RL agent , namely , off-policy evaluation ( OPE ) , is a challenging research question ( See [ 7 ] for more details ) . Considering this , deciding on all hyperparameters for fine-tuning a priori is beyond the scope of our work . One rule of thumb we observed to work well , however , is that reduced policy learning rates result in better fine-tuning stability . This observation was also made in a recent work on fine-tuning robotic agents [ 8 ] . In addition , we added experimental results concerning different hyperparameter selections for balanced replay scheme in Appendix G and H in the revised manuscript . We observed that BRED is robust to $ \\rho_ { 0 } ^ { \\tt { on } } $ and $ t_ { \\tt { final } } $ in halfcheetah and most hopper tasks , but sometimes sensitive to them in tasks involving walker2d , which is known to be a difficult control task ."}, "3": {"review_id": "9hgEG-k57Zj-3", "review_text": "Summary - Offline RL allows agents to be trained on static , offline datasets . If the agent has already undergone offline training however , distribution shift makes further online training difficult . The authors propose to address this problem by keeping separate offline and online replay buffers , and sampling different proportions of each . In addition , they propose to ensemble a distillation policy . The proposed method is evaluated on logged MuJoCo datasets answering whether BRED improves over other fine-tuning methods and whether balancing or replay contribute more to BRED 's success . Decision -- BRED is an interesting approach , but the experiments do not demonstrate that it is better than the baseline . As such , my preliminary rating of this paper is rejection . I appreciate that the sampling scheme proposed is simple and well motivated to address the problem of distribution shift . It is not clear whether the sampling scheme actually addresses the problem of distribution shift however , as it gives the agent access to additional experience . The distillation scheme also does not seem to address distribution shift directly and the experiments do not demonstrate that it helps performance outside of the Walker2d-random task . Moreover , it is not clear whether the baselines are also ensembles , which would make the experiment results less compelling . Originality -- The contributions are : a sampling scheme combining offline and online samples , as well as a distillation scheme . The sampling scheme is incremental and simple , but seems like a novel approach . Distillation on the other hand , is well-explored in RL ( Czarnecki et al.2019 ) and does not seem to contribute much to the problem addressed in the paper . Taken together , the paper has limited novelty . Quality and Clarity - The paper is easy-to-follow and well-written . However , I have some issues with the title . The title suggests that your aim is to address distribution shift in online RL . However , distribution shift is a problem due to offline training . Of course , the problem you are addressing is further online training after offline training . Perhaps the title should be reworded to make this more clear . Strengths - There is a clear exposition of the problem addressed , namely distribution shift . This is well motivated by the t-SNE visualization . Section 3 in particular highlights the correlation between distribution shift and the performance of offline RL algorithms . - BRED , the proposed method , is a straightforward solution via balancing offline and online replay buffers . In addition , the schedule for updating the proportion sampled of each replay buffer is well argued and also simple . - The results are clearly motivated by questions in the beginning of the section . Sensible baselines are used to compare the proposed method ( BRED ) , and these are discussed and reasoned in detail . Furthermore , there is a good coverage of tasks ( 3 MuJoCo ) and difficulties ( random , medium , medium-replay , medium-expert ) . Weaknesses - - While fine-tuning an offline RL agent can pose challenges , similar challenges can arise in supervised learning . For example , Ash and Adams ( 2019 ) note that in different regimes , warm-starting neural network training can hamper generalization . Overall , I feel that the correlation between distribution shift and online performance after offline training is not shown to be causal , but correlative . - It is unclear why distillation is included , as it does not directly address the problem of distribution shift in offline RL , outside of the intuition that it mitigates bad $ Q $ estimates . This is not enough motivation however , and the experiment results ( Figure 5 and 6 ) does not show any significant benefit of ensemble distillation over independent ensembles ( besides Walker2d-random ) . Furthermore , this may not be a fair comparison to other baseline methods which do not seem to be ensembles . - Despite a very well-motivated experiment section , the experimental conclusions are far too strong . The results do not back this up , when there is high overlap in the confidence intervals for many Figures ( in each section ) . Furthermore , there are unexplained peculiarities in the results , such as very big drops in performance or the fact that the agent trained with the offline dataset is substantially better than the medium one . Detailed Comments -- - `` pointed out that offline RL algorithms\u2026 are not amenable to fine-tuning , due to the difficulty of modeling the dataset-generating policy in the online setup . '' This is not clear to me , how does fine-tuning connect to modelling the dataset-generating policy ? Furthermore , how does the dataset-generating policy ( I assume this is the behavior policy ) connect to online RL ? - Figure 1b : the agent is trained on the offline dataset and then that dataset is thrown away for the online agent , whereas the agent in red gets to keep using it . This does show the difficulty of fine-tuning but does not provide conclusive evidence that distributional shift is the cause . Fine-tuning can be difficult even in supervised learning ( Ash and Adams 2019 ) . - `` On the other hand , when using online samples exclusively , the agent is exposed to unseen samples only , for which Q function does not provide a reliable value estimate . This may lead to bootstrapping error , and hence a dip in performance as seen in Figure 1b . '' Ca n't this be shown experimentally in an environment to provide conclusive evidence for distribution shift being the cause of the dip in performance ? - Figure 3 : It seems that halfcheetah-random results in a better policy than halfcheetah-medium . This does not make sense to me . In addition , there is a strange dip in performance for CQL-ft in Figure 3g . Many of the figures have overlapping confidence intervals ( although the shaded region is only the standard deviation , the corresponding confidence interval would be approximately the standard deviation.Anyway these should be confidence intervals rather than standard deviation ) . While BRED does perform statistically significantly better in some tasks , I do n't think the comparison is fair due to ensembling used only for BRED . - Figure 4 : Going from figure 4a to 4b , balanced replay is shown to be more beneficial for a dataset generated by a random behavior policy . This seems to suggest that the main benefit of BRED is artificial exploration . Counter-intuitvely , this trend is reversed in Figure 4c . Moreover , the online-only curve is very different in Figure 4c than 4a and 4b , and to my understanding there should not be any difference . Due to the noisiness of the curves , its hard to make any straightforward conclusion from these results . - Figure 5 and 6 : Again , there is no statistical difference in medium and medium-replay between independent ensembles and a distillation ensemble . Minor Comments -- - Equation 3 : missing brackets on the left side . Ash , Jordan T. , and Ryan P. Adams . 2019. \u201c On Warm-Starting Neural Network Training. \u201d * arXiv:1910.08475 * . < http : //arxiv.org/abs/1910.08475v2 > . Czarnecki , Wojciech Marian , Razvan Pascanu , Simon Osindero , Siddhant M. Jayakumar , Grzegorz Swirszcz , and Max Jaderberg . 2019. \u201c Distilling Policy Distillation. \u201d * arXiv:1902.02186 * . < http : //arxiv.org/abs/1902.02186v1 > .", "rating": "3: Clear rejection", "reply_text": "Dear R2 , We sincerely appreciate your valuable and insightful comments . We found them extremely helpful for improving our manuscript . We address each comment in detail , one by one below . * * ( Q1 ) Clarification on the meaning of title . * * ( A1 ) We agree with the author that the title may be misleading , for we mainly deal with fine-tuning offline RL agents , not randomly initialized agents . Accordingly , we changed the title from \u2018 Addressing Distribution Shift in Online Reinforcement Learning with Offline Datasets \u2019 to \u2018 Addressing Distribution Shift in Offline-to-Online Reinforcement Learning \u2019 . * * ( Q2 ) Clarification on dataset-generating policy and its relationship to online RL . * * ( A2 ) Thank you for pointing out a possibly confusing point . A prior work [ 1 ] pointed out that it is not feasible to extend such offline RL methods as BCQ [ 2 ] and BEAR [ 3 ] to the fine-tuning setup . This is because these early methods require modeling the policy used to generate the offline dataset , i.e.behavior policy/dataset-generating policy , and training such density models in the online fine-tuning setup is challenging [ 1 ] . Instead , we consider building upon a more recent offline RL algorithm , conservative Q-learning ( CQL ) [ 4 ] , which does not require modeling the behavior policy . Accordingly , our method does not model the behavior policy . We clarified this in Section 1 . * * ( Q3 ) Is instability in fine-tuning due to distribution shift ? * * ( A3 ) We would like to point out that there have been several prior works where fine-tuning has been successfully applied for robot learning [ 5,6 ] . These works rely on an ample amount of diverse offline data , whereas we considered various scenarios where such large and diverse dataset may not be available , in which case distribution shift may hamper the fine-tuning process . Also , we would like to emphasize that fine-tuning a Q-learning RL agent poses a unique challenge when compared to supervised learning , for it involves bootstrapping - a process especially vulnerable to distribution shift . Indeed , a prior work [ 1 ] trained a SAC agent from scratch , but with a pre-populated replay buffer , i.e. , an agent that suffers from distribution shift , but not from the difficulty of fine-tuning a neural network . Thusly trained SAC agent is shown to perform worse than a SAC agent trained from scratch with an empty replay buffer . This observation shows that distribution shift is indeed a non-trivial challenge when training an RL agent . * * ( Q4 ) Justification of ensemble distillation . * * ( A4 ) Ensemble distillation provides further robustness to Q-function approximation error , by reducing policy variance . In order to demonstrate this , we conducted an additional experiment : for each update during training , given an observation in the sampled mini-batch , we sampled 10 actions from the policy being trained , then measured the variance of Q values among these 10 actions . Then we averaged the variance in Q values across the sampled minibatch , i.e. , $ \\mathbb { E } _ { s\\sim \\mathcal { B } , a\\sim \\pi ( \\cdot|s ) } \\big [ Q_\\theta ( s , a ) \\big ] $ . As shown in Appendix K , Figure 16 , Q values for actions thusly sampled are of much higher variance for the independent ensemble learner . This shows that by training a single distilled policy , we obtain a stability gain . * * ( Q5 ) Direct evidence of distribution shift . * * ( A5 ) In order to demonstrate the distribution shift in a more direct manner , we additionally trained a variational autoencoder ( VAE ) and measured the log-likelihoods of offline and online samples ( Figure 1b of the revised manuscript ) . We observe that offline and online samples follow different distributions , which evidences that there exists a distribution shift ( see our response A3 to R4 for more formal description of distribution shift ) . Specifically , we first split a given offline dataset into a training dataset and validation dataset . Then , we trained a VAE on the training offline dataset , and measured log-likelihood for ( 1 ) offline samples from the held out validation dataset and ( 2 ) online samples gathered by the offline agent . We included the experimental detail in Section 3 of the revised manuscript also . In order to show the harmful effects of distribution shift more directly , we conducted additional experiments with walker2d tasks . Similar to [ 7 ] , we compared the average Q value estimate over 1000 state-action pairs sampled from the replay buffer , and the average of their true Q values . As shown in Figure 17 from Appendix L of the revised manuscript , we see that in three out of four tasks ( walker2d-random , walker2d-medium-replay , and walker2d-medium-expert ) , Online-only agents suffer from severe overestimation bias and high variance , especially during the early fine-tuning stage . Experimental details are also provided in Appendix L of the revised manuscript ."}}