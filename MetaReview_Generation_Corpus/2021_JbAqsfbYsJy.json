{"year": "2021", "forum": "JbAqsfbYsJy", "title": "Action and Perception as Divergence Minimization", "decision": "Reject", "meta_review": "The paper presents an KL-divergence minimisation approach to the action\u2013perception loop, and thus presents a unifying view on concepts such as Empowerment, entropy-based RL, optimal control, etc.  The paper does two things here: it serves as a survey paper, but on top of that puts these in a unifying theory.  While the direct merit of that may not be obvious, it does serve as a good basis to combine the fields more formally.\n\nUnfortunately, the paper suffers from the length restrictions.  With more than half of the paper in the appendix, it should be published at a journal or directly at arXiv.   Not having a page limit would improve the readability much.  ICLR may not be the best venue for review papers.", "reviews": [{"review_id": "JbAqsfbYsJy-0", "review_text": "The authors proposed to use the joint KL divergence between the generative joint distribution and the target distribution ( containing latent variables which could correspond to latent parts we wanted to model ( e.g.beliefs ) .It was illustrative to discuss decomposing the joint KL into different ways and thus forming information bounds in different scenarios . The decomposition of past and future in Eq.6 also provided a unified perspective for looking at the most currently used objectives . The examples shown in the paper and appendix give a good illustration of how people can make assumptions or design the terms to convert prevalent objectives into objectives that follow from this joint KL divergence framework . This is , in my mind , one of their key contributions for connecting the past progress in a general and unified way . However , one concern about this paper is that the proposal of such a unified KL minimization framework is in fact a bit too general and abstract . In fact , many methods mentioned in this work shared a similar insight of deriving objectives from a KL-minimization perspective , but some factors are omitted to better fit the corresponding tasks . The general decomposition discussed in this paper provides little hint on how new objectives could be derived for problems . The general framework does somehow serve as the guideline , but my worry is that its effect will be limited as we still need to design the mapping for the terms in the general objective accordingly in different tasks . Given the pros and cons of this paper , I 'm putting a borderline decision for now . The authors should clear any of my misunderstandings and perhaps show the potential for this general framework as a source for new objectives . == After reading the authors rebuttal , my major concerns are fully addressed and I decide to keep my decision as weak accept", "rating": "6: Marginally above acceptance threshold", "reply_text": "> The general decomposition discussed in this paper provides little hint on how new objectives could be derived for problems . The general framework does somehow serve as the guideline , but my worry is that its effect will be limited as we still need to design the mapping for the terms in the general objective accordingly in different tasks . The implementation of an agent in practice is influenced by three components , namely the agent 's target distribution , belief family , and optimizer . This is analogous to the specification of a model , belief family , and optimizer in probabilistic modeling and the specification of an architecture , loss function , and optimizer in deep learning . Our paper studies the first component , namely different target distributions . To this end , we motivate the choice of expressive targets ( Section 2.2\u20132.4 ) and give examples of 8 concrete target distributions ( Sections 3 and A.1\u20137 ) that result in known objective functions . Due to the page limit , we had to include part of this in the appendix and we are aware that reviewers are not required to read it . Additionally , when the goal is to design an agent that learns a lot about the world , a practitioner would traditionally look for objective functions in the intrinsic motivation literature . However , it is not clear which of the many proposed objectives will result in an agent that learns the most about the world . Our paper shows that the model by which the agent understands the world is also its target distribution . Thus , the practitioner has a clear path toward implementing the desired agent . This path is to design a powerful world model that after learning assigns high probability to input trajectories in the environment . Using this model as the target distribution defines the agent objective . When additionally trying to solve practical tasks , rewards can be incorporated into the world model as reward factors ( e.g.Section 3 ) . Concrete examples of under-explored world models that we believe are worth implementing in the future include ( 1 ) temporally abstract latent state space models that are conditioned on latent skills , ( 2 ) hierarchical latent state space model , ( 3 ) models that structure their state space into weakly interacting groups that can learn to represent objects , and ( 3 ) the use of energy-based models as world models . Minimizing the joint KL to each of these constitutes a novel agent objective with different instances of the representation learning and exploration terms . We will add a paragraph on `` Designing novel objectives '' to the paper to include these intuitions and examples . > Given the pros and cons of this paper , I 'm putting a borderline decision for now . The authors should clear any of my misunderstandings and perhaps show the potential for this general framework as a source for new objectives . We hope that we have cleared up your concern about the relation between the joint objective for representation learning and control we study and previous applications of KL minimization . We hope the examples of novel agent objectives have helped make leveraging the framework concrete . Please let us know if this addresses your concerns or whether there are any other points we should address . References : - Lee et al.Stochastic latent actor-critic : Deep reinforcement learning with a latent variable model . NeurIPS 2020 . - Houthooft et al.VIME : Variational Information Maximizing Exploration . NeurIPS 2016 . - Sekar et al.Planning to Explore via Self-Supervised World Models . ICML 2020 ."}, {"review_id": "JbAqsfbYsJy-1", "review_text": "The authors formulate a general framework that unifies inference , action/perception , control , and several other tasks . The framework is based on minimizing the KL divergence between a parameterized `` actual '' distribution and a `` target '' distribution . The authors argue that this formulation unifies a wide range of previously proposed objectives . They also argue that it has some advantages when compared to Friston 's `` free energy principle '' framework , with which it shares many similarities , in particular that probability matching is preferred to surprise minimization . The paper is clearly-written and provides a very thorough literature review . However , generally I question the scientific value of such all-encompassing unifying frameworks , and this paper in particular offers no concrete formal or empirical results , while promising a lot . At the end of the day , the divergence minimization objective is nothing more than MaxEnt , decorated with various interpretations and decompositions . Without empirical support , I do not find the interpretations and decompositions very convincing -- as one example , does divergence minimization * really * mean that `` expressive world models lead to autonomous agents that understand and inhabit large niches '' ? One of the issues is that the paper appears to treat the `` heart of the matter '' ( i.e. , the source of interesting solutions ) as if it lay in the elegant and generic objective . In my opinion , however , the real heart of the matter will be encoded in ( 1 ) the structure of the target distribution , ( 2 ) the structure/parameterization of the actual distribution , and ( 3 ) the optimization algorithm that can actually minimize the ( typically ) high-dimensional objective . The quality of resulting solutions depend on 1-3 -- all of which need to be exogenously specified because divergence minimization can not on its own produce interesting behavior . At the end of the day , I do think there is some value in providing a unifying framework , and developing information-theoretic decompositions and interpretation . However , I think the paper would be * much * stronger if it was considerably longer and had more room to breathe ( which it does n't have right now -- given all the connections it tries to make ) , and if qualitative statements ( of the type discussed above ) were accompanied by empirical results ( even if simulations with simple toy models ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> the real heart of the matter will be encoded in ( 1 ) the structure of the target distribution , ( 2 ) the structure/parameterization of the actual distribution , and ( 3 ) the optimization algorithm that can actually minimize the ( typically ) high-dimensional objective That is absolutely correct these three components determine the agent 's behavior in practice . We advocate breaking down the problem of designing agents into the three components , as it allows studying the components separately and mix-and-match them when implementing agents . This is analogous to the specification of a model , belief family , and optimizer in probabilistic modeling and the specification of an architecture , loss function , and optimizer in deep learning . We believe that breaking down the problem of designing agents into the structure of the target distribution , parameterization of the actual distribution , and the optimization algorithm , as suggested by the APD framework , can accelerate progress in reinforcement learning research . Our paper studies the first of the three components you mentioned , namely the target distribution , we summarized in our response above . We completely agree that the remaining two components also matter in practice and are worth studying . Our framework provides a map of the possible agent objectives that result from expressive target distributions to guide future work in empirically exploring the space of possible agent implementations in a structured manner . > At the end of the day , I do think there is some value in providing a unifying framework , and developing information-theoretic decompositions and interpretation . However , I think the paper would be much stronger if it was considerably longer and had more room to breathe ( which it does n't have right now -- given all the connections it tries to make ) , and if qualitative statements ( of the type discussed above ) were accompanied by empirical results ( even if simulations with simple toy models ) . We completely agree that having more space would allow for more detailed explanations . Unfortunately , we are bound to the page limit of ICLR in this regard . That said , we are allowed an additional page for the final version of the paper . We will leverage this page to include an additional example of a target factorization from the appendix into the main paper . While working on this paper , we concluded that empirical simulations would likely distract from the main message of the paper . While we could implement concrete instances of APD , such as the target factorizations studied in Sections 3 and A1\u20137 , these objectives have successful implementations in the prior literature already . Instead , we prefer to use the available page limit for developing our main contribution , which is to analyze the effects of various constraints on the target distribution and to develop a unified perspective on many existing algorithms in the literature . We believe these conceptual contributions offer substantial value for the reinforcement learning community and stand on their own . References : - Lee et al.Stochastic latent actor-critic : Deep reinforcement learning with a latent variable model . NeurIPS 2020 . - Haarnoja et al.Soft actor-critic : Off-policy maximum entropy deep reinforcement learning with a stochastic actor . ICML 2018 ."}, {"review_id": "JbAqsfbYsJy-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : In this manuscript , the authors propose a unifying framework for a large class of inference and reinforcement learning objectives , which have been studied in prior works by various authors . They demonstrate that approaches and central ideas from many different fields in the ML/AI community can be derived as limiting cases of their framework . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I vote for acceptance ( 7 ) . Like many , I have employed various variational approaches in the past and see its merit . While I agree with the main idea , this work is not without problems . This is especially problematic for such a broadly applicable work that will most likely influence plenty of future research . My main problems with this submission are : 1 . Presentation . While the paper is , for the most part , well written and well organized , there are some gaps/ jumps that render understanding difficult . Two examples : A ) The parameters \\phi . The authors start by introducing parameters \\phi as abstract placeholders for ( i ) parameters of the true joint distribution of data and latents of the underlying system and ( ii ) a set of actions an agent can perform to interact with this world . The agent 's target distribution has no explicit parameter dependence . So far , so good . Then , one is redirected to the appendix A.1 and A.2 . Section A.1 is already a bit confusing because suddenly , additional latents w are introduced that were not mentioned before . Then , suddenly in A.2 . the target \\tau is suddenly dependent on the parameters \\phi , which were initially parameters of the underlying system 's true joint distribution . This also happens in Figure 2 . C ) , which is also never referenced in the text . I find this strange mixing of parameters of agent and system very confusing . It also sheds some doubt about the generality of the framework . B ) I have read the paper carefully and still do not understand Figure 1 completely . This may also be due to the reason that it is only referenced in the appendix . Related : Why does Information gain play such a central role if all derived objectives only contain ( upper ) bounds for it appear ? 2 . ( Unsupported ) Claims : In the abstract , the authors promise to offer `` a recipe for designing novel objectives '' . As much as I can see , they only come back to this promise in the conclusion , where they say that one could look at other divergence measures to arrive at new objectives , and they will leave it for future work . I would not call this a recipe , but an outlook at most . 3.Too many ideas : It is hard , if not impossible , to explain a broad framework well in a conference proceeding . This work contains so many ideas and establishes many connections that following this work , and understanding them in detail becomes very hard . I would suggest sacrificing some connections in favor of a more concise presentation . 4.Fixation on KL-divergence : This is more of a suggestion . I understand that many works use the ( non-symmetric ) KL due to its favorable analytic properties . Thus , I agree that it makes sense to focus this framework on this measure . However , I believe this work 's main idea still holds if one would exchange the KL with some other measure of similarity between distributions . Maybe it would make sense to first introduce and discuss the abstract idea of aligning target and belief before fixation on a particular measure . This would also go well with resolving my concern 2 .. # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . Unifying framework of many inference , and RL objectives . 2.Well written . 3.Will be impactful to a lot of future research . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . See my Reasons for score . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during the rebuttal period : Please address and clarify the cons above . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Minor : \u00b7 Please consider citing Toussaint , M. , & Storkey , A . ( 2006 ) .Probabilistic inference for solving discrete and continuous state Markov Decision Processes . International Conference on Machine Learning ( ICML ) , 945\u2013952 . https : //doi.org/10.1145/1143844.1143963 in the `` control as inference '' section . To my knowledge , it is one of the first to establish the connection between planning and inference .", "rating": "7: Good paper, accept", "reply_text": "> In the abstract , the authors promise to offer `` a recipe for designing novel objectives '' . As much as I can see , they only come back to this promise in the conclusion , where they say that one could look at other divergence measures to arrive at new objectives , and they will leave it for future work . The presented framework suggests at least two ways of deriving novel objectives . The first approach is to study divergence measures other than KL , as you have identified . The second approach is to stick to the KL and change the three components that influence the agent , namely its target distribution , belief family , and optimizer . This is analogous to the specification of a model , belief family , and optimizer in probabilistic modeling and the specification of an architecture , loss function , and optimizer in deep learning . Our paper studies the first component , namely different target distributions . To this end , we motivate the choice of expressive targets ( Section 2.2\u20132.4 ) and give examples of 8 concrete target distributions ( Sections 3 and A.1\u20137 ) that result in known objective functions . Due to the page limit , we had to include part of this in the appendix and we are aware that reviewers are not required to read it . Additionally , when the goal is to design an agent that learns a lot about the world , a practitioner would traditionally look for objective functions in the intrinsic motivation literature . However , it is not clear which of the many proposed objectives will result in an agent that learns the most about the world . Our paper shows that the model by which the agent understands the world is also its target distribution . Thus , the practitioner has a clear path toward implementing the desired agent . This path is to design a powerful world model that after learning assigns high probability to input trajectories in the environment . Using this model as the target distribution defines the agent objective . When additionally trying to solve practical tasks , rewards can be incorporated into the world model as reward factors ( e.g.Section 3 ) . Concrete examples of underexplored world models that we believe are worth implementing in the future include ( 1 ) temporally abstract latent state space models that are conditioned on latent skills , ( 2 ) hierarchical latent state space model , ( 3 ) models that structure their state space into weakly interacting groups that can learn to represent objects , and ( 3 ) the use of energy-based models as world models . Minimizing the joint KL to each of these constitutes a novel agent objective with different instances of the representation learning and exploration terms . We have added a paragraph about `` Designing novel objectives '' in Section 3 to include these intuitions and examples . > I have read the paper carefully and still do not understand Figure 1 completely . This may also be due to the reason that it is only referenced in the appendix . Thank you for pointing this out we have added a reference to Figure 1 in Section 3 . The figure gives an overview of common concrete objectives within our introduced framework . Most of the nodes correspond to a section in the paper ( Sections 3 and A.1\u20137 ) . The two exceptions are `` expected reward '' ( penultimate paragraph of Section A.4 ) and `` maximum likelihood '' ( last paragraph of Section A.1 ) ."}, {"review_id": "JbAqsfbYsJy-3", "review_text": "The authors of this paper propose a unified optimisation objective for ( sequential ) decision-making ( i.e. , _action_ ) and representation learning ( i.e. , _perception_ ) , built on joint ( KL ) divergence minimisation . As also mentioned by the authors , this is a concept paper and it includes no empirical study . In particular , the authors demonstrate how existing ideas and approaches to ( sequential ) decision-making and representation learning can be expressed as a joint KL minimisation problem between a target and `` actual '' distribution . Such examples are ( a ) MaxEnt RL , ( b ) VI , ( c ) amortised VI , ( d ) KL control , ( e ) skill discovery and ( f ) empowerment , which are all cases of the KL minimisation between a target and an `` actual '' distributions . * * Concerns * * : 1 . Although the proposed perspective and language is rich and expressive , I question the novelty of the proposed framework , since the information-theoretic view of decision-making and perception is a rather established and old idea , even the term/idea of perception-action cycle is already defined [ 1 ] ! 2.The power of latent variables for decision-making and their interpretation is also a known idea [ 1 ] . * * References * * [ 1 ] Tishby , N. and Polani , D. , 2011 . Information theory of decisions and actions . In Perception-action cycle ( pp.601-636 ) .Springer , New York , NY .", "rating": "3: Clear rejection", "reply_text": "> The power of latent variables for decision-making and their interpretation is also a known idea [ 1 ] . Tishby & Polani ( 2011 ) only consider sensory inputs and actions , but not latent representations , such as latent state representations or latent model parameters that are the focus of our work . The research problem of unifying variational representation learning and control as inference has been posed in the SLAC paper ( Lee et al. , 2019 ) . The authors suggest a solution , however , they miss the exploration terms that should complement with representation learning terms to result in an objective that is consistent in time . Our paper considers 3 types of latent variables for decision-making ( Section 2.4 ) , namely future actions , future skills , and latent representations . Out of these , the objective function studied by Tishby & Polani ( 2011 ) only includes actions and only in the case of a factorized target distribution . Thus , their approach only includes entropy regularization for inputs and actions , but not variational representation learning , information gain , or empowerment . They neither consider latent representations , which are random variables that are never realized ( also see Section A.7 ) , nor skills , which are random variables that condition some number of actions and become realized during environment interaction ( also see Section A.6 ) . As our paper shows , an expressive target distribution leads to maximizing the mutual information between the internal latent variables and the sequence of sensory inputs ( Section 2.2 ) . This lets us show the different effects of the 3 types of latents on decision making . First , as past actions/skills are observed , their MI with past inputs is constant and they contribute no past terms . Second , latent representations are never observed and thus increase their MI with past inputs via a reconstruction loss . Third , all types of latents increase their MI with future inputs , known as information gain exploration and empowerment ( Section 2.4 , also see Section A.6\u20137 ) . In summary , our paper studies information maximizing agents , offers a solution to the question of integrating latent representations with control as inference as posed by Lee et al . ( 2019 ) to unify various KL objectives in the literature , and substantially expands our understanding of the different types of latent variables for decision-making over Tishby & Polani ( 2011 ) and other prior works . References : - Tishby & Polani . Information theory of decisions and actions . 2011.- Lee et al.Stochastic latent actor-critic : Deep reinforcement learning with a latent variable model . NeurIPS 2020 ."}], "0": {"review_id": "JbAqsfbYsJy-0", "review_text": "The authors proposed to use the joint KL divergence between the generative joint distribution and the target distribution ( containing latent variables which could correspond to latent parts we wanted to model ( e.g.beliefs ) .It was illustrative to discuss decomposing the joint KL into different ways and thus forming information bounds in different scenarios . The decomposition of past and future in Eq.6 also provided a unified perspective for looking at the most currently used objectives . The examples shown in the paper and appendix give a good illustration of how people can make assumptions or design the terms to convert prevalent objectives into objectives that follow from this joint KL divergence framework . This is , in my mind , one of their key contributions for connecting the past progress in a general and unified way . However , one concern about this paper is that the proposal of such a unified KL minimization framework is in fact a bit too general and abstract . In fact , many methods mentioned in this work shared a similar insight of deriving objectives from a KL-minimization perspective , but some factors are omitted to better fit the corresponding tasks . The general decomposition discussed in this paper provides little hint on how new objectives could be derived for problems . The general framework does somehow serve as the guideline , but my worry is that its effect will be limited as we still need to design the mapping for the terms in the general objective accordingly in different tasks . Given the pros and cons of this paper , I 'm putting a borderline decision for now . The authors should clear any of my misunderstandings and perhaps show the potential for this general framework as a source for new objectives . == After reading the authors rebuttal , my major concerns are fully addressed and I decide to keep my decision as weak accept", "rating": "6: Marginally above acceptance threshold", "reply_text": "> The general decomposition discussed in this paper provides little hint on how new objectives could be derived for problems . The general framework does somehow serve as the guideline , but my worry is that its effect will be limited as we still need to design the mapping for the terms in the general objective accordingly in different tasks . The implementation of an agent in practice is influenced by three components , namely the agent 's target distribution , belief family , and optimizer . This is analogous to the specification of a model , belief family , and optimizer in probabilistic modeling and the specification of an architecture , loss function , and optimizer in deep learning . Our paper studies the first component , namely different target distributions . To this end , we motivate the choice of expressive targets ( Section 2.2\u20132.4 ) and give examples of 8 concrete target distributions ( Sections 3 and A.1\u20137 ) that result in known objective functions . Due to the page limit , we had to include part of this in the appendix and we are aware that reviewers are not required to read it . Additionally , when the goal is to design an agent that learns a lot about the world , a practitioner would traditionally look for objective functions in the intrinsic motivation literature . However , it is not clear which of the many proposed objectives will result in an agent that learns the most about the world . Our paper shows that the model by which the agent understands the world is also its target distribution . Thus , the practitioner has a clear path toward implementing the desired agent . This path is to design a powerful world model that after learning assigns high probability to input trajectories in the environment . Using this model as the target distribution defines the agent objective . When additionally trying to solve practical tasks , rewards can be incorporated into the world model as reward factors ( e.g.Section 3 ) . Concrete examples of under-explored world models that we believe are worth implementing in the future include ( 1 ) temporally abstract latent state space models that are conditioned on latent skills , ( 2 ) hierarchical latent state space model , ( 3 ) models that structure their state space into weakly interacting groups that can learn to represent objects , and ( 3 ) the use of energy-based models as world models . Minimizing the joint KL to each of these constitutes a novel agent objective with different instances of the representation learning and exploration terms . We will add a paragraph on `` Designing novel objectives '' to the paper to include these intuitions and examples . > Given the pros and cons of this paper , I 'm putting a borderline decision for now . The authors should clear any of my misunderstandings and perhaps show the potential for this general framework as a source for new objectives . We hope that we have cleared up your concern about the relation between the joint objective for representation learning and control we study and previous applications of KL minimization . We hope the examples of novel agent objectives have helped make leveraging the framework concrete . Please let us know if this addresses your concerns or whether there are any other points we should address . References : - Lee et al.Stochastic latent actor-critic : Deep reinforcement learning with a latent variable model . NeurIPS 2020 . - Houthooft et al.VIME : Variational Information Maximizing Exploration . NeurIPS 2016 . - Sekar et al.Planning to Explore via Self-Supervised World Models . ICML 2020 ."}, "1": {"review_id": "JbAqsfbYsJy-1", "review_text": "The authors formulate a general framework that unifies inference , action/perception , control , and several other tasks . The framework is based on minimizing the KL divergence between a parameterized `` actual '' distribution and a `` target '' distribution . The authors argue that this formulation unifies a wide range of previously proposed objectives . They also argue that it has some advantages when compared to Friston 's `` free energy principle '' framework , with which it shares many similarities , in particular that probability matching is preferred to surprise minimization . The paper is clearly-written and provides a very thorough literature review . However , generally I question the scientific value of such all-encompassing unifying frameworks , and this paper in particular offers no concrete formal or empirical results , while promising a lot . At the end of the day , the divergence minimization objective is nothing more than MaxEnt , decorated with various interpretations and decompositions . Without empirical support , I do not find the interpretations and decompositions very convincing -- as one example , does divergence minimization * really * mean that `` expressive world models lead to autonomous agents that understand and inhabit large niches '' ? One of the issues is that the paper appears to treat the `` heart of the matter '' ( i.e. , the source of interesting solutions ) as if it lay in the elegant and generic objective . In my opinion , however , the real heart of the matter will be encoded in ( 1 ) the structure of the target distribution , ( 2 ) the structure/parameterization of the actual distribution , and ( 3 ) the optimization algorithm that can actually minimize the ( typically ) high-dimensional objective . The quality of resulting solutions depend on 1-3 -- all of which need to be exogenously specified because divergence minimization can not on its own produce interesting behavior . At the end of the day , I do think there is some value in providing a unifying framework , and developing information-theoretic decompositions and interpretation . However , I think the paper would be * much * stronger if it was considerably longer and had more room to breathe ( which it does n't have right now -- given all the connections it tries to make ) , and if qualitative statements ( of the type discussed above ) were accompanied by empirical results ( even if simulations with simple toy models ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "> the real heart of the matter will be encoded in ( 1 ) the structure of the target distribution , ( 2 ) the structure/parameterization of the actual distribution , and ( 3 ) the optimization algorithm that can actually minimize the ( typically ) high-dimensional objective That is absolutely correct these three components determine the agent 's behavior in practice . We advocate breaking down the problem of designing agents into the three components , as it allows studying the components separately and mix-and-match them when implementing agents . This is analogous to the specification of a model , belief family , and optimizer in probabilistic modeling and the specification of an architecture , loss function , and optimizer in deep learning . We believe that breaking down the problem of designing agents into the structure of the target distribution , parameterization of the actual distribution , and the optimization algorithm , as suggested by the APD framework , can accelerate progress in reinforcement learning research . Our paper studies the first of the three components you mentioned , namely the target distribution , we summarized in our response above . We completely agree that the remaining two components also matter in practice and are worth studying . Our framework provides a map of the possible agent objectives that result from expressive target distributions to guide future work in empirically exploring the space of possible agent implementations in a structured manner . > At the end of the day , I do think there is some value in providing a unifying framework , and developing information-theoretic decompositions and interpretation . However , I think the paper would be much stronger if it was considerably longer and had more room to breathe ( which it does n't have right now -- given all the connections it tries to make ) , and if qualitative statements ( of the type discussed above ) were accompanied by empirical results ( even if simulations with simple toy models ) . We completely agree that having more space would allow for more detailed explanations . Unfortunately , we are bound to the page limit of ICLR in this regard . That said , we are allowed an additional page for the final version of the paper . We will leverage this page to include an additional example of a target factorization from the appendix into the main paper . While working on this paper , we concluded that empirical simulations would likely distract from the main message of the paper . While we could implement concrete instances of APD , such as the target factorizations studied in Sections 3 and A1\u20137 , these objectives have successful implementations in the prior literature already . Instead , we prefer to use the available page limit for developing our main contribution , which is to analyze the effects of various constraints on the target distribution and to develop a unified perspective on many existing algorithms in the literature . We believe these conceptual contributions offer substantial value for the reinforcement learning community and stand on their own . References : - Lee et al.Stochastic latent actor-critic : Deep reinforcement learning with a latent variable model . NeurIPS 2020 . - Haarnoja et al.Soft actor-critic : Off-policy maximum entropy deep reinforcement learning with a stochastic actor . ICML 2018 ."}, "2": {"review_id": "JbAqsfbYsJy-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : In this manuscript , the authors propose a unifying framework for a large class of inference and reinforcement learning objectives , which have been studied in prior works by various authors . They demonstrate that approaches and central ideas from many different fields in the ML/AI community can be derived as limiting cases of their framework . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I vote for acceptance ( 7 ) . Like many , I have employed various variational approaches in the past and see its merit . While I agree with the main idea , this work is not without problems . This is especially problematic for such a broadly applicable work that will most likely influence plenty of future research . My main problems with this submission are : 1 . Presentation . While the paper is , for the most part , well written and well organized , there are some gaps/ jumps that render understanding difficult . Two examples : A ) The parameters \\phi . The authors start by introducing parameters \\phi as abstract placeholders for ( i ) parameters of the true joint distribution of data and latents of the underlying system and ( ii ) a set of actions an agent can perform to interact with this world . The agent 's target distribution has no explicit parameter dependence . So far , so good . Then , one is redirected to the appendix A.1 and A.2 . Section A.1 is already a bit confusing because suddenly , additional latents w are introduced that were not mentioned before . Then , suddenly in A.2 . the target \\tau is suddenly dependent on the parameters \\phi , which were initially parameters of the underlying system 's true joint distribution . This also happens in Figure 2 . C ) , which is also never referenced in the text . I find this strange mixing of parameters of agent and system very confusing . It also sheds some doubt about the generality of the framework . B ) I have read the paper carefully and still do not understand Figure 1 completely . This may also be due to the reason that it is only referenced in the appendix . Related : Why does Information gain play such a central role if all derived objectives only contain ( upper ) bounds for it appear ? 2 . ( Unsupported ) Claims : In the abstract , the authors promise to offer `` a recipe for designing novel objectives '' . As much as I can see , they only come back to this promise in the conclusion , where they say that one could look at other divergence measures to arrive at new objectives , and they will leave it for future work . I would not call this a recipe , but an outlook at most . 3.Too many ideas : It is hard , if not impossible , to explain a broad framework well in a conference proceeding . This work contains so many ideas and establishes many connections that following this work , and understanding them in detail becomes very hard . I would suggest sacrificing some connections in favor of a more concise presentation . 4.Fixation on KL-divergence : This is more of a suggestion . I understand that many works use the ( non-symmetric ) KL due to its favorable analytic properties . Thus , I agree that it makes sense to focus this framework on this measure . However , I believe this work 's main idea still holds if one would exchange the KL with some other measure of similarity between distributions . Maybe it would make sense to first introduce and discuss the abstract idea of aligning target and belief before fixation on a particular measure . This would also go well with resolving my concern 2 .. # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . Unifying framework of many inference , and RL objectives . 2.Well written . 3.Will be impactful to a lot of future research . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . See my Reasons for score . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during the rebuttal period : Please address and clarify the cons above . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Minor : \u00b7 Please consider citing Toussaint , M. , & Storkey , A . ( 2006 ) .Probabilistic inference for solving discrete and continuous state Markov Decision Processes . International Conference on Machine Learning ( ICML ) , 945\u2013952 . https : //doi.org/10.1145/1143844.1143963 in the `` control as inference '' section . To my knowledge , it is one of the first to establish the connection between planning and inference .", "rating": "7: Good paper, accept", "reply_text": "> In the abstract , the authors promise to offer `` a recipe for designing novel objectives '' . As much as I can see , they only come back to this promise in the conclusion , where they say that one could look at other divergence measures to arrive at new objectives , and they will leave it for future work . The presented framework suggests at least two ways of deriving novel objectives . The first approach is to study divergence measures other than KL , as you have identified . The second approach is to stick to the KL and change the three components that influence the agent , namely its target distribution , belief family , and optimizer . This is analogous to the specification of a model , belief family , and optimizer in probabilistic modeling and the specification of an architecture , loss function , and optimizer in deep learning . Our paper studies the first component , namely different target distributions . To this end , we motivate the choice of expressive targets ( Section 2.2\u20132.4 ) and give examples of 8 concrete target distributions ( Sections 3 and A.1\u20137 ) that result in known objective functions . Due to the page limit , we had to include part of this in the appendix and we are aware that reviewers are not required to read it . Additionally , when the goal is to design an agent that learns a lot about the world , a practitioner would traditionally look for objective functions in the intrinsic motivation literature . However , it is not clear which of the many proposed objectives will result in an agent that learns the most about the world . Our paper shows that the model by which the agent understands the world is also its target distribution . Thus , the practitioner has a clear path toward implementing the desired agent . This path is to design a powerful world model that after learning assigns high probability to input trajectories in the environment . Using this model as the target distribution defines the agent objective . When additionally trying to solve practical tasks , rewards can be incorporated into the world model as reward factors ( e.g.Section 3 ) . Concrete examples of underexplored world models that we believe are worth implementing in the future include ( 1 ) temporally abstract latent state space models that are conditioned on latent skills , ( 2 ) hierarchical latent state space model , ( 3 ) models that structure their state space into weakly interacting groups that can learn to represent objects , and ( 3 ) the use of energy-based models as world models . Minimizing the joint KL to each of these constitutes a novel agent objective with different instances of the representation learning and exploration terms . We have added a paragraph about `` Designing novel objectives '' in Section 3 to include these intuitions and examples . > I have read the paper carefully and still do not understand Figure 1 completely . This may also be due to the reason that it is only referenced in the appendix . Thank you for pointing this out we have added a reference to Figure 1 in Section 3 . The figure gives an overview of common concrete objectives within our introduced framework . Most of the nodes correspond to a section in the paper ( Sections 3 and A.1\u20137 ) . The two exceptions are `` expected reward '' ( penultimate paragraph of Section A.4 ) and `` maximum likelihood '' ( last paragraph of Section A.1 ) ."}, "3": {"review_id": "JbAqsfbYsJy-3", "review_text": "The authors of this paper propose a unified optimisation objective for ( sequential ) decision-making ( i.e. , _action_ ) and representation learning ( i.e. , _perception_ ) , built on joint ( KL ) divergence minimisation . As also mentioned by the authors , this is a concept paper and it includes no empirical study . In particular , the authors demonstrate how existing ideas and approaches to ( sequential ) decision-making and representation learning can be expressed as a joint KL minimisation problem between a target and `` actual '' distribution . Such examples are ( a ) MaxEnt RL , ( b ) VI , ( c ) amortised VI , ( d ) KL control , ( e ) skill discovery and ( f ) empowerment , which are all cases of the KL minimisation between a target and an `` actual '' distributions . * * Concerns * * : 1 . Although the proposed perspective and language is rich and expressive , I question the novelty of the proposed framework , since the information-theoretic view of decision-making and perception is a rather established and old idea , even the term/idea of perception-action cycle is already defined [ 1 ] ! 2.The power of latent variables for decision-making and their interpretation is also a known idea [ 1 ] . * * References * * [ 1 ] Tishby , N. and Polani , D. , 2011 . Information theory of decisions and actions . In Perception-action cycle ( pp.601-636 ) .Springer , New York , NY .", "rating": "3: Clear rejection", "reply_text": "> The power of latent variables for decision-making and their interpretation is also a known idea [ 1 ] . Tishby & Polani ( 2011 ) only consider sensory inputs and actions , but not latent representations , such as latent state representations or latent model parameters that are the focus of our work . The research problem of unifying variational representation learning and control as inference has been posed in the SLAC paper ( Lee et al. , 2019 ) . The authors suggest a solution , however , they miss the exploration terms that should complement with representation learning terms to result in an objective that is consistent in time . Our paper considers 3 types of latent variables for decision-making ( Section 2.4 ) , namely future actions , future skills , and latent representations . Out of these , the objective function studied by Tishby & Polani ( 2011 ) only includes actions and only in the case of a factorized target distribution . Thus , their approach only includes entropy regularization for inputs and actions , but not variational representation learning , information gain , or empowerment . They neither consider latent representations , which are random variables that are never realized ( also see Section A.7 ) , nor skills , which are random variables that condition some number of actions and become realized during environment interaction ( also see Section A.6 ) . As our paper shows , an expressive target distribution leads to maximizing the mutual information between the internal latent variables and the sequence of sensory inputs ( Section 2.2 ) . This lets us show the different effects of the 3 types of latents on decision making . First , as past actions/skills are observed , their MI with past inputs is constant and they contribute no past terms . Second , latent representations are never observed and thus increase their MI with past inputs via a reconstruction loss . Third , all types of latents increase their MI with future inputs , known as information gain exploration and empowerment ( Section 2.4 , also see Section A.6\u20137 ) . In summary , our paper studies information maximizing agents , offers a solution to the question of integrating latent representations with control as inference as posed by Lee et al . ( 2019 ) to unify various KL objectives in the literature , and substantially expands our understanding of the different types of latent variables for decision-making over Tishby & Polani ( 2011 ) and other prior works . References : - Tishby & Polani . Information theory of decisions and actions . 2011.- Lee et al.Stochastic latent actor-critic : Deep reinforcement learning with a latent variable model . NeurIPS 2020 ."}}