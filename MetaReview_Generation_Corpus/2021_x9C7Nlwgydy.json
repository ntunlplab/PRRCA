{"year": "2021", "forum": "x9C7Nlwgydy", "title": "Consensus Clustering with Unsupervised Representation Learning", "decision": "Reject", "meta_review": "This paper proposes a model for learning using ensemble clustering. The reviewers found the general idea promising.\nHowever, while promising, all reviewers noted that in its curent form the paper is not fit for publication. The reviewers pointed out missing references, issues with the abstract, lack of motivation for some of the algorithmic choices, limited novelty over clarity in the description of difference w.r.t. previous work.\nBecause of all these reasons, this paper does not meet the bar of acceptance. I recommend the authors take into account the feedback provided in the reviews and discussion and resubmit to another venue.", "reviews": [{"review_id": "x9C7Nlwgydy-0", "review_text": "The authors propose a learning-based approach for image clustering . In particular , similarly to recent algorithms fro unsupervised representation learning , such a DeepCluster , they propose to iterate between clustering the images in the feature space of the network and updating the network weights to respect the clusters . Two main differences with respect to DeepCluster-like algorithms is that they target the task of clustering itself , and do not evaluate the generalizability of the leaned representations for other tasks , and that they propose to use cluster ensembles to improve training robustness . In particular , they generate 2 clusterings of the images at every iteration by applying different sets of data augmentations and feature transformations to the input images . The objective is then not only to respect these clusterings but also to enforce consistency between them over time , thus improving representation invariance to irrelevant image details . In an experimental evaluation on a set of standard image clustering benchmarks they outperform prior work in most scenarios . The idea is reasonable and the method seems sound , however a similar approach has been proposed in Zhuang et al. , ICCV'19 ( Local Aggregation for Unsupervised Learning of Visual Embeddings ) . In contrast to this work , the authors of Zhuang et al. , used different runs of the clustering algorithm to obtain diverse clusterings , instead of transforming the images , but the overall approach is very similar . The authors seem to be not aware of that work . In the current from it is not clear whether the proposed approach has any advantages over Zhuang et al.In the rebuttal the authors need to provide a discussion of their novelty with respect to that method as well as an experimental comparison on ImageNet . I appreciate the authors ' efforts to fairly compare to Zhuang et al.in the rebuttal , and I do find the preliminary evidence sufficient to establish that their approach outperforms LA on clustering metrics . However , the authors seem to miss the the point that LA is not just another consensus clustering approach which they forgot to include into literature review since it does not report clustering metrics . The main contribution of their work is combining consensus clustering with representation learning , which is exactly what the authors of LA had done before . It does seems that the particular approach proposed in this paper results in a better clustering performance , so the submission contains a valid contribution , but the relationship between the two methods needs to be discussed in a lot more detail , and they have to be throughly compared experimentally . I encourage the authors to improve the manuscript in this direction and resubmit to a different venue .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank Reviewer 3 for the time and the feedback . * * 1 : * * `` In the current form it is not clear whether the proposed approach has any advantages over Zhuang et al.In the rebuttal the authors need to provide a discussion of their novelty with respect to that method as well as an experimental comparison on ImageNet . '' * * A : * * We thank the reviewer for bringing Zhuang et al. , ICCV \u2019 19 to our notice . It is unfortunate that we did not come across this paper . We added this discussion to the revised document . Our main contribution is to formulate a consensus function using feature space transformations . The current form of the algorithm ( and the consensus function ) were chosen based on the soft clustering block used , and it can be easily adapted to other soft clustering approaches ( such as IIC [ B ] ) . Whereas , the main contribution of LA is a novel local aggregation objective ( which specifically follows from IR tasks ) which uses a closest neighbors set generated through multiple runs of K-means clustering . Differences : Firstly , [ A ] does not show any clustering metric performance . It is not clear if [ A ] outperforms the other baselines discussed in our paper if evaluated on clustering metrics , as [ A ] uses labelled data to do evaluation ( linear evaluation ) . [ A ] builds a novel Local Aggregation method on non-parametric instance discrimination tasks . These techniques usually require a large memory bank which is memory intensive . Our algorithm does n't require memory intensive techniques like memory banks . During training , at each iteration [ A ] requires a nearest neighbor computation . Our method requires running a fast version of the iterative Sinkhorn-Knopp algorithm at each step [ C ] . The way in which we generate the ensemble allows us to control and measure the diversity of the ensemble which can be useful in making algorithm design choices . Although [ A ] controls the ensemble by varying $ H $ ( number of clusterings ) and $ m $ ( number of clusters in each clustering ) which aptly suits the objective they are solving , their ensembles are limited to use k-means clustering . The authors show that other clustering approaches are either not scalable , or are not optimal . In our case , by applying feature space transformations , we have much more freedom in generating the ensemble . We used random projections , diagonal transformations , but there could be other transformations on the feature space . * * REFERENCES * * [ A ] Zhuang , Chengxu , Alex Lin Zhai , and Daniel Yamins . `` Local aggregation for unsupervised learning of visual embeddings . '' Proceedings of the IEEE International Conference on Computer Vision . 2019 . [ B ] Ji , Xu , Jo\u00e3o F. Henriques , and Andrea Vedaldi . `` Invariant information clustering for unsupervised image classification and segmentation . '' Proceedings of the IEEE International Conference on Computer Vision . 2019 . [ C ] Caron , Mathilde , et al . `` Unsupervised learning of visual features by contrasting cluster assignments . '' Advances in Neural Information Processing Systems 33 ( 2020 ) ."}, {"review_id": "x9C7Nlwgydy-1", "review_text": "This paper studies the effect of combining ensemble learning approaches with deep clustering . The paper wants to show that ensemble learning methods , in particular consensus clustering , can improve the clustering accuracy when combined with general representation learning/clustering blocks . However , I am not sure that the results presented in the paper are enough to support the claims . The paper 's pros are : ( + ) It Is the first to combine ensemble methods with deep clustering models . Although ensemble methods have been widely applied , studying consensus clustering in the current problem setting is novel . ( + ) It Is the first to be able to have an ensemble deep clustering algorithm that gains empirically over other state-of-the-art models , showing the ideas to be potentially effective . ( + ) The writing is in general clear and undestandable . The paper 's cons are : ( - ) The wording in the abstract is a bit confusing in the sense that after reading it one might think the algorithm does consensus clustering first and uses the clustering to learn better representations of the input data . Although this is clarified later in the main body . ( - ) The description of the main algorithm seems to be more intuitive than innovative . Some algorimic design choices are not very convincing . For example , the choice of using random projections on embedding to produce different clusterings , although an interesting idea , makes me wonder why it is necessarily a good way to introduce randomness into the whole framework . The authors can expand their discussion on this . I also remain dubious about why different representation instead of different clustering methods Also , since the authors meant the idea to be applicable to general representation learning/deep clustering blocks , I 'm not sure the current experimental data in the paper can lead to that conclusion . ( - ) Using the performance metrics provided by the authors , I find it a bit hard to conclude that the proposed algorithm has a significant advantage over state of the art methods , especially PICA . Also , the fluctuation in performance metrics caused by different parameter settings seems to be , in magnitude , at least comparable to the margin of ConCURL over the baselines . Overall , I think this paper contains interesting seed ideas such as combining consensus clustering with representation learning and making use of the learned representation to generate multiple clusterings . These seed ideas could be good for this venue . However , the work is still premature and flawed by crude algorithm/experiment design . The quality can be significantly improved if the authors can give a more general algorithmic framework ( since the authors meant the ideas to be applicable to general representation learning/clustering algorithms ) , equipped with more thorough experimental investigation to support the applicability and superiority of the current approach .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank Reviewer 2 for the time and the feedback . * * 1 : * * \u201c For example , the choice of using random projections on embedding to produce different clusterings , although an interesting idea , makes me wonder why it is necessarily a good way to introduce randomness into the whole framework . The authors can expand their discussion on this . I also remain dubious about why different representation instead of different clustering methods \u201d * * A : * * We thank the reviewer for raising this interesting point . Indeed , there are many ways to introduce randomness into the framework . The next paragraph will explain why we preferred to use feature transformations as compared to multiple algorithms . Then , we discuss why we use Random Projections/Diagonal transformations on the representations Ideally , it is possible to use different clustering methods to generate the ensemble and it was pursued in the Consensus Clustering literature . However , since we are learning representations simultaneously using backpropagation , it is natural to use a clustering algorithm in which we can update its parameters via backpropagation . It is in-fact possible to use clustering algorithms like IIC that enable parameter updates , however , if multiple such algorithms are used to generate the ensemble , the backward computation graph consumes an enormous amount of memory . For large datasets ( and large architectures ) , such methods may not be scalable , however by using feature transformations , the amount of such memory overhead is small . By fixing a stable clustering algorithm , we can generate arbitrarily large ensembles by applying different transformations on the representations , and we can also control the diversity of the ensembles by trying several transformations . Random projections were successfully used in Consensus Clustering previously [ A ] . By generating ensembles using random projections , we have control over the amount of diversity we can induce into the framework , by varying the dimension of the random projection . We measure the diversity of the ensemble by performing the following experiment . For each component of the ensemble , we use the softmax probability estimates $ \\mathbf { p } $ and compute cluster assignments by taking an $ \\arg\\max $ on $ \\mathbf { p } $ of each image . If there are M components in the ensemble , we get M such cluster assignments . We then compute a pairwise NMI ( Normalized Mutual Information ) ( similar analysis to [ A ] ) between every two components of the ensemble , and compute the average and standard deviation of the pairwise NMI across the $ M * ( M-1 ) / 2 $ pairs . We observe from Figure 2 that the pairwise NMI increases as training progresses and becomes closer to 1 . At the beginning of training , the ensemble is very diverse ( small NMI score with a larger standard deviation ) ; and as training progresses , the diversity is reduced ( large NMI score with a smaller standard deviation ) . In addition to Random Projections , we also used diagonal transformations [ B ] where different components of the representation vector are scaled differently . [ B ] illustrate that such scaling enables a diverse set of clusterings which is helpful for their meta clustering task . * * 2 : * * \u201c Also , since the authors meant the idea to be applicable to general representation learning/deep clustering blocks , I 'm not sure the current experimental data in the paper can lead to that conclusion \u201d * * A : * * The motivation of this paper is to highlight that ensembles can be useful in deep clustering/representation learning . Therefore , our algorithm choices were based on some recent state-of-the-art results for unsupervised learning . However , due to the limited time/computation , we could not extend our analyses to other general representation learning/deep clustering blocks , but this is certainly a future direction we want to pursue . * * REFERENCES * * [ A ] Fern , Xiaoli Z. , and Carla E. Brodley . `` Random projection for high dimensional data clustering : A cluster ensemble approach . '' Proceedings of the 20th international conference on machine learning ( ICML-03 ) . 2003 . [ B ] Hsu , Kyle , Sergey Levine , and Chelsea Finn . `` Unsupervised learning via meta-learning . '' arXiv preprint arXiv:1810.02334 ( 2018 ) ."}, {"review_id": "x9C7Nlwgydy-2", "review_text": "- The idea of combining learning representations and ensemble clustering is interesting and possibly promising . I think this paper in its present form is not ready to be published though . - Cited literature on consensus clustering is old and outdated ( up to 2005 ) and does not capture the state of the art . More recent work exists that outperforms the ones cited in this paper . The authors should also consider comparing their method with the state of the art on consensus clustering which is not deep learning based . - The description of the algorithm is often too vague and is fragmented in mini-sections , and the flow that brings them together is unclear . The individual pieces are not novel and no novel technical challenges or contributions are presented . Several steps and settings are ad-hoc and not well-motivated . What does it mean exactly : `` Performing a forward pass on the two views results in feature vectors f1 , f2 . `` ? How are f1 and f2 obtained ? What 's the motivation for using a multi-layer perceptron to further reduce f1 and f2 ? Why should we expect this to produce a good representation for clustering ? How were the dimensionalities of the hidden and output layers chosen ? - The notation in Eq . ( 2 ) is not clearly defined . What is a transportation polytope ? - Writing and organization need major work . There are many vague statements that are not well-supported or motivated . - How does the set of observations X defined in 3.1 relate with the input batch X_b in 3.1.2 ? - The process of generating two views of the input images is not clearly defined . It seems that this process mainly consists in adding random noise to the images ? This is quite different from the notion of different * views * of a data object ( e.g.image ) in the literature . Also , why 2 views and not 3 or more ? - I do n't find Figure 1 particularly useful . Part ( a ) does not help at all in understanding the approach . - What 's the size of a typical ensemble being generated ? The paper mentions M transformations . What is the typical M value . In the experiments M ranges from 10 to 100 . How sensitive is the approach to the specific transformations being applied and to the number of transformations ? - The relationship and distinction between the views and the ensemble need to be better clarified . - I find the explanation above Eq ( 4 ) unconvincing . How diverse are the probability estimations generated by each component of the ensemble ? Have the authors performed any analysis on this ? - The authors should discuss assumptions and limitations of the proposed methodology . What assumptions does the proposed methodology make ? Under which scenarios and data distributions is the methodology expected to perform well ? What are its limitations ? - No insight or analysis on the results is provided . Often PICA outperforms the proposed method . I understand that PICA uses both training and test data , but since we are dealing with clustering and unsupervised data , I am not sure this is a real advantage for PICA .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank Reviewer4 for the time and the feedback . * * 1 : * * \u201c Cited literature on consensus clustering is old and outdated ( up to 2005 ) \u201d * * A : * * We thank the reviewer for pointing this out . We have added recent papers in the survey . * * 2 : * * \u201c The description of the algorithm is often too vague and is fragmented in mini-sections\u2026 \u201d * * A : * * We modified the description of the algorithm to address the reviewer \u2019 s comments . Our motivation for this work is to show that consensus techniques can be effective in unsupervised learning . The main challenge in such consensus techniques is to build a consensus function . * * 3 : * * \u201c What does it mean exactly : `` Performing a forward pass on the two views results in feature vectors f1 , f2 . `` ? How are f1 and f2 obtained ? What 's the motivation for using a multi-layer perceptron to further reduce f1 and f2 ? Why should we expect this to produce a good representation for clustering ? How were the dimensionalities of the hidden and output layers chosen ? \u201d * * A : * * Suppose the two augmented views of a given image are denoted as $ I^1 $ , $ I^2 $ , passing these augmented views through the network gives us features $ \\mathbf { f } ^1 $ , $ \\mathbf { f } ^2 $ of the two views . We use an MLP to further reduce these features motivated by the findings from SimCLR [ A ] . It was observed in [ A ] that using a non-linear projection layer improves the representation quality before it . This technique was then also used in [ B , C ] . We choose the hidden layer and output dimension similar to that in [ B ] , however since we used ResNet-34 ( which has feature space dimension 512 ) as compared to ResNet-50 ( which has feature space dimension 2048 ) in [ B ] , we used a hidden layer size of 2048 instead of 4096 . We have added this clarification in the paper . * * 4 : * * \u201c The notation in Eq . ( 2 ) is not clearly defined . What is a transportation polytope ? \u201d * * A : * * We updated and defined the notation of eq . ( 2 ) in the document . * * 5 : * * \u201c How does the set of observations X defined in 3.1 relate with the input batch X_b in 3.1.2 ? \u201d * * A : * * We updated the document to make the distinction clear . Set of observations $ \\mathcal { X } $ refers to all the data available . The algorithm is a minibatch based algorithm , and the input batch $ \\mathcal { X } _b \\subset \\mathcal { X } $ represents the minibatch of size $ B $ of the data used at a given iteration . * * 6 : * * \u201c The process of generating two views of the input images is not clearly defined\u2026 Also , why 2 views and not 3 or more ? \u201d * * A : * * We thank the reviewer for pointing out the inconsistency . The different views aren \u2019 t the same as views in multi-view datasets [ F ] . The views referred to in this paper correspond to different augmented views that are generated by image augmentation techniques , such as \u2018 RandomHorizontalFlip \u2019 , \u2018 RandomCrop \u2019 ( see [ A , B ] ) . In our algorithm , we use 2 augmented views of each image in the loss for the sake of simplicity . Indeed it is possible to use more than 2 views . Authors of [ C ] propose MultiCrop : a way to use multiple views ( 2 global views of higher resolution , and 10 local views of lower resolution ) . * * 7 : * * \u201c I do n't find Figure 1 particularly useful . Part ( a ) does not help at all in understanding the approach. \u201d * * A : * * We agree and we have moved the part ( a ) of the figure to appendix . We have kept part ( b ) of the figure in the main text and reference it in the main text for more clarity ."}], "0": {"review_id": "x9C7Nlwgydy-0", "review_text": "The authors propose a learning-based approach for image clustering . In particular , similarly to recent algorithms fro unsupervised representation learning , such a DeepCluster , they propose to iterate between clustering the images in the feature space of the network and updating the network weights to respect the clusters . Two main differences with respect to DeepCluster-like algorithms is that they target the task of clustering itself , and do not evaluate the generalizability of the leaned representations for other tasks , and that they propose to use cluster ensembles to improve training robustness . In particular , they generate 2 clusterings of the images at every iteration by applying different sets of data augmentations and feature transformations to the input images . The objective is then not only to respect these clusterings but also to enforce consistency between them over time , thus improving representation invariance to irrelevant image details . In an experimental evaluation on a set of standard image clustering benchmarks they outperform prior work in most scenarios . The idea is reasonable and the method seems sound , however a similar approach has been proposed in Zhuang et al. , ICCV'19 ( Local Aggregation for Unsupervised Learning of Visual Embeddings ) . In contrast to this work , the authors of Zhuang et al. , used different runs of the clustering algorithm to obtain diverse clusterings , instead of transforming the images , but the overall approach is very similar . The authors seem to be not aware of that work . In the current from it is not clear whether the proposed approach has any advantages over Zhuang et al.In the rebuttal the authors need to provide a discussion of their novelty with respect to that method as well as an experimental comparison on ImageNet . I appreciate the authors ' efforts to fairly compare to Zhuang et al.in the rebuttal , and I do find the preliminary evidence sufficient to establish that their approach outperforms LA on clustering metrics . However , the authors seem to miss the the point that LA is not just another consensus clustering approach which they forgot to include into literature review since it does not report clustering metrics . The main contribution of their work is combining consensus clustering with representation learning , which is exactly what the authors of LA had done before . It does seems that the particular approach proposed in this paper results in a better clustering performance , so the submission contains a valid contribution , but the relationship between the two methods needs to be discussed in a lot more detail , and they have to be throughly compared experimentally . I encourage the authors to improve the manuscript in this direction and resubmit to a different venue .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank Reviewer 3 for the time and the feedback . * * 1 : * * `` In the current form it is not clear whether the proposed approach has any advantages over Zhuang et al.In the rebuttal the authors need to provide a discussion of their novelty with respect to that method as well as an experimental comparison on ImageNet . '' * * A : * * We thank the reviewer for bringing Zhuang et al. , ICCV \u2019 19 to our notice . It is unfortunate that we did not come across this paper . We added this discussion to the revised document . Our main contribution is to formulate a consensus function using feature space transformations . The current form of the algorithm ( and the consensus function ) were chosen based on the soft clustering block used , and it can be easily adapted to other soft clustering approaches ( such as IIC [ B ] ) . Whereas , the main contribution of LA is a novel local aggregation objective ( which specifically follows from IR tasks ) which uses a closest neighbors set generated through multiple runs of K-means clustering . Differences : Firstly , [ A ] does not show any clustering metric performance . It is not clear if [ A ] outperforms the other baselines discussed in our paper if evaluated on clustering metrics , as [ A ] uses labelled data to do evaluation ( linear evaluation ) . [ A ] builds a novel Local Aggregation method on non-parametric instance discrimination tasks . These techniques usually require a large memory bank which is memory intensive . Our algorithm does n't require memory intensive techniques like memory banks . During training , at each iteration [ A ] requires a nearest neighbor computation . Our method requires running a fast version of the iterative Sinkhorn-Knopp algorithm at each step [ C ] . The way in which we generate the ensemble allows us to control and measure the diversity of the ensemble which can be useful in making algorithm design choices . Although [ A ] controls the ensemble by varying $ H $ ( number of clusterings ) and $ m $ ( number of clusters in each clustering ) which aptly suits the objective they are solving , their ensembles are limited to use k-means clustering . The authors show that other clustering approaches are either not scalable , or are not optimal . In our case , by applying feature space transformations , we have much more freedom in generating the ensemble . We used random projections , diagonal transformations , but there could be other transformations on the feature space . * * REFERENCES * * [ A ] Zhuang , Chengxu , Alex Lin Zhai , and Daniel Yamins . `` Local aggregation for unsupervised learning of visual embeddings . '' Proceedings of the IEEE International Conference on Computer Vision . 2019 . [ B ] Ji , Xu , Jo\u00e3o F. Henriques , and Andrea Vedaldi . `` Invariant information clustering for unsupervised image classification and segmentation . '' Proceedings of the IEEE International Conference on Computer Vision . 2019 . [ C ] Caron , Mathilde , et al . `` Unsupervised learning of visual features by contrasting cluster assignments . '' Advances in Neural Information Processing Systems 33 ( 2020 ) ."}, "1": {"review_id": "x9C7Nlwgydy-1", "review_text": "This paper studies the effect of combining ensemble learning approaches with deep clustering . The paper wants to show that ensemble learning methods , in particular consensus clustering , can improve the clustering accuracy when combined with general representation learning/clustering blocks . However , I am not sure that the results presented in the paper are enough to support the claims . The paper 's pros are : ( + ) It Is the first to combine ensemble methods with deep clustering models . Although ensemble methods have been widely applied , studying consensus clustering in the current problem setting is novel . ( + ) It Is the first to be able to have an ensemble deep clustering algorithm that gains empirically over other state-of-the-art models , showing the ideas to be potentially effective . ( + ) The writing is in general clear and undestandable . The paper 's cons are : ( - ) The wording in the abstract is a bit confusing in the sense that after reading it one might think the algorithm does consensus clustering first and uses the clustering to learn better representations of the input data . Although this is clarified later in the main body . ( - ) The description of the main algorithm seems to be more intuitive than innovative . Some algorimic design choices are not very convincing . For example , the choice of using random projections on embedding to produce different clusterings , although an interesting idea , makes me wonder why it is necessarily a good way to introduce randomness into the whole framework . The authors can expand their discussion on this . I also remain dubious about why different representation instead of different clustering methods Also , since the authors meant the idea to be applicable to general representation learning/deep clustering blocks , I 'm not sure the current experimental data in the paper can lead to that conclusion . ( - ) Using the performance metrics provided by the authors , I find it a bit hard to conclude that the proposed algorithm has a significant advantage over state of the art methods , especially PICA . Also , the fluctuation in performance metrics caused by different parameter settings seems to be , in magnitude , at least comparable to the margin of ConCURL over the baselines . Overall , I think this paper contains interesting seed ideas such as combining consensus clustering with representation learning and making use of the learned representation to generate multiple clusterings . These seed ideas could be good for this venue . However , the work is still premature and flawed by crude algorithm/experiment design . The quality can be significantly improved if the authors can give a more general algorithmic framework ( since the authors meant the ideas to be applicable to general representation learning/clustering algorithms ) , equipped with more thorough experimental investigation to support the applicability and superiority of the current approach .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank Reviewer 2 for the time and the feedback . * * 1 : * * \u201c For example , the choice of using random projections on embedding to produce different clusterings , although an interesting idea , makes me wonder why it is necessarily a good way to introduce randomness into the whole framework . The authors can expand their discussion on this . I also remain dubious about why different representation instead of different clustering methods \u201d * * A : * * We thank the reviewer for raising this interesting point . Indeed , there are many ways to introduce randomness into the framework . The next paragraph will explain why we preferred to use feature transformations as compared to multiple algorithms . Then , we discuss why we use Random Projections/Diagonal transformations on the representations Ideally , it is possible to use different clustering methods to generate the ensemble and it was pursued in the Consensus Clustering literature . However , since we are learning representations simultaneously using backpropagation , it is natural to use a clustering algorithm in which we can update its parameters via backpropagation . It is in-fact possible to use clustering algorithms like IIC that enable parameter updates , however , if multiple such algorithms are used to generate the ensemble , the backward computation graph consumes an enormous amount of memory . For large datasets ( and large architectures ) , such methods may not be scalable , however by using feature transformations , the amount of such memory overhead is small . By fixing a stable clustering algorithm , we can generate arbitrarily large ensembles by applying different transformations on the representations , and we can also control the diversity of the ensembles by trying several transformations . Random projections were successfully used in Consensus Clustering previously [ A ] . By generating ensembles using random projections , we have control over the amount of diversity we can induce into the framework , by varying the dimension of the random projection . We measure the diversity of the ensemble by performing the following experiment . For each component of the ensemble , we use the softmax probability estimates $ \\mathbf { p } $ and compute cluster assignments by taking an $ \\arg\\max $ on $ \\mathbf { p } $ of each image . If there are M components in the ensemble , we get M such cluster assignments . We then compute a pairwise NMI ( Normalized Mutual Information ) ( similar analysis to [ A ] ) between every two components of the ensemble , and compute the average and standard deviation of the pairwise NMI across the $ M * ( M-1 ) / 2 $ pairs . We observe from Figure 2 that the pairwise NMI increases as training progresses and becomes closer to 1 . At the beginning of training , the ensemble is very diverse ( small NMI score with a larger standard deviation ) ; and as training progresses , the diversity is reduced ( large NMI score with a smaller standard deviation ) . In addition to Random Projections , we also used diagonal transformations [ B ] where different components of the representation vector are scaled differently . [ B ] illustrate that such scaling enables a diverse set of clusterings which is helpful for their meta clustering task . * * 2 : * * \u201c Also , since the authors meant the idea to be applicable to general representation learning/deep clustering blocks , I 'm not sure the current experimental data in the paper can lead to that conclusion \u201d * * A : * * The motivation of this paper is to highlight that ensembles can be useful in deep clustering/representation learning . Therefore , our algorithm choices were based on some recent state-of-the-art results for unsupervised learning . However , due to the limited time/computation , we could not extend our analyses to other general representation learning/deep clustering blocks , but this is certainly a future direction we want to pursue . * * REFERENCES * * [ A ] Fern , Xiaoli Z. , and Carla E. Brodley . `` Random projection for high dimensional data clustering : A cluster ensemble approach . '' Proceedings of the 20th international conference on machine learning ( ICML-03 ) . 2003 . [ B ] Hsu , Kyle , Sergey Levine , and Chelsea Finn . `` Unsupervised learning via meta-learning . '' arXiv preprint arXiv:1810.02334 ( 2018 ) ."}, "2": {"review_id": "x9C7Nlwgydy-2", "review_text": "- The idea of combining learning representations and ensemble clustering is interesting and possibly promising . I think this paper in its present form is not ready to be published though . - Cited literature on consensus clustering is old and outdated ( up to 2005 ) and does not capture the state of the art . More recent work exists that outperforms the ones cited in this paper . The authors should also consider comparing their method with the state of the art on consensus clustering which is not deep learning based . - The description of the algorithm is often too vague and is fragmented in mini-sections , and the flow that brings them together is unclear . The individual pieces are not novel and no novel technical challenges or contributions are presented . Several steps and settings are ad-hoc and not well-motivated . What does it mean exactly : `` Performing a forward pass on the two views results in feature vectors f1 , f2 . `` ? How are f1 and f2 obtained ? What 's the motivation for using a multi-layer perceptron to further reduce f1 and f2 ? Why should we expect this to produce a good representation for clustering ? How were the dimensionalities of the hidden and output layers chosen ? - The notation in Eq . ( 2 ) is not clearly defined . What is a transportation polytope ? - Writing and organization need major work . There are many vague statements that are not well-supported or motivated . - How does the set of observations X defined in 3.1 relate with the input batch X_b in 3.1.2 ? - The process of generating two views of the input images is not clearly defined . It seems that this process mainly consists in adding random noise to the images ? This is quite different from the notion of different * views * of a data object ( e.g.image ) in the literature . Also , why 2 views and not 3 or more ? - I do n't find Figure 1 particularly useful . Part ( a ) does not help at all in understanding the approach . - What 's the size of a typical ensemble being generated ? The paper mentions M transformations . What is the typical M value . In the experiments M ranges from 10 to 100 . How sensitive is the approach to the specific transformations being applied and to the number of transformations ? - The relationship and distinction between the views and the ensemble need to be better clarified . - I find the explanation above Eq ( 4 ) unconvincing . How diverse are the probability estimations generated by each component of the ensemble ? Have the authors performed any analysis on this ? - The authors should discuss assumptions and limitations of the proposed methodology . What assumptions does the proposed methodology make ? Under which scenarios and data distributions is the methodology expected to perform well ? What are its limitations ? - No insight or analysis on the results is provided . Often PICA outperforms the proposed method . I understand that PICA uses both training and test data , but since we are dealing with clustering and unsupervised data , I am not sure this is a real advantage for PICA .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank Reviewer4 for the time and the feedback . * * 1 : * * \u201c Cited literature on consensus clustering is old and outdated ( up to 2005 ) \u201d * * A : * * We thank the reviewer for pointing this out . We have added recent papers in the survey . * * 2 : * * \u201c The description of the algorithm is often too vague and is fragmented in mini-sections\u2026 \u201d * * A : * * We modified the description of the algorithm to address the reviewer \u2019 s comments . Our motivation for this work is to show that consensus techniques can be effective in unsupervised learning . The main challenge in such consensus techniques is to build a consensus function . * * 3 : * * \u201c What does it mean exactly : `` Performing a forward pass on the two views results in feature vectors f1 , f2 . `` ? How are f1 and f2 obtained ? What 's the motivation for using a multi-layer perceptron to further reduce f1 and f2 ? Why should we expect this to produce a good representation for clustering ? How were the dimensionalities of the hidden and output layers chosen ? \u201d * * A : * * Suppose the two augmented views of a given image are denoted as $ I^1 $ , $ I^2 $ , passing these augmented views through the network gives us features $ \\mathbf { f } ^1 $ , $ \\mathbf { f } ^2 $ of the two views . We use an MLP to further reduce these features motivated by the findings from SimCLR [ A ] . It was observed in [ A ] that using a non-linear projection layer improves the representation quality before it . This technique was then also used in [ B , C ] . We choose the hidden layer and output dimension similar to that in [ B ] , however since we used ResNet-34 ( which has feature space dimension 512 ) as compared to ResNet-50 ( which has feature space dimension 2048 ) in [ B ] , we used a hidden layer size of 2048 instead of 4096 . We have added this clarification in the paper . * * 4 : * * \u201c The notation in Eq . ( 2 ) is not clearly defined . What is a transportation polytope ? \u201d * * A : * * We updated and defined the notation of eq . ( 2 ) in the document . * * 5 : * * \u201c How does the set of observations X defined in 3.1 relate with the input batch X_b in 3.1.2 ? \u201d * * A : * * We updated the document to make the distinction clear . Set of observations $ \\mathcal { X } $ refers to all the data available . The algorithm is a minibatch based algorithm , and the input batch $ \\mathcal { X } _b \\subset \\mathcal { X } $ represents the minibatch of size $ B $ of the data used at a given iteration . * * 6 : * * \u201c The process of generating two views of the input images is not clearly defined\u2026 Also , why 2 views and not 3 or more ? \u201d * * A : * * We thank the reviewer for pointing out the inconsistency . The different views aren \u2019 t the same as views in multi-view datasets [ F ] . The views referred to in this paper correspond to different augmented views that are generated by image augmentation techniques , such as \u2018 RandomHorizontalFlip \u2019 , \u2018 RandomCrop \u2019 ( see [ A , B ] ) . In our algorithm , we use 2 augmented views of each image in the loss for the sake of simplicity . Indeed it is possible to use more than 2 views . Authors of [ C ] propose MultiCrop : a way to use multiple views ( 2 global views of higher resolution , and 10 local views of lower resolution ) . * * 7 : * * \u201c I do n't find Figure 1 particularly useful . Part ( a ) does not help at all in understanding the approach. \u201d * * A : * * We agree and we have moved the part ( a ) of the figure to appendix . We have kept part ( b ) of the figure in the main text and reference it in the main text for more clarity ."}}