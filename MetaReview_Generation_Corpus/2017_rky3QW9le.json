{"year": "2017", "forum": "rky3QW9le", "title": "Transformational Sparse Coding", "decision": "Reject", "meta_review": "This paper learns affine transformations from images jointly with object features. The motivation is interesting and sound, but the experiments fail to deliver and demonstrate the validity of the claims advanced -- they are restricted to toy settings. What is presented as logical next steps for this work (extending to higher scale multilayer convolutional frameworks, beyond toy settings) seems necessary for the paper to hold its own and deliver the promised insights.", "reviews": [{"review_id": "rky3QW9le-0", "review_text": "This paper trains a generative model of image patches, where dictionary elements undergo gated linear transformations before being combined. The transformations are motivated in terms of Lie group operators, though in practice they are a set of fixed linear transformations. This is motivated strongly in terms of learning a hierarchy of transformations, though only one layer is used in the experiments (except for a toy case in the appendix). I like the motivation for this algorithm. The realization seems very similar to a group or block sparse coding implementation. I was disappointed by the restriction to linear transformations. The experiments were all toy cases, demonstrating that the algorithm can learn groups of Gabor- or center surround-like features. They would have been somewhat underpowered five years ago, and seemed extremely small by today's standards. Specific comments: Based on common practices in ML literature, I have a strong bias to think of $x$ as inputs and $w$ as network weights. Latent variables are often $z$ or $a$. Depending on your target audience, I would suggest permuting your choice of symbols so the reader can more quickly interpret your model. nit: number all equations for easier reference sec 2.2 -- It's weird that the transformation is fixed, but is still written as a function of x. sec 2.3 -- The updated text here confuses me actually. I had thought that you were using a fixed set of linear transformations, and were motivating in terms of Lie groups, but were not actually taking matrix exponentials in your algorithm. The equations in the second half of this section suggest you are working with matrix exponentials though. I'm not sure which direction I'm confused in, but probably good to clarify the text either way. BTW -- there's another possible solution to the local minima difficulty, which is the one used in Sohl-Dickstein, 2010. There, they introduce blurring operators matched to each transformation operator, and gradient descent can escape local minima by detouring through coarser (more blurred) scales. sec 3.2 -- I believe by degrees of freedom you mean the number of model parameters, not the number of latent coefficients that must be inferred? Should make this more clear. Is it more appropriate to compare reconstruction error while matching number of model parameters, or number of latent variables? I wonder if a convolutional version of this algorithm would be practical / would make it more suited as a generative model of whole images. ==== post rebuttal update Thank you for taking the time to write the rebuttal! I have read it, but it did not significantly effect my rating.", "rating": "5: Marginally below acceptance threshold", "reply_text": "-I like the motivation for this algorithm . The realization seems very similar to a group or block sparse coding implementation . I was disappointed by the restriction to linear transformations . Our intuition says that if we look close enough ( smaller patches ) , basic transformations are appropriate for modeling the input . We expect that these linear transformations combined at multiple levels can result in more complex , non-linear ones . -The experiments were all toy cases , demonstrating that the algorithm can learn groups of Gabor- or center surround-like features . They would have been somewhat underpowered five years ago , and seemed extremely small by today 's standards . Our goal in this paper is : 1 ) To show that affine transformations are appropriate for modeling the local structure of images and explain most of the small-scale variation of the input . 2 ) To provide an efficient learning framework that captures this structure . Scaling this approach to large images via a convolutional , multi-layer approach is the next step in our work . -Based on common practices in ML literature , I have a strong bias to think of $ x $ as inputs and $ w $ as network weights . Latent variables are often $ z $ or $ a $ . Depending on your target audience , I would suggest permuting your choice of symbols so the reader can more quickly interpret your model . -nit : number all equations for easier reference We will adjust our submission accordingly . -sec 2.2 -- It 's weird that the transformation is fixed , but is still written as a function of x . It is written as a function of x , since the transformation is parametrized by x . Even though the transformations are fixed across data points , their parameters still have to be learned from the data . -sec 2.3 -- The updated text here confuses me actually . I had thought that you were using a fixed set of linear transformations , and were motivating in terms of Lie groups , but were not actually taking matrix exponentials in your algorithm . The equations in the second half of this section suggest you are working with matrix exponentials though . I 'm not sure which direction I 'm confused in , but probably good to clarify the text either way . The model learns fixed linear transformations of sparse coding filters . Depending on which base transformations ( i.e.translations , rotations etc . ) we allow , the fixed transformations we learn belong to the associated Lie group . Specifically , each such transformation is a mixture of translations , rotations etc . To recover any element in the Lie group , we take the matrix exponential of an element of the Lie algebra of the group , which is a linear combination of the base transformation generators ( denoted as G in the paper ) . The transformation model ( and the matrix exponential ) are introduced in Section 2.1 . A more thorough discussion on the transformation model can be found in the references provided in that section ( Rao & Ruderman ( 1999 ) , Miao & Rao ( 2007 ) and Dodwell ( 1983 ) ) . -BTW -- there 's another possible solution to the local minima difficulty , which is the one used in Sohl-Dickstein , 2010 . There , they introduce blurring operators matched to each transformation operator , and gradient descent can escape local minima by detouring through coarser ( more blurred ) scales . The blurring approach in Sohl-Dickstein ( 2010 ) is indeed very interesting . However , it is introduced only for single-parameter transformations ( single generator , one-dimensional Lie group ) . In our work we use multi-dimensional Lie groups to model the learned transformations as a mixture of more basic ones ( translations , rotations etc . ) whereas Sohl and Dickstein `` chain '' multiple one-dimensional transformations together to recover bigger ones . We also decompose the input as a linear combination of transformed features or parts , whereas they focus on learning `` whole-image '' transformations between consecutive video frames . Therefore applying the blurring approach is not straight-forward and might not be computationally efficient . One of our next steps in this line of work is to fine-tune the features individually for each data-point ( preliminary results indicate that this significantly improves the reconstruction error ) . Then our model would resemble a coarse-to-fine optimization approach , where the selected sparse coding feature corresponds to a `` global '' step in the parameter space and fine-tuning corresponds to local optimization in that region . The set of sparse coding features forms a `` grid '' that spans the parameter space , learned from data . -sec 3.2 -- I believe by degrees of freedom you mean the number of model parameters , not the number of latent coefficients that must be inferred ? Should make this more clear . Is it more appropriate to compare reconstruction error while matching number of model parameters , or number of latent variables ? In the degrees of freedom we include all common across data-points variables that have to be learned . These are the pixel values of each feature plus the transformation parameters for each leaf . Since the norm of the features is constrained to be equal to 1 , we subtract one degree of freedom for each feature ( if we know all but one , the last pixel value is determined by the constraint ) . In the degrees of freedom of the model we do not include the weights , since these are individual to each data point . The degrees of freedom can be interpreted as the size of the code . -I wonder if a convolutional version of this algorithm would be practical / would make it more suited as a generative model of whole images . This is definitely the subject of ongoing work ."}, {"review_id": "rky3QW9le-1", "review_text": "This paper proposes an approach to unsupervised learning based on a modification to sparse coding that allows for explicit modeling of transformations (such as shift, rotation, etc.), as opposed to simple pooling as is typically done in convnets. Results are shown for training on natural images, demonstrating that the algorithm learns about features and their transformations in the data. A comparison to traditional sparse coding shows that it represents images with fewer degrees of freedom. This seems like a good and interesting approach, but the work seems like its still in its early formative stages rather than a complete work with a compelling punch line. For example one of the motivations is that you'd like to represent pose along with the identity of an object. While this work seems well on its way to that goal, it doesn't quite get there - it leaves a lot of dots still to be connected. Also there are a number of things that aren't clear in the paper: o The central idea of the paper it seems is the use of a transformational sparse coding tree to make tractable the inference of the Lie group parameters x_k. But how exactly this is done is not at all clear. For example, the sentence: \"The main idea is to gradually marginalize over an increasing range of transformations,\" is suggestive but not clear. This needs to be much better defined. What do you mean by marginalization in this context? o The connection between the Lie group operator and the tree leaves and weights w_b is not at all clear. The learning rule spells out the gradient for the Lie group operator, but how this is used to learn the leaves of the tree is not clear. A lot is left to the imagination here. This is especially confusing because although the Lie group operator is introduced earlier, it is then stated that its not tractable for inference because there are too many local minima, and this motivates the tree approach instead. So its not clear why you are learning the Lie group operator. o It is stated that \"Averaging over many data points, smoothens the surface of the error function.\" I don't understand why you would average over many data points. It seems each would have its own transformation, no? o What data do you train on? How is it generated? Do you generate patches with known transformations and then show that you can recover them? Please explain. The results shown in Figure 4 look very interesting, but given the lack of clarity in the above, difficult to interpret and understand what this means, and its significance. I would encourage the authors to rewrite the paper more clearly and also to put more work into further developing these ideas, which seem very promising. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "-The central idea of the paper it seems is the use of a transformational sparse coding tree to make tractable the inference of the Lie group parameters x_k . But how exactly this is done is not at all clear . For example , the sentence : `` The main idea is to gradually marginalize over an increasing range of transformations , '' is suggestive but not clear . This needs to be much better defined . What do you mean by marginalization in this context ? The main idea here is that a tree `` reduces '' a set of features to a smaller one by factoring out affine transformations . A single layer tree radically reduces a whole set to a single feature . A deeper tree gradually reduces a set of features to a single one , the root . Deeper trees could provide a continuum of intermediate representations , where some transformations are factored out . We understand that we do not have compelling evidence for the usefulness of deeper trees yet and will modify our claims in this section accordingly . -The connection between the Lie group operator and the tree leaves and weights w_b is not at all clear . The learning rule spells out the gradient for the Lie group operator , but how this is used to learn the leaves of the tree is not clear . A lot is left to the imagination here . This is especially confusing because although the Lie group operator is introduced earlier , it is then stated that its not tractable for inference because there are too many local minima , and this motivates the tree approach instead . So its not clear why you are learning the Lie group operator . A single layer tree consists of the root and the leaves . The leaves correspond to transformed versions of the same root . These transformations are derived by a linear combination of the group generators ( parametrized by `` x '' ) and the matrix exponential . The group generators as fixed and pre-selected to correspond to some interesting basic transformations . The linear combination parameters ( `` x '' ) define the transformation associated with each leaf as a mixture of such basic transformations and are learned from the data . We suggest that naively performing inference from scratch on each data-point is intractable due to the number of local minima . To counter that , we propose learning common transformations that can be used for all data points . Essentially what we are doing is using affine transformations to extract pose information from a sparse code . We reduce groups of features ( leaves ) to a single feature ( root ) by factoring out pose . -It is stated that `` Averaging over many data points , smoothens the surface of the error function . '' I do n't understand why you would average over many data points . It seems each would have its own transformation , no ? No.Each patch is modeled as a sparse linear combination of features . These features are modeled as leaves in a forest . Each leaf corresponds to a transformed version of its root . The transformations learned describe how to obtain the leaves . -What data do you train on ? How is it generated ? Do you generate patches with known transformations and then show that you can recover them ? Please explain . We train on patches that are randomly extracted from a set of 10 natural images . The dataset is the same as the one used by Lee et al . ( 2007 ) ( which is also the version of sparse coding that we compare with ) ."}, {"review_id": "rky3QW9le-2", "review_text": "A new sparse coding model is introduced that learns features jointly with their transformations. It is found that inference over per-image transformation variables is hard, so the authors suggest tying these variables across all data points, turning them into global parameters, and using multiple transformations for each feature. Furthermore, it is suggested to use a tree of transformations, where each path down the tree generates a feature by multiplying the root feature by the transformations associated with the edges. The one-layer tree model achieves similar reconstruction error as traditional sparse coding, while using fewer parameters. This is a nice addition to the literature on sparse coding and the literature on learning transformation models. The authors identify and deal with a difficult inference problem that can occur in transformation models. That said, I am skeptical about the usefulness of the general approach. The authors take it as a given that \u201clearning sparse features and transformations jointly\u201d is an important goal in itself, but this is never really argued or demonstrated with experiments. It doesn\u2019t seem like this method enables new applications, extends our understanding of learning what/where pathways in the brain, or improve our ability to model natural images. The authors claim that the model extracts pose information, but although the model explicitly captures the transformation that relates different features in a tree, at test time, inference is only performed over the (sparse) coefficient associated with each (feature, transformation) combination, just like in sparse coding. It is not clear what we gain by knowing that each coefficient is associated with a transformation, especially since there are many models that do this general \u201cwhat / where\u201d split. It would be good to check that the x_{v->b} actually change significantly from their initialization values. The loss surface still looks pretty bad even for tied transformations, so they may actually not move much. Does the proposed model work better according to some measure, compared to a model where x_{v->b} are fixed and chosen from some reasonable range of parameter values (either randomly or spaced evenly)? One of the conceptually interesting aspects of the paper is the idea of a tree of transformations, but the advantage of deeper trees is never demonstrated convincingly. It looks like the authors have only just gotten this approach to work on toy data with vertical and horizontal bars. Finally, it is not clear how the method could be extended to have multiple layers. The transformation operators T can be defined in the first layer because they act on the input space, but the same cannot be done in the learned feature space. It is also not clear how the pose information should be further processed in a hierarchical manner, or how learning in a deep version should work. In summary, I do not recommend this paper for publication, because it is not clear what problem is being solved, the method is only moderately novel and the novel aspects are not convincingly shown to be beneficial. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "-The authors take it as a given that \u201c learning sparse features and transformations jointly \u201d is an important goal in itself , but this is never really argued or demonstrated with experiments . It doesn \u2019 t seem like this method enables new applications , extends our understanding of learning what/where pathways in the brain , or improve our ability to model natural images . There are many reasons why learning features and their transformations is interesting , such as : - Learning to recognize objects in different poses ( location , orientation , scale etc . ) - Exploiting pose parameters in recognition and other tasks ( ie when recognition depends on finer grained spatial relations between parts of the object ) . - Encoding images more efficiently . The goal of this paper is to show that natural images have a low-level structure that allows for more concise representations using affine transformations and to provide a framework for learning these transformations efficiently . -The authors claim that the model extracts pose information , but although the model explicitly captures the transformation that relates different features in a tree , at test time , inference is only performed over the ( sparse ) coefficient associated with each ( feature , transformation ) combination , just like in sparse coding . It is not clear what we gain by knowing that each coefficient is associated with a transformation , especially since there are many models that do this general \u201c what / where \u201d split . The advantage is that we can now express an image patch as a much smaller set of transformed features . This combined with the fact that the model learns the transformation parameters results in an `` equivariant '' representation of the patch . As for the claim that there are many models that provide this `` what '' and `` where '' split , do you have any specific work/model in mind that does what our model does ? We would definitely want to include that in our references . -It would be good to check that the x_ { v- > b } actually change significantly from their initialization values . The loss surface still looks pretty bad even for tied transformations , so they may actually not move much . Does the proposed model work better according to some measure , compared to a model where x_ { v- > b } are fixed and chosen from some reasonable range of parameter values ( either randomly or spaced evenly ) ? This is an interesting point . The first version of our paper had a figure that showed the l1 norm of the deviation from the initialized values . When we initialize the parameters with very small variance ( with zero mean ) the learned values differ significantly from the initial ones . If we initialize the parameters with large variance and use regularization it is unclear whether we should attribute this deviation to regularization or to learning . We decided to err on the side of caution and removed the figure . -One of the conceptually interesting aspects of the paper is the idea of a tree of transformations , but the advantage of deeper trees is never demonstrated convincingly . It looks like the authors have only just gotten this approach to work on toy data with vertical and horizontal bars . While investigating deeper trees is certainly the subject of ongoing work , at submission time we had only tested them on toy examples . We included one of these examples to show that learning more structured representations using deeper trees is a possibility . -Finally , it is not clear how the method could be extended to have multiple layers . The transformation operators T can be defined in the first layer because they act on the input space , but the same can not be done in the learned feature space . It is also not clear how the pose information should be further processed in a hierarchical manner , or how learning in a deep version should work . This is a fair point . The model converts raw pixel values to feature weights and pose parameters in a manner consistent with capsules by Hinton et al.It is true that the same module can not be directly stacked to form a deep version . While building a deeper version is definitely our end goal and top priority , it is out of the scope of this paper ."}], "0": {"review_id": "rky3QW9le-0", "review_text": "This paper trains a generative model of image patches, where dictionary elements undergo gated linear transformations before being combined. The transformations are motivated in terms of Lie group operators, though in practice they are a set of fixed linear transformations. This is motivated strongly in terms of learning a hierarchy of transformations, though only one layer is used in the experiments (except for a toy case in the appendix). I like the motivation for this algorithm. The realization seems very similar to a group or block sparse coding implementation. I was disappointed by the restriction to linear transformations. The experiments were all toy cases, demonstrating that the algorithm can learn groups of Gabor- or center surround-like features. They would have been somewhat underpowered five years ago, and seemed extremely small by today's standards. Specific comments: Based on common practices in ML literature, I have a strong bias to think of $x$ as inputs and $w$ as network weights. Latent variables are often $z$ or $a$. Depending on your target audience, I would suggest permuting your choice of symbols so the reader can more quickly interpret your model. nit: number all equations for easier reference sec 2.2 -- It's weird that the transformation is fixed, but is still written as a function of x. sec 2.3 -- The updated text here confuses me actually. I had thought that you were using a fixed set of linear transformations, and were motivating in terms of Lie groups, but were not actually taking matrix exponentials in your algorithm. The equations in the second half of this section suggest you are working with matrix exponentials though. I'm not sure which direction I'm confused in, but probably good to clarify the text either way. BTW -- there's another possible solution to the local minima difficulty, which is the one used in Sohl-Dickstein, 2010. There, they introduce blurring operators matched to each transformation operator, and gradient descent can escape local minima by detouring through coarser (more blurred) scales. sec 3.2 -- I believe by degrees of freedom you mean the number of model parameters, not the number of latent coefficients that must be inferred? Should make this more clear. Is it more appropriate to compare reconstruction error while matching number of model parameters, or number of latent variables? I wonder if a convolutional version of this algorithm would be practical / would make it more suited as a generative model of whole images. ==== post rebuttal update Thank you for taking the time to write the rebuttal! I have read it, but it did not significantly effect my rating.", "rating": "5: Marginally below acceptance threshold", "reply_text": "-I like the motivation for this algorithm . The realization seems very similar to a group or block sparse coding implementation . I was disappointed by the restriction to linear transformations . Our intuition says that if we look close enough ( smaller patches ) , basic transformations are appropriate for modeling the input . We expect that these linear transformations combined at multiple levels can result in more complex , non-linear ones . -The experiments were all toy cases , demonstrating that the algorithm can learn groups of Gabor- or center surround-like features . They would have been somewhat underpowered five years ago , and seemed extremely small by today 's standards . Our goal in this paper is : 1 ) To show that affine transformations are appropriate for modeling the local structure of images and explain most of the small-scale variation of the input . 2 ) To provide an efficient learning framework that captures this structure . Scaling this approach to large images via a convolutional , multi-layer approach is the next step in our work . -Based on common practices in ML literature , I have a strong bias to think of $ x $ as inputs and $ w $ as network weights . Latent variables are often $ z $ or $ a $ . Depending on your target audience , I would suggest permuting your choice of symbols so the reader can more quickly interpret your model . -nit : number all equations for easier reference We will adjust our submission accordingly . -sec 2.2 -- It 's weird that the transformation is fixed , but is still written as a function of x . It is written as a function of x , since the transformation is parametrized by x . Even though the transformations are fixed across data points , their parameters still have to be learned from the data . -sec 2.3 -- The updated text here confuses me actually . I had thought that you were using a fixed set of linear transformations , and were motivating in terms of Lie groups , but were not actually taking matrix exponentials in your algorithm . The equations in the second half of this section suggest you are working with matrix exponentials though . I 'm not sure which direction I 'm confused in , but probably good to clarify the text either way . The model learns fixed linear transformations of sparse coding filters . Depending on which base transformations ( i.e.translations , rotations etc . ) we allow , the fixed transformations we learn belong to the associated Lie group . Specifically , each such transformation is a mixture of translations , rotations etc . To recover any element in the Lie group , we take the matrix exponential of an element of the Lie algebra of the group , which is a linear combination of the base transformation generators ( denoted as G in the paper ) . The transformation model ( and the matrix exponential ) are introduced in Section 2.1 . A more thorough discussion on the transformation model can be found in the references provided in that section ( Rao & Ruderman ( 1999 ) , Miao & Rao ( 2007 ) and Dodwell ( 1983 ) ) . -BTW -- there 's another possible solution to the local minima difficulty , which is the one used in Sohl-Dickstein , 2010 . There , they introduce blurring operators matched to each transformation operator , and gradient descent can escape local minima by detouring through coarser ( more blurred ) scales . The blurring approach in Sohl-Dickstein ( 2010 ) is indeed very interesting . However , it is introduced only for single-parameter transformations ( single generator , one-dimensional Lie group ) . In our work we use multi-dimensional Lie groups to model the learned transformations as a mixture of more basic ones ( translations , rotations etc . ) whereas Sohl and Dickstein `` chain '' multiple one-dimensional transformations together to recover bigger ones . We also decompose the input as a linear combination of transformed features or parts , whereas they focus on learning `` whole-image '' transformations between consecutive video frames . Therefore applying the blurring approach is not straight-forward and might not be computationally efficient . One of our next steps in this line of work is to fine-tune the features individually for each data-point ( preliminary results indicate that this significantly improves the reconstruction error ) . Then our model would resemble a coarse-to-fine optimization approach , where the selected sparse coding feature corresponds to a `` global '' step in the parameter space and fine-tuning corresponds to local optimization in that region . The set of sparse coding features forms a `` grid '' that spans the parameter space , learned from data . -sec 3.2 -- I believe by degrees of freedom you mean the number of model parameters , not the number of latent coefficients that must be inferred ? Should make this more clear . Is it more appropriate to compare reconstruction error while matching number of model parameters , or number of latent variables ? In the degrees of freedom we include all common across data-points variables that have to be learned . These are the pixel values of each feature plus the transformation parameters for each leaf . Since the norm of the features is constrained to be equal to 1 , we subtract one degree of freedom for each feature ( if we know all but one , the last pixel value is determined by the constraint ) . In the degrees of freedom of the model we do not include the weights , since these are individual to each data point . The degrees of freedom can be interpreted as the size of the code . -I wonder if a convolutional version of this algorithm would be practical / would make it more suited as a generative model of whole images . This is definitely the subject of ongoing work ."}, "1": {"review_id": "rky3QW9le-1", "review_text": "This paper proposes an approach to unsupervised learning based on a modification to sparse coding that allows for explicit modeling of transformations (such as shift, rotation, etc.), as opposed to simple pooling as is typically done in convnets. Results are shown for training on natural images, demonstrating that the algorithm learns about features and their transformations in the data. A comparison to traditional sparse coding shows that it represents images with fewer degrees of freedom. This seems like a good and interesting approach, but the work seems like its still in its early formative stages rather than a complete work with a compelling punch line. For example one of the motivations is that you'd like to represent pose along with the identity of an object. While this work seems well on its way to that goal, it doesn't quite get there - it leaves a lot of dots still to be connected. Also there are a number of things that aren't clear in the paper: o The central idea of the paper it seems is the use of a transformational sparse coding tree to make tractable the inference of the Lie group parameters x_k. But how exactly this is done is not at all clear. For example, the sentence: \"The main idea is to gradually marginalize over an increasing range of transformations,\" is suggestive but not clear. This needs to be much better defined. What do you mean by marginalization in this context? o The connection between the Lie group operator and the tree leaves and weights w_b is not at all clear. The learning rule spells out the gradient for the Lie group operator, but how this is used to learn the leaves of the tree is not clear. A lot is left to the imagination here. This is especially confusing because although the Lie group operator is introduced earlier, it is then stated that its not tractable for inference because there are too many local minima, and this motivates the tree approach instead. So its not clear why you are learning the Lie group operator. o It is stated that \"Averaging over many data points, smoothens the surface of the error function.\" I don't understand why you would average over many data points. It seems each would have its own transformation, no? o What data do you train on? How is it generated? Do you generate patches with known transformations and then show that you can recover them? Please explain. The results shown in Figure 4 look very interesting, but given the lack of clarity in the above, difficult to interpret and understand what this means, and its significance. I would encourage the authors to rewrite the paper more clearly and also to put more work into further developing these ideas, which seem very promising. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "-The central idea of the paper it seems is the use of a transformational sparse coding tree to make tractable the inference of the Lie group parameters x_k . But how exactly this is done is not at all clear . For example , the sentence : `` The main idea is to gradually marginalize over an increasing range of transformations , '' is suggestive but not clear . This needs to be much better defined . What do you mean by marginalization in this context ? The main idea here is that a tree `` reduces '' a set of features to a smaller one by factoring out affine transformations . A single layer tree radically reduces a whole set to a single feature . A deeper tree gradually reduces a set of features to a single one , the root . Deeper trees could provide a continuum of intermediate representations , where some transformations are factored out . We understand that we do not have compelling evidence for the usefulness of deeper trees yet and will modify our claims in this section accordingly . -The connection between the Lie group operator and the tree leaves and weights w_b is not at all clear . The learning rule spells out the gradient for the Lie group operator , but how this is used to learn the leaves of the tree is not clear . A lot is left to the imagination here . This is especially confusing because although the Lie group operator is introduced earlier , it is then stated that its not tractable for inference because there are too many local minima , and this motivates the tree approach instead . So its not clear why you are learning the Lie group operator . A single layer tree consists of the root and the leaves . The leaves correspond to transformed versions of the same root . These transformations are derived by a linear combination of the group generators ( parametrized by `` x '' ) and the matrix exponential . The group generators as fixed and pre-selected to correspond to some interesting basic transformations . The linear combination parameters ( `` x '' ) define the transformation associated with each leaf as a mixture of such basic transformations and are learned from the data . We suggest that naively performing inference from scratch on each data-point is intractable due to the number of local minima . To counter that , we propose learning common transformations that can be used for all data points . Essentially what we are doing is using affine transformations to extract pose information from a sparse code . We reduce groups of features ( leaves ) to a single feature ( root ) by factoring out pose . -It is stated that `` Averaging over many data points , smoothens the surface of the error function . '' I do n't understand why you would average over many data points . It seems each would have its own transformation , no ? No.Each patch is modeled as a sparse linear combination of features . These features are modeled as leaves in a forest . Each leaf corresponds to a transformed version of its root . The transformations learned describe how to obtain the leaves . -What data do you train on ? How is it generated ? Do you generate patches with known transformations and then show that you can recover them ? Please explain . We train on patches that are randomly extracted from a set of 10 natural images . The dataset is the same as the one used by Lee et al . ( 2007 ) ( which is also the version of sparse coding that we compare with ) ."}, "2": {"review_id": "rky3QW9le-2", "review_text": "A new sparse coding model is introduced that learns features jointly with their transformations. It is found that inference over per-image transformation variables is hard, so the authors suggest tying these variables across all data points, turning them into global parameters, and using multiple transformations for each feature. Furthermore, it is suggested to use a tree of transformations, where each path down the tree generates a feature by multiplying the root feature by the transformations associated with the edges. The one-layer tree model achieves similar reconstruction error as traditional sparse coding, while using fewer parameters. This is a nice addition to the literature on sparse coding and the literature on learning transformation models. The authors identify and deal with a difficult inference problem that can occur in transformation models. That said, I am skeptical about the usefulness of the general approach. The authors take it as a given that \u201clearning sparse features and transformations jointly\u201d is an important goal in itself, but this is never really argued or demonstrated with experiments. It doesn\u2019t seem like this method enables new applications, extends our understanding of learning what/where pathways in the brain, or improve our ability to model natural images. The authors claim that the model extracts pose information, but although the model explicitly captures the transformation that relates different features in a tree, at test time, inference is only performed over the (sparse) coefficient associated with each (feature, transformation) combination, just like in sparse coding. It is not clear what we gain by knowing that each coefficient is associated with a transformation, especially since there are many models that do this general \u201cwhat / where\u201d split. It would be good to check that the x_{v->b} actually change significantly from their initialization values. The loss surface still looks pretty bad even for tied transformations, so they may actually not move much. Does the proposed model work better according to some measure, compared to a model where x_{v->b} are fixed and chosen from some reasonable range of parameter values (either randomly or spaced evenly)? One of the conceptually interesting aspects of the paper is the idea of a tree of transformations, but the advantage of deeper trees is never demonstrated convincingly. It looks like the authors have only just gotten this approach to work on toy data with vertical and horizontal bars. Finally, it is not clear how the method could be extended to have multiple layers. The transformation operators T can be defined in the first layer because they act on the input space, but the same cannot be done in the learned feature space. It is also not clear how the pose information should be further processed in a hierarchical manner, or how learning in a deep version should work. In summary, I do not recommend this paper for publication, because it is not clear what problem is being solved, the method is only moderately novel and the novel aspects are not convincingly shown to be beneficial. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "-The authors take it as a given that \u201c learning sparse features and transformations jointly \u201d is an important goal in itself , but this is never really argued or demonstrated with experiments . It doesn \u2019 t seem like this method enables new applications , extends our understanding of learning what/where pathways in the brain , or improve our ability to model natural images . There are many reasons why learning features and their transformations is interesting , such as : - Learning to recognize objects in different poses ( location , orientation , scale etc . ) - Exploiting pose parameters in recognition and other tasks ( ie when recognition depends on finer grained spatial relations between parts of the object ) . - Encoding images more efficiently . The goal of this paper is to show that natural images have a low-level structure that allows for more concise representations using affine transformations and to provide a framework for learning these transformations efficiently . -The authors claim that the model extracts pose information , but although the model explicitly captures the transformation that relates different features in a tree , at test time , inference is only performed over the ( sparse ) coefficient associated with each ( feature , transformation ) combination , just like in sparse coding . It is not clear what we gain by knowing that each coefficient is associated with a transformation , especially since there are many models that do this general \u201c what / where \u201d split . The advantage is that we can now express an image patch as a much smaller set of transformed features . This combined with the fact that the model learns the transformation parameters results in an `` equivariant '' representation of the patch . As for the claim that there are many models that provide this `` what '' and `` where '' split , do you have any specific work/model in mind that does what our model does ? We would definitely want to include that in our references . -It would be good to check that the x_ { v- > b } actually change significantly from their initialization values . The loss surface still looks pretty bad even for tied transformations , so they may actually not move much . Does the proposed model work better according to some measure , compared to a model where x_ { v- > b } are fixed and chosen from some reasonable range of parameter values ( either randomly or spaced evenly ) ? This is an interesting point . The first version of our paper had a figure that showed the l1 norm of the deviation from the initialized values . When we initialize the parameters with very small variance ( with zero mean ) the learned values differ significantly from the initial ones . If we initialize the parameters with large variance and use regularization it is unclear whether we should attribute this deviation to regularization or to learning . We decided to err on the side of caution and removed the figure . -One of the conceptually interesting aspects of the paper is the idea of a tree of transformations , but the advantage of deeper trees is never demonstrated convincingly . It looks like the authors have only just gotten this approach to work on toy data with vertical and horizontal bars . While investigating deeper trees is certainly the subject of ongoing work , at submission time we had only tested them on toy examples . We included one of these examples to show that learning more structured representations using deeper trees is a possibility . -Finally , it is not clear how the method could be extended to have multiple layers . The transformation operators T can be defined in the first layer because they act on the input space , but the same can not be done in the learned feature space . It is also not clear how the pose information should be further processed in a hierarchical manner , or how learning in a deep version should work . This is a fair point . The model converts raw pixel values to feature weights and pose parameters in a manner consistent with capsules by Hinton et al.It is true that the same module can not be directly stacked to form a deep version . While building a deeper version is definitely our end goal and top priority , it is out of the scope of this paper ."}}