{"year": "2017", "forum": "HJpfMIFll", "title": "Geometry of Polysemy", "decision": "Accept (Poster)", "meta_review": "The paper considers an important problem largely ignored by continuous word representation learning: polysemy. The approach is mathematically grounded and interesting and well explored.", "reviews": [{"review_id": "HJpfMIFll-0", "review_text": "On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold). On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD), citing and comparing only some relatively recent papers. The experiments in this paper done on SemEval-2010 are not very persuasive. (It's difficult to evaluate the experiments done on the 2016 data, since they are not directly comparable to published results). For example, going back to the SemEval-2010 WSI task in [1], the best system seems to be UoY [2]. The F-measure seems to be a poor metric: always assigning one sense to every word (\"MFS\") yields the highest F-measure of 63.5%. The paper's result with \"2 clusters\" (with an average of about 1.9) seems to be close to MFS. So I don't think we can use F-measure to compare. The V-measure seems to be tilted towards systems that have high number of senses per word. UoY has V=15.7%, while the paper (with \"5 clusters\") has 14.4%. That isn't very convincing that the proposed method has captured the geometry of polysemy. In general, I have often wondered why people work on pure unsupervised WSI and WSD. The assessment is very difficult (as described above). More importantly, some very weakly supervised systems (with minimal labels) can work pretty well to bootstrap. See, e.g., the classic paper [3]. If the authors used the Grassmannian idea to solve higher-level NLP problems directly (such as analogies), that would be very persuasive. However, that's a very different paper than what was submitted. For an example of application of Grassmannian manifolds to analogies, see [4]. References: 1. Manandhar, Suresh, et al. \"SemEval-2010 task 14: Word sense induction & disambiguation.\" Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010. 2. Korkontzelos, Ioannis, and Suresh Manandhar. \"Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation.\" Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010. 3. Yarowsky, David. \"Unsupervised word sense disambiguation rivaling supervised methods.\" Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1995. 4. Mahadevan, Sridhar, and Sarath Chandar Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds https://arxiv.org/abs/1507.07636 ", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer3 , Thanks for your review and comments ! - Regarding the experiments on SemEval-2010 : As far as we understand , the comments focus on two aspects : ( a ) we are not comparing against participating systems in SemEval-2010 ( for instance UoY ) . ( b ) the F-measure is a poor metric . Both these aspects are readily fixed , especially in the innovative format of ICLR where editing the submission is encouraged based on clarification questions and open reviews . Our revised manuscript addresses both these issues . We also have the following comments . 1.We redid $ K $ -Grassmeans using the exact same setup as SemEval-2010 and have reported the results in Table 2 ( quick summary : we outperform SemEval 2010 results in all four metrics ) . 2.We did not explicitly compare to the reported systems in [ 2 ] in the originally submitted version since the task settings are different . In the shared task , the participating systems are trained on contexts given in the training set ( ABC/CNN news ) , but we trained directly using ( the mismatched ) Wikipedia corpus , so the comparison is not fair . Nevertheless , our unsupervised ( and mismatched domain ) performance is fairly close to the top performing systems of SemEval 2010 . On the other hand , relatively recent researches we compared to ( i.e. , Huang 2012 and Neelakantan 2014 ) , are of broadly the same flavor as our ours ( i.e. , unsupervised methods using word embeddings ) and and make natural comparison targets . Second , our approach yields lexeme representations as a direct downstream application , and do not want to limit the domain based on the training data from SemEval-2010 . 3.In terms of the best system from SemEval-2010 , Hermit is the best in terms of V-Measure , and MFS is the best in terms of paired F-score . UoY is the best in terms of supervised evaluations ( which we have now compared against in the revised manuscript -- our performance is better than UoY in all four metrics ) . Re whether F-score is appropriate : we point out that this metric was one used in the SemEval-2010 system ( and thus a natural measure to make comparisons ) . Regarding the evaluation metric , V-Measure favors those with a larger number of cluster and paired F-score favors those with a smaller number of cluster ( as discussed in Section 3 of our text ) -- one notices that UoY had a high V-measure score but at the expense of a large number of clusters ( roughly 11 ) and we perform a bit better than UoY with much smaller number of clusters ( roughly 4 ) . - Regarding the related work : our work is at the intersection of a large number of topical areas ( word vectors , sense disambiguation , sentence representations ) and given the page constraints , we could only cite the most relevant papers . We have now added citations to the key performing systems of some WSI and WSD evaluation tasks . - Regarding higher-level NLP tasks ( analogies ) : Thank you for the reference ( we very much appreciate the camaraderie ) . Based on our detailed reading of this article , we find the focus quite tangential to our paper : their representations are lexical-level while our goals are in disambiguating the sense by innovative representations of the contexts surrounding a given word . Although there is a similarity in terms of certain representations ( subspaces ) , we submit that the similarity is only superficial : the representations are for different objects and the algorithms significantly different too . - Regarding the unsupervised setting : We believe unsupervised approaches are of central scientific and practical interest to NLP : to quote from our introduction : `` since hand-crafted lexical resources sometimes do not reflect the actual meaning of a target word in a given context and , more importantly , such resources are lacking in many languages , we focus on the second approach ( i.e. , unsupervised approach ) in this paper ; such an approach is inherently scalable and potentially plausible with the right set of ideas . Indeed , a human expects the contexts to cue in on the particular sense of a specific word , and successful unsupervised sense representation and sense extraction algorithms would represent progress in the broader area of representation of natural language . Such are the goals of this work . '' The idea of bootstrapping from minimal training data is a useful one ( thank you for the citation to Yarrowsky 's classical paper ) -- but again , it it not clear how to obtain the training data in a principled way . Indeed , Yarrowsky 's work used WordNet , which we ( and others ) have found to have several senses that are too fine-grained ( see Appendix G of our paper for a concrete example ) . Thanks , Jiaqi"}, {"review_id": "HJpfMIFll-1", "review_text": "This paper describe a new method to capture word polysemy with word embeddings. In order to disambiguate a word in a given sentence, the word is represented by the subspace spanned by the word vectors of the context in which it appears. This departs from a traditional approach were the context is represented as a (weighted) sum of the word vectors. A clustering algorithm (very similar to k-means), is then used to cluster the different usages of a given word, and discover the different senses (each sense corresponding to a cluster). The proposed method is evaluated on various word sense induction datasets. It is compared to other word embedding techniques which model word polysemy. The method proposed in the paper to represent words in context is really interesting, simple to apply and seems very effective, based on the strong experimental results reported in the paper. My main concern about this paper is the writing, which is sometimes a bit verbose, making it hard to follow the description of the method. Some of the justification (\"intersection hypothesis\", \"polysemy intersection hypothesis\") might feel a bit like hand waving. Overall, the work presented in the paper looks solid. Pros: - I really liked the idea of representing a word in context by a subspace (as opposed to a weighted sum). Indeed, such representations captures much more information than a single vector. - The proposed method also obtain very good results, compared to existing polysemous word embeddings. - It can be used with any word vectors, making its application very easy. Cons: - I felt that the paper is sometimes a bit verbose and some justifications might be a bit hand waving. - I am also wondering how much of the improvement over existing approaches is due to the quality of the word2vec embeddings, or due to the proposed approach. It would therefore be nice to have a comparison with a regular k-means approach, where context are represented as sum of word vectors using the same embeddings.", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer2 , Thank you for your review and comments ! We admit that our writing is somewhat different from mainstream NLP papers . Given the eight-page limit , we decided to focus more on the insight , the hypothesis , the geometry and the model instead of the algorithm . Once the model is well validated ( so we did lots of experiments to check the rationale of the hypothesis and the model ) , $ k $ -Grassmeans is one of the inference algorithms . Due to the combinatorial nature of clustering and the inherent randomness in the samples , there is no * best * algorithm/method universally for all instances . The proposed $ k $ -Grassmeans is computationally simple and turns out to perform well enough ( i.e. , statistically significantly better than baselines ) in both synthetic tasks ( c.f.the end of page 5 and Figure 4 ( c ) ) and standard polysemy tasks ( c.f.the word sense induction task in Section 5 , the word similarity task in Appendix F.1 , and the police line-up task in Appendix F.2 ) . Alternative inference algorithms are possible in the context of the same model . For example , one can also do $ K $ -means on the subspace basis -- though we did n't check if this approach works well or not . We are happy to add more discussions on the comparison between $ K $ -Grassmeans and the other alternative algorithms . We did a direct comparison on our synthetic tasks ( c.f.the end of page 5 and Figure 4 ( c ) in the updated version ) between $ k $ -Grassmeans of subspaces and $ k $ -means of average word vectors . It turns out that $ k $ -Grassmeans+subspaces performs better ( in a statistically significant way ) than $ k $ -means+averages . We also did an indirect comparison on standard polysemy tasks : ( a ) we redid our experiment on Dec 5 , 2016 as you requested , our algorithm performs better than Huang et al . ( 2012 ) and Neelakantan et al . ( 2015 ) using the same 2010 snapshot of Wikipedia corpus ; ( b ) as reported in their papers , their algorithms are better than a simple ( weighted ) average of word vectors . Specifically , MSSG in Neelakantan et al . ( 2015 ) has the same flavor of $ k $ -means on average word vectors . As indicated by ( a ) and ( b ) , we are confident that subspaces capture more information than averages , and that $ k $ -Grassmeans on subspaces is more robust than $ k $ -means on averages in standardized tasks and datasets as well . Thanks , Jiaqi"}, {"review_id": "HJpfMIFll-2", "review_text": "This paper presents a study of the spaces around existing word embeddings. I proposes something unorthodox: instead of representing a word token by a vector, represent it by the subspace spanned by embeddings of the context word types around that token. These subspaces are fairly low-dimensional and are shown to capture some notions of polysemy (subspaces for tokens of the same sense should all roughly intersect in the same direction). While thinking about the subspace spanned by the context is fairly similar to thinking about a linear combination of the context embeddings, the subspace picture allows for a little more information to be preserved which can improve downstream semantic tasks. The paper is a little dense reading at times, and some things are hard to understand, but the perspective is original enough and the results are good enough that I think it belongs in ICLR.", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer1 , Thank you for your review and comments ! Yes , representing words by subspaces instead of vectors might also be a promising approach . We have n't dived deep into this yet , and we are happy to discuss this idea with you . Thanks , Jiaqi"}], "0": {"review_id": "HJpfMIFll-0", "review_text": "On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold). On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD), citing and comparing only some relatively recent papers. The experiments in this paper done on SemEval-2010 are not very persuasive. (It's difficult to evaluate the experiments done on the 2016 data, since they are not directly comparable to published results). For example, going back to the SemEval-2010 WSI task in [1], the best system seems to be UoY [2]. The F-measure seems to be a poor metric: always assigning one sense to every word (\"MFS\") yields the highest F-measure of 63.5%. The paper's result with \"2 clusters\" (with an average of about 1.9) seems to be close to MFS. So I don't think we can use F-measure to compare. The V-measure seems to be tilted towards systems that have high number of senses per word. UoY has V=15.7%, while the paper (with \"5 clusters\") has 14.4%. That isn't very convincing that the proposed method has captured the geometry of polysemy. In general, I have often wondered why people work on pure unsupervised WSI and WSD. The assessment is very difficult (as described above). More importantly, some very weakly supervised systems (with minimal labels) can work pretty well to bootstrap. See, e.g., the classic paper [3]. If the authors used the Grassmannian idea to solve higher-level NLP problems directly (such as analogies), that would be very persuasive. However, that's a very different paper than what was submitted. For an example of application of Grassmannian manifolds to analogies, see [4]. References: 1. Manandhar, Suresh, et al. \"SemEval-2010 task 14: Word sense induction & disambiguation.\" Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010. 2. Korkontzelos, Ioannis, and Suresh Manandhar. \"Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation.\" Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010. 3. Yarowsky, David. \"Unsupervised word sense disambiguation rivaling supervised methods.\" Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1995. 4. Mahadevan, Sridhar, and Sarath Chandar Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds https://arxiv.org/abs/1507.07636 ", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer3 , Thanks for your review and comments ! - Regarding the experiments on SemEval-2010 : As far as we understand , the comments focus on two aspects : ( a ) we are not comparing against participating systems in SemEval-2010 ( for instance UoY ) . ( b ) the F-measure is a poor metric . Both these aspects are readily fixed , especially in the innovative format of ICLR where editing the submission is encouraged based on clarification questions and open reviews . Our revised manuscript addresses both these issues . We also have the following comments . 1.We redid $ K $ -Grassmeans using the exact same setup as SemEval-2010 and have reported the results in Table 2 ( quick summary : we outperform SemEval 2010 results in all four metrics ) . 2.We did not explicitly compare to the reported systems in [ 2 ] in the originally submitted version since the task settings are different . In the shared task , the participating systems are trained on contexts given in the training set ( ABC/CNN news ) , but we trained directly using ( the mismatched ) Wikipedia corpus , so the comparison is not fair . Nevertheless , our unsupervised ( and mismatched domain ) performance is fairly close to the top performing systems of SemEval 2010 . On the other hand , relatively recent researches we compared to ( i.e. , Huang 2012 and Neelakantan 2014 ) , are of broadly the same flavor as our ours ( i.e. , unsupervised methods using word embeddings ) and and make natural comparison targets . Second , our approach yields lexeme representations as a direct downstream application , and do not want to limit the domain based on the training data from SemEval-2010 . 3.In terms of the best system from SemEval-2010 , Hermit is the best in terms of V-Measure , and MFS is the best in terms of paired F-score . UoY is the best in terms of supervised evaluations ( which we have now compared against in the revised manuscript -- our performance is better than UoY in all four metrics ) . Re whether F-score is appropriate : we point out that this metric was one used in the SemEval-2010 system ( and thus a natural measure to make comparisons ) . Regarding the evaluation metric , V-Measure favors those with a larger number of cluster and paired F-score favors those with a smaller number of cluster ( as discussed in Section 3 of our text ) -- one notices that UoY had a high V-measure score but at the expense of a large number of clusters ( roughly 11 ) and we perform a bit better than UoY with much smaller number of clusters ( roughly 4 ) . - Regarding the related work : our work is at the intersection of a large number of topical areas ( word vectors , sense disambiguation , sentence representations ) and given the page constraints , we could only cite the most relevant papers . We have now added citations to the key performing systems of some WSI and WSD evaluation tasks . - Regarding higher-level NLP tasks ( analogies ) : Thank you for the reference ( we very much appreciate the camaraderie ) . Based on our detailed reading of this article , we find the focus quite tangential to our paper : their representations are lexical-level while our goals are in disambiguating the sense by innovative representations of the contexts surrounding a given word . Although there is a similarity in terms of certain representations ( subspaces ) , we submit that the similarity is only superficial : the representations are for different objects and the algorithms significantly different too . - Regarding the unsupervised setting : We believe unsupervised approaches are of central scientific and practical interest to NLP : to quote from our introduction : `` since hand-crafted lexical resources sometimes do not reflect the actual meaning of a target word in a given context and , more importantly , such resources are lacking in many languages , we focus on the second approach ( i.e. , unsupervised approach ) in this paper ; such an approach is inherently scalable and potentially plausible with the right set of ideas . Indeed , a human expects the contexts to cue in on the particular sense of a specific word , and successful unsupervised sense representation and sense extraction algorithms would represent progress in the broader area of representation of natural language . Such are the goals of this work . '' The idea of bootstrapping from minimal training data is a useful one ( thank you for the citation to Yarrowsky 's classical paper ) -- but again , it it not clear how to obtain the training data in a principled way . Indeed , Yarrowsky 's work used WordNet , which we ( and others ) have found to have several senses that are too fine-grained ( see Appendix G of our paper for a concrete example ) . Thanks , Jiaqi"}, "1": {"review_id": "HJpfMIFll-1", "review_text": "This paper describe a new method to capture word polysemy with word embeddings. In order to disambiguate a word in a given sentence, the word is represented by the subspace spanned by the word vectors of the context in which it appears. This departs from a traditional approach were the context is represented as a (weighted) sum of the word vectors. A clustering algorithm (very similar to k-means), is then used to cluster the different usages of a given word, and discover the different senses (each sense corresponding to a cluster). The proposed method is evaluated on various word sense induction datasets. It is compared to other word embedding techniques which model word polysemy. The method proposed in the paper to represent words in context is really interesting, simple to apply and seems very effective, based on the strong experimental results reported in the paper. My main concern about this paper is the writing, which is sometimes a bit verbose, making it hard to follow the description of the method. Some of the justification (\"intersection hypothesis\", \"polysemy intersection hypothesis\") might feel a bit like hand waving. Overall, the work presented in the paper looks solid. Pros: - I really liked the idea of representing a word in context by a subspace (as opposed to a weighted sum). Indeed, such representations captures much more information than a single vector. - The proposed method also obtain very good results, compared to existing polysemous word embeddings. - It can be used with any word vectors, making its application very easy. Cons: - I felt that the paper is sometimes a bit verbose and some justifications might be a bit hand waving. - I am also wondering how much of the improvement over existing approaches is due to the quality of the word2vec embeddings, or due to the proposed approach. It would therefore be nice to have a comparison with a regular k-means approach, where context are represented as sum of word vectors using the same embeddings.", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer2 , Thank you for your review and comments ! We admit that our writing is somewhat different from mainstream NLP papers . Given the eight-page limit , we decided to focus more on the insight , the hypothesis , the geometry and the model instead of the algorithm . Once the model is well validated ( so we did lots of experiments to check the rationale of the hypothesis and the model ) , $ k $ -Grassmeans is one of the inference algorithms . Due to the combinatorial nature of clustering and the inherent randomness in the samples , there is no * best * algorithm/method universally for all instances . The proposed $ k $ -Grassmeans is computationally simple and turns out to perform well enough ( i.e. , statistically significantly better than baselines ) in both synthetic tasks ( c.f.the end of page 5 and Figure 4 ( c ) ) and standard polysemy tasks ( c.f.the word sense induction task in Section 5 , the word similarity task in Appendix F.1 , and the police line-up task in Appendix F.2 ) . Alternative inference algorithms are possible in the context of the same model . For example , one can also do $ K $ -means on the subspace basis -- though we did n't check if this approach works well or not . We are happy to add more discussions on the comparison between $ K $ -Grassmeans and the other alternative algorithms . We did a direct comparison on our synthetic tasks ( c.f.the end of page 5 and Figure 4 ( c ) in the updated version ) between $ k $ -Grassmeans of subspaces and $ k $ -means of average word vectors . It turns out that $ k $ -Grassmeans+subspaces performs better ( in a statistically significant way ) than $ k $ -means+averages . We also did an indirect comparison on standard polysemy tasks : ( a ) we redid our experiment on Dec 5 , 2016 as you requested , our algorithm performs better than Huang et al . ( 2012 ) and Neelakantan et al . ( 2015 ) using the same 2010 snapshot of Wikipedia corpus ; ( b ) as reported in their papers , their algorithms are better than a simple ( weighted ) average of word vectors . Specifically , MSSG in Neelakantan et al . ( 2015 ) has the same flavor of $ k $ -means on average word vectors . As indicated by ( a ) and ( b ) , we are confident that subspaces capture more information than averages , and that $ k $ -Grassmeans on subspaces is more robust than $ k $ -means on averages in standardized tasks and datasets as well . Thanks , Jiaqi"}, "2": {"review_id": "HJpfMIFll-2", "review_text": "This paper presents a study of the spaces around existing word embeddings. I proposes something unorthodox: instead of representing a word token by a vector, represent it by the subspace spanned by embeddings of the context word types around that token. These subspaces are fairly low-dimensional and are shown to capture some notions of polysemy (subspaces for tokens of the same sense should all roughly intersect in the same direction). While thinking about the subspace spanned by the context is fairly similar to thinking about a linear combination of the context embeddings, the subspace picture allows for a little more information to be preserved which can improve downstream semantic tasks. The paper is a little dense reading at times, and some things are hard to understand, but the perspective is original enough and the results are good enough that I think it belongs in ICLR.", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer1 , Thank you for your review and comments ! Yes , representing words by subspaces instead of vectors might also be a promising approach . We have n't dived deep into this yet , and we are happy to discuss this idea with you . Thanks , Jiaqi"}}