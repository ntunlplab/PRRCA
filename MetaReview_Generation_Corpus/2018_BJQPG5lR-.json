{"year": "2018", "forum": "BJQPG5lR-", "title": "Avoiding degradation in deep feed-forward networks by phasing out skip-connections", "decision": "Reject", "meta_review": "Pros:\n+ Interesting perspective on training deep networks\n\nCons:\n- Not a lot of practical significance: why would one want to use this algorithm over standard methods like ResNets or highway networks given that the proposed algorithm is more complex than established methods?\n", "reviews": [{"review_id": "BJQPG5lR--0", "review_text": "EDIT: The rating has been changed. See thread below for explanation / further comments. ORIGINAL REVIEW: In this paper, the authors present a new training strategy, VAN, for training very deep feed-forward networks without skip connections (henceforth called VDFFNWSC) by introducing skip connections early in training and then gradually removing them. I think the fact that the authors demonstrate the viability of training VDFFNWSCs that could have, in principle, arbitrary nonlinearities and normalization layers, is somewhat valuable and as such I would generally be inclined towards acceptance, even though the potential impact of this paper is limited because the training strategy proposed is (by deep learning standards) relatively complicated, requires tuning two additional hyperparameters in the initial value of \\lambda as well as the step size for updating \\lambda, and seems to have no significant advantage over just using skip connections throughout training. So my rating based on the message of the paper would be 6/10. However, there appear to be a range of issues. As long as those issues remain unresolved, my rating is at is but if those issues were resolved it could go up to a 6. +++ Section 3.1 problems +++ - I think the toy example presented in section 3.1 is more confusing than it is helpful because the skip connection you introduce in the toy example is different from the skip connection you introduce in VANs. In the toy example, you add (1 - \\alpha)wx whereas in the VANs you add (1 - \\alpha)x. Therefore, the type of vanishing gradient that is observed when tanh saturates, which you combat in the toy model, is not actually combated at all in the VAN model. While it is true that skip connections combat vanishing gradients in certain situations, your example does not capture how this is achieved in VANs. - The toy example seems to be an example where Lagrangian relaxation fails, not where it succeeds. Looking at figure 1, it appears that you start out with some alpha < 1 but then immediately alpha converges to 1, i.e. the skip connection is eliminated early in training, because wx is further away from y than tanh(wx). Most of the training takes place without the skip connection. In fact, after 10^4 iterations, training with and without skip connection seem to achieve the same error. It appears that introducing the skip connection was next to useless and the model failed to recognize the usefulness of the skip connection early in training. - Regarding the optimization algorithm involving \\alpha^* at the end of section 3: It looks to me like a hacky, unprincipled method with no guarantees that just happened to work in the particular example you studied. You motivate the choice of \\alpha^* by wanting to maximize the reduction in the local linear approximation to \\mathcal{C} induced by the update on w. However, this reduction grows to infinity the larger the update is. Does that mean that larger updates are always better? Clearly not. If we wanted to reduce the size of the objective according to the local linear approximation, why wouldn't we choose infinitely large step sizes? Hence, the motivation for the algorithm you present is invalid. Here is an example where this algorithm fails: consider the point (x,y,w,\\alpha,\\lambda) = (100, \\sigma(100), 1.0001, 1, 1). Here, w has almost converged to its optimum w* = 1. Correspondingly, the derivative of C is a small negative value. However, \\alpha* is actually 0, and this choice would catapult w far away from w*. If I haven't made a mistake in my criticisms above, I strongly suggest removing section 3.1 entirely or replacing it with a completely new example that does not suffer from the above issues. +++ ResNet scaling +++ There is a crucial difference between VANs and ResNets. In the VAN initial state (alpha = 0.5), both the residual path and the skip path are multiplied by 0.5 whereas for ResNet, neither is multiplied by 0.5. Because of this, the experimental results between the two architectures are incomparable. In a question I posed earlier, you claimed that this scaling makes no difference when batch normalization is used. I disagree. Let's look at an example. Consider ResNet first. It can be written as x + r_1 + r_2 + .. + r_B, where r_b is the value computed by residual block b. Now let's assume we insert a scaling constant after each residual block, say c = 0.5. Then the result is c^{B}x + c^{B-1}r_1 + c^{B-2}r_2 + .. + r_B. Therefore, contributions of lower blocks vanish exponentially. This effect is not combated by batch normalization. So the learning dynamics for VAN and ResNet are very different because of this scaling. Therefore, there is an open question: are the differences in results between VAN and ResNet in your experiments caused by the removal of skip connections during training or by this scaling? Without this information, the experiments have limited value. In fact, I suspect that the vanishing of the contribution of lower blocks bears more responsibility for the declining performance of VAN at higher depths than the removal of skip connections. If my assessment of the situation is correct, I would like to ask you to repeat your experiments with the following two settings: - ResNet where after each block you multiply the result of the addition by 0.5, i.e. x_{l+1} = 0.5\\mathcal{F}(x_l) + 0.5x_l - VAN with the following altered equation: x_{l+1} = \\mathcal{F}(x_l) + (1-\\alpha)x_l, i.e. please remove the alpha in front of \\mathcal{F}. Also, initialize \\alpha to zero. This ensures that VAN starts out as a regular ResNet. +++ writing issues +++ Title: - \"VARIABLE ACTIVATION NETWORKS: A SIMPLE METHOD TO TRAIN DEEP FEED-FORWARD NETWORKS WITHOUT SKIP-CONNECTIONS\" This title can be read in two different ways. (A) [Train] [deep feed-forward networks] [without skip-connections] and (B) [Train] [deep feed-forward networks without skip connections]. In (A), the `without skip-connections' modifies the `train' and suggests that training took place without skip connections. In (B), the `without skip-connections' modifies `deep feed-forward networks' and suggests that the network trained has no skip connections. You must mean (B), because (A) is false. Since it is not clear from reading the title whether (A) or (B) is true, please reword it. Abstract: - \"Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients). In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections.\" Again, this is ambiguous. To me, this sentence implies that you extend the benefit of avoiding vanishing and exploding gradients to fully-connected networks without skip connections. However, nowhere in your paper do you show that trained VANs have less exploding / vanishing gradients than fully-connected networks trained the old-fashioned way. Again, please reword or include evidence. - \"where the proposed method is shown to outperform many architectures without skip-connections\" Again, this sentence makes no sense to me. It seems to imply that VAN has skip connections. But in the abstract you defined VAN as an architecture without skip connections. Please make this more clear. Introduction: - \"Indeed, Zagoruyko & Komodakis (2016) demonstrate that it is better to increase the width of ResNets than the depth, suggesting that perhaps only a few layers are learning useful representations.\" Just because increasing width may be better than increasing depth does not mean that deep layers don't learn useful representations. In fact, the claim that deep layers don't learn useful representations is directly contradicted by the paper. section 3.1: - replace \"to to\" by \"to\" in the second line section 4: - \"This may be a result of the ensemble nature of ResNets (Veit et al., 2016), which does not play a significant role until the depth of the network increases.\" The ensemble nature of ResNet is a drawback, not an advantage, because it causes a lack of high-order co-adaptataion of layers. Therefore, it cannot contribute positively to the performance or ResNet. As mentioned in earlier comments, please reword / clarify your use of \"activation function\". It is generally used a synonym for \"nonlinearity\", so please use it in this way. Change your claim that VAN is equivalent to PReLU. Please include your description of how your method can be extended to networks which do allow for skip connections. +++ Hyperparameters +++ Since the initial values of \\lambda and \\eta' are new hyperparameters, include the values you chose for them, explain how you arrived at those values and plot the curve of how \\lambda evolves for at least some of the experiments.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for providing a thorough and thoughtful review . We have made many of the changes suggested in the review , leading to an improved manuscript in the process . Below we respond to each of the comments in turn . Toy problem : The reviewer raised important issues relating to the toy problem and its relevance to the proposed framework . As such , we have removed this motivating example from the updated manuscript . ResNet scaling : We thank the reviewer for alerting us to an important shortcoming of the proposed method . Following the reviewers suggestion , we have run experiments with the following residual block structure : \\mathcal { F } ( x_l , W_l ) + ( 1 - \\alpha_l ) x_l Further , as suggested by the reviewer we have initialized with \\alpha=0 resulting in networks that were equivalent to ResNets at initialization . Our experiments suggest that this formulation leads to improved results , especially in the context of very deep networks . Further , we have also run the ResNet scaling experiment suggested by the reviewer and find similar degradation in performance for deep networks . These results , which are reported in Appendix A , provide evidence for the reviewers hypothesis that modulating the \\mathcal { F } by some \\alpha_l \\neq 1 leads to a vanishing contribution from shallow blocks . Writing issues : We have the following changes : - Title : As suggested by the reviewer , we have amended the title to clearly resemble the goal of the proposed method and our contribution . The new title is : \u201c Avoiding degradation in deep feed-forward networks by phasing out skip-connections \u201d . - Abstract : The abstract has also been re-written in order to clarify the objectives and contributions of our work . - Introduction , claim about relationship between width and depth : the paragraph in question has been removed as it was not relevant to the goals of the proposed method . - Section 4 , comment about ensemble nature of ResNets : Whether the ensemble nature of ResNets is an advantage of disadvantage is unclear . While boosting will typically lead to better performance , the reviewer notes that the ensemble nature of ResNets may actually be detrimental as it leads to co-adaptation of features . In order to avoid entering into this discussion we have removed this sentence . - Use of activation function/non-linearity : this has been clarified throughout . Hyper-parameters : we have updated the manuscript to clearly state the choice of hyper-parameters . In particular , we now clearly state the choice of \\eta \u2019 and provide traces for the Lagrange multipliers in Appendix B ."}, {"review_id": "BJQPG5lR--1", "review_text": "UPDATED COMMENT I've improved my score to 6 to reflect the authors' revisions to the paper and their response to my and R2's comments. I still think the work is somewhat incremental, but they have done a good job of exploring the idea (which is nice). ORIGINAL REVIEW BELOW The paper introduces an architecture that linearly interpolates between ResNets and vanilla deep nets (without skip connections). The skip connections are penalized by Lagrange multipliers that are gradually phased out during training. The resulting architecture outperforms vanilla deep nets and sometimes approaches the performance of ResNets. It\u2019s a nice, simple idea. However, I don\u2019t think it\u2019s sufficient for acceptance. Unfortunately, this seems to be a simple idea that doesn't work as well as the simpler idea (ResNets) that inspired it. Moreover, the experiments are weak in two senses: (i) there are lots of obvious open questions that should have been explored and closed, see below, and (ii) the results just aren\u2019t that good. Comments: 1. Why force the Lag. multipliers to 1 at the end of training? It seems easy enough to treat the alphas as just more parameters to optimize with gradient descent. I would expect the resulting architecture to perform at least as well as variable action nets. If not, I\u2019d be curious as to why. 2.Similarly, it\u2019s not obvious that initializing the multipliers at 0.5 is the best choice. The \u201clooks linear\u201d initialization proposed in \u201cThe shattered gradients problem\u201d (Balduzzi et al) implies that alpha=0 may work better. Did the authors try any values besides 0.5? 3. The final paragraph of the paper discusses extending the approach to architectures with skip-connections. Firstly, it\u2019s not clear to me what this would add, since the method is already interpolating in some sense between vanilla and resnets. Secondly, why not just do it? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for raising important issues . We respond to each below . Not enforcing constraint on alphas : We have run the additional experiments suggested by the reviewer . As expected , when no constraint is enforced on the alphas ( this corresponds to setting the Lagrange multipliers to 0 in equation ( 6 ) ) , the performance of VAN networks does indeed improve . These additional experiments have been included in the updated version of the manuscript ( see Figures 2 and 3 as well as Table 2 ) . Initialization of alpha : Our original experiments focused on the choice of alpha=0.5 at initialization . However , the reviewer correctly notes that such an initialization will not necessarily be optimal . Following the suggestion of AnonReview2 we have reformulated the VAN equation as follows : \\mathcal { F } ( x_l ) + ( 1 - \\alpha_l ) x_l And initialize with alpha_l=0 . This ensures that VANs are equivalent to ResNets at initialization ( while this was not previously the case ) . More importantly , such a reparameterization leads to improved performance - an empirical comparison is provided in Appendix A . VANs with skip-connections : the reviewer correctly notes that it is intuitively obvious how to combine VAN residual blocks with architectures that contain skip-connections . One way the two could be combined would be by not enforcing the constraint on \\alpha_l in VANs ( as we have done in response to a previous comment ) . We have removed this sentence from the manuscript ."}, {"review_id": "BJQPG5lR--2", "review_text": "Update (original review below): The authors have addressed several of the reviewers' comments and improved the paper. The motivation has certainly been clarified, but in my opinion it is still hazy. The paper does use skip connections, but the difference is that they are phased out over training. So I think that the motivation behind introducing this specific difference should be clear. Is it to save the additional (small) overhead of using skip connections? Nevertheless, the additional experiments and clarifications are very welcome. For the newly added case of VAN(lambda=0), please note the strong similarity to https://arxiv.org/abs/1611.01260 (ICLR2017 reviews at https://openreview.net/forum?id=Sywh5KYex). In that report \\alpha_l is a scalar instead of a vector. Although it is interesting, the above case case also calls into question the additional value brought by the use of constrained optimization, a main contribution of the paper. In light of the above, I have increased my score since I find this to be an interesting approach, but in my opinion the significance of the results as they stand is low. The paper demonstrates that it is possible to obtain very deep plain networks (without skip connections) with improved performance through the use of constrained optimization that gradually removes skip connections, but the value of this demonstration is unclear because a) consistent improvements over past work or the \\lambda=0 case were not found, and b) The technique still relies on skip connections in a sense so it's not clear that it suggests a truly different method of addressing the degradation problem. Original Review ============= Summary: The contribution of this paper is a method for training deep networks such that skip connections are present at initialization, but gradually removed during training, resulting in a final network without any skip connections. The paper first proposes an approach based on a formulation of deep networks with (non-parameterized, non-gated) skip connections with an equality constraint that effectively removes the skip connections when satisfied. It is proposed to optimize the formulation using the method of Lagrange multipliers. A toy model with a single unit is used to illustrate the basic ideas behind the method. Finally, experimental results for the task of image classification are reported using the MNIST, Fashion-MNIST, and CIFAR datasets. Quality and significance: The proposed methodology is simple and straightforward. The analysis with the toy network is interesting and helps illustrate the method. However, my main concerns with this paper are related to motivation and experiments. The motivation of the work is not clear at all. The stated goal is to address some of the issues related to the role of depth in deep networks, but I think it should be clarified which specific issues in particular are relevant to this method and how they are addressed. One could additionally consider that removing the skip connections at the end of training reduces the computational expense (slightly), but beyond that the expected utility of this investigation is very hazy from the description in the paper. For MNIST and MNIST-Fashion experiments, the motivation is mentioned to be similar to Srivastava et al. (2015), but in that study the corresponding experiment was designed to test if deeper networks could be optimized. Here, the generalization error is measured instead, which is heavily influenced by regularization. Moreover, only some architectures appear to employ batch normalization, which is a potent regularizer. The general difference between plain and non-plain networks is very likely due to optimization difficulties alone, and due to the above issues further comparisons can not be made from the results. For the CIFAR experiments, the experiment design is reasonable for a general comparison. Similar experimental setups have been used in previous papers to report that a proposed method can achieve good results, but there is no doubt that this does not make a rigorous comparison without employing expensive hyper-parameter searches. This is not the fault of the present paper but an unfortunate tradition in the field. Nevertheless, it is important to note that direct comparison should not be made among approaches with key differences. For the reported results, Fitnets and Highway Networks did not use Batch Normalization (which is a powerful regularizer) while VANs and Resnets do. Moreover, it is important to report the training performance of deeper VANs (which have a worse generalization error) to clarify if the VANs suffered difficulties in optimization or generalization. Clarity: The paper is generally well-written and easy to read. There are some clarity issues related to the use of the term \"activation function\" and a typo in an equation but the authors are already aware of these.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for raising three important points , which we respond to below Motivation : We agree with the reviewer that the motivation of the proposed work was unclear in the original submission . As a result , we have updated the manuscript to clearly state out motivation : which is to address the degradation problem in deep feed-forward networks . While ResNets and Highway networks address this issue by introducing skip-connections or gating mechanisms , we look to tackle the problem from the perspective of constrained optimization . As such , the objective of the our work is to propose a new training regime for plain networks which explicitly addresses the issue of performance degradation with depth for plain feed-forward networks . In order to clearly reflect our motivation and contribution we have amended the title of the manuscript as well as clarified the abstract and the introduction . Regarding MNIST and MNIST-Fashion experiments : Batch-normalization was employed across all architectures in the MNIST experiments ( although we do acknowledge that the original Highway network experiments did not use batch norm ) . We have updated the manuscript to clearly state this . As a result , we do believe that comparisons across the different architectures are valid on this experiment . While Srivastava et al . ( 2015 ) reported the training cross-entropy , it is unclear to us why reporting the generalization performance does not serve as an indication of successful optimization across various networks given that all networks employed batch normalization . CIFAR experiments : The reviewer correctly notes that extensive hyper-parameter searches where not run for the CIFAR experiments . This is because the objective of the experiments in Section 4 was not to achieve state-of-the-art performance but rather to demonstrate that VAN networks do not suffer from the degradation problem to the same extent as plain feed-forward networks . We feel that these experiments serve to validate this claim . Regarding comparisons with alternative architectures ( e.g. , FitNets and Highway networks ) we have amended Table 2 to state which architectures did and did not use batch-normalization ."}], "0": {"review_id": "BJQPG5lR--0", "review_text": "EDIT: The rating has been changed. See thread below for explanation / further comments. ORIGINAL REVIEW: In this paper, the authors present a new training strategy, VAN, for training very deep feed-forward networks without skip connections (henceforth called VDFFNWSC) by introducing skip connections early in training and then gradually removing them. I think the fact that the authors demonstrate the viability of training VDFFNWSCs that could have, in principle, arbitrary nonlinearities and normalization layers, is somewhat valuable and as such I would generally be inclined towards acceptance, even though the potential impact of this paper is limited because the training strategy proposed is (by deep learning standards) relatively complicated, requires tuning two additional hyperparameters in the initial value of \\lambda as well as the step size for updating \\lambda, and seems to have no significant advantage over just using skip connections throughout training. So my rating based on the message of the paper would be 6/10. However, there appear to be a range of issues. As long as those issues remain unresolved, my rating is at is but if those issues were resolved it could go up to a 6. +++ Section 3.1 problems +++ - I think the toy example presented in section 3.1 is more confusing than it is helpful because the skip connection you introduce in the toy example is different from the skip connection you introduce in VANs. In the toy example, you add (1 - \\alpha)wx whereas in the VANs you add (1 - \\alpha)x. Therefore, the type of vanishing gradient that is observed when tanh saturates, which you combat in the toy model, is not actually combated at all in the VAN model. While it is true that skip connections combat vanishing gradients in certain situations, your example does not capture how this is achieved in VANs. - The toy example seems to be an example where Lagrangian relaxation fails, not where it succeeds. Looking at figure 1, it appears that you start out with some alpha < 1 but then immediately alpha converges to 1, i.e. the skip connection is eliminated early in training, because wx is further away from y than tanh(wx). Most of the training takes place without the skip connection. In fact, after 10^4 iterations, training with and without skip connection seem to achieve the same error. It appears that introducing the skip connection was next to useless and the model failed to recognize the usefulness of the skip connection early in training. - Regarding the optimization algorithm involving \\alpha^* at the end of section 3: It looks to me like a hacky, unprincipled method with no guarantees that just happened to work in the particular example you studied. You motivate the choice of \\alpha^* by wanting to maximize the reduction in the local linear approximation to \\mathcal{C} induced by the update on w. However, this reduction grows to infinity the larger the update is. Does that mean that larger updates are always better? Clearly not. If we wanted to reduce the size of the objective according to the local linear approximation, why wouldn't we choose infinitely large step sizes? Hence, the motivation for the algorithm you present is invalid. Here is an example where this algorithm fails: consider the point (x,y,w,\\alpha,\\lambda) = (100, \\sigma(100), 1.0001, 1, 1). Here, w has almost converged to its optimum w* = 1. Correspondingly, the derivative of C is a small negative value. However, \\alpha* is actually 0, and this choice would catapult w far away from w*. If I haven't made a mistake in my criticisms above, I strongly suggest removing section 3.1 entirely or replacing it with a completely new example that does not suffer from the above issues. +++ ResNet scaling +++ There is a crucial difference between VANs and ResNets. In the VAN initial state (alpha = 0.5), both the residual path and the skip path are multiplied by 0.5 whereas for ResNet, neither is multiplied by 0.5. Because of this, the experimental results between the two architectures are incomparable. In a question I posed earlier, you claimed that this scaling makes no difference when batch normalization is used. I disagree. Let's look at an example. Consider ResNet first. It can be written as x + r_1 + r_2 + .. + r_B, where r_b is the value computed by residual block b. Now let's assume we insert a scaling constant after each residual block, say c = 0.5. Then the result is c^{B}x + c^{B-1}r_1 + c^{B-2}r_2 + .. + r_B. Therefore, contributions of lower blocks vanish exponentially. This effect is not combated by batch normalization. So the learning dynamics for VAN and ResNet are very different because of this scaling. Therefore, there is an open question: are the differences in results between VAN and ResNet in your experiments caused by the removal of skip connections during training or by this scaling? Without this information, the experiments have limited value. In fact, I suspect that the vanishing of the contribution of lower blocks bears more responsibility for the declining performance of VAN at higher depths than the removal of skip connections. If my assessment of the situation is correct, I would like to ask you to repeat your experiments with the following two settings: - ResNet where after each block you multiply the result of the addition by 0.5, i.e. x_{l+1} = 0.5\\mathcal{F}(x_l) + 0.5x_l - VAN with the following altered equation: x_{l+1} = \\mathcal{F}(x_l) + (1-\\alpha)x_l, i.e. please remove the alpha in front of \\mathcal{F}. Also, initialize \\alpha to zero. This ensures that VAN starts out as a regular ResNet. +++ writing issues +++ Title: - \"VARIABLE ACTIVATION NETWORKS: A SIMPLE METHOD TO TRAIN DEEP FEED-FORWARD NETWORKS WITHOUT SKIP-CONNECTIONS\" This title can be read in two different ways. (A) [Train] [deep feed-forward networks] [without skip-connections] and (B) [Train] [deep feed-forward networks without skip connections]. In (A), the `without skip-connections' modifies the `train' and suggests that training took place without skip connections. In (B), the `without skip-connections' modifies `deep feed-forward networks' and suggests that the network trained has no skip connections. You must mean (B), because (A) is false. Since it is not clear from reading the title whether (A) or (B) is true, please reword it. Abstract: - \"Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients). In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections.\" Again, this is ambiguous. To me, this sentence implies that you extend the benefit of avoiding vanishing and exploding gradients to fully-connected networks without skip connections. However, nowhere in your paper do you show that trained VANs have less exploding / vanishing gradients than fully-connected networks trained the old-fashioned way. Again, please reword or include evidence. - \"where the proposed method is shown to outperform many architectures without skip-connections\" Again, this sentence makes no sense to me. It seems to imply that VAN has skip connections. But in the abstract you defined VAN as an architecture without skip connections. Please make this more clear. Introduction: - \"Indeed, Zagoruyko & Komodakis (2016) demonstrate that it is better to increase the width of ResNets than the depth, suggesting that perhaps only a few layers are learning useful representations.\" Just because increasing width may be better than increasing depth does not mean that deep layers don't learn useful representations. In fact, the claim that deep layers don't learn useful representations is directly contradicted by the paper. section 3.1: - replace \"to to\" by \"to\" in the second line section 4: - \"This may be a result of the ensemble nature of ResNets (Veit et al., 2016), which does not play a significant role until the depth of the network increases.\" The ensemble nature of ResNet is a drawback, not an advantage, because it causes a lack of high-order co-adaptataion of layers. Therefore, it cannot contribute positively to the performance or ResNet. As mentioned in earlier comments, please reword / clarify your use of \"activation function\". It is generally used a synonym for \"nonlinearity\", so please use it in this way. Change your claim that VAN is equivalent to PReLU. Please include your description of how your method can be extended to networks which do allow for skip connections. +++ Hyperparameters +++ Since the initial values of \\lambda and \\eta' are new hyperparameters, include the values you chose for them, explain how you arrived at those values and plot the curve of how \\lambda evolves for at least some of the experiments.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for providing a thorough and thoughtful review . We have made many of the changes suggested in the review , leading to an improved manuscript in the process . Below we respond to each of the comments in turn . Toy problem : The reviewer raised important issues relating to the toy problem and its relevance to the proposed framework . As such , we have removed this motivating example from the updated manuscript . ResNet scaling : We thank the reviewer for alerting us to an important shortcoming of the proposed method . Following the reviewers suggestion , we have run experiments with the following residual block structure : \\mathcal { F } ( x_l , W_l ) + ( 1 - \\alpha_l ) x_l Further , as suggested by the reviewer we have initialized with \\alpha=0 resulting in networks that were equivalent to ResNets at initialization . Our experiments suggest that this formulation leads to improved results , especially in the context of very deep networks . Further , we have also run the ResNet scaling experiment suggested by the reviewer and find similar degradation in performance for deep networks . These results , which are reported in Appendix A , provide evidence for the reviewers hypothesis that modulating the \\mathcal { F } by some \\alpha_l \\neq 1 leads to a vanishing contribution from shallow blocks . Writing issues : We have the following changes : - Title : As suggested by the reviewer , we have amended the title to clearly resemble the goal of the proposed method and our contribution . The new title is : \u201c Avoiding degradation in deep feed-forward networks by phasing out skip-connections \u201d . - Abstract : The abstract has also been re-written in order to clarify the objectives and contributions of our work . - Introduction , claim about relationship between width and depth : the paragraph in question has been removed as it was not relevant to the goals of the proposed method . - Section 4 , comment about ensemble nature of ResNets : Whether the ensemble nature of ResNets is an advantage of disadvantage is unclear . While boosting will typically lead to better performance , the reviewer notes that the ensemble nature of ResNets may actually be detrimental as it leads to co-adaptation of features . In order to avoid entering into this discussion we have removed this sentence . - Use of activation function/non-linearity : this has been clarified throughout . Hyper-parameters : we have updated the manuscript to clearly state the choice of hyper-parameters . In particular , we now clearly state the choice of \\eta \u2019 and provide traces for the Lagrange multipliers in Appendix B ."}, "1": {"review_id": "BJQPG5lR--1", "review_text": "UPDATED COMMENT I've improved my score to 6 to reflect the authors' revisions to the paper and their response to my and R2's comments. I still think the work is somewhat incremental, but they have done a good job of exploring the idea (which is nice). ORIGINAL REVIEW BELOW The paper introduces an architecture that linearly interpolates between ResNets and vanilla deep nets (without skip connections). The skip connections are penalized by Lagrange multipliers that are gradually phased out during training. The resulting architecture outperforms vanilla deep nets and sometimes approaches the performance of ResNets. It\u2019s a nice, simple idea. However, I don\u2019t think it\u2019s sufficient for acceptance. Unfortunately, this seems to be a simple idea that doesn't work as well as the simpler idea (ResNets) that inspired it. Moreover, the experiments are weak in two senses: (i) there are lots of obvious open questions that should have been explored and closed, see below, and (ii) the results just aren\u2019t that good. Comments: 1. Why force the Lag. multipliers to 1 at the end of training? It seems easy enough to treat the alphas as just more parameters to optimize with gradient descent. I would expect the resulting architecture to perform at least as well as variable action nets. If not, I\u2019d be curious as to why. 2.Similarly, it\u2019s not obvious that initializing the multipliers at 0.5 is the best choice. The \u201clooks linear\u201d initialization proposed in \u201cThe shattered gradients problem\u201d (Balduzzi et al) implies that alpha=0 may work better. Did the authors try any values besides 0.5? 3. The final paragraph of the paper discusses extending the approach to architectures with skip-connections. Firstly, it\u2019s not clear to me what this would add, since the method is already interpolating in some sense between vanilla and resnets. Secondly, why not just do it? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for raising important issues . We respond to each below . Not enforcing constraint on alphas : We have run the additional experiments suggested by the reviewer . As expected , when no constraint is enforced on the alphas ( this corresponds to setting the Lagrange multipliers to 0 in equation ( 6 ) ) , the performance of VAN networks does indeed improve . These additional experiments have been included in the updated version of the manuscript ( see Figures 2 and 3 as well as Table 2 ) . Initialization of alpha : Our original experiments focused on the choice of alpha=0.5 at initialization . However , the reviewer correctly notes that such an initialization will not necessarily be optimal . Following the suggestion of AnonReview2 we have reformulated the VAN equation as follows : \\mathcal { F } ( x_l ) + ( 1 - \\alpha_l ) x_l And initialize with alpha_l=0 . This ensures that VANs are equivalent to ResNets at initialization ( while this was not previously the case ) . More importantly , such a reparameterization leads to improved performance - an empirical comparison is provided in Appendix A . VANs with skip-connections : the reviewer correctly notes that it is intuitively obvious how to combine VAN residual blocks with architectures that contain skip-connections . One way the two could be combined would be by not enforcing the constraint on \\alpha_l in VANs ( as we have done in response to a previous comment ) . We have removed this sentence from the manuscript ."}, "2": {"review_id": "BJQPG5lR--2", "review_text": "Update (original review below): The authors have addressed several of the reviewers' comments and improved the paper. The motivation has certainly been clarified, but in my opinion it is still hazy. The paper does use skip connections, but the difference is that they are phased out over training. So I think that the motivation behind introducing this specific difference should be clear. Is it to save the additional (small) overhead of using skip connections? Nevertheless, the additional experiments and clarifications are very welcome. For the newly added case of VAN(lambda=0), please note the strong similarity to https://arxiv.org/abs/1611.01260 (ICLR2017 reviews at https://openreview.net/forum?id=Sywh5KYex). In that report \\alpha_l is a scalar instead of a vector. Although it is interesting, the above case case also calls into question the additional value brought by the use of constrained optimization, a main contribution of the paper. In light of the above, I have increased my score since I find this to be an interesting approach, but in my opinion the significance of the results as they stand is low. The paper demonstrates that it is possible to obtain very deep plain networks (without skip connections) with improved performance through the use of constrained optimization that gradually removes skip connections, but the value of this demonstration is unclear because a) consistent improvements over past work or the \\lambda=0 case were not found, and b) The technique still relies on skip connections in a sense so it's not clear that it suggests a truly different method of addressing the degradation problem. Original Review ============= Summary: The contribution of this paper is a method for training deep networks such that skip connections are present at initialization, but gradually removed during training, resulting in a final network without any skip connections. The paper first proposes an approach based on a formulation of deep networks with (non-parameterized, non-gated) skip connections with an equality constraint that effectively removes the skip connections when satisfied. It is proposed to optimize the formulation using the method of Lagrange multipliers. A toy model with a single unit is used to illustrate the basic ideas behind the method. Finally, experimental results for the task of image classification are reported using the MNIST, Fashion-MNIST, and CIFAR datasets. Quality and significance: The proposed methodology is simple and straightforward. The analysis with the toy network is interesting and helps illustrate the method. However, my main concerns with this paper are related to motivation and experiments. The motivation of the work is not clear at all. The stated goal is to address some of the issues related to the role of depth in deep networks, but I think it should be clarified which specific issues in particular are relevant to this method and how they are addressed. One could additionally consider that removing the skip connections at the end of training reduces the computational expense (slightly), but beyond that the expected utility of this investigation is very hazy from the description in the paper. For MNIST and MNIST-Fashion experiments, the motivation is mentioned to be similar to Srivastava et al. (2015), but in that study the corresponding experiment was designed to test if deeper networks could be optimized. Here, the generalization error is measured instead, which is heavily influenced by regularization. Moreover, only some architectures appear to employ batch normalization, which is a potent regularizer. The general difference between plain and non-plain networks is very likely due to optimization difficulties alone, and due to the above issues further comparisons can not be made from the results. For the CIFAR experiments, the experiment design is reasonable for a general comparison. Similar experimental setups have been used in previous papers to report that a proposed method can achieve good results, but there is no doubt that this does not make a rigorous comparison without employing expensive hyper-parameter searches. This is not the fault of the present paper but an unfortunate tradition in the field. Nevertheless, it is important to note that direct comparison should not be made among approaches with key differences. For the reported results, Fitnets and Highway Networks did not use Batch Normalization (which is a powerful regularizer) while VANs and Resnets do. Moreover, it is important to report the training performance of deeper VANs (which have a worse generalization error) to clarify if the VANs suffered difficulties in optimization or generalization. Clarity: The paper is generally well-written and easy to read. There are some clarity issues related to the use of the term \"activation function\" and a typo in an equation but the authors are already aware of these.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for raising three important points , which we respond to below Motivation : We agree with the reviewer that the motivation of the proposed work was unclear in the original submission . As a result , we have updated the manuscript to clearly state out motivation : which is to address the degradation problem in deep feed-forward networks . While ResNets and Highway networks address this issue by introducing skip-connections or gating mechanisms , we look to tackle the problem from the perspective of constrained optimization . As such , the objective of the our work is to propose a new training regime for plain networks which explicitly addresses the issue of performance degradation with depth for plain feed-forward networks . In order to clearly reflect our motivation and contribution we have amended the title of the manuscript as well as clarified the abstract and the introduction . Regarding MNIST and MNIST-Fashion experiments : Batch-normalization was employed across all architectures in the MNIST experiments ( although we do acknowledge that the original Highway network experiments did not use batch norm ) . We have updated the manuscript to clearly state this . As a result , we do believe that comparisons across the different architectures are valid on this experiment . While Srivastava et al . ( 2015 ) reported the training cross-entropy , it is unclear to us why reporting the generalization performance does not serve as an indication of successful optimization across various networks given that all networks employed batch normalization . CIFAR experiments : The reviewer correctly notes that extensive hyper-parameter searches where not run for the CIFAR experiments . This is because the objective of the experiments in Section 4 was not to achieve state-of-the-art performance but rather to demonstrate that VAN networks do not suffer from the degradation problem to the same extent as plain feed-forward networks . We feel that these experiments serve to validate this claim . Regarding comparisons with alternative architectures ( e.g. , FitNets and Highway networks ) we have amended Table 2 to state which architectures did and did not use batch-normalization ."}}