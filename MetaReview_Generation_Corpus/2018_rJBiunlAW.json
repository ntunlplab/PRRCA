{"year": "2018", "forum": "rJBiunlAW", "title": "Training RNNs as Fast as CNNs", "decision": "Reject", "meta_review": "The paper presents Simple Recurrent Unit, which is characterised by the lack of state-to-gates connections as used in conventional recurrent architectures. This allows for efficient implementation, and leads to results competitive with the recurrent baselines, as shown on several benchmarks.\n\nThe submission lacks novelty, as the proposed method is essentially a special case of Quasi-RNN [Bradbury et al.], published at ICLR 2017. The comparison in Appendix A confirms that, as well as similar results of SRU and Quasi-RNN in Figures 4 and 5. Quasi-RNN has already been demonstrated to be amenable to efficient implementation and perform on a par with the recurrent baselines, so this submission doesn\u2019t add much to that.", "reviews": [{"review_id": "rJBiunlAW-0", "review_text": "This work presents the Simple Recurrent Unit architecture which allows more parallelism than the LSTM architecture while maintaining high performance. Significance, Quality and clarity: The idea is well motivated: Faster training is important for rapid experimentation, and altering the RNN cell so it can be paralleled makes sense. The idea is well explained and the experiments convince that the new architecture is indeed much faster yet performs very well. A few constructive comments: - The experiment\u2019s tables alternate between \u201ctime\u201d and \u201cspeed\u201d, It will be good to just have one of them. - Table 4 has time/epoch yet only time is stated", "rating": "7: Good paper, accept", "reply_text": "Thank you for the comments and feedback . We agree that having both \u201c time \u201d and \u201c speed \u201d in the tables are confusing . \u201c Time/epoch \u201d in Table 4 is misleading . We will use \u201c Time per epoch \u201d or simply \u201c Time \u201d instead . We will address your feedback in the next version . Thanks !"}, {"review_id": "rJBiunlAW-1", "review_text": "The authors introduce SRU, the Simple Recurrent Unit that can be used as a substitute for LSTM or GRU cells in RNNs. SRU is much more parallel than the standard LSTM or GRU, so it trains much faster: almost as fast as a convolutional layer with properly optimized CUDA code. Authors perform experiments on numerous tasks showing that SRU performs on par with LSTMs, but the baselines for these tasks are a little problematic (see below). On the positive side, the paper is very clear and well-written, the SRU is a superbly elegant architecture with a fair bit of originality in its structure, and the results show that it could be a significant contribution to the field as it can probably replace LSTMs in most cases but yield fast training. On the negative side, the authors present the results without fully referencing and acknowledging state-of-the-art. Some of this has been pointed out in the comments below already. As another example: Table 5 that presents results for English-German WMT translation only compares to OpenNMT setups with maximum BLEU about 21. But already a long time ago Wu et. al. presented LSTMs reaching 25 BLEU and current SOTA is above 28 with training time much faster than those early models (https://arxiv.org/abs/1706.03762). While the latest are non-RNN architectures, a table like Table 5 should include them too, for a fair presentation. In conclusion: the authors seem to avoid discussing the problem that current non-RNN architectures could be both faster and yield better results on some of the studied problems. That's bad presentation of related work and should be improved in the next versions (at which point this reviewer is willing to revise the score). But in all cases, this is a significant contribution to deep learning and deserves acceptance. Update: the revised version of the paper addresses all my concerns and the comments show new evidence of potential applications, so I'm increasing my score.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for the comments and feedback . == Paper revision == We will include missing SOTA results and related work for translation as pointed by R3 , as we already included for language modeling and speech . We will update the table in the next version . == Clarification on our experiments == The goal of our experiments is not to outperform previous SOTA . Instead , the experiments were designed to study SRU \u2019 s effectiveness on a broad set of realistic applications via fair comparison . Therefore , we emphasized using existing open source implementations for MT and QA . Different implementations ( network architectures , data processing etc . ) have non-trivial impact on the final numbers . To the best of our effort , we aimed to avoid this influencing our experiments . Therefore , in the current version , Tables 1 , 3 , and 5 only compare the results of using LSTM / SRU / Conv2d as building blocks in existing models such DrQA and OpenNMT . We definitely agree that including SOTA models in these tables will improve our presentation . Thank you for the suggestion . == Non-RNN architectures == Thank you for the comment . We will include discussions of non-RNN architectures . Our contribution is orthogonal to recent architectures , such as Transformer ( https : //arxiv.org/abs/1706.03762 ) , which is a novel combination of multi-head attention and feed-forward networks . Part of the motivation behind the Transformer architecture is the computational bottleneck of recurrent architectures . With SRU this is not longer the case . In fact , we observe in the translation model that only 4 minutes are spent per SRU layer , and 96 minutes are spent in the attention+softmax computation . An interesting direction for future work is combining the SRU and Transformer architectures to gain the benefits of both . While this is an important problem , it is beyond the scope of our experiments ."}, {"review_id": "rJBiunlAW-2", "review_text": "The authors propose to drop the recurrent state-to-gates connections from RNNs to speed up the model. The recurrent connections however are core to an RNN. Without them, the RNN defaults simply to a CNN with gated incremental pooling. This results in a somewhat unfortunate naming (simple *recurrent* unit), but most importantly makes a comparison with autoregressive sequence CNNs [ Bytenet (Kalchbrenner et al 2016), Conv Seq2Seq (Dauphin et al, 2017) ] crucial in order to show that gated incremental pooling is beneficial over a simple CNN architecture baseline. In essence, the paper shows that autoregressive CNNs with gated incremental pooling perform comparably to RNNs on a number of tasks while being faster to compute. Since it is already extensively known that autoregressive CNNs and attentional models can achieve this, the *CNN* part of the paper cannot be counted as a novel contribution. What is left is the gated incremental pooling operation; but to show that this operation is beneficial when added to autoregressive CNNs, a thorough comparison with an autoregressive CNN baseline is necessary. Pros: - Fairly well presented - Wide range of experiments, despite underwhelming absolute results Cons: - Quasi-RNNs are almost identical and already have results on small-scale tasks. - Slightly unfortunate naming that does not account for autoregressive CNNs - Lack of comparison with autoregressive CNN baselines, which signals a major conceptual error in the paper. - I would suggest to focus on a small set of tasks and show that the model achieves very good or SOTA performance on them, instead of focussing on many tasks with just relative improvements over the RNN baseline. I recommend showing exhaustively and experimentally that gated incremental pooling can be helpful for autoregressive CNNs on sequence tasks (MT, LM and ASR). I will adjust my score accordingly if the experiments are presented. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the comments and feedback . We respond to the concerns and questions raised in three section . == Recurrent or convolution == We wish to certain aspects pertaining to the distinction between recurrent and convolution architectures as we use in the paper : ( 1 ) SRU only applies simple matrix multiplications ( Wx_t ) for each x_t . This is not a typical convolution operation that is applied over k consecutive tokens . While matrix multiplication can be considered a convolution operation of k=1 , this entails that feed-forward networks ( FFN ) are also a convolutional network . More important , with k=1 there is no convolution over the words , which is the key aim of CNNs for text processing , for example to reason about n-gram patterns . Therefore , while notationaly correct , we consider the k=1 case to empty the term convolution from the meaning it is intended to convey , and do not use it in this way in the paper . That said , we discuss the relationship of these two types of computations in Appendix A , and will be happy to clarify it further in the body of the paper . ( 2 ) This being said , the effectiveness of SRU comes from the recurrent computation of its internal state c [ t ] ( rather than applying conv operations ) . This internal state computation ( referred to in the review as gated incremental pooling ) is commonly used as the key component in gated RNN variants , including LSTM , GRU , RAN , MGU , etc . ( 3 ) Beyond the choice of terms , and even if we were to consider SRU as a special type of CNN ( with k=1 ) , to the best of our knowledge , our study is the first to demonstrate that k=1 suffices to work effectively across a range of NLP and speech tasks . This emphasis on efficiency goes beyond prior work ( e.g.Bytenet , ConvS2S and Quasi-RNN ) , where conv operations of k=3,4 , etc are used throughout the experiments . This allows us to simplify architecture tuning and significantly speeds up the network , which is the main focus of this work . As shown in Figure 2 , SRU operates faster than a single conv operation of k=3 . ( 4 ) Quasi-RNN , T-RNN and T-LSTM ( https : //arxiv.org/pdf/1602.02218.pdf ) have also used \u201c RNN \u201d in naming , despite defaulting to CNN with gated incremental pooling . Broadly speaking , we consider any unit that successively updates state c [ t ] based on current input x [ t ] and the previous vector c [ t-1 ] ( as a function c [ t ] =f ( x [ t ] , c [ t-1 ] ) ) as a recurrent unit . We will clarify this better in the paper . == Quasi-RNN and scale of tasks == We discuss the comparison to Quasi-RNN in Appendix A , and emphasize the critical differences . In our experiments , the training time of a single run on machine translation takes about 2 days , and 4 days on speech on a Titan X GPU . == Wide experiments vs deep experiments == Our experiments are aimed to study SRU \u2019 s effectiveness on a broad set of realistic applications via fair comparison . We discuss this more in our response to Reviewer 3 . Our work focuses on practical simplifications , optimizations , and the applicability of SRU to a wide range of realistic tasks . Although we do not perform an exhaustive hyper-parameter / architecture tuning on each task given space and time constraints , we do see an improvement over deep CNNs on speech recognition . Similar results have been reported in prior work such as RCNN ( Lei et al ; 15,16 ) , KNN ( Lei et al ; 17 ) and Quasi-RNN ( Bradbury et al ; 17 ) , demonstrating that gated pooling is helpful for CNN-type models on tasks such as classification , retrieval , LM etc ."}], "0": {"review_id": "rJBiunlAW-0", "review_text": "This work presents the Simple Recurrent Unit architecture which allows more parallelism than the LSTM architecture while maintaining high performance. Significance, Quality and clarity: The idea is well motivated: Faster training is important for rapid experimentation, and altering the RNN cell so it can be paralleled makes sense. The idea is well explained and the experiments convince that the new architecture is indeed much faster yet performs very well. A few constructive comments: - The experiment\u2019s tables alternate between \u201ctime\u201d and \u201cspeed\u201d, It will be good to just have one of them. - Table 4 has time/epoch yet only time is stated", "rating": "7: Good paper, accept", "reply_text": "Thank you for the comments and feedback . We agree that having both \u201c time \u201d and \u201c speed \u201d in the tables are confusing . \u201c Time/epoch \u201d in Table 4 is misleading . We will use \u201c Time per epoch \u201d or simply \u201c Time \u201d instead . We will address your feedback in the next version . Thanks !"}, "1": {"review_id": "rJBiunlAW-1", "review_text": "The authors introduce SRU, the Simple Recurrent Unit that can be used as a substitute for LSTM or GRU cells in RNNs. SRU is much more parallel than the standard LSTM or GRU, so it trains much faster: almost as fast as a convolutional layer with properly optimized CUDA code. Authors perform experiments on numerous tasks showing that SRU performs on par with LSTMs, but the baselines for these tasks are a little problematic (see below). On the positive side, the paper is very clear and well-written, the SRU is a superbly elegant architecture with a fair bit of originality in its structure, and the results show that it could be a significant contribution to the field as it can probably replace LSTMs in most cases but yield fast training. On the negative side, the authors present the results without fully referencing and acknowledging state-of-the-art. Some of this has been pointed out in the comments below already. As another example: Table 5 that presents results for English-German WMT translation only compares to OpenNMT setups with maximum BLEU about 21. But already a long time ago Wu et. al. presented LSTMs reaching 25 BLEU and current SOTA is above 28 with training time much faster than those early models (https://arxiv.org/abs/1706.03762). While the latest are non-RNN architectures, a table like Table 5 should include them too, for a fair presentation. In conclusion: the authors seem to avoid discussing the problem that current non-RNN architectures could be both faster and yield better results on some of the studied problems. That's bad presentation of related work and should be improved in the next versions (at which point this reviewer is willing to revise the score). But in all cases, this is a significant contribution to deep learning and deserves acceptance. Update: the revised version of the paper addresses all my concerns and the comments show new evidence of potential applications, so I'm increasing my score.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for the comments and feedback . == Paper revision == We will include missing SOTA results and related work for translation as pointed by R3 , as we already included for language modeling and speech . We will update the table in the next version . == Clarification on our experiments == The goal of our experiments is not to outperform previous SOTA . Instead , the experiments were designed to study SRU \u2019 s effectiveness on a broad set of realistic applications via fair comparison . Therefore , we emphasized using existing open source implementations for MT and QA . Different implementations ( network architectures , data processing etc . ) have non-trivial impact on the final numbers . To the best of our effort , we aimed to avoid this influencing our experiments . Therefore , in the current version , Tables 1 , 3 , and 5 only compare the results of using LSTM / SRU / Conv2d as building blocks in existing models such DrQA and OpenNMT . We definitely agree that including SOTA models in these tables will improve our presentation . Thank you for the suggestion . == Non-RNN architectures == Thank you for the comment . We will include discussions of non-RNN architectures . Our contribution is orthogonal to recent architectures , such as Transformer ( https : //arxiv.org/abs/1706.03762 ) , which is a novel combination of multi-head attention and feed-forward networks . Part of the motivation behind the Transformer architecture is the computational bottleneck of recurrent architectures . With SRU this is not longer the case . In fact , we observe in the translation model that only 4 minutes are spent per SRU layer , and 96 minutes are spent in the attention+softmax computation . An interesting direction for future work is combining the SRU and Transformer architectures to gain the benefits of both . While this is an important problem , it is beyond the scope of our experiments ."}, "2": {"review_id": "rJBiunlAW-2", "review_text": "The authors propose to drop the recurrent state-to-gates connections from RNNs to speed up the model. The recurrent connections however are core to an RNN. Without them, the RNN defaults simply to a CNN with gated incremental pooling. This results in a somewhat unfortunate naming (simple *recurrent* unit), but most importantly makes a comparison with autoregressive sequence CNNs [ Bytenet (Kalchbrenner et al 2016), Conv Seq2Seq (Dauphin et al, 2017) ] crucial in order to show that gated incremental pooling is beneficial over a simple CNN architecture baseline. In essence, the paper shows that autoregressive CNNs with gated incremental pooling perform comparably to RNNs on a number of tasks while being faster to compute. Since it is already extensively known that autoregressive CNNs and attentional models can achieve this, the *CNN* part of the paper cannot be counted as a novel contribution. What is left is the gated incremental pooling operation; but to show that this operation is beneficial when added to autoregressive CNNs, a thorough comparison with an autoregressive CNN baseline is necessary. Pros: - Fairly well presented - Wide range of experiments, despite underwhelming absolute results Cons: - Quasi-RNNs are almost identical and already have results on small-scale tasks. - Slightly unfortunate naming that does not account for autoregressive CNNs - Lack of comparison with autoregressive CNN baselines, which signals a major conceptual error in the paper. - I would suggest to focus on a small set of tasks and show that the model achieves very good or SOTA performance on them, instead of focussing on many tasks with just relative improvements over the RNN baseline. I recommend showing exhaustively and experimentally that gated incremental pooling can be helpful for autoregressive CNNs on sequence tasks (MT, LM and ASR). I will adjust my score accordingly if the experiments are presented. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the comments and feedback . We respond to the concerns and questions raised in three section . == Recurrent or convolution == We wish to certain aspects pertaining to the distinction between recurrent and convolution architectures as we use in the paper : ( 1 ) SRU only applies simple matrix multiplications ( Wx_t ) for each x_t . This is not a typical convolution operation that is applied over k consecutive tokens . While matrix multiplication can be considered a convolution operation of k=1 , this entails that feed-forward networks ( FFN ) are also a convolutional network . More important , with k=1 there is no convolution over the words , which is the key aim of CNNs for text processing , for example to reason about n-gram patterns . Therefore , while notationaly correct , we consider the k=1 case to empty the term convolution from the meaning it is intended to convey , and do not use it in this way in the paper . That said , we discuss the relationship of these two types of computations in Appendix A , and will be happy to clarify it further in the body of the paper . ( 2 ) This being said , the effectiveness of SRU comes from the recurrent computation of its internal state c [ t ] ( rather than applying conv operations ) . This internal state computation ( referred to in the review as gated incremental pooling ) is commonly used as the key component in gated RNN variants , including LSTM , GRU , RAN , MGU , etc . ( 3 ) Beyond the choice of terms , and even if we were to consider SRU as a special type of CNN ( with k=1 ) , to the best of our knowledge , our study is the first to demonstrate that k=1 suffices to work effectively across a range of NLP and speech tasks . This emphasis on efficiency goes beyond prior work ( e.g.Bytenet , ConvS2S and Quasi-RNN ) , where conv operations of k=3,4 , etc are used throughout the experiments . This allows us to simplify architecture tuning and significantly speeds up the network , which is the main focus of this work . As shown in Figure 2 , SRU operates faster than a single conv operation of k=3 . ( 4 ) Quasi-RNN , T-RNN and T-LSTM ( https : //arxiv.org/pdf/1602.02218.pdf ) have also used \u201c RNN \u201d in naming , despite defaulting to CNN with gated incremental pooling . Broadly speaking , we consider any unit that successively updates state c [ t ] based on current input x [ t ] and the previous vector c [ t-1 ] ( as a function c [ t ] =f ( x [ t ] , c [ t-1 ] ) ) as a recurrent unit . We will clarify this better in the paper . == Quasi-RNN and scale of tasks == We discuss the comparison to Quasi-RNN in Appendix A , and emphasize the critical differences . In our experiments , the training time of a single run on machine translation takes about 2 days , and 4 days on speech on a Titan X GPU . == Wide experiments vs deep experiments == Our experiments are aimed to study SRU \u2019 s effectiveness on a broad set of realistic applications via fair comparison . We discuss this more in our response to Reviewer 3 . Our work focuses on practical simplifications , optimizations , and the applicability of SRU to a wide range of realistic tasks . Although we do not perform an exhaustive hyper-parameter / architecture tuning on each task given space and time constraints , we do see an improvement over deep CNNs on speech recognition . Similar results have been reported in prior work such as RCNN ( Lei et al ; 15,16 ) , KNN ( Lei et al ; 17 ) and Quasi-RNN ( Bradbury et al ; 17 ) , demonstrating that gated pooling is helpful for CNN-type models on tasks such as classification , retrieval , LM etc ."}}