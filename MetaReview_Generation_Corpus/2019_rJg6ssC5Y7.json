{"year": "2019", "forum": "rJg6ssC5Y7", "title": "DeepOBS: A Deep Learning Optimizer Benchmark Suite", "decision": "Accept (Poster)", "meta_review": "The field of deep learning optimization suffers from a lack of standard benchmarks, and every paper reports results on a different set of models and architectures, likely with different protocols for tuning the baselines. This paper takes the useful step of providing a single benchmark suite for neural net optimizers. \n\nThe set of benchmarks seems well-designed, and covers the range of baselines with a variety of representative architectures. It seems like a useful contribution that will improve the rigor of neural net optimizer evaluation. \n\nOne reviewer had a long back-and-forth with the authors about whether to provide a standard protocol for hyperparameter tuning. I side with the authors on this one: it seems like a bad idea to force a one-size-fits-all protocol here. \n\nAs a lesser point, I'm a little concerned about the strength of some of the baselines. As reviewers point out, some of the baseline results are weaker than typical implementations of those methods. One explanation might be the lack of learning rate schedules, something that's critical to get reasonable performance on some of these tasks. I get that using a fixed learning rate simplifies the grid search protocol, but I'm worried it will hurt the baselines enough that effective learning rate schedules and normalization issues come to dominate the comparisons.\n\nStill, the benchmark suite seems well constructed on the whole, and will probably be useful for evaluation of neural net optimizers. I recommend acceptance.\n\n", "reviews": [{"review_id": "rJg6ssC5Y7-0", "review_text": "This paper presents a new benchmark suite to compare optimizer on deep neural networks. It provides a pipeline to help streamlining the analysis of new optimizers which would favor easily reproducible results and fair comparisons. Quality The paper covers well the problems underlying the construction of such a benchmark, discussing the problems and models selection, runtime estimation, hyper-parameter selection and visualizations. It falls short however in some cases: 1. Hyper-parameter optimization While they mention the importance of hyper-parameter tuning for the benchmark, they leave it to the user to tune them without providing any standard procedure. Furthermore, they use grid search to build the baselines while this is known to be a poor optimizer [1]. 2. Estimated runtime Runtime is estimated for a single set of hyper-parameters of the optimizer, but some optimizer may have similar or roughly similar results for a large set of hyper-parameters that widely affects the runtime. The effect of the hyper-parameters should be taken into account for this part of the benchmark. 3. Interpretation Such a benchmark should makes it easier for interpretation of results as the authors suggests. However, the paper does not convey much interpretation in section 4, beside the fact that results are not conclusive for any baseline. Results of the paper seem low, but they are difficult to verify since the plots are not very precise. For instance Wide ResNet-18-8 reports 1.54% test accuracy on SVHN [6] while this paper reports ~ 15% for the Wide ResNet 18-4 version. Figure 2 is a good attempt at making interpretations of sensitivity of optimizers' hyper-parameters but has limited interpretability compared to what can be found in the literature [2]. 4. Problems There is an effort to provide varied types of problem, including classical optimization functions, image classification, image generation and language modeling. The number of problems consists mostly of image classification however and is very limited for image generation and language modeling. Clarity The paper is well written and easy to understand in general. On a minor note, most figures are difficult to read. Side nodes on figure 1 does not divide clearly without any capital letter or punctuation at the end of sentence. Figure 2 should be self contained with its own legend. Figure 3 is useful for a visual impression of the speed of convergence but a histogram would be necessary for a better visual comparison of the different performances. Section 2.2 has a confusing terminology for the \"train valid set\". Is it a standard validation set? Originality There is virtually no benchmarks for optimizers available for the community. I believe a standardized procedure for comparing optimizers can be viewed as an original contribution. Significance Reproducibility is a problem in machine learning [3, 4] and optimizers' efficiency on deep neural networks generalization performance is still not very well understood [5]. Therefore, there is a strong need for a benchmark for sound comparisons and to favor better reproducibility. Conclusion The benchmark presented in this paper would be an important contribution to the community but lacks a few important features in my opinion, in particular, sound hyper-parameter optimization procedure and sound interpretation tools. On a skeptical note, I doubt the benchmark will be used extensively if the results it provides yield no conclusive interpretation as reported for the baselines. As I feel there is more work needed to support the goals of the paper, I would suggest this paper for a workshop. Nevertheless, I would not be upset if it was accepted because of the importance of the subject and the originality of this work. [1] Bergstra, James, and Yoshua Bengio. \"Random search for hyper-parameter optimization.\" Journal of Machine Learning Research 13, no. Feb (2012): 281-305. [2] Biedenkapp, Andre, Joshua Marben, Marius Lindauer and Frank Hutter. \u201cCAVE : Configuration Assessment , Visualization and Evaluation.\u201d In International Conference on Learning and Intelligent Optimization (2018). [3] Lucic, Mario, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. \u201cAre GANs Created Equal? A Large-Scale Study.\u201d arXiv preprint arXiv:1711.10337 (2017). [4] Melis, G\u00e1bor, Chris Dyer, and Phil Blunsom. \u201cOn the state of the art of evaluation in neural language models.\u201d arXiv preprint arXiv:1707.05589 (2017). [5] Wilson, Ashia C., Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. \"The marginal value of adaptive gradient methods in machine learning.\" In Advances in Neural Information Processing Systems, pp. 4148-4158. 2017. [6] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016 ----------- Revision ----------- In light of the discussion with the authors, the revision made to chapter 4 and in particular the proposed modifications to section 2.4 for a camera-ready paper, I revise my score to 6.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer 2 , thank you very much for your constructive review . We are happy that you agree with us that a benchmarking suite would be an important step . While we acknowledge that the presented solution is not optimal , we would argue that it significantly improves on the current status quo . Just like Reviewer 1 , we worry `` that people will still find minor quibbles with particular choices or tasks in this suite , and therefore continue to use bespoke comparisons '' . We believe that the improvement of DeepOBS compared to the status quo ( which often is to just use MNIST and CIFAR10 and compare to SGD or Adam ) is larger than the step from DeepOBS to where we hope to be . We also want to address the shortcomings you mentioned in your review . 1.We believe that we can split this critique in two aspects . Firstly , the hyper-parameter optimization that we do for our baselines . While , we agree that grid search is not at all an optimal approach , we would argue that it is the method most common in practice ( for example [ 1 , 2 ] ) . The main goal of our baselines are to be a realistic comparison . We plan to include more sophisticated baselines in the future , for example ones that include learning rate decay schedules . We could tune these schedules with more complex methods than grid search , to provide a more challenging competition . The second part is that we do n't provide a hyperparameter tuning method for the user . We did this on purpose . A hypothetical user of DeepOBS might want to highlight that their new optimization method gets good results using default hyperparameter values , while also showing that tuning those parameters a little bit can give you even better results . Therefore , we believe that the choice of hyperparameter tuning method should be left to the user . As long as they document this tuning process , and report the final hyperparameters on each test problem , the results are still comparable even when different tuning methods are used . 2.It is an interesting point you raised here . Indeed we only estimate the runtime for a single set of hyperparameters . However , the used hyperparameters for this estimation is flexible . In the scenario that you describe , the best option for the user of DeepOBS would be to do the estimation step twice for both settings and report both numbers . 3.We will indeed double-check the results of the Wide ResNet on SVHN . In contrast to the original paper , we do not use Nesterov momentum , nor a learning rate decay schedule . We also train for less epochs . The point of the test problems is not to provide state of the art results , but to compare the performances of optimization methods . Nevertheless , we will check our SVHN results and are currently running new experiments . Thanks for pointing this out . 4.While we agree that the set of test problems is a bit biased towards image classification , we also believe that this set is much more exhaustive than what is currently used in practice ( which is often just MNIST and CIFAR10 ) . If there is a specific test problem that you would like us to add , we would gladly do so . We see this set of test problems as a starting point and DeepOBS can be continuously improved and extended . We also tried to address the notes on clarity and changed the figures accordingly . In section 2.2 we mention a `` train eval set '' , which is not a standard validation set . We use this train eval set , whenever we want to evaluate our training performance . We distinguish between using the training data to train , and using the training data to evaluate the performance on it . During this `` training evaluation phase '' , we evaluate on the a set that is as large as the test set and also use the neural network in architecture in `` evaluation mode '' ( for example we do not use dropout ) . This allows for a fairer comparison between test loss and train loss as both are computed in the same way . We hope that by addressing your points we were able to alleviate some of your concerns . You agree with us and the other reviewers that a benchmarking suite for deep learning optimizer would be a significant step and a useful tool for the field and that currently no such tool exists . We kindly ask you to reconsider your evaluation of the paper in light of this response . [ 1 ] Diederik Kingma , and Jimmy Ba . `` Adam : A Method for Stochastic Optimziation '' Proceedings of 3rd International Conference on Learning Representations ( ICLR ) , 2015 . [ 2 ] Tao Lin , Sebastian Stich , and Martin Jaggi . `` Do n't Use Large Mini-Batches , Use Local SGD '' arxiv , 2018 ."}, {"review_id": "rJg6ssC5Y7-1", "review_text": "The authors propose a benchmark for optimization algorithms specific to deep learning called DeepOBS. They provide code to evaluate an optimizer against a suite of standard tasks in deep learning, and provide well tuned baselines for a comparison. The authors discuss important considerations when comparing optimizers, including how to measure speed and tunability of an optimizer, what metric(s) to compare against, and how to deal with stochasticity. A clear, standardized optimization benchmark suite would be very valuable for the field. As the others clearly state in the introduction, there have been many proposed optimization algorithms, but it is hard to compare many of these due to differences in how the optimizers were evaluated in the original papers. In general, people have different requirements for what the expect from an optimizer. However, this paper does a good job of discussing most of the factors that people should consider when choosing or comparing optimizers. Providing a set of well tuned baselines would save people a lot of time in making comparisons with a new optimizer, as well as providing a canonical set of tasks to evaluate against. I particularly appreciated the breadth and diversity of the included tasks. I am a little worried that people will still find minor quibbles with particular choices or tasks in this suite, and therefore continue to use bespoke comparisons, but I think this benchmark would be a valuable resource for the community. Some minor comments: - In section 2.3, there is a recommendation for how to estimate per-iteration cost. I would mention in this section that this procedure is automated and part of the benchmark suite. - I wanted to see how the baselines performed on all of the tasks in the suite (not just on the 8 tasks in the benchmark sets). Perhaps those figures could be included in an appendix. - The authors might want to consider including an automated way of generating performance profiles (https://arxiv.org/abs/cs/0102001) across tasks as part of DeepOBS, as a way of getting a sense of how optimizers performed generally across all tasks.", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 1. thank you very much for your positive review . We want to address the minor comments you raised . - We added a remark in section 2.3 regarding the automated estimation of per-iteration cost in DeepOBS . - With the current setup , computing the baseline performances on all 26 test problems would require more than 3500 runs . As these test problems also include the ImageNet data set , this could take quite a while . We therefore doubt , whether we could finish this in time for ICLR . However , we will add these results to the DeepOBS package as soon as they are finished so that the software package has baselines performances for all test problems . - Thank you for the reference . We will look into performance profiles to see how we can use them ."}, {"review_id": "rJg6ssC5Y7-2", "review_text": "As the paper claims there is no common accept system for benchmarking deep learning optimizer. It is also hard to repeat others' results. The paper describes a benchmarking framework for deep learning optimizer. It proposes three performance indicators, and includes 20 test problems and a core set of benchmarks. Pro: 1) It is a very relevant project. There is a need for unified benchmarking framework. In traditional optimization field, benchmarking is well studied and architectured. See an example at http://plato.asu.edu/bench.html 2) The system is at its early stage, but its design seems complete 3) The paper shows some performance of vanilla SGD, momentum, and Adam Con: 1) It will take tremendous efforts to convince others to join the party and contribute 2) It only support tensorflow right now 3) Writing can be better In Figure 1, make sure the names of components are consistent: either all start with nouns or verbs. The whole picture is not too illustrative. Can switch the order of Figure 2 and Figure 3? In Table 1, the description of ALL-CNN-C has a '?'. Is it intended? Why not explain Table 2? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer 3. thank you for your positive review . We are happy that you agree with us , that benchmarking stochastic optimization methods is a relevant project . We also want to address some of the points you have raised . Cons : 1 ) We agree , that it might take a large effort to convince others to use and contribute to DeepOBS . We designed DeepOBS to be as easy as possible to add new optimization methods . As long as you can implement your new optimizer in TensorFlow , you can add it to DeepOBS by sending us a pull request . We will invest time to run new optimization methods ourselfs , and , provided they give state-of-the-art performance , add them to the baselines . Additonally , benchmarking new optimization methods can take a lot of time , from setting up realistic test problems to computing fair baselines . With DeepOBS , this is unnecessary and researchers can spend more time on developing their optimization methods and less time on thinking about the benchmarking aspect . We hope that this is incentive enough to use DeepOBS . 2 ) We agree , that offering DeepOBS in other frameworks could be beneficial . However , we chose TensorFlow as it is arguably the most popular framework at the moment , and we had to start somewhere . We want to note that the actual software implementation is only a part of this paper . 3 ) If you can point us to some examples of bad writing in the paper , we would be very happy to address and re-write them and improve or clarify the sections . We also addressed the minor points you mentioned : We changed the names in Figure 1 to be more consistent . We hope that the picture is now more informative . In the current version , Figure 2 and 3 are switched now . We fixed the `` ? '' in Table 1 . It was the result of a typo in a citation . Thanks for noting this . We added an explanation for Table 2 . Please note , that by making these changes the paper is now longer than 8 pages . We will work to reduce it to 8 pages again for the final version ."}], "0": {"review_id": "rJg6ssC5Y7-0", "review_text": "This paper presents a new benchmark suite to compare optimizer on deep neural networks. It provides a pipeline to help streamlining the analysis of new optimizers which would favor easily reproducible results and fair comparisons. Quality The paper covers well the problems underlying the construction of such a benchmark, discussing the problems and models selection, runtime estimation, hyper-parameter selection and visualizations. It falls short however in some cases: 1. Hyper-parameter optimization While they mention the importance of hyper-parameter tuning for the benchmark, they leave it to the user to tune them without providing any standard procedure. Furthermore, they use grid search to build the baselines while this is known to be a poor optimizer [1]. 2. Estimated runtime Runtime is estimated for a single set of hyper-parameters of the optimizer, but some optimizer may have similar or roughly similar results for a large set of hyper-parameters that widely affects the runtime. The effect of the hyper-parameters should be taken into account for this part of the benchmark. 3. Interpretation Such a benchmark should makes it easier for interpretation of results as the authors suggests. However, the paper does not convey much interpretation in section 4, beside the fact that results are not conclusive for any baseline. Results of the paper seem low, but they are difficult to verify since the plots are not very precise. For instance Wide ResNet-18-8 reports 1.54% test accuracy on SVHN [6] while this paper reports ~ 15% for the Wide ResNet 18-4 version. Figure 2 is a good attempt at making interpretations of sensitivity of optimizers' hyper-parameters but has limited interpretability compared to what can be found in the literature [2]. 4. Problems There is an effort to provide varied types of problem, including classical optimization functions, image classification, image generation and language modeling. The number of problems consists mostly of image classification however and is very limited for image generation and language modeling. Clarity The paper is well written and easy to understand in general. On a minor note, most figures are difficult to read. Side nodes on figure 1 does not divide clearly without any capital letter or punctuation at the end of sentence. Figure 2 should be self contained with its own legend. Figure 3 is useful for a visual impression of the speed of convergence but a histogram would be necessary for a better visual comparison of the different performances. Section 2.2 has a confusing terminology for the \"train valid set\". Is it a standard validation set? Originality There is virtually no benchmarks for optimizers available for the community. I believe a standardized procedure for comparing optimizers can be viewed as an original contribution. Significance Reproducibility is a problem in machine learning [3, 4] and optimizers' efficiency on deep neural networks generalization performance is still not very well understood [5]. Therefore, there is a strong need for a benchmark for sound comparisons and to favor better reproducibility. Conclusion The benchmark presented in this paper would be an important contribution to the community but lacks a few important features in my opinion, in particular, sound hyper-parameter optimization procedure and sound interpretation tools. On a skeptical note, I doubt the benchmark will be used extensively if the results it provides yield no conclusive interpretation as reported for the baselines. As I feel there is more work needed to support the goals of the paper, I would suggest this paper for a workshop. Nevertheless, I would not be upset if it was accepted because of the importance of the subject and the originality of this work. [1] Bergstra, James, and Yoshua Bengio. \"Random search for hyper-parameter optimization.\" Journal of Machine Learning Research 13, no. Feb (2012): 281-305. [2] Biedenkapp, Andre, Joshua Marben, Marius Lindauer and Frank Hutter. \u201cCAVE : Configuration Assessment , Visualization and Evaluation.\u201d In International Conference on Learning and Intelligent Optimization (2018). [3] Lucic, Mario, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. \u201cAre GANs Created Equal? A Large-Scale Study.\u201d arXiv preprint arXiv:1711.10337 (2017). [4] Melis, G\u00e1bor, Chris Dyer, and Phil Blunsom. \u201cOn the state of the art of evaluation in neural language models.\u201d arXiv preprint arXiv:1707.05589 (2017). [5] Wilson, Ashia C., Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. \"The marginal value of adaptive gradient methods in machine learning.\" In Advances in Neural Information Processing Systems, pp. 4148-4158. 2017. [6] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016 ----------- Revision ----------- In light of the discussion with the authors, the revision made to chapter 4 and in particular the proposed modifications to section 2.4 for a camera-ready paper, I revise my score to 6.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer 2 , thank you very much for your constructive review . We are happy that you agree with us that a benchmarking suite would be an important step . While we acknowledge that the presented solution is not optimal , we would argue that it significantly improves on the current status quo . Just like Reviewer 1 , we worry `` that people will still find minor quibbles with particular choices or tasks in this suite , and therefore continue to use bespoke comparisons '' . We believe that the improvement of DeepOBS compared to the status quo ( which often is to just use MNIST and CIFAR10 and compare to SGD or Adam ) is larger than the step from DeepOBS to where we hope to be . We also want to address the shortcomings you mentioned in your review . 1.We believe that we can split this critique in two aspects . Firstly , the hyper-parameter optimization that we do for our baselines . While , we agree that grid search is not at all an optimal approach , we would argue that it is the method most common in practice ( for example [ 1 , 2 ] ) . The main goal of our baselines are to be a realistic comparison . We plan to include more sophisticated baselines in the future , for example ones that include learning rate decay schedules . We could tune these schedules with more complex methods than grid search , to provide a more challenging competition . The second part is that we do n't provide a hyperparameter tuning method for the user . We did this on purpose . A hypothetical user of DeepOBS might want to highlight that their new optimization method gets good results using default hyperparameter values , while also showing that tuning those parameters a little bit can give you even better results . Therefore , we believe that the choice of hyperparameter tuning method should be left to the user . As long as they document this tuning process , and report the final hyperparameters on each test problem , the results are still comparable even when different tuning methods are used . 2.It is an interesting point you raised here . Indeed we only estimate the runtime for a single set of hyperparameters . However , the used hyperparameters for this estimation is flexible . In the scenario that you describe , the best option for the user of DeepOBS would be to do the estimation step twice for both settings and report both numbers . 3.We will indeed double-check the results of the Wide ResNet on SVHN . In contrast to the original paper , we do not use Nesterov momentum , nor a learning rate decay schedule . We also train for less epochs . The point of the test problems is not to provide state of the art results , but to compare the performances of optimization methods . Nevertheless , we will check our SVHN results and are currently running new experiments . Thanks for pointing this out . 4.While we agree that the set of test problems is a bit biased towards image classification , we also believe that this set is much more exhaustive than what is currently used in practice ( which is often just MNIST and CIFAR10 ) . If there is a specific test problem that you would like us to add , we would gladly do so . We see this set of test problems as a starting point and DeepOBS can be continuously improved and extended . We also tried to address the notes on clarity and changed the figures accordingly . In section 2.2 we mention a `` train eval set '' , which is not a standard validation set . We use this train eval set , whenever we want to evaluate our training performance . We distinguish between using the training data to train , and using the training data to evaluate the performance on it . During this `` training evaluation phase '' , we evaluate on the a set that is as large as the test set and also use the neural network in architecture in `` evaluation mode '' ( for example we do not use dropout ) . This allows for a fairer comparison between test loss and train loss as both are computed in the same way . We hope that by addressing your points we were able to alleviate some of your concerns . You agree with us and the other reviewers that a benchmarking suite for deep learning optimizer would be a significant step and a useful tool for the field and that currently no such tool exists . We kindly ask you to reconsider your evaluation of the paper in light of this response . [ 1 ] Diederik Kingma , and Jimmy Ba . `` Adam : A Method for Stochastic Optimziation '' Proceedings of 3rd International Conference on Learning Representations ( ICLR ) , 2015 . [ 2 ] Tao Lin , Sebastian Stich , and Martin Jaggi . `` Do n't Use Large Mini-Batches , Use Local SGD '' arxiv , 2018 ."}, "1": {"review_id": "rJg6ssC5Y7-1", "review_text": "The authors propose a benchmark for optimization algorithms specific to deep learning called DeepOBS. They provide code to evaluate an optimizer against a suite of standard tasks in deep learning, and provide well tuned baselines for a comparison. The authors discuss important considerations when comparing optimizers, including how to measure speed and tunability of an optimizer, what metric(s) to compare against, and how to deal with stochasticity. A clear, standardized optimization benchmark suite would be very valuable for the field. As the others clearly state in the introduction, there have been many proposed optimization algorithms, but it is hard to compare many of these due to differences in how the optimizers were evaluated in the original papers. In general, people have different requirements for what the expect from an optimizer. However, this paper does a good job of discussing most of the factors that people should consider when choosing or comparing optimizers. Providing a set of well tuned baselines would save people a lot of time in making comparisons with a new optimizer, as well as providing a canonical set of tasks to evaluate against. I particularly appreciated the breadth and diversity of the included tasks. I am a little worried that people will still find minor quibbles with particular choices or tasks in this suite, and therefore continue to use bespoke comparisons, but I think this benchmark would be a valuable resource for the community. Some minor comments: - In section 2.3, there is a recommendation for how to estimate per-iteration cost. I would mention in this section that this procedure is automated and part of the benchmark suite. - I wanted to see how the baselines performed on all of the tasks in the suite (not just on the 8 tasks in the benchmark sets). Perhaps those figures could be included in an appendix. - The authors might want to consider including an automated way of generating performance profiles (https://arxiv.org/abs/cs/0102001) across tasks as part of DeepOBS, as a way of getting a sense of how optimizers performed generally across all tasks.", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 1. thank you very much for your positive review . We want to address the minor comments you raised . - We added a remark in section 2.3 regarding the automated estimation of per-iteration cost in DeepOBS . - With the current setup , computing the baseline performances on all 26 test problems would require more than 3500 runs . As these test problems also include the ImageNet data set , this could take quite a while . We therefore doubt , whether we could finish this in time for ICLR . However , we will add these results to the DeepOBS package as soon as they are finished so that the software package has baselines performances for all test problems . - Thank you for the reference . We will look into performance profiles to see how we can use them ."}, "2": {"review_id": "rJg6ssC5Y7-2", "review_text": "As the paper claims there is no common accept system for benchmarking deep learning optimizer. It is also hard to repeat others' results. The paper describes a benchmarking framework for deep learning optimizer. It proposes three performance indicators, and includes 20 test problems and a core set of benchmarks. Pro: 1) It is a very relevant project. There is a need for unified benchmarking framework. In traditional optimization field, benchmarking is well studied and architectured. See an example at http://plato.asu.edu/bench.html 2) The system is at its early stage, but its design seems complete 3) The paper shows some performance of vanilla SGD, momentum, and Adam Con: 1) It will take tremendous efforts to convince others to join the party and contribute 2) It only support tensorflow right now 3) Writing can be better In Figure 1, make sure the names of components are consistent: either all start with nouns or verbs. The whole picture is not too illustrative. Can switch the order of Figure 2 and Figure 3? In Table 1, the description of ALL-CNN-C has a '?'. Is it intended? Why not explain Table 2? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer 3. thank you for your positive review . We are happy that you agree with us , that benchmarking stochastic optimization methods is a relevant project . We also want to address some of the points you have raised . Cons : 1 ) We agree , that it might take a large effort to convince others to use and contribute to DeepOBS . We designed DeepOBS to be as easy as possible to add new optimization methods . As long as you can implement your new optimizer in TensorFlow , you can add it to DeepOBS by sending us a pull request . We will invest time to run new optimization methods ourselfs , and , provided they give state-of-the-art performance , add them to the baselines . Additonally , benchmarking new optimization methods can take a lot of time , from setting up realistic test problems to computing fair baselines . With DeepOBS , this is unnecessary and researchers can spend more time on developing their optimization methods and less time on thinking about the benchmarking aspect . We hope that this is incentive enough to use DeepOBS . 2 ) We agree , that offering DeepOBS in other frameworks could be beneficial . However , we chose TensorFlow as it is arguably the most popular framework at the moment , and we had to start somewhere . We want to note that the actual software implementation is only a part of this paper . 3 ) If you can point us to some examples of bad writing in the paper , we would be very happy to address and re-write them and improve or clarify the sections . We also addressed the minor points you mentioned : We changed the names in Figure 1 to be more consistent . We hope that the picture is now more informative . In the current version , Figure 2 and 3 are switched now . We fixed the `` ? '' in Table 1 . It was the result of a typo in a citation . Thanks for noting this . We added an explanation for Table 2 . Please note , that by making these changes the paper is now longer than 8 pages . We will work to reduce it to 8 pages again for the final version ."}}