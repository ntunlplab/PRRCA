{"year": "2020", "forum": "Hkl_bCVKDr", "title": "Scaleable input gradient regularization for adversarial robustness", "decision": "Reject", "meta_review": "(1) the authors emphasize the theoretical contribution and claims the bound are tighter. However, they did not directly compare with any certified robust methods, or previous bounds to support the argument. \nHM, not sure, need to check this\n\n(2) The empirical results look suboptimal. The authors did not convince me why they sampled 1000 images for test for a small CIFAR-10 dataset. The proposed method is 10% less robust comparing to Madry's in table 1.  \nSeems ok, understand authors response\n\n1) The theoretical analysis are not terribly new, which is just a straightforward application of first-order Taylor expansion. This idea could be traced back to the very first paper on adversarial examples FGSM (Goodfellow et al 2014).\nTrue\n\n2) The novelty of the paper is to replace exact gradient (w.r.t input) by their finite difference and use it as a regularization. However, there is a misalignment between the theory and the proposed algorithm. The theory only encourages input gradient regularization, regardless to how it is evaluated, and previous studies have shown that this is not a very effective way to improve robustness. According to the experiments, the main empirical improvement comes from the finite difference implementation but the benefit of finite difference is not justified/discussed by the theory. Therefore, the empirical improvement are not supported by the theory. Authors have briefly respond to this issue in the discussion but I believe a more rigorous analysis is needed.  \nThis seems okay based on author response\n\n3) Moreover, the empirical performance does not achieve state-of-the-art result. Indeed, there is a non-negligible  gap (12%) between the obtained performance and some well-known baseline. Thus the empirical contribution is also limited. \nYea, for some cases", "reviews": [{"review_id": "Hkl_bCVKDr-0", "review_text": "This paper proposed new regularizer for training robust models that can defend against evasion attack/adversarial examples. It looks to me there are two major novelties here, the authors suggest that 1) use dual norm on the gradient/jacobian as regularizer is tighter/better than the same norm or simply l2 norm; 2) the gradient/jacobian can be estimated by finite difference when moving the weight with a small step towards the gradient direction. I review the paper with a standard for empirical paper. Please kindly clarify the theoretical contributions if the authors thought the theory is novel and important. I donot think the empirical results are ready for publication. The final objective, when plugin finite difference into the norm regularizer, looks like logits squeezing https://openreview.net/forum?id=BJlr0j0ctX, or logits pairing https://arxiv.org/abs/1803.06373. Both methods are a little bit controversial. There are several issues in the experiments. Several important baselines are missing. The paper did not compare with any of the regularizer-based robust models. When considering efficiency and fast training, the authors also did not compare with recent fast method Shafahi et al. 2019 Free and Zhang et al. 2019 YOPO. When comparing with PGD adversarial training (Madry), in table 1, there is a more than 10% drop on robust accuracy for CIFAR-10 when \\epsilon=8. For l2 norm attack, how to interpret table 2? Why not provide accuracy under norm constraint like table 1? Some uncommon settings are used in the experiments such as ResNeXt-34 and attacking randomly selected 1000 images. Some relatively minor issues, could the authors elaborate on why the optimal value of max_v l( x+v) - c(v) is the squared dual norm of \\grad l? Typo in title: scaleable -> scalable ========= after rebuttal ============= I thank the authors for detailed replies. I still cannot support paper because (1) the authors emphasize the theoretical contribution and claims the bound are tighter. However, they did not directly compare with any certified robust methods, or previous bounds to support the argument. (2) The empirical results look suboptimal. The authors did not convince me why they sampled 1000 images for test for a small CIFAR-10 dataset. The proposed method is 10% less robust comparing to Madry's in table 1. ", "rating": "3: Weak Reject", "reply_text": "Thanks for your review , we appreciate you taking the time to review our paper . We will address your criticisms in order . The work is a theory paper deriving robustness bounds which justify gradient regularization as a method for training adversarially robust models . It is not \u2018 just \u2019 an empirical work ; the empirical results we present in the last two pages backup our theoretical results . We are not claiming that gradient regularization is superior to adversarial training . However our empirical results show that gradient regularization can achieve comparable robustness to adversarial training , at a fraction of the training time of \u2018 traditional \u2019 adversarial training ( in the style of say Madry et al ) . We don \u2019 t see the connection to logit squeezing . We are unaware of a reason why small logits should imply loss gradients are small , or vice versa . Our implementation of gradient regularization with finite differences indeed has some similarities to logit pairing . Logit pairing was proposed with a heuristic justification and lacked theoretical robustness guarantees . Our method is backed theoretically via robustness bounds . Our method could be seen as complimentary to logit pairing , in that for a special choice of the pairing function ( difference of the training loss , which is not discussed in the original logit pairing paper ) , the implementations of logit pairing and finite difference gradient regularization coincide . Our work could then be interpreted as providing some theoretical justification for logit pairing , in certain scernarios . As requested , we have amended our submission to include a comparison with \u2018 For Free \u2019 adversarial training ( Shafahi et al 2019 ) , using the same hyperparameters as in that work . Please see the revised Table 1 . Currently we have updated CIFAR-10 ; ImageNet-1k results will take awhile , and could be included later . We found that ` For Free \u2019 adversarial training was not able to outperform traditional adversarial training , nor did it outperform gradient regularization . It is very fast though ! We did not compare against regularizer-based robust models because they are not the current state-of-the-art . Do you have a particular citation against which we should compare ? Please take a look at the empirical results as a whole : our aim was not to outperform adversarial training in all circumstances . Instead we have proposed a method that is considerably faster , and which provides good robustness that is on the same order as the robustness of adversarial training . In many circumstances we would argue our approach is very desirable , for example when training is limited by time constraints . Regarding Table 2 . Is it that you would prefer results reported at certain distances in L2 ? We chose instead to report the median distance , because ( unlike in L-infinity ) there is no commonly accepted distances to report robustness results at . Until the community agrees on a standard L2 distance for reporting , we think reporting the median distance is a fair metric . We did not want to be accused of cherry-picking our L2 results by selecting arbitrary L2 thresholds . There are many adversarial robustness papers which report results on only a subset of test images . It is very computationally demanding to obtain results on all test images . In our experience we have found ResNeXt outperforms ResNet and Wide-ResNets in terms of both robustness and test accuracy , and is faster to train . It is entirely fair to use newer & better architectures as they are developed . We don \u2019 t believe researchers should be constrained to use a particular architecture for reporting results . We don \u2019 t claim that max_v l ( x+v ) - c ( v ) is the squared dual norm of \\grad l. However it is approximately equivalent ( up to second order ) when c ( v ) is the squared norm and l is differentiable . To see this , Taylor expand l ( x+v ) ; and discard second order terms . Then use Legendre duality to solve the ( approximate ) maximization problem directly . There is a derivation in Amir Beck \u2019 s book \u201c First Order Methods in Optimization \u201d I believe . About the spelling of \u2018 scaleable \u2019 : it depends which side of the Atlantic your grammatical allegiances lie : - ) ... we used British English spelling . Thanks again for your review . We hope that this reply is helpful and provides some additional context to our paper ."}, {"review_id": "Hkl_bCVKDr-1", "review_text": "Summary: This paper provides new understandings on adversarial robustness from the perspective of input gradient regularization. Input gradient regularization hasn't been able to achieve comparable robustness to adversarial training. Built upon existing works, this paper derives two minimum perturbation bounds (L-bound and w-bound) to explain this, as well as other defenses such as Lipschitz regularization and defensive distillation. Taking a step further, this paper proposes to use the finite difference to estimate the input gradients, which not only gives a nice property for reduced modulus of continuity (eg. the w-bound), but also makes the regulation scalable to large networks and datasets. I quite like the theoretical connections derived in this paper. Empirical evidences support their claims, and demonstrate indeed comparable robustness of input gradient regularization to adversarial training. The empirical results can be strengthened by including the normal input gradient regularization baseline (using double backpropagation), at least on cifar-10. This is less likely to change the conclusions, but would be interesting to see the comparisons. Note that, there are already new progresses in adversarial training: [1] Wang, Yisen, et al. \"On the Convergence and Robustness of Adversarial Training.\" ICML, 2019. [2] Zhang, Hongyang, et al. \"Theoretically principled trade-off between robustness and accuracy.\" ICML, 2019. [3] Carmon, Yair, et al. \"Unlabeled data improves adversarial robustness.\" NeurIPS, 2019. [4] Uesato, et al. \"Are Labels Required for Improving Adversarial Robustness?\" NeurIPS, 2019. ========== After rebuttal: Thanks for the new results. My rating remains the same.", "rating": "6: Weak Accept", "reply_text": "Thank you for review , and for the referrals to new results . Following your and Reviewer 2 \u2019 s comments , we have updated the empirical results to compare against double backpropagation ( DBP ) . DBP does offer some improvement in adversarial robustness , although not as significant as with finite differences . We hope that it is clear that the use of finite differences is important for controlling the modulus of continuity , and so keeping ( w ) -bound small . Moreover , on CIFAR-10 at least , finite differences are nearly twice as fast as DBP . Due to computational constraints we have not attempted DBP training on ImageNet-1k ."}, {"review_id": "Hkl_bCVKDr-2", "review_text": "This paper revisits the method of gradient regularization, which regularizes the loss function by adding the norm of the input gradient, aiming to improve adversarial robustness. The standard gradient regularization is implemented with \"double backpropagation\", which could be time consuming for large networks. This paper proposes to replace the exact gradient by its discretization via finite difference, which is computationally more efficient comparing to \"double backpropagation\". Experiments are conducted to show the effectiveness of the method in reducing training time. Overall, the paper is well written but the contribution is very limited and I find some of the experimental comparisons unfair. Thus I do not support publication of the paper. Here are my detailed arguments. 1) The gradient regularization is not novel The main contribution of the paper is to use the finite difference in the gradient regularization in order to improve the training time. The method of gradient regularization is not new, hence the contribution is only the computational effectiveness. This would be fine if one could improve the state-of-the-art method's training time by a lot, but according to the experiments, the performance of adversarial robustness is far from the adversarial training, for example in CIFAR10 epsilon=8/255, there is a 12% drop, which is a huge gap. 2) Unfair comparison in the experiments In table 2, the performance of robustness with respect to the l2 norm is presented. However, in the baseline method of adversarial training, the used attack is in l_{infty} norm. This is unfair since l2 norm and l_{infty} norm has very different characteristics. Moreover, the gradient regularization uses l2 norm instead of l_{1} as in table 1, which makes the comparison unfair. Furthermore, the finite difference improves the training time versus the standard gradient regularization, but it does not imply that the performance will be the same. In particular, it would be better to include the performance of standard gradient regularization in table 1 and 2 as well. 3) Comment on the motivation/theory The theory part is fairly straightforward and it clearly shows that the norm of gradient (w.r.t input) itself is not sufficient to guarantee robustness. As a evidence, even under standard training (for example on MNIST), the gradient norm could be very small, in order of 10^{-4} but still have adversarial example with very small perturbation. Thus, what we also need is to control how fast this gradient changes (Lipschitz constant or w-bound). However, the gradient norm regularization does not take into account how gradient changes. It is claimed that the finite discretization implicitly reduces how the gradient changes, however, I am not convinced by the argument since as long as we take h to be small, it is still a very local measure. An interesting question would be how h affect the performance and it is not discussed in the paper. ", "rating": "3: Weak Reject", "reply_text": "Thanks for taking the time to review our work . We \u2019 ll address each criticism directly . 1 ) Our main contribution is not gradient regularization and its finite difference implementation , taken separately these are both well-established techniques . Our main contribution is the theoretical justification for gradient regularization , and empirical results showing that gradient regularization is indeed effective for adversarial training . Prior to our work there has been much debate as to whether gradient regularization could be effective for robustness ; some had suggested gradient regularization results in gradient obfuscation . We show this is not so when finite differences are used . Moreover finite differences yield significant speed-ups in training time . Our results show gradient regularization yields comparable adversarial robustness to adversarial training , at a fraction at the training time . That is , their respective robustness results are on the same order . Focusing on one number ( L-inf results at \\eps=8/255 ) without considering context is a poor characterization of our work . We are not claiming gradient regularization is a replacement for adversarial training , or that it is better . However we do show empirically that it can achieve quite good results . Given that our implementation is scaleable , practitioners may want to use gradient regularization when training time is a limiting factor . This is an important point . 2 ) There are many works showing that L-infinity adversarial training provides robustness in L2 , and vice versa ( see eg Madry et al \u2019 s oft-cited work ) . We chose to compare against L-infinity trained models , because there is a common set of agreed upon hyper-parameters used in the literature for L-infinity training . Other L2 robustness papers have compared against L-infinity trained models . For example Qian and Wegman ( 2019 , https : //openreview.net/forum ? id=ByxGSsR9FQ ) compare against L-infinity adversarially trained models , even though their results ( which at the time were state-of-the-art ) are for robustness in L2 . Following your comments and that of Reviewer 3 , we have now included comparisons with \u2018 double backprop \u2019 gradient regularization , updated in Tables 2 and 3 of the revised manuscript . 3 ) This is exactly the point we are trying to make : ( w ) -bound must be small , however controlling the estimate of the gradient ( with double backprop ) does not suffice to control the modulus of continuity . In contrast , the modulus of continuity is controlled via finite differences . See Table 3 , which lists modulus of continuity estimates , and is now updated to compare finite differences against double backpropagation . As you can see in the updated Tables 1 & 2 , double backpropagation yields worse adversarial robustness than with finite differences , and is much slower . In other words , the error of the finite difference approximation is quite important for controlling the modulus of continuity . It is important not to make h small . As an aside , since these models are not even differentiable , in the limit as h goes to zero , finite differences do not provide the same gradient estimate as backpropagation . For illustration , take f ( x ) = max ( x,0 ) + min ( x,0 ) . Finite differences yield f \u2019 ( 0 ) =1 , whereas backpropagation yields f \u2019 ( 0 ) = 2 . Finite differences are analytically correct , but backpropagation is not ."}], "0": {"review_id": "Hkl_bCVKDr-0", "review_text": "This paper proposed new regularizer for training robust models that can defend against evasion attack/adversarial examples. It looks to me there are two major novelties here, the authors suggest that 1) use dual norm on the gradient/jacobian as regularizer is tighter/better than the same norm or simply l2 norm; 2) the gradient/jacobian can be estimated by finite difference when moving the weight with a small step towards the gradient direction. I review the paper with a standard for empirical paper. Please kindly clarify the theoretical contributions if the authors thought the theory is novel and important. I donot think the empirical results are ready for publication. The final objective, when plugin finite difference into the norm regularizer, looks like logits squeezing https://openreview.net/forum?id=BJlr0j0ctX, or logits pairing https://arxiv.org/abs/1803.06373. Both methods are a little bit controversial. There are several issues in the experiments. Several important baselines are missing. The paper did not compare with any of the regularizer-based robust models. When considering efficiency and fast training, the authors also did not compare with recent fast method Shafahi et al. 2019 Free and Zhang et al. 2019 YOPO. When comparing with PGD adversarial training (Madry), in table 1, there is a more than 10% drop on robust accuracy for CIFAR-10 when \\epsilon=8. For l2 norm attack, how to interpret table 2? Why not provide accuracy under norm constraint like table 1? Some uncommon settings are used in the experiments such as ResNeXt-34 and attacking randomly selected 1000 images. Some relatively minor issues, could the authors elaborate on why the optimal value of max_v l( x+v) - c(v) is the squared dual norm of \\grad l? Typo in title: scaleable -> scalable ========= after rebuttal ============= I thank the authors for detailed replies. I still cannot support paper because (1) the authors emphasize the theoretical contribution and claims the bound are tighter. However, they did not directly compare with any certified robust methods, or previous bounds to support the argument. (2) The empirical results look suboptimal. The authors did not convince me why they sampled 1000 images for test for a small CIFAR-10 dataset. The proposed method is 10% less robust comparing to Madry's in table 1. ", "rating": "3: Weak Reject", "reply_text": "Thanks for your review , we appreciate you taking the time to review our paper . We will address your criticisms in order . The work is a theory paper deriving robustness bounds which justify gradient regularization as a method for training adversarially robust models . It is not \u2018 just \u2019 an empirical work ; the empirical results we present in the last two pages backup our theoretical results . We are not claiming that gradient regularization is superior to adversarial training . However our empirical results show that gradient regularization can achieve comparable robustness to adversarial training , at a fraction of the training time of \u2018 traditional \u2019 adversarial training ( in the style of say Madry et al ) . We don \u2019 t see the connection to logit squeezing . We are unaware of a reason why small logits should imply loss gradients are small , or vice versa . Our implementation of gradient regularization with finite differences indeed has some similarities to logit pairing . Logit pairing was proposed with a heuristic justification and lacked theoretical robustness guarantees . Our method is backed theoretically via robustness bounds . Our method could be seen as complimentary to logit pairing , in that for a special choice of the pairing function ( difference of the training loss , which is not discussed in the original logit pairing paper ) , the implementations of logit pairing and finite difference gradient regularization coincide . Our work could then be interpreted as providing some theoretical justification for logit pairing , in certain scernarios . As requested , we have amended our submission to include a comparison with \u2018 For Free \u2019 adversarial training ( Shafahi et al 2019 ) , using the same hyperparameters as in that work . Please see the revised Table 1 . Currently we have updated CIFAR-10 ; ImageNet-1k results will take awhile , and could be included later . We found that ` For Free \u2019 adversarial training was not able to outperform traditional adversarial training , nor did it outperform gradient regularization . It is very fast though ! We did not compare against regularizer-based robust models because they are not the current state-of-the-art . Do you have a particular citation against which we should compare ? Please take a look at the empirical results as a whole : our aim was not to outperform adversarial training in all circumstances . Instead we have proposed a method that is considerably faster , and which provides good robustness that is on the same order as the robustness of adversarial training . In many circumstances we would argue our approach is very desirable , for example when training is limited by time constraints . Regarding Table 2 . Is it that you would prefer results reported at certain distances in L2 ? We chose instead to report the median distance , because ( unlike in L-infinity ) there is no commonly accepted distances to report robustness results at . Until the community agrees on a standard L2 distance for reporting , we think reporting the median distance is a fair metric . We did not want to be accused of cherry-picking our L2 results by selecting arbitrary L2 thresholds . There are many adversarial robustness papers which report results on only a subset of test images . It is very computationally demanding to obtain results on all test images . In our experience we have found ResNeXt outperforms ResNet and Wide-ResNets in terms of both robustness and test accuracy , and is faster to train . It is entirely fair to use newer & better architectures as they are developed . We don \u2019 t believe researchers should be constrained to use a particular architecture for reporting results . We don \u2019 t claim that max_v l ( x+v ) - c ( v ) is the squared dual norm of \\grad l. However it is approximately equivalent ( up to second order ) when c ( v ) is the squared norm and l is differentiable . To see this , Taylor expand l ( x+v ) ; and discard second order terms . Then use Legendre duality to solve the ( approximate ) maximization problem directly . There is a derivation in Amir Beck \u2019 s book \u201c First Order Methods in Optimization \u201d I believe . About the spelling of \u2018 scaleable \u2019 : it depends which side of the Atlantic your grammatical allegiances lie : - ) ... we used British English spelling . Thanks again for your review . We hope that this reply is helpful and provides some additional context to our paper ."}, "1": {"review_id": "Hkl_bCVKDr-1", "review_text": "Summary: This paper provides new understandings on adversarial robustness from the perspective of input gradient regularization. Input gradient regularization hasn't been able to achieve comparable robustness to adversarial training. Built upon existing works, this paper derives two minimum perturbation bounds (L-bound and w-bound) to explain this, as well as other defenses such as Lipschitz regularization and defensive distillation. Taking a step further, this paper proposes to use the finite difference to estimate the input gradients, which not only gives a nice property for reduced modulus of continuity (eg. the w-bound), but also makes the regulation scalable to large networks and datasets. I quite like the theoretical connections derived in this paper. Empirical evidences support their claims, and demonstrate indeed comparable robustness of input gradient regularization to adversarial training. The empirical results can be strengthened by including the normal input gradient regularization baseline (using double backpropagation), at least on cifar-10. This is less likely to change the conclusions, but would be interesting to see the comparisons. Note that, there are already new progresses in adversarial training: [1] Wang, Yisen, et al. \"On the Convergence and Robustness of Adversarial Training.\" ICML, 2019. [2] Zhang, Hongyang, et al. \"Theoretically principled trade-off between robustness and accuracy.\" ICML, 2019. [3] Carmon, Yair, et al. \"Unlabeled data improves adversarial robustness.\" NeurIPS, 2019. [4] Uesato, et al. \"Are Labels Required for Improving Adversarial Robustness?\" NeurIPS, 2019. ========== After rebuttal: Thanks for the new results. My rating remains the same.", "rating": "6: Weak Accept", "reply_text": "Thank you for review , and for the referrals to new results . Following your and Reviewer 2 \u2019 s comments , we have updated the empirical results to compare against double backpropagation ( DBP ) . DBP does offer some improvement in adversarial robustness , although not as significant as with finite differences . We hope that it is clear that the use of finite differences is important for controlling the modulus of continuity , and so keeping ( w ) -bound small . Moreover , on CIFAR-10 at least , finite differences are nearly twice as fast as DBP . Due to computational constraints we have not attempted DBP training on ImageNet-1k ."}, "2": {"review_id": "Hkl_bCVKDr-2", "review_text": "This paper revisits the method of gradient regularization, which regularizes the loss function by adding the norm of the input gradient, aiming to improve adversarial robustness. The standard gradient regularization is implemented with \"double backpropagation\", which could be time consuming for large networks. This paper proposes to replace the exact gradient by its discretization via finite difference, which is computationally more efficient comparing to \"double backpropagation\". Experiments are conducted to show the effectiveness of the method in reducing training time. Overall, the paper is well written but the contribution is very limited and I find some of the experimental comparisons unfair. Thus I do not support publication of the paper. Here are my detailed arguments. 1) The gradient regularization is not novel The main contribution of the paper is to use the finite difference in the gradient regularization in order to improve the training time. The method of gradient regularization is not new, hence the contribution is only the computational effectiveness. This would be fine if one could improve the state-of-the-art method's training time by a lot, but according to the experiments, the performance of adversarial robustness is far from the adversarial training, for example in CIFAR10 epsilon=8/255, there is a 12% drop, which is a huge gap. 2) Unfair comparison in the experiments In table 2, the performance of robustness with respect to the l2 norm is presented. However, in the baseline method of adversarial training, the used attack is in l_{infty} norm. This is unfair since l2 norm and l_{infty} norm has very different characteristics. Moreover, the gradient regularization uses l2 norm instead of l_{1} as in table 1, which makes the comparison unfair. Furthermore, the finite difference improves the training time versus the standard gradient regularization, but it does not imply that the performance will be the same. In particular, it would be better to include the performance of standard gradient regularization in table 1 and 2 as well. 3) Comment on the motivation/theory The theory part is fairly straightforward and it clearly shows that the norm of gradient (w.r.t input) itself is not sufficient to guarantee robustness. As a evidence, even under standard training (for example on MNIST), the gradient norm could be very small, in order of 10^{-4} but still have adversarial example with very small perturbation. Thus, what we also need is to control how fast this gradient changes (Lipschitz constant or w-bound). However, the gradient norm regularization does not take into account how gradient changes. It is claimed that the finite discretization implicitly reduces how the gradient changes, however, I am not convinced by the argument since as long as we take h to be small, it is still a very local measure. An interesting question would be how h affect the performance and it is not discussed in the paper. ", "rating": "3: Weak Reject", "reply_text": "Thanks for taking the time to review our work . We \u2019 ll address each criticism directly . 1 ) Our main contribution is not gradient regularization and its finite difference implementation , taken separately these are both well-established techniques . Our main contribution is the theoretical justification for gradient regularization , and empirical results showing that gradient regularization is indeed effective for adversarial training . Prior to our work there has been much debate as to whether gradient regularization could be effective for robustness ; some had suggested gradient regularization results in gradient obfuscation . We show this is not so when finite differences are used . Moreover finite differences yield significant speed-ups in training time . Our results show gradient regularization yields comparable adversarial robustness to adversarial training , at a fraction at the training time . That is , their respective robustness results are on the same order . Focusing on one number ( L-inf results at \\eps=8/255 ) without considering context is a poor characterization of our work . We are not claiming gradient regularization is a replacement for adversarial training , or that it is better . However we do show empirically that it can achieve quite good results . Given that our implementation is scaleable , practitioners may want to use gradient regularization when training time is a limiting factor . This is an important point . 2 ) There are many works showing that L-infinity adversarial training provides robustness in L2 , and vice versa ( see eg Madry et al \u2019 s oft-cited work ) . We chose to compare against L-infinity trained models , because there is a common set of agreed upon hyper-parameters used in the literature for L-infinity training . Other L2 robustness papers have compared against L-infinity trained models . For example Qian and Wegman ( 2019 , https : //openreview.net/forum ? id=ByxGSsR9FQ ) compare against L-infinity adversarially trained models , even though their results ( which at the time were state-of-the-art ) are for robustness in L2 . Following your comments and that of Reviewer 3 , we have now included comparisons with \u2018 double backprop \u2019 gradient regularization , updated in Tables 2 and 3 of the revised manuscript . 3 ) This is exactly the point we are trying to make : ( w ) -bound must be small , however controlling the estimate of the gradient ( with double backprop ) does not suffice to control the modulus of continuity . In contrast , the modulus of continuity is controlled via finite differences . See Table 3 , which lists modulus of continuity estimates , and is now updated to compare finite differences against double backpropagation . As you can see in the updated Tables 1 & 2 , double backpropagation yields worse adversarial robustness than with finite differences , and is much slower . In other words , the error of the finite difference approximation is quite important for controlling the modulus of continuity . It is important not to make h small . As an aside , since these models are not even differentiable , in the limit as h goes to zero , finite differences do not provide the same gradient estimate as backpropagation . For illustration , take f ( x ) = max ( x,0 ) + min ( x,0 ) . Finite differences yield f \u2019 ( 0 ) =1 , whereas backpropagation yields f \u2019 ( 0 ) = 2 . Finite differences are analytically correct , but backpropagation is not ."}}