{"year": "2017", "forum": "HkNEuToge", "title": "Energy-Based Spherical Sparse Coding", "decision": "Reject", "meta_review": "This paper proposes a variant of convolutional sparse coding with unit norm code vectors using cosine distance to evaluate reconstructions. The performance gains over baseline networks are quite minimal and demonstrated on limited datasets, therefore this work fails to demonstrate practical usefulness, while the novelty of the contribution is too slight to stand on its own merit.", "reviews": [{"review_id": "HkNEuToge-0", "review_text": "The paper introduces an efficient variant of sparse coding and uses it as a building block in CNNs for image classification. The coding method incorporates both the input signal reconstruction objective as well as top down information from a class label. The proposed block is evaluated against the recently proposed CReLU activation block. Positives: The proposed method seems technically sound, and it introduces a new way to efficiently train a CNN layer-wise by combining reconstruction and discriminative objectives. Negatives: The performance gain (in terms of classification accuracy) over the previous state-of-the-art is not clear. Using only one dataset (CIFAR-10), the proposed method performs slightly better than the CRelu baseline, but the improvement is quite small (0.5% in the test set). The paper can be strengthened if the authors can demonstrate that the proposed method can be generally applicable to various CNN architectures and datasets with clear and consistent performance gains over strong CNN baselines. Without such results, the practical significance of this work seems unclear. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review . We evaluated our method on a pollen grain classification problem . The dataset contains electron microscopic image scans of the pollen grain surface . There are 10 species of pollens in the dataset , with 2052 training images and 108 testing images per class . The problem can be seen as a texture classification problem . To save a little bit of time , we chopped the network down to 6 blocks from 7 , but the network architecture is otherwise the same as the one used for CIFAR-10 . We see that with this dataset as well , our proposed method outperform the baseline , as we saw with CIFAR-10 . Model Train Err ( % ) Test Err ( % ) CReLU+LC_6 3.51 12.77 CReLU ( SN ) +LC_6 3.48 10.46 SSC+LC_6 2.63 10.27 SSC+EBC_6 1.16 9.58 SSC+EBC_ { 5-6 } 0.86 9.33"}, {"review_id": "HkNEuToge-1", "review_text": "This paper proposes sparse coding problem with cosine-loss and integrated it as a feed-forward layer in a neural network as an energy based learning approach. The bi-directional extension makes the proximal operator equivalent to a certain non-linearity (CReLu, although unnecessary). The experiments do not show significant improvement against baselines. Pros: - Minimizing the cosine-distance seems useful in many settings where compute inner-product between features are required. - The findings that the bidirectional sparse coding is corresponding to a feed-forward net with CReLu non-linearity. Cons: - Unrolling sparse coding inference as a feed-foward network is not new. - The class-wise encoding makes the algorithm unpractical in multi-class cases, due to the requirement of sparse coding net for each class. - It does not show the proposed method could outperform baseslines in real-world tasks.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review . We evaluated our method on a pollen grain classification problem . The dataset contains electron microscopic image scans of the pollen grain surface . There are 10 species of pollens in the dataset , with 2052 training images and 108 testing images per class . The problem can be seen as a texture classification problem . To save a little bit of time , we chopped the network down to 6 blocks from 7 , but the network architecture is otherwise the same as the one used for CIFAR-10 . We see that with this dataset as well , our proposed method outperform the baseline , as we saw with CIFAR-10 . Model Train Err ( % ) Test Err ( % ) CReLU+LC_6 3.51 12.77 CReLU ( SN ) +LC_6 3.48 10.46 SSC+LC_6 2.63 10.27 SSC+EBC_6 1.16 9.58 SSC+EBC_ { 5-6 } 0.86 9.33"}, {"review_id": "HkNEuToge-2", "review_text": " First, I'd like to thank the authors for their answers and clarifications. I find, the presentation of the multi-stage version of the model much clearer now. Pros: + The paper states a sparse coding problem using cosine loss, which allows to solve the problem in a single pass. + The energy-based formulation allows bi-directional coding that incorporates top-down and bottom-up information in the feature extraction process. Cons: + The cost of running the evaluation could be large in the multi-class setting, rendering the approach less attractive and the computational cost comparable to recurrent architectures. + While the model is competitive and improves over the baseline, the paper would be more convincing with other comparisons (see text). The experimental evaluation is limited (a single database and a single baseline) ------ The motivation of the sparse coding scheme is to perform inference in a feed forward manner. This property does not hold in the multi stage setting, thus optimization would be required (as clarified by the authors). Having an efficient way of performing a bi-directional coding scheme is very interesting. As the authors clarified, this could not necessarily be the case, as the model needs to be evaluated many times for performing a classification. Maybe an interesting combination would be to run the model without any class-specific bias, and evaluation only the top K predictions with the energy-based setting. Having said this, it would be good to include a discussion (if not direct comparisons) of the trade-offs of using a model as the one proposed by Cao et al. Eg. computational costs, performance. Using the bidirectional coding only on the top layers seems reasonable: one can get a good low level representation in a class agnostic way. This, however could be studied in more detail, for instance showing empirically the trade offs. If I understand correctly, now only one setting is being reported. Finally, the authors mention that one benefit of using the architecture derived from the proposed coding method is the spherical normalization scheme, which can lead to smoother optimization dynamics. Does the baseline (or model) use batch-normalization? If not, seems relevant to test. Minor comments: I find figure 2 (d) confusing. I would not plot this setting as it does not lead to a function (as the authors state in the text). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your review . Cao et al.proposed a binarized feedback network that propagates semantic information back to the image representation to maximize the target score , which is obtained from an existing pretrained network . The main focus of their work is object localization , and so is not comparable to ours as we focus on object classification . Another reason the work is not comparable is because their feedback model is not tied to the bottom-up model like ours . In our work top-down ( feedback ) and bottom-up are tied mathematically , unlike Cao et al . 's ad-hoc feedback model . In fact , there is no learning involved with their feedback model as they optimize the activations to maximize the target score of interest . We tried evaluating multiple blocks on top of class-agnostic features . In the table below , we show the number of multiplication operations and test error rates when using multiple energy-based classifiers . The number of blocks for the entire network is always fixed at 7 , so if 3 energy-based classifiers are stacked at the very top , then 4 class-agnostic feature extraction blocks are stacked below them . Blocks=2 is SSC+EBC_ { 6-7 } in the paper . Performance at blocks=3 is the same as blocks=2 , but at 53 % more computation . The increased computational cost makes training much more difficult , while the increased network capacity increases the likelihood of the model overfitting the training data ( as seen with blocks=4 ) . Further decrease in test error will likely require more training data . Enery-Based Computation Cost Performance Blocks ( # Multiply Ops ) ( Train Error ) ( Test Error ) 1 647M 0.0085 0.1019 2 713M 0.0021 0.0923 3 1,096M 0.0014 0.0923 4 2,624M 0.0013 0.0954 5 4,153M 6 4,918M 7 6,446M"}], "0": {"review_id": "HkNEuToge-0", "review_text": "The paper introduces an efficient variant of sparse coding and uses it as a building block in CNNs for image classification. The coding method incorporates both the input signal reconstruction objective as well as top down information from a class label. The proposed block is evaluated against the recently proposed CReLU activation block. Positives: The proposed method seems technically sound, and it introduces a new way to efficiently train a CNN layer-wise by combining reconstruction and discriminative objectives. Negatives: The performance gain (in terms of classification accuracy) over the previous state-of-the-art is not clear. Using only one dataset (CIFAR-10), the proposed method performs slightly better than the CRelu baseline, but the improvement is quite small (0.5% in the test set). The paper can be strengthened if the authors can demonstrate that the proposed method can be generally applicable to various CNN architectures and datasets with clear and consistent performance gains over strong CNN baselines. Without such results, the practical significance of this work seems unclear. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review . We evaluated our method on a pollen grain classification problem . The dataset contains electron microscopic image scans of the pollen grain surface . There are 10 species of pollens in the dataset , with 2052 training images and 108 testing images per class . The problem can be seen as a texture classification problem . To save a little bit of time , we chopped the network down to 6 blocks from 7 , but the network architecture is otherwise the same as the one used for CIFAR-10 . We see that with this dataset as well , our proposed method outperform the baseline , as we saw with CIFAR-10 . Model Train Err ( % ) Test Err ( % ) CReLU+LC_6 3.51 12.77 CReLU ( SN ) +LC_6 3.48 10.46 SSC+LC_6 2.63 10.27 SSC+EBC_6 1.16 9.58 SSC+EBC_ { 5-6 } 0.86 9.33"}, "1": {"review_id": "HkNEuToge-1", "review_text": "This paper proposes sparse coding problem with cosine-loss and integrated it as a feed-forward layer in a neural network as an energy based learning approach. The bi-directional extension makes the proximal operator equivalent to a certain non-linearity (CReLu, although unnecessary). The experiments do not show significant improvement against baselines. Pros: - Minimizing the cosine-distance seems useful in many settings where compute inner-product between features are required. - The findings that the bidirectional sparse coding is corresponding to a feed-forward net with CReLu non-linearity. Cons: - Unrolling sparse coding inference as a feed-foward network is not new. - The class-wise encoding makes the algorithm unpractical in multi-class cases, due to the requirement of sparse coding net for each class. - It does not show the proposed method could outperform baseslines in real-world tasks.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your review . We evaluated our method on a pollen grain classification problem . The dataset contains electron microscopic image scans of the pollen grain surface . There are 10 species of pollens in the dataset , with 2052 training images and 108 testing images per class . The problem can be seen as a texture classification problem . To save a little bit of time , we chopped the network down to 6 blocks from 7 , but the network architecture is otherwise the same as the one used for CIFAR-10 . We see that with this dataset as well , our proposed method outperform the baseline , as we saw with CIFAR-10 . Model Train Err ( % ) Test Err ( % ) CReLU+LC_6 3.51 12.77 CReLU ( SN ) +LC_6 3.48 10.46 SSC+LC_6 2.63 10.27 SSC+EBC_6 1.16 9.58 SSC+EBC_ { 5-6 } 0.86 9.33"}, "2": {"review_id": "HkNEuToge-2", "review_text": " First, I'd like to thank the authors for their answers and clarifications. I find, the presentation of the multi-stage version of the model much clearer now. Pros: + The paper states a sparse coding problem using cosine loss, which allows to solve the problem in a single pass. + The energy-based formulation allows bi-directional coding that incorporates top-down and bottom-up information in the feature extraction process. Cons: + The cost of running the evaluation could be large in the multi-class setting, rendering the approach less attractive and the computational cost comparable to recurrent architectures. + While the model is competitive and improves over the baseline, the paper would be more convincing with other comparisons (see text). The experimental evaluation is limited (a single database and a single baseline) ------ The motivation of the sparse coding scheme is to perform inference in a feed forward manner. This property does not hold in the multi stage setting, thus optimization would be required (as clarified by the authors). Having an efficient way of performing a bi-directional coding scheme is very interesting. As the authors clarified, this could not necessarily be the case, as the model needs to be evaluated many times for performing a classification. Maybe an interesting combination would be to run the model without any class-specific bias, and evaluation only the top K predictions with the energy-based setting. Having said this, it would be good to include a discussion (if not direct comparisons) of the trade-offs of using a model as the one proposed by Cao et al. Eg. computational costs, performance. Using the bidirectional coding only on the top layers seems reasonable: one can get a good low level representation in a class agnostic way. This, however could be studied in more detail, for instance showing empirically the trade offs. If I understand correctly, now only one setting is being reported. Finally, the authors mention that one benefit of using the architecture derived from the proposed coding method is the spherical normalization scheme, which can lead to smoother optimization dynamics. Does the baseline (or model) use batch-normalization? If not, seems relevant to test. Minor comments: I find figure 2 (d) confusing. I would not plot this setting as it does not lead to a function (as the authors state in the text). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your review . Cao et al.proposed a binarized feedback network that propagates semantic information back to the image representation to maximize the target score , which is obtained from an existing pretrained network . The main focus of their work is object localization , and so is not comparable to ours as we focus on object classification . Another reason the work is not comparable is because their feedback model is not tied to the bottom-up model like ours . In our work top-down ( feedback ) and bottom-up are tied mathematically , unlike Cao et al . 's ad-hoc feedback model . In fact , there is no learning involved with their feedback model as they optimize the activations to maximize the target score of interest . We tried evaluating multiple blocks on top of class-agnostic features . In the table below , we show the number of multiplication operations and test error rates when using multiple energy-based classifiers . The number of blocks for the entire network is always fixed at 7 , so if 3 energy-based classifiers are stacked at the very top , then 4 class-agnostic feature extraction blocks are stacked below them . Blocks=2 is SSC+EBC_ { 6-7 } in the paper . Performance at blocks=3 is the same as blocks=2 , but at 53 % more computation . The increased computational cost makes training much more difficult , while the increased network capacity increases the likelihood of the model overfitting the training data ( as seen with blocks=4 ) . Further decrease in test error will likely require more training data . Enery-Based Computation Cost Performance Blocks ( # Multiply Ops ) ( Train Error ) ( Test Error ) 1 647M 0.0085 0.1019 2 713M 0.0021 0.0923 3 1,096M 0.0014 0.0923 4 2,624M 0.0013 0.0954 5 4,153M 6 4,918M 7 6,446M"}}