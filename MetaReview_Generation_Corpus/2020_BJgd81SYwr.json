{"year": "2020", "forum": "BJgd81SYwr", "title": "Meta Dropout: Learning to Perturb Latent Features for Generalization", "decision": "Accept (Poster)", "meta_review": "This paper proposes a type of adaptive dropout to regularize gradient based meta-learning models. The reviewers found the idea interesting and it is supported by improvements on standard benchmarks. The authors addressed several concerns of the reviewers during the rebutal phase. In particular, revisions added results against other regularization mthods. We recommend that further attention is given to ablations, in particular the baseline proposed by Reviewer 1.", "reviews": [{"review_id": "BJgd81SYwr-0", "review_text": "The authors propose to meta-learn, using MAML, the mean of an elementwise, input-dependent, multiplicative noise to improve generalization in few-shot learning. The motivation is that meta-learning the noise allows to learn how to best perturb examples in order to improve generlization. This claim is supported by ample experimental evidence and comparisons against many baselines, as well as additional ablation studies w.r.t design choices of the algorithm itself. The paper is well written and easy to read. Consequently, I think this is a nice paper and should be accepted. Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model? Edit: Thank you for your response. I will leave my score as is. I would strongly encourage the authors to incorporate the baseline \"(1)\" as proposed by R3 in a future version of the paper as I agree with them that this is a relevant baseline.", "rating": "8: Accept", "reply_text": "We sincerely appreciate your constructive comments . We respond to your main concerns below : 1 . The Deterministic Meta-Dropout performs better than baseline MAML - maybe it 's an effect of a larger number of parameters in the model ? - To demonstrate that strong generalization performance of Meta-Dropout is not the effect of using larger number of model parameters , we doubled the number of channels for the base model and report its performances ( MAML ( x2 ) ) . Models # param . Omni-1shot Omni-5shot mimg-1shot mimg-5shot MAML x1 95.23+-0.17 98.38+-0.07 49.58+-0.65 64.55+-0.52 MAML ( x2 ) x4 94.96+-0.16 98.36+-0.08 48.19+-0.64 65.84+-0.52 Meta-SGD x2 96.16+-0.14 98.54+-0.07 48.30+-0.64 65.55+-0.56 Meta-dropout x2 96.63+-0.13 98.73+-0.06 51.93+-0.67 67.42+-0.52 The number of parameters of MAML ( chx2 ) is four times of that of MAML , while Meta-dropout is only doubled . Nonetheless , MAML ( chx2 ) does not improve on MAML , demonstrating that the effectiveness of meta-dropout does not simply come from using larger number of parameters . Meta-SGD also doubles the number of parameters in the base MAML model , but is significantly outperformed by Meta-dropout . We want to emphasize that Deterministic meta-dropout is also one of our models , and that its good performance does not hurt our claim on the effectiveness of the multiplicative noise . This is because meta-dropout consists of two parts : meta-learned deterministic multiplicative perturbation and random noise . Thus the deterministic meta-dropout still \u201c learns to perturb \u201d , although not random , and is actually a core component of meta-dropout ( See Table 3 in the revision ) . Please also see our response to the Reviewer # 3 , comment # 4 ."}, {"review_id": "BJgd81SYwr-1", "review_text": "The paper proposes learning to add input-dependent noise to improve the generalization of MAML-style meta-learning algorithm. The proposed method is evaluated on OmniGlot and miniImageNet. The paper reports improvements upon MAML, MAML with meta-learned parameter-wise learning rates, as well as a few regularization methods that are based on input/hidden state perturbations (Mixup, Variational Information Bottleneck). An ablation study also compares the proposed meta-dropout algorithm with a number of modifications, such as a fixed noise, input-independent noise, etc. It is furthermore shown that meta-dropout somewhat improves the model\u2019s robustness against an adversarial attack. The paper is somewhat incremental considering that Li et al, (2017) and Balaji et al, (2018) have already proposed meta-learning parameter-wise learning rates and parameter-wise regularization coefficient respectively. One difference from the methods above is that in the proposed method noise is controlled by the input. The ablation however shows that in 5-shot classification case simply adding non-trainable noise works quite well. It seems like the choice of the particular method for adding the noise was performed using the test set. If it\u2019s true, this is methodologically wrong: model selection should be performed on a development set (or meta-development) set. Futhermore, Table 2 contains some results named \u201cAdd.\u201d, which I guess stands for additive noise. I did not find an explanation of what is the specific method for adding noise used in this case. Such additive noise is also missing from ablation experiments. Overall, it seems that paper falls short of clearly proving that back-propagating through MAML to the noise parameters is helpful. The \u201cDeterministic Meta-Dropout\u201d performs better than baseline MAML, and arguably, meaning that some part of the improvement upon MAML can be due to the architectural differences and not due to noise. \u201cIndependent Gaussian\u201d and \u201cWeight Gaussian\u201d baselines perform worse than non-trainable noise (\u201cFixed Gaussian\u201d). Learning the variance for the noise is shown to be detrimental. There is just too much confusion in the results, the improvements are not very robust. The paper writing is okay, but there are serious issues. I am not sure I understand the argument in Section 3.2 that meta-dropout performs variational inference. It seems like Equation 7 is wrong because y_i is missing from the second argument of the KL divergence term. The transition to Equation 8 is therefore also wrong, and as far as I can understand, the whole argument breaks down. Line 7 in Algorithm 1 in Appendix A (which by the way should really be in the main text) does not make sense. Other issues: - the second sentence of the abstract is not implied by the first, the usage of \u201cthus\u201d does not seem appropriate - the intro should probably mention L1 and L2 regularization as well - in Section 3.1 there is a forward reference to Equation 5, makes understanding the text quite hard - \u201cmeta-droput\u201d, \u201crobustenss\u201d: typos in many places - Figure 4 visualization is not clear. - the architectural change required to add noise is not explained in the paper (i.e. what is \\phi and how it\u2019s used) - no comparison to meta-learned L1 regularization - a baseline is missing in which \\phi is treated as a part of \\theta and trained with vanilla MAML ", "rating": "3: Weak Reject", "reply_text": "5.It seems like Equation 7 is wrong because y_i is missing from the second argument of the KL divergence term . The transition to Equation 8 is therefore also wrong , and as far as I can understand , the whole argument breaks down . - This is a critical misunderstanding . Equation 7 is correct . In Equation 7 , the second argument p ( z|x ) is called a \u201c conditional prior \u201d , which does not observe the label y since p ( z|x ) is part of the generative process . See the Equation 4 of Sohn et al . [ 1 ] which is exactly the same as the Equation 7 of our paper . It seems that you are confused with the following decomposition of log evidence : Log evidence = ELBO + $ KL [ q ( z|x , y ) \\|p ( z|x , y ) ] $ , ( a ) ELBO = E [ Log-likelihood ] - $ KL [ q ( z|x , y ) \\|p ( z|x ) ] $ ( b ) The log evidence term is decomposed of ELBO and the KL term involving $ p ( z|x , y ) $ in Eq . ( a ) .However , what Eq.7 in the main paper describes is the actual ELBO expression in Eq . ( b ) , which contains the KL divergence between the approximate posterior and the prior $ p ( z|x ) $ . Reference : [ 1 ] Sohn et al. , Learning Structured Output Representation using Deep Conditional Generative Models , In NIPS , 2015 . 6.Line 7 in Algorithm 1 in Appendix A ( which by the way should really be in the main text ) does not make sense . The second sentence of the abstract is not implied by the first , the usage of \u201c thus \u201d does not seem appropriate . The intro should probably mention L1 and L2 regularization as well . \u201c meta-droput \u201d , \u201c robustenss \u201d : typos in many places . - Thank you for pointing them out . We updated the revision based on your suggestions . 7.Figure 4 visualization is not clear . - Figure 4 visualizes the perturbations generated by our Meta-Dropout and the decision boundary for classification . Could you provide us more specific comments on which part of Figure 4 is not clear ? 8.The architectural change required to add noise is not explained in the paper ( i.e.what is \\phi and how it \u2019 s used ) . - The architectural change and \\phi are clearly explained in Figure 2 and the paragraph \u201c form of the noise \u201d at the end of page 4 . 9.Additional baselines - We conduct experiments on additional baselines as requested : ( 1 ) MetaReg : MAML + L1 regularization ( 2 ) MAML ( R3 ) : \\phi treated as \\theta , as R3 requested Models Omni-1shot Omni-5shot mimg-1shot mimg-5shot MAML 95.23+-0.17 98.38+-0.07 49.58+-0.65 64.55+-0.52 MetaReg ( Ours ) 95.28+-0.15 98.85+-0.06 49.76+-0.67 65.42+-0.53 MAML ( R3 ) 96.15+-0.15 98.69+-0.07 42.08+-0.63 62.82+-0.53 Meta-dropout 96.63+-0.13 98.73+-0.06 51.93+-0.67 67.42+-0.52 As shown , all the suggested baseline models largely underperform Meta-Dropout ."}, {"review_id": "BJgd81SYwr-2", "review_text": "This paper proposes meta dropout, which leverages adaptive dropout training for regularizing gradient based meta learning models, e.g., MAML and MetaSGD. Experiments on few shot learning show that meta dropout achieves better performance. Overally, I think this paper is well motivated and experiments on few shot learning are impressive. I have only two major concerns. 1. Sec 3.2. According to my understanding, Meta dropout introduces a learnable prior for latent $z$, but the training objective does not require posterior inference and thus no variational inference is needed. I think it is ok to say that meta dropout tries to optimize a lower bound of log p(Y|X;\\theta,\\phi^*), but meta dropout does not regularize the variational framework because there is no variational inference framework. 2. Experiments on adversarial robustness can be further improved. (1) the settings and the analysis of adversarial robustness experiment can be discussed in details. For example, how to build \"adversarial learning baseline\" in meta learning settings and why the result implies the perturbation directions for generalization and robustness relates to each other; (2) how other regularization methods (e.g., Mixup, VIB and Information dropout) perform on adversarial robustness? Does Meta dropout performs better than them? (3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading. I suggest trying some other STOA attack methods (e.g., iterative methods). Some typos: Page 3, Regularization methods, 3rd line, \"\"wwwdiscuss Page 7, 2nd line from the bottom, FSGM->FGSM ", "rating": "6: Weak Accept", "reply_text": "We really appreciate your constructive comments . We respond to each comment as follows . 1.Meta dropout does not regularize the variational framework because there is no variational inference framework . - Thank you for your comment . We agree with you that the current lower bound is not a variational form due to the assumption of q=p . In Section 3.2 , we toned down the original expression \u201c Learning to regularize variational inference \u201c into \u201c Connection to variational inference \u201d , and corrected the corresponding sentences . Still , there exists a clear connection between standard variational inference and our learning framework . Thus we believe that discussion in Section 3.2 will be helpful to readers who want to understand the meaning of learning objective Eq . ( 2 ) in depth . 2.Improving adversarial robustness experiment . - Thank you for the helpful suggestion . During the rebuttal period , we conducted additional experiments on adversarial robustness as you suggested : a ) We replaced the previous FGSM attack with stronger PGD attack ( 200 iter . ) , with $ L_1 $ , $ L_2 $ , and $ L_\\infty $ norm constraints . b ) We included more baselines ( e.g.Mixup , VIB , and information dropout ) , and show that our meta-dropout largely and consistently outperforms all of them . c ) We added more detailed descriptions of the adversarial meta-learning baseline and in-depth analysis on the results . d ) We further show that the learned perturbation from our Meta-dropout also generalize across different types of adversarial attacks with $ L_1 $ , $ L_2 $ , and $ L_\\infty $ attacks . The generalization to different types of attacks is an important problem in adversarial learning , and most existing models fail to achieve this goal . Please see the corresponding section in the revision . We believe that the adversarial robustness part of our paper has become much stronger than before , thanks to your suggestion ."}], "0": {"review_id": "BJgd81SYwr-0", "review_text": "The authors propose to meta-learn, using MAML, the mean of an elementwise, input-dependent, multiplicative noise to improve generalization in few-shot learning. The motivation is that meta-learning the noise allows to learn how to best perturb examples in order to improve generlization. This claim is supported by ample experimental evidence and comparisons against many baselines, as well as additional ablation studies w.r.t design choices of the algorithm itself. The paper is well written and easy to read. Consequently, I think this is a nice paper and should be accepted. Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model? Edit: Thank you for your response. I will leave my score as is. I would strongly encourage the authors to incorporate the baseline \"(1)\" as proposed by R3 in a future version of the paper as I agree with them that this is a relevant baseline.", "rating": "8: Accept", "reply_text": "We sincerely appreciate your constructive comments . We respond to your main concerns below : 1 . The Deterministic Meta-Dropout performs better than baseline MAML - maybe it 's an effect of a larger number of parameters in the model ? - To demonstrate that strong generalization performance of Meta-Dropout is not the effect of using larger number of model parameters , we doubled the number of channels for the base model and report its performances ( MAML ( x2 ) ) . Models # param . Omni-1shot Omni-5shot mimg-1shot mimg-5shot MAML x1 95.23+-0.17 98.38+-0.07 49.58+-0.65 64.55+-0.52 MAML ( x2 ) x4 94.96+-0.16 98.36+-0.08 48.19+-0.64 65.84+-0.52 Meta-SGD x2 96.16+-0.14 98.54+-0.07 48.30+-0.64 65.55+-0.56 Meta-dropout x2 96.63+-0.13 98.73+-0.06 51.93+-0.67 67.42+-0.52 The number of parameters of MAML ( chx2 ) is four times of that of MAML , while Meta-dropout is only doubled . Nonetheless , MAML ( chx2 ) does not improve on MAML , demonstrating that the effectiveness of meta-dropout does not simply come from using larger number of parameters . Meta-SGD also doubles the number of parameters in the base MAML model , but is significantly outperformed by Meta-dropout . We want to emphasize that Deterministic meta-dropout is also one of our models , and that its good performance does not hurt our claim on the effectiveness of the multiplicative noise . This is because meta-dropout consists of two parts : meta-learned deterministic multiplicative perturbation and random noise . Thus the deterministic meta-dropout still \u201c learns to perturb \u201d , although not random , and is actually a core component of meta-dropout ( See Table 3 in the revision ) . Please also see our response to the Reviewer # 3 , comment # 4 ."}, "1": {"review_id": "BJgd81SYwr-1", "review_text": "The paper proposes learning to add input-dependent noise to improve the generalization of MAML-style meta-learning algorithm. The proposed method is evaluated on OmniGlot and miniImageNet. The paper reports improvements upon MAML, MAML with meta-learned parameter-wise learning rates, as well as a few regularization methods that are based on input/hidden state perturbations (Mixup, Variational Information Bottleneck). An ablation study also compares the proposed meta-dropout algorithm with a number of modifications, such as a fixed noise, input-independent noise, etc. It is furthermore shown that meta-dropout somewhat improves the model\u2019s robustness against an adversarial attack. The paper is somewhat incremental considering that Li et al, (2017) and Balaji et al, (2018) have already proposed meta-learning parameter-wise learning rates and parameter-wise regularization coefficient respectively. One difference from the methods above is that in the proposed method noise is controlled by the input. The ablation however shows that in 5-shot classification case simply adding non-trainable noise works quite well. It seems like the choice of the particular method for adding the noise was performed using the test set. If it\u2019s true, this is methodologically wrong: model selection should be performed on a development set (or meta-development) set. Futhermore, Table 2 contains some results named \u201cAdd.\u201d, which I guess stands for additive noise. I did not find an explanation of what is the specific method for adding noise used in this case. Such additive noise is also missing from ablation experiments. Overall, it seems that paper falls short of clearly proving that back-propagating through MAML to the noise parameters is helpful. The \u201cDeterministic Meta-Dropout\u201d performs better than baseline MAML, and arguably, meaning that some part of the improvement upon MAML can be due to the architectural differences and not due to noise. \u201cIndependent Gaussian\u201d and \u201cWeight Gaussian\u201d baselines perform worse than non-trainable noise (\u201cFixed Gaussian\u201d). Learning the variance for the noise is shown to be detrimental. There is just too much confusion in the results, the improvements are not very robust. The paper writing is okay, but there are serious issues. I am not sure I understand the argument in Section 3.2 that meta-dropout performs variational inference. It seems like Equation 7 is wrong because y_i is missing from the second argument of the KL divergence term. The transition to Equation 8 is therefore also wrong, and as far as I can understand, the whole argument breaks down. Line 7 in Algorithm 1 in Appendix A (which by the way should really be in the main text) does not make sense. Other issues: - the second sentence of the abstract is not implied by the first, the usage of \u201cthus\u201d does not seem appropriate - the intro should probably mention L1 and L2 regularization as well - in Section 3.1 there is a forward reference to Equation 5, makes understanding the text quite hard - \u201cmeta-droput\u201d, \u201crobustenss\u201d: typos in many places - Figure 4 visualization is not clear. - the architectural change required to add noise is not explained in the paper (i.e. what is \\phi and how it\u2019s used) - no comparison to meta-learned L1 regularization - a baseline is missing in which \\phi is treated as a part of \\theta and trained with vanilla MAML ", "rating": "3: Weak Reject", "reply_text": "5.It seems like Equation 7 is wrong because y_i is missing from the second argument of the KL divergence term . The transition to Equation 8 is therefore also wrong , and as far as I can understand , the whole argument breaks down . - This is a critical misunderstanding . Equation 7 is correct . In Equation 7 , the second argument p ( z|x ) is called a \u201c conditional prior \u201d , which does not observe the label y since p ( z|x ) is part of the generative process . See the Equation 4 of Sohn et al . [ 1 ] which is exactly the same as the Equation 7 of our paper . It seems that you are confused with the following decomposition of log evidence : Log evidence = ELBO + $ KL [ q ( z|x , y ) \\|p ( z|x , y ) ] $ , ( a ) ELBO = E [ Log-likelihood ] - $ KL [ q ( z|x , y ) \\|p ( z|x ) ] $ ( b ) The log evidence term is decomposed of ELBO and the KL term involving $ p ( z|x , y ) $ in Eq . ( a ) .However , what Eq.7 in the main paper describes is the actual ELBO expression in Eq . ( b ) , which contains the KL divergence between the approximate posterior and the prior $ p ( z|x ) $ . Reference : [ 1 ] Sohn et al. , Learning Structured Output Representation using Deep Conditional Generative Models , In NIPS , 2015 . 6.Line 7 in Algorithm 1 in Appendix A ( which by the way should really be in the main text ) does not make sense . The second sentence of the abstract is not implied by the first , the usage of \u201c thus \u201d does not seem appropriate . The intro should probably mention L1 and L2 regularization as well . \u201c meta-droput \u201d , \u201c robustenss \u201d : typos in many places . - Thank you for pointing them out . We updated the revision based on your suggestions . 7.Figure 4 visualization is not clear . - Figure 4 visualizes the perturbations generated by our Meta-Dropout and the decision boundary for classification . Could you provide us more specific comments on which part of Figure 4 is not clear ? 8.The architectural change required to add noise is not explained in the paper ( i.e.what is \\phi and how it \u2019 s used ) . - The architectural change and \\phi are clearly explained in Figure 2 and the paragraph \u201c form of the noise \u201d at the end of page 4 . 9.Additional baselines - We conduct experiments on additional baselines as requested : ( 1 ) MetaReg : MAML + L1 regularization ( 2 ) MAML ( R3 ) : \\phi treated as \\theta , as R3 requested Models Omni-1shot Omni-5shot mimg-1shot mimg-5shot MAML 95.23+-0.17 98.38+-0.07 49.58+-0.65 64.55+-0.52 MetaReg ( Ours ) 95.28+-0.15 98.85+-0.06 49.76+-0.67 65.42+-0.53 MAML ( R3 ) 96.15+-0.15 98.69+-0.07 42.08+-0.63 62.82+-0.53 Meta-dropout 96.63+-0.13 98.73+-0.06 51.93+-0.67 67.42+-0.52 As shown , all the suggested baseline models largely underperform Meta-Dropout ."}, "2": {"review_id": "BJgd81SYwr-2", "review_text": "This paper proposes meta dropout, which leverages adaptive dropout training for regularizing gradient based meta learning models, e.g., MAML and MetaSGD. Experiments on few shot learning show that meta dropout achieves better performance. Overally, I think this paper is well motivated and experiments on few shot learning are impressive. I have only two major concerns. 1. Sec 3.2. According to my understanding, Meta dropout introduces a learnable prior for latent $z$, but the training objective does not require posterior inference and thus no variational inference is needed. I think it is ok to say that meta dropout tries to optimize a lower bound of log p(Y|X;\\theta,\\phi^*), but meta dropout does not regularize the variational framework because there is no variational inference framework. 2. Experiments on adversarial robustness can be further improved. (1) the settings and the analysis of adversarial robustness experiment can be discussed in details. For example, how to build \"adversarial learning baseline\" in meta learning settings and why the result implies the perturbation directions for generalization and robustness relates to each other; (2) how other regularization methods (e.g., Mixup, VIB and Information dropout) perform on adversarial robustness? Does Meta dropout performs better than them? (3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading. I suggest trying some other STOA attack methods (e.g., iterative methods). Some typos: Page 3, Regularization methods, 3rd line, \"\"wwwdiscuss Page 7, 2nd line from the bottom, FSGM->FGSM ", "rating": "6: Weak Accept", "reply_text": "We really appreciate your constructive comments . We respond to each comment as follows . 1.Meta dropout does not regularize the variational framework because there is no variational inference framework . - Thank you for your comment . We agree with you that the current lower bound is not a variational form due to the assumption of q=p . In Section 3.2 , we toned down the original expression \u201c Learning to regularize variational inference \u201c into \u201c Connection to variational inference \u201d , and corrected the corresponding sentences . Still , there exists a clear connection between standard variational inference and our learning framework . Thus we believe that discussion in Section 3.2 will be helpful to readers who want to understand the meaning of learning objective Eq . ( 2 ) in depth . 2.Improving adversarial robustness experiment . - Thank you for the helpful suggestion . During the rebuttal period , we conducted additional experiments on adversarial robustness as you suggested : a ) We replaced the previous FGSM attack with stronger PGD attack ( 200 iter . ) , with $ L_1 $ , $ L_2 $ , and $ L_\\infty $ norm constraints . b ) We included more baselines ( e.g.Mixup , VIB , and information dropout ) , and show that our meta-dropout largely and consistently outperforms all of them . c ) We added more detailed descriptions of the adversarial meta-learning baseline and in-depth analysis on the results . d ) We further show that the learned perturbation from our Meta-dropout also generalize across different types of adversarial attacks with $ L_1 $ , $ L_2 $ , and $ L_\\infty $ attacks . The generalization to different types of attacks is an important problem in adversarial learning , and most existing models fail to achieve this goal . Please see the corresponding section in the revision . We believe that the adversarial robustness part of our paper has become much stronger than before , thanks to your suggestion ."}}