{"year": "2021", "forum": "ETBc_MIMgoX", "title": "Learning with AMIGo: Adversarially Motivated Intrinsic Goals", "decision": "Accept (Poster)", "meta_review": "This paper was reviewed by four experts in the field. Based on the reviewers' feedback, the decision is to recommend the paper for acceptance to ICLR 2021. The reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. The authors are encouraged to make the necessary changes and include the missing references.", "reviews": [{"review_id": "ETBc_MIMgoX-0", "review_text": "The authors introduce AMIGo , an approach to curiosity in which an adversarial `` teacher '' agent proposes goals that the `` student '' agent attempts to achieve . The student obtains an intrinsic reward of +1 when it achieves the goal , and this augments the extrinsic reward additively . The teacher proposes a goal at the start of each episode , and whenever the student reaches a goal . The teacher is positively rewarded if the student achieves the goal after some threshold t * , and negatively rewarded otherwise . t * is incremented at fixed intervals . They then demonstrate that this outperforms a variety of suitable baselines . The algorithm proposed has great strength in its simplicity . It does bear a considerable similarity to Sukhbaatar et al 2017 ( their ASP baseline ) , but AMIGo appears to be more flexible , and they put forth evidence that it considerably outperforms ASP on their benchmarks . AMIGo is something that could be adapted very easily to many other settings , and it is a quite agnostic framework . The experimental results do show considerable advantages over baselines . I am , however , concerned about how the evaluations were done . At the very least , I find that the main text fails to describe what seem to be important caveats that can only be found in the appendix . In particular : -For baselines , suitable hyperparameters were found using a subset of tasks ( tasks thought to be easier ) , and , if positive results were achieved , the same hyperparameters were run on the rest of the tasks . If scores of zero were obtained for the initial subset , the baselines were not run at all and were assumed to score zero . -As noted in the main text , some of the baselines were developed for partially observed environments with an egocentric view , so they let the baselines run in both modes ( with algorithms with multiple modules , they varied this independently for both ) . While this might seem like simply giving the baselines more chances to succeed , there is a problem when this is coupled with the first practice of assuming zero scores . Some of these baselines obtained no score in the fully observed setting on the `` easy '' task subset , and hence were not run on the hard environments . They did better in partially observed modes , and hence obtained their hard environment scores in these partially observed modes . These partially observed modes are potentially much harder ! While one might be able to make some claim of the form `` if they ca n't do the easy task , they ca n't do the hard , '' I am worried that these choices , used together , really obfuscate the performance differential . Note the quite high score variance that AMIGo achieves on the harder environments . Given such high variance , models which were simply not run in the fully observed setting could have actually performed better were they given the chance ( and also perhaps needed further hyperparameter tuning ) . At the very least , I think this is a really unclear way of benchmarking things and should be corrected so that we do not have these statistical worries . As a result , I can not at this time recommend acceptance , though I am certainly open to being persuaded that my worries are unfounded . I could have missed something about their experimental methods , or environmental particulars could somehow make the above treatments perfectly reasonable . A more minor concern : I do wonder about the extent to which this method applies in other settings . Is this time-threshold reward method applicable to a broad array of settings , or is this particularly useful in minigrid and similar sorts of maze navigations ? Are there situations in which time-to-complete is not a good proxy for difficulty ? Is it important that the goals be represented in some very compact way , as they are for these experiments ? These are questions that would , I think , need additional experiments . The authors did a considerable amount of evaluation , so I hesitate to simply ask for more , but I do wonder if this approach is particularly suited to a narrow range of environments . I am also curious as to why the ASP baseline did so poorly . Its method seems to be quite similar , so more commentary about what makes one fail badly while the other succeeds would be really useful ! * Updated score based on below discussion * * Updated score again based on below discussion *", "rating": "7: Good paper, accept", "reply_text": "Hello , and thank you for your detailed review . It sounds like you like the paper and the method , especially regarding the potential to apply it to a wider range of RL settings . We agree , and would be significantly empowered to pursue further research in this area if we can convince you the paper proves the concept and is worthy of publication ! # # # Experimental Protocol Our understanding is that the main thing preventing you from wholeheartedly supporting acceptance is the concerns you outline about the experimental method . We aim to address those during this discussion , both through clarification and through running the outlying experiments you wish us to verify . We hope that this will not only address your concerns , but give you the comfort you need to fully support the paper with your score . We believe we performed an extremely thorough experimental analysis and comparison of AMIGo against comparable benchmarks , ( actually ) running 85 experiments ( each with 5 seeds ) , each of which requires non-trivial computational resources ( 1-2 dedicated GPU days per experimental run , per seed ! ) . Available resources barely permitted this , and because individual experimental runs showed that no method would solve harder experiments if flatlining ( constant zero return ) on easier tasks , it seemed fair and intuitive to avoid running methods on harder tasks if they can not train * at all * on easier tasks . We were as transparent as we could be about this when reporting results , and hope you will understand this was not an attempt at obfuscation . However , for the avoidance of doubt , * * we ~will~ have run all the remaining experiments projected to flatline , following the same experimental protocol used for the reported experiments , to confirm our projections * * . We confirm that our results stand in [ this post ] ( https : //openreview.net/forum ? id=ETBc_MIMgoX & noteId=4dnHW4XDd_M ) . Experiment code will be open-sourced so the community can check our results for maximum transparency . # # # Choice of Threshold Cost The reviewer astutely suggests that alternatives to the number of steps could be used as a cost , for the purpose of training the teacher . We do emphasise in Section 6.2 that there are several options available for measuring policy performance in this context , and that step count was a simple choice for the purpose of our experiments . In more complex settings , other measurable costs might be suitable , for example power consumption during the operation of a robotic arm in a continuous control setting . You are correct that to properly evaluate the general framework in more general settings , further experiments are required . However , to prove the concept for this method , the MiniGrid task suite offered sufficient complexity and range of difficulty and has been a popular benchmark in the related literature , e.g.in [ Igl et al. \u2019 s ( NeurIPS 2019 ) paper on SNI ] ( https : //arxiv.org/abs/1910.12911 ) , [ Loynd et al. \u2019 s ( ICML 2020 ) paper on WMG ] ( https : //proceedings.icml.cc/paper/2020/hash/5cf21ce30208cfffaa832c6e44bb567d-Abstract.html ) , or [ Raileanu and Rockt\u00e4schel \u2019 s ( ICLR 2020 ) paper on RIDE ] ( https : //iclr.cc/virtual_2020/poster_rkg-TJBFPB.html ) . We hope you will agree that the extensive experiments done here at least show the robustness of our method within a difficult class of exploration problems , and that future work seeking to adapt it to different contexts warrants further research , to be incorporated into future papers . # # # ASP Baseline We used the authors ' own [ PyTorch implementation of ASP ] ( https : //github.com/tesatory/hsp ) , and did a thorough hyperparameter sweep , so we are confident the empirical results are a fair comparison . We are unsure how to explain this result , and are not certain it is our place to proffer negative analysis of others \u2019 work , but we should clarify that ASP is not that similar to AMIGo . ASP requires two policies acting , and can only propose goals which have been reached by one of them . Furthermore , ASP requires reversible and resettable environments , which is a pretty strong assumption that AMIGo does not make . We hope these crucial differences go some way towards giving you an intuition as to why there is a difference in performance , and wider potential for application . We are happy to add something about this in the final paper if you think it essential , as there is space for it with the additional page , but we initially decided not to focus on this too much to avoid the impression of talking down other researcher \u2019 s methods , as the numbers spoke for themselves . # # # Summary We hope the clarifications above , paired with the experimental confirmation of our projections for the harder environments ( which we expect to provide sometime next week ) , are sufficient for you to reconsider your assessment of the paper . If you have any outstanding concerns , please don \u2019 t hesitate to let us know so that we can discuss them , and properly understand what\u2014if anything\u2014stands between us and a strong recommendation for acceptance ."}, {"review_id": "ETBc_MIMgoX-1", "review_text": "# # After Author Response The author response for most of the points of confusion and weaker sections of the draft are satisfactory . I thank the authors for incorporating some of the feedback during this discussion . I am still on the fence about the generality of this idea , and how much the teacher objective is specific to the minigrid tasks . But overall I find this idea interesting and believe it adds value to the field . # # Summary of the paper : This paper proposes a goal- '' generating '' module ( called the teacher ) that will propose goals in procedurally generated grid world domains . The way they train this teacher is through policy gradient updates , with the teacher getting positive rewards if the agent achieves the provided goal but takes longer than a threshold to reach it , and negative rewards if the agent is unable to reach the goal or reaches it too quickly . The threshold is gradually increased based on the agent 's capabilities , allowing for more and more difficult goals . Both the agent and the teacher additionally get rewarded for solving the external task . So the teacher is directed to provide goals that will also solve the given task , and the agent also learns to not just solve the given goal . Other auxiliary objectives for the teacher are entropy regularization for diverse goals , and episode boundary awareness . # # Strengths : + This paper presents a clear reward based mechanism to train the teacher . It also presents a reasonable answer to the question of how a difficulty threshold should be set for the teacher . It provides evidence that this goal-generation approach helps in directed exploration , is useful for general RL tasks ( beyond goal-based RL ) , and speeds up solving of a variety of hard grid world environments . + Figure 3 shows the changing thresholds and the subsequently generated targets for the agent , which is a good experiment to show that this increasing threshold mechanism works as intended . + The ablations in the appendix are illustrative and show the different considerations and how different components of the system interact . # # Weak Points : - The suggestion that the goal generation is adversarial seems slightly strong , with not much evidence to show that the generated goals are adversarial to the agent . The goals could also be generated progressively farther and farther due to the moving threshold . - The way the problem is set up and communicated makes it seem oriented towards grid-world problems exclusively . There is no definition of what a goal is . In fact , the problem setup itself is communicated rather vaguely , with terms being introduced as required rather than the entire setup being introduced clearly up front . Setting up the problem more clearly , with respect to RL literature ( maybe refer to the Sutton and Barto book or alternative setups ) , being clear about what a goal is and how that goal is being used in the context of the grid world would make the paper clearer to read and understand . - It is also unclear whether the teacher rewards are discounted according to the number of steps taken by the agent , or if it operates at a lower temporal frequency . The entire teacher policy training regimen needs more explanation and clarity . - Given that the goal-generation here is to give an ( x , y ) location to travel to , it is unclear why an approach like GoalGAN can not be adapted to this domain . - An additional question to ask would be whether the approach of Zhang et al can be adapted as a baseline to compare to this work . I can understand if it is not applicable , since that approach is specific to goal-conditioned RL , but would like some clarity on it . - When explaining the type of goals , the authors mention that this `` includes picking up keys , opening doors and dropping objects onto empty tiles '' . How is an agent supposed to know whether it is supposed to move on to a square or drop an object there . The teacher does not seem to communicate what kind of goal is being generated . If the kind of goal is not communicated , would n't the agent do the simplest thing possible , and just move over the square , instead of figuring out that it should have dropped an object at the location ? - The auxiliary task of `` Episode Boundary Awareness '' is a little unclear . My understanding is that the teacher is rewarded for selecting goals where the object type at that location changes if the episode changes . An illustrative example or clearer language is necessary to understand this additional objective and how it is helping the agent . This is very important as it seems that this auxiliary reward is essential in getting the teacher to learn goals that are useful for eventual agent success ( as evidenced by the ablations in Table 6 ) . In fact , I am curious as to why the performance of the system if this particular reward is removed drops drastically and is similar to the extrinsic reward being removed . - The main weakness of the approach for me is that it is unclear how this system scales to other RL problems . Since there is no definition of what a goal is considered to be , and no other examples given except grid worlds , it is hard to see what a broad application of this idea would be . - The learning curves ( Figure 4 ) seem to show that the training with AMIGo is pretty high variance , compared to the baselines . Is there an explanation for this variance ? Example , performance on OMmedium drops to 0 pretty regularly . # # Additional Comments : The related work section needs some additional work . While the authors present a broad array of related work , there are some suggestions to round off this section . For intrinsic motivation , the authors should include work by Andy Barto ( `` Intrinsic Motivation and Reinforcement Learning '' , 2013 ) and other related work , as well as the optimal rewards framework by Satinder Singh 's group ( `` Where do rewards come from ? '' ) along with work by Jonathan Sorg ( ICML 2010 ) and more recently work by Zeyu Zheng ( `` On Learning Intrinsic Rewards for Policy Gradient Methods '' ) . These views of intrinsic motivation as a vehicle for more than exploration is something that is not very clear in the current draft , even though the authors do present some alternative work like empowerment . For curriculum learning , there are recent surveys that on curriculum learning for RL ( `` Curriculum Learning for Reinforcement Learning Domains : A Framework and Survey '' , Narvekar et al . ; `` Automatic Curriculum Learning For Deep RL : A Short Survey '' , Portelas et al . ) that should be referred to when explaining curriculum generation in the context of RL . # # Conclusion Overall I would like to see clearer communication of some of the ideas as well as some explanation from the authors that will give me confidence that this idea is more broadly applicable .", "rating": "7: Good paper, accept", "reply_text": "We appreciate your constructive comments and detailed review which will help us to improve and clarify the paper . We will respond to your points and will improve our communication in the paper with the hope we can convince you our idea is broadly applicable to the point where you would consider improving your assessment . # # # On the adversarial nature of AMIGo The loss function of the generator is partially adversarial by construction : the teacher is rewarded for proposing goals which are difficult for the student policy . We further qualify this as being \u201c constructively adversarial \u201d , because such goals must also be achievable , and thus not too hard . Empirically , learning trajectories ( e.g.Figure 4 ) often show phases where the loss of the teacher decreases while that of the policy increases , demonstrating some notion of opposition in their respective objectives . Ultimately , you are right that the equilibrium condition is for the teacher to converge on proposing extrinsic goals once they are achievable , so in this sense it ceases to be adversarial at the end of training , but again , this is what motivates our terminology regarding \u201c constructively adversarial \u201d , which is used throughout the paper . We hope you agree with this characterization . # # # Clarification regarding Goals AMIGo is applicable to a wide range of goal types , although whether it works or not is dependent on the implementation and modelling choices , and is thus the subject of further experiments . We intentionally underspecify the notion of goal at play to encourage such exploration . However , in our experimental setting , used to prove the concept for this general learning process , a goal is formally defined as a change in the observation on a tile , as specified by an x , y coordinate . We will add that statement up front along with a better clarification . We want to emphasize that AMIGo is applicable beyond grid worlds , although these are a good setting to prove the concept of this research direction , in line with many papers in the related literature . Specifically , the MiniGrid task suite was used because it offers sufficient complexity and range of difficulty and has been a popular benchmark in the related literature , e.g.in [ Igl et al. \u2019 s ( NeurIPS 2019 ) paper on SNI ] ( https : //arxiv.org/abs/1910.12911 ) , [ Loynd et al. \u2019 s ( ICML 2020 ) paper on WMG ] ( https : //proceedings.icml.cc/paper/2020/hash/5cf21ce30208cfffaa832c6e44bb567d-Abstract.html ) , or [ Raileanu and Rockt\u00e4schel \u2019 s ( ICLR 2020 ) paper on RIDE ] ( https : //iclr.cc/virtual_2020/poster_rkg-TJBFPB.html ) . Going beyond grid worlds , e.g. , by first running semantic segmentation pipelines in photorealistic 3D environments and then defining intrinsic reward procedures around changes to detected objects would be an interesting research avenue , but outside of the scope of the paper . The overall idea and algorithm behind AMIGo is not grid world specific \u2014 only our particular choice of a goal proposal distribution is . We agree our choice of words was perhaps a bit unprecise when we say `` includes picking up keys , opening doors and dropping objects onto empty tiles '' . We will formally introduce and clarify that those are among the different available actions for the agent to achieve a goal , namely a change in the ( x , y ) coordinate . In some cases in our setting , moving over a square is not the simplest thing possible , such as when an obstacle can be removed or a door opened . Similarly , in other tasks and environments it could be easier to affect a cell by throwing something at it than by reaching it . In our view , this is one way in which our goal definition can be achieved more generally . We hope these clarifications can convince you our setup is not oriented towards grid world problems exclusively . # # # Clarification regarding the Teacher Training The teacher \u2019 s rewards are not discounted according to the number of steps taken by the agent . More specifically , the teacher receives reward after several steps depending on whether the agent reached a goal , so it does operate at a different temporal frequency . We will further clarify this on section 3.2 : We did experiment with a fixed threshold , and found that the loss function alone was sufficient to induce harder goals and curriculum learning . We omitted to mention this due to lack of space and because fixed thresholds have to be carefully fixed for each environment ( depending on its size and complexity ) , while the single moving heuristic is general and applies to all environments . We will explain this more clearly in the updated draft . Episode Boundary Awareness : Your understanding is correct and we will describe what we mean with an example . Observed Higher Variance in some Environments : We think the observed higher variance is due to the fact the learning dynamics of the two modules are coupled . Similar remarks have been made about GANs which can be difficult to train . In OMmedium , we did observe that the training stabilized after 27M steps for all runs ."}, {"review_id": "ETBc_MIMgoX-2", "review_text": "This paper introduces a teacher agent in reinforcement learning framework . The teacher 's responsible for setting goal for the student agent ( the `` main '' agent ) . The teacher acts in adversarial way ( but also somewhat cooperative ) in setting the goal . That is , the goal should be achievable by the student , yet the goal should not be too simple that the student stops learning towards harder goals , and eventually achieve the original goal from the environment . Experiments show proposed method is able to learn okay performance for hard environments , even though most other methods fail to score at all . Pros : I think this is a very interesting approach and has great potential . The idea is natural and experiments make a lot of sense . This should partially credit to clear writing and examples . The authors provide only one set of results in main paper , but have more ablation studies and variants of reward forms . Those results will be helpful to provide more insight into how well the agent handles the task . Cons : My major concern with proposed method is that it seems limited to particular applications , such as grid world . As the authors also pointed out , even proposing goals in this paper 's setting is a challenging problem . For now , I do n't see an automatic way to proposing goals for other reinforcement learning tasks , such as Atari games and Go . In those tasks , I feel teacher 's goals may need to be designed incorporating human 's understanding of the tasks ( i.e.capture a stone in Go ) . This makes me feel the proposed method is not very generalizable , but I am open to change my opinion if authors provide a good answer . If for those tasks , a human-generated set of goals are needed by teacher agent , then how do we correctly attribute the performance gain we saw in this paper ? That is , are the gains over baseline methods from 1 ) we have a better training framework , such as curriculum learning , or 2 ) the goals are set in a way that incorporate our understanding of the environment ? My second concern is about scalability of proposed method . For some applications , it may be unnecessary to have a teacher that sets intermediate goals ( e.g.AlphaGo - > AlphaZero ) . How does sample efficiency for AMIGo compared to baselines ? I saw results related to number of steps for AMIGo alone , but not baseline methods . Conceptually if a teacher sets many intermediate goals , it 's likely that the student will train more number of steps and possible to achieve better performance . However if this procedure requires 10x more number of steps to achieve on-par performance for easy/medium level tasks , then it will be important to acknowledge that . When will the student need a teacher vs. w/o a teacher seems also worthy studying .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their comments . It seems that the two main concerns the reviewer has have to do with the applicability of AMIGo to different settings or with different goal types , and the scalability and sample efficiency of the method . We answer both of these concerns below . # # # Limitations of AMIGo to Grid Worlds , and other Goal Forms The review is concerned that the choice of intrinsic goal specifications limits the applicability of AMIGo to grid worlds . We should be clear about the aim and claim of this paper : it is to demonstrate that this general \u201c constructively adversarial \u201d form of teacher-student training works as an exploration mechanism . We are transparent about the general requirements of this method throughout section 3 : namely that there exists a form of intrinsic goal the student policy can condition on , that there exists a ( learned or hard-coded ) valuation function over these goals , and that there exists a measure of the student \u2019 s performance . These requirements do not restrict us to grid-worlds or any specific environment , but particular choices of goals and performance measures might . To prove the concept , we have focussed on a particular choice of goals ( coordinates in a grid world , which gives us a valuation function for free ) and performance ( steps ) , in order to have an in-depth comparison with other exploration methods on a well-known exploration problem , MiniGrid . The MiniGrid task suite offers sufficient complexity and range of difficulty and has been a popular benchmark in the related literature , e.g.in [ Igl et al. \u2019 s ( NeurIPS 2019 ) paper on SNI ] ( https : //arxiv.org/abs/1910.12911 ) , [ Loynd et al. \u2019 s ( ICML 2020 ) paper on WMG ] ( https : //proceedings.icml.cc/paper/2020/hash/5cf21ce30208cfffaa832c6e44bb567d-Abstract.html ) , or [ Raileanu and Rockt\u00e4schel \u2019 s ( ICLR 2020 ) paper on RIDE ] ( https : //iclr.cc/virtual_2020/poster_rkg-TJBFPB.html ) . We can not and do not claim that this method will work out-of-the-box in other domains , with other goal types , but explicitly leave this for further work , as pointed out at the end of Section 5 . Nevertheless , we do not see a substantial difference between many tasks such as Atari and our setting in terms of requiring deeper human understanding of the environment . Many MDPs discount rewards based on the number of steps , and often difficulty can be correlated with the amount of effort or time needed to obtain a certain outcome , this could be sufficient for the teacher to learn to propose appropriately difficult goals . However , we agree with you that investigating other settings would be interesting , and present particular challenges , such as depending upon human knowledge to model the goal valuation function . There is prior work in this area incorporating aspects of inverse reinforcement learning , e.g . [ Bahdanau _et al._ ( ICLR 2019 ) ] ( https : //openreview.net/forum ? id=H1xsSjC9Ym ) where a language-conditional goal-verifier is learned jointly with a policy , based on a finite set of expert examples of goal states , all done on pixel input . But it is clear that this is ambitious and non-trivial research that must be the subject of further papers , rather than trying to cram all possible work and evaluation in this one ( which has no fewer than 85 experiments ) . We hope you will agree this paper makes a significant enough contribution to both warrant publication , and enable such follow up work to be done . # # # Scalability and sample efficiency The reviewer will be happy to hear that their concerns about scalability and sample efficiency are already addressed in the submitted version of the paper , namely in Appendix C , as pointed to from Section 4.4 . In this appendix , we show that on the two easiest environments , * * KCmedium * * and * * OMmedium * * , agents need about 10 million steps to converge while on the other four more challenging environments , they need an order of 100 million steps to learn the tasks , showcasing AMIGo 's contributions not just to solving the exploration problem , but also to improving the sample complexity of agent training . Naturally , for the harder environments , sample efficiency is not comparable since AMIGo is the only method obtaining non-zero returns . We hope this addresses your second main concern , and are open to hearing how we could make things clearer in the paper . # # # Summary We believe that both the response above , and the existing plots and discussion from Appendix C , sufficiently address your concerns to the point where you \u2019 ll consider revising your assessment in support of the paper . To repeat our main point , the paper proves the concept for a new class of exploration methods , and thoroughly compares it to existing state of the art . We are confident that it does so , and unlocks further avenues of investigation , including some of those astutely suggested by the reviewer . We hope for your support in these matters , and are at your disposition to further answer any questions or concerns you may have ."}, {"review_id": "ETBc_MIMgoX-3", "review_text": "This paper proposes a setter-solver or teacher-student scheme for training goal-conditioned agents ( student/solver ) in a discrete environment MiniGrid . In their method called AMIGo , the teacher takes the initial state of the student at the beginning of each episode as its input and proposes an intrinsic goal for the student to fetch , the student in each step takes an action and gets a reward combining both the extrinsic reward from the environment and the intrinsic reward , and the teacher gets a reward at the end of an episode or when the intrinsic goal is reached . The major idea is to make the goals proposed by the teacher staying at an appropriate/medium level of difficulty , i.e. , fetchable for the student but not too easy . To do so , they use a threshold to define the teacher reward : they issue a positive ( negative ) reward to the teacher if the student spends more ( less ) steps than the threshold to reach the goal . And they adaptively increase the threshold if the student keeps successfully reaching the intrinsic goals . In experiments on five environments of MiniGrid , AMIGo outperforms five baselines with different types of intrinsic reward and set up new SoTA results on the studied tasks . This paper is well-written and demonstrates a reasonable curriculum strategy by training a teacher model to propose goals in a discrete grid world ( though largely simplified from realistic scenarios ) . The experiments show remarkable improvements over existing curiosity-driven or intrinsic reward-based methods and set up a new SoTA on those tasks . My major concerns are the novelty of the idea and the generality of the proposed method . ( 1 ) Though the authors discussed the difference of AMIGo to GoalGAN , in which the teacher is instead a GAN generating continuous goals , the main idea behind the two are very similar , i.e. , to train a teacher model proposing goals of medium difficulty , and they both use threshold ( s ) on rewards for this purpose . Although I agree that they use different architectures for the teacher model ( for producing different types of goals ) , and the GAN in GoalGAN does not take student 's initial state as input since it is not dealing with procedurally-generated environments , these are natural choices adapted to the specific environments/tasks for testing these methods . The main contribution of this paper , from the perspective of proposing a novel curriculum generator , is incremental . ( 2 ) Another related curriculum generating strategy is to balance the difficulty and diversity/representativeness of goals/tasks when proposing them to train the goal-conditioned student . It worth discussing the difference/advantage/limitation of the proposed method when compared to them . To name a few , Portelas et al. , `` Teacher algorithms for curriculum learning of Deep RL in continuously parameterized environments '' , CoRL 2019 . Fang et al. , `` Curriculum-guided hindsight experience replay '' , NeurIPS 2019 . ( 3 ) The experiments only demonstrate the method in one type of environment . Although the MiniGrid is an appropriate environment to test the proposed curriculum generating method due to its simplicity , which allows the authors to rule out other factors and mainly focus on the quality of selected goals , it is not clear whether/how the proposed method can be easily generalized to other more practical and/or complicated environments/tasks such as robotic control , navigation , traffic , and gaming . More discussion on how to adapt AMIGo in those tasks can benefit researchers following this work . ( 4 ) The student is a goal-conditioned policy and it can be conditioned on one goal per time . During training , when is the student conditioned on the extrinsic goal or the intrinsic goal ? Is there a schedule of assigning which goal to the student ? ( 5 ) The student can also self-propose a goal by hindsight experience replay ( HER ) and this has been demonstrated to be effective in alleviating the sparse reward problem . It is interesting to see a discussion or comparison to HER methods .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments and questions , and for your support for the paper . We will respond to your points and answer your questions in the hope that our discussion will not only help us improve the paper , but also help us understand what outstanding issues ( if any ) stand between us and a stronger endorsement on your part after discussion . # # # Novelty with regard to GoalGAN We respectfully but robustly disagree that novelty is an issue here , but are keen to make the point more clearly in the paper and hope you will help us improve on this axis . First , GoalGAN uses a discriminator to evaluate the difficulty of goals , as trained in an adversarial setting . In contrast , our teacher network is a \u201c constructive adversary \u201d , as its own objective forces it not only to model what is difficult for the student , but also what is feasible , and propose appropriate goals . Second , the fact that GoalGAN does not take the initial state into consideration can be considerably limiting and is not just an easy adaptation chosen because of the environment . For example , Racaniere et al . ( 2019 ) do compare against GoalGAN in the locomotion tasks but are unable to adapt it to their Alchemy environment which changes from episode to episode . Furthermore , to this point they argue that training generative models with non-trivial conditioning is challenging in general , and in this context , discovering the relevant environment and goal structures is difficult . This also yields fairly significant additional differences : 1 . Their training is considerably more complex as it requires a three-step iterative procedure and depends on the coordination of three different modules ( the generator , the discriminator , and the policy ) , while in our method the teacher and student are trained simultaneously . 2.GoalGAN requires a memory buffer of previous goals proposed . Maintaining and sampling this large memory can be very costly . In addition , as noted , previous goals can become unfeasible or nonsensical through time or if the environment changes . 3.Having to run the agent on each goal multiple times to get a label of whether it has an intermediate difficulty requires additional wallclock time and computation for the same number of agent steps . # # # Relations to Other Curriculum Learning Strategies Thank you for flagging the work of Fang _et al._ , and Portelas _et al._ We were not acquainted with these papers . We are happy to include a discussion of this work given the additional page afforded us for the final paper , and will do so during the discussion period . While there are fairly significant differences with regard to these papers and ours both in terms of to the methods and applications ( e.g.Fang _et al._ focus on off-policy learning , whereas our method is primarily aimed at on-policy learning ) , you are right that further investigating the balance between difficulty and diversity of goals is worth investigating in further work . We are confident complementary gains are to be made , but hope you agree that further investigation of this is outside of the scope of the present paper . # # # On the use of Minigrid We completely agree that the ubiquitous applicability of this method to other domains needs to be empirically tested , and we are careful not to make predictions here . We limit ourselves to stating what limiting assumptions are made in the general approach , and in our specific implementations , to demonstrate the _potential_ for application to other domains . Given the space constraints , we chose to focus on an in-depth evaluation and comparison on a single domain with a diversity of exploration tasks and settings , to prove the concept for this exploration method , in line with other work in the literature . On this point , we emphasise that despite its visual simplicity , the MiniGrid task suite offers sufficient complexity and range of difficulty and has been a popular benchmark in the related literature , e.g.in [ Igl et al. \u2019 s ( NeurIPS 2019 ) paper on SNI ] ( https : //arxiv.org/abs/1910.12911 ) , [ Loynd et al. \u2019 s ( ICML 2020 ) paper on WMG ] ( https : //proceedings.icml.cc/paper/2020/hash/5cf21ce30208cfffaa832c6e44bb567d-Abstract.html ) , or [ Raileanu and Rockt\u00e4schel \u2019 s ( ICLR 2020 ) paper on RIDE ] ( https : //iclr.cc/virtual_2020/poster_rkg-TJBFPB.html ) ."}], "0": {"review_id": "ETBc_MIMgoX-0", "review_text": "The authors introduce AMIGo , an approach to curiosity in which an adversarial `` teacher '' agent proposes goals that the `` student '' agent attempts to achieve . The student obtains an intrinsic reward of +1 when it achieves the goal , and this augments the extrinsic reward additively . The teacher proposes a goal at the start of each episode , and whenever the student reaches a goal . The teacher is positively rewarded if the student achieves the goal after some threshold t * , and negatively rewarded otherwise . t * is incremented at fixed intervals . They then demonstrate that this outperforms a variety of suitable baselines . The algorithm proposed has great strength in its simplicity . It does bear a considerable similarity to Sukhbaatar et al 2017 ( their ASP baseline ) , but AMIGo appears to be more flexible , and they put forth evidence that it considerably outperforms ASP on their benchmarks . AMIGo is something that could be adapted very easily to many other settings , and it is a quite agnostic framework . The experimental results do show considerable advantages over baselines . I am , however , concerned about how the evaluations were done . At the very least , I find that the main text fails to describe what seem to be important caveats that can only be found in the appendix . In particular : -For baselines , suitable hyperparameters were found using a subset of tasks ( tasks thought to be easier ) , and , if positive results were achieved , the same hyperparameters were run on the rest of the tasks . If scores of zero were obtained for the initial subset , the baselines were not run at all and were assumed to score zero . -As noted in the main text , some of the baselines were developed for partially observed environments with an egocentric view , so they let the baselines run in both modes ( with algorithms with multiple modules , they varied this independently for both ) . While this might seem like simply giving the baselines more chances to succeed , there is a problem when this is coupled with the first practice of assuming zero scores . Some of these baselines obtained no score in the fully observed setting on the `` easy '' task subset , and hence were not run on the hard environments . They did better in partially observed modes , and hence obtained their hard environment scores in these partially observed modes . These partially observed modes are potentially much harder ! While one might be able to make some claim of the form `` if they ca n't do the easy task , they ca n't do the hard , '' I am worried that these choices , used together , really obfuscate the performance differential . Note the quite high score variance that AMIGo achieves on the harder environments . Given such high variance , models which were simply not run in the fully observed setting could have actually performed better were they given the chance ( and also perhaps needed further hyperparameter tuning ) . At the very least , I think this is a really unclear way of benchmarking things and should be corrected so that we do not have these statistical worries . As a result , I can not at this time recommend acceptance , though I am certainly open to being persuaded that my worries are unfounded . I could have missed something about their experimental methods , or environmental particulars could somehow make the above treatments perfectly reasonable . A more minor concern : I do wonder about the extent to which this method applies in other settings . Is this time-threshold reward method applicable to a broad array of settings , or is this particularly useful in minigrid and similar sorts of maze navigations ? Are there situations in which time-to-complete is not a good proxy for difficulty ? Is it important that the goals be represented in some very compact way , as they are for these experiments ? These are questions that would , I think , need additional experiments . The authors did a considerable amount of evaluation , so I hesitate to simply ask for more , but I do wonder if this approach is particularly suited to a narrow range of environments . I am also curious as to why the ASP baseline did so poorly . Its method seems to be quite similar , so more commentary about what makes one fail badly while the other succeeds would be really useful ! * Updated score based on below discussion * * Updated score again based on below discussion *", "rating": "7: Good paper, accept", "reply_text": "Hello , and thank you for your detailed review . It sounds like you like the paper and the method , especially regarding the potential to apply it to a wider range of RL settings . We agree , and would be significantly empowered to pursue further research in this area if we can convince you the paper proves the concept and is worthy of publication ! # # # Experimental Protocol Our understanding is that the main thing preventing you from wholeheartedly supporting acceptance is the concerns you outline about the experimental method . We aim to address those during this discussion , both through clarification and through running the outlying experiments you wish us to verify . We hope that this will not only address your concerns , but give you the comfort you need to fully support the paper with your score . We believe we performed an extremely thorough experimental analysis and comparison of AMIGo against comparable benchmarks , ( actually ) running 85 experiments ( each with 5 seeds ) , each of which requires non-trivial computational resources ( 1-2 dedicated GPU days per experimental run , per seed ! ) . Available resources barely permitted this , and because individual experimental runs showed that no method would solve harder experiments if flatlining ( constant zero return ) on easier tasks , it seemed fair and intuitive to avoid running methods on harder tasks if they can not train * at all * on easier tasks . We were as transparent as we could be about this when reporting results , and hope you will understand this was not an attempt at obfuscation . However , for the avoidance of doubt , * * we ~will~ have run all the remaining experiments projected to flatline , following the same experimental protocol used for the reported experiments , to confirm our projections * * . We confirm that our results stand in [ this post ] ( https : //openreview.net/forum ? id=ETBc_MIMgoX & noteId=4dnHW4XDd_M ) . Experiment code will be open-sourced so the community can check our results for maximum transparency . # # # Choice of Threshold Cost The reviewer astutely suggests that alternatives to the number of steps could be used as a cost , for the purpose of training the teacher . We do emphasise in Section 6.2 that there are several options available for measuring policy performance in this context , and that step count was a simple choice for the purpose of our experiments . In more complex settings , other measurable costs might be suitable , for example power consumption during the operation of a robotic arm in a continuous control setting . You are correct that to properly evaluate the general framework in more general settings , further experiments are required . However , to prove the concept for this method , the MiniGrid task suite offered sufficient complexity and range of difficulty and has been a popular benchmark in the related literature , e.g.in [ Igl et al. \u2019 s ( NeurIPS 2019 ) paper on SNI ] ( https : //arxiv.org/abs/1910.12911 ) , [ Loynd et al. \u2019 s ( ICML 2020 ) paper on WMG ] ( https : //proceedings.icml.cc/paper/2020/hash/5cf21ce30208cfffaa832c6e44bb567d-Abstract.html ) , or [ Raileanu and Rockt\u00e4schel \u2019 s ( ICLR 2020 ) paper on RIDE ] ( https : //iclr.cc/virtual_2020/poster_rkg-TJBFPB.html ) . We hope you will agree that the extensive experiments done here at least show the robustness of our method within a difficult class of exploration problems , and that future work seeking to adapt it to different contexts warrants further research , to be incorporated into future papers . # # # ASP Baseline We used the authors ' own [ PyTorch implementation of ASP ] ( https : //github.com/tesatory/hsp ) , and did a thorough hyperparameter sweep , so we are confident the empirical results are a fair comparison . We are unsure how to explain this result , and are not certain it is our place to proffer negative analysis of others \u2019 work , but we should clarify that ASP is not that similar to AMIGo . ASP requires two policies acting , and can only propose goals which have been reached by one of them . Furthermore , ASP requires reversible and resettable environments , which is a pretty strong assumption that AMIGo does not make . We hope these crucial differences go some way towards giving you an intuition as to why there is a difference in performance , and wider potential for application . We are happy to add something about this in the final paper if you think it essential , as there is space for it with the additional page , but we initially decided not to focus on this too much to avoid the impression of talking down other researcher \u2019 s methods , as the numbers spoke for themselves . # # # Summary We hope the clarifications above , paired with the experimental confirmation of our projections for the harder environments ( which we expect to provide sometime next week ) , are sufficient for you to reconsider your assessment of the paper . If you have any outstanding concerns , please don \u2019 t hesitate to let us know so that we can discuss them , and properly understand what\u2014if anything\u2014stands between us and a strong recommendation for acceptance ."}, "1": {"review_id": "ETBc_MIMgoX-1", "review_text": "# # After Author Response The author response for most of the points of confusion and weaker sections of the draft are satisfactory . I thank the authors for incorporating some of the feedback during this discussion . I am still on the fence about the generality of this idea , and how much the teacher objective is specific to the minigrid tasks . But overall I find this idea interesting and believe it adds value to the field . # # Summary of the paper : This paper proposes a goal- '' generating '' module ( called the teacher ) that will propose goals in procedurally generated grid world domains . The way they train this teacher is through policy gradient updates , with the teacher getting positive rewards if the agent achieves the provided goal but takes longer than a threshold to reach it , and negative rewards if the agent is unable to reach the goal or reaches it too quickly . The threshold is gradually increased based on the agent 's capabilities , allowing for more and more difficult goals . Both the agent and the teacher additionally get rewarded for solving the external task . So the teacher is directed to provide goals that will also solve the given task , and the agent also learns to not just solve the given goal . Other auxiliary objectives for the teacher are entropy regularization for diverse goals , and episode boundary awareness . # # Strengths : + This paper presents a clear reward based mechanism to train the teacher . It also presents a reasonable answer to the question of how a difficulty threshold should be set for the teacher . It provides evidence that this goal-generation approach helps in directed exploration , is useful for general RL tasks ( beyond goal-based RL ) , and speeds up solving of a variety of hard grid world environments . + Figure 3 shows the changing thresholds and the subsequently generated targets for the agent , which is a good experiment to show that this increasing threshold mechanism works as intended . + The ablations in the appendix are illustrative and show the different considerations and how different components of the system interact . # # Weak Points : - The suggestion that the goal generation is adversarial seems slightly strong , with not much evidence to show that the generated goals are adversarial to the agent . The goals could also be generated progressively farther and farther due to the moving threshold . - The way the problem is set up and communicated makes it seem oriented towards grid-world problems exclusively . There is no definition of what a goal is . In fact , the problem setup itself is communicated rather vaguely , with terms being introduced as required rather than the entire setup being introduced clearly up front . Setting up the problem more clearly , with respect to RL literature ( maybe refer to the Sutton and Barto book or alternative setups ) , being clear about what a goal is and how that goal is being used in the context of the grid world would make the paper clearer to read and understand . - It is also unclear whether the teacher rewards are discounted according to the number of steps taken by the agent , or if it operates at a lower temporal frequency . The entire teacher policy training regimen needs more explanation and clarity . - Given that the goal-generation here is to give an ( x , y ) location to travel to , it is unclear why an approach like GoalGAN can not be adapted to this domain . - An additional question to ask would be whether the approach of Zhang et al can be adapted as a baseline to compare to this work . I can understand if it is not applicable , since that approach is specific to goal-conditioned RL , but would like some clarity on it . - When explaining the type of goals , the authors mention that this `` includes picking up keys , opening doors and dropping objects onto empty tiles '' . How is an agent supposed to know whether it is supposed to move on to a square or drop an object there . The teacher does not seem to communicate what kind of goal is being generated . If the kind of goal is not communicated , would n't the agent do the simplest thing possible , and just move over the square , instead of figuring out that it should have dropped an object at the location ? - The auxiliary task of `` Episode Boundary Awareness '' is a little unclear . My understanding is that the teacher is rewarded for selecting goals where the object type at that location changes if the episode changes . An illustrative example or clearer language is necessary to understand this additional objective and how it is helping the agent . This is very important as it seems that this auxiliary reward is essential in getting the teacher to learn goals that are useful for eventual agent success ( as evidenced by the ablations in Table 6 ) . In fact , I am curious as to why the performance of the system if this particular reward is removed drops drastically and is similar to the extrinsic reward being removed . - The main weakness of the approach for me is that it is unclear how this system scales to other RL problems . Since there is no definition of what a goal is considered to be , and no other examples given except grid worlds , it is hard to see what a broad application of this idea would be . - The learning curves ( Figure 4 ) seem to show that the training with AMIGo is pretty high variance , compared to the baselines . Is there an explanation for this variance ? Example , performance on OMmedium drops to 0 pretty regularly . # # Additional Comments : The related work section needs some additional work . While the authors present a broad array of related work , there are some suggestions to round off this section . For intrinsic motivation , the authors should include work by Andy Barto ( `` Intrinsic Motivation and Reinforcement Learning '' , 2013 ) and other related work , as well as the optimal rewards framework by Satinder Singh 's group ( `` Where do rewards come from ? '' ) along with work by Jonathan Sorg ( ICML 2010 ) and more recently work by Zeyu Zheng ( `` On Learning Intrinsic Rewards for Policy Gradient Methods '' ) . These views of intrinsic motivation as a vehicle for more than exploration is something that is not very clear in the current draft , even though the authors do present some alternative work like empowerment . For curriculum learning , there are recent surveys that on curriculum learning for RL ( `` Curriculum Learning for Reinforcement Learning Domains : A Framework and Survey '' , Narvekar et al . ; `` Automatic Curriculum Learning For Deep RL : A Short Survey '' , Portelas et al . ) that should be referred to when explaining curriculum generation in the context of RL . # # Conclusion Overall I would like to see clearer communication of some of the ideas as well as some explanation from the authors that will give me confidence that this idea is more broadly applicable .", "rating": "7: Good paper, accept", "reply_text": "We appreciate your constructive comments and detailed review which will help us to improve and clarify the paper . We will respond to your points and will improve our communication in the paper with the hope we can convince you our idea is broadly applicable to the point where you would consider improving your assessment . # # # On the adversarial nature of AMIGo The loss function of the generator is partially adversarial by construction : the teacher is rewarded for proposing goals which are difficult for the student policy . We further qualify this as being \u201c constructively adversarial \u201d , because such goals must also be achievable , and thus not too hard . Empirically , learning trajectories ( e.g.Figure 4 ) often show phases where the loss of the teacher decreases while that of the policy increases , demonstrating some notion of opposition in their respective objectives . Ultimately , you are right that the equilibrium condition is for the teacher to converge on proposing extrinsic goals once they are achievable , so in this sense it ceases to be adversarial at the end of training , but again , this is what motivates our terminology regarding \u201c constructively adversarial \u201d , which is used throughout the paper . We hope you agree with this characterization . # # # Clarification regarding Goals AMIGo is applicable to a wide range of goal types , although whether it works or not is dependent on the implementation and modelling choices , and is thus the subject of further experiments . We intentionally underspecify the notion of goal at play to encourage such exploration . However , in our experimental setting , used to prove the concept for this general learning process , a goal is formally defined as a change in the observation on a tile , as specified by an x , y coordinate . We will add that statement up front along with a better clarification . We want to emphasize that AMIGo is applicable beyond grid worlds , although these are a good setting to prove the concept of this research direction , in line with many papers in the related literature . Specifically , the MiniGrid task suite was used because it offers sufficient complexity and range of difficulty and has been a popular benchmark in the related literature , e.g.in [ Igl et al. \u2019 s ( NeurIPS 2019 ) paper on SNI ] ( https : //arxiv.org/abs/1910.12911 ) , [ Loynd et al. \u2019 s ( ICML 2020 ) paper on WMG ] ( https : //proceedings.icml.cc/paper/2020/hash/5cf21ce30208cfffaa832c6e44bb567d-Abstract.html ) , or [ Raileanu and Rockt\u00e4schel \u2019 s ( ICLR 2020 ) paper on RIDE ] ( https : //iclr.cc/virtual_2020/poster_rkg-TJBFPB.html ) . Going beyond grid worlds , e.g. , by first running semantic segmentation pipelines in photorealistic 3D environments and then defining intrinsic reward procedures around changes to detected objects would be an interesting research avenue , but outside of the scope of the paper . The overall idea and algorithm behind AMIGo is not grid world specific \u2014 only our particular choice of a goal proposal distribution is . We agree our choice of words was perhaps a bit unprecise when we say `` includes picking up keys , opening doors and dropping objects onto empty tiles '' . We will formally introduce and clarify that those are among the different available actions for the agent to achieve a goal , namely a change in the ( x , y ) coordinate . In some cases in our setting , moving over a square is not the simplest thing possible , such as when an obstacle can be removed or a door opened . Similarly , in other tasks and environments it could be easier to affect a cell by throwing something at it than by reaching it . In our view , this is one way in which our goal definition can be achieved more generally . We hope these clarifications can convince you our setup is not oriented towards grid world problems exclusively . # # # Clarification regarding the Teacher Training The teacher \u2019 s rewards are not discounted according to the number of steps taken by the agent . More specifically , the teacher receives reward after several steps depending on whether the agent reached a goal , so it does operate at a different temporal frequency . We will further clarify this on section 3.2 : We did experiment with a fixed threshold , and found that the loss function alone was sufficient to induce harder goals and curriculum learning . We omitted to mention this due to lack of space and because fixed thresholds have to be carefully fixed for each environment ( depending on its size and complexity ) , while the single moving heuristic is general and applies to all environments . We will explain this more clearly in the updated draft . Episode Boundary Awareness : Your understanding is correct and we will describe what we mean with an example . Observed Higher Variance in some Environments : We think the observed higher variance is due to the fact the learning dynamics of the two modules are coupled . Similar remarks have been made about GANs which can be difficult to train . In OMmedium , we did observe that the training stabilized after 27M steps for all runs ."}, "2": {"review_id": "ETBc_MIMgoX-2", "review_text": "This paper introduces a teacher agent in reinforcement learning framework . The teacher 's responsible for setting goal for the student agent ( the `` main '' agent ) . The teacher acts in adversarial way ( but also somewhat cooperative ) in setting the goal . That is , the goal should be achievable by the student , yet the goal should not be too simple that the student stops learning towards harder goals , and eventually achieve the original goal from the environment . Experiments show proposed method is able to learn okay performance for hard environments , even though most other methods fail to score at all . Pros : I think this is a very interesting approach and has great potential . The idea is natural and experiments make a lot of sense . This should partially credit to clear writing and examples . The authors provide only one set of results in main paper , but have more ablation studies and variants of reward forms . Those results will be helpful to provide more insight into how well the agent handles the task . Cons : My major concern with proposed method is that it seems limited to particular applications , such as grid world . As the authors also pointed out , even proposing goals in this paper 's setting is a challenging problem . For now , I do n't see an automatic way to proposing goals for other reinforcement learning tasks , such as Atari games and Go . In those tasks , I feel teacher 's goals may need to be designed incorporating human 's understanding of the tasks ( i.e.capture a stone in Go ) . This makes me feel the proposed method is not very generalizable , but I am open to change my opinion if authors provide a good answer . If for those tasks , a human-generated set of goals are needed by teacher agent , then how do we correctly attribute the performance gain we saw in this paper ? That is , are the gains over baseline methods from 1 ) we have a better training framework , such as curriculum learning , or 2 ) the goals are set in a way that incorporate our understanding of the environment ? My second concern is about scalability of proposed method . For some applications , it may be unnecessary to have a teacher that sets intermediate goals ( e.g.AlphaGo - > AlphaZero ) . How does sample efficiency for AMIGo compared to baselines ? I saw results related to number of steps for AMIGo alone , but not baseline methods . Conceptually if a teacher sets many intermediate goals , it 's likely that the student will train more number of steps and possible to achieve better performance . However if this procedure requires 10x more number of steps to achieve on-par performance for easy/medium level tasks , then it will be important to acknowledge that . When will the student need a teacher vs. w/o a teacher seems also worthy studying .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their comments . It seems that the two main concerns the reviewer has have to do with the applicability of AMIGo to different settings or with different goal types , and the scalability and sample efficiency of the method . We answer both of these concerns below . # # # Limitations of AMIGo to Grid Worlds , and other Goal Forms The review is concerned that the choice of intrinsic goal specifications limits the applicability of AMIGo to grid worlds . We should be clear about the aim and claim of this paper : it is to demonstrate that this general \u201c constructively adversarial \u201d form of teacher-student training works as an exploration mechanism . We are transparent about the general requirements of this method throughout section 3 : namely that there exists a form of intrinsic goal the student policy can condition on , that there exists a ( learned or hard-coded ) valuation function over these goals , and that there exists a measure of the student \u2019 s performance . These requirements do not restrict us to grid-worlds or any specific environment , but particular choices of goals and performance measures might . To prove the concept , we have focussed on a particular choice of goals ( coordinates in a grid world , which gives us a valuation function for free ) and performance ( steps ) , in order to have an in-depth comparison with other exploration methods on a well-known exploration problem , MiniGrid . The MiniGrid task suite offers sufficient complexity and range of difficulty and has been a popular benchmark in the related literature , e.g.in [ Igl et al. \u2019 s ( NeurIPS 2019 ) paper on SNI ] ( https : //arxiv.org/abs/1910.12911 ) , [ Loynd et al. \u2019 s ( ICML 2020 ) paper on WMG ] ( https : //proceedings.icml.cc/paper/2020/hash/5cf21ce30208cfffaa832c6e44bb567d-Abstract.html ) , or [ Raileanu and Rockt\u00e4schel \u2019 s ( ICLR 2020 ) paper on RIDE ] ( https : //iclr.cc/virtual_2020/poster_rkg-TJBFPB.html ) . We can not and do not claim that this method will work out-of-the-box in other domains , with other goal types , but explicitly leave this for further work , as pointed out at the end of Section 5 . Nevertheless , we do not see a substantial difference between many tasks such as Atari and our setting in terms of requiring deeper human understanding of the environment . Many MDPs discount rewards based on the number of steps , and often difficulty can be correlated with the amount of effort or time needed to obtain a certain outcome , this could be sufficient for the teacher to learn to propose appropriately difficult goals . However , we agree with you that investigating other settings would be interesting , and present particular challenges , such as depending upon human knowledge to model the goal valuation function . There is prior work in this area incorporating aspects of inverse reinforcement learning , e.g . [ Bahdanau _et al._ ( ICLR 2019 ) ] ( https : //openreview.net/forum ? id=H1xsSjC9Ym ) where a language-conditional goal-verifier is learned jointly with a policy , based on a finite set of expert examples of goal states , all done on pixel input . But it is clear that this is ambitious and non-trivial research that must be the subject of further papers , rather than trying to cram all possible work and evaluation in this one ( which has no fewer than 85 experiments ) . We hope you will agree this paper makes a significant enough contribution to both warrant publication , and enable such follow up work to be done . # # # Scalability and sample efficiency The reviewer will be happy to hear that their concerns about scalability and sample efficiency are already addressed in the submitted version of the paper , namely in Appendix C , as pointed to from Section 4.4 . In this appendix , we show that on the two easiest environments , * * KCmedium * * and * * OMmedium * * , agents need about 10 million steps to converge while on the other four more challenging environments , they need an order of 100 million steps to learn the tasks , showcasing AMIGo 's contributions not just to solving the exploration problem , but also to improving the sample complexity of agent training . Naturally , for the harder environments , sample efficiency is not comparable since AMIGo is the only method obtaining non-zero returns . We hope this addresses your second main concern , and are open to hearing how we could make things clearer in the paper . # # # Summary We believe that both the response above , and the existing plots and discussion from Appendix C , sufficiently address your concerns to the point where you \u2019 ll consider revising your assessment in support of the paper . To repeat our main point , the paper proves the concept for a new class of exploration methods , and thoroughly compares it to existing state of the art . We are confident that it does so , and unlocks further avenues of investigation , including some of those astutely suggested by the reviewer . We hope for your support in these matters , and are at your disposition to further answer any questions or concerns you may have ."}, "3": {"review_id": "ETBc_MIMgoX-3", "review_text": "This paper proposes a setter-solver or teacher-student scheme for training goal-conditioned agents ( student/solver ) in a discrete environment MiniGrid . In their method called AMIGo , the teacher takes the initial state of the student at the beginning of each episode as its input and proposes an intrinsic goal for the student to fetch , the student in each step takes an action and gets a reward combining both the extrinsic reward from the environment and the intrinsic reward , and the teacher gets a reward at the end of an episode or when the intrinsic goal is reached . The major idea is to make the goals proposed by the teacher staying at an appropriate/medium level of difficulty , i.e. , fetchable for the student but not too easy . To do so , they use a threshold to define the teacher reward : they issue a positive ( negative ) reward to the teacher if the student spends more ( less ) steps than the threshold to reach the goal . And they adaptively increase the threshold if the student keeps successfully reaching the intrinsic goals . In experiments on five environments of MiniGrid , AMIGo outperforms five baselines with different types of intrinsic reward and set up new SoTA results on the studied tasks . This paper is well-written and demonstrates a reasonable curriculum strategy by training a teacher model to propose goals in a discrete grid world ( though largely simplified from realistic scenarios ) . The experiments show remarkable improvements over existing curiosity-driven or intrinsic reward-based methods and set up a new SoTA on those tasks . My major concerns are the novelty of the idea and the generality of the proposed method . ( 1 ) Though the authors discussed the difference of AMIGo to GoalGAN , in which the teacher is instead a GAN generating continuous goals , the main idea behind the two are very similar , i.e. , to train a teacher model proposing goals of medium difficulty , and they both use threshold ( s ) on rewards for this purpose . Although I agree that they use different architectures for the teacher model ( for producing different types of goals ) , and the GAN in GoalGAN does not take student 's initial state as input since it is not dealing with procedurally-generated environments , these are natural choices adapted to the specific environments/tasks for testing these methods . The main contribution of this paper , from the perspective of proposing a novel curriculum generator , is incremental . ( 2 ) Another related curriculum generating strategy is to balance the difficulty and diversity/representativeness of goals/tasks when proposing them to train the goal-conditioned student . It worth discussing the difference/advantage/limitation of the proposed method when compared to them . To name a few , Portelas et al. , `` Teacher algorithms for curriculum learning of Deep RL in continuously parameterized environments '' , CoRL 2019 . Fang et al. , `` Curriculum-guided hindsight experience replay '' , NeurIPS 2019 . ( 3 ) The experiments only demonstrate the method in one type of environment . Although the MiniGrid is an appropriate environment to test the proposed curriculum generating method due to its simplicity , which allows the authors to rule out other factors and mainly focus on the quality of selected goals , it is not clear whether/how the proposed method can be easily generalized to other more practical and/or complicated environments/tasks such as robotic control , navigation , traffic , and gaming . More discussion on how to adapt AMIGo in those tasks can benefit researchers following this work . ( 4 ) The student is a goal-conditioned policy and it can be conditioned on one goal per time . During training , when is the student conditioned on the extrinsic goal or the intrinsic goal ? Is there a schedule of assigning which goal to the student ? ( 5 ) The student can also self-propose a goal by hindsight experience replay ( HER ) and this has been demonstrated to be effective in alleviating the sparse reward problem . It is interesting to see a discussion or comparison to HER methods .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments and questions , and for your support for the paper . We will respond to your points and answer your questions in the hope that our discussion will not only help us improve the paper , but also help us understand what outstanding issues ( if any ) stand between us and a stronger endorsement on your part after discussion . # # # Novelty with regard to GoalGAN We respectfully but robustly disagree that novelty is an issue here , but are keen to make the point more clearly in the paper and hope you will help us improve on this axis . First , GoalGAN uses a discriminator to evaluate the difficulty of goals , as trained in an adversarial setting . In contrast , our teacher network is a \u201c constructive adversary \u201d , as its own objective forces it not only to model what is difficult for the student , but also what is feasible , and propose appropriate goals . Second , the fact that GoalGAN does not take the initial state into consideration can be considerably limiting and is not just an easy adaptation chosen because of the environment . For example , Racaniere et al . ( 2019 ) do compare against GoalGAN in the locomotion tasks but are unable to adapt it to their Alchemy environment which changes from episode to episode . Furthermore , to this point they argue that training generative models with non-trivial conditioning is challenging in general , and in this context , discovering the relevant environment and goal structures is difficult . This also yields fairly significant additional differences : 1 . Their training is considerably more complex as it requires a three-step iterative procedure and depends on the coordination of three different modules ( the generator , the discriminator , and the policy ) , while in our method the teacher and student are trained simultaneously . 2.GoalGAN requires a memory buffer of previous goals proposed . Maintaining and sampling this large memory can be very costly . In addition , as noted , previous goals can become unfeasible or nonsensical through time or if the environment changes . 3.Having to run the agent on each goal multiple times to get a label of whether it has an intermediate difficulty requires additional wallclock time and computation for the same number of agent steps . # # # Relations to Other Curriculum Learning Strategies Thank you for flagging the work of Fang _et al._ , and Portelas _et al._ We were not acquainted with these papers . We are happy to include a discussion of this work given the additional page afforded us for the final paper , and will do so during the discussion period . While there are fairly significant differences with regard to these papers and ours both in terms of to the methods and applications ( e.g.Fang _et al._ focus on off-policy learning , whereas our method is primarily aimed at on-policy learning ) , you are right that further investigating the balance between difficulty and diversity of goals is worth investigating in further work . We are confident complementary gains are to be made , but hope you agree that further investigation of this is outside of the scope of the present paper . # # # On the use of Minigrid We completely agree that the ubiquitous applicability of this method to other domains needs to be empirically tested , and we are careful not to make predictions here . We limit ourselves to stating what limiting assumptions are made in the general approach , and in our specific implementations , to demonstrate the _potential_ for application to other domains . Given the space constraints , we chose to focus on an in-depth evaluation and comparison on a single domain with a diversity of exploration tasks and settings , to prove the concept for this exploration method , in line with other work in the literature . On this point , we emphasise that despite its visual simplicity , the MiniGrid task suite offers sufficient complexity and range of difficulty and has been a popular benchmark in the related literature , e.g.in [ Igl et al. \u2019 s ( NeurIPS 2019 ) paper on SNI ] ( https : //arxiv.org/abs/1910.12911 ) , [ Loynd et al. \u2019 s ( ICML 2020 ) paper on WMG ] ( https : //proceedings.icml.cc/paper/2020/hash/5cf21ce30208cfffaa832c6e44bb567d-Abstract.html ) , or [ Raileanu and Rockt\u00e4schel \u2019 s ( ICLR 2020 ) paper on RIDE ] ( https : //iclr.cc/virtual_2020/poster_rkg-TJBFPB.html ) ."}}