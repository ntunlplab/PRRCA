{"year": "2017", "forum": "S1c2cvqee", "title": "Designing Neural Network Architectures using Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "This paper comes up with a novel approach to searching the space of architectures for deep neural networks using reinforcement learning. The idea is straightforward and sensible: use a reinforcement learning strategy to iteratively grow a deep net graph (the space of actions is e.g. adding different layer types) via Q-learning. The reviewers agree that the idea is interesting, novel and promising but are underwhelmed with the execution of the experiments and the empirical results. \n \n The idea behind the paper and the formulation of the problem are quite similar to a concurrent submission (https://openreview.net/forum?id=r1Ue8Hcxg) which has much higher scores. That paper formulates their reinforcement learning strategy using the REINFORCE algorithm while this one uses Q-learning. The major discrepancy between the papers is in the execution of the experiments. This paper had a much more restricted set of experiments on a smaller search space and thus had less impressive results. The other paper explored a much larger space of architectures and explored recurrent neural networks, achieving state-of-the-art on multiple tasks and finding exciting novel architectures that challenge current standard models (LSTMs). As a result that paper received three 9's and this one three 6's.\n \n The authors argue that they had almost the same idea but did not have access to the same amount of computing resources as the other paper. That certainly is warranted as the other paper used an *extreme* amount of compute resources for their experiments. \n \n Given the average scores of ICLR and target acceptance rate, three 6s should normally be considered a reject. Specifically, in the absence of the other paper this would be rejected. However, the authors argue that this would be unfair, since the major difference was simply in compute resources. A major concern of the reviewers was that this approach was too computationally expensive, which is strongly contradictory to the scores for the other paper. In reality, the other paper really is just much more interesting because of the empirical results. However, I sympathize with the authors as the other paper essentially demonstrates that their idea is sensible and effective (albeit only with obscene compute resources), and the paper would effectively be \"scooped\" from further submissions.\n \n (Note a contrarian viewpoint might take an analogy from other fields. e.g. experimental physics papers are likely rejected if they don't have multi-billion dollar equipment to run the right set of exhaustive experiments, which is the difference between e.g. the large hadron collider and most university depts.)\n \n Pros:\n - The paper proposes a novel approach to architecture search using reinforcement learning\n - The approach is technically sound and interesting\n - The authors achieve good results automatically on benchmark tasks using their approach\n - They empirically demonstrate that their method works (via plots demonstrating the behavior of the algorithm while varying how exploratory the algorithm is)\n \n Cons:\n - The search space is very constrained (too much for us to learn anything interesting from the results)\n - The experimental results are good but underwhelming\n - Compared to e.g. Bayesian optimization methods, this approach seems very wasteful in compute resources (e.g. 1500 runs of training before the approach starts exploiting).\n \n The Program Chairs have also reviewed this paper, and taken everything into account, have reccommended this paper for poster presentation at the main conference.", "reviews": [{"review_id": "S1c2cvqee-0", "review_text": "This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices. Strengths: - A novel approach for automatic design of neural network architectures. - Shows quite promising results on several datasets (MNIST, CIFAR-10). Weakness: - Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.) - The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space. Overall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review ! Limited architecture design choices : We agree that our current implementation had the weakness of somewhat limited architecture space , which we chose to constrain due to our limited computation resources . We would like to stress , however , that this is a limitation of our current hardware setup but not a limitation of the method overall . Small state-action space and tabular Q-learning : While our current state-action space size is fairly small -- -4296 state-action pairs -- -we do vary the station-action space size between experiments and still obtain consistent results . For both SVHN/MNIST experiments ( with state-action space size 4296 ) and the larger CIFAR-10 experiment ( with state-action space size 6834 ) , our meta-modeling method was able to progressively improve neural network architectures . Moreover , we plan to experiment with efficient Q-function approximation methods ( as used in Minh et al. , Nature , 2015 , for example ) instead of tabular Q-learning , for exploring larger state-action spaces ."}, {"review_id": "S1c2cvqee-1", "review_text": "The paper looks solid and the idea is natural. Results seem promising as well. I am mostly concerned about the computational cost of the method. 8-10 days on 10 GPUs for relatively tiny datasets is quite prohibitive for most applications I would ever encounter. I think the main question is how this approach scales to larger images and also when applied to more exotic and possibly tiny datasets. Can you run an experiment on Caltech-101 for instance? I would be very curious to see if your approach is suitable for the low-data regime and areas where we all do not know right away how a suitable architecture looks like. For Cifar-10/100, MNIST and SVHN, everyone knows very well what a reasonable model initialization looks like. If you show proof that you can discover a competitive architecture for something like Caltech-101, I would recommend the paper for publication. Minor: - ResNets should be mentioned in Table ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review ! Computational cost : The computational cost of our current implementation might be prohibitive for a single researcher with minimal resources , but it is still within the reach of academic or industry research groups . In the foreseeable future when the increase in deep learning computational resources may outstrip the number deep learning engineers/researchers , we believe that metamodeling methods for deep learning will become extremely useful . We are also exploring efficient Q-function approximation methods ( as used in Minh et al. , Nature , 2015 , for example ) to reduce the computational cost . Performance on tiny datasets and larger images : During the rebuttal period , we conducted two experiments to demonstrate that our method is applicable to tiny datasets and larger images . First , we used our method to discover network architectures using 10 % of the SVHN dataset ( containing only 7000 training images ) and identified a network architecture with 87.9 % validation accuracy , which is only 7.5 % less than the best model from the full SVHN experiment ( both with short training schedules ) . Second , we ran an experiment with the Caltech-101 dataset , according to your suggestion , using the same state-action space as the SVHN experiment . Our best model ( a three-layer network ) obtained an accuracy of 29.64 % over 10 train-test splits when training from scratch on 224x224 size images . We compared our results with Alexnet ( trained from scratch with the procedure described by Zeiler and Fergus , ECCV 2014 ) , which obtained an average accuracy of 15.85 % over the same train-test splits . Models for standard datasets : You mention that \u201c For Cifar-10/100 , MNIST and SVHN , everyone knows very well what a reasonable model initialization looks like. \u201d However , it \u2019 s worth noting that the top model architectures found by our method ( Tables A1-A3 ) are quite dissimilar to the hand-crafted networks found in literature . Human designers should be able to gain insights into the network design process by analyzing metamodeling methods . Moreover , metamodeling methods can generate several top-performing models with varied architectures , which is not practical for human design . Resnet results : Table 4 includes published resnet results . We will be happy to include latest unpublished versions of resnet in our updated manuscript ."}, {"review_id": "S1c2cvqee-2", "review_text": "Authors learn deep architectures on a few small vision problems using Q-learning and obtain solid results, SOTA results when limiting to certain types of layers and competitive against everything else. It would be good to know how well this performs when allowing more complex structures. Paper would be much more convincing on a real-size task such as ImageNet.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review ! Due to our limited computation resources we were not able to perform an experiment on a much larger dataset such as Imagenet or with more complex structures . We would like to stress , however , that this is a limitation of our current hardware setup but not a limitation of the method overall ."}], "0": {"review_id": "S1c2cvqee-0", "review_text": "This paper introduces a reinforcement learning framework for designing a neural network architecture. For each time-step, the agent picks a new layer type with corresponding layer parameters (e.g., #filters). In order to reduce the size of state-action space, they used a small set of design choices. Strengths: - A novel approach for automatic design of neural network architectures. - Shows quite promising results on several datasets (MNIST, CIFAR-10). Weakness: - Limited architecture design choices due to many prior assumptions (e.g., a set of possible number of convolution filters, at most 2 fully-connected layers, maximum depth, hard-coded dropout, etc.) - The method is demonstrated in tabular Q-learning setting, but it is unclear whether the proposed method would work in a large state-action space. Overall, this is an interesting and novel approach for neural network architecture design, and it seems to be worth publication despite some weaknesses.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review ! Limited architecture design choices : We agree that our current implementation had the weakness of somewhat limited architecture space , which we chose to constrain due to our limited computation resources . We would like to stress , however , that this is a limitation of our current hardware setup but not a limitation of the method overall . Small state-action space and tabular Q-learning : While our current state-action space size is fairly small -- -4296 state-action pairs -- -we do vary the station-action space size between experiments and still obtain consistent results . For both SVHN/MNIST experiments ( with state-action space size 4296 ) and the larger CIFAR-10 experiment ( with state-action space size 6834 ) , our meta-modeling method was able to progressively improve neural network architectures . Moreover , we plan to experiment with efficient Q-function approximation methods ( as used in Minh et al. , Nature , 2015 , for example ) instead of tabular Q-learning , for exploring larger state-action spaces ."}, "1": {"review_id": "S1c2cvqee-1", "review_text": "The paper looks solid and the idea is natural. Results seem promising as well. I am mostly concerned about the computational cost of the method. 8-10 days on 10 GPUs for relatively tiny datasets is quite prohibitive for most applications I would ever encounter. I think the main question is how this approach scales to larger images and also when applied to more exotic and possibly tiny datasets. Can you run an experiment on Caltech-101 for instance? I would be very curious to see if your approach is suitable for the low-data regime and areas where we all do not know right away how a suitable architecture looks like. For Cifar-10/100, MNIST and SVHN, everyone knows very well what a reasonable model initialization looks like. If you show proof that you can discover a competitive architecture for something like Caltech-101, I would recommend the paper for publication. Minor: - ResNets should be mentioned in Table ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review ! Computational cost : The computational cost of our current implementation might be prohibitive for a single researcher with minimal resources , but it is still within the reach of academic or industry research groups . In the foreseeable future when the increase in deep learning computational resources may outstrip the number deep learning engineers/researchers , we believe that metamodeling methods for deep learning will become extremely useful . We are also exploring efficient Q-function approximation methods ( as used in Minh et al. , Nature , 2015 , for example ) to reduce the computational cost . Performance on tiny datasets and larger images : During the rebuttal period , we conducted two experiments to demonstrate that our method is applicable to tiny datasets and larger images . First , we used our method to discover network architectures using 10 % of the SVHN dataset ( containing only 7000 training images ) and identified a network architecture with 87.9 % validation accuracy , which is only 7.5 % less than the best model from the full SVHN experiment ( both with short training schedules ) . Second , we ran an experiment with the Caltech-101 dataset , according to your suggestion , using the same state-action space as the SVHN experiment . Our best model ( a three-layer network ) obtained an accuracy of 29.64 % over 10 train-test splits when training from scratch on 224x224 size images . We compared our results with Alexnet ( trained from scratch with the procedure described by Zeiler and Fergus , ECCV 2014 ) , which obtained an average accuracy of 15.85 % over the same train-test splits . Models for standard datasets : You mention that \u201c For Cifar-10/100 , MNIST and SVHN , everyone knows very well what a reasonable model initialization looks like. \u201d However , it \u2019 s worth noting that the top model architectures found by our method ( Tables A1-A3 ) are quite dissimilar to the hand-crafted networks found in literature . Human designers should be able to gain insights into the network design process by analyzing metamodeling methods . Moreover , metamodeling methods can generate several top-performing models with varied architectures , which is not practical for human design . Resnet results : Table 4 includes published resnet results . We will be happy to include latest unpublished versions of resnet in our updated manuscript ."}, "2": {"review_id": "S1c2cvqee-2", "review_text": "Authors learn deep architectures on a few small vision problems using Q-learning and obtain solid results, SOTA results when limiting to certain types of layers and competitive against everything else. It would be good to know how well this performs when allowing more complex structures. Paper would be much more convincing on a real-size task such as ImageNet.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review ! Due to our limited computation resources we were not able to perform an experiment on a much larger dataset such as Imagenet or with more complex structures . We would like to stress , however , that this is a limitation of our current hardware setup but not a limitation of the method overall ."}}