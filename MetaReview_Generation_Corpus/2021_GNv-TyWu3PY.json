{"year": "2021", "forum": "GNv-TyWu3PY", "title": "Robust Learning for Congestion-Aware Routing", "decision": "Reject", "meta_review": "The paper proposes an algorithm with sublinear regret for the problem of routing users through a network with unknown congestion functions over an infinite time horizon. The reviewers generally appreciated the main contribution of this work. One of the reviewers also felt that, although it may be possible to obtain the main result using more standard techniques, it is not clear whether doing so is an easy extension of the prior work. Following the discussion, all of the reviewers agreed that the paper missed important related work and it needs a major revision that incorporates the extensive feedback of Reviewer 2. For these reasons, I recommend reject.", "reviews": [{"review_id": "GNv-TyWu3PY-0", "review_text": "In this paper the authors study the problem of routing users according to their requests through a network with unknown congestion functions over infinite time horizon . They model the problem as follows . A directed graph G ( V , E ) is given , where each edge is associated with a congestion function f_e that maps the flow along edge e to some positive real . The mild assumption the authors make on the congestion functions of all edges is that they are L-Lipschitz . At every timetick a car enters the routing app , and asks to move from a source to destination . Given the collection of paths , the routing app is required to make a choice of a path . This choice incurs a cost , which is the noisy version of the sum of the congestions along the edges of the path Again , the author ( s ) make the reasonable assumption that the noise is zero mean and bounded by some value beta . The model is nicely motivated by real-world aspects of routing apps , and is a clean mathematical model . The key result is stated as Theorem 1.1 . I checked the proof , and it appears solid . The authors design algorithm 1 . ITs intuition is well described in section 2.1 Some comments to the authors of the paper follow - Is the Lipschitz constant known ? If not , is there a way to test this assumption ? - I think the authors mean that the regret R_t is \\sum_ { r=1 } ^t E [ c_r-c * _r ] . - Can you elaborate on your conjecture ( e.g. , page 3 ) concerning the right asymptotics of the regret as a function of t ? Is your conjecture related to the Awerbuch-Kleinberg SODA paper ? - Is the dependence of |E| tight ? It felt while reading the proof ( e.g. , in Lemma A.7 where the summation over time and edges along the path are exchanged ) that perhaps the analysis could be improved . - The experimental part is the weakest part in this paper . I would urge the authors to try settings where the assumptions of the theorem start breaking down , and see how the regret changes . For instance , what if some congestion functions are not L-Lipschitz but all the rest are . What if these functions correspond to edges with high betweenness centrality ? One can think of many other settings that could have made the paper ( including the supplementary material ) more interesting Overall this is a well-written paper with a clear contribution . The weak parts are the experiments which are practically sanity check for the validity of theorem 1.1 , and the fact that from a technical point of view there is not much novelty . The novelty in my opinion lies in the design of the estimation algorithm of the cost . [ Score updated From 6 to 5 ]", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the helpful comments and questions . We respond to the reviewer \u2019 s comments one by one . > Is the Lipschitz constant known ? If not , is there a way to test this assumption ? Yes , the Lipschitz constant must be known to the algorithm . Following the reviewer \u2019 s suggestion , we have run some experiments where the Lipschitz assumption is violated , i.e. , where some congestion functions have Lipschitz constants larger than the Lipschitz constant used by the algorithm . Theoretically , this can result in some bad worst-case scenarios ( i.e. , linear regret in the worst case ) , but it seems to improve the experimental performance . Our current hypothesis is that this essentially sacrifices worst-case performance to improve average-case performance . To elaborate , the reason the algorithm needs to know the Lipschitz constant is in order to make sure that our cost estimates are always underestimates with high probability . This ensures that we don \u2019 t accidentally ignore a good edge indefinitely . ( The idea is similar to the standard \u201c optimism in the face of uncertainty \u201d principle . ) When the Lipschitz assumption is violated ( essentially , the algorithm is using too small of a Lipschitz constant ) , the estimates will not always be underestimates , but will on average be closer to the true value , improving \u201c average case \u201d performance . It could be interesting to theoretically analyze the \u201c average case \u201d scenario ( e.g. , if flows are drawn randomly instead of adversarially ) . We will mention this as an open question in the next revision of the paper . > I think the authors mean that the regret $ R_t $ is $ \\sum_ { r=1 } ^t E [ c_r-c * _r ] $ . This is correct , thank you . > Can you elaborate on your conjecture ( e.g. , page 3 ) concerning the right asymptotics of the regret as a function of t ? Is your conjecture related to the Awerbuch-Kleinberg SODA paper ? We conjecture that it is impossible to achieve regret $ \\tilde { O } ( \\sqrt { n } ) $ for our problem . The conjecture is related to the Awerbuch-Kleinberg paper in a conceptual sense , but the model is mathematically different , so their results do not directly apply . To elaborate on the conjecture , reviewer 4 notes that \u201c When f_e is a constant function for each edge , then we recover a version of the stochastic combinatorial semi-bandit. \u201d In that case , $ \\tilde { O } ( \\sqrt { n } ) $ is known to be the best possible , and when estimating the average cost of an arm ( i.e. , an edge ) , one can simply use all the samples . However , when $ f_e $ is not constant , there is a tension between wanting to use many samples ( in order to decrease the effect of noise ) and wanting to only use samples y that are close to the target flow x ( so that $ |f_e ( y ) - f_e ( x ) | $ is small ) . We conjecture that this tension inevitably increases the dependence on T. > Is the dependence of $ |E| $ tight ? It felt while reading the proof ( e.g. , in Lemma A.7 where the summation over time and edges along the path are exchanged ) that perhaps the analysis could be improved . We do not have a tight lower bound for the dependence on $ |E| $ and we also conjecture that the linear dependence is not tight ( at least not for all graphs ) . We also do not have a lower bound for the dependence on $ \\beta $ , although we conjecture that the linear dependence on $ \\beta $ _is_ tight ."}, {"review_id": "GNv-TyWu3PY-1", "review_text": "In this submission a routing problem is studied . In the considered model with each edge of the given graph a congestion function is associated that specifies the congestion depending on the current load of the edge . Then cars have to be routed through the network where each car has a source and a destination and one aims at choosing a path from the source to the destination with the smallest total congestion . However , the congestion functions of the edges are a priori unknown and hence one can not trivially use a shortest path algorithm . Instead one gains information about the congestion functions only by routing the cars . When a car is routed one observes for each edge on its path the current congestion up to some random additive term . These observations can then be used for future routing decisions . The main result of the submission is an algorithm that achieves a cumulative sublinear regret of O ( |E|t^ { 2/3 } ) where the regret is defined as the difference between the expected path length chosen by the algorithm and the length of the shortest path . Some experiments are also conducted with this algorithm but the focus of the submission is clearly on the theoretical results . The algorithm itself is non-trivial but also not too surprising . Due to the random noise one needs enough samples to estimate the congestion on an edge . The difficulty is that the congestion depends on the current load , i.e. , one needs enough samples close to the current load ( here one uses that the congestion functions are assumed to be Lipschitz continuous ) . The algorithm cleverly and adaptively partitions the samples of an edge into buckets where each bucket represents a certain range of loads . Then there are two factors that determine the precision of the congestion estimation : the range of the buckets and the number of samples per bucket . The more samples one has in a certain range the smaller can the ranges of the buckets be . Basically the algorithm splits a bucket if it contains already enough samples . I find the results interesting . The proposed algorithm is natural and its analysis is non-trivial . I am not completely sure if the model is very realistic though . It is assumed that the congestion functions are unknown ( which makes sense ) . However , if I understand it correctly it is assumed that each driver knows exactly the current load on all the edges . It is not clear to me why this makes sense . It might also be the other way round : While the congestion functions are more or less known from historic data , the current load is unknown to the drivers . In my opinion the authors could discuss the model in more detail .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the constructive response . The reviewer \u2019 s main concern is the following : > I am not completely sure if the model is very realistic though . It is assumed that the congestion functions are unknown ( which makes sense ) . However , if I understand it correctly it is assumed that each driver knows exactly the current load on all the edges . It is not clear to me why this makes sense . It might also be the other way round : While the congestion functions are more or less known from historic data , the current load is unknown to the drivers . In my opinion the authors could discuss the model in more detail . From the viewpoint of a navigation app , the location of each car using the app is known , and thus the number of cars on an edge that are using the app is known . Of course , not every car is using the app , but if the app has a good estimate of the percentage of cars that tend to use it , a good estimate of the current load should still be possible . While our model does assume that the algorithm knows exactly the current load on each edge , the bucketing approach of our algorithm should be robust to small inaccuracies in the load . Even if the inaccuracy leads to the algorithm using a different bucket , this error decreases as the length of each bucket shrinks . Inspired by this reviewer 's concern , we have run additional experiments where the algorithm only sees a noisy version of the load ( with the optimal algorithm still seeing the exact load ) . The results indicate that the algorithm is indeed robust to inaccuracy in the load , at least empirically . Specifically , we see a sublinear regret curve similar to the results of the original experiments ( where the algorithm always saw the exact load ) . Supporting our observations with theoretical analysis is an interesting direction for future work ."}, {"review_id": "GNv-TyWu3PY-2", "review_text": "This work introduces an interesting generalization of stochastic combinatorial semi-bandits for routing in a static graph . The main differences are : ( 1 ) the expected loss of an edge e is f_e ( x^t_e ) where the flow x^t_e is revealed at the beginning of each round ( for each edge ) and f_e is an unknown Lipschitz function ( with known Lipschitz constant ) ; ( 2 ) the regret is dynamic , computed against the sequence of optimal paths . When f_e is a constant function for each edge , then we recover a version of the stochastic combinatorial semi-bandit . The main contribution is a novel UCB-like algorithm with a dynamic regret bound after T steps of |E|T^ { 2/3 } ( ignoring log factors in T ) . This is larger than the rate |E|T^ { 1/2 } achievable for * adversarial * combinatorial semi-bandits , but as we said the problem studied here is more general . The algorithm uses a hierarchical and dynamical bin structure to produce convergent estimates of f_e ( x ) for different values of x . This is a nice idea which is not standard in the bandit literature . The analysis of the algorithm is quite involved and apparently novel for the most part . The main ideas behind the analysis are well explained at an intuitive level . The source of the T^ { 2/3 } dependence should be independent of the combinatorial nature of the semi-bandit problem . It would be interesting to know what happens in the simpler setting of parallel edges . Can the upper bound be improved ? And if not , can a tight lower bound be proven ? The definition of x_ { max } ^t on page 2 looks wrong because the max is over t .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the constructive comments and interesting ideas for extending our work which we will take into account while preparing the next version of our paper ."}, {"review_id": "GNv-TyWu3PY-3", "review_text": "The paper uses the bandit learning framework to study the online learning problem for routing in a city network . After each routing decision , the learning agent observes the actual delay on each edge , which is given by the congestion function on the given flow plus a random noise , and the reward is the total delay on all edges . The paper proposes a learning algorithm similar to the UCB approach , provide the regret bound result , and conduct simulations on the New York City network to verify performance of the algorithm . The contribution of the paper in my view is mainly on the setting where the nonlinear congestion function ( satisfying a Lipschitz condition ) with arbitrarily given flow is considered , and on using dynamic splitting to refine the budgets based on the number of observations . However , there are a number of issues in the paper that makes the overall contribution questionable , as I list below . - The first issue is that the authors is missing an entire line of very relevant result work and results . In particular , the work is closely related to the combinatorial semi-bandit research . The following are several most relevant studies , while many others are available in the literature . [ 1 ] Chen , Wang , Yuan , and Wang . Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms . JMLR'2016 , Conference version appeared in ICML'2013 [ 2 ] Qin , Chen , and Zhu . Contextual Combinatorial Bandit and its Application on Diversified Online Recommendation . SDM'2014 The routing problem in the current paper has a number of similarities with the CMAB framework in [ 1 ] : each road segment ( edge ) is a base arm , and it can be individually observed when the selected path contains the edge semi-bandit feedback , and each action is a set of base arms ( a path ) that follows certain constraints . The adversarial chosen path source and destination and the current flow can be viewed as the context in CMAB . The contextual CMAB problem is studied in [ 2 ] . The current paper has a bit more complication due to the congestion function . But the authors treat it by discretizing the flow into buckets . Essentially this is treating the pair of ( edge , flow budget ) as a base arm in the CMAB framework . Therefore , I believe that if we do not do dynamic bucketing and use a fixed discretization budget , the current problem fits as a special instance in the CMAB framework , and thus can be solved by the CUCB algorithm for CMAB . Noticeably the CUCB algorithm has O ( \\sqrt { t } ) regret ( ignoring the additional log t term ) . However , the current paper only has a result for O ( t^ { 2/3 } ) , and the authors mention both in the introduction and conclusion that achieving O ( \\sqrt { t } ) regret as a future research work . But with proper setup as the above , I believe the O ( \\sqrt { t } ) regret has been achieved . Therefore the authors are seriously missing some existing work in this regard . Of course , the above discussion only uses static buckets . But applying dynamic buckets should only improve the results . The static bucketing may give some extra factor in the regret bound in terms of the number of budgets , but that is independent of time t. Therefore , I believe the dependency on time t should be O ( \\sqrt { t } ) , and it should be easily achieved under the CMAB framework , such as using the CUCB algorithm in [ 1 ] . - The second issue is that in terms of the theoretical analysis , the authors only provide the regret bound result as a theorem in the introduction . There is no more discussion on the factors related to the regret bound , such as |E| , and \\beta . Are their dependency tight or not ? There is also no explanation on the outline of the analysis . In particular , how to incorporate the dynamic splitting of the buckets into the analysis . Dynamic splitting is the only thing that is different from existing work in my view , and its advantage should be further discussed . - The third issue is on the experimental evaluation . The authors do not compare the proposed algorithm with any baseline algorithms . Baseline algorithms that could be considered include epsilon-greedy algorithms and Thompson Sampling based algorithms . Without such comparison , it is hard to understand the benefit of the proposed algorithm . - Another issue is that the proposed algorithm is essentially very close to using the lower confidence bound ( LCB ) for the edge delays in the CMAB setting . Since the optimization problem is minimizing the delay , so it is understandable that the standard UCB is replaced by the LCB . The authors need to discuss whether there is any more difference between their algorithm and the LCB/UCB based algorithms .", "rating": "3: Clear rejection", "reply_text": "# # Responses to the reviewer 's other comments \u201c The second issue is that in terms of the theoretical analysis , the authors only provide the regret bound result as a theorem in the introduction . There is no more discussion on the factors related to the regret bound , such as $ |E| $ , and $ \\beta $ . Is their dependency tight or not ? \u201d Please see the response to Reviewer 1 on this topic . > There is also no explanation on the outline of the analysis . In particular , how to incorporate the dynamic splitting of the buckets into the analysis . Dynamic splitting is the only thing that is different from existing work in my view , and its advantage should be further discussed. \u201d Space constraints limited our ability to provide an outline of the analysis in the main body . Sections 2.1 and 2.2 do provide some intuition about this analysis , but we plan to further emphasize dynamic splitting in the next revision . For example , we will discuss how dynamic splitting gives us an increasingly fine-grained discretization of the flow , leading to perfect cost estimation in the limit . Dynamic splitting also allows us to carefully balance the number of samples in a bucket with the length of the bucket ( i.e. , the range of flows it covers ) , which is crucial for achieving sublinear regret . > The third issue is on the experimental evaluation . The authors do not compare the proposed algorithm with any baseline algorithms . Baseline algorithms that could be considered include epsilon-greedy algorithms and Thompson Sampling based algorithms . Without such comparison , it is hard to understand the benefit of the proposed algorithm . Since our work is not a special instance of CMAB , we are not aware of any other algorithms that yield sublinear regret for our problem . Standard CMAB algorithms like epsilon-greedy and Thompson sampling would lead to linear regret in our model , since they essentially treat the congestion functions as constant . Consequently , it is not clear if any prior algorithms would be appropriate for an experimental baseline . > Another issue is that the proposed algorithm is essentially very close to using the lower confidence bound ( LCB ) for the edge delays in the CMAB setting . Since the optimization problem is minimizing the delay , so it is understandable that the standard UCB is replaced by the LCB . The authors need to discuss whether there is any more difference between their algorithm and the LCB/UCB based algorithms . As the reviewer noted earlier , the crucial distinction here is the bucketing approach , and in particular , the dynamic splitting . This allows us to handle the parameterization of the congestion function , as discussed above . We will make this comparison clear in the paper ."}], "0": {"review_id": "GNv-TyWu3PY-0", "review_text": "In this paper the authors study the problem of routing users according to their requests through a network with unknown congestion functions over infinite time horizon . They model the problem as follows . A directed graph G ( V , E ) is given , where each edge is associated with a congestion function f_e that maps the flow along edge e to some positive real . The mild assumption the authors make on the congestion functions of all edges is that they are L-Lipschitz . At every timetick a car enters the routing app , and asks to move from a source to destination . Given the collection of paths , the routing app is required to make a choice of a path . This choice incurs a cost , which is the noisy version of the sum of the congestions along the edges of the path Again , the author ( s ) make the reasonable assumption that the noise is zero mean and bounded by some value beta . The model is nicely motivated by real-world aspects of routing apps , and is a clean mathematical model . The key result is stated as Theorem 1.1 . I checked the proof , and it appears solid . The authors design algorithm 1 . ITs intuition is well described in section 2.1 Some comments to the authors of the paper follow - Is the Lipschitz constant known ? If not , is there a way to test this assumption ? - I think the authors mean that the regret R_t is \\sum_ { r=1 } ^t E [ c_r-c * _r ] . - Can you elaborate on your conjecture ( e.g. , page 3 ) concerning the right asymptotics of the regret as a function of t ? Is your conjecture related to the Awerbuch-Kleinberg SODA paper ? - Is the dependence of |E| tight ? It felt while reading the proof ( e.g. , in Lemma A.7 where the summation over time and edges along the path are exchanged ) that perhaps the analysis could be improved . - The experimental part is the weakest part in this paper . I would urge the authors to try settings where the assumptions of the theorem start breaking down , and see how the regret changes . For instance , what if some congestion functions are not L-Lipschitz but all the rest are . What if these functions correspond to edges with high betweenness centrality ? One can think of many other settings that could have made the paper ( including the supplementary material ) more interesting Overall this is a well-written paper with a clear contribution . The weak parts are the experiments which are practically sanity check for the validity of theorem 1.1 , and the fact that from a technical point of view there is not much novelty . The novelty in my opinion lies in the design of the estimation algorithm of the cost . [ Score updated From 6 to 5 ]", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the helpful comments and questions . We respond to the reviewer \u2019 s comments one by one . > Is the Lipschitz constant known ? If not , is there a way to test this assumption ? Yes , the Lipschitz constant must be known to the algorithm . Following the reviewer \u2019 s suggestion , we have run some experiments where the Lipschitz assumption is violated , i.e. , where some congestion functions have Lipschitz constants larger than the Lipschitz constant used by the algorithm . Theoretically , this can result in some bad worst-case scenarios ( i.e. , linear regret in the worst case ) , but it seems to improve the experimental performance . Our current hypothesis is that this essentially sacrifices worst-case performance to improve average-case performance . To elaborate , the reason the algorithm needs to know the Lipschitz constant is in order to make sure that our cost estimates are always underestimates with high probability . This ensures that we don \u2019 t accidentally ignore a good edge indefinitely . ( The idea is similar to the standard \u201c optimism in the face of uncertainty \u201d principle . ) When the Lipschitz assumption is violated ( essentially , the algorithm is using too small of a Lipschitz constant ) , the estimates will not always be underestimates , but will on average be closer to the true value , improving \u201c average case \u201d performance . It could be interesting to theoretically analyze the \u201c average case \u201d scenario ( e.g. , if flows are drawn randomly instead of adversarially ) . We will mention this as an open question in the next revision of the paper . > I think the authors mean that the regret $ R_t $ is $ \\sum_ { r=1 } ^t E [ c_r-c * _r ] $ . This is correct , thank you . > Can you elaborate on your conjecture ( e.g. , page 3 ) concerning the right asymptotics of the regret as a function of t ? Is your conjecture related to the Awerbuch-Kleinberg SODA paper ? We conjecture that it is impossible to achieve regret $ \\tilde { O } ( \\sqrt { n } ) $ for our problem . The conjecture is related to the Awerbuch-Kleinberg paper in a conceptual sense , but the model is mathematically different , so their results do not directly apply . To elaborate on the conjecture , reviewer 4 notes that \u201c When f_e is a constant function for each edge , then we recover a version of the stochastic combinatorial semi-bandit. \u201d In that case , $ \\tilde { O } ( \\sqrt { n } ) $ is known to be the best possible , and when estimating the average cost of an arm ( i.e. , an edge ) , one can simply use all the samples . However , when $ f_e $ is not constant , there is a tension between wanting to use many samples ( in order to decrease the effect of noise ) and wanting to only use samples y that are close to the target flow x ( so that $ |f_e ( y ) - f_e ( x ) | $ is small ) . We conjecture that this tension inevitably increases the dependence on T. > Is the dependence of $ |E| $ tight ? It felt while reading the proof ( e.g. , in Lemma A.7 where the summation over time and edges along the path are exchanged ) that perhaps the analysis could be improved . We do not have a tight lower bound for the dependence on $ |E| $ and we also conjecture that the linear dependence is not tight ( at least not for all graphs ) . We also do not have a lower bound for the dependence on $ \\beta $ , although we conjecture that the linear dependence on $ \\beta $ _is_ tight ."}, "1": {"review_id": "GNv-TyWu3PY-1", "review_text": "In this submission a routing problem is studied . In the considered model with each edge of the given graph a congestion function is associated that specifies the congestion depending on the current load of the edge . Then cars have to be routed through the network where each car has a source and a destination and one aims at choosing a path from the source to the destination with the smallest total congestion . However , the congestion functions of the edges are a priori unknown and hence one can not trivially use a shortest path algorithm . Instead one gains information about the congestion functions only by routing the cars . When a car is routed one observes for each edge on its path the current congestion up to some random additive term . These observations can then be used for future routing decisions . The main result of the submission is an algorithm that achieves a cumulative sublinear regret of O ( |E|t^ { 2/3 } ) where the regret is defined as the difference between the expected path length chosen by the algorithm and the length of the shortest path . Some experiments are also conducted with this algorithm but the focus of the submission is clearly on the theoretical results . The algorithm itself is non-trivial but also not too surprising . Due to the random noise one needs enough samples to estimate the congestion on an edge . The difficulty is that the congestion depends on the current load , i.e. , one needs enough samples close to the current load ( here one uses that the congestion functions are assumed to be Lipschitz continuous ) . The algorithm cleverly and adaptively partitions the samples of an edge into buckets where each bucket represents a certain range of loads . Then there are two factors that determine the precision of the congestion estimation : the range of the buckets and the number of samples per bucket . The more samples one has in a certain range the smaller can the ranges of the buckets be . Basically the algorithm splits a bucket if it contains already enough samples . I find the results interesting . The proposed algorithm is natural and its analysis is non-trivial . I am not completely sure if the model is very realistic though . It is assumed that the congestion functions are unknown ( which makes sense ) . However , if I understand it correctly it is assumed that each driver knows exactly the current load on all the edges . It is not clear to me why this makes sense . It might also be the other way round : While the congestion functions are more or less known from historic data , the current load is unknown to the drivers . In my opinion the authors could discuss the model in more detail .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the constructive response . The reviewer \u2019 s main concern is the following : > I am not completely sure if the model is very realistic though . It is assumed that the congestion functions are unknown ( which makes sense ) . However , if I understand it correctly it is assumed that each driver knows exactly the current load on all the edges . It is not clear to me why this makes sense . It might also be the other way round : While the congestion functions are more or less known from historic data , the current load is unknown to the drivers . In my opinion the authors could discuss the model in more detail . From the viewpoint of a navigation app , the location of each car using the app is known , and thus the number of cars on an edge that are using the app is known . Of course , not every car is using the app , but if the app has a good estimate of the percentage of cars that tend to use it , a good estimate of the current load should still be possible . While our model does assume that the algorithm knows exactly the current load on each edge , the bucketing approach of our algorithm should be robust to small inaccuracies in the load . Even if the inaccuracy leads to the algorithm using a different bucket , this error decreases as the length of each bucket shrinks . Inspired by this reviewer 's concern , we have run additional experiments where the algorithm only sees a noisy version of the load ( with the optimal algorithm still seeing the exact load ) . The results indicate that the algorithm is indeed robust to inaccuracy in the load , at least empirically . Specifically , we see a sublinear regret curve similar to the results of the original experiments ( where the algorithm always saw the exact load ) . Supporting our observations with theoretical analysis is an interesting direction for future work ."}, "2": {"review_id": "GNv-TyWu3PY-2", "review_text": "This work introduces an interesting generalization of stochastic combinatorial semi-bandits for routing in a static graph . The main differences are : ( 1 ) the expected loss of an edge e is f_e ( x^t_e ) where the flow x^t_e is revealed at the beginning of each round ( for each edge ) and f_e is an unknown Lipschitz function ( with known Lipschitz constant ) ; ( 2 ) the regret is dynamic , computed against the sequence of optimal paths . When f_e is a constant function for each edge , then we recover a version of the stochastic combinatorial semi-bandit . The main contribution is a novel UCB-like algorithm with a dynamic regret bound after T steps of |E|T^ { 2/3 } ( ignoring log factors in T ) . This is larger than the rate |E|T^ { 1/2 } achievable for * adversarial * combinatorial semi-bandits , but as we said the problem studied here is more general . The algorithm uses a hierarchical and dynamical bin structure to produce convergent estimates of f_e ( x ) for different values of x . This is a nice idea which is not standard in the bandit literature . The analysis of the algorithm is quite involved and apparently novel for the most part . The main ideas behind the analysis are well explained at an intuitive level . The source of the T^ { 2/3 } dependence should be independent of the combinatorial nature of the semi-bandit problem . It would be interesting to know what happens in the simpler setting of parallel edges . Can the upper bound be improved ? And if not , can a tight lower bound be proven ? The definition of x_ { max } ^t on page 2 looks wrong because the max is over t .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the constructive comments and interesting ideas for extending our work which we will take into account while preparing the next version of our paper ."}, "3": {"review_id": "GNv-TyWu3PY-3", "review_text": "The paper uses the bandit learning framework to study the online learning problem for routing in a city network . After each routing decision , the learning agent observes the actual delay on each edge , which is given by the congestion function on the given flow plus a random noise , and the reward is the total delay on all edges . The paper proposes a learning algorithm similar to the UCB approach , provide the regret bound result , and conduct simulations on the New York City network to verify performance of the algorithm . The contribution of the paper in my view is mainly on the setting where the nonlinear congestion function ( satisfying a Lipschitz condition ) with arbitrarily given flow is considered , and on using dynamic splitting to refine the budgets based on the number of observations . However , there are a number of issues in the paper that makes the overall contribution questionable , as I list below . - The first issue is that the authors is missing an entire line of very relevant result work and results . In particular , the work is closely related to the combinatorial semi-bandit research . The following are several most relevant studies , while many others are available in the literature . [ 1 ] Chen , Wang , Yuan , and Wang . Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms . JMLR'2016 , Conference version appeared in ICML'2013 [ 2 ] Qin , Chen , and Zhu . Contextual Combinatorial Bandit and its Application on Diversified Online Recommendation . SDM'2014 The routing problem in the current paper has a number of similarities with the CMAB framework in [ 1 ] : each road segment ( edge ) is a base arm , and it can be individually observed when the selected path contains the edge semi-bandit feedback , and each action is a set of base arms ( a path ) that follows certain constraints . The adversarial chosen path source and destination and the current flow can be viewed as the context in CMAB . The contextual CMAB problem is studied in [ 2 ] . The current paper has a bit more complication due to the congestion function . But the authors treat it by discretizing the flow into buckets . Essentially this is treating the pair of ( edge , flow budget ) as a base arm in the CMAB framework . Therefore , I believe that if we do not do dynamic bucketing and use a fixed discretization budget , the current problem fits as a special instance in the CMAB framework , and thus can be solved by the CUCB algorithm for CMAB . Noticeably the CUCB algorithm has O ( \\sqrt { t } ) regret ( ignoring the additional log t term ) . However , the current paper only has a result for O ( t^ { 2/3 } ) , and the authors mention both in the introduction and conclusion that achieving O ( \\sqrt { t } ) regret as a future research work . But with proper setup as the above , I believe the O ( \\sqrt { t } ) regret has been achieved . Therefore the authors are seriously missing some existing work in this regard . Of course , the above discussion only uses static buckets . But applying dynamic buckets should only improve the results . The static bucketing may give some extra factor in the regret bound in terms of the number of budgets , but that is independent of time t. Therefore , I believe the dependency on time t should be O ( \\sqrt { t } ) , and it should be easily achieved under the CMAB framework , such as using the CUCB algorithm in [ 1 ] . - The second issue is that in terms of the theoretical analysis , the authors only provide the regret bound result as a theorem in the introduction . There is no more discussion on the factors related to the regret bound , such as |E| , and \\beta . Are their dependency tight or not ? There is also no explanation on the outline of the analysis . In particular , how to incorporate the dynamic splitting of the buckets into the analysis . Dynamic splitting is the only thing that is different from existing work in my view , and its advantage should be further discussed . - The third issue is on the experimental evaluation . The authors do not compare the proposed algorithm with any baseline algorithms . Baseline algorithms that could be considered include epsilon-greedy algorithms and Thompson Sampling based algorithms . Without such comparison , it is hard to understand the benefit of the proposed algorithm . - Another issue is that the proposed algorithm is essentially very close to using the lower confidence bound ( LCB ) for the edge delays in the CMAB setting . Since the optimization problem is minimizing the delay , so it is understandable that the standard UCB is replaced by the LCB . The authors need to discuss whether there is any more difference between their algorithm and the LCB/UCB based algorithms .", "rating": "3: Clear rejection", "reply_text": "# # Responses to the reviewer 's other comments \u201c The second issue is that in terms of the theoretical analysis , the authors only provide the regret bound result as a theorem in the introduction . There is no more discussion on the factors related to the regret bound , such as $ |E| $ , and $ \\beta $ . Is their dependency tight or not ? \u201d Please see the response to Reviewer 1 on this topic . > There is also no explanation on the outline of the analysis . In particular , how to incorporate the dynamic splitting of the buckets into the analysis . Dynamic splitting is the only thing that is different from existing work in my view , and its advantage should be further discussed. \u201d Space constraints limited our ability to provide an outline of the analysis in the main body . Sections 2.1 and 2.2 do provide some intuition about this analysis , but we plan to further emphasize dynamic splitting in the next revision . For example , we will discuss how dynamic splitting gives us an increasingly fine-grained discretization of the flow , leading to perfect cost estimation in the limit . Dynamic splitting also allows us to carefully balance the number of samples in a bucket with the length of the bucket ( i.e. , the range of flows it covers ) , which is crucial for achieving sublinear regret . > The third issue is on the experimental evaluation . The authors do not compare the proposed algorithm with any baseline algorithms . Baseline algorithms that could be considered include epsilon-greedy algorithms and Thompson Sampling based algorithms . Without such comparison , it is hard to understand the benefit of the proposed algorithm . Since our work is not a special instance of CMAB , we are not aware of any other algorithms that yield sublinear regret for our problem . Standard CMAB algorithms like epsilon-greedy and Thompson sampling would lead to linear regret in our model , since they essentially treat the congestion functions as constant . Consequently , it is not clear if any prior algorithms would be appropriate for an experimental baseline . > Another issue is that the proposed algorithm is essentially very close to using the lower confidence bound ( LCB ) for the edge delays in the CMAB setting . Since the optimization problem is minimizing the delay , so it is understandable that the standard UCB is replaced by the LCB . The authors need to discuss whether there is any more difference between their algorithm and the LCB/UCB based algorithms . As the reviewer noted earlier , the crucial distinction here is the bucketing approach , and in particular , the dynamic splitting . This allows us to handle the parameterization of the congestion function , as discussed above . We will make this comparison clear in the paper ."}}