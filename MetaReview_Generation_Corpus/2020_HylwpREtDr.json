{"year": "2020", "forum": "HylwpREtDr", "title": "Active Learning Graph Neural Networks via Node Feature Propagation", "decision": "Reject", "meta_review": "The authors propose a method of selecting nodes to label in a graph neural network setting to reduce the loss as efficiently as possible. Building atop Sener & Savarese 2017 the authors propose an alternative distance metric and clustering algorithm. In comparison to the just mentioned work, they show that their upper bound is smaller than the previous art's upper bound. While one cannot conclude from this that their algorithm is better, at least empirically the method appears to have a advantage over state of the art.\n\nHowever, reviewers were concerned about the assumptions necessary to prove the theorem, despite the modifications made by the authors after the initial round. \n\nThe work proposes a simple estimator and shows promising results but reviewers felt improvements like reducing the number of assumptions and potentially a lower bound may greatly strengthen the paper.", "reviews": [{"review_id": "HylwpREtDr-0", "review_text": "This paper introduces active learning for graphs using graph neural networks The bound is not very meaningful as it requires unrealistic assumptions and is loose. Figure 2 shows that even random selection performs quite well compared to this elaborate method. This Area if research and the data sets don\u2019t seem to have many actual real applications in the world with much impact. .................................................................\\.\\\\........................................,,..", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the comments . [ for assumptions ] We would like to emphasize that our assumptions follow from the common settings in deep learning/active learning theory which is the general way of real data approximation . For instance , both assumptions 2 and 3 are used in the paper by Sener & Savarese ( 2017 ) [ 3 ] and Lipschitz ( assumption 2 ) is natural and common for loss functions such as hinge loss , mean squared error , and cross-entropy as long as the model output is bounded . This assumption is also widely used in deep learning theory e.g. , [ 1,2 ] . As for assumption 3 , although the constant $ \\alpha $ can be unbounded , it can be made arbitrarily small without changing the predicted labels of the network ; this is because dividing all input weights by a constant $ t $ will also divide the output by a constant $ t $ . For the other assumptions , assumption 1 assumes a zero training loss , which is a typical setting in neural networks [ 3 ] . As for assumption 4 , ReLU is activated with probability 1/2 , which is justified by the observations in practice that usually half of all the ReLU neurons can activate . As mentioned in the paper , this is a common assumption in the literature . In a related paper on graph learning [ 4 ] , the authors also assume that the ReLU activations are random . Moreover , our theoretical analysis only gives worst-case guarantees of our method , and its purpose is to justify our method against other clustering methods ( e.g. , the coreset approach , clustering the raw features , etc . ) . The advantage of our method is evident in our strong experiment results that our simple method can beat previous baselines with elaborately designed heuristics . Our method is just a clustering of transformed features , which is very easy to implement . It is much simpler than previous active graph learning methods like AGE and ANRMAB , which combine several hand-made heuristics through weighting . [ for Random in Figure 2 ] In Figure 2 , please notice that Degree , Uncertainty , Coreset are general active learning methods which can not leverage graph-based feature propagationwhile AGE , ANRMAB and our method ( FeatProp ) are graph-based active learning methods . Our method substantially outperform random sampling on all the four benchmark datasets in this paper - see Table 4 in Appendix for details . [ for application ] The main contribution of our paper is to enhance the effectiveness of active learning on graphs . The proposed method is generic and directly applicable to real-world applications where graphical data are available and labeled data are hard to acquire . For example , our methods can be used to enrich user/item representations in recommendation systems and social networks . We hope that our changes and comments can resolve your question towards our submission - and please reply if you still have further questions , and we would love to provide more details . If we resolve your questions , we are grateful if you can consider updating your review score . Thank you for your time and effort in reviewing our paper ! [ 1 ] Allen-Zhu , Z. , Li , Y. , & Song , Z . ( 2019 ) .A convergence theory for deep learning via over-parameterization . ICML 2019 . [ 2 ] Du , S. S. , & Lee , J. D. ( 2018 ) . On the power of over-parametrization in neural networks with quadratic activation . ICML 2018 . [ 3 ] Sener , O. , & Savarese , S. ( 2017 ) . Active learning for convolutional neural networks : A core-set approach . arXiv preprint arXiv:1708.00489 . [ 4 ] Xu , K. , Li , C. , Tian , Y. , Sonobe , T. , Kawarabayashi , K. I. , & Jegelka , S. ( 2018 ) . Representation Learning on Graphs with Jumping Knowledge Networks . ICML 2018"}, {"review_id": "HylwpREtDr-1", "review_text": "The authors propose to incorporate active learning into the graph neural network training and claim some guarantee on the proposed method. I have some concerns about the correctness of the proof. For theorem 1, how is the Hoeffding applied so that the \\sqrt{n} term appears? My worry is naively applying Hoeffding as is done in the proof only gives a bound on a fixed model, but in the theorem A_t is not fixed. You may need to apply a union bound or more sophisticated set cover theory to claim the result. Or if I missed something could the authors add more details on the step using Hoeffding bound to the proof? Other than that I also feel the assumptions on theorem 1 are way too strong. Especially assumption 2 and 3. They are simply not true in application. The assumptions are so strong that the theorem, even if the proof can be fixed, is not interesting any more. ", "rating": "1: Reject", "reply_text": "We thank the reviewer for the comments . About applying Hoeffding \u2019 s inequality in our proof : we \u2019 ve updated the proof to include more details for this step , and please check out the highlighted part of Appendix B . Briefly , here the randomness in applying Hoeffding \u2019 s inequality only comes from the random draws of the hidden labels , $ y_i\\sim \\eta ( i ) $ , for each unlabeled node $ i $ . This is determined after we fix the graph $ G $ and feature matrix $ X $ . Since our model $ A_0 $ does not depend on the hidden labels ( it can not see them ) , making $ l ( A_0 , y_i ) $ being independent random variables , we can apply Hoeffding \u2019 s inequality without any problem here . Actually , this step of using Hoeffding \u2019 s inequality is also present in the coreset paper ( Sener & Savarese , 2017 , see the last two lines of their proof of Theorem 1 ) . About the assumptions on theorem 1 : our assumptions align with existing works in deep learning/active learning theory which is the general way of real data approximation . For instance , both assumptions 2 and 3 are used in the paper by Sener & Savarese ( 2017 ) [ 3 ] and Lipschitz ( assumption 2 ) is natural and common for loss functions such as hinge loss , mean squared error , and cross-entropy as long as the model output is bounded . This assumption is also widely used in deep learning theory e.g. , [ 1,2 ] . As for assumption 3 , although the constant $ \\alpha $ can be unbounded , it can be made arbitrarily small without changing the predicted labels of the network ; this is because dividing all input weights by a constant $ t $ will also divide the output by a constant $ t $ . For the other assumptions , assumption 1 assumes a zero training loss , which is a typical setting in neural networks [ 3 ] . As for assumption 4 , ReLU is activated with probability 1/2 , which is justified by the observations in practice that usually half of all the ReLU neurons can activate . As mentioned in the paper , this is a common assumption in the literature . In a related paper on graph learning [ 4 ] , the authors also assume that the ReLU activations are random . Moreover , our theoretical analysis only gives worst-case guarantees of our method , and its purpose is to justify our method against other clustering methods ( e.g. , the coreset approach , clustering the raw features , etc . ) . The advantage of our method is evident in our strong experiment results that our simple method can beat previous baselines with elaborately designed heuristics . We hope that our changes and comments can resolve your question towards our submission - and please reply if you still have further questions , and we would love to provide more details . If we resolve your questions , we are grateful if you can consider updating your review score . Thank you for your time and effort in reviewing our paper ! [ 1 ] Allen-Zhu , Z. , Li , Y. , & Song , Z . ( 2019 ) .A convergence theory for deep learning via over-parameterization . ICML 2019 . [ 2 ] Du , S. S. , & Lee , J. D. ( 2018 ) . On the power of over-parametrization in neural networks with quadratic activation . ICML 2018 . [ 3 ] Sener , O. , & Savarese , S. ( 2017 ) . Active learning for convolutional neural networks : A core-set approach . arXiv preprint arXiv:1708.00489 . [ 4 ] Xu , K. , Li , C. , Tian , Y. , Sonobe , T. , Kawarabayashi , K. I. , & Jegelka , S. ( 2018 ) . Representation Learning on Graphs with Jumping Knowledge Networks . ICML 2018"}, {"review_id": "HylwpREtDr-2", "review_text": "The authors propose an interesting method to actively select samples using the embeddings learned from GNNs. The proposed method combines graph embeddings and clustering to intelligently select new node samples. Theoretical analysis is provided to support the effectiveness and experimental results shows that this method can outperform many other active learning methods. This paper can be improved on the following aspects: 1. The proposed method conducts clustering using node embeddings. Although these embeddings have encoded graph structure to some extent, I would suggest explicitly incorporating the graph structure in clustering or at least comparing to a baseline on that. The proposed method conducts embedding learning and clustering in two consecutive but separate steps. It would be interesting to see that the clustering can also leverage the graph information. 2. It would be better to provide more details about network settings (some hyperparams have already been given in the paper), and more analysis would be helpful. For example, how the number of clusters affects the performance? 3. Is it possible to create a scenario where there are more labeled data from one cluster but less data from another cluster? In this case, should we still take equal amount of samples from different clusters? ", "rating": "8: Accept", "reply_text": "We thank the reviewer for the comments . We agree that incorporating neighborhood information into the clustering process can also be helpful . For example , we can compute the distance based on a weighted combination of $ S , S^2 , ... $ . We will conduct additional experiments on this and report the results in our revised version of the paper . We train our framework on all datasets with 5 different runs and show the averaged results . We will release our code shortly for people to reproduce our experiments . Currently , we take one sample near each cluster center and so the number of clusters is equal to the label budget . In general , the model performance increases with the number of clusters ( labels ) , as shown in Figure 2 . We agree that varying the number of selected nodes from each cluster is an interesting idea . For example , we may set the number of nodes from each cluster to be proportional to the cluster size , or use hierarchical clustering . It would be meaningful future work to explore more in this aspect . We hope that our changes and comments can resolve your question towards our submission - and please reply if you still have further questions . Thank you for your time and effort in reviewing our paper !"}], "0": {"review_id": "HylwpREtDr-0", "review_text": "This paper introduces active learning for graphs using graph neural networks The bound is not very meaningful as it requires unrealistic assumptions and is loose. Figure 2 shows that even random selection performs quite well compared to this elaborate method. This Area if research and the data sets don\u2019t seem to have many actual real applications in the world with much impact. .................................................................\\.\\\\........................................,,..", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the comments . [ for assumptions ] We would like to emphasize that our assumptions follow from the common settings in deep learning/active learning theory which is the general way of real data approximation . For instance , both assumptions 2 and 3 are used in the paper by Sener & Savarese ( 2017 ) [ 3 ] and Lipschitz ( assumption 2 ) is natural and common for loss functions such as hinge loss , mean squared error , and cross-entropy as long as the model output is bounded . This assumption is also widely used in deep learning theory e.g. , [ 1,2 ] . As for assumption 3 , although the constant $ \\alpha $ can be unbounded , it can be made arbitrarily small without changing the predicted labels of the network ; this is because dividing all input weights by a constant $ t $ will also divide the output by a constant $ t $ . For the other assumptions , assumption 1 assumes a zero training loss , which is a typical setting in neural networks [ 3 ] . As for assumption 4 , ReLU is activated with probability 1/2 , which is justified by the observations in practice that usually half of all the ReLU neurons can activate . As mentioned in the paper , this is a common assumption in the literature . In a related paper on graph learning [ 4 ] , the authors also assume that the ReLU activations are random . Moreover , our theoretical analysis only gives worst-case guarantees of our method , and its purpose is to justify our method against other clustering methods ( e.g. , the coreset approach , clustering the raw features , etc . ) . The advantage of our method is evident in our strong experiment results that our simple method can beat previous baselines with elaborately designed heuristics . Our method is just a clustering of transformed features , which is very easy to implement . It is much simpler than previous active graph learning methods like AGE and ANRMAB , which combine several hand-made heuristics through weighting . [ for Random in Figure 2 ] In Figure 2 , please notice that Degree , Uncertainty , Coreset are general active learning methods which can not leverage graph-based feature propagationwhile AGE , ANRMAB and our method ( FeatProp ) are graph-based active learning methods . Our method substantially outperform random sampling on all the four benchmark datasets in this paper - see Table 4 in Appendix for details . [ for application ] The main contribution of our paper is to enhance the effectiveness of active learning on graphs . The proposed method is generic and directly applicable to real-world applications where graphical data are available and labeled data are hard to acquire . For example , our methods can be used to enrich user/item representations in recommendation systems and social networks . We hope that our changes and comments can resolve your question towards our submission - and please reply if you still have further questions , and we would love to provide more details . If we resolve your questions , we are grateful if you can consider updating your review score . Thank you for your time and effort in reviewing our paper ! [ 1 ] Allen-Zhu , Z. , Li , Y. , & Song , Z . ( 2019 ) .A convergence theory for deep learning via over-parameterization . ICML 2019 . [ 2 ] Du , S. S. , & Lee , J. D. ( 2018 ) . On the power of over-parametrization in neural networks with quadratic activation . ICML 2018 . [ 3 ] Sener , O. , & Savarese , S. ( 2017 ) . Active learning for convolutional neural networks : A core-set approach . arXiv preprint arXiv:1708.00489 . [ 4 ] Xu , K. , Li , C. , Tian , Y. , Sonobe , T. , Kawarabayashi , K. I. , & Jegelka , S. ( 2018 ) . Representation Learning on Graphs with Jumping Knowledge Networks . ICML 2018"}, "1": {"review_id": "HylwpREtDr-1", "review_text": "The authors propose to incorporate active learning into the graph neural network training and claim some guarantee on the proposed method. I have some concerns about the correctness of the proof. For theorem 1, how is the Hoeffding applied so that the \\sqrt{n} term appears? My worry is naively applying Hoeffding as is done in the proof only gives a bound on a fixed model, but in the theorem A_t is not fixed. You may need to apply a union bound or more sophisticated set cover theory to claim the result. Or if I missed something could the authors add more details on the step using Hoeffding bound to the proof? Other than that I also feel the assumptions on theorem 1 are way too strong. Especially assumption 2 and 3. They are simply not true in application. The assumptions are so strong that the theorem, even if the proof can be fixed, is not interesting any more. ", "rating": "1: Reject", "reply_text": "We thank the reviewer for the comments . About applying Hoeffding \u2019 s inequality in our proof : we \u2019 ve updated the proof to include more details for this step , and please check out the highlighted part of Appendix B . Briefly , here the randomness in applying Hoeffding \u2019 s inequality only comes from the random draws of the hidden labels , $ y_i\\sim \\eta ( i ) $ , for each unlabeled node $ i $ . This is determined after we fix the graph $ G $ and feature matrix $ X $ . Since our model $ A_0 $ does not depend on the hidden labels ( it can not see them ) , making $ l ( A_0 , y_i ) $ being independent random variables , we can apply Hoeffding \u2019 s inequality without any problem here . Actually , this step of using Hoeffding \u2019 s inequality is also present in the coreset paper ( Sener & Savarese , 2017 , see the last two lines of their proof of Theorem 1 ) . About the assumptions on theorem 1 : our assumptions align with existing works in deep learning/active learning theory which is the general way of real data approximation . For instance , both assumptions 2 and 3 are used in the paper by Sener & Savarese ( 2017 ) [ 3 ] and Lipschitz ( assumption 2 ) is natural and common for loss functions such as hinge loss , mean squared error , and cross-entropy as long as the model output is bounded . This assumption is also widely used in deep learning theory e.g. , [ 1,2 ] . As for assumption 3 , although the constant $ \\alpha $ can be unbounded , it can be made arbitrarily small without changing the predicted labels of the network ; this is because dividing all input weights by a constant $ t $ will also divide the output by a constant $ t $ . For the other assumptions , assumption 1 assumes a zero training loss , which is a typical setting in neural networks [ 3 ] . As for assumption 4 , ReLU is activated with probability 1/2 , which is justified by the observations in practice that usually half of all the ReLU neurons can activate . As mentioned in the paper , this is a common assumption in the literature . In a related paper on graph learning [ 4 ] , the authors also assume that the ReLU activations are random . Moreover , our theoretical analysis only gives worst-case guarantees of our method , and its purpose is to justify our method against other clustering methods ( e.g. , the coreset approach , clustering the raw features , etc . ) . The advantage of our method is evident in our strong experiment results that our simple method can beat previous baselines with elaborately designed heuristics . We hope that our changes and comments can resolve your question towards our submission - and please reply if you still have further questions , and we would love to provide more details . If we resolve your questions , we are grateful if you can consider updating your review score . Thank you for your time and effort in reviewing our paper ! [ 1 ] Allen-Zhu , Z. , Li , Y. , & Song , Z . ( 2019 ) .A convergence theory for deep learning via over-parameterization . ICML 2019 . [ 2 ] Du , S. S. , & Lee , J. D. ( 2018 ) . On the power of over-parametrization in neural networks with quadratic activation . ICML 2018 . [ 3 ] Sener , O. , & Savarese , S. ( 2017 ) . Active learning for convolutional neural networks : A core-set approach . arXiv preprint arXiv:1708.00489 . [ 4 ] Xu , K. , Li , C. , Tian , Y. , Sonobe , T. , Kawarabayashi , K. I. , & Jegelka , S. ( 2018 ) . Representation Learning on Graphs with Jumping Knowledge Networks . ICML 2018"}, "2": {"review_id": "HylwpREtDr-2", "review_text": "The authors propose an interesting method to actively select samples using the embeddings learned from GNNs. The proposed method combines graph embeddings and clustering to intelligently select new node samples. Theoretical analysis is provided to support the effectiveness and experimental results shows that this method can outperform many other active learning methods. This paper can be improved on the following aspects: 1. The proposed method conducts clustering using node embeddings. Although these embeddings have encoded graph structure to some extent, I would suggest explicitly incorporating the graph structure in clustering or at least comparing to a baseline on that. The proposed method conducts embedding learning and clustering in two consecutive but separate steps. It would be interesting to see that the clustering can also leverage the graph information. 2. It would be better to provide more details about network settings (some hyperparams have already been given in the paper), and more analysis would be helpful. For example, how the number of clusters affects the performance? 3. Is it possible to create a scenario where there are more labeled data from one cluster but less data from another cluster? In this case, should we still take equal amount of samples from different clusters? ", "rating": "8: Accept", "reply_text": "We thank the reviewer for the comments . We agree that incorporating neighborhood information into the clustering process can also be helpful . For example , we can compute the distance based on a weighted combination of $ S , S^2 , ... $ . We will conduct additional experiments on this and report the results in our revised version of the paper . We train our framework on all datasets with 5 different runs and show the averaged results . We will release our code shortly for people to reproduce our experiments . Currently , we take one sample near each cluster center and so the number of clusters is equal to the label budget . In general , the model performance increases with the number of clusters ( labels ) , as shown in Figure 2 . We agree that varying the number of selected nodes from each cluster is an interesting idea . For example , we may set the number of nodes from each cluster to be proportional to the cluster size , or use hierarchical clustering . It would be meaningful future work to explore more in this aspect . We hope that our changes and comments can resolve your question towards our submission - and please reply if you still have further questions . Thank you for your time and effort in reviewing our paper !"}}