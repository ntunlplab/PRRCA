{"year": "2020", "forum": "rkxUfANKwB", "title": "All SMILES Variational Autoencoder for Molecular Property Prediction and Optimization", "decision": "Reject", "meta_review": "The paper proposes All SMILES VAE which can capture the chemical properties of small molecules and also optimize the structures of these molecules. The model achieves significantly performance improvement over existing methods on the Zinc250K and Tox21 datasets. \n\nOverall it is a very solid paper - it addresses an important problem, provides detailed description of the proposed method and shows promising experiment results. The work could be a landmark piece, leading to major impacts in the field. However, given its potential,  the paper could benefit from major revisions of the draft. Below are some suggestions on improving the work:\n1. The current version contains a lot of materials. It tries to strike the balance between machine learning methodology and details of the application domain. But the reality is that the lack of architecture details and some sloppy definitions of ML terms make it hard for readers to fully appreciate the methodology novelty. \n\n2. There is still room for improvement in experiments. As suggested in the review, more datasets should be used to evaluate the proposed model. Since it is hard to provide theoretic analysis of the proposed model,  extensive experiments should be provided. \n\n3. The complexity analysis is not fully convincing. Some fair comparison with the alternative approaches should be provided. \n\nIn summary, it is a paper with big potentials. The current version is a step away from being ready for publication. We hope the reviews can help improve the paper for a strong publication in the future. ", "reviews": [{"review_id": "rkxUfANKwB-0", "review_text": "This paper presents a VAE-based method to predict molecular properties as well as to design a novel molecular architecture. The method employs as a basic input the SMILES representation of molecules which are not well defined in terms of the representation, though. To tackle the issue, the authors formulate the method based on (1) multiple inputs of SMILES strings, (2) the character-wise feature fusion across those multiple strings and (3) network training by multiple output targets of SMILES strings different from the input ones. As a result, the method provides the fixed-length latent representation which is robust against the variations of the SMILES representation and is useful for predicting the molecular properties and optimizing the molecular design. The experimental results on the tasks of the property prediction and molecular optimization using the benchmark datasets demonstrate the effectiveness of the proposed method in comparison with the others. Though this paper provides some contributions to the field of molecular graph representation, it contains flaws in presentation, lacking details of technical contents; thus, the paper is regarded as \"borderline\". I would like the authors to properly provide the details in the following points. * This paper lacks some important technical contents, making it hard to understand. - What is the actual form of the property predictor used in p(\\rho|z). And, in the first place of the paper, it would be better to explain what kind of and how many properties of the molecular are considered. - How is the decoder constructed and trained? Though the authors state that a single-layer LSTM is employed as the decoder, there is no clear description how to cope with the multiple outputs (decoder targets) in training the LSTM. - What does \\theta in p_\\theta(z) mean? In the VAE, p(z) works as a simple prior on z. - The description, especially in Sec. 3 for the proposed method, is rather poorly presented by using less amount of math. At least, the authors should first depict the overall architecture of the method through clarifying the above-mentioned technical points. - It is unclear how to apply the proposed method to the semi-supervised learning framework? Is it re-formulated in the SS-VAE [a] framework? - The comparison experiments seem to be inconsistent. The proposed method is compared with different methods in different datasets/tasks. Toward fair comparison, it should be evaluated in comparison with some baselines including such as JT-VAE consistently. [a] Kingma, D. P.; Mohamed, S.; Rezende, D. J.; Welling, M. Semi-Supervised Learning with Deep Generative Models. Proceedings of the 28th Annual Conference on Neural Information Processing Systems. 2014; pp 3581\u20133589. * The authors cope with the fluctuations regarding the SMILES representation by means of RNN. There, however, are some approaches to canonicalize the SMILES representation itself such as by canonical SMILES. The authors have to mention those other approaches and discuss the superiority of the method over them; hopefully the proposed method should be compared with the naive one simply combining VAE [Kang&Cho18] and canonical SMILES representation. And to further understand the difficulty in SMILES variations, it would be better to show the averaged cardinality of SMILES representation per molecular. Minor comment: * In aggregating the RNN features for each character in SMILES strings, it might be possible to incorporate some structural knowledge into the process; for example, \"C\"s in benzene share the identical feature representation through fusing all those features.", "rating": "3: Weak Reject", "reply_text": "Comment 1 : - What is the actual form of the property predictor used in p ( rho|z ) . And , in the first place of the paper , it would be better to explain what kind of and how many properties of the molecular are considered . Response 1 : In general , p ( rho|z ) is a simple linear transformation of the latent variables into a scalar , followed by an activation function that transforms the property prediction into the correct range . QED is in the range [ 0,1 ] and Tox21 is binary , so we use a sigmoid activation function ; penalized logP is in R ( the real numbers ) , so we use the identity activation function ( no activation function ) ; and molecular weight is in R^+ ( non-negative real numbers ) , so we use a softplus activation function . This is described in Section 4.1 of the text , but we have added additional explanation in Section 3 to enhance the clarity . Models are trained on one type of property : either QED , penalized logP , molecular weight , or all twelve binary Tox21 toxicity categories . For instance , in Figure 5a we only train on logP ; in Figure 5b , we train a separate instance of our model on molecular weight ; in Table 1b , we train an independent instance of our model on Tox21 ; etc . Comment 2 : How is the decoder constructed and trained ? Though the authors state that a single-layer LSTM is employed as the decoder , there is no clear description how to cope with the multiple outputs ( decoder targets ) in training the LSTM . Response 2 : The decoder is trained using teacher forcing on a set of n SMILES strings of a single molecule in parallel , all using the same latent representation . This is analogous to training using minibatch of molecules , but all n SMILES strings of a single , common molecule are decoded from the same latent representation . The total decoder log-likelihood is the sum of the log-likelihoods on each of these SMILES strings . The decoder is trained using teacher forcing to be able to produce all SMILES strings of a target molecule . When the LSTM starts decoding , it will be uncertain as to which SMILES is being decoded . For each successive character of the target SMILES string , the log-probability assigned by the decoder LSTM to the correct character is evaluated , and the correct character is fed into the decoder LSTM . As the LSTM sees more of the previous true outputs , its predictions as to which SMILES string of the molecule it needs to decode becomes more accurate . This is described in Section 3 of the decoder architecture description , but we have edited the text for increased clarity . Comment 3 : What does theta in p_ { theta } ( z ) mean ? In the VAE , p ( z ) works as a simple prior on z . Response 3 : In the VAE literature , work has been done in parametrizing prior distributions to make them more expressive than the standard Gaussian used in the vanilla VAE [ Rolfe 2016 ; Chen et al. , 2016 ; Tomczak , 2017 ] . The theta in p_ { theta } ( z ) denotes the parameters of such parametrized priors . We describe the architecture of the parametrized prior in Section 3 , but have edited the text for increased clarity . Briefly , we use a hierarchical prior , where each autoregressive conditional Gaussian distribution is parametrized by a two layer neural network conditioned on the concatenated latent variables sampled from the previous layer . With mathematical notation : p_ { \\theta } ( z_1 , z_2 , z_3 , z_4 ) = p ( z_1 ) \\cdot \\prod^4_ { i=2 } p_ { \\theta_i } ( z_i|z_ { < i } ) where \\theta_i represents the parameters of the two layer neural network which takes z_ { < i } as input , and p ( z_1 ) is just the unparameterized standard Gaussian ."}, {"review_id": "rkxUfANKwB-1", "review_text": "The authors describie a novel variational autoencoder like method for molecules. Instead of using graph neural networks, the authors hava an approach based on SMILES which encode molecules as strings. To avoid the problem that any given molecule may be represented by multiple SMILES strings, the authors consider an encoder that makes use of several random SMILES representations of the input molecule. These are preprocessed using recurrent neural networks generating an average representation by pooling the representations generated with each SMILES sequence for each atom in the input molecule. This reduces the number of operations needed to share information across all the atom in the molecule (from N^2 in graph neural networks to MN in the proposed approach with M different SMILES representations of the input molecule). The model decodes then into a disjoint set of SMILES strings different from those used at the input. This enforces the model to learn a bijective mapping between molecules and latent representations. The model trains also jointly a property regressor, linear or logistic. They do constrained optimization in the latent space, satying within a reparameterized shell with most of the probability mass for the data. The proposed model can also do semi-supervised and supervised prediction tasks. Clarity: The paper is very clearly written and it is very easy to read. It contains a very detailed description of previous work. Quality: The proposed model is very reasonable and well-motivated. The experiments performed are exhaustive and informative enough to show that the proposed model and algorithms are useful in practice. Novelty: The proposed encoder/decoder model based on multiple SMILES representation is novel up to my knowledge. Significance: The experiments show that the proposed method can outperform previous ones. However, I miss additional evaluations using existing frameworks such as Guacamol. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your encouraging comments and precise description of our work . We agree that more benchmarks like Guacamol and MOSES would further strengthen our results [ Brown et al. , 2019 ; Polykovskiy et al. , 2018 ] . We are currently working on this along with other new benchmarks to be disclosed in a future publication . 1 : Brown , N. , Fiscato , M. , Segler , M. H. , & Vaucher , A. C. ( 2019 ) . Guacamol : benchmarking models for de novo molecular design . Journal of chemical information and modeling , 59 ( 3 ) , 1096-1108 . 2 : Polykovskiy , D. , Zhebrak , A. , Sanchez-Lengeling , B. , Golovanov , S. , Tatanov , O. , Belyaev , S. , ... & Kadurin , A . ( 2018 ) .Molecular sets ( moses ) : a benchmarking platform for molecular generation models . arXiv preprint arXiv:1811.12823 ."}, {"review_id": "rkxUfANKwB-2", "review_text": "The authors present a method All SMILES VAE that\u2019s used for predicting chemical properties of small molecules and also for optimizing the structures of these molecules. The authors evaluate their model on the Zinc250K and Tox21 dataset and report that they are able to exceed the previous SOTA. This paper was well written, and did a good job with explanations and illustrations. The central idea is to use a RNN to learn representations of strings encoding molecular structures (SMILE strings). SMILE strings are constructed by a DFS traversal over molecular graph structures. The authors choose to feed in distinct SMILE encodings of the same molecule in parallel, resulting in a stacked RNN architecture. They observe that since SMILE strings are DFS traversals of the molecular graph, propagation in the RNN corresponds to sequential message passing steps between nodes in the graph. This is in contrast to a graph neural network wherein all nodes simultaneously broadcast messages to their neighbors at every propagation step. While it\u2019s interesting to be able to optimize molecules in the space of SMILE strings, the impact is less clear. The authors mention in Sec 3.1, that graph models e.g. GCNs have higher overall compute complexity O(b^2), compared to their method which has O(Mb); however this does not do complete justice. Most graph propagation operations benefit from heavily parallelizable sparse matrix operations on GPUs. Infact, they can actually be much faster than RNNs in practice since GCNs need only a fixed number of propagation steps (independent of the number of bonds). In Sec 3.2, the authors describe their approach for constraining the space of molecular optimization. While this may lead to directed searches, it will prevent truly novel molecules from being synthesized. The authors will need to address and provide experiments with unconstrained search. It was not clear if the authors implemented any of the baselines? It seems from the text of figure 5, that the SSVAE and GraphConv results have been taken directly from the paper. The authors can strengthen their claim by replicating the results of baselines and making sure that they agree. Some clarification questions: - For optimized/predicted strings, which are presumably novel molecules not in the dataset, how is the true chemical property e.g. logP, determined? - In Figure 6, from steps 30 - 40, it seems that the predicted logP goes up but the true logP stays the same in a few cases. Why is this the case? Overall, I think that this paper has some interesting ideas and is well written. However, the novelty and impact of the model is somewhat lacking. If this were introducing a new application area to this field, then I think the case for acceptance could have been stronger, however there has already been a lot of work related to molecular property prediction/design. Also, AC please note, I would have given the paper a score of 5, but Openreview only gave me a choice between 3 and 6. So, I went with 3, but please consider this to be a 5.", "rating": "3: Weak Reject", "reply_text": "Comment 1 : it \u2019 s interesting to be able to optimize molecules in the space of SMILE strings , the impact is less clear . Response1 : Our model is over molecules , rather than individual SMILES strings . The VAE architecture effectively takes a molecular graph as input , and dynamically generates n SMILES strings representing that molecule as the target of the decoder . Simultaneously , the encoder takes the molecular graph as input and simultaneously generates m independents SMILES strings representing the molecule to feed into the subsequent stacks of RNNs . Since the SMILES-encoding of the encoder is independent of the SMILES-encoding of the decoder , the latent representation must capture the molecule as a whole , and decode to all possible SMILES strings representing the molecule . As a result , optimization in the latent space is properly in the space of molecules , rather than SMILES strings . Rather , we use SMILES strings as the initial component of a SMILES-based encoder and the final component of a SMILES-based decoder for molecular graphs , analogous to graph convolutions and iterative graph construction in other molecular graph-based generative models . We train the model to be able to decode to any SMILES string from a given point in latent space , we expect that point to manifest as an abstract representation of the molecule , as opposed to a specific SMILES . Our optimization is in the space of molecules , rather than SMILES strings . Comment 2 : Most graph propagation operations benefit from heavily parallelizable sparse matrix operations on GPUs . Response 2 : We agree that sparse matrix operations can be efficiently parallelized on GPUs , and take this into account in our analysis by conservatively assuming that each graph convolutional ( GC ) layer takes only O ( b ) . To further highlight this fact , we have added a note to the paper that GC operations can be efficiently parallelized with sparse GPU operations , as it is important for practical implementation . We further agree that in practise , a single RNN layer is slower than a single GC layer , although big O does not traditionally differentiate parallel and sequential processes . However , to produce a nonlinear representation that encompasses an entire molecule , GCs utilizing sparse matrix operations require a number of iterations equal to at least half the graph diameter ( which is O ( b ) ) , giving us O ( b^2 ) . Without a number of iterations equal to at least half the graph diameter , information can not be transferred from all atoms to any single location in the molecular graph . To transfer information from all atoms to all locations in the molecular graph , a number of iterations equal to the graph diameter is required . In practice , many times this number of iterations will generally be required to flexibly combine information from throughout the molecule . Correspondingly , convolutional networks on images use hundreds of layers ; moreover , they reduce the resolution with pooling layers to sizes as small as 7x7 , so a few layers are sufficient to integrate information throughout the image . While many previous works have used a fixed number of graph convolution layers ( generally 3-7 ) , the complexity of GCs is O ( b^2 ) if information must be passed across the entire graph . While it is possible to lower-bound the graph diameter in terms of the number of bonds , this bound is only realized in densely connected graphs , whereas bonds amongst atoms in molecules are sparse and often planar . As a result , the graph diameter can be large , even when the number of bonds is modest . In the ZINC250k dataset , the mean graph diameter is 11.1 ; the maximum is 24 . Typical implementations of graph convolution use only 3 to 7 rounds of message passing [ Duvenaud et al. , 2015 ; Gilmeret al. , 2017 ; Jin et al. , 2018 ; Kearnes et al. , 2016 ; Liu et al. , 2018 ; Samanta et al. , 2018 ; You et al. , 2018 ] , and so can not propagate information across most molecules in this dataset . In contrast , an RNN on a SMILES string of the molecule , in which some bound atoms are processed sequentially , while other bound atoms are far apart in the string , will pass information across the whole string in a single layer . Our novel use of multiple SMILES strings allows information to pass quickly through long paths in the molecule , while efficiently representing all paths ( using multiple SMILES strings ) ."}], "0": {"review_id": "rkxUfANKwB-0", "review_text": "This paper presents a VAE-based method to predict molecular properties as well as to design a novel molecular architecture. The method employs as a basic input the SMILES representation of molecules which are not well defined in terms of the representation, though. To tackle the issue, the authors formulate the method based on (1) multiple inputs of SMILES strings, (2) the character-wise feature fusion across those multiple strings and (3) network training by multiple output targets of SMILES strings different from the input ones. As a result, the method provides the fixed-length latent representation which is robust against the variations of the SMILES representation and is useful for predicting the molecular properties and optimizing the molecular design. The experimental results on the tasks of the property prediction and molecular optimization using the benchmark datasets demonstrate the effectiveness of the proposed method in comparison with the others. Though this paper provides some contributions to the field of molecular graph representation, it contains flaws in presentation, lacking details of technical contents; thus, the paper is regarded as \"borderline\". I would like the authors to properly provide the details in the following points. * This paper lacks some important technical contents, making it hard to understand. - What is the actual form of the property predictor used in p(\\rho|z). And, in the first place of the paper, it would be better to explain what kind of and how many properties of the molecular are considered. - How is the decoder constructed and trained? Though the authors state that a single-layer LSTM is employed as the decoder, there is no clear description how to cope with the multiple outputs (decoder targets) in training the LSTM. - What does \\theta in p_\\theta(z) mean? In the VAE, p(z) works as a simple prior on z. - The description, especially in Sec. 3 for the proposed method, is rather poorly presented by using less amount of math. At least, the authors should first depict the overall architecture of the method through clarifying the above-mentioned technical points. - It is unclear how to apply the proposed method to the semi-supervised learning framework? Is it re-formulated in the SS-VAE [a] framework? - The comparison experiments seem to be inconsistent. The proposed method is compared with different methods in different datasets/tasks. Toward fair comparison, it should be evaluated in comparison with some baselines including such as JT-VAE consistently. [a] Kingma, D. P.; Mohamed, S.; Rezende, D. J.; Welling, M. Semi-Supervised Learning with Deep Generative Models. Proceedings of the 28th Annual Conference on Neural Information Processing Systems. 2014; pp 3581\u20133589. * The authors cope with the fluctuations regarding the SMILES representation by means of RNN. There, however, are some approaches to canonicalize the SMILES representation itself such as by canonical SMILES. The authors have to mention those other approaches and discuss the superiority of the method over them; hopefully the proposed method should be compared with the naive one simply combining VAE [Kang&Cho18] and canonical SMILES representation. And to further understand the difficulty in SMILES variations, it would be better to show the averaged cardinality of SMILES representation per molecular. Minor comment: * In aggregating the RNN features for each character in SMILES strings, it might be possible to incorporate some structural knowledge into the process; for example, \"C\"s in benzene share the identical feature representation through fusing all those features.", "rating": "3: Weak Reject", "reply_text": "Comment 1 : - What is the actual form of the property predictor used in p ( rho|z ) . And , in the first place of the paper , it would be better to explain what kind of and how many properties of the molecular are considered . Response 1 : In general , p ( rho|z ) is a simple linear transformation of the latent variables into a scalar , followed by an activation function that transforms the property prediction into the correct range . QED is in the range [ 0,1 ] and Tox21 is binary , so we use a sigmoid activation function ; penalized logP is in R ( the real numbers ) , so we use the identity activation function ( no activation function ) ; and molecular weight is in R^+ ( non-negative real numbers ) , so we use a softplus activation function . This is described in Section 4.1 of the text , but we have added additional explanation in Section 3 to enhance the clarity . Models are trained on one type of property : either QED , penalized logP , molecular weight , or all twelve binary Tox21 toxicity categories . For instance , in Figure 5a we only train on logP ; in Figure 5b , we train a separate instance of our model on molecular weight ; in Table 1b , we train an independent instance of our model on Tox21 ; etc . Comment 2 : How is the decoder constructed and trained ? Though the authors state that a single-layer LSTM is employed as the decoder , there is no clear description how to cope with the multiple outputs ( decoder targets ) in training the LSTM . Response 2 : The decoder is trained using teacher forcing on a set of n SMILES strings of a single molecule in parallel , all using the same latent representation . This is analogous to training using minibatch of molecules , but all n SMILES strings of a single , common molecule are decoded from the same latent representation . The total decoder log-likelihood is the sum of the log-likelihoods on each of these SMILES strings . The decoder is trained using teacher forcing to be able to produce all SMILES strings of a target molecule . When the LSTM starts decoding , it will be uncertain as to which SMILES is being decoded . For each successive character of the target SMILES string , the log-probability assigned by the decoder LSTM to the correct character is evaluated , and the correct character is fed into the decoder LSTM . As the LSTM sees more of the previous true outputs , its predictions as to which SMILES string of the molecule it needs to decode becomes more accurate . This is described in Section 3 of the decoder architecture description , but we have edited the text for increased clarity . Comment 3 : What does theta in p_ { theta } ( z ) mean ? In the VAE , p ( z ) works as a simple prior on z . Response 3 : In the VAE literature , work has been done in parametrizing prior distributions to make them more expressive than the standard Gaussian used in the vanilla VAE [ Rolfe 2016 ; Chen et al. , 2016 ; Tomczak , 2017 ] . The theta in p_ { theta } ( z ) denotes the parameters of such parametrized priors . We describe the architecture of the parametrized prior in Section 3 , but have edited the text for increased clarity . Briefly , we use a hierarchical prior , where each autoregressive conditional Gaussian distribution is parametrized by a two layer neural network conditioned on the concatenated latent variables sampled from the previous layer . With mathematical notation : p_ { \\theta } ( z_1 , z_2 , z_3 , z_4 ) = p ( z_1 ) \\cdot \\prod^4_ { i=2 } p_ { \\theta_i } ( z_i|z_ { < i } ) where \\theta_i represents the parameters of the two layer neural network which takes z_ { < i } as input , and p ( z_1 ) is just the unparameterized standard Gaussian ."}, "1": {"review_id": "rkxUfANKwB-1", "review_text": "The authors describie a novel variational autoencoder like method for molecules. Instead of using graph neural networks, the authors hava an approach based on SMILES which encode molecules as strings. To avoid the problem that any given molecule may be represented by multiple SMILES strings, the authors consider an encoder that makes use of several random SMILES representations of the input molecule. These are preprocessed using recurrent neural networks generating an average representation by pooling the representations generated with each SMILES sequence for each atom in the input molecule. This reduces the number of operations needed to share information across all the atom in the molecule (from N^2 in graph neural networks to MN in the proposed approach with M different SMILES representations of the input molecule). The model decodes then into a disjoint set of SMILES strings different from those used at the input. This enforces the model to learn a bijective mapping between molecules and latent representations. The model trains also jointly a property regressor, linear or logistic. They do constrained optimization in the latent space, satying within a reparameterized shell with most of the probability mass for the data. The proposed model can also do semi-supervised and supervised prediction tasks. Clarity: The paper is very clearly written and it is very easy to read. It contains a very detailed description of previous work. Quality: The proposed model is very reasonable and well-motivated. The experiments performed are exhaustive and informative enough to show that the proposed model and algorithms are useful in practice. Novelty: The proposed encoder/decoder model based on multiple SMILES representation is novel up to my knowledge. Significance: The experiments show that the proposed method can outperform previous ones. However, I miss additional evaluations using existing frameworks such as Guacamol. ", "rating": "6: Weak Accept", "reply_text": "Thank you for your encouraging comments and precise description of our work . We agree that more benchmarks like Guacamol and MOSES would further strengthen our results [ Brown et al. , 2019 ; Polykovskiy et al. , 2018 ] . We are currently working on this along with other new benchmarks to be disclosed in a future publication . 1 : Brown , N. , Fiscato , M. , Segler , M. H. , & Vaucher , A. C. ( 2019 ) . Guacamol : benchmarking models for de novo molecular design . Journal of chemical information and modeling , 59 ( 3 ) , 1096-1108 . 2 : Polykovskiy , D. , Zhebrak , A. , Sanchez-Lengeling , B. , Golovanov , S. , Tatanov , O. , Belyaev , S. , ... & Kadurin , A . ( 2018 ) .Molecular sets ( moses ) : a benchmarking platform for molecular generation models . arXiv preprint arXiv:1811.12823 ."}, "2": {"review_id": "rkxUfANKwB-2", "review_text": "The authors present a method All SMILES VAE that\u2019s used for predicting chemical properties of small molecules and also for optimizing the structures of these molecules. The authors evaluate their model on the Zinc250K and Tox21 dataset and report that they are able to exceed the previous SOTA. This paper was well written, and did a good job with explanations and illustrations. The central idea is to use a RNN to learn representations of strings encoding molecular structures (SMILE strings). SMILE strings are constructed by a DFS traversal over molecular graph structures. The authors choose to feed in distinct SMILE encodings of the same molecule in parallel, resulting in a stacked RNN architecture. They observe that since SMILE strings are DFS traversals of the molecular graph, propagation in the RNN corresponds to sequential message passing steps between nodes in the graph. This is in contrast to a graph neural network wherein all nodes simultaneously broadcast messages to their neighbors at every propagation step. While it\u2019s interesting to be able to optimize molecules in the space of SMILE strings, the impact is less clear. The authors mention in Sec 3.1, that graph models e.g. GCNs have higher overall compute complexity O(b^2), compared to their method which has O(Mb); however this does not do complete justice. Most graph propagation operations benefit from heavily parallelizable sparse matrix operations on GPUs. Infact, they can actually be much faster than RNNs in practice since GCNs need only a fixed number of propagation steps (independent of the number of bonds). In Sec 3.2, the authors describe their approach for constraining the space of molecular optimization. While this may lead to directed searches, it will prevent truly novel molecules from being synthesized. The authors will need to address and provide experiments with unconstrained search. It was not clear if the authors implemented any of the baselines? It seems from the text of figure 5, that the SSVAE and GraphConv results have been taken directly from the paper. The authors can strengthen their claim by replicating the results of baselines and making sure that they agree. Some clarification questions: - For optimized/predicted strings, which are presumably novel molecules not in the dataset, how is the true chemical property e.g. logP, determined? - In Figure 6, from steps 30 - 40, it seems that the predicted logP goes up but the true logP stays the same in a few cases. Why is this the case? Overall, I think that this paper has some interesting ideas and is well written. However, the novelty and impact of the model is somewhat lacking. If this were introducing a new application area to this field, then I think the case for acceptance could have been stronger, however there has already been a lot of work related to molecular property prediction/design. Also, AC please note, I would have given the paper a score of 5, but Openreview only gave me a choice between 3 and 6. So, I went with 3, but please consider this to be a 5.", "rating": "3: Weak Reject", "reply_text": "Comment 1 : it \u2019 s interesting to be able to optimize molecules in the space of SMILE strings , the impact is less clear . Response1 : Our model is over molecules , rather than individual SMILES strings . The VAE architecture effectively takes a molecular graph as input , and dynamically generates n SMILES strings representing that molecule as the target of the decoder . Simultaneously , the encoder takes the molecular graph as input and simultaneously generates m independents SMILES strings representing the molecule to feed into the subsequent stacks of RNNs . Since the SMILES-encoding of the encoder is independent of the SMILES-encoding of the decoder , the latent representation must capture the molecule as a whole , and decode to all possible SMILES strings representing the molecule . As a result , optimization in the latent space is properly in the space of molecules , rather than SMILES strings . Rather , we use SMILES strings as the initial component of a SMILES-based encoder and the final component of a SMILES-based decoder for molecular graphs , analogous to graph convolutions and iterative graph construction in other molecular graph-based generative models . We train the model to be able to decode to any SMILES string from a given point in latent space , we expect that point to manifest as an abstract representation of the molecule , as opposed to a specific SMILES . Our optimization is in the space of molecules , rather than SMILES strings . Comment 2 : Most graph propagation operations benefit from heavily parallelizable sparse matrix operations on GPUs . Response 2 : We agree that sparse matrix operations can be efficiently parallelized on GPUs , and take this into account in our analysis by conservatively assuming that each graph convolutional ( GC ) layer takes only O ( b ) . To further highlight this fact , we have added a note to the paper that GC operations can be efficiently parallelized with sparse GPU operations , as it is important for practical implementation . We further agree that in practise , a single RNN layer is slower than a single GC layer , although big O does not traditionally differentiate parallel and sequential processes . However , to produce a nonlinear representation that encompasses an entire molecule , GCs utilizing sparse matrix operations require a number of iterations equal to at least half the graph diameter ( which is O ( b ) ) , giving us O ( b^2 ) . Without a number of iterations equal to at least half the graph diameter , information can not be transferred from all atoms to any single location in the molecular graph . To transfer information from all atoms to all locations in the molecular graph , a number of iterations equal to the graph diameter is required . In practice , many times this number of iterations will generally be required to flexibly combine information from throughout the molecule . Correspondingly , convolutional networks on images use hundreds of layers ; moreover , they reduce the resolution with pooling layers to sizes as small as 7x7 , so a few layers are sufficient to integrate information throughout the image . While many previous works have used a fixed number of graph convolution layers ( generally 3-7 ) , the complexity of GCs is O ( b^2 ) if information must be passed across the entire graph . While it is possible to lower-bound the graph diameter in terms of the number of bonds , this bound is only realized in densely connected graphs , whereas bonds amongst atoms in molecules are sparse and often planar . As a result , the graph diameter can be large , even when the number of bonds is modest . In the ZINC250k dataset , the mean graph diameter is 11.1 ; the maximum is 24 . Typical implementations of graph convolution use only 3 to 7 rounds of message passing [ Duvenaud et al. , 2015 ; Gilmeret al. , 2017 ; Jin et al. , 2018 ; Kearnes et al. , 2016 ; Liu et al. , 2018 ; Samanta et al. , 2018 ; You et al. , 2018 ] , and so can not propagate information across most molecules in this dataset . In contrast , an RNN on a SMILES string of the molecule , in which some bound atoms are processed sequentially , while other bound atoms are far apart in the string , will pass information across the whole string in a single layer . Our novel use of multiple SMILES strings allows information to pass quickly through long paths in the molecule , while efficiently representing all paths ( using multiple SMILES strings ) ."}}