{"year": "2021", "forum": "fylclEqgvgd", "title": "Transformer protein language models are unsupervised structure learners", "decision": "Accept (Poster)", "meta_review": "The authors have done a very thorough job of responding to the comments from reviewers. The paper has a clear contribution, namely that attention maps predict contacts as well as existing unsupervised pipelines. This paper deserves to be published.\n\nIn the final version, the authors should discuss briefly \"BERTology Meets Biology: Interpreting Attention in Protein Language Models\"(https://openreview.net/forum?id=YWtLZvLmud7) and \"Improving Generalizability of Protein Sequence Models via Data Augmentations\" (https://openreview.net/forum?id=Kkw3shxszSd). However, the authors should also make sure that the final version respects the ICLR length limits.\n\nI am recommending poster acceptance because the results are anticlimactic given the recent success of Deepmind at CASP 2020.\n\n", "reviews": [{"review_id": "fylclEqgvgd-0", "review_text": "# # Summary The paper shows that Transformers trained unsupervised on millions of protein sequences learn information about protein contacts by using attention maps for contact prediction . The paper is mostly clearly written and discusses server interesting ablation experiments . However , two recent papers that appeared on arXiv before the ICLR submission deadlines also use Transformers for protein contact prediction . These papers and other methods for contact prediction beyond Gremlin are not described . I therefore consider the contributions as insufficient for an ICLR submission . # # Major comments 1 . Using Transformer attention maps for protein contact prediction is not new . See Rives et al , 2020 , \u2018 Biological structures and functions emerge\u2026 \u2019 , section 5.2 , and Vig et al , 2020 , \u2018 Bertology \u2019 section 4.2 . Both publications appeared on arXiv at least one month before the ICLR submission deadline and are not clearly discussed in the paper . 2.The introductions discusses existing work on Transformers for protein languages models . Existing methods for contact prediction ( beyond Gremlin ) , however , are not described sufficiently . 3.It is unclear which sequences were used for training the Transformer models and how similar they are to test sequences . 4.The paper compares Transformers to Gremlin . However , it is unclear how well they perform to the CASP state-of-the art ( see also Rives et al , 2020 ) . 5.Section 3.4 does not describe clearly enough how attention maps were used for predicting contact maps . How were attention maps symmetrized ? Which layers and heads were used and how were they aggregated ? What is the number of resulting features that were used to train the logistic regression model ? APC is not described or cited . 6.Section 4.5 discusses that Transformers can be also used for secondary structure prediction . This is not new ( see Rives 2020 and Vig 2020 ) and does not fit well to the rest of the paper , which is about contact prediction . 6.Section 4.8 : Using transformers for generating proteins with natural properties is not new ( see Madani et al , 2020 , \u2018 ProGen \u2019 or Rives et al , 2020 ) . \u2018 Wang & Cho \u2019 were not the first who used Transformers generativity ( see Vaswani , 2017 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "\\ > \\ > The paper shows that Transformers trained unsupervised on millions of protein sequences learn information about protein contacts by using attention maps for contact prediction ... We thank the reviewer for their time , interest in the paper , and constructive feedback . \\ > \\ > However , two recent papers that appeared on arXiv before the ICLR submission deadlines also use Transformers for protein contact prediction . See Rives et al , 2020 , \u2018 Biological structures and functions emerge\u2026 \u2019 , section 5.2 , and Vig et al , 2020 , \u2018 Bertology \u2019 section 4.2 . This is the first paper to show state-of-the-art results for unsupervised contact prediction from a transformer protein language model . Prior work Rives et al.2020 benchmarks supervised contact prediction with deep residual networks . Vig et al.2020 show that one specific head of the TAPE transformer is correlated with contacts ( see Vig et al.2020 Fig 4 ) , but make no comparison to state-of-the-art methods for unsupervised contact prediction . In contrast , our work provides a new method that results in state-of-the-art performance on the unsupervised contact prediction problem . Moreover this is the first paper to demonstrate a state-of-the-art result for contact prediction from protein language modeling -- this is an important result for protein language modeling as previous work e.g.Rao et al.2019 , and Rives et al.2020 have shown that protein language models fall well below state-of-the-art performance on supervised contact prediction tasks . \\ > \\ > These papers and other methods for contact prediction beyond Gremlin are not described ... We will add a more thorough description of related work addressing unsupervised and supervised contact prediction ."}, {"review_id": "fylclEqgvgd-1", "review_text": "In this paper , the authors show that transformer protein language models can learn protein contacts from the unsupervised language modelling objectives . They also show that the residue-residue contacts can be extracted by sparse logistic regression to learn coefficients on the attention heads . One of the advantages of using transformers models is that they do not require an alignment step nor the use of specialized bioinformatics tools ( which are computationally expensive ) . When compared to a method based on multiple sequence alignment , the transformers models can obtain a similar or higher precision . Contributions of this paper are : - showing that the attention maps built in Transformer-based protein languages learn protein contacts , and when extracted , they perform competitively for protein contact prediction ; - a method for extracting attention maps from Transformer models ; - a comparison between a recent protein transformer protein language model ( which does dot require sequence alignment ) , and a pseudo-likelihood-based optimization method that uses multiple sequence alignment ; - an analysis of how much the supervised learning ( logistic regression ) contributes to the results . The paper covers a relevant topic and it is easy to read . However , I have a number of concerns . The main contribution of the paper is that attention maps built in Transformer-based protein languages learn protein contacts and can be used for protein contact prediction . However , this was reported before in Rives et al . ( 2019 ) ( doi : 10.1101/622803 ) . Also , several methods have been developed for this problem , but are not included in the comparisons . Finally , the provided implementation details are not sufficient to reproduce the results of the paper . I detail some of these concerns below , together with questions/suggestions for improvements : 1 ) I would recommend comparing transformers to other methods besides Gremlin , or justify why other methods were not included . This review can be helpful : ( Adhikari B , Cheng J. , 2016 .. doi : 10.1007/978-1-4939-3572-7_24 ) Also , more recent methods that were published after the review are : ( Badri Adhikari , 2020. https : //doi.org/10.1093/bioinformatics/btz593 ) ( Luttrell et al. , 2019. https : //doi.org/10.1186/s12859-019-2627-6 ) ( Gao et al.,2019.https : //doi.org/10.1038/s41598-019-40314-1 ) ( Ji S et al. , 2019. https : //doi.org/10.1371/journal.pone.0205214 ) 2 ) On page 7 , the authors state that `` We find that the logistic regression probabilities are reasonably well calibrated estimators of true contact probability and can be used directly as a measure of the model 's confidence ( Figure 10a ) '' . However , from the plot in Figure 10a , it is not totally clear that the probabilities are well calibrated . Could the authors add more justifications of why they consider it well calibrated ? Could they also show a comparison of the calibration of the other transformer models , perhaps using MSE as a calibration metric ? 3 ) To understand the occurence of false positives , the authors analyze the Manhattan distance between the predicted contact and the true contact , which is between 1 and 4 for most false positives . They also show an example of a homodimer , for which predictions were far from the true contacts , and explain that the model is picking up inter-chain interactions . Could the authors report how many predictions have a Manhattan distance larger than 4 ? Is this one example representative of the group of false positives far from the true contact ? Maybe the authors could analyse whether this happens in most of the cases . 4 ) While ESM-1 is open-source and publicly available , this is not the case for ESM-1b . In section A.5 , the authors provide implementation details as differences between ESM-1 and ESM-1b , stating \u201c Compared to ESM-1 , the main changes in ESM-1b are : higher learning rate ; dropout after word embedding ; learned positional embeddings ; final layer norm before the output ; and tied input/output word embeddings . The weights of all ESM models throughout the training process were provided to us by the authors. \u201d . In my opinion , this is not enough to reproduce the results in this paper . To make it reproducible , the authors need to provide a detailed enough description of the differences to make the reader able to implement ESM-1b , or provide the weights and hyperparameters required to reproduce their results .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their time , interest , and helpful critique . \\ > \\ > Contributions of this paper are [ ... ] showing that the attention maps built in Transformer-based protein languages learn protein contacts , and when extracted , they perform competitively for protein contact prediction ... We agree on the contributions the reviewer has identified . \\ > \\ > However , I have a number of concerns . Below and in the comment to all reviewers we outline a plan to address these concerns . \\ > \\ > However , this was reported before in Rives et al ( 2019 ) Rives et al.2019 does not report an analysis of attention maps . Rather , it uses the output from the final layer for supervised contact prediction . \\ > \\ > Also , several methods have been developed for this problem , but are not included in the comparisons . We believe that pseudolikelihood maximization ( and by extension Gremlin , which implements this method ) is the current state-of-the-art for unsupervised contact prediction . To address the concern we will add a comparison to the Evcouplings implementation of mean-field inference , and to the Psicov implementation of sparse inverse covariance matrix estimation . \\ > \\ > I would recommend comparing transformers to other methods besides Gremlin ... Thank you for the suggestion . Section 2.2. of Adhikari 2016 describes evolutionary coupling-based methods . We note that Gremlin is an implementation of the pseudolikelihood based methods discussed in this section . The mean field approximation is also discussed here , as well as sparse inverse covariance matrix estimation . We propose adding comparisons to the mean field approximation ( Evcouplings implementation ) and the sparse inverse covariance matrix ( Psicov implementation ) . \\ > \\ > Also , more recent methods that were published after the review are ... These citations are for supervised contact prediction methods which are all deep neural networks trained with supervision from many protein structures . A comparison to our unsupervised contact prediction method is not appropriate as the problem settings are fundamentally distinct . We will add a discussion of supervised methods to the related work section . \\ > \\ > However , from the plot in Figure 10a , it is not totally clear that the probabilities are well calibrated ... We agree that asserting the model is \u201c well calibrated \u201d is unclear without a baseline . Since it is not obvious what the correct baseline should be , we will reword this \u201c we see that the model \u2019 s predicted probability is correlated with the actual contact probability. \u201d We will add Pearson correlation between predicted and actual contact probability for the ESM1b model as well as the other transformer models as suggested . \\ > \\ > Could the authors report how many predictions have a Manhattan distance larger than 4 ... We appreciate the suggestions and will update the manuscript with the number of predictions with Manhattan distance greater than 4 . After the submission deadline we have analyzed an alternate failure mode where the hallucinated contact is not representative of a true contact . We will add an example of this failure mode as well . We note that in both the old and new failure mode , hallucinated contacts appear in the Gremlin contacts as well . An analysis of which failure modes are most common is a very interesting idea , but would require too much manual work to be completed in the rebuttal period . \\ > \\ > To make it reproducible ... We agree on the importance of reproducibility . We will make available contact prediction weights for ESM-1 and ESM-1b models allowing loading the models released by the ESM authors , along with a ` predict_contacts ` API . If you would like to review the code yourself , we will make the effort to anonymize it as much as possible ."}, {"review_id": "fylclEqgvgd-2", "review_text": "* * Summary * * The paper performs a number of analyses centered around the ability of transformer-based language models trained on protein sequence data to learn representations useful for predicting protein secondary and tertiary structure ( the latter as contact maps ) . Specifically , the paper studies several pre-trained transformer models by fitting an L1-penalized logistic regression to amino acid pair contacts . Several experiments are performed to showcase that ( i ) transformer-based representations can outperform state-of-the art methods based on MSA in terms of contact prediction precision ; ( ii ) that the necessary information for contact predictions in these representations is learned in an unsupervised manner ( and not by the logistic regression put on top of these representations ) ; and ( iii ) that the contact prediction probabilities are reasonably well calibrated . * * Score justification * * In its current form the paper presents interesting analyses , but has overall limited novelty . The ability of transformer models to learn representations predictive of secondary and tertiary structure has been demonstrated before ( including in the papers proposing the models used by the authors ) . Furthermore , I have some questions regarding the methodology employed by the authors . * * Major comments * * * The main metric employed by the authors is the precision of the top L ( protein length ) contact prediction for a given range ( P @ L ) . I wonder why the authors do not also consider recall at L as an accompanying metric for reporting the results . * When comparing ESM to the baseline Gremlin method , the authors consider two scenarios : ( i ) Gremlin trained on the trRosetta data ; and ( ii ) Gremlin trained on the same data as the ESM transformer model . Overall , Gremlin trained on the ESM data - which is arguably the correct baseline for the ESM model - performs worse than Gremlin trained on the trRosetta data . Why is that the case ? How does the procedure for preparing MSA for the ESM data compare to that of the trRosetta data ? Can it be tuned to improve Gremlin 's performance ? * The paper compares several transformer models that differ primarily in the model size , dataset size and hyper-parameters . As can be seen from Table 1 of the manuscript , these differences are clearly important for the contact prediction task and thus should be summarized and discussed in more detail . * From what I understand the sequences from the testing set of the contact prediction problem ( or sequences highly similar to them ) could appear in the training sets of the considered transformer models . This creates some information leakage . It 's unclear from the results presented in the paper whether it is an issue or not - how does contact prediction precision / recall change as sequence similarity to the ESM training set drops ? * The authors present analysis on the usefulness of the representations learned by various attention heads for contact prediction ; and on robustness of such predictions . I wonder how robust the results of these analyses are - they appear to have been performed using a single checkpoint of the ESM model , which is a result of stochastic training from random initialization . * In the Appendix the authors talk about the benefit of using predicted contact maps for inferring the all-atom protein 3D structure . However no results on this are presented . I would be very eager to see the comparison of 3D structure accuracy inferred with ESM-predicted and Gremlin-predicted contacts . * * Minor comments * * * Introduction talks about the ESM-1b model but ( as far as I can tell ) a reference is n't provided until a later section .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their time and interest in the paper and for helpful comments . \\ > \\ > Several experiments are performed to showcase that ( i ) transformer-based representations can outperform state-of-the art methods based on MSA in terms of contact prediction precision ; ( ii ) that the necessary information for contact predictions in these representations is learned in an unsupervised manner ( and not by the logistic regression put on top of these representations ) ; and ( iii ) that the contact prediction probabilities are reasonably well calibrated . \\ > \\ > In its current form the paper presents interesting analyses , but has overall limited novelty . The ability of transformer models to learn representations predictive of secondary and tertiary structure has been demonstrated before The reviewer \u2019 s main concern appears to be novelty . However we note that the reviewer agrees in point ( i ) above that the paper shows \u201c transformer-based representations can outperform state-of-the art methods based on MSA in terms of contact prediction precision. \u201d No prior work has shown state-of-the-art performance for unsupervised contact prediction from a protein language model . Unsupervised contact prediction is an important and well studied problem ( discussed in more depth in the comment to all reviewers ) that has seen little progress since the introduction of pseudolikelihood maximization -- the state-of-the-art baseline we use in this paper . Additionally in point ( ii ) the reviewer agrees \u201c that the necessary information for contact predictions in these representations is learned in an unsupervised manner. \u201d This is also an important contribution of the work -- this paper is the first to show that state-of-the-art contacts are learned by Transformer language models in an unsupervised and interpretable manner . We realize we have not well situated the paper w.r.t.the supervised contact prediction literature , and prior work with protein language models in the supervised setting . We will endeavor to address this in the revision incorporating feedback from reviewers and additional references ."}, {"review_id": "fylclEqgvgd-3", "review_text": "In this manuscript , the authors present a method for predicting residue-residue contacts within protein structures using the attention layers learned by transformer language models . Using the largest transformer language models trained to data , the authors show good performance for contact prediction . The paper is clearly written and easy to follow . The general concept of fine tuning protein language models for contact prediction has circulated for some time which lessens the core contribution , but the authors approach is surprisingly data efficient and accurate . Overall this is an interesting work , though there is quite a bit of background on contact prediction missing . This paper is also very application specific and may not present new machine learning methods of general interest to the ICLR community . The existence of previous language model-based contact prediction methods reduces the novelty of this work , especially given that the model used here is from Rives et al.2019 , who already look at contact prediction . Furthermore , no comparisons with state-of-the-art evolutionary coupling-based or language model-based contact prediction methods are performed . With this in mind , the manuscript may be better suited to submission at a biology specific venue . Additional specific comments follow below . Major comments : 1 . Missing related work : there are a number of highly relevant prior works that are not mentioned/discussed . In particular , \u201c Deep generative models of genetic variation capture the effects of mutations \u201d \u2013 Riesselman et al.2018 was , as far as I know , the first paper to show that deep generative models capture structure information ( see Figure 6 ) . Following that , \u201c Learning protein sequence embeddings using information from structure \u201d \u2013 Bepler & Berger 2019 was , to my knowledge , the first paper to propose deep language models ( alignment free ) for learning protein sequence representations and used those unsupervised representations for contact prediction . Furthermore , there has been extensive work in improving contact prediction using sequence + co-evolutionary features . See , for example , \u201c Enhancing Evolutionary Couplings with Deep Convolutional Neural Networks \u201d Liu et al.2018 and \u201c Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model \u201d Wang et al.2017.Other papers looking at protein structure prediction from sequence with deep learning , though they are less directly relevant , include \u201c End-to-End Differentiable Learning of Protein Structure \u201d AlQuraishi 2018 and \u201c Learning Protein Structure with a Differentiable Simulator \u201d Ingraham 2019 . 2.Before this work , others have looked at fine tuning language models for contact prediction . How do those approaches compare with the approach presented here ? Rives et al look at contact prediction in their manuscript describing the transformer model ( which is the same model used here ) on CASP 11-13 ( see Table 5 in their manuscript ) . How does that approach compare with this one ? Likewise for Bepler & Berger 3 . Many methods have surpassed GREMLIN for contact prediction using evolutionary couplings . How do those approaches compare with this one ? It would be helpful to see how this approach compares with truly state-of-the-art contact prediction methods . Reporting results on the CASP data would help to make this comparison . Minor Comments : 1 . Although multiple sequence alignment methods have challenges especially as related to evolutionary coupling prediction , these methods have been heavily optimized for decades . The authors should provide citations for claimed failings such as \u201c failure to find an optimal alignment \u201d and \u201c suboptimality of the substitution matrix and gap penalty. \u201d Certainly , these may be sources of error in alignments , but I am not aware of any studies of the frequency or impacts of these errors on evolutionary coupling analysis . If these studies exist , I encourage the authors to cite them . If they do not exist , I suggest the authors focus on well known sources of error here ( namely , alignment depth ) and provide references . 2.The authors use the language model without fine tuning , but the model could be fine tuned for each protein using its MSA . It \u2019 s great that contacts can be predicted without fine tuning , but it would be interesting to investigate whether additional gains can be made . 3.Eight iterations of jackhmmer is a lot . In my personal experience , jackhmmer often diverges at 3+ iterations . By this I mean , the set of sequences and HMM learned by jackhmmer drift far away from the original sequence/family . Did the authors perform and quality checks of these alignments to ensure jackhmmer did not diverge ? 4.How are sequence depths in Figure 3 calculated ? Is this the raw number of sequences in each MSA or is it after applying some sort of neighborhood weighting to calculate an effective number of sequences ? Things that would improve my rating : 1 . Provide a more comprehensive background review . 2.Compare with state-of-the-art evolutionary coupling-based contact prediction methods . 3.Compare with other language model-based contact prediction methods . 4.What should interest the general machine learning community about this paper ? What can we learn that might lead to better ML methods in the future ? Convince me that this doesn \u2019 t belong in a bioinformatics venue !", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their time and attention to the paper and for detailed comments . \\ > > The general concept of fine tuning protein language models for contact prediction has circulated for some time which lessens the core contribution , \\ > > The existence of previous language model-based contact prediction methods reduces the novelty of this work , especially given that the model used here is from Rives et al.2019 , who already look at contact prediction . While protein language models have been studied for contact prediction , e.g.Rives et al.2019 , Rao et al.2019 , this has been in the supervised setting . No existing work applies the models to the unsupervised contact prediction problem . This is the first work to demonstrate that unsupervised learning from a protein language model exceeds performance of state-of-the-art evolutionary couplings based unsupervised contact prediction . \\ > > Overall this is an interesting work , though there is quite a bit of background on contact prediction missing . Thank you for pointing out additional references . We will add a related work section covering contact prediction and other topics . \\ > > This paper is also very application specific and may not present new machine learning methods of general interest to the ICLR community . \\ > > With this in mind , the manuscript may be better suited to submission at a biology specific venue . We respectfully disagree . In this paper we propose an interpretable machine learning model that achieves state-of-the-art performance on an important unsupervised learning task in structural biology . This provides strong evidence that attention-based representations produced by unsupervised language modeling objectives can directly represent physical structures , which is of interest to the ICLR community . \\ > > Furthermore , no comparisons with state-of-the-art evolutionary coupling-based or language model-based contact prediction methods are performed . Pseudolikelihood maximization is the current state-of-the-art for unsupervised contact prediction ( we use the Gremlin implementation ) . We will also add mean-field DCA ( as implemented by Evcouplings ) and sparse inverse covariance ( Psicov implementation ) as comparisons . There are no unsupervised language model-based contact prediction methods for comparison ."}], "0": {"review_id": "fylclEqgvgd-0", "review_text": "# # Summary The paper shows that Transformers trained unsupervised on millions of protein sequences learn information about protein contacts by using attention maps for contact prediction . The paper is mostly clearly written and discusses server interesting ablation experiments . However , two recent papers that appeared on arXiv before the ICLR submission deadlines also use Transformers for protein contact prediction . These papers and other methods for contact prediction beyond Gremlin are not described . I therefore consider the contributions as insufficient for an ICLR submission . # # Major comments 1 . Using Transformer attention maps for protein contact prediction is not new . See Rives et al , 2020 , \u2018 Biological structures and functions emerge\u2026 \u2019 , section 5.2 , and Vig et al , 2020 , \u2018 Bertology \u2019 section 4.2 . Both publications appeared on arXiv at least one month before the ICLR submission deadline and are not clearly discussed in the paper . 2.The introductions discusses existing work on Transformers for protein languages models . Existing methods for contact prediction ( beyond Gremlin ) , however , are not described sufficiently . 3.It is unclear which sequences were used for training the Transformer models and how similar they are to test sequences . 4.The paper compares Transformers to Gremlin . However , it is unclear how well they perform to the CASP state-of-the art ( see also Rives et al , 2020 ) . 5.Section 3.4 does not describe clearly enough how attention maps were used for predicting contact maps . How were attention maps symmetrized ? Which layers and heads were used and how were they aggregated ? What is the number of resulting features that were used to train the logistic regression model ? APC is not described or cited . 6.Section 4.5 discusses that Transformers can be also used for secondary structure prediction . This is not new ( see Rives 2020 and Vig 2020 ) and does not fit well to the rest of the paper , which is about contact prediction . 6.Section 4.8 : Using transformers for generating proteins with natural properties is not new ( see Madani et al , 2020 , \u2018 ProGen \u2019 or Rives et al , 2020 ) . \u2018 Wang & Cho \u2019 were not the first who used Transformers generativity ( see Vaswani , 2017 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "\\ > \\ > The paper shows that Transformers trained unsupervised on millions of protein sequences learn information about protein contacts by using attention maps for contact prediction ... We thank the reviewer for their time , interest in the paper , and constructive feedback . \\ > \\ > However , two recent papers that appeared on arXiv before the ICLR submission deadlines also use Transformers for protein contact prediction . See Rives et al , 2020 , \u2018 Biological structures and functions emerge\u2026 \u2019 , section 5.2 , and Vig et al , 2020 , \u2018 Bertology \u2019 section 4.2 . This is the first paper to show state-of-the-art results for unsupervised contact prediction from a transformer protein language model . Prior work Rives et al.2020 benchmarks supervised contact prediction with deep residual networks . Vig et al.2020 show that one specific head of the TAPE transformer is correlated with contacts ( see Vig et al.2020 Fig 4 ) , but make no comparison to state-of-the-art methods for unsupervised contact prediction . In contrast , our work provides a new method that results in state-of-the-art performance on the unsupervised contact prediction problem . Moreover this is the first paper to demonstrate a state-of-the-art result for contact prediction from protein language modeling -- this is an important result for protein language modeling as previous work e.g.Rao et al.2019 , and Rives et al.2020 have shown that protein language models fall well below state-of-the-art performance on supervised contact prediction tasks . \\ > \\ > These papers and other methods for contact prediction beyond Gremlin are not described ... We will add a more thorough description of related work addressing unsupervised and supervised contact prediction ."}, "1": {"review_id": "fylclEqgvgd-1", "review_text": "In this paper , the authors show that transformer protein language models can learn protein contacts from the unsupervised language modelling objectives . They also show that the residue-residue contacts can be extracted by sparse logistic regression to learn coefficients on the attention heads . One of the advantages of using transformers models is that they do not require an alignment step nor the use of specialized bioinformatics tools ( which are computationally expensive ) . When compared to a method based on multiple sequence alignment , the transformers models can obtain a similar or higher precision . Contributions of this paper are : - showing that the attention maps built in Transformer-based protein languages learn protein contacts , and when extracted , they perform competitively for protein contact prediction ; - a method for extracting attention maps from Transformer models ; - a comparison between a recent protein transformer protein language model ( which does dot require sequence alignment ) , and a pseudo-likelihood-based optimization method that uses multiple sequence alignment ; - an analysis of how much the supervised learning ( logistic regression ) contributes to the results . The paper covers a relevant topic and it is easy to read . However , I have a number of concerns . The main contribution of the paper is that attention maps built in Transformer-based protein languages learn protein contacts and can be used for protein contact prediction . However , this was reported before in Rives et al . ( 2019 ) ( doi : 10.1101/622803 ) . Also , several methods have been developed for this problem , but are not included in the comparisons . Finally , the provided implementation details are not sufficient to reproduce the results of the paper . I detail some of these concerns below , together with questions/suggestions for improvements : 1 ) I would recommend comparing transformers to other methods besides Gremlin , or justify why other methods were not included . This review can be helpful : ( Adhikari B , Cheng J. , 2016 .. doi : 10.1007/978-1-4939-3572-7_24 ) Also , more recent methods that were published after the review are : ( Badri Adhikari , 2020. https : //doi.org/10.1093/bioinformatics/btz593 ) ( Luttrell et al. , 2019. https : //doi.org/10.1186/s12859-019-2627-6 ) ( Gao et al.,2019.https : //doi.org/10.1038/s41598-019-40314-1 ) ( Ji S et al. , 2019. https : //doi.org/10.1371/journal.pone.0205214 ) 2 ) On page 7 , the authors state that `` We find that the logistic regression probabilities are reasonably well calibrated estimators of true contact probability and can be used directly as a measure of the model 's confidence ( Figure 10a ) '' . However , from the plot in Figure 10a , it is not totally clear that the probabilities are well calibrated . Could the authors add more justifications of why they consider it well calibrated ? Could they also show a comparison of the calibration of the other transformer models , perhaps using MSE as a calibration metric ? 3 ) To understand the occurence of false positives , the authors analyze the Manhattan distance between the predicted contact and the true contact , which is between 1 and 4 for most false positives . They also show an example of a homodimer , for which predictions were far from the true contacts , and explain that the model is picking up inter-chain interactions . Could the authors report how many predictions have a Manhattan distance larger than 4 ? Is this one example representative of the group of false positives far from the true contact ? Maybe the authors could analyse whether this happens in most of the cases . 4 ) While ESM-1 is open-source and publicly available , this is not the case for ESM-1b . In section A.5 , the authors provide implementation details as differences between ESM-1 and ESM-1b , stating \u201c Compared to ESM-1 , the main changes in ESM-1b are : higher learning rate ; dropout after word embedding ; learned positional embeddings ; final layer norm before the output ; and tied input/output word embeddings . The weights of all ESM models throughout the training process were provided to us by the authors. \u201d . In my opinion , this is not enough to reproduce the results in this paper . To make it reproducible , the authors need to provide a detailed enough description of the differences to make the reader able to implement ESM-1b , or provide the weights and hyperparameters required to reproduce their results .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their time , interest , and helpful critique . \\ > \\ > Contributions of this paper are [ ... ] showing that the attention maps built in Transformer-based protein languages learn protein contacts , and when extracted , they perform competitively for protein contact prediction ... We agree on the contributions the reviewer has identified . \\ > \\ > However , I have a number of concerns . Below and in the comment to all reviewers we outline a plan to address these concerns . \\ > \\ > However , this was reported before in Rives et al ( 2019 ) Rives et al.2019 does not report an analysis of attention maps . Rather , it uses the output from the final layer for supervised contact prediction . \\ > \\ > Also , several methods have been developed for this problem , but are not included in the comparisons . We believe that pseudolikelihood maximization ( and by extension Gremlin , which implements this method ) is the current state-of-the-art for unsupervised contact prediction . To address the concern we will add a comparison to the Evcouplings implementation of mean-field inference , and to the Psicov implementation of sparse inverse covariance matrix estimation . \\ > \\ > I would recommend comparing transformers to other methods besides Gremlin ... Thank you for the suggestion . Section 2.2. of Adhikari 2016 describes evolutionary coupling-based methods . We note that Gremlin is an implementation of the pseudolikelihood based methods discussed in this section . The mean field approximation is also discussed here , as well as sparse inverse covariance matrix estimation . We propose adding comparisons to the mean field approximation ( Evcouplings implementation ) and the sparse inverse covariance matrix ( Psicov implementation ) . \\ > \\ > Also , more recent methods that were published after the review are ... These citations are for supervised contact prediction methods which are all deep neural networks trained with supervision from many protein structures . A comparison to our unsupervised contact prediction method is not appropriate as the problem settings are fundamentally distinct . We will add a discussion of supervised methods to the related work section . \\ > \\ > However , from the plot in Figure 10a , it is not totally clear that the probabilities are well calibrated ... We agree that asserting the model is \u201c well calibrated \u201d is unclear without a baseline . Since it is not obvious what the correct baseline should be , we will reword this \u201c we see that the model \u2019 s predicted probability is correlated with the actual contact probability. \u201d We will add Pearson correlation between predicted and actual contact probability for the ESM1b model as well as the other transformer models as suggested . \\ > \\ > Could the authors report how many predictions have a Manhattan distance larger than 4 ... We appreciate the suggestions and will update the manuscript with the number of predictions with Manhattan distance greater than 4 . After the submission deadline we have analyzed an alternate failure mode where the hallucinated contact is not representative of a true contact . We will add an example of this failure mode as well . We note that in both the old and new failure mode , hallucinated contacts appear in the Gremlin contacts as well . An analysis of which failure modes are most common is a very interesting idea , but would require too much manual work to be completed in the rebuttal period . \\ > \\ > To make it reproducible ... We agree on the importance of reproducibility . We will make available contact prediction weights for ESM-1 and ESM-1b models allowing loading the models released by the ESM authors , along with a ` predict_contacts ` API . If you would like to review the code yourself , we will make the effort to anonymize it as much as possible ."}, "2": {"review_id": "fylclEqgvgd-2", "review_text": "* * Summary * * The paper performs a number of analyses centered around the ability of transformer-based language models trained on protein sequence data to learn representations useful for predicting protein secondary and tertiary structure ( the latter as contact maps ) . Specifically , the paper studies several pre-trained transformer models by fitting an L1-penalized logistic regression to amino acid pair contacts . Several experiments are performed to showcase that ( i ) transformer-based representations can outperform state-of-the art methods based on MSA in terms of contact prediction precision ; ( ii ) that the necessary information for contact predictions in these representations is learned in an unsupervised manner ( and not by the logistic regression put on top of these representations ) ; and ( iii ) that the contact prediction probabilities are reasonably well calibrated . * * Score justification * * In its current form the paper presents interesting analyses , but has overall limited novelty . The ability of transformer models to learn representations predictive of secondary and tertiary structure has been demonstrated before ( including in the papers proposing the models used by the authors ) . Furthermore , I have some questions regarding the methodology employed by the authors . * * Major comments * * * The main metric employed by the authors is the precision of the top L ( protein length ) contact prediction for a given range ( P @ L ) . I wonder why the authors do not also consider recall at L as an accompanying metric for reporting the results . * When comparing ESM to the baseline Gremlin method , the authors consider two scenarios : ( i ) Gremlin trained on the trRosetta data ; and ( ii ) Gremlin trained on the same data as the ESM transformer model . Overall , Gremlin trained on the ESM data - which is arguably the correct baseline for the ESM model - performs worse than Gremlin trained on the trRosetta data . Why is that the case ? How does the procedure for preparing MSA for the ESM data compare to that of the trRosetta data ? Can it be tuned to improve Gremlin 's performance ? * The paper compares several transformer models that differ primarily in the model size , dataset size and hyper-parameters . As can be seen from Table 1 of the manuscript , these differences are clearly important for the contact prediction task and thus should be summarized and discussed in more detail . * From what I understand the sequences from the testing set of the contact prediction problem ( or sequences highly similar to them ) could appear in the training sets of the considered transformer models . This creates some information leakage . It 's unclear from the results presented in the paper whether it is an issue or not - how does contact prediction precision / recall change as sequence similarity to the ESM training set drops ? * The authors present analysis on the usefulness of the representations learned by various attention heads for contact prediction ; and on robustness of such predictions . I wonder how robust the results of these analyses are - they appear to have been performed using a single checkpoint of the ESM model , which is a result of stochastic training from random initialization . * In the Appendix the authors talk about the benefit of using predicted contact maps for inferring the all-atom protein 3D structure . However no results on this are presented . I would be very eager to see the comparison of 3D structure accuracy inferred with ESM-predicted and Gremlin-predicted contacts . * * Minor comments * * * Introduction talks about the ESM-1b model but ( as far as I can tell ) a reference is n't provided until a later section .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their time and interest in the paper and for helpful comments . \\ > \\ > Several experiments are performed to showcase that ( i ) transformer-based representations can outperform state-of-the art methods based on MSA in terms of contact prediction precision ; ( ii ) that the necessary information for contact predictions in these representations is learned in an unsupervised manner ( and not by the logistic regression put on top of these representations ) ; and ( iii ) that the contact prediction probabilities are reasonably well calibrated . \\ > \\ > In its current form the paper presents interesting analyses , but has overall limited novelty . The ability of transformer models to learn representations predictive of secondary and tertiary structure has been demonstrated before The reviewer \u2019 s main concern appears to be novelty . However we note that the reviewer agrees in point ( i ) above that the paper shows \u201c transformer-based representations can outperform state-of-the art methods based on MSA in terms of contact prediction precision. \u201d No prior work has shown state-of-the-art performance for unsupervised contact prediction from a protein language model . Unsupervised contact prediction is an important and well studied problem ( discussed in more depth in the comment to all reviewers ) that has seen little progress since the introduction of pseudolikelihood maximization -- the state-of-the-art baseline we use in this paper . Additionally in point ( ii ) the reviewer agrees \u201c that the necessary information for contact predictions in these representations is learned in an unsupervised manner. \u201d This is also an important contribution of the work -- this paper is the first to show that state-of-the-art contacts are learned by Transformer language models in an unsupervised and interpretable manner . We realize we have not well situated the paper w.r.t.the supervised contact prediction literature , and prior work with protein language models in the supervised setting . We will endeavor to address this in the revision incorporating feedback from reviewers and additional references ."}, "3": {"review_id": "fylclEqgvgd-3", "review_text": "In this manuscript , the authors present a method for predicting residue-residue contacts within protein structures using the attention layers learned by transformer language models . Using the largest transformer language models trained to data , the authors show good performance for contact prediction . The paper is clearly written and easy to follow . The general concept of fine tuning protein language models for contact prediction has circulated for some time which lessens the core contribution , but the authors approach is surprisingly data efficient and accurate . Overall this is an interesting work , though there is quite a bit of background on contact prediction missing . This paper is also very application specific and may not present new machine learning methods of general interest to the ICLR community . The existence of previous language model-based contact prediction methods reduces the novelty of this work , especially given that the model used here is from Rives et al.2019 , who already look at contact prediction . Furthermore , no comparisons with state-of-the-art evolutionary coupling-based or language model-based contact prediction methods are performed . With this in mind , the manuscript may be better suited to submission at a biology specific venue . Additional specific comments follow below . Major comments : 1 . Missing related work : there are a number of highly relevant prior works that are not mentioned/discussed . In particular , \u201c Deep generative models of genetic variation capture the effects of mutations \u201d \u2013 Riesselman et al.2018 was , as far as I know , the first paper to show that deep generative models capture structure information ( see Figure 6 ) . Following that , \u201c Learning protein sequence embeddings using information from structure \u201d \u2013 Bepler & Berger 2019 was , to my knowledge , the first paper to propose deep language models ( alignment free ) for learning protein sequence representations and used those unsupervised representations for contact prediction . Furthermore , there has been extensive work in improving contact prediction using sequence + co-evolutionary features . See , for example , \u201c Enhancing Evolutionary Couplings with Deep Convolutional Neural Networks \u201d Liu et al.2018 and \u201c Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model \u201d Wang et al.2017.Other papers looking at protein structure prediction from sequence with deep learning , though they are less directly relevant , include \u201c End-to-End Differentiable Learning of Protein Structure \u201d AlQuraishi 2018 and \u201c Learning Protein Structure with a Differentiable Simulator \u201d Ingraham 2019 . 2.Before this work , others have looked at fine tuning language models for contact prediction . How do those approaches compare with the approach presented here ? Rives et al look at contact prediction in their manuscript describing the transformer model ( which is the same model used here ) on CASP 11-13 ( see Table 5 in their manuscript ) . How does that approach compare with this one ? Likewise for Bepler & Berger 3 . Many methods have surpassed GREMLIN for contact prediction using evolutionary couplings . How do those approaches compare with this one ? It would be helpful to see how this approach compares with truly state-of-the-art contact prediction methods . Reporting results on the CASP data would help to make this comparison . Minor Comments : 1 . Although multiple sequence alignment methods have challenges especially as related to evolutionary coupling prediction , these methods have been heavily optimized for decades . The authors should provide citations for claimed failings such as \u201c failure to find an optimal alignment \u201d and \u201c suboptimality of the substitution matrix and gap penalty. \u201d Certainly , these may be sources of error in alignments , but I am not aware of any studies of the frequency or impacts of these errors on evolutionary coupling analysis . If these studies exist , I encourage the authors to cite them . If they do not exist , I suggest the authors focus on well known sources of error here ( namely , alignment depth ) and provide references . 2.The authors use the language model without fine tuning , but the model could be fine tuned for each protein using its MSA . It \u2019 s great that contacts can be predicted without fine tuning , but it would be interesting to investigate whether additional gains can be made . 3.Eight iterations of jackhmmer is a lot . In my personal experience , jackhmmer often diverges at 3+ iterations . By this I mean , the set of sequences and HMM learned by jackhmmer drift far away from the original sequence/family . Did the authors perform and quality checks of these alignments to ensure jackhmmer did not diverge ? 4.How are sequence depths in Figure 3 calculated ? Is this the raw number of sequences in each MSA or is it after applying some sort of neighborhood weighting to calculate an effective number of sequences ? Things that would improve my rating : 1 . Provide a more comprehensive background review . 2.Compare with state-of-the-art evolutionary coupling-based contact prediction methods . 3.Compare with other language model-based contact prediction methods . 4.What should interest the general machine learning community about this paper ? What can we learn that might lead to better ML methods in the future ? Convince me that this doesn \u2019 t belong in a bioinformatics venue !", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their time and attention to the paper and for detailed comments . \\ > > The general concept of fine tuning protein language models for contact prediction has circulated for some time which lessens the core contribution , \\ > > The existence of previous language model-based contact prediction methods reduces the novelty of this work , especially given that the model used here is from Rives et al.2019 , who already look at contact prediction . While protein language models have been studied for contact prediction , e.g.Rives et al.2019 , Rao et al.2019 , this has been in the supervised setting . No existing work applies the models to the unsupervised contact prediction problem . This is the first work to demonstrate that unsupervised learning from a protein language model exceeds performance of state-of-the-art evolutionary couplings based unsupervised contact prediction . \\ > > Overall this is an interesting work , though there is quite a bit of background on contact prediction missing . Thank you for pointing out additional references . We will add a related work section covering contact prediction and other topics . \\ > > This paper is also very application specific and may not present new machine learning methods of general interest to the ICLR community . \\ > > With this in mind , the manuscript may be better suited to submission at a biology specific venue . We respectfully disagree . In this paper we propose an interpretable machine learning model that achieves state-of-the-art performance on an important unsupervised learning task in structural biology . This provides strong evidence that attention-based representations produced by unsupervised language modeling objectives can directly represent physical structures , which is of interest to the ICLR community . \\ > > Furthermore , no comparisons with state-of-the-art evolutionary coupling-based or language model-based contact prediction methods are performed . Pseudolikelihood maximization is the current state-of-the-art for unsupervised contact prediction ( we use the Gremlin implementation ) . We will also add mean-field DCA ( as implemented by Evcouplings ) and sparse inverse covariance ( Psicov implementation ) as comparisons . There are no unsupervised language model-based contact prediction methods for comparison ."}}