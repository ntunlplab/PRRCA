{"year": "2018", "forum": "rylejExC-", "title": "Stochastic Training of Graph Convolutional Networks", "decision": "Reject", "meta_review": "The paper studies subsampling techniques necessary to handle large graphs with graph convolutional networks.  The paper introduces two ideas: (1) preprocessing for GCNs (basically replacing dropout followed by linear transformation with linear transformation followed by drop out); (2) adding control variates based on historical activations.  Both ideas seem useful (but (1) is more empirically useful than (2), Figure 4*). The paper contains a fair bit of math (analysis / justification of the method).\n\nOverall, the ideas are interesting and can be useful in practice. However, not all reviewers are convinced that the methods constitute a significant contribution.  There is also a question whether the math has much value (strong assumptions - also, from interpretation, may be too specific to the formulation of Kipf & Welling making it a bit narrow?).  Though I share these feelings and recommend rejection, I think that the reviewers 2 and 3 were a bit too harsh, and the scores do not reflect the quality of the paper.\n\n*Potential typo: Figure 4 -- should it be CV +PP rather than CV?\n\n+ an important problem\n+ can be useful in practical applications\n+ generally solid and sufficiently well written\n- significance not sufficient\n- math seems not terribly useful\n\n", "reviews": [{"review_id": "rylejExC--0", "review_text": "This paper proposes a new training method for graph convolutional networks. The experimental results look interesting. However, this paper has some issues. This paper is hard to read. There are some undefined or multi-used notations. For instance, sigma is used for two different meanings: an activation function and variance. Some details that need to be explained are omitted. For example, what kind of dropout is used to obtain the table and figures in Section 5? Forward and backward propagation processes are not clearly explained In section 4.2, it is not clear why we have to multiply sqrt{D}. Why should we make the variance from dropout sigma^2? Proposition 1 is wrong. First, \\|A\\|_\\infty should be max_{ij} |A_ij| not A_{ij}. Second, there is no order between \\|AB\\|_\\infty and \\|A\\|_\\infty \\|B\\|_\\infty. When A=[1 1] and B is the transpose matrix of A, \\|AB\\|_\\infty =2 and \\|A\\|_\\infty \\|B\\|_\\infty = 1. When, A\u2019=[1 -1] and B is the same matrix defined just before, \\|A\u2019 B \\|_\\infty = 0 and \\|A\u2019\\|_\\infty \\|B\\|_\\infty =1. So, both \\|AB\\|_\\infty \\le \\|A\\|_\\infty \\|B\\|_\\infty and \\|AB\\|_\\infty \\ge \\|A\\|_\\infty \\|B\\|_\\infty are not true. I cannot believe the proof of Theorem 2. ", "rating": "3: Clear rejection", "reply_text": "Thanks for your review ! The review mostly concerns about some unclarified details and typos in the paper . We addressed these concerns below . Meanwhile , the review does not mention any aspects about technical contribution itself . We think stochastic training for graph convolutional networks is very important for scaling up neural networks towards practical graphs and helping develop more expressive models . Our approach is significant both practically and theoretically . We can compute approximate gradients for GCNs at a cost similar with MLPs , while losing little convergence speed . We also provide new theoretical guarantees to reach the same training and testing performance of the exact algorithm . After we clarified all the mentioned details and typos , could you please also assess the work based on the technical contribution ? We are happy to give more clarifications if needed . Q1 : This paper is hard to read . There are some undefined or multi-used notations . For instance , sigma is used for two different meanings : an activation function and variance . Some details that need to be explained are omitted . For example , what kind of dropout is used to obtain the table and figures in Section 5 ? Forward and backward propagation processes are not clearly explained . We change the notation for variance from \\sigma^2 to s^2 . Throughout the paper we only have one kind of dropout ( Srivastava et al. , 2014 ) , which randomly zeros out features . The dropout operation in our paper is already explained in Eq . ( 1 ) .We add a pseudocode in appendix E to explain the forward and backward propagation processes . Basically , forward propagation is defined using Eq . ( 5 ) and Eq . ( 6 ) and backward propagation is simply computing the gradient of the objective with respect to the parameters automatically . Q2 : In section 4.2 , it is not clear why we have to multiply sqrt { D } . Why should we make the variance from dropout sigma^2 ? We multiply sqrt { D } so that the approximated term and the original term have the same mean and variance , based on the case study under the independent Gaussian assumption in Sec.4.3 ( See Q3 of Reviewer 2 for the justification of the assumption ) . Under the assumption , the activations h_1 , \u2026 , h_D are approximated by independent Gaussian random variables h_v~N ( \\mu_v , s_v^2 ) , and the randomness comes from randomly dropping out features from the feature vector x while computing h_v = activation ( PWx ) . We define s_v^2 to be the variance of the Gaussian random variable p_1h_1+\u2026+p_Dh_D . We separate p_1h_1+\u2026+p_Dh_D as p_1 ( h_1-\\mu_1 ) +\u2026+p_D ( h_D-\\mu_D ) ( which has zero mean ) and p_1\\mu_1+\u2026+p_D\\mu_D ( which is deterministic ) . We approximate the first term as sqrt { D } ( h_v \u2019 -\\mu_v \u2019 ) , where v \u2019 is selected uniformly from { 1 , \u2026 , D } . Because sqrt { D } ( h_v \u2019 -\\mu_v \u2019 ) and p_1 ( h_1-\\mu_1 ) +\u2026+p_D ( h_D-\\mu_D ) have the same expected mean and variance , as shown in Appendix C. For short , without loss of generality we assume that \\mu_1=\u2026=\\mu_D=0 . Then , Var [ h_1+\u2026+h_D ] =Var [ h_1 ] +\u2026+Var [ h_D ] =s_1^2+\u2026+s_D^2 ( because of independence ) . And E_ { v \u2019 } [ Var [ sqrt { D } h_v \u2019 ] ] =E_ { v \u2019 } [ Ds_v \u2019 ^2 ] = s_1^2+\u2026+s_D^2 . Q3 : Proposition 1 is wrong . First , \\|A\\|_\\infty should be max_ { ij } |A_ij| not A_ { ij } . Second , there is no order between \\|AB\\|_\\infty and \\|A\\|_\\infty \\|B\\|_\\infty . I can not believe the proof of Theorem 2 . Thanks for pointing out . The correct version should be \\|AB\\|_\\infty < = col ( A ) \\|A\\|_\\infty \\|B\\|_\\infty , where col ( A ) is the number of columns of A ( or number of rows of B ) . We updated Proposition 1 and its proof . Note that the constant col ( A ) is absorbed and does not affect the proof of Theorem 2 . Besides the proof , Theorem 2 is also verified empirically in Fig.2 , where the algorithm using CV \u2019 s approximated gradients ( CV+PP ) has almost an overlapping convergence curve with the algorithm using exact stochastic gradients ( Exact ) ."}, {"review_id": "rylejExC--1", "review_text": "The paper proposes a method to speed up the training of graph convolutional networks, which are quite slow for large graphs. The key insight is to improve the estimates of the average neighbor activations (via neighbor sampling) so that we can either sample less neighbors or have higher accuracy for the same number of sampled neighbors. The idea is quite simple: estimate the current average neighbor activations as a delta over the minibatch running average. I was hoping the method would also include importance sampling, but it doesn\u2019t. The assumption that activations in a graph convolution are independent Gaussians is quite odd (and unproven). Quality: Statistically, the paper seems sound. There are some odd assumptions (independent Gaussian activations in a graph convolution embedding?!?) but otherwise the proposed methodology is rather straightforward. Clarity: It is well written and the reader is able to follow most of the details. I wish the authors had spent more time discussing the independent Gaussian assumption, rather than just arguing that a graph convolution (where units are not interacting through a simple grid like in a CNN) is equivalent to the setting of Wang and Manning (I don\u2019t see the equivalence). Wang and Manning are looking at MLPs, not even CNNs, which clearly have more independent activations than a CNN or a graph convolution. Significance: Not very significant. The problem of computing better averages for a specific problem (neighbor embedding average) seems a bit too narrow. The solution is straightforward, while some of the approximations make some odd simplifying assumptions (independent activations in a convolution, infinitesimal learning rates). Theorem 2 is not too useful, unfortunately: Showing that the estimated gradient is asymptotically unbiased with learning rates approaching zero over Lipchitz functions does not seem like an useful statement. Learning rates will never be close enough to zero (specially for large batch sizes). And if the running activation average converges to the true value, the training is probably over. The method should show it helps when the values are oscillating in the early stages of the training, not when the training is done near the local optimum. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the valuable comments . We address the detailed questions below . Q1 : I was hoping the method would also include importance sampling : Importance sampling is a useful technique . There is another submission about importance sampling for graph convolutional networks [ 1 ] . We have some remarks regarding to importance sampling [ 1 ] : 1 ) Our result is already close to the best we can possibly do , so importance sampling may only have marginal improvement . Despite using the much cheaper control-variate based gradient , we almost lost no convergence speed comparing with exact gradients ( without neighbor sampling ) , according to Fig.2 and Fig.3.2 ) Importance sampling and our control variate & preprocessing are orthogonal techniques for reducing the bias and variance of the gradient . 3 ) Our control variate based gradient estimator is asymptotically * unbiased * . As the learning rate goes to zero , our estimator yields unbiased stochastic gradient , * regardless of the neighbor sampling size * . On the other hand , the importance sampling based estimator is only * consistent * . It is unbiased * only when the neighbor sampling size goes to infinity * . This result is also shown experimentally : our work uses a very small neighbor sampling size ( e.g. , 2 neighbors for each node ) , while the neighbor sampling size of [ 1 ] is still hundreds . It takes only 50 seconds for our algorithm training on the largest Reddit dataset , while [ 1 ] takes 638.6 seconds . [ 1 ] FastGCN : Fast Learning with Graph Convolutional Networks via Importance Sampling . https : //openreview.net/forum ? id=rytstxWAW Q2 : The assumption that activations in a graph convolution are independent Gaussians is quite odd ( and unproven ) . I wish the authors had spent more time discussing the independent Gaussian assumption , rather than just arguing that a graph convolution ( where units are not interacting through a simple grid like in a CNN ) is equivalent to the setting of Wang and Manning ( I don \u2019 t see the equivalence ) . Wang and Manning are looking at MLPs , not even CNNs , which clearly have more independent activations than a CNN or a graph convolution . The assumption makes some sense intuitively . 1 ) If all the nodes are isolated , it reduces to the MLP case that Wang and Manning considered . 2 ) In two-layer GCNs where the first layer is pre-processed , which is the most popular architecture , we can show that the neighbors \u2019 activations are indeed independent with each other . 3 ) In deeper GCNs , the correlations between neighbor \u2019 s may still be weak in our algorithm , because the sampled subgraph is very sparse ( each node only picks itself and another random neighbor ) . Now we show that in two-layer GCNs where the first layer is pre-processed , the neighbors \u2019 activations are indeed independent with each other ( we added the discussions in Appendix G ) . Assume that we want to compute the gradient w.r.t.node \u201c a \u201d on the second layer , the computational graph looks like : Layer 2 : a Layer 1 : a b ( b is a random neighbor of a ) By Eq . ( 3 ) , h_a^1 = \\sigma ( Dropout ( u^0_a ) W^0 ) and h_b^1 = \\sigma ( Dropout ( u^0_b ) W^0 ) , where U^0=PH^0 . The independent Gaussian assumption states that h_a^1 and h_b^1 are independent . To show this , we need the Lemma ( function of independent r.v.s ) : If a and b are independent r.v . s , then f_1 ( a ) and f_2 ( b ) are independent r.v . s https : //math.stackexchange.com/questions/8742/are-functions-of-independent-variables-also-independent Let . * be the element-wise product . We have h_a^1 = f_1 ( \\phi_a ) : = \\sigma ( \\phi_a . * u^0_a ) W^0 and h_b^1 : = f_2 ( \\phi_b ) = \\sigma ( \\phi_b . * u^0_b ) W^0 . Because the dropout masks \\phi_a and \\phi_b are independent , we know that h_a^1 and h_b^1 are independent by the lemma . The rest assumptions about the Gaussian approximation and the independence between feature dimensions are discussed in Wang and Manning . We admit that the independent Gaussian assumption is somewhat rough . However , we do not explicitly rely on the independent Gaussian assumption like Wang and Manning , where they directly compute the mean and variance for the activation , and manually derive update rules of the mean and variance after each layer . Our algorithm only requires the samples , and the algorithm itself can execute regardless of the distribution of activation . Overall , the assumption is more like a motivating case ( in which the algorithm works perfectly ) rather than a must-hold condition for the algorithm to work . In practice , our estimator does have smaller bias & variance than the estimator without control variates ( Fig.5 ) , although the condition does not hold perfectly . Furthermore , our main theoretical result ( Theorem 2 ) does not depend on the independent Gaussian assumption ."}, {"review_id": "rylejExC--2", "review_text": "Existing training algorithms for graph convolutional nets are slow. This paper develops new novel methods, with a nice mix of theory, practicalities, and experiments. Let me caution that I am not familiar with convolutional nets applied to graph data. Clearly, the existing best algorithm - neighborhood sampling is slow as well as not theoretically sound. This paper proposes two key ideas - preprocessing and better sampling based on historical activations. The value of these ideas is demonstrated very well via theoretical and experimental analysis. I have skimmed through the theoretical analysis. They seem fine, but I haven't carefully gone through the details in the appendices. All the nets considered in the experiments have two layers. The role of preprocessing to add efficiency is important here. It would be useful to know how much the training speed will suffer if we use three or more layers, say, via one more experiment on a couple of key datasets. This will help see the limitations of the ideas proposed in this paper. In subsection 4.3 the authors prove reduced variance under certain assumptions. While I can see that this is done to make the analysis simple, how well does this analysis correlate with what is seen in practice? For example, how well does the analysis results given in Table 2 correlate with the standard deviation numbers of Figure 5 especially when comparing NS+PP and CV+PP?", "rating": "7: Good paper, accept", "reply_text": "Thanks for the review ! We addressed the comments below . Q1 : How much the training speed will suffer if we use three or more layers , say , via one more experiment on a couple of key datasets . Thanks for the suggestion . We added the results for three-layer networks in appendix F on the Reddit dataset . The exact algorithm takes tens of thousands per epoch on the original graph ( max degree is 128 ) . We subsampled the graph so that the max degree is 10 , CVD+PP is about 6 times faster than Exact to converge to 0.94 testing accuracy , and the convergences speed are reported in Fig.6.The observations are pretty much the same , that control variate based algorithms are much better than those without control variates . Q2 : Subsection 4.3 : the authors prove reduced variance under certain assumptions . How well does this analysis correlate with what is seen in practice ? For example , how well does the analysis results given in Table 2 correlate with the standard deviation numbers of Fig.5 especially when comparing NS+PP and CV+PP ? For models without dropout , the main theoretical result is Theorem 1 , which states that CV+PP has zero bias & variance as the learning rate goes to zero , and the independent Gaussian assumption is not needed . Fig.5 ( top row ) shows that the bias and variance of CV+PP are quite close to zero in practice , which matches the theoretical result . For models with dropout , we found that the standard deviations ( Fig.5 bottom right ) of CV+PP and CVD+PP were greatly reduced from NS+PP , mostly because of the reduction of VMCA . The bias was not always reduced , which calls better treatment of the term ( h_v - \\mu_v ) in Sec.4.2.We do not use historical values for this term . Incorporating historical values for this term may further reduce the bias and generalize Theorem 2 ( which does not rely on the independent Gaussian assumption ) to the dropout case . This is one possible future direction . Q3 : The paper may not appeal to a general audience since the ideas are very specific to graph convolutions , which itself is restricted only to data connected by a graph structure . Graph-structured data is prevalent , e.g. , user-graphs , citation graphs , web pages , knowledge graphs , etc . Moreover , graphs are generalization of many data structures , e.g. , an image can be represented by 2d lattices ; and a document categorization task can be improved by utilizing the citations between them . We therefore think extending deep learning to graph-structured data is important ."}], "0": {"review_id": "rylejExC--0", "review_text": "This paper proposes a new training method for graph convolutional networks. The experimental results look interesting. However, this paper has some issues. This paper is hard to read. There are some undefined or multi-used notations. For instance, sigma is used for two different meanings: an activation function and variance. Some details that need to be explained are omitted. For example, what kind of dropout is used to obtain the table and figures in Section 5? Forward and backward propagation processes are not clearly explained In section 4.2, it is not clear why we have to multiply sqrt{D}. Why should we make the variance from dropout sigma^2? Proposition 1 is wrong. First, \\|A\\|_\\infty should be max_{ij} |A_ij| not A_{ij}. Second, there is no order between \\|AB\\|_\\infty and \\|A\\|_\\infty \\|B\\|_\\infty. When A=[1 1] and B is the transpose matrix of A, \\|AB\\|_\\infty =2 and \\|A\\|_\\infty \\|B\\|_\\infty = 1. When, A\u2019=[1 -1] and B is the same matrix defined just before, \\|A\u2019 B \\|_\\infty = 0 and \\|A\u2019\\|_\\infty \\|B\\|_\\infty =1. So, both \\|AB\\|_\\infty \\le \\|A\\|_\\infty \\|B\\|_\\infty and \\|AB\\|_\\infty \\ge \\|A\\|_\\infty \\|B\\|_\\infty are not true. I cannot believe the proof of Theorem 2. ", "rating": "3: Clear rejection", "reply_text": "Thanks for your review ! The review mostly concerns about some unclarified details and typos in the paper . We addressed these concerns below . Meanwhile , the review does not mention any aspects about technical contribution itself . We think stochastic training for graph convolutional networks is very important for scaling up neural networks towards practical graphs and helping develop more expressive models . Our approach is significant both practically and theoretically . We can compute approximate gradients for GCNs at a cost similar with MLPs , while losing little convergence speed . We also provide new theoretical guarantees to reach the same training and testing performance of the exact algorithm . After we clarified all the mentioned details and typos , could you please also assess the work based on the technical contribution ? We are happy to give more clarifications if needed . Q1 : This paper is hard to read . There are some undefined or multi-used notations . For instance , sigma is used for two different meanings : an activation function and variance . Some details that need to be explained are omitted . For example , what kind of dropout is used to obtain the table and figures in Section 5 ? Forward and backward propagation processes are not clearly explained . We change the notation for variance from \\sigma^2 to s^2 . Throughout the paper we only have one kind of dropout ( Srivastava et al. , 2014 ) , which randomly zeros out features . The dropout operation in our paper is already explained in Eq . ( 1 ) .We add a pseudocode in appendix E to explain the forward and backward propagation processes . Basically , forward propagation is defined using Eq . ( 5 ) and Eq . ( 6 ) and backward propagation is simply computing the gradient of the objective with respect to the parameters automatically . Q2 : In section 4.2 , it is not clear why we have to multiply sqrt { D } . Why should we make the variance from dropout sigma^2 ? We multiply sqrt { D } so that the approximated term and the original term have the same mean and variance , based on the case study under the independent Gaussian assumption in Sec.4.3 ( See Q3 of Reviewer 2 for the justification of the assumption ) . Under the assumption , the activations h_1 , \u2026 , h_D are approximated by independent Gaussian random variables h_v~N ( \\mu_v , s_v^2 ) , and the randomness comes from randomly dropping out features from the feature vector x while computing h_v = activation ( PWx ) . We define s_v^2 to be the variance of the Gaussian random variable p_1h_1+\u2026+p_Dh_D . We separate p_1h_1+\u2026+p_Dh_D as p_1 ( h_1-\\mu_1 ) +\u2026+p_D ( h_D-\\mu_D ) ( which has zero mean ) and p_1\\mu_1+\u2026+p_D\\mu_D ( which is deterministic ) . We approximate the first term as sqrt { D } ( h_v \u2019 -\\mu_v \u2019 ) , where v \u2019 is selected uniformly from { 1 , \u2026 , D } . Because sqrt { D } ( h_v \u2019 -\\mu_v \u2019 ) and p_1 ( h_1-\\mu_1 ) +\u2026+p_D ( h_D-\\mu_D ) have the same expected mean and variance , as shown in Appendix C. For short , without loss of generality we assume that \\mu_1=\u2026=\\mu_D=0 . Then , Var [ h_1+\u2026+h_D ] =Var [ h_1 ] +\u2026+Var [ h_D ] =s_1^2+\u2026+s_D^2 ( because of independence ) . And E_ { v \u2019 } [ Var [ sqrt { D } h_v \u2019 ] ] =E_ { v \u2019 } [ Ds_v \u2019 ^2 ] = s_1^2+\u2026+s_D^2 . Q3 : Proposition 1 is wrong . First , \\|A\\|_\\infty should be max_ { ij } |A_ij| not A_ { ij } . Second , there is no order between \\|AB\\|_\\infty and \\|A\\|_\\infty \\|B\\|_\\infty . I can not believe the proof of Theorem 2 . Thanks for pointing out . The correct version should be \\|AB\\|_\\infty < = col ( A ) \\|A\\|_\\infty \\|B\\|_\\infty , where col ( A ) is the number of columns of A ( or number of rows of B ) . We updated Proposition 1 and its proof . Note that the constant col ( A ) is absorbed and does not affect the proof of Theorem 2 . Besides the proof , Theorem 2 is also verified empirically in Fig.2 , where the algorithm using CV \u2019 s approximated gradients ( CV+PP ) has almost an overlapping convergence curve with the algorithm using exact stochastic gradients ( Exact ) ."}, "1": {"review_id": "rylejExC--1", "review_text": "The paper proposes a method to speed up the training of graph convolutional networks, which are quite slow for large graphs. The key insight is to improve the estimates of the average neighbor activations (via neighbor sampling) so that we can either sample less neighbors or have higher accuracy for the same number of sampled neighbors. The idea is quite simple: estimate the current average neighbor activations as a delta over the minibatch running average. I was hoping the method would also include importance sampling, but it doesn\u2019t. The assumption that activations in a graph convolution are independent Gaussians is quite odd (and unproven). Quality: Statistically, the paper seems sound. There are some odd assumptions (independent Gaussian activations in a graph convolution embedding?!?) but otherwise the proposed methodology is rather straightforward. Clarity: It is well written and the reader is able to follow most of the details. I wish the authors had spent more time discussing the independent Gaussian assumption, rather than just arguing that a graph convolution (where units are not interacting through a simple grid like in a CNN) is equivalent to the setting of Wang and Manning (I don\u2019t see the equivalence). Wang and Manning are looking at MLPs, not even CNNs, which clearly have more independent activations than a CNN or a graph convolution. Significance: Not very significant. The problem of computing better averages for a specific problem (neighbor embedding average) seems a bit too narrow. The solution is straightforward, while some of the approximations make some odd simplifying assumptions (independent activations in a convolution, infinitesimal learning rates). Theorem 2 is not too useful, unfortunately: Showing that the estimated gradient is asymptotically unbiased with learning rates approaching zero over Lipchitz functions does not seem like an useful statement. Learning rates will never be close enough to zero (specially for large batch sizes). And if the running activation average converges to the true value, the training is probably over. The method should show it helps when the values are oscillating in the early stages of the training, not when the training is done near the local optimum. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the valuable comments . We address the detailed questions below . Q1 : I was hoping the method would also include importance sampling : Importance sampling is a useful technique . There is another submission about importance sampling for graph convolutional networks [ 1 ] . We have some remarks regarding to importance sampling [ 1 ] : 1 ) Our result is already close to the best we can possibly do , so importance sampling may only have marginal improvement . Despite using the much cheaper control-variate based gradient , we almost lost no convergence speed comparing with exact gradients ( without neighbor sampling ) , according to Fig.2 and Fig.3.2 ) Importance sampling and our control variate & preprocessing are orthogonal techniques for reducing the bias and variance of the gradient . 3 ) Our control variate based gradient estimator is asymptotically * unbiased * . As the learning rate goes to zero , our estimator yields unbiased stochastic gradient , * regardless of the neighbor sampling size * . On the other hand , the importance sampling based estimator is only * consistent * . It is unbiased * only when the neighbor sampling size goes to infinity * . This result is also shown experimentally : our work uses a very small neighbor sampling size ( e.g. , 2 neighbors for each node ) , while the neighbor sampling size of [ 1 ] is still hundreds . It takes only 50 seconds for our algorithm training on the largest Reddit dataset , while [ 1 ] takes 638.6 seconds . [ 1 ] FastGCN : Fast Learning with Graph Convolutional Networks via Importance Sampling . https : //openreview.net/forum ? id=rytstxWAW Q2 : The assumption that activations in a graph convolution are independent Gaussians is quite odd ( and unproven ) . I wish the authors had spent more time discussing the independent Gaussian assumption , rather than just arguing that a graph convolution ( where units are not interacting through a simple grid like in a CNN ) is equivalent to the setting of Wang and Manning ( I don \u2019 t see the equivalence ) . Wang and Manning are looking at MLPs , not even CNNs , which clearly have more independent activations than a CNN or a graph convolution . The assumption makes some sense intuitively . 1 ) If all the nodes are isolated , it reduces to the MLP case that Wang and Manning considered . 2 ) In two-layer GCNs where the first layer is pre-processed , which is the most popular architecture , we can show that the neighbors \u2019 activations are indeed independent with each other . 3 ) In deeper GCNs , the correlations between neighbor \u2019 s may still be weak in our algorithm , because the sampled subgraph is very sparse ( each node only picks itself and another random neighbor ) . Now we show that in two-layer GCNs where the first layer is pre-processed , the neighbors \u2019 activations are indeed independent with each other ( we added the discussions in Appendix G ) . Assume that we want to compute the gradient w.r.t.node \u201c a \u201d on the second layer , the computational graph looks like : Layer 2 : a Layer 1 : a b ( b is a random neighbor of a ) By Eq . ( 3 ) , h_a^1 = \\sigma ( Dropout ( u^0_a ) W^0 ) and h_b^1 = \\sigma ( Dropout ( u^0_b ) W^0 ) , where U^0=PH^0 . The independent Gaussian assumption states that h_a^1 and h_b^1 are independent . To show this , we need the Lemma ( function of independent r.v.s ) : If a and b are independent r.v . s , then f_1 ( a ) and f_2 ( b ) are independent r.v . s https : //math.stackexchange.com/questions/8742/are-functions-of-independent-variables-also-independent Let . * be the element-wise product . We have h_a^1 = f_1 ( \\phi_a ) : = \\sigma ( \\phi_a . * u^0_a ) W^0 and h_b^1 : = f_2 ( \\phi_b ) = \\sigma ( \\phi_b . * u^0_b ) W^0 . Because the dropout masks \\phi_a and \\phi_b are independent , we know that h_a^1 and h_b^1 are independent by the lemma . The rest assumptions about the Gaussian approximation and the independence between feature dimensions are discussed in Wang and Manning . We admit that the independent Gaussian assumption is somewhat rough . However , we do not explicitly rely on the independent Gaussian assumption like Wang and Manning , where they directly compute the mean and variance for the activation , and manually derive update rules of the mean and variance after each layer . Our algorithm only requires the samples , and the algorithm itself can execute regardless of the distribution of activation . Overall , the assumption is more like a motivating case ( in which the algorithm works perfectly ) rather than a must-hold condition for the algorithm to work . In practice , our estimator does have smaller bias & variance than the estimator without control variates ( Fig.5 ) , although the condition does not hold perfectly . Furthermore , our main theoretical result ( Theorem 2 ) does not depend on the independent Gaussian assumption ."}, "2": {"review_id": "rylejExC--2", "review_text": "Existing training algorithms for graph convolutional nets are slow. This paper develops new novel methods, with a nice mix of theory, practicalities, and experiments. Let me caution that I am not familiar with convolutional nets applied to graph data. Clearly, the existing best algorithm - neighborhood sampling is slow as well as not theoretically sound. This paper proposes two key ideas - preprocessing and better sampling based on historical activations. The value of these ideas is demonstrated very well via theoretical and experimental analysis. I have skimmed through the theoretical analysis. They seem fine, but I haven't carefully gone through the details in the appendices. All the nets considered in the experiments have two layers. The role of preprocessing to add efficiency is important here. It would be useful to know how much the training speed will suffer if we use three or more layers, say, via one more experiment on a couple of key datasets. This will help see the limitations of the ideas proposed in this paper. In subsection 4.3 the authors prove reduced variance under certain assumptions. While I can see that this is done to make the analysis simple, how well does this analysis correlate with what is seen in practice? For example, how well does the analysis results given in Table 2 correlate with the standard deviation numbers of Figure 5 especially when comparing NS+PP and CV+PP?", "rating": "7: Good paper, accept", "reply_text": "Thanks for the review ! We addressed the comments below . Q1 : How much the training speed will suffer if we use three or more layers , say , via one more experiment on a couple of key datasets . Thanks for the suggestion . We added the results for three-layer networks in appendix F on the Reddit dataset . The exact algorithm takes tens of thousands per epoch on the original graph ( max degree is 128 ) . We subsampled the graph so that the max degree is 10 , CVD+PP is about 6 times faster than Exact to converge to 0.94 testing accuracy , and the convergences speed are reported in Fig.6.The observations are pretty much the same , that control variate based algorithms are much better than those without control variates . Q2 : Subsection 4.3 : the authors prove reduced variance under certain assumptions . How well does this analysis correlate with what is seen in practice ? For example , how well does the analysis results given in Table 2 correlate with the standard deviation numbers of Fig.5 especially when comparing NS+PP and CV+PP ? For models without dropout , the main theoretical result is Theorem 1 , which states that CV+PP has zero bias & variance as the learning rate goes to zero , and the independent Gaussian assumption is not needed . Fig.5 ( top row ) shows that the bias and variance of CV+PP are quite close to zero in practice , which matches the theoretical result . For models with dropout , we found that the standard deviations ( Fig.5 bottom right ) of CV+PP and CVD+PP were greatly reduced from NS+PP , mostly because of the reduction of VMCA . The bias was not always reduced , which calls better treatment of the term ( h_v - \\mu_v ) in Sec.4.2.We do not use historical values for this term . Incorporating historical values for this term may further reduce the bias and generalize Theorem 2 ( which does not rely on the independent Gaussian assumption ) to the dropout case . This is one possible future direction . Q3 : The paper may not appeal to a general audience since the ideas are very specific to graph convolutions , which itself is restricted only to data connected by a graph structure . Graph-structured data is prevalent , e.g. , user-graphs , citation graphs , web pages , knowledge graphs , etc . Moreover , graphs are generalization of many data structures , e.g. , an image can be represented by 2d lattices ; and a document categorization task can be improved by utilizing the citations between them . We therefore think extending deep learning to graph-structured data is important ."}}