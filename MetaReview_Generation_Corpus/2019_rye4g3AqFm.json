{"year": "2019", "forum": "rye4g3AqFm", "title": "Deep learning generalizes because the parameter-function map is biased towards simple functions", "decision": "Accept (Poster)", "meta_review": "Dear authors,\n\nThere was some disagreement among reviewers on the significance of your results, in particular because of the limited experimental section.\n\nDespite this issues, which is not minor, your work adds yet another piece of the generalization puzzle. However, I would encourage the authors to make sure they do not oversell their results, either in the title or in their text, for the final version.", "reviews": [{"review_id": "rye4g3AqFm-0", "review_text": "The paper brings a fresh study to the generalization capabilities of (deep) neural networks, with the help of an original use of PAC-Bayesian learning theory and some empirically backed intuitions. Expressing the prior over the input-output function space generated by the neural network is very interesting. This provides an original analysis compared to the common PAC-Bayesian analysis of neural networks that express the prior over network parameters space. The theoretical study here appears simple (noteworthy, it is based one of the very first PAC-Bayesian theorems of McAllester that is not the most used nowadays), and the study is conducted mainly by empirical observation. Nevertheless, the experiments leading to these observations are cleverly designed, and I think it gives great insights and might open the way to other interesting studies. Overall, the paper is enjoyable to read. I also appreciate the completeness of the supplementary material. I recommend the paper acceptance, but I would like the authors to consider the concerns I rise below: - The paper title is a bit presumptuous. The paper presents a conjunction backed by empirical evidence on some not-so-deep neural networks. Even if I consider it as an important piece of work, it does not provide any definitive answer to the generalization puzzle. - Many peer-reviewed publications are cited as arXiv preprints. Please carefully complete the bibliography. Some papers are referenced by the name, title and year only (Smith and Le 2018; Zhang et al, 2017) - I recommend adding to the learning curves of Figures 2 and 3 the loss on the training set. Other minor comments and typos: - Intro: Please define \"parameter-function\" map - Page 4: Missing parentheses around Mand et al. (2017) - SGD has not had time ==> SGD did not have time - Please refers to the definition in the supplementary material/information the first time you mention Lempel-Ziv complexity. - Please mention that SI stands for Supplementary Information ", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 2 for the constructive comments and feedback . We have submitted a new draft were we address concerns raised , and sharpen some of the main points we make . In particular , we have clarified what we are trying to explain in terms of the generalization puzzle . We are trying to explain the big picture of why overparametrized DNNs generalize at all , and have tried to clarify in the text that we are not trying to explain for example why SGD or dropout or other similar techniques improve further on generalization . We \u2019 re still happy to add qualifiers to the title if the reviewer wants us to . We have fixed the bibliography , correctly citing peer-reviewed publications , and completing those with incomplete information . The classification error on the training set is 0 in all the experiments in Figures 2 and 3 . We have updated the captions to make this clear in the figures themselves . If instead \u2018 loss \u2019 referred to the cross-entropy loss , that will of course not be exactly zero , but our discussion is centered on classification error , and so we think adding that would be distract from the point of the figures . We have added a definition of parameter-function map in Section 2 , and addressed all the other minor typos and comments . We also changed to \u201c Supplementary information \u201d to Appendices ."}, {"review_id": "rye4g3AqFm-1", "review_text": "This paper propose an interesting perspective to explain the generalization behaviors of large over-parameterized neural networks by saying that the parameter-function map in neural networks are biased towards \"simple\" functions, and through a PAC-Bayes argument, the generalization behavior will be good if the target concept is also \"simple\". I like the perspective of view that combines the \"complexity\" of both the algorithm bias and the target concept in the view of generalization. However, the implementation and presentation of the paper could be improved. First of all, the paper is a bit difficult to follow as some important information is either missing or only available in the appendix. For example, in Section 2, to measure the properties of the parameter-function mapping, a simple boolean neural network is explored. However, it is not clear how the sampling procedure is carried out. There is also a 'training set of 64 examples', and it not obvious to the reader how this training set is used in this sample of neural network parameters. Following that, the paper uses Gaussian Process and Expectation-Propagation to approximately compute P(U). But the description is brief and vague (to non-expert in GP or EP). As one of the main contribution stated in the introduction, it would be better if more details are included. Moreover, the generalization bound is derived with the assumption that the learning algorithm uniformly sample from the set of all hypothesis that is consistent with a given training set. It is unlikely that this is what SGD is doing. But explicit experiments to verify how close is the real-world behavior to the hypothetical behavior would be helpful. The experiment in section 6 that verify the 'complexity' of 'high-probability' functions in the given prior is very interesting. It would be good if some kind of measurements more directly on the real world tasks could be done, which will better support the argument made in the paper. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank Reviewer 1 for the constructive comments and feedback . __Tiny networks__ The reviewer uses as a title \u201c Interesting perspective but most relevant experiments are on very tiny networks \u201d -- While we do use a small model network for our direct sampling , we perform a significant amount of work on more standard architectures and datasets , including 4 hidden layer CNNs with 200 filters per layer for all databases , and a FC network that has 1 hidden layer , with 784 neurons for MNIST and fashion MNIST , and 1024 neurons for CIFAR10 . ( We also used FC networks with more layers , but there results are similar , with only a small improvement in generalization ) . While they are not the latest state of the art , we don \u2019 t think that these DNNs are tiny . __Clarity of exposition__ We agree that we could have been clearer in what we are trying to achieve . To this end , we have expanded the introduction , and throughout the paper tried to make our arguments more clear ( see also our general response above ) . In response to the reviewer , we have in particular improved the exposition in the sections which were a bit difficult to follow . In Sections 2 and 3 , we explained the experiment and sampling procedure in more detail . In Section 2 we also defined the parameter-function map more clearly as per Reviewer 2 \u2019 s advice . The mention of a \u201c training set of 64 examples \u201d was a typo , as the experiment in Section 2 did not involve any training . In response to the referee , we have expanded the description of the Gaussian processes ( GPs ) and the Expectation-Propagation in section 4.1 to help people unfamiliar with the topic . Nevertheless the link between DNNS and GPs is a vast topic , going back to the famous 1995 work by Radfod Neal . See also the pioneering recent papers we cite as [ ( Lee et al . ( 2017 ) ; Matthews et al . ( 2018 ) ; Garriga-Alonso et al . ( 2018 ) ; Novak et al . ( 2018 ) ) ] .But we hope that what we write , is sufficient for a non-expert to catch a flavour of the method . Some more detail on GPs can be found in Appendix C , and of course m We are planning a longer publication explaining in much more detail how all this works for PAC-Bayes . __SGD__ Here we quote the full paragraph on SGD because it raises an important issue that merits a longer response . The reviewer writes \u201c Moreover , the generalization bound is derived with the assumption that the learning algorithm uniformly sample from the set of all hypothesis that is consistent with a given training set . It is unlikely that this is what SGD is doing . But explicit experiments to verify how close is the real-world behavior to the hypothetical behavior would be helpful. \u201d __Our new experiments on SGD sampling__ As also described above in our general response , in the new section 6 , we performed experiments which test the behaviour of SGD in a more direct way than most previous approaches , at the expense of being constrained to very small input spaces ( we use the neural network with 7 Boolean inputs and one Boolean output ) . We performed experiments directly comparing the probability of finding individual Boolean functions when training the neural network with two variants of SGD , versus using the Gaussian process corresponding to the neural network architecture ( which approximates Bayesian sampling of the parameters under i.i.d.Gaussian prior ) . We find good agreement . __Complexity of real-world functions__ The reviewer also asks for direct measurements of the complexity of real world functions . This is indeed an interesting question . While the simplicity bias bound means that large P ( f ) must mean low complexity , it is not so easy to calculate the complexity for real world functions using most of the measures we consider in the Appendix . We are currently working on this question using the critical sample ratio , which is the most scalable measure . Preliminary results are encouraging , but they weren \u2019 t ready by the deadline to submit the manuscript .. It \u2019 s not hard to imagine that function for in Figs 3 and 4 are more complex as we corrupt the data more ."}, {"review_id": "rye4g3AqFm-2", "review_text": " The authors make a case that deep networks are biased toward fitting data with simple functions. The start by examining the priors on classifiers obtained by sampling the weights of a neural network according to different distributions. They do this in two ways. First, they examine properties of the distribution on binary-valued functions on seven boolean inputs obtained by sampling the weights of a small neural network. They also empirically compare the labelings obtained by sampling the weights of a network with labelings obtained from a Gaussian process model arising from earlier work. Next, they analyze the complexity of the functions produced, using different measures of the complexity of boolean functions. A favorite of theirs is something that they call Lempel-Ziv complexity, which is measured by choosing an arbitrarily ordering of the domain, writing the outputs of the function in that ordering, and looking at how well the Lempel-Ziv algorithm compresses this sequence. I am not convinced that this is the most meaningful and fundamental measure of the complexity of functions. (In the supplementary material, they examine some others. They show plots relating the different measures in the body of the paper. None of the measures is specified in detail in the body of the paper. They provide plots relating these complexity measures, but they don't demonstrate a very close connection.) The authors then evaluate the generalization bound obtained by applying a PAC Bayes bound, together with the assumption that the training process produces weights sampled from the distribution obtained by conditioning weights chosen according to the random initialization on the event that they fit they fit the training data perfectly. They do this for small networks and simple datasets. They bounds are loose, but not vacuous, and follow the same order of difficulty on a handful of datasets as the true generalization error. In all of their experiments, they stop training when the training accuracy reaches 100%, where papers like https://arxiv.org/pdf/1706.08947.pdf have found that continuing training past this point further improves test accuracy. The experiments all use architectures that are quite dissimilar to what is commonly used in practice, and achieve much worse accuracy, so that a reader is concerned that the results differ qualitatively in other respects. I do not find it surprising that randomly sampling parameters of deep networks leads to simple functions. Papers like the Soudry, et al paper cited in this submission are inconsistent with the assumption in the paper that SGD samples parameters uniformly. It is not clear to me how many hidden layers were used for the results in Table 1 (is it four?). I did find it interesting to see exactly how concentrated the distribution of functions obtained in their 7-input experiment was, and also found results on the agreement of the Gaussian process models with the randomly sampled weight interesting, as far as they went. Overall, I am not sure that this paper provided enough fundamental new insight to be published in ICLR. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank Reviewer 3 for the constructive comments and feedback . __not surprising ? __ The reviewer \u2019 s title says \u201c not surprising \u201d and in the text they write \u201c I do not find it surprising that randomly sampling parameters of deep networks leads to simple functions. \u201d We are happy that the reviewer does not find this surprising . Be that as it may , to our knowledge we are the first to directly measure the parameter-function map for a DNN , showing simplicity bias over many orders of magnitude . We demonstrate that the parameter-function map obeys the conditions that allow the simplicity bias bound to hold , which for biased maps , gives an exponential drop in probability with a linear increase in descriptional complexity . ( Note that it is not hard to see that many other machine learning methods don \u2019 t satisfy these necessary conditions , and therefore overfit when there are more parameters than data ) . We then show that this simplicity bias provides the implicit regularization that explains the remarkable generalization properties of highly overparameterized DNNs . In parallel , we provide a novel PAC-Bayes analysis based on the parameter-function map that generates the * * first non-vacuous generalization bounds for DNNs that correctly scale with varying generalization performance for MNIST , fashion MNIST and CIFAR10 * * . These bounds would not work unless the parameter-function map is extremely biased , an effect we capture in our version of PAC-Bayes for DNNs . Regardless of whether or not the referee finds all this surprising , we believe that these results are significant , and have not been published in the literature on DNNs . __Complexity measures__ The reviewer complains about our use of Lempel-Ziv ( LZ ) , and that we put other complexity measures into the Appendix . In our original manuscript we write \u201c Here we simply note that there is nothing fundamental about LZ . Other approximate complexity measures that capture essential aspects of Kolmogorov complexity also show similar correlations ( see Appendix E.4 ) . \u201d In the current manuscript we have expanded this sentence slighlty , but also note that the question of complexity measures has been further discussed in the Dingle et al ( 2018 ) paper we cite above . In more detail , as we write in the Appendix E.1 in the original manuscript \u201c [ the ordering ] may affect the LZ complexity , although for simple input orderings , it will typically have a negligible effect. \u201d , Therefore , the ordering of the domain is not totally arbitrary . It must be a Kolmogorov-simple ordering to ensure that the complexity of the resulting bit string is close to the complexity of the function . Furthermore , we don \u2019 t claim that LZ is fundamental , or necessarily the best choice . The motivation behind using LZ is that it is commonly used to approximate Kolmogorov complexity , and it seemed to be the one that correlated best with the probability of Boolean functions , for the small fully connected network , although other measures ( such as Boolean complexity ) also do well . Regarding meaningfulness , we think some of the measures offered in the Appendix are perhaps more meaningful ( or at least , interpretable ) as they are truly only dependent on the function , and not domain ordering . At any rate , while It is true that the literature on complexity measures is vast , and much more could be said about them , for the basic argument we are making in the paper we believe that the measures we use are sufficient ."}], "0": {"review_id": "rye4g3AqFm-0", "review_text": "The paper brings a fresh study to the generalization capabilities of (deep) neural networks, with the help of an original use of PAC-Bayesian learning theory and some empirically backed intuitions. Expressing the prior over the input-output function space generated by the neural network is very interesting. This provides an original analysis compared to the common PAC-Bayesian analysis of neural networks that express the prior over network parameters space. The theoretical study here appears simple (noteworthy, it is based one of the very first PAC-Bayesian theorems of McAllester that is not the most used nowadays), and the study is conducted mainly by empirical observation. Nevertheless, the experiments leading to these observations are cleverly designed, and I think it gives great insights and might open the way to other interesting studies. Overall, the paper is enjoyable to read. I also appreciate the completeness of the supplementary material. I recommend the paper acceptance, but I would like the authors to consider the concerns I rise below: - The paper title is a bit presumptuous. The paper presents a conjunction backed by empirical evidence on some not-so-deep neural networks. Even if I consider it as an important piece of work, it does not provide any definitive answer to the generalization puzzle. - Many peer-reviewed publications are cited as arXiv preprints. Please carefully complete the bibliography. Some papers are referenced by the name, title and year only (Smith and Le 2018; Zhang et al, 2017) - I recommend adding to the learning curves of Figures 2 and 3 the loss on the training set. Other minor comments and typos: - Intro: Please define \"parameter-function\" map - Page 4: Missing parentheses around Mand et al. (2017) - SGD has not had time ==> SGD did not have time - Please refers to the definition in the supplementary material/information the first time you mention Lempel-Ziv complexity. - Please mention that SI stands for Supplementary Information ", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 2 for the constructive comments and feedback . We have submitted a new draft were we address concerns raised , and sharpen some of the main points we make . In particular , we have clarified what we are trying to explain in terms of the generalization puzzle . We are trying to explain the big picture of why overparametrized DNNs generalize at all , and have tried to clarify in the text that we are not trying to explain for example why SGD or dropout or other similar techniques improve further on generalization . We \u2019 re still happy to add qualifiers to the title if the reviewer wants us to . We have fixed the bibliography , correctly citing peer-reviewed publications , and completing those with incomplete information . The classification error on the training set is 0 in all the experiments in Figures 2 and 3 . We have updated the captions to make this clear in the figures themselves . If instead \u2018 loss \u2019 referred to the cross-entropy loss , that will of course not be exactly zero , but our discussion is centered on classification error , and so we think adding that would be distract from the point of the figures . We have added a definition of parameter-function map in Section 2 , and addressed all the other minor typos and comments . We also changed to \u201c Supplementary information \u201d to Appendices ."}, "1": {"review_id": "rye4g3AqFm-1", "review_text": "This paper propose an interesting perspective to explain the generalization behaviors of large over-parameterized neural networks by saying that the parameter-function map in neural networks are biased towards \"simple\" functions, and through a PAC-Bayes argument, the generalization behavior will be good if the target concept is also \"simple\". I like the perspective of view that combines the \"complexity\" of both the algorithm bias and the target concept in the view of generalization. However, the implementation and presentation of the paper could be improved. First of all, the paper is a bit difficult to follow as some important information is either missing or only available in the appendix. For example, in Section 2, to measure the properties of the parameter-function mapping, a simple boolean neural network is explored. However, it is not clear how the sampling procedure is carried out. There is also a 'training set of 64 examples', and it not obvious to the reader how this training set is used in this sample of neural network parameters. Following that, the paper uses Gaussian Process and Expectation-Propagation to approximately compute P(U). But the description is brief and vague (to non-expert in GP or EP). As one of the main contribution stated in the introduction, it would be better if more details are included. Moreover, the generalization bound is derived with the assumption that the learning algorithm uniformly sample from the set of all hypothesis that is consistent with a given training set. It is unlikely that this is what SGD is doing. But explicit experiments to verify how close is the real-world behavior to the hypothetical behavior would be helpful. The experiment in section 6 that verify the 'complexity' of 'high-probability' functions in the given prior is very interesting. It would be good if some kind of measurements more directly on the real world tasks could be done, which will better support the argument made in the paper. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank Reviewer 1 for the constructive comments and feedback . __Tiny networks__ The reviewer uses as a title \u201c Interesting perspective but most relevant experiments are on very tiny networks \u201d -- While we do use a small model network for our direct sampling , we perform a significant amount of work on more standard architectures and datasets , including 4 hidden layer CNNs with 200 filters per layer for all databases , and a FC network that has 1 hidden layer , with 784 neurons for MNIST and fashion MNIST , and 1024 neurons for CIFAR10 . ( We also used FC networks with more layers , but there results are similar , with only a small improvement in generalization ) . While they are not the latest state of the art , we don \u2019 t think that these DNNs are tiny . __Clarity of exposition__ We agree that we could have been clearer in what we are trying to achieve . To this end , we have expanded the introduction , and throughout the paper tried to make our arguments more clear ( see also our general response above ) . In response to the reviewer , we have in particular improved the exposition in the sections which were a bit difficult to follow . In Sections 2 and 3 , we explained the experiment and sampling procedure in more detail . In Section 2 we also defined the parameter-function map more clearly as per Reviewer 2 \u2019 s advice . The mention of a \u201c training set of 64 examples \u201d was a typo , as the experiment in Section 2 did not involve any training . In response to the referee , we have expanded the description of the Gaussian processes ( GPs ) and the Expectation-Propagation in section 4.1 to help people unfamiliar with the topic . Nevertheless the link between DNNS and GPs is a vast topic , going back to the famous 1995 work by Radfod Neal . See also the pioneering recent papers we cite as [ ( Lee et al . ( 2017 ) ; Matthews et al . ( 2018 ) ; Garriga-Alonso et al . ( 2018 ) ; Novak et al . ( 2018 ) ) ] .But we hope that what we write , is sufficient for a non-expert to catch a flavour of the method . Some more detail on GPs can be found in Appendix C , and of course m We are planning a longer publication explaining in much more detail how all this works for PAC-Bayes . __SGD__ Here we quote the full paragraph on SGD because it raises an important issue that merits a longer response . The reviewer writes \u201c Moreover , the generalization bound is derived with the assumption that the learning algorithm uniformly sample from the set of all hypothesis that is consistent with a given training set . It is unlikely that this is what SGD is doing . But explicit experiments to verify how close is the real-world behavior to the hypothetical behavior would be helpful. \u201d __Our new experiments on SGD sampling__ As also described above in our general response , in the new section 6 , we performed experiments which test the behaviour of SGD in a more direct way than most previous approaches , at the expense of being constrained to very small input spaces ( we use the neural network with 7 Boolean inputs and one Boolean output ) . We performed experiments directly comparing the probability of finding individual Boolean functions when training the neural network with two variants of SGD , versus using the Gaussian process corresponding to the neural network architecture ( which approximates Bayesian sampling of the parameters under i.i.d.Gaussian prior ) . We find good agreement . __Complexity of real-world functions__ The reviewer also asks for direct measurements of the complexity of real world functions . This is indeed an interesting question . While the simplicity bias bound means that large P ( f ) must mean low complexity , it is not so easy to calculate the complexity for real world functions using most of the measures we consider in the Appendix . We are currently working on this question using the critical sample ratio , which is the most scalable measure . Preliminary results are encouraging , but they weren \u2019 t ready by the deadline to submit the manuscript .. It \u2019 s not hard to imagine that function for in Figs 3 and 4 are more complex as we corrupt the data more ."}, "2": {"review_id": "rye4g3AqFm-2", "review_text": " The authors make a case that deep networks are biased toward fitting data with simple functions. The start by examining the priors on classifiers obtained by sampling the weights of a neural network according to different distributions. They do this in two ways. First, they examine properties of the distribution on binary-valued functions on seven boolean inputs obtained by sampling the weights of a small neural network. They also empirically compare the labelings obtained by sampling the weights of a network with labelings obtained from a Gaussian process model arising from earlier work. Next, they analyze the complexity of the functions produced, using different measures of the complexity of boolean functions. A favorite of theirs is something that they call Lempel-Ziv complexity, which is measured by choosing an arbitrarily ordering of the domain, writing the outputs of the function in that ordering, and looking at how well the Lempel-Ziv algorithm compresses this sequence. I am not convinced that this is the most meaningful and fundamental measure of the complexity of functions. (In the supplementary material, they examine some others. They show plots relating the different measures in the body of the paper. None of the measures is specified in detail in the body of the paper. They provide plots relating these complexity measures, but they don't demonstrate a very close connection.) The authors then evaluate the generalization bound obtained by applying a PAC Bayes bound, together with the assumption that the training process produces weights sampled from the distribution obtained by conditioning weights chosen according to the random initialization on the event that they fit they fit the training data perfectly. They do this for small networks and simple datasets. They bounds are loose, but not vacuous, and follow the same order of difficulty on a handful of datasets as the true generalization error. In all of their experiments, they stop training when the training accuracy reaches 100%, where papers like https://arxiv.org/pdf/1706.08947.pdf have found that continuing training past this point further improves test accuracy. The experiments all use architectures that are quite dissimilar to what is commonly used in practice, and achieve much worse accuracy, so that a reader is concerned that the results differ qualitatively in other respects. I do not find it surprising that randomly sampling parameters of deep networks leads to simple functions. Papers like the Soudry, et al paper cited in this submission are inconsistent with the assumption in the paper that SGD samples parameters uniformly. It is not clear to me how many hidden layers were used for the results in Table 1 (is it four?). I did find it interesting to see exactly how concentrated the distribution of functions obtained in their 7-input experiment was, and also found results on the agreement of the Gaussian process models with the randomly sampled weight interesting, as far as they went. Overall, I am not sure that this paper provided enough fundamental new insight to be published in ICLR. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank Reviewer 3 for the constructive comments and feedback . __not surprising ? __ The reviewer \u2019 s title says \u201c not surprising \u201d and in the text they write \u201c I do not find it surprising that randomly sampling parameters of deep networks leads to simple functions. \u201d We are happy that the reviewer does not find this surprising . Be that as it may , to our knowledge we are the first to directly measure the parameter-function map for a DNN , showing simplicity bias over many orders of magnitude . We demonstrate that the parameter-function map obeys the conditions that allow the simplicity bias bound to hold , which for biased maps , gives an exponential drop in probability with a linear increase in descriptional complexity . ( Note that it is not hard to see that many other machine learning methods don \u2019 t satisfy these necessary conditions , and therefore overfit when there are more parameters than data ) . We then show that this simplicity bias provides the implicit regularization that explains the remarkable generalization properties of highly overparameterized DNNs . In parallel , we provide a novel PAC-Bayes analysis based on the parameter-function map that generates the * * first non-vacuous generalization bounds for DNNs that correctly scale with varying generalization performance for MNIST , fashion MNIST and CIFAR10 * * . These bounds would not work unless the parameter-function map is extremely biased , an effect we capture in our version of PAC-Bayes for DNNs . Regardless of whether or not the referee finds all this surprising , we believe that these results are significant , and have not been published in the literature on DNNs . __Complexity measures__ The reviewer complains about our use of Lempel-Ziv ( LZ ) , and that we put other complexity measures into the Appendix . In our original manuscript we write \u201c Here we simply note that there is nothing fundamental about LZ . Other approximate complexity measures that capture essential aspects of Kolmogorov complexity also show similar correlations ( see Appendix E.4 ) . \u201d In the current manuscript we have expanded this sentence slighlty , but also note that the question of complexity measures has been further discussed in the Dingle et al ( 2018 ) paper we cite above . In more detail , as we write in the Appendix E.1 in the original manuscript \u201c [ the ordering ] may affect the LZ complexity , although for simple input orderings , it will typically have a negligible effect. \u201d , Therefore , the ordering of the domain is not totally arbitrary . It must be a Kolmogorov-simple ordering to ensure that the complexity of the resulting bit string is close to the complexity of the function . Furthermore , we don \u2019 t claim that LZ is fundamental , or necessarily the best choice . The motivation behind using LZ is that it is commonly used to approximate Kolmogorov complexity , and it seemed to be the one that correlated best with the probability of Boolean functions , for the small fully connected network , although other measures ( such as Boolean complexity ) also do well . Regarding meaningfulness , we think some of the measures offered in the Appendix are perhaps more meaningful ( or at least , interpretable ) as they are truly only dependent on the function , and not domain ordering . At any rate , while It is true that the literature on complexity measures is vast , and much more could be said about them , for the basic argument we are making in the paper we believe that the measures we use are sufficient ."}}