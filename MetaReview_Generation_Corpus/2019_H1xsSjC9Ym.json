{"year": "2019", "forum": "H1xsSjC9Ym", "title": "Learning to Understand Goal Specifications by Modelling Reward", "decision": "Accept (Poster)", "meta_review": "Pros:\n- The paper is well-written and clear and presented with helpful illustrations and videos.\n- The  training methodology seems sound (multiple random seeds etc.)\n- The results are encouraging.\n\nCons:\n- There was some concern generally about how this work is positioned relative to related work and the completeness of the related work.  However, the authors have made this clearer in their rebuttal.\n\nThere was a considerable amount of discussion between the authors and all reviewers to pin down some unclear aspects of the paper. I believe in the end there was good convergence and I thank both the authors and reviewers for their persistence and dilligence in working through this.  The final paper is much better I think and I recommend acceptance.", "reviews": [{"review_id": "H1xsSjC9Ym-0", "review_text": "The previous version of the paper was not clear enough in the motivation and uniqueness of the work. After a long and devoted discussion with the authors, we agreed on certain ways of improving the paper presentation, including connection to some related work. The current paper is much better, so I would like to raise my score to 6. My revised review is: [orginality and significance] + The paper deals with a challenging navigation problem where natural language instructions can be underspecified and the environment is complex---thus a correct reward function being extremely hard to craft. + The paper proposed to use a <instruction, state> discriminator D to compute a pseudo reward at each step, which is then used to reinforce an agent in natural-language-guided navigation task. The paper proposed to train the discriminator in an adversarial way---with expert supervised data. The idea is neat, and its effectiveness is empirically supported by extensive experimental results. [clarity] + The paper is well-written. The method is introduced with clear textual description, rigorous math formulations, and good illustration (Figure-1 and -2). The experiments are also well-documented, including training and testing details, results and analysis. [quality] + The paper was not clear at certain points but the authors had helpful discussions with me and the paper was revised accordingly. + The experiments were done with multiple random seeds, so I believe the results are convincing. The authors did not only show the numerical results but also shared qualitative videos through anonymous URL. Overall, it is a good paper. ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Below is my original review ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ [PROS] [originality and significance] The paper proposed to use a <instruction, state> discriminator D to compute the reward at each step, which is then used to reinforce an agent in natural-language-guided navigation task. The paper proposed to train the discriminator in adversarial way. The idea is neat, and its effectiveness is empirically supported by extensive experimental results. [clarity] The paper is well-written. The method is introduced with clear textual description, rigorous math formulations, and good illustration (Figure-1). The experiments are also well-documented, including training and testing details, results and analysis. The experiments were done with multiple random seeds, so I believe the results are convincing. The authors did not only show the numerical results but also shared qualitative videos through anonymous URL. Overall, it is a good paper. [CONS] [quality] The major issue of this paper is the lack of connection to existing related work in the field of dealing with reward sparsity problem. This is a long-standing problem in RL (very common in, but not only restricted to, navigation tasks) and people have proposed reward shaping techniques to handle it. But the paper did not discuss any work in this direction. For references, please first check this seminal work and then follow the line of research: Ng, Andrew Y and Harada, Daishi and Russell, Stuart, ICML 1999, Policy invariance under reward transformations: Theory and application to reward shaping The method proposed in this paper seems a way of automatically shaping the reward, but loses the optimal policy invariance (for how this invariance is ensured in reward shaping, please check out this tutorial: http://www-users.cs.york.ac.uk/~devlin/presentations/pbrs-tut.pdf). The proposed method has two key components: 1) the discriminator D; and 2) the adversarial training. The method is shown effective in experiments and outperforms appropriate baselines with actual reward. But the design of D and how it is used as reward function seems somewhat ad-hoc. D is only trained on the final states of episodes (please correct me if I am wrong), but is used at all the steps as part of reward function to determine the stepwise reward, which seems odd. The authors should discuss what (implicit) assumptions they are relying upon to make this method work in this way. The transformation function from D to reward value seems ad-hoc---e.g. why 0.5, why indicator function instead of others (e.g. scaling of indicator function), how it is generalized to non-1/0 (but still sparse) reward cases, etc? Is the method only designed for 1/0-reward cases? The authors should clearly specify if it is the case. Moreover, the paper compared to RP (Jaderberg 2016), which still reinforces the agent with actual reward but only *shapes the features of the agent* by multi-tasking on predicting the reward of next step (please correct me if this is wrong). Interestingly, the RP method achieves better performance than the proposed method, although it does not address the reward sparsity problem. Could the authors provide any insight about why this happened? Is there any trade-off between these two methods? Is there any setting, in the authors\u2019 opinion, where the proposed method should outperform RP? [SUMMARY] I think this is good work---neat idea, nice results and clear writing. But there are indeed some issues that I hope the authors could address. So I gave a score of 5. ", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 2 for their kind words and detailed review , and for clearly stating what they believe is the limitation of the paper they would like to see addressed . With all due respect , we would be happy to discuss the relation of this work to the sparse reward problem , and to that of learning shaped rewards , but we believe this recommendation stems from a misunderstanding which we hope to clarify through this discussion period . Simply put , the primary use case of frameworks like AGILE specifically , and of reward modelling / inverse reinforcement learning in general , is where there is * no * reward function implemented ( or even obtainable ) , rather than a sparse one . In such cases , we must learn a reward function from expert-provided information ( trajectories , goal states ) . In contrast , from our understanding of reward shaping in the context of the reward sparsity problem , the ground truth reward is accessible in order to update the policy ( or Q/V functions ) , and a shaped reward function is learned or defined to give a more dense \u201c fake \u201d reward ( while preserving optimality guarantees ) and make credit assignment easier . We understand that the source of this confusion may come from the fact that in one of our experiments ( GridLU-Relations ) , the ground truth reward * is * implemented and * is * sparse , but this is only for the purpose of automated evaluation and the training of baselines for comparison . When training agents with AGILE , or in the other experiments ( GridLU-Arrangements ) there is no reward provided from the environment . Perhaps we have misunderstood the point the reviewer is making , in which case we hope they can clarify how reward shaping and the sparse reward problem relate to the no-reward scenario in which our work and other work in inverse RL is situated . However , on the assumption that the reviewer has misunderstood the motivation and problem setting for our approach , we would be grateful if they could re-evaluate their assessment in this light , and/or perhaps let us know where would could have been clearer so as to not potentially confuse future readers along similar lines ."}, {"review_id": "H1xsSjC9Ym-1", "review_text": " ========== Update ========== Upon reviewing the paper revision and the author comments to my and the other reviewers' comments, I will revise my suggestion to that of acceptance. As I said in my summary, my primary concern was novelty with respect to prior work which the authors have clarified. They have also increased the rigor of their experimental results by providing variances in the plots. I think this work will be of interest to the community. ========== Strengths: ========== - The problem of learning to predict state rewards given language in interesting and useful. - The proposed AGILE framework is intuitively simple and works with any existing RL framework. - With the models and tasks explored in this paper, the approach does seem to learn to evaluate whether a state matches the instructions quite well. - The writing is very clear and direct. ========== Concerns: ========== [A] The discussion of differences to the closely related GAIL methodology is left until the related work after experiments. Given the similarities between GAIL and AGILE, this seems too late. The authors list three major differences between AGILE and GAIL: 1) AGILE is conditioned on a goal specification, language in this case. GAIL is unconditioned and trained for one task. 2) AGILE takes only the final/goal state rather than a trajectory like in GAIL. 3) AGILE discretizes the discriminator probability when assigning reward, GAIL does not. Some concerns about each: 1) This is an interesting and fair difference but also a necessary and somewhat obvious modification to GAIL in tasks with explicit goal-specification. 2) This does not seem like an improvement, but rather a loss of generality. The authors justify this change saying \"in AGILE the reward model observes only states s_i (either goal states from an expert, or states from the agent acting on the environment) rather than traces (s1, a1),(s2, a2), . . ., learning to reward the agent based on \u201cwhat\u201d needs to be done rather than according to \u201chow\u201d it must be done.\" In many real applications, the how is deeply important. For instance, navigation in the world is both a \"what\" (arrive at location X) and a \"how\" (in fastest time without hitting anything or in such a way that humans aren't frightened). Further, the trace includes the final state such that the \"what\" is recoverable in instances where the \"how\" is unimportant, as in the set of tasks presented in this paper. 3) Letting the paper speak on this subject: \"We considered this change of objective necessary because the GAIL-style reward would take arbitrarily low values for intermediate states visited by the agent, as the reward model will be confident as those are not goal states. The binary reward in AGILE carries a clear message to the policy that all non-goal states are equally undesirable.\" Firstly, all non-goal states are not equally undesirable in that some lead more easily to goal states though it is fair to argue this should be learned by the policy through expected reward. My primary gripe is the footnote following these sentences which says: \"We tried values other than 0.5 for the binarization threshold, as well as not binarizing and using D\u03c6(c, st) directly as the reward. We got similar but slightly worse results.\" This seems to imply that this difference does not matter significantly, especially if different thresholds received significantly different hyperparameter tuning effort or were not conducted under multiple runs of random seeds. A pessimistic summary would place AGILE to be a conditional GAIL with reduced ability to represent intermediate or trajectory based rewards and a possibly slightly helpful reward discretization scheme. Don't get me wrong, I think an conditional extension to GAIL is interesting and worth sharing with the community. However, this discussion comes very late and includes a design decisions (2/3) that I find poorly justified in text and completely unjustified experimentally. I would like to hear from the authors if any of these criticisms are inaccurate. I would also welcome experiments evaluating the effect of these design decisions. [B] In 3.2 its reported that each experiment was repeated five times however the presented results are not described as means and no variances are shown. I would like to see the results plots with shaded variances from at least 5 runs with differing random seeds. [C] Unless I'm mistaken, the proposed architecture could also be trained with reward prediction. It would be interested in that case to see if improvement seen between A3C and A3C-AGILE extend to A3C-RP and A3C-RP-AGILE. As the authors note, the AGILE framework simply changes the source of the reward and is amicable to any RL approach. I would like to see this comparison. [D] The reward generalization experiments seemed surprising to me. The policy was fine-tuned on the test environments but only improved from 52% to 69.3%. Trying to think about this more, I'm having trouble disentangling whether this implies poor generalization of the reward function or increased difficulty in policy learning. Could the authors provide the A3C and A3C-RP baselines for this experiment to help clarify? [E] Just a Curiosity: What exactly is done in L2 weight clipping? (Training details in supplement) [F] Just a Thought: In the reward-prediction (RP) setting, both the RP model and the policy share parameters. It would be possible with such an architecture to still apply the AGILE loss and I would be curious to see if this leads to interesting changes in performance. I understand that one of the advantages to learning a separate reward model is to generalize to new policies, but it is unclear if this approach would generalize less well (and finding it out would be cool!) ========== Overview: ========== I think extending generative adversarial imitation learning to a task-conditional setting a cool step made even more interesting in this work by having the task-specification be in compositional language. Further, the results and analysis are generally interesting though I do note some weaknesses above. Aside from some questions about the experiments, I'm mostly concerned about the positioning of the paper -- specifically with respect to prior work. I'm looking forward to hearing from the authors and other reviewers. ", "rating": "7: Good paper, accept", "reply_text": "We thank reviewer 3 for their very detailed comments , which while critical of the paper , give us much to respond to , and much to consider with regard to improving the paper . We will be honest : if indeed the crucial failing of the paper is its failure to successfully position itself with regard to GAIL , in particular due to the placement of the paragraph discussing comparison , we find that a score of 4 is a little strict . However , we appreciate the reviewer has been very rigorous in explaining their points regarding this weakness . We hope that through discussion we will both be able to present cogent counter-arguments to these objections where applicable , and otherwise satisfy the reviewer that the positioning of the paper can be improved , to their satisfaction , with minor revisions to the paper . As the reviewer correctly summarized and if we put the conditioning on instructions aside , a key key difference ( item 2 ) of AGILE from GAIL is the fact that AGILE uses images of goal-states as the training data instead of state-action trajectories required for GAIL . We believe that this difference does make AGILE applicable to situations where GAIL would not be . For example , consider the case when the expert that provides demonstrations has effectors different from that of the trained agent ( image a human teaching their household robot to arrange objects on a table : the robot won \u2019 t necessarily have 5 fingers ) . Besides , methods such as GAIL force the learner to imitate the training trajectories in their entirety , which can be a limitation when training trajectories are suboptimal ( the above example with the human is applicable again , human may not use the shortest path to the goal when they perform their manipulations ) . Relying on goal-states only brings a certain extra flexibility that GAIL-like methods do not provide , admittedly , at the cost of restricting the applicability of AGILE to declarative instructions , i.e.those which can be verified by the final state only . We believe , however , that such instructions make a common and frequent case in instruction-following setups , and methods that are optimized for this case ( such as AGILE ) are worth investigating . Reviewer has also requested a clarification with regard to the difference between rewards provided by the discriminator in GAIL and AGILE . We admit that the paper may have been not clear enough on this point , as it stressed the discretization of the reward , whereas the real key difference is the switch from log D ( ... ) as the reward in GAIL to ( possibly discretized ) D ( ... ) in AGILE . The discriminator is trained to output arbitrary low values of log D ( c , s ) in AGILE for the non-goal-states , and using log D ( c , s ) as the reward would arbitrarily punish the policy for entering intermediate states that are clearly not goal-states but yet may be useful in the near future . We are currently performing an extra experiment to validate this claim , in which we use log D ( c , s ) as the reward , and so far we do not see the job getting above 70 % success rate , confirming our intuition that GAIL-style log D ( c , s ) is not an appropriate reward for AGILE . We will update you on the progress of this additional investigation . Notwithstanding the above differences with GAIL , it \u2019 s true that GAIL and AGILE both can be broadly characterised as inverse RL methods , and as such AGILE could be presented as \u201c just \u201d extending IRL to the instruction-conditional case ( and we believe we are reasonably up-front about this in the introduction ) . With all due respect , it is unfair to characterise this observation as a concern , or qualify it as an \u201c obvious extension \u201d : it is , to our knowledge , one of the first substantial studies of instruction-conditional IRL over pixel observations . Experiments show generalisation of the reward model to both new observed states and held-out instructions . The model and objective design underpinning these results are , while inspired by GAIL and related techniques , original insofar as new methods ( e.g.discriminator rejection ) had to be incorporated to make it work . We think that , on the strength of extending IRL techniques to multi-task contexts , where the task is specified by language , is new enough to warrant publication , and hope you will agree . ( see part 2 for continuation )"}, {"review_id": "H1xsSjC9Ym-2", "review_text": "The paper presents an approach for simultaneously learning policies and reward functions for reaching goals that are described by an instruction providing spatial relations among objects. The proposed platform, called Adversarial Goal-Induced Learning from Examples (AGILE), is composed of an off-the-shelf RL module like A3C and a separate module for learning a reward function, implemented using the NMN paradigm. The RL module is trained using the reward function learned by the reward module. The reward module is trained to map a given <instruction, state> into a score between 0 and 1 depending on how well the provided state satisfies the instructions provided in the instruction. The returned score is used as a reward function. The training of the reward function is performed by using a dataset of positive examples, and using the states visited by the agent while it's learning as negative examples. To account for the fact that the agent becomes better over time and its visited states can no longer be used as negative examples, the authors proposed a heuristic where the states visited by the agent are not all used as negative examples, but only those that have the lowest scores. The paper also presents an empirical evaluation of the proposed approach on a synthetic task where the agent is tasked with move bocks of different shapes and colors to a desired final configuration. The AGILE approach was compared to the baseline A3C algorithm where a sparse binary reward signal was used only whenever the agent reaches the goal state. AGILE is also compared to A3C with an auxiliary task of reward prediction. The paper is clearly written and technically strong. However, I have two issues with this paper: 1) the proposed approach is a simple combination of A3C and the NMN architecture, 2) the experiments are performed on simple synthetic tasks that make learning spatial relations fairly easy, I would love to see more real images as it has been demonstrated in prior works on learning spatial relations. It is not clear from these experiments if the proposed approach will scale up to higher-dimensional inputs. Moreover, there are several stability issues that can be caused by the proposed approach. For instance, the reward function is changing over time, how does that affect the learning rate? Also, instead of using the learned policy itself to generate negative examples and run into non IID data, instabilities, and increasingly good negative examples, why not use a fixed dataset of negative examples generated with a random policy? It would be interesting to do perform an experiment where you compare to the classical reward learning setup where you simply provided labeled positive and negative examples and classify them offline, then use the learned reward function online for RL. How did you tune the hyper-parameter \\rho (percentage of negative examples to discard) for specific tasks? Do you have any guarantees for this approach? In the generalization experiments, it is mentioned that 10% of the instructions are held out. Are these 10% randomized?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank Reviewer 1 for their review and statement of support for the paper \u2019 s technical contributions . We hope , during this discussion period , to get a better understanding of your concerns and hopefully address them , making clarifications in the paper where needed . First , could you please clarify your first concern that \u201c the proposed approach is a simple combination of A3C and the NMN architecture \u201d ? While this is an accurate portrayal of some of our key results , we stress that : 1 ) we also report results for LSTM + FiLM-ConvNet architectures for both policy and reward model networks , to showcase performance within AGILE when the syntax of the language is not given ( see \u201c AGILE with Structure-Agnostic Models \u201d in Section 3 ) . 2 ) use of A3C for our policy network is not an essential part of AGILE . AGILE is a general framework for jointly training policies and reward models , both conditioned on language instructions . Any RL method and network architecture can be used , since the only difference between \u201c traditional \u201d RL and AGILE is the source of the reward : in the former case , it comes from the environment , and in the latter , from the jointly learned reward model . As such , for both the baseline and the AGILE-based model , the fact that we used A3C is not important ; all that is important is that we use the same RL algorithm and architecture for both the baseline and AGILE . With this in mind , do you still believe this aspect of our evaluation is cause for concern ? Second , you suggest experiments which are more visually realistic with more complex relations . We agree that this is where this research should be going , and discuss such further work in section 5 , along with the particular challenges we anticipate it will bring . We would be remiss not to point out two things , with regard to our current experiments : 1 ) While simple , the environment and tasks are actually quite diverse and approached without the simplifying assumptions typically seen in grid worlds : a ) There are over 1,000 instructions in GridLU-Relations , each specifying a different task with millions of different initial environment states to solve . b ) The environment is observed , by both policy and reward model , at the pixel level , without predefined notions of what and where objects and their boundaries are , or \u201c privileged features \u201d indicating which predicates apply to which objects ( or even which predicates exist ) . 2 ) As we see in the left plot of Figure 3 , the A3C baseline results show that , due to the two points made above , even when the reward is specified by the environment ( for our baselines ) , this is quite a difficult multi-task RL problem for fairly modern agent architectures and RL algorithms . This is because it may take over a dozen steps to optimally obtain a goal state , the task is changing every episode , and reward is sparse . When you add to that the need to jointly infer the reward function from a limited number of expert examples , this constitutes a fairly significant machine learning problem . While we agree that there are no guarantees with regard to scalability to more realistic environments \u201c out of the box \u201d , we hope you will agree this makes a substantial contribution by showing that it is possible to obtain agents which align with expert notions of reward through the proxy of learned reward functions , from examples , and that it is reasonable to leave further developments which scale to more complex environments for further research ."}], "0": {"review_id": "H1xsSjC9Ym-0", "review_text": "The previous version of the paper was not clear enough in the motivation and uniqueness of the work. After a long and devoted discussion with the authors, we agreed on certain ways of improving the paper presentation, including connection to some related work. The current paper is much better, so I would like to raise my score to 6. My revised review is: [orginality and significance] + The paper deals with a challenging navigation problem where natural language instructions can be underspecified and the environment is complex---thus a correct reward function being extremely hard to craft. + The paper proposed to use a <instruction, state> discriminator D to compute a pseudo reward at each step, which is then used to reinforce an agent in natural-language-guided navigation task. The paper proposed to train the discriminator in an adversarial way---with expert supervised data. The idea is neat, and its effectiveness is empirically supported by extensive experimental results. [clarity] + The paper is well-written. The method is introduced with clear textual description, rigorous math formulations, and good illustration (Figure-1 and -2). The experiments are also well-documented, including training and testing details, results and analysis. [quality] + The paper was not clear at certain points but the authors had helpful discussions with me and the paper was revised accordingly. + The experiments were done with multiple random seeds, so I believe the results are convincing. The authors did not only show the numerical results but also shared qualitative videos through anonymous URL. Overall, it is a good paper. ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Below is my original review ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ [PROS] [originality and significance] The paper proposed to use a <instruction, state> discriminator D to compute the reward at each step, which is then used to reinforce an agent in natural-language-guided navigation task. The paper proposed to train the discriminator in adversarial way. The idea is neat, and its effectiveness is empirically supported by extensive experimental results. [clarity] The paper is well-written. The method is introduced with clear textual description, rigorous math formulations, and good illustration (Figure-1). The experiments are also well-documented, including training and testing details, results and analysis. The experiments were done with multiple random seeds, so I believe the results are convincing. The authors did not only show the numerical results but also shared qualitative videos through anonymous URL. Overall, it is a good paper. [CONS] [quality] The major issue of this paper is the lack of connection to existing related work in the field of dealing with reward sparsity problem. This is a long-standing problem in RL (very common in, but not only restricted to, navigation tasks) and people have proposed reward shaping techniques to handle it. But the paper did not discuss any work in this direction. For references, please first check this seminal work and then follow the line of research: Ng, Andrew Y and Harada, Daishi and Russell, Stuart, ICML 1999, Policy invariance under reward transformations: Theory and application to reward shaping The method proposed in this paper seems a way of automatically shaping the reward, but loses the optimal policy invariance (for how this invariance is ensured in reward shaping, please check out this tutorial: http://www-users.cs.york.ac.uk/~devlin/presentations/pbrs-tut.pdf). The proposed method has two key components: 1) the discriminator D; and 2) the adversarial training. The method is shown effective in experiments and outperforms appropriate baselines with actual reward. But the design of D and how it is used as reward function seems somewhat ad-hoc. D is only trained on the final states of episodes (please correct me if I am wrong), but is used at all the steps as part of reward function to determine the stepwise reward, which seems odd. The authors should discuss what (implicit) assumptions they are relying upon to make this method work in this way. The transformation function from D to reward value seems ad-hoc---e.g. why 0.5, why indicator function instead of others (e.g. scaling of indicator function), how it is generalized to non-1/0 (but still sparse) reward cases, etc? Is the method only designed for 1/0-reward cases? The authors should clearly specify if it is the case. Moreover, the paper compared to RP (Jaderberg 2016), which still reinforces the agent with actual reward but only *shapes the features of the agent* by multi-tasking on predicting the reward of next step (please correct me if this is wrong). Interestingly, the RP method achieves better performance than the proposed method, although it does not address the reward sparsity problem. Could the authors provide any insight about why this happened? Is there any trade-off between these two methods? Is there any setting, in the authors\u2019 opinion, where the proposed method should outperform RP? [SUMMARY] I think this is good work---neat idea, nice results and clear writing. But there are indeed some issues that I hope the authors could address. So I gave a score of 5. ", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 2 for their kind words and detailed review , and for clearly stating what they believe is the limitation of the paper they would like to see addressed . With all due respect , we would be happy to discuss the relation of this work to the sparse reward problem , and to that of learning shaped rewards , but we believe this recommendation stems from a misunderstanding which we hope to clarify through this discussion period . Simply put , the primary use case of frameworks like AGILE specifically , and of reward modelling / inverse reinforcement learning in general , is where there is * no * reward function implemented ( or even obtainable ) , rather than a sparse one . In such cases , we must learn a reward function from expert-provided information ( trajectories , goal states ) . In contrast , from our understanding of reward shaping in the context of the reward sparsity problem , the ground truth reward is accessible in order to update the policy ( or Q/V functions ) , and a shaped reward function is learned or defined to give a more dense \u201c fake \u201d reward ( while preserving optimality guarantees ) and make credit assignment easier . We understand that the source of this confusion may come from the fact that in one of our experiments ( GridLU-Relations ) , the ground truth reward * is * implemented and * is * sparse , but this is only for the purpose of automated evaluation and the training of baselines for comparison . When training agents with AGILE , or in the other experiments ( GridLU-Arrangements ) there is no reward provided from the environment . Perhaps we have misunderstood the point the reviewer is making , in which case we hope they can clarify how reward shaping and the sparse reward problem relate to the no-reward scenario in which our work and other work in inverse RL is situated . However , on the assumption that the reviewer has misunderstood the motivation and problem setting for our approach , we would be grateful if they could re-evaluate their assessment in this light , and/or perhaps let us know where would could have been clearer so as to not potentially confuse future readers along similar lines ."}, "1": {"review_id": "H1xsSjC9Ym-1", "review_text": " ========== Update ========== Upon reviewing the paper revision and the author comments to my and the other reviewers' comments, I will revise my suggestion to that of acceptance. As I said in my summary, my primary concern was novelty with respect to prior work which the authors have clarified. They have also increased the rigor of their experimental results by providing variances in the plots. I think this work will be of interest to the community. ========== Strengths: ========== - The problem of learning to predict state rewards given language in interesting and useful. - The proposed AGILE framework is intuitively simple and works with any existing RL framework. - With the models and tasks explored in this paper, the approach does seem to learn to evaluate whether a state matches the instructions quite well. - The writing is very clear and direct. ========== Concerns: ========== [A] The discussion of differences to the closely related GAIL methodology is left until the related work after experiments. Given the similarities between GAIL and AGILE, this seems too late. The authors list three major differences between AGILE and GAIL: 1) AGILE is conditioned on a goal specification, language in this case. GAIL is unconditioned and trained for one task. 2) AGILE takes only the final/goal state rather than a trajectory like in GAIL. 3) AGILE discretizes the discriminator probability when assigning reward, GAIL does not. Some concerns about each: 1) This is an interesting and fair difference but also a necessary and somewhat obvious modification to GAIL in tasks with explicit goal-specification. 2) This does not seem like an improvement, but rather a loss of generality. The authors justify this change saying \"in AGILE the reward model observes only states s_i (either goal states from an expert, or states from the agent acting on the environment) rather than traces (s1, a1),(s2, a2), . . ., learning to reward the agent based on \u201cwhat\u201d needs to be done rather than according to \u201chow\u201d it must be done.\" In many real applications, the how is deeply important. For instance, navigation in the world is both a \"what\" (arrive at location X) and a \"how\" (in fastest time without hitting anything or in such a way that humans aren't frightened). Further, the trace includes the final state such that the \"what\" is recoverable in instances where the \"how\" is unimportant, as in the set of tasks presented in this paper. 3) Letting the paper speak on this subject: \"We considered this change of objective necessary because the GAIL-style reward would take arbitrarily low values for intermediate states visited by the agent, as the reward model will be confident as those are not goal states. The binary reward in AGILE carries a clear message to the policy that all non-goal states are equally undesirable.\" Firstly, all non-goal states are not equally undesirable in that some lead more easily to goal states though it is fair to argue this should be learned by the policy through expected reward. My primary gripe is the footnote following these sentences which says: \"We tried values other than 0.5 for the binarization threshold, as well as not binarizing and using D\u03c6(c, st) directly as the reward. We got similar but slightly worse results.\" This seems to imply that this difference does not matter significantly, especially if different thresholds received significantly different hyperparameter tuning effort or were not conducted under multiple runs of random seeds. A pessimistic summary would place AGILE to be a conditional GAIL with reduced ability to represent intermediate or trajectory based rewards and a possibly slightly helpful reward discretization scheme. Don't get me wrong, I think an conditional extension to GAIL is interesting and worth sharing with the community. However, this discussion comes very late and includes a design decisions (2/3) that I find poorly justified in text and completely unjustified experimentally. I would like to hear from the authors if any of these criticisms are inaccurate. I would also welcome experiments evaluating the effect of these design decisions. [B] In 3.2 its reported that each experiment was repeated five times however the presented results are not described as means and no variances are shown. I would like to see the results plots with shaded variances from at least 5 runs with differing random seeds. [C] Unless I'm mistaken, the proposed architecture could also be trained with reward prediction. It would be interested in that case to see if improvement seen between A3C and A3C-AGILE extend to A3C-RP and A3C-RP-AGILE. As the authors note, the AGILE framework simply changes the source of the reward and is amicable to any RL approach. I would like to see this comparison. [D] The reward generalization experiments seemed surprising to me. The policy was fine-tuned on the test environments but only improved from 52% to 69.3%. Trying to think about this more, I'm having trouble disentangling whether this implies poor generalization of the reward function or increased difficulty in policy learning. Could the authors provide the A3C and A3C-RP baselines for this experiment to help clarify? [E] Just a Curiosity: What exactly is done in L2 weight clipping? (Training details in supplement) [F] Just a Thought: In the reward-prediction (RP) setting, both the RP model and the policy share parameters. It would be possible with such an architecture to still apply the AGILE loss and I would be curious to see if this leads to interesting changes in performance. I understand that one of the advantages to learning a separate reward model is to generalize to new policies, but it is unclear if this approach would generalize less well (and finding it out would be cool!) ========== Overview: ========== I think extending generative adversarial imitation learning to a task-conditional setting a cool step made even more interesting in this work by having the task-specification be in compositional language. Further, the results and analysis are generally interesting though I do note some weaknesses above. Aside from some questions about the experiments, I'm mostly concerned about the positioning of the paper -- specifically with respect to prior work. I'm looking forward to hearing from the authors and other reviewers. ", "rating": "7: Good paper, accept", "reply_text": "We thank reviewer 3 for their very detailed comments , which while critical of the paper , give us much to respond to , and much to consider with regard to improving the paper . We will be honest : if indeed the crucial failing of the paper is its failure to successfully position itself with regard to GAIL , in particular due to the placement of the paragraph discussing comparison , we find that a score of 4 is a little strict . However , we appreciate the reviewer has been very rigorous in explaining their points regarding this weakness . We hope that through discussion we will both be able to present cogent counter-arguments to these objections where applicable , and otherwise satisfy the reviewer that the positioning of the paper can be improved , to their satisfaction , with minor revisions to the paper . As the reviewer correctly summarized and if we put the conditioning on instructions aside , a key key difference ( item 2 ) of AGILE from GAIL is the fact that AGILE uses images of goal-states as the training data instead of state-action trajectories required for GAIL . We believe that this difference does make AGILE applicable to situations where GAIL would not be . For example , consider the case when the expert that provides demonstrations has effectors different from that of the trained agent ( image a human teaching their household robot to arrange objects on a table : the robot won \u2019 t necessarily have 5 fingers ) . Besides , methods such as GAIL force the learner to imitate the training trajectories in their entirety , which can be a limitation when training trajectories are suboptimal ( the above example with the human is applicable again , human may not use the shortest path to the goal when they perform their manipulations ) . Relying on goal-states only brings a certain extra flexibility that GAIL-like methods do not provide , admittedly , at the cost of restricting the applicability of AGILE to declarative instructions , i.e.those which can be verified by the final state only . We believe , however , that such instructions make a common and frequent case in instruction-following setups , and methods that are optimized for this case ( such as AGILE ) are worth investigating . Reviewer has also requested a clarification with regard to the difference between rewards provided by the discriminator in GAIL and AGILE . We admit that the paper may have been not clear enough on this point , as it stressed the discretization of the reward , whereas the real key difference is the switch from log D ( ... ) as the reward in GAIL to ( possibly discretized ) D ( ... ) in AGILE . The discriminator is trained to output arbitrary low values of log D ( c , s ) in AGILE for the non-goal-states , and using log D ( c , s ) as the reward would arbitrarily punish the policy for entering intermediate states that are clearly not goal-states but yet may be useful in the near future . We are currently performing an extra experiment to validate this claim , in which we use log D ( c , s ) as the reward , and so far we do not see the job getting above 70 % success rate , confirming our intuition that GAIL-style log D ( c , s ) is not an appropriate reward for AGILE . We will update you on the progress of this additional investigation . Notwithstanding the above differences with GAIL , it \u2019 s true that GAIL and AGILE both can be broadly characterised as inverse RL methods , and as such AGILE could be presented as \u201c just \u201d extending IRL to the instruction-conditional case ( and we believe we are reasonably up-front about this in the introduction ) . With all due respect , it is unfair to characterise this observation as a concern , or qualify it as an \u201c obvious extension \u201d : it is , to our knowledge , one of the first substantial studies of instruction-conditional IRL over pixel observations . Experiments show generalisation of the reward model to both new observed states and held-out instructions . The model and objective design underpinning these results are , while inspired by GAIL and related techniques , original insofar as new methods ( e.g.discriminator rejection ) had to be incorporated to make it work . We think that , on the strength of extending IRL techniques to multi-task contexts , where the task is specified by language , is new enough to warrant publication , and hope you will agree . ( see part 2 for continuation )"}, "2": {"review_id": "H1xsSjC9Ym-2", "review_text": "The paper presents an approach for simultaneously learning policies and reward functions for reaching goals that are described by an instruction providing spatial relations among objects. The proposed platform, called Adversarial Goal-Induced Learning from Examples (AGILE), is composed of an off-the-shelf RL module like A3C and a separate module for learning a reward function, implemented using the NMN paradigm. The RL module is trained using the reward function learned by the reward module. The reward module is trained to map a given <instruction, state> into a score between 0 and 1 depending on how well the provided state satisfies the instructions provided in the instruction. The returned score is used as a reward function. The training of the reward function is performed by using a dataset of positive examples, and using the states visited by the agent while it's learning as negative examples. To account for the fact that the agent becomes better over time and its visited states can no longer be used as negative examples, the authors proposed a heuristic where the states visited by the agent are not all used as negative examples, but only those that have the lowest scores. The paper also presents an empirical evaluation of the proposed approach on a synthetic task where the agent is tasked with move bocks of different shapes and colors to a desired final configuration. The AGILE approach was compared to the baseline A3C algorithm where a sparse binary reward signal was used only whenever the agent reaches the goal state. AGILE is also compared to A3C with an auxiliary task of reward prediction. The paper is clearly written and technically strong. However, I have two issues with this paper: 1) the proposed approach is a simple combination of A3C and the NMN architecture, 2) the experiments are performed on simple synthetic tasks that make learning spatial relations fairly easy, I would love to see more real images as it has been demonstrated in prior works on learning spatial relations. It is not clear from these experiments if the proposed approach will scale up to higher-dimensional inputs. Moreover, there are several stability issues that can be caused by the proposed approach. For instance, the reward function is changing over time, how does that affect the learning rate? Also, instead of using the learned policy itself to generate negative examples and run into non IID data, instabilities, and increasingly good negative examples, why not use a fixed dataset of negative examples generated with a random policy? It would be interesting to do perform an experiment where you compare to the classical reward learning setup where you simply provided labeled positive and negative examples and classify them offline, then use the learned reward function online for RL. How did you tune the hyper-parameter \\rho (percentage of negative examples to discard) for specific tasks? Do you have any guarantees for this approach? In the generalization experiments, it is mentioned that 10% of the instructions are held out. Are these 10% randomized?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank Reviewer 1 for their review and statement of support for the paper \u2019 s technical contributions . We hope , during this discussion period , to get a better understanding of your concerns and hopefully address them , making clarifications in the paper where needed . First , could you please clarify your first concern that \u201c the proposed approach is a simple combination of A3C and the NMN architecture \u201d ? While this is an accurate portrayal of some of our key results , we stress that : 1 ) we also report results for LSTM + FiLM-ConvNet architectures for both policy and reward model networks , to showcase performance within AGILE when the syntax of the language is not given ( see \u201c AGILE with Structure-Agnostic Models \u201d in Section 3 ) . 2 ) use of A3C for our policy network is not an essential part of AGILE . AGILE is a general framework for jointly training policies and reward models , both conditioned on language instructions . Any RL method and network architecture can be used , since the only difference between \u201c traditional \u201d RL and AGILE is the source of the reward : in the former case , it comes from the environment , and in the latter , from the jointly learned reward model . As such , for both the baseline and the AGILE-based model , the fact that we used A3C is not important ; all that is important is that we use the same RL algorithm and architecture for both the baseline and AGILE . With this in mind , do you still believe this aspect of our evaluation is cause for concern ? Second , you suggest experiments which are more visually realistic with more complex relations . We agree that this is where this research should be going , and discuss such further work in section 5 , along with the particular challenges we anticipate it will bring . We would be remiss not to point out two things , with regard to our current experiments : 1 ) While simple , the environment and tasks are actually quite diverse and approached without the simplifying assumptions typically seen in grid worlds : a ) There are over 1,000 instructions in GridLU-Relations , each specifying a different task with millions of different initial environment states to solve . b ) The environment is observed , by both policy and reward model , at the pixel level , without predefined notions of what and where objects and their boundaries are , or \u201c privileged features \u201d indicating which predicates apply to which objects ( or even which predicates exist ) . 2 ) As we see in the left plot of Figure 3 , the A3C baseline results show that , due to the two points made above , even when the reward is specified by the environment ( for our baselines ) , this is quite a difficult multi-task RL problem for fairly modern agent architectures and RL algorithms . This is because it may take over a dozen steps to optimally obtain a goal state , the task is changing every episode , and reward is sparse . When you add to that the need to jointly infer the reward function from a limited number of expert examples , this constitutes a fairly significant machine learning problem . While we agree that there are no guarantees with regard to scalability to more realistic environments \u201c out of the box \u201d , we hope you will agree this makes a substantial contribution by showing that it is possible to obtain agents which align with expert notions of reward through the proxy of learned reward functions , from examples , and that it is reasonable to leave further developments which scale to more complex environments for further research ."}}