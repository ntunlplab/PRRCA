{"year": "2021", "forum": "DAaaaqPv9-q", "title": "Self-supervised Graph-level Representation Learning with Local and Global Structure", "decision": "Reject", "meta_review": "This paper proposes a self-supervised learning method for learning representations for graph-structured data, with both local and global objectives. The local objective aims to maximize the mutual information between two correlated graphs generated with attribute masking [Hu et al. 19], with the InfoNCE loss [van den Oord et al. 18], and the global objective aims to cluster the graphs using the RPCL [Xu et al. 93] objective, which pulls the sample toward the closest cluster while pushing it away from the rival clusters. The proposed method is validated on standard graph classification benchmarks by training a linear classifier on top of the GNN pre-trained with it, and the results show that it largely outperforms existing graph pre-training methods. \n\nThis paper fell into a borderline case, receiving split reviews with two of the reviewers learning toward rejection, and two others proposing to accept. The reviewers in general agreed that the experimental validation is thorough (except for one reviewer), and some of the reviewers mentioned that the proposed idea of performing self-supervised learning at both local and global level makes sense. However, the negative reviewers were concerned with the limited novelty of the proposed method, since the proposed method seems like a simple combination of two objectives each of which are based on existing ideas (although the latter has not been explored for GNN pre-training). The reviewers had interactive discussions with the authors, and the authors provided detailed feedback. Yet, the reviewers were not convinced that the method has sufficient novelty to warrant publication even after the internal discussion period, and decided to keep their negative ratings.\n\nI believe that this is a simple yet effective pre-training method for GNNs on graph-structured data. The proposed method of combining the local and global objective seems like a promising solution to learn a metric space that well-captures the graph-level similarity and also is well-separated for discriminative classification, and it may have some practical impact given its good performance on benchmark datasets. However, as the two negative reviewers mentioned, the paper in its current form is presented as a simple combination of existing approaches. The local objective is a slight modification of attribute masking strategy of [Hu et al. 19], and the global objective of clustering has been explored in self-supervised learning of CNNs for image data [Asano et al. 20]. Thus, I lean toward rejecting the paper, considering its relative novelty and quality. \n\nHowever, I find the proposed work highly promising, and encourage the authors to further develop the method while also improving on the paper writing. I suggest the authors to focus more on the main idea of learning with both local and global objectives, without specifically tying each objective to any of the existing methods. The authors may consider various techniques for both local and global objectives (such as hinge loss-based contrastive loss with k-means clustering as shown in the response to R3), and suggest the proposed work as a more general framework.  \n\n[Asano et al. 20] Self-Labeling via Simultaneous Clustering and Representation Learning, ICLR 2020", "reviews": [{"review_id": "DAaaaqPv9-q-0", "review_text": "This paper proposes an unsupervised framework to perform graph representation learning . The local-instance structure is learned by first gets patch-level and graph-level representations for each graph , then maximize the mutual information between both correlated patches and correlated graphs , which are decided by attribute masking strategy . The global-semantic structure is maintained by leveraging RPCL to derive hierarchical prototypes of the representation and maximizing the mutual information between correlated graph representation and the searching path in the prototypes . Strengths : + This paper presents a framework to jointly consider the local instance structure and global-semantic structure of graphs . It is a meaningful direction and could be beneficial for explainability . + The experimental results are quite thorough with comparisons to several baseline methods . Moreover , the ablation study of different mechanisms is provided in the experiments . Weaknesses : - The proposed model seems like a simple combination of several existing techniques and thus lacks novelty . - The performance of this model seems to heavily rely on the attribute masking strategy as all the operations are built upon the correlated graph pairing from the attribute masking strategy . But how reliable is this technique ? It seems to be a bottleneck of the model , and I think there should be an explanation on this either theoretically or experimentally . Overall , the proposes a reasonable model for learning hierarchical graph representations . However , the novelty is limited since the proposed method seems like a simple combination of several existing techniques . Questions : 1 . As I mentioned earlier , I wonder how reliable the attribute masking strategy is . As graph matching is an extremely hard problem , can this strategy provide a reliable pairing between correlated graphs ? 2.It is not clear how to leverage prototypes in classification tasks ? I understand that the prototypes serve to ensure a better structure of the embeddings , but when classifying graphs , I wonder whether embeddings and prototypes are both used or not ? 3.In the Constraint for the global-semantic structure part , the loss for a graph embedding includes both the representations for its correlated graph and its searching path consisting of several prototypes . When minimizing the loss , I wonder both representations of the correlated graph and prototypes are updated together ? Or the prototypes are only updated in eq . ( 11 ) and ( 12 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your insightful reviews on this work ! We first would like to re-emphasize the novelty of this work . This paper is dedicated to self-supervised graph representation learning which preserves both the local-instance and global-semantic structure of a set of unlabeled graphs . The proposed GraphLoG model is novel as a whole , since this learning problem , to the best of our knowledge , has not been studied by previous works . During constructing the entire model , we adopt some existing techniques , i.e.attribute masking [ a ] , InfoNCE loss [ b ] and RPCL clustering [ c ] , to promote model \u2019 s performance . However , these techniques can be substituted with the vanilla counterpart , e.g.InfoNCE loss - > hinge-loss-based contrastive loss , RPCL - > K-means , without too much hurt to model \u2019 s effectiveness , which is analyzed by the additional ablation studies in the * * Section E * * of appendix . In summary , we utilize these techniques as the performance-boosting modules serving for the core idea , local-instance and global-semantic learning , instead of simply combining them together . We respond to your questions as follows : Q1 : The theoretical and experimental analysis about the reliability of attribute masking strategy should be supplemented . A1 : We give a theoretical analysis about GNN \u2019 s capability of repairing the information lost by attribute masking in the * * Section A * * of appendix , and also empirically show that the correlated graphs derived by attribute masking is more reliable in * * Section E.1 * * . Q2 : How to use hierarchical prototypes in graph classification tasks ? A2 : In the self-supervised model GraphLoG , since the hierarchical prototypes can not directly correspond to the categories of downstream tasks , they are not employed during graph classification . To study this problem in-depth , we additionally design a supervised learning variant , sup-GraphLoG , in * * Section 4 * * to verify the effectiveness of hierarchical prototypes on graph classification with explicit supervision . The sup-GraphLoG model outperforms the vanilla GNN on chemical and biological benchmark datasets ( shown in * * Tabs.1 and 2 * * ) . Q3 : In the global-semantic loss , are graph embedding and hierarchical prototypes optimized jointly ? A3 : Appreciate for this good question . In the current model , only the representation of correlated graph is updated by the global-semantic loss ( Eq.13 ) , and hierarchical prototypes are only updated by Eqs . 11 and 12 . The joint optimization of these two types of variables will be the direction of our future exploration . * * In the revised paper , you can refer to the bolded sections above for the detailed contents related to your questions . * * [ a ] Hu , Weihua , et al . `` Strategies for Pre-training Graph Neural Networks . '' ICLR , 2020 . [ b ] Oord , Aaron van den , Yazhe Li , and Oriol Vinyals . `` Representation learning with contrastive predictive coding . '' arXiv:1807.03748 ( 2018 ) . [ c ] Xu , Lei , Adam Krzyzak , and Erkki Oja . `` Rival penalized competitive learning for clustering analysis , RBF net , and curve detection . '' IEEE Transactions on Neural networks , 1993 ."}, {"review_id": "DAaaaqPv9-q-1", "review_text": "This paper proposed a method for self-supervised graph-level representation learning . The main idea is to enforce both the instance level smoothness embedding constraints , and a so-called global , semantic grouping structures across all instance graphs in the training data set . To achieve this goal , the authors have adopted a global clustering framework to encourage the embedding of the graphs belonging to the same clusters to be close to each other , and by using a hierarchically organized set of prototypes . The proposed method is applied to pre-train GNN on massive unlabeled graphs , which is then fine-tuned to downstream learning tasks . Enforcing a global clustering structure can be useful in capturing the distribution of large number of graphs in the training data set and hopefully carry the learned representations over to other tasks . However , it appears to me that the paper has combined the carefully devised ideas from too many existing work , each of which alone has shown great success in improving the learning performance . Therefore it can be difficult to judge which part of the choices really leads to the final improvement , and in particular whether it is the local and global structure preserving part , which seems to be the core theme of the paper ( with the other theme being sel-supervised learning ) , that can fully explain the result . In more detail , the authors have used ( 1 ) masking strategy by Hu et al. , 2019 to generate correlated graph pairs , ( 2 ) mutual information estimation technique InfoNCE to enforce the correlation between paired graphs , and ( 3 ) Rival Penalized Competitive Learning ( RPCL ) as the main building block for hierarchical prototype-based learning . Therefore , a natural question to ask is , if one uses plain GNN architecture of each graph and plug it in a ( hierarchical ) clustering framework ( without self-supervised learning and RPCL ) , whether similar improvements in the learning performance can still be obtained ? If not , then the gains are merely due to the effectiveness of these specially designed components from the literatures and not by the general idea of local and global structures . With regard to this concern , I would suggest the authors to clarify on what is the main theme of the work and demonstrate that the empirical performance gains are truly due to a novel , focused idea they propose rather than by combining some of the existing algorithms which have shown great impact and performance gains in their respective context . In the current form , the novelrity of the work seems less significant by introducing so many components from other works .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your insightful comments and great suggestions , which definitely help us improve the quality of this work . We respond to your concerns about the novelty of this work as follows : Q1 : The combination of too many existing techniques shadows the novelty of this work . A1 : Our core idea is to learn both the local-instance and global-semantic structure of a set of graphs in a self-supervised fashion . This problem , to the best of our knowledge , has not been explored by previous works , which makes the proposed GraphLoG model novel as a whole . Indeed , some existing techniques , i.e.attribute masking [ a ] , InfoNCE loss [ b ] and RPCL clustering [ c ] , have been adopted to construct the entire model . However , the additional ablation studies in the * * Section E * * of appendix show that these techniques ( except attribute masking ) can be replaced by their vanilla counterparts without too much hurt to model \u2019 s performance , which demonstrates the effectiveness of local-instance and global-semantic learning , i.e.the key idea of this work . Q2 : If we combine a plain GNN with hierarchical prototypes , can this model achieve superior performance ? A2 : Following your suggestion , we further design a supervised variant of GraphLoG , named as sup-GraphLoG , in * * Section 4 * * . The sup-GraphLoG model establishes hierarchical prototypes on top of a plain GNN and performs graph classification by measuring the similarity between graph embeddings and prototypes . This model outperforms the plain GNN on chemical and biological benchmark datasets ( shown in * * Tabs.1 and 2 * * ) , which illustrates the effectiveness of global-semantic learning under the supervised setting . In addition , we would like to point out that the sup-GraphLoG model does not perform as well as the GraphLoG model , which demonstrates the necessity of self-supervised pre-training on massive unlabeled graphs . * * In the revised paper , you can refer to the bolded sections above for the detailed contents related to your concerns . * * [ a ] Hu , Weihua , et al . `` Strategies for Pre-training Graph Neural Networks . '' ICLR , 2020 . [ b ] Oord , Aaron van den , Yazhe Li , and Oriol Vinyals . `` Representation learning with contrastive predictive coding . '' arXiv:1807.03748 ( 2018 ) . [ c ] Xu , Lei , Adam Krzyzak , and Erkki Oja . `` Rival penalized competitive learning for clustering analysis , RBF net , and curve detection . '' IEEE Transactions on Neural networks , 1993 ."}, {"review_id": "DAaaaqPv9-q-2", "review_text": "The motivation and novelty of the proposed method are good . However , the validation is kind of weak . I can understand that this papers follows the validation in Hu et al ( 2019 ) , however , I feel that two tasks ( one on chemical benchmark and one on biological benchmark ) may not be sufficient to give a detailed idea of the improvement of the proposed GraphLoG over other baselines . I think 3-5 tasks are much better . For the ablation study in Section 5.4 , these ablated items are good . However , I more would like to see fluctuated parts in the proposed GraphLoG . One example may be : is there any different choice/option for hierarchical prototype ? Which one is good/bad ? What is the reason . Or other potential and similar examples exist in the proposed GraphLoG . I think this will help us to understand GraphLoG more .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your support to the motivation and methodology of this work ! We address your concern on the experimental verification as follows : Q1 : More benchmark tasks should be added to evaluate the proposed method . A1 : We additionally evaluate the proposed model on five graph classification benchmarks in the * * Section D * * of appendix . These five benchmark datasets involve the classification on the molecular graphs and social networks , and they are commonly used in previous self-supervised graph representation learning literature [ a , b ] . Q2 : More ablation studies should be conducted in order to better understand the proposed GraphLoG model . A2 : We conduct more thorough ablation studies on the correlated graph construction , loss constraint and clustering algorithm in the * * Section E * * of appendix . * * In the revised paper , you can refer to the bolded sections above for the detailed experimental results . * * [ a ] Sun , Fan-Yun , et al . `` Infograph : Unsupervised and semi-supervised graph-level representation learning via mutual information maximization . '' ICLR , 2020 . [ b ] Hassani , Kaveh , and Amir Hosein Khasahmadi . `` Contrastive Multi-View Representation Learning on Graphs . '' ICML , 2020 ."}, {"review_id": "DAaaaqPv9-q-3", "review_text": "Pros : - The paper proposed a novel self-supervised learning method to embed graphs to vector space . Different from previous methods , the method proposed a global-semantic learning strategy to encourage the embeddings to form a hierarchical clustering structure . Both the embedding network and the hierarchical structure can be jointly learned . - Authors have provided extensive and convincing comparison results and numerical analysis to show the effectiveness of the method . - The paper is well-organized and clearly written . To the best of my knowledge , the proposed method is technically feasible . Cons : - The number of prototypes is determined by RPCL and can not be adjusted in training . - Clustering algorithms are usually not very robust . Since the prototypes of GraphLoG is initialized by RPCL , is the performance of GraphLoG robust ?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks very much for your recognition in our work ! We address your two concerns as follows : Q1 : The number of prototypes determined by RPCL can not be adjusted during training . A1 : In the * * Section E.3 * * of appendix , we design an adaptive variant of RPCL clustering ( Adaptive-RPCL ) which is able to adjust the number of prototypes during training . Its performance on biological downstream task is comparable with that of vanilla RPCL clustering algorithm in our method , which shows that the proposed GraphLoG model is not too sensitive to the selection of clustering algorithm . Q2 : Is the performance of GraphLoG robust to different clustering outputs ? A2 : In the * * Section E.4 * * of appendix , we compare the performance of six pre-trained models derived by different clustering results . It is observed that the downstream task performance of these models is comparable with each other , which demonstrates that the GraphLoG model is fairly robust to different clustering outputs . * * In the revised paper , you can refer to the bolded sections above for the detailed experimental results . * *"}], "0": {"review_id": "DAaaaqPv9-q-0", "review_text": "This paper proposes an unsupervised framework to perform graph representation learning . The local-instance structure is learned by first gets patch-level and graph-level representations for each graph , then maximize the mutual information between both correlated patches and correlated graphs , which are decided by attribute masking strategy . The global-semantic structure is maintained by leveraging RPCL to derive hierarchical prototypes of the representation and maximizing the mutual information between correlated graph representation and the searching path in the prototypes . Strengths : + This paper presents a framework to jointly consider the local instance structure and global-semantic structure of graphs . It is a meaningful direction and could be beneficial for explainability . + The experimental results are quite thorough with comparisons to several baseline methods . Moreover , the ablation study of different mechanisms is provided in the experiments . Weaknesses : - The proposed model seems like a simple combination of several existing techniques and thus lacks novelty . - The performance of this model seems to heavily rely on the attribute masking strategy as all the operations are built upon the correlated graph pairing from the attribute masking strategy . But how reliable is this technique ? It seems to be a bottleneck of the model , and I think there should be an explanation on this either theoretically or experimentally . Overall , the proposes a reasonable model for learning hierarchical graph representations . However , the novelty is limited since the proposed method seems like a simple combination of several existing techniques . Questions : 1 . As I mentioned earlier , I wonder how reliable the attribute masking strategy is . As graph matching is an extremely hard problem , can this strategy provide a reliable pairing between correlated graphs ? 2.It is not clear how to leverage prototypes in classification tasks ? I understand that the prototypes serve to ensure a better structure of the embeddings , but when classifying graphs , I wonder whether embeddings and prototypes are both used or not ? 3.In the Constraint for the global-semantic structure part , the loss for a graph embedding includes both the representations for its correlated graph and its searching path consisting of several prototypes . When minimizing the loss , I wonder both representations of the correlated graph and prototypes are updated together ? Or the prototypes are only updated in eq . ( 11 ) and ( 12 ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your insightful reviews on this work ! We first would like to re-emphasize the novelty of this work . This paper is dedicated to self-supervised graph representation learning which preserves both the local-instance and global-semantic structure of a set of unlabeled graphs . The proposed GraphLoG model is novel as a whole , since this learning problem , to the best of our knowledge , has not been studied by previous works . During constructing the entire model , we adopt some existing techniques , i.e.attribute masking [ a ] , InfoNCE loss [ b ] and RPCL clustering [ c ] , to promote model \u2019 s performance . However , these techniques can be substituted with the vanilla counterpart , e.g.InfoNCE loss - > hinge-loss-based contrastive loss , RPCL - > K-means , without too much hurt to model \u2019 s effectiveness , which is analyzed by the additional ablation studies in the * * Section E * * of appendix . In summary , we utilize these techniques as the performance-boosting modules serving for the core idea , local-instance and global-semantic learning , instead of simply combining them together . We respond to your questions as follows : Q1 : The theoretical and experimental analysis about the reliability of attribute masking strategy should be supplemented . A1 : We give a theoretical analysis about GNN \u2019 s capability of repairing the information lost by attribute masking in the * * Section A * * of appendix , and also empirically show that the correlated graphs derived by attribute masking is more reliable in * * Section E.1 * * . Q2 : How to use hierarchical prototypes in graph classification tasks ? A2 : In the self-supervised model GraphLoG , since the hierarchical prototypes can not directly correspond to the categories of downstream tasks , they are not employed during graph classification . To study this problem in-depth , we additionally design a supervised learning variant , sup-GraphLoG , in * * Section 4 * * to verify the effectiveness of hierarchical prototypes on graph classification with explicit supervision . The sup-GraphLoG model outperforms the vanilla GNN on chemical and biological benchmark datasets ( shown in * * Tabs.1 and 2 * * ) . Q3 : In the global-semantic loss , are graph embedding and hierarchical prototypes optimized jointly ? A3 : Appreciate for this good question . In the current model , only the representation of correlated graph is updated by the global-semantic loss ( Eq.13 ) , and hierarchical prototypes are only updated by Eqs . 11 and 12 . The joint optimization of these two types of variables will be the direction of our future exploration . * * In the revised paper , you can refer to the bolded sections above for the detailed contents related to your questions . * * [ a ] Hu , Weihua , et al . `` Strategies for Pre-training Graph Neural Networks . '' ICLR , 2020 . [ b ] Oord , Aaron van den , Yazhe Li , and Oriol Vinyals . `` Representation learning with contrastive predictive coding . '' arXiv:1807.03748 ( 2018 ) . [ c ] Xu , Lei , Adam Krzyzak , and Erkki Oja . `` Rival penalized competitive learning for clustering analysis , RBF net , and curve detection . '' IEEE Transactions on Neural networks , 1993 ."}, "1": {"review_id": "DAaaaqPv9-q-1", "review_text": "This paper proposed a method for self-supervised graph-level representation learning . The main idea is to enforce both the instance level smoothness embedding constraints , and a so-called global , semantic grouping structures across all instance graphs in the training data set . To achieve this goal , the authors have adopted a global clustering framework to encourage the embedding of the graphs belonging to the same clusters to be close to each other , and by using a hierarchically organized set of prototypes . The proposed method is applied to pre-train GNN on massive unlabeled graphs , which is then fine-tuned to downstream learning tasks . Enforcing a global clustering structure can be useful in capturing the distribution of large number of graphs in the training data set and hopefully carry the learned representations over to other tasks . However , it appears to me that the paper has combined the carefully devised ideas from too many existing work , each of which alone has shown great success in improving the learning performance . Therefore it can be difficult to judge which part of the choices really leads to the final improvement , and in particular whether it is the local and global structure preserving part , which seems to be the core theme of the paper ( with the other theme being sel-supervised learning ) , that can fully explain the result . In more detail , the authors have used ( 1 ) masking strategy by Hu et al. , 2019 to generate correlated graph pairs , ( 2 ) mutual information estimation technique InfoNCE to enforce the correlation between paired graphs , and ( 3 ) Rival Penalized Competitive Learning ( RPCL ) as the main building block for hierarchical prototype-based learning . Therefore , a natural question to ask is , if one uses plain GNN architecture of each graph and plug it in a ( hierarchical ) clustering framework ( without self-supervised learning and RPCL ) , whether similar improvements in the learning performance can still be obtained ? If not , then the gains are merely due to the effectiveness of these specially designed components from the literatures and not by the general idea of local and global structures . With regard to this concern , I would suggest the authors to clarify on what is the main theme of the work and demonstrate that the empirical performance gains are truly due to a novel , focused idea they propose rather than by combining some of the existing algorithms which have shown great impact and performance gains in their respective context . In the current form , the novelrity of the work seems less significant by introducing so many components from other works .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your insightful comments and great suggestions , which definitely help us improve the quality of this work . We respond to your concerns about the novelty of this work as follows : Q1 : The combination of too many existing techniques shadows the novelty of this work . A1 : Our core idea is to learn both the local-instance and global-semantic structure of a set of graphs in a self-supervised fashion . This problem , to the best of our knowledge , has not been explored by previous works , which makes the proposed GraphLoG model novel as a whole . Indeed , some existing techniques , i.e.attribute masking [ a ] , InfoNCE loss [ b ] and RPCL clustering [ c ] , have been adopted to construct the entire model . However , the additional ablation studies in the * * Section E * * of appendix show that these techniques ( except attribute masking ) can be replaced by their vanilla counterparts without too much hurt to model \u2019 s performance , which demonstrates the effectiveness of local-instance and global-semantic learning , i.e.the key idea of this work . Q2 : If we combine a plain GNN with hierarchical prototypes , can this model achieve superior performance ? A2 : Following your suggestion , we further design a supervised variant of GraphLoG , named as sup-GraphLoG , in * * Section 4 * * . The sup-GraphLoG model establishes hierarchical prototypes on top of a plain GNN and performs graph classification by measuring the similarity between graph embeddings and prototypes . This model outperforms the plain GNN on chemical and biological benchmark datasets ( shown in * * Tabs.1 and 2 * * ) , which illustrates the effectiveness of global-semantic learning under the supervised setting . In addition , we would like to point out that the sup-GraphLoG model does not perform as well as the GraphLoG model , which demonstrates the necessity of self-supervised pre-training on massive unlabeled graphs . * * In the revised paper , you can refer to the bolded sections above for the detailed contents related to your concerns . * * [ a ] Hu , Weihua , et al . `` Strategies for Pre-training Graph Neural Networks . '' ICLR , 2020 . [ b ] Oord , Aaron van den , Yazhe Li , and Oriol Vinyals . `` Representation learning with contrastive predictive coding . '' arXiv:1807.03748 ( 2018 ) . [ c ] Xu , Lei , Adam Krzyzak , and Erkki Oja . `` Rival penalized competitive learning for clustering analysis , RBF net , and curve detection . '' IEEE Transactions on Neural networks , 1993 ."}, "2": {"review_id": "DAaaaqPv9-q-2", "review_text": "The motivation and novelty of the proposed method are good . However , the validation is kind of weak . I can understand that this papers follows the validation in Hu et al ( 2019 ) , however , I feel that two tasks ( one on chemical benchmark and one on biological benchmark ) may not be sufficient to give a detailed idea of the improvement of the proposed GraphLoG over other baselines . I think 3-5 tasks are much better . For the ablation study in Section 5.4 , these ablated items are good . However , I more would like to see fluctuated parts in the proposed GraphLoG . One example may be : is there any different choice/option for hierarchical prototype ? Which one is good/bad ? What is the reason . Or other potential and similar examples exist in the proposed GraphLoG . I think this will help us to understand GraphLoG more .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your support to the motivation and methodology of this work ! We address your concern on the experimental verification as follows : Q1 : More benchmark tasks should be added to evaluate the proposed method . A1 : We additionally evaluate the proposed model on five graph classification benchmarks in the * * Section D * * of appendix . These five benchmark datasets involve the classification on the molecular graphs and social networks , and they are commonly used in previous self-supervised graph representation learning literature [ a , b ] . Q2 : More ablation studies should be conducted in order to better understand the proposed GraphLoG model . A2 : We conduct more thorough ablation studies on the correlated graph construction , loss constraint and clustering algorithm in the * * Section E * * of appendix . * * In the revised paper , you can refer to the bolded sections above for the detailed experimental results . * * [ a ] Sun , Fan-Yun , et al . `` Infograph : Unsupervised and semi-supervised graph-level representation learning via mutual information maximization . '' ICLR , 2020 . [ b ] Hassani , Kaveh , and Amir Hosein Khasahmadi . `` Contrastive Multi-View Representation Learning on Graphs . '' ICML , 2020 ."}, "3": {"review_id": "DAaaaqPv9-q-3", "review_text": "Pros : - The paper proposed a novel self-supervised learning method to embed graphs to vector space . Different from previous methods , the method proposed a global-semantic learning strategy to encourage the embeddings to form a hierarchical clustering structure . Both the embedding network and the hierarchical structure can be jointly learned . - Authors have provided extensive and convincing comparison results and numerical analysis to show the effectiveness of the method . - The paper is well-organized and clearly written . To the best of my knowledge , the proposed method is technically feasible . Cons : - The number of prototypes is determined by RPCL and can not be adjusted in training . - Clustering algorithms are usually not very robust . Since the prototypes of GraphLoG is initialized by RPCL , is the performance of GraphLoG robust ?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks very much for your recognition in our work ! We address your two concerns as follows : Q1 : The number of prototypes determined by RPCL can not be adjusted during training . A1 : In the * * Section E.3 * * of appendix , we design an adaptive variant of RPCL clustering ( Adaptive-RPCL ) which is able to adjust the number of prototypes during training . Its performance on biological downstream task is comparable with that of vanilla RPCL clustering algorithm in our method , which shows that the proposed GraphLoG model is not too sensitive to the selection of clustering algorithm . Q2 : Is the performance of GraphLoG robust to different clustering outputs ? A2 : In the * * Section E.4 * * of appendix , we compare the performance of six pre-trained models derived by different clustering results . It is observed that the downstream task performance of these models is comparable with each other , which demonstrates that the GraphLoG model is fairly robust to different clustering outputs . * * In the revised paper , you can refer to the bolded sections above for the detailed experimental results . * *"}}