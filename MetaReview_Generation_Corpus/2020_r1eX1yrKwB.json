{"year": "2020", "forum": "r1eX1yrKwB", "title": "Distribution Matching Prototypical Network for Unsupervised Domain Adaptation", "decision": "Reject", "meta_review": "This paper addresses the problem of unsupervised domain adaptation and proposes explicit modeling of the source and target feature distributions to aid in cross-domain alignment. \n\nThe reviewers all recommended rejection of this work. Though they all understood the paper\u2019s position of explicit feature distribution modeling, there was a lack of understanding as to why this explicit modeling should be superior to the common implicit modeling done in related literature. As some reviewers raised concern that the empirical performance of the proposed approach was marginally better than competing methods, this experimental evidence alone was not sufficient justification of the explicit modeling. There was also a secondary concern about whether the two proposed loss functions were simultaneously necessary. \n\nOverall, after reading the reviewers and authors comments, the AC recommends this paper not be accepted. ", "reviews": [{"review_id": "r1eX1yrKwB-0", "review_text": "<Paper summary> The authors proposed Distribution Matching Prototypical Network (DMPN) for unsupervised domain adaptation. DMPN extracts features from the input data and models them as Gaussian mixture distributions. By explicitly modeling the distributions that the features follow, the discrepancy between the distribution of source data and that of target data can be easily evaluated. DMPN is trained by jointly minimizing two kinds of loss, which are classification loss on the source data and domain discrepancy loss that is calculated via the explicit models. Experimental results on two popular benchmark datasets validate the advantage of DMPN over other state-of-the-art methods. <Review summary> The proposed method seems simple but empirically performs well. The paper is well written and easy to follow, so we can maybe easily implement it. However, I have several concerns mainly about the details and theories of the proposed method, which makes my score a bit lower than the border line. Given clarifications in an author response, I would be willing to increase the score. <Details> * Strength + The motivation of using ProtoNet for domain adaptation seems reasonable. + The proposed method performs well in the experiments. + The paper, especially the experiment section, is well written and easy to follow. * Weakness and concerns - Several points on the proposed loss (GCMM and PDM) are not sufficiently discussed. -- Why do we need two kinds of loss? These losses seem to play almost same role. Since PDM loss corresponds to target-side log likelihood regularization term (Eq. (3)), I wonder if we really need GCMM loss. -- Since the authors explicitly model the feature distributions by Gaussian mixtures (GMs), it might be possible to calculate a standard divergence between source and target data distributions by using the parameters of GMs. Compared with such a straightforward approach, the proposed method seems to be ad-hoc and is not theoretically validated. What term of divergence (or distance) does it minimize? -- When a certain class does not appear in pseudo-labeled target data, how can we calculate GCMM loss? (specifically, \\mu^{et}_c) -- Are Eq. (3) and Eq. (6) correct? These are defined as total loss, not average, over each domain. It means that the scale of the coefficients for these terms changes according to the number of training data, but the sensitivity analysis in Fig. 2 does not show such effect. -- Since the proposed losses heavily depend on the pseudo labels on the target data, it should be important to carefully set a proper threshold for the confidence. Is the proposed method sensitive against the change of this threshold? If so, how can we tune it? -- How can we know p(c) in advance? - The theory shown in 3.5 is not sufficiently validated. -- The authors state \"\"we minimize the first term through minimizing the domain discrepancy losses,\" but it is not sufficiently supported, because the relationship between the proposed losses and H-delta-H divergence is not clear. - I am concerned about whether the proposed method works well with harder datasets such as Office-Home dataset, because each class data are modeled by a simple Gaussian distribution in the proposed method. * Minor concerns that do not have an impact on the score - Using both f^s_i and F(x^s_i; \\theta) is confusing. - Typo in Eq. (7): PMD -> PDM ", "rating": "3: Weak Reject", "reply_text": "Thanks for reviewing our paper and your appreciation of our idea . Here we answer your concerns and clarify some of the weak points you mentioned : 1 ) . These two losses actually serve with different purposes when we design them . The GCMM loss brings the two distribution closer via minimizing the corresponding Gaussian Component means of the source and target data . And the PDM loss shapes the target feature distribution similar as the source feature distribution via minimizing the likelihood of generating the target feature from the source feature distribution . In this sense , they complement each other , to match the target feature distribution to be exactly like the source feature distribution . Furthermore , these two losses also reduce distribution discrepancy at different levels , GCMM reduces distribution discrepancy at the class-level and PDM reduces distribution discrepancy at the sample-level , thus in this sense , they also complete each other for domain adaptation . 2 ) .We want to clarify here . Our method does not learn the distribution parameters for the target data . We learn the distribution parameters of the source data . We use the empirically calculated distribution parameter estimator of the source and target data to minimize the distribution discrepancy loss function . Thus , we can not `` calculate a standard divergence between source and target data distributions by using the parameters of GMs . '' For the GCMM loss , our method minimizes the euclidean distance between the corresponding Gaussian Component means of the source and target data for each class . PDM loss minimizes the likelihood of generating the target feature with the source feature distribution . 3 ) .We will ignore data from that class in the batch in that training iteration . As training data are sampled randomly in each iteration , and in the end , all data updates the model . 4 ) .Yes , you are correct . We forgot to average the term when writing the paper . We have corrected it in the revised paper . Thanks for pointing this out . 5 ) .We have added a sensitivity experiment on the confidence threshold . The results are in the appendix of the revised paper . Here is the summary : confidence-threshold : 0.6 , 0.7 , 0.8 , 0.9 Mean-accuracy : 81.3 , 81.4 , 81.4 , 81.5 The results show that our method is also robust against confidence threshold . For our proposed probability based weighting mechanism , as there is no hyper-parameter in there , so there is no need to provide sensitivity analysis on it . 6 ) .We know p ( c ) in the source domain , as it has labels . We do not know p ( c ) in the target domain , but we can estimate it . In this paper , we assume p ( c ) is uniform , as we focus on co-variate shift in this paper . Our work can be easily augmented to work for label shift too , once we estimate the target label distribution . However , we leave this as future work . 7 ) .The H-delta-H divergence is small when the two distribution discrepancy is small . As GCMM loss brings the two distribution closer , and PDM loss shapes the two distribution to be alike , the source and target feature distribution discrepancy will be smaller . Thus H-delta-H becomes smaller as we minimize GCMM loss and PDM loss . We have updated the paper on this part to make it clearer . Thanks for indicating this . 8 ) .We have added an experiment on the Office-Home dataset in the appendix of our paper . Our paper performs the best in all the transfer tasks in Office-Home compared to state-of-the-art UDA methods , showing that it also works for this more challenging dataset . 9 ) .Thanks for pointing out some of our typos , we have made the changes in our revised paper ."}, {"review_id": "r1eX1yrKwB-1", "review_text": "The paper develops a new method for adapting models trained on labeled data from some source domain to unlabeled data in a target domain. The authors accomplish this by adapting a technique from [1] and [2] enforcing that the deep features learned during training approximately follow a Gaussian mixture distribution. With the learned features in this form, the authors ensure domain adaptation by minimizing the discrepancy between the distributions arising from the source and target datasets. Strengths: + The paper's experiments show an improvement in the model's performance relative to past work, utilizing a large number of comparison models. + The use of explicit distributional information within the learned representations seems like a good fit for the task at hand, and the authors' experiments back this up. Weaknesses: - The proposed method for unsupervised domain adaptation is very similar to the prototypical networks approach in [3], with the primary difference being a loss term incentivizing a Gaussian mixture distribution over features. - While the authors achieve improved performance over [3], the gains in classification accuracy on the target dataset aren't especially huge (~1-3%). - The paper is a bit hard to follow, and would be improved by giving a more explicit comparison of the methods used here to past work, especially [1] and [3]. [1] Weitao Wan, Yuanyi Zhong, Tianpeng Li, and Jiansheng Chen. Rethinking feature distribution for loss functions in image classification. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9117\u20139126, 2018. [2] Hong-Ming Yang, Xu-Yao Zhang, Fangying Yin, and Chenglin Liu. Robust classification with convolutional prototype learning. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3474\u20133482, 2018. [3] Yingwei Pan, Ting Yao, Yehao Li, Yu Wang, Chong-Wah Ngo, and Tao Mei. Transferrable prototypical networks for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2239\u20132247, 2019.", "rating": "3: Weak Reject", "reply_text": "3 ) .For your advice of adding more explicit comparison between our work and [ 1,7 ] , we have added some comparisons in the paper , hopefully it will make the paper clearer . As suggested by one of the reviewers , we have provided further sensitivity analysis of our method on the confidence threshold . Our default confidence threshold value is set to be 0.8 . And we have experimented it with some other confidence values , 0.6 , 0.7 and 0.9 . The experiment results are on the revised paper . Please check the results in Figure 4 in the Appendix . The results show our method is also robust against confidence threshold value . As also suggested by one of the reviewers , we have provided further experiment on the more challenging Office-Home dataset . Our method performs the best in all transfer tasks than state-of-the-art UDA methods . The results are in our revised paper ( Table 3 in the Appendix ) , you are welcomed to check that out . Finally , after our clarifications , we hope you have a better understanding of our work and give a more fair grade to our work . Thanks . [ 1 ] Yingwei Pan , Ting Yao , Yehao Li , Yu Wang , Chong-Wah Ngo , and Tao Mei . Transferrable proto-typical networks for unsupervised domain adaptation . InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp . 2239\u20132247 , 2019 . [ 2 ] Jake Snell , Kevin Swersky , and Richard Zemel . Prototypical networks for few-shot learning . InAdvances in Neural Information Processing Systems , pp . 4077\u20134087 , 2017 . [ 3 ] Mingsheng Long , Yue Cao , Jianmin Wang , and Michael I Jordan . Learning transferable featureswith deep adaptation networks.arXiv preprint arXiv:1502.02791 , 2015 . [ 4 ] Baochen Sun and Kate Saenko . Deep coral : Correlation alignment for deep domain adaptation . InEuropean Conference on Computer Vision , pp . 443\u2013450.Springer , 2016 . [ 5 ] Werner Zellinger , Thomas Grubinger , Edwin Lughofer , Thomas Natschl \u0308ager , and SusanneSaminger-Platz . Central moment discrepancy ( cmd ) for domain-invariant representation learn-ing.arXiv preprint arXiv:1702.08811 , 2017 . [ 6 ] Geoffrey French , Michal Mackiewicz , and Mark H. Fisher . Self-ensembling for visual domainadaptation . InICLR , 2018 . [ 7 ] Weitao Wan , Yuanyi Zhong , Tianpeng Li , and Jiansheng Chen . Rethinking feature distributionfor loss functions in image classification.2018 IEEE/CVF Conference on Computer Vision andPattern Recognition , pp . 9117\u20139126 , 2018 ."}, {"review_id": "r1eX1yrKwB-2", "review_text": "This paper introduces Distribution Matching Prototypical Network (DMPN) for Unsupervised Domain Adaptation (UDA). The proposed method explicitly models the feature distribution as a Gaussian mixture model in both source and target domains. Then the method aligns the target distribution with the source distribution by minimizing losses, which are called Gaussian Component Mean Matching (GCMM) and Pseudo Distribution Matching (PDM). This paper should be rejected because (1) the novelty of the main idea is marginal, and (2) the performance gain over the baseline methods is also marginal. Pan et al. already proposed the idea of transferring the knowledge from the source to the target using the prototype of each class. It is required to explain why explicit modeling performs better than implicit modeling of prototypes by theory or practice. In table 2, the proposed method seems better than TPN, but in the appendix, by comparing then in each category, the proposed method wins six categories, whereas TPN also wins six categories. Therefore, it is hard to say the proposed DMPN is more effective than another method. Each prototype is modeled using a mean and a covariance matrix. Why the authors don't use the estimated covariance matrix to measure the distance in eq.5? Because the proposed method uses pseudo-labeling for the target domain, it seems that the weights to determine unreliable examples are crucial. The paper should show the sensitivity of ways to determine the weights. What happens if values of 0.1 and 0.9 are changed in (pi-0.1)/0.9 on page 6?", "rating": "1: Reject", "reply_text": "For the second reason of rejection , we do not agree as well . For the digits image transfer tasks , state-of-the-art results are already quite high , all above 92 % , thus a 1~3 % of accuracy increase should be considered as significant . Our method has improved on transfer M- > U by 2.6 % and on transfer S- > M by 3.8 % compared to the second best . Taking the results in context , it is not fair to consider these improvements as marginal . For VisDA-2017 dataset , our method improved from the second best by 1 % , having a accuracy results of 81.4 % . And it is 1.4 % lower than [ 7 ] , which won the first place in the VisDA-2017 competition . Thus , our improvement of 1 % in this task should not be considered as marginal either . For some further questions , you mentioned `` Why the authors do n't use the estimated covariance matrix to measure the distance in eq.5 ? '' Yes , we have tried that to come up with a correlation distance similar as Deep Coral [ 5 ] , however it does not perform well , so we do not report it in the paper . For your question `` The paper should show the sensitivity of ways to determine the weights . What happens if values of 0.1 and 0.9 are changed in ( pi-0.1 ) /0.9 on page 6 '' . The value 0.1 , and 0.9 are set based on probability , because we have 10 classes for the digits image transfer , so a random prediction would have probability 0.1 . If we directly use the probability based weighting , then a random prediction would also contribute to the training , we do not want that to happen , so we weight with ( pi-0.1 ) /0.9 , thus random predictions will have weight 0 and the perfect prediction has weight 1 . So if the task has n classes , our method will weight the samples by ( pi-1/n ) / ( 1-1/n ) . There is really no reason why we want to tune these two parameters , as we are weighting the data points by probability . As suggested by one of the reviewers , we have provided further sensitivity analysis of our method on the confidence threshold . Our default confidence threshold value is set to be 0.8 . And we have experiment it with some other confidence values , 0.6 , 0.7 and 0.9 . The experiment results are on the revised paper . Please check the results in Figure 4 in the Appendix . The results show our method is also robust against confidence threshold value . As also suggested by one of the reviewers , we have provided further experiment on the more challenging Office-Home dataset . Our method performs the best in all transfer tasks than state-of-the-art UDA methods . The results are in our revised paper ( Table 3 in the Appendix ) , you are welcomed to check that out . We argue that our work has been severely undervalued by the reviewers . If you think our argument is invalid in some aspects , please indicate . Thanks . [ 1 ] Yingwei Pan , Ting Yao , Yehao Li , Yu Wang , Chong-Wah Ngo , and Tao Mei . Transferrable proto-typical networks for unsupervised domain adaptation . InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp . 2239\u20132247 , 2019 . [ 2 ] Weitao Wan , Yuanyi Zhong , Tianpeng Li , and Jiansheng Chen . Rethinking feature distributionfor loss functions in image classification.2018 IEEE/CVF Conference on Computer Vision andPattern Recognition , pp . 9117\u20139126 , 2018 . [ 3 ] Jake Snell , Kevin Swersky , and Richard Zemel . Prototypical networks for few-shot learning . InAdvances in Neural Information Processing Systems , pp . 4077\u20134087 , 2017 . [ 4 ] Mingsheng Long , Yue Cao , Jianmin Wang , and Michael I Jordan . Learning transferable featureswith deep adaptation networks.arXiv preprint arXiv:1502.02791 , 2015 . [ 5 ] Baochen Sun and Kate Saenko . Deep coral : Correlation alignment for deep domain adaptation . InEuropean Conference on Computer Vision , pp . 443\u2013450.Springer , 2016 . [ 6 ] Werner Zellinger , Thomas Grubinger , Edwin Lughofer , Thomas Natschl \u0308ager , and SusanneSaminger-Platz . Central moment discrepancy ( cmd ) for domain-invariant representation learn-ing.arXiv preprint arXiv:1702.08811 , 2017 . [ 7 ] Geoffrey French , Michal Mackiewicz , and Mark H. Fisher . Self-ensembling for visual domainadaptation . InICLR , 2018 ."}], "0": {"review_id": "r1eX1yrKwB-0", "review_text": "<Paper summary> The authors proposed Distribution Matching Prototypical Network (DMPN) for unsupervised domain adaptation. DMPN extracts features from the input data and models them as Gaussian mixture distributions. By explicitly modeling the distributions that the features follow, the discrepancy between the distribution of source data and that of target data can be easily evaluated. DMPN is trained by jointly minimizing two kinds of loss, which are classification loss on the source data and domain discrepancy loss that is calculated via the explicit models. Experimental results on two popular benchmark datasets validate the advantage of DMPN over other state-of-the-art methods. <Review summary> The proposed method seems simple but empirically performs well. The paper is well written and easy to follow, so we can maybe easily implement it. However, I have several concerns mainly about the details and theories of the proposed method, which makes my score a bit lower than the border line. Given clarifications in an author response, I would be willing to increase the score. <Details> * Strength + The motivation of using ProtoNet for domain adaptation seems reasonable. + The proposed method performs well in the experiments. + The paper, especially the experiment section, is well written and easy to follow. * Weakness and concerns - Several points on the proposed loss (GCMM and PDM) are not sufficiently discussed. -- Why do we need two kinds of loss? These losses seem to play almost same role. Since PDM loss corresponds to target-side log likelihood regularization term (Eq. (3)), I wonder if we really need GCMM loss. -- Since the authors explicitly model the feature distributions by Gaussian mixtures (GMs), it might be possible to calculate a standard divergence between source and target data distributions by using the parameters of GMs. Compared with such a straightforward approach, the proposed method seems to be ad-hoc and is not theoretically validated. What term of divergence (or distance) does it minimize? -- When a certain class does not appear in pseudo-labeled target data, how can we calculate GCMM loss? (specifically, \\mu^{et}_c) -- Are Eq. (3) and Eq. (6) correct? These are defined as total loss, not average, over each domain. It means that the scale of the coefficients for these terms changes according to the number of training data, but the sensitivity analysis in Fig. 2 does not show such effect. -- Since the proposed losses heavily depend on the pseudo labels on the target data, it should be important to carefully set a proper threshold for the confidence. Is the proposed method sensitive against the change of this threshold? If so, how can we tune it? -- How can we know p(c) in advance? - The theory shown in 3.5 is not sufficiently validated. -- The authors state \"\"we minimize the first term through minimizing the domain discrepancy losses,\" but it is not sufficiently supported, because the relationship between the proposed losses and H-delta-H divergence is not clear. - I am concerned about whether the proposed method works well with harder datasets such as Office-Home dataset, because each class data are modeled by a simple Gaussian distribution in the proposed method. * Minor concerns that do not have an impact on the score - Using both f^s_i and F(x^s_i; \\theta) is confusing. - Typo in Eq. (7): PMD -> PDM ", "rating": "3: Weak Reject", "reply_text": "Thanks for reviewing our paper and your appreciation of our idea . Here we answer your concerns and clarify some of the weak points you mentioned : 1 ) . These two losses actually serve with different purposes when we design them . The GCMM loss brings the two distribution closer via minimizing the corresponding Gaussian Component means of the source and target data . And the PDM loss shapes the target feature distribution similar as the source feature distribution via minimizing the likelihood of generating the target feature from the source feature distribution . In this sense , they complement each other , to match the target feature distribution to be exactly like the source feature distribution . Furthermore , these two losses also reduce distribution discrepancy at different levels , GCMM reduces distribution discrepancy at the class-level and PDM reduces distribution discrepancy at the sample-level , thus in this sense , they also complete each other for domain adaptation . 2 ) .We want to clarify here . Our method does not learn the distribution parameters for the target data . We learn the distribution parameters of the source data . We use the empirically calculated distribution parameter estimator of the source and target data to minimize the distribution discrepancy loss function . Thus , we can not `` calculate a standard divergence between source and target data distributions by using the parameters of GMs . '' For the GCMM loss , our method minimizes the euclidean distance between the corresponding Gaussian Component means of the source and target data for each class . PDM loss minimizes the likelihood of generating the target feature with the source feature distribution . 3 ) .We will ignore data from that class in the batch in that training iteration . As training data are sampled randomly in each iteration , and in the end , all data updates the model . 4 ) .Yes , you are correct . We forgot to average the term when writing the paper . We have corrected it in the revised paper . Thanks for pointing this out . 5 ) .We have added a sensitivity experiment on the confidence threshold . The results are in the appendix of the revised paper . Here is the summary : confidence-threshold : 0.6 , 0.7 , 0.8 , 0.9 Mean-accuracy : 81.3 , 81.4 , 81.4 , 81.5 The results show that our method is also robust against confidence threshold . For our proposed probability based weighting mechanism , as there is no hyper-parameter in there , so there is no need to provide sensitivity analysis on it . 6 ) .We know p ( c ) in the source domain , as it has labels . We do not know p ( c ) in the target domain , but we can estimate it . In this paper , we assume p ( c ) is uniform , as we focus on co-variate shift in this paper . Our work can be easily augmented to work for label shift too , once we estimate the target label distribution . However , we leave this as future work . 7 ) .The H-delta-H divergence is small when the two distribution discrepancy is small . As GCMM loss brings the two distribution closer , and PDM loss shapes the two distribution to be alike , the source and target feature distribution discrepancy will be smaller . Thus H-delta-H becomes smaller as we minimize GCMM loss and PDM loss . We have updated the paper on this part to make it clearer . Thanks for indicating this . 8 ) .We have added an experiment on the Office-Home dataset in the appendix of our paper . Our paper performs the best in all the transfer tasks in Office-Home compared to state-of-the-art UDA methods , showing that it also works for this more challenging dataset . 9 ) .Thanks for pointing out some of our typos , we have made the changes in our revised paper ."}, "1": {"review_id": "r1eX1yrKwB-1", "review_text": "The paper develops a new method for adapting models trained on labeled data from some source domain to unlabeled data in a target domain. The authors accomplish this by adapting a technique from [1] and [2] enforcing that the deep features learned during training approximately follow a Gaussian mixture distribution. With the learned features in this form, the authors ensure domain adaptation by minimizing the discrepancy between the distributions arising from the source and target datasets. Strengths: + The paper's experiments show an improvement in the model's performance relative to past work, utilizing a large number of comparison models. + The use of explicit distributional information within the learned representations seems like a good fit for the task at hand, and the authors' experiments back this up. Weaknesses: - The proposed method for unsupervised domain adaptation is very similar to the prototypical networks approach in [3], with the primary difference being a loss term incentivizing a Gaussian mixture distribution over features. - While the authors achieve improved performance over [3], the gains in classification accuracy on the target dataset aren't especially huge (~1-3%). - The paper is a bit hard to follow, and would be improved by giving a more explicit comparison of the methods used here to past work, especially [1] and [3]. [1] Weitao Wan, Yuanyi Zhong, Tianpeng Li, and Jiansheng Chen. Rethinking feature distribution for loss functions in image classification. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9117\u20139126, 2018. [2] Hong-Ming Yang, Xu-Yao Zhang, Fangying Yin, and Chenglin Liu. Robust classification with convolutional prototype learning. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3474\u20133482, 2018. [3] Yingwei Pan, Ting Yao, Yehao Li, Yu Wang, Chong-Wah Ngo, and Tao Mei. Transferrable prototypical networks for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2239\u20132247, 2019.", "rating": "3: Weak Reject", "reply_text": "3 ) .For your advice of adding more explicit comparison between our work and [ 1,7 ] , we have added some comparisons in the paper , hopefully it will make the paper clearer . As suggested by one of the reviewers , we have provided further sensitivity analysis of our method on the confidence threshold . Our default confidence threshold value is set to be 0.8 . And we have experimented it with some other confidence values , 0.6 , 0.7 and 0.9 . The experiment results are on the revised paper . Please check the results in Figure 4 in the Appendix . The results show our method is also robust against confidence threshold value . As also suggested by one of the reviewers , we have provided further experiment on the more challenging Office-Home dataset . Our method performs the best in all transfer tasks than state-of-the-art UDA methods . The results are in our revised paper ( Table 3 in the Appendix ) , you are welcomed to check that out . Finally , after our clarifications , we hope you have a better understanding of our work and give a more fair grade to our work . Thanks . [ 1 ] Yingwei Pan , Ting Yao , Yehao Li , Yu Wang , Chong-Wah Ngo , and Tao Mei . Transferrable proto-typical networks for unsupervised domain adaptation . InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp . 2239\u20132247 , 2019 . [ 2 ] Jake Snell , Kevin Swersky , and Richard Zemel . Prototypical networks for few-shot learning . InAdvances in Neural Information Processing Systems , pp . 4077\u20134087 , 2017 . [ 3 ] Mingsheng Long , Yue Cao , Jianmin Wang , and Michael I Jordan . Learning transferable featureswith deep adaptation networks.arXiv preprint arXiv:1502.02791 , 2015 . [ 4 ] Baochen Sun and Kate Saenko . Deep coral : Correlation alignment for deep domain adaptation . InEuropean Conference on Computer Vision , pp . 443\u2013450.Springer , 2016 . [ 5 ] Werner Zellinger , Thomas Grubinger , Edwin Lughofer , Thomas Natschl \u0308ager , and SusanneSaminger-Platz . Central moment discrepancy ( cmd ) for domain-invariant representation learn-ing.arXiv preprint arXiv:1702.08811 , 2017 . [ 6 ] Geoffrey French , Michal Mackiewicz , and Mark H. Fisher . Self-ensembling for visual domainadaptation . InICLR , 2018 . [ 7 ] Weitao Wan , Yuanyi Zhong , Tianpeng Li , and Jiansheng Chen . Rethinking feature distributionfor loss functions in image classification.2018 IEEE/CVF Conference on Computer Vision andPattern Recognition , pp . 9117\u20139126 , 2018 ."}, "2": {"review_id": "r1eX1yrKwB-2", "review_text": "This paper introduces Distribution Matching Prototypical Network (DMPN) for Unsupervised Domain Adaptation (UDA). The proposed method explicitly models the feature distribution as a Gaussian mixture model in both source and target domains. Then the method aligns the target distribution with the source distribution by minimizing losses, which are called Gaussian Component Mean Matching (GCMM) and Pseudo Distribution Matching (PDM). This paper should be rejected because (1) the novelty of the main idea is marginal, and (2) the performance gain over the baseline methods is also marginal. Pan et al. already proposed the idea of transferring the knowledge from the source to the target using the prototype of each class. It is required to explain why explicit modeling performs better than implicit modeling of prototypes by theory or practice. In table 2, the proposed method seems better than TPN, but in the appendix, by comparing then in each category, the proposed method wins six categories, whereas TPN also wins six categories. Therefore, it is hard to say the proposed DMPN is more effective than another method. Each prototype is modeled using a mean and a covariance matrix. Why the authors don't use the estimated covariance matrix to measure the distance in eq.5? Because the proposed method uses pseudo-labeling for the target domain, it seems that the weights to determine unreliable examples are crucial. The paper should show the sensitivity of ways to determine the weights. What happens if values of 0.1 and 0.9 are changed in (pi-0.1)/0.9 on page 6?", "rating": "1: Reject", "reply_text": "For the second reason of rejection , we do not agree as well . For the digits image transfer tasks , state-of-the-art results are already quite high , all above 92 % , thus a 1~3 % of accuracy increase should be considered as significant . Our method has improved on transfer M- > U by 2.6 % and on transfer S- > M by 3.8 % compared to the second best . Taking the results in context , it is not fair to consider these improvements as marginal . For VisDA-2017 dataset , our method improved from the second best by 1 % , having a accuracy results of 81.4 % . And it is 1.4 % lower than [ 7 ] , which won the first place in the VisDA-2017 competition . Thus , our improvement of 1 % in this task should not be considered as marginal either . For some further questions , you mentioned `` Why the authors do n't use the estimated covariance matrix to measure the distance in eq.5 ? '' Yes , we have tried that to come up with a correlation distance similar as Deep Coral [ 5 ] , however it does not perform well , so we do not report it in the paper . For your question `` The paper should show the sensitivity of ways to determine the weights . What happens if values of 0.1 and 0.9 are changed in ( pi-0.1 ) /0.9 on page 6 '' . The value 0.1 , and 0.9 are set based on probability , because we have 10 classes for the digits image transfer , so a random prediction would have probability 0.1 . If we directly use the probability based weighting , then a random prediction would also contribute to the training , we do not want that to happen , so we weight with ( pi-0.1 ) /0.9 , thus random predictions will have weight 0 and the perfect prediction has weight 1 . So if the task has n classes , our method will weight the samples by ( pi-1/n ) / ( 1-1/n ) . There is really no reason why we want to tune these two parameters , as we are weighting the data points by probability . As suggested by one of the reviewers , we have provided further sensitivity analysis of our method on the confidence threshold . Our default confidence threshold value is set to be 0.8 . And we have experiment it with some other confidence values , 0.6 , 0.7 and 0.9 . The experiment results are on the revised paper . Please check the results in Figure 4 in the Appendix . The results show our method is also robust against confidence threshold value . As also suggested by one of the reviewers , we have provided further experiment on the more challenging Office-Home dataset . Our method performs the best in all transfer tasks than state-of-the-art UDA methods . The results are in our revised paper ( Table 3 in the Appendix ) , you are welcomed to check that out . We argue that our work has been severely undervalued by the reviewers . If you think our argument is invalid in some aspects , please indicate . Thanks . [ 1 ] Yingwei Pan , Ting Yao , Yehao Li , Yu Wang , Chong-Wah Ngo , and Tao Mei . Transferrable proto-typical networks for unsupervised domain adaptation . InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp . 2239\u20132247 , 2019 . [ 2 ] Weitao Wan , Yuanyi Zhong , Tianpeng Li , and Jiansheng Chen . Rethinking feature distributionfor loss functions in image classification.2018 IEEE/CVF Conference on Computer Vision andPattern Recognition , pp . 9117\u20139126 , 2018 . [ 3 ] Jake Snell , Kevin Swersky , and Richard Zemel . Prototypical networks for few-shot learning . InAdvances in Neural Information Processing Systems , pp . 4077\u20134087 , 2017 . [ 4 ] Mingsheng Long , Yue Cao , Jianmin Wang , and Michael I Jordan . Learning transferable featureswith deep adaptation networks.arXiv preprint arXiv:1502.02791 , 2015 . [ 5 ] Baochen Sun and Kate Saenko . Deep coral : Correlation alignment for deep domain adaptation . InEuropean Conference on Computer Vision , pp . 443\u2013450.Springer , 2016 . [ 6 ] Werner Zellinger , Thomas Grubinger , Edwin Lughofer , Thomas Natschl \u0308ager , and SusanneSaminger-Platz . Central moment discrepancy ( cmd ) for domain-invariant representation learn-ing.arXiv preprint arXiv:1702.08811 , 2017 . [ 7 ] Geoffrey French , Michal Mackiewicz , and Mark H. Fisher . Self-ensembling for visual domainadaptation . InICLR , 2018 ."}}