{"year": "2021", "forum": "HC5VgCHtU10", "title": "Disentangling style and content for low resource video domain adaptation: a case study on keystroke inference attacks", "decision": "Reject", "meta_review": "The paper focuses proposes a new framework for low-resource video domain adaptation leveraging synthetic data with supervised disentangled learning for tackling keystroke inference attacks.\n\nThe paper received contrasting reviews, 2 positive and 2 negative, and the overall confidence of the reviewers is not so high.\nOverall, it is recognized that the work has some merit, but also some problems, which the rebuttal has not fully fixed, and I mainly refer to R2, R3 and R4 remarks. \n\nThe first issue is the level of novelty, which is not much high as compared to the former work of (Moiitan et al. 2017). Besides, the questions raised by some reviewers, also discussed in the rebuttal, also denote a certain lack of clarity, despite the paper is considered well organised in general. \n\nThe other main issue regards the experimental evaluation. To start with, the application addressed is very specific and it is not clear how this approach can be extended to other problems too, since no evidence is provided in this sense. The reported comparative analysis wrt baselines are in fact quite \"simple\" (e.g., ADDA is a work dated back to 2017, so as CycleGAN). Moreover, although the considered dataset is the only one in this scenario, its significance is a bit limited since only 3 subjects were considered, and this likely raised the comment of one reviewer questioning if this paper was not better suited to an application-oriented, security conference. A discussion for the setting of the lambda parameters is also missing.\n\nOverall, given the above issues, I consider the paper not yet ready for publication in ICLR 2021.\n", "reviews": [{"review_id": "HC5VgCHtU10-0", "review_text": "This paper is about keystroke inference attacks and proposes a method to assess the threat of deep learning based approaches when only limited real-life data are available . To this end , it is introduced a video domain adaptation technique that is able to generate data into separate style and content representations . This data augmentation scheme is shown to be effective and prevents overfitting . This is an interesting applicative approach , but I believe that the introduced novelty is probably limited for ICLR . Two claimed contributions are the assessment of deep learning methods for keystroke inference attacks when limited data are available and the domain adaptation approach to generate synthetic data . Now these contributions have been also introduced recently in Lim et al . ( 2020 ) .One difference is that the threat scenario is based on single keypresses , while in this submission complete sequences are tackled . In addition , in Lim et al.it is used the adversarial discriminative domain adaptation ( ADDA ) technique introduced by Tzeng et al.2017 , while in this submission a supervised disentangled learning based approach is proposed . Given these similarities I would have expected more discussion in the related work about this paper and also some comparison between single keypresses results vs complete sequences . Furthermore , from Table 1 it appears that ADDA provides much worse results with respect to the proposed method . Then a discussion is needed since it has been used in Lim et al.achieving good performance in a similar context . For what concern the supervised disentangled learning based approach , I think that the authors should make very clear which is the introduced novelty with respect to current state-of-the-art methods . In particular , which are the new components of the proposed method . In this respect it would be helpful to make comparisons with some baselines . This would help to assess the relevance of the proposal . As a final minor comment I think it would be important to provide more information about the real life dataset used in the experiments , for example by indicating how many participants where involved . Post rebuttal comments First of all , I want to thank the authors for answering my questions . The clarifications confirm my previous concern about the limited technical novelty . In addition , they highlight that only three participants were involved to build the real life dataset used in the experiments . In my opinion this is not sufficient to carry out a significant evaluation . For these reasons I keep my original rating .", "rating": "5: Marginally below acceptance threshold", "reply_text": "- > \u201c it would be important to provide more information about the real life dataset used in the experiments \u201d We have added additional details regarding the real-life dataset in the datasets subsection . Additional details regarding the real-life dataset is located in A.2 . Again , thank you for taking the time to review our work and leaving your constructive feedback ."}, {"review_id": "HC5VgCHtU10-1", "review_text": "In this paper , the authors introduce a video domain adaptation technique that learns to disentangle style and content of a video in order to generate as form of data augmentation . The main idea is to train a style and content encoder and enforce 1 ) The output of the style encoder can not be used to determine keystrokes 2 ) The output distribution of the content encoder for real and synthetic data can not be distinguished . They motivate their problem by recognizing that deep learning-based keystroke inference attacks are trained with a small number of real data along with a larger number of synthetic data . This results in the need for data augmentation via domain adaptation . Pros : The proposed method of disentanglement and combination in the feature space for classification is novel and interesting . Figures are well done and informative . Multiple evaluation metrics were used , and it is clear that the framework is useful under the considered setting . Cons/Comments : The output of style encoder gives you a representation that maximizes the prediction loss in order to lose content information . The output of content encoder on the other hand , gives a domain invariant representations that presumably holds all the information needed for classification . The authors propose combining them with a decoder in various combination in order to do data augmentation . However , it is not clear to me how would style representation be useful in this data augmentation . In the case where style representation is totally ignored and decoder only looks at the content representation , it minimizes the prediction loss ( Eq 5 ) and also the semantic alignment loss ( Eq 8 ) . While Table 2 shows it clearly improves the results , I am unsure of the exact reasoning on how the losses help the style encoder in encoding anything useful for the final classification . In general , I find this paper well-written and well-motivated . The method is novel and produces good results over other baselines . I recommend this paper for acceptance .", "rating": "7: Good paper, accept", "reply_text": "We thank you for taking the time to read our paper and leave a review . We also thank you for the concise and informative summary of our work as it gives us confidence that we have conveyed our message well . We appreciate your thoughtful feedback to improve our paper . To address your concerns see below : - > \u201c However , it is not clear to me how would style representation be useful in this data augmentation . In the case where style representation is totally ignored and decoder only looks at the content representation , it minimizes the prediction loss ( Eq 5 ) and also the semantic alignment loss ( Eq 8 ) . While Table 2 shows it clearly improves the results , I am unsure of the exact reasoning on how the losses help the style encoder in encoding anything useful for the final classification. \u201d We may be misinterpreting your question , but hopefully this clears up the potential confusion . In Table 2 we perform various ablations to see how the different loss components affect our model . When we train Model II ( Base + Style ) , we are not making predictions only on the style information . We still use the content encoder , but we do not add any additional losses in the content latent space . We hope this addresses your concerns in a satisfactory manner . Again , we thank you for taking the time to review our work and finding our work interesting and novel ."}, {"review_id": "HC5VgCHtU10-2", "review_text": "The authors investigate video domain adaption for keystroke inference attacks on synthetic and real-life data . They propose a disentangle model to separate the representation of content and style . In their problem , style encapsulates the trajectory of the finger in between keys or speed of the user typing , while content represents the typed sentence . The model is naive by simply applying the adversarial domain adaptation into the keystroke inference attacks . Also , I suspect the paper is more suitable for the security conference , e.g. , S & P . In addition , there still exists the following concerns . + lack comparison to the state-of-the-art methods , such as : John Lim et al , 'Revisiting the Threat Space for Vision-based Keystroke Inference Attacks ' , in arXiv preprint arXiv:2009.05796 , 2020 . + lacks comparison to the state-of-the-art domain adaptation methods , such as : Ehsan Hosseini-Asl et al , 'AUGMENTED CYCLIC ADVERSARIAL LEARNING FOR LOW RESOURCE DOMAIN ADAPTATION ' , in ICLR 2019 . + In addition to CycleGAN , video to video translation methods should be included : Ting-Chun Wang et al , 'Video-to-Video Synthesis ' , in NeurIPS 2018 . + I notice that the generated results quality of CycleGAN ( in Figure 6 ) is poor . The background is black and only a thumb is generated . Please explain the reason . + The paper claims that the proposed method can separate the style ( ) and content . The visualization or demonstration is needed to support their claims .", "rating": "5: Marginally below acceptance threshold", "reply_text": "- > \u201c In addition to CycleGAN , video to video translation methods should be included : Ting-Chun Wang et al , 'Video-to-Video Synthesis ' , in NeurIPS 2018. \u201d This is a very good point and we are glad you brought this up ! We have spent significant time applying vid2vid in our scenario , but struggled to generate output videos with plausible thumb trajectories . Here is the setup we used to train vid2vid : 1 ) Generate synthetic versions of the real-life training data 2 ) We need to trim the videos such that the lengths of the videos are the same . 3 ) Train the vid2vid setup using the official github repo ( https : //github.com/NVIDIA/vid2vid ) using the pairs of synthetic and real videos 4 ) Run the vid2vid test script on the real-life test videos to transform real videos to synthetic 5 ) Either transform the real-life training videos to synthetic and finetune on these videos OR directly test on the transformed real-life test videos We believe we have issues in generating plausible videos from vid2vid because we do not have the level of supervision that this method assumes . Vid2Vid ( https : //arxiv.org/abs/1808.06601 ) and Few shot Vid2Vid ( https : //arxiv.org/abs/1910.12713 ) are supervised methods , requiring paired examples . The datasets used in vid2vid all have supervision on a per-frame basis . Cityscapes and Apolloscape are street scene videos where one has access to real-life video foottage along with the semantic labels . The face video dataset has pairs of real-life talking faces alongside facial landmark labels . The dance video dataset has pairs of real-life dancing videos alongside pose labels generated from OpenPose and DensePose . For a given pair of real and synthetic videos , we have global supervision ( i.e. , the same sentences ) , but we do not have \u201c local \u201d supervision ( i.e.the frames are not paired ) because the synthetic and real thumbs have different temporal dynamics . A training pair for Cityscapes has the same temporal dynamics because one domain is the real-life videos and the other domain is that frame 's semantic labels . The similarity between our keystroke dataset setup and the authors of vid2vid is that the source and target videos are semantically similar in a global setting ( The entire videos are of the same thing ) . The difference between our keystroke dataset setup and the authors of vid2vid is that the individual frames for the source and target videos are not the same whereas the individual frames for the source and target videos is the same for the authors of vid2vid . For example , any given frame of the cityscapes dataset is always paired with the semantic segmentation labels for the frame . In the dancing videos dataset , any given frame of a human dancing is always paired with the pose labels . In the context of keystroke videos , the thumbs would need to be in the same place . We believe that vid2vid only works when the temporal dynamics between the source and target domains are exactly the same , which is not true for the case of pairs of synthetic and real keystroke videos . - > \u201c I notice that the generated results quality of CycleGAN ( in Figure 6 ) is poor . The background is black and only a thumb is generated . Please explain the reason. \u201d We happened to choose an example that has a very dark background which may and will confuse the reader . In this example , the background is not black just very dark . The background is so dark because some of the videos in our synthetic data have very dark backgrounds . We use the simulator introduced by ( Lim et.al 2020 ) to generate synthetic videos and the darkness varies . We will find another sample in our generated outputs that do not have such a dark background for the camera ready version . Thanks for pointing this out ! - > \u201c The paper claims that the proposed method can separate the style ( ) and content . The visualization or demonstration is needed to support their claims. \u201d Please take a look at Figure 4 as such a visualization exists there . We thank you for your detailed review and feedback . We hope we addressed all of your concerns in a satisfactory manner ."}, {"review_id": "HC5VgCHtU10-3", "review_text": "In this paper , the authors focus on keystroke inference attacks in which an attacker leverages machine learning approaches , In particular , a new framework is proposed for low-resource video domain adaptation using supervised disentangled learning , and another method to assess the threat of keystroke inference attacks by an attacker using a deep learning system , given limited real-life data . The novelty of the approach and its theoretical foundation is appreciated . For a given domain , they decompose the data into real-life style , synthetic style , real-life content , and synthetic content , and then combine them into feature representations from all combinations of style-content pairings across domains to train a model , This allows classify the content of a sample in the style of another domain . Results indicate that training with these pairs to disentangle style and content prevents their model from overfitting to a small real-world training sets , and thereby provides an effective form of data augmentation that prevents overfitting . The paper is clearly written and well organized , and all the key concepts and motivations are described in enough detail to understand the paper . However , it is not clear from the outset the amount of limited real-world data should be collected from the target domain . It should also be clarified at the beginning why their data augmentation that prevents models overfitting , and why translated to better security against keystroke inference attacks . The experimental validation should include an analysis of the impact on the performance of the amount of real-world target domain data , the class imbalance , and capture conditions . The supplementary material provides additional information on the training setup that should be useful to the reader . It however seems like their code is not made available , so there is a concern that the results in this paper would be very difficult for a reader to reproduce .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their interest in our work . We appreciate the time you took to read the paper and review our work . Below we address any concerns and comments that you have provided . - > \u201c In this paper , the authors focus on keystroke inference attacks in which an attacker leverages machine learning approaches , In particular , a new framework is proposed for low-resource video domain adaptation using supervised disentangled learning , and another method to assess the threat of keystroke inference attacks by an attacker using a deep learning system , given limited real-life data . The novelty of the approach and its theoretical foundation is appreciated. \u201d We thank the reviewers for this characterization of our work . This is exactly the message we hoped to convey in our paper . - > \u201c it is not clear from the outset the amount of limited real-world data should be collected from the target domain. \u201d In Table 2 , we show performance when using the full real-life training dataset ( 175 videos ) as well as a truncated version of the real-life training dataset ( 100 ) . - > \u201c It should also be clarified at the beginning why their data augmentation that prevents models overfitting \u201d When dealing with scarce data in the target domain , training on pairs of source and target data samples have been proven to be effective in dealing with overfitting ( Motiian et al 2017 , ICCV https : //arxiv.org/abs/1709.10190 , Motiian et.al 2017 , Neurips https : //arxiv.org/abs/1711.02536 , Wang et . al 2019 , CVPR https : //arxiv.org/abs/1903.09372 ) . These pairing mechanisms , as well as ours , attempts to augment the target dataset size by the order of the source domain . We have added additional text in the introduction to highlight that this training recipe has been shown as an effective form of data augmentation , and to highlight the novelty in our pairing/grouping strategy . - > \u201c The experimental validation should include an analysis of the impact on the performance of the amount of real-world target domain data , the class imbalance , and capture conditions \u201d In Table 2 , we show performance when using the full real-life training dataset ( 175 videos ) as well as a truncated version of the real-life training dataset ( 100 ) . We have added further details in the datasets subsection in our experiments section to further detail the capture conditions . Unfortunately , we did not keep track of additional data to indicate different capture conditions so it is not possible to give an analysis as to how outdoor vs indoor capture scenarios , for example , effect performance . - > \u201c The supplementary material provides additional information on the training setup that should be useful to the reader . It however seems like their code is not made available , so there is a concern that the results in this paper would be very difficult for a reader to reproduce. \u201d We are glad that you found the training details sufficient for future reproducibility . We plan on releasing the code + dataset ( real and synthetic ) to the public after the review process . Again , we thank you for taking the time to review our work . We are grateful for the positive review and have incorporated your feedback into our paper ."}], "0": {"review_id": "HC5VgCHtU10-0", "review_text": "This paper is about keystroke inference attacks and proposes a method to assess the threat of deep learning based approaches when only limited real-life data are available . To this end , it is introduced a video domain adaptation technique that is able to generate data into separate style and content representations . This data augmentation scheme is shown to be effective and prevents overfitting . This is an interesting applicative approach , but I believe that the introduced novelty is probably limited for ICLR . Two claimed contributions are the assessment of deep learning methods for keystroke inference attacks when limited data are available and the domain adaptation approach to generate synthetic data . Now these contributions have been also introduced recently in Lim et al . ( 2020 ) .One difference is that the threat scenario is based on single keypresses , while in this submission complete sequences are tackled . In addition , in Lim et al.it is used the adversarial discriminative domain adaptation ( ADDA ) technique introduced by Tzeng et al.2017 , while in this submission a supervised disentangled learning based approach is proposed . Given these similarities I would have expected more discussion in the related work about this paper and also some comparison between single keypresses results vs complete sequences . Furthermore , from Table 1 it appears that ADDA provides much worse results with respect to the proposed method . Then a discussion is needed since it has been used in Lim et al.achieving good performance in a similar context . For what concern the supervised disentangled learning based approach , I think that the authors should make very clear which is the introduced novelty with respect to current state-of-the-art methods . In particular , which are the new components of the proposed method . In this respect it would be helpful to make comparisons with some baselines . This would help to assess the relevance of the proposal . As a final minor comment I think it would be important to provide more information about the real life dataset used in the experiments , for example by indicating how many participants where involved . Post rebuttal comments First of all , I want to thank the authors for answering my questions . The clarifications confirm my previous concern about the limited technical novelty . In addition , they highlight that only three participants were involved to build the real life dataset used in the experiments . In my opinion this is not sufficient to carry out a significant evaluation . For these reasons I keep my original rating .", "rating": "5: Marginally below acceptance threshold", "reply_text": "- > \u201c it would be important to provide more information about the real life dataset used in the experiments \u201d We have added additional details regarding the real-life dataset in the datasets subsection . Additional details regarding the real-life dataset is located in A.2 . Again , thank you for taking the time to review our work and leaving your constructive feedback ."}, "1": {"review_id": "HC5VgCHtU10-1", "review_text": "In this paper , the authors introduce a video domain adaptation technique that learns to disentangle style and content of a video in order to generate as form of data augmentation . The main idea is to train a style and content encoder and enforce 1 ) The output of the style encoder can not be used to determine keystrokes 2 ) The output distribution of the content encoder for real and synthetic data can not be distinguished . They motivate their problem by recognizing that deep learning-based keystroke inference attacks are trained with a small number of real data along with a larger number of synthetic data . This results in the need for data augmentation via domain adaptation . Pros : The proposed method of disentanglement and combination in the feature space for classification is novel and interesting . Figures are well done and informative . Multiple evaluation metrics were used , and it is clear that the framework is useful under the considered setting . Cons/Comments : The output of style encoder gives you a representation that maximizes the prediction loss in order to lose content information . The output of content encoder on the other hand , gives a domain invariant representations that presumably holds all the information needed for classification . The authors propose combining them with a decoder in various combination in order to do data augmentation . However , it is not clear to me how would style representation be useful in this data augmentation . In the case where style representation is totally ignored and decoder only looks at the content representation , it minimizes the prediction loss ( Eq 5 ) and also the semantic alignment loss ( Eq 8 ) . While Table 2 shows it clearly improves the results , I am unsure of the exact reasoning on how the losses help the style encoder in encoding anything useful for the final classification . In general , I find this paper well-written and well-motivated . The method is novel and produces good results over other baselines . I recommend this paper for acceptance .", "rating": "7: Good paper, accept", "reply_text": "We thank you for taking the time to read our paper and leave a review . We also thank you for the concise and informative summary of our work as it gives us confidence that we have conveyed our message well . We appreciate your thoughtful feedback to improve our paper . To address your concerns see below : - > \u201c However , it is not clear to me how would style representation be useful in this data augmentation . In the case where style representation is totally ignored and decoder only looks at the content representation , it minimizes the prediction loss ( Eq 5 ) and also the semantic alignment loss ( Eq 8 ) . While Table 2 shows it clearly improves the results , I am unsure of the exact reasoning on how the losses help the style encoder in encoding anything useful for the final classification. \u201d We may be misinterpreting your question , but hopefully this clears up the potential confusion . In Table 2 we perform various ablations to see how the different loss components affect our model . When we train Model II ( Base + Style ) , we are not making predictions only on the style information . We still use the content encoder , but we do not add any additional losses in the content latent space . We hope this addresses your concerns in a satisfactory manner . Again , we thank you for taking the time to review our work and finding our work interesting and novel ."}, "2": {"review_id": "HC5VgCHtU10-2", "review_text": "The authors investigate video domain adaption for keystroke inference attacks on synthetic and real-life data . They propose a disentangle model to separate the representation of content and style . In their problem , style encapsulates the trajectory of the finger in between keys or speed of the user typing , while content represents the typed sentence . The model is naive by simply applying the adversarial domain adaptation into the keystroke inference attacks . Also , I suspect the paper is more suitable for the security conference , e.g. , S & P . In addition , there still exists the following concerns . + lack comparison to the state-of-the-art methods , such as : John Lim et al , 'Revisiting the Threat Space for Vision-based Keystroke Inference Attacks ' , in arXiv preprint arXiv:2009.05796 , 2020 . + lacks comparison to the state-of-the-art domain adaptation methods , such as : Ehsan Hosseini-Asl et al , 'AUGMENTED CYCLIC ADVERSARIAL LEARNING FOR LOW RESOURCE DOMAIN ADAPTATION ' , in ICLR 2019 . + In addition to CycleGAN , video to video translation methods should be included : Ting-Chun Wang et al , 'Video-to-Video Synthesis ' , in NeurIPS 2018 . + I notice that the generated results quality of CycleGAN ( in Figure 6 ) is poor . The background is black and only a thumb is generated . Please explain the reason . + The paper claims that the proposed method can separate the style ( ) and content . The visualization or demonstration is needed to support their claims .", "rating": "5: Marginally below acceptance threshold", "reply_text": "- > \u201c In addition to CycleGAN , video to video translation methods should be included : Ting-Chun Wang et al , 'Video-to-Video Synthesis ' , in NeurIPS 2018. \u201d This is a very good point and we are glad you brought this up ! We have spent significant time applying vid2vid in our scenario , but struggled to generate output videos with plausible thumb trajectories . Here is the setup we used to train vid2vid : 1 ) Generate synthetic versions of the real-life training data 2 ) We need to trim the videos such that the lengths of the videos are the same . 3 ) Train the vid2vid setup using the official github repo ( https : //github.com/NVIDIA/vid2vid ) using the pairs of synthetic and real videos 4 ) Run the vid2vid test script on the real-life test videos to transform real videos to synthetic 5 ) Either transform the real-life training videos to synthetic and finetune on these videos OR directly test on the transformed real-life test videos We believe we have issues in generating plausible videos from vid2vid because we do not have the level of supervision that this method assumes . Vid2Vid ( https : //arxiv.org/abs/1808.06601 ) and Few shot Vid2Vid ( https : //arxiv.org/abs/1910.12713 ) are supervised methods , requiring paired examples . The datasets used in vid2vid all have supervision on a per-frame basis . Cityscapes and Apolloscape are street scene videos where one has access to real-life video foottage along with the semantic labels . The face video dataset has pairs of real-life talking faces alongside facial landmark labels . The dance video dataset has pairs of real-life dancing videos alongside pose labels generated from OpenPose and DensePose . For a given pair of real and synthetic videos , we have global supervision ( i.e. , the same sentences ) , but we do not have \u201c local \u201d supervision ( i.e.the frames are not paired ) because the synthetic and real thumbs have different temporal dynamics . A training pair for Cityscapes has the same temporal dynamics because one domain is the real-life videos and the other domain is that frame 's semantic labels . The similarity between our keystroke dataset setup and the authors of vid2vid is that the source and target videos are semantically similar in a global setting ( The entire videos are of the same thing ) . The difference between our keystroke dataset setup and the authors of vid2vid is that the individual frames for the source and target videos are not the same whereas the individual frames for the source and target videos is the same for the authors of vid2vid . For example , any given frame of the cityscapes dataset is always paired with the semantic segmentation labels for the frame . In the dancing videos dataset , any given frame of a human dancing is always paired with the pose labels . In the context of keystroke videos , the thumbs would need to be in the same place . We believe that vid2vid only works when the temporal dynamics between the source and target domains are exactly the same , which is not true for the case of pairs of synthetic and real keystroke videos . - > \u201c I notice that the generated results quality of CycleGAN ( in Figure 6 ) is poor . The background is black and only a thumb is generated . Please explain the reason. \u201d We happened to choose an example that has a very dark background which may and will confuse the reader . In this example , the background is not black just very dark . The background is so dark because some of the videos in our synthetic data have very dark backgrounds . We use the simulator introduced by ( Lim et.al 2020 ) to generate synthetic videos and the darkness varies . We will find another sample in our generated outputs that do not have such a dark background for the camera ready version . Thanks for pointing this out ! - > \u201c The paper claims that the proposed method can separate the style ( ) and content . The visualization or demonstration is needed to support their claims. \u201d Please take a look at Figure 4 as such a visualization exists there . We thank you for your detailed review and feedback . We hope we addressed all of your concerns in a satisfactory manner ."}, "3": {"review_id": "HC5VgCHtU10-3", "review_text": "In this paper , the authors focus on keystroke inference attacks in which an attacker leverages machine learning approaches , In particular , a new framework is proposed for low-resource video domain adaptation using supervised disentangled learning , and another method to assess the threat of keystroke inference attacks by an attacker using a deep learning system , given limited real-life data . The novelty of the approach and its theoretical foundation is appreciated . For a given domain , they decompose the data into real-life style , synthetic style , real-life content , and synthetic content , and then combine them into feature representations from all combinations of style-content pairings across domains to train a model , This allows classify the content of a sample in the style of another domain . Results indicate that training with these pairs to disentangle style and content prevents their model from overfitting to a small real-world training sets , and thereby provides an effective form of data augmentation that prevents overfitting . The paper is clearly written and well organized , and all the key concepts and motivations are described in enough detail to understand the paper . However , it is not clear from the outset the amount of limited real-world data should be collected from the target domain . It should also be clarified at the beginning why their data augmentation that prevents models overfitting , and why translated to better security against keystroke inference attacks . The experimental validation should include an analysis of the impact on the performance of the amount of real-world target domain data , the class imbalance , and capture conditions . The supplementary material provides additional information on the training setup that should be useful to the reader . It however seems like their code is not made available , so there is a concern that the results in this paper would be very difficult for a reader to reproduce .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their interest in our work . We appreciate the time you took to read the paper and review our work . Below we address any concerns and comments that you have provided . - > \u201c In this paper , the authors focus on keystroke inference attacks in which an attacker leverages machine learning approaches , In particular , a new framework is proposed for low-resource video domain adaptation using supervised disentangled learning , and another method to assess the threat of keystroke inference attacks by an attacker using a deep learning system , given limited real-life data . The novelty of the approach and its theoretical foundation is appreciated. \u201d We thank the reviewers for this characterization of our work . This is exactly the message we hoped to convey in our paper . - > \u201c it is not clear from the outset the amount of limited real-world data should be collected from the target domain. \u201d In Table 2 , we show performance when using the full real-life training dataset ( 175 videos ) as well as a truncated version of the real-life training dataset ( 100 ) . - > \u201c It should also be clarified at the beginning why their data augmentation that prevents models overfitting \u201d When dealing with scarce data in the target domain , training on pairs of source and target data samples have been proven to be effective in dealing with overfitting ( Motiian et al 2017 , ICCV https : //arxiv.org/abs/1709.10190 , Motiian et.al 2017 , Neurips https : //arxiv.org/abs/1711.02536 , Wang et . al 2019 , CVPR https : //arxiv.org/abs/1903.09372 ) . These pairing mechanisms , as well as ours , attempts to augment the target dataset size by the order of the source domain . We have added additional text in the introduction to highlight that this training recipe has been shown as an effective form of data augmentation , and to highlight the novelty in our pairing/grouping strategy . - > \u201c The experimental validation should include an analysis of the impact on the performance of the amount of real-world target domain data , the class imbalance , and capture conditions \u201d In Table 2 , we show performance when using the full real-life training dataset ( 175 videos ) as well as a truncated version of the real-life training dataset ( 100 ) . We have added further details in the datasets subsection in our experiments section to further detail the capture conditions . Unfortunately , we did not keep track of additional data to indicate different capture conditions so it is not possible to give an analysis as to how outdoor vs indoor capture scenarios , for example , effect performance . - > \u201c The supplementary material provides additional information on the training setup that should be useful to the reader . It however seems like their code is not made available , so there is a concern that the results in this paper would be very difficult for a reader to reproduce. \u201d We are glad that you found the training details sufficient for future reproducibility . We plan on releasing the code + dataset ( real and synthetic ) to the public after the review process . Again , we thank you for taking the time to review our work . We are grateful for the positive review and have incorporated your feedback into our paper ."}}