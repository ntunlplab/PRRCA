{"year": "2020", "forum": "BJlXgkHYvS", "title": "Information-Theoretic Local Minima Characterization and Regularization", "decision": "Reject", "meta_review": "This paper proposes using the Fisher information matrix to characterize local minima of deep network loss landscapes to indicate generalizability of a local minimum. While the reviewers agree that this paper contains interesting ideas and its presentation has been substantially improved during the discussion period, there are still issues that remain unanswered, in particular between the main objective/claims and the presented evidence. The paper will benefit from a revision and resubmission to another venue.", "reviews": [{"review_id": "BJlXgkHYvS-0", "review_text": "Post-rebuttal update: I have just noticed the authors modified their summary post below and claimed \"[my concerns] are all minor or resolved\". This is not true. Here is my summary of unresolved concerns written after the discussion period. This work has been substantially improved during the rebuttal process, and some of my concerns are addressed. But there are still major issues, as raised in my [last comment]( https://openreview.net/forum?id=BJlXgkHYvS&noteId=r1xAnokijS ), that remains unanswered. Specifically, (A) the relation between this work and information theory In the revision, the authors have make it very clear that the relation between FIA and their proposed regularized objective is very vague, relying on the crude approximation of expected Fisher information with observed Fisher information. Therefore the \"information-theoretic\" part in the title seems awkward and to some extent, misleading. As Reviewer 1 has pointed out, it would have been better if the authors relate their theory and method to the observed FIM, instead of information theory, from the beginning. Since the observed FIM and the neural tangent kernel (NTK) share the same eigenspectrum, it would also be interesting to relate this work to the NTK. (B) the different behavior of the proposed regularization (log det(I)) and its bound that is actually implemented (log tr(I)) This is the more important issue. My concern is that the observed FIM (or the NTK) is known to have fast decaying spectrum; (Karakida et al) has shown empirically that the decay can be exponential. Thus log det(I) would be dominated by the long tail (since after taking logarithm it is the sum (or average) of an arithmetic sequence), while log tr(I) would be dominated by the first largest few values. The authors claim that this is not an issue since they replaced the observed FIM with a subsampled, low-rank (<=10), version. It corresponds to consider a small submatrix of (the gram matrix of) the NTK. Denote this matrix as . (a) This does not help with the problem, since we now have no chance of recovering the smaller eigenvalues that would have dominated log(det(I)), and it is impossible that the proposed regularizer has a similar behavior to log(det(I)). (b) One could verify easily, using small feed-forward networks (or even simpler, computing a gram matrix using RBF kernels, since the FIM shares its eigenspectrum with NTK which is a p.d. kernel), that the new matrix still has a fast-decaying eigenspectrum, so the behavior of and \\tilde{I} are still significantly different, even though this cannot be established by concentration bounds as the authors argue. While FFN and modern deep architectures can have different behaviors, I believe the above evidence suggests that a numerical experiment comparing the behavior of the two bounds is a must. Following this argument we can see *another issue* of this work, namely the proposed generalization bound will be vacuous given the fast-decaying spectrum of the FIM, since it contains gamma=log(det(I)). Reviewer 1 mentioned this work could enlighten future discussions on this subject. While I agree this paper presents interesting empirical observations (namely its final algorithm, which is vaguely connected to the proposed objective, leads to improved performance on CV tasks), I think this submission in its current form is a bit too misleading to serve this purpose well, and overall I believe it would be better to go through another round of revision. Original Review ============================================ This paper presents a generalization bound based on Fisher information at a local optima, and proposes to optimize (an approximation to) it to get better generalization guarantees. There are issues in both parts, and I don't think it should be accepted. Specifically, 1. The definition of Fisher information is incorrect (for almost every parameter). The expectation should be taken w.r.t the model distribution p(c_x|x;w), instead of the data distribution S. 2. Assumption (1) (loss locally quadratic) is not reasonable for DNNs, since local optimas will not be unique in their neighborhoods. See e.g. Section 12.2.2, \"Information Geometry and Its Applications\". 3. Regarding the approximation to the bound, approximating log det(I) with log trace(I) is not a good idea: adding a very small eigenvalue will lead to noticeable change in the former, but negligible change in the latter. This is particularly problematic for DNNs, since the spectrum of their Fisher information matrix varies in a wide range: see \"Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach\". (Edit 11.8: * regarding point (1), there is a quantity called observed Fisher information in e.g. Grunwald (2007) that coincide with Eq (1) in the paper, but it is a function of the dataset instead of the model parameter, and can only used to study model parameters near the *global optima* (as it is applied in Grunwald (2007)); it cannot help with choosing between different local optimas as this work claims. Additionally, the FIA criterion, which is used in this paper to devleop the generalization bound, is defined using the standard form of Fisher information (i.e. taking expectation w.r.t model distribution), see Rissanen (1996). These facts lead me to believe this is a confusion on the authors' part. * in point (2) I was referring to the authors' argument \" Since L(S,w) is analytic and w_0 is the *only local minimum* of L(S,w) in M(w_0)\", which is incorrect.)", "rating": "1: Reject", "reply_text": "Dear reviewer2 , Thanks for your time and effort . There seems to be quite a lot of misunderstandings and confusions from the reviewer . Q1 : \u201c The definition of Fisher information is incorrect \u2026 The expectation should be taken w.r.t the model distribution \u201d . The statement in this question is factually wrong . First of all , what we use in the paper is the observed Fisher information , not the ( expected ) Fisher information . Secondly , by definition , the Fisher information , no matter the observed or the expected one , has nothing to do with expectation w.r.t.the model distribution . Q2 : \u201c There is a quantity called observed Fisher information that coincide with Eq ( 1 ) in the paper. \u201d This is by no means a coincidence . We clearly and precisely describe the term as observed Fisher information in the very first place when we introduce Eq ( 1 ) . Q3 : \u201c The observed Fisher information is a function of the dataset instead of the model parameter , as in Gr\u00fcnwald ( 2007 ) \u201d This is factually wrong . Professor Peter Gr\u00fcnwald has never said so . The Fisher information , no matter the observed one or the expected version , is a quantity involving both the model parameters and the input data . In Gr\u00fcnwald ( 2007 ) , simply omitting model parameter \u03b8 in the notation I ( X ) does not mean I ( X ) is not a function of \u03b8. Q4 : \u201c The observed Fisher information can only used to study model parameters near the global optima ; it can not help with choosing between different local optima as the work claims \u201d It is clearly stated in Section 4 that the different local minima we focus on to compare are also global minima . The comparison between these local minima is well-motivated , as pointed out at the beginning of Section 1 , that learning algorithms such as SGD tend to end up in one of the many local ( global ) minima that are not distinguishable from their similar close-to-zero training loss [ 2 , 3 , 4 , 5 ] . Q5 : \u201c The FIA criterion , which is used in this paper to develop the generalization bound , is defined using the expected Fisher information rather than the observed one. \u201d We give the FIA criterion precisely in Section 5.1 , only to illustrate the connection between our approach and Rissanen 's formulation of the MDL principle . In fact , we do not use the FIA criterion to derive or describe the generalization bound . Q6 : \u201c local optima will not be unique in their neighborhoods , as in [ 1 ] '' In our paper , we assume that local minima we care about are well isolated , mentioned at the end of Section 5.1 . For state-of-the-art network architectures used in practice , this isolation assumption is often the fact . The reviewer pointed out that , introduced in [ 1 ] , two kinds of singularity in neural networks prevent the local minima from being unique , namely the eliminating singularity and the overlapping singularity . As well demonstrated in [ 6 ] , network with skip connections ( such as ResNet , WRN , and DenseNet used in our experiments ) can effectively eliminate both . We would like to add a discussion paragraph about the isolation assumption in our paper . Q7 : \u201c Regarding the approximation to the bound , approximating log det ( I ) with log trace ( I ) is not a good idea. \u201d We do not use log trace ( I ) as an approximation to measure local minima as indeed it can be inaccurate . Instead , we use it as an upper bound of what we intend to optimize during training . Optimizing such upper bound , in return , enables us to develop a tractable regularization technique in search of the good local minima . Our experiments in Section 7.2 well demonstrate the effectiveness of our proposed regularizer in finding better local minima of greater generalizability . [ 1 ] Amari , Shun-ichi . Information geometry and its applications . Vol.194.Berlin : Springer , 2016 . [ 2 ] Dauphin , Yann N. , et al . `` Identifying and attacking the saddle point problem in high-dimensional non-convex optimization . '' Advances in neural information processing systems . 2014 . [ 3 ] Kawaguchi , Kenji . `` Deep learning without poor local minima . '' Advances in neural information processing systems . 2016 . [ 4 ] Nguyen , Quynh , and Matthias Hein . `` Optimization Landscape and Expressivity of Deep CNNs . '' International Conference on Machine Learning . 2018 . [ 5 ] Du , Simon S. , et al . `` Gradient Descent Finds Global Minima of Deep Neural Networks . '' International Conference on Machine Learning , 2019 . [ 6 ] Orhan , A. Emin , and Xaq Pitkow . `` Skip connections eliminate singularities . '' International Conference on Learning Representations , 2018 ."}, {"review_id": "BJlXgkHYvS-1", "review_text": "This paper contributes to the deep learning generalization theory, mainly from the theoretical perspective with experimental verifications. The key proposition is given by the unnumbered simple equation in the middle of page 4 (please number it), where \\mathcal{I} is the Fisher information matrix. According to the authors, this simple metric, which is the log-determinant of the Fisher information matrix, can characterize the generalization of a DNN. Remarkably, this piece of work is well written in terms of English and formulations, and complete, with a rigorous theoretical analysis (section 5.1, 5.2), practical approximations (section 5.3) and empirical verifications (section 6). On the theoretical side, this work builds upon Rissanen's formulation of the MDL principle, which has two parts (describing data given the model as well as the model complexity). Under rough approximations, the complexity term becomes the log-determinant of the Fisher information matrix evaluated at the local (global) optimum. This simple approximation is further proved to upper-bounds the generalization error as stated in theorem 1. To make the criterion to be practically useful, the author used the Jensen inequality so that the metric simply depends on the trace of the Fisher information matrix. The empirical study showed the usefulness of the proposed metric which can well approximate the testing error and a regularization term (based on the trace of the Fisher information matrix) that can improve generalization on real DNN experiments. The reviewer has the following minor comments to further improve this contribution: section 5.1, explain the abbreviation FIA Regarding the choice of the neighborhood \\mathcal{M}(w_0), what is the reason to define the model (neighbourhood of w_0) based on the loss? Why not simply take a coordinate neighborhood? According to your metric, the smaller the scale of the Fisher information matrix, the better the generalization. In section 5.1, there has to be some remarks on the intuition and related works on the flatness of the local minimum that is related to generalization. As this contribution is related to the spectral properties of the Fisher information matrix, the reviewer points the authors to \"Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach. Karakida et al. 2018.\" and \"Lightlike Neuromanifolds, Occam's Razor and Deep Learning. Sun and Nielsen. 2019\", which deals with asymptotic cases and have similar MDL formulations expressed in terms of the spectrum of the Fisher information matrix. ", "rating": "8: Accept", "reply_text": "Dear reviewer1 , Thanks for your appreciation . Q1 : \u201c Section 5.1 , explain the abbreviation FIA \u201d FIA in Section 5.1 stands for Fisher information approximation , originally coined for Normalized Maximum Likelihood Estimation in [ 1 ] . Q2 : \u201c Regarding the choice of the neighborhood \\mathcal { M } ( w_0 ) , what is the reason to define the model ( neighbourhood of w_0 ) based on the loss ? Why not simply take a coordinate neighborhood ? \u201d Given the local minimum at w_0 , we find it natural to define its neighborhood by a sublevel set w.r.t.the training loss . The issue of using the local coordinate ( e.g. , using an \u0190-ball to define the neighborhood ) is that the amount of change of the underlying model measured by training loss varies for different dimensions of the parameter space . For instance , moving in one direction might change the model a lot while moving in the other might change little . Q3 : \u201c In section 5.1 , there have to be some remarks on the intuition and related works on the flatness of the local minimum that is related to generalization. \u201d We will update the paper to add a discussion in Section 5.1. regarding the intuition and related works on \u201c flatness/sharpness \u201d , some of which are briefly discussed in Section 2 . Q4 : \u201c the reviewer points the authors to [ 2 ] and [ 3 ] , which have similar MDL formulations expressed in terms of the spectrum of the Fisher information matrix \u201d We will definitely consider mentioning the relation with these two papers in our next version . [ 1 ] Rissanen , Jorma J . `` Fisher information and stochastic complexity . '' IEEE transactions on information theory 42.1 ( 1996 ) : 40-47 . [ 2 ] Karakida , Ryo , Shotaro Akaho , and Shun-ichi Amari . `` Universal Statistics of Fisher Information in Deep Neural Networks : Mean Field Approach . '' The 22nd International Conference on Artificial Intelligence and Statistics . 2019 . [ 3 ] Sun , Ke , and Frank Nielsen . `` Lightlike Neuromanifolds , Occam 's Razor and Deep Learning . '' arXiv preprint arXiv:1905.11027 ( 2019 ) ."}, {"review_id": "BJlXgkHYvS-2", "review_text": "This paper provides a metric to characterize local minima of deep network loss landscapes based on the Fisher information matrix of the model parameterized by the deep network. The authors connect the Fisher information to the curvature of the loss landscape (the loss considered is the negative loss likelihood) and obtain generalization bounds through PAC Bayes analysis. They further propose regularizing the training of deep networks using the local curvature of the loss as a regularizer. In the final experimental section of the paper, the relationship between the empirical measures and generalization is shown on a variety of networks. This is an interesting paper, but I have a few concerns. 1. The information-theoretic measure that is proposed is essentially the (log) determinant of the hessian of the loss function. If there are degenerate eigendirections (zero eigenvalues) then the proposed measure would not be able to distinguish between minima with different numbers of degenerate directions / same number of degenerate directions but different spectral norms of the hessians. If the authors contention is that there will be no zero eigenvalues, that suggests that local minima of deep networks are all strict, isolated minima, contrary to recent work on connected solutions (See Draxler et. al. 2018, Essentially No Barriers in Neural Network Energy Landscapes, ICML 2018). 2. I would like to see how the authors believe their measure deals with rescalings layer parameters in deep networks, ie the issue brought up by Dinh et. al. in \"Sharp Minima can Generalize for Deep Networks\" ICML 2017. While I can see that the log determinant is invariant, it is not clear that the proposed approximation will be invariant to rescaling of deep network layer parameters. If the parameters corresponding to the eigenvalues sampled in the approximation are rescaled, I believe the proposed measure will not be invariant. 3. The experiments regarding the local minima characterization are well constructed, though some details are missing such as how the authors decided that training had converged to a local minimum. As far as regularization based on the local curvature is concerned, I would like to see some more experiments that compare the proposed technique to adagrad/adam and other techniques that purport to condition the gradient based on local curvature. It would also be interesting to see whether the regularization indeed converges to flatter minima characterized by the proposed flatness measure. Since the claim is that the regularizer gets you flatter solutions, that information is important to decide whether the proposed technique is performing as advertised. I am willing to update my score based on responses to these concerns.", "rating": "3: Weak Reject", "reply_text": "Dear reviewer3 , Thank you for the constructive review . Q1 : \u201c There are degenerate eigendirections \u201d or otherwise \u201c local minima are all isolated \u201d , which is \u201c contrary to recent work [ 2 ] ... \u201d Throughout our paper , we make the assumption that the local minima we care about are isolated , mentioned at the end of Section 5.1 . We would like to update our paper to add a discussion on this subject , including the answer to this and the next question . This assumption is not necessarily contradictory to the argument that local minima are connected , as suggested by [ 2 ] that a relatively flat path exists between any pair of local minima of low training loss . We point out that the claim in [ 2 ] is not conclusive , that all the local minima are \u201c perhaps best seen as \u201d one connected component . In other words , the local minima can still be isolated . For state-of-the-art network architectures used in practice , the isolation assumption is often the fact . To be precise , this assumption is violated when the Hessian matrix at a local minimum is singular . Specifically , [ 3 ] summarizes three sources of the singularity : ( i ) due to a dead neuron , ( ii ) due to identical neurons , and ( iii ) linear dependence of the neurons . As well demonstrated in [ 3 ] , network with skip connection ( such as ResNet , WRN , and DenseNet used in our experiments ) can effectively eliminate all the aforementioned singularity . There is another kind of singularity specifically for ReLU networks , which we will discuss next . Q2 : In practice how can the proposed metric \u201c deal with rescaling layer parameters in deep networks \u201d , i.e. , the rescaling issue described in [ 1 ] ? In practice , the rescaling issue is not critical . There are three reasons : ( I ) This issue can only happen in neural networks equipped with scale-invariant activation functions , such as ReLU . Many state-of-the-art models use other activation functions such as ELU [ 7 ] that is not scale-invariant . ( II ) Even for ReLU networks , most modern DNNs are free of this issue , since they have normalization layers such as BatchNorm [ 8 ] applied before the activation . BatchNorm shifts all the inputs to the ReLU function , which is equivalent to shifting the ReLU function horizontally . The shifted ReLU is no longer scale-invariant . The ResNet , WRN , and DenseNet used in our experiments all fall into this category . ( III ) Due to the ubiquitous use of normal distribution based weights initialization scheme and the L2 regularization / weight decay , most of the local minima obtained by gradient-based learning algorithms have weights of a relatively small norm . Consequently , in practice , we will not compare two local minima essentially the same but have one as the rescaled version of the other with a much larger norm of the weights . In summary , the rescaling issue is another source of the singularity but only for networks equipped with scale-invariant activation functions . And in practice , it is effectively eliminated . Q3 : \u201c How the authors decided that training had converged to a local minimum \u201d in Section 7.1 ? For the experiments of local minima characterization in Section 7.1 , in all scenarios , we train the model for 200 epochs with an initial learning rate 0.1 , divided by 10 when the training loss plateaus . Within each scenario , we find the final training loss very small and very similar across different models and the training accuracy essentially equal to 1 , indicating the convergence . Q4 : How is the proposed regularization method compared to \u201c AdaGrad/Adam and other techniques that purport to condition the gradient based on local curvature \u201d ? Our regularizer aims to find better \u201c flatter \u201d minima to improve generalization whereas adaptive optimization methods such as AdaGrad and Adam try to boost up convergence , yet at the cost of generalizability . Recent works such as [ 4 ] and [ 5 ] show that adaptive methods generalize worse than SGD+Momentum . In specific , very similar to our setup , [ 5 ] demonstrates that SGD+Momentum consistently outperforms the others on ResNet and DenseNet for CIFAR-10 and CIFAR-100 . Other approaches that also utilize local curvature , such as the Entropy-SGD [ 6 ] mentioned in Section 2 , have empirical results rather preliminary compared to ours . Furthermore , as described in Algorithm 1 , our proposed regularizer is not specific to a certain optimizer . We perform experiments with SGD+Momentum because it is chosen to be used in ResNet , WRN , and DenseNet , helping all of them achieve current or previous state-of-the-art results ."}], "0": {"review_id": "BJlXgkHYvS-0", "review_text": "Post-rebuttal update: I have just noticed the authors modified their summary post below and claimed \"[my concerns] are all minor or resolved\". This is not true. Here is my summary of unresolved concerns written after the discussion period. This work has been substantially improved during the rebuttal process, and some of my concerns are addressed. But there are still major issues, as raised in my [last comment]( https://openreview.net/forum?id=BJlXgkHYvS&noteId=r1xAnokijS ), that remains unanswered. Specifically, (A) the relation between this work and information theory In the revision, the authors have make it very clear that the relation between FIA and their proposed regularized objective is very vague, relying on the crude approximation of expected Fisher information with observed Fisher information. Therefore the \"information-theoretic\" part in the title seems awkward and to some extent, misleading. As Reviewer 1 has pointed out, it would have been better if the authors relate their theory and method to the observed FIM, instead of information theory, from the beginning. Since the observed FIM and the neural tangent kernel (NTK) share the same eigenspectrum, it would also be interesting to relate this work to the NTK. (B) the different behavior of the proposed regularization (log det(I)) and its bound that is actually implemented (log tr(I)) This is the more important issue. My concern is that the observed FIM (or the NTK) is known to have fast decaying spectrum; (Karakida et al) has shown empirically that the decay can be exponential. Thus log det(I) would be dominated by the long tail (since after taking logarithm it is the sum (or average) of an arithmetic sequence), while log tr(I) would be dominated by the first largest few values. The authors claim that this is not an issue since they replaced the observed FIM with a subsampled, low-rank (<=10), version. It corresponds to consider a small submatrix of (the gram matrix of) the NTK. Denote this matrix as . (a) This does not help with the problem, since we now have no chance of recovering the smaller eigenvalues that would have dominated log(det(I)), and it is impossible that the proposed regularizer has a similar behavior to log(det(I)). (b) One could verify easily, using small feed-forward networks (or even simpler, computing a gram matrix using RBF kernels, since the FIM shares its eigenspectrum with NTK which is a p.d. kernel), that the new matrix still has a fast-decaying eigenspectrum, so the behavior of and \\tilde{I} are still significantly different, even though this cannot be established by concentration bounds as the authors argue. While FFN and modern deep architectures can have different behaviors, I believe the above evidence suggests that a numerical experiment comparing the behavior of the two bounds is a must. Following this argument we can see *another issue* of this work, namely the proposed generalization bound will be vacuous given the fast-decaying spectrum of the FIM, since it contains gamma=log(det(I)). Reviewer 1 mentioned this work could enlighten future discussions on this subject. While I agree this paper presents interesting empirical observations (namely its final algorithm, which is vaguely connected to the proposed objective, leads to improved performance on CV tasks), I think this submission in its current form is a bit too misleading to serve this purpose well, and overall I believe it would be better to go through another round of revision. Original Review ============================================ This paper presents a generalization bound based on Fisher information at a local optima, and proposes to optimize (an approximation to) it to get better generalization guarantees. There are issues in both parts, and I don't think it should be accepted. Specifically, 1. The definition of Fisher information is incorrect (for almost every parameter). The expectation should be taken w.r.t the model distribution p(c_x|x;w), instead of the data distribution S. 2. Assumption (1) (loss locally quadratic) is not reasonable for DNNs, since local optimas will not be unique in their neighborhoods. See e.g. Section 12.2.2, \"Information Geometry and Its Applications\". 3. Regarding the approximation to the bound, approximating log det(I) with log trace(I) is not a good idea: adding a very small eigenvalue will lead to noticeable change in the former, but negligible change in the latter. This is particularly problematic for DNNs, since the spectrum of their Fisher information matrix varies in a wide range: see \"Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach\". (Edit 11.8: * regarding point (1), there is a quantity called observed Fisher information in e.g. Grunwald (2007) that coincide with Eq (1) in the paper, but it is a function of the dataset instead of the model parameter, and can only used to study model parameters near the *global optima* (as it is applied in Grunwald (2007)); it cannot help with choosing between different local optimas as this work claims. Additionally, the FIA criterion, which is used in this paper to devleop the generalization bound, is defined using the standard form of Fisher information (i.e. taking expectation w.r.t model distribution), see Rissanen (1996). These facts lead me to believe this is a confusion on the authors' part. * in point (2) I was referring to the authors' argument \" Since L(S,w) is analytic and w_0 is the *only local minimum* of L(S,w) in M(w_0)\", which is incorrect.)", "rating": "1: Reject", "reply_text": "Dear reviewer2 , Thanks for your time and effort . There seems to be quite a lot of misunderstandings and confusions from the reviewer . Q1 : \u201c The definition of Fisher information is incorrect \u2026 The expectation should be taken w.r.t the model distribution \u201d . The statement in this question is factually wrong . First of all , what we use in the paper is the observed Fisher information , not the ( expected ) Fisher information . Secondly , by definition , the Fisher information , no matter the observed or the expected one , has nothing to do with expectation w.r.t.the model distribution . Q2 : \u201c There is a quantity called observed Fisher information that coincide with Eq ( 1 ) in the paper. \u201d This is by no means a coincidence . We clearly and precisely describe the term as observed Fisher information in the very first place when we introduce Eq ( 1 ) . Q3 : \u201c The observed Fisher information is a function of the dataset instead of the model parameter , as in Gr\u00fcnwald ( 2007 ) \u201d This is factually wrong . Professor Peter Gr\u00fcnwald has never said so . The Fisher information , no matter the observed one or the expected version , is a quantity involving both the model parameters and the input data . In Gr\u00fcnwald ( 2007 ) , simply omitting model parameter \u03b8 in the notation I ( X ) does not mean I ( X ) is not a function of \u03b8. Q4 : \u201c The observed Fisher information can only used to study model parameters near the global optima ; it can not help with choosing between different local optima as the work claims \u201d It is clearly stated in Section 4 that the different local minima we focus on to compare are also global minima . The comparison between these local minima is well-motivated , as pointed out at the beginning of Section 1 , that learning algorithms such as SGD tend to end up in one of the many local ( global ) minima that are not distinguishable from their similar close-to-zero training loss [ 2 , 3 , 4 , 5 ] . Q5 : \u201c The FIA criterion , which is used in this paper to develop the generalization bound , is defined using the expected Fisher information rather than the observed one. \u201d We give the FIA criterion precisely in Section 5.1 , only to illustrate the connection between our approach and Rissanen 's formulation of the MDL principle . In fact , we do not use the FIA criterion to derive or describe the generalization bound . Q6 : \u201c local optima will not be unique in their neighborhoods , as in [ 1 ] '' In our paper , we assume that local minima we care about are well isolated , mentioned at the end of Section 5.1 . For state-of-the-art network architectures used in practice , this isolation assumption is often the fact . The reviewer pointed out that , introduced in [ 1 ] , two kinds of singularity in neural networks prevent the local minima from being unique , namely the eliminating singularity and the overlapping singularity . As well demonstrated in [ 6 ] , network with skip connections ( such as ResNet , WRN , and DenseNet used in our experiments ) can effectively eliminate both . We would like to add a discussion paragraph about the isolation assumption in our paper . Q7 : \u201c Regarding the approximation to the bound , approximating log det ( I ) with log trace ( I ) is not a good idea. \u201d We do not use log trace ( I ) as an approximation to measure local minima as indeed it can be inaccurate . Instead , we use it as an upper bound of what we intend to optimize during training . Optimizing such upper bound , in return , enables us to develop a tractable regularization technique in search of the good local minima . Our experiments in Section 7.2 well demonstrate the effectiveness of our proposed regularizer in finding better local minima of greater generalizability . [ 1 ] Amari , Shun-ichi . Information geometry and its applications . Vol.194.Berlin : Springer , 2016 . [ 2 ] Dauphin , Yann N. , et al . `` Identifying and attacking the saddle point problem in high-dimensional non-convex optimization . '' Advances in neural information processing systems . 2014 . [ 3 ] Kawaguchi , Kenji . `` Deep learning without poor local minima . '' Advances in neural information processing systems . 2016 . [ 4 ] Nguyen , Quynh , and Matthias Hein . `` Optimization Landscape and Expressivity of Deep CNNs . '' International Conference on Machine Learning . 2018 . [ 5 ] Du , Simon S. , et al . `` Gradient Descent Finds Global Minima of Deep Neural Networks . '' International Conference on Machine Learning , 2019 . [ 6 ] Orhan , A. Emin , and Xaq Pitkow . `` Skip connections eliminate singularities . '' International Conference on Learning Representations , 2018 ."}, "1": {"review_id": "BJlXgkHYvS-1", "review_text": "This paper contributes to the deep learning generalization theory, mainly from the theoretical perspective with experimental verifications. The key proposition is given by the unnumbered simple equation in the middle of page 4 (please number it), where \\mathcal{I} is the Fisher information matrix. According to the authors, this simple metric, which is the log-determinant of the Fisher information matrix, can characterize the generalization of a DNN. Remarkably, this piece of work is well written in terms of English and formulations, and complete, with a rigorous theoretical analysis (section 5.1, 5.2), practical approximations (section 5.3) and empirical verifications (section 6). On the theoretical side, this work builds upon Rissanen's formulation of the MDL principle, which has two parts (describing data given the model as well as the model complexity). Under rough approximations, the complexity term becomes the log-determinant of the Fisher information matrix evaluated at the local (global) optimum. This simple approximation is further proved to upper-bounds the generalization error as stated in theorem 1. To make the criterion to be practically useful, the author used the Jensen inequality so that the metric simply depends on the trace of the Fisher information matrix. The empirical study showed the usefulness of the proposed metric which can well approximate the testing error and a regularization term (based on the trace of the Fisher information matrix) that can improve generalization on real DNN experiments. The reviewer has the following minor comments to further improve this contribution: section 5.1, explain the abbreviation FIA Regarding the choice of the neighborhood \\mathcal{M}(w_0), what is the reason to define the model (neighbourhood of w_0) based on the loss? Why not simply take a coordinate neighborhood? According to your metric, the smaller the scale of the Fisher information matrix, the better the generalization. In section 5.1, there has to be some remarks on the intuition and related works on the flatness of the local minimum that is related to generalization. As this contribution is related to the spectral properties of the Fisher information matrix, the reviewer points the authors to \"Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach. Karakida et al. 2018.\" and \"Lightlike Neuromanifolds, Occam's Razor and Deep Learning. Sun and Nielsen. 2019\", which deals with asymptotic cases and have similar MDL formulations expressed in terms of the spectrum of the Fisher information matrix. ", "rating": "8: Accept", "reply_text": "Dear reviewer1 , Thanks for your appreciation . Q1 : \u201c Section 5.1 , explain the abbreviation FIA \u201d FIA in Section 5.1 stands for Fisher information approximation , originally coined for Normalized Maximum Likelihood Estimation in [ 1 ] . Q2 : \u201c Regarding the choice of the neighborhood \\mathcal { M } ( w_0 ) , what is the reason to define the model ( neighbourhood of w_0 ) based on the loss ? Why not simply take a coordinate neighborhood ? \u201d Given the local minimum at w_0 , we find it natural to define its neighborhood by a sublevel set w.r.t.the training loss . The issue of using the local coordinate ( e.g. , using an \u0190-ball to define the neighborhood ) is that the amount of change of the underlying model measured by training loss varies for different dimensions of the parameter space . For instance , moving in one direction might change the model a lot while moving in the other might change little . Q3 : \u201c In section 5.1 , there have to be some remarks on the intuition and related works on the flatness of the local minimum that is related to generalization. \u201d We will update the paper to add a discussion in Section 5.1. regarding the intuition and related works on \u201c flatness/sharpness \u201d , some of which are briefly discussed in Section 2 . Q4 : \u201c the reviewer points the authors to [ 2 ] and [ 3 ] , which have similar MDL formulations expressed in terms of the spectrum of the Fisher information matrix \u201d We will definitely consider mentioning the relation with these two papers in our next version . [ 1 ] Rissanen , Jorma J . `` Fisher information and stochastic complexity . '' IEEE transactions on information theory 42.1 ( 1996 ) : 40-47 . [ 2 ] Karakida , Ryo , Shotaro Akaho , and Shun-ichi Amari . `` Universal Statistics of Fisher Information in Deep Neural Networks : Mean Field Approach . '' The 22nd International Conference on Artificial Intelligence and Statistics . 2019 . [ 3 ] Sun , Ke , and Frank Nielsen . `` Lightlike Neuromanifolds , Occam 's Razor and Deep Learning . '' arXiv preprint arXiv:1905.11027 ( 2019 ) ."}, "2": {"review_id": "BJlXgkHYvS-2", "review_text": "This paper provides a metric to characterize local minima of deep network loss landscapes based on the Fisher information matrix of the model parameterized by the deep network. The authors connect the Fisher information to the curvature of the loss landscape (the loss considered is the negative loss likelihood) and obtain generalization bounds through PAC Bayes analysis. They further propose regularizing the training of deep networks using the local curvature of the loss as a regularizer. In the final experimental section of the paper, the relationship between the empirical measures and generalization is shown on a variety of networks. This is an interesting paper, but I have a few concerns. 1. The information-theoretic measure that is proposed is essentially the (log) determinant of the hessian of the loss function. If there are degenerate eigendirections (zero eigenvalues) then the proposed measure would not be able to distinguish between minima with different numbers of degenerate directions / same number of degenerate directions but different spectral norms of the hessians. If the authors contention is that there will be no zero eigenvalues, that suggests that local minima of deep networks are all strict, isolated minima, contrary to recent work on connected solutions (See Draxler et. al. 2018, Essentially No Barriers in Neural Network Energy Landscapes, ICML 2018). 2. I would like to see how the authors believe their measure deals with rescalings layer parameters in deep networks, ie the issue brought up by Dinh et. al. in \"Sharp Minima can Generalize for Deep Networks\" ICML 2017. While I can see that the log determinant is invariant, it is not clear that the proposed approximation will be invariant to rescaling of deep network layer parameters. If the parameters corresponding to the eigenvalues sampled in the approximation are rescaled, I believe the proposed measure will not be invariant. 3. The experiments regarding the local minima characterization are well constructed, though some details are missing such as how the authors decided that training had converged to a local minimum. As far as regularization based on the local curvature is concerned, I would like to see some more experiments that compare the proposed technique to adagrad/adam and other techniques that purport to condition the gradient based on local curvature. It would also be interesting to see whether the regularization indeed converges to flatter minima characterized by the proposed flatness measure. Since the claim is that the regularizer gets you flatter solutions, that information is important to decide whether the proposed technique is performing as advertised. I am willing to update my score based on responses to these concerns.", "rating": "3: Weak Reject", "reply_text": "Dear reviewer3 , Thank you for the constructive review . Q1 : \u201c There are degenerate eigendirections \u201d or otherwise \u201c local minima are all isolated \u201d , which is \u201c contrary to recent work [ 2 ] ... \u201d Throughout our paper , we make the assumption that the local minima we care about are isolated , mentioned at the end of Section 5.1 . We would like to update our paper to add a discussion on this subject , including the answer to this and the next question . This assumption is not necessarily contradictory to the argument that local minima are connected , as suggested by [ 2 ] that a relatively flat path exists between any pair of local minima of low training loss . We point out that the claim in [ 2 ] is not conclusive , that all the local minima are \u201c perhaps best seen as \u201d one connected component . In other words , the local minima can still be isolated . For state-of-the-art network architectures used in practice , the isolation assumption is often the fact . To be precise , this assumption is violated when the Hessian matrix at a local minimum is singular . Specifically , [ 3 ] summarizes three sources of the singularity : ( i ) due to a dead neuron , ( ii ) due to identical neurons , and ( iii ) linear dependence of the neurons . As well demonstrated in [ 3 ] , network with skip connection ( such as ResNet , WRN , and DenseNet used in our experiments ) can effectively eliminate all the aforementioned singularity . There is another kind of singularity specifically for ReLU networks , which we will discuss next . Q2 : In practice how can the proposed metric \u201c deal with rescaling layer parameters in deep networks \u201d , i.e. , the rescaling issue described in [ 1 ] ? In practice , the rescaling issue is not critical . There are three reasons : ( I ) This issue can only happen in neural networks equipped with scale-invariant activation functions , such as ReLU . Many state-of-the-art models use other activation functions such as ELU [ 7 ] that is not scale-invariant . ( II ) Even for ReLU networks , most modern DNNs are free of this issue , since they have normalization layers such as BatchNorm [ 8 ] applied before the activation . BatchNorm shifts all the inputs to the ReLU function , which is equivalent to shifting the ReLU function horizontally . The shifted ReLU is no longer scale-invariant . The ResNet , WRN , and DenseNet used in our experiments all fall into this category . ( III ) Due to the ubiquitous use of normal distribution based weights initialization scheme and the L2 regularization / weight decay , most of the local minima obtained by gradient-based learning algorithms have weights of a relatively small norm . Consequently , in practice , we will not compare two local minima essentially the same but have one as the rescaled version of the other with a much larger norm of the weights . In summary , the rescaling issue is another source of the singularity but only for networks equipped with scale-invariant activation functions . And in practice , it is effectively eliminated . Q3 : \u201c How the authors decided that training had converged to a local minimum \u201d in Section 7.1 ? For the experiments of local minima characterization in Section 7.1 , in all scenarios , we train the model for 200 epochs with an initial learning rate 0.1 , divided by 10 when the training loss plateaus . Within each scenario , we find the final training loss very small and very similar across different models and the training accuracy essentially equal to 1 , indicating the convergence . Q4 : How is the proposed regularization method compared to \u201c AdaGrad/Adam and other techniques that purport to condition the gradient based on local curvature \u201d ? Our regularizer aims to find better \u201c flatter \u201d minima to improve generalization whereas adaptive optimization methods such as AdaGrad and Adam try to boost up convergence , yet at the cost of generalizability . Recent works such as [ 4 ] and [ 5 ] show that adaptive methods generalize worse than SGD+Momentum . In specific , very similar to our setup , [ 5 ] demonstrates that SGD+Momentum consistently outperforms the others on ResNet and DenseNet for CIFAR-10 and CIFAR-100 . Other approaches that also utilize local curvature , such as the Entropy-SGD [ 6 ] mentioned in Section 2 , have empirical results rather preliminary compared to ours . Furthermore , as described in Algorithm 1 , our proposed regularizer is not specific to a certain optimizer . We perform experiments with SGD+Momentum because it is chosen to be used in ResNet , WRN , and DenseNet , helping all of them achieve current or previous state-of-the-art results ."}}