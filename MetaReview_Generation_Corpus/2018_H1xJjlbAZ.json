{"year": "2018", "forum": "H1xJjlbAZ", "title": "INTERPRETATION OF NEURAL NETWORK IS FRAGILE", "decision": "Reject", "meta_review": "The paper tries to show that many of the state-of-the-art interpretability methods are brittle and do not provide consistent stable explanations. The authors show this by perturbing (even randomly) the inputs so that the differences are imperceptible to a human observer but the interpretability methods provide completely different explanations. Although the output class is maintained before and after the perturbation it is not clear to me or the reviewers why one shouldn't have different explanations. The difference in explanations can be attributed to the fragility of the learned models (highly non-smooth decision boundaries) rather than the explanation methods. This is a critical point and has to come out more clearly in the paper.", "reviews": [{"review_id": "H1xJjlbAZ-0", "review_text": "The authors study cases where interpretation of deep learning predictions is extremely fragile. They systematically characterize the fragility of several widely-used feature-importance interpretation methods. In general, questioning the reliability of the visualization techniques is interesting. Regarding the technical details, the reviewer has the following comments: - What's the limitation of this attack method? - How reliable are the interpretations? - The authors use spearman's rank order correlation and Top-k intersection as metrics for interpretation similarity. - Understanding whether influence functions provide meaningful explanations is very important and challenging problem in medical imaging applications. The authors showed that across the test images, they were able to perturb the ordering of the training image influences. I am wondering how this will be used and evaluated in medical imaging setting. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review and feedback . The main contribution of our paper is to systematically demonstrate for the first time that interpretation of neural networks are fragile to attacks . This is an important topic and your questions raise interesting future research directions . 1.The limitation of the attack method is a very interesting research direction . The attacks that we designed in our paper are all white-box attacks that need to know the NN model . Our next question to answer would be the dangers of interpretations attacks in the black-box setting without access to the model . 2.The reviewer asks how reliable are the interpretation methods . Although these methods are widely used ( e.g.Quang and Xie 17 , Kelly and Reshev 17 ) , there is not a unified definition of reliability that has been investigated and it is an active area of research ( Doshi-Velez & Kim , 2017 ) . One of the contributions of our work is to systematically compare the robustness of the interpretations generated by these different methods . Our work shows that is possible to change regions of high saliency through careful perturbations of the test images ( see Fig.2 ) .So the methods are correctly identifying new interpretations , but these interpretations disagree with human notions of what part of an image is most related to interpretation . 3.As the reviewer mentions , we defined metrics ( rank correlation and top-k intersection , and also center shift metric in Appendix D ) to compare the interpretations of two different images . In order to make it clear how these metrics correspond to intuitive notions of stability , we have included a new figure , Figure 3 , and a new appendix , Appendix C , which provides an example of how rank correlations and top-k intersection change as randomly sampled validation images are adversarially perturbed . 4 .We agree that medical case is one of the most important problems for the application influence functions and one way to evaluate the perturbations is to look at the concordance with human studies ."}, {"review_id": "H1xJjlbAZ-1", "review_text": "The key observation is that it is possible to generate adversarial perturbations wherein the behavior of feature importance methods (e.g. simple gradient method (Simonyan et al, 2013), integrated gradient (Sundararajan et al, 2017), and DeepLIFT ( Shrikumar et al, 2016) ) have large variation while predicting same output. Thus the authors claim that one has to be careful about using feature importance maps. Pro: The paper raises an interesting point about the stability of feature importance maps generated by gradient based schemes. Cons: The main problem I have with the paper is that there is no precise definition of what constitutes the stable feature importance map. The examples in the paper seem to be cherry picked to illustrate dramatic effects. The experimental protocol used does not provide enough information of the variability of the salience maps shown around small perturbations of adversarial inputs. The paper would benefit from more systematic experimentation and a better definition of what authors believe are important attributes of stability of human interpretability of neural net behavior.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the review and feedback . Here , we address the points made in the review as well as describe changes to the original submission to incorporate the reviewer \u2019 s feedback . Our paper proposes a precise definition of what it means for an interpretation to be fragile . As we stated in the abstract , our definition is \u201c two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations \u201d . A stable feature map is one that is not fragile by this definition . We have included additional discussions of our definitions in the Our Contributions section to clarify any question . Moreover , we proposed two clear metrics , rank correlation and top-K intersect , to quantify exactly how different two interpretations are . We have also included a new figure , Figure 3 , and a new appendix , Appendix C , which provides an example of how rank correlations and top-k intersection change as randomly sampled validation images are adversarially perturbed . The examples in Figure 1 are representative of how interpretations can be attacked by our perturbations . We have released our code at [ https : //goo.gl/6usSEk ] and the reviewer can verify for him/herself that our attacks are reproducible and consistent for ImageNET and CIFAR10 . Moreover , our experiments on ImageNET and CIFAR10 does systematically support that the interpretations are fragile by our definitions ( Figs 4 , 5 in the main text and Figs 12 , 13 in the Appendix ) . Could you please let us know if you have any more questions regarding the paper or if there are specific experiments that you \u2019 d like to see ? We \u2019 d like to engage in a dialogue until we resolve all of your questions ."}, {"review_id": "H1xJjlbAZ-2", "review_text": "The paper shows that interpretations for DNN decisions, e.g. computed by methods such as sensitivity analysis or DeepLift, are fragile: Visually (to a human) inperceptibly different image cause greatly different explanations (and also to an extent different classifier outputs). The authors perturb input images and create explanations using different methods. Even though the image is inperceptibly different to a human observer, the authors observe large changes in the heatmaps visualizing the explanation maps. This is true even for random perturbations. The images have been modified wrt. to some noise, such that they deviate from the natural statistics for images of that kind. Since the explanation algorithms investigated in this papers merely react to the interactions of the model to the input and thus are unsupervised processes in nature, the explanation methods merely show the model's reaction to the change. For one, the model itself reacts to the perturbation, which can be measured by the (considarbly) increased class probability. Since the prediction score is given in probabilty values, the reviewer assumes the final layer of the model is a SoftMax activation. In order to see change in the softmax output score, especially if the already dominant prediction score is further increased, a lot of change has to happen to the outputs of the layer serving as input to the SoftMax layer. It can thus be expected, that the input- and class specific explanations change as well, to an also not so small extent. The explanation maps mirror for the considered methods the model's reaction to the input. They are thus not meaningless, but are a measure to model reaction instead of an independent process. The excellent Figure 2 supports this point. Not the interpretation itself is fragile, but the model. Adding a small delta to the sample x shifts its position in data space, completely altering the prediction rule applied by the model due to the change in proximity to another section of the decision hyperplane. The fragility of DNN models to marginally perturbed inputs themselves is well known. This especially true for adversial perturbations, which have been used as test cases in this work. The explanation methods are expected to highlight highly important areas in an image, which have been targetet by these perturbation approaches. The authors give an example of an adversary manipulating the input in order to draw the activation to specific features to draw confusing/malignant explanation maps. In a settig of model verification, the explanation via heatmaps is exactly what one wants to have: If tiny change to the image causes lots of change to the prediction (and explanation) we can visualize the instability of the model not the explanation method. Further do targeted perturbations not show the fragility of explanation methods, but rather that the models actually find what is important to the model. It can be expected, that after a change to these parts of the input, the model will decide differently, albeit coming to the same conclusion (in terms of predicted class membership), which reflects in the explanation map computed for the perturbed input. Further remarks: It would be interesting to see the size and position of the center of mass attacks in the appendix. The reviewer closely follows and is experienced with various explanation methods, their application and the quality of the expected explanations. The reviewer is therefore surprised by the poor quality and lack of structure in the maps obtained from the DeepLift method. Can bugs and suboptimal configurations be ruled out during the experiments? The DeepLift explanations are almost as noisy as the ones obtained for Sensitivity Analysis (i.e. the gradient at the input point). However, recent work (e.g. Samek et al., IEEE TNNLS, 2017 or Montavon et al., Digital Signal Processing, 2017) showed that decomposition-based methods (such as DeepLift) provide less noisy explanations than Sensitivity Analysis. Have the authors considered training the net with small random perturbations added to the samples, to compare the \"vanilla\" model to the more robust one, which has seen noisy samples, and compared explanations? Why not train (finetune) the considered models using softplus activations instead of exchanging activation nodes? Appendix B: Heatmaps through the different stages of perturbation should be normalized using a common factor, not individually, in order to better reflect the change in the explanation Conclusion: The paper follows an interesting approach, but ultimately takes the wrong view point: The authors try to attribute fragility to explaining methods, which visualize/measure the reaction of the model to the perturbed inputs. A major rework should be considered.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the thorough review and feedback . Our conclusions are actually exactly the same as yours ! Our main contribution is to show that the interpretation ( e.g.the saliency map of an image ) can be significantly perturbed by adversarial attacks ; we do not claim that the interpretation method ( e.g.DeepLift ) is broken by the attacks . This completely agrees with your point . Our operational definition that an interpretation is fragile is \u201c two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations \u201d . All of our experiments support our conclusion that interpretations can be adversarially attacked by this definition . We used this definition because it \u2019 s analogous to the fragility notion used in the broader adversarial attack and ML security community . We do not claim in the paper that the interpretation method itself is broken by our attacks . This is a subtle and important distinction . We have added a new paragraph in the Conclusion section to clarify this : \u201c Our results demonstrate that the \\emph { interpretations } ( e.g.saliency maps ) are vulnerable to perturbations , but this does not imply that the \\emph { interpretation methods } are broken by the perturbations . This is a subtle but important distinction . Methods such as saliency measure the infinitesimal sensitivity of the neural network at a particular input $ \\xb $ . After a perturbation , the input has changed to $ \\tilde { \\xb } = \\xb + \\deltb $ , and the salency now measures the sensitivity at the perturbed input . The saliency \\emph { correctly } captures the infinitesimal sensitivity at the two inputs ; it 's doing what it is supposed to do . The fact that the two resulting saliency maps are very different is fundamentally due to the network itself being fragile to such perturbations , as we illustrate with Fig.~\\ref { fig : concept } . \u201d You asked about the quality of our DeepLIFT saliency maps . We implemented DeepLIFT with rescale rule as described by the original authors [ https : //arxiv.org/abs/1704.02685 ] . We have released the code for our implementation at [ https : //goo.gl/6usSEk ] . We have checked the code several times and we also closely work with the the lab developing DeepLIFT in the area of interpretability . Regarding your remark that heat-maps should be normalized using a common factor . It \u2019 s important to notice that the measures used for comparing the original and perturbed saliency maps ( top-K intersection and rank order correlation ) are independent from the normalizing scheme . Regarding your suggestion a training strategy to make networks more robust to such adversarial examples , and suggested retraining a network using softplus activations . These are very helpful suggestions that we will consider for future direction . Does this help to address your question ? We believe that our conclusions are the same as yours . Please let us know if the new discussions we \u2019 ve added to the revision clarify this confusion . The key contribution of our work is to extend adversarial attacks to interpretations for the first time , and this raises interesting security questions given how important interpretations are . Please let us know if there are additional analysis you \u2019 d like to see . We hope to engage in a dialogue until we can resolve all of your concerns ."}], "0": {"review_id": "H1xJjlbAZ-0", "review_text": "The authors study cases where interpretation of deep learning predictions is extremely fragile. They systematically characterize the fragility of several widely-used feature-importance interpretation methods. In general, questioning the reliability of the visualization techniques is interesting. Regarding the technical details, the reviewer has the following comments: - What's the limitation of this attack method? - How reliable are the interpretations? - The authors use spearman's rank order correlation and Top-k intersection as metrics for interpretation similarity. - Understanding whether influence functions provide meaningful explanations is very important and challenging problem in medical imaging applications. The authors showed that across the test images, they were able to perturb the ordering of the training image influences. I am wondering how this will be used and evaluated in medical imaging setting. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review and feedback . The main contribution of our paper is to systematically demonstrate for the first time that interpretation of neural networks are fragile to attacks . This is an important topic and your questions raise interesting future research directions . 1.The limitation of the attack method is a very interesting research direction . The attacks that we designed in our paper are all white-box attacks that need to know the NN model . Our next question to answer would be the dangers of interpretations attacks in the black-box setting without access to the model . 2.The reviewer asks how reliable are the interpretation methods . Although these methods are widely used ( e.g.Quang and Xie 17 , Kelly and Reshev 17 ) , there is not a unified definition of reliability that has been investigated and it is an active area of research ( Doshi-Velez & Kim , 2017 ) . One of the contributions of our work is to systematically compare the robustness of the interpretations generated by these different methods . Our work shows that is possible to change regions of high saliency through careful perturbations of the test images ( see Fig.2 ) .So the methods are correctly identifying new interpretations , but these interpretations disagree with human notions of what part of an image is most related to interpretation . 3.As the reviewer mentions , we defined metrics ( rank correlation and top-k intersection , and also center shift metric in Appendix D ) to compare the interpretations of two different images . In order to make it clear how these metrics correspond to intuitive notions of stability , we have included a new figure , Figure 3 , and a new appendix , Appendix C , which provides an example of how rank correlations and top-k intersection change as randomly sampled validation images are adversarially perturbed . 4 .We agree that medical case is one of the most important problems for the application influence functions and one way to evaluate the perturbations is to look at the concordance with human studies ."}, "1": {"review_id": "H1xJjlbAZ-1", "review_text": "The key observation is that it is possible to generate adversarial perturbations wherein the behavior of feature importance methods (e.g. simple gradient method (Simonyan et al, 2013), integrated gradient (Sundararajan et al, 2017), and DeepLIFT ( Shrikumar et al, 2016) ) have large variation while predicting same output. Thus the authors claim that one has to be careful about using feature importance maps. Pro: The paper raises an interesting point about the stability of feature importance maps generated by gradient based schemes. Cons: The main problem I have with the paper is that there is no precise definition of what constitutes the stable feature importance map. The examples in the paper seem to be cherry picked to illustrate dramatic effects. The experimental protocol used does not provide enough information of the variability of the salience maps shown around small perturbations of adversarial inputs. The paper would benefit from more systematic experimentation and a better definition of what authors believe are important attributes of stability of human interpretability of neural net behavior.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the review and feedback . Here , we address the points made in the review as well as describe changes to the original submission to incorporate the reviewer \u2019 s feedback . Our paper proposes a precise definition of what it means for an interpretation to be fragile . As we stated in the abstract , our definition is \u201c two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations \u201d . A stable feature map is one that is not fragile by this definition . We have included additional discussions of our definitions in the Our Contributions section to clarify any question . Moreover , we proposed two clear metrics , rank correlation and top-K intersect , to quantify exactly how different two interpretations are . We have also included a new figure , Figure 3 , and a new appendix , Appendix C , which provides an example of how rank correlations and top-k intersection change as randomly sampled validation images are adversarially perturbed . The examples in Figure 1 are representative of how interpretations can be attacked by our perturbations . We have released our code at [ https : //goo.gl/6usSEk ] and the reviewer can verify for him/herself that our attacks are reproducible and consistent for ImageNET and CIFAR10 . Moreover , our experiments on ImageNET and CIFAR10 does systematically support that the interpretations are fragile by our definitions ( Figs 4 , 5 in the main text and Figs 12 , 13 in the Appendix ) . Could you please let us know if you have any more questions regarding the paper or if there are specific experiments that you \u2019 d like to see ? We \u2019 d like to engage in a dialogue until we resolve all of your questions ."}, "2": {"review_id": "H1xJjlbAZ-2", "review_text": "The paper shows that interpretations for DNN decisions, e.g. computed by methods such as sensitivity analysis or DeepLift, are fragile: Visually (to a human) inperceptibly different image cause greatly different explanations (and also to an extent different classifier outputs). The authors perturb input images and create explanations using different methods. Even though the image is inperceptibly different to a human observer, the authors observe large changes in the heatmaps visualizing the explanation maps. This is true even for random perturbations. The images have been modified wrt. to some noise, such that they deviate from the natural statistics for images of that kind. Since the explanation algorithms investigated in this papers merely react to the interactions of the model to the input and thus are unsupervised processes in nature, the explanation methods merely show the model's reaction to the change. For one, the model itself reacts to the perturbation, which can be measured by the (considarbly) increased class probability. Since the prediction score is given in probabilty values, the reviewer assumes the final layer of the model is a SoftMax activation. In order to see change in the softmax output score, especially if the already dominant prediction score is further increased, a lot of change has to happen to the outputs of the layer serving as input to the SoftMax layer. It can thus be expected, that the input- and class specific explanations change as well, to an also not so small extent. The explanation maps mirror for the considered methods the model's reaction to the input. They are thus not meaningless, but are a measure to model reaction instead of an independent process. The excellent Figure 2 supports this point. Not the interpretation itself is fragile, but the model. Adding a small delta to the sample x shifts its position in data space, completely altering the prediction rule applied by the model due to the change in proximity to another section of the decision hyperplane. The fragility of DNN models to marginally perturbed inputs themselves is well known. This especially true for adversial perturbations, which have been used as test cases in this work. The explanation methods are expected to highlight highly important areas in an image, which have been targetet by these perturbation approaches. The authors give an example of an adversary manipulating the input in order to draw the activation to specific features to draw confusing/malignant explanation maps. In a settig of model verification, the explanation via heatmaps is exactly what one wants to have: If tiny change to the image causes lots of change to the prediction (and explanation) we can visualize the instability of the model not the explanation method. Further do targeted perturbations not show the fragility of explanation methods, but rather that the models actually find what is important to the model. It can be expected, that after a change to these parts of the input, the model will decide differently, albeit coming to the same conclusion (in terms of predicted class membership), which reflects in the explanation map computed for the perturbed input. Further remarks: It would be interesting to see the size and position of the center of mass attacks in the appendix. The reviewer closely follows and is experienced with various explanation methods, their application and the quality of the expected explanations. The reviewer is therefore surprised by the poor quality and lack of structure in the maps obtained from the DeepLift method. Can bugs and suboptimal configurations be ruled out during the experiments? The DeepLift explanations are almost as noisy as the ones obtained for Sensitivity Analysis (i.e. the gradient at the input point). However, recent work (e.g. Samek et al., IEEE TNNLS, 2017 or Montavon et al., Digital Signal Processing, 2017) showed that decomposition-based methods (such as DeepLift) provide less noisy explanations than Sensitivity Analysis. Have the authors considered training the net with small random perturbations added to the samples, to compare the \"vanilla\" model to the more robust one, which has seen noisy samples, and compared explanations? Why not train (finetune) the considered models using softplus activations instead of exchanging activation nodes? Appendix B: Heatmaps through the different stages of perturbation should be normalized using a common factor, not individually, in order to better reflect the change in the explanation Conclusion: The paper follows an interesting approach, but ultimately takes the wrong view point: The authors try to attribute fragility to explaining methods, which visualize/measure the reaction of the model to the perturbed inputs. A major rework should be considered.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the thorough review and feedback . Our conclusions are actually exactly the same as yours ! Our main contribution is to show that the interpretation ( e.g.the saliency map of an image ) can be significantly perturbed by adversarial attacks ; we do not claim that the interpretation method ( e.g.DeepLift ) is broken by the attacks . This completely agrees with your point . Our operational definition that an interpretation is fragile is \u201c two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations \u201d . All of our experiments support our conclusion that interpretations can be adversarially attacked by this definition . We used this definition because it \u2019 s analogous to the fragility notion used in the broader adversarial attack and ML security community . We do not claim in the paper that the interpretation method itself is broken by our attacks . This is a subtle and important distinction . We have added a new paragraph in the Conclusion section to clarify this : \u201c Our results demonstrate that the \\emph { interpretations } ( e.g.saliency maps ) are vulnerable to perturbations , but this does not imply that the \\emph { interpretation methods } are broken by the perturbations . This is a subtle but important distinction . Methods such as saliency measure the infinitesimal sensitivity of the neural network at a particular input $ \\xb $ . After a perturbation , the input has changed to $ \\tilde { \\xb } = \\xb + \\deltb $ , and the salency now measures the sensitivity at the perturbed input . The saliency \\emph { correctly } captures the infinitesimal sensitivity at the two inputs ; it 's doing what it is supposed to do . The fact that the two resulting saliency maps are very different is fundamentally due to the network itself being fragile to such perturbations , as we illustrate with Fig.~\\ref { fig : concept } . \u201d You asked about the quality of our DeepLIFT saliency maps . We implemented DeepLIFT with rescale rule as described by the original authors [ https : //arxiv.org/abs/1704.02685 ] . We have released the code for our implementation at [ https : //goo.gl/6usSEk ] . We have checked the code several times and we also closely work with the the lab developing DeepLIFT in the area of interpretability . Regarding your remark that heat-maps should be normalized using a common factor . It \u2019 s important to notice that the measures used for comparing the original and perturbed saliency maps ( top-K intersection and rank order correlation ) are independent from the normalizing scheme . Regarding your suggestion a training strategy to make networks more robust to such adversarial examples , and suggested retraining a network using softplus activations . These are very helpful suggestions that we will consider for future direction . Does this help to address your question ? We believe that our conclusions are the same as yours . Please let us know if the new discussions we \u2019 ve added to the revision clarify this confusion . The key contribution of our work is to extend adversarial attacks to interpretations for the first time , and this raises interesting security questions given how important interpretations are . Please let us know if there are additional analysis you \u2019 d like to see . We hope to engage in a dialogue until we can resolve all of your concerns ."}}