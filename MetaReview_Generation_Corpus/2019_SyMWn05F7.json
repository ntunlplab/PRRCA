{"year": "2019", "forum": "SyMWn05F7", "title": "Learning Exploration Policies for Navigation", "decision": "Accept (Poster)", "meta_review": "The authors have proposed an approach for directly learning a spatial exploration policy which is effective in unseen environments. Rather than use external task rewards, the proposed approach uses an internally computed coverage reward derived from on-board sensors. The authors use imitation learning to bootstrap the training and then fine-tune using the intrinsic coverage reward. Multiple experiments and ablations are given to support and understand the approach. The paper is well-written and interesting. The experiments are appropriate, although further evaluations in real-world settings really ought to be done to fully explore the significance of the approach. The reviewers were divided, with one reviewer finding fault with the paper in terms of the claims made, the positioning against prior art, and the chosen baselines. The other two reviewers supported publication even after considering the opposition of R1, noting that they believe that the baselines are sufficient, and the contribution is novel. After reviewing the long exchange and discussion, the AC sides with accepting the paper. Although R1 raises some valid concerns, the authors defend themselves convincingly and the arguments do not, in any case, detract substantially from what is a solid submission.", "reviews": [{"review_id": "SyMWn05F7-0", "review_text": "This paper proposes learning exploration policies for navigation. The problem is motivated well. The learning is conducted using reinforcement learning, bootstrapped by imitation learning. Notably, RL is done using sensor-derived intrinsic rewards, rather than extrinsic rewards provided by the environment. The results are good. I like this paper a lot. It addresses an important problem. It is written well. The approach is not surprising but is reasonable and is a good addition to the literature. One reservation is that the method relies on an oracle for state estimation. In some experiments, synthetic noise is added, but this is not a realistic noise model and the underlying data still comes from an oracle that would not be available in real-world deployment. I recommend that the authors do one of the following: (a) use a real (monocular, stereo, or visual-inertial) odometry system for state estimation, or (b) acknowledge clearly that the presented method relies on unrealistic oracle odometry. Even with this reservation, I support accepting the paper. Minor: In Section 3.4, \"existing a room\" -> \"exiting a room\"", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments and suggestions . We acknowledge that most of our evaluation is in the perfect odometry setting which is unrealistic . We experimented with a reasonable noise model that compounds over time within the episode , but we admit it may not be very realistic . We will prominently note both these points in the final version of the paper upon acceptance ."}, {"review_id": "SyMWn05F7-1", "review_text": "This paper proposes a method for learning how to explore environments. The paper mentions that the \u201cexploration task\u201d that is defined in this paper can be used for improving the well-known navigation tasks. For solving this task, a reward function a network architecture that uses RGBD images + reconstructed map + imitation learning + PPO is designed. <<Pros>> -The paper is well-written (except for a few typos). -The overall approach is simple and does not have much complications. -The underling idea and motivation is clearly narrated in the intro and abstract and the paper has a easy-to-understand flow. <<Cons>> **The technical novelty is not significant** -This paper does not provide significant technical novelty. It is a combination of known prior methods: imitation learning + ppo (prior RL work). The presented exploration task is not properly justified as to how it could be useful for the navigation task. The reconstruction of maps for solving the navigation problem is a well-explored problem in prior SLAM and 3D reconstruction methods. Overall the novelty of the approach and the proposed problem is incremental. **The paper has major short comings in the experimental section. The presented experiments do not support the main claim of the paper which is improving the performance in the well-known navigation task. Major baselines are missing. Also, the provided results are not convincing in doing the right comparison with the baselines. ** -Experimental details are missing. The major experimental evaluations (Fig. 2 and Fig. 3) are based on the m^2 coverage after k steps and the plots are cut at 1000 steps. What are the statistical properties of the 3D houses used for training and testing? E.g what is their area in m^2? How big is each step in meters? Why are the graphs cut at 1000 steps? How would different methods converge after more than 1000 steps, e.g. 2000 steps? I would like to see how would the different methods converge after larger number of steps? How long would each step take in terms of time? How could these numbers convey the significance of the proposed method in a real would problem settings? -The experiments do not convey if learning has significantly resulted in improved exploration. Consider a simple baseline that follows a similar approach as explained in the paper for constructing the occupancy map using the depth sensor. A non-learning agent could use this map at each step to make a greedy choice about its next action which greedily maximizes the coverage gain based on its current belief of the map. While the performance of random policy is shown in Fig.2 the performance of this greedy baseline is a better representative of the lower bound of the performance on the proposed task and problem setup. -What is the performance of a learning-based method that only performs collision avoidance? Collision avoidance methods tend to implicitly learn to do a good map coverage. This simple baseline can show a tangible lower bound of a learning-based approach that does not rely on map. -The major promise of the paper is that the proposed exploration task can improve navigation. However, the navigation experiment does not compare the proposed method with any of prior works in navigation. There is a huge list of prior methods for navigation some of which are cited in the \u201clearning for Navigation\u201d section of the related works and the comparison provided in Fig. 4 is incomplete compared to the state-of-the-arts in navigation. For example, while the curiosity driven approach is compared for the exploration, the more related curiosity based navigation method which uses both \u201cexploration strategy\u201d and \u201cimitation learning\u201d : \u201cPathak, Deepak, et al. \"Zero-shot visual imitation.\" International Conference on Learning Representations. 2018. \u201c is missed in navigation comparison. The aforementioned paper is also missed in the references. -Algorithmic-wise, it would make the argument of the paper clearer if results were conducted by running different exploration strategies for navigation to see if running RL with a good exploration strategy could solve the exploration challenge of the navigation problem without needing an explicit exploration stage (similar to the proposed method) which first explores and constructs the map and then does navigation by planning. -The navigation problem as explained in section is solved based on planning approach that uses a reconstructed map. This is a fairly conventional approach that SLAM based methods use. Therefore, comparison with a SLAM method that constructs the map and then does navigation would be necessary. ** Technical details are missing or not explained clearly** - Section 3.1 does not clearly explain the map construction. It seems that the constructed map is just a 2D reconstruction of the space (and not 3D) using the depth sensor which does not need transformation of the 3D point cloud. What is the exact 3D transformation that you have done using the intrinsic camera parameters? This section mentions that there can be error in such map reconstruction because of robot noise but alignment is not needed because the proposed learning method provides robustness against miss-alignment. How is this justified? Why not using the known loop closure techniques in SLAM? -The technical details about the incorporated imitation learning method are missing. What imitation learning method is used? How is the policy trained during the imitation learning phase? -Last paragraph of intro mentions that the proposed method uses 3D information efficiently for doing exploration. The point of this sentence is unclear. What 3D information is used efficiently in the paper? Isn\u2019t it only 2.5D (information obtained by depth sensor) used in the proposed method? **Presentation can be improved** -The left and right plots of the Figure 3 contains lots of repetitions which brings in confusion in comparing the performance of runs with different settings. These two plots should be presented in a single plot. - Interpretation of \u201cgreen vs white vs black\u201d in the reconstructed maps is left to the reader in Fig. 1. - Last line in page 5: there is no need for reiteration. It is already clear. **Missing references** -Since the paper is about learning to explore, discussion about \u201cexploration techniques in RL\u201d is recommended to be added in at least the related work section. -A big list papers for 3D map reconstruction is missing. Since the proposed method relies on a map reconstruction, those papers are relevant to this work and can potentially be used for comparison (as explained above). It is highly recommended that relevant prior 3D map reconstruction papers be added to the related work sections. ", "rating": "3: Clear rejection", "reply_text": "We thank R1 for their comments . R1 \u2019 s primary concerns are about novelty and missing empirical comparison . These perhaps stem from some misunderstandings about our paper as some requested comparisons are either irrelevant or stronger comparisons are already presented in the paper . Therefore , we urge the reviewer to take a second look at the paper in light of the rebuttal . 1.Novelty : In this paper , we learn policies for exploring novel 3D environments ( Section 3 through Section 4.2 ) , and show that exploration data , gathered by executing our learned exploration policies , improves performance at downstream navigation tasks ( Section 4.3 ) . To the best of our knowledge , this is the first work that studies learned exploration policies for navigation , systematically compares them to classical and learning-based baselines , and shows the effectiveness of exploration data for downstream tasks . In doing so , we adopt existing learning techniques ( imitation learning + reinforcement learning ) , and map building techniques . Our novelties are orthogonal to these aspects : ( a ) Problem formulation : Framing exploration as a learning problem , and showing the utility of exploration data for downstream tasks . ( b ) Map based policy architectures and reward functions . Classical SLAM based approaches indeed produce maps but : ( a ) it still needs a policy for exploration during the map-building phase ; ( b ) does not solve navigation rather uses geometric analysis for path planning . Our approach focuses on ( a ) and unlike heuristic approaches used in SLAM , we use a learning-based approach . ( c ) We also show maps can also be used for learning effective policies , and for computing reward signals . ( d ) Use of IL + RL to optimize our policy , as opposed to pure RL that is typically used . 2.Comparison with other exploration approaches : a ) Simple Greedy Baseline : We experimented with the suggested one-step greedy policy . Here we virtually simulate all possible actions that the agent can take , and compute the gain in coverage . We then execute the action that results in the maximum gain in coverage . At 1000 steps such a policy only covers 40m^2 , as opposed to our policies that cover up to 125 m^2 . This is not surprising as the policy gets stuck inside local regions of full coverage . No action leads to any increase in coverage and the agents move back and forth . The full performance plot is provided in Fig C3 ( a ) in the updated PDF . Note , in the paper , we have provided a more compelling comparison point to classical exploration approaches : frontier-based method . Reviewer seems to have missed this comparison as R1 still asks for comparisons to classical approaches . b ) Collision Avoiding Policy : A policy that purely avoids collisions has a degenerate solution of the agent staying in-place , resulting in negligible coverage ( Fig C4 ( a ) ) . We also tried a more sophisticated version , where the agent moves straight unless a collision happens ( Fig C3 ( a ) ) , at which point it randomly rotates ( by angle between 0 and 2pi ) , and continues to move straight . To help the policy further , we used ground truth collision-checking . This policy covers 75m^2 , still much lower than our performance ( 125m^2 ) ."}, {"review_id": "SyMWn05F7-2", "review_text": "This is a well explained and well executed paper on using classical SLAM-like 2D maps for helping a standard Deep RL navigation agent (convnet + LSTM) explore efficiently an environment and without the need for extrinsinc rewards. The agent relies on 3 convnets, one processing RGB images, one the image of a coarse map in egocentric referential, and one of the image of a fine-grained map in egocentric referential (using pre-trained ResNet-18 convnets). Features produced by the convnets are fed into a recurrent policy trained using PPO. Two rewards are used: the increase in the map's coverage and an obstacle avoidance penalty. The agent is further bootstrapped through imitation learning in a goal-driven task executed by a human controlling the agent. The authors analyze the behavior of the navigation algorithm by various ablations, a baseline consisting of Pathak's (2017) Intrinsic Curiosity Module-based navigation and, commendably, a classical SLAM baseline with path planning to empty, unexplored spaces. Using an explicit map is a great idea but the authors need to acknowledge how hand-engineered all this is, when comparing it to actual end-to-end methods. First, the map reconstruction is done by back-projections of a depth image (using known projective geometry parameters) onto a 3D point cloud, then by slicing it to get a 2D map, accumulated over time using nearly perfect odometry. SLAM was an extremely hard problem to start with, and it took decades and particle filters to get to the quality of the images shown in this paper as obvious. Normally, there is drift and catastrophic map errors, whereas the videos show a nearly perfect map reconstruction. Is the motion model of the agent unrealistic? Would this ever work out of the box on a robot in a real world? The authors brush off the need for bundle adjustment, saying that the convnet can handle noisy local maps. Second, how do you get and maintain such nice ego-centric maps? Compared to other end-to-end work on learning how to map (see Wayne et al. or Zhang et al. or Parisotto et al., referred to later in the paper), it looks like the authors took a giant shortcut. All this SLAM apparatus should be learned! One crucial baseline that is missing is that of explicit extrinsic rewards encouraging exploration. These rewards merely scatter reward-yielding objects throughout the environment; over the course of an episode, an object reward that is picked does not re-appear until the next exploration episode, meaning that the agent needs to cover the whole space to forage for rewards. Examples of such rewards have been published in Mnih et al. (2016) \"Asynchronous methods for deep reinforcement learning\" and are implemented in DeepMind Lab (Beatie et al., 2016). Such an extrinsic reward would be directly related to the increase of coverage. A second point of discussion that is missing is that of the collision avoidance penalty: roboticists working on SLAM know well that they need to keep their robot away from plain-texture walls, otherwise the image processing cannot pick useful features for visual odometry, image matching or ICP. What happens if that penalty is dropped in this navigation agent? Finally, the authors mention the Neural Map paper but do not discuss Zhang et al. (2017) \"Neural SLAM\" or Wayne et al. (2018) \"Unsupervised Predictive Memory in a Goal-Directed Agent\", where a differentiable memory is used to store map information over the course of an episode and can store information relative to the agent's position and objects' / obstacles' positions as well. Minor remark: the word \"finally\" is repeated twice at the end of the introduction.", "rating": "7: Good paper, accept", "reply_text": "Thanks for your comments and suggestions . We address your specific concerns below : 1 . Explicit mapping is hand-engineering . We acknowledge ( and will explicitly state in the paper ) that using occupancy map as the policy input is based on domain/task knowledge . Using the occupancy map gives the agent a better representation of long-horizon memory and show great improvement compared to the policy without the map as input . We do agree ego-motion estimation in real-world might be noisy . To handle that we performed experiments with noise and show that our model seems robust ( See video on the website , Fig 4b in the paper ) . With regard to end-to-end approaches , approaches like Zhang et al . ( 2017 ) uses a differentiable map structure to mimic the SLAM techniques . These works are orthogonal to our effort on exploration . Indeed , our exploration policy can benefit from their learned maps instead of only using reconstructed occupancy map . We also believe our current approach provides a strong baseline for future end-to-end versions . 2.Explicit environment rewards for exploration : We agree that the use of reward yielding objects throughout the environment will lead to a very similar outcome as our approach . The key distinction is that our approach instruments the agent ( with a depth sensor ) as opposed to instrumenting the environment . This makes our proposed formulation more amenable to being trained and deployed in the real world : all we need is an RGB-D sensor . This is a big advantage over spreading reward yielding objects that disappear as the agents arrive at those locations , which is almost impractical in the real world . With this key distinction being said , we did do several experiments where our policy is trained with external rewards . The performance is shown in Fig C4 ( c ) in Appendix C4 . The results show that our coverage map reward is much more effective than external rewards generated by reward-yielding objects . Our method covers 125m^2 on average while even 4 reward yielding objects per square meter is 91m^2 . 3.Role of collision avoidance penalty : We added the performance of the agent trained with our policy but with only coverage reward ( no collision penalty ) in Fig C4 ( b ) in Appendix C4 . We observe that adding collision penalty indeed helps improve performance slightly ( 125m^2 with penalty as opposed to 120m^2 without penalty ) . Thus , our policy explores well even without explicit collision avoidance penalty . We will add more references to the related work and improve the writing as you suggested in the final version of the paper ."}], "0": {"review_id": "SyMWn05F7-0", "review_text": "This paper proposes learning exploration policies for navigation. The problem is motivated well. The learning is conducted using reinforcement learning, bootstrapped by imitation learning. Notably, RL is done using sensor-derived intrinsic rewards, rather than extrinsic rewards provided by the environment. The results are good. I like this paper a lot. It addresses an important problem. It is written well. The approach is not surprising but is reasonable and is a good addition to the literature. One reservation is that the method relies on an oracle for state estimation. In some experiments, synthetic noise is added, but this is not a realistic noise model and the underlying data still comes from an oracle that would not be available in real-world deployment. I recommend that the authors do one of the following: (a) use a real (monocular, stereo, or visual-inertial) odometry system for state estimation, or (b) acknowledge clearly that the presented method relies on unrealistic oracle odometry. Even with this reservation, I support accepting the paper. Minor: In Section 3.4, \"existing a room\" -> \"exiting a room\"", "rating": "7: Good paper, accept", "reply_text": "Thank you for your comments and suggestions . We acknowledge that most of our evaluation is in the perfect odometry setting which is unrealistic . We experimented with a reasonable noise model that compounds over time within the episode , but we admit it may not be very realistic . We will prominently note both these points in the final version of the paper upon acceptance ."}, "1": {"review_id": "SyMWn05F7-1", "review_text": "This paper proposes a method for learning how to explore environments. The paper mentions that the \u201cexploration task\u201d that is defined in this paper can be used for improving the well-known navigation tasks. For solving this task, a reward function a network architecture that uses RGBD images + reconstructed map + imitation learning + PPO is designed. <<Pros>> -The paper is well-written (except for a few typos). -The overall approach is simple and does not have much complications. -The underling idea and motivation is clearly narrated in the intro and abstract and the paper has a easy-to-understand flow. <<Cons>> **The technical novelty is not significant** -This paper does not provide significant technical novelty. It is a combination of known prior methods: imitation learning + ppo (prior RL work). The presented exploration task is not properly justified as to how it could be useful for the navigation task. The reconstruction of maps for solving the navigation problem is a well-explored problem in prior SLAM and 3D reconstruction methods. Overall the novelty of the approach and the proposed problem is incremental. **The paper has major short comings in the experimental section. The presented experiments do not support the main claim of the paper which is improving the performance in the well-known navigation task. Major baselines are missing. Also, the provided results are not convincing in doing the right comparison with the baselines. ** -Experimental details are missing. The major experimental evaluations (Fig. 2 and Fig. 3) are based on the m^2 coverage after k steps and the plots are cut at 1000 steps. What are the statistical properties of the 3D houses used for training and testing? E.g what is their area in m^2? How big is each step in meters? Why are the graphs cut at 1000 steps? How would different methods converge after more than 1000 steps, e.g. 2000 steps? I would like to see how would the different methods converge after larger number of steps? How long would each step take in terms of time? How could these numbers convey the significance of the proposed method in a real would problem settings? -The experiments do not convey if learning has significantly resulted in improved exploration. Consider a simple baseline that follows a similar approach as explained in the paper for constructing the occupancy map using the depth sensor. A non-learning agent could use this map at each step to make a greedy choice about its next action which greedily maximizes the coverage gain based on its current belief of the map. While the performance of random policy is shown in Fig.2 the performance of this greedy baseline is a better representative of the lower bound of the performance on the proposed task and problem setup. -What is the performance of a learning-based method that only performs collision avoidance? Collision avoidance methods tend to implicitly learn to do a good map coverage. This simple baseline can show a tangible lower bound of a learning-based approach that does not rely on map. -The major promise of the paper is that the proposed exploration task can improve navigation. However, the navigation experiment does not compare the proposed method with any of prior works in navigation. There is a huge list of prior methods for navigation some of which are cited in the \u201clearning for Navigation\u201d section of the related works and the comparison provided in Fig. 4 is incomplete compared to the state-of-the-arts in navigation. For example, while the curiosity driven approach is compared for the exploration, the more related curiosity based navigation method which uses both \u201cexploration strategy\u201d and \u201cimitation learning\u201d : \u201cPathak, Deepak, et al. \"Zero-shot visual imitation.\" International Conference on Learning Representations. 2018. \u201c is missed in navigation comparison. The aforementioned paper is also missed in the references. -Algorithmic-wise, it would make the argument of the paper clearer if results were conducted by running different exploration strategies for navigation to see if running RL with a good exploration strategy could solve the exploration challenge of the navigation problem without needing an explicit exploration stage (similar to the proposed method) which first explores and constructs the map and then does navigation by planning. -The navigation problem as explained in section is solved based on planning approach that uses a reconstructed map. This is a fairly conventional approach that SLAM based methods use. Therefore, comparison with a SLAM method that constructs the map and then does navigation would be necessary. ** Technical details are missing or not explained clearly** - Section 3.1 does not clearly explain the map construction. It seems that the constructed map is just a 2D reconstruction of the space (and not 3D) using the depth sensor which does not need transformation of the 3D point cloud. What is the exact 3D transformation that you have done using the intrinsic camera parameters? This section mentions that there can be error in such map reconstruction because of robot noise but alignment is not needed because the proposed learning method provides robustness against miss-alignment. How is this justified? Why not using the known loop closure techniques in SLAM? -The technical details about the incorporated imitation learning method are missing. What imitation learning method is used? How is the policy trained during the imitation learning phase? -Last paragraph of intro mentions that the proposed method uses 3D information efficiently for doing exploration. The point of this sentence is unclear. What 3D information is used efficiently in the paper? Isn\u2019t it only 2.5D (information obtained by depth sensor) used in the proposed method? **Presentation can be improved** -The left and right plots of the Figure 3 contains lots of repetitions which brings in confusion in comparing the performance of runs with different settings. These two plots should be presented in a single plot. - Interpretation of \u201cgreen vs white vs black\u201d in the reconstructed maps is left to the reader in Fig. 1. - Last line in page 5: there is no need for reiteration. It is already clear. **Missing references** -Since the paper is about learning to explore, discussion about \u201cexploration techniques in RL\u201d is recommended to be added in at least the related work section. -A big list papers for 3D map reconstruction is missing. Since the proposed method relies on a map reconstruction, those papers are relevant to this work and can potentially be used for comparison (as explained above). It is highly recommended that relevant prior 3D map reconstruction papers be added to the related work sections. ", "rating": "3: Clear rejection", "reply_text": "We thank R1 for their comments . R1 \u2019 s primary concerns are about novelty and missing empirical comparison . These perhaps stem from some misunderstandings about our paper as some requested comparisons are either irrelevant or stronger comparisons are already presented in the paper . Therefore , we urge the reviewer to take a second look at the paper in light of the rebuttal . 1.Novelty : In this paper , we learn policies for exploring novel 3D environments ( Section 3 through Section 4.2 ) , and show that exploration data , gathered by executing our learned exploration policies , improves performance at downstream navigation tasks ( Section 4.3 ) . To the best of our knowledge , this is the first work that studies learned exploration policies for navigation , systematically compares them to classical and learning-based baselines , and shows the effectiveness of exploration data for downstream tasks . In doing so , we adopt existing learning techniques ( imitation learning + reinforcement learning ) , and map building techniques . Our novelties are orthogonal to these aspects : ( a ) Problem formulation : Framing exploration as a learning problem , and showing the utility of exploration data for downstream tasks . ( b ) Map based policy architectures and reward functions . Classical SLAM based approaches indeed produce maps but : ( a ) it still needs a policy for exploration during the map-building phase ; ( b ) does not solve navigation rather uses geometric analysis for path planning . Our approach focuses on ( a ) and unlike heuristic approaches used in SLAM , we use a learning-based approach . ( c ) We also show maps can also be used for learning effective policies , and for computing reward signals . ( d ) Use of IL + RL to optimize our policy , as opposed to pure RL that is typically used . 2.Comparison with other exploration approaches : a ) Simple Greedy Baseline : We experimented with the suggested one-step greedy policy . Here we virtually simulate all possible actions that the agent can take , and compute the gain in coverage . We then execute the action that results in the maximum gain in coverage . At 1000 steps such a policy only covers 40m^2 , as opposed to our policies that cover up to 125 m^2 . This is not surprising as the policy gets stuck inside local regions of full coverage . No action leads to any increase in coverage and the agents move back and forth . The full performance plot is provided in Fig C3 ( a ) in the updated PDF . Note , in the paper , we have provided a more compelling comparison point to classical exploration approaches : frontier-based method . Reviewer seems to have missed this comparison as R1 still asks for comparisons to classical approaches . b ) Collision Avoiding Policy : A policy that purely avoids collisions has a degenerate solution of the agent staying in-place , resulting in negligible coverage ( Fig C4 ( a ) ) . We also tried a more sophisticated version , where the agent moves straight unless a collision happens ( Fig C3 ( a ) ) , at which point it randomly rotates ( by angle between 0 and 2pi ) , and continues to move straight . To help the policy further , we used ground truth collision-checking . This policy covers 75m^2 , still much lower than our performance ( 125m^2 ) ."}, "2": {"review_id": "SyMWn05F7-2", "review_text": "This is a well explained and well executed paper on using classical SLAM-like 2D maps for helping a standard Deep RL navigation agent (convnet + LSTM) explore efficiently an environment and without the need for extrinsinc rewards. The agent relies on 3 convnets, one processing RGB images, one the image of a coarse map in egocentric referential, and one of the image of a fine-grained map in egocentric referential (using pre-trained ResNet-18 convnets). Features produced by the convnets are fed into a recurrent policy trained using PPO. Two rewards are used: the increase in the map's coverage and an obstacle avoidance penalty. The agent is further bootstrapped through imitation learning in a goal-driven task executed by a human controlling the agent. The authors analyze the behavior of the navigation algorithm by various ablations, a baseline consisting of Pathak's (2017) Intrinsic Curiosity Module-based navigation and, commendably, a classical SLAM baseline with path planning to empty, unexplored spaces. Using an explicit map is a great idea but the authors need to acknowledge how hand-engineered all this is, when comparing it to actual end-to-end methods. First, the map reconstruction is done by back-projections of a depth image (using known projective geometry parameters) onto a 3D point cloud, then by slicing it to get a 2D map, accumulated over time using nearly perfect odometry. SLAM was an extremely hard problem to start with, and it took decades and particle filters to get to the quality of the images shown in this paper as obvious. Normally, there is drift and catastrophic map errors, whereas the videos show a nearly perfect map reconstruction. Is the motion model of the agent unrealistic? Would this ever work out of the box on a robot in a real world? The authors brush off the need for bundle adjustment, saying that the convnet can handle noisy local maps. Second, how do you get and maintain such nice ego-centric maps? Compared to other end-to-end work on learning how to map (see Wayne et al. or Zhang et al. or Parisotto et al., referred to later in the paper), it looks like the authors took a giant shortcut. All this SLAM apparatus should be learned! One crucial baseline that is missing is that of explicit extrinsic rewards encouraging exploration. These rewards merely scatter reward-yielding objects throughout the environment; over the course of an episode, an object reward that is picked does not re-appear until the next exploration episode, meaning that the agent needs to cover the whole space to forage for rewards. Examples of such rewards have been published in Mnih et al. (2016) \"Asynchronous methods for deep reinforcement learning\" and are implemented in DeepMind Lab (Beatie et al., 2016). Such an extrinsic reward would be directly related to the increase of coverage. A second point of discussion that is missing is that of the collision avoidance penalty: roboticists working on SLAM know well that they need to keep their robot away from plain-texture walls, otherwise the image processing cannot pick useful features for visual odometry, image matching or ICP. What happens if that penalty is dropped in this navigation agent? Finally, the authors mention the Neural Map paper but do not discuss Zhang et al. (2017) \"Neural SLAM\" or Wayne et al. (2018) \"Unsupervised Predictive Memory in a Goal-Directed Agent\", where a differentiable memory is used to store map information over the course of an episode and can store information relative to the agent's position and objects' / obstacles' positions as well. Minor remark: the word \"finally\" is repeated twice at the end of the introduction.", "rating": "7: Good paper, accept", "reply_text": "Thanks for your comments and suggestions . We address your specific concerns below : 1 . Explicit mapping is hand-engineering . We acknowledge ( and will explicitly state in the paper ) that using occupancy map as the policy input is based on domain/task knowledge . Using the occupancy map gives the agent a better representation of long-horizon memory and show great improvement compared to the policy without the map as input . We do agree ego-motion estimation in real-world might be noisy . To handle that we performed experiments with noise and show that our model seems robust ( See video on the website , Fig 4b in the paper ) . With regard to end-to-end approaches , approaches like Zhang et al . ( 2017 ) uses a differentiable map structure to mimic the SLAM techniques . These works are orthogonal to our effort on exploration . Indeed , our exploration policy can benefit from their learned maps instead of only using reconstructed occupancy map . We also believe our current approach provides a strong baseline for future end-to-end versions . 2.Explicit environment rewards for exploration : We agree that the use of reward yielding objects throughout the environment will lead to a very similar outcome as our approach . The key distinction is that our approach instruments the agent ( with a depth sensor ) as opposed to instrumenting the environment . This makes our proposed formulation more amenable to being trained and deployed in the real world : all we need is an RGB-D sensor . This is a big advantage over spreading reward yielding objects that disappear as the agents arrive at those locations , which is almost impractical in the real world . With this key distinction being said , we did do several experiments where our policy is trained with external rewards . The performance is shown in Fig C4 ( c ) in Appendix C4 . The results show that our coverage map reward is much more effective than external rewards generated by reward-yielding objects . Our method covers 125m^2 on average while even 4 reward yielding objects per square meter is 91m^2 . 3.Role of collision avoidance penalty : We added the performance of the agent trained with our policy but with only coverage reward ( no collision penalty ) in Fig C4 ( b ) in Appendix C4 . We observe that adding collision penalty indeed helps improve performance slightly ( 125m^2 with penalty as opposed to 120m^2 without penalty ) . Thus , our policy explores well even without explicit collision avoidance penalty . We will add more references to the related work and improve the writing as you suggested in the final version of the paper ."}}