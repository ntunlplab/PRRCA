{"year": "2017", "forum": "r1aPbsFle", "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling", "decision": "Accept (Poster)", "meta_review": "pros:\n - nice results on the tasks that justify acceptance of the paper\n \n cons:\n - In my opinion its a big stretch to describe this paper as a novel framework. The reasons for using the specific contrived augmented loss is based on the good results it produces. I view it more as regularization.\n - The \"theoretical justification\" for coupling of the input and output layers is based on the premise that the above regularization is the correct thing to do. Since that's really not justified by some kind of theory, I think its questionable to call this simple observation a theoretical justification.\n - Tying weights on the inputs and output layers is far from novel.", "reviews": [{"review_id": "r1aPbsFle-0", "review_text": "This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7. It is not obvious why the projection matrix L in Eq 3.6 (let's rename it to L') should be the same as that in Eq. 2.1. For example, L' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L' is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer, as is usually done. The experimental results are good and provide support for the approximate derivation done in section 4, particularly the distance plots in figure 1. Minor comments: Third line in abstract: where model -> where the model Second line in section 7: into space -> into the space Shouldn't the RHS in Eq 3.5 be \\sum \\tilde{y_{t,i}}(\\frac{\\hat{y}_t}{\\tilde{y_{t,i}}} - e_i) ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your kind assessment . We list below the responses to your major and minor comments : Major comments : Comment : The construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs . 3.6 and 3.7 . Response : Although the analysis in the paper may naturally lead one to believe that this is the case , we would like to note that this work was not initiated with the intention of finding an explanation for sharing the embeddings and the classifier . In fact , in our early report in [ 1 ] , we did not establish such a tight analytical link between the augmented loss and reusing word embeddings although we introduced them both . We also certainly hope that our work could be primarily appreciated for the particular framework which exploits metric similarity of words which it introduces . At this point , would like to also explain our motives for eqns 3.6 and 3.7 , namely the calculation of y~ . Eqn 3.6 is meant to select the word vector corresponding to the target token . Eqn 3.7 is a very elementary way to capture the word similarities through inner products and then normalize the similarity vector to be interpreted as a probability distribution ; this normalization was in fact used in [ 2 ] followed by a KL distance term for a different problem . From our perspective ( which certainly includes some authorship bias ) , the fact that this simple and plausible method leads to tying ( even if seemingly obviously ) points to the intuitiveness of shared embeddings in language processing tasks . Comment : It is not obvious why the projection matrix L in Eq 3.6 ( let 's rename it to L ' ) should be the same as that in Eq.2.1.For example , L ' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters . Response : This is exactly right ; and in fact , we do not claim that metric used for the augmented loss should clearly be the word embeddings of the model . Our method is simply by choice , guided by observations . As a matter of fact , very early on in our work ( this is when we did not have tying in mind , and were experimenting with the augmented loss idea ) and inspired by the work in [ 3 ] , we used both word2vec and gloVe embeddings ( both 300 dimensional ) as static metrics . However , the performance was significantly inferior to that when using the word embeddings of the model . In the interest of full disclosure , we should mention that we did not meticulously tune the models with static embeddings after not being able to see improvement ( and we only tried them on PTB ) ; however , the results did not seem promising . Instead , we decided to adopt the idea of information transfer within the model , and this lead to the current framework . The experimental performance of tying alone on large networks might slightly overshadow this core idea , but it was this idea that enabled us to reach tying , and link our two improvements . Comment : In the case that L ' is a new learned matrix , it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer , as is usually done . Response : Actually , not quite - although the above is not an incorrect conclusion . We will address this comment in the context of sharing embeddings . One takeaway from Section 4 is that the output projection matrix , W , will tend to the matrix used for capturing the word metric ( in the subspace sense ) . In this case , this matrix is L ' . Therefore , under the same assumptions , one would expect W to get close to L ' , and it could be tempting to consider using W=L ' -this is the `` not quite '' part- . However , this may not be feasible from a practical perspective , because we potentially wish the classifier to be relevant to the current dataset , and not too much to an externally learned metric , for compatibility reasons -this is the `` not incorrect '' part- . You are certainly right in that in order to tie the embeddings and the classifier , one would need to use the the word embeddings of the model itself , and not an external embedding matrix . Then again , the former is what our framework is based on , and it is the setting where we can make such plausible technical arguments . Minor comments : Comment : Third line in abstract : where model - > where the model , Second line in section 7 : into space - > into the space Response : Just corrected this in our internal version . We appreciate it , thank you . Comment : Should n't the RHS in Eq 3.5 be \\sum \\tilde { y_ { t , i } } ( \\frac { \\hat { y } _t } { \\tilde { y_ { t , i } } } - e_i ) ? Response : That is correct ; in fact y~ should not have been factored out . We fixed it as a direct multiplication with e_i . Thank you very much for pointing this out . References : [ 1 ] Hakan Inan and Khashayar Khosravi . Improved learning through augmenting the loss , 2016 [ 2 ] Geoffrey Hinton , Oriol Vinyals , and Jeff Dean . Distilling the knowledge in a neural network , 2015 [ 3 ] Charlie Frogner , Chiyuan Zhang , Hossein Mobahi , Mauricio Araya , and Tomaso A Poggio . Learning with a wasserstein loss , 2015 * * This comment was slightly edited after initial posting for clarity * *"}, {"review_id": "r1aPbsFle-1", "review_text": "This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax. Experiments on PTB shows significant improvement. The idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting. I was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted. Have you considered using character or sub-word units in that context? ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your comments . We address your comments/questions separately below . Comment : The idea of sharing or tying weights between input and output word embeddings is not new ( as noted by others in this thread ) , which I see as the main negative side of the paper . The proposed justification appears new to me though , and certainly interesting . Response : We 're pleased to be able to offer a convincing mechanism for tying the embeddings and the classifier ; indeed , this framework is our main contribution . Our work investigates this new framework in three directions , leading to the following : ( 1 ) Tying is a natural outcome of applying this framework ( 2 ) For relatively small models , using an additional loss function improves learning ( 3 ) Tying alone leads to significant improvements in a broader context . Regarding the third point : Although we certainly agree that using a shared representation for inputs and outputs is not a new idea , we find it worth mentioning that the possibility of improvement by doing so was not particularly explored previously . Rather , shared representations were employed as constituent elements of the underlying models . Comment : I was concerned that results are only given on one dataset , PTB , which is now kind of old in that literature . I 'm glad the authors tried at least one more dataset , and I think it would be nice to find a way to include these results in the paper if accepted . Response : We would like to thank you for raising this point previously ; we have since been working on optimizing all our models ( hyper-parameters ) for the wikitext-2 dataset , and we have achieved satisfactory results which are in line with our predictions . Hopefully we will add the results without causing much additional reading burden for the reader . We really appreciate your feedback on this matter . Question : Have you considered using character or sub-word units in that context ? Answer : This is a very good question ; however we predict that the framework wo n't provide as significant gains in character based models . Our reasoning is as follows : Our loss framework aims to exploit the metric attached to the elements in the vocabulary . In the case of characters , the vocabulary is smaller and it perhaps carries less meaning . For instance , one might expect `` terrific '' and `` great '' to be in very close proximity in English language , but it 's not as reasonable to expect a particular pair of characters to be very close together , with certain exceptions . And even in the case where it might be possible ( some languages might have rules with regard to character ordering ) , there usually is n't enough volume in the set of characters to offer very serious gains . Also , the small vocabulary size will possibly reduce the effect of tying significantly owing to losing the advantage of reducing the redundant complexity . Finally , the large effective increase in the training set by reducing to characters will potentially eliminate the advantage of using the augmented loss ( since AL is most effective when model can use better estimated output statistics ) . Most arguments in this paragraph are also valid for sub-word units , but of course , the situation could be different with languages which contain many prefixed and suffixed words , and when the tokenization is done according to the language rules ."}, {"review_id": "r1aPbsFle-2", "review_text": "This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding. I see two main drawbacks from this framework: The augmented loss function has no trainable parameters and is used for only for regularization. This is not expected to give gains with large enough datasets. The augmented loss is heavily \u201cengineered\u201d to produce the desired result of parameter tying. It\u2019s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way. Nevertheless the argument is very interesting, and clearly written. The simulated results indeed validate the argument, and the PTB results seem promising. Minor comments: Section 3: Can you clarify if y~ is conditioned on the t example or on the entire history. Eq. 3.5: i is enumerated over V (not |V|) ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for your kind review . We address your major and minor comments below . Response to the major comments : The augmented loss is meant to be a light machinery on top of inner products in the euclidean space of word embeddings . Provided that we stick to the notion of word similarity for estimating a target distribution , we believe that this method ( i.e.computing inner products and normalizing via softmax ) is a simple and natural way to obtain a y~ . Having obtained a distribution , we believe that minimizing a KL divergence term is a natural next step . We would like to also note here that this method was used by authors in [ 1 ] for obtaining an augmented loss term , although their method is proposed for a different problem . Despite being simple and not having many parameters , we believe it is an expressive term . We certainly agree that augmented loss might not be as effective on large datasets , but we argue that this is also due to large datasets having enough information to mitigate the supervision problem ( we discuss this also below in response to a reviewer question ) . As a final remark regarding the efficacy of the augmented loss , in addition to PTB we observe non-trivial improvements with the augmented loss on Wikitext-2 , which is a larger dataset ( results of which we will include soon in a forthcoming revision ) . To comment on different ways for introducing the augmented loss , we believe that this is a very well made point and that it is definitely worth pursuing given the improvement which we are able to achieve by investigating it in a certain direction . This would potentially be pursued in two steps : ( 1 ) Decide how to find a y~ . Although we believe that computing inner products and applying softmax is a natural way for exploiting the metric similarity , one can proceed in other ways . Frankly , we did not initially intend to reach tying by selecting a loss function ( we did not build this link in our earlier work ) ; however our feeling is that any method which uses euclidean distance between words could easily lead to tying . The reasoning is the following : The classification step of the language model is essentially a computation of inner products between the output hidden state and the classifier weights . Choosing classifier weights as word vectors is a compelling choice to enforce similarity since the loss function aims to distribute its predictions according to word similarity . Of course , to show this mathematically , one needs to be able to show something similar to eqn . 4.5 in our paper ( i.e.L =~ W ) .Based on our derivation , one can possibly attain such an identity provided that the mapping from the word embeddings to y~ is differentiable and could be well represented by its first order approximation ( e.g.its derivative is Lipschitz with a controllable constant ) . The idea , also exploited in our derivation , is to be able to obtain a linear mapping from L to W. ( 2 ) Using a loss other than KL or cross entropy . This could be very interesting ; in fact one can directly construct a loss function with the metric space ( word embeddings ) built into it ( a very simple example is generalized euclidean distance ) . However , practically speaking , one should be careful to control the interaction between the two loss terms . For instance , depending on the phase in training one can take over and cause instabilities or null results . My collaborators and I have also been interested in exploring different options , and we would be very glad to see other works researching losses which exploit metric similarities in language processing . Responses to minor comments : Comment : Can you clarify if y~ is conditioned on the t example or on the entire history . Response : y~ is indeed conditioned on the history , and not just the t'th example . This is a recurring mistake in the paper , and we have made the necessary corrections in our internal version . Comment : Eq.3.5 : i is enumerated over V ( not |V| ) Response : Also corrected , thank you for pointing this out . References : [ 1 ] Geoffrey Hinton , Oriol Vinyals , and Jeff Dean . Distilling the knowledge in a neural network , 2015"}], "0": {"review_id": "r1aPbsFle-0", "review_text": "This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7. It is not obvious why the projection matrix L in Eq 3.6 (let's rename it to L') should be the same as that in Eq. 2.1. For example, L' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L' is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer, as is usually done. The experimental results are good and provide support for the approximate derivation done in section 4, particularly the distance plots in figure 1. Minor comments: Third line in abstract: where model -> where the model Second line in section 7: into space -> into the space Shouldn't the RHS in Eq 3.5 be \\sum \\tilde{y_{t,i}}(\\frac{\\hat{y}_t}{\\tilde{y_{t,i}}} - e_i) ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your kind assessment . We list below the responses to your major and minor comments : Major comments : Comment : The construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs . 3.6 and 3.7 . Response : Although the analysis in the paper may naturally lead one to believe that this is the case , we would like to note that this work was not initiated with the intention of finding an explanation for sharing the embeddings and the classifier . In fact , in our early report in [ 1 ] , we did not establish such a tight analytical link between the augmented loss and reusing word embeddings although we introduced them both . We also certainly hope that our work could be primarily appreciated for the particular framework which exploits metric similarity of words which it introduces . At this point , would like to also explain our motives for eqns 3.6 and 3.7 , namely the calculation of y~ . Eqn 3.6 is meant to select the word vector corresponding to the target token . Eqn 3.7 is a very elementary way to capture the word similarities through inner products and then normalize the similarity vector to be interpreted as a probability distribution ; this normalization was in fact used in [ 2 ] followed by a KL distance term for a different problem . From our perspective ( which certainly includes some authorship bias ) , the fact that this simple and plausible method leads to tying ( even if seemingly obviously ) points to the intuitiveness of shared embeddings in language processing tasks . Comment : It is not obvious why the projection matrix L in Eq 3.6 ( let 's rename it to L ' ) should be the same as that in Eq.2.1.For example , L ' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters . Response : This is exactly right ; and in fact , we do not claim that metric used for the augmented loss should clearly be the word embeddings of the model . Our method is simply by choice , guided by observations . As a matter of fact , very early on in our work ( this is when we did not have tying in mind , and were experimenting with the augmented loss idea ) and inspired by the work in [ 3 ] , we used both word2vec and gloVe embeddings ( both 300 dimensional ) as static metrics . However , the performance was significantly inferior to that when using the word embeddings of the model . In the interest of full disclosure , we should mention that we did not meticulously tune the models with static embeddings after not being able to see improvement ( and we only tried them on PTB ) ; however , the results did not seem promising . Instead , we decided to adopt the idea of information transfer within the model , and this lead to the current framework . The experimental performance of tying alone on large networks might slightly overshadow this core idea , but it was this idea that enabled us to reach tying , and link our two improvements . Comment : In the case that L ' is a new learned matrix , it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer , as is usually done . Response : Actually , not quite - although the above is not an incorrect conclusion . We will address this comment in the context of sharing embeddings . One takeaway from Section 4 is that the output projection matrix , W , will tend to the matrix used for capturing the word metric ( in the subspace sense ) . In this case , this matrix is L ' . Therefore , under the same assumptions , one would expect W to get close to L ' , and it could be tempting to consider using W=L ' -this is the `` not quite '' part- . However , this may not be feasible from a practical perspective , because we potentially wish the classifier to be relevant to the current dataset , and not too much to an externally learned metric , for compatibility reasons -this is the `` not incorrect '' part- . You are certainly right in that in order to tie the embeddings and the classifier , one would need to use the the word embeddings of the model itself , and not an external embedding matrix . Then again , the former is what our framework is based on , and it is the setting where we can make such plausible technical arguments . Minor comments : Comment : Third line in abstract : where model - > where the model , Second line in section 7 : into space - > into the space Response : Just corrected this in our internal version . We appreciate it , thank you . Comment : Should n't the RHS in Eq 3.5 be \\sum \\tilde { y_ { t , i } } ( \\frac { \\hat { y } _t } { \\tilde { y_ { t , i } } } - e_i ) ? Response : That is correct ; in fact y~ should not have been factored out . We fixed it as a direct multiplication with e_i . Thank you very much for pointing this out . References : [ 1 ] Hakan Inan and Khashayar Khosravi . Improved learning through augmenting the loss , 2016 [ 2 ] Geoffrey Hinton , Oriol Vinyals , and Jeff Dean . Distilling the knowledge in a neural network , 2015 [ 3 ] Charlie Frogner , Chiyuan Zhang , Hossein Mobahi , Mauricio Araya , and Tomaso A Poggio . Learning with a wasserstein loss , 2015 * * This comment was slightly edited after initial posting for clarity * *"}, "1": {"review_id": "r1aPbsFle-1", "review_text": "This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax. Experiments on PTB shows significant improvement. The idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting. I was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted. Have you considered using character or sub-word units in that context? ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your comments . We address your comments/questions separately below . Comment : The idea of sharing or tying weights between input and output word embeddings is not new ( as noted by others in this thread ) , which I see as the main negative side of the paper . The proposed justification appears new to me though , and certainly interesting . Response : We 're pleased to be able to offer a convincing mechanism for tying the embeddings and the classifier ; indeed , this framework is our main contribution . Our work investigates this new framework in three directions , leading to the following : ( 1 ) Tying is a natural outcome of applying this framework ( 2 ) For relatively small models , using an additional loss function improves learning ( 3 ) Tying alone leads to significant improvements in a broader context . Regarding the third point : Although we certainly agree that using a shared representation for inputs and outputs is not a new idea , we find it worth mentioning that the possibility of improvement by doing so was not particularly explored previously . Rather , shared representations were employed as constituent elements of the underlying models . Comment : I was concerned that results are only given on one dataset , PTB , which is now kind of old in that literature . I 'm glad the authors tried at least one more dataset , and I think it would be nice to find a way to include these results in the paper if accepted . Response : We would like to thank you for raising this point previously ; we have since been working on optimizing all our models ( hyper-parameters ) for the wikitext-2 dataset , and we have achieved satisfactory results which are in line with our predictions . Hopefully we will add the results without causing much additional reading burden for the reader . We really appreciate your feedback on this matter . Question : Have you considered using character or sub-word units in that context ? Answer : This is a very good question ; however we predict that the framework wo n't provide as significant gains in character based models . Our reasoning is as follows : Our loss framework aims to exploit the metric attached to the elements in the vocabulary . In the case of characters , the vocabulary is smaller and it perhaps carries less meaning . For instance , one might expect `` terrific '' and `` great '' to be in very close proximity in English language , but it 's not as reasonable to expect a particular pair of characters to be very close together , with certain exceptions . And even in the case where it might be possible ( some languages might have rules with regard to character ordering ) , there usually is n't enough volume in the set of characters to offer very serious gains . Also , the small vocabulary size will possibly reduce the effect of tying significantly owing to losing the advantage of reducing the redundant complexity . Finally , the large effective increase in the training set by reducing to characters will potentially eliminate the advantage of using the augmented loss ( since AL is most effective when model can use better estimated output statistics ) . Most arguments in this paragraph are also valid for sub-word units , but of course , the situation could be different with languages which contain many prefixed and suffixed words , and when the tokenization is done according to the language rules ."}, "2": {"review_id": "r1aPbsFle-2", "review_text": "This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding. I see two main drawbacks from this framework: The augmented loss function has no trainable parameters and is used for only for regularization. This is not expected to give gains with large enough datasets. The augmented loss is heavily \u201cengineered\u201d to produce the desired result of parameter tying. It\u2019s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way. Nevertheless the argument is very interesting, and clearly written. The simulated results indeed validate the argument, and the PTB results seem promising. Minor comments: Section 3: Can you clarify if y~ is conditioned on the t example or on the entire history. Eq. 3.5: i is enumerated over V (not |V|) ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for your kind review . We address your major and minor comments below . Response to the major comments : The augmented loss is meant to be a light machinery on top of inner products in the euclidean space of word embeddings . Provided that we stick to the notion of word similarity for estimating a target distribution , we believe that this method ( i.e.computing inner products and normalizing via softmax ) is a simple and natural way to obtain a y~ . Having obtained a distribution , we believe that minimizing a KL divergence term is a natural next step . We would like to also note here that this method was used by authors in [ 1 ] for obtaining an augmented loss term , although their method is proposed for a different problem . Despite being simple and not having many parameters , we believe it is an expressive term . We certainly agree that augmented loss might not be as effective on large datasets , but we argue that this is also due to large datasets having enough information to mitigate the supervision problem ( we discuss this also below in response to a reviewer question ) . As a final remark regarding the efficacy of the augmented loss , in addition to PTB we observe non-trivial improvements with the augmented loss on Wikitext-2 , which is a larger dataset ( results of which we will include soon in a forthcoming revision ) . To comment on different ways for introducing the augmented loss , we believe that this is a very well made point and that it is definitely worth pursuing given the improvement which we are able to achieve by investigating it in a certain direction . This would potentially be pursued in two steps : ( 1 ) Decide how to find a y~ . Although we believe that computing inner products and applying softmax is a natural way for exploiting the metric similarity , one can proceed in other ways . Frankly , we did not initially intend to reach tying by selecting a loss function ( we did not build this link in our earlier work ) ; however our feeling is that any method which uses euclidean distance between words could easily lead to tying . The reasoning is the following : The classification step of the language model is essentially a computation of inner products between the output hidden state and the classifier weights . Choosing classifier weights as word vectors is a compelling choice to enforce similarity since the loss function aims to distribute its predictions according to word similarity . Of course , to show this mathematically , one needs to be able to show something similar to eqn . 4.5 in our paper ( i.e.L =~ W ) .Based on our derivation , one can possibly attain such an identity provided that the mapping from the word embeddings to y~ is differentiable and could be well represented by its first order approximation ( e.g.its derivative is Lipschitz with a controllable constant ) . The idea , also exploited in our derivation , is to be able to obtain a linear mapping from L to W. ( 2 ) Using a loss other than KL or cross entropy . This could be very interesting ; in fact one can directly construct a loss function with the metric space ( word embeddings ) built into it ( a very simple example is generalized euclidean distance ) . However , practically speaking , one should be careful to control the interaction between the two loss terms . For instance , depending on the phase in training one can take over and cause instabilities or null results . My collaborators and I have also been interested in exploring different options , and we would be very glad to see other works researching losses which exploit metric similarities in language processing . Responses to minor comments : Comment : Can you clarify if y~ is conditioned on the t example or on the entire history . Response : y~ is indeed conditioned on the history , and not just the t'th example . This is a recurring mistake in the paper , and we have made the necessary corrections in our internal version . Comment : Eq.3.5 : i is enumerated over V ( not |V| ) Response : Also corrected , thank you for pointing this out . References : [ 1 ] Geoffrey Hinton , Oriol Vinyals , and Jeff Dean . Distilling the knowledge in a neural network , 2015"}}