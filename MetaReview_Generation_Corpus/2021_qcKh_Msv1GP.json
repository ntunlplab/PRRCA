{"year": "2021", "forum": "qcKh_Msv1GP", "title": "Motif-Driven Contrastive Learning of Graph Representations", "decision": "Reject", "meta_review": "This paper presents a pre-training strategy for learning graph representations using a graph-to-subgraph contrastive learning objective that also simultaneously discovers motifs. Pre-training for graph representation learning is an important research topic and this work presents a unique solution leveraging the fact that graphs sharing a lot of motifs should be similar to one another. The approach is novel and interesting, the ability to simultaneously identify motifs are highly desirable. The results are promising showing that the proposed approach, when pretrained on the ogbn-molhiv molecule dataset, worked well for several downstream chemical property prediction tasks. \n\nHowever, the paper is not without weaknesses and the reviewers noticed several of them. There are many parts of the system, the graph segmenter, which relies on spectral clustering (on the affinity matrix), the EM style clustering component to extract the motifs based on the subgraphs,  the sampling loss based on the subgraph-to-motif similarity, and the graph-to-subgraph contrastive learning loss. These parts are tied together through different mechanisms and the training procedure becomes very confusing. It is unclear which parts are updated on the backpropagation path from which loss, and what choices are decided offline (i.e., not integrated into the backpropagation).  This presents great difficulty in understanding and probably using /building-on the method. The paper has improved some aspects of its presentation during the review/discussion process, but the training/optimization procedure of the current version still appears quite opaque, and the reviewers heavily relied on the back and forth discussion to understand what is really going on. \n\nAnother concern is that the intuition behind some aspects of the approach and the connections between different components of the approach are a bit difficult to get/digest at places.  The intuition behind graph to subgraph contrastive learning appeared weak to the reviewer. It would be desirable to see a directly comparison to the subgraph-to-subgraph version. The connection between the motif discovery and the representation learning can be somewhat lost as we try to keep the many moving parts straight in the mind.   For these reasons, the paper, in its current form, cannot be accepted.", "reviews": [{"review_id": "qcKh_Msv1GP-0", "review_text": "=Summary= In this work , the authors study how to leverage motif discovery to learn graph representations . MICRO-Graph is proposed to enable simultaneous motif discovery and graph representation learning . Empirical results on public benchmark datasets suggest the effectiveness of the proposed method . =Reason for the rating= At this moment , I am leaning to reject . Overall , the technical quality is my main concern . This paper 's presentation also makes its technical details confusing . Hopefully , the authors could address my concern in the rebuttal . =Strong points= 1 . The authors investigate the problem of graph learning from a unique angle of motif discovery . 2.The authors propose the MICRO-Graph framework that enables simultaneous motif discovery and graph representation learning . 3.From multiple public benchmark datasets , the empirical results suggest the proposed technique could be promising in graph classification tasks . =Weak points= 1 . My major concern is on the technical quality of this paper . - Graph representations generated by a node embedding system may not be comparable across different graphs . A node embedding system is assumed to be available in MICRO-Graph . In most cases , a node embedding system is trained in an unsupervised manner . The generated node representations usually only work in a transductive setting such that the learned model may not be able to be generalized for unseen graphs . In other words , node embedding from different graphs may not be comparable in a meaningful way , although they are vectors of numerical values . For the input node representations , the authors may need to clarify how the node representations are obtained , and why they are comparable across different graphs . - The authors may need to clarify whether MICRO-Graph could tune the parameters in the assumed node embedding system . If the parameters in the embedding system are trainable , the authors may need to discuss more on the assumed embedding system and how the parameters impact node representations . If the parameters in the embedding system are fixed , the subgraph segmenter basically generates fixed subgraphs by spectral clustering , and it is difficult to see the point of Equation ( 6 ) . In sum , without clarification , the technical discussion is confusing . - For the motif discovery discussed in 3.2 , it seems to be a typical clustering problem . The authors may need to clarify what the unique aspect is in the proposed method . - For the discussion in 3.4 , it is hard to see why it is reasonable to use subgraph relations to define positive/negative sets , as the difference between a graph and its subgraph could be significant in many cases . - In section 4.1.1 , the term `` fine-tune '' is mentioned . Could the authors provide more details on fine-tuning ? 2.It is still unclear how motif discovery impacts graph representations . As graph representations are the mean over node representations , the coupling between motif discovery and graph representations seems to be weak . 3.The empirical evaluation could be stronger . - For the evaluation in Table 1 , `` direct supervised learning '' should consider existing GNN techniques , such as GraphSage , GAT , GIN , and so on . - For the assumed node embedding system , the authors may evaluate how different node embedding systems impact the proposed method . In addition , the symbol usage in the presentation makes the paper hard to read . =Questions during rebuttal period= Please address and clarify the weak points above . =Post rebuttal= I appreciate the authors ' great effort on answering my questions . The response clears many confusing points from the original draft . Meanwhile , I still have concerns on how the idea of contrastive learning is handled in this paper , which could have been better shaped . In sum , I have increased the rating accordingly .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Regarding the questions you raised . Please see our answers below . Q : Compare representations across different graphs A : Since we are doing GNN pre-training , we are not explicitly learning a node embedding table for each node , but GNN as a mapping function f is learned . Suppose we want to get node embeddings E given a graph G , we can use f to generate the embedding , e.g.E = f ( G ) .As long as the node features are in the same domain across different graphs , e.g.molecules share atoms , and papers in citation graphs share words , f will map them into the same space . We don \u2019 t consider cross domain training and evaluation , say , pre-train with the citation graph and evaluate on molecules . Q : Tune node embedding parameters A : As we described above , we are not learning a node embedding system explicitly . To get the node embeddings , we just embed the node ( graph ) using the GNN , e.g.E = f ( G ) .Since the parameters of the GNN will be updated , i.e.f will change , the node embeddings will change as well , and thus `` tunable '' in this sense . Our loss function , including equation ( 6 ) , is meant to provide learning signals for the GNN parameters to update . Q : The unique aspect of the proposed method A : Simple clustering only finds clustering labels of fixed data points . In the case of representation learning , each point we want to cluster is a representation and will change during learning . The purpose of clustering here is to guide the representation learning . Clustering guidance has been shown to be helpful for representation learning [ 7,8 ] . At the same time , we want to keep the representations from degenerate solutions , e.g.collapse to a single point , which is a harder task than simple clustering . Q : Subgraph relation for positive/negative sets A : For a whole graph G1 , we first use the GNN to do message passing on the G1 and generate all the contextualized node embeddings . Both the embedding of G1 and a sampled subgraph S1 are aggregations of those node embeddings . During the whole graph message passing stage , the nodes within S1 have collected information from other nodes not in S1 , so that they contain information of the whole graph G1 , and can be treated as a positive sample to G1 . On the other hand , a subgraph S2 of a different whole graph G2 has never gone through message passing with any node in G1 . Therefore , S2 and G1 form a negative pair . Q : Details on fine-tuning . A : Fine-tuning is a conventional evaluation method for a pre-trained model . During the pre-training stage , we train a GNN by a self-supervised task on a large dataset , usually without labels , this GNN can be leveraged to benefit downstream tasks by `` fine-tuning '' . During the fine-tune stage , usually on a smaller supervised dataset , we train the pre-trained GNN using the supervised labels , i.e.we fine tune parameters in it . The high-level idea is that general knowledge of the data has been learned during the pre-train stage . Only a few extra labels are needed to further tune it to adapt to a supervised task . For example on molecule data , during pre-training the GNN has already learned generic knowledge of molecules not restricted to any specific property . During fine-tuning , for questions regarding a specific chemical property , we only need a small labeled dataset to teach the pre-trained GNN about it . Q : How motif discovery impacts graph representation ? A : As we described above , the motif discovery part will guide the GNN parameters to update , and thus update the node embeddings and further graph embeddings . In particular , equation ( 6 ) was meant to make nodes belong to a subgraph to have similar embeddings if this subgraph is similar to a learned motif . We also showed in our ablation study that heuristic sample methods like random walk are hard to generalize , which demonstrates the importance of the motif guidance . Q : Empirical evaluation . A : The MICRO-Graph framework is built on GNNs , but not explicit node embedding systems like node2vec . Also , our framework can work with any GNN model architectures , like GraphSage , GIN , and etc . The results shown in the current version are based on the DeeperGCN model . In general , we summarize our method and highlight our contribution as the following . 1.Our representation learning is under the pre-train framework , and our contribution is a novel pre-training strategy . 2.Besides learning representations , our method can also extract meaning motifs We hope our answer has made things clear . We are happy for any further discussions or questions , so please feel free to let us know if any of the concerns are not fully addressed . [ 7 ] Caron , M. , Bojanowski , P. , Joulin , A. , Douze , M. : Deep clustering for unsupervised learning of visual features . In : Proceedings of the European Conference on Computer Vision ( ECCV ) ( 2018 ) [ 8 ] Asano YM. , Rupprecht C. , and Vedaldi A. Self-labelling via simultaneous clustering and representation learning . In International Conference on Learning Representations ( ICLR ) , 2020"}, {"review_id": "qcKh_Msv1GP-1", "review_text": "This paper proposes to learn the sub-graph patterns from a collection of training graphs . The key idea is to partition each graph into segments and enforce a global clustering of the subgraphs . The partitioning is also guided through contrastive learning , i.e. , subgraphs should have a larger similarity with the graph it is drawn from , compared with other graphs . The learned GNN ( that generates node embedding ) will then be used to some downstream learning tasks with or without further fine-tuning . The notation of the paper is very hard to follow . For example , no where is N defined , which I assume is the number of sug-graphs . Capital latters S_i \u2019 s represent vector ( not sure if my guess is correct ) , as lower-case letter q_j \u2019 s , which somewhat affects the reading . Also the definition of n_i , and n_ { i , s } is not quite following the custom of matrices . Furthermore , what is the relation between s_i and S_i ( both board ) , and what is the difference between S ( bold ) and S ( not bold ) ? The h_i is defined but not used , and in equation ( 7 ) g_i is used , which I assume is the summary embedding vector for graph G_i ; in ( 7 ) s_i seems to be subgraph embedding vector , and this makes me more confused about the s_ { j , k } appearing in the beginning of section 3.2 which is defined as the cosine similarity between subgraph j and motif k. The E [ G_i ] is with bold E somewhere and non-bold letter E elsewhere . Altogether , there are S ( bold ) , S ( not bold ) , s_ { j , k } ( bold ) that seems to be a similarity measure , and s_i ( bold ) which seems to represent subgraph vector . It \u2019 s quite confusing to me . Can authors clearly define these symbols when they are used , and make sure they are consistent throughout the paper , and explicitly mark their dimensionality to avoid confusion ? Usually upper-case letters are for matrices and lower-case bold letters for vectors . The idea of partitioning graphs into sub-graphs are a useful idea in breaking the complexity of graph-structured objects and exploring the potential hierarchical organizations of the graph . Using contrastive learning as a self-supervision may further improve the partitioning of each graph . However , the grouping part and the contrastive part may conflict with each other in that some sub-graphs are shared among different graphs , which can be quite common in chemical compounds . Under ( 5 ) , it is mentioned that spectral clustering is used to partition the graphs ; is it done end-to-end and if so where is the loss function corresponding to this operation ? In ( 6 ) , ( s , t ) in g ( i , j ) : What is g ( i , j ) in particular ? a threshold eta is used in the indicator function and how to choose eta ( considering that it is used directly on a set of variables ) ? Is ( 6 ) end-to-end optimizable ? In Figure~2 , what is meant by the blue and red markers ( like + and - ) ? Experimental results are quite strange in that the transfer learning setting ( which further finetunes the learned GNN based on a small set of labels , as in Table 1 ) leads to even worse performance than the feature extraction setting ( in which no fine-tuning is performance , as shown in Table2 ) and the gap can be as huge as 15 % in accuracy !", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q : Spectral clustering for partition graphs and the segmenter loss ( equation ( 6 ) ) A : The spectral clustering for partition graphs is not end-to-end . It is only used for grouping nodes into subgraphs , and the subgraph assignment is only served as \u201c labels \u201d for the loss function in equation ( 6 ) to encourage nodes within the same cluster to have closer embeddings . Specifically , Given a graph $ G_i $ with n nodes , we first constructs a n-by-n node-node affinity matrix $ A^ { ( i ) } $ , and then apply spectral clustering to this matrix $ A^ { ( i ) } $ , as if $ A^ { ( i ) } $ is a n-vertices complete graph with affinity scores as edge weights . Then we are done with spectral clustering . The segmenter loss term $ L_s $ in equation ( 6 ) is meant to make nodes belong to a subgraph ( predicted by spectral clustering ) to have similar node embeddings , if this subgraph is similar to a learned motif . Q : How to choose $ \\eta $ ? $ \\eta $ is the threshold for deciding whether a subgraph is similar enough to a learned motif . It is dynamically set to take the top 10 % most similar subgraphs to each motif . We hope this part is clear in the updated paper version , please refer to the pseudocode in Appendix A as well . Q : Blue and red markers in Figure 2 A : For contrastive learning , we need to define positive pairs and negative pairs . In Figure 2 , when two arrows are pointing a red plus sign , the two objects these arrows are referring to are considered as positive pairs . Vice versa for the negative pairs . Q : Experimental results A : We are sorry for the confusion caused by the fine tune evaluation and linear evaluation . The original Table1 and Table2 in the paper were not directly comparable , as they were meant to compare with two previous works [ 1 ] and [ 2 ] . Therefore , we closely followed the settings of these two works respectively , where the evaluation metric of Table1 was AUC-ROC score and the evaluation score of Table 2 was accuracy . We have now updated Table2 , so both cases use AUC-ROC score . The old version of Table2 has been moved to Appendix . Below we show the updated Table 2 with AUC-ROC score . |SSL methods | bace | bbbp |clintox |hiv |sider | tox21 |toxcast |Avg| |-| -- | -- |-|-|-|-|-|-| |ContextPred | 53.09 \u00b1 0.84 | 55.51 \u00b1 0.08 | 40.73 \u00b1 0.02 | 53.31 \u00b1 0.15 | 52.28 \u00b1 0.08 | 35.31 \u00b1 0.25 | 47.06 \u00b1 0.06 | 48.18 | |InfoGraph | 66.06 \u00b1 0.82 | 75.34 \u00b1 0.51 | 75.71 \u00b1 0.53 | 61.45 \u00b1 0.74 | 54.7 \u00b1 0.24 | 63.95 \u00b1 0.24 | 52.69 \u00b1 0.07 | 64.27 | |GPT-GNN | 59.43 \u00b1 0.66 | 71.58 \u00b1 0.54 | 62.78 \u00b1 0.58 | 64.08 \u00b1 0.36 | 54.67 \u00b1 0.16 | 68.2 \u00b1 0.14 | 57.06 \u00b1 0.13 | 62.53 | |MICRO-Graph | 69.54 \u00b1 0.39 | 81.07 \u00b1 0.42 | 63.69 \u00b1 0.56 | 72.74 \u00b1 0.15 | 55.39 \u00b1 0.26 | 72.91 \u00b1 0.12 | 61.04 \u00b1 0.07 | 68.05| The updated linear evaluation result shows that our framework outperforms all three baselines . The comparison between linear evaluation and fine-tune evaluation is also consistent with our expectation ( and also yours~ ) . As you pointed out , the linear results should be lower than the fine-tune result . In particular , MICRO-Graph \u2019 s performance is 5.14 % lower than fine-tune on average , and for three baselines , the average performance is lower by 22.47 % , 7.84 % , and 9.65 % respectively . This shows that MICRO-Graph has learned more robust representations during pre-training , and thus a simple linear classifier on top of it can achieve reasonably good results . We hope our answer has made things clear . We are happy for any further discussions or questions , so please feel free to let us know if any of the concerns are not fully addressed . Reference [ 1 ] Weihua Hu , Bowen Liu , Joseph Gomes , Marinka Zitnik , Percy Liang , Vijay Pande , and Jure Leskovec . Strategies for pre-training graph neural networks . In International Conference on Learning Representations , 2020b [ 2 ] Fan-Yun Sun , Jordan Hoffmann , Vikas Verma , and Jian Tang . Infograph : Unsupervised and semi- supervised graph-level representation learning via mutual information maximization , 2019 ."}, {"review_id": "qcKh_Msv1GP-2", "review_text": "Overall : This paper proposes an interesting framework + It extracts subgraph ( s ) for each graph from node affinity matrix and spectral clustering , together with the help of motifs . + It learns the motifs by clustering the subgraphs . + It applies contrastive self-supervised learning on the graph-subgraph pair . + It overcomes the combinatorial problem by learning the motifs on the continuous representation space . Strengths : + The idea is novel and seems promising . + This paper is technically correct . + Nice visualization of the learned motifs . Weaknesses : + This paper is not well written , especially the notations . I list the main concerns here , and hope the authors can help clarify them later . 1.S3 , N is not defined , and M is defined as # graph . But some following sections are implying N as # graph and M as # subgraph . 2.S3.2 , should be ' ... we can extract M subgraphs \u2026 ' 3 . S3.2 , according to 'sampling a subgraph from a graph ' , does this mean each graph has one and only one subgraph ? 4.S3.2 , \u2018 ... apply the subgraph index as a mask to its subgraph embedding \u2026 \u2019 I can understand the following equation but not this sentence . 5.S3.2 , should be ' { ... , m_K } ' 6 . Actually without clarifying the above detailed notations , it \u2019 s a little hard to follow the remaining ( sub ) sections . e.g. , what are S and Q ? Because M is # subgraph , and it should be [ ... , S_M ] and [ ... , q_M ] according to the descriptions . 7.S3.3 , { g_ { i , j } } _ { j=1 } ^M , I guess this is saying each graph can have multiple groups/subgraphs and such mapping is represented in the N * M binary matrix . Then it contradicts with the ( 3 ) mentioned above , with 1-1 mapping between the two views . 8.In Eq 5 , not sure if softmax_s matches with the description above . 9.In Eq 6 , it would be better to add \u2018 1 \\le k \\le K \u2019 in the last term . + Experiments , authors can consider adding more baselines . 1.GROVER [ 1 ] is the SOTA , where it randomly masks a subgraph , which is highly relevant to this paper . 2.GNNExplainer [ 2 ] is learning the motif in an end-to-end way , the authors could also consider comparing with it . Recommendation : Considering the notation issues listed above and lack of baselines , I would encourage the authors to polish up this paper . It has the potential to be a much better paper . For now , I would reject the current version . Questions : + OGB [ 3 ] was first released in May 2020 , and GROVER [ 1 ] was released one month after that . So I think at least the authors should cite [ 1 ] . + In S1 , \u2018 Previous approaches , such as \u2026 \u2019 Deep Graph Informax and InforGraph , in the last layer it is indeed node-to-graph views , but if we take it under the GNN setting , where each node representation in the last layer actually encodes a K-hop neighborhood around that node , then it is subgraph-to-graph views . + The ContextPred in Figure 2 is not correct : it should be contrasting the k-hop neighborhood and pre-defined context graph . Check Figure 2 in [ 4 ] . + In Table 1 , the ContextPred is the worst , which is not expected based on my own experience , any reason why ? I couldn \u2019 t find it in the supplementary files . + Comparing Table 1 and 2 , the frozen pre-trained GNN seems to be much better to the fine-tuning ones . Can authors discuss this further ? + Figure 3 to 6 are blurry . [ 1 ] https : //arxiv.org/abs/2007.02835 [ 2 ] https : //arxiv.org/abs/1903.03894 [ 3 ] https : //arxiv.org/abs/2005.00687 [ 4 ] https : //arxiv.org/abs/1905.12265", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q : Counter-intuitive experimental results in Table 1 and Table 2 A : We are sorry for the confusion caused by the fine-tune evaluation and linear evaluation . The original Table1 and Table2 in the paper were not directly comparable , as they were meant to compare with two previous works [ 1 ] and [ 2 ] . Therefore , we closely followed the settings of these two works respectively , where the evaluation metric of Table1 was AUC-ROC and the evaluation of Table 2 was accuracy . We have now updated Table2 , so both cases use AUC-ROC . The old version of Table2 has been moved to Appendix . A copy of Table2 is shown below . |SSL methods | bace | bbbp |clintox |hiv |sider | tox21 |toxcast |Avg| |-| -- | -- |-|-|-|-|-|-| |ContextPred | 53.09 \u00b1 0.84 | 55.51 \u00b1 0.08 | 40.73 \u00b1 0.02 | 53.31 \u00b1 0.15 | 52.28 \u00b1 0.08 | 35.31 \u00b1 0.25 | 47.06 \u00b1 0.06 | 48.18 | |InfoGraph | 66.06 \u00b1 0.82 | 75.34 \u00b1 0.51 | 75.71 \u00b1 0.53 | 61.45 \u00b1 0.74 | 54.7 \u00b1 0.24 | 63.95 \u00b1 0.24 | 52.69 \u00b1 0.07 | 64.27 | |GPT-GNN | 59.43 \u00b1 0.66 | 71.58 \u00b1 0.54 | 62.78 \u00b1 0.58 | 64.08 \u00b1 0.36 | 54.67 \u00b1 0.16 | 68.2 \u00b1 0.14 | 57.06 \u00b1 0.13 | 62.53 | |GROVER|65.67\u00b10.38|78.47\u00b10.36|53.19\u00b10.68|69.03\u00b10.23|54.94\u00b10.12|67.63\u00b10.13|57.28\u00b10.05| 63.74| |MICRO-Graph | 69.54 \u00b1 0.39 | 81.07 \u00b1 0.42 | 63.69 \u00b1 0.56 | 72.74 \u00b1 0.15 | 55.39 \u00b1 0.26 | 72.91 \u00b1 0.12 | 61.04 \u00b1 0.07 | 68.05| The comparison between the updated linear evaluation and fine-tune evaluation is consistent with our expectation ( and also yours~ ) . As you pointed out , linear evaluation results should be lower than the fine-tune results . In particular , MICRO-Graph \u2019 s linear evaluation result is 5.14 % lower than fine-tune on average , and for baselines , the average linear results are lower by 22.47 % , 7.84 % , 9.65 % , and 8.76 % respectively . This shows MICRO-Graph has learned more robust representations during pre-training , and a simple linear classifier on top can achieve good performance . Q : Views of Deep Graph Infomax and InfoGraph in Figure 2 . A : Yes , we totally agree that for both DGI and InfoGraph , their last layer node representations contain subgraph-level information . However , in Figure 2 , what we want to emphasize is the view construction difference rather than the representation difference . How we construct views is an important difference between MICRO-Graph and others . Specifically , we construct views by pooling nodes from a motif-guided subgraph sample , while others rely on single nodes . Like in computer vision , when input an image into a CNN , each pixel representation in the last layer corresponds to not a single pixel but a receptive field . However , the SOTA approach is using a group of pixels to represent a region in the image rather than using a single pixel . The difficulty of adopting similar approaches on graphs is to figure out which nodes we should group , i.e.which nodes form a meaningful subgraph . This is exactly our contribution , i.e.the motifs discovered by our framework . Models like InfoGraph don \u2019 t utilize this information . Q : Incorrect illustration of ContextPred in Figure 2 A : Thank you for pointing this out , we have updated Figure 2 . We would like to emphasize that even though the figure was incorrect , our implementation is taken from the official code by the paper authors . Therefore , our experiment result is still valid . In the next question we discuss more about it . Q : ContextPred result is lower than other models A : ContextPred has lower performance because it is compared to strong baselines , which are concurrent or later work to ContextPred and not compared in the original ContextPred paper . Our ContextPred implementation and hyperparameters are taken from the official code and paper . To eliminate possible influence due to different hyperparameters , we add two things in the Appendix . 1 ) a detailed description of hyperparameters used for ContextPred 2 ) Additional experiments with ContextPred using different hyperparameters . However , we didn \u2019 t observe significant differences between these results and the results in Table 1 and Table 2 . Q : Figure 3 to 6 are blurry . A : These figures are updated . We hope our answer has made things clear . We are happy for any further discussions or questions , so please feel free to let us know if any of the concerns are not fully addressed . Reference [ 1 ] Weihua Hu , Bowen Liu , Joseph Gomes , Marinka Zitnik , Percy Liang , Vijay Pande , and Jure Leskovec . Strategies for pre-training graph neural networks . In International Conference on Learning Representations , 2020b [ 2 ] Fan-Yun Sun , Jordan Hoffmann , Vikas Verma , and Jian Tang . Infograph : Unsupervised and semi- supervised graph-level representation learning via mutual information maximization , 2019 ."}, {"review_id": "qcKh_Msv1GP-3", "review_text": "Paper Summary The paper describes a self-supervised framework to extract graph motifs and use them as input for downstream contrastive learning . The framework contains three components : ( a ) motif guided segmenter to derive node subgraphs , ( b ) a motif learning - a clustering task among the subgraphs to identify concrete graph motifs and ( c ) contrastive learner for downstream graph tasks . The global objective is defined as the sum of the likelihoods of the three components . The framework is evaluated using from a large scale chemical compound graph dataset . The evaluation is performed for both transfer learning and utility of extracted features and outperforms the tested competing methods . Positives : * The framework presented to identity graph motifs and then use of learned motifs in contrastive learning for graph representations is very interesting and does lead to substantial gains in performance at least in the datasets tested * Experimental design to evaluate the framework is well thought too to specifically test the different pieces and components . * The results with synthetic dataset was a nice addition and a more thorough treatment with quantification of results , particularly recovery of true motifs would be a good addition . * The paper is generally well written . An algorithm/pseudo-code in the supplement would have made it even easier to follow given the many moving pieces . Concerns * A concern is that the motif learner enforces all clusters to be have similar size . I do not think this is a very realistic assumption in real world datasets . * The authors should provide some commentary on how the number of clusters used for spectral clustering and what their impact is on downstream results are * The whole graph embedding on Page 5 uses an average of all node embeddings for the graph . Does this put a limit on the size of the graph itself for the framework to work ? If the graph is large , the average node embedding is probably not an accurate representation for the graph ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer1 , We really appreciate your questions and comments . We hope our response can address your concerns . The paper is updated . Modifications are highlighted in red . As you suggested , the pseudocode of our algorithm has been included in Appendix A. Q : Similar-size constraint of clusters A : We totally agree that strictly forcing all clusters to have the same sizes is an unrealistic assumption . In our case , this constraint is introduced to avoid one cluster being too large that distorts the representation space , which in the extreme case leads to a degenerate solution , i.e.all representations collapse to a single cluster . Our approach can achieve this goal while maintaining relatively flexible cluster sizes . Theoretically , we use the Sinkhorn-Knopp algorithm to approximate the constraint optimization problem . Equal-partition is not strictly forced , and it is less of a constraint but more of a regularization . It is also shown in [ 1 ] Section 3.2 that this regularization can be interpreted as maximizing mutual information between subgraph-index and motif-labels . Therefore , it is reasonable from the information theory perspective . Empirically , we observe that the final distribution of motif cluster sizes is not uniform . We have included this distribution plot in Appendix G. Also , notice that these clusters do not need to perfectly correspond to the \u201c natural \u201d clusters . As our subgraph embeddings contain contextual information of the whole graph it belongs to , even two subgraphs with the same structure can have different embeddings . In this sense , the clusters are more fine-grained . Large clusters corresponding to the same common subgraph structure can be broken down , resulting in multiple small clusters of the same subgraph structure in different contexts . We thus consider the \u201c overclustering \u201d scheme in our ablation study . As shown in Table 4 , we increase the number of clusters from 20 to 100 and observe similar performance for these two cases . Q : Choosing the number of clusters used in spectral clustering A : In our case , the number of clusters used in spectral clustering is controlled by a hyper-parameter minNode . minNode describes how many nodes we expect the smallest subgraph sample to have . The cluster number of spectral clustering is set to be $ round ( ( graph size / minNode ) ^ { ( 1/2 ) } ) $ + 1 . In other words , we choose the number of clusters to be a smooth function of both the graph size and the minNode . In our experiments , we set minNode equal to the smallest graph size in the dataset , which equals 4 for the molecule pre-train dataset . The setting makes sure the number of clusters is always > = 2 . A larger minNode value means the number of clusters equals one for small graphs in the dataset , so spectral clustering can \u2019 t segment them . On the other hand , we observed that smaller minNode fragments the whole graph and lead to bad results for both motif learning and downstream tasks . Q : Aggregation for graph embedding A : How to aggregate node embeddings to get graph embeddings is a very interesting and important question . However , this remains an open problem and it is not the focus of our paper . We would like to point out that our framework can work with any aggregation methods , e.g.mean function , sum function , and entrywise max function . We choose to use the mean function in our experiment , which is the state or the art for empirical performance . We add investigation of aggregation methods to our future work . We hope our answer has made things clear . We are happy for any further discussions or questions , so please feel free to let us know if any of the concerns are not fully addressed . Reference [ 1 ] Asano YM. , Rupprecht C. , and Vedaldi A. Self-labelling via simultaneous clustering and representation learning . In International Conference on Learning Representations ( ICLR ) , 2020"}], "0": {"review_id": "qcKh_Msv1GP-0", "review_text": "=Summary= In this work , the authors study how to leverage motif discovery to learn graph representations . MICRO-Graph is proposed to enable simultaneous motif discovery and graph representation learning . Empirical results on public benchmark datasets suggest the effectiveness of the proposed method . =Reason for the rating= At this moment , I am leaning to reject . Overall , the technical quality is my main concern . This paper 's presentation also makes its technical details confusing . Hopefully , the authors could address my concern in the rebuttal . =Strong points= 1 . The authors investigate the problem of graph learning from a unique angle of motif discovery . 2.The authors propose the MICRO-Graph framework that enables simultaneous motif discovery and graph representation learning . 3.From multiple public benchmark datasets , the empirical results suggest the proposed technique could be promising in graph classification tasks . =Weak points= 1 . My major concern is on the technical quality of this paper . - Graph representations generated by a node embedding system may not be comparable across different graphs . A node embedding system is assumed to be available in MICRO-Graph . In most cases , a node embedding system is trained in an unsupervised manner . The generated node representations usually only work in a transductive setting such that the learned model may not be able to be generalized for unseen graphs . In other words , node embedding from different graphs may not be comparable in a meaningful way , although they are vectors of numerical values . For the input node representations , the authors may need to clarify how the node representations are obtained , and why they are comparable across different graphs . - The authors may need to clarify whether MICRO-Graph could tune the parameters in the assumed node embedding system . If the parameters in the embedding system are trainable , the authors may need to discuss more on the assumed embedding system and how the parameters impact node representations . If the parameters in the embedding system are fixed , the subgraph segmenter basically generates fixed subgraphs by spectral clustering , and it is difficult to see the point of Equation ( 6 ) . In sum , without clarification , the technical discussion is confusing . - For the motif discovery discussed in 3.2 , it seems to be a typical clustering problem . The authors may need to clarify what the unique aspect is in the proposed method . - For the discussion in 3.4 , it is hard to see why it is reasonable to use subgraph relations to define positive/negative sets , as the difference between a graph and its subgraph could be significant in many cases . - In section 4.1.1 , the term `` fine-tune '' is mentioned . Could the authors provide more details on fine-tuning ? 2.It is still unclear how motif discovery impacts graph representations . As graph representations are the mean over node representations , the coupling between motif discovery and graph representations seems to be weak . 3.The empirical evaluation could be stronger . - For the evaluation in Table 1 , `` direct supervised learning '' should consider existing GNN techniques , such as GraphSage , GAT , GIN , and so on . - For the assumed node embedding system , the authors may evaluate how different node embedding systems impact the proposed method . In addition , the symbol usage in the presentation makes the paper hard to read . =Questions during rebuttal period= Please address and clarify the weak points above . =Post rebuttal= I appreciate the authors ' great effort on answering my questions . The response clears many confusing points from the original draft . Meanwhile , I still have concerns on how the idea of contrastive learning is handled in this paper , which could have been better shaped . In sum , I have increased the rating accordingly .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Regarding the questions you raised . Please see our answers below . Q : Compare representations across different graphs A : Since we are doing GNN pre-training , we are not explicitly learning a node embedding table for each node , but GNN as a mapping function f is learned . Suppose we want to get node embeddings E given a graph G , we can use f to generate the embedding , e.g.E = f ( G ) .As long as the node features are in the same domain across different graphs , e.g.molecules share atoms , and papers in citation graphs share words , f will map them into the same space . We don \u2019 t consider cross domain training and evaluation , say , pre-train with the citation graph and evaluate on molecules . Q : Tune node embedding parameters A : As we described above , we are not learning a node embedding system explicitly . To get the node embeddings , we just embed the node ( graph ) using the GNN , e.g.E = f ( G ) .Since the parameters of the GNN will be updated , i.e.f will change , the node embeddings will change as well , and thus `` tunable '' in this sense . Our loss function , including equation ( 6 ) , is meant to provide learning signals for the GNN parameters to update . Q : The unique aspect of the proposed method A : Simple clustering only finds clustering labels of fixed data points . In the case of representation learning , each point we want to cluster is a representation and will change during learning . The purpose of clustering here is to guide the representation learning . Clustering guidance has been shown to be helpful for representation learning [ 7,8 ] . At the same time , we want to keep the representations from degenerate solutions , e.g.collapse to a single point , which is a harder task than simple clustering . Q : Subgraph relation for positive/negative sets A : For a whole graph G1 , we first use the GNN to do message passing on the G1 and generate all the contextualized node embeddings . Both the embedding of G1 and a sampled subgraph S1 are aggregations of those node embeddings . During the whole graph message passing stage , the nodes within S1 have collected information from other nodes not in S1 , so that they contain information of the whole graph G1 , and can be treated as a positive sample to G1 . On the other hand , a subgraph S2 of a different whole graph G2 has never gone through message passing with any node in G1 . Therefore , S2 and G1 form a negative pair . Q : Details on fine-tuning . A : Fine-tuning is a conventional evaluation method for a pre-trained model . During the pre-training stage , we train a GNN by a self-supervised task on a large dataset , usually without labels , this GNN can be leveraged to benefit downstream tasks by `` fine-tuning '' . During the fine-tune stage , usually on a smaller supervised dataset , we train the pre-trained GNN using the supervised labels , i.e.we fine tune parameters in it . The high-level idea is that general knowledge of the data has been learned during the pre-train stage . Only a few extra labels are needed to further tune it to adapt to a supervised task . For example on molecule data , during pre-training the GNN has already learned generic knowledge of molecules not restricted to any specific property . During fine-tuning , for questions regarding a specific chemical property , we only need a small labeled dataset to teach the pre-trained GNN about it . Q : How motif discovery impacts graph representation ? A : As we described above , the motif discovery part will guide the GNN parameters to update , and thus update the node embeddings and further graph embeddings . In particular , equation ( 6 ) was meant to make nodes belong to a subgraph to have similar embeddings if this subgraph is similar to a learned motif . We also showed in our ablation study that heuristic sample methods like random walk are hard to generalize , which demonstrates the importance of the motif guidance . Q : Empirical evaluation . A : The MICRO-Graph framework is built on GNNs , but not explicit node embedding systems like node2vec . Also , our framework can work with any GNN model architectures , like GraphSage , GIN , and etc . The results shown in the current version are based on the DeeperGCN model . In general , we summarize our method and highlight our contribution as the following . 1.Our representation learning is under the pre-train framework , and our contribution is a novel pre-training strategy . 2.Besides learning representations , our method can also extract meaning motifs We hope our answer has made things clear . We are happy for any further discussions or questions , so please feel free to let us know if any of the concerns are not fully addressed . [ 7 ] Caron , M. , Bojanowski , P. , Joulin , A. , Douze , M. : Deep clustering for unsupervised learning of visual features . In : Proceedings of the European Conference on Computer Vision ( ECCV ) ( 2018 ) [ 8 ] Asano YM. , Rupprecht C. , and Vedaldi A. Self-labelling via simultaneous clustering and representation learning . In International Conference on Learning Representations ( ICLR ) , 2020"}, "1": {"review_id": "qcKh_Msv1GP-1", "review_text": "This paper proposes to learn the sub-graph patterns from a collection of training graphs . The key idea is to partition each graph into segments and enforce a global clustering of the subgraphs . The partitioning is also guided through contrastive learning , i.e. , subgraphs should have a larger similarity with the graph it is drawn from , compared with other graphs . The learned GNN ( that generates node embedding ) will then be used to some downstream learning tasks with or without further fine-tuning . The notation of the paper is very hard to follow . For example , no where is N defined , which I assume is the number of sug-graphs . Capital latters S_i \u2019 s represent vector ( not sure if my guess is correct ) , as lower-case letter q_j \u2019 s , which somewhat affects the reading . Also the definition of n_i , and n_ { i , s } is not quite following the custom of matrices . Furthermore , what is the relation between s_i and S_i ( both board ) , and what is the difference between S ( bold ) and S ( not bold ) ? The h_i is defined but not used , and in equation ( 7 ) g_i is used , which I assume is the summary embedding vector for graph G_i ; in ( 7 ) s_i seems to be subgraph embedding vector , and this makes me more confused about the s_ { j , k } appearing in the beginning of section 3.2 which is defined as the cosine similarity between subgraph j and motif k. The E [ G_i ] is with bold E somewhere and non-bold letter E elsewhere . Altogether , there are S ( bold ) , S ( not bold ) , s_ { j , k } ( bold ) that seems to be a similarity measure , and s_i ( bold ) which seems to represent subgraph vector . It \u2019 s quite confusing to me . Can authors clearly define these symbols when they are used , and make sure they are consistent throughout the paper , and explicitly mark their dimensionality to avoid confusion ? Usually upper-case letters are for matrices and lower-case bold letters for vectors . The idea of partitioning graphs into sub-graphs are a useful idea in breaking the complexity of graph-structured objects and exploring the potential hierarchical organizations of the graph . Using contrastive learning as a self-supervision may further improve the partitioning of each graph . However , the grouping part and the contrastive part may conflict with each other in that some sub-graphs are shared among different graphs , which can be quite common in chemical compounds . Under ( 5 ) , it is mentioned that spectral clustering is used to partition the graphs ; is it done end-to-end and if so where is the loss function corresponding to this operation ? In ( 6 ) , ( s , t ) in g ( i , j ) : What is g ( i , j ) in particular ? a threshold eta is used in the indicator function and how to choose eta ( considering that it is used directly on a set of variables ) ? Is ( 6 ) end-to-end optimizable ? In Figure~2 , what is meant by the blue and red markers ( like + and - ) ? Experimental results are quite strange in that the transfer learning setting ( which further finetunes the learned GNN based on a small set of labels , as in Table 1 ) leads to even worse performance than the feature extraction setting ( in which no fine-tuning is performance , as shown in Table2 ) and the gap can be as huge as 15 % in accuracy !", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q : Spectral clustering for partition graphs and the segmenter loss ( equation ( 6 ) ) A : The spectral clustering for partition graphs is not end-to-end . It is only used for grouping nodes into subgraphs , and the subgraph assignment is only served as \u201c labels \u201d for the loss function in equation ( 6 ) to encourage nodes within the same cluster to have closer embeddings . Specifically , Given a graph $ G_i $ with n nodes , we first constructs a n-by-n node-node affinity matrix $ A^ { ( i ) } $ , and then apply spectral clustering to this matrix $ A^ { ( i ) } $ , as if $ A^ { ( i ) } $ is a n-vertices complete graph with affinity scores as edge weights . Then we are done with spectral clustering . The segmenter loss term $ L_s $ in equation ( 6 ) is meant to make nodes belong to a subgraph ( predicted by spectral clustering ) to have similar node embeddings , if this subgraph is similar to a learned motif . Q : How to choose $ \\eta $ ? $ \\eta $ is the threshold for deciding whether a subgraph is similar enough to a learned motif . It is dynamically set to take the top 10 % most similar subgraphs to each motif . We hope this part is clear in the updated paper version , please refer to the pseudocode in Appendix A as well . Q : Blue and red markers in Figure 2 A : For contrastive learning , we need to define positive pairs and negative pairs . In Figure 2 , when two arrows are pointing a red plus sign , the two objects these arrows are referring to are considered as positive pairs . Vice versa for the negative pairs . Q : Experimental results A : We are sorry for the confusion caused by the fine tune evaluation and linear evaluation . The original Table1 and Table2 in the paper were not directly comparable , as they were meant to compare with two previous works [ 1 ] and [ 2 ] . Therefore , we closely followed the settings of these two works respectively , where the evaluation metric of Table1 was AUC-ROC score and the evaluation score of Table 2 was accuracy . We have now updated Table2 , so both cases use AUC-ROC score . The old version of Table2 has been moved to Appendix . Below we show the updated Table 2 with AUC-ROC score . |SSL methods | bace | bbbp |clintox |hiv |sider | tox21 |toxcast |Avg| |-| -- | -- |-|-|-|-|-|-| |ContextPred | 53.09 \u00b1 0.84 | 55.51 \u00b1 0.08 | 40.73 \u00b1 0.02 | 53.31 \u00b1 0.15 | 52.28 \u00b1 0.08 | 35.31 \u00b1 0.25 | 47.06 \u00b1 0.06 | 48.18 | |InfoGraph | 66.06 \u00b1 0.82 | 75.34 \u00b1 0.51 | 75.71 \u00b1 0.53 | 61.45 \u00b1 0.74 | 54.7 \u00b1 0.24 | 63.95 \u00b1 0.24 | 52.69 \u00b1 0.07 | 64.27 | |GPT-GNN | 59.43 \u00b1 0.66 | 71.58 \u00b1 0.54 | 62.78 \u00b1 0.58 | 64.08 \u00b1 0.36 | 54.67 \u00b1 0.16 | 68.2 \u00b1 0.14 | 57.06 \u00b1 0.13 | 62.53 | |MICRO-Graph | 69.54 \u00b1 0.39 | 81.07 \u00b1 0.42 | 63.69 \u00b1 0.56 | 72.74 \u00b1 0.15 | 55.39 \u00b1 0.26 | 72.91 \u00b1 0.12 | 61.04 \u00b1 0.07 | 68.05| The updated linear evaluation result shows that our framework outperforms all three baselines . The comparison between linear evaluation and fine-tune evaluation is also consistent with our expectation ( and also yours~ ) . As you pointed out , the linear results should be lower than the fine-tune result . In particular , MICRO-Graph \u2019 s performance is 5.14 % lower than fine-tune on average , and for three baselines , the average performance is lower by 22.47 % , 7.84 % , and 9.65 % respectively . This shows that MICRO-Graph has learned more robust representations during pre-training , and thus a simple linear classifier on top of it can achieve reasonably good results . We hope our answer has made things clear . We are happy for any further discussions or questions , so please feel free to let us know if any of the concerns are not fully addressed . Reference [ 1 ] Weihua Hu , Bowen Liu , Joseph Gomes , Marinka Zitnik , Percy Liang , Vijay Pande , and Jure Leskovec . Strategies for pre-training graph neural networks . In International Conference on Learning Representations , 2020b [ 2 ] Fan-Yun Sun , Jordan Hoffmann , Vikas Verma , and Jian Tang . Infograph : Unsupervised and semi- supervised graph-level representation learning via mutual information maximization , 2019 ."}, "2": {"review_id": "qcKh_Msv1GP-2", "review_text": "Overall : This paper proposes an interesting framework + It extracts subgraph ( s ) for each graph from node affinity matrix and spectral clustering , together with the help of motifs . + It learns the motifs by clustering the subgraphs . + It applies contrastive self-supervised learning on the graph-subgraph pair . + It overcomes the combinatorial problem by learning the motifs on the continuous representation space . Strengths : + The idea is novel and seems promising . + This paper is technically correct . + Nice visualization of the learned motifs . Weaknesses : + This paper is not well written , especially the notations . I list the main concerns here , and hope the authors can help clarify them later . 1.S3 , N is not defined , and M is defined as # graph . But some following sections are implying N as # graph and M as # subgraph . 2.S3.2 , should be ' ... we can extract M subgraphs \u2026 ' 3 . S3.2 , according to 'sampling a subgraph from a graph ' , does this mean each graph has one and only one subgraph ? 4.S3.2 , \u2018 ... apply the subgraph index as a mask to its subgraph embedding \u2026 \u2019 I can understand the following equation but not this sentence . 5.S3.2 , should be ' { ... , m_K } ' 6 . Actually without clarifying the above detailed notations , it \u2019 s a little hard to follow the remaining ( sub ) sections . e.g. , what are S and Q ? Because M is # subgraph , and it should be [ ... , S_M ] and [ ... , q_M ] according to the descriptions . 7.S3.3 , { g_ { i , j } } _ { j=1 } ^M , I guess this is saying each graph can have multiple groups/subgraphs and such mapping is represented in the N * M binary matrix . Then it contradicts with the ( 3 ) mentioned above , with 1-1 mapping between the two views . 8.In Eq 5 , not sure if softmax_s matches with the description above . 9.In Eq 6 , it would be better to add \u2018 1 \\le k \\le K \u2019 in the last term . + Experiments , authors can consider adding more baselines . 1.GROVER [ 1 ] is the SOTA , where it randomly masks a subgraph , which is highly relevant to this paper . 2.GNNExplainer [ 2 ] is learning the motif in an end-to-end way , the authors could also consider comparing with it . Recommendation : Considering the notation issues listed above and lack of baselines , I would encourage the authors to polish up this paper . It has the potential to be a much better paper . For now , I would reject the current version . Questions : + OGB [ 3 ] was first released in May 2020 , and GROVER [ 1 ] was released one month after that . So I think at least the authors should cite [ 1 ] . + In S1 , \u2018 Previous approaches , such as \u2026 \u2019 Deep Graph Informax and InforGraph , in the last layer it is indeed node-to-graph views , but if we take it under the GNN setting , where each node representation in the last layer actually encodes a K-hop neighborhood around that node , then it is subgraph-to-graph views . + The ContextPred in Figure 2 is not correct : it should be contrasting the k-hop neighborhood and pre-defined context graph . Check Figure 2 in [ 4 ] . + In Table 1 , the ContextPred is the worst , which is not expected based on my own experience , any reason why ? I couldn \u2019 t find it in the supplementary files . + Comparing Table 1 and 2 , the frozen pre-trained GNN seems to be much better to the fine-tuning ones . Can authors discuss this further ? + Figure 3 to 6 are blurry . [ 1 ] https : //arxiv.org/abs/2007.02835 [ 2 ] https : //arxiv.org/abs/1903.03894 [ 3 ] https : //arxiv.org/abs/2005.00687 [ 4 ] https : //arxiv.org/abs/1905.12265", "rating": "5: Marginally below acceptance threshold", "reply_text": "Q : Counter-intuitive experimental results in Table 1 and Table 2 A : We are sorry for the confusion caused by the fine-tune evaluation and linear evaluation . The original Table1 and Table2 in the paper were not directly comparable , as they were meant to compare with two previous works [ 1 ] and [ 2 ] . Therefore , we closely followed the settings of these two works respectively , where the evaluation metric of Table1 was AUC-ROC and the evaluation of Table 2 was accuracy . We have now updated Table2 , so both cases use AUC-ROC . The old version of Table2 has been moved to Appendix . A copy of Table2 is shown below . |SSL methods | bace | bbbp |clintox |hiv |sider | tox21 |toxcast |Avg| |-| -- | -- |-|-|-|-|-|-| |ContextPred | 53.09 \u00b1 0.84 | 55.51 \u00b1 0.08 | 40.73 \u00b1 0.02 | 53.31 \u00b1 0.15 | 52.28 \u00b1 0.08 | 35.31 \u00b1 0.25 | 47.06 \u00b1 0.06 | 48.18 | |InfoGraph | 66.06 \u00b1 0.82 | 75.34 \u00b1 0.51 | 75.71 \u00b1 0.53 | 61.45 \u00b1 0.74 | 54.7 \u00b1 0.24 | 63.95 \u00b1 0.24 | 52.69 \u00b1 0.07 | 64.27 | |GPT-GNN | 59.43 \u00b1 0.66 | 71.58 \u00b1 0.54 | 62.78 \u00b1 0.58 | 64.08 \u00b1 0.36 | 54.67 \u00b1 0.16 | 68.2 \u00b1 0.14 | 57.06 \u00b1 0.13 | 62.53 | |GROVER|65.67\u00b10.38|78.47\u00b10.36|53.19\u00b10.68|69.03\u00b10.23|54.94\u00b10.12|67.63\u00b10.13|57.28\u00b10.05| 63.74| |MICRO-Graph | 69.54 \u00b1 0.39 | 81.07 \u00b1 0.42 | 63.69 \u00b1 0.56 | 72.74 \u00b1 0.15 | 55.39 \u00b1 0.26 | 72.91 \u00b1 0.12 | 61.04 \u00b1 0.07 | 68.05| The comparison between the updated linear evaluation and fine-tune evaluation is consistent with our expectation ( and also yours~ ) . As you pointed out , linear evaluation results should be lower than the fine-tune results . In particular , MICRO-Graph \u2019 s linear evaluation result is 5.14 % lower than fine-tune on average , and for baselines , the average linear results are lower by 22.47 % , 7.84 % , 9.65 % , and 8.76 % respectively . This shows MICRO-Graph has learned more robust representations during pre-training , and a simple linear classifier on top can achieve good performance . Q : Views of Deep Graph Infomax and InfoGraph in Figure 2 . A : Yes , we totally agree that for both DGI and InfoGraph , their last layer node representations contain subgraph-level information . However , in Figure 2 , what we want to emphasize is the view construction difference rather than the representation difference . How we construct views is an important difference between MICRO-Graph and others . Specifically , we construct views by pooling nodes from a motif-guided subgraph sample , while others rely on single nodes . Like in computer vision , when input an image into a CNN , each pixel representation in the last layer corresponds to not a single pixel but a receptive field . However , the SOTA approach is using a group of pixels to represent a region in the image rather than using a single pixel . The difficulty of adopting similar approaches on graphs is to figure out which nodes we should group , i.e.which nodes form a meaningful subgraph . This is exactly our contribution , i.e.the motifs discovered by our framework . Models like InfoGraph don \u2019 t utilize this information . Q : Incorrect illustration of ContextPred in Figure 2 A : Thank you for pointing this out , we have updated Figure 2 . We would like to emphasize that even though the figure was incorrect , our implementation is taken from the official code by the paper authors . Therefore , our experiment result is still valid . In the next question we discuss more about it . Q : ContextPred result is lower than other models A : ContextPred has lower performance because it is compared to strong baselines , which are concurrent or later work to ContextPred and not compared in the original ContextPred paper . Our ContextPred implementation and hyperparameters are taken from the official code and paper . To eliminate possible influence due to different hyperparameters , we add two things in the Appendix . 1 ) a detailed description of hyperparameters used for ContextPred 2 ) Additional experiments with ContextPred using different hyperparameters . However , we didn \u2019 t observe significant differences between these results and the results in Table 1 and Table 2 . Q : Figure 3 to 6 are blurry . A : These figures are updated . We hope our answer has made things clear . We are happy for any further discussions or questions , so please feel free to let us know if any of the concerns are not fully addressed . Reference [ 1 ] Weihua Hu , Bowen Liu , Joseph Gomes , Marinka Zitnik , Percy Liang , Vijay Pande , and Jure Leskovec . Strategies for pre-training graph neural networks . In International Conference on Learning Representations , 2020b [ 2 ] Fan-Yun Sun , Jordan Hoffmann , Vikas Verma , and Jian Tang . Infograph : Unsupervised and semi- supervised graph-level representation learning via mutual information maximization , 2019 ."}, "3": {"review_id": "qcKh_Msv1GP-3", "review_text": "Paper Summary The paper describes a self-supervised framework to extract graph motifs and use them as input for downstream contrastive learning . The framework contains three components : ( a ) motif guided segmenter to derive node subgraphs , ( b ) a motif learning - a clustering task among the subgraphs to identify concrete graph motifs and ( c ) contrastive learner for downstream graph tasks . The global objective is defined as the sum of the likelihoods of the three components . The framework is evaluated using from a large scale chemical compound graph dataset . The evaluation is performed for both transfer learning and utility of extracted features and outperforms the tested competing methods . Positives : * The framework presented to identity graph motifs and then use of learned motifs in contrastive learning for graph representations is very interesting and does lead to substantial gains in performance at least in the datasets tested * Experimental design to evaluate the framework is well thought too to specifically test the different pieces and components . * The results with synthetic dataset was a nice addition and a more thorough treatment with quantification of results , particularly recovery of true motifs would be a good addition . * The paper is generally well written . An algorithm/pseudo-code in the supplement would have made it even easier to follow given the many moving pieces . Concerns * A concern is that the motif learner enforces all clusters to be have similar size . I do not think this is a very realistic assumption in real world datasets . * The authors should provide some commentary on how the number of clusters used for spectral clustering and what their impact is on downstream results are * The whole graph embedding on Page 5 uses an average of all node embeddings for the graph . Does this put a limit on the size of the graph itself for the framework to work ? If the graph is large , the average node embedding is probably not an accurate representation for the graph ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer1 , We really appreciate your questions and comments . We hope our response can address your concerns . The paper is updated . Modifications are highlighted in red . As you suggested , the pseudocode of our algorithm has been included in Appendix A. Q : Similar-size constraint of clusters A : We totally agree that strictly forcing all clusters to have the same sizes is an unrealistic assumption . In our case , this constraint is introduced to avoid one cluster being too large that distorts the representation space , which in the extreme case leads to a degenerate solution , i.e.all representations collapse to a single cluster . Our approach can achieve this goal while maintaining relatively flexible cluster sizes . Theoretically , we use the Sinkhorn-Knopp algorithm to approximate the constraint optimization problem . Equal-partition is not strictly forced , and it is less of a constraint but more of a regularization . It is also shown in [ 1 ] Section 3.2 that this regularization can be interpreted as maximizing mutual information between subgraph-index and motif-labels . Therefore , it is reasonable from the information theory perspective . Empirically , we observe that the final distribution of motif cluster sizes is not uniform . We have included this distribution plot in Appendix G. Also , notice that these clusters do not need to perfectly correspond to the \u201c natural \u201d clusters . As our subgraph embeddings contain contextual information of the whole graph it belongs to , even two subgraphs with the same structure can have different embeddings . In this sense , the clusters are more fine-grained . Large clusters corresponding to the same common subgraph structure can be broken down , resulting in multiple small clusters of the same subgraph structure in different contexts . We thus consider the \u201c overclustering \u201d scheme in our ablation study . As shown in Table 4 , we increase the number of clusters from 20 to 100 and observe similar performance for these two cases . Q : Choosing the number of clusters used in spectral clustering A : In our case , the number of clusters used in spectral clustering is controlled by a hyper-parameter minNode . minNode describes how many nodes we expect the smallest subgraph sample to have . The cluster number of spectral clustering is set to be $ round ( ( graph size / minNode ) ^ { ( 1/2 ) } ) $ + 1 . In other words , we choose the number of clusters to be a smooth function of both the graph size and the minNode . In our experiments , we set minNode equal to the smallest graph size in the dataset , which equals 4 for the molecule pre-train dataset . The setting makes sure the number of clusters is always > = 2 . A larger minNode value means the number of clusters equals one for small graphs in the dataset , so spectral clustering can \u2019 t segment them . On the other hand , we observed that smaller minNode fragments the whole graph and lead to bad results for both motif learning and downstream tasks . Q : Aggregation for graph embedding A : How to aggregate node embeddings to get graph embeddings is a very interesting and important question . However , this remains an open problem and it is not the focus of our paper . We would like to point out that our framework can work with any aggregation methods , e.g.mean function , sum function , and entrywise max function . We choose to use the mean function in our experiment , which is the state or the art for empirical performance . We add investigation of aggregation methods to our future work . We hope our answer has made things clear . We are happy for any further discussions or questions , so please feel free to let us know if any of the concerns are not fully addressed . Reference [ 1 ] Asano YM. , Rupprecht C. , and Vedaldi A. Self-labelling via simultaneous clustering and representation learning . In International Conference on Learning Representations ( ICLR ) , 2020"}}