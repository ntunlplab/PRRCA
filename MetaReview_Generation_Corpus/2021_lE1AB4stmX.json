{"year": "2021", "forum": "lE1AB4stmX", "title": "A Transformer-based Framework for Multivariate Time Series Representation Learning", "decision": "Reject", "meta_review": "The authors extends the transformer to multivariate time series. The proposed extension is simple, and lacks novelty. Some design decisions of the proposed method should be better justified. Similar works that also use the transformer for timeseries are not compared.\n\nExperimental results are not convincing. The settings are unclear, and the selection of datasets needs more justifications. Some important experiments are missing.\n\nFinally, writing can also be improved.", "reviews": [{"review_id": "lE1AB4stmX-0", "review_text": "# Summary : The paper proposes an unsupervised learning framework , similar to BERT idea but for multivariate time series . # # # Architecture : - Inputs are projected into d-dimensional vector space where d is the dimension of the transformer model sequence element representations . - A learned positional encoding is added to the input which is passed to the transformer encoder . -The output of the encoder is then passed to another projection layer that depends on the application ( i.e classification or regression ) -The transformer architecture is similar to Vaswani et al . ( 2017 ) , however , they replaced layer norm with batch norm . # # # Unsupervised Learning Training : For unsupervised learning , the paper proposed taking the input data masking it , and predicting an output sequence , the loss is then calculated as the mean square error between the actual input and the predicted input . # # # Evaluation and Results : - The paper considered two tasks both regression and classification . For regression , the proposed method was evaluated on datasets from the Monash University , UEA , UCR Time Series Regression Archive Tan et al . ( 2020a ) ; for classification , the paper used the UEA Time Series Classification Archive ( Bagnall et al. , 2018 ) . - For each dataset , they showed the results of using the proposed transformer architecture in a supervised manner and then using the same dataset for pretraining the transformer by the proposed masking approach in an unsupervised manner followed by fine-tuning using labels . Both methods were compared to other state of the art methods for the same task . - For multivariate regression , the pretraining followed by supervised training showed improvements in 3 out of 6 datasets . For classification 7 out of 11 datasets showed improvements . # Strength : - The paper focues an important yet a relatively unexplored area . - The paper is clear , well written and well motivated . - The paper benchmarked accorss multiple datasets on both classification and regression tasks . # Weakness : - My main concern is the lack of novelty the paper is basically suggesting to use a transformer encoder and add a dense layer before and after , and if we use unsupervised training of the transformer with the same dataset we * * * may * * * achieve better results . - The main success of BERT is the ability to transfer and improve the performance on unretaleted task however here they did not include any experiments showing that if we train on model we can use to transfer on another model . I believe critical experiments that are missing is unsupervised training say with dataset 1 and the fine tuning on dataset 2 and showing improvements on multiple task ( However , even if this experiments are provided I still believe the paper lacks novelty , maybe invistaginting what properties are being transferred in multivariate time series and showing difference between transferring from unvariate to multivariate and vice versa will help ) . - The paper replaced layer norm with batch norm and only stated `` here we instead use batch normalization , because it can mitigate the effect of outlier values in time series , an issue that does not arise in NLP word embedding '' they did not show the effect of using batch norm on the accuracy or gave any insights on why it * mitigate the effect of outlier values in time series * . - The code implementing the paper is not provided . - The effect of changing values variable r in masking is not investigated .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their feedback , which we will use for improving the presentation of the paper and complementing it with additional experiments . Our response to the comments follows . \u201c My main concern is the lack of novelty the paper is basically suggesting to use a transformer encoder and add a dense layer before and after , and if we use unsupervised training of the transformer with the same dataset we may achieve better results. \u201d We consider our main contribution to be developing a general framework which allows the same model to be conveniently used ( and fine-tuned/re-used ) for different objectives : imputation , forecasting , classification and regression of multivariate time series , while at the same time allowing to effectively leverage unsupervised representation learning . This required suitably adapting a model architecture which has never before been used for the examined problems , similarly to other recently published work in deep learning for this data modality ( e.g. , InceptionTime [ Fawaz et al. , 2019a ] and ResNet [ Fawaz et al. , 2019b ] , which were adapted from models used for computer vision ) ; however , we additionally consider the versatility/generality of our framework an important differentiator . Moreover , our proposed method represents an outstanding improvement on the state of the art in the context of time series classification and regression : it performs significantly better than the best currently available methods , while no other method , including the aforementioned ones , manages to meaningfully differentiate itself from the rest in terms of performance . Despite decades of innovation and meticulously engineered approaches on those problems , it is telling that the second best methods after ours are XGBoost , which does not even take sequence order into account , and ROCKET , which is based on a linear classifier on top of a flat collection of randomly initialized convolutional filters . We additionally believe that our method constitutes an important landmark , because through the use of our novel unsupervised objective it becomes the only method which has been shown to successfully leverage unlabeled data in order to improve performance and push the state of the art in time series regression and classification . Utilizing unlabeled time series data is immensely interesting for nearly all domains in the sciences and in industry . Interestingly , we show that it can accomplish this even when the number of available unlabeled data points is very limited , and that it can in fact benefit even from reusing labeled data samples through unsupervised learning . Nevertheless , we can not guarantee that every single dataset will benefit from unsupervised learning , because datasets can display great variability in characteristics such as number of samples , data dimensionality , length of time series and overall difficulty of the task ( which can be additionally affected by the nature of the relationship between input variables themselves or input variables and the target ) . For example , it is possible that enough samples exist such that a relatively \u201c easy task \u201d can be learned sufficiently well directly through unsupervised learning . Conversely , a very challenging task ( where the target can hardly be predicted from the input variables ) , or a task that involves input variables which are completely independent from one another , will not benefit much from unsupervised learning , which aims at learning to model inter-dependencies between variables ( besides learning to use past and future values of a given single variable ) ."}, {"review_id": "lE1AB4stmX-1", "review_text": "This paper uses transformer to improve mutlivariate time series classification and regression using a BERT inspired self-supervised loss . The authors show improvement over multiple standard datasets . The use of self-supervision improves performance in lower ( labeled ) data regime . # # # # Strong points - The method is well explained and take good inspiration of BERT pretraining . - The data-available experiment shows that training with self-supervision helps for classification . - This work is well positioned in the literature of the field . # # # # Weak points - The timeserie representation is obtained by concatenation of the tokens ' representations . This means that the representations are not of fixed size ( or contain padding representations ) . - The introduced masking loss could suffer from discrepancy between training and inference where all the covariates at a given timesteps need to be predicted . Moreover , the model could rely on very correlated covariates ( which frequently happens ) to recover the masked variables limiting the effect of self-supervised training . I would like to see if masking all variables at some times step helps/hurts the pretraining . - The paper does not consider if finetuning is needed or if the network could be frozen and only a small MLP could be learned on top of the `` pre-extracted times-series representations '' . - The hyperparameters of the network are chosen per dataset but are not reported . It is not clear if the tuning is done with cross validation or looking at the test error . The authors should report the performance of the transformer with common hyperparameters specified in the Appendix . This would be more fair to compare with Franceschi et al. , who do not tune the hyperparameters of their encoder . - The claim in the abstract that the method offers `` computational efficiency '' is not backed with evidence . Section A.4 `` Execution Time '' does not report execution time of the method compared to the baselines . # # # # Decision I tend to reject this paper . The idea is interesting but some design decisions of the method ( representation pooling , self-supervised loss , need for finetuning ) should be better justified . Secondly the evaluation of the\u00b4 method should be more precise ( hyperparameters tuning , speed ) . # # # # Questions/Remarks - In section 3.1 `` because the computational complexity and the number of parameters of the model scale as $ O ( w^2 ) $ with the input sequence length $ w $ '' . The number of parameters of the transformer architecture does not scale with the input length . Only the memory footprint and the computation scales quadratically . - `` we note that similar to the case of word embeddings , the positional encodings generally appear not to interfere with the numerical information of the time series '' . Do you have evidence for this ? - Have you tried PowerNorm by Shen et al . ( 2020 ) that you cite in 3.1 ? - You could mention Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting ( Lim et al. , 2019 ) arXiv:1912.09363 for another use of transformers for univariate ( quantile ) forecasting . - Add the dataset you are experimenting with in the caption of Figure 2 . # # # # Additional feedback - Figure 2 : chose different colors for left and right figures . - Table 1 : consider using booktabs for table format and transpose dataset and models to make it fit in the margin . - Section 3.2 : The sentence starting with `` The reason ... '' that spans 6 lines could be split and reworded .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the thoughtful feedback , questions and comments , which give us the opportunity to explain our approach more thoroughly and to improve the paper in terms of phrasing , presentation and experiments [ updated version will be uploaded soon ] . Our response follows : \u201c The timeserie representation is obtained by concatenation of the tokens ' representations . This means that the representations are not of fixed size ( or contain padding representations ) . \u201d [ We first note that , when computing the output , the representations corresponding to padding are zeroed-out and do not contribute to the computation . ] We do not advocate using the concatenation of representations as a general way of representing the time series : the concatenation was only used for the particular objectives evaluated , i.e.regression and classification , and is not meant for extraction and storage as a universal representation of the time series . As we note in A.1 of the appendix , for other purposes , the individual representations at each time step can be used separately ( i.e.as done for imputation ) or combined in other ways ( e.g.weighted mean over some or all time steps ) , depending on the application . We consider this flexibility an advantage of our approach : for example , if we wanted to compare time series with one another only based on their beginning/ending , we could selectively choose to average the encoder output embeddings corresponding to the first/last few time steps . In future work , we plan to explore which types of representations work best for each application , e.g.addressing retrieval , clustering , anomaly detection etc . However , contrary to Franseschi et al. , whose goal is to extract and store * universal representations of general utility * , our ( less ambitious ) goal is to * use the appropriate representations for each intended objective * . This goal is guided by the observed shortcomings of universal representations , e.g.in fields such as medical time series ( Lyu et al , NeurIPS ML4Health Workshop 2018 ) , as well as insights on transformer representations in the field of natural language processing : in practice , researchers fine-tune pre-trained transformer models instead of extracting and storing representations , e.g.BERT , T5 ( Raffel et al , JMLR 2020 ) ."}, {"review_id": "lE1AB4stmX-2", "review_text": "This paper aims to develop a transformer-based pre-trained model for multivariate time series representation learning . Specifically , the transformer \u2019 s encoder is only used and a time-series imputation task is constructed as their unsupervised learning objective . This is a bit similar to the BERT model in NLP . But authors added a mask for each variable of the time series . After pretraining with this imputation loss , the transformer can be used for downstream tasks , such as regression and classification . As the authors mentioned on page 6 , this is achieved by further fine-tuning all weights of the pre-trained transformer . It is natural to use the transformer model from NLP for time series modeling since both sentences and time series are sequential data . In this work , the authors \u2019 contributions or changes should include two-points : 1 ) constructing that imputation task for multivariate time series data . 2 ) using a learned positional encoding ( page 4 ) . I think these two things seem a bit interesting . My concerns mainly include : 1 . Actually , there are existing works that have tried to use the transformer for time series . But you didn \u2019 t compare them in your experiments . At least , I think you should clarify what \u2019 s your advantages comparing to these existing works : - 2019 NeurIPS Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting - 2020 CVPR Sketch-BERT : Learning Sketch Bidirectional Encoder Representation From Transformers by self-supervised Learning of Sketch Gestalt 3 . As a core topic in this work , I encourage authors to clarify the definition of the time series representation . What \u2019 s a good representation in time series ? There is a related work for time series representation learning : - 2019 NeurIPS Unsupervised Scalable Representation Learning for Multivariate Time Series I notice that they also train a dilated CNN model to get a single feature for a segment of time series . But in your case , you directly concatenate states from all-time steps to form your final vector . I just wonder if your strategy is reasonable . Because when we deal with long time series , concatenating them will create another long time-series/hidden state sequence . This may not be representation learning and it can not handle the segment-level tasks such as classification . Thus , I encourage authors to give more insights into the questions what \u2019 s a good representation of the time series . 3.Besides , in the above NeurIPS representation learning paper , I find they didn \u2019 t fine-tune their model \u2019 s parameters . They just added SVM to test the performance of learned representations . But in this work , the authors fine-tuning their model . Thus , I wonder what \u2019 s the performance of your model without additional fine-tuning . What 's your parameter settings for the fine-tuning procedure . 4.Another question I am concerned about is your learnable position encoding . It is with a shape of w-by-d where w should be your window length and d is your input dimension . Since time series is dynamic data and its length would be much longer , this design maybe not good as it largely increases the number of parameters . 5.For experimental results , authors mainly consider their model \u2019 s performance on regression and classification tasks . But I would like to see more analysis of their learned presentations . Adding more visualization analysis would be helpful to demonstrate your framework \u2019 s effectiveness . 6.Experimental settings are unclear . For example , since your datasets only contain train/test sets . How to pick your hyperparameters ? Is it based on that performance on that test set ? What kind of regression task is used in your regression experiments ? Is it a one-step-ahead prediction task ? 7.In Table 5 , you claim your model is faster than the ROCKET model . But the running time reported for your model is per-epoch training time , not the total training time . I think this seems a bit unreasonable . Please check it .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the interesting questions and comments , which give us the opportunity to more thoroughly explain our approach and to improve the paper [ updated version will be uploaded soon ] . Our response follows : \u201c 1 . Actually , there are existing works that have tried to use the transformer for time series . But you didn \u2019 t compare them in your experiments . At least , I think you should clarify what \u2019 s your advantages comparing to these existing works : 2019 NeurIPS Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting 2020 CVPR Sketch-BERT : Learning Sketch Bidirectional Encoder Representation From Transformers by self-supervised Learning of Sketch Gestalt \u201d Regarding \u201c 2019 NeurIPS Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting \u201d : This work ( Li et al . ) , which we cite in the related work section , employs a _full transformer encoder-decoder_ architecture ( as the original by Vaswani et al ) for the purpose of * forecasting * in univariate time series . We could of course extend this method , in a similar way as our model , to work for multivariate instead of univariate time series , so this was not the main limitation . The main reason why it is not included in our comparison is that , although the transformer decoder is well suited for the generative task of subsequent values prediction and as such it is used for forecasting , it is not suitable for sequence classification and regression , the tasks our model and all other baselines are evaluated on . This is not simply a problem of parameter parsimony ( although the additional decoder would indeed necessitate roughly as many parameters as the encoder ) : the decoder requires a _target sequence as input which differs from the input of the encoder_ and constitutes what the decoder should generate , it employs a decoder-encoder attention scheme where target representations are compared to the encoder \u2019 s input representations , and it generates a _sequence of successive output values_ . This objective is very different from our objectives of classification and regression . Note that , having classification in mind , the BERT paper similarly dispensed with the decoder . At this point we take the opportunity to clarify what * regression * means in the specific context of our work and the time series literature we refer to , because we realize that this was unfortunately not very clear in our original manuscript . We thus address the question raised by the reviewer in their point 6 : \u201c What kind of regression task is used in your regression experiments ? Is it a one-step-ahead prediction task ? \u201d . Regression here means predicting a single numeric value for a given sequence ( time series sample ) . This numeric value is of a different nature than the numerical data appearing in the time series : for example , given a sequence of simultaneous measurements of two-channel PPG ( photoplethysmogram ) signals , three-axis acceleration signals , and one-channel ECG ( electrocardiogram ) signals , the regression task is to predict the heart rate ; given a sequence of simultaneous temperature and humidity measurements of 9 rooms in a house , as well as weather and climate data such as temperature , pressure , humidity , wind speed , visibility and dewpoint , predict the total energy consumption in kWh of a house for that day . These scalar values are the numerical \u201c labels \u201d provided by the datasets to train and evaluate models . In the general case , one may want to predict multiple scalars ( or a vector ) instead of a single scalar ; this corresponds to the dimensionality $ n $ entering the equations in Section 3.1 . It is important to note here that the goal of our work is to develop a general framework for time series representation learning , which easily facilitates both unsupervised learning as well as several downstream objectives . As such , besides unsupervised pre-training , our method can perform imputation , regression , classification as well as forecasting ( the latter is implemented by modifying the objective mask , as outlined in the Appendix A.1 and Figure 4 ) . Instead , the model by Li et al.can only perform forecasting . We do not evaluate our method on forecasting datasets in the scope of this work , but we will certainly compare the two methods in future work on forecasting , because it would be interesting to assess the effect of the decoder . We also note that the method of Li et al.includes a \u201c log-sparse \u201d attention mechanism , which is employed to mitigate the $ O ( w^2 ) $ dependence of computational complexity and memory of self-attention on the input sequence length $ w $ , at the possible expense of predictive performance . This is useful for their intended purpose of forecasting in their datasets , because they have to consider very long input sequences ; by contrast , the datasets in the archives we have considered can be handled by the full attention mechanism , and thus we do not need to employ such a modification ."}, {"review_id": "lE1AB4stmX-3", "review_text": "The authors targeted an important data format , multivariate time series , and extended the usage of the transformer to this format . The effort is appreciated as multivariate time series data is an important problem while the researches there is limited comparing to other sequences problems , e.g.language.Due to the simple idea of extension and similar works before [ 1 ] , I expected a higher quality of experiments and presentation of this paper . However , experiments and writing need significant improvement for the next submission . Major Concerns : 1 . The experiment 's presentation is confusing and I just picked a few examples from Table 1 : a . There is no standard deviation of the result . b.Some methods are consistently worse than others , like 1-NN-DTWD comparing to 5-NN-DTWD . I am not sure why they are needed in the table . c. Averaging the RMSE does not deliver much information , especially when there is no normalization on each dataset . For example , the BeijingPM10 will dominate the averaged result . 2.The structure of writing only has a lot of problems . For example , usually , the fully supervised should be presented before semi-supervised while the paper did in a reverse way . Another problem is the classification result , which seems an important part but the table is shown in the Appendix . 3.The selection of datasets needs more justifications . Reference : 1. https : //arxiv.org/pdf/2001.08317.pdf", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the formatting suggestions , all of which we will adopt in the revised manuscript . We will also add a section in the Appendix with a thorough explanation on how the datasets were selected . Finally , we will show standard deviations for our models . Thus , all voiced concerns by the reviewer will be addressed . Regarding presentation of results in Table 1 : with the exception of our models , which achieve consistently top performance across datasets , the performance of all other baseline methods significantly fluctuates across datasets , which necessitates displaying all models . In the reviewer \u2019 s example comparison pair , 5-NN-DTWD indeed always outperforms 1-NN-DTWD , however the weaker model in the pair , 1-NN-DTWD , outperforms model 5-NN-ED on some datasets , which in turn outperforms 5-NN-DTWD on some other datasets . To conclude , we don \u2019 t find a model in the table that is consistently worse than all other models and should thus be excluded . Additionally , the performance of the NN-DTWD method is typically sensitive on the number of Nearest Neighbors , and including models with 2 parameter values helps to better outline the performance of this model . Regarding reporting the average Root MSE in Table 1 : We attempt to illustrate performance differences from different perspectives . We agree that one should not primarily rely on the average RMSE to establish which method is best . This is why in the discussion we rely on the average rank ( Table 1 ) and individual ranks ( Table 7 ) , and also we compare our method with the overall second best method : \u201c On average , our models attain approx . 16 % lower RMSE than the overall second best model ( XGBoost ) , with absolute improvements varying among datasets from approx . 4 % to 36 % \u201d . We also show the relative performance difference between our method and whichever method ranks 2nd for each dataset in Table 7 . As the reviewer correctly points out , the average RMSE will be primarily affected by the \u201c most challenging \u201d datasets . Despite this limitation , we reported it in Table 1 because we found that in the case of some datasets , many models had very similar RMSE values , while their discrete rank would inevitably strongly differentiate between them ( e.g.the RMSE of models with rank 3 and rank 6 can be almost identical ) . Interestingly , sorting models by average RMSE leads to almost the same list as sorting by average rank , especially with respect to top performers , which indicates that this metric conveys information . However , we will now report a new metric $ r_j $ for each model $ j $ instead , the `` average relative difference from mean '' over $ N $ datasets ( details to follow ) . \u201c Due to the simple idea of extension and similar works before [ 1 ] , I expected a higher quality of experiments \u201d The work cited by the reviewer ( Wu et al . ) , which we also cite in the related work section , employs a _full transformer encoder-decoder_ architecture ( as the original by Vaswani et al ) for the sole purpose of * forecasting * in * univariate * time series , and evaluates it on a single dataset , against 3 reasonably competitive baselines . Instead , we employ a _transformer encoder_ architecture which supports forecasting , imputation , regression and classification for univariate and multivariate time series . Additionally : - Our proposed method represents an outstanding improvement on the state of the art in the context of time series classification and regression : evaluating it against 11 baselines on 6 different datasets for regression , and against 5 baselines on 11 different datasets for classification , our results clearly show that it performs significantly better than the best currently available methods , while no other method manages to meaningfully differentiate itself from the rest . Despite decades of innovation and meticulously engineered approaches on those problems , it is telling that the second best methods are XGBoost , which does not even take sequence order into account , and ROCKET , which is based on a linear classifier on top of a flat collection of randomly initialized convolutional filters . - Our method is the only method which has been shown to successfully leverage unlabeled data in order to improve performance and push the state of the art in time series regression and classification . Leveraging unlabeled time series data is immensely interesting for nearly all domains in the sciences and in industry . Importantly , we show that it can accomplish this even when the number of available unlabeled data points is very limited , and that it can in fact benefit even from reusing labeled data samples through unsupervised learning . - These were accomplished by 1 ) suitably adapting a model architecture which has never before been used for these problems , 2 ) employing a novel unsupervised objective which allows unsupervised representation learning , 3 ) developing a framework which allows the same model to be used for different objectives ( imputation , forecasting , classification , regression ) ."}], "0": {"review_id": "lE1AB4stmX-0", "review_text": "# Summary : The paper proposes an unsupervised learning framework , similar to BERT idea but for multivariate time series . # # # Architecture : - Inputs are projected into d-dimensional vector space where d is the dimension of the transformer model sequence element representations . - A learned positional encoding is added to the input which is passed to the transformer encoder . -The output of the encoder is then passed to another projection layer that depends on the application ( i.e classification or regression ) -The transformer architecture is similar to Vaswani et al . ( 2017 ) , however , they replaced layer norm with batch norm . # # # Unsupervised Learning Training : For unsupervised learning , the paper proposed taking the input data masking it , and predicting an output sequence , the loss is then calculated as the mean square error between the actual input and the predicted input . # # # Evaluation and Results : - The paper considered two tasks both regression and classification . For regression , the proposed method was evaluated on datasets from the Monash University , UEA , UCR Time Series Regression Archive Tan et al . ( 2020a ) ; for classification , the paper used the UEA Time Series Classification Archive ( Bagnall et al. , 2018 ) . - For each dataset , they showed the results of using the proposed transformer architecture in a supervised manner and then using the same dataset for pretraining the transformer by the proposed masking approach in an unsupervised manner followed by fine-tuning using labels . Both methods were compared to other state of the art methods for the same task . - For multivariate regression , the pretraining followed by supervised training showed improvements in 3 out of 6 datasets . For classification 7 out of 11 datasets showed improvements . # Strength : - The paper focues an important yet a relatively unexplored area . - The paper is clear , well written and well motivated . - The paper benchmarked accorss multiple datasets on both classification and regression tasks . # Weakness : - My main concern is the lack of novelty the paper is basically suggesting to use a transformer encoder and add a dense layer before and after , and if we use unsupervised training of the transformer with the same dataset we * * * may * * * achieve better results . - The main success of BERT is the ability to transfer and improve the performance on unretaleted task however here they did not include any experiments showing that if we train on model we can use to transfer on another model . I believe critical experiments that are missing is unsupervised training say with dataset 1 and the fine tuning on dataset 2 and showing improvements on multiple task ( However , even if this experiments are provided I still believe the paper lacks novelty , maybe invistaginting what properties are being transferred in multivariate time series and showing difference between transferring from unvariate to multivariate and vice versa will help ) . - The paper replaced layer norm with batch norm and only stated `` here we instead use batch normalization , because it can mitigate the effect of outlier values in time series , an issue that does not arise in NLP word embedding '' they did not show the effect of using batch norm on the accuracy or gave any insights on why it * mitigate the effect of outlier values in time series * . - The code implementing the paper is not provided . - The effect of changing values variable r in masking is not investigated .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their feedback , which we will use for improving the presentation of the paper and complementing it with additional experiments . Our response to the comments follows . \u201c My main concern is the lack of novelty the paper is basically suggesting to use a transformer encoder and add a dense layer before and after , and if we use unsupervised training of the transformer with the same dataset we may achieve better results. \u201d We consider our main contribution to be developing a general framework which allows the same model to be conveniently used ( and fine-tuned/re-used ) for different objectives : imputation , forecasting , classification and regression of multivariate time series , while at the same time allowing to effectively leverage unsupervised representation learning . This required suitably adapting a model architecture which has never before been used for the examined problems , similarly to other recently published work in deep learning for this data modality ( e.g. , InceptionTime [ Fawaz et al. , 2019a ] and ResNet [ Fawaz et al. , 2019b ] , which were adapted from models used for computer vision ) ; however , we additionally consider the versatility/generality of our framework an important differentiator . Moreover , our proposed method represents an outstanding improvement on the state of the art in the context of time series classification and regression : it performs significantly better than the best currently available methods , while no other method , including the aforementioned ones , manages to meaningfully differentiate itself from the rest in terms of performance . Despite decades of innovation and meticulously engineered approaches on those problems , it is telling that the second best methods after ours are XGBoost , which does not even take sequence order into account , and ROCKET , which is based on a linear classifier on top of a flat collection of randomly initialized convolutional filters . We additionally believe that our method constitutes an important landmark , because through the use of our novel unsupervised objective it becomes the only method which has been shown to successfully leverage unlabeled data in order to improve performance and push the state of the art in time series regression and classification . Utilizing unlabeled time series data is immensely interesting for nearly all domains in the sciences and in industry . Interestingly , we show that it can accomplish this even when the number of available unlabeled data points is very limited , and that it can in fact benefit even from reusing labeled data samples through unsupervised learning . Nevertheless , we can not guarantee that every single dataset will benefit from unsupervised learning , because datasets can display great variability in characteristics such as number of samples , data dimensionality , length of time series and overall difficulty of the task ( which can be additionally affected by the nature of the relationship between input variables themselves or input variables and the target ) . For example , it is possible that enough samples exist such that a relatively \u201c easy task \u201d can be learned sufficiently well directly through unsupervised learning . Conversely , a very challenging task ( where the target can hardly be predicted from the input variables ) , or a task that involves input variables which are completely independent from one another , will not benefit much from unsupervised learning , which aims at learning to model inter-dependencies between variables ( besides learning to use past and future values of a given single variable ) ."}, "1": {"review_id": "lE1AB4stmX-1", "review_text": "This paper uses transformer to improve mutlivariate time series classification and regression using a BERT inspired self-supervised loss . The authors show improvement over multiple standard datasets . The use of self-supervision improves performance in lower ( labeled ) data regime . # # # # Strong points - The method is well explained and take good inspiration of BERT pretraining . - The data-available experiment shows that training with self-supervision helps for classification . - This work is well positioned in the literature of the field . # # # # Weak points - The timeserie representation is obtained by concatenation of the tokens ' representations . This means that the representations are not of fixed size ( or contain padding representations ) . - The introduced masking loss could suffer from discrepancy between training and inference where all the covariates at a given timesteps need to be predicted . Moreover , the model could rely on very correlated covariates ( which frequently happens ) to recover the masked variables limiting the effect of self-supervised training . I would like to see if masking all variables at some times step helps/hurts the pretraining . - The paper does not consider if finetuning is needed or if the network could be frozen and only a small MLP could be learned on top of the `` pre-extracted times-series representations '' . - The hyperparameters of the network are chosen per dataset but are not reported . It is not clear if the tuning is done with cross validation or looking at the test error . The authors should report the performance of the transformer with common hyperparameters specified in the Appendix . This would be more fair to compare with Franceschi et al. , who do not tune the hyperparameters of their encoder . - The claim in the abstract that the method offers `` computational efficiency '' is not backed with evidence . Section A.4 `` Execution Time '' does not report execution time of the method compared to the baselines . # # # # Decision I tend to reject this paper . The idea is interesting but some design decisions of the method ( representation pooling , self-supervised loss , need for finetuning ) should be better justified . Secondly the evaluation of the\u00b4 method should be more precise ( hyperparameters tuning , speed ) . # # # # Questions/Remarks - In section 3.1 `` because the computational complexity and the number of parameters of the model scale as $ O ( w^2 ) $ with the input sequence length $ w $ '' . The number of parameters of the transformer architecture does not scale with the input length . Only the memory footprint and the computation scales quadratically . - `` we note that similar to the case of word embeddings , the positional encodings generally appear not to interfere with the numerical information of the time series '' . Do you have evidence for this ? - Have you tried PowerNorm by Shen et al . ( 2020 ) that you cite in 3.1 ? - You could mention Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting ( Lim et al. , 2019 ) arXiv:1912.09363 for another use of transformers for univariate ( quantile ) forecasting . - Add the dataset you are experimenting with in the caption of Figure 2 . # # # # Additional feedback - Figure 2 : chose different colors for left and right figures . - Table 1 : consider using booktabs for table format and transpose dataset and models to make it fit in the margin . - Section 3.2 : The sentence starting with `` The reason ... '' that spans 6 lines could be split and reworded .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the thoughtful feedback , questions and comments , which give us the opportunity to explain our approach more thoroughly and to improve the paper in terms of phrasing , presentation and experiments [ updated version will be uploaded soon ] . Our response follows : \u201c The timeserie representation is obtained by concatenation of the tokens ' representations . This means that the representations are not of fixed size ( or contain padding representations ) . \u201d [ We first note that , when computing the output , the representations corresponding to padding are zeroed-out and do not contribute to the computation . ] We do not advocate using the concatenation of representations as a general way of representing the time series : the concatenation was only used for the particular objectives evaluated , i.e.regression and classification , and is not meant for extraction and storage as a universal representation of the time series . As we note in A.1 of the appendix , for other purposes , the individual representations at each time step can be used separately ( i.e.as done for imputation ) or combined in other ways ( e.g.weighted mean over some or all time steps ) , depending on the application . We consider this flexibility an advantage of our approach : for example , if we wanted to compare time series with one another only based on their beginning/ending , we could selectively choose to average the encoder output embeddings corresponding to the first/last few time steps . In future work , we plan to explore which types of representations work best for each application , e.g.addressing retrieval , clustering , anomaly detection etc . However , contrary to Franseschi et al. , whose goal is to extract and store * universal representations of general utility * , our ( less ambitious ) goal is to * use the appropriate representations for each intended objective * . This goal is guided by the observed shortcomings of universal representations , e.g.in fields such as medical time series ( Lyu et al , NeurIPS ML4Health Workshop 2018 ) , as well as insights on transformer representations in the field of natural language processing : in practice , researchers fine-tune pre-trained transformer models instead of extracting and storing representations , e.g.BERT , T5 ( Raffel et al , JMLR 2020 ) ."}, "2": {"review_id": "lE1AB4stmX-2", "review_text": "This paper aims to develop a transformer-based pre-trained model for multivariate time series representation learning . Specifically , the transformer \u2019 s encoder is only used and a time-series imputation task is constructed as their unsupervised learning objective . This is a bit similar to the BERT model in NLP . But authors added a mask for each variable of the time series . After pretraining with this imputation loss , the transformer can be used for downstream tasks , such as regression and classification . As the authors mentioned on page 6 , this is achieved by further fine-tuning all weights of the pre-trained transformer . It is natural to use the transformer model from NLP for time series modeling since both sentences and time series are sequential data . In this work , the authors \u2019 contributions or changes should include two-points : 1 ) constructing that imputation task for multivariate time series data . 2 ) using a learned positional encoding ( page 4 ) . I think these two things seem a bit interesting . My concerns mainly include : 1 . Actually , there are existing works that have tried to use the transformer for time series . But you didn \u2019 t compare them in your experiments . At least , I think you should clarify what \u2019 s your advantages comparing to these existing works : - 2019 NeurIPS Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting - 2020 CVPR Sketch-BERT : Learning Sketch Bidirectional Encoder Representation From Transformers by self-supervised Learning of Sketch Gestalt 3 . As a core topic in this work , I encourage authors to clarify the definition of the time series representation . What \u2019 s a good representation in time series ? There is a related work for time series representation learning : - 2019 NeurIPS Unsupervised Scalable Representation Learning for Multivariate Time Series I notice that they also train a dilated CNN model to get a single feature for a segment of time series . But in your case , you directly concatenate states from all-time steps to form your final vector . I just wonder if your strategy is reasonable . Because when we deal with long time series , concatenating them will create another long time-series/hidden state sequence . This may not be representation learning and it can not handle the segment-level tasks such as classification . Thus , I encourage authors to give more insights into the questions what \u2019 s a good representation of the time series . 3.Besides , in the above NeurIPS representation learning paper , I find they didn \u2019 t fine-tune their model \u2019 s parameters . They just added SVM to test the performance of learned representations . But in this work , the authors fine-tuning their model . Thus , I wonder what \u2019 s the performance of your model without additional fine-tuning . What 's your parameter settings for the fine-tuning procedure . 4.Another question I am concerned about is your learnable position encoding . It is with a shape of w-by-d where w should be your window length and d is your input dimension . Since time series is dynamic data and its length would be much longer , this design maybe not good as it largely increases the number of parameters . 5.For experimental results , authors mainly consider their model \u2019 s performance on regression and classification tasks . But I would like to see more analysis of their learned presentations . Adding more visualization analysis would be helpful to demonstrate your framework \u2019 s effectiveness . 6.Experimental settings are unclear . For example , since your datasets only contain train/test sets . How to pick your hyperparameters ? Is it based on that performance on that test set ? What kind of regression task is used in your regression experiments ? Is it a one-step-ahead prediction task ? 7.In Table 5 , you claim your model is faster than the ROCKET model . But the running time reported for your model is per-epoch training time , not the total training time . I think this seems a bit unreasonable . Please check it .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the interesting questions and comments , which give us the opportunity to more thoroughly explain our approach and to improve the paper [ updated version will be uploaded soon ] . Our response follows : \u201c 1 . Actually , there are existing works that have tried to use the transformer for time series . But you didn \u2019 t compare them in your experiments . At least , I think you should clarify what \u2019 s your advantages comparing to these existing works : 2019 NeurIPS Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting 2020 CVPR Sketch-BERT : Learning Sketch Bidirectional Encoder Representation From Transformers by self-supervised Learning of Sketch Gestalt \u201d Regarding \u201c 2019 NeurIPS Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting \u201d : This work ( Li et al . ) , which we cite in the related work section , employs a _full transformer encoder-decoder_ architecture ( as the original by Vaswani et al ) for the purpose of * forecasting * in univariate time series . We could of course extend this method , in a similar way as our model , to work for multivariate instead of univariate time series , so this was not the main limitation . The main reason why it is not included in our comparison is that , although the transformer decoder is well suited for the generative task of subsequent values prediction and as such it is used for forecasting , it is not suitable for sequence classification and regression , the tasks our model and all other baselines are evaluated on . This is not simply a problem of parameter parsimony ( although the additional decoder would indeed necessitate roughly as many parameters as the encoder ) : the decoder requires a _target sequence as input which differs from the input of the encoder_ and constitutes what the decoder should generate , it employs a decoder-encoder attention scheme where target representations are compared to the encoder \u2019 s input representations , and it generates a _sequence of successive output values_ . This objective is very different from our objectives of classification and regression . Note that , having classification in mind , the BERT paper similarly dispensed with the decoder . At this point we take the opportunity to clarify what * regression * means in the specific context of our work and the time series literature we refer to , because we realize that this was unfortunately not very clear in our original manuscript . We thus address the question raised by the reviewer in their point 6 : \u201c What kind of regression task is used in your regression experiments ? Is it a one-step-ahead prediction task ? \u201d . Regression here means predicting a single numeric value for a given sequence ( time series sample ) . This numeric value is of a different nature than the numerical data appearing in the time series : for example , given a sequence of simultaneous measurements of two-channel PPG ( photoplethysmogram ) signals , three-axis acceleration signals , and one-channel ECG ( electrocardiogram ) signals , the regression task is to predict the heart rate ; given a sequence of simultaneous temperature and humidity measurements of 9 rooms in a house , as well as weather and climate data such as temperature , pressure , humidity , wind speed , visibility and dewpoint , predict the total energy consumption in kWh of a house for that day . These scalar values are the numerical \u201c labels \u201d provided by the datasets to train and evaluate models . In the general case , one may want to predict multiple scalars ( or a vector ) instead of a single scalar ; this corresponds to the dimensionality $ n $ entering the equations in Section 3.1 . It is important to note here that the goal of our work is to develop a general framework for time series representation learning , which easily facilitates both unsupervised learning as well as several downstream objectives . As such , besides unsupervised pre-training , our method can perform imputation , regression , classification as well as forecasting ( the latter is implemented by modifying the objective mask , as outlined in the Appendix A.1 and Figure 4 ) . Instead , the model by Li et al.can only perform forecasting . We do not evaluate our method on forecasting datasets in the scope of this work , but we will certainly compare the two methods in future work on forecasting , because it would be interesting to assess the effect of the decoder . We also note that the method of Li et al.includes a \u201c log-sparse \u201d attention mechanism , which is employed to mitigate the $ O ( w^2 ) $ dependence of computational complexity and memory of self-attention on the input sequence length $ w $ , at the possible expense of predictive performance . This is useful for their intended purpose of forecasting in their datasets , because they have to consider very long input sequences ; by contrast , the datasets in the archives we have considered can be handled by the full attention mechanism , and thus we do not need to employ such a modification ."}, "3": {"review_id": "lE1AB4stmX-3", "review_text": "The authors targeted an important data format , multivariate time series , and extended the usage of the transformer to this format . The effort is appreciated as multivariate time series data is an important problem while the researches there is limited comparing to other sequences problems , e.g.language.Due to the simple idea of extension and similar works before [ 1 ] , I expected a higher quality of experiments and presentation of this paper . However , experiments and writing need significant improvement for the next submission . Major Concerns : 1 . The experiment 's presentation is confusing and I just picked a few examples from Table 1 : a . There is no standard deviation of the result . b.Some methods are consistently worse than others , like 1-NN-DTWD comparing to 5-NN-DTWD . I am not sure why they are needed in the table . c. Averaging the RMSE does not deliver much information , especially when there is no normalization on each dataset . For example , the BeijingPM10 will dominate the averaged result . 2.The structure of writing only has a lot of problems . For example , usually , the fully supervised should be presented before semi-supervised while the paper did in a reverse way . Another problem is the classification result , which seems an important part but the table is shown in the Appendix . 3.The selection of datasets needs more justifications . Reference : 1. https : //arxiv.org/pdf/2001.08317.pdf", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the formatting suggestions , all of which we will adopt in the revised manuscript . We will also add a section in the Appendix with a thorough explanation on how the datasets were selected . Finally , we will show standard deviations for our models . Thus , all voiced concerns by the reviewer will be addressed . Regarding presentation of results in Table 1 : with the exception of our models , which achieve consistently top performance across datasets , the performance of all other baseline methods significantly fluctuates across datasets , which necessitates displaying all models . In the reviewer \u2019 s example comparison pair , 5-NN-DTWD indeed always outperforms 1-NN-DTWD , however the weaker model in the pair , 1-NN-DTWD , outperforms model 5-NN-ED on some datasets , which in turn outperforms 5-NN-DTWD on some other datasets . To conclude , we don \u2019 t find a model in the table that is consistently worse than all other models and should thus be excluded . Additionally , the performance of the NN-DTWD method is typically sensitive on the number of Nearest Neighbors , and including models with 2 parameter values helps to better outline the performance of this model . Regarding reporting the average Root MSE in Table 1 : We attempt to illustrate performance differences from different perspectives . We agree that one should not primarily rely on the average RMSE to establish which method is best . This is why in the discussion we rely on the average rank ( Table 1 ) and individual ranks ( Table 7 ) , and also we compare our method with the overall second best method : \u201c On average , our models attain approx . 16 % lower RMSE than the overall second best model ( XGBoost ) , with absolute improvements varying among datasets from approx . 4 % to 36 % \u201d . We also show the relative performance difference between our method and whichever method ranks 2nd for each dataset in Table 7 . As the reviewer correctly points out , the average RMSE will be primarily affected by the \u201c most challenging \u201d datasets . Despite this limitation , we reported it in Table 1 because we found that in the case of some datasets , many models had very similar RMSE values , while their discrete rank would inevitably strongly differentiate between them ( e.g.the RMSE of models with rank 3 and rank 6 can be almost identical ) . Interestingly , sorting models by average RMSE leads to almost the same list as sorting by average rank , especially with respect to top performers , which indicates that this metric conveys information . However , we will now report a new metric $ r_j $ for each model $ j $ instead , the `` average relative difference from mean '' over $ N $ datasets ( details to follow ) . \u201c Due to the simple idea of extension and similar works before [ 1 ] , I expected a higher quality of experiments \u201d The work cited by the reviewer ( Wu et al . ) , which we also cite in the related work section , employs a _full transformer encoder-decoder_ architecture ( as the original by Vaswani et al ) for the sole purpose of * forecasting * in * univariate * time series , and evaluates it on a single dataset , against 3 reasonably competitive baselines . Instead , we employ a _transformer encoder_ architecture which supports forecasting , imputation , regression and classification for univariate and multivariate time series . Additionally : - Our proposed method represents an outstanding improvement on the state of the art in the context of time series classification and regression : evaluating it against 11 baselines on 6 different datasets for regression , and against 5 baselines on 11 different datasets for classification , our results clearly show that it performs significantly better than the best currently available methods , while no other method manages to meaningfully differentiate itself from the rest . Despite decades of innovation and meticulously engineered approaches on those problems , it is telling that the second best methods are XGBoost , which does not even take sequence order into account , and ROCKET , which is based on a linear classifier on top of a flat collection of randomly initialized convolutional filters . - Our method is the only method which has been shown to successfully leverage unlabeled data in order to improve performance and push the state of the art in time series regression and classification . Leveraging unlabeled time series data is immensely interesting for nearly all domains in the sciences and in industry . Importantly , we show that it can accomplish this even when the number of available unlabeled data points is very limited , and that it can in fact benefit even from reusing labeled data samples through unsupervised learning . - These were accomplished by 1 ) suitably adapting a model architecture which has never before been used for these problems , 2 ) employing a novel unsupervised objective which allows unsupervised representation learning , 3 ) developing a framework which allows the same model to be used for different objectives ( imputation , forecasting , classification , regression ) ."}}