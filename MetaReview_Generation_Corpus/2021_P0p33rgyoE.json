{"year": "2021", "forum": "P0p33rgyoE", "title": "Variational Intrinsic Control Revisited", "decision": "Accept (Poster)", "meta_review": "This paper revisits the under-explored \"implicit\" variant of Variational Intrinsic Control introduced by Gregor et al. They identify a flaw that biases the original formulation in stochastic environments and propose a fix.\n\nReviewers agree that there is a [at least a potential, R4] contribution here: \"even the description of what implicit VIC is trying to do is a novel contribution of this work\", in the words of R2, and \"the derivation has theoretical value and is not a simple re-derivation of VIC\", in R4's post-rebuttal remarks. Several reviewers raised significant concerns around clarity, which were addressed in an updated manuscript, which also provided new visualizations and new experiments which reviewers found compelling. All reviewers agreed that the revised manuscript was considerably improved.\n\nR4's score stands at the 5, with the other reviewers all standing at 6. R4's main concerns are around whether the missing term in the mutual information identified by the authors is a problem in practice on non-toy tasks (echoing somewhat R3's concerns re: high-dimensional tasks). While this is a valid concern, the function of a conference paper needn't necessarily be to (even attempt) to provide the final word on a matter. Identifying subtle issues such as the one brought forth in this manuscript and re-examining old ideas is a valuable service to the community, and this paper will serve as a beginning to a conversation rather than an end. The AC also considers themselves rather familiar with the original VIC paper, and found the results herein somewhat surprising and noteworthy.\n\nI recommend acceptance, but encourage the authors to incorporate remaining feedback in the camera-ready.", "reviews": [{"review_id": "P0p33rgyoE-0", "review_text": "# # Summary The paper points out a limitation of the implicit option version of the Variational Intrinsic Control ( VIC ) [ Gregor et al. , 2016 ] algorithm in the form of a bias in stochastic environments . Two algorithms are proposed that fix the limitation : the first requiring the size of state space to be known and the second which does not make such assumptions . Experiments on simple discrete state environments demonstrate that the original VIC algorithm works well only on deterministic environments whereas the proposed fix works well on the stochastic environments as well . # # Strengths - The paper provides a sound theoretical analysis of the limitation of the VIC implicit-option algorithm , the proposed fix and a practical algorithm ( * Algorithm 2 * ) . - A clear distinction is presented with respect to prior work . The differences between the proposed algorithm and VIC shows that the intrinsic reward now has an added term which depends on an approximate model of the transition probability distribution . # # Weaknesses - The paper focuses on a very specific and narrow topic without providing much stand-alone motivation for the same . It implicitly borrows motivation from prior work ( VIC , etc ) without providing its own . The rigorous mathematical derivations are simply re-deriving the VIC mutual information bounds with a new added term and with some extra details on how to do it with a gaussian mixture model . Overall , the paper seems like a minor extension of prior work . - Considering the experiments on partially observed environments presented in the VIC paper , this paper chooses a much simpler set of discrete environments for empirical analysis instead of stepping up to more complicated environments which would have strengthened both the motivation for fixing the bias of VIC and the empirical evidence of the GMM algorithm ( * Algorithm 2 * ) . - The paper 's contributions boil down to Eqn 6 ( the bias in VIC ) and the two proposed algorithms . However , the difficulty in reading the mathematical notations and expressions severely handicaps the reader 's ability to carefully understand these contributions . Some suggestions on improving the notation are provided below . # # Feedback to authors - The paper introduces extremely dense notation . The frequent overloading of symbols or use of similar looking symbols ( e.g. $ p , p^p , \\rho $ ) makes it quite difficult for the reader to parse each expression . I would recommend usage of longer variable names , e.g . : replace p - > gen , for generative model and replace q - > inf for inference models . Phrases are easier to parse than single character symbols . Also , colorizing certain important symbols can help -- especially for important distinctions such as the true probability distributions vs their estimates . - # # Post-rebuttal update Having read through all reviews and the author 's response , I am updating my assessment in light of the responses and new experiments . I agree with the authors that the derivation has theoretical value and is not a simple re-derivation of VIC . The new experiments and visualizations have been helpful ( I am happy with the author 's responses to R3 ) , but the overall clarity of the paper is still lacking due to the dense mathematical notation . In light of this , I am increasing my score from 4 - > 6 , slightly leaning towards acceptance .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your time in reviewing our work . We truly appreciate your comments and would like to improve our work based on your review . We thank you for the clear summary , strengths , weaknesses and feedback . As you mentioned , our paper is focused on improving the theoretical limitation of Variational Intrinsic Control ( VIC ) [ Gregor et al. , 2016 ] and does not provide our motivation and intuition . We will provide our motivation and intuition in the introduction during the discussion phase . Also , we think that we should run experiments in complicated environments to show the scalability and empirical applicability of Algorithm 2 . We will try to run additional experiments for this . We agree that the readability of our paper is poor due to the dense notations and complicated equations . We will try to modify our notations based on your feedback for better readability . Overall , we agree with your comments , however , we want to discuss one weakness mentioned above . 1 ) \u201d The rigorous mathematical derivations are simply re-deriving the VIC mutual information bounds with a new added term and with some extra details on how to do it with a gaussian mixture model . Overall , the paper seems like a minor extension of prior work. \u201d We want to explain that it is not simply re-deriving with the additional term and claim that our derivation is super-set of the one in Gregor et al . ( 2016 ) for implicit VIC . The estimate on MI with transitional models is not a variational lower bound on the true MI . This means that simply maximizing estimation with respect to each parameter like Gregor et al . ( 2016 ) doesn \u2019 t work in this case , hence it requires new and complex analysis on MI . ( That is why we express our estimate on MI as \u2018 variational estimation \u2019 instead of \u2018 variational bound \u2019 . ) To tackle this problem , we start with the absolute difference between the true MI and our estimate . However , this absolute symbol can not be removed until applying equation 5 which is only characteristic of implicit VIC , not of explicit VIC ( please see appendix B ) . We want to stress that this is not just simple re-derivation . The reason why we claim that our derivation is a superset of implicit VIC in Gregor et al . ( 2016 ) is that our Algorithm 1 is equal to implicit VIC under deterministic dynamics as explained in this paper . It looks like just a simple additional intrinsic reward term and extra updates are added . However , to ensure that they make our estimation more correct and maximize the mutual information under stochastic dynamics , we believe that complex analysis is inevitable ."}, {"review_id": "P0p33rgyoE-1", "review_text": "This paper studies the problem of maximizing empowerment in the context of RL , where the aim is to maximize the mutual information between some latent variable and future outcomes ( e.g. , future states ) . The paper first observes that a procedure proposed in prior work [ Gregor 16 ] is biased and hence does not recover a ( latent-conditioned ) policy that maximizes mutual information . The paper then proposes a new method , based on learning the transition dynamics . that does recover the optimal ( mutual information maximizing ) policy . Experiments on a few simple tasks show that the proposed method outperforms prior methods . * * Significance * * : Empowerment remains one of the main methods for autonomous skill discovery . Thus , a better understanding of how to optimize empowerment would be an important contribution to this area . This paper identifies a limitation ( a biased objective ) in a commmon formulation of empowerment [ Gregor 16 ] , and proposes a method to correct for this . I think the significance of this paper hinges on ( 1 ) how large this bias is for reasonably complex tasks , and ( 2 ) if this type of bias might occur in other RL objectives , besides empowerment . The paper only convincingly shows that removing this bias is useful for maximizing empowerment on small scale problems . * * Novelty * * : To the best of my knowledge , this limitation of VIC has not been discussed in prior work . * * Experiments * * * It would be great to include visualizations or ablation experiments to illustrate why implicit VIC has a lower empowerment than the two proposed methods . * It 'd also be good to include * explicit * VIC as a baseline , even though it requires pre-specifying the number of skills . * The experiments are limited to very simple gridworlds and tree domains . * * Clarity * * * The derivation of the variational bias in S2 is pretty hard to follow . I 'd recommend including a bit more discussion of what implicit VIC is and how it differs from explicit VIC , before continuing with the formal derivation . * S3 and especially S4 are also hard to follow . I 'd recommend moving most of the derivation to the appendix and just stating the final objective as an equation . Theoretical guarantees can then be stated as Theorems/Lemmas with proper proofs . Overall , I give this paper a score of 5 / 10 , primarily because of ( 1 ) a lack of clarity and ( 2 ) the limited experiments . I would increase my score if the clarify of writing were ( greatly ) improved , if experiments on higher dimensional tasks were added ( e.g. , see those in [ Achaim 18 , Eysenbach 18 ] ) , and if additional visualizations of the ( suboptimal ) behavior of implicit VIC were added . * * Questions for discussion : * * * How significant is the bias in implicit VIC [ Gregor ] in more complex tasks ? Is it significant enough to warrant the additional complexity of the proposed approach ? * Is the GMM approach in S4 just a special case of the model learning approach in S3 , where the model is taken to be a GMM ? * Does * explicit * VIC have the same bias as * implicit * VIC ? * * Minor comments * * * `` methods for measuring it '' -- This makes it sound like Arimoto + Blahut proposed methods for measuring empowerment . I 'd revise to `` along with methods for measuring it based on Expectation Maximization [ Arimoto + Blahut ] '' * `` This can severely degrade the empowerment '' -- Clarify what this means . * `` This type of option differs ... '' -- Are n't there two differences ? ( 1 ) Closed loop options depend on the state at each time step , and ( 2 ) these options include a termination condition . * `` which hinders the maximal level of learning '' -- Add a citation . * `` implicit VIC which defines the option as the trajectory until the termination '' -- This sentence is confusing without knowing about the method apriori . * `` becomes possible to learn the maximum number of options for the given environment '' -- Add a citation . * `` achieve the maximal empowerment '' -- It 'd be good to formally define what the `` maximal empowerment '' means . * `` environment dynamics modeling incorporating the transitional probability '' -- Grammar error . * `` is the inference to be trained '' - > `` is the inference network/model to be trained '' * Eq 4 : Where are \\tau_t and s_ { g | \\Omega } defined ? * In S3 , it might be clearer to use q ( ... ) instead of p^q ( ... ) . * In Eq 15 , it 's unclear how \\log p_\\rho^p depends on \\theta_\\pi^q . * `` when the cardinality of the state space is unknown '' -- Where does the * cardinality * of the state space show up as a dependency ? Perhaps what is meant is that the method is most readily applicable to * discrete * settings , where the distribution over future states can be approximated exactly , without sampling . * S4 : I 'd recommend providing some intuition for why this alternative method is being derived . Is it going to address a limitation of the method in S3 ? * `` Gaussian Mixture Model ( GMM ) ( Reynolds & Rose , 1995 ) '' -- I think GMMs existed before 1995 . E.g. , see early work by Karl Pearson . * `` extreme gradient '' -- What is an extreme gradient ? * `` revisited the variational '' - > `` revisited variational '' - * * Update after author response * * : Thanks to the authors for answering my questions during the rebuttal paper and for incorporating the feedback into the paper ! My original concerns were about clarity , high-dimensional experiments , and visualizations . Since the paper has been revised to include nice visualizations and improve the clarity , I am increasing my score 5 - > 6 . I think the experiments on HalfCheetah are a great proof-of-concept of the method ! I 'd encourage the authors to include some comparisons against baselines for that task .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your time in reviewing our work . We truly appreciate your comments and would like to improve our work based on your review . We thank you for the clear organization of your feedback on our paper . We would like to respond to your feedback based on your organization . * * Significance * * : We agree with the suggested significances . We will explain the significance ( 1 ) by answering the questions for discussion . However , it is hard to mention significance ( 2 ) by now since it requires more surveys and analyses . * * Experiments * * : We will visualize the agent 's trajectory after training to show the difference in behavior . We agree that we should expand our experiments to complicated environments and we will try to run an additional experiment for this . We agree that it would be good to include results of explicit VIC and we will try to include the training result of explicit VIC if time allows . * * Clarity * * : We agree that our paper is lack of explanation about implicit VIC and especially for explicit VIC . We will explain them in S2 for better understanding . Also , we apologize for the poor readability of S3 and S4 . We will try to simplify and re-write our notations for better readability and push more details to appendices . * * Answer to questions for discussion * * : 1 ) '' How significant is the bias in implicit VIC [ Gregor ] in more complex tasks ? Is it significant enough to warrant the additional complexity of the proposed approach ? '' As we can see from equation 8 , this bias comes from the difference between transitional probabilities with given and ungiven final state . For bigger bias , their difference should be large and it means that $ s_f $ should play a crucial role in the nominator . If $ s_f $ does not play a role at all , i.e. , $ s_f $ is independent of $ s_t $ , a_t and $ s_ { t+1 } $ then the nominator and the denominator are the same which results in no bias . Large bias happens when $ s_ { t+1 } $ is the necessary state to reach $ s_f $ while $ s_ { t+1 } $ is not the only transition from $ s_ { t } $ and $ a_ { t } $ . In this case , the nominator is $ 1 $ and it generates a large bias . That is why implicit VIC shows severe degradation in our stochastic tree environment . For a given trajectory , all transitions are necessary with the given $ s_f $ in this environment . We think that the amount of bias depends on the characteristic of the environment , not the complexity of it . Another example with large bias is the environment with the pairs of keys and doors since a key is necessary to pass a door . 2 ) '' Is the GMM approach in S4 just a special case of the model learning approach in S3 , where the model is taken to be a GMM ? '' It is not a special case of S3 and they are different . S3 directly models the transitional probabilities . If we model them using a neural network , it requires the cardinality of state space to set the number of nodes for softmax function . The prerequisite of known cardinality is a clear limitation in this case and S4 is suggested to overcome this limitation . If we can model discrete probability without knowing the cardinality of the state space , S3 will be a better solution than S4 . 3 ) '' Does explicit VIC have the same bias as implicit VIC ? '' No , explicit VIC does not have the transitional bias since its option does not contain any transition . * * minor comments * * : We appreciate all your detailed comments in here . We will improve our paper based on comments . Let us answer questions in minor comments . 1 ) '' This type of option differs ... '' Yes , there are two differences but we neglected closed-loop options since it is mentioned just before this sentence . We will make it clear . 2 ) '' Eq 4 : Where are \\tau_t and s_ { g | \\Omega } defined ? '' We use the same definition of $ \\tau $ as Eq 3 so we omitted it . We will define $ \\tau_t $ in the text since both Eq 3 and Eq 4 use this . 3 ) '' In Eq 15 , it 's unclear how \\log p_\\rho^p depends on \\theta_\\pi^q . '' We can not find that \\log p_\\rho^p depends on \\theta_\\pi^q in Eq.4 ) '' Where does the cardinality of the state space show up as a dependency ? ... '' The dependency on the cardinality of the state space is explained in 'Answer to questions for discussion - 2 ) ' . 5 ) '' S4 : I 'd recommend providing some intuition for why this alternative method is being derived . Is it going to address a limitation of the method in S3 ? '' The dependency on the cardinality of the state space is a limitation of S3 . S4 is suggested to overcome this limitation . 6 ) '' extreme gradient -- What is an extreme gradient ? '' The expression of 'extreme gradient ' means that both huge and tiny gradient exists in a very narrow region . For a very small value of standard deviation , the Gaussian distribution function becomes like a delta function which has zero gradient at the mean , infinite gradient around the mean and zero gradient away from the mean . This can cause drastic changes in a small update near mean which results in instability of training ."}, {"review_id": "P0p33rgyoE-2", "review_text": "Summary of paper : The authors study a version of VIC that represents options as partial trajectories . They point out that , in stochastic environments , the implicit VIC formulation in the original paper is missing a term in the mutual information ( involving log likelihood ratios of state transitions ) . They introduce two ways of estimating the missing term : one appropriate for discrete state spaces , and another appropriate for continuous state spaces . They then compare the empowerment of implicit VIC with and without their corrections in a few toy domains , showing that their corrections do not hurt in deterministic environments , and provide a small increase in empowerment in stochastic environments . Pros : The missing term highlighted , and the derivations of solutions generally looked correct ( at least at the level I followed them ) . This is a potentially interesting contribution . Cons : 1 ) The experiments are a ) done only in toy domains and b ) even there demonstrate only a 5-10 % improvement in empowerment . These experiments are maybe fine as a sanity check , but they are not enough to demonstrate the importance of the authors \u2019 correction term . This is because empowerment is not an important objective in and of itself . Empowerment is used as a unsupervised pre-training step for RL , or as an exploration bonus in conjunction with RL . It is useful to the extent it aids performance in those settings . It is not clear to me whether the correction term is important in those pursuits . It could even be that focussing on the part of empowerment due to stochastic state transitions actually degrades the usefulness of the learnt policy . Additional experiments are needed . 2 ) In addition to experiments justifying the extra term , it would also be useful for the authors to include more motivation and intuition about why and when it is important . It is not immediately intuitively clear to me why incentivizing an agent to drastically alter its trajectory based on random state transitions is useful . 3 ) In general , the paper is hard to follow . There are long blocks of equations without enough exposition . New terms are defined without motivating first why they are being introduced . The notation is often too dense ( e.g.p^p_p ? really ? ) .I mostly felt \u201c in-the-dark \u201d as to where the authors were going while reading the paper . Perhaps they could streamline the derivations , push some of it to the appendix , and spend more time in the main text on motivation and intuition . 4 ) Given the environments aren \u2019 t standard and are very simple , they should definitely be introduced in the main text , and not pushed to the appendix .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your time in reviewing our work . We truly appreciate your comments and would like to improve our work based on your review . We thank you for the well-organized cons . We generally agree with the above cons but for some part of it , we want to add our explanations . Before we start the discussion , please let us make clear the strength of Algorithm 2 . Algorithm 2 alleviates the limitation of Algorithm 1 , the prerequisite of the known cardinality of the state space and is not only for continuous state space . ( It is also applicable to continuous state space . ) We will appreciate you if you recognize this point . Now please let us discuss the cons . 1 ) \u201c The experiments are a ) done only in toy domains and b ) even there demonstrate only a 5-10 % improvement in empowerment. \u201d We want to explain that the empowerment of a random agent ( uniform distribution of the policy ) is not zero and the improvement of the empowerment by our extension depends on the stochasticity of a given environment . In that sense , the empowerment gain compared to a random agent could be much larger than 5-10 % in more stochasticity . From this con , we think that the stochasticities used in this paper were not enough to show meaningful improvement in empowerment . We will increase the stochasticity of the environments , re-run the experiments , and show the empowerment gain compared to a random agent . 2 ) \u201c Empowerment is used as a unsupervised pre-training step for RL , or as an exploration bonus in conjunction with RL . It is useful to the extent it aids performance in those settings. \u201d We agree with this con . We will run additional experiments with external rewards in the same environments after finishing training by VIC . 3 ) \u201c In addition to experiments justifying the extra term , it would also be useful for the authors to include more motivation and intuition about why and when it is important. \u201d We agree with this con . We will try to include more motivation and intuition in the introduction . 4 ) \u201c In general , the paper is hard to follow . There are long blocks of equations without enough exposition. \u201d We apologize for the poor readability of this paper . We will try to improve our notations and write motivations of definitions . 5 ) \u201c Given the environments aren \u2019 t standard and are very simple , they should definitely be introduced in the main text , and not pushed to the appendix. \u201d We wanted to put our environmental details in the main text , however , the limit of 8 pages made us unavoidable to push it to the appendix . Since our estimate of MI with transitional models is not a variational lower bound on the true MI as in Variational Intrinsic Control ( VIC ) [ Gregor et al. , 2016 ] , it requires new and complex analysis on MI which makes our paper full of dense equations even though we pushed all of the derivations to appendices . We will try to simplify our notations and expressions and we will introduce it in the main text if the page limit allows ."}, {"review_id": "P0p33rgyoE-3", "review_text": "EDIT : The qualitative results help illustrate what the variational bias entails in practice , and indeed the worse coverage constitutes a problem worth overcoming . The Ant experiment was a good attempt at showing scalability , but the deterministic version is n't terribly informative since then the correction term does nothing . Could add stochasticity by taking some number of random actions between the states the agent sees . I suspect that as you increase stochasticity in this way the uncorrected method would degenerate . Would be a clear accept if you could show that , but as is the paper 's contribution is bordering on acceptance . 5 -- > 6 The authors show that implicit VIC is biased in stochastic environment due to its blindness to the effect of its 'option ' on the state transition dynamics . This is addressed by learning a model of these dynamics to allow for the calculation of the missing terms . Toy experiments are then performed that show the boost in mutual information caused by eliminating this bias . First off , its worth mentioning that this is the first real investigation into what intrinsic VIC actually optimizes . The original VIC paper only really explains things for the explicit case , so even the description of what implicit VIC is trying to do is a novel contribution of this work . That said , the primary merit of implicit VIC was its scalability , and the new requirement of a generative model of state dynamics can only hurt this . For this paper 's extension to be truly significant , it should show a case with significant state cardinality ( e.g.the 3D environment from the VIC paper ) where the bias hurts more than the reliance on the learned model . The GMM approximation suggests this could be possible , there are n't any experiments that really require its usage . In addition to the scale of the experiments , the breadth of evaluation could be expanded -- showing a gap in the MI is suggestive of a more more interesting gap in behavior . For example , can you use the reverse predictor q ( a | s , s_f ) to reach all possible states by forcing s_f to equal an arbitrary state ? 'Percent of states empirically achievable ' should be an easy metric to evaluate in these toy environments and would strengthen the case for your extension . I have a few minor complaints about the analysis itself . While generally easy to follow , the notation is a bit cumbersome . Once the options are explained in terms of the underlying trajectories , why stick with the option notation ? Eliminating the Omegas from the loss terms would be the implications of your extension much for explicit , though perhaps this would just make everything a bit too ugly . I do n't follow the significance of equation 5 , and would appreciate it being unpacked a bit more . All of the options considered are fixed-length in practice , so I 'd omit all of the bits regarding termination actions . What 's the difference between showing an upper-bound on the approximation error versus a lower-bound on the true value ? If the extension is n't a lower-bound , then the implications of this should be explained . Overall , I like this paper and wish more papers would be like this . Illuminating a previously neglected algorithm is worthwhile and I want to reward that . But suggesting a model-based approach for an algorithm which cited its model-free nature as a primary motivation requires a more thorough investigation . I 'm convinced that you 've found a flaw , but I 'm not convinced your solution actually improves things .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your time in reviewing our work . We truly appreciate your comments and would like to improve our work based on your review . We agree that we should run the experiments in high-dimensional environments to show the scalability of Algorithm 2 , but for the lack of time and resources , we focused on the main text . However , we will try to run additional experiments in high-dimensional environments during the discussion phase . Also , we will try to simplify the notation for better readability . We agree that the gap in MI may show interesting differences in behavior . We will try to show this difference if time allows . Now please let us discuss about your comments . 1 ) '' I do n't follow the significance of equation 5 '' We apologize for the absence of the mention of this significance . This equation is a key characteristic of implicit VIC which is not of explicit VIC . We often use this equation for simple expressions and derivations . The next discussion is also related to the significance of this equation . 2 ) '' What 's the difference between showing an upper-bound on the approximation error versus a lower-bound on the true value ? If the extension is n't a lower-bound , then the implications of this should be explained . '' The estimated MI with transitional models is not a lower bound on the true value . So we can not apply the variational lower bound technique used in Variational Intrinsic Control ( VIC ) [ Gregor et al. , 2016 ] and that is why we express as 'variational estimation ' instead of 'variational bound ' and we start from the absolute difference . For this reason , it requires a complex analysis on MI under stochastic dynamics which makes this paper full of dense expressions . Also , without utilizing equation 5 , we can not derive the upper bound in S3 . If you see inside Appendix B , right before applying equation 5 , the latter part is not a KL divergence , hence the absolute value can not be removed . The latter part can be simplified to KL divergence by using equation 5 and then the absolute value can be removed . 3 ) '' why stick with the option notation ? Eliminating the Omegas from the loss terms would be the implications of your extension much for explicit , though perhaps this would just make everything a bit too ugly . '' Since the length of the trajectory varies , using Omega allows us the simplicity of expressions . Otherwise , we need to sum loss over two variables , length of the trajectory and the transitions in the trajectory . We think that the implications of our extension are explicit in Algorithm 1 and Algorithm 2 pseudo-code with the additional term in intrinsic reward . 4 ) '' All of the options considered are fixed-length in practice , so I 'd omit all of the bits regarding termination actions . '' Only the maximum length of the trajectory is fixed in this paper . If we do not have this assumption , an infinite ( or very long ) length trajectory may occur during the training and it makes the experiment inefficient . Also , we did n't understand 'bits regarding termination actions ' . We will appreciate if you explain more detail about this ."}], "0": {"review_id": "P0p33rgyoE-0", "review_text": "# # Summary The paper points out a limitation of the implicit option version of the Variational Intrinsic Control ( VIC ) [ Gregor et al. , 2016 ] algorithm in the form of a bias in stochastic environments . Two algorithms are proposed that fix the limitation : the first requiring the size of state space to be known and the second which does not make such assumptions . Experiments on simple discrete state environments demonstrate that the original VIC algorithm works well only on deterministic environments whereas the proposed fix works well on the stochastic environments as well . # # Strengths - The paper provides a sound theoretical analysis of the limitation of the VIC implicit-option algorithm , the proposed fix and a practical algorithm ( * Algorithm 2 * ) . - A clear distinction is presented with respect to prior work . The differences between the proposed algorithm and VIC shows that the intrinsic reward now has an added term which depends on an approximate model of the transition probability distribution . # # Weaknesses - The paper focuses on a very specific and narrow topic without providing much stand-alone motivation for the same . It implicitly borrows motivation from prior work ( VIC , etc ) without providing its own . The rigorous mathematical derivations are simply re-deriving the VIC mutual information bounds with a new added term and with some extra details on how to do it with a gaussian mixture model . Overall , the paper seems like a minor extension of prior work . - Considering the experiments on partially observed environments presented in the VIC paper , this paper chooses a much simpler set of discrete environments for empirical analysis instead of stepping up to more complicated environments which would have strengthened both the motivation for fixing the bias of VIC and the empirical evidence of the GMM algorithm ( * Algorithm 2 * ) . - The paper 's contributions boil down to Eqn 6 ( the bias in VIC ) and the two proposed algorithms . However , the difficulty in reading the mathematical notations and expressions severely handicaps the reader 's ability to carefully understand these contributions . Some suggestions on improving the notation are provided below . # # Feedback to authors - The paper introduces extremely dense notation . The frequent overloading of symbols or use of similar looking symbols ( e.g. $ p , p^p , \\rho $ ) makes it quite difficult for the reader to parse each expression . I would recommend usage of longer variable names , e.g . : replace p - > gen , for generative model and replace q - > inf for inference models . Phrases are easier to parse than single character symbols . Also , colorizing certain important symbols can help -- especially for important distinctions such as the true probability distributions vs their estimates . - # # Post-rebuttal update Having read through all reviews and the author 's response , I am updating my assessment in light of the responses and new experiments . I agree with the authors that the derivation has theoretical value and is not a simple re-derivation of VIC . The new experiments and visualizations have been helpful ( I am happy with the author 's responses to R3 ) , but the overall clarity of the paper is still lacking due to the dense mathematical notation . In light of this , I am increasing my score from 4 - > 6 , slightly leaning towards acceptance .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your time in reviewing our work . We truly appreciate your comments and would like to improve our work based on your review . We thank you for the clear summary , strengths , weaknesses and feedback . As you mentioned , our paper is focused on improving the theoretical limitation of Variational Intrinsic Control ( VIC ) [ Gregor et al. , 2016 ] and does not provide our motivation and intuition . We will provide our motivation and intuition in the introduction during the discussion phase . Also , we think that we should run experiments in complicated environments to show the scalability and empirical applicability of Algorithm 2 . We will try to run additional experiments for this . We agree that the readability of our paper is poor due to the dense notations and complicated equations . We will try to modify our notations based on your feedback for better readability . Overall , we agree with your comments , however , we want to discuss one weakness mentioned above . 1 ) \u201d The rigorous mathematical derivations are simply re-deriving the VIC mutual information bounds with a new added term and with some extra details on how to do it with a gaussian mixture model . Overall , the paper seems like a minor extension of prior work. \u201d We want to explain that it is not simply re-deriving with the additional term and claim that our derivation is super-set of the one in Gregor et al . ( 2016 ) for implicit VIC . The estimate on MI with transitional models is not a variational lower bound on the true MI . This means that simply maximizing estimation with respect to each parameter like Gregor et al . ( 2016 ) doesn \u2019 t work in this case , hence it requires new and complex analysis on MI . ( That is why we express our estimate on MI as \u2018 variational estimation \u2019 instead of \u2018 variational bound \u2019 . ) To tackle this problem , we start with the absolute difference between the true MI and our estimate . However , this absolute symbol can not be removed until applying equation 5 which is only characteristic of implicit VIC , not of explicit VIC ( please see appendix B ) . We want to stress that this is not just simple re-derivation . The reason why we claim that our derivation is a superset of implicit VIC in Gregor et al . ( 2016 ) is that our Algorithm 1 is equal to implicit VIC under deterministic dynamics as explained in this paper . It looks like just a simple additional intrinsic reward term and extra updates are added . However , to ensure that they make our estimation more correct and maximize the mutual information under stochastic dynamics , we believe that complex analysis is inevitable ."}, "1": {"review_id": "P0p33rgyoE-1", "review_text": "This paper studies the problem of maximizing empowerment in the context of RL , where the aim is to maximize the mutual information between some latent variable and future outcomes ( e.g. , future states ) . The paper first observes that a procedure proposed in prior work [ Gregor 16 ] is biased and hence does not recover a ( latent-conditioned ) policy that maximizes mutual information . The paper then proposes a new method , based on learning the transition dynamics . that does recover the optimal ( mutual information maximizing ) policy . Experiments on a few simple tasks show that the proposed method outperforms prior methods . * * Significance * * : Empowerment remains one of the main methods for autonomous skill discovery . Thus , a better understanding of how to optimize empowerment would be an important contribution to this area . This paper identifies a limitation ( a biased objective ) in a commmon formulation of empowerment [ Gregor 16 ] , and proposes a method to correct for this . I think the significance of this paper hinges on ( 1 ) how large this bias is for reasonably complex tasks , and ( 2 ) if this type of bias might occur in other RL objectives , besides empowerment . The paper only convincingly shows that removing this bias is useful for maximizing empowerment on small scale problems . * * Novelty * * : To the best of my knowledge , this limitation of VIC has not been discussed in prior work . * * Experiments * * * It would be great to include visualizations or ablation experiments to illustrate why implicit VIC has a lower empowerment than the two proposed methods . * It 'd also be good to include * explicit * VIC as a baseline , even though it requires pre-specifying the number of skills . * The experiments are limited to very simple gridworlds and tree domains . * * Clarity * * * The derivation of the variational bias in S2 is pretty hard to follow . I 'd recommend including a bit more discussion of what implicit VIC is and how it differs from explicit VIC , before continuing with the formal derivation . * S3 and especially S4 are also hard to follow . I 'd recommend moving most of the derivation to the appendix and just stating the final objective as an equation . Theoretical guarantees can then be stated as Theorems/Lemmas with proper proofs . Overall , I give this paper a score of 5 / 10 , primarily because of ( 1 ) a lack of clarity and ( 2 ) the limited experiments . I would increase my score if the clarify of writing were ( greatly ) improved , if experiments on higher dimensional tasks were added ( e.g. , see those in [ Achaim 18 , Eysenbach 18 ] ) , and if additional visualizations of the ( suboptimal ) behavior of implicit VIC were added . * * Questions for discussion : * * * How significant is the bias in implicit VIC [ Gregor ] in more complex tasks ? Is it significant enough to warrant the additional complexity of the proposed approach ? * Is the GMM approach in S4 just a special case of the model learning approach in S3 , where the model is taken to be a GMM ? * Does * explicit * VIC have the same bias as * implicit * VIC ? * * Minor comments * * * `` methods for measuring it '' -- This makes it sound like Arimoto + Blahut proposed methods for measuring empowerment . I 'd revise to `` along with methods for measuring it based on Expectation Maximization [ Arimoto + Blahut ] '' * `` This can severely degrade the empowerment '' -- Clarify what this means . * `` This type of option differs ... '' -- Are n't there two differences ? ( 1 ) Closed loop options depend on the state at each time step , and ( 2 ) these options include a termination condition . * `` which hinders the maximal level of learning '' -- Add a citation . * `` implicit VIC which defines the option as the trajectory until the termination '' -- This sentence is confusing without knowing about the method apriori . * `` becomes possible to learn the maximum number of options for the given environment '' -- Add a citation . * `` achieve the maximal empowerment '' -- It 'd be good to formally define what the `` maximal empowerment '' means . * `` environment dynamics modeling incorporating the transitional probability '' -- Grammar error . * `` is the inference to be trained '' - > `` is the inference network/model to be trained '' * Eq 4 : Where are \\tau_t and s_ { g | \\Omega } defined ? * In S3 , it might be clearer to use q ( ... ) instead of p^q ( ... ) . * In Eq 15 , it 's unclear how \\log p_\\rho^p depends on \\theta_\\pi^q . * `` when the cardinality of the state space is unknown '' -- Where does the * cardinality * of the state space show up as a dependency ? Perhaps what is meant is that the method is most readily applicable to * discrete * settings , where the distribution over future states can be approximated exactly , without sampling . * S4 : I 'd recommend providing some intuition for why this alternative method is being derived . Is it going to address a limitation of the method in S3 ? * `` Gaussian Mixture Model ( GMM ) ( Reynolds & Rose , 1995 ) '' -- I think GMMs existed before 1995 . E.g. , see early work by Karl Pearson . * `` extreme gradient '' -- What is an extreme gradient ? * `` revisited the variational '' - > `` revisited variational '' - * * Update after author response * * : Thanks to the authors for answering my questions during the rebuttal paper and for incorporating the feedback into the paper ! My original concerns were about clarity , high-dimensional experiments , and visualizations . Since the paper has been revised to include nice visualizations and improve the clarity , I am increasing my score 5 - > 6 . I think the experiments on HalfCheetah are a great proof-of-concept of the method ! I 'd encourage the authors to include some comparisons against baselines for that task .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your time in reviewing our work . We truly appreciate your comments and would like to improve our work based on your review . We thank you for the clear organization of your feedback on our paper . We would like to respond to your feedback based on your organization . * * Significance * * : We agree with the suggested significances . We will explain the significance ( 1 ) by answering the questions for discussion . However , it is hard to mention significance ( 2 ) by now since it requires more surveys and analyses . * * Experiments * * : We will visualize the agent 's trajectory after training to show the difference in behavior . We agree that we should expand our experiments to complicated environments and we will try to run an additional experiment for this . We agree that it would be good to include results of explicit VIC and we will try to include the training result of explicit VIC if time allows . * * Clarity * * : We agree that our paper is lack of explanation about implicit VIC and especially for explicit VIC . We will explain them in S2 for better understanding . Also , we apologize for the poor readability of S3 and S4 . We will try to simplify and re-write our notations for better readability and push more details to appendices . * * Answer to questions for discussion * * : 1 ) '' How significant is the bias in implicit VIC [ Gregor ] in more complex tasks ? Is it significant enough to warrant the additional complexity of the proposed approach ? '' As we can see from equation 8 , this bias comes from the difference between transitional probabilities with given and ungiven final state . For bigger bias , their difference should be large and it means that $ s_f $ should play a crucial role in the nominator . If $ s_f $ does not play a role at all , i.e. , $ s_f $ is independent of $ s_t $ , a_t and $ s_ { t+1 } $ then the nominator and the denominator are the same which results in no bias . Large bias happens when $ s_ { t+1 } $ is the necessary state to reach $ s_f $ while $ s_ { t+1 } $ is not the only transition from $ s_ { t } $ and $ a_ { t } $ . In this case , the nominator is $ 1 $ and it generates a large bias . That is why implicit VIC shows severe degradation in our stochastic tree environment . For a given trajectory , all transitions are necessary with the given $ s_f $ in this environment . We think that the amount of bias depends on the characteristic of the environment , not the complexity of it . Another example with large bias is the environment with the pairs of keys and doors since a key is necessary to pass a door . 2 ) '' Is the GMM approach in S4 just a special case of the model learning approach in S3 , where the model is taken to be a GMM ? '' It is not a special case of S3 and they are different . S3 directly models the transitional probabilities . If we model them using a neural network , it requires the cardinality of state space to set the number of nodes for softmax function . The prerequisite of known cardinality is a clear limitation in this case and S4 is suggested to overcome this limitation . If we can model discrete probability without knowing the cardinality of the state space , S3 will be a better solution than S4 . 3 ) '' Does explicit VIC have the same bias as implicit VIC ? '' No , explicit VIC does not have the transitional bias since its option does not contain any transition . * * minor comments * * : We appreciate all your detailed comments in here . We will improve our paper based on comments . Let us answer questions in minor comments . 1 ) '' This type of option differs ... '' Yes , there are two differences but we neglected closed-loop options since it is mentioned just before this sentence . We will make it clear . 2 ) '' Eq 4 : Where are \\tau_t and s_ { g | \\Omega } defined ? '' We use the same definition of $ \\tau $ as Eq 3 so we omitted it . We will define $ \\tau_t $ in the text since both Eq 3 and Eq 4 use this . 3 ) '' In Eq 15 , it 's unclear how \\log p_\\rho^p depends on \\theta_\\pi^q . '' We can not find that \\log p_\\rho^p depends on \\theta_\\pi^q in Eq.4 ) '' Where does the cardinality of the state space show up as a dependency ? ... '' The dependency on the cardinality of the state space is explained in 'Answer to questions for discussion - 2 ) ' . 5 ) '' S4 : I 'd recommend providing some intuition for why this alternative method is being derived . Is it going to address a limitation of the method in S3 ? '' The dependency on the cardinality of the state space is a limitation of S3 . S4 is suggested to overcome this limitation . 6 ) '' extreme gradient -- What is an extreme gradient ? '' The expression of 'extreme gradient ' means that both huge and tiny gradient exists in a very narrow region . For a very small value of standard deviation , the Gaussian distribution function becomes like a delta function which has zero gradient at the mean , infinite gradient around the mean and zero gradient away from the mean . This can cause drastic changes in a small update near mean which results in instability of training ."}, "2": {"review_id": "P0p33rgyoE-2", "review_text": "Summary of paper : The authors study a version of VIC that represents options as partial trajectories . They point out that , in stochastic environments , the implicit VIC formulation in the original paper is missing a term in the mutual information ( involving log likelihood ratios of state transitions ) . They introduce two ways of estimating the missing term : one appropriate for discrete state spaces , and another appropriate for continuous state spaces . They then compare the empowerment of implicit VIC with and without their corrections in a few toy domains , showing that their corrections do not hurt in deterministic environments , and provide a small increase in empowerment in stochastic environments . Pros : The missing term highlighted , and the derivations of solutions generally looked correct ( at least at the level I followed them ) . This is a potentially interesting contribution . Cons : 1 ) The experiments are a ) done only in toy domains and b ) even there demonstrate only a 5-10 % improvement in empowerment . These experiments are maybe fine as a sanity check , but they are not enough to demonstrate the importance of the authors \u2019 correction term . This is because empowerment is not an important objective in and of itself . Empowerment is used as a unsupervised pre-training step for RL , or as an exploration bonus in conjunction with RL . It is useful to the extent it aids performance in those settings . It is not clear to me whether the correction term is important in those pursuits . It could even be that focussing on the part of empowerment due to stochastic state transitions actually degrades the usefulness of the learnt policy . Additional experiments are needed . 2 ) In addition to experiments justifying the extra term , it would also be useful for the authors to include more motivation and intuition about why and when it is important . It is not immediately intuitively clear to me why incentivizing an agent to drastically alter its trajectory based on random state transitions is useful . 3 ) In general , the paper is hard to follow . There are long blocks of equations without enough exposition . New terms are defined without motivating first why they are being introduced . The notation is often too dense ( e.g.p^p_p ? really ? ) .I mostly felt \u201c in-the-dark \u201d as to where the authors were going while reading the paper . Perhaps they could streamline the derivations , push some of it to the appendix , and spend more time in the main text on motivation and intuition . 4 ) Given the environments aren \u2019 t standard and are very simple , they should definitely be introduced in the main text , and not pushed to the appendix .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your time in reviewing our work . We truly appreciate your comments and would like to improve our work based on your review . We thank you for the well-organized cons . We generally agree with the above cons but for some part of it , we want to add our explanations . Before we start the discussion , please let us make clear the strength of Algorithm 2 . Algorithm 2 alleviates the limitation of Algorithm 1 , the prerequisite of the known cardinality of the state space and is not only for continuous state space . ( It is also applicable to continuous state space . ) We will appreciate you if you recognize this point . Now please let us discuss the cons . 1 ) \u201c The experiments are a ) done only in toy domains and b ) even there demonstrate only a 5-10 % improvement in empowerment. \u201d We want to explain that the empowerment of a random agent ( uniform distribution of the policy ) is not zero and the improvement of the empowerment by our extension depends on the stochasticity of a given environment . In that sense , the empowerment gain compared to a random agent could be much larger than 5-10 % in more stochasticity . From this con , we think that the stochasticities used in this paper were not enough to show meaningful improvement in empowerment . We will increase the stochasticity of the environments , re-run the experiments , and show the empowerment gain compared to a random agent . 2 ) \u201c Empowerment is used as a unsupervised pre-training step for RL , or as an exploration bonus in conjunction with RL . It is useful to the extent it aids performance in those settings. \u201d We agree with this con . We will run additional experiments with external rewards in the same environments after finishing training by VIC . 3 ) \u201c In addition to experiments justifying the extra term , it would also be useful for the authors to include more motivation and intuition about why and when it is important. \u201d We agree with this con . We will try to include more motivation and intuition in the introduction . 4 ) \u201c In general , the paper is hard to follow . There are long blocks of equations without enough exposition. \u201d We apologize for the poor readability of this paper . We will try to improve our notations and write motivations of definitions . 5 ) \u201c Given the environments aren \u2019 t standard and are very simple , they should definitely be introduced in the main text , and not pushed to the appendix. \u201d We wanted to put our environmental details in the main text , however , the limit of 8 pages made us unavoidable to push it to the appendix . Since our estimate of MI with transitional models is not a variational lower bound on the true MI as in Variational Intrinsic Control ( VIC ) [ Gregor et al. , 2016 ] , it requires new and complex analysis on MI which makes our paper full of dense equations even though we pushed all of the derivations to appendices . We will try to simplify our notations and expressions and we will introduce it in the main text if the page limit allows ."}, "3": {"review_id": "P0p33rgyoE-3", "review_text": "EDIT : The qualitative results help illustrate what the variational bias entails in practice , and indeed the worse coverage constitutes a problem worth overcoming . The Ant experiment was a good attempt at showing scalability , but the deterministic version is n't terribly informative since then the correction term does nothing . Could add stochasticity by taking some number of random actions between the states the agent sees . I suspect that as you increase stochasticity in this way the uncorrected method would degenerate . Would be a clear accept if you could show that , but as is the paper 's contribution is bordering on acceptance . 5 -- > 6 The authors show that implicit VIC is biased in stochastic environment due to its blindness to the effect of its 'option ' on the state transition dynamics . This is addressed by learning a model of these dynamics to allow for the calculation of the missing terms . Toy experiments are then performed that show the boost in mutual information caused by eliminating this bias . First off , its worth mentioning that this is the first real investigation into what intrinsic VIC actually optimizes . The original VIC paper only really explains things for the explicit case , so even the description of what implicit VIC is trying to do is a novel contribution of this work . That said , the primary merit of implicit VIC was its scalability , and the new requirement of a generative model of state dynamics can only hurt this . For this paper 's extension to be truly significant , it should show a case with significant state cardinality ( e.g.the 3D environment from the VIC paper ) where the bias hurts more than the reliance on the learned model . The GMM approximation suggests this could be possible , there are n't any experiments that really require its usage . In addition to the scale of the experiments , the breadth of evaluation could be expanded -- showing a gap in the MI is suggestive of a more more interesting gap in behavior . For example , can you use the reverse predictor q ( a | s , s_f ) to reach all possible states by forcing s_f to equal an arbitrary state ? 'Percent of states empirically achievable ' should be an easy metric to evaluate in these toy environments and would strengthen the case for your extension . I have a few minor complaints about the analysis itself . While generally easy to follow , the notation is a bit cumbersome . Once the options are explained in terms of the underlying trajectories , why stick with the option notation ? Eliminating the Omegas from the loss terms would be the implications of your extension much for explicit , though perhaps this would just make everything a bit too ugly . I do n't follow the significance of equation 5 , and would appreciate it being unpacked a bit more . All of the options considered are fixed-length in practice , so I 'd omit all of the bits regarding termination actions . What 's the difference between showing an upper-bound on the approximation error versus a lower-bound on the true value ? If the extension is n't a lower-bound , then the implications of this should be explained . Overall , I like this paper and wish more papers would be like this . Illuminating a previously neglected algorithm is worthwhile and I want to reward that . But suggesting a model-based approach for an algorithm which cited its model-free nature as a primary motivation requires a more thorough investigation . I 'm convinced that you 've found a flaw , but I 'm not convinced your solution actually improves things .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your time in reviewing our work . We truly appreciate your comments and would like to improve our work based on your review . We agree that we should run the experiments in high-dimensional environments to show the scalability of Algorithm 2 , but for the lack of time and resources , we focused on the main text . However , we will try to run additional experiments in high-dimensional environments during the discussion phase . Also , we will try to simplify the notation for better readability . We agree that the gap in MI may show interesting differences in behavior . We will try to show this difference if time allows . Now please let us discuss about your comments . 1 ) '' I do n't follow the significance of equation 5 '' We apologize for the absence of the mention of this significance . This equation is a key characteristic of implicit VIC which is not of explicit VIC . We often use this equation for simple expressions and derivations . The next discussion is also related to the significance of this equation . 2 ) '' What 's the difference between showing an upper-bound on the approximation error versus a lower-bound on the true value ? If the extension is n't a lower-bound , then the implications of this should be explained . '' The estimated MI with transitional models is not a lower bound on the true value . So we can not apply the variational lower bound technique used in Variational Intrinsic Control ( VIC ) [ Gregor et al. , 2016 ] and that is why we express as 'variational estimation ' instead of 'variational bound ' and we start from the absolute difference . For this reason , it requires a complex analysis on MI under stochastic dynamics which makes this paper full of dense expressions . Also , without utilizing equation 5 , we can not derive the upper bound in S3 . If you see inside Appendix B , right before applying equation 5 , the latter part is not a KL divergence , hence the absolute value can not be removed . The latter part can be simplified to KL divergence by using equation 5 and then the absolute value can be removed . 3 ) '' why stick with the option notation ? Eliminating the Omegas from the loss terms would be the implications of your extension much for explicit , though perhaps this would just make everything a bit too ugly . '' Since the length of the trajectory varies , using Omega allows us the simplicity of expressions . Otherwise , we need to sum loss over two variables , length of the trajectory and the transitions in the trajectory . We think that the implications of our extension are explicit in Algorithm 1 and Algorithm 2 pseudo-code with the additional term in intrinsic reward . 4 ) '' All of the options considered are fixed-length in practice , so I 'd omit all of the bits regarding termination actions . '' Only the maximum length of the trajectory is fixed in this paper . If we do not have this assumption , an infinite ( or very long ) length trajectory may occur during the training and it makes the experiment inefficient . Also , we did n't understand 'bits regarding termination actions ' . We will appreciate if you explain more detail about this ."}}