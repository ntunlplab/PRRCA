{"year": "2019", "forum": "S1xjdoC9Fm", "title": "Offline Deep models calibration with bayesian neural networks", "decision": "Reject", "meta_review": "Reviewers are in a consensus and recommended to reject after engaging with the authors. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.\n", "reviews": [{"review_id": "S1xjdoC9Fm-0", "review_text": "This paper presents an approach for calibrating the predictions of deep neural networks. The idea is quite simple and straightforward - simply use a more expressive model (Bayesian neural network with amortized inference). Surprisingly, the results show that this simple approach outperforms many of the recent approaches, such as those based on temperature scaling. A trick that they use is to control the KL term in the approximation of the ELBO using a hyperparameter (but this has been used in prior work on Bayesian neural nets). Overall, the idea as such is not that novel (just applying a Bayesian neural network with amortized inference) but the results look quite impressive. However, although the paper seems to advocate that a simple Bayesian neural network is enough to get well-calibrated probabilistic predictions, recent work has shown that even Bayesian uncertainties may be inaccurate, especially in case of model mis-specifications or due to the use of approximate inference. For example, see \"Accurate Uncertainties for Deep Learning Using Calibrated Regression\" ( Kuleshov et al, 2018). It is quite surprising that the proposed approach works so well but there isn't much of an insight as to why it works well. Is it the amortized inference that helps, or something else? I think a more detailed analysis needs to be done. Even some empirical analysis that, for example, shows that using MCMC gives inferior results than amortized inference would help here. Besides, I would also like to point out that there is some recent work on trainable calibration measures. See \"Trainable Calibration Measures For Neural Networks From Kernel Mean Embeddings\" (Kumar et al, 2018). It would be good to discuss this. Overall, the paper has a rather straightforward idea which seems to give good results. However, it doesn't offer any new insights as to why it works, especially since recent work, such as the one I mentioned above, has shown that taking a simple Bayesian approach that provides uncertainty estimates doesn't quite address the problem being studied here.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your review . We first you encourage to read the comment we post in this review with the title GENERAL COMMENTS . There , we have answered important issues commonly highlighted by more than one reviewer . We have checked your suggested article , `` Accurate Uncertainties for Deep Learning Using Calibrated Regression '' ( Kuleshov et al , 2018 ) . We think that in this work the Bayesian approach they used is obtained by Monte Carlo Dropout . However , as argued by the original authors in the appendix section 5.1.1 , this method yields outputs that are not calibrated . This can explain our calibration improvement using Bayesian techniques . We do not use Monte Carlo Dropout to get estimates of the predictive distribution . We use a separate Bayesian model instead . With respect to the comparison with MCMC algorithm , we respectfully disagree on the use of that technique , because MCMC algorithms are expensive when having big datasets . Moreover , MCMC algorithms have shown poorer performance than approximate Bayesian inference for this kind of task . However , as we discuss in section 5 , there is recent work that shows that approximate Bayesian inference leads to an additional gap in the bound . The good point of our approach is that we can incorporate the solutions proposed in those works to our method . Any Bayesian improvement can be incorporated to our approach , and we believe that this is one of its strengths . We will incorporate this in the discussion thanks for that : `` Trainable Calibration Measures for Neural Networks from Kernel Mean Embeddings '' ( Kumar et al , 2018 ) . However , if you check the results of that paper , our approach is competitive with them , and even better in some cases . Additionally , this work applies TS to their method to improve calibration , so again our method can be also applied instead of TS ."}, {"review_id": "S1xjdoC9Fm-1", "review_text": "-- Paper summary -- The primary goal of this paper is to investigate the suitability of BNNs for carrying out post-calibration on trained deep learning models. The results are compared to equivalent models calibrated using temperature scaling, and the proposed technique is shown to yield superior uncertainty calibration. -- General Commentary -- The overall goal of this work is rather modest and the scope of the evaluation is limited. While not without challenges, carrying out offline calibration as a corrective measure is a simpler problem to tackle than developing well-calibrated models upfront, and limiting the comparison to just one other post-calibration method greatly narrows the overall vision such a paper should have. For instance, isn\u2019t post-calibration more likely to result in overfitting than a model that is implicitly calibrated at training time? I have plenty of concerns with the submission itself, listed below: - First and foremost, the paper is full of typos and grammatical errors. I genuinely struggled to read the paper end-to-end without being continually distracted by these issues. While some mistakes may indeed be genuine, others are only there due to sheer negligence and because the authors didn\u2019t properly check the paper before submission. - While the overall objective of this work (i.e. improving calibration of deep models) is clearly established, the overall presentation of ideas is very muddled and I initially struggled to properly understand what\u2019s being proposed. A simple diagram or illustration would have clarified some of the notation at the very least. - The sloppiness in the presentation is also manifested in other ways. For example, in Figure 1, the plots should be individually titled (\u2018uncalibrated\u2019, \u2018temp-scal' and \u2018BNN') in order to immediately distinguish between them; instead, all this information is contained in the caption whereas it could just as easily have been added to the plot. - As alluded to earlier, I am disappointed by the lack of scope in the paper. The experimental evaluation should have been widened to include direct comparisons against BNN models which one might expect to be slightly better-calibrated upfront. There has also been significant interest in improving the calibration of deep models by stacking different architectures in such a way that the model is implicitly calibrated at training time. Examples of such papers include \u2018Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks\u2019 (Bradshaw et al, 2017), and \u2018Calibrating Deep Convolutional Gaussian Processes\u2019 (Tran et al, 2018). The deep kernel learning schemes developed by Wilson et al. also discuss similar hybrid models. - With reference to the papers cited above, one possible extension the authors could consider is to use a Gaussian process for post-calibration instead of a BNN, although I suspect this may have already been investigated in the past. In any case, this warrants further discussion. - I can\u2019t disentangle the two contributions listed at the bottom of Pg 2 and the top of Pg 3. There is no theoretical evaluation of the \u2018alternative hypothesis\u2019 being mentioned, and the investigation is entirely limited to the offline setting, so I\u2019m not entirely sure what distinction the authors are trying to make here. - In the same section, the authors then remark that \u2018Our results open new perspectives to improve the variational approximation\u2026\u2019 and \u2018we believe our results might foster further research in\u2026\u2019, before proceeding to list a dozen or so papers which might be inspired by this work. However, I can\u2019t really see how the single contribution being presented in this paper can have significant impact on the related work. I encourage the authors to substantiate their claims with more concrete examples rather than simply include vague mentions of other papers. - The structure and content of Section 3 is quite perplexing. Effectively, up until Equation 3, the authors are simply restating how to use VI for BNNs, with no mention whatsoever of how this fits in the storyline of model calibration. Whereas such a section should have contained novel methodology and/or intuition, the only reference to using BNNs for post-calibration is found in a small paragraph at the end of Pg 4, before immediately proceeding to the Experiments section. Once again, this makes any contributions of the paper unclear and inconclusive. Spurious comments such as the inconsequential connection to MDL further accentuate the paper\u2019s lack of identity and focus. - There are also some problematic technical details in this section, such as the definitive choice of using a two-layered BNN with no justification whatsoever. It is well known than plain BNNs also struggle to deliver well-calibrated outputs, and yet the authors immediately settle on a two-layered fully-connected network without stopping to consider whether some other network configuration or initialisation scheme might be more appropriate. Some introspection is later given in the experiment accompanied by Figure 2, but the analysis carried out there is just not sufficient. - There are some instances where the authors use text while in math mode, which gives poor formatting as exemplified by \u2018conf\u2019 in Equation 4. - Referring to \u2018datasets\u2019 as \u2018databases\u2019 in Section 4.1 is unusual. Some of the commentary in this subsection is also very difficult to interpret. For example, what is meant by \u2018uses BNNs\u2019? Does this mean that a BNN appears in the model being calibrated or is this referring to the BNN used to carry out calibration? The majority of these ambiguous statements could have been avoided had more care been given to checking the paper properly before submission. - In their discussion of the results, the authors state that \u2018We cannot conclude that BNNs are calibrating at the cost of losing accuracy\u2019, which I consider to be an overly sunny view of the results. Even if minor, a dip in accuracy is observed in almost every example provided in the Experiments section, dropping as much as 3% for CIFAR-100. Given that calibration is the primary focus of this paper, it might also be worth including another metic for validating this criteria, such as the Brier score. -- Recommendation -- Unfortunately, the material presented here is neither significant enough nor sufficiently explored to spark much interest. The overall scope of the paper is disappointingly limited, while novel ideas and design choices are poorly motivated and communicated throughout. This submission feels rushed and incomplete, and consequently well below the conference\u2019s standards. Pros/Cons summary: + The proposal yields good results in the provided experiments - Minor contributions that are not convincing enough - Muddled presentation of ideas - Dubious or weakly motivated design choices - Poorly written with plenty of typos - Difficult to follow", "rating": "3: Clear rejection", "reply_text": "Thanks for your review . We first you encourage to read the comment we post in this review with the title GENERAL COMMENTS . There , we have answered important issues commonly highlighted by more than one reviewer . We thank the reviewer for all the comments . We will immediately take your recommendations on figure formats , correct typos , etc . We apologize for the presentation and writing of the paper . You are totally right that this might be successfully corrected in future versions of the article , in order to make the reading more understandable . We found most of the rest of your points interesting to be discussed , and that is why we have incorporated them in the general comments in our separate post . Thanks for pointing all this , we think these kind of discussions help to make our approach stronger . About your point on the accuracy degradation of our proposed model , you are right . However , we did not see strong evidence of that , mainly because accuracy degradation is not present significantly in all the results presented . Moreover , please consider that our calibration is performed on very state of the art results in an off-line manner while other approaches mentioned in the literature calibrate models that are far from that state of the art . We are currently exploring approaches to solve this issue , obtaining promising results . We are also extending our experimental results with more datasets and tasks ."}, {"review_id": "S1xjdoC9Fm-2", "review_text": "This paper proposes the use of Bayesian inference techniques to mitigate the issues of miscalibration of modern Deep and Conv Nets. The presentation form of the paper is unsatisfactory. The paper seems to imply that Bayesian Deep Nets are used to calibrate Deep/Conv Nets, so I was expecting something like post-calibration using Bayesian Deep Nets. After reading through the paper a few times, it seems that the Authors are proposing the use of Bayesian inference techniques to infer parameters of Deep/Conv Nets in order to improve their calibration compared to non-Bayesian counterparts. This is the only contribution of the paper, and I believe it is insufficient. Guo et al., (2017) already points out that regularization of modern Deep/Conv nets improves calibration, so the fact that Bayesian Deep/Conv Nets are calibrated is not surprising, giving that the prior over the parameters act as a regularizer. It is surprising to see ECE values above one - unless these have been scaled by a factor of 100 - but this is not mentioned anywhere. Previous work shows that Monte Carlo Dropout for Conv Nets offers well calibrated predictions (https://arxiv.org/abs/1805.10522), so I think a comparison against this inference method should be included in the paper. The paper makes a number of imprecise claims/statements. A few examples: - \"Bayesian statistics make use of the predictive distribution to infer a random variable by computing the expected value of all the possible likelihood distributions. This is done under the posterior distribution of the likelihood parameters\" - very unclear and imprecise explanation of Bayesian inference - \"When using neural networks to model the likelihood\" - the likelihood is a function of the labels given model parameters", "rating": "3: Clear rejection", "reply_text": "Thanks for your review . We first you encourage to read the comment we post in this review with the title GENERAL COMMENTS . There , we have answered important issues commonly highlighted by more than one reviewer . In order to make our work clearer , in the overall answer you can find a much wider explanation . There , you will find that we did not calibrated directly the deep model , but their outputs , and this is what make our method more efficient and flexible than other approaches . We apologize for not being so clear in the submitted paper . On the other hand , maximum posterior maximization also induces a prior over the parameters for regularization . In this case the calibration is achieved as we compute an expected value using the posterior ( or its approximation ) of the parameters . This posterior includes the prior and the likelihood . However , we did not take the maximum of it , but just a weighted average , and we believe that is this average what gives us confidence calibrated outputs , not the fact that the prior is acting as a regularizer . Additionally , we have provided in the general comments a discussion on why this approach can give us calibrated probabilities . Regarding ECE , yes , it is given as a percentage . Although we stated that in the caption of the tables , we will make it more clear in later versions of the paper . We will make clearer definitions for : \u201c Bayesian Statistics make use of the predictive ... \u201d . Regarding the other comments by the reviewer , the likelihood , in this case , is a function of the labels , given model parameters and data . We use neural networks to parameterize this likelihood , but instead of training a point-estimated neural network , we train a distribution on model parameters . Maybe we should change the word \u201c model \u201d by the word \u201c parametrize \u201d in order to make the whole explanation clearer . Thanks for your fruitful suggestions in this sense ."}], "0": {"review_id": "S1xjdoC9Fm-0", "review_text": "This paper presents an approach for calibrating the predictions of deep neural networks. The idea is quite simple and straightforward - simply use a more expressive model (Bayesian neural network with amortized inference). Surprisingly, the results show that this simple approach outperforms many of the recent approaches, such as those based on temperature scaling. A trick that they use is to control the KL term in the approximation of the ELBO using a hyperparameter (but this has been used in prior work on Bayesian neural nets). Overall, the idea as such is not that novel (just applying a Bayesian neural network with amortized inference) but the results look quite impressive. However, although the paper seems to advocate that a simple Bayesian neural network is enough to get well-calibrated probabilistic predictions, recent work has shown that even Bayesian uncertainties may be inaccurate, especially in case of model mis-specifications or due to the use of approximate inference. For example, see \"Accurate Uncertainties for Deep Learning Using Calibrated Regression\" ( Kuleshov et al, 2018). It is quite surprising that the proposed approach works so well but there isn't much of an insight as to why it works well. Is it the amortized inference that helps, or something else? I think a more detailed analysis needs to be done. Even some empirical analysis that, for example, shows that using MCMC gives inferior results than amortized inference would help here. Besides, I would also like to point out that there is some recent work on trainable calibration measures. See \"Trainable Calibration Measures For Neural Networks From Kernel Mean Embeddings\" (Kumar et al, 2018). It would be good to discuss this. Overall, the paper has a rather straightforward idea which seems to give good results. However, it doesn't offer any new insights as to why it works, especially since recent work, such as the one I mentioned above, has shown that taking a simple Bayesian approach that provides uncertainty estimates doesn't quite address the problem being studied here.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your review . We first you encourage to read the comment we post in this review with the title GENERAL COMMENTS . There , we have answered important issues commonly highlighted by more than one reviewer . We have checked your suggested article , `` Accurate Uncertainties for Deep Learning Using Calibrated Regression '' ( Kuleshov et al , 2018 ) . We think that in this work the Bayesian approach they used is obtained by Monte Carlo Dropout . However , as argued by the original authors in the appendix section 5.1.1 , this method yields outputs that are not calibrated . This can explain our calibration improvement using Bayesian techniques . We do not use Monte Carlo Dropout to get estimates of the predictive distribution . We use a separate Bayesian model instead . With respect to the comparison with MCMC algorithm , we respectfully disagree on the use of that technique , because MCMC algorithms are expensive when having big datasets . Moreover , MCMC algorithms have shown poorer performance than approximate Bayesian inference for this kind of task . However , as we discuss in section 5 , there is recent work that shows that approximate Bayesian inference leads to an additional gap in the bound . The good point of our approach is that we can incorporate the solutions proposed in those works to our method . Any Bayesian improvement can be incorporated to our approach , and we believe that this is one of its strengths . We will incorporate this in the discussion thanks for that : `` Trainable Calibration Measures for Neural Networks from Kernel Mean Embeddings '' ( Kumar et al , 2018 ) . However , if you check the results of that paper , our approach is competitive with them , and even better in some cases . Additionally , this work applies TS to their method to improve calibration , so again our method can be also applied instead of TS ."}, "1": {"review_id": "S1xjdoC9Fm-1", "review_text": "-- Paper summary -- The primary goal of this paper is to investigate the suitability of BNNs for carrying out post-calibration on trained deep learning models. The results are compared to equivalent models calibrated using temperature scaling, and the proposed technique is shown to yield superior uncertainty calibration. -- General Commentary -- The overall goal of this work is rather modest and the scope of the evaluation is limited. While not without challenges, carrying out offline calibration as a corrective measure is a simpler problem to tackle than developing well-calibrated models upfront, and limiting the comparison to just one other post-calibration method greatly narrows the overall vision such a paper should have. For instance, isn\u2019t post-calibration more likely to result in overfitting than a model that is implicitly calibrated at training time? I have plenty of concerns with the submission itself, listed below: - First and foremost, the paper is full of typos and grammatical errors. I genuinely struggled to read the paper end-to-end without being continually distracted by these issues. While some mistakes may indeed be genuine, others are only there due to sheer negligence and because the authors didn\u2019t properly check the paper before submission. - While the overall objective of this work (i.e. improving calibration of deep models) is clearly established, the overall presentation of ideas is very muddled and I initially struggled to properly understand what\u2019s being proposed. A simple diagram or illustration would have clarified some of the notation at the very least. - The sloppiness in the presentation is also manifested in other ways. For example, in Figure 1, the plots should be individually titled (\u2018uncalibrated\u2019, \u2018temp-scal' and \u2018BNN') in order to immediately distinguish between them; instead, all this information is contained in the caption whereas it could just as easily have been added to the plot. - As alluded to earlier, I am disappointed by the lack of scope in the paper. The experimental evaluation should have been widened to include direct comparisons against BNN models which one might expect to be slightly better-calibrated upfront. There has also been significant interest in improving the calibration of deep models by stacking different architectures in such a way that the model is implicitly calibrated at training time. Examples of such papers include \u2018Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks\u2019 (Bradshaw et al, 2017), and \u2018Calibrating Deep Convolutional Gaussian Processes\u2019 (Tran et al, 2018). The deep kernel learning schemes developed by Wilson et al. also discuss similar hybrid models. - With reference to the papers cited above, one possible extension the authors could consider is to use a Gaussian process for post-calibration instead of a BNN, although I suspect this may have already been investigated in the past. In any case, this warrants further discussion. - I can\u2019t disentangle the two contributions listed at the bottom of Pg 2 and the top of Pg 3. There is no theoretical evaluation of the \u2018alternative hypothesis\u2019 being mentioned, and the investigation is entirely limited to the offline setting, so I\u2019m not entirely sure what distinction the authors are trying to make here. - In the same section, the authors then remark that \u2018Our results open new perspectives to improve the variational approximation\u2026\u2019 and \u2018we believe our results might foster further research in\u2026\u2019, before proceeding to list a dozen or so papers which might be inspired by this work. However, I can\u2019t really see how the single contribution being presented in this paper can have significant impact on the related work. I encourage the authors to substantiate their claims with more concrete examples rather than simply include vague mentions of other papers. - The structure and content of Section 3 is quite perplexing. Effectively, up until Equation 3, the authors are simply restating how to use VI for BNNs, with no mention whatsoever of how this fits in the storyline of model calibration. Whereas such a section should have contained novel methodology and/or intuition, the only reference to using BNNs for post-calibration is found in a small paragraph at the end of Pg 4, before immediately proceeding to the Experiments section. Once again, this makes any contributions of the paper unclear and inconclusive. Spurious comments such as the inconsequential connection to MDL further accentuate the paper\u2019s lack of identity and focus. - There are also some problematic technical details in this section, such as the definitive choice of using a two-layered BNN with no justification whatsoever. It is well known than plain BNNs also struggle to deliver well-calibrated outputs, and yet the authors immediately settle on a two-layered fully-connected network without stopping to consider whether some other network configuration or initialisation scheme might be more appropriate. Some introspection is later given in the experiment accompanied by Figure 2, but the analysis carried out there is just not sufficient. - There are some instances where the authors use text while in math mode, which gives poor formatting as exemplified by \u2018conf\u2019 in Equation 4. - Referring to \u2018datasets\u2019 as \u2018databases\u2019 in Section 4.1 is unusual. Some of the commentary in this subsection is also very difficult to interpret. For example, what is meant by \u2018uses BNNs\u2019? Does this mean that a BNN appears in the model being calibrated or is this referring to the BNN used to carry out calibration? The majority of these ambiguous statements could have been avoided had more care been given to checking the paper properly before submission. - In their discussion of the results, the authors state that \u2018We cannot conclude that BNNs are calibrating at the cost of losing accuracy\u2019, which I consider to be an overly sunny view of the results. Even if minor, a dip in accuracy is observed in almost every example provided in the Experiments section, dropping as much as 3% for CIFAR-100. Given that calibration is the primary focus of this paper, it might also be worth including another metic for validating this criteria, such as the Brier score. -- Recommendation -- Unfortunately, the material presented here is neither significant enough nor sufficiently explored to spark much interest. The overall scope of the paper is disappointingly limited, while novel ideas and design choices are poorly motivated and communicated throughout. This submission feels rushed and incomplete, and consequently well below the conference\u2019s standards. Pros/Cons summary: + The proposal yields good results in the provided experiments - Minor contributions that are not convincing enough - Muddled presentation of ideas - Dubious or weakly motivated design choices - Poorly written with plenty of typos - Difficult to follow", "rating": "3: Clear rejection", "reply_text": "Thanks for your review . We first you encourage to read the comment we post in this review with the title GENERAL COMMENTS . There , we have answered important issues commonly highlighted by more than one reviewer . We thank the reviewer for all the comments . We will immediately take your recommendations on figure formats , correct typos , etc . We apologize for the presentation and writing of the paper . You are totally right that this might be successfully corrected in future versions of the article , in order to make the reading more understandable . We found most of the rest of your points interesting to be discussed , and that is why we have incorporated them in the general comments in our separate post . Thanks for pointing all this , we think these kind of discussions help to make our approach stronger . About your point on the accuracy degradation of our proposed model , you are right . However , we did not see strong evidence of that , mainly because accuracy degradation is not present significantly in all the results presented . Moreover , please consider that our calibration is performed on very state of the art results in an off-line manner while other approaches mentioned in the literature calibrate models that are far from that state of the art . We are currently exploring approaches to solve this issue , obtaining promising results . We are also extending our experimental results with more datasets and tasks ."}, "2": {"review_id": "S1xjdoC9Fm-2", "review_text": "This paper proposes the use of Bayesian inference techniques to mitigate the issues of miscalibration of modern Deep and Conv Nets. The presentation form of the paper is unsatisfactory. The paper seems to imply that Bayesian Deep Nets are used to calibrate Deep/Conv Nets, so I was expecting something like post-calibration using Bayesian Deep Nets. After reading through the paper a few times, it seems that the Authors are proposing the use of Bayesian inference techniques to infer parameters of Deep/Conv Nets in order to improve their calibration compared to non-Bayesian counterparts. This is the only contribution of the paper, and I believe it is insufficient. Guo et al., (2017) already points out that regularization of modern Deep/Conv nets improves calibration, so the fact that Bayesian Deep/Conv Nets are calibrated is not surprising, giving that the prior over the parameters act as a regularizer. It is surprising to see ECE values above one - unless these have been scaled by a factor of 100 - but this is not mentioned anywhere. Previous work shows that Monte Carlo Dropout for Conv Nets offers well calibrated predictions (https://arxiv.org/abs/1805.10522), so I think a comparison against this inference method should be included in the paper. The paper makes a number of imprecise claims/statements. A few examples: - \"Bayesian statistics make use of the predictive distribution to infer a random variable by computing the expected value of all the possible likelihood distributions. This is done under the posterior distribution of the likelihood parameters\" - very unclear and imprecise explanation of Bayesian inference - \"When using neural networks to model the likelihood\" - the likelihood is a function of the labels given model parameters", "rating": "3: Clear rejection", "reply_text": "Thanks for your review . We first you encourage to read the comment we post in this review with the title GENERAL COMMENTS . There , we have answered important issues commonly highlighted by more than one reviewer . In order to make our work clearer , in the overall answer you can find a much wider explanation . There , you will find that we did not calibrated directly the deep model , but their outputs , and this is what make our method more efficient and flexible than other approaches . We apologize for not being so clear in the submitted paper . On the other hand , maximum posterior maximization also induces a prior over the parameters for regularization . In this case the calibration is achieved as we compute an expected value using the posterior ( or its approximation ) of the parameters . This posterior includes the prior and the likelihood . However , we did not take the maximum of it , but just a weighted average , and we believe that is this average what gives us confidence calibrated outputs , not the fact that the prior is acting as a regularizer . Additionally , we have provided in the general comments a discussion on why this approach can give us calibrated probabilities . Regarding ECE , yes , it is given as a percentage . Although we stated that in the caption of the tables , we will make it more clear in later versions of the paper . We will make clearer definitions for : \u201c Bayesian Statistics make use of the predictive ... \u201d . Regarding the other comments by the reviewer , the likelihood , in this case , is a function of the labels , given model parameters and data . We use neural networks to parameterize this likelihood , but instead of training a point-estimated neural network , we train a distribution on model parameters . Maybe we should change the word \u201c model \u201d by the word \u201c parametrize \u201d in order to make the whole explanation clearer . Thanks for your fruitful suggestions in this sense ."}}