{"year": "2021", "forum": "pAq1h9sQhqd", "title": "Stochastic Canonical Correlation Analysis: A Riemannian Approach", "decision": "Reject", "meta_review": "This paper gives a new algorithm for the CCA problem. The main idea of the new algorithm is to reformulate the matrices in the CCA problem as a product of three matrices: one orthonormal matrix, one rotation and one upper-diagonal matrix. The algorithm then performs remannian gradient descent to these components. The per-iteration complexity of the algorithm is O(d^2k) while the (local) convergence rate is O(1/t). Overall the reformulation is interesting and the algorithm seems effective in practice. On the other hand the convergence rate proof relies on local strong convexity and it's not clear why the algorithm converges globally (or even what is the radius of convergence locally).", "reviews": [{"review_id": "pAq1h9sQhqd-0", "review_text": "The paper presents an approach to find canonical directions in a streaming fashion , i.e.without direct calculation of covariance matrices ( which becomes hard when the number of examples is large ) . This solution to that task is not obvious , because the objective function of CCA , together with whitening constraints , does not allow simple additive decomposition . First , the optimization task is reformulated as a task over certain Riemannian manifolds , and a natural initialization is suggested . It is shown that under a certain assumption , this initialization is already a solution of good quality . Then , a natural minimization algorithm is presented , which is based on stochastic gradient descent on a Riemannian manifold . The key aspect of the algorithm is a combination of 2 types of gradient , the gradient for top-k principal vectors and standard gradient . The experimental part shows that the algorithm successfully solves CCA in a streaming fashion . Also , it can be effectively combined with deep feature learning ( Andrew , 2013 ) to find common features for multi-view representation learning tasks . Experiments look convincing .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for appreciating our work . We are happy to answer any additional questions or address any outstanding concerns . Yes , we included ( Andrew , 2013 ) in our deepCCA experiments shown in Table 1 ."}, {"review_id": "pAq1h9sQhqd-1", "review_text": "This paper aims to reduce the computational complexity of canonical correlation analysis.By decomposing the CCA projection matrices into a product of several structured matrices , a stochastic gradient-based optimization on a Riemannian manifold is provided reducing the computational complexity from $ d^3 $ to $ d^2k $ . Strength : CCA is a classic and still important method , especially in combination with deep neural networks ( e.g. , multi-view learnings ) . The proposed method enables the applications of CCA to high-dimensional vectors with small memory . This makes it easier to use CCA as an objective function of deep neural networks , which is trained on GPUs . Experiments show the benefits of their proposed method , in particular , the computational speed is 5-10 times faster than the existing method ( MSG ) . Weakness : Although the authors claim that their proposed method captures more correlation than MSG , two of the three datasets in which their method is superior ( MNIST and CIFAR ) are not realistic setting ( i.e. , correlation between left/right half of images ) . Question : Is it possible to make a ( numerical ) comparison with Yger+2012 , which reformulates CCA as an optimization on the generalized Stiefel manifold ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "1.Claim that their proposed method captures more correlation than MSG , two of the three datasets in which their method is superior ( MNIST and CIFAR ) are not realistic settings ( i.e. , correlation between left/right half of images ) . Ans : We chose these datasets mainly because they are commonly used test-beds in other papers which study the CCA model ( such as Ge et al.2016 and Andrew et al.2013 ) .At a minimum , they help in evaluating whether a model works satisfactorily in settings where one would certainly expect sufficient correlation . The use of CCA for the fairness experiment was designed to leverage the model \u2019 s ability to work in a high dimensional setting , which will otherwise present a key bottleneck . Other use cases which could benefit from a CCA type objective during the training process , appear to be feasible . 2.Is it possible to make a ( numerical ) comparison with Yger+2012 , which reformulates CCA as an optimization on the generalized Stiefel manifold ? Ans : We thank the reviewer for the suggestion . Yes , we will shortly update the paper with these experiments ."}, {"review_id": "pAq1h9sQhqd-2", "review_text": "main contribution This work offers a stochastic CCA algorithm based on Riemannian optimization approach Strength - The paper offers a theory-backed algorithm for CCA under the assumption that the two views are sub-Gaussian . The complexity-accuracy tradeoff of the algorithm seems to be appealing according to the experiments . Weakness Overall , the biggest concern is readability . The paper is very packed and many treatments seem to be unbalanced . Important details are missing , and proofs seem to be hastily . The paper may need some re-packaging and re-organization before its core technical contents could be easily followed . - readability . The biggest concern of the work is that it is very hard to read . This creates a lot of barriers in understanding key aspects of the paper , e.g. , contributions , formulations , novelty of the proof , just to name a few . The key formulation and the definitions of the manifolds were not clearly defined . The equivalence of ( 1 ) and ( 3 ) was not clearly shown . The theorems were presented in a bit abrupt way . Even the algorithm does not appear in the maintext but the appendix ( which is also hard to read ) . The color code is used in a way that is a bit confusing ( does ICLR allow writing in different colors ? ) . - Clarify about the contribution . It is hard to clearly see how much is the contribution . The proofs seem to be short and most of the proofs are presented using \u201c propositions \u201d cited from existing papers . If the authors think the contributions lie in reformulating the CCA problem as a PCA problem , then the reformulating part is perhaps the contribution . But from the current writing it is hard to follow how the reformulation comes through and how the reformulation enables using these existing propositions to prove convergence of the proposed algorithm . - It is unclear why the reformulation in ( 3 ) is a good approximation for CCA . Some attempts for justifying this were offered in Theorem 1 , but it was based on the assumption X and Y are sub-Gaussian , which is at best a special case , even if the proof is correct ( which , due to the current organization of the paper , is hard to read and fully understand ) . - \u201c SO ( n ) : group/manifold of nxn special orthogonal matrices. \u201d what is the definition of \u201c special \u201d here ? are they the commonly understood orthogonal matrices ? - The paper has this upper triangular structure of the S matrices but this point seems to have no detailed explanation . If one understands U = \\tilde { U } S as the QR decomposition , then indeed S is upper triangular , but why is there a Q in the middle ? Why is this Q useful ? - the \u201c high-level \u201d description of the algorithm says the idea is to connect CCA with PCA using this reformulation , but this point was not clearly explained . - Complexity . From the current writing , it is very hard to see how the complexity of the algorithm is calculated . The gradients needed are tabulated in the supplementary materials , but it is very hard for a reader to directly see why the algorithm saves computational complexity . - It is also unclear how the convergence and convergence rate analyses come together . These parts may need to be elaborated .", "rating": "4: Ok but not good enough - rejection", "reply_text": "1 . * Paper is packed * We hope that the reviewer agrees that most papers that draw upon a few different areas to derive the algorithm or to analyze its properties may appear to be packed . Indeed , some of the concepts can only be reviewed briefly given the page limits . This is not necessarily due to sloppy presentation . 2 . * Improve readability : contributions , formulations , etc * We request the reviewer to briefly look at page 3 again , if possible . Putting the analysis components aside temporarily , we are happy to clarify anything specific on page 3 regarding the contribution/formulation that is unclear . Page 3 describes what the overall model is ( shown in 2 ) , what a minor adjustment to it yields ( in 3a -- 3b ) and also provides an intuition about why this may potentially help . The subsequent sections actualize this intuition and show that this is reasonable by describing the analysis and the mechanics of the numerical scheme . 3 . * Equivalence of ( 1 ) and ( 3 ) was not clearly shown . * We should note that ( 1 ) and ( 2 ) are both standard for CCA . So we can focus on ( 3 ) . The reviewer can check that the adjustment from ( 2 ) -- ( 3 ) is actually minor : everything from ( 2 ) stays the same in ( 3 ) except that we further tease out the structure in $ U $ as $ U = \\tilde { U } A $ where $ A = QS $ , similarly for $ V $ . Q10 below will help easily clarify this doubt completely . 4 . * Theorems presented in a bit abrupt way . * We will much appreciate any guidance on a segue to Theorem 1 that will help address this concern . 5 . * Even the algorithm does not appear in the main text . * Alg.1 in the main paper is indeed the full pseudocode . The appendix simply writes out the low-level details of the gradient calculations , needed only if someone wants to cross-reference with our code . We thought that an explicit description of these calculations ( tedious but not difficult ) will only add marginal value and impact readability . 6 . * Color code is confusing . * The colors were used to easily reference the description in Section 2.2 with the algorithm blocks in Algorithm 1 . We are happy to modify it . 7 . * Main Contribution * We hope that the clarifications here and the other reviews will help resolve this concern fully . 8 . * Unclear why the reformulation in ( 3 ) is a good approximation for CCA . * We can clarify this doubt in two parts . First notice that ( 3 ) rewrites the standard CCA in ( 2 ) using the additional structure on U and V ( see Q10 below ) . Now , the second part deals with whether this is a sensible approximation . This is where we can check Theorem 1 : we show that under some assumptions , solution of ( 3 ) is a consistent estimator of the CC directions . For the non-Gaussian distribution , one can resort to a case by case analysis but will involve additional assumptions as well as adjustments to the algorithm making a concise presentation of the ideas difficult . 9 . * \u201c SO ( n ) \u201d : what is \u201c special \u201d here ? * $ SO ( n ) $ is the group of $ n\\times n $ orthogonal matrices with determinant 1 . Referring to it as the special orthogonal group is common practice ( https : //mathworld.wolfram.com/SpecialOrthogonalGroup.html ) . 10 . * Why is there a Q in the middle ? * Here $ \\tilde { U } $ is the matrix of principal vectors and $ U $ is the canonical directions ( similarly for $ V $ ) . A key observation is that $ U $ lies in the column span of $ \\tilde { U } $ , i.e. , $ U = \\tilde { U } A $ where we have a full rank matrix $ A $ . But optimization on the space of full rank matrices is more challenging since we need to satisfy the rank constraint . So , we may decompose $ A=QS $ where $ Q $ is orthogonal and $ S $ is an upper triangular matrix . Substituting $ QS $ by $ A $ , we get $ U = \\tilde { U } QS $ . We hope this addresses the doubt regarding ( 3a ) - ( 3b ) . This adjustment makes the optimization convenient . Constraints for orthogonal and upper triangular matrices can be implicitly preserved throughout the Riemannian gradient descent optimization without any explicit regularization . 11 . * How high-level description connects CCA with PCA using this reformulation * The key premise ( see Reviews 2 , 4 ) is that by treating the PCA and maximizing canonical correlation objective separately , we can achieve potential benefits . The PCA part enforces the whitening constraint . The CC directions lie in the principal subspaces , hence with an efficient scheme to compute principal directions we need to find the appropriate coefficients , which is enabled by ( 3 ) . 12 . * Hard to see how the complexity of algorithm is calculated . * Our algorithm involves structured matrices of size $ d\\times k $ and $ k\\times k $ , so any matrix operation should not exceed a cost of $ O ( \\max ( d^2k , k^3 ) ) $ , since in general $ d \\gg k $ , which directly gives $ O ( d^2k ) $ . Specifically , as mentioned in the paper , the most expensive calculation is SVD of matrices of size $ d\\times k $ , which is $ O ( d^2k ) $ ( Cline and Dhilon , Handbook of Linear Algebra , 2006 ) . All other calculations are dominated by this term ."}, {"review_id": "pAq1h9sQhqd-3", "review_text": "1.Paper summary : This paper proposes a method for solving linear CCA on high dimensional data . Linear CCA has a closed form solution . The solution requires a whitening step that costs O ( d^3 ) . This makes it not applicable to data in high dimensional spaces , e.g.representations learnt by deep networks . To resolve the issue , the authors propose a reformulation of linear CCA which decomposes the transformation matrix U ( V ) into a product of three matrices . Those three matrices have the following properties : - Their initial values can be obtained by efficient streaming PCA on original view matrix X ( Y ) . Streaming PCA costs O ( d^2 * k ) only where k is the top k eigenvectors . - They allow for Riemannian stochastic gradient descent which ensures their updated values lie on the same manifold . 2.Strong points of the paper : The new linear CCA formulation justifies the rationale of batch CCA training . Under Gaussian distribution assumption : - The absolute difference between correlation found by original linear CCA and stochastic one is bounded . - The convergence of the training process is proven . The experiments are performed on different aspects : - Recovering groundtruth transformations on MNIST , CIFAR and Mediamill data sets . - Learning deep features on MNIST data set . - Improving fairness in deep learning by adding CCA term to the loss function . 3.Weak points of the paper : The theoretical results are obtained under a strong assumption that X and Y both have Gaussian distribution . Most propositions are from other papers .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the review and appreciating our work . We clarify the main questions below , Q : The theoretical results are obtained under a strong assumption that X and Y both have Gaussian distribution . Ans : We answer in two parts , ( 1 ) Our setting closely followed the formulation described in a well known result on probabilistic CCA ( A probabilistic interpretation of canonical correlation analysis , ( Bach and Jordan , 2005 Tech report ) . Our convergence theorem ( Theorem 1 ) holds under identical assumptions on $ X $ and $ Y $ as described in probabilistic CCA . The reviewer will likely agree that this assumption is common , even in the batch setting of CCA , to derive convergence or consistency guarantees in the finite sample regime . If we inspect the analysis described in Appendix A.2 that accompanies the result in Prop 2 , we see that the steps will carry through as long as $ \\Delta $ is bounded . On a case by case basis , depending on the distributional assumption , some other assumptions will need to be imposed to ensure that this condition holds so that the analysis leads to the desired guarantees . ( 2 ) The key computational advantages that the algorithm offers , as noted by the reviewer , emerge from breaking down the CCA objective into PCA ( which satisfies the whitening constraint ) and the module which maximizes correlation . This modification leads to the computational complexity benefits . On the other hand , the consistency of our estimator to compute canonical directions requires consistency of the PC estimator as well . It is for this reason that the Gaussian assumption seemed sensible . Q : Most propositions are from other papers . Ans : The main theorem ( Theorem 1 ) is original to this paper while we do utilize or restate results from other articles , as needed , at various places in the analysis ."}], "0": {"review_id": "pAq1h9sQhqd-0", "review_text": "The paper presents an approach to find canonical directions in a streaming fashion , i.e.without direct calculation of covariance matrices ( which becomes hard when the number of examples is large ) . This solution to that task is not obvious , because the objective function of CCA , together with whitening constraints , does not allow simple additive decomposition . First , the optimization task is reformulated as a task over certain Riemannian manifolds , and a natural initialization is suggested . It is shown that under a certain assumption , this initialization is already a solution of good quality . Then , a natural minimization algorithm is presented , which is based on stochastic gradient descent on a Riemannian manifold . The key aspect of the algorithm is a combination of 2 types of gradient , the gradient for top-k principal vectors and standard gradient . The experimental part shows that the algorithm successfully solves CCA in a streaming fashion . Also , it can be effectively combined with deep feature learning ( Andrew , 2013 ) to find common features for multi-view representation learning tasks . Experiments look convincing .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for appreciating our work . We are happy to answer any additional questions or address any outstanding concerns . Yes , we included ( Andrew , 2013 ) in our deepCCA experiments shown in Table 1 ."}, "1": {"review_id": "pAq1h9sQhqd-1", "review_text": "This paper aims to reduce the computational complexity of canonical correlation analysis.By decomposing the CCA projection matrices into a product of several structured matrices , a stochastic gradient-based optimization on a Riemannian manifold is provided reducing the computational complexity from $ d^3 $ to $ d^2k $ . Strength : CCA is a classic and still important method , especially in combination with deep neural networks ( e.g. , multi-view learnings ) . The proposed method enables the applications of CCA to high-dimensional vectors with small memory . This makes it easier to use CCA as an objective function of deep neural networks , which is trained on GPUs . Experiments show the benefits of their proposed method , in particular , the computational speed is 5-10 times faster than the existing method ( MSG ) . Weakness : Although the authors claim that their proposed method captures more correlation than MSG , two of the three datasets in which their method is superior ( MNIST and CIFAR ) are not realistic setting ( i.e. , correlation between left/right half of images ) . Question : Is it possible to make a ( numerical ) comparison with Yger+2012 , which reformulates CCA as an optimization on the generalized Stiefel manifold ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "1.Claim that their proposed method captures more correlation than MSG , two of the three datasets in which their method is superior ( MNIST and CIFAR ) are not realistic settings ( i.e. , correlation between left/right half of images ) . Ans : We chose these datasets mainly because they are commonly used test-beds in other papers which study the CCA model ( such as Ge et al.2016 and Andrew et al.2013 ) .At a minimum , they help in evaluating whether a model works satisfactorily in settings where one would certainly expect sufficient correlation . The use of CCA for the fairness experiment was designed to leverage the model \u2019 s ability to work in a high dimensional setting , which will otherwise present a key bottleneck . Other use cases which could benefit from a CCA type objective during the training process , appear to be feasible . 2.Is it possible to make a ( numerical ) comparison with Yger+2012 , which reformulates CCA as an optimization on the generalized Stiefel manifold ? Ans : We thank the reviewer for the suggestion . Yes , we will shortly update the paper with these experiments ."}, "2": {"review_id": "pAq1h9sQhqd-2", "review_text": "main contribution This work offers a stochastic CCA algorithm based on Riemannian optimization approach Strength - The paper offers a theory-backed algorithm for CCA under the assumption that the two views are sub-Gaussian . The complexity-accuracy tradeoff of the algorithm seems to be appealing according to the experiments . Weakness Overall , the biggest concern is readability . The paper is very packed and many treatments seem to be unbalanced . Important details are missing , and proofs seem to be hastily . The paper may need some re-packaging and re-organization before its core technical contents could be easily followed . - readability . The biggest concern of the work is that it is very hard to read . This creates a lot of barriers in understanding key aspects of the paper , e.g. , contributions , formulations , novelty of the proof , just to name a few . The key formulation and the definitions of the manifolds were not clearly defined . The equivalence of ( 1 ) and ( 3 ) was not clearly shown . The theorems were presented in a bit abrupt way . Even the algorithm does not appear in the maintext but the appendix ( which is also hard to read ) . The color code is used in a way that is a bit confusing ( does ICLR allow writing in different colors ? ) . - Clarify about the contribution . It is hard to clearly see how much is the contribution . The proofs seem to be short and most of the proofs are presented using \u201c propositions \u201d cited from existing papers . If the authors think the contributions lie in reformulating the CCA problem as a PCA problem , then the reformulating part is perhaps the contribution . But from the current writing it is hard to follow how the reformulation comes through and how the reformulation enables using these existing propositions to prove convergence of the proposed algorithm . - It is unclear why the reformulation in ( 3 ) is a good approximation for CCA . Some attempts for justifying this were offered in Theorem 1 , but it was based on the assumption X and Y are sub-Gaussian , which is at best a special case , even if the proof is correct ( which , due to the current organization of the paper , is hard to read and fully understand ) . - \u201c SO ( n ) : group/manifold of nxn special orthogonal matrices. \u201d what is the definition of \u201c special \u201d here ? are they the commonly understood orthogonal matrices ? - The paper has this upper triangular structure of the S matrices but this point seems to have no detailed explanation . If one understands U = \\tilde { U } S as the QR decomposition , then indeed S is upper triangular , but why is there a Q in the middle ? Why is this Q useful ? - the \u201c high-level \u201d description of the algorithm says the idea is to connect CCA with PCA using this reformulation , but this point was not clearly explained . - Complexity . From the current writing , it is very hard to see how the complexity of the algorithm is calculated . The gradients needed are tabulated in the supplementary materials , but it is very hard for a reader to directly see why the algorithm saves computational complexity . - It is also unclear how the convergence and convergence rate analyses come together . These parts may need to be elaborated .", "rating": "4: Ok but not good enough - rejection", "reply_text": "1 . * Paper is packed * We hope that the reviewer agrees that most papers that draw upon a few different areas to derive the algorithm or to analyze its properties may appear to be packed . Indeed , some of the concepts can only be reviewed briefly given the page limits . This is not necessarily due to sloppy presentation . 2 . * Improve readability : contributions , formulations , etc * We request the reviewer to briefly look at page 3 again , if possible . Putting the analysis components aside temporarily , we are happy to clarify anything specific on page 3 regarding the contribution/formulation that is unclear . Page 3 describes what the overall model is ( shown in 2 ) , what a minor adjustment to it yields ( in 3a -- 3b ) and also provides an intuition about why this may potentially help . The subsequent sections actualize this intuition and show that this is reasonable by describing the analysis and the mechanics of the numerical scheme . 3 . * Equivalence of ( 1 ) and ( 3 ) was not clearly shown . * We should note that ( 1 ) and ( 2 ) are both standard for CCA . So we can focus on ( 3 ) . The reviewer can check that the adjustment from ( 2 ) -- ( 3 ) is actually minor : everything from ( 2 ) stays the same in ( 3 ) except that we further tease out the structure in $ U $ as $ U = \\tilde { U } A $ where $ A = QS $ , similarly for $ V $ . Q10 below will help easily clarify this doubt completely . 4 . * Theorems presented in a bit abrupt way . * We will much appreciate any guidance on a segue to Theorem 1 that will help address this concern . 5 . * Even the algorithm does not appear in the main text . * Alg.1 in the main paper is indeed the full pseudocode . The appendix simply writes out the low-level details of the gradient calculations , needed only if someone wants to cross-reference with our code . We thought that an explicit description of these calculations ( tedious but not difficult ) will only add marginal value and impact readability . 6 . * Color code is confusing . * The colors were used to easily reference the description in Section 2.2 with the algorithm blocks in Algorithm 1 . We are happy to modify it . 7 . * Main Contribution * We hope that the clarifications here and the other reviews will help resolve this concern fully . 8 . * Unclear why the reformulation in ( 3 ) is a good approximation for CCA . * We can clarify this doubt in two parts . First notice that ( 3 ) rewrites the standard CCA in ( 2 ) using the additional structure on U and V ( see Q10 below ) . Now , the second part deals with whether this is a sensible approximation . This is where we can check Theorem 1 : we show that under some assumptions , solution of ( 3 ) is a consistent estimator of the CC directions . For the non-Gaussian distribution , one can resort to a case by case analysis but will involve additional assumptions as well as adjustments to the algorithm making a concise presentation of the ideas difficult . 9 . * \u201c SO ( n ) \u201d : what is \u201c special \u201d here ? * $ SO ( n ) $ is the group of $ n\\times n $ orthogonal matrices with determinant 1 . Referring to it as the special orthogonal group is common practice ( https : //mathworld.wolfram.com/SpecialOrthogonalGroup.html ) . 10 . * Why is there a Q in the middle ? * Here $ \\tilde { U } $ is the matrix of principal vectors and $ U $ is the canonical directions ( similarly for $ V $ ) . A key observation is that $ U $ lies in the column span of $ \\tilde { U } $ , i.e. , $ U = \\tilde { U } A $ where we have a full rank matrix $ A $ . But optimization on the space of full rank matrices is more challenging since we need to satisfy the rank constraint . So , we may decompose $ A=QS $ where $ Q $ is orthogonal and $ S $ is an upper triangular matrix . Substituting $ QS $ by $ A $ , we get $ U = \\tilde { U } QS $ . We hope this addresses the doubt regarding ( 3a ) - ( 3b ) . This adjustment makes the optimization convenient . Constraints for orthogonal and upper triangular matrices can be implicitly preserved throughout the Riemannian gradient descent optimization without any explicit regularization . 11 . * How high-level description connects CCA with PCA using this reformulation * The key premise ( see Reviews 2 , 4 ) is that by treating the PCA and maximizing canonical correlation objective separately , we can achieve potential benefits . The PCA part enforces the whitening constraint . The CC directions lie in the principal subspaces , hence with an efficient scheme to compute principal directions we need to find the appropriate coefficients , which is enabled by ( 3 ) . 12 . * Hard to see how the complexity of algorithm is calculated . * Our algorithm involves structured matrices of size $ d\\times k $ and $ k\\times k $ , so any matrix operation should not exceed a cost of $ O ( \\max ( d^2k , k^3 ) ) $ , since in general $ d \\gg k $ , which directly gives $ O ( d^2k ) $ . Specifically , as mentioned in the paper , the most expensive calculation is SVD of matrices of size $ d\\times k $ , which is $ O ( d^2k ) $ ( Cline and Dhilon , Handbook of Linear Algebra , 2006 ) . All other calculations are dominated by this term ."}, "3": {"review_id": "pAq1h9sQhqd-3", "review_text": "1.Paper summary : This paper proposes a method for solving linear CCA on high dimensional data . Linear CCA has a closed form solution . The solution requires a whitening step that costs O ( d^3 ) . This makes it not applicable to data in high dimensional spaces , e.g.representations learnt by deep networks . To resolve the issue , the authors propose a reformulation of linear CCA which decomposes the transformation matrix U ( V ) into a product of three matrices . Those three matrices have the following properties : - Their initial values can be obtained by efficient streaming PCA on original view matrix X ( Y ) . Streaming PCA costs O ( d^2 * k ) only where k is the top k eigenvectors . - They allow for Riemannian stochastic gradient descent which ensures their updated values lie on the same manifold . 2.Strong points of the paper : The new linear CCA formulation justifies the rationale of batch CCA training . Under Gaussian distribution assumption : - The absolute difference between correlation found by original linear CCA and stochastic one is bounded . - The convergence of the training process is proven . The experiments are performed on different aspects : - Recovering groundtruth transformations on MNIST , CIFAR and Mediamill data sets . - Learning deep features on MNIST data set . - Improving fairness in deep learning by adding CCA term to the loss function . 3.Weak points of the paper : The theoretical results are obtained under a strong assumption that X and Y both have Gaussian distribution . Most propositions are from other papers .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the review and appreciating our work . We clarify the main questions below , Q : The theoretical results are obtained under a strong assumption that X and Y both have Gaussian distribution . Ans : We answer in two parts , ( 1 ) Our setting closely followed the formulation described in a well known result on probabilistic CCA ( A probabilistic interpretation of canonical correlation analysis , ( Bach and Jordan , 2005 Tech report ) . Our convergence theorem ( Theorem 1 ) holds under identical assumptions on $ X $ and $ Y $ as described in probabilistic CCA . The reviewer will likely agree that this assumption is common , even in the batch setting of CCA , to derive convergence or consistency guarantees in the finite sample regime . If we inspect the analysis described in Appendix A.2 that accompanies the result in Prop 2 , we see that the steps will carry through as long as $ \\Delta $ is bounded . On a case by case basis , depending on the distributional assumption , some other assumptions will need to be imposed to ensure that this condition holds so that the analysis leads to the desired guarantees . ( 2 ) The key computational advantages that the algorithm offers , as noted by the reviewer , emerge from breaking down the CCA objective into PCA ( which satisfies the whitening constraint ) and the module which maximizes correlation . This modification leads to the computational complexity benefits . On the other hand , the consistency of our estimator to compute canonical directions requires consistency of the PC estimator as well . It is for this reason that the Gaussian assumption seemed sensible . Q : Most propositions are from other papers . Ans : The main theorem ( Theorem 1 ) is original to this paper while we do utilize or restate results from other articles , as needed , at various places in the analysis ."}}