{"year": "2017", "forum": "HyY4Owjll", "title": "Boosted Generative Models", "decision": "Reject", "meta_review": "The idea of boosting has recently seen a revival, and the ideas presented here are stimulating. After discussion, the reviewers agreed that the latest updates and clarifications have improved the paper, but overall they still felt that the paper is not quite ready, especially in making the case for when taking this approach is desirable, which was the common thread of concern for everyone. For this reason, this paper is not yet ready for acceptance at this year's conference.", "reviews": [{"review_id": "HyY4Owjll-0", "review_text": "The authors propose two approaches to combine multiple weak generative models into a stronger one using principles from boosting. The approach is simple and elegant and basically creates an unnormalized product of experts model, where the individual experts are trained greedily to optimize the overall joint model. Unfortunately, this approach results in a joint model that has some undesirable properties: a unknown normalisation constant for the joint model and therefore an intractable log-likelihood on the test set; and it makes drawing exact samples from the joint model intractable. These problems can unfortunately not be fixed by using different base learners, but are a direct result of the product of experts formulation of boosting. The experiments on 2 dimensional toy data illustrate that the proposed procedure works in principle and that the boosting formulation produces better results than individual weak learners and better results than e.g. bagging. But the experiments on MNIST are less convincing: Without an undisputable measure like e.g. log-likelihood it is hard to draw conclusions from the samples in Figure 2; and visually they look weak compared to even simple models like e.g. NADE. I think the paper could be improved significantly by adding a quantitative analysis: investigating the effect of combining undirected (e.g. RBM), undirected (e.g. VAE) and autoregressive (e.g. NADE) models and by measuring the improvement over the number of base learners. But this would require a method to estimate the partition function Z or estimating some proxy. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your helpful comments . Please refer to the common rebuttal posted for response to questions regarding MNIST experiments . Regarding the concerns with the unnormalized final density , we highlight that probabilistic models frequently make a trade-off between expressiveness and tractability . Simple models such as NADE and mixture of Bernoullis make assumptions that lend tractability but are not very flexible in modeling complex structure in the data . On the other hand , latent variable models such as VAEs and RBMs have great expressive power but need to approximate intractable integrals . Boosted generative models also have a computationally intractable partition function with the trade-off made in expressiveness just like VAEs , RBMs , GANs ( where even the unnormalized log-likelihood can not be directly computed ) , etc . We argue that intractability is however not a deal-breaker since a ) . many use cases of generative models ( such as sampling , unsupervised feature learning ) do not require the partition function , b ) . wherever required , there are some generic techniques available for estimating the partition function ."}, {"review_id": "HyY4Owjll-1", "review_text": "This paper extends boosting to the task of learning generative models of data. The strong learner is obtained as a geometric average of \u201cweak learners\u201d, which can themselves be normalized (e.g. VAE) or un-normalized (e.g. RBMs) generative models (genBGM), or a classifier trained to discriminate between the strong learner at iteration T-1 and the true data distribution (discBGM). This latter method is closely related to Noise Contrastive Estimation, GANs, etc. The approach benefits from strong theoretical guarantees, with strict conditions under which each boosting iteration is guaranteed to improve the log-likelihood. The downside of the method appears to be the lack of normalization constant for the resulting strong learner and the use of heuristics to weight each weak learner (which seems to matter in practice, from Sec. 3.2). The discriminative approach further suffers from an expensive training procedure: each round of boosting first requires generating a \u201ctraining set\u201d worth of samples from the previous strong learner, where samples are obtained via MCMC. The experimental section is clearly the weak point of the paper. The method is evaluated on a synthetic dataset, and a single real-world dataset, MNIST: both for generation and as a feature extraction mechanism for classification. Of these, the synthetic experiments were the clearest in showcasing the method. On MNIST, the baseline models are much too weak for the results to be convincing. A modestly sized VAE can obtain 90 nats within hours on a single GPU, clearly an achievable goal. Furthermore, despite arguments to the contrary, I firmly believe that mixing base learners is an academic exercise, if only because of the burden of implementing K different models & training algorithms. This section fails to answer a more fundamental question: is it better to train a large VAE by maximizing the elbow, or e.g. train 10 iterations of boosting, using VAEs 1/10th the size of the baseline model ? Experimental details are also lacking, especially with respect to the sampling procedure used to draw samples from the BGM. The paper would also benefit from likelihood estimates obtained via AIS. With regards to novelty and prior work, there is also a missing reference to \u201cSelf Supervised Boosting\u201d by Welling et al [R1]. After a cursory read through, there seems to be strong similarities to the GenBGM approach which ought to be discussed. Overall, I am on the fence. The idea of boosting generative models is intriguing, seems well motivated and has potential for impact. For this reason, and given the theoretical contributions, I am willing to overlook some of the issues highlighted above, and hope the authors can address some of them in time for the rebuttal. [R1] https://papers.nips.cc/paper/2275-self-supervised-boosting.pdf PROS: Novel and intriguing idea Strong theoretical guarantees CONS: Resulting boosted model is un-normalized Discriminator based boosting is expensive, due to sampling via MCMC Weak experimental section ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your helpful comments . Please refer to the common rebuttal posted for response to questions regarding MNIST experiments . Additional comments : 1 . We have followed up on the reviewer \u2019 s suggestion of removing experiments that mix different base learners . We hope this makes the presentation clean . 2.We have restructured the related works section entirely for improved clarity , and also included a discussion on the differences of our framework with \u201c self-supervised boosting \u201d -- a reference we had missed out earlier and are duly thankful to the reviewer for highlighting . We have also included references to some very recent relevant preprints that came after our work for a more complete and up-to-date discussion . 3.Please read the response to Reviewer 4 below for an argument on why we feel that unnormalized models such as BGMs while certainly being limited are not a deal-breaker ."}, {"review_id": "HyY4Owjll-2", "review_text": "The paper proposes two approaches to boosting generative models, both based on likelihood ratio estimates. The approaches are evaluated on synthetic data, as well as on MNIST dataset for the tasks of generating samples and semi-supervised learning. While the idea of boosting generative models and the proposed methods are interesting, the reviewer finds the experiments unconvincing for the following reasons. 1. The bagging baseline in section 3.1 seems to be just refitting a model to the same dataset, raising the probability to power alpha, and renormalizing. This makes it more peaked, but it's not clear why this is a good baseline. Please let me know if I misunderstood the procedure. 2. The sample generation experiment in section 3.2 uses a very slowly converging Markov chain, as can be seen in the similarity of plots c and f, d and g, e and h. It seems unlikely therefore that the resulting samples are from the stationary distribution. A qualitative evaluation using AIS seems to be necessary here. 3. In the same section the choices for alphas seem quite arbitrary - what happens when a more obvious choice of alpha_i=1 for all i is made? 4. It seems hard to infer anything from the semisupervised classification results reported: the baseline RBM seems to perform as well as the boosted models. The work is mostly clearly written and (as far as the reviewer knows) original.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your helpful comments . Please refer to the common rebuttal posted for response to questions regarding MNIST experiments ( points 2 and 3 ) . 1.Yes , the bagging procedure described is correct . The purpose of including bagging as a baseline was to demonstrate that naively increasing model capacity by adding new models to the ensemble is not very effective in correcting for model misspecification , unlike boosting which involves a reweighting step in the GenBGM version or classifier training for the DiscBGM version . A similar approach ( called bagging ) competes with boosting for the supervised case . For brevity , we have removed this baseline from the latest version . 4.The experiments on semi-supervised classification have been removed in the latest version since the gain in accuracy with the current algorithm is unconvincing for a few cases . For completeness , we would like to clarify the purpose of including them earlier was to demonstrate a ) the computational gains that boosted generative models can provide on this task ( for example , the training time of RBM- > RBM was slightly more than half of the baseline RBM model yet it was able to match the performance of the baseline RBM model ) b ) improving models that are weaker at this task due to different inductive biases ( for example , VAE- > RBM gives better accuracy than the baseline VAE ) . Based on our preliminary experiments , we believe this task is an important use case of boosted generative models . However , in the absence of sufficient empirical evidence , we defer improved algorithms for semi-supervised classification based on the boosting framework to future work ."}], "0": {"review_id": "HyY4Owjll-0", "review_text": "The authors propose two approaches to combine multiple weak generative models into a stronger one using principles from boosting. The approach is simple and elegant and basically creates an unnormalized product of experts model, where the individual experts are trained greedily to optimize the overall joint model. Unfortunately, this approach results in a joint model that has some undesirable properties: a unknown normalisation constant for the joint model and therefore an intractable log-likelihood on the test set; and it makes drawing exact samples from the joint model intractable. These problems can unfortunately not be fixed by using different base learners, but are a direct result of the product of experts formulation of boosting. The experiments on 2 dimensional toy data illustrate that the proposed procedure works in principle and that the boosting formulation produces better results than individual weak learners and better results than e.g. bagging. But the experiments on MNIST are less convincing: Without an undisputable measure like e.g. log-likelihood it is hard to draw conclusions from the samples in Figure 2; and visually they look weak compared to even simple models like e.g. NADE. I think the paper could be improved significantly by adding a quantitative analysis: investigating the effect of combining undirected (e.g. RBM), undirected (e.g. VAE) and autoregressive (e.g. NADE) models and by measuring the improvement over the number of base learners. But this would require a method to estimate the partition function Z or estimating some proxy. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your helpful comments . Please refer to the common rebuttal posted for response to questions regarding MNIST experiments . Regarding the concerns with the unnormalized final density , we highlight that probabilistic models frequently make a trade-off between expressiveness and tractability . Simple models such as NADE and mixture of Bernoullis make assumptions that lend tractability but are not very flexible in modeling complex structure in the data . On the other hand , latent variable models such as VAEs and RBMs have great expressive power but need to approximate intractable integrals . Boosted generative models also have a computationally intractable partition function with the trade-off made in expressiveness just like VAEs , RBMs , GANs ( where even the unnormalized log-likelihood can not be directly computed ) , etc . We argue that intractability is however not a deal-breaker since a ) . many use cases of generative models ( such as sampling , unsupervised feature learning ) do not require the partition function , b ) . wherever required , there are some generic techniques available for estimating the partition function ."}, "1": {"review_id": "HyY4Owjll-1", "review_text": "This paper extends boosting to the task of learning generative models of data. The strong learner is obtained as a geometric average of \u201cweak learners\u201d, which can themselves be normalized (e.g. VAE) or un-normalized (e.g. RBMs) generative models (genBGM), or a classifier trained to discriminate between the strong learner at iteration T-1 and the true data distribution (discBGM). This latter method is closely related to Noise Contrastive Estimation, GANs, etc. The approach benefits from strong theoretical guarantees, with strict conditions under which each boosting iteration is guaranteed to improve the log-likelihood. The downside of the method appears to be the lack of normalization constant for the resulting strong learner and the use of heuristics to weight each weak learner (which seems to matter in practice, from Sec. 3.2). The discriminative approach further suffers from an expensive training procedure: each round of boosting first requires generating a \u201ctraining set\u201d worth of samples from the previous strong learner, where samples are obtained via MCMC. The experimental section is clearly the weak point of the paper. The method is evaluated on a synthetic dataset, and a single real-world dataset, MNIST: both for generation and as a feature extraction mechanism for classification. Of these, the synthetic experiments were the clearest in showcasing the method. On MNIST, the baseline models are much too weak for the results to be convincing. A modestly sized VAE can obtain 90 nats within hours on a single GPU, clearly an achievable goal. Furthermore, despite arguments to the contrary, I firmly believe that mixing base learners is an academic exercise, if only because of the burden of implementing K different models & training algorithms. This section fails to answer a more fundamental question: is it better to train a large VAE by maximizing the elbow, or e.g. train 10 iterations of boosting, using VAEs 1/10th the size of the baseline model ? Experimental details are also lacking, especially with respect to the sampling procedure used to draw samples from the BGM. The paper would also benefit from likelihood estimates obtained via AIS. With regards to novelty and prior work, there is also a missing reference to \u201cSelf Supervised Boosting\u201d by Welling et al [R1]. After a cursory read through, there seems to be strong similarities to the GenBGM approach which ought to be discussed. Overall, I am on the fence. The idea of boosting generative models is intriguing, seems well motivated and has potential for impact. For this reason, and given the theoretical contributions, I am willing to overlook some of the issues highlighted above, and hope the authors can address some of them in time for the rebuttal. [R1] https://papers.nips.cc/paper/2275-self-supervised-boosting.pdf PROS: Novel and intriguing idea Strong theoretical guarantees CONS: Resulting boosted model is un-normalized Discriminator based boosting is expensive, due to sampling via MCMC Weak experimental section ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your helpful comments . Please refer to the common rebuttal posted for response to questions regarding MNIST experiments . Additional comments : 1 . We have followed up on the reviewer \u2019 s suggestion of removing experiments that mix different base learners . We hope this makes the presentation clean . 2.We have restructured the related works section entirely for improved clarity , and also included a discussion on the differences of our framework with \u201c self-supervised boosting \u201d -- a reference we had missed out earlier and are duly thankful to the reviewer for highlighting . We have also included references to some very recent relevant preprints that came after our work for a more complete and up-to-date discussion . 3.Please read the response to Reviewer 4 below for an argument on why we feel that unnormalized models such as BGMs while certainly being limited are not a deal-breaker ."}, "2": {"review_id": "HyY4Owjll-2", "review_text": "The paper proposes two approaches to boosting generative models, both based on likelihood ratio estimates. The approaches are evaluated on synthetic data, as well as on MNIST dataset for the tasks of generating samples and semi-supervised learning. While the idea of boosting generative models and the proposed methods are interesting, the reviewer finds the experiments unconvincing for the following reasons. 1. The bagging baseline in section 3.1 seems to be just refitting a model to the same dataset, raising the probability to power alpha, and renormalizing. This makes it more peaked, but it's not clear why this is a good baseline. Please let me know if I misunderstood the procedure. 2. The sample generation experiment in section 3.2 uses a very slowly converging Markov chain, as can be seen in the similarity of plots c and f, d and g, e and h. It seems unlikely therefore that the resulting samples are from the stationary distribution. A qualitative evaluation using AIS seems to be necessary here. 3. In the same section the choices for alphas seem quite arbitrary - what happens when a more obvious choice of alpha_i=1 for all i is made? 4. It seems hard to infer anything from the semisupervised classification results reported: the baseline RBM seems to perform as well as the boosted models. The work is mostly clearly written and (as far as the reviewer knows) original.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your helpful comments . Please refer to the common rebuttal posted for response to questions regarding MNIST experiments ( points 2 and 3 ) . 1.Yes , the bagging procedure described is correct . The purpose of including bagging as a baseline was to demonstrate that naively increasing model capacity by adding new models to the ensemble is not very effective in correcting for model misspecification , unlike boosting which involves a reweighting step in the GenBGM version or classifier training for the DiscBGM version . A similar approach ( called bagging ) competes with boosting for the supervised case . For brevity , we have removed this baseline from the latest version . 4.The experiments on semi-supervised classification have been removed in the latest version since the gain in accuracy with the current algorithm is unconvincing for a few cases . For completeness , we would like to clarify the purpose of including them earlier was to demonstrate a ) the computational gains that boosted generative models can provide on this task ( for example , the training time of RBM- > RBM was slightly more than half of the baseline RBM model yet it was able to match the performance of the baseline RBM model ) b ) improving models that are weaker at this task due to different inductive biases ( for example , VAE- > RBM gives better accuracy than the baseline VAE ) . Based on our preliminary experiments , we believe this task is an important use case of boosted generative models . However , in the absence of sufficient empirical evidence , we defer improved algorithms for semi-supervised classification based on the boosting framework to future work ."}}