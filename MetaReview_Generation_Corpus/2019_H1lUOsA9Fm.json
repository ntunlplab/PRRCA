{"year": "2019", "forum": "H1lUOsA9Fm", "title": "Synthnet: Learning synthesizers end-to-end", "decision": "Reject", "meta_review": "The paper describes a WaveNet-like model for MIDI-conditional music audio generation. As noted by all reviewers, the major limitation of the paper is that the method is evaluated on a synthetic dataset. The rebuttal and post-rebuttal discussion didn't change the reviewers' opinion.", "reviews": [{"review_id": "H1lUOsA9Fm-0", "review_text": "This paper proposes several architecture changes to a WaveNet-like dilated convolutional audio model to improve performance for MIDI-conditioned single-instrument polyphonic music generation. The experimental results and provided samples do clearly show that the proposed architecture does well at reproducing the sounds of the training instruments for new MIDI scores, as measured by CQT error and human preference. However, the fact that the model is able to nearly-exactly reproduce CQT is contrary to intuition; given only note on/off times, for most instruments there would be many perceptually-distinct performances of those notes. This suggests that the task is too heavily restricted. It isn't clearly stated until Section 4 that the goal of the work is to model SoundFont-rendered music. (The title \"SynthNet\" is suggestive but any music generated by such an audio model could be considered \"synthesized\".) Using a SoundFont instead of \"real\" musical recordings greatly diminishes the usefulness of this work; adding and concatenating outputs from the single-note model of Engel et al. removes any real need to model polyphony, and there's no compelling argument that the proposed architecture changes should help in other domains. One change that could potentially increase the paper's impact is to train and evaluate the model on MusicNet (https://homes.cs.washington.edu/~thickstn/musicnet.html), which contains 10+ minutes of recorded audio and aligned note labels for each of ~5 single instruments (as well as many ensembles). This would provide evidence that the proposed architecture changes improve performance on a more realistic class of polyphonic music. Another improvement would be to perform an ablation study over the many architecture changes. This idea is mentioned in 4.2 but seemingly dismissed due to the impracticality of performing listening studies, which motivates the use of RMSE-CQT. However, no ablation study is actually performed, so it's not obvious what readers of the paper should learn from the new architecture even restricted to the domain of SoundFont-rendered music generation. Minor points / nitpicks: One of the claimed contributions is dithering before quantization to 8-bits. How does this compare to using mixtures of logistics as in Salimans et al. 2017? S2P3 claims SynthNet does not use note velocity information; this is stated as an advantage but seems to make the task easier while reducing applicability to \"real\" music. S4P1 and S4.1P4 state MIDI is upsampled using linear interpolation. What exactly does this mean? Also, the representation is pianoroll if I understand correctly, so what does it mean to say that each frame is a 128-valued vector with \"note on-off times\"? My guess is it's a standard pianoroll with 0s for inactive notes and 1s for active notes, where onsets and offsets contain linear fades, but this could be explained more clearly. What is the explanation of the delay in the DeepVoice samples? If correcting this is just a matter of shifting the conditioning signal, it seems like an unfair comparison. S1P2 points (2) and (3) arguing why music is more challenging than speech are questionable. The timbre of a real musical instrument may be more complex than speech, but is this true for SoundFonts where the same samples are used for multiple notes? It's not clear what the word \"semantically\" even means with regard to music. The definition of timbre in S3.2P2 is incomplete. Timbre is not just a spectral envelope, but also includes e.g. temporal dynamics like ADSR. Spelling/grammar: S1P3L4 laborius -> laborious S1P3L5 bypassses -> bypasses S3.2P2L-1 due -> due to S4.4P2L1 twice as better than -> twice as good as S4.4P2L3 basline -> baseline ", "rating": "4: Ok but not good enough - rejection", "reply_text": "`` Minor points / nitpicks : One of the claimed contributions is dithering before quantization to 8-bits . How does this compare to using mixtures of logistics as in Salimans et al.2017 ? `` Thank you for your comment . We were limited by the page limit and left this for future work . `` S2P3 claims SynthNet does not use note velocity information ; this is stated as an advantage but seems to make the task easier while reducing applicability to `` real '' music . '' Thank you for your comment . SynthNet produces similar results to Engel et al ( 2017 ) despite not using any velocity information . This does not make the task easier since the labels are less informative . The only pitch information is the note on / off times . `` S4P1 and S4.1P4 state MIDI is upsampled using linear interpolation . What exactly does this mean ? Also , the representation is pianoroll if I understand correctly , so what does it mean to say that each frame is a 128-valued vector with `` note on-off times '' ? My guess is it 's a standard pianoroll with 0s for inactive notes and 1s for active notes , where onsets and offsets contain linear fades , but this could be explained more clearly . '' Thank you for your comment . Your understanding is correct . No linear interpolation is needed , rather we forward fill the binary midi data to obtain our temporal grid version at the appropriate sampling rate . During the piano roll data construction , we simply treat the midi note on/offs as points in continuous time and look up whether the note is on at the given grid point , where the grid may be arbitrarily fine grained . One could call this \u201c forward filling \u201d rather than \u201c linear interpolation \u201d . `` What is the explanation of the delay in the DeepVoice samples ? If correcting this is just a matter of shifting the conditioning signal , it seems like an unfair comparison . '' Thank you for your comment . This is not just a matter of shifting the conditional signal . It is possibly due to initialization ( zeros , the same across all models ) and more importantly due to the autoregressive process at generation time which causes the generated series to diverge abruptly once a major mistake is made . This results in other types of errors / audio artefacts , not only delay but also playing the wrong pitch , sustained pitches or just skipping pitches altogether . Furthermore , the generation process is inherently stochastic so even though the generated signals are very close to the ground truth , they will be different every time even though the same tune is played ( the differences can be observed at a \u2018 microscopic \u2019 level ) but the big picture , overall shape of the waveforms are visibly very close . SynthNet learns a mapping from pitch to fundamental frequency in the first layer , unlike the baselines where all dilated layers are conditioned as in Van Den Oord et al . ( 2016 ) .We also use an auxiliary learning task on the midi component . These two changes result in fewer errors at generation time . `` S1P2 points ( 2 ) and ( 3 ) arguing why music is more challenging than speech are questionable . The timbre of a real musical instrument may be more complex than speech , but is this true for SoundFonts where the same samples are used for multiple notes ? It 's not clear what the word `` semantically '' even means with regard to music . '' Thank you for your comment . Yes , it is true , and this is also because multiple notes can overlap . Semantically can be understood as the melodic component or the content / song if a parallel is made between music and language ."}, {"review_id": "H1lUOsA9Fm-1", "review_text": "This paper describes the use of a wavenet synthesizer conditioned on a piano-roll representation to synthesize one of seven different instruments playing approximately monophonic melodic lines. The system is trained on MIDI syntheses rendered by a traditional synthesizer. While the idea of end-to-end training of musical synthesizers is interesting and timely, this formulation of the problem limits the benefits that such a system could provide. Specifically, it would be useful for learning expressive performance from real recordings of very expressive instruments. For example, in the provided training data, the trumpet syntheses used to train this wavenet sound quite unconvincing and unexpressive. Using real trumpet performances could potentially learn a mapping from notes to expressive performance, including the details of transitions between notes, articulation, dynamics, breath control, etc. MIDI syntheses have none of these, and so cannot train an expressive model. While the experiments show that the proposed system can achieve high fidelity synthesis, it seems to be on a very limited sub-set of musical material. The model doesn't have to learn monophonic lines, but that seems to be what it is applied on. It is not clear why that is better than training on individual notes, as Engel et al (2017) do. In addition, it is trained on only 9 minutes of audio, but takes 6 days to do so. This slow processing is somewhat concerning. In addition, the 9 minutes of audio seems to be the same pieces played by each instrument, so really it is much less than 9 minutes of musical material. This may have implications for generalization to new musical situations and contexts. Overall, this is an interesting idea, and potentially an interesting system, but the experiments do not demonstrate its strengths to the extent that they could. Minor comments: * The related work section repeats a good amount of information from the introduction. It could be removed from one of them * Table 1: I don't understand what this table is describing. SynthNet is described as having 1 scalar input, but in the previous section said that it had 256-valued encoded audio and 128-valued binary encoded MIDI as input. * The use of the term \"style\" to mean \"timbre\" is confusing throughout and should be corrected. * Figure 1: I do not see a clear reason why there should be a discontinuity between L13 and L14, so I think it is just a poor choice of colormap. Please fix this. * Page 5: MIDI files were upsampled through linear interpolation. This is a puzzling choice as the piano-roll representation is supposed to be binary. * Page 7: \"(Table 3 slanted)\" I would either say \"(Table 3, slanted text)\" or \"(Table 3, italics)\". * Page 8: \"are rated to be almost twice as better\" this should be re-worded as \"twice as good\" or something similar. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "`` Minor comments : * The related work section repeats a good amount of information from the introduction . It could be removed from one of them `` Thank you for your comment . `` * Table 1 : I do n't understand what this table is describing . SynthNet is described as having 1 scalar input , but in the previous section said that it had 256-valued encoded audio and 128-valued binary encoded MIDI as input. `` Mu-law encoding reduces the bit depth from 16 bit ( 2^16 = 65k ) to 8 bit ( 2^8 = 256 ) . So , the input is scalar and can have scalar values ranging from 0 to 255 . These can also be encoded as a 1-hot vector , but this is not done for SynthNet . The midi is processed into piano roll where each frame is a binary vector with 128 values . `` * The use of the term `` style '' to mean `` timbre '' is confusing throughout and should be corrected. `` Thank you for your comment . `` * Figure 1 : I do not see a clear reason why there should be a discontinuity between L13 and L14 , so I think it is just a poor choice of colormap . Please fix this. `` Thank you for your comment . Every block of repeated dilations is coloured differently . `` * Page 5 : MIDI files were upsampled through linear interpolation . This is a puzzling choice as the piano-roll representation is supposed to be binary. `` Thank you for your comment . Indeed , the article miswords this - no linear interpolation is needed , rather we forward fill the binary midi data to obtain our temporal grid version at the appropriate sampling rate . During the piano roll data construction , we simply treat the midi note on/offs as points in continuous time and look up whether the note is on at the given grid point , where the grid may be arbitrarily fine grained . One could call this \u201c forward filling \u201d rather than \u201c linear interpolation \u201d . `` * Page 7 : `` ( Table 3 slanted ) '' I would either say `` ( Table 3 , slanted text ) '' or `` ( Table 3 , italics ) '' . `` Thank you for your comment . `` * Page 8 : `` are rated to be almost twice as better '' this should be re-worded as `` twice as good '' or something similar. `` Thank you for your comment ."}, {"review_id": "H1lUOsA9Fm-2", "review_text": "This paper proposes a neural model for synthesizing instrument sounds, using an architecture based on the WaveNet and DeepVoice models. The model generates raw waveforms conditioned on a piano roll representation of aligned MIDI input. My biggest gripe with this work is that the model is trained entirely on a synthetic dataset generated from a sample-based synthesizer using a sound font. I feel that this defeats the purpose, as it will never work better than just sampling the original sound library. One potential argument in favour would be to save storage space, but the sound font used for the work is only ~140 MB, which is not prohibitive these days (indeed, many neural models require a comparable amount of storage). It would be much more interesting to train the model on real instrument recordings, because then it could capture all the nuances of the instruments that sample-based synthesizers cannot replicate. As it stands, all the model has to do is reproduce a fixed (and fairly small) set of audio samples. This is arguably a much simpler task, which could also explain why reducing the model size (SynthNet's depthwise convolutions have many fewer parameters than the regular convolutions used in WaveNet and DeepVoice) works so well here. That said, I think the proposed architectural modifications for raw audio models could be interesting and should be tested for other, more challenging tasks. The proposed RMSE-CQT error measure is potentially quite valuable for music generation research, and its correlation with MOS scores is promising (but this should also be tested on more realistic audio). The fact that the models were trained to convergence on only 9 minutes of data per instrument is also impressive, despite the limitations of the dataset. The use of dithering to reduce perceptual noise is also interesting and some comparison experiments there would have been interesting, especially to corroborate the claim that it is critical for the learning process. I think the paper slightly overstates its contributions in terms of providing insight into the representations that are learned in generative convolutional models. The Gram matrix projections showing that the activations of different layers diverge for different input types as we advance through the model is not particularly surprising, and similar plots could probably be made for almost any residual model. Overall, I feel the work has some fundamental flaws, mostly stemming from the dataset that was used. Miscellany: - In the abstract: \"is substantially better in quality\", compared to what? - In the introduction, it is mentioned that words in a speech signal cannot overlap, but notes in a musical signal can. I would argue that these are not comparable abstractions though, words themselves are composed of a sequence of phonemes, which are probably a better point of comparison (and phonemes, while they don't tend to overlap, can affect neighbouring phonemes in various ways). That said, I appreciate that this is probably quite subjective. - Overall, the formulation of paragraph 2 of the introduction is a bit unusual, I think the same things are said in a much better way in Section 3. - \"Conditioning Deep Generative Raw Audio Models for Structured Automatic Music\" by Manzelli et al. (2018) also proposes a MIDI-conditional neural audio generation model, trained on real instrument recordings from the MusicNet dataset. I think this is a very relevant reference. - In the contributions of the paper, it is stated that \"the generated audio is practically identical to ground truth as can be seen in Figure 4\" but the CQTs in this figure are visibly different. - I don't think it is fair to directly compare this setup to Engel et al. (2017) and Mor et al. (2018) as is done in the last paragraph of Section 2, as these are simply different tasks (mapping from audio to audio as opposed to generating audio). - At the start of Section 3.1 it would be good to explicitly mention whether 8-bit mu-law audio is used, to explain why the waveform is 256-valued. - Why is the conditioning causal? It does not need to be, as the piano roll is fully available in advance of the audio generation. I guess one argument in favour would be to enable real-time generation, but it would still be good to compare causal and non-causal conditioning. - Since the piano roll representation is binary, does that mean MIDI velocity is not captured in the conditioning signal? It would probably be useful for the model to provide this information, so it can capture the differences in timbre between different velocities. - The use of a MIDI prediction loss to regularise the conditioning part of the model is interesting, but I would have liked to see a comparison experiment (with/without). - In Section 4.3, specify the unit, i.e. \"Delta < 1 second\". - For the task of recreating synthetic audio samples, the WaveNet models seem to be quite large. As far as I can tell the size hyperparameters were chosen based on the literature, but the inherited parameters were originally optimised for different tasks. - In Section 4.3 under \"global conditioning\", the benchmark is said to be between DeepVoice L26 and SynthNet L24, but Table 4 lists DeepVoice L26 and SynthNet L26, which version was actually used?", "rating": "3: Clear rejection", "reply_text": "`` - In the contributions of the paper , it is stated that `` the generated audio is practically identical to ground truth as can be seen in Figure 4 '' but the CQTs in this figure are visibly different . '' Thank you for your comment . Indeed , there are minute differences . The CQTs are quite similar and very accurate , especially when compared to the baselines , as it can also be seen in Appendix 1 . To the best of our knowledge this is the first time that such accurate signals are generated and measured directly vs the ground truth . `` - I do n't think it is fair to directly compare this setup to Engel et al . ( 2017 ) and Mor et al . ( 2018 ) as is done in the last paragraph of Section 2 , as these are simply different tasks ( mapping from audio to audio as opposed to generating audio ) . '' The architectural components in Engel et al . ( 2017 ) and Mor et al . ( 2018 ) are almost identical to the original work of Van Den Oord et al . ( 2016 ) with the difference that a separate simpler residual network is used for the encoder . In our work , we learn an auxiliary task based on the midi data instead of a separate encoder network and this is how the tasks are different . `` - At the start of Section 3.1 it would be good to explicitly mention whether 8-bit mu-law audio is used , to explain why the waveform is 256-valued . '' Thank you for your comment . `` - Why is the conditioning causal ? It does not need to be , as the piano roll is fully available in advance of the audio generation . I guess one argument in favour would be to enable real-time generation , but it would still be good to compare causal and non-causal conditioning . '' Thank you for your comment . Indeed , it is necessary to restrict to a causal model if we are to synthesise in real time ( i.e.use it to generate sound from a midi keyboard ) . It would be interesting to consider non-causal models but it is somewhat orthogonal to the main aim of our work . `` - Since the piano roll representation is binary , does that mean MIDI velocity is not captured in the conditioning signal ? It would probably be useful for the model to provide this information , so it can capture the differences in timbre between different velocities . '' Thank you for your comment . Indeed , we do not use velocity information and doing so can improve expressiveness . `` - The use of a MIDI prediction loss to regularise the conditioning part of the model is interesting , but I would have liked to see a comparison experiment ( with/without ) . '' Thank you for your comment . Again , we were limited by the page limit . The additional task helps with learning a joint representation. `` - In Section 4.3 , specify the unit , i.e . `` Delta < 1 second '' . `` Thank you for your comment. `` - For the task of recreating synthetic audio samples , the WaveNet models seem to be quite large . As far as I can tell the size hyperparameters were chosen based on the literature , but the inherited parameters were originally optimised for different tasks. `` Thank you for your comment . The tasks in Engel et al . ( 2017 ) and Mor et al . ( 2018 ) are similar as mentioned before . Learning the timbre ( harmonic series ) is correlated with model size . `` - In Section 4.3 under `` global conditioning '' , the benchmark is said to be between DeepVoice L26 and SynthNet L24 , but Table 4 lists DeepVoice L26 and SynthNet L26 , which version was actually used ? '' Thank you for your comment . We will update the table to L24 ."}], "0": {"review_id": "H1lUOsA9Fm-0", "review_text": "This paper proposes several architecture changes to a WaveNet-like dilated convolutional audio model to improve performance for MIDI-conditioned single-instrument polyphonic music generation. The experimental results and provided samples do clearly show that the proposed architecture does well at reproducing the sounds of the training instruments for new MIDI scores, as measured by CQT error and human preference. However, the fact that the model is able to nearly-exactly reproduce CQT is contrary to intuition; given only note on/off times, for most instruments there would be many perceptually-distinct performances of those notes. This suggests that the task is too heavily restricted. It isn't clearly stated until Section 4 that the goal of the work is to model SoundFont-rendered music. (The title \"SynthNet\" is suggestive but any music generated by such an audio model could be considered \"synthesized\".) Using a SoundFont instead of \"real\" musical recordings greatly diminishes the usefulness of this work; adding and concatenating outputs from the single-note model of Engel et al. removes any real need to model polyphony, and there's no compelling argument that the proposed architecture changes should help in other domains. One change that could potentially increase the paper's impact is to train and evaluate the model on MusicNet (https://homes.cs.washington.edu/~thickstn/musicnet.html), which contains 10+ minutes of recorded audio and aligned note labels for each of ~5 single instruments (as well as many ensembles). This would provide evidence that the proposed architecture changes improve performance on a more realistic class of polyphonic music. Another improvement would be to perform an ablation study over the many architecture changes. This idea is mentioned in 4.2 but seemingly dismissed due to the impracticality of performing listening studies, which motivates the use of RMSE-CQT. However, no ablation study is actually performed, so it's not obvious what readers of the paper should learn from the new architecture even restricted to the domain of SoundFont-rendered music generation. Minor points / nitpicks: One of the claimed contributions is dithering before quantization to 8-bits. How does this compare to using mixtures of logistics as in Salimans et al. 2017? S2P3 claims SynthNet does not use note velocity information; this is stated as an advantage but seems to make the task easier while reducing applicability to \"real\" music. S4P1 and S4.1P4 state MIDI is upsampled using linear interpolation. What exactly does this mean? Also, the representation is pianoroll if I understand correctly, so what does it mean to say that each frame is a 128-valued vector with \"note on-off times\"? My guess is it's a standard pianoroll with 0s for inactive notes and 1s for active notes, where onsets and offsets contain linear fades, but this could be explained more clearly. What is the explanation of the delay in the DeepVoice samples? If correcting this is just a matter of shifting the conditioning signal, it seems like an unfair comparison. S1P2 points (2) and (3) arguing why music is more challenging than speech are questionable. The timbre of a real musical instrument may be more complex than speech, but is this true for SoundFonts where the same samples are used for multiple notes? It's not clear what the word \"semantically\" even means with regard to music. The definition of timbre in S3.2P2 is incomplete. Timbre is not just a spectral envelope, but also includes e.g. temporal dynamics like ADSR. Spelling/grammar: S1P3L4 laborius -> laborious S1P3L5 bypassses -> bypasses S3.2P2L-1 due -> due to S4.4P2L1 twice as better than -> twice as good as S4.4P2L3 basline -> baseline ", "rating": "4: Ok but not good enough - rejection", "reply_text": "`` Minor points / nitpicks : One of the claimed contributions is dithering before quantization to 8-bits . How does this compare to using mixtures of logistics as in Salimans et al.2017 ? `` Thank you for your comment . We were limited by the page limit and left this for future work . `` S2P3 claims SynthNet does not use note velocity information ; this is stated as an advantage but seems to make the task easier while reducing applicability to `` real '' music . '' Thank you for your comment . SynthNet produces similar results to Engel et al ( 2017 ) despite not using any velocity information . This does not make the task easier since the labels are less informative . The only pitch information is the note on / off times . `` S4P1 and S4.1P4 state MIDI is upsampled using linear interpolation . What exactly does this mean ? Also , the representation is pianoroll if I understand correctly , so what does it mean to say that each frame is a 128-valued vector with `` note on-off times '' ? My guess is it 's a standard pianoroll with 0s for inactive notes and 1s for active notes , where onsets and offsets contain linear fades , but this could be explained more clearly . '' Thank you for your comment . Your understanding is correct . No linear interpolation is needed , rather we forward fill the binary midi data to obtain our temporal grid version at the appropriate sampling rate . During the piano roll data construction , we simply treat the midi note on/offs as points in continuous time and look up whether the note is on at the given grid point , where the grid may be arbitrarily fine grained . One could call this \u201c forward filling \u201d rather than \u201c linear interpolation \u201d . `` What is the explanation of the delay in the DeepVoice samples ? If correcting this is just a matter of shifting the conditioning signal , it seems like an unfair comparison . '' Thank you for your comment . This is not just a matter of shifting the conditional signal . It is possibly due to initialization ( zeros , the same across all models ) and more importantly due to the autoregressive process at generation time which causes the generated series to diverge abruptly once a major mistake is made . This results in other types of errors / audio artefacts , not only delay but also playing the wrong pitch , sustained pitches or just skipping pitches altogether . Furthermore , the generation process is inherently stochastic so even though the generated signals are very close to the ground truth , they will be different every time even though the same tune is played ( the differences can be observed at a \u2018 microscopic \u2019 level ) but the big picture , overall shape of the waveforms are visibly very close . SynthNet learns a mapping from pitch to fundamental frequency in the first layer , unlike the baselines where all dilated layers are conditioned as in Van Den Oord et al . ( 2016 ) .We also use an auxiliary learning task on the midi component . These two changes result in fewer errors at generation time . `` S1P2 points ( 2 ) and ( 3 ) arguing why music is more challenging than speech are questionable . The timbre of a real musical instrument may be more complex than speech , but is this true for SoundFonts where the same samples are used for multiple notes ? It 's not clear what the word `` semantically '' even means with regard to music . '' Thank you for your comment . Yes , it is true , and this is also because multiple notes can overlap . Semantically can be understood as the melodic component or the content / song if a parallel is made between music and language ."}, "1": {"review_id": "H1lUOsA9Fm-1", "review_text": "This paper describes the use of a wavenet synthesizer conditioned on a piano-roll representation to synthesize one of seven different instruments playing approximately monophonic melodic lines. The system is trained on MIDI syntheses rendered by a traditional synthesizer. While the idea of end-to-end training of musical synthesizers is interesting and timely, this formulation of the problem limits the benefits that such a system could provide. Specifically, it would be useful for learning expressive performance from real recordings of very expressive instruments. For example, in the provided training data, the trumpet syntheses used to train this wavenet sound quite unconvincing and unexpressive. Using real trumpet performances could potentially learn a mapping from notes to expressive performance, including the details of transitions between notes, articulation, dynamics, breath control, etc. MIDI syntheses have none of these, and so cannot train an expressive model. While the experiments show that the proposed system can achieve high fidelity synthesis, it seems to be on a very limited sub-set of musical material. The model doesn't have to learn monophonic lines, but that seems to be what it is applied on. It is not clear why that is better than training on individual notes, as Engel et al (2017) do. In addition, it is trained on only 9 minutes of audio, but takes 6 days to do so. This slow processing is somewhat concerning. In addition, the 9 minutes of audio seems to be the same pieces played by each instrument, so really it is much less than 9 minutes of musical material. This may have implications for generalization to new musical situations and contexts. Overall, this is an interesting idea, and potentially an interesting system, but the experiments do not demonstrate its strengths to the extent that they could. Minor comments: * The related work section repeats a good amount of information from the introduction. It could be removed from one of them * Table 1: I don't understand what this table is describing. SynthNet is described as having 1 scalar input, but in the previous section said that it had 256-valued encoded audio and 128-valued binary encoded MIDI as input. * The use of the term \"style\" to mean \"timbre\" is confusing throughout and should be corrected. * Figure 1: I do not see a clear reason why there should be a discontinuity between L13 and L14, so I think it is just a poor choice of colormap. Please fix this. * Page 5: MIDI files were upsampled through linear interpolation. This is a puzzling choice as the piano-roll representation is supposed to be binary. * Page 7: \"(Table 3 slanted)\" I would either say \"(Table 3, slanted text)\" or \"(Table 3, italics)\". * Page 8: \"are rated to be almost twice as better\" this should be re-worded as \"twice as good\" or something similar. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "`` Minor comments : * The related work section repeats a good amount of information from the introduction . It could be removed from one of them `` Thank you for your comment . `` * Table 1 : I do n't understand what this table is describing . SynthNet is described as having 1 scalar input , but in the previous section said that it had 256-valued encoded audio and 128-valued binary encoded MIDI as input. `` Mu-law encoding reduces the bit depth from 16 bit ( 2^16 = 65k ) to 8 bit ( 2^8 = 256 ) . So , the input is scalar and can have scalar values ranging from 0 to 255 . These can also be encoded as a 1-hot vector , but this is not done for SynthNet . The midi is processed into piano roll where each frame is a binary vector with 128 values . `` * The use of the term `` style '' to mean `` timbre '' is confusing throughout and should be corrected. `` Thank you for your comment . `` * Figure 1 : I do not see a clear reason why there should be a discontinuity between L13 and L14 , so I think it is just a poor choice of colormap . Please fix this. `` Thank you for your comment . Every block of repeated dilations is coloured differently . `` * Page 5 : MIDI files were upsampled through linear interpolation . This is a puzzling choice as the piano-roll representation is supposed to be binary. `` Thank you for your comment . Indeed , the article miswords this - no linear interpolation is needed , rather we forward fill the binary midi data to obtain our temporal grid version at the appropriate sampling rate . During the piano roll data construction , we simply treat the midi note on/offs as points in continuous time and look up whether the note is on at the given grid point , where the grid may be arbitrarily fine grained . One could call this \u201c forward filling \u201d rather than \u201c linear interpolation \u201d . `` * Page 7 : `` ( Table 3 slanted ) '' I would either say `` ( Table 3 , slanted text ) '' or `` ( Table 3 , italics ) '' . `` Thank you for your comment . `` * Page 8 : `` are rated to be almost twice as better '' this should be re-worded as `` twice as good '' or something similar. `` Thank you for your comment ."}, "2": {"review_id": "H1lUOsA9Fm-2", "review_text": "This paper proposes a neural model for synthesizing instrument sounds, using an architecture based on the WaveNet and DeepVoice models. The model generates raw waveforms conditioned on a piano roll representation of aligned MIDI input. My biggest gripe with this work is that the model is trained entirely on a synthetic dataset generated from a sample-based synthesizer using a sound font. I feel that this defeats the purpose, as it will never work better than just sampling the original sound library. One potential argument in favour would be to save storage space, but the sound font used for the work is only ~140 MB, which is not prohibitive these days (indeed, many neural models require a comparable amount of storage). It would be much more interesting to train the model on real instrument recordings, because then it could capture all the nuances of the instruments that sample-based synthesizers cannot replicate. As it stands, all the model has to do is reproduce a fixed (and fairly small) set of audio samples. This is arguably a much simpler task, which could also explain why reducing the model size (SynthNet's depthwise convolutions have many fewer parameters than the regular convolutions used in WaveNet and DeepVoice) works so well here. That said, I think the proposed architectural modifications for raw audio models could be interesting and should be tested for other, more challenging tasks. The proposed RMSE-CQT error measure is potentially quite valuable for music generation research, and its correlation with MOS scores is promising (but this should also be tested on more realistic audio). The fact that the models were trained to convergence on only 9 minutes of data per instrument is also impressive, despite the limitations of the dataset. The use of dithering to reduce perceptual noise is also interesting and some comparison experiments there would have been interesting, especially to corroborate the claim that it is critical for the learning process. I think the paper slightly overstates its contributions in terms of providing insight into the representations that are learned in generative convolutional models. The Gram matrix projections showing that the activations of different layers diverge for different input types as we advance through the model is not particularly surprising, and similar plots could probably be made for almost any residual model. Overall, I feel the work has some fundamental flaws, mostly stemming from the dataset that was used. Miscellany: - In the abstract: \"is substantially better in quality\", compared to what? - In the introduction, it is mentioned that words in a speech signal cannot overlap, but notes in a musical signal can. I would argue that these are not comparable abstractions though, words themselves are composed of a sequence of phonemes, which are probably a better point of comparison (and phonemes, while they don't tend to overlap, can affect neighbouring phonemes in various ways). That said, I appreciate that this is probably quite subjective. - Overall, the formulation of paragraph 2 of the introduction is a bit unusual, I think the same things are said in a much better way in Section 3. - \"Conditioning Deep Generative Raw Audio Models for Structured Automatic Music\" by Manzelli et al. (2018) also proposes a MIDI-conditional neural audio generation model, trained on real instrument recordings from the MusicNet dataset. I think this is a very relevant reference. - In the contributions of the paper, it is stated that \"the generated audio is practically identical to ground truth as can be seen in Figure 4\" but the CQTs in this figure are visibly different. - I don't think it is fair to directly compare this setup to Engel et al. (2017) and Mor et al. (2018) as is done in the last paragraph of Section 2, as these are simply different tasks (mapping from audio to audio as opposed to generating audio). - At the start of Section 3.1 it would be good to explicitly mention whether 8-bit mu-law audio is used, to explain why the waveform is 256-valued. - Why is the conditioning causal? It does not need to be, as the piano roll is fully available in advance of the audio generation. I guess one argument in favour would be to enable real-time generation, but it would still be good to compare causal and non-causal conditioning. - Since the piano roll representation is binary, does that mean MIDI velocity is not captured in the conditioning signal? It would probably be useful for the model to provide this information, so it can capture the differences in timbre between different velocities. - The use of a MIDI prediction loss to regularise the conditioning part of the model is interesting, but I would have liked to see a comparison experiment (with/without). - In Section 4.3, specify the unit, i.e. \"Delta < 1 second\". - For the task of recreating synthetic audio samples, the WaveNet models seem to be quite large. As far as I can tell the size hyperparameters were chosen based on the literature, but the inherited parameters were originally optimised for different tasks. - In Section 4.3 under \"global conditioning\", the benchmark is said to be between DeepVoice L26 and SynthNet L24, but Table 4 lists DeepVoice L26 and SynthNet L26, which version was actually used?", "rating": "3: Clear rejection", "reply_text": "`` - In the contributions of the paper , it is stated that `` the generated audio is practically identical to ground truth as can be seen in Figure 4 '' but the CQTs in this figure are visibly different . '' Thank you for your comment . Indeed , there are minute differences . The CQTs are quite similar and very accurate , especially when compared to the baselines , as it can also be seen in Appendix 1 . To the best of our knowledge this is the first time that such accurate signals are generated and measured directly vs the ground truth . `` - I do n't think it is fair to directly compare this setup to Engel et al . ( 2017 ) and Mor et al . ( 2018 ) as is done in the last paragraph of Section 2 , as these are simply different tasks ( mapping from audio to audio as opposed to generating audio ) . '' The architectural components in Engel et al . ( 2017 ) and Mor et al . ( 2018 ) are almost identical to the original work of Van Den Oord et al . ( 2016 ) with the difference that a separate simpler residual network is used for the encoder . In our work , we learn an auxiliary task based on the midi data instead of a separate encoder network and this is how the tasks are different . `` - At the start of Section 3.1 it would be good to explicitly mention whether 8-bit mu-law audio is used , to explain why the waveform is 256-valued . '' Thank you for your comment . `` - Why is the conditioning causal ? It does not need to be , as the piano roll is fully available in advance of the audio generation . I guess one argument in favour would be to enable real-time generation , but it would still be good to compare causal and non-causal conditioning . '' Thank you for your comment . Indeed , it is necessary to restrict to a causal model if we are to synthesise in real time ( i.e.use it to generate sound from a midi keyboard ) . It would be interesting to consider non-causal models but it is somewhat orthogonal to the main aim of our work . `` - Since the piano roll representation is binary , does that mean MIDI velocity is not captured in the conditioning signal ? It would probably be useful for the model to provide this information , so it can capture the differences in timbre between different velocities . '' Thank you for your comment . Indeed , we do not use velocity information and doing so can improve expressiveness . `` - The use of a MIDI prediction loss to regularise the conditioning part of the model is interesting , but I would have liked to see a comparison experiment ( with/without ) . '' Thank you for your comment . Again , we were limited by the page limit . The additional task helps with learning a joint representation. `` - In Section 4.3 , specify the unit , i.e . `` Delta < 1 second '' . `` Thank you for your comment. `` - For the task of recreating synthetic audio samples , the WaveNet models seem to be quite large . As far as I can tell the size hyperparameters were chosen based on the literature , but the inherited parameters were originally optimised for different tasks. `` Thank you for your comment . The tasks in Engel et al . ( 2017 ) and Mor et al . ( 2018 ) are similar as mentioned before . Learning the timbre ( harmonic series ) is correlated with model size . `` - In Section 4.3 under `` global conditioning '' , the benchmark is said to be between DeepVoice L26 and SynthNet L24 , but Table 4 lists DeepVoice L26 and SynthNet L26 , which version was actually used ? '' Thank you for your comment . We will update the table to L24 ."}}