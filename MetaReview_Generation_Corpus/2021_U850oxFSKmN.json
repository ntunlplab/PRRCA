{"year": "2021", "forum": "U850oxFSKmN", "title": "Learning Continuous-Time Dynamics by Stochastic Differential Networks", "decision": "Reject", "meta_review": "This paper sits right at the borderline: the reviewers agree that it is interesting and addresses a relevant problem. On the negative side, the presentation could be improved (including some incorrect claims), and the experiments could be strengthened (both in terms of baselines and datasets used). Ultimately, the paper will probably require another round of reviews before it is ready for publication.", "reviews": [{"review_id": "U850oxFSKmN-0", "review_text": "In this paper the author/s study/ies the fundamental problem of learning continuous-time stochastic dynamics , in the case where the available time series data suffer irregularity and sparseness . The paper assumes that high dimensional data are generated from a system where latent states are observed . The paper tackles the problem In such a settings , it is generally impossible to derive a continuous-time stochastic process which precisely describes the many behaviors of the system under study . The paper develops a method base on Variational Bayes applied to learn flexible continuous-time stochastic recurrent neural network , they call Variational Stochastic Differential Networks . The particualr feature of such model is to capture the stochastic dependency among latent states and observations . The paper provides theoretichal tool , under the form of lower bounds for the efficient training of the neural model . THe author/s state that the results from a rich set of numerical experiments witnesses that the proposed approach outperforms state-of-the-art continuous-time deep learning models to solve the prediction and interpolation tasks , in the particular case when irrelugar and sporading time series data is available . - Reason for Score : Overall , I vote for rejecting . I like the idea to mix parametric and non parametric components to learn time series dynamics . However , I have many concerns that are made explicit in the cons section . Hopefully the authors can address my concerns in the rebuttal period . - Pros.1 ) the paper tackles a relevant problem . 2 ) I appreciate the idea to combine parametric and non parametric models 3 ) the proposed model is clear 3 ) numerical experiments , on the selected data sets , witness in favour of the proposed method - Cons . 1 ) why other models for time series modeling and forecasting have not been taken into accout ? what about dynamic Bayesian networks , hidden Markov models , contiuous time Bayesian networks , and many other ... 2 ) the paper states the interest is related to high dimensional time series , but the data sets used for numerical experiments , in my humble opinion , are not as such , they are of very small or moderate dimensionality , in terms of number of variables . 3 ) I found no discussion about the choice of the dimension of the latent space , maybe my fauls and if this is the case I apologize , but still I think that more attention and more motivations must be given in the numerical experiments section . 4 ) I 'm under the impression that the proposed solution is a sensible combination of existing results and thus the work is somewhat of incremental nature , which I do not know whether is relevant for ICLR . 5 ) I 'm confused on the formulation of the inference problems , the paper considers both filtering and smoothing . However , later in the paper , the numerical experiments section , I read about `` prediction '' and `` interpolation '' which in my uderstanding is the same as filtering and smoothing . Thus , I would like the authors to better frame their problem . 6 ) at page 2 , I can not understand the difference between path and history , if they are used as synonims , please do not do it , it increases confusion to the interested reader , try to be more formal and clear as possible to let interested reader to appreciate your contributions . 7 ) at page 3 I read `` However , RG only use the historical data as input , as we observe no improvement by including the current state . `` , I would ask to motivate , personally I 'm not that surprised by this but I would like the paper to help understand the why of this occuring . 8 ) In equation ( 5 ) you are using the entire trajectory and thus you are performing smoothing , I think the problem/s tackled must be clearly described . 9 ) at page 5 I read `` In real-world applications , the latent state may not have strong dependency on future observations '' . This is somewhat surprising me , which has not strong dependency on which ? the latent does not influence the future ? the future does not tell much about latent ? This sentence in obscure to my elementary mind , and thus I kindly ask to better clarify on it . 10 ) Table 1 and 2 , please call the macro column filtering and smoothing in place of prediction and interpolation , maybe I completely miss the meaning of the difference you made between prediction and filtering and the difference between smoothing and interpolation . If this is the case , please better clarify on these aspects . 11 ) NLL and MSE of VSDN-S ( VAE ) are always better that those achieved by VSDN-F ( VAE ) , this is not that strange , one is using more info than the other , as well as I understood . The same does not apply to VSDN-S ( IWAE ) and VSDN-F ( IWAE ) , and this is surprising in my humble opinion , maybe not found the optimum ? 12 ) In table 2 , NLL is positive for LatentSDE , can you comment on this ? 13 ) The same as 12 ) in Table 3 , I kindly ask to clarify under which circumstances this happens . - Questions during rebuttal period : Please address and clarify the cons above - Some typos : page 3 : I read `` However , RG only use the historical '' , it should read `` However , RG only uses the historical '' I read `` HQ also use the '' , it should read `` HQ also uses the '' -", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Q1 : why other models for time series modeling and forecasting have not been taken into account ? what about dynamic Bayesian networks , hidden Markov models , continuous time Bayesian networks , and many other ... * * - There are rich literature works showing that deep neural net based models outperform the traditional probabilistic graphical models ( e.g. , HMM , DBN ) in many applications . Therefore , we focus more on the comparison with the deep neural net based models . Some references are given as follows : HMM : 1 . Nicolas Boulanger-Lewandowski , Yoshua Bengio , and Pascal Vincent . `` Modeling temporal dependencies in high-dimensional sequences : application to polyphonic music generation and transcription . '' In Proceedings of the 29th International Conference on International Conference on Machine Learning ( ICML'12 ) . Omnipress , Madison , WI , USA , 1881\u20131888 . 2.Lefebvre , Gr\u00e9goire , et al . `` BLSTM-RNN based 3D gesture classification . '' International conference on artificial neural networks . Springer , Berlin , Heidelberg , 2013 . 3.Rajib Ghosh , Chirumavila Vamshi , Prabhat Kumar , `` RNN based online handwritten word recognition in Devanagari and Bengali scripts using horizontal zoning '' , Pattern Recognition , Volume 92 , 2019 , Pages 203-218 . 4.Deshmukh , Akshay Madhav . `` Comparison of Hidden Markov Model and Recurrent Neural Network in Automatic Speech Recognition . '' European Journal of Engineering Research and Science 5 , no . 8 ( 2020 ) : 958-965 . DBN : 1.W\u00f6llmer , M. , Eyben , F. , Graves , A. , Schuller , B. , & Rigoll , G. `` Bidirectional LSTM networks for context-sensitive keyword detection in a cognitive virtual agent framework . '' Cognitive Computation , 2 ( 3 ) , 180-190 . 2.Nguyen , Duc-Canh , G\u00e9rard Bailly , and Fr\u00e9d\u00e9ric Elisei . `` Learning off-line vs. on-line models of interactive multimodal behaviors with recurrent neural networks . '' Pattern Recognition Letters 100 ( 2017 ) : 29-36 . * * Q2 : the paper states the interest is related to high dimensional time series , but the data sets used for numerical experiments , in my humble opinion , are not as such , they are of very small or moderate dimensionality , in terms of number of variables . * * - In our motion experiments , the dimension of each data frame is 62 in MoCap dataset and 51 in h36m dataset . The dimensionalities of the data in our experiments are similar as those in the literature works on deep generative models of sporadic data [ 1 - 4 ] . 1.Wei Cao , Dong Wang , Jian Li , Hao Zhou , Yitan Li , and Lei Li . 2018 . `` BRITS : bidirectional recurrent imputation for time series . '' In Proceedings of the 32nd International Conference on Neural Information Processing Systems ( NIPS'18 ) . 6776\u20136786 . 2.Yonghong Luo , Xiangrui Cai , Ying Zhang , Jun Xu , and Xiaojie Yuan . 2018 . `` Multivariate time series imputation with generative adversarial networks . '' In Proceedings of the 32nd International Conference on Neural Information Processing Systems ( NIPS'18 ) . 1603\u20131614 . 3.Yulia Rubanova , Tian Qi Chen , and David K Duvenaud . `` Latent ordinary differential equations for irregularly-sampled time series . '' InAdvances in Neural Information Processing Systems 32 , pp.5321\u20135331 . 2019.4.Edward De Brouwer , Jaak Simm , Adam Arany , and Yves Moreau . `` GRU-ODE-Bayes : Continuous Modeling of sporadically-observed time series . '' InAdvances in Neural Information ProcessingSystems 32 , pp . 7379\u20137390 . 2019.We also run an experiment on high-dimensional ( above 1k ) data . Our model has similar mean square errors with baselines but has much better NLL , which indicates that our model can better estimate the stochastic process of the data . In our future works , we will extend our work on more challenging very high-dimensional data ( e.g.video ) . * * Q3 : I found no discussion about the choice of the dimension of the latent space * * - The detailed model configuration of our experiments is given in Appendix C. In our preliminary studies , we tried different number of dimensions of the latent state . When the number of the dimensions of the latent state is too small , the performance of our model will degenerate . With large dimensions of the latent state , the performance is not sensitive with respect to the dimensions of latent state ."}, {"review_id": "U850oxFSKmN-1", "review_text": "The paper introduces Variational Stochastic Differential Networks to filter and smooth sporadically observed time series . The authors adopt a Bayesian perceptive on the smoothing problem for time series living a latent space and irregularly observed . In particular , the random evolution of the process in latent space is clearly accounted for in the paper by embedding an SDE into an RNN . The authors derive a variational loss for their model and describe in fact multiple losses with different improvements such as importance sampling . In the end the authors do report issues caused by excess variance in training and settle for a convex combination of the two losses they propose . The paper becomes quite interesting in its experimental part as the authors show the superiority of their method as compared to previous approaches on data sets concerned with Mocap , synthetic OU process data and meteorological data .", "rating": "7: Good paper, accept", "reply_text": "- Thanks for acknowledging our effort in this paper ."}, {"review_id": "U850oxFSKmN-2", "review_text": "This paper aims to model the complicated continuous time-series by using SDE for modeling the latent state trajectories . The authors claim that using SDE instead of ODE for the latent states has higher flexibility to capture more complex dynamics . Then they propose a continuous-time versions of variational evidence lower bounds ( ELBO ) which can be trained using ODE-RNNs as the inference networks . The proposed VSDN model has the capability to capture the latent state stochasticity not via the initial states but rather via SDEs , as opposed to other methods like latent ODE and latent SDE . Overall , the paper is interesting as it employs many recent advances in the field to introduce a richer model for the continuous complex time-series with the capability of taking into account the stochasticity of the latent state dynamics . Nevertheless , I have some questions as follows : 1- The first motivation of the paper to define the VSDN is that the methods based on the neural ODEs can not model complicated time-series . I wonder why is this claim true ? When the underlying latent state is an ODE , the stochasticity of the time-series could be modeled via rich conditional observation distributions . Why do we need to have a double stochasticity , one in latent state and one in the observation data ? 2- How many dimensions can VSDN handle ? I am wondering how it will work in a high dimensional regime ( say dim is several thousands ) . How many dimension are used for states and observation data ? 3- As the VSDN is using SDEs for the latent states , I wonder if it can quantify the uncertainty of the inferred latent state trajectories ? 4- Why in eq . ( 13 ) , $ h_ { t , pre } $ and $ h_t $ are summed ? Could they be concatenated instead of summation ? 5- Minor comment : I guess $ W_t $ in eq . ( 2 ) refers to the Wiener process , right ? But there is no definition on the text . So , the authors should define it in the paper so that the readers know what it is . # # # # # # # # # # EDIT # # # # # # # # # # The authors have addressed all my questions .", "rating": "7: Good paper, accept", "reply_text": "* * Q1 : Why do we need to have a double stochasticity , one in latent state and one in the observation data ? * * - If we do not introduce the latent state , the stochastic process of the data can only be modeled by the parameterization of the conditional observation distribution . But in many cases , it is hard to find good parameterization of the distribution , especially when the data distribution is very complicated . Classic graphical models ( i.e.Mixture Model ) solve this problem by introducing a discrete-value latent variable . Similarly , when we introduce the latent state into our model , we are able to define a more general and flexible parameterization of the data distribution ( as the integral in Eq . ( 4 ) ) under the same conditional observation distribution ( In Neural ODEs , it is $ P ( y_i|y_ { 1 : i-1 } ) $ , in VSDN and NeuralSDE , it is $ P ( y_i|y_ { 1 : i-1 } , X_ { t_i } ) ) $ . * * Q2 : How many dimensions can VSDN handle ? * * - In principle , our modal is quite scalable as a continuous-time stochastic RNN . In our motion experiments , the dimension of each data frame is 62 in MoCap dataset and 51 in h36m dataset . We also run an experiment on high-dimensional ( above 1k ) data . Our model has similar mean square errors with baselines but has much better NLL , which indicates that our model can better estimate the stochastic process of the data . In our future works , we will extend our work on more challenging very high-dimensional data ( e.g.video ) . * * Q3 : quantify the uncertainty of the inferred latent state trajectories * * - The quantification of the uncertainty can be achieved by analyzing the learned SDEs ( e.g.compute the distribution of the latent state by numerically solving Fokker-Planck equation ) . * * Q4 : Why in eq . ( 13 ) , $ \\overrightarrow { h } _ { t , pre } $ and $ \\overleftarrow { h } _t $ are summed ? Could they be concatenated instead of summation ? * * - In our model , the inference model and the generative model share the SAME drift network , which is a feed-forward network . If we concatenate the forward and backward features , the input \u2019 s dimension is not consistent . We have to define a DIFFERENT posterior drift network . But the number of parameters will increase and we don \u2019 t see obvious improvement in the experiment . Thus , we apply summation to use the SAME drift network . * * Q5 : Wiener process in Eq ( 2 ) . * * - Yes. $ W_t $ is the Wiener process . We will add the definition in the revision ( under Eq . ( 2 ) in Page 3 ) . > where $ H_ { \\mathcal { G } } $ and $ R_ { \\mathcal { G } } $ are the drift and diffusion functions of the latent SDE . $ W_t $ denotes the a Wiener process , which is also called standard Brownian motion ."}, {"review_id": "U850oxFSKmN-3", "review_text": "This paper introduces a latent variable model for high dimensional stochastic time-series . The model is akin to a VAE with RNNs that incorporate time-series data . The authors introduce two variants of the model , one which only contains a feedforward RNN ( filtering ) and another that contains feedforward and feedback RNNs ( smoothing ) . The authors use two inference procedures for the model , one the standard VAE , and the other importance weighted IWAE . The work is reasonably clearly presented and the experiments are multiple data sets are a nice addition . I have two primary concerns -- 1 ) I do not see how the choice of the two inference procedures ( which makes up a fair portion of the submission ) is well motivated , and 2 ) I find the arguments for them achieving state-of-the-art performance unconvincing . for 1 ) The use of these two inference methods feels ad hoc . Why not just choose one ? Using importance waiting and the standard ELBO does not seem to add to the paper . I see the primary contribution of the paper to be the model , not the inference procedure . If there is an important distinction between the inference methods , and the authors feel that is important to presenting their work , I would devote more space to explaining why there are two approaches -- the details of the objectives themselves could be moved to an appendix if needed . Additionally , regarding the inference procedure , it looks like a beta-VAE objective is used ( equation 6 ) , but the authors do not say why the hyperparameter beta is needed , and do not cite the Beta-VAE paper ( Higgins et al.2017 ) For 2 ) I would like to see more comparing the performance of this model to a stochastic high dimensional . The authors discuss neural ODEs , but there is no comparison against them for performance in the experiment section . If the comparison can not be made for some specific reason , the authors should explain why . Another model which comes to mind that might warrant some discussion/comparison is lfads ( Sussillo et al.2016 ) .One other point : I feel similarly to the choice of two models ( VSDN-F and VSDN-S ) as I do about the IWAE and VAE . I think it would be better to hone in on a stronger take home point -- do one of the methods ( say , VSDN-S ) achieve better performance than existing methods . Showing this on more tasks or compared to more models would , in my mind , make this a stronger submission . Some other concerns : It is hard to get anything out of figure two . The human skeletons are very small and there are many of them . If the authors wish to show how the distinct positions for each of the methods differ from ground truth , they should remove some of the panels ( again , not sure here why the different inference procedure warrants a different panel.I do n't think the authors want to communicate how the different inference procedures make an important difference , do they ? ) An additional task or more clear plot showing accuracy of recovered latents , or accuracy of generated time-series data would be useful . Typo in the final sentence of the first paragraph of the experiments section . You say `` two '' but compare to three things .", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * Q1 : important distinction between the inference methods and objectives , and the motivations . * * - VAE vs IWAE : In principle , IWAE can provide tighter evidence lower bound than VAE . In our experiments , the models trained by IWAE generally have better log-likelihood than those trained by VAE . - Filtering vs Smoothing : The motivation of using both inference models has been explained in the paragraph before and after Eq ( 13 ) . The scenarios of using these two models both exist . Smoothing is used when both history and future observations are given and we want to infer the latent states of some missing data . Filtering is used when only the history observations are given . In principle , the latent state in inference model has dependency on the whole data sequence . Therefore , the smoothing model is more accurate than filtering model , as the filtering model infers the latent state based up till current data points , while the smoothing model infers it by using the whole data obervations.In experiments , it is shown that VSDN-S has equal or better performance than VSDN-F . Besides , the filtering model does not have the backward RNN component ( which is used to access the future information ) . Compared with VSDN-S , VSDN-F has promising performance with less parameters and smaller model complexity . * * Q2 : $ \\beta $ -VAE objective is used ( equation 6 ) , but the authors do not say why the hyperparameter beta is needed , and do not cite the Beta-VAE paper ( Higgins et al.2017 ) * * - Thank you for pointing out the misleading part . Beta is a hyperparameter to weight the impact of KL term ( as in $ \\beta $ -VAE ) . However , during the experiment , we fix $ \\beta $ to 1 . So we still use the original VAE objective to train our model . We will clarify it in the revision and also cite the $ \\beta $ -VAE paper . The revision is given as follows : > Page 3 : \u201c \u03b2 is a hyper-parameter to weight the effect of the KL terms . In this paper , we fix \u03b2 as1.0 and $ \\mathcal { L } _ { VAE } $ is the original VAE objective ( Kingma & Welling , 2014 ) . In \u03b2-VAE ( Higginset al. , 2017 ; Burgess et al. , 2018 ) , it is shown that a larger \u03b2 can encourage the model to learn more efficient and disentangled representation from the data . \u201d * * Q3 : The authors discuss neural ODEs , but there is no comparison against them for performance in the experiment section . If the comparison can not be made for some specific reason , the authors should explain why . * * - In the SOTA papers we compared with ( GRU-ODE , ODE-RNN and LatentSDE ) , they have showed that NeuralODE ( called LatentODE in the neuralODE paper ( Chen et al. , 2018 ) ) has worse performance . Our experiments ( not shown in the paper ) are also consistent with their results that NeuralODE has worse performance . That was why we didn \u2019 t include NeuralODE as a SOTA to compare with . We will add LatentODE in the revised version . Besides , GRU-ODE and ODE-RNN are two extended instances of NeuralODEs . * * Q4 : Another model which comes to mind that might warrant some discussion/comparison is lfads ( Sussillo et al.2016 ) . * * - Thank you for pointing out this missing reference . We will include the discussion of LFADS into our paper . To our knowledge , LFADS is a discrete-time sequential variational auto-encoder . Our work is a continuous-time stochastic variational model for sporadic data . That was why we didn \u2019 t include it in our previous version . According to your comment , we add a discussion in Page 3 : > \u201c The VAE objective has been widely used for discrete-time stochastic recurrent modals , such as LFADS ( Sussillo et al. , 2016 ) , VRNN ( Chung et al. , 2015 ) and SRNN ( Fraccaro et al. , 2016 ) . The major difference between these models and our work is that we incorporate a continuous-time latent state into our model while the latent state of the discrete-time models evolves only at distinct and separate time slots . \u201d * * Q5 : It is hard to get anything out of figure two . * * - Thank you for your suggestion . We will remove some panels and make the skeletons larger in Figure 2 ( Page 7 ) . Besides , we have provided the video of the synthesized skeletons in the supplementary materials ."}], "0": {"review_id": "U850oxFSKmN-0", "review_text": "In this paper the author/s study/ies the fundamental problem of learning continuous-time stochastic dynamics , in the case where the available time series data suffer irregularity and sparseness . The paper assumes that high dimensional data are generated from a system where latent states are observed . The paper tackles the problem In such a settings , it is generally impossible to derive a continuous-time stochastic process which precisely describes the many behaviors of the system under study . The paper develops a method base on Variational Bayes applied to learn flexible continuous-time stochastic recurrent neural network , they call Variational Stochastic Differential Networks . The particualr feature of such model is to capture the stochastic dependency among latent states and observations . The paper provides theoretichal tool , under the form of lower bounds for the efficient training of the neural model . THe author/s state that the results from a rich set of numerical experiments witnesses that the proposed approach outperforms state-of-the-art continuous-time deep learning models to solve the prediction and interpolation tasks , in the particular case when irrelugar and sporading time series data is available . - Reason for Score : Overall , I vote for rejecting . I like the idea to mix parametric and non parametric components to learn time series dynamics . However , I have many concerns that are made explicit in the cons section . Hopefully the authors can address my concerns in the rebuttal period . - Pros.1 ) the paper tackles a relevant problem . 2 ) I appreciate the idea to combine parametric and non parametric models 3 ) the proposed model is clear 3 ) numerical experiments , on the selected data sets , witness in favour of the proposed method - Cons . 1 ) why other models for time series modeling and forecasting have not been taken into accout ? what about dynamic Bayesian networks , hidden Markov models , contiuous time Bayesian networks , and many other ... 2 ) the paper states the interest is related to high dimensional time series , but the data sets used for numerical experiments , in my humble opinion , are not as such , they are of very small or moderate dimensionality , in terms of number of variables . 3 ) I found no discussion about the choice of the dimension of the latent space , maybe my fauls and if this is the case I apologize , but still I think that more attention and more motivations must be given in the numerical experiments section . 4 ) I 'm under the impression that the proposed solution is a sensible combination of existing results and thus the work is somewhat of incremental nature , which I do not know whether is relevant for ICLR . 5 ) I 'm confused on the formulation of the inference problems , the paper considers both filtering and smoothing . However , later in the paper , the numerical experiments section , I read about `` prediction '' and `` interpolation '' which in my uderstanding is the same as filtering and smoothing . Thus , I would like the authors to better frame their problem . 6 ) at page 2 , I can not understand the difference between path and history , if they are used as synonims , please do not do it , it increases confusion to the interested reader , try to be more formal and clear as possible to let interested reader to appreciate your contributions . 7 ) at page 3 I read `` However , RG only use the historical data as input , as we observe no improvement by including the current state . `` , I would ask to motivate , personally I 'm not that surprised by this but I would like the paper to help understand the why of this occuring . 8 ) In equation ( 5 ) you are using the entire trajectory and thus you are performing smoothing , I think the problem/s tackled must be clearly described . 9 ) at page 5 I read `` In real-world applications , the latent state may not have strong dependency on future observations '' . This is somewhat surprising me , which has not strong dependency on which ? the latent does not influence the future ? the future does not tell much about latent ? This sentence in obscure to my elementary mind , and thus I kindly ask to better clarify on it . 10 ) Table 1 and 2 , please call the macro column filtering and smoothing in place of prediction and interpolation , maybe I completely miss the meaning of the difference you made between prediction and filtering and the difference between smoothing and interpolation . If this is the case , please better clarify on these aspects . 11 ) NLL and MSE of VSDN-S ( VAE ) are always better that those achieved by VSDN-F ( VAE ) , this is not that strange , one is using more info than the other , as well as I understood . The same does not apply to VSDN-S ( IWAE ) and VSDN-F ( IWAE ) , and this is surprising in my humble opinion , maybe not found the optimum ? 12 ) In table 2 , NLL is positive for LatentSDE , can you comment on this ? 13 ) The same as 12 ) in Table 3 , I kindly ask to clarify under which circumstances this happens . - Questions during rebuttal period : Please address and clarify the cons above - Some typos : page 3 : I read `` However , RG only use the historical '' , it should read `` However , RG only uses the historical '' I read `` HQ also use the '' , it should read `` HQ also uses the '' -", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Q1 : why other models for time series modeling and forecasting have not been taken into account ? what about dynamic Bayesian networks , hidden Markov models , continuous time Bayesian networks , and many other ... * * - There are rich literature works showing that deep neural net based models outperform the traditional probabilistic graphical models ( e.g. , HMM , DBN ) in many applications . Therefore , we focus more on the comparison with the deep neural net based models . Some references are given as follows : HMM : 1 . Nicolas Boulanger-Lewandowski , Yoshua Bengio , and Pascal Vincent . `` Modeling temporal dependencies in high-dimensional sequences : application to polyphonic music generation and transcription . '' In Proceedings of the 29th International Conference on International Conference on Machine Learning ( ICML'12 ) . Omnipress , Madison , WI , USA , 1881\u20131888 . 2.Lefebvre , Gr\u00e9goire , et al . `` BLSTM-RNN based 3D gesture classification . '' International conference on artificial neural networks . Springer , Berlin , Heidelberg , 2013 . 3.Rajib Ghosh , Chirumavila Vamshi , Prabhat Kumar , `` RNN based online handwritten word recognition in Devanagari and Bengali scripts using horizontal zoning '' , Pattern Recognition , Volume 92 , 2019 , Pages 203-218 . 4.Deshmukh , Akshay Madhav . `` Comparison of Hidden Markov Model and Recurrent Neural Network in Automatic Speech Recognition . '' European Journal of Engineering Research and Science 5 , no . 8 ( 2020 ) : 958-965 . DBN : 1.W\u00f6llmer , M. , Eyben , F. , Graves , A. , Schuller , B. , & Rigoll , G. `` Bidirectional LSTM networks for context-sensitive keyword detection in a cognitive virtual agent framework . '' Cognitive Computation , 2 ( 3 ) , 180-190 . 2.Nguyen , Duc-Canh , G\u00e9rard Bailly , and Fr\u00e9d\u00e9ric Elisei . `` Learning off-line vs. on-line models of interactive multimodal behaviors with recurrent neural networks . '' Pattern Recognition Letters 100 ( 2017 ) : 29-36 . * * Q2 : the paper states the interest is related to high dimensional time series , but the data sets used for numerical experiments , in my humble opinion , are not as such , they are of very small or moderate dimensionality , in terms of number of variables . * * - In our motion experiments , the dimension of each data frame is 62 in MoCap dataset and 51 in h36m dataset . The dimensionalities of the data in our experiments are similar as those in the literature works on deep generative models of sporadic data [ 1 - 4 ] . 1.Wei Cao , Dong Wang , Jian Li , Hao Zhou , Yitan Li , and Lei Li . 2018 . `` BRITS : bidirectional recurrent imputation for time series . '' In Proceedings of the 32nd International Conference on Neural Information Processing Systems ( NIPS'18 ) . 6776\u20136786 . 2.Yonghong Luo , Xiangrui Cai , Ying Zhang , Jun Xu , and Xiaojie Yuan . 2018 . `` Multivariate time series imputation with generative adversarial networks . '' In Proceedings of the 32nd International Conference on Neural Information Processing Systems ( NIPS'18 ) . 1603\u20131614 . 3.Yulia Rubanova , Tian Qi Chen , and David K Duvenaud . `` Latent ordinary differential equations for irregularly-sampled time series . '' InAdvances in Neural Information Processing Systems 32 , pp.5321\u20135331 . 2019.4.Edward De Brouwer , Jaak Simm , Adam Arany , and Yves Moreau . `` GRU-ODE-Bayes : Continuous Modeling of sporadically-observed time series . '' InAdvances in Neural Information ProcessingSystems 32 , pp . 7379\u20137390 . 2019.We also run an experiment on high-dimensional ( above 1k ) data . Our model has similar mean square errors with baselines but has much better NLL , which indicates that our model can better estimate the stochastic process of the data . In our future works , we will extend our work on more challenging very high-dimensional data ( e.g.video ) . * * Q3 : I found no discussion about the choice of the dimension of the latent space * * - The detailed model configuration of our experiments is given in Appendix C. In our preliminary studies , we tried different number of dimensions of the latent state . When the number of the dimensions of the latent state is too small , the performance of our model will degenerate . With large dimensions of the latent state , the performance is not sensitive with respect to the dimensions of latent state ."}, "1": {"review_id": "U850oxFSKmN-1", "review_text": "The paper introduces Variational Stochastic Differential Networks to filter and smooth sporadically observed time series . The authors adopt a Bayesian perceptive on the smoothing problem for time series living a latent space and irregularly observed . In particular , the random evolution of the process in latent space is clearly accounted for in the paper by embedding an SDE into an RNN . The authors derive a variational loss for their model and describe in fact multiple losses with different improvements such as importance sampling . In the end the authors do report issues caused by excess variance in training and settle for a convex combination of the two losses they propose . The paper becomes quite interesting in its experimental part as the authors show the superiority of their method as compared to previous approaches on data sets concerned with Mocap , synthetic OU process data and meteorological data .", "rating": "7: Good paper, accept", "reply_text": "- Thanks for acknowledging our effort in this paper ."}, "2": {"review_id": "U850oxFSKmN-2", "review_text": "This paper aims to model the complicated continuous time-series by using SDE for modeling the latent state trajectories . The authors claim that using SDE instead of ODE for the latent states has higher flexibility to capture more complex dynamics . Then they propose a continuous-time versions of variational evidence lower bounds ( ELBO ) which can be trained using ODE-RNNs as the inference networks . The proposed VSDN model has the capability to capture the latent state stochasticity not via the initial states but rather via SDEs , as opposed to other methods like latent ODE and latent SDE . Overall , the paper is interesting as it employs many recent advances in the field to introduce a richer model for the continuous complex time-series with the capability of taking into account the stochasticity of the latent state dynamics . Nevertheless , I have some questions as follows : 1- The first motivation of the paper to define the VSDN is that the methods based on the neural ODEs can not model complicated time-series . I wonder why is this claim true ? When the underlying latent state is an ODE , the stochasticity of the time-series could be modeled via rich conditional observation distributions . Why do we need to have a double stochasticity , one in latent state and one in the observation data ? 2- How many dimensions can VSDN handle ? I am wondering how it will work in a high dimensional regime ( say dim is several thousands ) . How many dimension are used for states and observation data ? 3- As the VSDN is using SDEs for the latent states , I wonder if it can quantify the uncertainty of the inferred latent state trajectories ? 4- Why in eq . ( 13 ) , $ h_ { t , pre } $ and $ h_t $ are summed ? Could they be concatenated instead of summation ? 5- Minor comment : I guess $ W_t $ in eq . ( 2 ) refers to the Wiener process , right ? But there is no definition on the text . So , the authors should define it in the paper so that the readers know what it is . # # # # # # # # # # EDIT # # # # # # # # # # The authors have addressed all my questions .", "rating": "7: Good paper, accept", "reply_text": "* * Q1 : Why do we need to have a double stochasticity , one in latent state and one in the observation data ? * * - If we do not introduce the latent state , the stochastic process of the data can only be modeled by the parameterization of the conditional observation distribution . But in many cases , it is hard to find good parameterization of the distribution , especially when the data distribution is very complicated . Classic graphical models ( i.e.Mixture Model ) solve this problem by introducing a discrete-value latent variable . Similarly , when we introduce the latent state into our model , we are able to define a more general and flexible parameterization of the data distribution ( as the integral in Eq . ( 4 ) ) under the same conditional observation distribution ( In Neural ODEs , it is $ P ( y_i|y_ { 1 : i-1 } ) $ , in VSDN and NeuralSDE , it is $ P ( y_i|y_ { 1 : i-1 } , X_ { t_i } ) ) $ . * * Q2 : How many dimensions can VSDN handle ? * * - In principle , our modal is quite scalable as a continuous-time stochastic RNN . In our motion experiments , the dimension of each data frame is 62 in MoCap dataset and 51 in h36m dataset . We also run an experiment on high-dimensional ( above 1k ) data . Our model has similar mean square errors with baselines but has much better NLL , which indicates that our model can better estimate the stochastic process of the data . In our future works , we will extend our work on more challenging very high-dimensional data ( e.g.video ) . * * Q3 : quantify the uncertainty of the inferred latent state trajectories * * - The quantification of the uncertainty can be achieved by analyzing the learned SDEs ( e.g.compute the distribution of the latent state by numerically solving Fokker-Planck equation ) . * * Q4 : Why in eq . ( 13 ) , $ \\overrightarrow { h } _ { t , pre } $ and $ \\overleftarrow { h } _t $ are summed ? Could they be concatenated instead of summation ? * * - In our model , the inference model and the generative model share the SAME drift network , which is a feed-forward network . If we concatenate the forward and backward features , the input \u2019 s dimension is not consistent . We have to define a DIFFERENT posterior drift network . But the number of parameters will increase and we don \u2019 t see obvious improvement in the experiment . Thus , we apply summation to use the SAME drift network . * * Q5 : Wiener process in Eq ( 2 ) . * * - Yes. $ W_t $ is the Wiener process . We will add the definition in the revision ( under Eq . ( 2 ) in Page 3 ) . > where $ H_ { \\mathcal { G } } $ and $ R_ { \\mathcal { G } } $ are the drift and diffusion functions of the latent SDE . $ W_t $ denotes the a Wiener process , which is also called standard Brownian motion ."}, "3": {"review_id": "U850oxFSKmN-3", "review_text": "This paper introduces a latent variable model for high dimensional stochastic time-series . The model is akin to a VAE with RNNs that incorporate time-series data . The authors introduce two variants of the model , one which only contains a feedforward RNN ( filtering ) and another that contains feedforward and feedback RNNs ( smoothing ) . The authors use two inference procedures for the model , one the standard VAE , and the other importance weighted IWAE . The work is reasonably clearly presented and the experiments are multiple data sets are a nice addition . I have two primary concerns -- 1 ) I do not see how the choice of the two inference procedures ( which makes up a fair portion of the submission ) is well motivated , and 2 ) I find the arguments for them achieving state-of-the-art performance unconvincing . for 1 ) The use of these two inference methods feels ad hoc . Why not just choose one ? Using importance waiting and the standard ELBO does not seem to add to the paper . I see the primary contribution of the paper to be the model , not the inference procedure . If there is an important distinction between the inference methods , and the authors feel that is important to presenting their work , I would devote more space to explaining why there are two approaches -- the details of the objectives themselves could be moved to an appendix if needed . Additionally , regarding the inference procedure , it looks like a beta-VAE objective is used ( equation 6 ) , but the authors do not say why the hyperparameter beta is needed , and do not cite the Beta-VAE paper ( Higgins et al.2017 ) For 2 ) I would like to see more comparing the performance of this model to a stochastic high dimensional . The authors discuss neural ODEs , but there is no comparison against them for performance in the experiment section . If the comparison can not be made for some specific reason , the authors should explain why . Another model which comes to mind that might warrant some discussion/comparison is lfads ( Sussillo et al.2016 ) .One other point : I feel similarly to the choice of two models ( VSDN-F and VSDN-S ) as I do about the IWAE and VAE . I think it would be better to hone in on a stronger take home point -- do one of the methods ( say , VSDN-S ) achieve better performance than existing methods . Showing this on more tasks or compared to more models would , in my mind , make this a stronger submission . Some other concerns : It is hard to get anything out of figure two . The human skeletons are very small and there are many of them . If the authors wish to show how the distinct positions for each of the methods differ from ground truth , they should remove some of the panels ( again , not sure here why the different inference procedure warrants a different panel.I do n't think the authors want to communicate how the different inference procedures make an important difference , do they ? ) An additional task or more clear plot showing accuracy of recovered latents , or accuracy of generated time-series data would be useful . Typo in the final sentence of the first paragraph of the experiments section . You say `` two '' but compare to three things .", "rating": "4: Ok but not good enough - rejection", "reply_text": "* * Q1 : important distinction between the inference methods and objectives , and the motivations . * * - VAE vs IWAE : In principle , IWAE can provide tighter evidence lower bound than VAE . In our experiments , the models trained by IWAE generally have better log-likelihood than those trained by VAE . - Filtering vs Smoothing : The motivation of using both inference models has been explained in the paragraph before and after Eq ( 13 ) . The scenarios of using these two models both exist . Smoothing is used when both history and future observations are given and we want to infer the latent states of some missing data . Filtering is used when only the history observations are given . In principle , the latent state in inference model has dependency on the whole data sequence . Therefore , the smoothing model is more accurate than filtering model , as the filtering model infers the latent state based up till current data points , while the smoothing model infers it by using the whole data obervations.In experiments , it is shown that VSDN-S has equal or better performance than VSDN-F . Besides , the filtering model does not have the backward RNN component ( which is used to access the future information ) . Compared with VSDN-S , VSDN-F has promising performance with less parameters and smaller model complexity . * * Q2 : $ \\beta $ -VAE objective is used ( equation 6 ) , but the authors do not say why the hyperparameter beta is needed , and do not cite the Beta-VAE paper ( Higgins et al.2017 ) * * - Thank you for pointing out the misleading part . Beta is a hyperparameter to weight the impact of KL term ( as in $ \\beta $ -VAE ) . However , during the experiment , we fix $ \\beta $ to 1 . So we still use the original VAE objective to train our model . We will clarify it in the revision and also cite the $ \\beta $ -VAE paper . The revision is given as follows : > Page 3 : \u201c \u03b2 is a hyper-parameter to weight the effect of the KL terms . In this paper , we fix \u03b2 as1.0 and $ \\mathcal { L } _ { VAE } $ is the original VAE objective ( Kingma & Welling , 2014 ) . In \u03b2-VAE ( Higginset al. , 2017 ; Burgess et al. , 2018 ) , it is shown that a larger \u03b2 can encourage the model to learn more efficient and disentangled representation from the data . \u201d * * Q3 : The authors discuss neural ODEs , but there is no comparison against them for performance in the experiment section . If the comparison can not be made for some specific reason , the authors should explain why . * * - In the SOTA papers we compared with ( GRU-ODE , ODE-RNN and LatentSDE ) , they have showed that NeuralODE ( called LatentODE in the neuralODE paper ( Chen et al. , 2018 ) ) has worse performance . Our experiments ( not shown in the paper ) are also consistent with their results that NeuralODE has worse performance . That was why we didn \u2019 t include NeuralODE as a SOTA to compare with . We will add LatentODE in the revised version . Besides , GRU-ODE and ODE-RNN are two extended instances of NeuralODEs . * * Q4 : Another model which comes to mind that might warrant some discussion/comparison is lfads ( Sussillo et al.2016 ) . * * - Thank you for pointing out this missing reference . We will include the discussion of LFADS into our paper . To our knowledge , LFADS is a discrete-time sequential variational auto-encoder . Our work is a continuous-time stochastic variational model for sporadic data . That was why we didn \u2019 t include it in our previous version . According to your comment , we add a discussion in Page 3 : > \u201c The VAE objective has been widely used for discrete-time stochastic recurrent modals , such as LFADS ( Sussillo et al. , 2016 ) , VRNN ( Chung et al. , 2015 ) and SRNN ( Fraccaro et al. , 2016 ) . The major difference between these models and our work is that we incorporate a continuous-time latent state into our model while the latent state of the discrete-time models evolves only at distinct and separate time slots . \u201d * * Q5 : It is hard to get anything out of figure two . * * - Thank you for your suggestion . We will remove some panels and make the skeletons larger in Figure 2 ( Page 7 ) . Besides , we have provided the video of the synthesized skeletons in the supplementary materials ."}}