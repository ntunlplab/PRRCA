{"year": "2017", "forum": "BJluGHcee", "title": "Tensorial Mixture Models", "decision": "Reject", "meta_review": "The authors have recently made several connections between deep learning and tensor algebra. While their earlier works dealt with supervised learning, the current work analyzes generative models through the lens of tensor algebra. \n The authors show propose a tensorial mixture model over local structures where the mixture components are expressed as tensor decompositions. They show that hierarchical tensor decomposition is exponentially more expressive compared to the shallow models. \n The paper makes original contributions in terms of establishing expressivity of deep generative models. The connections with tensor algebra could lead to further innovations, e.g. in training algorithms.\n However, the paper can be improved in two aspects: \n (1) It will be nice if the authors make a connection between the algebraic view presented here with the geometric view presented by: https://arxiv.org/abs/1606.05340\n (2) It is also desirable if more extensive experiments are performed.  \n Given the improvements outlined for the paper, it does not meet the bar for acceptance at ICLR. ", "reviews": [{"review_id": "BJluGHcee-0", "review_text": "This paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor. They use tensor decomposition and the result of their earlier paper on expressive power of CNNs along with hierarchical Tucker to provide an inference mechanism. However, this is conditioned on the existence of decomposition. The authors do not discuss how applicable their method is for a general case, what is the subspace where this decomposition exists/is efficient/has low approximation error. Their answer to this question is that in deep learning era these theoretical analysis is not needed. While this claim is subjective, I need to emphasize that the paper does not clarify this claim and does not mention the restrictions. Hence, from theoretical perspective, the paper has flaws and the claims are not justified completely. Some claims cannot be justified with the current results in tensor literature as the authors also mentioned in the discussions. Therefore, they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors. If we ignore the theoretical aspect and only consider the paper from empirical perspective, the experiments the appear in the paper are not enough to accept the paper. MNIST and CIFAR-10 are very simple baselines and more extensive experiments are required. Also, the experiments for missing data are not covering real cases and are too synthetic. Also, the paper lacks the extension beyond images. Since the authors repeatedly mention that their approach goes beyond images, and since the theory part is not complete, those experiments are essential for acceptance of this paper. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for reading our paper and taking the time reviewing it . Our response follows : Regarding our experiments , it is important to stress that beyond the usual hurdles of implementing and training a completely new computational model , we also had to benchmark several other methods on the same task , tremendously increasing the resources required to add each new dataset to our experiments . Unlike the case of standard classification , for which many existing results are available , the task of classification with missing data lack current results for comparison , and specifically when we wish to examine the case where the missingness distribution of the test set is different from the training set . Nevertheless , we will consider adding another experiment on a different domain . In regard to the reviewer \u2019 s comments on theoretical justifications of our model : we begin by clarifying that we did not wish to imply that theoretical questions , such as the ones the reviewer mentioned ( e.g.convergence guarantees ) , are unimportant . What we do wish to convey is that these are generally very hard open questions in the field of deep learning , which are outside the scope of our work , and from a practical perspective , typically do not pose significant obstacles to learnability . Despite the reviewer \u2019 s comments , we do allocate a significant amount of space to the topic of expressivity of our model , as seen throughout the body of the paper , and specifically in appendices B , C and D of our ICLR submission . Additionally , the reviewer suggests our paper contains theoretical flaws and unjustified claims , however , does not support these remarks with concrete instances where this is actually the case . Considering that our submission is part of a larger line of works relating tensor decompositions to deep networks , which had already been heavily scrutinized before they were published at reputable venues , such as COLT and ICML , we expect any claims of theoretical flaws of our model to be argued in a more specific manner . We wish to thank the reviewer again , and will be happy to answer any followup questions to our response ."}, {"review_id": "BJluGHcee-1", "review_text": "This paper uses Tensors to build generative models. The main idea is to divide the input into regions represented with mixture models, and represent the joint distribution of the mixture components with a tensor. Then, by restricting themselves to tensors that have an efficient decomposition, they train convolutional arithmetic circuits to generate the probability of the input and class label, providing a generative model of the input and labels. This approach seems quite elegant. It is not completely clear to me how the authors choose the specific architecture for their model, and how these choices relate to the class of joint distributions that they can represent, but even if these choices are somewhat heuristic, the overall framework provides a nice way of controlling the generality of the distributions that are represented. The experiments are on simple, synthetic examples of missing data. This is somewhat of a limitation, and the paper would be more convincing if it could include experiments on a real-world problem that contained missing data. One issue here is that it must be known which elements of the input are missing, which somewhat limits applicability. Could experiments be run on problems relating to the Netflix challenge, which is the classic example of a prediction problem with missing data? In spite of these limitations, the experiments provide appropriate comparisons to prior work, and form a reasonable initial evaluation. I was a little confused about how the input of missing data is handled experimentally. From the introductory discussion my impression was that the generative model was built over region patches in the image. This led me to believe that they would marginalize over missing regions. However, when the missing data consists of IID randomly missing pixels, it seems that every region will be missing some information. Why is it appropriate to marginalize over missing pixels? Specifically, $x_i$ in Equation 6 represents a local region, and the ensuing discussion shows how to marginalize over missing regions. How is this done when only a subset of a region is missing? It also seems like the summation in the equation following Equation 6 could be quite large. What is the run time of this? The paper is also a bit schizophrenic about the extent to which the results are applicable beyond images. The motivation for the probabilistic model is mostly in terms of images. But in the experiments, the authors state that they do not use state-of-the-art inpainting algorithms because their method is not limited to images and they want to compare to methods that are restricted to images. This would be more convincing if there were experiments outside the image domain. It was also not clear to me how, if at all, the proposed network makes use of translation invariance. It is widely assumed that much of the success of CNNs comes from their encoding of translation invariance through weight sharing. Is such invariance built into the authors\u2019 network? If not, why would we expect it to work well in challenging image domains? As a minor point, the paper is not carefully proofread. To just give a few examples from the first page or so: \u201csignificantly lesser\u201d -> \u201csignificantly less\u201d \u201cthe the\u201d \u201cprovenly\u201d -> provably ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the time and detailed remarks . Our response follows : 1 ) Regarding our experiments : when it comes to the missing data problem , it could be divided into two parts : ( 1 ) learning from data containing missing data , ( 2 ) predicting based on data containing missing values . There are many studies that have focused on the first part , and while our model is well suited to solve this task through the marginalized Maximum Likelihood objective , this is not where its advantages lies . Our advantage is when we wish to predict based on data containing missing values , and the missingness distribution is not guaranteed to be the same as in the training set ( e.g.due to sensors malfunction , or unexpected / rare behavior not modeled by the limited training set ) . This is why we chose to simulate different missingness distribution to highlight this advantage . 2 ) Regarding the requirement to know which elements are missing : In this paper we focused on the case where data is strictly missing , where we know when a value is missing or not , a case which is quite common in many circumstances . We do not discuss the case where data is corrupted due to noise , however , it is easy to prove similar guaranties for the noisy case as well . The limitation of using our model for classification under noise is that we have to know something regarding the noise mechanism , and in the extreme case where we know exactly the noise distribution , we get similar result of optimal classification . This is a topic which we might discuss more in a future work . 3 ) Regarding how missing data is handled : In the paper we discussed a simplified scenario , where only complete regions are missing . However , it is just as easy to marginalize over parts of region , where rep ( i , d ) equals the marginalized probability over the parts of the that region . If the whole region is missing , this results to 1 , as in our example . 4 ) Regarding the run time of marginalization : the run-time is exactly the same as prediction without marginalization . Eq.6 after marginalization is realized by the same ConvAC network of figure 3 , where the representation layer is adjusted to handle the missing data according to ( 3 ) . In practice , our model is implemented to use GPUs as part of Caffe \u2019 s neural network framework ( our code is available on GitHub ) , and though there is room for farther optimization of our code , it can handle missing data in real-time . 5 ) Regarding the applicability of our model to other domains : we conjecture that our model should work just as well on other domains exhibiting strong local correlations and weak global correlations as in images , which means it should probably work well on audio , video and text . We agree that this point would have been more convincing had we run experiments on other domains as well , however , limited resources and time led us to focus on the domain of images , which we are more acquainted with . If time will allow , we will gladly add an experiment on audio domain as well . 6 ) Regarding translation invariance : we do briefly discuss both the shared and the unshared cases , and expand on it more in appendix F , which explain the details of the models used in our experiments . There we explain that in the first two layers we are using weight sharing , where translation invariance matter most , and in the rest of the layers we use unshared parameters , which is analogue to the use of fully connected layers in standard ConvNets , which also do not employ parameter sharing . We have experimented with many other configurations of shared and unshared layers , and have found that the above function best on the datasets we have tested . Thank you once more for taking the time to read and review our article , and we would be happy to answer any followup questions ."}, {"review_id": "BJluGHcee-2", "review_text": "The paper provides an interesting use of generative models to address the classification with missing data problem. The tensorial mixture models proposed take into account the general problem of dependent samples. This is an nice extension of current mixture models where samples are usually considered as independent. Indeed the TMM model is reduced to the conventional latent variable models. As much as I love the ideas behind the paper, I feel pitiful about the sloppiness of the presentation (such as missing notations) and flaws in the technical derivations. Before going into the technical details, my high level concerns are as follows: (1) The joint density over all samples is modeled as a tensorial mixture generative model. The interpretation of the CP decomposition or HT decomposition on the prior density tensor is not clear. The authors have an interpretation of TMM as product of mixture models when samples are independent, however their interpretation seems flawed to me, and I will elaborate on this in the detailed technical comments below. (2) The authors employ convolution operators to compute an inner product. It is realizable by zero padding, but the invariance structure, which is the advantage of CNN compared to feed-forward neural network, will be lost. However, I am not sure how much this would affect the performance in practice. (3) The author could comment in the paper a little bit on the sample complexity of this method given the complexity of the model. Because I liked the ideas of the paper so much, and the ICLR paper submitted didn't present the technical details well due to sloppiness of notations, so I read the technical details in the arXiv version the authors pointed out. There are a few technical typos that I would like to point out (my reference to equations are to the ones in the arXiv paper). (1) The generative model as in figure (5) is flawed. P(x_i|d_i;\\theta_{d_i}) are vectors of length s, there the product of vectors is not well defined. It is obvious that the dimensions of the terms between two sides of the equation are not equal. In fact, this should be a tucker decomposition instead of multiplication. It should be P(X) = \\sum_{d1,\\ldots,d_N} P(d_1,\\ldots,d_N) (P(x_1|d_1;theta_{d_1},P(x_2|d_2;theta_{d_2},\\ldots,P(x_N|d_N;theta_{d_N}), which means a sum of multi-linear operation on tensor P(d_1,\\ldots,d_N), and each mode is projected onto P(x_i|d_i;theta_{d_i}. (2) I suspect the special case for diagonal Gaussian Mixture Models has some typos as I couldn't derive the third last equation on page 6. But it might be just I didn't understand this example. (3) The claim that TMM reduces to product of mixture model is not accurate. The first equation on page 7 is only right when \"sum of product\" operation is equal to \"product of sum\" operation. Similarly, in equation (6), the second equality doesn't hold unless in some special cases. However, this is not true. This might be just a typo, but it is good if the authors could fix this. I also suspect that if the authors correct this typo,the performance on MNIST might be improved. Overall, I like the ideas behind this paper very much. I suggest the authors fix the technical typos if the paper is accepted. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the time and the detailed remarks . We first want to emphasize that we acknowledge that our ICLR version is less elaborate than our long-form arXiv version , and that this may have led to the misunderstandings we address below . 1 ) Regarding your comments on equation 5 : P ( x_i|d_i ; \\theta_ { d_i } ) are scalars and not vectors , and thus the product over these scalars is defined as usual . More generally , equation 5 should be taken as a composition of functions for an arbitrary X=x_1 , \u2026 , x_N . The reviewer is correct to notice the similarity to the Tucker Decomposition , however , it is not a tucker decomposition of an algebraic tensor in the tensor product space of R^s vector spaces as the reviewer suggests , but a Tucker decomposition of a topological tensor over the tensor product of L^1 ( R^s ) Banach spaces , where the tensor product of functions f and g is equal to the product function ( f * g ) ( x ) = f ( x ) * g ( x ) . We chose not to discuss this perspective of equation 5 in the paper itself , as it only complicates the introduction of the model in an already highly technical paper . 2 ) Regarding the special case of diagonal Gaussian Mixture Models : We simply employ the fact that the product of diagonal Gaussian PDFs is itself a diagonal Gaussian PDF , where its mean is the concatenation of the means in the product , and the diagonal of the covariance matrix is the concatenation of the diagonals in the product . This can be directly proven by explicitly writing the product of these PDFs as a product of exponential functions . We will make sure to clarify this step in the final version of the article . 3 ) Regarding our claim that TMM reduces to product of mixture models : It is important to emphasize that we * do not * claim that a \u201c sum of product \u201d is equal to \u201c product of sum \u201d . What we do claim is that multiple sums , each with its respective running index d_i , over an expression which can be factorized to a product of separable expressions , each depending only on a single index d_i , can be rearranged to form a product of sums . This claim is always true as long as the expression over which we sum is indeed factorable as described above , which is the case when we assume the priors tensor from equation 5 holds P ( d_1 , \u2026 , d_N ) = \\prod_ { i=1 } ^N P ( d_i ) . 4 ) Regarding equation 6 : there is indeed a minor typo in the second equality , where P ( x_i|d_i ; \\theta_ { d_i } ) should be P ( x_i|d_i = d ; \\theta_d ) . Beyond this minor typo , the equality itself is correct , as the transition from multiple sums to product of sum satisfy the same condition as the one mentioned above in point ( 3 ) . 5 ) Regarding the loss of invariance structure of typical CNNs : This invariance can be mostly attributed to the use of parameter sharing , which we do discuss , although briefly , in the article . Do note that complex parameter sharing schemes could be devised to encourage different types of invariances . We also give more details on the exact parameter sharing scheme we use in appendix F , which describes the models used in our experiments . There we use shared parameters in the first two layers , where invariance matter most , while using unshared parameters scheme in the rest of the network , analogue to the use of fully-connected layers in standard ConvNets . We hope the above addresses the concerns of the reviewer , and would be happy to answer any further questions the reviewer might have ."}], "0": {"review_id": "BJluGHcee-0", "review_text": "This paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor. They use tensor decomposition and the result of their earlier paper on expressive power of CNNs along with hierarchical Tucker to provide an inference mechanism. However, this is conditioned on the existence of decomposition. The authors do not discuss how applicable their method is for a general case, what is the subspace where this decomposition exists/is efficient/has low approximation error. Their answer to this question is that in deep learning era these theoretical analysis is not needed. While this claim is subjective, I need to emphasize that the paper does not clarify this claim and does not mention the restrictions. Hence, from theoretical perspective, the paper has flaws and the claims are not justified completely. Some claims cannot be justified with the current results in tensor literature as the authors also mentioned in the discussions. Therefore, they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors. If we ignore the theoretical aspect and only consider the paper from empirical perspective, the experiments the appear in the paper are not enough to accept the paper. MNIST and CIFAR-10 are very simple baselines and more extensive experiments are required. Also, the experiments for missing data are not covering real cases and are too synthetic. Also, the paper lacks the extension beyond images. Since the authors repeatedly mention that their approach goes beyond images, and since the theory part is not complete, those experiments are essential for acceptance of this paper. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for reading our paper and taking the time reviewing it . Our response follows : Regarding our experiments , it is important to stress that beyond the usual hurdles of implementing and training a completely new computational model , we also had to benchmark several other methods on the same task , tremendously increasing the resources required to add each new dataset to our experiments . Unlike the case of standard classification , for which many existing results are available , the task of classification with missing data lack current results for comparison , and specifically when we wish to examine the case where the missingness distribution of the test set is different from the training set . Nevertheless , we will consider adding another experiment on a different domain . In regard to the reviewer \u2019 s comments on theoretical justifications of our model : we begin by clarifying that we did not wish to imply that theoretical questions , such as the ones the reviewer mentioned ( e.g.convergence guarantees ) , are unimportant . What we do wish to convey is that these are generally very hard open questions in the field of deep learning , which are outside the scope of our work , and from a practical perspective , typically do not pose significant obstacles to learnability . Despite the reviewer \u2019 s comments , we do allocate a significant amount of space to the topic of expressivity of our model , as seen throughout the body of the paper , and specifically in appendices B , C and D of our ICLR submission . Additionally , the reviewer suggests our paper contains theoretical flaws and unjustified claims , however , does not support these remarks with concrete instances where this is actually the case . Considering that our submission is part of a larger line of works relating tensor decompositions to deep networks , which had already been heavily scrutinized before they were published at reputable venues , such as COLT and ICML , we expect any claims of theoretical flaws of our model to be argued in a more specific manner . We wish to thank the reviewer again , and will be happy to answer any followup questions to our response ."}, "1": {"review_id": "BJluGHcee-1", "review_text": "This paper uses Tensors to build generative models. The main idea is to divide the input into regions represented with mixture models, and represent the joint distribution of the mixture components with a tensor. Then, by restricting themselves to tensors that have an efficient decomposition, they train convolutional arithmetic circuits to generate the probability of the input and class label, providing a generative model of the input and labels. This approach seems quite elegant. It is not completely clear to me how the authors choose the specific architecture for their model, and how these choices relate to the class of joint distributions that they can represent, but even if these choices are somewhat heuristic, the overall framework provides a nice way of controlling the generality of the distributions that are represented. The experiments are on simple, synthetic examples of missing data. This is somewhat of a limitation, and the paper would be more convincing if it could include experiments on a real-world problem that contained missing data. One issue here is that it must be known which elements of the input are missing, which somewhat limits applicability. Could experiments be run on problems relating to the Netflix challenge, which is the classic example of a prediction problem with missing data? In spite of these limitations, the experiments provide appropriate comparisons to prior work, and form a reasonable initial evaluation. I was a little confused about how the input of missing data is handled experimentally. From the introductory discussion my impression was that the generative model was built over region patches in the image. This led me to believe that they would marginalize over missing regions. However, when the missing data consists of IID randomly missing pixels, it seems that every region will be missing some information. Why is it appropriate to marginalize over missing pixels? Specifically, $x_i$ in Equation 6 represents a local region, and the ensuing discussion shows how to marginalize over missing regions. How is this done when only a subset of a region is missing? It also seems like the summation in the equation following Equation 6 could be quite large. What is the run time of this? The paper is also a bit schizophrenic about the extent to which the results are applicable beyond images. The motivation for the probabilistic model is mostly in terms of images. But in the experiments, the authors state that they do not use state-of-the-art inpainting algorithms because their method is not limited to images and they want to compare to methods that are restricted to images. This would be more convincing if there were experiments outside the image domain. It was also not clear to me how, if at all, the proposed network makes use of translation invariance. It is widely assumed that much of the success of CNNs comes from their encoding of translation invariance through weight sharing. Is such invariance built into the authors\u2019 network? If not, why would we expect it to work well in challenging image domains? As a minor point, the paper is not carefully proofread. To just give a few examples from the first page or so: \u201csignificantly lesser\u201d -> \u201csignificantly less\u201d \u201cthe the\u201d \u201cprovenly\u201d -> provably ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the time and detailed remarks . Our response follows : 1 ) Regarding our experiments : when it comes to the missing data problem , it could be divided into two parts : ( 1 ) learning from data containing missing data , ( 2 ) predicting based on data containing missing values . There are many studies that have focused on the first part , and while our model is well suited to solve this task through the marginalized Maximum Likelihood objective , this is not where its advantages lies . Our advantage is when we wish to predict based on data containing missing values , and the missingness distribution is not guaranteed to be the same as in the training set ( e.g.due to sensors malfunction , or unexpected / rare behavior not modeled by the limited training set ) . This is why we chose to simulate different missingness distribution to highlight this advantage . 2 ) Regarding the requirement to know which elements are missing : In this paper we focused on the case where data is strictly missing , where we know when a value is missing or not , a case which is quite common in many circumstances . We do not discuss the case where data is corrupted due to noise , however , it is easy to prove similar guaranties for the noisy case as well . The limitation of using our model for classification under noise is that we have to know something regarding the noise mechanism , and in the extreme case where we know exactly the noise distribution , we get similar result of optimal classification . This is a topic which we might discuss more in a future work . 3 ) Regarding how missing data is handled : In the paper we discussed a simplified scenario , where only complete regions are missing . However , it is just as easy to marginalize over parts of region , where rep ( i , d ) equals the marginalized probability over the parts of the that region . If the whole region is missing , this results to 1 , as in our example . 4 ) Regarding the run time of marginalization : the run-time is exactly the same as prediction without marginalization . Eq.6 after marginalization is realized by the same ConvAC network of figure 3 , where the representation layer is adjusted to handle the missing data according to ( 3 ) . In practice , our model is implemented to use GPUs as part of Caffe \u2019 s neural network framework ( our code is available on GitHub ) , and though there is room for farther optimization of our code , it can handle missing data in real-time . 5 ) Regarding the applicability of our model to other domains : we conjecture that our model should work just as well on other domains exhibiting strong local correlations and weak global correlations as in images , which means it should probably work well on audio , video and text . We agree that this point would have been more convincing had we run experiments on other domains as well , however , limited resources and time led us to focus on the domain of images , which we are more acquainted with . If time will allow , we will gladly add an experiment on audio domain as well . 6 ) Regarding translation invariance : we do briefly discuss both the shared and the unshared cases , and expand on it more in appendix F , which explain the details of the models used in our experiments . There we explain that in the first two layers we are using weight sharing , where translation invariance matter most , and in the rest of the layers we use unshared parameters , which is analogue to the use of fully connected layers in standard ConvNets , which also do not employ parameter sharing . We have experimented with many other configurations of shared and unshared layers , and have found that the above function best on the datasets we have tested . Thank you once more for taking the time to read and review our article , and we would be happy to answer any followup questions ."}, "2": {"review_id": "BJluGHcee-2", "review_text": "The paper provides an interesting use of generative models to address the classification with missing data problem. The tensorial mixture models proposed take into account the general problem of dependent samples. This is an nice extension of current mixture models where samples are usually considered as independent. Indeed the TMM model is reduced to the conventional latent variable models. As much as I love the ideas behind the paper, I feel pitiful about the sloppiness of the presentation (such as missing notations) and flaws in the technical derivations. Before going into the technical details, my high level concerns are as follows: (1) The joint density over all samples is modeled as a tensorial mixture generative model. The interpretation of the CP decomposition or HT decomposition on the prior density tensor is not clear. The authors have an interpretation of TMM as product of mixture models when samples are independent, however their interpretation seems flawed to me, and I will elaborate on this in the detailed technical comments below. (2) The authors employ convolution operators to compute an inner product. It is realizable by zero padding, but the invariance structure, which is the advantage of CNN compared to feed-forward neural network, will be lost. However, I am not sure how much this would affect the performance in practice. (3) The author could comment in the paper a little bit on the sample complexity of this method given the complexity of the model. Because I liked the ideas of the paper so much, and the ICLR paper submitted didn't present the technical details well due to sloppiness of notations, so I read the technical details in the arXiv version the authors pointed out. There are a few technical typos that I would like to point out (my reference to equations are to the ones in the arXiv paper). (1) The generative model as in figure (5) is flawed. P(x_i|d_i;\\theta_{d_i}) are vectors of length s, there the product of vectors is not well defined. It is obvious that the dimensions of the terms between two sides of the equation are not equal. In fact, this should be a tucker decomposition instead of multiplication. It should be P(X) = \\sum_{d1,\\ldots,d_N} P(d_1,\\ldots,d_N) (P(x_1|d_1;theta_{d_1},P(x_2|d_2;theta_{d_2},\\ldots,P(x_N|d_N;theta_{d_N}), which means a sum of multi-linear operation on tensor P(d_1,\\ldots,d_N), and each mode is projected onto P(x_i|d_i;theta_{d_i}. (2) I suspect the special case for diagonal Gaussian Mixture Models has some typos as I couldn't derive the third last equation on page 6. But it might be just I didn't understand this example. (3) The claim that TMM reduces to product of mixture model is not accurate. The first equation on page 7 is only right when \"sum of product\" operation is equal to \"product of sum\" operation. Similarly, in equation (6), the second equality doesn't hold unless in some special cases. However, this is not true. This might be just a typo, but it is good if the authors could fix this. I also suspect that if the authors correct this typo,the performance on MNIST might be improved. Overall, I like the ideas behind this paper very much. I suggest the authors fix the technical typos if the paper is accepted. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the time and the detailed remarks . We first want to emphasize that we acknowledge that our ICLR version is less elaborate than our long-form arXiv version , and that this may have led to the misunderstandings we address below . 1 ) Regarding your comments on equation 5 : P ( x_i|d_i ; \\theta_ { d_i } ) are scalars and not vectors , and thus the product over these scalars is defined as usual . More generally , equation 5 should be taken as a composition of functions for an arbitrary X=x_1 , \u2026 , x_N . The reviewer is correct to notice the similarity to the Tucker Decomposition , however , it is not a tucker decomposition of an algebraic tensor in the tensor product space of R^s vector spaces as the reviewer suggests , but a Tucker decomposition of a topological tensor over the tensor product of L^1 ( R^s ) Banach spaces , where the tensor product of functions f and g is equal to the product function ( f * g ) ( x ) = f ( x ) * g ( x ) . We chose not to discuss this perspective of equation 5 in the paper itself , as it only complicates the introduction of the model in an already highly technical paper . 2 ) Regarding the special case of diagonal Gaussian Mixture Models : We simply employ the fact that the product of diagonal Gaussian PDFs is itself a diagonal Gaussian PDF , where its mean is the concatenation of the means in the product , and the diagonal of the covariance matrix is the concatenation of the diagonals in the product . This can be directly proven by explicitly writing the product of these PDFs as a product of exponential functions . We will make sure to clarify this step in the final version of the article . 3 ) Regarding our claim that TMM reduces to product of mixture models : It is important to emphasize that we * do not * claim that a \u201c sum of product \u201d is equal to \u201c product of sum \u201d . What we do claim is that multiple sums , each with its respective running index d_i , over an expression which can be factorized to a product of separable expressions , each depending only on a single index d_i , can be rearranged to form a product of sums . This claim is always true as long as the expression over which we sum is indeed factorable as described above , which is the case when we assume the priors tensor from equation 5 holds P ( d_1 , \u2026 , d_N ) = \\prod_ { i=1 } ^N P ( d_i ) . 4 ) Regarding equation 6 : there is indeed a minor typo in the second equality , where P ( x_i|d_i ; \\theta_ { d_i } ) should be P ( x_i|d_i = d ; \\theta_d ) . Beyond this minor typo , the equality itself is correct , as the transition from multiple sums to product of sum satisfy the same condition as the one mentioned above in point ( 3 ) . 5 ) Regarding the loss of invariance structure of typical CNNs : This invariance can be mostly attributed to the use of parameter sharing , which we do discuss , although briefly , in the article . Do note that complex parameter sharing schemes could be devised to encourage different types of invariances . We also give more details on the exact parameter sharing scheme we use in appendix F , which describes the models used in our experiments . There we use shared parameters in the first two layers , where invariance matter most , while using unshared parameters scheme in the rest of the network , analogue to the use of fully-connected layers in standard ConvNets . We hope the above addresses the concerns of the reviewer , and would be happy to answer any further questions the reviewer might have ."}}