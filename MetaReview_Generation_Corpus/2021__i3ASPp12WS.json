{"year": "2021", "forum": "_i3ASPp12WS", "title": "Online Adversarial Purification based on Self-supervised Learning", "decision": "Accept (Poster)", "meta_review": "This paper presents a defense scheme for adversarial attacks, called self-supervised online adversarial purification (SOAP), by purifying the adversarial examples at test time. The novelty of this work is in its incorporation of self-supervised representation learning into adversarial defense through purification via optimizing an auxiliary self-supervised loss. This is done by jointly training the model on a self-supervised task while it is learning to perform the target classification task in a multi-task learning setting. Compared with existing adversarial defense schemes such as adversarial training and purification techniques, SOAP has a lower computation overhead during the training stage.\n\n**Strengths:**\n  * It is novel to incorporate self-supervised learning for adversarial purification at test time.\n  * SOAP\u2019s training stage based on multi-task learning incurs low computation overhead compared with the original classification task.\n\n**Weaknesses:**\n  * Although the proposed adversarial defense scheme is computationally cheaper than the other existing methods during the training stage, it does incur some overhead during test time. This may be undesirable for some applications in which efficiency during test time is an important factor to consider.\n  * The choice of a suitable self-supervised auxiliary task is somewhat ad hoc. The performance varies a lot for different auxiliary tasks.\n  * The experimental evaluation is only based on relatively small and unrealistic datasets even after new experiments on CIFAR-100 have been added by the authors.\n\nIt is said in the paper that SOAP can exploit a wider range of self-supervised signals for purification and hence conceptually can be applied to any format of data and not just images, given an appropriate self-supervised task. However, this claim has not been substantiated in the paper using non-image data.\n\nDespite some limitations and that some claims still need to be better substantiated, the paper presents some novel ideas which are expected to arouse interest for follow-up work in the adversarial attack and defense research community.\n", "reviews": [{"review_id": "_i3ASPp12WS-0", "review_text": "[ Summary ] Online defenses of adversarial examples is an old topic : Given an input x ( potentially adversarially perturbed ) at test time , we want to sanitize x to get x ' , on which the trained classifier $ g \\circ f $ gives the correct answer . This paper proposes a new architecture for online defenses via self supervision . There are two new things in the proposal : 1 . There is an explicit representation function f , namely the classifier is decomposed into $ g \\circ f $ . And the auxiliary self-supervised component h works on the same representation . This thus creates a Y-shape architecture that is `` syntactically '' similar to the training structure in unsupervised domain adaptation ( e.g. , domain adversarial neural networks ) . This architecture for online defense seems new ( as far as I know ) . 2.The paper leverages an interesting hypothesis that for a common f , a large classification loss happens if and only if a large self-supervision loss happens . And this paper provides solid evidence to justify this -- namely in Section 4.1 ( auxiliary-aware attacks ) , it evaluates the defense against an adversary that is aware of h , in order to create adversarial examples that explicitly breaks the hypothesis ( i.e.large classification loss but small self-supervision loss ) . 3.For the experiments -- the paper trained f , g , and h under Gaussian corruptions , and indeed found that this online purification strategy provides robustness under adversarial perturbations , even for auxiliary-aware attacks , which is interesting . [ Assessment ] 1 . My first worry is that the performance of the defense is still much worse than the performance from direct adversarial training ( for example , check the MNIST numbers ) . For example , under PGD , on CNN architecture we can achieve 80 % ish accuracy . Note that for MNST , a simple discretization can already achieve almost-perfect accuracy . This is especially the case if we consider auxiliary-aware attacks . 2.Following ( 1 ) , what worries me more is that online-purification still needs to be aware of the attack type . Namely if one looks into equation ( 4 ) , the objective has encoded norm-based attacks within it . This makes the results less interesting . 3.All in all , my major doubt is what is really the benefit of reduced training complexity if we can not achieve better robustness , and also the defense still needs to be fully aware of the attack type ? For these reasons , I vote for a weak reject . [ Questions ] 1 . Why do we need to know the results for FCN ( fully connected networks ) ? 2.I am not sure the numbers reported for adversarial training match the state of the art reported in the MNIST challenge leaderboard : https : //github.com/MadryLab/mnist_challenge . There the SOTA MNIST model always has > 88 % accuracy ( so I am a bit skeptical about DF can bring down the accuracy to 78 % for PGD AT ) . Also , how about applying those attacks for the self-supervision defense ? ( that 's an additional request ) . Similarly , for CIFAR10 , as shown by https : //github.com/MadryLab/cifar10_challenge , PGD AT is never under 43 % , but in Table 2 , the robust accuracy is only 2 % under CW attack . This is suspicious . [ Post rebuttal ] After more discussion and reading through the revision , I think this is a good paper and will be useful to the community for an instance of test-time defenses .", "rating": "7: Good paper, accept", "reply_text": "We thank you for your valuable feedback on our paper . We would like to address your concerns accordingly : 1 . By including the FCN results , we show that SOAP works for various architectures , and is not specific to CNNs . Though real-world visual understanding algorithms almost always rely on CNNs , the paradigm of SOAP is not specific to images and can easily generalize to other data formats , such as text and graphs . FCN or other architectures can be useful in those scenarios , combined with an appropriate self-supervised task . 2.The reason that our CW results seem low compared to the results posted on the leaderboard is that the two attacks are actually different despite bearing the same name . Specifically , our CW attack is the optimization-based l_2 bounded ( as is DF ) version from the original paper ( please see Sec.4.1 ) , while the leaderboard \u2019 s CW attack is the PGD-based l_inf bounded version from the PGD paper . For optimization-based attacks , the accuracy of any classifier can always go down to 0 if the bound is large enough . We apologize for the lack of clarity . In response to some other concerns : 1 . While it \u2019 s true that SOAP does not out-perform PGD AT with CNNs on MNIST ( we actually close the gap by tuning the CNNs more carefully ) , SOAP does out-perform PGD and FGSM AT on MNIST when the model capacity is low , and on CIFAR10 and CIFAR100 in general ( which are more realistic image datasets ) . SOAP also achieves very competitive accuracies on MNIST on black-box attacks . We also want to mention that , though not revealed in the paper , our experiments show that using uniform noises rather than Gaussian noises during training can improve the PGD accuracy of SOAP-DR to 90.13 % with CNNs on MNIST . We choose to use Gaussian noises for overall performance on all considered attacks . 2.Our defense is not based on knowledge of the attack . Rather we need to constrain the form of our defense ( the perturbation used to purify the image ) in order to solve the optimization problem . While our defense does parallel the l_inf bounded attack , note that in our experiments we show it performs well on both l_inf bounded attacks ( FGSM , PGD ) and l_2 bounded attacks ( CW , DF ) . This is in contrast to AT which uses knowledge of the attack type ( l_inf ) and performs poorly on l_2 attacks . Although our purification is based on l_inf-norm perturbations in Eq.4 ( because the purifier needs some budget ) , we show that SOAP can defend at least against l_inf and l_2 and potentially more . We have further clarified this in Sec.3.3.3.As above , our defense is not aware of the attack and we do obtain better accuracy on more difficult datasets than MNIST , e.g.CIFAR10 and CIFAR100 . Note that we significantly out-perform AT on l_2 attacks ."}, {"review_id": "_i3ASPp12WS-1", "review_text": "# # # # # # # # # # # # # # # # # # # Summary : This paper studies adversarial defense by combing purification and self-supervised loss . During inference , the authors propose an online-purification method based on ( clipped ) iterative gradient ascent . The loss used by purification is from some pre-defined self-supervised tasks . During training , joint loss of softmax and self-supervised loss are used to match the purification process in inference . Experiments on MNIST10 and CIFAR10 demonstrate the effectiveness of the proposed method over several SOTA baselines . The evaluation considers both the white-box and black-box attack setup . # # # # # # # # # # # # # # # # # # # Pros 1 . The proposed method is well-motivated and reasonable . 2.The paper is clear and easy-to-follow . # # # # # # # # # # # # # # # # # # # Cons 1 . What is the T for the online purification ? Large T will significantly slow down the test time efficiency . 2.In Table 2 , `` FGSM AT '' + `` PGD '' , why it is `` 37.4 % '' ? My understanding is this should be very small value , since multi-step PGD attack is pretty strong . 3.I am curious to see the gain by purely online-purification , maybe using the encoder by `` PGD AT '' . 4.Seems like self-supervised tasks are pretty ad-hoc . Is there a principled way of selecting a good self-supervised task ? 5.The two datasets used in the paper represents limited visual patterns . I think larger-dataset needs to be used , like cifar100 , tiny imagenet . # # # # # # # # # # # # # # # # # # # # # # # # # post-rebuttal I appreciate the additional explanations and experiments by the authors . I also read the public discussion threads . I raise my score to 6 . Two things for future : - Make it work on bigger and more realistic images , imagenet , pascal , coco , etc . Now the adversarial community and deep learning community in general , highly relies on experiments , because theoretical guarantee is still mysterious . So we should push the field forward , by proving ideas on harder datasets . - Explore stronger attacks , particularly gradient-free attack to avoid the obfuscated gradients .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for your valuable feedback on our paper . We would like to address your concerns accordingly 1 . We agree that a large T will diminish test-time efficiency . As shown in the Sec.4 Experiment , the value of T is 5 in each case which is relatively small . Practically , we found that increasing the value of T beyond 5 doesn \u2019 t help much . For more discussion on the choice of T , please see Sec.A2 in the appendix . 2.You are absolutely right that PGD is stronger than FGSM , but it is possible that FGSM-AT achieves relatively high PGD accuracy on CIFAR10 ( e.g.SAT accuracy in table 1 ( c ) from Song et.al . 2019 ) .We believe the reason for this is that label leaking ( Kurakin et.al.2017 ) , an issue standard FGSM-AT suffers from , does not happen in our case due to the low capacity of ResNet18 . When label leaking happens , as in other cases in our paper , we see FGSM accuracy that is higher than No-Atk accuracy as well as low PGD accuracy ( almost zero ) . 3.Our purification approach is purely online - the purification is performed at test-time . Also in contrast to Defense-GAN and Pixel-Defend which train the main model and the purifier separately , SOAP relies on the encoder f to learn a good auxiliary device h. It is not possible to use a different encoder with our auxiliary device h , since h operates with respect to the internal representation space ( output of f ) , therefore the two need to be trained together . Thus a trained auxiliary device h can not be simply composed onto a different network . We agree that it would be interesting to see how SOAP can be combined with AT for improved robustness in future work . 4.The whole field of self-supervised learning is growing fast but a theoretical foundation is still lacking , which makes such selection of appropriate self-supervised tasks empirical at this point . Nevertheless , we discuss some of the principles of selecting appropriate self-supervised tasks in Sec.3.4.Our main suggestion is that the selection self-supervision should be differentiable ( wrt inputs ) , comprehensive and efficient , and appropriate to the dataset . For example , RP will not work with MNIST since digits such as 0 , 1 , 6 , 8 and 9 can be invariant/interchangeable to 0 and 180 degrees . 5.We have included experiments on CIFAR100 and we out-perform AT on both ResNet and WideResNet architectures , and on both l_2 and l_inf attacks ."}, {"review_id": "_i3ASPp12WS-2", "review_text": "Summary : The paper introduces a defence for adversarial attack based on minimising a self-supervised loss on the test examples . Authors work under the assumption that minimising the self-supervised loss would be equivalent to minimising the supervised loss ( to which they do n't have access at test time ) . Authors evaluate their method on MNIST and CIFAR . Strengths : - The paper address the important topic of adversarial defence . Given the numerous adversarial attacks that have appeared in the last years and the deployment of novel classification methods into real world tools , I believe the field of adversarial defence is relevant . - Authors use in a smart way the self-supervised loss which is traditionally used for learning good representation as pretrainining networks . I think self-supervised learning should extend its usage over the traditional framework and this paper is one example of its potential . - Authors are also able to compute its own budget using the self-supervised loss , which I believe it 's additional evidence that the usage of self-supervised learning for adversarial defence is interesting and useful . - Quantitative results show how the proposed method is competitive with methods . - Authors also evaluate the effectiveness of the method when faced with an attacker knowing which defence method is using . Although I consider the proposed attack a baseline attack ( maybe other alternatives can be used ) , I believe it 's a relevant result . Weaknesses : - I am missing evaluation of the method in larger scale datasets , or more natural images dataset . I think for this methods to be applicable and useful , authors should demonstrate its usefulness into real data . - I am missing some images of the CIFAR dataset similar to Figure 2 . I know the supplementary material shows some , but it would be good to include some into the main paper . - I think authors should at least have one of the self-supervised methods ( LC , RP or DR ) show performance in both dataset . Given a new dataset , which methods should I select ? Conclusion : I believe the paper presents an interesting method with strong experimental results . The paper deserves acceptance as I believe it contains enough evidence to proof the effectiveness of the method .", "rating": "7: Good paper, accept", "reply_text": "We thank you for your positive feedback on our paper . We would like to address your remaining concerns accordingly : 1 . We have included results for CIFAR100 in Table 3 , demonstrating that we out-perform adversarial training on both l_2 and l_inf attacks and for two architectures ( ResNet and WideResNet ) . 2.We have added a visualization of adversarial and purified images for CIFAR10 . Note that in figure 2 , we leverage the autoencoder in the DR auxiliary task to visualize how the network \u2018 sees \u2019 the adversarial examples and purified images . There is no equivalent possibility for the RP and LC auxiliary tasks . Therefore we include the adversarial images and their purified versions . 3.We have added experimental results for CIFAR100 , where LC mostly performs better than RP . An explanation for this is that LC is a more difficult task , and therefore leads to a \u201c richer \u201d representation space . In addition , note that there are practical reasons why some self-supervised tasks can not work on both datasets . For example , RP will not work on MNIST because digits such as 0 , 1 , 6 , 8 and 9 can be invariant/interchangeable to 0 and 180 degrees of rotation . We have included a discussion on this in the paper . We also want to stress that the scope of self-supervised tasks is not limited to the three we discussed . We encourage readers to explore more possibilities and choose one that is the most appropriate for the dataset ."}], "0": {"review_id": "_i3ASPp12WS-0", "review_text": "[ Summary ] Online defenses of adversarial examples is an old topic : Given an input x ( potentially adversarially perturbed ) at test time , we want to sanitize x to get x ' , on which the trained classifier $ g \\circ f $ gives the correct answer . This paper proposes a new architecture for online defenses via self supervision . There are two new things in the proposal : 1 . There is an explicit representation function f , namely the classifier is decomposed into $ g \\circ f $ . And the auxiliary self-supervised component h works on the same representation . This thus creates a Y-shape architecture that is `` syntactically '' similar to the training structure in unsupervised domain adaptation ( e.g. , domain adversarial neural networks ) . This architecture for online defense seems new ( as far as I know ) . 2.The paper leverages an interesting hypothesis that for a common f , a large classification loss happens if and only if a large self-supervision loss happens . And this paper provides solid evidence to justify this -- namely in Section 4.1 ( auxiliary-aware attacks ) , it evaluates the defense against an adversary that is aware of h , in order to create adversarial examples that explicitly breaks the hypothesis ( i.e.large classification loss but small self-supervision loss ) . 3.For the experiments -- the paper trained f , g , and h under Gaussian corruptions , and indeed found that this online purification strategy provides robustness under adversarial perturbations , even for auxiliary-aware attacks , which is interesting . [ Assessment ] 1 . My first worry is that the performance of the defense is still much worse than the performance from direct adversarial training ( for example , check the MNIST numbers ) . For example , under PGD , on CNN architecture we can achieve 80 % ish accuracy . Note that for MNST , a simple discretization can already achieve almost-perfect accuracy . This is especially the case if we consider auxiliary-aware attacks . 2.Following ( 1 ) , what worries me more is that online-purification still needs to be aware of the attack type . Namely if one looks into equation ( 4 ) , the objective has encoded norm-based attacks within it . This makes the results less interesting . 3.All in all , my major doubt is what is really the benefit of reduced training complexity if we can not achieve better robustness , and also the defense still needs to be fully aware of the attack type ? For these reasons , I vote for a weak reject . [ Questions ] 1 . Why do we need to know the results for FCN ( fully connected networks ) ? 2.I am not sure the numbers reported for adversarial training match the state of the art reported in the MNIST challenge leaderboard : https : //github.com/MadryLab/mnist_challenge . There the SOTA MNIST model always has > 88 % accuracy ( so I am a bit skeptical about DF can bring down the accuracy to 78 % for PGD AT ) . Also , how about applying those attacks for the self-supervision defense ? ( that 's an additional request ) . Similarly , for CIFAR10 , as shown by https : //github.com/MadryLab/cifar10_challenge , PGD AT is never under 43 % , but in Table 2 , the robust accuracy is only 2 % under CW attack . This is suspicious . [ Post rebuttal ] After more discussion and reading through the revision , I think this is a good paper and will be useful to the community for an instance of test-time defenses .", "rating": "7: Good paper, accept", "reply_text": "We thank you for your valuable feedback on our paper . We would like to address your concerns accordingly : 1 . By including the FCN results , we show that SOAP works for various architectures , and is not specific to CNNs . Though real-world visual understanding algorithms almost always rely on CNNs , the paradigm of SOAP is not specific to images and can easily generalize to other data formats , such as text and graphs . FCN or other architectures can be useful in those scenarios , combined with an appropriate self-supervised task . 2.The reason that our CW results seem low compared to the results posted on the leaderboard is that the two attacks are actually different despite bearing the same name . Specifically , our CW attack is the optimization-based l_2 bounded ( as is DF ) version from the original paper ( please see Sec.4.1 ) , while the leaderboard \u2019 s CW attack is the PGD-based l_inf bounded version from the PGD paper . For optimization-based attacks , the accuracy of any classifier can always go down to 0 if the bound is large enough . We apologize for the lack of clarity . In response to some other concerns : 1 . While it \u2019 s true that SOAP does not out-perform PGD AT with CNNs on MNIST ( we actually close the gap by tuning the CNNs more carefully ) , SOAP does out-perform PGD and FGSM AT on MNIST when the model capacity is low , and on CIFAR10 and CIFAR100 in general ( which are more realistic image datasets ) . SOAP also achieves very competitive accuracies on MNIST on black-box attacks . We also want to mention that , though not revealed in the paper , our experiments show that using uniform noises rather than Gaussian noises during training can improve the PGD accuracy of SOAP-DR to 90.13 % with CNNs on MNIST . We choose to use Gaussian noises for overall performance on all considered attacks . 2.Our defense is not based on knowledge of the attack . Rather we need to constrain the form of our defense ( the perturbation used to purify the image ) in order to solve the optimization problem . While our defense does parallel the l_inf bounded attack , note that in our experiments we show it performs well on both l_inf bounded attacks ( FGSM , PGD ) and l_2 bounded attacks ( CW , DF ) . This is in contrast to AT which uses knowledge of the attack type ( l_inf ) and performs poorly on l_2 attacks . Although our purification is based on l_inf-norm perturbations in Eq.4 ( because the purifier needs some budget ) , we show that SOAP can defend at least against l_inf and l_2 and potentially more . We have further clarified this in Sec.3.3.3.As above , our defense is not aware of the attack and we do obtain better accuracy on more difficult datasets than MNIST , e.g.CIFAR10 and CIFAR100 . Note that we significantly out-perform AT on l_2 attacks ."}, "1": {"review_id": "_i3ASPp12WS-1", "review_text": "# # # # # # # # # # # # # # # # # # # Summary : This paper studies adversarial defense by combing purification and self-supervised loss . During inference , the authors propose an online-purification method based on ( clipped ) iterative gradient ascent . The loss used by purification is from some pre-defined self-supervised tasks . During training , joint loss of softmax and self-supervised loss are used to match the purification process in inference . Experiments on MNIST10 and CIFAR10 demonstrate the effectiveness of the proposed method over several SOTA baselines . The evaluation considers both the white-box and black-box attack setup . # # # # # # # # # # # # # # # # # # # Pros 1 . The proposed method is well-motivated and reasonable . 2.The paper is clear and easy-to-follow . # # # # # # # # # # # # # # # # # # # Cons 1 . What is the T for the online purification ? Large T will significantly slow down the test time efficiency . 2.In Table 2 , `` FGSM AT '' + `` PGD '' , why it is `` 37.4 % '' ? My understanding is this should be very small value , since multi-step PGD attack is pretty strong . 3.I am curious to see the gain by purely online-purification , maybe using the encoder by `` PGD AT '' . 4.Seems like self-supervised tasks are pretty ad-hoc . Is there a principled way of selecting a good self-supervised task ? 5.The two datasets used in the paper represents limited visual patterns . I think larger-dataset needs to be used , like cifar100 , tiny imagenet . # # # # # # # # # # # # # # # # # # # # # # # # # post-rebuttal I appreciate the additional explanations and experiments by the authors . I also read the public discussion threads . I raise my score to 6 . Two things for future : - Make it work on bigger and more realistic images , imagenet , pascal , coco , etc . Now the adversarial community and deep learning community in general , highly relies on experiments , because theoretical guarantee is still mysterious . So we should push the field forward , by proving ideas on harder datasets . - Explore stronger attacks , particularly gradient-free attack to avoid the obfuscated gradients .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for your valuable feedback on our paper . We would like to address your concerns accordingly 1 . We agree that a large T will diminish test-time efficiency . As shown in the Sec.4 Experiment , the value of T is 5 in each case which is relatively small . Practically , we found that increasing the value of T beyond 5 doesn \u2019 t help much . For more discussion on the choice of T , please see Sec.A2 in the appendix . 2.You are absolutely right that PGD is stronger than FGSM , but it is possible that FGSM-AT achieves relatively high PGD accuracy on CIFAR10 ( e.g.SAT accuracy in table 1 ( c ) from Song et.al . 2019 ) .We believe the reason for this is that label leaking ( Kurakin et.al.2017 ) , an issue standard FGSM-AT suffers from , does not happen in our case due to the low capacity of ResNet18 . When label leaking happens , as in other cases in our paper , we see FGSM accuracy that is higher than No-Atk accuracy as well as low PGD accuracy ( almost zero ) . 3.Our purification approach is purely online - the purification is performed at test-time . Also in contrast to Defense-GAN and Pixel-Defend which train the main model and the purifier separately , SOAP relies on the encoder f to learn a good auxiliary device h. It is not possible to use a different encoder with our auxiliary device h , since h operates with respect to the internal representation space ( output of f ) , therefore the two need to be trained together . Thus a trained auxiliary device h can not be simply composed onto a different network . We agree that it would be interesting to see how SOAP can be combined with AT for improved robustness in future work . 4.The whole field of self-supervised learning is growing fast but a theoretical foundation is still lacking , which makes such selection of appropriate self-supervised tasks empirical at this point . Nevertheless , we discuss some of the principles of selecting appropriate self-supervised tasks in Sec.3.4.Our main suggestion is that the selection self-supervision should be differentiable ( wrt inputs ) , comprehensive and efficient , and appropriate to the dataset . For example , RP will not work with MNIST since digits such as 0 , 1 , 6 , 8 and 9 can be invariant/interchangeable to 0 and 180 degrees . 5.We have included experiments on CIFAR100 and we out-perform AT on both ResNet and WideResNet architectures , and on both l_2 and l_inf attacks ."}, "2": {"review_id": "_i3ASPp12WS-2", "review_text": "Summary : The paper introduces a defence for adversarial attack based on minimising a self-supervised loss on the test examples . Authors work under the assumption that minimising the self-supervised loss would be equivalent to minimising the supervised loss ( to which they do n't have access at test time ) . Authors evaluate their method on MNIST and CIFAR . Strengths : - The paper address the important topic of adversarial defence . Given the numerous adversarial attacks that have appeared in the last years and the deployment of novel classification methods into real world tools , I believe the field of adversarial defence is relevant . - Authors use in a smart way the self-supervised loss which is traditionally used for learning good representation as pretrainining networks . I think self-supervised learning should extend its usage over the traditional framework and this paper is one example of its potential . - Authors are also able to compute its own budget using the self-supervised loss , which I believe it 's additional evidence that the usage of self-supervised learning for adversarial defence is interesting and useful . - Quantitative results show how the proposed method is competitive with methods . - Authors also evaluate the effectiveness of the method when faced with an attacker knowing which defence method is using . Although I consider the proposed attack a baseline attack ( maybe other alternatives can be used ) , I believe it 's a relevant result . Weaknesses : - I am missing evaluation of the method in larger scale datasets , or more natural images dataset . I think for this methods to be applicable and useful , authors should demonstrate its usefulness into real data . - I am missing some images of the CIFAR dataset similar to Figure 2 . I know the supplementary material shows some , but it would be good to include some into the main paper . - I think authors should at least have one of the self-supervised methods ( LC , RP or DR ) show performance in both dataset . Given a new dataset , which methods should I select ? Conclusion : I believe the paper presents an interesting method with strong experimental results . The paper deserves acceptance as I believe it contains enough evidence to proof the effectiveness of the method .", "rating": "7: Good paper, accept", "reply_text": "We thank you for your positive feedback on our paper . We would like to address your remaining concerns accordingly : 1 . We have included results for CIFAR100 in Table 3 , demonstrating that we out-perform adversarial training on both l_2 and l_inf attacks and for two architectures ( ResNet and WideResNet ) . 2.We have added a visualization of adversarial and purified images for CIFAR10 . Note that in figure 2 , we leverage the autoencoder in the DR auxiliary task to visualize how the network \u2018 sees \u2019 the adversarial examples and purified images . There is no equivalent possibility for the RP and LC auxiliary tasks . Therefore we include the adversarial images and their purified versions . 3.We have added experimental results for CIFAR100 , where LC mostly performs better than RP . An explanation for this is that LC is a more difficult task , and therefore leads to a \u201c richer \u201d representation space . In addition , note that there are practical reasons why some self-supervised tasks can not work on both datasets . For example , RP will not work on MNIST because digits such as 0 , 1 , 6 , 8 and 9 can be invariant/interchangeable to 0 and 180 degrees of rotation . We have included a discussion on this in the paper . We also want to stress that the scope of self-supervised tasks is not limited to the three we discussed . We encourage readers to explore more possibilities and choose one that is the most appropriate for the dataset ."}}