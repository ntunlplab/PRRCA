{"year": "2020", "forum": "SylR6n4tPS", "title": "Learning to Generate Grounded Visual Captions without Localization Supervision", "decision": "Reject", "meta_review": "This paper proposes a cyclical training scheme for grounded visual captioning, where a localization model is trained to identify the regions in the image referred to by caption words, and a reconstruction step is added conditioned on this information. This extends prior work which required grounding supervision. \n\nWhile the proposed approach is sensible and grounding of generated captions is an important requirement, some reviewers (me included) pointed out concerns about the relevance of this paper's contributions. I found the authors\u2019 explanation that the objective is not to improve the captioning accuracy but to refine its grounding performance without any localization supervision a bit unconvincing -- I would expect that better grounding would be reflected in overall better captioning performance, which seems to have happened with the supervised model of Zhou et al. (2019). In fact, even the localization gains seem rather small: \u201cThe attention accuracy for localizer is 20.4% and is higher than the 19.3% from the decoder at the end of training.\u201d Overall, the proposed model is an incremental change on the training of an image captioning system, by adding a localizer component, which is not used at test time. The authors' claim that \u201cThe network is implicitly regularized to update its attention mechanism to match with the localized image regions\u201d is also unclear to me -- there is nothing in the loss function that penalizes the difference between these two attentions, as the gradient doesn\u2019t backprop from one component to another. Sharing the LSTM and Language LSTM doesn\u2019t imply this, as the localizer is just providing guidance to the decoder, but there is no reason this will help the attention of the original model. \n\nOther natural questions left unanswered by this paper are:\n- What happens if we use the localizer also in test time (calling the decoder twice)? Will the captions improve? This experiment would be needed to assess the potential of this method to help image captioning.\n- Can we keep refining this iteratively?\n- Can we add a loss term on the disagreement of the two attentions to actually achieve the said regularisation effect?\n\nFinally, the paper [1] (cited by the authors) seems to employ a similar strategy (encoder-decoder with reconstructor) with shown benefits in video captioning.\n\n[1] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Reconstruction network for video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7622\u20137631, 2018.\n\nI suggest addressing some of these concerns in a revised version of the paper.", "reviews": [{"review_id": "SylR6n4tPS-0", "review_text": "# 1. Summary The paper deals with the problem of learning grounded captions from images without joint text-location information, but instead texts (words in captions) and locations are provided independently and the model needs to figure out their link. The model is built upon GVD (Zhou et al., 2019): each word generated by the encoder is grounded to the locations provided by the region proposal module (Up-Down model (Anderson et al., 2018)), used to reconstruct the ground-truth caption. My weak reject decision was guided by the following strengths and weaknesses of the paper. Strengths: * The reconstruction formulation of the problem is interesting and relevant for the task * Proposed new metric to measure grounding performance Weaknesses: * Questionable motivations: it is not clear what application is grounding text to the image useful for? * Marginal (not statistically significant?) improvement on image captioning metrics by using grounded text (proposed method) compared to not grounding (GVD) * Limited novelty: extension of GVD, where attention is removed and object locations are used instead # 2. Clarity and Motivation The paper is generally well written, however there are some concerns on motivations. One concern is related to the motivations of the paper. The authors use grounding as a proxy to improve image captioning results, which improvement is marginal wrt GVD (see Table 1). Why do we need to localize text if this has very marginal impact on the captioning metrics? It is missing the link between the potential applications where the localization of words is relevant. The authors claim that they do not uses any grounding annotation, however the pre-trained Faster-RCNN has been trained using annotations which consist of bounding boxes + categories. Therefore, the model do (in an implicit way) rely on grounding annotations, especially because there might be an overlap between the words (classes) used to pretrain the detector and the words in the captions. The authors should assess if this ovelap/bias in the pre-trained Faster-RCNN exists or not. Some other questions are still to be answered: * How are the regions R parametrized? Is it the visual representation or bounding box locations? * What happens to words that are not grounded to the image (e.g., verbs or articles)? Do you have a special way to deal with those? * What is the intuition of multiplying word embedding and region embeddings to generate z in Eq. 6? # 3. Novelty The proposed method is an extension of the existing model GVD, where the attentional module is removed and its functionality is replaced by the cyclical training mode with reconstruction of the object locations. From the technical point of view this is limited novelty, but still an interesting improvement of the model; however the experiments and results do not support the claim that using such model improves image captioning result in a significant way. One way to answer to this question would have been by showing an application where the outputted locations are used for downstream tasks. # 4. Experimentation The experiments are carried out in a scrupulous way, by showing the comparing with GVD (with and without attention; with and without grounding supervision). The non-convincing part of them (as mentioned above already) is the fact that the improvements on these datasets might be non significant for image captioning. For example, let's consider the image captioning results in Table 1 (Flickr30k Entities): cyclical have a max improvement of 0.7 (CIDER) and min of 0 (B@4) when compared with GVD without grounding supervision. There is an obvious huge improvement on the grounding evaluation, which is obvious since GVD does not do it explicitly. The same trend is in Table 2. These results are not convincing, combined by the fact that it is not clear in which applications one would want a very accurate grounded text. # Minor Points * Sec. 3.1: it is not clear that the Language LSTM is the decoder. Please explicitly say it before Eq. 1 * Caption of Table 3: which dataset is this? ", "rating": "3: Weak Reject", "reply_text": "We would like to thank the reviewer for the thoughtful and detailed feedback . We address the reviewer \u2019 s questions as below . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 1 . We focus on making captioning model more visually-grounded , not improving its captioning performance -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- As mentioned in the \u201c contributions summary \u201d in the introduction section , we focus on improving the grounding accuracy and retaining high captioning quality . We did not claim that the proposed method would significantly improve captioning scores . Recently , visual grounding has received a great deal of attention [ 2 ] - [ 11 ] . The primary reason for this is that in many vision-and-language downstream tasks researchers have discovered that the model can learn to heavily exploit and rely on linguistic priors or dataset priors . These vision and language models suffer from poor visual grounding . They often fall back on easy-to-learn priors rather than basing model predictions on visual concepts . They do this to ( seemingly ) achieve good performance according to the evaluation metrics . For example , in the visual captioning task , the SoTA models can achieve CIDEr score > 1.0 on MS-COCO which is higher than the human \u2019 s 0.85 [ 1 ] . However , the quality of the generated caption is still far from the quality of those generated by humans . For this reason , we believe that there are other important aspects besides improving captioning metrics that the research community should focus on , and much existing work has been actively discussing why visual grounding is an important topic and potentially mitigate other issues in visual question answering ( VQA ) , embedding question answering ( EQA ) , vision-and-language navigation ( VLN ) , visual captioning , etc [ 2 ] - [ 11 ] . Toward this direction , to the best of our knowledge , we are the first ones proposing to significantly improve grounding accuracy for captioning model without relying on the ground-truth grounding annotation and without introducing extra computation at test time . We showed that the improvements are consistent across image and video datasets . Our results in Figure 4 also indicates that our performance on rare words ( which is better than the supervised method ) shows that improving grounding in an unsupervised way can lead to less bias due to long-tail distribution of the grounding annotation . Finally , enforcing the model to be more visually-grounded makes the model more trustworthy and interpretable . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 2 . Grounding annotation and its relation to pre-trained object detector -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Grounding annotation is typically referred to as the direct links/correspondences between the words in the sentence and regions in the image/video [ 9 ] [ 10 ] . We argue that while the faster-RCNN is pre-trained from the object detection dataset , the pre-trained object classes do not directly translate to the words in the captions . Besides , the same image region could have different words describing it depending on the sentence context . For example , image regions on a tree could correspond to : \u201c tree \u201d , \u201c forest \u201d , \u201c mountain \u201d , \u201c shrubs \u201d , and \u201c bushes \u201d in sentences like : \u201c A man is cutting a tree \u201d , \u201c A woman is entering a forest \u201d , \u201c A man is hiking in the mountain \u201c , \u201c A woman is trimming shrubs \u201d , etc . If the model is biased towards the visually-groundable words ( object words ) in the dataset , it will , however , have poor captioning performance as we discussed in Sec 4.3 , Table 3 , and Table 4 . Note that in all three different cases ( original object detector , grounding-biased object detector , and unrealistically perfect object detector ) , we showed that grounding is still an important issue and our proposed method can successfully improve grounding accuracy regardless of whether the object detector is biased ."}, {"review_id": "SylR6n4tPS-1", "review_text": "This paper addresses captioning generation for images and videos, by proposing a novel cyclical training regimen consisting of three steps: decoding, localization, and reconstruction. The experimental results show that the performance on image captioning and video captioning are improved without grounding supervision. I lean to accept this paper. The motivation using cyclic feedback itself is not so novel for language generation, but focusing on grounding without localization supervision for visual captioning is interesting. The experimental results show that the proposed method can boost performance both qualitatively and qualitatively. I have several comments and questions below. - Why do the authors introduce GVD without self-attention as a baseline? Table 1 and 2 show that removing self-attention degrades the performance. If the combination of self-attention in GVD and cyclical training proposed in this paper is complementary to each other, it does help to improve the overall accuracy. - While the authors develop a cyclical training pipeline, including decoding, localization, and reconstruction, Figure 1 does not show which part corresponds to the decoding phase. The authors should clarify it to make the paper easier to be understood. - Equation (5) seems to be strange. $\\theta^*$, a sum of two parameters for each arg max operator, doesn't guarantee that each term in the right side of Eq. (5) keeps its max. This equation seems to be a conceptual one, and the actual training would be performed according to Eq. (7). Therefore, the experimental results might not be influenced by the error in Eq. (5). - $\\hat{r}^l_t=\\beta_t^\\top R$ between Eq. (6) and Eq. (7) means that $\\hat{r}^l_t$ is a row vector while $r_n$ seems to be a column vector. Since $R = [r_1, r_2, ..., r_N]$ for $N$ regions, $\\hat{r}^l_t= R \\beta_t$ seems to be appropriate. The authors should correct it. I have a similar comment for $\\hat{r}_t = \\alpha_t^\\top R$ in Eq. (2). - In the caption of Table 5, the number equal to or smaller than ten should be spelled out; \"5 runs\" should be \"five runs.\" There are similar errors, such as \"5 GT captions\" and \"1 GT caption\" in Sec. 4. - The format of items in References is not consistent. - According to Sec. A.4.1, $\\lambda_1$ and $\\lambda_2$ are tuned between 0 and 1. How are the experimental results sensitive to these hyperparameters? Additional experiments using different $\\lambda_1$ and $\\lambda_2$ would be helpful.", "rating": "6: Weak Accept", "reply_text": "We would like to thank the reviewer for the thoughtful , constructive feedback , and especially paying extra attention to the details . With the reviewer \u2019 s suggestions , we have clarified the caption of Figure 1 , corrected the equations , and made the format of the references consistent . Please see the revised pdf version . We address the other questions as below . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 1 . Why do the authors introduce GVD without self-attention as a baseline ? -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- From Table 2 in the GVD paper ( also copied below ) , removing the self-attention does not seem to degrade the performance . In fact , both captioning and grounding accuracy seem to be better without self-attention . What is more , grounding accuracy is also significantly better without self-attention . We thus decided to use the model without self-attention as our baseline model . When comparing our baseline results ( averaged across five runs ) with GVD ( only single run ) , they are comparable in terms of both captioning and grounding performances . To make it clear that our baseline is still comparable with GVD , we also report the standard deviation across five runs in the tables below . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ActivityNet Entities val set -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- B @ 1 B @ 4 M C S F1_all F1_loc -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- GVD ( single run ) 23.0 2.27 10.7 44.6 13.8 0.28 1.13 GVD"}, {"review_id": "SylR6n4tPS-2", "review_text": "The paper proposes an architecture that grounds words from a captioning model, but without requiring explicit per-word grounding training data. Instead, they show that it is sufficient to use cycle consistency, verifying that by predicting word->grounding->word the two words are the same. General: Cycle consistency has been shown to be very useful in replacing explicit paired data, for eample in image-to-image translation (CycleGAN, or the more recent FUNIT). This paper takes it to the domain of vision and language. While the novelty is not very large it seems like a solid step in an interesting direction. Evaluated on both image and video captioning with substantial localization improvement in the specific relevant eval settings. Specific comments: -- In Table 1, the part of \"Caption Evaluation\" the proposed method is in bold, but it seems that \"Up-Down\" method out-performs the proposed method in B@1 and B@4. -- Are words that are not nouns/verbs (the/a/are/with/etc) handled differently? It doesn't really make sense to localize them just like object words. -- The localization model is linear? What would be the effect of richer models on localization accuracy? -- Qualitative analysis: It would have been useful to add evaluations by human-raters to measure the perceptual quality of the localization. -- Error analysis? examples and analysis of failure cases? ", "rating": "6: Weak Accept", "reply_text": "We would like to thank the reviewer for the thoughtful and constructive feedback . We address the reviewer \u2019 s questions as below . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 1 . Are words that are not nouns/verbs handled differently ? -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- In the currently proposed method , all the words are handled the same regardless of whether they are nouns or verbs . We thank the reviewers ( R1 and R2 ) for the comment , and we explored a few variants to handle nouns and verbs differently from other words ( such as articles ) as suggested . The experimental results , however , were mixed , sometimes better and sometimes worse . Mainly , we explored with two variants : ( 1 ) The reconstruction loss is only computed when the target word is either nouns or verbs . ( 2 ) The localized region representation will be invalid ( set to zero ) if the target word is neither nouns nor verbs . For the first variant ( 1 ) , we observed that the captioning performance stays the same while grounding accuracy has a small improvement . On the other hand , for the second variant ( 2 ) , we can see that all captioning scores are improved over baseline with CIDEr improved 2.4 ( averaged across five runs ) . We can also see that grounding accuracy on per sentence basis further improved as well . We then conducted further experiments on both ActivityNet-Entities and Flickr30k Entities with unrealistically perfect object detector ( see tables below ) , the improvements however are not consistent . In summary : 1 ) On the Flickr30k Entities test set , we observed that CIDEr is better and grounding per sentence is also better 2 ) On the ActivityNet-Entities val set , the captioning performances are about the same but grounding accuracy became worse . 3 ) On the Flickr30k Entities test set with unrealistically perfect object detector , captioning performances are slightly worse but grounding accuracies are improved . We have included the additional experimental results/discussions of Flickr30k Entities and ActivityNet-Entities in Tables 6 , 7 , and 8 in the Appendix in the revision . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Flickr30k-Entities test set ( average five runs ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- B @ 1 B @ 4 M C S F1_all F1_loc F1_all_per_sent F1_loc_per_sent -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"}], "0": {"review_id": "SylR6n4tPS-0", "review_text": "# 1. Summary The paper deals with the problem of learning grounded captions from images without joint text-location information, but instead texts (words in captions) and locations are provided independently and the model needs to figure out their link. The model is built upon GVD (Zhou et al., 2019): each word generated by the encoder is grounded to the locations provided by the region proposal module (Up-Down model (Anderson et al., 2018)), used to reconstruct the ground-truth caption. My weak reject decision was guided by the following strengths and weaknesses of the paper. Strengths: * The reconstruction formulation of the problem is interesting and relevant for the task * Proposed new metric to measure grounding performance Weaknesses: * Questionable motivations: it is not clear what application is grounding text to the image useful for? * Marginal (not statistically significant?) improvement on image captioning metrics by using grounded text (proposed method) compared to not grounding (GVD) * Limited novelty: extension of GVD, where attention is removed and object locations are used instead # 2. Clarity and Motivation The paper is generally well written, however there are some concerns on motivations. One concern is related to the motivations of the paper. The authors use grounding as a proxy to improve image captioning results, which improvement is marginal wrt GVD (see Table 1). Why do we need to localize text if this has very marginal impact on the captioning metrics? It is missing the link between the potential applications where the localization of words is relevant. The authors claim that they do not uses any grounding annotation, however the pre-trained Faster-RCNN has been trained using annotations which consist of bounding boxes + categories. Therefore, the model do (in an implicit way) rely on grounding annotations, especially because there might be an overlap between the words (classes) used to pretrain the detector and the words in the captions. The authors should assess if this ovelap/bias in the pre-trained Faster-RCNN exists or not. Some other questions are still to be answered: * How are the regions R parametrized? Is it the visual representation or bounding box locations? * What happens to words that are not grounded to the image (e.g., verbs or articles)? Do you have a special way to deal with those? * What is the intuition of multiplying word embedding and region embeddings to generate z in Eq. 6? # 3. Novelty The proposed method is an extension of the existing model GVD, where the attentional module is removed and its functionality is replaced by the cyclical training mode with reconstruction of the object locations. From the technical point of view this is limited novelty, but still an interesting improvement of the model; however the experiments and results do not support the claim that using such model improves image captioning result in a significant way. One way to answer to this question would have been by showing an application where the outputted locations are used for downstream tasks. # 4. Experimentation The experiments are carried out in a scrupulous way, by showing the comparing with GVD (with and without attention; with and without grounding supervision). The non-convincing part of them (as mentioned above already) is the fact that the improvements on these datasets might be non significant for image captioning. For example, let's consider the image captioning results in Table 1 (Flickr30k Entities): cyclical have a max improvement of 0.7 (CIDER) and min of 0 (B@4) when compared with GVD without grounding supervision. There is an obvious huge improvement on the grounding evaluation, which is obvious since GVD does not do it explicitly. The same trend is in Table 2. These results are not convincing, combined by the fact that it is not clear in which applications one would want a very accurate grounded text. # Minor Points * Sec. 3.1: it is not clear that the Language LSTM is the decoder. Please explicitly say it before Eq. 1 * Caption of Table 3: which dataset is this? ", "rating": "3: Weak Reject", "reply_text": "We would like to thank the reviewer for the thoughtful and detailed feedback . We address the reviewer \u2019 s questions as below . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 1 . We focus on making captioning model more visually-grounded , not improving its captioning performance -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- As mentioned in the \u201c contributions summary \u201d in the introduction section , we focus on improving the grounding accuracy and retaining high captioning quality . We did not claim that the proposed method would significantly improve captioning scores . Recently , visual grounding has received a great deal of attention [ 2 ] - [ 11 ] . The primary reason for this is that in many vision-and-language downstream tasks researchers have discovered that the model can learn to heavily exploit and rely on linguistic priors or dataset priors . These vision and language models suffer from poor visual grounding . They often fall back on easy-to-learn priors rather than basing model predictions on visual concepts . They do this to ( seemingly ) achieve good performance according to the evaluation metrics . For example , in the visual captioning task , the SoTA models can achieve CIDEr score > 1.0 on MS-COCO which is higher than the human \u2019 s 0.85 [ 1 ] . However , the quality of the generated caption is still far from the quality of those generated by humans . For this reason , we believe that there are other important aspects besides improving captioning metrics that the research community should focus on , and much existing work has been actively discussing why visual grounding is an important topic and potentially mitigate other issues in visual question answering ( VQA ) , embedding question answering ( EQA ) , vision-and-language navigation ( VLN ) , visual captioning , etc [ 2 ] - [ 11 ] . Toward this direction , to the best of our knowledge , we are the first ones proposing to significantly improve grounding accuracy for captioning model without relying on the ground-truth grounding annotation and without introducing extra computation at test time . We showed that the improvements are consistent across image and video datasets . Our results in Figure 4 also indicates that our performance on rare words ( which is better than the supervised method ) shows that improving grounding in an unsupervised way can lead to less bias due to long-tail distribution of the grounding annotation . Finally , enforcing the model to be more visually-grounded makes the model more trustworthy and interpretable . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 2 . Grounding annotation and its relation to pre-trained object detector -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Grounding annotation is typically referred to as the direct links/correspondences between the words in the sentence and regions in the image/video [ 9 ] [ 10 ] . We argue that while the faster-RCNN is pre-trained from the object detection dataset , the pre-trained object classes do not directly translate to the words in the captions . Besides , the same image region could have different words describing it depending on the sentence context . For example , image regions on a tree could correspond to : \u201c tree \u201d , \u201c forest \u201d , \u201c mountain \u201d , \u201c shrubs \u201d , and \u201c bushes \u201d in sentences like : \u201c A man is cutting a tree \u201d , \u201c A woman is entering a forest \u201d , \u201c A man is hiking in the mountain \u201c , \u201c A woman is trimming shrubs \u201d , etc . If the model is biased towards the visually-groundable words ( object words ) in the dataset , it will , however , have poor captioning performance as we discussed in Sec 4.3 , Table 3 , and Table 4 . Note that in all three different cases ( original object detector , grounding-biased object detector , and unrealistically perfect object detector ) , we showed that grounding is still an important issue and our proposed method can successfully improve grounding accuracy regardless of whether the object detector is biased ."}, "1": {"review_id": "SylR6n4tPS-1", "review_text": "This paper addresses captioning generation for images and videos, by proposing a novel cyclical training regimen consisting of three steps: decoding, localization, and reconstruction. The experimental results show that the performance on image captioning and video captioning are improved without grounding supervision. I lean to accept this paper. The motivation using cyclic feedback itself is not so novel for language generation, but focusing on grounding without localization supervision for visual captioning is interesting. The experimental results show that the proposed method can boost performance both qualitatively and qualitatively. I have several comments and questions below. - Why do the authors introduce GVD without self-attention as a baseline? Table 1 and 2 show that removing self-attention degrades the performance. If the combination of self-attention in GVD and cyclical training proposed in this paper is complementary to each other, it does help to improve the overall accuracy. - While the authors develop a cyclical training pipeline, including decoding, localization, and reconstruction, Figure 1 does not show which part corresponds to the decoding phase. The authors should clarify it to make the paper easier to be understood. - Equation (5) seems to be strange. $\\theta^*$, a sum of two parameters for each arg max operator, doesn't guarantee that each term in the right side of Eq. (5) keeps its max. This equation seems to be a conceptual one, and the actual training would be performed according to Eq. (7). Therefore, the experimental results might not be influenced by the error in Eq. (5). - $\\hat{r}^l_t=\\beta_t^\\top R$ between Eq. (6) and Eq. (7) means that $\\hat{r}^l_t$ is a row vector while $r_n$ seems to be a column vector. Since $R = [r_1, r_2, ..., r_N]$ for $N$ regions, $\\hat{r}^l_t= R \\beta_t$ seems to be appropriate. The authors should correct it. I have a similar comment for $\\hat{r}_t = \\alpha_t^\\top R$ in Eq. (2). - In the caption of Table 5, the number equal to or smaller than ten should be spelled out; \"5 runs\" should be \"five runs.\" There are similar errors, such as \"5 GT captions\" and \"1 GT caption\" in Sec. 4. - The format of items in References is not consistent. - According to Sec. A.4.1, $\\lambda_1$ and $\\lambda_2$ are tuned between 0 and 1. How are the experimental results sensitive to these hyperparameters? Additional experiments using different $\\lambda_1$ and $\\lambda_2$ would be helpful.", "rating": "6: Weak Accept", "reply_text": "We would like to thank the reviewer for the thoughtful , constructive feedback , and especially paying extra attention to the details . With the reviewer \u2019 s suggestions , we have clarified the caption of Figure 1 , corrected the equations , and made the format of the references consistent . Please see the revised pdf version . We address the other questions as below . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 1 . Why do the authors introduce GVD without self-attention as a baseline ? -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- From Table 2 in the GVD paper ( also copied below ) , removing the self-attention does not seem to degrade the performance . In fact , both captioning and grounding accuracy seem to be better without self-attention . What is more , grounding accuracy is also significantly better without self-attention . We thus decided to use the model without self-attention as our baseline model . When comparing our baseline results ( averaged across five runs ) with GVD ( only single run ) , they are comparable in terms of both captioning and grounding performances . To make it clear that our baseline is still comparable with GVD , we also report the standard deviation across five runs in the tables below . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- ActivityNet Entities val set -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- B @ 1 B @ 4 M C S F1_all F1_loc -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- GVD ( single run ) 23.0 2.27 10.7 44.6 13.8 0.28 1.13 GVD"}, "2": {"review_id": "SylR6n4tPS-2", "review_text": "The paper proposes an architecture that grounds words from a captioning model, but without requiring explicit per-word grounding training data. Instead, they show that it is sufficient to use cycle consistency, verifying that by predicting word->grounding->word the two words are the same. General: Cycle consistency has been shown to be very useful in replacing explicit paired data, for eample in image-to-image translation (CycleGAN, or the more recent FUNIT). This paper takes it to the domain of vision and language. While the novelty is not very large it seems like a solid step in an interesting direction. Evaluated on both image and video captioning with substantial localization improvement in the specific relevant eval settings. Specific comments: -- In Table 1, the part of \"Caption Evaluation\" the proposed method is in bold, but it seems that \"Up-Down\" method out-performs the proposed method in B@1 and B@4. -- Are words that are not nouns/verbs (the/a/are/with/etc) handled differently? It doesn't really make sense to localize them just like object words. -- The localization model is linear? What would be the effect of richer models on localization accuracy? -- Qualitative analysis: It would have been useful to add evaluations by human-raters to measure the perceptual quality of the localization. -- Error analysis? examples and analysis of failure cases? ", "rating": "6: Weak Accept", "reply_text": "We would like to thank the reviewer for the thoughtful and constructive feedback . We address the reviewer \u2019 s questions as below . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 1 . Are words that are not nouns/verbs handled differently ? -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- In the currently proposed method , all the words are handled the same regardless of whether they are nouns or verbs . We thank the reviewers ( R1 and R2 ) for the comment , and we explored a few variants to handle nouns and verbs differently from other words ( such as articles ) as suggested . The experimental results , however , were mixed , sometimes better and sometimes worse . Mainly , we explored with two variants : ( 1 ) The reconstruction loss is only computed when the target word is either nouns or verbs . ( 2 ) The localized region representation will be invalid ( set to zero ) if the target word is neither nouns nor verbs . For the first variant ( 1 ) , we observed that the captioning performance stays the same while grounding accuracy has a small improvement . On the other hand , for the second variant ( 2 ) , we can see that all captioning scores are improved over baseline with CIDEr improved 2.4 ( averaged across five runs ) . We can also see that grounding accuracy on per sentence basis further improved as well . We then conducted further experiments on both ActivityNet-Entities and Flickr30k Entities with unrealistically perfect object detector ( see tables below ) , the improvements however are not consistent . In summary : 1 ) On the Flickr30k Entities test set , we observed that CIDEr is better and grounding per sentence is also better 2 ) On the ActivityNet-Entities val set , the captioning performances are about the same but grounding accuracy became worse . 3 ) On the Flickr30k Entities test set with unrealistically perfect object detector , captioning performances are slightly worse but grounding accuracies are improved . We have included the additional experimental results/discussions of Flickr30k Entities and ActivityNet-Entities in Tables 6 , 7 , and 8 in the Appendix in the revision . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Flickr30k-Entities test set ( average five runs ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- B @ 1 B @ 4 M C S F1_all F1_loc F1_all_per_sent F1_loc_per_sent -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"}}