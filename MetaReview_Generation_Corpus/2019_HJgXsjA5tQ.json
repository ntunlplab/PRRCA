{"year": "2019", "forum": "HJgXsjA5tQ", "title": "On the loss landscape of a class of deep neural networks with no bad local valleys", "decision": "Accept (Poster)", "meta_review": "This paper introduces a class of deep neural nets that provably have no bad local valleys. By constructing a new class of network this paper avoids having to rely on unrealistic assumptions and manages to provide a relatively concise proof that the network family has no strict local minima. Furthermore, it is demonstrated that this type of network yields reasonable experimental results on some benchmarks. The reviewers identified issues such as missing measurements of the training loss, which is the actual quantity studied in the theoretical results, as well as some issues with the presentation of the results. After revisions the reviewers are satisfied that their comments have been addressed. This paper continues an interesting line of theoretical research and brings it closer to practice and so it should be of interest to the ICLR community.", "reviews": [{"review_id": "HJgXsjA5tQ-0", "review_text": "This paper shows that a class of deep neural networks have no spurious local valleys \u2013--implying no strict local-minima. The family of neural networks studied includes a wide variety of network structure such as (a variant of) DenseNet. Overall, this paper makes some progress, improving previous results on over-parametrized networks. Pros: The flexibility of the network structure is an interesting point. Cons: CNN was covered in previous related works (so weight sharing is not a new contribution); DenseNet is not explicitly covered in this work (I mean current DenseNet does not have N skip-connections to output; correct me if wrong). The simulation part is not that clear, and I have a few questions that I hope the authors can answer. Some comments/suggestions: 1) Training error needs to be discussed. Page 8 says \u201cThis effect can be directly related to our result of Theorem 3.3 that the loss landscape of skip-networks has no bad local valley and thus it is not difficult to reach a solution with zero training error\u201d. This relation is not justified. The implication of Thm 3.3 is that getting zero training error is easier, but the tables are only for test error. Showing training error is the only way to connect to Thm 3.3. I expect to see a high training error for C-10, original VGG and sigmoid activation functions, and zero training error for both skip-SGD (rand) and skip-SGD (SGD). This paper has no theory on generalization, thus if a whole section is just about \u201cinvestigating generalization error\u201d, then the connection to theoretical parts is weak --btw, one connection is the comparison of two algorithms, which fits the context well, and thus interesting (though comparison result itself probably not surprising). 2) Data augmentation. \u201cNote that the rand algorithm cannot be used with data augmentation in a straightforward way and thus we skip it for this part.\u201d Why? With data augmentation, is M still larger than N? If yes, then the number of added skip connection is different for C-10 and C-10-plus, which is not mentioned in the instruction of Table 2. 3)It may be better to mention explicitly that \"it is possible to have bad local min\" \u2013perhaps in abstract and/or introduction. --Although \u201cno sub-optimal strict local minima\u201d is mentioned, readers, especially non-optimizers, might not notice \"strict\". --In fact, in the 1st round read, I do not have a strong impression of \"strict\". Later I realized it. Mentioning this can be helpful. 4) Some references I suggest to include: [R1] Yu, X. and Chen, G. On the local minima free condition of backpropagation learning. 1995. --related work. [R2] Lu, H., Kawaguchi, K. Depth creates no bad local minima. 2017. --also deep nets. [R3] Liang, S., Sun, R., Li, Y., & Srikant, R. \"Understanding the loss surface of neural networks for binary classification.\" 2018. --Also study SoftPlus neurons. [R4] Nouiehed, M., & Razaviyayn, M. Learning Deep Models: Critical Points and Local Openness. 2018. --also deep nets. Minor questions: --Exact 10% test accuracy for a few cases. Why exact 10%? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the feedback . Below are answers to your comments/questions by their numbering . 1 ) We agree with the reviewer about the training error matter . Thus we have added Section F in the appendix to discuss training error in details . As expected , the training error is zero except the case where sigmoid activation is used with original VGGs from Table 2 or original CNN13 from Table 1 . Moreover , we show in this section that adding skip-connections to the output is also helpful for training extremely deep ( narrow ) networks with softplus activation . This together show that skip-connections are helpful for training deep networks with both sigmoid and softplus activation . In Section E in the appendix , we provide a visual example of the loss landscape of a small network , before and after adding skip-connections , where one can see that adding skip-connections to the output layer help to smooth the loss surface and get rid of bad local valleys , which is helpful for local search algorithms like SGD to succeed . 2 ) As described in our experiments , the number of skip-connections is fixed to M=N in both cases ( with and without data-augmentation ) , where N is the size of the original data set . We quote the following sentence from our experimental section for the convenience of the reviewer : `` ... we aggregate all neurons of all the hidden layers in a pool and randomly choose from there a subset of N neurons to be connected to the output layer ... '' . In the setting of data-augmentation , at each training iteration the network uses additional examples ( randomly ) generated from the original dataset , and thus it is not clear in this case how the number of training samples should be defined . That 's why we fixed the number of skip-connections in both cases to be the size of the original data set . 3 ) We agree that this might be overlook by non-optimizers . Nevertheless we want to keep our abstract short and precise . Thus we have added the following sentence in the introduction to make this further clear : `` We note that this implies the loss landscape has no strict local minima , but theoretically non-strict local minima can still exist . '' 4 ) We have included the references suggested by the reviewer , and can add more detailed comparisons if the reviewer think that it 's necessary . Regarding 10 % test accuracy , we added a discussion on this issue under Section F in the appendix . Briefly , the reason , as observed in our experiments , is that the network converges quickly to a constant zero classifier ( i.e.the output of last hidden layer converges quickly to zero ) , and thus the training/test accuracy converge to 10 % and the cross-entropy loss in Equation ( 2 ) converges to \u2212 log ( 1/10 ) . We realized later that this is actually a known issue of sigmoid activation when training plain networks with depth > 5 , as pointed out earlier by Glorot & Bengio [ 1 ] . [ 1 ] Understanding the difficulty of training deep feedforward neural networks . Xavier Glorot , Yoshua Bengio . ICML 2010 ."}, {"review_id": "HJgXsjA5tQ-1", "review_text": "This paper presents a class of neural networks that does not have bad local valleys. The \u201cno bad local valleys\u201d implies that for any point on the loss surface there exists a continuous path starting from it, on which the loss doesn\u2019t increase and gets arbitrarily smaller and close to zero. The key idea is to add direct skip connections from hidden nodes (from any hidden layer) to the output. The good property of loss surface for networks with skip connections is impressive and the authors present interesting experimental results pointing out that * adding skip connections doesn\u2019t harm the generalization. * adding skip connections sometimes enables training for networks with sigmoid activation functions, while the networks without skip connections fail to achieve reasonable performance. * comparison of the generalization performance for the random sampling algorithm vs SGD and its connection to implicit bias is interesting. However, from a theoretical point of view, I would say the contribution of this work doesn\u2019t seem to be very significant, for the following reasons: * In the first place, figuring out \u201cwhy existing models work\u201d would be more meaningful than suggesting a new architecture which is on par with existing ones, unless one can show a significant performance improvement over the other ones. * The proof of the main theorem (Thm 3.3) is not very interesting, nor develops novel proof techniques. It heavily relies on Lemma 3.2, which I think is the main technical contribution of this paper. Apart from its technicality in the proof, the statement of Lemma 3.2 is just as expected and gives me little surprise, because having more than N hidden nodes connected directly to the output looks morally \u201cequivalent\u201d to having a layer as wide as N, and it is known that in such settings (e.g. Nguyen & Hein 17\u2019) it is easy to attain global minima. * I also think that having more than N skip connections can be problematic if N is very large, for example N>10^6. Then the network requires at least 1M nodes to fall in this class of networks without bad local valleys. If it is possible to remove this N-hidden-node requirement, it will be much more impressive. Below, I\u2019ll list specific comments/questions about the paper. * Assumption 3.1.2 doesn\u2019t make sense. Assumption 3.1.2 says \u201cthere exists N neurons satisfying\u2026\u201d and then the first bullet point says \u201cfor all j = 1, \u2026, M\u201d. Also, the statement \u201cone of the following conditions\u201d is unclear. Does it mean that we must have either \u201cN satisfying the first bullet\u201d or \u201cN satisfying the second bullet\u201d, or does it mean we can have N/2 satisfying the first and N/2 satisfying the second? * The paper does not describe where the assumptions are used. They are never used in the proof of Theorem 3.3, are they? I believe that they are used in the proof of Lemma 3.2 in the appendix, but if you can sketch/mention how the assumptions come into play in the proofs, that will be more helpful in understanding the meaning of the assumptions. * Are there any specific reasons for considering cross-entropy loss only? Lemma 3.2 looks general, so this result seems to be applicable to other losses. I wonder if there is any difficulty with different losses. * Are hidden nodes with skip connections connected to ALL m output nodes or just some of the output nodes? I think it\u2019s implicitly assumed in the proof that they are connected to all output nodes, but in this case Figure 2 is a bit misleading because there are hidden nodes with skip connections to only one of the output nodes. * For the experiments, how did you deal with pooling layers in the VGG and DenseNet architectures? Does max-pooling satisfy the assumptions? Or the experimental setting doesn\u2019t necessarily satisfy the assumptions? * Can you show the \u201cimprovement\u201d of loss surface by adding skip connections? Maybe coming up with a toy dataset and network WITH bad local valleys will be sufficient, because after adding N skip connections the network will be free of bad local valleys. Minor points * In the Assumption 3.1.3, the $N$ in $r \\neq s \\in N$ means $[N]$? * In the introduction, there is a sentence \u201cpotentially has many local minima, even for simple models like deep linear networks (Kawaguchi, 2016),\u201d which is not true. Deep linear networks have only global minima and saddle points, even for general differentiable convex losses (Laurent & von Brecht 18\u2019 and Yun et al. 18\u2019). * Assumption 3.1.3 looked a bit confusing to me at first glance. You might want to add some clarification such as \u201cfor example, in the fully connected network case, this means that all data points are distinct.\u201d", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for the detailed feedbacks . Below are answers to your comments/questions in the order that they appear . * `` In the first place , figuring out \u201c why existing models work \u201d would be more meaningful than suggesting a new architecture which is on par with existing ones , unless one can show a significant performance improvement over the other ones . '' We absolutely agree that understanding why existing models work is what one desires to achieve in the end . But to reach that point , one has to start somewhere , and make progress continually . This is the reason for the existence of a bunch of recent work on this topic : A. Choromanska , M. Hena , M. Mathieu , G. B. Arous , and Y. LeCun . The loss surfaces of multilayer networks . 2015.I . Safran and O. Shamir . On the quality of the initial basin in overspecified networks . 2016.B . D. Haeffele and R. Vidal . Global optimality in neural network training . 2017.H . Lu , K. Kawaguchi . Depth creates no bad local minima . 2017.M . Hardt and T. Ma . Identity matters in deep learning . 2017.C . Yun , S. Sra , and A. Jadbabaie . Global optimality conditions for deep neural networks . 2017.D . Soudry and E. Hoffer . Exponentially vanishing sub-optimal local minima in multilayer neural networks . 2017.M . Nouiehed and M. Razaviyayn . Learning Deep Models : Critical Points and Local Openness . 2018.T . Laurent and J. H. von Brecht . The Multilinear Structure of ReLU Networks . 2018.S . Liang , R. Sun , J. D. Lee , and R. Srikant . Adding one neuron can eliminate all bad local minima . 2018.At the moment , we are not aware of any previous work which can prove directly strong theoretical results on the loss landscape of `` existing models '' which actually work in practice . Moreover in this paper , we show that the presented class of networks enjoy both strong theoretical properties and good empirical performance . We do not make great claim about the result , but we believe that this is a significant contribution to the literature , especially w.r.t.the recent great effort of the community in trying to make progress on theoretical understanding of deep learning models . * `` The proof of the main theorem ( Thm 3.3 ) is not very interesting , nor develops novel proof techniques . It heavily relies on Lemma 3.2 , which I think is the main technical contribution of this paper . Apart from its technicality in the proof , the statement of Lemma 3.2 is just as expected and gives me little surprise , because having more than N hidden nodes connected directly to the output looks morally \u201c equivalent \u201d to having a layer as wide as N , and it is known that in such settings ( e.g.Nguyen & Hein 17 \u2019 ) it is easy to attain global minima . '' The proof of our main result is simple and elegant , as also noted by AnonReviewer2 . Simple proofs are often generalizable better to complex models . Thus we think that it is actually an advantage of this work . Can the reviewer elaborate on why the statement of Lemma 3.2 is just as expected ? Given that said , does the reviewer have in mind an easier proof for this lemma ? - which we would be very happy to know We would like to note that the class of networks analyzed in this Lemma is quite general and hence the mathematical proof is non-trivial . We agree that one can view the N skip-connections as an implicit wide layer , but this is just an intuition and very weak argument to conclude that the statements are just as expected . There are things that might look `` intuitive '' and `` as expected '' but it 's completely wrong , for instance , a deep linear network with N skip-connections to the output does not satisfy our conditions and results if the training data has very low rank . * `` I also think that having more than N skip connections can be problematic if N is very large , for example N > 10^6 . Then the network requires at least 1M nodes to fall in this class of networks without bad local valleys . If it is possible to remove this N-hidden-node requirement , it will be much more impressive . '' We agree that the current condition on the number of skip-connections is quite strong . But on the other hand , it 's not necessarily too restrictive at the level as mentioned by the reviewer . We would like to refer to Table 1 in [ 1 ] for some information on the number of neurons of the first layer of several existing networks . For instance , the first hidden layer of original VGG-Nets has already more than 3M nodes , and so if one sum up this number for all the hidden layers the total will be much than that . Moreover , in the literature it is common to find theoretical work which requires extremely larger number of neurons than the number of training samples , see e.g.https : //openreview.net/forum ? id=S1eK3i09YQ which requires N^6 neurons for gradient descent to find a zero training error solution for one hidden layer networks . Nevertheless , we agree with the reviewer that it would be interesting to relax this condition in future work . [ 1 ] Nguyen & Hein . Optimization landscape and expressivity of deep cnns . 2017 ."}, {"review_id": "HJgXsjA5tQ-2", "review_text": "The paper analyzes the loss landscape of a class of deep neural networks with skip connections added to the output layer. It proves that with the proposed structure of DNN, there are uncountably many solutions with zero training error, and the landscape has no bad local valley or local extrema. Overall I really enjoy reading the paper. The assumptions to aid the proof are very natural and much softer than the existing literature. As far as I\u2019m concerned, the setting is very close to real deep neural networks and the paper is a breakthrough in the area. The experiments also consolidate that the theoretical settings are natural and useful, namely, with enough skip connections and specially chosen activation functions. The presentation of the paper is intuitive and easy to follow. I\u2019ve also checked all the proof and think it\u2019s brilliantly and elegantly written. My only complaint is about the experiments. As we all know that both VGG and the sigmoid activation are commonly used DL tools, and why do they fail to generalize when used together? Does the network fail to converge or is it overfitting? The authors should try tuning the parameters and present a proper result. With that said, since the paper is more about theoretical findings, this issue doesn\u2019t influence my recommendation to accept the paper. Minor issues: I think it\u2019s better to formally define \u201cbad local valley\u201d somewhere in the paper. From what I read, the definition of \u201cbad local valley\u201d is implied by the abstract and in the proof of Theorem 3.3(2), but I did not find a formal definition anywhere else. In proof number 4 (of Theorem 3.3), the statement should be \u201cany *principle* submatrices of negative semi-definite matrices are also NSD\u201d, and it\u2019s not true otherwise. But this typo doesn\u2019t influence the proof. Also, it seems the proof of 3 is somewhat redundant, since local minimum is a special case of your \u201cbad local valley\u201d. It seems the analysis could not possibly be extended to the ReLU activation, since it will break the analytical property of the function. Just out of curiosity, do the authors have some further thoughts on non-differentiable activations? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for the support . Below are our answers to your comments/questions in the order that they appear . Regarding the failure of original VGG with sigmoid activation , we have added a discussion on this issue under Section F in the appendix ( please see also our response to AnonReviewer3 on the 10 % accuracy matter ) . Basically , we have observed that the network in this case converges to a constant zero classifier , regardless of our effort in tuning the learning rate . This behavior is actually not restricted to the specific architecture of VGG , but has been shown before as an issue of sigmoid activation when training plain networks with depth > 5 , see e.g . [ 1 ] .Answers to minor issues : Actually the definition of bad local valleys has previously appeared just above Theorem 3.3 in the text . However we follow the reviewer 's suggestion by putting this in a formal definition 3.3 now . `` In proof number 4 ( of Theorem 3.3 ) , the statement should be \u201c any * principle * submatrices of negative semi-definite matrices are also NSD \u201d , and it \u2019 s not true otherwise . But this typo doesn \u2019 t influence the proof . '' Yes , the reviewer is completely right . We fixed this typo . Thanks ! `` Also , it seems the proof of 3 is somewhat redundant , since local minimum is a special case of your \u201c bad local valley \u201d . '' We agree.We keep it there as we wanted to make all our statements and results become clear and as rigorous as possible . `` It seems the analysis could not possibly be extended to the ReLU activation , since it will break the analytical property of the function . Just out of curiosity , do the authors have some further thoughts on non-differentiable activations ? '' Thank you for an interesting question . At the moment , we do not really have a clear clue how to extend the result to general non-differentiable activations , so this could be an interesting question for future research . For ReLU , we think that it might be possible to exploit the fact that softplus can approximate ReLU arbitrarily well , and so perhaps a limiting argument on their corresponding loss functions can be helpful .. [ 1 ] Understanding the difficulty of training deep feedforward neural networks . Xavier Glorot , Yoshua Bengio . ICML 2010 ."}], "0": {"review_id": "HJgXsjA5tQ-0", "review_text": "This paper shows that a class of deep neural networks have no spurious local valleys \u2013--implying no strict local-minima. The family of neural networks studied includes a wide variety of network structure such as (a variant of) DenseNet. Overall, this paper makes some progress, improving previous results on over-parametrized networks. Pros: The flexibility of the network structure is an interesting point. Cons: CNN was covered in previous related works (so weight sharing is not a new contribution); DenseNet is not explicitly covered in this work (I mean current DenseNet does not have N skip-connections to output; correct me if wrong). The simulation part is not that clear, and I have a few questions that I hope the authors can answer. Some comments/suggestions: 1) Training error needs to be discussed. Page 8 says \u201cThis effect can be directly related to our result of Theorem 3.3 that the loss landscape of skip-networks has no bad local valley and thus it is not difficult to reach a solution with zero training error\u201d. This relation is not justified. The implication of Thm 3.3 is that getting zero training error is easier, but the tables are only for test error. Showing training error is the only way to connect to Thm 3.3. I expect to see a high training error for C-10, original VGG and sigmoid activation functions, and zero training error for both skip-SGD (rand) and skip-SGD (SGD). This paper has no theory on generalization, thus if a whole section is just about \u201cinvestigating generalization error\u201d, then the connection to theoretical parts is weak --btw, one connection is the comparison of two algorithms, which fits the context well, and thus interesting (though comparison result itself probably not surprising). 2) Data augmentation. \u201cNote that the rand algorithm cannot be used with data augmentation in a straightforward way and thus we skip it for this part.\u201d Why? With data augmentation, is M still larger than N? If yes, then the number of added skip connection is different for C-10 and C-10-plus, which is not mentioned in the instruction of Table 2. 3)It may be better to mention explicitly that \"it is possible to have bad local min\" \u2013perhaps in abstract and/or introduction. --Although \u201cno sub-optimal strict local minima\u201d is mentioned, readers, especially non-optimizers, might not notice \"strict\". --In fact, in the 1st round read, I do not have a strong impression of \"strict\". Later I realized it. Mentioning this can be helpful. 4) Some references I suggest to include: [R1] Yu, X. and Chen, G. On the local minima free condition of backpropagation learning. 1995. --related work. [R2] Lu, H., Kawaguchi, K. Depth creates no bad local minima. 2017. --also deep nets. [R3] Liang, S., Sun, R., Li, Y., & Srikant, R. \"Understanding the loss surface of neural networks for binary classification.\" 2018. --Also study SoftPlus neurons. [R4] Nouiehed, M., & Razaviyayn, M. Learning Deep Models: Critical Points and Local Openness. 2018. --also deep nets. Minor questions: --Exact 10% test accuracy for a few cases. Why exact 10%? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the feedback . Below are answers to your comments/questions by their numbering . 1 ) We agree with the reviewer about the training error matter . Thus we have added Section F in the appendix to discuss training error in details . As expected , the training error is zero except the case where sigmoid activation is used with original VGGs from Table 2 or original CNN13 from Table 1 . Moreover , we show in this section that adding skip-connections to the output is also helpful for training extremely deep ( narrow ) networks with softplus activation . This together show that skip-connections are helpful for training deep networks with both sigmoid and softplus activation . In Section E in the appendix , we provide a visual example of the loss landscape of a small network , before and after adding skip-connections , where one can see that adding skip-connections to the output layer help to smooth the loss surface and get rid of bad local valleys , which is helpful for local search algorithms like SGD to succeed . 2 ) As described in our experiments , the number of skip-connections is fixed to M=N in both cases ( with and without data-augmentation ) , where N is the size of the original data set . We quote the following sentence from our experimental section for the convenience of the reviewer : `` ... we aggregate all neurons of all the hidden layers in a pool and randomly choose from there a subset of N neurons to be connected to the output layer ... '' . In the setting of data-augmentation , at each training iteration the network uses additional examples ( randomly ) generated from the original dataset , and thus it is not clear in this case how the number of training samples should be defined . That 's why we fixed the number of skip-connections in both cases to be the size of the original data set . 3 ) We agree that this might be overlook by non-optimizers . Nevertheless we want to keep our abstract short and precise . Thus we have added the following sentence in the introduction to make this further clear : `` We note that this implies the loss landscape has no strict local minima , but theoretically non-strict local minima can still exist . '' 4 ) We have included the references suggested by the reviewer , and can add more detailed comparisons if the reviewer think that it 's necessary . Regarding 10 % test accuracy , we added a discussion on this issue under Section F in the appendix . Briefly , the reason , as observed in our experiments , is that the network converges quickly to a constant zero classifier ( i.e.the output of last hidden layer converges quickly to zero ) , and thus the training/test accuracy converge to 10 % and the cross-entropy loss in Equation ( 2 ) converges to \u2212 log ( 1/10 ) . We realized later that this is actually a known issue of sigmoid activation when training plain networks with depth > 5 , as pointed out earlier by Glorot & Bengio [ 1 ] . [ 1 ] Understanding the difficulty of training deep feedforward neural networks . Xavier Glorot , Yoshua Bengio . ICML 2010 ."}, "1": {"review_id": "HJgXsjA5tQ-1", "review_text": "This paper presents a class of neural networks that does not have bad local valleys. The \u201cno bad local valleys\u201d implies that for any point on the loss surface there exists a continuous path starting from it, on which the loss doesn\u2019t increase and gets arbitrarily smaller and close to zero. The key idea is to add direct skip connections from hidden nodes (from any hidden layer) to the output. The good property of loss surface for networks with skip connections is impressive and the authors present interesting experimental results pointing out that * adding skip connections doesn\u2019t harm the generalization. * adding skip connections sometimes enables training for networks with sigmoid activation functions, while the networks without skip connections fail to achieve reasonable performance. * comparison of the generalization performance for the random sampling algorithm vs SGD and its connection to implicit bias is interesting. However, from a theoretical point of view, I would say the contribution of this work doesn\u2019t seem to be very significant, for the following reasons: * In the first place, figuring out \u201cwhy existing models work\u201d would be more meaningful than suggesting a new architecture which is on par with existing ones, unless one can show a significant performance improvement over the other ones. * The proof of the main theorem (Thm 3.3) is not very interesting, nor develops novel proof techniques. It heavily relies on Lemma 3.2, which I think is the main technical contribution of this paper. Apart from its technicality in the proof, the statement of Lemma 3.2 is just as expected and gives me little surprise, because having more than N hidden nodes connected directly to the output looks morally \u201cequivalent\u201d to having a layer as wide as N, and it is known that in such settings (e.g. Nguyen & Hein 17\u2019) it is easy to attain global minima. * I also think that having more than N skip connections can be problematic if N is very large, for example N>10^6. Then the network requires at least 1M nodes to fall in this class of networks without bad local valleys. If it is possible to remove this N-hidden-node requirement, it will be much more impressive. Below, I\u2019ll list specific comments/questions about the paper. * Assumption 3.1.2 doesn\u2019t make sense. Assumption 3.1.2 says \u201cthere exists N neurons satisfying\u2026\u201d and then the first bullet point says \u201cfor all j = 1, \u2026, M\u201d. Also, the statement \u201cone of the following conditions\u201d is unclear. Does it mean that we must have either \u201cN satisfying the first bullet\u201d or \u201cN satisfying the second bullet\u201d, or does it mean we can have N/2 satisfying the first and N/2 satisfying the second? * The paper does not describe where the assumptions are used. They are never used in the proof of Theorem 3.3, are they? I believe that they are used in the proof of Lemma 3.2 in the appendix, but if you can sketch/mention how the assumptions come into play in the proofs, that will be more helpful in understanding the meaning of the assumptions. * Are there any specific reasons for considering cross-entropy loss only? Lemma 3.2 looks general, so this result seems to be applicable to other losses. I wonder if there is any difficulty with different losses. * Are hidden nodes with skip connections connected to ALL m output nodes or just some of the output nodes? I think it\u2019s implicitly assumed in the proof that they are connected to all output nodes, but in this case Figure 2 is a bit misleading because there are hidden nodes with skip connections to only one of the output nodes. * For the experiments, how did you deal with pooling layers in the VGG and DenseNet architectures? Does max-pooling satisfy the assumptions? Or the experimental setting doesn\u2019t necessarily satisfy the assumptions? * Can you show the \u201cimprovement\u201d of loss surface by adding skip connections? Maybe coming up with a toy dataset and network WITH bad local valleys will be sufficient, because after adding N skip connections the network will be free of bad local valleys. Minor points * In the Assumption 3.1.3, the $N$ in $r \\neq s \\in N$ means $[N]$? * In the introduction, there is a sentence \u201cpotentially has many local minima, even for simple models like deep linear networks (Kawaguchi, 2016),\u201d which is not true. Deep linear networks have only global minima and saddle points, even for general differentiable convex losses (Laurent & von Brecht 18\u2019 and Yun et al. 18\u2019). * Assumption 3.1.3 looked a bit confusing to me at first glance. You might want to add some clarification such as \u201cfor example, in the fully connected network case, this means that all data points are distinct.\u201d", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for the detailed feedbacks . Below are answers to your comments/questions in the order that they appear . * `` In the first place , figuring out \u201c why existing models work \u201d would be more meaningful than suggesting a new architecture which is on par with existing ones , unless one can show a significant performance improvement over the other ones . '' We absolutely agree that understanding why existing models work is what one desires to achieve in the end . But to reach that point , one has to start somewhere , and make progress continually . This is the reason for the existence of a bunch of recent work on this topic : A. Choromanska , M. Hena , M. Mathieu , G. B. Arous , and Y. LeCun . The loss surfaces of multilayer networks . 2015.I . Safran and O. Shamir . On the quality of the initial basin in overspecified networks . 2016.B . D. Haeffele and R. Vidal . Global optimality in neural network training . 2017.H . Lu , K. Kawaguchi . Depth creates no bad local minima . 2017.M . Hardt and T. Ma . Identity matters in deep learning . 2017.C . Yun , S. Sra , and A. Jadbabaie . Global optimality conditions for deep neural networks . 2017.D . Soudry and E. Hoffer . Exponentially vanishing sub-optimal local minima in multilayer neural networks . 2017.M . Nouiehed and M. Razaviyayn . Learning Deep Models : Critical Points and Local Openness . 2018.T . Laurent and J. H. von Brecht . The Multilinear Structure of ReLU Networks . 2018.S . Liang , R. Sun , J. D. Lee , and R. Srikant . Adding one neuron can eliminate all bad local minima . 2018.At the moment , we are not aware of any previous work which can prove directly strong theoretical results on the loss landscape of `` existing models '' which actually work in practice . Moreover in this paper , we show that the presented class of networks enjoy both strong theoretical properties and good empirical performance . We do not make great claim about the result , but we believe that this is a significant contribution to the literature , especially w.r.t.the recent great effort of the community in trying to make progress on theoretical understanding of deep learning models . * `` The proof of the main theorem ( Thm 3.3 ) is not very interesting , nor develops novel proof techniques . It heavily relies on Lemma 3.2 , which I think is the main technical contribution of this paper . Apart from its technicality in the proof , the statement of Lemma 3.2 is just as expected and gives me little surprise , because having more than N hidden nodes connected directly to the output looks morally \u201c equivalent \u201d to having a layer as wide as N , and it is known that in such settings ( e.g.Nguyen & Hein 17 \u2019 ) it is easy to attain global minima . '' The proof of our main result is simple and elegant , as also noted by AnonReviewer2 . Simple proofs are often generalizable better to complex models . Thus we think that it is actually an advantage of this work . Can the reviewer elaborate on why the statement of Lemma 3.2 is just as expected ? Given that said , does the reviewer have in mind an easier proof for this lemma ? - which we would be very happy to know We would like to note that the class of networks analyzed in this Lemma is quite general and hence the mathematical proof is non-trivial . We agree that one can view the N skip-connections as an implicit wide layer , but this is just an intuition and very weak argument to conclude that the statements are just as expected . There are things that might look `` intuitive '' and `` as expected '' but it 's completely wrong , for instance , a deep linear network with N skip-connections to the output does not satisfy our conditions and results if the training data has very low rank . * `` I also think that having more than N skip connections can be problematic if N is very large , for example N > 10^6 . Then the network requires at least 1M nodes to fall in this class of networks without bad local valleys . If it is possible to remove this N-hidden-node requirement , it will be much more impressive . '' We agree that the current condition on the number of skip-connections is quite strong . But on the other hand , it 's not necessarily too restrictive at the level as mentioned by the reviewer . We would like to refer to Table 1 in [ 1 ] for some information on the number of neurons of the first layer of several existing networks . For instance , the first hidden layer of original VGG-Nets has already more than 3M nodes , and so if one sum up this number for all the hidden layers the total will be much than that . Moreover , in the literature it is common to find theoretical work which requires extremely larger number of neurons than the number of training samples , see e.g.https : //openreview.net/forum ? id=S1eK3i09YQ which requires N^6 neurons for gradient descent to find a zero training error solution for one hidden layer networks . Nevertheless , we agree with the reviewer that it would be interesting to relax this condition in future work . [ 1 ] Nguyen & Hein . Optimization landscape and expressivity of deep cnns . 2017 ."}, "2": {"review_id": "HJgXsjA5tQ-2", "review_text": "The paper analyzes the loss landscape of a class of deep neural networks with skip connections added to the output layer. It proves that with the proposed structure of DNN, there are uncountably many solutions with zero training error, and the landscape has no bad local valley or local extrema. Overall I really enjoy reading the paper. The assumptions to aid the proof are very natural and much softer than the existing literature. As far as I\u2019m concerned, the setting is very close to real deep neural networks and the paper is a breakthrough in the area. The experiments also consolidate that the theoretical settings are natural and useful, namely, with enough skip connections and specially chosen activation functions. The presentation of the paper is intuitive and easy to follow. I\u2019ve also checked all the proof and think it\u2019s brilliantly and elegantly written. My only complaint is about the experiments. As we all know that both VGG and the sigmoid activation are commonly used DL tools, and why do they fail to generalize when used together? Does the network fail to converge or is it overfitting? The authors should try tuning the parameters and present a proper result. With that said, since the paper is more about theoretical findings, this issue doesn\u2019t influence my recommendation to accept the paper. Minor issues: I think it\u2019s better to formally define \u201cbad local valley\u201d somewhere in the paper. From what I read, the definition of \u201cbad local valley\u201d is implied by the abstract and in the proof of Theorem 3.3(2), but I did not find a formal definition anywhere else. In proof number 4 (of Theorem 3.3), the statement should be \u201cany *principle* submatrices of negative semi-definite matrices are also NSD\u201d, and it\u2019s not true otherwise. But this typo doesn\u2019t influence the proof. Also, it seems the proof of 3 is somewhat redundant, since local minimum is a special case of your \u201cbad local valley\u201d. It seems the analysis could not possibly be extended to the ReLU activation, since it will break the analytical property of the function. Just out of curiosity, do the authors have some further thoughts on non-differentiable activations? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for the support . Below are our answers to your comments/questions in the order that they appear . Regarding the failure of original VGG with sigmoid activation , we have added a discussion on this issue under Section F in the appendix ( please see also our response to AnonReviewer3 on the 10 % accuracy matter ) . Basically , we have observed that the network in this case converges to a constant zero classifier , regardless of our effort in tuning the learning rate . This behavior is actually not restricted to the specific architecture of VGG , but has been shown before as an issue of sigmoid activation when training plain networks with depth > 5 , see e.g . [ 1 ] .Answers to minor issues : Actually the definition of bad local valleys has previously appeared just above Theorem 3.3 in the text . However we follow the reviewer 's suggestion by putting this in a formal definition 3.3 now . `` In proof number 4 ( of Theorem 3.3 ) , the statement should be \u201c any * principle * submatrices of negative semi-definite matrices are also NSD \u201d , and it \u2019 s not true otherwise . But this typo doesn \u2019 t influence the proof . '' Yes , the reviewer is completely right . We fixed this typo . Thanks ! `` Also , it seems the proof of 3 is somewhat redundant , since local minimum is a special case of your \u201c bad local valley \u201d . '' We agree.We keep it there as we wanted to make all our statements and results become clear and as rigorous as possible . `` It seems the analysis could not possibly be extended to the ReLU activation , since it will break the analytical property of the function . Just out of curiosity , do the authors have some further thoughts on non-differentiable activations ? '' Thank you for an interesting question . At the moment , we do not really have a clear clue how to extend the result to general non-differentiable activations , so this could be an interesting question for future research . For ReLU , we think that it might be possible to exploit the fact that softplus can approximate ReLU arbitrarily well , and so perhaps a limiting argument on their corresponding loss functions can be helpful .. [ 1 ] Understanding the difficulty of training deep feedforward neural networks . Xavier Glorot , Yoshua Bengio . ICML 2010 ."}}