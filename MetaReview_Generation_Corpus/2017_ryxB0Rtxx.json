{"year": "2017", "forum": "ryxB0Rtxx", "title": "Identity Matters in Deep Learning", "decision": "Accept (Poster)", "meta_review": "The paper begins by presenting a simple analysis for deep linear networks. This is more to demonstrate the intuitions behind their derivations, and does not have practical relevance. They then extend to non-linear resnets with ReLU units and demonstrate that they have finite sample expressivity. They formally establish these results. Inspired by their theory, they perform experiments using simpler architectures without any batch norm, dropout or other regularizations and fix the last layer and still attain competitive results. Indeed, they admit that data augmentation is a form of regularization and can replace other regularization schemes. \n I think the paper meets the threshold to be accepted.", "reviews": [{"review_id": "ryxB0Rtxx-0", "review_text": "This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity. This paper is well written and studied a fundamental problem in deep neural network. I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks. One of key questions is how to extent the result in this paper to the more general nonlinear actuation function case. Minors: one line before Eq. (3.1), U \\in R ? \\times k ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the comments ! We will fix the typo . ( It should be U \\in R^ { k\\times k } )"}, {"review_id": "ryxB0Rtxx-1", "review_text": "Paper Summary: Authors investigate identity re-parametrization in the linear and the non linear case. Detailed comments: \u2014 Linear Residual Network: The paper shows that for a linear residual network any critical point is a global optimum. This problem is non convex it is interesting that this simple re-parametrization leads to such a result. \u2014 Non linear Residual Network: Authors propose a construction that maps the points to their labels via a resnet , using an initial random projection, followed by a residual block that clusters the data based on their label, and a last layer that maps the clusters to the label. 1- In Eq 3.4 seems the dimensions are not matching q_j in R^k and e_j in R^r. please clarify 2- The construction seems fine, but what is special about the resnet here in this construction? One can do a similar construction if we did not have the identity? can you discuss this point? In the linear case it is clear from a spectral point of view how the identity is helping the optimization. Please provide some intuition. 3- Existence of a network in the residual class that overfits does it give us any intuition on why residual network outperform other architectures? What does an existence result of such a network tell us about its representation power ? A simple linear model under the assumption that points can not be too close can overfit the data, and get fast convergence rate (see for instance tsybakov noise condition). 4- What does the construction tell us about the number of layers? 5- clustering the activation independently from the label, is an old way to pretrain the network. One could use those centroids as weights for the next layer (this is also related to Nystrom approximation see for instance https://www.cse.ust.hk/~twinsen/nystrom.pdf ). Your clustering is very strongly connected to the label at each residual block. I don't think this is appealing or useful since no feature extraction is happening. Moreover the number of layers in this construction does not matter. Can you weaken the clustering to be independent to the label at least in the early layers? then one could you use your construction as an initialization in the training. \u2014 Experiments : - last layer is not trained means the layer before the linear layer preceding the softmax? Minor comments: Abstract: how the identity mapping motivated batch normalization? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your valuable suggestions and comments that we address below . 1.There is a typo in the equation 3.4 ( and at several related places ) . There shouldn \u2019 t be q_j in Eqn 3.4 . In other words , the last layer doesn \u2019 t use identity parameterization because we need to change the dimension to match the number of labels . We will update the paper very soon to correct these typos . 2.As far as we know , the proof for finite sample expressivity for feedforward nets requires the network to have a very large width ( the width needs to be larger than the number of examples ) . The benefit of ResNets is that it allows such construction to use a network with a small width . We think this is a more natural construction , which is also both more similar to the nets used in practice , and exemplifies the appealing properties of ResNets . 3.Indeed , finite sample expressivity result does show that representational power of the net is great , though it doesn \u2019 t imply overfitting . ( As a matter of fact , such nets don \u2019 t overfit ) . Moreover , as argued before , such expressivity result is not easily obtained for other architecture like feed-forward neural nets or linear model ( specifically , with a linear model , one can use effectively at most d ( =dimension ) parameters , and we mainly consider the case when n is much larger than d ) . 4.Our construction suggests that if we want the width of the net to be smaller , it would be better to use deeper nets . 5.We don \u2019 t expect our construction to be used as an initialization of the training . Moreover , we don \u2019 t expect this construction is be close to what is found by the SGD . It serves mainly as theoretical evidence that the network is powerful in its residual parameterization . Regarding experiment : yes , the linear layer before softmax is not trained \u2014 this is motivated by our construction where the last layer is independent of the data , and it helps improves the result by roughly 1 % ."}, {"review_id": "ryxB0Rtxx-2", "review_text": "This paper investigates the identity parametrization also known as shortcuts where the output of each layer has the form h(x)+x instead of h(x). This has been shown to perform well in practice (eg. ResNet). The discussions and experiments in the paper are interesting. Here's a few comments on the paper: -Section 2: Studying the linear networks is interesting by itself. However, it is not clear that how this could translate to any insight about non-linear networks. For example, you have proved that every critical point is global minimum. I think it is helpful to add some discussion about the relationship between linear and non-linear networks. -Section 3: The construction is interesting but the expressive power of residual network is within a constant factor of general feedforward networks and I don't see why we need a different proof given all the results on finite sample expressivity of feedforward networks. I appreciate if you clarify this. -Section 4: I like the experiments. The choice of random projection on the top layer is brilliant. However, since you have combined this choice with all-convolutional residual networks, it is hard for the reader to separate the affect of each of them. Therefore, I suggest reporting the numbers for all-convolutional residual networks with learned top layer and also ResNet with random projection on the top layer. Minor comments: 1- I don't agree that Batch Normalization can be reduced to identity transformation and I don't know if bringing that in the abstract without proper discussion is a good idea. 2- Page 5 above assumption 3.1 : x^(i)=1 ==> ||x^(i)||_2=1 ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your valuable suggestions and comments that we address below . Section 2 : That \u2019 s certainly a valid concern for all results on linearized neural nets . However , we show that the norm of the weights for the linear network can be very small if the number of layers is deep . When the weights are small , we can hope that the non-linear case can be approximated by the linear version via Taylor expansion . However , there are technical difficulties to make this intuition precise and rigorous , and these are the major bottleneck of the theoretical understanding of deep learning . One concrete thing we can say about the benefit of re-parameterization in the non-linear case is this : With standard feed-forward networks , all zero weights form an obvious bad critical point . However , with the re-parameterization , such an obvious bad critical point no longer exists . This may be seen as mild evidence that our result may be true more generally . Adding this discussion is a good suggestion ! Section 3 : As far as we know , the proof for finite sample expressivity for feedforward nets requires the network to have a very large width ( the width needs to be larger than the number of examples ) . The benefit of ResNets is that it allows such construction to use a network with a small width . We think this is a more natural construction , which is also both more similar to the nets used in practice , and exemplifies the appealing properties of ResNets . Section 4 : Great suggestion . We will run all experiments with and without batch-norm to tease out the effect of random projections on the standard architecture ."}], "0": {"review_id": "ryxB0Rtxx-0", "review_text": "This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity. This paper is well written and studied a fundamental problem in deep neural network. I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks. One of key questions is how to extent the result in this paper to the more general nonlinear actuation function case. Minors: one line before Eq. (3.1), U \\in R ? \\times k ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the comments ! We will fix the typo . ( It should be U \\in R^ { k\\times k } )"}, "1": {"review_id": "ryxB0Rtxx-1", "review_text": "Paper Summary: Authors investigate identity re-parametrization in the linear and the non linear case. Detailed comments: \u2014 Linear Residual Network: The paper shows that for a linear residual network any critical point is a global optimum. This problem is non convex it is interesting that this simple re-parametrization leads to such a result. \u2014 Non linear Residual Network: Authors propose a construction that maps the points to their labels via a resnet , using an initial random projection, followed by a residual block that clusters the data based on their label, and a last layer that maps the clusters to the label. 1- In Eq 3.4 seems the dimensions are not matching q_j in R^k and e_j in R^r. please clarify 2- The construction seems fine, but what is special about the resnet here in this construction? One can do a similar construction if we did not have the identity? can you discuss this point? In the linear case it is clear from a spectral point of view how the identity is helping the optimization. Please provide some intuition. 3- Existence of a network in the residual class that overfits does it give us any intuition on why residual network outperform other architectures? What does an existence result of such a network tell us about its representation power ? A simple linear model under the assumption that points can not be too close can overfit the data, and get fast convergence rate (see for instance tsybakov noise condition). 4- What does the construction tell us about the number of layers? 5- clustering the activation independently from the label, is an old way to pretrain the network. One could use those centroids as weights for the next layer (this is also related to Nystrom approximation see for instance https://www.cse.ust.hk/~twinsen/nystrom.pdf ). Your clustering is very strongly connected to the label at each residual block. I don't think this is appealing or useful since no feature extraction is happening. Moreover the number of layers in this construction does not matter. Can you weaken the clustering to be independent to the label at least in the early layers? then one could you use your construction as an initialization in the training. \u2014 Experiments : - last layer is not trained means the layer before the linear layer preceding the softmax? Minor comments: Abstract: how the identity mapping motivated batch normalization? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your valuable suggestions and comments that we address below . 1.There is a typo in the equation 3.4 ( and at several related places ) . There shouldn \u2019 t be q_j in Eqn 3.4 . In other words , the last layer doesn \u2019 t use identity parameterization because we need to change the dimension to match the number of labels . We will update the paper very soon to correct these typos . 2.As far as we know , the proof for finite sample expressivity for feedforward nets requires the network to have a very large width ( the width needs to be larger than the number of examples ) . The benefit of ResNets is that it allows such construction to use a network with a small width . We think this is a more natural construction , which is also both more similar to the nets used in practice , and exemplifies the appealing properties of ResNets . 3.Indeed , finite sample expressivity result does show that representational power of the net is great , though it doesn \u2019 t imply overfitting . ( As a matter of fact , such nets don \u2019 t overfit ) . Moreover , as argued before , such expressivity result is not easily obtained for other architecture like feed-forward neural nets or linear model ( specifically , with a linear model , one can use effectively at most d ( =dimension ) parameters , and we mainly consider the case when n is much larger than d ) . 4.Our construction suggests that if we want the width of the net to be smaller , it would be better to use deeper nets . 5.We don \u2019 t expect our construction to be used as an initialization of the training . Moreover , we don \u2019 t expect this construction is be close to what is found by the SGD . It serves mainly as theoretical evidence that the network is powerful in its residual parameterization . Regarding experiment : yes , the linear layer before softmax is not trained \u2014 this is motivated by our construction where the last layer is independent of the data , and it helps improves the result by roughly 1 % ."}, "2": {"review_id": "ryxB0Rtxx-2", "review_text": "This paper investigates the identity parametrization also known as shortcuts where the output of each layer has the form h(x)+x instead of h(x). This has been shown to perform well in practice (eg. ResNet). The discussions and experiments in the paper are interesting. Here's a few comments on the paper: -Section 2: Studying the linear networks is interesting by itself. However, it is not clear that how this could translate to any insight about non-linear networks. For example, you have proved that every critical point is global minimum. I think it is helpful to add some discussion about the relationship between linear and non-linear networks. -Section 3: The construction is interesting but the expressive power of residual network is within a constant factor of general feedforward networks and I don't see why we need a different proof given all the results on finite sample expressivity of feedforward networks. I appreciate if you clarify this. -Section 4: I like the experiments. The choice of random projection on the top layer is brilliant. However, since you have combined this choice with all-convolutional residual networks, it is hard for the reader to separate the affect of each of them. Therefore, I suggest reporting the numbers for all-convolutional residual networks with learned top layer and also ResNet with random projection on the top layer. Minor comments: 1- I don't agree that Batch Normalization can be reduced to identity transformation and I don't know if bringing that in the abstract without proper discussion is a good idea. 2- Page 5 above assumption 3.1 : x^(i)=1 ==> ||x^(i)||_2=1 ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your valuable suggestions and comments that we address below . Section 2 : That \u2019 s certainly a valid concern for all results on linearized neural nets . However , we show that the norm of the weights for the linear network can be very small if the number of layers is deep . When the weights are small , we can hope that the non-linear case can be approximated by the linear version via Taylor expansion . However , there are technical difficulties to make this intuition precise and rigorous , and these are the major bottleneck of the theoretical understanding of deep learning . One concrete thing we can say about the benefit of re-parameterization in the non-linear case is this : With standard feed-forward networks , all zero weights form an obvious bad critical point . However , with the re-parameterization , such an obvious bad critical point no longer exists . This may be seen as mild evidence that our result may be true more generally . Adding this discussion is a good suggestion ! Section 3 : As far as we know , the proof for finite sample expressivity for feedforward nets requires the network to have a very large width ( the width needs to be larger than the number of examples ) . The benefit of ResNets is that it allows such construction to use a network with a small width . We think this is a more natural construction , which is also both more similar to the nets used in practice , and exemplifies the appealing properties of ResNets . Section 4 : Great suggestion . We will run all experiments with and without batch-norm to tease out the effect of random projections on the standard architecture ."}}