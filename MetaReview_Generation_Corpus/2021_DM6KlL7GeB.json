{"year": "2021", "forum": "DM6KlL7GeB", "title": "Semi-Relaxed Quantization with DropBits: Training Low-Bit Neural Networks via Bitwise Regularization", "decision": "Reject", "meta_review": "# Summary\nThe paper was initially well received by reviewers, remarking the new gradient estimator, a new dropbits technique and an interesting observations of better performance when the bitwidth is learned. The experimental results also look promising: showing improved training performance and test performance (including on ImageNet with ResNet-18), properties to reduce quantization error of learned weights, possibility to learn number of bits via learning stochastic bit-dropping masks.\n\nA deeper verification of the specific methods proposed however showed principal issues:\n- The methods proposed in the paper are not sufficiently justified by verifiable formal arguments. The proposed intuitive explanations are entangled and actually lead to wrong conclusions. In particular a main claim of the paper that the proposed estimator reduces bias and variance of Gumbel-Softmax estimator was shown wrong and was removed in the revision. The remaining claim that the estimator reduces quantization error is also wrong (see below). With these issues, the gradient part of the paper is largely incorrect, which is in a strong discrepancy with good experimental results.\n\n- Other parts of the paper, comprising the remaining technical contributions are not properly positioned with respect to the SOTA and thus are not necessary novel / improving.\n\nThe main technical issues were discussed with all reviewers and were either supported or not objected. Therefore, I am confident that the submission has critical problems and must be rejected. I recommend the authors to thoroughly investigate all the raised issues (by all reviewers) before resubmitting to other venues. \n\n# Details\n\n## Gradient\n\nThe overclaim of reducing bias and variance / resolving bias/variance tradeoff has been removed in the revision, but the new gradient estimator remains a central innovation proposed. It is however not justified and cannot indeed be regarded as a good estimator:\n\n* The justification argues about the bias of the Gumbel-Softmax sampling distribution, but the proposed estimator does not use a sampling distribution in the forward pass, and thus by design cannot address this problem.\n\n* The backward pass to use gradient in i_max only (Eq. 3) is not based on any justification at all. \n\n* The remaining claimed good property: \"to reduce the quantization error\" is, according to the definition in sect. 3.4, not a property of a gradient estimator, but of the stochastic relaxation alone. There is an experimental evidence Fig.2 that the estimator _leads_ to lowering the quantization error. This is however in a contradiction with a direct verification of the proposed estimator that was conducted:\nThe verification inspects gradient in a single variable $x$ and a linear loss function of the quantized variable $\\hat x$.  It shows that the gradient is zero at grid points and discontinuously reverses the direction at half-grid points. Because of such zigzagging, *it does not correspond to minimizing the loss function*, i.e. not a reasonable estimator. The grid points, where the gradient vanishes, may correspond to either local minima or to local maxima of the estimator. Which of the two cases occurs depends exclusively on the sign of the incoming gradient from the loss function. For $L(\\hat x) = \\hat x$ we observe that the negative gradient points towards nearest grid point, but for $L(\\hat x) = -\\hat x$ it points away from the nearest grid point, i.e. a step would *increase the quantization error*. The implementation of this verification is attached anonymously:\nhttps://colab.research.google.com/drive/1PibzRMXQ-NVZMUdfgTIK0Q5FxUKyxfqI?usp=sharing\n\n* Alternative existing estimators are not sufficiently discussed: e.g. common deterministic STE, as used in quantization papers: to just treat the quantization operation as identity on the backward pass. Estimator used by Shayer et al. (2018),  Ajanthan et al. 2019 \u201cMirror descent view for neural network quantization\u201d, Unbiased estimators (e.g. Yin et al. 2019 \u201cARSM: Augment-REINFORCE-Swap-Merge Estimator for Gradient Backpropagation Through Categorical Variables\u201d). While unbiased estimators may still have too high variance and or be too computationally demanding for deep networks, they can be used for verification purposes. \n\n* The claim that it is not possible to apply unbiased estimators, in particular score function estimator, because of dependency on x is incorrect. See e.g. Schulman et al. 2015 \u201cGradient Estimation Using Stochastic Computation Graphs\u201d. Many works on advanced unbiased estimators also demonstrate experiments with 2 or more layers of hidden discrete stochastic variables. From this and technical discussion with authors, it is seen that the experimental study is Sec 3.4 is very limited and erroneous. \n\n* The rule by which the probability mass of the dropped bits is uniformly spread over the remaining bits is not justified and appears methodologically incorrect. In Fig.4 it is not clear what bits were dropped and why the mass at $-2\\alpha$ has decreased.\n\n## Gradient and Other Techniques relative to SOTA\n\n* The bias problem of GS estimator, detailed in Fig.1. is not novel to me, it is in fact known that the mean under the concrete distribution (of linear or non-linear objective) differs from the mean under the categorical distribution, see e.g.\n\nLorberbom et al. (2018) Direct Optimization through argmax for Discrete Variational Auto-Encoder (Fig.1)\n\nAndriyash et al. (2018) Improved Gradient-Based Optimization Over Discrete Distributions\n\nThus analysis of individual samples in Fig.1 appears unnecessary detailed. The issue that the relaxed distribution of Gumbel-Softmax may cause a large estimation error for gradients downstream is already discussed by Louizos et al. (2018) and other works, e.g. \n\nChoi 2017, \"Unsupervised Learning of Task-Specific Tree Structures with Tree-LSTMs\" Sec 3.2\n\nand Andriyash (2018). This later problem was previously addressed in many cases by the ST Gumbel-Softmax heuristic. This heuristic indeed performs better in CIFAR-10 experiments in the submission / Louizos (2018), which is likely to be a better tuned and more controlled experiment than ImageNet.\n* More methods should be discussed that reduce the quantization error during learning. E.g.\n\nCong et al. (2018): \u201cExtremely low bit neural network: Squeeze the last bit out with ADMM\u201d, \n\nwho include terms explicitly minimizing the \nquantization error. In fact most works quantizing network weights primarily focus on reducing the quantization error, e.g.\n\nNagel et al. (2019)\" Data-Free Quantization Through Weight Equalization and Bias Correction\n\n* The prior works on learning bit width should be more extensively discussed / compared to, especially if this part becomes central to the submission. E.g.\n\nBaalen et al. (2020) \u201cBayesian Bits: Unifying Quantization and Pruning\u201d (or references therein if this is considered contemporaneous).  \n\nCourbariaux & David (2015): Training deep neural networks with low precision multiplications\n\n* The new hypothesis for quantization is in fact similar to the effect observed elsewhere that quantizing neural networks progressively leads to better results.  E.g. \n\nZhou et al. (2017) Incremental Network Quantization- Towards Lossless CNNs with Low-Precision Weights.\n\nIt is questionable whether the link to the lottery ticket hypothesis is justified, since the latter shows quite the opposite, as reviewers have pointed.", "reviews": [{"review_id": "DM6KlL7GeB-0", "review_text": "This paper deals with network quantization . It proposes Semi-Relaxed Quantization ( SRQ ) that uses a multi-class straight-through estimator to effectively reduce the bias and variance , along with a new regularization technique , DropBits that replaces dropout regularization to randomly drop the bits . Extensive experiments are conducted to validate our method on various benchmark datasets and network architectures . Pros : - Reducing variance of gradient estimator in Gumbel of RQ using multi-class STE . - A novel dropbits technique to reduce bias in gradient estimator of SRQ , as well as supporting the mixed-precision scheme . - A `` quantized winning tickets '' is introduced to train probs of binary masks so that learning proper bitwise for each layer . Cons : - The SRQ process seems not to be novel enough . Actually here pi is just a multi-class sigmoid-output , and y is derived directly by argmax ( prob_pi ) in forward process ( and calculate gradients using STE . ) . This is quite similar to existing quantization methods using softmax + STE . [ 1 ] - Only 3/4 bit results of Res18/MobileNetV2 showed in ImageNet . I 'd appreciate it if authors could offer more quantitive analysis on more architectures and tasks . - More comparisons on latency/energy/flops during the training and evaluation process should be provided to validate the SRQ + DropBits . [ 1 ] Hardware-aware Softmax Approximation for Deep Neural Networks . Xue Geng et al . * * * After rebuttal and discussion This paper proposes a new network quantization framework . In particular , the proposed DropBits is somewhat novel . However , it lacks sufficient and accurate analysis of SRQ+DropBits . For example , why SRQ can reduce quantization error has not been well motivated and explained . The definition of distribution bias is still unclear . I think that an accurate description of terminology is crucial and required for scientific research . Hence , the paper still needs minor polishing for publishing . I would like to decrease my rating to 5 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer 3 , Thank you for your constructive feedback . [ Q1 : The SRQ process seems not to be novel enough . This is quite similar to existing quantization methods using softmax + STE . [ 1 ] ] Response : Thank you for letting us know [ 1 ] . We added it to the reference in the revision . However , our SRQ is quite different from [ 1 ] . The main purpose of [ 1 ] is to design a cost-efficient \u201c softmax layer \u201d for inference . Toward this , [ 1 ] introduces a hardware-aware softmax layer , which is achieved by reducing the operand bit-width and approximating the operations in softmax ( such as exponential and division ) via a look-up table . On the other hand , our work focus on network quantization of * * all * * layers in extremely low-bitwidth with the novel multi-class STE . Also , our experiments consider significantly lower bit-width than those of [ 1 ] . [ Q2 : I 'd appreciate it if authors could offer more quantitative analysis on more architectures and tasks . ] Response : We agree with the reviewer \u2019 s opinion . However , particularly because our model as well as all major baselines such as RQ take much longer than the regular training process , it is quite difficult to show results beyond the experiments generally conducted in the existing literature . For large scale ImageNet experiments , the majority of papers in this field provided the result of * either * ResNet ( deeper than ResNet-18 ) or MobileNet . In addition to the result of MobileNet V2 we already provided in the original submission , now we are trying to quantize ResNet-34 . But , due to the time constraint of the rebuttal period , we will add this result in the final revision . [ Q3 : More comparisons on latency/energy/flops during the training and evaluation process should be provided to validate the SRQ + DropBits . ] Response : Note that counting FLOPs to compare two algorithms generally known to be meaningful * only * when both algorithms employ exactly the same operations . The * learning * process of RQ and SRQ not only consists of different sets of operations , but also includes several PyTorch built-in functions beyond the matrix/vector multiply-add operations , which can further mislead us . Instead , we compare the training time per epoch . For training ResNet-18 on ImageNet in 3-bit with a batch size of 128 , the training time of SRQ + DropBits is around 90 minutes per epoch , which is 15 % longer than that of RQ , about 76 minutes . Meanwhile , in the context of network quantization , latency/energy/flops at inference are much more important than those during the training phase since we use deployed models . In this sense , the inference procedure of SRQ + DropBits is the same as that of other uniform quantization methods , so are latency/energy/flops at inference ."}, {"review_id": "DM6KlL7GeB-1", "review_text": "# # # Summary This work presents 1 ) Semi-Relaxed Quantization ( SRQ ) , a method that targets learning low-bit neural networks , 2 ) DropBits , a method that performs dropout-like regularization on the bit width of the quantizers with an option to also automatically optimise the bit-width per layer according to the data , and 3 ) quantised lottery ticket hypothesis . SRQ is an extension of Relaxed Quantization ( RQ ) , which is prior work , in two ways ; firstly the authors replace the sampling from the concrete relaxation during training to deterministically selecting the mode ( which is non-differentiable ) and , secondly , they propose a specific straight-through gradient estimator ( STE ) than only propagates the gradient backwards for the elements that were selected in the forward pass . DropBits is motivated from the perspective of reducing the bias of the STE gradient estimator by randomly dropping grid points associated with a specific bit-width and then renormalising the SRQ distribution over the grid . This essentially induces stochasticity in the sampling distribution for the quantised value ( which was removed before by selecting the mode in SRQ ) . The authors further extend DropBits in a way that allows for learning the drop probabilities for each bit-width , thus allowing for learning mixed-precision networks . Finally the authors postulate the quantised lottery ticket hypothesis , which refers to that \u201c one can find the learned bit-width network which can perform better than the network with the same but fixed bit-widths from scratch \u201d . # # # Pros - This work provides a set of additions that improves upon prior work - The DropBits method is novel and allows for learning the bit-width in a straightforward manner - The results improve upon recent works that learn quantised neural networks # # # Cons - Some claims from the authors are misleading while others are not precise - The computational complexity of the method is not discussed - Some experimental settings might not be consistent with some of the baselines # # # Detailed feedback This work tackles the problem of learning quantized neural networks and the authors show empirically that their proposed method achieves good results . The DropBits extension is particularly interesting in that it allows for learning the appropriate bit-width of a given tensor via traditional pruning approaches . I also like the fact that the authors explain illustratively their proposed approach via several figures , which provide a nice boost to clarity . Nevertheless , I believe that there are still some important aspects that need to be addressed before I recommend acceptance for this work . First of all , the comparison against the prior work on figure 1 is misleading ; the authors compare the * entire categorical distribution * ( i.e. , the one obtained after discretising the logistic onto the quantization grid ) of SRQ at Fig 1 ( b ) with a * single sample * from the concrete relaxation of the same distribution at Fig.1 ( a ) right for RQ . In fact , the underlying categorical distribution will be the same for both SRQ and RQ in the specific example of figure 1 . Furthermore , it is worthwhile to notice that the underlying categorical distribution ( i.e. , pre relaxation ) does have support for the value of -a ( as the p ( g_i = -a ) is nonzero at Fig.1 ( b ) ) , thus it is not unreasonable that there are specific samples which lead to the quantised value being -a , thus incurring larger quantization loss . Furthermore , I believe that the discussion about SRQ misses some important points that would improve the clarity of the work if they are addressed . Selecting the most probable point in the categorical distribution for the forward pass is equivalent to rounding to the nearest grid point , which can be done much more efficiently than computing the entire categorical over the grid and then taking the argmax . In addition , this is also the same as taking sigma - > 0 for the logistic distribution that is to be discretized in the forward pass . Finally the authors argue that their novel multi-class STE reduces the variance of the gradient estimator but no formal justification is given apart from some hand-wavy arguments . Why is the variance lower if the gradient only flows through to r_imax ? Furthermore , for the second point with respect to the benefits of the multi-class STE ; while it does seem desirable that it aggressively clusters the weights and activations around the grid-points , I wonder how much can that hinder convergence . Do you ever observe that the weights can be prematurely \u201c stuck \u201d ( and thus lead to a bad local minimum ) and do the weights ever move further away than just the closest grid point ? DropBits could potentially help with the latter part , but it would be interesting to see what happens without it . DropBits in my opinion is the main novel idea of this work , and it is an interesting way to learn the bit-precision of each tensor in the network . I have two main points for this section in general and DropBits in particular . The main motivation behind DropBits seems to be converting the sampling distribution of SRQ ( which is deterministic ) to a stochastic one . If this is the case , then why have it be deterministic in the first place ? You could just sample from the original categorical distribution ( by , e.g. , using the Gumbel-Softmax STE which gives samples exactly on the grid ) in the forward pass and use your multi-class STE approximation in the backward pass . It would be interesting to see how that fares with SRQ + DropBits and would highlight whether the main benefit of DropBits was the regularization aspect ( and not that of improving the sampling distribution ) . As for DropBits in particular ; it seems that you drop bits independently with each of the gates z_1 , z_2 , \u2026 ( i.e. , figure 3 ) . If this is the case , then you could end up with a non-uniform grid that can not be exactly represented as a fixed point tensor ( e.g. , on figure 3b you could have z_1 = 0 and z_2 = 1 ) . If this is the case , then comparison against other approaches that use uniform quantization is not apples-to-apples . Finally , a couple of other things that I believe should be addressed ; the authors don \u2019 t make any discussions about the computational and memory complexity of the resulting algorithm . It seems that for every individual weight and activation they first have to construct the categorical distribution over the entire grid ( which can quickly become very large , e.g. , for 8 bits there are 256 categories ) , in order to take the weighted sum . This doesn \u2019 t seem to scale very well . How expensive is something like this in practice and how long do experiments take on , e.g. , Imagenet ? Furthermore , the quantised lottery ticket hypothesis ( QLTH ) is a bit peculiar . The original lottery ticket hypothesis ( LTH ) was about finding sparse networks at initialisation that can be trained from scratch and achieve the same accuracy as the original dense equivalents . This is different than what the authors articulate here , specifically that it is about finding sparse networks that are easier to train compared to sparse networks obtained from pruning . As a result , their QLTH seems to state the opposite than what the original LTH was about ; it states that a QLT is obtained when you manage to find a network X that a . ) has smaller bit width than the original network and b . ) has better performance than a network initialised to the bit-width of X and trained to convergence . Following the arguments of the LTH , I would expect a QLT to be obtained when you can quantise a neural network to a specific bit-width at initialisation and when you train from scratch that particular quantised network , you obtain the same performance as the full precision equivalent . I would thus encourage the authors to clarify this point and better align with the original LTH . Based on the aforementioned points , I can not at the moment recommend acceptance for this work . Nevertheless , as I believe DropBits is an interesting idea , I would encourage the authors to put in the effort and rework the paper by addressing these points over the rebuttal .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer 1 , Thank you for your constructive feedback . [ Q1 : The comparison against the prior work on figure 1 is misleading . ] Response : As the reviewer astutely pointed out , we only consider a single sample in Figure 1 in the original submission . In the revision , we exhibit more cases for RQ so as not to be misunderstood . Also as mentioned by the reviewer , since the underlying categorical distribution has support for $ -\\alpha $ and $ -2\\alpha $ , it appears reasonable that there are specific samples which lead to the quantized value being below zero as shown in Figure 1- ( b , d , e ) in the revision . It may be okay * on average * , but RQ computes * only one * sample in each forward pass due to computational burden , which can accidentally lead to very large quantization error for this particular sample . On the other hand , SRQ deliberately performs biased estimation on the underlying categorical distribution to prevent large quantization error from even occurring in the forward pass while sharing this underlying distribution with RQ in the backward pass . Instead , we devised DropBits to reduce the incurred bias of SRQ . For more details , see new Section 3.4 in the revision . - [ Q2 : Selecting the most probable point in the categorical distribution for the forward pass is equivalent to rounding to the nearest grid point . ] Response : As the reviewer mentioned , the forward pass of SRQ is equivalent to that of rounding to the nearest grid point , but the backward pass of SRQ is totally different from that of rounding . As mentioned in the last paragraph of Section 3.2 , the backward pass of SRQ enables weights and activations to be cohesively clustered around grid points , whereas there is no guarantee that rounding to the nearest grid point could cluster them around grid points . Such a difference in the backward pass makes SRQ perform well . As AC and Reviewer 1 mentioned , the forward pass of SRQ is identical to a deterministic quantization like rounding to the nearest bin . It may seem a bit complicated for a forward pass , but expressing the nearest grid selection in this equivalent form ( the combination of logistic and argmax ) is essential to derive our backward pass . As a result , the backward pass of SRQ is totally different from that of other deterministic quantization methods . As mentioned in the last paragraph of Section 3.2 , the backward pass of SRQ enables weights and activations to be cohesively clustered around grid points , whereas there is no guarantee that other deterministic quantization methods could cluster them around grid points . In order to improve clarity by giving high-level ideas for forward and backward passes of our model in advance , we slightly changed the opening of Section 3 as \u201c propose Semi-Relaxed Quantization , which selects the nearest grid point in the forward pass to decrease the quantization error . To make it learnable and to cluster compressed parameters cohesively , SRQ expresses the nearest grid selection of the forward pass as the equivalent form , the combination of logistic distribution and argmax , and performs the backward pass on it. \u201d -- [ Q3 : In addition , this is also the same as taking sigma - > 0 for the logistic distribution that is to be discretized in the forward pass . ] Response : As the reviewer mentioned , the result of taking $ \\sigma = 0 $ is the same as that of taking argmax . However , exact zero $ \\sigma $ makes the network untrainable . To be concrete , the probability distribution of $ \\widetilde { x } $ for $ \\sigma = 0 $ becomes the Dirac delta function as $ p ( \\tilde { x } ) = \\delta ( x ) $ whose CDF is just the step function . Then , the gradient $ { \\partial \\pi_ { i_ { max } } \\over \\partial x } $ becomes exactly zero almost everywhere , so is the gradient $ { \\partial \\mathcal { L } \\over \\partial x } $ . Thus , the statement is absolutely correct but the network parameters would never be updated via gradient-based optimization . [ Q4 : Why is the variance lower if the gradient only flows through to r_imax ? ] Response : As we mentioned in our response to AC , the variance of gradient estimator using our multi-class STE is exactly zero due to the fact that there is no randomness in SRQ . Hence , the variance of our gradient estimator is trivially lower ( actually , exactly zero ) than that of RQ based on sampling from Gumbel distribution . We explained it in more detail in the second last paragraph of Section 3.2 in the revision . --"}, {"review_id": "DM6KlL7GeB-2", "review_text": "This paper proposed a novel network quantization method to reduce the bit-lengths of the network weights and activations , which is one of the most important problem for resource-limited devices . The presentation of this paper is well written and organized . The problem definition is clear , i.e. , relaxed quantization with the Gumbel-Softmax relaxation suffers from bias-variance trade-off depending on the temperature parameter of Gumbel-Softmax . The proposed method to solve this problem is reasonable . The experimental results are also convincing . Minor comments : 1 . In Table 3 , `` MNIST '' should be LeNet-5 . 2.Section 3.4 is a little dense . 3.I understand that there is little space , but embedding formulas in sentences is difficult to read . 4.I recommend you to show the result only using SQR for ImageNet in Table 2 as in MNIST and CIFER10 in Table 1 .", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 4 , Thank you for your constructive feedback . We are sorry for the inconvenience with typo and small embedding formulas . We corrected them in the revision . As your suggestion , we are conducting experiments on ImageNet by only using SRQ in 3-bit . When quantizing ResNet-18 to 3-bit by only using SRQ , top-1 error is 37.45 % and top-5 error is 15.47 % after performing fine-tuning throughout 110 epochs with a batch size of 128 . Although SRQ in 3-bit already outperforms RQ in 4-bit , it seems that its performance could be enhanced as the training progresses further . Due to the time constraint of the rebuttal period and the fact that more epochs are required to train SRQ than SRQ + DropBits , we unfortunately have not yet completed this experiment , but we will add it in the final revision ."}], "0": {"review_id": "DM6KlL7GeB-0", "review_text": "This paper deals with network quantization . It proposes Semi-Relaxed Quantization ( SRQ ) that uses a multi-class straight-through estimator to effectively reduce the bias and variance , along with a new regularization technique , DropBits that replaces dropout regularization to randomly drop the bits . Extensive experiments are conducted to validate our method on various benchmark datasets and network architectures . Pros : - Reducing variance of gradient estimator in Gumbel of RQ using multi-class STE . - A novel dropbits technique to reduce bias in gradient estimator of SRQ , as well as supporting the mixed-precision scheme . - A `` quantized winning tickets '' is introduced to train probs of binary masks so that learning proper bitwise for each layer . Cons : - The SRQ process seems not to be novel enough . Actually here pi is just a multi-class sigmoid-output , and y is derived directly by argmax ( prob_pi ) in forward process ( and calculate gradients using STE . ) . This is quite similar to existing quantization methods using softmax + STE . [ 1 ] - Only 3/4 bit results of Res18/MobileNetV2 showed in ImageNet . I 'd appreciate it if authors could offer more quantitive analysis on more architectures and tasks . - More comparisons on latency/energy/flops during the training and evaluation process should be provided to validate the SRQ + DropBits . [ 1 ] Hardware-aware Softmax Approximation for Deep Neural Networks . Xue Geng et al . * * * After rebuttal and discussion This paper proposes a new network quantization framework . In particular , the proposed DropBits is somewhat novel . However , it lacks sufficient and accurate analysis of SRQ+DropBits . For example , why SRQ can reduce quantization error has not been well motivated and explained . The definition of distribution bias is still unclear . I think that an accurate description of terminology is crucial and required for scientific research . Hence , the paper still needs minor polishing for publishing . I would like to decrease my rating to 5 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer 3 , Thank you for your constructive feedback . [ Q1 : The SRQ process seems not to be novel enough . This is quite similar to existing quantization methods using softmax + STE . [ 1 ] ] Response : Thank you for letting us know [ 1 ] . We added it to the reference in the revision . However , our SRQ is quite different from [ 1 ] . The main purpose of [ 1 ] is to design a cost-efficient \u201c softmax layer \u201d for inference . Toward this , [ 1 ] introduces a hardware-aware softmax layer , which is achieved by reducing the operand bit-width and approximating the operations in softmax ( such as exponential and division ) via a look-up table . On the other hand , our work focus on network quantization of * * all * * layers in extremely low-bitwidth with the novel multi-class STE . Also , our experiments consider significantly lower bit-width than those of [ 1 ] . [ Q2 : I 'd appreciate it if authors could offer more quantitative analysis on more architectures and tasks . ] Response : We agree with the reviewer \u2019 s opinion . However , particularly because our model as well as all major baselines such as RQ take much longer than the regular training process , it is quite difficult to show results beyond the experiments generally conducted in the existing literature . For large scale ImageNet experiments , the majority of papers in this field provided the result of * either * ResNet ( deeper than ResNet-18 ) or MobileNet . In addition to the result of MobileNet V2 we already provided in the original submission , now we are trying to quantize ResNet-34 . But , due to the time constraint of the rebuttal period , we will add this result in the final revision . [ Q3 : More comparisons on latency/energy/flops during the training and evaluation process should be provided to validate the SRQ + DropBits . ] Response : Note that counting FLOPs to compare two algorithms generally known to be meaningful * only * when both algorithms employ exactly the same operations . The * learning * process of RQ and SRQ not only consists of different sets of operations , but also includes several PyTorch built-in functions beyond the matrix/vector multiply-add operations , which can further mislead us . Instead , we compare the training time per epoch . For training ResNet-18 on ImageNet in 3-bit with a batch size of 128 , the training time of SRQ + DropBits is around 90 minutes per epoch , which is 15 % longer than that of RQ , about 76 minutes . Meanwhile , in the context of network quantization , latency/energy/flops at inference are much more important than those during the training phase since we use deployed models . In this sense , the inference procedure of SRQ + DropBits is the same as that of other uniform quantization methods , so are latency/energy/flops at inference ."}, "1": {"review_id": "DM6KlL7GeB-1", "review_text": "# # # Summary This work presents 1 ) Semi-Relaxed Quantization ( SRQ ) , a method that targets learning low-bit neural networks , 2 ) DropBits , a method that performs dropout-like regularization on the bit width of the quantizers with an option to also automatically optimise the bit-width per layer according to the data , and 3 ) quantised lottery ticket hypothesis . SRQ is an extension of Relaxed Quantization ( RQ ) , which is prior work , in two ways ; firstly the authors replace the sampling from the concrete relaxation during training to deterministically selecting the mode ( which is non-differentiable ) and , secondly , they propose a specific straight-through gradient estimator ( STE ) than only propagates the gradient backwards for the elements that were selected in the forward pass . DropBits is motivated from the perspective of reducing the bias of the STE gradient estimator by randomly dropping grid points associated with a specific bit-width and then renormalising the SRQ distribution over the grid . This essentially induces stochasticity in the sampling distribution for the quantised value ( which was removed before by selecting the mode in SRQ ) . The authors further extend DropBits in a way that allows for learning the drop probabilities for each bit-width , thus allowing for learning mixed-precision networks . Finally the authors postulate the quantised lottery ticket hypothesis , which refers to that \u201c one can find the learned bit-width network which can perform better than the network with the same but fixed bit-widths from scratch \u201d . # # # Pros - This work provides a set of additions that improves upon prior work - The DropBits method is novel and allows for learning the bit-width in a straightforward manner - The results improve upon recent works that learn quantised neural networks # # # Cons - Some claims from the authors are misleading while others are not precise - The computational complexity of the method is not discussed - Some experimental settings might not be consistent with some of the baselines # # # Detailed feedback This work tackles the problem of learning quantized neural networks and the authors show empirically that their proposed method achieves good results . The DropBits extension is particularly interesting in that it allows for learning the appropriate bit-width of a given tensor via traditional pruning approaches . I also like the fact that the authors explain illustratively their proposed approach via several figures , which provide a nice boost to clarity . Nevertheless , I believe that there are still some important aspects that need to be addressed before I recommend acceptance for this work . First of all , the comparison against the prior work on figure 1 is misleading ; the authors compare the * entire categorical distribution * ( i.e. , the one obtained after discretising the logistic onto the quantization grid ) of SRQ at Fig 1 ( b ) with a * single sample * from the concrete relaxation of the same distribution at Fig.1 ( a ) right for RQ . In fact , the underlying categorical distribution will be the same for both SRQ and RQ in the specific example of figure 1 . Furthermore , it is worthwhile to notice that the underlying categorical distribution ( i.e. , pre relaxation ) does have support for the value of -a ( as the p ( g_i = -a ) is nonzero at Fig.1 ( b ) ) , thus it is not unreasonable that there are specific samples which lead to the quantised value being -a , thus incurring larger quantization loss . Furthermore , I believe that the discussion about SRQ misses some important points that would improve the clarity of the work if they are addressed . Selecting the most probable point in the categorical distribution for the forward pass is equivalent to rounding to the nearest grid point , which can be done much more efficiently than computing the entire categorical over the grid and then taking the argmax . In addition , this is also the same as taking sigma - > 0 for the logistic distribution that is to be discretized in the forward pass . Finally the authors argue that their novel multi-class STE reduces the variance of the gradient estimator but no formal justification is given apart from some hand-wavy arguments . Why is the variance lower if the gradient only flows through to r_imax ? Furthermore , for the second point with respect to the benefits of the multi-class STE ; while it does seem desirable that it aggressively clusters the weights and activations around the grid-points , I wonder how much can that hinder convergence . Do you ever observe that the weights can be prematurely \u201c stuck \u201d ( and thus lead to a bad local minimum ) and do the weights ever move further away than just the closest grid point ? DropBits could potentially help with the latter part , but it would be interesting to see what happens without it . DropBits in my opinion is the main novel idea of this work , and it is an interesting way to learn the bit-precision of each tensor in the network . I have two main points for this section in general and DropBits in particular . The main motivation behind DropBits seems to be converting the sampling distribution of SRQ ( which is deterministic ) to a stochastic one . If this is the case , then why have it be deterministic in the first place ? You could just sample from the original categorical distribution ( by , e.g. , using the Gumbel-Softmax STE which gives samples exactly on the grid ) in the forward pass and use your multi-class STE approximation in the backward pass . It would be interesting to see how that fares with SRQ + DropBits and would highlight whether the main benefit of DropBits was the regularization aspect ( and not that of improving the sampling distribution ) . As for DropBits in particular ; it seems that you drop bits independently with each of the gates z_1 , z_2 , \u2026 ( i.e. , figure 3 ) . If this is the case , then you could end up with a non-uniform grid that can not be exactly represented as a fixed point tensor ( e.g. , on figure 3b you could have z_1 = 0 and z_2 = 1 ) . If this is the case , then comparison against other approaches that use uniform quantization is not apples-to-apples . Finally , a couple of other things that I believe should be addressed ; the authors don \u2019 t make any discussions about the computational and memory complexity of the resulting algorithm . It seems that for every individual weight and activation they first have to construct the categorical distribution over the entire grid ( which can quickly become very large , e.g. , for 8 bits there are 256 categories ) , in order to take the weighted sum . This doesn \u2019 t seem to scale very well . How expensive is something like this in practice and how long do experiments take on , e.g. , Imagenet ? Furthermore , the quantised lottery ticket hypothesis ( QLTH ) is a bit peculiar . The original lottery ticket hypothesis ( LTH ) was about finding sparse networks at initialisation that can be trained from scratch and achieve the same accuracy as the original dense equivalents . This is different than what the authors articulate here , specifically that it is about finding sparse networks that are easier to train compared to sparse networks obtained from pruning . As a result , their QLTH seems to state the opposite than what the original LTH was about ; it states that a QLT is obtained when you manage to find a network X that a . ) has smaller bit width than the original network and b . ) has better performance than a network initialised to the bit-width of X and trained to convergence . Following the arguments of the LTH , I would expect a QLT to be obtained when you can quantise a neural network to a specific bit-width at initialisation and when you train from scratch that particular quantised network , you obtain the same performance as the full precision equivalent . I would thus encourage the authors to clarify this point and better align with the original LTH . Based on the aforementioned points , I can not at the moment recommend acceptance for this work . Nevertheless , as I believe DropBits is an interesting idea , I would encourage the authors to put in the effort and rework the paper by addressing these points over the rebuttal .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Reviewer 1 , Thank you for your constructive feedback . [ Q1 : The comparison against the prior work on figure 1 is misleading . ] Response : As the reviewer astutely pointed out , we only consider a single sample in Figure 1 in the original submission . In the revision , we exhibit more cases for RQ so as not to be misunderstood . Also as mentioned by the reviewer , since the underlying categorical distribution has support for $ -\\alpha $ and $ -2\\alpha $ , it appears reasonable that there are specific samples which lead to the quantized value being below zero as shown in Figure 1- ( b , d , e ) in the revision . It may be okay * on average * , but RQ computes * only one * sample in each forward pass due to computational burden , which can accidentally lead to very large quantization error for this particular sample . On the other hand , SRQ deliberately performs biased estimation on the underlying categorical distribution to prevent large quantization error from even occurring in the forward pass while sharing this underlying distribution with RQ in the backward pass . Instead , we devised DropBits to reduce the incurred bias of SRQ . For more details , see new Section 3.4 in the revision . - [ Q2 : Selecting the most probable point in the categorical distribution for the forward pass is equivalent to rounding to the nearest grid point . ] Response : As the reviewer mentioned , the forward pass of SRQ is equivalent to that of rounding to the nearest grid point , but the backward pass of SRQ is totally different from that of rounding . As mentioned in the last paragraph of Section 3.2 , the backward pass of SRQ enables weights and activations to be cohesively clustered around grid points , whereas there is no guarantee that rounding to the nearest grid point could cluster them around grid points . Such a difference in the backward pass makes SRQ perform well . As AC and Reviewer 1 mentioned , the forward pass of SRQ is identical to a deterministic quantization like rounding to the nearest bin . It may seem a bit complicated for a forward pass , but expressing the nearest grid selection in this equivalent form ( the combination of logistic and argmax ) is essential to derive our backward pass . As a result , the backward pass of SRQ is totally different from that of other deterministic quantization methods . As mentioned in the last paragraph of Section 3.2 , the backward pass of SRQ enables weights and activations to be cohesively clustered around grid points , whereas there is no guarantee that other deterministic quantization methods could cluster them around grid points . In order to improve clarity by giving high-level ideas for forward and backward passes of our model in advance , we slightly changed the opening of Section 3 as \u201c propose Semi-Relaxed Quantization , which selects the nearest grid point in the forward pass to decrease the quantization error . To make it learnable and to cluster compressed parameters cohesively , SRQ expresses the nearest grid selection of the forward pass as the equivalent form , the combination of logistic distribution and argmax , and performs the backward pass on it. \u201d -- [ Q3 : In addition , this is also the same as taking sigma - > 0 for the logistic distribution that is to be discretized in the forward pass . ] Response : As the reviewer mentioned , the result of taking $ \\sigma = 0 $ is the same as that of taking argmax . However , exact zero $ \\sigma $ makes the network untrainable . To be concrete , the probability distribution of $ \\widetilde { x } $ for $ \\sigma = 0 $ becomes the Dirac delta function as $ p ( \\tilde { x } ) = \\delta ( x ) $ whose CDF is just the step function . Then , the gradient $ { \\partial \\pi_ { i_ { max } } \\over \\partial x } $ becomes exactly zero almost everywhere , so is the gradient $ { \\partial \\mathcal { L } \\over \\partial x } $ . Thus , the statement is absolutely correct but the network parameters would never be updated via gradient-based optimization . [ Q4 : Why is the variance lower if the gradient only flows through to r_imax ? ] Response : As we mentioned in our response to AC , the variance of gradient estimator using our multi-class STE is exactly zero due to the fact that there is no randomness in SRQ . Hence , the variance of our gradient estimator is trivially lower ( actually , exactly zero ) than that of RQ based on sampling from Gumbel distribution . We explained it in more detail in the second last paragraph of Section 3.2 in the revision . --"}, "2": {"review_id": "DM6KlL7GeB-2", "review_text": "This paper proposed a novel network quantization method to reduce the bit-lengths of the network weights and activations , which is one of the most important problem for resource-limited devices . The presentation of this paper is well written and organized . The problem definition is clear , i.e. , relaxed quantization with the Gumbel-Softmax relaxation suffers from bias-variance trade-off depending on the temperature parameter of Gumbel-Softmax . The proposed method to solve this problem is reasonable . The experimental results are also convincing . Minor comments : 1 . In Table 3 , `` MNIST '' should be LeNet-5 . 2.Section 3.4 is a little dense . 3.I understand that there is little space , but embedding formulas in sentences is difficult to read . 4.I recommend you to show the result only using SQR for ImageNet in Table 2 as in MNIST and CIFER10 in Table 1 .", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer 4 , Thank you for your constructive feedback . We are sorry for the inconvenience with typo and small embedding formulas . We corrected them in the revision . As your suggestion , we are conducting experiments on ImageNet by only using SRQ in 3-bit . When quantizing ResNet-18 to 3-bit by only using SRQ , top-1 error is 37.45 % and top-5 error is 15.47 % after performing fine-tuning throughout 110 epochs with a batch size of 128 . Although SRQ in 3-bit already outperforms RQ in 4-bit , it seems that its performance could be enhanced as the training progresses further . Due to the time constraint of the rebuttal period and the fact that more epochs are required to train SRQ than SRQ + DropBits , we unfortunately have not yet completed this experiment , but we will add it in the final revision ."}}