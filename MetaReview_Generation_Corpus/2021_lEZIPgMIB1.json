{"year": "2021", "forum": "lEZIPgMIB1", "title": "Parametric UMAP: learning embeddings with deep neural networks for representation and semi-supervised learning", "decision": "Reject", "meta_review": "We thank the authors (and reviewers) for engaging in a detailed and constructive discussion, and providing a revised version of the paper after the initial round of reviews.\n\nRegarding quality, the work is technically correct and the amount of experiments significant. However, as highlighted by reviewers 2 and 3, some important questions remain unanswered, in particular 1) more empirical evidence to support the claim that the UMAP loss is a relevant for neural networks, and 2) more comparison with existing approaches (beyond t-SNE).\n\nRegarding clarity, the paper is overall clear and pleasant to read. However, after the revision round, all details about the proposed methods have been moved to the annex. While the initial version was criticized for the opposite reason (all experiments were in a annex), the balance may not be found yet; e.g., the equation for the UMAP loss, which is at the core of the paper, would certainly find its place in the main part of the manuscript for an ICLR paper.\n\nThe originality is the weakest aspect of the paper (besides the lack of comparison with related work). As mentioned by several reviewers, plugging the UMAP loss to a differentiable model is nowadays an idea that lacks originality. What would be important to justify that such a \"straightforward\" idea makes it to ICLR would be to demonstrate convincingly that it outperforms existing alternative approaches.\n\nFinally, regarding the significance of the work, it is limited by the lack of thorough comparison with existing method. On the other hand, if the method is implemented in a fast and easy-to-use package, it may find its public as illustrated by the positive evaluation of Reviewer 1 from a potential user point of view.", "reviews": [{"review_id": "lEZIPgMIB1-0", "review_text": "The authors propose a parametric version of UMAP by replacing sampling embeddings in the optimization of UMAP with directly learning weights of a neural network . The paper is very well and clearly written , but I have several significant concerns : 1 . I do n't see significant methodological novelty . Replacing embeddings with neural networks learning seems to be quite basic and straightforward . It is certainly a cherry on top of original UMAP , but I am not sure it could be counted as a separate method . The simplicity of methodology could be neglected , if the authors demonstrated significant improvement in their experiments , especially on downstream tasks . 2.A large part of the experiments is devoted to the comparison with tSNE , however it is not very clear why there is a lack of comparison with other parametric methods , such as Topological Autoencoders . Also not very clear why the authors mention these very relevant methods only in Appendix and not in the introduction in the beginning . 3.The performance of parametric UMAP achieves similar results to non-parametric UMAP , which is certainly nice , but also quite expected . Therefore , I would consider applications of parametric UMAP to other downstream tasks as a more significant and interesting contribution . However , experiments on this part are not convincing at all ( especially on CIFAR10 dataset ) . Would be interesting to see the performance on some other datasets . Also , it would be very interesting to see confidence intervals for Figures 15 , 16 , 18 . 4.In terms of speed I also do n't see an improvement compared to non-parametric UMAP ( TF ) . I see clear improvement compared to UMAP-learn version , but this as far as I understood due to a different implementation of the original UMAP and not in particular novelty of this paper . After authors ' response to revisions , I reconsidered my evaluation and updated the score .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their thoughtful review of our paper . In this comment , we provide a brief summary of the reviewer \u2019 s main points and our efforts to address them . We then give point by point responses in the following comments . Summary of the reviewer \u2019 s main points : 1 . * * Embedding comparisons should be made with other Parametric methods , such as Topological Autoencoders . * * - * Summary of response : As the reviewer mentions in other parts of their review , the focus of this paper is on downstream applications and not the learned embeddings per-se . Embedding comparisons are made to confirm similar embeddings to non-parametric baselines . Because algorithms such as Topological Autoencoders have already been compared to the same non-parametric baselines , and our methods do not confer a substantial improvement over those baselines , additional comparisons to e.g.Topological Autoencoders would likely yield little additional insight . To ensure the readers are aware of these related algorithms however , we expanded our discussion of related parametric methods , now referenced in the main text . * 2 . * * Downstream results are not convincing , especially on CIFAR10 . It would be interesting to see performance on other datasets . * * - * Summary of response : We included CIFAR10 in our analysis to demonstrate a failure case -- that , in datasets where little categorically-relevant structure is found by UMAP ( or t-SNE ) downstream gains by including a UMAP loss are minimal without further adaptations . We now emphasize that in the main text . In addition , we took the reviewer \u2019 s advice and added two real-world datasets to this analysis ( one bioacoustics , one single cell transcriptomes ) to demonstrate that in real-world datasets in which UMAP does capture categorically-relevant structure , including UMAP loss in the classifier does substantially improve performance in SSL settings . * 3 . * * The reviewer asks for clarification on the speed improvements with Parametric UMAP over non-parametric UMAP . * * - * Inference for Parametric UMAP is ~2-3 orders of magnitude faster for Parametric UMAP than non-parametric UMAP ( either tensorflow or UMAP learn ) . Optimization is slower for Parametric UMAP and non-parametric UMAP , but within the same order of magnitude . * We hope that our updates to downstream analyses ( 2 ) , as well as clarifications on speed improvements ( 3 ) and further inclusion of related works ( 1 ) are sufficient to convince the reviewer that our proposed method provides a novel and impactful contribution to the field , and that the reviewer will update their score accordingly . Thank you ."}, {"review_id": "lEZIPgMIB1-1", "review_text": "Response to authors : The authors have largely responded well to my original concerns . However , after reading through the discussions with other reviewers , I agree with reviewer 2 that more work is required to make this publishable . In particular , this should include comparisons to the other methods suggested and justification of the use of the UMAP loss function . Given this , I have downgraded my score accordingly . Original Review : This paper presents a parametric approach for UMAP , a dimensionality reduction method . This area is of interest to the community as dimensionality reduction can be useful in a lot of different tasks such as visualization , semisupervised learning , etc . If my comments below can be addressed , I would be willing to increase my score . Pros The parametric version presented here appears to work well in the experiments given . The incorporation of the UMAP loss directly in a neural network as a regularization is also interesting . Cons Some of the results do n't appear to have a corresponding figure or table , e.g . `` Reconstruction accuracy '' in section 3.3 . These should be included . UMAP tends to inherit some of the weaknesses of t-SNE as it tends to overemphasize local structure at the expense of global structure . In particular , it 's been shown [ R1 ] that UMAP and t-SNE are basically equivalent when using the same initialization . Could similar results be obtained by using the same initialization as UMAP instead of including the regularization term ? UMAP is traditionally used for visualization . Some of the applications presented ( e.g.SSL ) require/would benefit from higher dimensions than 2 or 3. t-SNE is known to be considerably slower for higher dimensions . Does UMAP inherit this problem ? If so , that should be mentioned as a potential drawback . All of the figures are given in the appendix . While this allows for more results , I think it would be a better paper if some of the figures were included in the main paper and some results were moved to the appendix . The authors should verify that their references are as up to date as possible . For example , the PHATE paper should be updated to the Nature Biotechnology version ( not bioRxiv ) . [ R1 ] Kobak and Linderman , https : //www.biorxiv.org/content/10.1101/2019.12.19.877522v1", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear reviewer , Thank you for your review . We are preparing a full response to each point made along with updates to the manuscript , which will be posted soon . I hope that you could provide a quick clarification to this point : `` UMAP tends to inherit some of the weaknesses of t-SNE as it tends to overemphasize local structure at the expense of global structure . In particular , it 's been shown [ R1 ] that UMAP and t-SNE are basically equivalent when using the same initialization . Could similar results be obtained by using the same initialization as UMAP instead of including the regularization term ? '' I agree with the remark , I 'm just unsure of what results / initialization / regularization term the reviewer is referring to . Parametric UMAP does not initialize embeddings , as the embeddings are learned projections through a neural network . The initial embeddings are projections through the weights of an untrained neural network . The same is true for Parametric t-SNE ( i.e.here the two parametric networks have the same random weight initialization ) . Could you clarify what results you are referring to ? Or perhaps this answers your question ? Cheers * edit 11/13 * : We responded to the question in our main response thread ."}, {"review_id": "lEZIPgMIB1-2", "review_text": "* * Update following discussion : * * Following the revision by the authors and the discussion with them , I am updating my score from 3 ( Clear rejection ) to 4 ( OK , but not good enough - rejection ) . This reflects in great part the revision the authors made to have the main paper ( limited to 8 pages ) be self contain and present their main results , while using the appendices for complementary and technical information . However , I still maintain the paper is not ready for publication in its current form . The extension of UMAP to implement the optimization via a neural network applied to input data rather than directly assigning coordinates is rather straightforward . The advantages it provides over UMAP in terms of natural inference on new data without the need for separate ( more computationally intensive ) out of sample extension method are a direct result of this neural network implementation , and they would be true not only for UMAP , but in fact for any method implemented in a `` parametric '' way via a neural network compared to nonparametric coordinate assignment . Similarly , allowing the addition of reconstruction or classification objectives in training is clearly a direct byproduct of this neural network implementation as well , and not unique to UMAP . Therefore , an important question has to be asked here for whether the UMAP loss is indeed a good choice for a loss term to impose on networks , for example , to enable visualization or improve various tasks . The authors already look into this to some extent by comparing to parametric tSNE as one alternative approach , but there are many others , as I mention in the initial review , relying on constructions from topological data analysis and manifold learning - most , if not all , of which rely on some graph construction on the data and then ensuring the coordinates provided by a hidden layer in the network match the relations encoded in the graph , similar to the proposed UMAP loss term . How are reconstruction and classification affected by using such other regularizations compared to the UMAP one ? Is inference speed the same for these other approaches ? How does training speed compare between them ? One can clearly expect some tradeoff between such properties and the geometric information encoded by different methods ( UMAP and tSNE emphasize clusters , while other methods may emphasize other patterns ) , but this should be discussed and demonstrated clearly rather than just ignoring the vast amount of related work on parametric approaches to capturing intrinsic geometry in data . Now , beyond the described lack of relevant comparisons for autoencoding and semi-supervised classification , even simply as a parametric implementation of UMAP ( which would be a rather narrow scope , which is not very enticing as a motivation on its own ) , I am not sure this work is sufficient to establish the presented approach . First , for the inference or embedding speed - this is essentially and out of sample extension task . As such , even if one insists on only comparing to UMAP-based methods , there are multiple OOS methods that can be used , such as Nystrom , geometric harmonics , etc . Some analysis of the tradeoff between extension quality and speed seems warranted here , but as I said previously - I think a comparison should also be provided to other parametric embedding methods beyond just OOS of UMAP ( and tSNE for that matter ) . Second , as the authors clarified in discussion - their approach relies on the suitability of the UMAP loss to be incorporated directly in the network optimization , essentially comparing activations to the UMAP graph . However , an alternative approach presented in related work is to provide a loss term between activations and a UMAP embedding . This second approach is more general , since other embeddings can also be considered there , but also probably has some disadvantages ( for example , the a priori fixed dimensionality , as the authors suggest ) . The differences between these two approaches should be addressed better in the manuscript , and importantly , since previous work exists already on the embedding loss approach , the authors should present a comparison establishing the benefits of the graph-based loss one , in addition to discussion regarding them . To conclude , the idea behind this work seems reasonable , albeit rather straightforward since it 's a reimplementation of the UMAP optimization . However , as it currently stands , I find it is not mature enough for publication and would need nonnegligible amount of work to properly position the contribution provided by this work compared to previous and related ones . I would like to encourage the authors to invest the time in adding such comparisons and clarifying not only how they are different from other methods , but also how they are better , and why choose UMAP to begin with as the basis for their proposed loss terms ( compared to various other approaches - not just tSNE ) . * * Initial review : * * Before getting into the details of this work , I note that in my opinion it should have been desk rejected for violating the page limit base on the way it is written . The main 8 pages of the paper are far from being self contained , and regularly reference materials from the appendix as integral parts that are crucial for the presentation and understanding here . These include not only methodological illustrations , but also all results establishing the method . In fact , the main paper here does not show ANY result - it only describes the setting for getting them . ALL the figures and tables showing results appear solely in the appendix . If we are to ignore the appendices and only judge the paper based on these main eight pages , then there is no support , no results , and very little in the way of presenting the method here . If , on the other hand , we include the result figures as integral parts of the main paper ( as they should be ) , then it clearly has significantly more than eight pages . Considering most papers submitted to this conference do try to provide a coherent and relatively self-contained presentation of their work within the page limit , according to the guidelines of the conference , while only leaving technical and supporting details to the appendix , I believe it would be inappropriate to consider this work as meeting the conference page limit . As for the work itself , this paper presents a rather na\u00efve attempt to combine together the UMAP visualization with deep learning . It essentially proposes to consider the coordinates optimized by UMAP as resulting from a neural network applied to input data . Then , instead of adjusting directly these coordinates via the UMAP optimization , the method here continues to backpropagate the coordinate optimization through the network to provide a parametric model , via a feed forward neural network , that embeds the data in low dimensions while preserving the local neighborhood structure in the same sense that UMAP , tSNE and LargeVis do with their nonparametric approach . This neural network formulation can also naturally be extended to consider other loss terms , such as reconstruction loss of autoencoders or any predictive loss ( classification , regression , etc . ) enabling supervised visualization . From a methodological perspective , this is a pretty straight-forward extension of the UMAP optimization , and does not indicate a clear advantage over it for the main task of unsupervised visualization or dimensionality reduction , neither in embedding quality or scalability . The authors show some interesting results ( albeit only in the appendix and not in the main paper ) on supervised visualization and out of sample extension speed , but these are not compared to relevant baselines that directly aim to address these tasks . Moreover , there is significant related work that is either ignored by the authors , or just mentioned in passing in the appendix without providing proper discussion and comparison with the proposed method . For example , in A.4 , the authors mention topological autoencoders , connectivity-optimized representation learning , SCVIS , VAE-SNE , geometry regularized autoencoders , IVIS , and Differential Embedding Networks , but they do not compare their work to any of these , even though such comparison seems highly relevant here . More work that is completely ignored here includes , for example , Diffusion Nets ( Mishne et al. , 2015 ) , Laplacian Autoencoders ( Jia et al. , 2015 ) , DIMAL ( Pai et al. , 2019 ) . Finally , briefly looking at Duque et al . ( 2020 ) cited here , while the main method there uses PHATE coordinates to regularize autoencoders , it seem they have also proposed the incorporation of UMAP loss terms in autoencoders , albeit only mentioned as somewhat of a sidenote together with tSNE regularization in their appendix . A discussion about the difference between these two approaches should be added to the main paper here , and it seems some comparison between them should also be presented to establish the advantages of the proposed approach here . Hence , even without the page limit argument , it does not seem the work presented here reaches the ICLR acceptance threshold without major revision to its presentation , discussion , and results . I must therefore recommend its rejection at this stage .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their thoughtful review of our paper . In this comment , we provide a brief summary of the reviewer \u2019 s main points and our efforts to address them . We then give point by point responses in the following comments . Summary of the reviewer \u2019 s main points : 1 . * * Figures and Tables integral to understanding the paper were included in the appendix , violating the page limit . * * - * Summary of response : We moved the figures and tables into the main text , and a section of the introduction walking through the algorithms underlying UMAP and t-SNE to the appendix . * 2 . * * The paper presents a naive attempt to combine UMAP with deep learning . * * - * Summary of response : It appears as if there may be some discrepancy between our method , and the reviewers interpretation of our method . We updated the main text to increase clarity . * 3 . * * Embedding quality comparisons with a greater range of papers should have been provided . * * - * Summary of response : The intention of the embedding quality section of our paper was to show comparable quality between parametric UMAP and its non-parametric form in relation to common embedding algorithms ( t-SNE , Parametric t-SNE , VAE , AE , and PCA ) . Parametric UMAP is not meant to improve upon these methods . It \u2019 s improvements lie in downstream applications ( faster inference , acting as a regularization in classifier networks , etc ) . Because we have shown that parametric and non-parametric embeddings are similar , additionally comparing algorithms that have already between compared with the non-parametric form of UMAP are not likely to extend previous results . * We hope that moving the locations of the figures to the main text ( 1 ) , alongside our explanations and updates for clarity in the text for ( 2 ) and ( 3 are sufficient for the reviewer to update their review accordingly.Thank you ."}, {"review_id": "lEZIPgMIB1-3", "review_text": "In the manuscript , the authors introduce a parametric version of UMAP , replacing the original embedding optimization step with a deep learning solution detecting a parametric relationship between data and embedding . The novel approach compares favourably with the standard algorithm and , as a major contribution , defines a loss function that can be employed for other important applications such as constraining the latent distribution of autoencoders , and improving classifier accuracy for semi-supervised learning . The paper is well written , complete and thoroughly detailed , both in the theoretical and the experimental section . The introduced material represents a significant advancement in the field , becoming a valuable resource for researchers in several areas . A couple of notes : - An application to one or more large real world dataset ( e.g.single-cell sequencing , or weather radar data ) would strengthen even more the authors \u2019 claims and the paper \u2019 s impact , so I would suggest to include it , at least in the Appendix . - Fig.3 in the Appendix is extremely useful to graphically explain the algorithm to a broader audience - I understand the page length limit , but I would strongly recommend to fit it in the main text . - I would also suggest to include ( maybe in the Appendix ) a kind of \u201c how-to \u201d fully worked example to help researchers in optimising the use of novel algorithm in a data exploration pipeline - I would point out ( within the limitation of the anonimity requirement ) the availability of the code for the algorithm", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank the reviewer for their thoughtful review of our paper . We appreciate the reviewer \u2019 s positive view , and hope that in addressing the issues described we have further improved the paper . In this comment , we provide a brief summary of the reviewer \u2019 s main points and our efforts to address them . We then give point by point responses in the following comments . Summary of the reviewer \u2019 s main points : 1 . * * An application to one or more large real-world datasets should be included . * * - * Summary of response : On the reviewer \u2019 s suggestion , we included a single-cell sequencing , as well as a bioacoustics dataset in the semisupervised learning section . * 2 . * * Figure 3 should be included in the main text . * * - * Summary of response : We included a modified version of Figure 3 in the main text ( to accommodate for room ) and the full version in the appendix . * 3 . * * The reviewer suggests including a \u201c how to \u201d example to help researchers extend our code to their own pipelines . * * - * Summary of response : We added a link to an anonymous colab notebook walking through the main steps of the algorithm . * We hope that our additions properly address the reviewer \u2019 s request and suggestions . Thank you ."}], "0": {"review_id": "lEZIPgMIB1-0", "review_text": "The authors propose a parametric version of UMAP by replacing sampling embeddings in the optimization of UMAP with directly learning weights of a neural network . The paper is very well and clearly written , but I have several significant concerns : 1 . I do n't see significant methodological novelty . Replacing embeddings with neural networks learning seems to be quite basic and straightforward . It is certainly a cherry on top of original UMAP , but I am not sure it could be counted as a separate method . The simplicity of methodology could be neglected , if the authors demonstrated significant improvement in their experiments , especially on downstream tasks . 2.A large part of the experiments is devoted to the comparison with tSNE , however it is not very clear why there is a lack of comparison with other parametric methods , such as Topological Autoencoders . Also not very clear why the authors mention these very relevant methods only in Appendix and not in the introduction in the beginning . 3.The performance of parametric UMAP achieves similar results to non-parametric UMAP , which is certainly nice , but also quite expected . Therefore , I would consider applications of parametric UMAP to other downstream tasks as a more significant and interesting contribution . However , experiments on this part are not convincing at all ( especially on CIFAR10 dataset ) . Would be interesting to see the performance on some other datasets . Also , it would be very interesting to see confidence intervals for Figures 15 , 16 , 18 . 4.In terms of speed I also do n't see an improvement compared to non-parametric UMAP ( TF ) . I see clear improvement compared to UMAP-learn version , but this as far as I understood due to a different implementation of the original UMAP and not in particular novelty of this paper . After authors ' response to revisions , I reconsidered my evaluation and updated the score .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their thoughtful review of our paper . In this comment , we provide a brief summary of the reviewer \u2019 s main points and our efforts to address them . We then give point by point responses in the following comments . Summary of the reviewer \u2019 s main points : 1 . * * Embedding comparisons should be made with other Parametric methods , such as Topological Autoencoders . * * - * Summary of response : As the reviewer mentions in other parts of their review , the focus of this paper is on downstream applications and not the learned embeddings per-se . Embedding comparisons are made to confirm similar embeddings to non-parametric baselines . Because algorithms such as Topological Autoencoders have already been compared to the same non-parametric baselines , and our methods do not confer a substantial improvement over those baselines , additional comparisons to e.g.Topological Autoencoders would likely yield little additional insight . To ensure the readers are aware of these related algorithms however , we expanded our discussion of related parametric methods , now referenced in the main text . * 2 . * * Downstream results are not convincing , especially on CIFAR10 . It would be interesting to see performance on other datasets . * * - * Summary of response : We included CIFAR10 in our analysis to demonstrate a failure case -- that , in datasets where little categorically-relevant structure is found by UMAP ( or t-SNE ) downstream gains by including a UMAP loss are minimal without further adaptations . We now emphasize that in the main text . In addition , we took the reviewer \u2019 s advice and added two real-world datasets to this analysis ( one bioacoustics , one single cell transcriptomes ) to demonstrate that in real-world datasets in which UMAP does capture categorically-relevant structure , including UMAP loss in the classifier does substantially improve performance in SSL settings . * 3 . * * The reviewer asks for clarification on the speed improvements with Parametric UMAP over non-parametric UMAP . * * - * Inference for Parametric UMAP is ~2-3 orders of magnitude faster for Parametric UMAP than non-parametric UMAP ( either tensorflow or UMAP learn ) . Optimization is slower for Parametric UMAP and non-parametric UMAP , but within the same order of magnitude . * We hope that our updates to downstream analyses ( 2 ) , as well as clarifications on speed improvements ( 3 ) and further inclusion of related works ( 1 ) are sufficient to convince the reviewer that our proposed method provides a novel and impactful contribution to the field , and that the reviewer will update their score accordingly . Thank you ."}, "1": {"review_id": "lEZIPgMIB1-1", "review_text": "Response to authors : The authors have largely responded well to my original concerns . However , after reading through the discussions with other reviewers , I agree with reviewer 2 that more work is required to make this publishable . In particular , this should include comparisons to the other methods suggested and justification of the use of the UMAP loss function . Given this , I have downgraded my score accordingly . Original Review : This paper presents a parametric approach for UMAP , a dimensionality reduction method . This area is of interest to the community as dimensionality reduction can be useful in a lot of different tasks such as visualization , semisupervised learning , etc . If my comments below can be addressed , I would be willing to increase my score . Pros The parametric version presented here appears to work well in the experiments given . The incorporation of the UMAP loss directly in a neural network as a regularization is also interesting . Cons Some of the results do n't appear to have a corresponding figure or table , e.g . `` Reconstruction accuracy '' in section 3.3 . These should be included . UMAP tends to inherit some of the weaknesses of t-SNE as it tends to overemphasize local structure at the expense of global structure . In particular , it 's been shown [ R1 ] that UMAP and t-SNE are basically equivalent when using the same initialization . Could similar results be obtained by using the same initialization as UMAP instead of including the regularization term ? UMAP is traditionally used for visualization . Some of the applications presented ( e.g.SSL ) require/would benefit from higher dimensions than 2 or 3. t-SNE is known to be considerably slower for higher dimensions . Does UMAP inherit this problem ? If so , that should be mentioned as a potential drawback . All of the figures are given in the appendix . While this allows for more results , I think it would be a better paper if some of the figures were included in the main paper and some results were moved to the appendix . The authors should verify that their references are as up to date as possible . For example , the PHATE paper should be updated to the Nature Biotechnology version ( not bioRxiv ) . [ R1 ] Kobak and Linderman , https : //www.biorxiv.org/content/10.1101/2019.12.19.877522v1", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear reviewer , Thank you for your review . We are preparing a full response to each point made along with updates to the manuscript , which will be posted soon . I hope that you could provide a quick clarification to this point : `` UMAP tends to inherit some of the weaknesses of t-SNE as it tends to overemphasize local structure at the expense of global structure . In particular , it 's been shown [ R1 ] that UMAP and t-SNE are basically equivalent when using the same initialization . Could similar results be obtained by using the same initialization as UMAP instead of including the regularization term ? '' I agree with the remark , I 'm just unsure of what results / initialization / regularization term the reviewer is referring to . Parametric UMAP does not initialize embeddings , as the embeddings are learned projections through a neural network . The initial embeddings are projections through the weights of an untrained neural network . The same is true for Parametric t-SNE ( i.e.here the two parametric networks have the same random weight initialization ) . Could you clarify what results you are referring to ? Or perhaps this answers your question ? Cheers * edit 11/13 * : We responded to the question in our main response thread ."}, "2": {"review_id": "lEZIPgMIB1-2", "review_text": "* * Update following discussion : * * Following the revision by the authors and the discussion with them , I am updating my score from 3 ( Clear rejection ) to 4 ( OK , but not good enough - rejection ) . This reflects in great part the revision the authors made to have the main paper ( limited to 8 pages ) be self contain and present their main results , while using the appendices for complementary and technical information . However , I still maintain the paper is not ready for publication in its current form . The extension of UMAP to implement the optimization via a neural network applied to input data rather than directly assigning coordinates is rather straightforward . The advantages it provides over UMAP in terms of natural inference on new data without the need for separate ( more computationally intensive ) out of sample extension method are a direct result of this neural network implementation , and they would be true not only for UMAP , but in fact for any method implemented in a `` parametric '' way via a neural network compared to nonparametric coordinate assignment . Similarly , allowing the addition of reconstruction or classification objectives in training is clearly a direct byproduct of this neural network implementation as well , and not unique to UMAP . Therefore , an important question has to be asked here for whether the UMAP loss is indeed a good choice for a loss term to impose on networks , for example , to enable visualization or improve various tasks . The authors already look into this to some extent by comparing to parametric tSNE as one alternative approach , but there are many others , as I mention in the initial review , relying on constructions from topological data analysis and manifold learning - most , if not all , of which rely on some graph construction on the data and then ensuring the coordinates provided by a hidden layer in the network match the relations encoded in the graph , similar to the proposed UMAP loss term . How are reconstruction and classification affected by using such other regularizations compared to the UMAP one ? Is inference speed the same for these other approaches ? How does training speed compare between them ? One can clearly expect some tradeoff between such properties and the geometric information encoded by different methods ( UMAP and tSNE emphasize clusters , while other methods may emphasize other patterns ) , but this should be discussed and demonstrated clearly rather than just ignoring the vast amount of related work on parametric approaches to capturing intrinsic geometry in data . Now , beyond the described lack of relevant comparisons for autoencoding and semi-supervised classification , even simply as a parametric implementation of UMAP ( which would be a rather narrow scope , which is not very enticing as a motivation on its own ) , I am not sure this work is sufficient to establish the presented approach . First , for the inference or embedding speed - this is essentially and out of sample extension task . As such , even if one insists on only comparing to UMAP-based methods , there are multiple OOS methods that can be used , such as Nystrom , geometric harmonics , etc . Some analysis of the tradeoff between extension quality and speed seems warranted here , but as I said previously - I think a comparison should also be provided to other parametric embedding methods beyond just OOS of UMAP ( and tSNE for that matter ) . Second , as the authors clarified in discussion - their approach relies on the suitability of the UMAP loss to be incorporated directly in the network optimization , essentially comparing activations to the UMAP graph . However , an alternative approach presented in related work is to provide a loss term between activations and a UMAP embedding . This second approach is more general , since other embeddings can also be considered there , but also probably has some disadvantages ( for example , the a priori fixed dimensionality , as the authors suggest ) . The differences between these two approaches should be addressed better in the manuscript , and importantly , since previous work exists already on the embedding loss approach , the authors should present a comparison establishing the benefits of the graph-based loss one , in addition to discussion regarding them . To conclude , the idea behind this work seems reasonable , albeit rather straightforward since it 's a reimplementation of the UMAP optimization . However , as it currently stands , I find it is not mature enough for publication and would need nonnegligible amount of work to properly position the contribution provided by this work compared to previous and related ones . I would like to encourage the authors to invest the time in adding such comparisons and clarifying not only how they are different from other methods , but also how they are better , and why choose UMAP to begin with as the basis for their proposed loss terms ( compared to various other approaches - not just tSNE ) . * * Initial review : * * Before getting into the details of this work , I note that in my opinion it should have been desk rejected for violating the page limit base on the way it is written . The main 8 pages of the paper are far from being self contained , and regularly reference materials from the appendix as integral parts that are crucial for the presentation and understanding here . These include not only methodological illustrations , but also all results establishing the method . In fact , the main paper here does not show ANY result - it only describes the setting for getting them . ALL the figures and tables showing results appear solely in the appendix . If we are to ignore the appendices and only judge the paper based on these main eight pages , then there is no support , no results , and very little in the way of presenting the method here . If , on the other hand , we include the result figures as integral parts of the main paper ( as they should be ) , then it clearly has significantly more than eight pages . Considering most papers submitted to this conference do try to provide a coherent and relatively self-contained presentation of their work within the page limit , according to the guidelines of the conference , while only leaving technical and supporting details to the appendix , I believe it would be inappropriate to consider this work as meeting the conference page limit . As for the work itself , this paper presents a rather na\u00efve attempt to combine together the UMAP visualization with deep learning . It essentially proposes to consider the coordinates optimized by UMAP as resulting from a neural network applied to input data . Then , instead of adjusting directly these coordinates via the UMAP optimization , the method here continues to backpropagate the coordinate optimization through the network to provide a parametric model , via a feed forward neural network , that embeds the data in low dimensions while preserving the local neighborhood structure in the same sense that UMAP , tSNE and LargeVis do with their nonparametric approach . This neural network formulation can also naturally be extended to consider other loss terms , such as reconstruction loss of autoencoders or any predictive loss ( classification , regression , etc . ) enabling supervised visualization . From a methodological perspective , this is a pretty straight-forward extension of the UMAP optimization , and does not indicate a clear advantage over it for the main task of unsupervised visualization or dimensionality reduction , neither in embedding quality or scalability . The authors show some interesting results ( albeit only in the appendix and not in the main paper ) on supervised visualization and out of sample extension speed , but these are not compared to relevant baselines that directly aim to address these tasks . Moreover , there is significant related work that is either ignored by the authors , or just mentioned in passing in the appendix without providing proper discussion and comparison with the proposed method . For example , in A.4 , the authors mention topological autoencoders , connectivity-optimized representation learning , SCVIS , VAE-SNE , geometry regularized autoencoders , IVIS , and Differential Embedding Networks , but they do not compare their work to any of these , even though such comparison seems highly relevant here . More work that is completely ignored here includes , for example , Diffusion Nets ( Mishne et al. , 2015 ) , Laplacian Autoencoders ( Jia et al. , 2015 ) , DIMAL ( Pai et al. , 2019 ) . Finally , briefly looking at Duque et al . ( 2020 ) cited here , while the main method there uses PHATE coordinates to regularize autoencoders , it seem they have also proposed the incorporation of UMAP loss terms in autoencoders , albeit only mentioned as somewhat of a sidenote together with tSNE regularization in their appendix . A discussion about the difference between these two approaches should be added to the main paper here , and it seems some comparison between them should also be presented to establish the advantages of the proposed approach here . Hence , even without the page limit argument , it does not seem the work presented here reaches the ICLR acceptance threshold without major revision to its presentation , discussion , and results . I must therefore recommend its rejection at this stage .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their thoughtful review of our paper . In this comment , we provide a brief summary of the reviewer \u2019 s main points and our efforts to address them . We then give point by point responses in the following comments . Summary of the reviewer \u2019 s main points : 1 . * * Figures and Tables integral to understanding the paper were included in the appendix , violating the page limit . * * - * Summary of response : We moved the figures and tables into the main text , and a section of the introduction walking through the algorithms underlying UMAP and t-SNE to the appendix . * 2 . * * The paper presents a naive attempt to combine UMAP with deep learning . * * - * Summary of response : It appears as if there may be some discrepancy between our method , and the reviewers interpretation of our method . We updated the main text to increase clarity . * 3 . * * Embedding quality comparisons with a greater range of papers should have been provided . * * - * Summary of response : The intention of the embedding quality section of our paper was to show comparable quality between parametric UMAP and its non-parametric form in relation to common embedding algorithms ( t-SNE , Parametric t-SNE , VAE , AE , and PCA ) . Parametric UMAP is not meant to improve upon these methods . It \u2019 s improvements lie in downstream applications ( faster inference , acting as a regularization in classifier networks , etc ) . Because we have shown that parametric and non-parametric embeddings are similar , additionally comparing algorithms that have already between compared with the non-parametric form of UMAP are not likely to extend previous results . * We hope that moving the locations of the figures to the main text ( 1 ) , alongside our explanations and updates for clarity in the text for ( 2 ) and ( 3 are sufficient for the reviewer to update their review accordingly.Thank you ."}, "3": {"review_id": "lEZIPgMIB1-3", "review_text": "In the manuscript , the authors introduce a parametric version of UMAP , replacing the original embedding optimization step with a deep learning solution detecting a parametric relationship between data and embedding . The novel approach compares favourably with the standard algorithm and , as a major contribution , defines a loss function that can be employed for other important applications such as constraining the latent distribution of autoencoders , and improving classifier accuracy for semi-supervised learning . The paper is well written , complete and thoroughly detailed , both in the theoretical and the experimental section . The introduced material represents a significant advancement in the field , becoming a valuable resource for researchers in several areas . A couple of notes : - An application to one or more large real world dataset ( e.g.single-cell sequencing , or weather radar data ) would strengthen even more the authors \u2019 claims and the paper \u2019 s impact , so I would suggest to include it , at least in the Appendix . - Fig.3 in the Appendix is extremely useful to graphically explain the algorithm to a broader audience - I understand the page length limit , but I would strongly recommend to fit it in the main text . - I would also suggest to include ( maybe in the Appendix ) a kind of \u201c how-to \u201d fully worked example to help researchers in optimising the use of novel algorithm in a data exploration pipeline - I would point out ( within the limitation of the anonimity requirement ) the availability of the code for the algorithm", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank the reviewer for their thoughtful review of our paper . We appreciate the reviewer \u2019 s positive view , and hope that in addressing the issues described we have further improved the paper . In this comment , we provide a brief summary of the reviewer \u2019 s main points and our efforts to address them . We then give point by point responses in the following comments . Summary of the reviewer \u2019 s main points : 1 . * * An application to one or more large real-world datasets should be included . * * - * Summary of response : On the reviewer \u2019 s suggestion , we included a single-cell sequencing , as well as a bioacoustics dataset in the semisupervised learning section . * 2 . * * Figure 3 should be included in the main text . * * - * Summary of response : We included a modified version of Figure 3 in the main text ( to accommodate for room ) and the full version in the appendix . * 3 . * * The reviewer suggests including a \u201c how to \u201d example to help researchers extend our code to their own pipelines . * * - * Summary of response : We added a link to an anonymous colab notebook walking through the main steps of the algorithm . * We hope that our additions properly address the reviewer \u2019 s request and suggestions . Thank you ."}}