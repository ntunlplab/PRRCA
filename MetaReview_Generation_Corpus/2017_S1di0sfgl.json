{"year": "2017", "forum": "S1di0sfgl", "title": "Hierarchical Multiscale Recurrent Neural Networks", "decision": "Accept (Poster)", "meta_review": "This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model.", "reviews": [{"review_id": "S1di0sfgl-0", "review_text": "This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written. Question) Can you extend it to bidirectional RNN? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We can think of two types of simple approach that exploit a backward RNN . 1 ) building representation based on intrinsic hierarchical structure of the data . We can think of two HM-LSTM , forward RNN and backward RNN , let these RNNs to not interfere into inference of each other . Then , we concatenate the hidden representations of these two HM-LSTMs and provide as input to the output module . A downside of this approach is that , the model can not take the full advantage of using bidirectional RNN - building representation of the next level based on the combined representation ( forward RNN and backward RNN ) of the previous level . 2 ) use future information to yield better segmentation . We can run the backward RNN ( can be standard RNN , e.g. , LSTM-RNN ) first and use the hidden states as auxiliary input to the forward RNN , which is HM-LSTM . The downside here is that , the backward RNN is not a HM-LSTM , it can not take any advantage such as conditional computation and etc ."}, {"review_id": "S1di0sfgl-1", "review_text": "The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection. Pros: - Paper is well-motivated, exceptionally well-composed - Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation - The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative. Cons: - In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated. - It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for a constructive review . > > Empirical results on computational savings are not given It is not entirely clear how we should implement the conditional computation in a mini-batch setting , however it is very straight forward when the batch_size is equal to 1 ( e.g. , at test time ) . We are planning to include this experiment in a near future . > > It 's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization , something which could hopefully be addressed . A good generalization performance is a result that we obtain by having the structural inductive bias ( i.e. , the hierarchical multiscale ) incorporated in the RNN architecture . This is similar to the receptive field structure implemented in the CNNs . If we see the CNN architecture as an architectural regularizer , our model also implements an architectural regularizer . It depends on how broadly we define the term 'regularizer ' . It is a regularizer in the sense that it restricts the model search space to a specific class ( hierarchical multiscale ) ."}, {"review_id": "S1di0sfgl-2", "review_text": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016). Their model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries. Overall this paper presents a strong and novel model with promising experimental results. On a minor note, I have few remarks/complaints about the writing and the related work: - In the introduction: \u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim. \u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references. \u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996 - in the related work: \u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995. While the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks. \u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010. Missing references: \u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010 \u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996 \u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007 \u201cLearning sequential tasks by incrementally adding higher orders\u201d, Ring, 1993 ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for pointing out the missing references , I will fix the citation accordingly . Apologies to the authors of related work that we 've missed ."}], "0": {"review_id": "S1di0sfgl-0", "review_text": "This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written. Question) Can you extend it to bidirectional RNN? ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We can think of two types of simple approach that exploit a backward RNN . 1 ) building representation based on intrinsic hierarchical structure of the data . We can think of two HM-LSTM , forward RNN and backward RNN , let these RNNs to not interfere into inference of each other . Then , we concatenate the hidden representations of these two HM-LSTMs and provide as input to the output module . A downside of this approach is that , the model can not take the full advantage of using bidirectional RNN - building representation of the next level based on the combined representation ( forward RNN and backward RNN ) of the previous level . 2 ) use future information to yield better segmentation . We can run the backward RNN ( can be standard RNN , e.g. , LSTM-RNN ) first and use the hidden states as auxiliary input to the forward RNN , which is HM-LSTM . The downside here is that , the backward RNN is not a HM-LSTM , it can not take any advantage such as conditional computation and etc ."}, "1": {"review_id": "S1di0sfgl-1", "review_text": "The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection. Pros: - Paper is well-motivated, exceptionally well-composed - Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation - The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative. Cons: - In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated. - It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for a constructive review . > > Empirical results on computational savings are not given It is not entirely clear how we should implement the conditional computation in a mini-batch setting , however it is very straight forward when the batch_size is equal to 1 ( e.g. , at test time ) . We are planning to include this experiment in a near future . > > It 's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization , something which could hopefully be addressed . A good generalization performance is a result that we obtain by having the structural inductive bias ( i.e. , the hierarchical multiscale ) incorporated in the RNN architecture . This is similar to the receptive field structure implemented in the CNNs . If we see the CNN architecture as an architectural regularizer , our model also implements an architectural regularizer . It depends on how broadly we define the term 'regularizer ' . It is a regularizer in the sense that it restricts the model search space to a specific class ( hierarchical multiscale ) ."}, "2": {"review_id": "S1di0sfgl-2", "review_text": "This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016). Their model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries. Overall this paper presents a strong and novel model with promising experimental results. On a minor note, I have few remarks/complaints about the writing and the related work: - In the introduction: \u201cOne of the key principles of learning in deep neural networks as well as in the human brain\u201d : please provide evidence for the \u201chuman brain\u201d part of this claim. \u201cFor modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances\u201d I believe you re missing Mikolov et al. 2010 in the references. \u201cin spite of the fact that hierarchical multiscale structures naturally exist in many temporal data\u201d: missing reference to Lin et al., 1996 - in the related work: \u201cA more recent model, the clockwork RNN (CW-RNN) (Koutn\u00edk et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)\u201d : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995. While the above models focus on online prediction problems, where a prediction needs to be made\u2026\u201d: I believe there is a lot of missing references, in particular to Socher\u2019s work or older recursive networks. \u201cThe norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)\u201d: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010. Missing references: \u201cRecurrent neural network based language model.\u201d, Mikolov et al. 2010 \u201cLearning long-term dependencies in NARX recurrent neural networks\u201d, Lin et al. 1996 \u201cSequence labelling in structured domains with hierarchical recurrent neural networks\u201c, Fernandez et al. 2007 \u201cLearning sequential tasks by incrementally adding higher orders\u201d, Ring, 1993 ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for pointing out the missing references , I will fix the citation accordingly . Apologies to the authors of related work that we 've missed ."}}