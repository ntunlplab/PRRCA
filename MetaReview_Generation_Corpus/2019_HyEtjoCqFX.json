{"year": "2019", "forum": "HyEtjoCqFX", "title": "Soft Q-Learning with Mutual-Information Regularization", "decision": "Accept (Poster)", "meta_review": "The paper proposes a new RL algorithm (MIRL) in the control-as-inference framework that learns a state-independent action prior.  A connection is provided to mutual information regularization.  Compared to entropic regularization, this approach is expected to work better when actions have significantly different importance.    The algorithm is shown to beat baselines in 11 out of 19 Atari games.\n\nThe paper is well written.  The derivation is novel, and the resulting algorithm is interesting and has good empirical results.  A few concerns were raised in initial reviews, including certain questions about experiments and potential negative impacts of the use of nonuniform action priors in MIRL.  The author responses and the new version were quite helpful, and all reviewers agree the paper is an interesting contribution.\n\nIn a revised version, the authors are encouraged to\n  (1) include a discussion of when MIRL might fail, and\n  (2) improve the related work section to compare the proposed method to other entropy regularized RL (sometimes under a different name in the literature), for example the following recent works and the references therein:\n    https://arxiv.org/abs/1705.07798\n    http://proceedings.mlr.press/v70/asadi17a.html\n    http://papers.nips.cc/paper/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning\n    http://proceedings.mlr.press/v80/dai18c.html", "reviews": [{"review_id": "HyEtjoCqFX-0", "review_text": "** Summary: ** The authors use the reformulation of RL as inference and propose to learn the prior policy. The novelty lies in learning a state-independent prior (instead of a state-dependent one) that can help exploration in the presence of universally unnecessary actions. They derive an equivalence to regularizing the mutual information between states and actions. ** Quality: ** The paper is mathematically detailed and correct. ** Clarity: ** The paper is sufficiently easy to follow and explains all the necessary background. ** Originality & Significance: ** The paper proposes a novel idea: Using a learned state-independent prior as opposed to using a learned state-dependent prior. While not a big change in terms of mathematical theory, this could lead to positive and interesting results empirically for exploration. Indeed they show promising results on Atari games: It is easy to see how Atari games could benefit as they have up to 18 different actions, many of which are redundant. My two main points where I think the paper could improve are: - More experimental results, in particular, how strong are the negative effects of MIRL if we have actions that are important, but have a lower probability in the stationary action distribution? - A related work section comparing their approach to the many recent similar papers in Maximum Entropy RL", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the comments . We have updated the manuscript with additional experiments in a grid-world domain aimed at answering the reviewer \u2019 s concerns . The additional experiments are aimed at better understanding the behaviour of our mutual-information constraint . We demonstrate that our method clearly improves learning speed when there is a strong preference for a single action in the optimal policy . We also examine an example in which the optimal policy crucially depends on an action with low probability in the marginal distribution . While MIRL does not improve performance in this case , it does not exhibit negative effects . We show that the learnt policy overcomes the prior when necessary for performance . Additionally , we have added a related work section that positions and compares our work to the existing literature on inference-based RL and maximum entropy RL in particular ."}, {"review_id": "HyEtjoCqFX-1", "review_text": "The authors take the control-as-inference viewpoint and learn a state-independent prior (which is typically held fixed). They claim that this leads to better exploration when actions have different importance. They relate this objective to a mutual information constrained RL objective in a limiting case. They then propose a practical algorithm, MIRL and compare their algorithm against DQN and Soft Q-learning (SQL) on 19 Atari games and demonstrate improvements over both. Generally I found the idea interesting and at a high level the deficiency of entropy regularization makes sense. However, I had great trouble understanding the reasoning behind their method and did not find the connection to mutual information helpful. Furthermore, I had a number of questions about the experiments. If the authors can clarify their motivation and reasoning and strengthen the experiments, I'd be happy to raise my score. In Sec 3.1, why is it sensible to optimize the prior? Can the authors give intuition for maximizing \\log p(R = 1) wrt to the prior? This is critical for justifying their approach. Currently, the authors provide a connection to MI, but don't explain why this matters. Does it justify the method? What insight are we supposed to take away from that? The experiments could be strengthened by addressing the following: * What was epsilon during training? Why was epsilon = 0.05 in evaluation? This is quite high compared to previous work, and it makes sense that this would degrade MIRLs performance less than DQN and SQL. * What is the performance of SQL if we use \\rho as the action selector in \\epsilon-greedy. This would help understand if the performance gains are due to the impact on the policy or due to the changes in the behavior policy. * Plotting beta over time * Comparing the action distributions for SQL and MIRL to understand the impact of the penalty. In general, a deeper analysis of the impact on the policy is important. * Are their environments we would expect MIRL to outperform SQL based on your theoretical understanding? Does it? * How many seeds were run per game? * How and why were the 19 games selected from the full set? Comments: The abstract claims state-of-the-art performance, however, what is actually shown is that MIRL outperforms DQN and SQL. With a fixed prior, the action prior can be absorbed into the reward (e.g., Levine 2018), so it is of no loss of generality to assume a uniform prior. Could state that the stationary distribution is assumed to exist and be unique. In Sec 3.1, why is the prior state independent? In Sec 3.1, p(R = 1|\\tau) is defined to be proportional to exp(\\beta \\sum_t r_t). Is this well-specified? How would we compute the normalizing constant since p(R = 0 | \\tau) is not defined? Throughout, I suggest that the authors not use the phrases \"closed form\" and \"analytic\" for expressions that are in terms of intractable quantities. It should be noted that Sec 3.2 Optimal policy for a fixed prior \\rho follows from Levine 2018 and others by transforming the fixed prior into a reward bonus. In Sec 3.2, the last statement does not appear to be necessary for the next subsection. Remove or clarify? I believe that the connection to MI can be simplified. Plugging in the optimal \\rho into Eq 3, we can see that Eq 3 simplifies to \\max_\\pi E_q[ \\sum_t \\gamma^t r_t] - (1 - gamma)/\\beta MI_p(s, a) where p(s, a) = d^\\pi(s) * \\pi(a | s) and d^\\pi is the discounted state visitation distribution. Thus Eq 3 can be thought of as a lower bound on the MI regularized objective. In Sec 4, the authors state the main difference between their soft operator and the typical soft operator. What other differences are there? Is that the only one? Sec 5 references the wrong Haarnoja reference in the first paragraph. In Sec 5, alpha_beta = 3 * 10^5. Is that correct? ===== 11/26 At this time, the authors have not responded to the reviews. I have read the other reviews and comments, and I'm not inclined to change my score. ==== 12/7 The authors have addressed most of my concerns, so I have raised my score. I'm still concerned that the exploration epsilon is quite different than existing work (e.g., https://github.com/google/dopamine/tree/master/baselines).", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are sorry for the delayed reply ( the deadline was extended to the end of 26th November Anywhere on Earth time ) . We state the reviewers comments and denote with arrows ( -- -- -- -- - > ) our replies . The authors take the control-as-inference viewpoint and learn a state-independent prior ( which is typically held fixed ) . They claim that this leads to better exploration when actions have different importance . They relate this objective to a mutual information constrained RL objective in a limiting case . They then propose a practical algorithm , MIRL and compare their algorithm against DQN and Soft Q-learning ( SQL ) on 19 Atari games and demonstrate improvements over both . Generally I found the idea interesting and at a high level the deficiency of entropy regularization makes sense . However , I had great trouble understanding the reasoning behind their method and did not find the connection to mutual information helpful . Furthermore , I had a number of questions about the experiments . If the authors can clarify their motivation and reasoning and strengthen the experiments , I 'd be happy to raise my score . In Sec 3.1 , why is it sensible to optimize the prior ? Can the authors give intuition for maximizing \\log p ( R = 1 ) wrt to the prior ? This is critical for justifying their approach . Currently , the authors provide a connection to MI , but do n't explain why this matters . Does it justify the method ? What insight are we supposed to take away from that ? -- -- -- -- -- -- - > [ On prior optimization and mutual-information ] We extended the paper with an explanation on mutual information and rate distortion theory , in order to help with an intuitive understanding of why this prior can help learning . We also added a related work section to note that other algorithms have considered optimizing the ELBO with respect to both variational and prior policy . However , these approaches do not use the marginal prior or have any connection to mutual information but instead optimise the policy while staying close to the previous policy . Additionally , we moved the connection to Mutual information for the case of gamma - > 1 to the appendix , and adopted another way to show this connection similar to what the reviewer has proposed . The experiments could be strengthened by addressing the following : * What was epsilon during training ? Why was epsilon = 0.05 in evaluation ? This is quite high compared to previous work , and it makes sense that this would degrade MIRLs performance less than DQN and SQL . -- -- -- -- -- - > [ Epsilon in training and evaluation ] Epsilon during training was decayed from 1.0 to 0.1 over the first 10^6 steps of the experiment . We used a fixed evaluation epsilon of 0.05 . This procedure is standard in the literature for ATARI , as introduced by the DQN paper ( see e.g.Mnih et al , 2015 ) . We understand that in later DQN papers ( e.g.Rainbow ) different values for these hyperparameters have been used but we feel our choice is not unreasonable . * What is the performance of SQL if we use \\rho as the action selector in \\epsilon-greedy . This would help understand if the performance gains are due to the impact on the policy or due to the changes in the behavior policy . -- -- -- -- -- - > [ On marginal exploration ] We have run additional experiments combining SQL with marginal exploration . Using the marginal exploration helps SQL , but MIRL still achieves the best performance . * Plotting beta over time -- -- -- -- -- - > [ Plotting beta ] We include the beta values evolving over time in the appendix . Additionally , we also include a more relevant term ( beta x Qvalues ) . * Comparing the action distributions for SQL and MIRL to understand the impact of the penalty . In general , a deeper analysis of the impact on the policy is important . * Are their environments we would expect MIRL to outperform SQL based on your theoretical understanding ? Does it ? * How many seeds were run per game ? -- -- -- -- -- - > [ Policy and grid world ] Responding the previous two questions : We have added additional experiments and plots to the paper in an effort to provide more insight into the behavior of our method . These experiments include a simple grid world in which we expect MIRL to outperform SQL and a grid world in which we expect the prior to have negative effects ( as suggested by another reviewer ) . * How and why were the 19 games selected from the full set ? -- -- -- -- -- -- - > [ On other aspects ] Due to computational constraints we were not able to run experiments on the full set of ATARI games . Therefore , we selected a subset of 20 random games , without prior experimentation on any of the games . We then evaluated our method using a single seed for every game . Data for experiments on 1 game were lost because of a cloud instance failure ."}, {"review_id": "HyEtjoCqFX-2", "review_text": "This work introduces SoftQ with a learned, state-independent prior. One derivation of this objective follows standard approaches from an RL as inference to derive the ELBO objective. A more novel view derived here connects this objective with the rate-distortion problem to view the objective as an RL objective subject to a constraint on the mutual information between the state and action distribution. They also outline a practical off-policy algorithm for optimizing this objective and compare it with Soft Q Learning (essentially, the same method but with a flat-prior) and DQN. They find that this results in small gains across most Atari games, with big gains for a few games. This work is well-explained except in one-aspect. The rate-distortion view of the objective is not well-justified. In particular, why is it desirable in the context of RL to constrain this mutual information? Empirical Deep RL performance is notoriously difficult to test (e.g. Henderson et al., 2017). The hyper-parameters are simply stated here, but no justification is given for how they are chosen / whether the baselines perform better under different choices. Given the gains compared with SoftQ are not that large, this information is important for understanding how much weight to place on the empirical result. The fact that the prior does not converge in some environments (e.g. Seaquest) is noted, but it seems this bears further discussion. Overall it appears this work provides: - An algorithm for Soft Q learning with a learned independent prior - Moderate evidence for gains compared with a flat prior on Atari. - A connection with this approach and regularization by constraining the mutual information between state and action distributions. It could be made a stronger piece of work by showing improvements in domains others than Atari, justifying the choice of regularization more. It would also benefit from positioning this work more clearly in relation to related approaches such as MPO (non-parametric state-dependent prior) and DistRL (state-dependent prior but shared across all games).", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the comments . Below we attempt to address each of the points raised by the reviewer . Background and related work : We have expanded the paper with a section highlighting the connection between the rate distortion framework and the mutual information constraint . We hope that this connection can help providing some intuitive insight into why our method can improve performance . We have also added a related work section more clearly positioning our work with respect to existing algorithms ( such as MPO and DistRL ) . Experiments : We have included a new set of experiments on a small tabular domain . While simple , we hope that this domain can provide more insight into the performance of the algorithm . Due to computational constraints we were not able to perform a complete search for optimal hyperparameter combinations in the Atari domain . Hyperparameter values were chosen by using values reported in the literature . Values for the new parameters introduced by MIRL were fixed by running a small number of exploratory experiments . Overall , we found the algorithm to be robust to changes in these values . All other hyperparameters were kept the same for all algorithms . While it is true that the prior does not converge in all of our ATARI experiments , we note that during the later stages of learning the plots do show a higher probability for subsets of actions . We have empirically observed that convergence of the prior can take a very long time , especially when the learner is still improving . We expect that , given enough time , the probabilities of the marginal policy will eventually settle . Additionally , in these experiments we used a non-decaying learning rate for the marginal policy . This means that we can expect some oscillation due to tracking behaviour of our approximation , while the policy and state distribution still change ."}], "0": {"review_id": "HyEtjoCqFX-0", "review_text": "** Summary: ** The authors use the reformulation of RL as inference and propose to learn the prior policy. The novelty lies in learning a state-independent prior (instead of a state-dependent one) that can help exploration in the presence of universally unnecessary actions. They derive an equivalence to regularizing the mutual information between states and actions. ** Quality: ** The paper is mathematically detailed and correct. ** Clarity: ** The paper is sufficiently easy to follow and explains all the necessary background. ** Originality & Significance: ** The paper proposes a novel idea: Using a learned state-independent prior as opposed to using a learned state-dependent prior. While not a big change in terms of mathematical theory, this could lead to positive and interesting results empirically for exploration. Indeed they show promising results on Atari games: It is easy to see how Atari games could benefit as they have up to 18 different actions, many of which are redundant. My two main points where I think the paper could improve are: - More experimental results, in particular, how strong are the negative effects of MIRL if we have actions that are important, but have a lower probability in the stationary action distribution? - A related work section comparing their approach to the many recent similar papers in Maximum Entropy RL", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the comments . We have updated the manuscript with additional experiments in a grid-world domain aimed at answering the reviewer \u2019 s concerns . The additional experiments are aimed at better understanding the behaviour of our mutual-information constraint . We demonstrate that our method clearly improves learning speed when there is a strong preference for a single action in the optimal policy . We also examine an example in which the optimal policy crucially depends on an action with low probability in the marginal distribution . While MIRL does not improve performance in this case , it does not exhibit negative effects . We show that the learnt policy overcomes the prior when necessary for performance . Additionally , we have added a related work section that positions and compares our work to the existing literature on inference-based RL and maximum entropy RL in particular ."}, "1": {"review_id": "HyEtjoCqFX-1", "review_text": "The authors take the control-as-inference viewpoint and learn a state-independent prior (which is typically held fixed). They claim that this leads to better exploration when actions have different importance. They relate this objective to a mutual information constrained RL objective in a limiting case. They then propose a practical algorithm, MIRL and compare their algorithm against DQN and Soft Q-learning (SQL) on 19 Atari games and demonstrate improvements over both. Generally I found the idea interesting and at a high level the deficiency of entropy regularization makes sense. However, I had great trouble understanding the reasoning behind their method and did not find the connection to mutual information helpful. Furthermore, I had a number of questions about the experiments. If the authors can clarify their motivation and reasoning and strengthen the experiments, I'd be happy to raise my score. In Sec 3.1, why is it sensible to optimize the prior? Can the authors give intuition for maximizing \\log p(R = 1) wrt to the prior? This is critical for justifying their approach. Currently, the authors provide a connection to MI, but don't explain why this matters. Does it justify the method? What insight are we supposed to take away from that? The experiments could be strengthened by addressing the following: * What was epsilon during training? Why was epsilon = 0.05 in evaluation? This is quite high compared to previous work, and it makes sense that this would degrade MIRLs performance less than DQN and SQL. * What is the performance of SQL if we use \\rho as the action selector in \\epsilon-greedy. This would help understand if the performance gains are due to the impact on the policy or due to the changes in the behavior policy. * Plotting beta over time * Comparing the action distributions for SQL and MIRL to understand the impact of the penalty. In general, a deeper analysis of the impact on the policy is important. * Are their environments we would expect MIRL to outperform SQL based on your theoretical understanding? Does it? * How many seeds were run per game? * How and why were the 19 games selected from the full set? Comments: The abstract claims state-of-the-art performance, however, what is actually shown is that MIRL outperforms DQN and SQL. With a fixed prior, the action prior can be absorbed into the reward (e.g., Levine 2018), so it is of no loss of generality to assume a uniform prior. Could state that the stationary distribution is assumed to exist and be unique. In Sec 3.1, why is the prior state independent? In Sec 3.1, p(R = 1|\\tau) is defined to be proportional to exp(\\beta \\sum_t r_t). Is this well-specified? How would we compute the normalizing constant since p(R = 0 | \\tau) is not defined? Throughout, I suggest that the authors not use the phrases \"closed form\" and \"analytic\" for expressions that are in terms of intractable quantities. It should be noted that Sec 3.2 Optimal policy for a fixed prior \\rho follows from Levine 2018 and others by transforming the fixed prior into a reward bonus. In Sec 3.2, the last statement does not appear to be necessary for the next subsection. Remove or clarify? I believe that the connection to MI can be simplified. Plugging in the optimal \\rho into Eq 3, we can see that Eq 3 simplifies to \\max_\\pi E_q[ \\sum_t \\gamma^t r_t] - (1 - gamma)/\\beta MI_p(s, a) where p(s, a) = d^\\pi(s) * \\pi(a | s) and d^\\pi is the discounted state visitation distribution. Thus Eq 3 can be thought of as a lower bound on the MI regularized objective. In Sec 4, the authors state the main difference between their soft operator and the typical soft operator. What other differences are there? Is that the only one? Sec 5 references the wrong Haarnoja reference in the first paragraph. In Sec 5, alpha_beta = 3 * 10^5. Is that correct? ===== 11/26 At this time, the authors have not responded to the reviews. I have read the other reviews and comments, and I'm not inclined to change my score. ==== 12/7 The authors have addressed most of my concerns, so I have raised my score. I'm still concerned that the exploration epsilon is quite different than existing work (e.g., https://github.com/google/dopamine/tree/master/baselines).", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are sorry for the delayed reply ( the deadline was extended to the end of 26th November Anywhere on Earth time ) . We state the reviewers comments and denote with arrows ( -- -- -- -- - > ) our replies . The authors take the control-as-inference viewpoint and learn a state-independent prior ( which is typically held fixed ) . They claim that this leads to better exploration when actions have different importance . They relate this objective to a mutual information constrained RL objective in a limiting case . They then propose a practical algorithm , MIRL and compare their algorithm against DQN and Soft Q-learning ( SQL ) on 19 Atari games and demonstrate improvements over both . Generally I found the idea interesting and at a high level the deficiency of entropy regularization makes sense . However , I had great trouble understanding the reasoning behind their method and did not find the connection to mutual information helpful . Furthermore , I had a number of questions about the experiments . If the authors can clarify their motivation and reasoning and strengthen the experiments , I 'd be happy to raise my score . In Sec 3.1 , why is it sensible to optimize the prior ? Can the authors give intuition for maximizing \\log p ( R = 1 ) wrt to the prior ? This is critical for justifying their approach . Currently , the authors provide a connection to MI , but do n't explain why this matters . Does it justify the method ? What insight are we supposed to take away from that ? -- -- -- -- -- -- - > [ On prior optimization and mutual-information ] We extended the paper with an explanation on mutual information and rate distortion theory , in order to help with an intuitive understanding of why this prior can help learning . We also added a related work section to note that other algorithms have considered optimizing the ELBO with respect to both variational and prior policy . However , these approaches do not use the marginal prior or have any connection to mutual information but instead optimise the policy while staying close to the previous policy . Additionally , we moved the connection to Mutual information for the case of gamma - > 1 to the appendix , and adopted another way to show this connection similar to what the reviewer has proposed . The experiments could be strengthened by addressing the following : * What was epsilon during training ? Why was epsilon = 0.05 in evaluation ? This is quite high compared to previous work , and it makes sense that this would degrade MIRLs performance less than DQN and SQL . -- -- -- -- -- - > [ Epsilon in training and evaluation ] Epsilon during training was decayed from 1.0 to 0.1 over the first 10^6 steps of the experiment . We used a fixed evaluation epsilon of 0.05 . This procedure is standard in the literature for ATARI , as introduced by the DQN paper ( see e.g.Mnih et al , 2015 ) . We understand that in later DQN papers ( e.g.Rainbow ) different values for these hyperparameters have been used but we feel our choice is not unreasonable . * What is the performance of SQL if we use \\rho as the action selector in \\epsilon-greedy . This would help understand if the performance gains are due to the impact on the policy or due to the changes in the behavior policy . -- -- -- -- -- - > [ On marginal exploration ] We have run additional experiments combining SQL with marginal exploration . Using the marginal exploration helps SQL , but MIRL still achieves the best performance . * Plotting beta over time -- -- -- -- -- - > [ Plotting beta ] We include the beta values evolving over time in the appendix . Additionally , we also include a more relevant term ( beta x Qvalues ) . * Comparing the action distributions for SQL and MIRL to understand the impact of the penalty . In general , a deeper analysis of the impact on the policy is important . * Are their environments we would expect MIRL to outperform SQL based on your theoretical understanding ? Does it ? * How many seeds were run per game ? -- -- -- -- -- - > [ Policy and grid world ] Responding the previous two questions : We have added additional experiments and plots to the paper in an effort to provide more insight into the behavior of our method . These experiments include a simple grid world in which we expect MIRL to outperform SQL and a grid world in which we expect the prior to have negative effects ( as suggested by another reviewer ) . * How and why were the 19 games selected from the full set ? -- -- -- -- -- -- - > [ On other aspects ] Due to computational constraints we were not able to run experiments on the full set of ATARI games . Therefore , we selected a subset of 20 random games , without prior experimentation on any of the games . We then evaluated our method using a single seed for every game . Data for experiments on 1 game were lost because of a cloud instance failure ."}, "2": {"review_id": "HyEtjoCqFX-2", "review_text": "This work introduces SoftQ with a learned, state-independent prior. One derivation of this objective follows standard approaches from an RL as inference to derive the ELBO objective. A more novel view derived here connects this objective with the rate-distortion problem to view the objective as an RL objective subject to a constraint on the mutual information between the state and action distribution. They also outline a practical off-policy algorithm for optimizing this objective and compare it with Soft Q Learning (essentially, the same method but with a flat-prior) and DQN. They find that this results in small gains across most Atari games, with big gains for a few games. This work is well-explained except in one-aspect. The rate-distortion view of the objective is not well-justified. In particular, why is it desirable in the context of RL to constrain this mutual information? Empirical Deep RL performance is notoriously difficult to test (e.g. Henderson et al., 2017). The hyper-parameters are simply stated here, but no justification is given for how they are chosen / whether the baselines perform better under different choices. Given the gains compared with SoftQ are not that large, this information is important for understanding how much weight to place on the empirical result. The fact that the prior does not converge in some environments (e.g. Seaquest) is noted, but it seems this bears further discussion. Overall it appears this work provides: - An algorithm for Soft Q learning with a learned independent prior - Moderate evidence for gains compared with a flat prior on Atari. - A connection with this approach and regularization by constraining the mutual information between state and action distributions. It could be made a stronger piece of work by showing improvements in domains others than Atari, justifying the choice of regularization more. It would also benefit from positioning this work more clearly in relation to related approaches such as MPO (non-parametric state-dependent prior) and DistRL (state-dependent prior but shared across all games).", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the comments . Below we attempt to address each of the points raised by the reviewer . Background and related work : We have expanded the paper with a section highlighting the connection between the rate distortion framework and the mutual information constraint . We hope that this connection can help providing some intuitive insight into why our method can improve performance . We have also added a related work section more clearly positioning our work with respect to existing algorithms ( such as MPO and DistRL ) . Experiments : We have included a new set of experiments on a small tabular domain . While simple , we hope that this domain can provide more insight into the performance of the algorithm . Due to computational constraints we were not able to perform a complete search for optimal hyperparameter combinations in the Atari domain . Hyperparameter values were chosen by using values reported in the literature . Values for the new parameters introduced by MIRL were fixed by running a small number of exploratory experiments . Overall , we found the algorithm to be robust to changes in these values . All other hyperparameters were kept the same for all algorithms . While it is true that the prior does not converge in all of our ATARI experiments , we note that during the later stages of learning the plots do show a higher probability for subsets of actions . We have empirically observed that convergence of the prior can take a very long time , especially when the learner is still improving . We expect that , given enough time , the probabilities of the marginal policy will eventually settle . Additionally , in these experiments we used a non-decaying learning rate for the marginal policy . This means that we can expect some oscillation due to tracking behaviour of our approximation , while the policy and state distribution still change ."}}