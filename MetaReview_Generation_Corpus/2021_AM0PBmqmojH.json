{"year": "2021", "forum": "AM0PBmqmojH", "title": "Warpspeed Computation of Optimal Transport, Graph Distances, and Embedding Alignment", "decision": "Reject", "meta_review": "The authors propose to approximate the kernel matrix used in the Sinkhorn algorithm by a combination of sparse + low rank approximation. To do so, the authors propose to compute a low rank approximation of a sparsified (thresholded below a certain value to be 0) kernel matrix using Nystr\u00f6m, and then correct it by adding back the true entries at non-sparse entries, after removing those obtained from the approximation. This results in a matrix whose application then results in sparse + low-rank.\n\nThe first version of the paper contained mostly experimental evidence, which was deemed a bit short by some reviewers.The authors have added theoretical material on the way. Although I believe these are worthy additions, as AC, I do not feel comfortable accepting the paper as of now, because I believe these additions were not properly reviewed. I understand this must be disappointing for the authors, who have sprinted to add new content during the rebuttal phase, but I hope they agree that the rebuttal process is not here to handle entirely new sections, but rather to improve existing parts. In particular, that section should be reviewed by authors knowledgeable on low rank kernel factorization, something I did not see in the pool of reviewers. I also believe the paper still has a few shortcomings. Taken together, I therefore recommend a re-submission.\n\nideas to improve the paper\n\n- the authors claim to use Nystr\u00f6m on a sparsified matrix (see eq. 4). The sparsified kernel is no longer positive definite. I would like the authors to comment on this. I understand Nystr\u00f6m could be used naively without any psd-ness guarantees, but I think a heads-up is needed.There are, furthermore, several local/global factorizations of kernel matrices available out there (e.g. MEKA, https://www.jmlr.org/papers/v18/15-025.html), the main difference here being that the product by such approximation must be guaranteed to be positive for it to work in the Sinkhorn algorithm. I would expect that bounds in expectation to break down sometimes, and therefore result in \"catastrophic\" failures (i.e. nan's). I think that an algorithm that claims to improve or replace another one, and which has such blind spots, needs such additional experiments (I have read the Limitations section in Appendix B, something more precise would improve the paper). I understand these were not part of the original Nystr\u00f6m paper for Sinkhorn, but since this is an increment over that previous work, therefore lacking a bit its originality, more knowledge needs to be contributed.\n\n- For instance, since the authors write an entire paragraph on this (Appendix B), I am surprised that there is not direct mention to the fact that a sparse sinkhorn may simply *not* converge, because it may not satisfy the fully indecomposable property required of matrices for Sinkhorn's algorithm to converge. \n\n- i dont think that users have the various identities (14,15) in mind when they think about \"backpropagating\" through Sinkhorn. What is typically needed is to compute the differentiable properties of the regularized OT matric and/or of the regularized OT cost w.r.t. *point locations* (i.e. x_i). The statement \"LCN-Sinkhorn works flawlessly with automatic backpropagation\" is misleading in the sense that it ignores that problem altogether. Since so many extensions of OT today relay on that differentiability, the section, as it is written now, is problematic.\n\n- several methods claim to be faster of more efficient than Sinkhorn to solve OT. Either these methods display faster theoretical convergence (e.g. by using acceleration) or display faster practical convergence (e.g. heavy ball variants) using synthetic, controlled datasets. Using synthetic data helps exhibit highlight relevant regimes for regularization parameters, including those where LSE Sinkhorn may converge but LCN does not work, or vice-versa. I understand that the authors' wanted to use real data, but it would be great to clarify whether that setup was used because LCN works better there (in which case this becomes more of a paper at the intersection of OT and word embeddings) or because this happened to be the first and only example the authors thought of.", "reviews": [{"review_id": "AM0PBmqmojH-0", "review_text": "Overall , I found this paper interesting , and I think it does address a relevant problem for the community . I have been playing with Nystrom approximations myself and I know the results are a bit disappointing , but it is grounded on strong theory . Then , it is pretty much welcome that attempts to patch the problems of Nystroms are made , so Optimal Transport becomes more scalable . The reason for why my judgement is below acceptance is that I believe both theory and results altogether are not strong enough so they live up to what is promised in the abstract . Typically when new methods are proposed with the promise of bridging a gap and solving a relevant problem , I hope they will have a thorough theoretical justification , or the results will be compelling . None of this convincing enough here . 1 ) On the theoretical side , I missed a convergence analysis , as the one in the Nystrom method . The theory side focuses on some derivative calculations , but I would love to see how the interplay between sparsity , Nystrom method and LSH lead to better convergence . I acknowledge this can be hard to do , though . Without having the theory it is hard to understand whether any reported experimental result is a consequence of choosing particularly good example for their method . 2 ) I found the experimental/methodological side was a bit disconnected from the rest ; the paper contains several vignettes about some applications/improvements in the practical side , but when reading that felt like belonging to other paper . I recommend authors work in creating a more coherent story 2a ) I found the discussion on Multi-Head OT unjustified and even a bit misleading . The Authors refer to some NLP papers , like arguing OT plays a role there . But none of these papers have any OT at all . The analogy of `` softmax for rows '' is not convincing as this is simply a softmax applied many times . There is a world of difference between that and the result of sinkhorn algorithm , but the narrative seems to downplay the actual difference between them . I recommend the authors elaborate more on this connection , because otherwise it is hard to follow ( and the subsequent results ) . 2b ) Results on translation seem impressive ( Table 2 ) , but raise a concern . Why would your method outperform Sinkhorn if it is only approximation ? is it perhaps the result of randomness ? since an explanation of this phenomenon is missing I am led to believing . Authors should improve the exposition of the baseline `` original '' . Why does full Sinkhorn does better than original ? . In summary , I think authors should improve the discussion about the validity/significance of their empirical results , highlighting the regimes when they are supposed to express and when they are not . 2c ) The main figure is Fig 2 . I recommend authors build on this and expand those results so it is clear when their method is better and when it is not", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # Ad 1 ) New theoretical convergence analysis As suggested , we have now added a full convergence analysis to the paper ( see Section 4 ) . In our understanding , such an analysis requires 2 parts : 1 . How good is the approximation ? 2.How fast does the approximation converge ? To answer ( 1 ) we first analyze the maximum error of the similarity matrix computed by LCN in comparison to regular Nystr\u00f6m in ( a ) a uniformly distributed data model and ( b ) a clustered data model . We then use the result by Altschuler et al.2019 to relate these bounds to bounds for the final Sinkhorn approximation . To answer ( 2 ) we only need to slightly modify the bound by Dvurechensky et al.2018 to show that both sparse Sinkhorn and LCN-Sinkhorn have the same convergence rate bound as regular Sinkhorn . # # Ad 2 ) Streamlined text We tried to make the paper more coherent by improving the introduction and integrating the section on enhanced OT in the graph transport network section , for which these are an important component . Ad 2a ) We agree and have mostly removed this part of the motivation . Ad 2b ) LCN-Sinkhorn performing better than full Sinkhorn on word embedding alignment is most likely due to regularization effects caused by the approximation , which lead to better generalization . The original method used by Grave et al.2019 performs Sinkhorn on an iteratively increasing , randomly sampled subset of the embeddings . Our experiments clearly show that this approach is far from ideal . We have added these explanations to the paper . _All_ figures and tables in the paper show standard deviations , thus the significance of our results is clearly marked . Furthermore , please note that as opposed to previous works we don \u2019 t just show results on some potentially cherry-picked datasets but make the effort of fully integrating our approximations in a pipeline and evaluate end-to-end real-world performance . Ad 2c ) We have restructured the text to make this figure \u2019 s description more prominent ."}, {"review_id": "AM0PBmqmojH-1", "review_text": "This paper studies how to approximate Sinkhorn computation using more efficient kernel matrix representations ( low-rank approach + LSH based sparse approach ) . Neither of both ideas is completely new , the authors studies a combination of them that has n't been explored in the literature , and use the proposed tech in three applications : ranking , embedding alignment , graph distance regression . Pro : - Authors have attempted multiple applications to prove the effectiveness with quantitative metrics . The main contribution seems the empirical validations . Con : - I think the presentation is a bit out of focus . Some sections can be left for appendix ( e.g. , Backpropagation in Sec 4. and Sec 5 . ) since some are either very standard in the literature or not really solidly experimented . Since I consider the main contribution to be the empirical evidences , the space should left for more details on those empirical experiments ( for reproducibility purpose ) . - the paper 's idea is not particularly innovative . ( yet it is not the sole reason for my scoring ) . - Some relevant works on low-rank ideas have not been compared/cited . e.g.Forrow , Aden , et al . `` Statistical optimal transport via factored couplings . '' The 22nd International Conference on Artificial Intelligence and Statistics . PMLR , 2019 . - The writing can be improved . A lot citations/references are not particularly relevant to what this paper is about and make some parts not enough self-explained . - The authors addressed my concerns and I raised my evaluation ratings .", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # Streamlined text We have removed Section 5 and integrated its content into the graph transport network section , for which these improvements are an important part . We have already tested these improvements separately in Table 3 . We have furthermore improved the introduction ( removing many less relevant citations ) , pushed back the related work section so it doesn \u2019 t overwhelm the reader , and added many explanations and definitions . # # Experimental details We strived to include all relevant details for fully reproducing the experiments in the main paper and in Appendices H , I , and J . The source code is available in the supplementary material . If there is some missing detail , we are happy to include it . # # Contribution Please note that we don \u2019 t just study one particular combination of representations , but we are the first to study a sparse approximation for Sinkhorn and also the first to combine a sparse approximation with a low-rank approximation . Also , please note that in the first experiment we directly investigate the Sinkhorn approximation . Approximating the transport plan might be viewed as a ranking task , but calling this an application seems rather misleading . # # Factored OT We have added a comparison to factored OT ( from Forrow , Aden , et al . `` Statistical optimal transport via factored couplings . '' ) to our experiments . The method performs comparatively well for high regularization , where it performs similarly to LCN-Sinkhorn ( see Fig.4 , 9 ) .However , similar to Nystr\u00f6m-Sinkhorn , it largely fails to approximate the transport plan at any reasonably low regularization ( see Table 1 , Fig.3 ) .Moreover , due to its iterative approach its runtime is multiple times higher than those of the other methods ( see Table 9 ) . # # Theoretical analysis To complement our empirical results we added a new section with a theoretical analysis . This should also improve the reader \u2019 s understanding of our method ."}, {"review_id": "AM0PBmqmojH-2", "review_text": "Summary : The paper considers the problem of approximating Sinkhorn divergence and corresponding transportation plan by combining low-rank and sparse approximation for the Sinkhorn kernel and using Nystrom iterations as a substitute for Sinkhorn 's iterations . The corresponding approach is amenable to differentiation and can be used as a building block in different architectures . Numerical experiments in several settings are performed to compare the proposed approach with existing ones and demonstrate its scalability . Evaluation : I believe the proposed framework is a valuable contribution in terms of practical performance and wide list of applications where OT could not be used before because of the high computational cost . So , I would recommend accepting this paper . Pros : 1.High scalability of the proposed approach and linear up to log factors in dimension complexity . 2.Flexibility of the framework due to a combination of sparse and low-rank approximations , which are complementary to each other . Cons : 1.Some parts of the paper seem to be not clear for a general audience . a.First page . $ n $ is undefined . b.First paragraph of Sect . 2.What is `` set of embeddings '' ? c. Last but one paragraph on p.2 . $ d $ is not defined . d. In ( 1 ) $ F $ stands for the Frobenius product , does it ? e. Proposition 1 . $ N $ is not defined . f. First paragraph of Sect . 5.What is `` OT with multiple heads '' ? g. What is meant as embedding ? h. In the experiments , what is used as the cost function to define the Sinkhorn kernel $ K $ ? If this is an $ L_2 $ distance , the convolutions can be used to accelerate the standard Sinkhorn and it would be nice to see the comparisons with convolutional Sinkhorn , which is also log linear . i.Appendix A . $ B , r , b $ are not defined when they are first used . j.In ( 17 ) , how was the last equality obtained ? k. In ( 19 ) , ( 20 ) , how were the first equalities obtained ? l. Appendix E , first paragraph . What is `` log-sum-exp trick '' ? m. Appendix G. What is `` similarity matrix '' ? 2.As far as I understood , the proposed approach is not amenable to parallel computations on GPU as opposed to standard Sinkhorn . Minor comments 1 . Maybe it is too strong to state in the abstract that this is the first log-linear time algorithm given that when the Sinkhorn kernel corresponds to a convolution , the Sinkhorn 's algorithm is log-linear by using the FFT . 2.Bibliographical note . ( Altschuler et al. , 2017 ) did not show $ 1/\\varepsilon^2 $ bound for the Greenkhorn . Their bound for Greenkhorn is the same $ 1/\\varepsilon^3 $ as for the Sinkhorn . The bound for Sinkhorn was improved to $ 1/\\varepsilon^2 $ in http : //proceedings.mlr.press/v80/dvurechensky18a.html and the bound for Greenkhorn was improved to $ 1/\\varepsilon^2 $ in http : //proceedings.mlr.press/v97/lin19a.html . 3.Bibliographical note . Quadratic regularization for OT was proposed in https : //arxiv.org/abs/1704.08200 . 4.Appendix A. I believe that in this framework a general value of the regularization parameter $ \\lambda $ is used . If it is the case , then the number of Sinkhorn iterations to find an $ \\varepsilon $ -solution to the regularized problem is $ 1/ ( \\varepsilon \\lambda ) $ . This follows from http : //proceedings.mlr.press/v80/dvurechensky18a.html Theorem 1 and an estimate for $ R $ in Lemma 1 . The bound $ 1/\\varepsilon^2 $ corresponds to finding an $ \\varepsilon $ -solution for the non-regularized problem . In this case one has to set $ \\lambda=\\varepsilon/ ( 4 \\ln n ) $ , which may be too small .", "rating": "7: Good paper, accept", "reply_text": "# # Improved readability We have incorporated all of your suggestions and clarified all your questions in the revised version of the paper ( including your minor comments ) . We have furthermore improved the introduction , pushed back the related work section , and integrated the enhanced OT section in the graph transport section to make the paper easier to understand . Regarding your particular comments : - We have added definitions and explanations to address your Cons 1a-g , i-m - Ad h1 ) The cost functions are described in Appendix H. We use the negative dot-product for word embeddings ( experiments 1 & 2 ) and the L2 distance in GTN ( experiment 3 ) . - Ad h2 & comment 1 ) There are many special cases for which log-linear algorithms exist and we tried to cover them in our related work section . The convolutional approach you mentioned ( we assume you refer to Solomon et al.2015. \u201c Convolutional wasserstein distances \u201d ) is only applicable to very low-dimensional ( i.e.2-3 dimensional ) data , not the high-dimensional spaces we focus on . For example , the word embeddings we work with have 300 dimensions . - Ad comments 2-3 ) We have changed the related work accordingly . - Ad minor comment 4 ) We now include Dvurechensky \u2019 s result directly in the paper and have made the bound in Appendix A more precise . The $ \\lambda $ we chose in our experiments was the one that performed best for full Sinkhorn . Also , sparse Sinkhorn actually performs especially well for very low $ \\lambda $ ( see Fig.4 ) , so we don \u2019 t expect this to be a problem . # # Fully parallel Our method is actually fully parallelizable . All experiments and runtime measurements were performed with a massively parallel PyTorch implementation running on GPUs ."}, {"review_id": "AM0PBmqmojH-3", "review_text": "Proposal : - First log-linear time algorithms for entropy-regularized OT that work for complex real-world tasks using high-dimensional spaces with little to no loss in accuracy ... many claims in one statement - Locally Corrected Nystr\u00f6m ... this would deserve a single paper - just to proof everything is still fine and valid in particular to alternatives - In the way this paper is written I am positive it gets accepted ( because it fits the writing style of the more recent papers in the field ) and contains sufficient novelty ... but I always wonder if good ( published ) science originates from clarity ( or confusion ) comments : - sorry but the text is very hard to follow ! - 'Optimal transport is concerned with the problem ' ( on p.3 ) - I think some introductive work may not harm in the first page - the reader is thrown up by terms and references - in my view more confusing than enlighting -- > it may not harm to add some brief explainations of terms ( from Cuturi : ) ' A transport plan is a flow on that graph satisfying source ( a i flowing out of each node i ) and sink ( b j flowing into each node j 0 ) ... which is simple an optimal flow in a graph ... I am not sure why we not simple can call it like this but need to come up with new terms - The paper is written ( following the very strange title ... although Cuturi did the same it would be nice if we can stop having marketing titles but focus a bit on science again ... in particular in 10 years many things proposed nowadays are not lightspeed or warpspeed anymore ) like providing an all issues solving theory -- > this does not improve the readability of the paper For example Eq.2 what is the 'meaning ' of ( s ) and ( t ) -- I have an idea but it is not written there - it is hard to proofread and verify a paper if it is written with the objective to confuse the reviewer ; - ) - widely incremental work by combing some known ideas ( Nystreom , LSH , ... ) - with a lot of addon theory which is probably correct but not very clear in the presentation - 'Since the Nystr\u00f6m method is a low-rank approximation it only accounts for the global structure of the kernel matrix K and not for the local structure around each point x . ' - this is actually wrong ( ! ) - if the landmarks are indepentent and the number of landmarks aligns with the rank of the matrix - Nystroem will provide a perfect ( ! ) reconstruction -- > there is a lot of work on the approximation bound of Nystroem ( and related methods ) - see e.g.work by Dhillon - where is the definition of 'sparse approximation K^sp ' used in Eq 3 ? - how precisely does \\bar { P } ( after Eq 3 ) link to the part around Eq 2 ? - is the Kernel K_ { ij } used here as well and / or where is the actually input data Kernel matrix - in Eq 1 what should be a cost function here and how do you obtain C_ { ij } ? - Ok Eq 4 is an actual proposal by balancing ( and joining ) sinkhorn and Nystroem in one distance formulation and it would not harm to motivate why and where you need such a distance in advance ( problem statement -- > solutions -- > particular strategy -- > outlined proposal -- > evaluation + proofs ) - 'Most modern ML models are trained using backpropagation ' - lets rephrase it as : nowadays neural network approaches are trained by backprop ... there are many other methods which are not at all trained by backprob for good reasons - ' ... Usually we want to learn embeddings which act as point sets X p and X q and therefore need gradients ' - well yes , if we stick on neural networks we need vectorial inputs and hence are often looking for ( costly ) embeddings - if we do not use NN we may not have this problem ( but others ) - 'We can either estimate these gradients via automatic differentiation ' - this is in general the more costly way to do things and I am happy to see that explicit derivations are given - regarding table 1 -- > before you come with numbers ( where you measured something ) it would be good to specify details of your scenario ( which are omitted before ) - in particular which data , which cost function , which parameters a.s.o . -- and although I understand that you like your method most it would still be good to provide some oldfashion baseline ( and not - not from sinkhorn ) - 'We propose the graph transport network ( GTN ) to evaluate approximate Sinkhorn and enhanced optimal transport and advance the state of the art on this task . ' fantastic on page 6 you actually outline a more userfriendly motivation", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # Contributions It seems to us like there has been some confusion about our paper \u2019 s contributions . Our method does not \u201c balance ( and join ) Sinkhorn and Nystr\u00f6m in one distance \u201d . Instead , we approximate the similarity matrix $ K=\\exp ( -C/\\lambda ) $ using ( 1 ) a sparse approximation and ( 2 ) a fused sparse/Nystr\u00f6m approximation ( which we call locally corrected Nystr\u00f6m ) . We then use this approximate K _inside_ Sinkhorn and show that doing so yields a better and faster approximation of Sinkhorn than previous methods . # # Improved readability We have incorporated all of your helpful suggestions in the revised version . In particular : - We have significantly improved the introduction , now starting with a general problem description , as suggested . - We added many more explanations and definitions to address your various questions , e.g.regarding $ s $ , $ t $ , $ K^ { \\text { sp } } $ , $ \\bar { P } _ { \\text { LCN } } $ , and the cost function . This should prevent future misinterpretations of Eq.5 ( previously Eq.4 ) .- We improved the motivation for providing explicit gradients . - We have removed the OT enhancements section and moved its content to the graph transport network section , where we use them . # # Nystr\u00f6m approximation Sinkhorn uses kernels of the form $ \\exp ( -C/\\lambda ) $ . Kernels like these ( e.g.the Gaussian kernel ) typically have a reproducing kernel Hilbert space that is infinitely dimensional . The resulting Gram matrix thus always has full rank and can not be reconstructed by a low-rank matrix such as the one provided by the Nystr\u00f6m method ( ! ) . We have made this more clear in the paper . # # Table 1 We are not sure if we understand your comment . The text describes the experimental setup before referring to Table 1 . The goal of our method is to approximate Sinkhorn , so in this experiment it is not a baseline but the ground truth . # # Theoretical analysis To address your comment on validity we provide a new theoretical analysis in Section 4 . In particular , we show that ( a ) LCN provides significant benefits over Nystr\u00f6m in both a uniform and clustered data model , ( b ) the error of approximate Sinkhorn is bounded , and ( c ) approximate Sinkhorn enjoys the same converge bounds as regular Sinkhorn ."}], "0": {"review_id": "AM0PBmqmojH-0", "review_text": "Overall , I found this paper interesting , and I think it does address a relevant problem for the community . I have been playing with Nystrom approximations myself and I know the results are a bit disappointing , but it is grounded on strong theory . Then , it is pretty much welcome that attempts to patch the problems of Nystroms are made , so Optimal Transport becomes more scalable . The reason for why my judgement is below acceptance is that I believe both theory and results altogether are not strong enough so they live up to what is promised in the abstract . Typically when new methods are proposed with the promise of bridging a gap and solving a relevant problem , I hope they will have a thorough theoretical justification , or the results will be compelling . None of this convincing enough here . 1 ) On the theoretical side , I missed a convergence analysis , as the one in the Nystrom method . The theory side focuses on some derivative calculations , but I would love to see how the interplay between sparsity , Nystrom method and LSH lead to better convergence . I acknowledge this can be hard to do , though . Without having the theory it is hard to understand whether any reported experimental result is a consequence of choosing particularly good example for their method . 2 ) I found the experimental/methodological side was a bit disconnected from the rest ; the paper contains several vignettes about some applications/improvements in the practical side , but when reading that felt like belonging to other paper . I recommend authors work in creating a more coherent story 2a ) I found the discussion on Multi-Head OT unjustified and even a bit misleading . The Authors refer to some NLP papers , like arguing OT plays a role there . But none of these papers have any OT at all . The analogy of `` softmax for rows '' is not convincing as this is simply a softmax applied many times . There is a world of difference between that and the result of sinkhorn algorithm , but the narrative seems to downplay the actual difference between them . I recommend the authors elaborate more on this connection , because otherwise it is hard to follow ( and the subsequent results ) . 2b ) Results on translation seem impressive ( Table 2 ) , but raise a concern . Why would your method outperform Sinkhorn if it is only approximation ? is it perhaps the result of randomness ? since an explanation of this phenomenon is missing I am led to believing . Authors should improve the exposition of the baseline `` original '' . Why does full Sinkhorn does better than original ? . In summary , I think authors should improve the discussion about the validity/significance of their empirical results , highlighting the regimes when they are supposed to express and when they are not . 2c ) The main figure is Fig 2 . I recommend authors build on this and expand those results so it is clear when their method is better and when it is not", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # Ad 1 ) New theoretical convergence analysis As suggested , we have now added a full convergence analysis to the paper ( see Section 4 ) . In our understanding , such an analysis requires 2 parts : 1 . How good is the approximation ? 2.How fast does the approximation converge ? To answer ( 1 ) we first analyze the maximum error of the similarity matrix computed by LCN in comparison to regular Nystr\u00f6m in ( a ) a uniformly distributed data model and ( b ) a clustered data model . We then use the result by Altschuler et al.2019 to relate these bounds to bounds for the final Sinkhorn approximation . To answer ( 2 ) we only need to slightly modify the bound by Dvurechensky et al.2018 to show that both sparse Sinkhorn and LCN-Sinkhorn have the same convergence rate bound as regular Sinkhorn . # # Ad 2 ) Streamlined text We tried to make the paper more coherent by improving the introduction and integrating the section on enhanced OT in the graph transport network section , for which these are an important component . Ad 2a ) We agree and have mostly removed this part of the motivation . Ad 2b ) LCN-Sinkhorn performing better than full Sinkhorn on word embedding alignment is most likely due to regularization effects caused by the approximation , which lead to better generalization . The original method used by Grave et al.2019 performs Sinkhorn on an iteratively increasing , randomly sampled subset of the embeddings . Our experiments clearly show that this approach is far from ideal . We have added these explanations to the paper . _All_ figures and tables in the paper show standard deviations , thus the significance of our results is clearly marked . Furthermore , please note that as opposed to previous works we don \u2019 t just show results on some potentially cherry-picked datasets but make the effort of fully integrating our approximations in a pipeline and evaluate end-to-end real-world performance . Ad 2c ) We have restructured the text to make this figure \u2019 s description more prominent ."}, "1": {"review_id": "AM0PBmqmojH-1", "review_text": "This paper studies how to approximate Sinkhorn computation using more efficient kernel matrix representations ( low-rank approach + LSH based sparse approach ) . Neither of both ideas is completely new , the authors studies a combination of them that has n't been explored in the literature , and use the proposed tech in three applications : ranking , embedding alignment , graph distance regression . Pro : - Authors have attempted multiple applications to prove the effectiveness with quantitative metrics . The main contribution seems the empirical validations . Con : - I think the presentation is a bit out of focus . Some sections can be left for appendix ( e.g. , Backpropagation in Sec 4. and Sec 5 . ) since some are either very standard in the literature or not really solidly experimented . Since I consider the main contribution to be the empirical evidences , the space should left for more details on those empirical experiments ( for reproducibility purpose ) . - the paper 's idea is not particularly innovative . ( yet it is not the sole reason for my scoring ) . - Some relevant works on low-rank ideas have not been compared/cited . e.g.Forrow , Aden , et al . `` Statistical optimal transport via factored couplings . '' The 22nd International Conference on Artificial Intelligence and Statistics . PMLR , 2019 . - The writing can be improved . A lot citations/references are not particularly relevant to what this paper is about and make some parts not enough self-explained . - The authors addressed my concerns and I raised my evaluation ratings .", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # Streamlined text We have removed Section 5 and integrated its content into the graph transport network section , for which these improvements are an important part . We have already tested these improvements separately in Table 3 . We have furthermore improved the introduction ( removing many less relevant citations ) , pushed back the related work section so it doesn \u2019 t overwhelm the reader , and added many explanations and definitions . # # Experimental details We strived to include all relevant details for fully reproducing the experiments in the main paper and in Appendices H , I , and J . The source code is available in the supplementary material . If there is some missing detail , we are happy to include it . # # Contribution Please note that we don \u2019 t just study one particular combination of representations , but we are the first to study a sparse approximation for Sinkhorn and also the first to combine a sparse approximation with a low-rank approximation . Also , please note that in the first experiment we directly investigate the Sinkhorn approximation . Approximating the transport plan might be viewed as a ranking task , but calling this an application seems rather misleading . # # Factored OT We have added a comparison to factored OT ( from Forrow , Aden , et al . `` Statistical optimal transport via factored couplings . '' ) to our experiments . The method performs comparatively well for high regularization , where it performs similarly to LCN-Sinkhorn ( see Fig.4 , 9 ) .However , similar to Nystr\u00f6m-Sinkhorn , it largely fails to approximate the transport plan at any reasonably low regularization ( see Table 1 , Fig.3 ) .Moreover , due to its iterative approach its runtime is multiple times higher than those of the other methods ( see Table 9 ) . # # Theoretical analysis To complement our empirical results we added a new section with a theoretical analysis . This should also improve the reader \u2019 s understanding of our method ."}, "2": {"review_id": "AM0PBmqmojH-2", "review_text": "Summary : The paper considers the problem of approximating Sinkhorn divergence and corresponding transportation plan by combining low-rank and sparse approximation for the Sinkhorn kernel and using Nystrom iterations as a substitute for Sinkhorn 's iterations . The corresponding approach is amenable to differentiation and can be used as a building block in different architectures . Numerical experiments in several settings are performed to compare the proposed approach with existing ones and demonstrate its scalability . Evaluation : I believe the proposed framework is a valuable contribution in terms of practical performance and wide list of applications where OT could not be used before because of the high computational cost . So , I would recommend accepting this paper . Pros : 1.High scalability of the proposed approach and linear up to log factors in dimension complexity . 2.Flexibility of the framework due to a combination of sparse and low-rank approximations , which are complementary to each other . Cons : 1.Some parts of the paper seem to be not clear for a general audience . a.First page . $ n $ is undefined . b.First paragraph of Sect . 2.What is `` set of embeddings '' ? c. Last but one paragraph on p.2 . $ d $ is not defined . d. In ( 1 ) $ F $ stands for the Frobenius product , does it ? e. Proposition 1 . $ N $ is not defined . f. First paragraph of Sect . 5.What is `` OT with multiple heads '' ? g. What is meant as embedding ? h. In the experiments , what is used as the cost function to define the Sinkhorn kernel $ K $ ? If this is an $ L_2 $ distance , the convolutions can be used to accelerate the standard Sinkhorn and it would be nice to see the comparisons with convolutional Sinkhorn , which is also log linear . i.Appendix A . $ B , r , b $ are not defined when they are first used . j.In ( 17 ) , how was the last equality obtained ? k. In ( 19 ) , ( 20 ) , how were the first equalities obtained ? l. Appendix E , first paragraph . What is `` log-sum-exp trick '' ? m. Appendix G. What is `` similarity matrix '' ? 2.As far as I understood , the proposed approach is not amenable to parallel computations on GPU as opposed to standard Sinkhorn . Minor comments 1 . Maybe it is too strong to state in the abstract that this is the first log-linear time algorithm given that when the Sinkhorn kernel corresponds to a convolution , the Sinkhorn 's algorithm is log-linear by using the FFT . 2.Bibliographical note . ( Altschuler et al. , 2017 ) did not show $ 1/\\varepsilon^2 $ bound for the Greenkhorn . Their bound for Greenkhorn is the same $ 1/\\varepsilon^3 $ as for the Sinkhorn . The bound for Sinkhorn was improved to $ 1/\\varepsilon^2 $ in http : //proceedings.mlr.press/v80/dvurechensky18a.html and the bound for Greenkhorn was improved to $ 1/\\varepsilon^2 $ in http : //proceedings.mlr.press/v97/lin19a.html . 3.Bibliographical note . Quadratic regularization for OT was proposed in https : //arxiv.org/abs/1704.08200 . 4.Appendix A. I believe that in this framework a general value of the regularization parameter $ \\lambda $ is used . If it is the case , then the number of Sinkhorn iterations to find an $ \\varepsilon $ -solution to the regularized problem is $ 1/ ( \\varepsilon \\lambda ) $ . This follows from http : //proceedings.mlr.press/v80/dvurechensky18a.html Theorem 1 and an estimate for $ R $ in Lemma 1 . The bound $ 1/\\varepsilon^2 $ corresponds to finding an $ \\varepsilon $ -solution for the non-regularized problem . In this case one has to set $ \\lambda=\\varepsilon/ ( 4 \\ln n ) $ , which may be too small .", "rating": "7: Good paper, accept", "reply_text": "# # Improved readability We have incorporated all of your suggestions and clarified all your questions in the revised version of the paper ( including your minor comments ) . We have furthermore improved the introduction , pushed back the related work section , and integrated the enhanced OT section in the graph transport section to make the paper easier to understand . Regarding your particular comments : - We have added definitions and explanations to address your Cons 1a-g , i-m - Ad h1 ) The cost functions are described in Appendix H. We use the negative dot-product for word embeddings ( experiments 1 & 2 ) and the L2 distance in GTN ( experiment 3 ) . - Ad h2 & comment 1 ) There are many special cases for which log-linear algorithms exist and we tried to cover them in our related work section . The convolutional approach you mentioned ( we assume you refer to Solomon et al.2015. \u201c Convolutional wasserstein distances \u201d ) is only applicable to very low-dimensional ( i.e.2-3 dimensional ) data , not the high-dimensional spaces we focus on . For example , the word embeddings we work with have 300 dimensions . - Ad comments 2-3 ) We have changed the related work accordingly . - Ad minor comment 4 ) We now include Dvurechensky \u2019 s result directly in the paper and have made the bound in Appendix A more precise . The $ \\lambda $ we chose in our experiments was the one that performed best for full Sinkhorn . Also , sparse Sinkhorn actually performs especially well for very low $ \\lambda $ ( see Fig.4 ) , so we don \u2019 t expect this to be a problem . # # Fully parallel Our method is actually fully parallelizable . All experiments and runtime measurements were performed with a massively parallel PyTorch implementation running on GPUs ."}, "3": {"review_id": "AM0PBmqmojH-3", "review_text": "Proposal : - First log-linear time algorithms for entropy-regularized OT that work for complex real-world tasks using high-dimensional spaces with little to no loss in accuracy ... many claims in one statement - Locally Corrected Nystr\u00f6m ... this would deserve a single paper - just to proof everything is still fine and valid in particular to alternatives - In the way this paper is written I am positive it gets accepted ( because it fits the writing style of the more recent papers in the field ) and contains sufficient novelty ... but I always wonder if good ( published ) science originates from clarity ( or confusion ) comments : - sorry but the text is very hard to follow ! - 'Optimal transport is concerned with the problem ' ( on p.3 ) - I think some introductive work may not harm in the first page - the reader is thrown up by terms and references - in my view more confusing than enlighting -- > it may not harm to add some brief explainations of terms ( from Cuturi : ) ' A transport plan is a flow on that graph satisfying source ( a i flowing out of each node i ) and sink ( b j flowing into each node j 0 ) ... which is simple an optimal flow in a graph ... I am not sure why we not simple can call it like this but need to come up with new terms - The paper is written ( following the very strange title ... although Cuturi did the same it would be nice if we can stop having marketing titles but focus a bit on science again ... in particular in 10 years many things proposed nowadays are not lightspeed or warpspeed anymore ) like providing an all issues solving theory -- > this does not improve the readability of the paper For example Eq.2 what is the 'meaning ' of ( s ) and ( t ) -- I have an idea but it is not written there - it is hard to proofread and verify a paper if it is written with the objective to confuse the reviewer ; - ) - widely incremental work by combing some known ideas ( Nystreom , LSH , ... ) - with a lot of addon theory which is probably correct but not very clear in the presentation - 'Since the Nystr\u00f6m method is a low-rank approximation it only accounts for the global structure of the kernel matrix K and not for the local structure around each point x . ' - this is actually wrong ( ! ) - if the landmarks are indepentent and the number of landmarks aligns with the rank of the matrix - Nystroem will provide a perfect ( ! ) reconstruction -- > there is a lot of work on the approximation bound of Nystroem ( and related methods ) - see e.g.work by Dhillon - where is the definition of 'sparse approximation K^sp ' used in Eq 3 ? - how precisely does \\bar { P } ( after Eq 3 ) link to the part around Eq 2 ? - is the Kernel K_ { ij } used here as well and / or where is the actually input data Kernel matrix - in Eq 1 what should be a cost function here and how do you obtain C_ { ij } ? - Ok Eq 4 is an actual proposal by balancing ( and joining ) sinkhorn and Nystroem in one distance formulation and it would not harm to motivate why and where you need such a distance in advance ( problem statement -- > solutions -- > particular strategy -- > outlined proposal -- > evaluation + proofs ) - 'Most modern ML models are trained using backpropagation ' - lets rephrase it as : nowadays neural network approaches are trained by backprop ... there are many other methods which are not at all trained by backprob for good reasons - ' ... Usually we want to learn embeddings which act as point sets X p and X q and therefore need gradients ' - well yes , if we stick on neural networks we need vectorial inputs and hence are often looking for ( costly ) embeddings - if we do not use NN we may not have this problem ( but others ) - 'We can either estimate these gradients via automatic differentiation ' - this is in general the more costly way to do things and I am happy to see that explicit derivations are given - regarding table 1 -- > before you come with numbers ( where you measured something ) it would be good to specify details of your scenario ( which are omitted before ) - in particular which data , which cost function , which parameters a.s.o . -- and although I understand that you like your method most it would still be good to provide some oldfashion baseline ( and not - not from sinkhorn ) - 'We propose the graph transport network ( GTN ) to evaluate approximate Sinkhorn and enhanced optimal transport and advance the state of the art on this task . ' fantastic on page 6 you actually outline a more userfriendly motivation", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # Contributions It seems to us like there has been some confusion about our paper \u2019 s contributions . Our method does not \u201c balance ( and join ) Sinkhorn and Nystr\u00f6m in one distance \u201d . Instead , we approximate the similarity matrix $ K=\\exp ( -C/\\lambda ) $ using ( 1 ) a sparse approximation and ( 2 ) a fused sparse/Nystr\u00f6m approximation ( which we call locally corrected Nystr\u00f6m ) . We then use this approximate K _inside_ Sinkhorn and show that doing so yields a better and faster approximation of Sinkhorn than previous methods . # # Improved readability We have incorporated all of your helpful suggestions in the revised version . In particular : - We have significantly improved the introduction , now starting with a general problem description , as suggested . - We added many more explanations and definitions to address your various questions , e.g.regarding $ s $ , $ t $ , $ K^ { \\text { sp } } $ , $ \\bar { P } _ { \\text { LCN } } $ , and the cost function . This should prevent future misinterpretations of Eq.5 ( previously Eq.4 ) .- We improved the motivation for providing explicit gradients . - We have removed the OT enhancements section and moved its content to the graph transport network section , where we use them . # # Nystr\u00f6m approximation Sinkhorn uses kernels of the form $ \\exp ( -C/\\lambda ) $ . Kernels like these ( e.g.the Gaussian kernel ) typically have a reproducing kernel Hilbert space that is infinitely dimensional . The resulting Gram matrix thus always has full rank and can not be reconstructed by a low-rank matrix such as the one provided by the Nystr\u00f6m method ( ! ) . We have made this more clear in the paper . # # Table 1 We are not sure if we understand your comment . The text describes the experimental setup before referring to Table 1 . The goal of our method is to approximate Sinkhorn , so in this experiment it is not a baseline but the ground truth . # # Theoretical analysis To address your comment on validity we provide a new theoretical analysis in Section 4 . In particular , we show that ( a ) LCN provides significant benefits over Nystr\u00f6m in both a uniform and clustered data model , ( b ) the error of approximate Sinkhorn is bounded , and ( c ) approximate Sinkhorn enjoys the same converge bounds as regular Sinkhorn ."}}