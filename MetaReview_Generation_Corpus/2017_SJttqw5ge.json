{"year": "2017", "forum": "SJttqw5ge", "title": "Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization", "decision": "Reject", "meta_review": "The paper looks at how natural language instructions can be decomposed into sub-tasks for as-yet-unseen new tasks\n hence the zero-shot generalization, which is considered to be the primary challenge to be solved. \n The precise problem being solved by the original paper is not clearly expressed in the writing. This left some reviewers asking for comparisons, while the authors note that for the specific nature of the problem being solved, they could not seen any particular methods that is known to be capable of tackling this kind of problem. The complexity of the system and simplicity of the final examples were also found to be contradictory by a subset of the reviewers, although this is again related to the understanding of the problem being solved.\n \n With scores of 3/4/5/7, the ideas in this paper were appreciated by a subset of reviewers. \n At the time of the writing of this metareview, the authors have posted a fairly lengthy rebuttal (Jan 18) and significant revisions (Jan 18), with no further responses from reviewers as of yet. However, it is difficult for reviewers to do a full re-evaluation of the paper on such short notice. \n \n While the latest revisions are commendable, it is unfortunately difficult to argue strongly in favor of this paper at present.", "reviews": [{"review_id": "SJttqw5ge-0", "review_text": "Description: This paper presents a reinforcement learning architecture where, based on \"natural-language\" input, a meta-controller chooses subtasks and communicates them to a subtask controller that choose primitive actions, based on the communicated subtask. The goal is to scale up reinforcement learning agents to large-scale tasks. The subtask controller embeds the subtask definition (arguments) into vectors by a multi-layer perceptron including an \"analogy-making\" regularization. The subtask vectors are combined with inputs at each layer of a CNN. CNN outputs (given the observation and the subtask) are then fed to one of two MLPs; one to compute action probabilities in the policy (exponential falloff of MLP outputs) and the other to compute termination probability (sigmoid from MLP outputs). The meta controller takes a list of sentences as instructions embeds them into a sequence of subtask arguments (not necessarily a one-to-one mapping). A context vector is computed by a CNN from the observation, the previous sentence embedding, the previous subtask and its completion state. The subtask arguments are computed from the context vector through further mechanisms involving instruction retrieval from memory pointers, and hard/soft decisions whether to update the subtask or not. Training involves policy distillation+actor-critic training for the subtask controller, and actor-critic training for the meta controller keeping the subtask controller frozen. The system is tested in a grid world where the agent moves and interacts with (picks up/transforms) various item/enemy types. It is compared to a) a flat controller not using a subtask controller, and b) subtask control by mere concatenation of the subtask embedding to the input with/without the analogy-making regularization. Evaluation: The proposed architecture seems reasonable, although it is not clear why the specific way of combining subtask embeddings in the subtask controller would be the \"right\" way to do it. I do not feel the grid world here really represents a \"large-scale task\": in particular the 10x10 size of the grid is very small. This is disappointing since this was a main motivation of the work. Moreover, the method is not compared to any state of the art alternatives. This is especially problematic because the test is not on established benchmarks. It is not really possible, based on the shown results, to put the performance in context of other works. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the review . We will post our full response soon . May I ask what the state-of-the-art alternatives are ? We would like to implement and compare them with our method in the next revision if possible ."}, {"review_id": "SJttqw5ge-1", "review_text": "This paper presents an architecture and corresponding algorithms for learning to act across multiple tasks, described in natural language. The proposed system is hierarchical and is closely related to the options framework. However, rather than learning a discrete set of options, it learns a mapping from natural instructions to an embedding which implicitly (dynamically) defines an option. This is a novel and interesting new perspective on options which had only slightly been explored in the linear setting (see comments below). I find the use of policy distillation particularly relevant for this setting. This, on its own, could be a takeaway for many RL readers who might not necessarily be interested about NLP applications. In general, the paper does not describe a single, simple, end-to-end, recipe for learning with this architecture. It rather relies on many recent advances skillfully combined: generalized advantage estimation, analogy-making regularizers, L1 regularization, memory addressing, matrix factorization, policy distillation. I would have liked to see some analysis but understand that it would have certainly been no easy task. For example, when you say \"while the parameters of the subtask controller are frozen\", this sounds to me like you're having some kind of two-timescale stochastic gradient descent. I'm also unsure how you deal with the SMDP structure in your gradient updates when you move to the \"temporal abstractions\" setting. I am inclined to believe that this approach has the potential to scale up to very large domains, but paper currently does not demonstrate this empirically. Like any typical reviewer, I would be tempted to say that you should perform larger experiments. However, I'm also glad that you have shown that your system also performs well in a \"toy\" domain. The characterization in figure 3 is insightful and makes a good point for the analogy regularizer and need for hierarchy. Overall, I think that the proposed architecture would inspire other researchers and would be worth being presented at ICLR. It also contains novel elements (subtask embeddings) which could be useful outside the deep and NLP communities into the more \"traditional\" RL communities. # Parameterized Options Sutton et. al (1999) did not explore the concept of *parameterized* options originally. It only came later, perhaps first with [\"Optimal policy switching algorithms for reinforcement learning, Comanici & Precup, 2010\"] or [\"Unified Inter and Intra Options Learning Using Policy Gradient Methods\", Levy & Shimkin, 2011]. Konidaris also has a line of work on \"parametrized skills\": [\"Learning Parameterized Skills\". da Silva, Konidaris, Barto, 2012)] or [\"Reinforcement Learning with Parameterized Actions\". Masson, Ranchod, Konidaris, 2015]. Also, I feel that there is a very important distinction to be made with the expression \"parametrized options\". In your work, \"parametrized\" comes in two flavors. In the spirit of policy gradient methods, we can have options whose policies and termination functions are represented by function approximators (in the same way that we have function approximation for value functions). Those options have parameters and we might call them \"parameterized\" because of that. This is the setting of Comanicy & Precup (2010), Levy & Shimkin (2011) Bacon & Precup (2015), Mankowitz, Mann, and Mannor (2016) for example. Now, there a second case where options/policies/skills take parameters *as inputs* and act accordingly. This is what Konidaris & al. means by \"parameterized\", whose meaning differs from the \"function approximation\" case above. In your work, the embedding of subtasks arguments is the \"input\" to your options and therefore behave as \"parameters\" in the sense of Konidaris. # Related Work I CTRL-F through the PDF but couldn't find references to any of S.R.K. Branavan's work. Branavan's PhD thesis had to do with using control techniques from RL in order to interpret natural instructions so as to achieve a goal. For example, in \"Reinforcement Learning for Mapping Instructions to Actions\", an RL agent learns from \"Windows troubleshooting articles\" to interact with UI elements (environment) through a Softmax policy (over linear features) learned by policy gradient methods. As you mention under \"Instruction execution\" the focus of your work in on generalization, which is not treated explicitely (afaik) in Branavan's work. Still, it shares some important algorithmic and architectural similarities which should be discussed explicitly or perhaps even compared to in your experiments (as a baseline). ## Zero-shot and UVFA It might also want to consider \"Learning Shared Representations for Value Functions in Multi-task Reinforcement Learning\", Borsa, Graepel, Shawe-Taylor] under the section \"zero-shot tasks generalization\". # Minor Issues I first read the abstract without knowing what the paper would be about and got confused in the second sentence. You talk about \"longer sequences of previously seen instructions\", but I didn't know what clearly meant by \"instructions\" until the second to last sentence where you specify \"instructions described by *natural language*.\" You could perhaps re-order the sentences to make it clear in the second sentence that you are interested in NLP problems. Zero-generalization: I was familiar with the term \"one-shot\" but not \"zero-shot\". The way that the second sentence \"[...] to have *similar* zero-shot [...]\" follows from the first sentence might as well hold for the \"one-shot\" setting. You could perhaps add a citation to \"zero-shot\", or define it more explicitly from the beginning and compare it to the one-shot setting. It could also be useful if you explain how zero-shot relates to just the notion of learning with \"priors\". Under section 3, you say \"cooperate with each other\" which sounds to me very much like a multi-agent setting, which your work does not explore in this way. You might want to choose a different terminology or explain more precisely if there is any connection with the multi-agent setting. The second sentence of section 6 is way to long and difficult to parse. You could probably split it in two or three sentences. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the comment and pointing out relevant work . We \u2019 ve posted a common response to the all reviewers as a separate comment above . We \u2019 d appreciate it if you go through the common response as well as this comment . - Regarding \u201c it relies on many recent advances skillfully combined \u201d Many recent advances are indeed combined in our paper . We explain why each of technique is needed in the common response . In addition , we revised the paper so that readers can easily differentiate between our own idea and existing recent techniques . - Regarding dealing with SMDP structure in gradient update in temporal abstraction setting Just to clarify , the subtask controller is trained first and serves as a parameterized option for the meta controller . So , the two controllers are trained separately . The SMDP structure of the meta controller is implicitly determined by a binary variable c_t in the meta controller itself . The gradient update of the meta controller is also affected by this variable as shown in the Equation ( 10 ) in the appendix . - Regarding parameterized options and other related work Thank you for pointing our relevant work . We included many of them in the \u201c related work \u201d section . - Regarding the clarity of the paper ( \u201c minor issues \u201d ) Thank you for the detailed comments ! We reflected your comments in the revision . The term \u201c zero-shot learning/generalization \u201d means that the agent generalizes to new tasks \u201c without additional learning process \u201d . This term has been widely used in supervised learning problems where the model should predict previously unseen labels . As you mentioned , zero-shot learning is closely related to \u201c learning with priors \u201d . In order to make zero-shot learning possible , the model ( or the agent ) should learn prior knowledge about problems ( or tasks ) and infer the underlying goal given a new problem from the prior . In our work , this prior corresponds to two assumptions . The first assumption is that subtask arguments ( e.g. , \u2018 visit / transform / pick up \u2019 v.s. \u2018 cow / duck / stone \u2026 \u2019 ) are independent of each other , and this is learned through analogy-making . The second assumption is that instructions should be executed sequentially , which is embedded in the structure of the meta controller . By learning or having such priors , the agent can successfully generalize to previously unseen and longer instructions ."}, {"review_id": "SJttqw5ge-2", "review_text": "This paper can be seen as instantiating a famous paper by the founder of AI John McCarthy on learning to take advice (which was studied in depth by other later researchers, such as Jack Mostow in the card game Hearts). The idea is that the agent is given high level instructions on how to solve a problem, and must distill from it a low level policy. This is quite related to how humans learn complex tasks in many domains (e.g., driving, where a driving instructor may provide advice such as \"keep a certain distance from the car in front\"). A fairly complex neural deep learning controller architecture is used, although the details of this system are somewhat confusing in terms of many details that are presented. A simpler approach might have been easier to follow, at least initially. The experiments unfortunately are on a rather simplistic 2D maze, and it would have been worthwhile to see how the approach scaled to more complex tasks of the sort usually seen in deep RL papers these days (e.g, Atari, physics simulators etc.). Nice overall idea, somewhat confusing description of the solution, and an inadequate set of experiments on a less than satisfactory domain of 2D grid worlds. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the comment . We \u2019 ve posted a common response to the all reviewers as a separate comment above . We \u2019 d appreciate it if you go through the common response . - Regarding the complexity of our architecture : We have clarified and justified why we need each component of our architecture in the common response . We also revised the paper so that readers can understand the system and its motivation more easily . - Regarding the domain : We found that most of the existing RL benchmarks ( e.g , Atari ) are not flexible enough to define sequences of seen and unseen tasks . So , we chose to build our own tasks in a 2d grid-world as we aim to solve generalization problems rather than a particular domain . In addition , we also added a new experiment on a 3D visual domain in the current version of the paper ."}, {"review_id": "SJttqw5ge-3", "review_text": "The paper presents a hierarchical DRL algorithm that solves sequences of navigate-and-act tasks in a 2D maze domain. During training and evaluation, a list of sub-goals represented by text is given to the agent and its goal is to learn to use pre-learned skills in order to solve a list of sub-goals. The authors demonstrate that their method generalizes well to sequences of varying length as well as to new combinations of sub-goals (i.e., if the agent knows how to pick up a diamond and how to visit an apple, it can also visit the diamond). Overall, the paper is of high technical quality and presents an interesting and non-trivial combination of state-of-the-art advancements in Deep Learning (DL) and Deep Reinforcement Learning (DRL). In particular, the authors presents a DRL agent that is hierarchical in the sense that it can learn skills and plan using them. The skills are learned using a differential temporally extended memory networks with an attention mechanism. The authors also make a novel use of analogy making and parameter prediction. However, I find it difficult to understand from the paper why the presented problem is interesting and why hadn't it bee solved before. Since the domain being evaluated is a simple 2D maze, using deep networks is not well motivated. Similar problems have been solved using simpler models. In particular, there is a reach literature about planning with skills that had been ignored completely by the authors. Since all of the skills are trained prior to the evaluation of the hierarchical agent, the problem that is being solved is much more similar to supervised learning than reinforcement learning (since when using the pre-trained skills the reward is not particularly delayed). The generalization that is demonstrated seems to be limited to breaking a sentence (describing the subtask) into words (item, location, action). The paper is difficult to read, it is constantly switching between describing the algorithm and giving technical details. In particular, I find it to be overloaded with details that interfere with the general understanding of the paper. I suggest moving many of the implementation details into the appendix. The paper should be self-contained, please do not assume that the reader is familiar with all the methods that you use and introduce all the relevant notations. I believe that the paper will benefit from addressing the problems I described above and will make a better contribution to the community in a future conference. ", "rating": "3: Clear rejection", "reply_text": "Thank you for the comment . We \u2019 ve posted a common response to the all reviewers as a separate comment above . We \u2019 d appreciate it if you go through the common response as well as this comment . - Regarding the simplicity of the domain and the problem and \u201c similar problems have been solved using simpler models \u201d : We believe that our problem has unique challenges compared to traditional RL tasks . We have discussed these challenges with justification of our method in the common response above . We also added a new result from a 3D visual domain in the revision ( Section 6.5 ) . Please let us know if you have further comments or questions about this . - Regarding \u201c Previous work on planning with skills are ignored by the authors \u201d Thank you for pointing out important prior work on planning with skills . We added and discussed this line of work [ 1 , 2 , 3 ] in the \u201c related work \u201d section . To summarize , we found that the problem considered in the prior work is quite different from our problem . ( i.e. , their problems lack the challenges discussed in the common response . ) Please let us know if there is still missing work . - Regarding \u201c Generalization is limited to breaking a sentence into words \u201d Although we demonstrated generalization to unseen pairs of subtask arguments ( two arguments ) in the main experiment , the proposed analogy-making regularizer can be applied to any number of arguments . In addition , it can also be used for more complex generalization scenarios . For example , if objects should be handled in a different way given the same subtask ( i.e. , verb and noun are not independent ) , our analogy-making regularizer can be used to provide prior knowledge so that the agent can generalize to unseen target objects in a desired way without needing to experience them . This is discussed in Appendix B . - Regarding \u201c the paper is difficult to read \u201d : Thank you for the suggestions about the structure of our paper . We revised the paper by moving less important details to the appendix and by reorganizing the section structure . Just to clarify , Algorithm 1 and 2 tables are not algorithms in the sense that they describe the neural network architecture . Please let us know if there are still some sections that need to be removed or improved . [ References ] [ 1 ] George Konidaris , Andrew G. Barto . Building Portable Options : Skill Transfer in Reinforcement Learning , IJCAI 2007 . [ 2 ] George Konidaris , Ilya Scheidwasser , Andrew G. Barto . Transfer in Reinforcement Learning via Shared Features , Journal of Machine Learning Research 2012 . [ 3 ] Bruno Castro da Silva , George Konidaris , Andrew G. Barto , Learning Parameterized Skills , ICML 2012 ."}], "0": {"review_id": "SJttqw5ge-0", "review_text": "Description: This paper presents a reinforcement learning architecture where, based on \"natural-language\" input, a meta-controller chooses subtasks and communicates them to a subtask controller that choose primitive actions, based on the communicated subtask. The goal is to scale up reinforcement learning agents to large-scale tasks. The subtask controller embeds the subtask definition (arguments) into vectors by a multi-layer perceptron including an \"analogy-making\" regularization. The subtask vectors are combined with inputs at each layer of a CNN. CNN outputs (given the observation and the subtask) are then fed to one of two MLPs; one to compute action probabilities in the policy (exponential falloff of MLP outputs) and the other to compute termination probability (sigmoid from MLP outputs). The meta controller takes a list of sentences as instructions embeds them into a sequence of subtask arguments (not necessarily a one-to-one mapping). A context vector is computed by a CNN from the observation, the previous sentence embedding, the previous subtask and its completion state. The subtask arguments are computed from the context vector through further mechanisms involving instruction retrieval from memory pointers, and hard/soft decisions whether to update the subtask or not. Training involves policy distillation+actor-critic training for the subtask controller, and actor-critic training for the meta controller keeping the subtask controller frozen. The system is tested in a grid world where the agent moves and interacts with (picks up/transforms) various item/enemy types. It is compared to a) a flat controller not using a subtask controller, and b) subtask control by mere concatenation of the subtask embedding to the input with/without the analogy-making regularization. Evaluation: The proposed architecture seems reasonable, although it is not clear why the specific way of combining subtask embeddings in the subtask controller would be the \"right\" way to do it. I do not feel the grid world here really represents a \"large-scale task\": in particular the 10x10 size of the grid is very small. This is disappointing since this was a main motivation of the work. Moreover, the method is not compared to any state of the art alternatives. This is especially problematic because the test is not on established benchmarks. It is not really possible, based on the shown results, to put the performance in context of other works. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the review . We will post our full response soon . May I ask what the state-of-the-art alternatives are ? We would like to implement and compare them with our method in the next revision if possible ."}, "1": {"review_id": "SJttqw5ge-1", "review_text": "This paper presents an architecture and corresponding algorithms for learning to act across multiple tasks, described in natural language. The proposed system is hierarchical and is closely related to the options framework. However, rather than learning a discrete set of options, it learns a mapping from natural instructions to an embedding which implicitly (dynamically) defines an option. This is a novel and interesting new perspective on options which had only slightly been explored in the linear setting (see comments below). I find the use of policy distillation particularly relevant for this setting. This, on its own, could be a takeaway for many RL readers who might not necessarily be interested about NLP applications. In general, the paper does not describe a single, simple, end-to-end, recipe for learning with this architecture. It rather relies on many recent advances skillfully combined: generalized advantage estimation, analogy-making regularizers, L1 regularization, memory addressing, matrix factorization, policy distillation. I would have liked to see some analysis but understand that it would have certainly been no easy task. For example, when you say \"while the parameters of the subtask controller are frozen\", this sounds to me like you're having some kind of two-timescale stochastic gradient descent. I'm also unsure how you deal with the SMDP structure in your gradient updates when you move to the \"temporal abstractions\" setting. I am inclined to believe that this approach has the potential to scale up to very large domains, but paper currently does not demonstrate this empirically. Like any typical reviewer, I would be tempted to say that you should perform larger experiments. However, I'm also glad that you have shown that your system also performs well in a \"toy\" domain. The characterization in figure 3 is insightful and makes a good point for the analogy regularizer and need for hierarchy. Overall, I think that the proposed architecture would inspire other researchers and would be worth being presented at ICLR. It also contains novel elements (subtask embeddings) which could be useful outside the deep and NLP communities into the more \"traditional\" RL communities. # Parameterized Options Sutton et. al (1999) did not explore the concept of *parameterized* options originally. It only came later, perhaps first with [\"Optimal policy switching algorithms for reinforcement learning, Comanici & Precup, 2010\"] or [\"Unified Inter and Intra Options Learning Using Policy Gradient Methods\", Levy & Shimkin, 2011]. Konidaris also has a line of work on \"parametrized skills\": [\"Learning Parameterized Skills\". da Silva, Konidaris, Barto, 2012)] or [\"Reinforcement Learning with Parameterized Actions\". Masson, Ranchod, Konidaris, 2015]. Also, I feel that there is a very important distinction to be made with the expression \"parametrized options\". In your work, \"parametrized\" comes in two flavors. In the spirit of policy gradient methods, we can have options whose policies and termination functions are represented by function approximators (in the same way that we have function approximation for value functions). Those options have parameters and we might call them \"parameterized\" because of that. This is the setting of Comanicy & Precup (2010), Levy & Shimkin (2011) Bacon & Precup (2015), Mankowitz, Mann, and Mannor (2016) for example. Now, there a second case where options/policies/skills take parameters *as inputs* and act accordingly. This is what Konidaris & al. means by \"parameterized\", whose meaning differs from the \"function approximation\" case above. In your work, the embedding of subtasks arguments is the \"input\" to your options and therefore behave as \"parameters\" in the sense of Konidaris. # Related Work I CTRL-F through the PDF but couldn't find references to any of S.R.K. Branavan's work. Branavan's PhD thesis had to do with using control techniques from RL in order to interpret natural instructions so as to achieve a goal. For example, in \"Reinforcement Learning for Mapping Instructions to Actions\", an RL agent learns from \"Windows troubleshooting articles\" to interact with UI elements (environment) through a Softmax policy (over linear features) learned by policy gradient methods. As you mention under \"Instruction execution\" the focus of your work in on generalization, which is not treated explicitely (afaik) in Branavan's work. Still, it shares some important algorithmic and architectural similarities which should be discussed explicitly or perhaps even compared to in your experiments (as a baseline). ## Zero-shot and UVFA It might also want to consider \"Learning Shared Representations for Value Functions in Multi-task Reinforcement Learning\", Borsa, Graepel, Shawe-Taylor] under the section \"zero-shot tasks generalization\". # Minor Issues I first read the abstract without knowing what the paper would be about and got confused in the second sentence. You talk about \"longer sequences of previously seen instructions\", but I didn't know what clearly meant by \"instructions\" until the second to last sentence where you specify \"instructions described by *natural language*.\" You could perhaps re-order the sentences to make it clear in the second sentence that you are interested in NLP problems. Zero-generalization: I was familiar with the term \"one-shot\" but not \"zero-shot\". The way that the second sentence \"[...] to have *similar* zero-shot [...]\" follows from the first sentence might as well hold for the \"one-shot\" setting. You could perhaps add a citation to \"zero-shot\", or define it more explicitly from the beginning and compare it to the one-shot setting. It could also be useful if you explain how zero-shot relates to just the notion of learning with \"priors\". Under section 3, you say \"cooperate with each other\" which sounds to me very much like a multi-agent setting, which your work does not explore in this way. You might want to choose a different terminology or explain more precisely if there is any connection with the multi-agent setting. The second sentence of section 6 is way to long and difficult to parse. You could probably split it in two or three sentences. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the comment and pointing out relevant work . We \u2019 ve posted a common response to the all reviewers as a separate comment above . We \u2019 d appreciate it if you go through the common response as well as this comment . - Regarding \u201c it relies on many recent advances skillfully combined \u201d Many recent advances are indeed combined in our paper . We explain why each of technique is needed in the common response . In addition , we revised the paper so that readers can easily differentiate between our own idea and existing recent techniques . - Regarding dealing with SMDP structure in gradient update in temporal abstraction setting Just to clarify , the subtask controller is trained first and serves as a parameterized option for the meta controller . So , the two controllers are trained separately . The SMDP structure of the meta controller is implicitly determined by a binary variable c_t in the meta controller itself . The gradient update of the meta controller is also affected by this variable as shown in the Equation ( 10 ) in the appendix . - Regarding parameterized options and other related work Thank you for pointing our relevant work . We included many of them in the \u201c related work \u201d section . - Regarding the clarity of the paper ( \u201c minor issues \u201d ) Thank you for the detailed comments ! We reflected your comments in the revision . The term \u201c zero-shot learning/generalization \u201d means that the agent generalizes to new tasks \u201c without additional learning process \u201d . This term has been widely used in supervised learning problems where the model should predict previously unseen labels . As you mentioned , zero-shot learning is closely related to \u201c learning with priors \u201d . In order to make zero-shot learning possible , the model ( or the agent ) should learn prior knowledge about problems ( or tasks ) and infer the underlying goal given a new problem from the prior . In our work , this prior corresponds to two assumptions . The first assumption is that subtask arguments ( e.g. , \u2018 visit / transform / pick up \u2019 v.s. \u2018 cow / duck / stone \u2026 \u2019 ) are independent of each other , and this is learned through analogy-making . The second assumption is that instructions should be executed sequentially , which is embedded in the structure of the meta controller . By learning or having such priors , the agent can successfully generalize to previously unseen and longer instructions ."}, "2": {"review_id": "SJttqw5ge-2", "review_text": "This paper can be seen as instantiating a famous paper by the founder of AI John McCarthy on learning to take advice (which was studied in depth by other later researchers, such as Jack Mostow in the card game Hearts). The idea is that the agent is given high level instructions on how to solve a problem, and must distill from it a low level policy. This is quite related to how humans learn complex tasks in many domains (e.g., driving, where a driving instructor may provide advice such as \"keep a certain distance from the car in front\"). A fairly complex neural deep learning controller architecture is used, although the details of this system are somewhat confusing in terms of many details that are presented. A simpler approach might have been easier to follow, at least initially. The experiments unfortunately are on a rather simplistic 2D maze, and it would have been worthwhile to see how the approach scaled to more complex tasks of the sort usually seen in deep RL papers these days (e.g, Atari, physics simulators etc.). Nice overall idea, somewhat confusing description of the solution, and an inadequate set of experiments on a less than satisfactory domain of 2D grid worlds. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the comment . We \u2019 ve posted a common response to the all reviewers as a separate comment above . We \u2019 d appreciate it if you go through the common response . - Regarding the complexity of our architecture : We have clarified and justified why we need each component of our architecture in the common response . We also revised the paper so that readers can understand the system and its motivation more easily . - Regarding the domain : We found that most of the existing RL benchmarks ( e.g , Atari ) are not flexible enough to define sequences of seen and unseen tasks . So , we chose to build our own tasks in a 2d grid-world as we aim to solve generalization problems rather than a particular domain . In addition , we also added a new experiment on a 3D visual domain in the current version of the paper ."}, "3": {"review_id": "SJttqw5ge-3", "review_text": "The paper presents a hierarchical DRL algorithm that solves sequences of navigate-and-act tasks in a 2D maze domain. During training and evaluation, a list of sub-goals represented by text is given to the agent and its goal is to learn to use pre-learned skills in order to solve a list of sub-goals. The authors demonstrate that their method generalizes well to sequences of varying length as well as to new combinations of sub-goals (i.e., if the agent knows how to pick up a diamond and how to visit an apple, it can also visit the diamond). Overall, the paper is of high technical quality and presents an interesting and non-trivial combination of state-of-the-art advancements in Deep Learning (DL) and Deep Reinforcement Learning (DRL). In particular, the authors presents a DRL agent that is hierarchical in the sense that it can learn skills and plan using them. The skills are learned using a differential temporally extended memory networks with an attention mechanism. The authors also make a novel use of analogy making and parameter prediction. However, I find it difficult to understand from the paper why the presented problem is interesting and why hadn't it bee solved before. Since the domain being evaluated is a simple 2D maze, using deep networks is not well motivated. Similar problems have been solved using simpler models. In particular, there is a reach literature about planning with skills that had been ignored completely by the authors. Since all of the skills are trained prior to the evaluation of the hierarchical agent, the problem that is being solved is much more similar to supervised learning than reinforcement learning (since when using the pre-trained skills the reward is not particularly delayed). The generalization that is demonstrated seems to be limited to breaking a sentence (describing the subtask) into words (item, location, action). The paper is difficult to read, it is constantly switching between describing the algorithm and giving technical details. In particular, I find it to be overloaded with details that interfere with the general understanding of the paper. I suggest moving many of the implementation details into the appendix. The paper should be self-contained, please do not assume that the reader is familiar with all the methods that you use and introduce all the relevant notations. I believe that the paper will benefit from addressing the problems I described above and will make a better contribution to the community in a future conference. ", "rating": "3: Clear rejection", "reply_text": "Thank you for the comment . We \u2019 ve posted a common response to the all reviewers as a separate comment above . We \u2019 d appreciate it if you go through the common response as well as this comment . - Regarding the simplicity of the domain and the problem and \u201c similar problems have been solved using simpler models \u201d : We believe that our problem has unique challenges compared to traditional RL tasks . We have discussed these challenges with justification of our method in the common response above . We also added a new result from a 3D visual domain in the revision ( Section 6.5 ) . Please let us know if you have further comments or questions about this . - Regarding \u201c Previous work on planning with skills are ignored by the authors \u201d Thank you for pointing out important prior work on planning with skills . We added and discussed this line of work [ 1 , 2 , 3 ] in the \u201c related work \u201d section . To summarize , we found that the problem considered in the prior work is quite different from our problem . ( i.e. , their problems lack the challenges discussed in the common response . ) Please let us know if there is still missing work . - Regarding \u201c Generalization is limited to breaking a sentence into words \u201d Although we demonstrated generalization to unseen pairs of subtask arguments ( two arguments ) in the main experiment , the proposed analogy-making regularizer can be applied to any number of arguments . In addition , it can also be used for more complex generalization scenarios . For example , if objects should be handled in a different way given the same subtask ( i.e. , verb and noun are not independent ) , our analogy-making regularizer can be used to provide prior knowledge so that the agent can generalize to unseen target objects in a desired way without needing to experience them . This is discussed in Appendix B . - Regarding \u201c the paper is difficult to read \u201d : Thank you for the suggestions about the structure of our paper . We revised the paper by moving less important details to the appendix and by reorganizing the section structure . Just to clarify , Algorithm 1 and 2 tables are not algorithms in the sense that they describe the neural network architecture . Please let us know if there are still some sections that need to be removed or improved . [ References ] [ 1 ] George Konidaris , Andrew G. Barto . Building Portable Options : Skill Transfer in Reinforcement Learning , IJCAI 2007 . [ 2 ] George Konidaris , Ilya Scheidwasser , Andrew G. Barto . Transfer in Reinforcement Learning via Shared Features , Journal of Machine Learning Research 2012 . [ 3 ] Bruno Castro da Silva , George Konidaris , Andrew G. Barto , Learning Parameterized Skills , ICML 2012 ."}}