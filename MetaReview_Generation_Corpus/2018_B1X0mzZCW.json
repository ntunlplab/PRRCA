{"year": "2018", "forum": "B1X0mzZCW", "title": "Fidelity-Weighted Learning", "decision": "Accept (Poster)", "meta_review": "This paper introduces a student-teacher method for learning from labels of varying quality (i.e. varying fidelity data). This is an interesting idea which shows promising results.\n\nSome further connections to various kinds of semi-supervised and multi-fidelity learning would strengthen the paper, although understandably it is not easy to cover the vast literature, which also spans different scientific domains. One reviewer had a concern about some design decisions that seemed ad-hoc, but at least the authors have intuitively and experimentally justified them.", "reviews": [{"review_id": "B1X0mzZCW-0", "review_text": "This paper suggests a simple yet effective approach for learning with weak supervision. This learning scenario involves two datasets, one with clean data (i.e., labeled by the true function) and one with noisy data, collected using a weak source of supervision. The suggested approach assumes a teacher and student networks, and builds the final representation incrementally, by taking into account the \"fidelity\" of the weak label when training the student at the final step. The fidelity score is given by the teacher, after being trained over the clean data, and it's used to build a cost-sensitive loss function for the students. The suggested method seems to work well on several document classification tasks. Overall, I liked the paper. I would like the authors to consider the following questions - - Over the last 10 years or so, many different frameworks for learning with weak supervision were suggested (e.g., indirect supervision, distant supervision, response-based, constraint-based, to name a few). First, I'd suggest acknowledging these works and discussing the differences to your work. Second - Is your approach applicable to these frameworks? It would be an interesting to compare to one of those methods (e.g., distant supervision for relation extraction using a knowledge base), and see if by incorporating fidelity score, results improve. - Can this approach be applied to semi-supervised learning? Is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self-training framework? - The paper emphasizes that the teacher uses the student's initial representation, when trained over the clean data. Is it clear that this step in needed? Can you add an additional variant of your framework when the fidelity score are computed by the teacher when trained from scratch? using different architecture than the student? - I went over the authors comments and I appreciate their efforts to help clarify the issues raised.", "rating": "7: Good paper, accept", "reply_text": "Q5- The paper emphasizes that the teacher uses the student 's initial representation when trained over the clean data . Is it clear that this step in needed ? Can you add an additional variant of your framework when the fidelity sores are computed by the teacher when trained from scratch ? using a different architecture than the student ? A5- In the current model , first the representation of the data is learned by the student using weakly annotated data , then , using the learned representation , we fit the teacher on the data with strong ( true ) labels . Providing the teacher with the learned representation by the student has three main reasons : First of all , it has been shown that we can learn effective representation of the data if we have a large quantity of data available , this can be either by learning the distribution of the data using unlabeled example , or learning representation of the data downstream of the main task using a large set of weakly labeled data . However , for many tasks using just a small amount of data with true labels , we will not be able to model the underlying distribution of the data . Since in FWL , the teacher is trained only on data with strong labels ( which is a small set ) , sharing the learned representation of the previous step alleviates this problem and the teacher can enjoy the learned knowledge from the large quantity of the weakly annotated data . In our setup , we make use of a Gaussian Process as the teacher . It is an interesting direction to search for meaningful kernels on structured data , i.e strings in our case . There exist some works to define such non-vectorial kernels that are designed by experts and are domain specific [ 2,3 ] . However , our goal here is to learn the representation along with solving the main classification task . Even though sme papers connect Gaussian process and deep neural networks , we are not aware of a reliable method for end to end training to learn the input features of GP better than the features learned by a neural network . So we do not learn the representation of the data as part of the step # 2 , but borrow it from step # 1 . Likewise , we do not learn the kernels of the GPs and only learn the vectorial representation of their inputs . As another possible advantage of the current setup , we let the teacher see the data through the student 's lens . This may in particular help the teacher , in step # 3 , to provide better annotation ( and confidence ) for the training of the student when the teacher is aware of the idea of the student about metric properties of the input space learned in step # 1 . Note that the input representation of the student is trained in the step # 3 and is not fully identical with that of the teacher which is kept fixed . We tested the case where the teacher used the input representation of the student in step # 3 but the accuracy dropped considerably . We ascribe this observation to the covariate shift in the input of a trained GP . We made these points more explicit in the revised version of the submission ( Section 2 , where we are explaining step # 2 ) . A variant of FWL would be to use one ( or more ) neural network ( s ) as the teacher [ 1 ] to be able to learn representation in step # 2 , but we believe it is necessary for the teacher and student to both agree on the metric space they see in the input . [ 1 ] Anonymous , Deep Neural Networks as Gaussian Processes , under submission at ICLR2018 , https : //openreview.net/forum ? id=B1EA-M-0Z [ 2 ] Eskin E , Weston J , Noble WS , Leslie CS . Mismatch string kernels for SVM protein classification . InAdvances in neural information processing systems 2003 ( pp.1441-1448 ) . [ 3 ] G\u00e4rtner T. A survey of kernels for structured data . ACM SIGKDD Explorations Newsletter . 2003 Jul 1 ; 5 ( 1 ) :49-58"}, {"review_id": "B1X0mzZCW-1", "review_text": "The problem of interest is to train deep neural network models with few labelled training samples. The specific assumption is there is a large pool of unlabelled data, and a heuristic function that can provide label annotations, possibly with varying levels of noises, to those unlabelled data. The adopted learning model is of a student/teacher framework as in privileged learning/knowledge distillation/model compression, and also machine teaching. The student (deep neural network) model will learn from both labelled and unlabelled training data with the labels provided by the teacher (Gaussian process) model. The teacher also supplies an uncertainty estimate to each predicted label. How about the heuristic function? This is used for learning initial feature representation of the student model. Crucially, the teacher model will also rely on these learned features. Labelled data and unlabelled data are therefore lie in the same dimensional space. Specific questions to be addressed: 1) Clustering of strongly-labelled data points. Thinking about the statement \u201ceach an expert on this specific region of data space\u201d, if this is the case, I am expecting a clustering for both strongly-labelled data points and weakly-labelled data points. Each teacher model is trained on a portion of strongly-labelled data, and will only predict similar weakly-labelled data. On a related remark, the nice side-effect is not right as it was emphasized that data points with a high-quality label will be limited. As well, GP models, are quite scalable nowadays (experiments with millions to billions of data points are available in recent NIPS/ICML papers, though, they are all rely on low dimensionality of the feature space for optimizing the inducing point locations). It will be informative to provide results with a single GP model. 2) From modifying learning rates to weighting samples. Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate, it is more \u201cintuitive\u201d to use it to modify the sampling procedure of mini-batches (akin to baseline #4); sample with higher probability data points with higher certainty. Here, experimental comparison with, for example, an SVM model that takes into account instance weighting will be informative, and a student model trained with logits (as in knowledge distillation/model compression). ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Regarding the point : \u201c I am expecting a clustering for both strongly-labelled data points and weakly-labelled data points \u201d : In FWL , during training , in step # 2 , we train multiple GPs as the teacher using only the data with \u201c strong labels \u201d which is a rather small set . In step # 3 , we go through both data with strong and weak labels and for each data point , we assign each point to a teacher based on the centroid of the teacher \u2019 s corresponding cluster . Therefore , each teacher predicts the new label in its territory . The predicted labels are almost the same as the original labels for the strongly labeled points and hopefully better labels for the weakly labeled data points . The confidence for the newly labeled point is also reported by its corresponding GP ( teacher ) . Clustering the weakly-labeled data points means having multiple student models as well . However , during the recall , we do not want to have multiple students which is computationally and space-wise prohibitive . Having a separate student corresponding to each teacher prevent the makes each student almost blind with respect to other clusters which is not desirable . The single student defined in our framework enables it to have a holistic view of the entire input space . We want our main task to be solved by a single student which is assumed expressive enough . The entire framework is designed to help this single student to settle on a better local optimum enjoying multiple teachers in the distillation framework . One important point here is that in FWL the teacher can be implemented by any predictor that can provide uncertainty for its prediction [ 1 ] . Even though , we resort to GP and a small strongly labeled dataset to capture this uncertainty , we should argue that the concept is applicable also when the uncertainty signal is provided from outside the dataset . -- -- - [ 1 ] Anonymous , Deep Neural Networks as Gaussian Processes , under submission at ICLR2018 , https : //openreview.net/forum ? id=B1EA-M-0Z Q2- From modifying learning rates to weighting samples . Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate , it is more \u201c intuitive \u201d to use it to modify the sampling procedure of mini-batches ( akin to baseline # 4 ) ; sample with higher probability data points with higher certainty . Here , experimental comparison with , for example , an SVM model that takes into account instance weighting will be informative , and a student model trained with logits ( as in knowledge distillation/model compression ) . A2- We think the suggested comparison is the case mainly when samples are seen by the model with the frequency proportional to the certainty of their label . We designed a new experiment in which , we kept the architectures of the student and the teacher and the procedure of the first two steps of the FWL fixed . We changed the step # 3 as follows : For each sample in D_ { sw } ( dataset consisting of strongly and weakly labeled data points which are relabeled by the teacher and each label is associated with a confidence ) , we normalize the confidence scores for all training samples and set the normalized score of each sample as its probability to be sampled . Afterwards , we train student model by sampling mini-batches from D_ { sw } with respect to the probabilities associated with each sample , without considering their confidence as a multiplicative factor for the learning rate . This means that more confident the teacher is about the generated label for each sample , the more chance that sample has to be seen by the student model . We have added a new subsection , \u201c 4.4 . From modifying the learning rates to weighted sampling \u201d , to the revised manuscript to report our observations . Based on the results , compared to the original FWL , the performance of FWL with sampling increases rapidly in the beginning but it slows down afterward . We have looked into the sampling procedure and noticed that the confidence scores provided by the teacher form a rather skewed distribution and there is a strong bias toward sampling from data points that are either in or closed to the points in the dataset with strong labels , as GP has less uncertainty around these points and the confidence scores are high . We observed that the performance of the FWL with sampling gets closer to the performance of FWL after many epochs , while FWL had already a long convergence . The skewness of the confidence distribution makes FWL with sampling to have a tendency for more exploitation than exploration , however , FWL has more chance to explore the input space , while it controls the effect of updates on the parameters for samples based on their merit . We believe FWL with sampling can be improved by having a better strategy for sampling from a skewed distribution or using approaches for active learning and selective sampling which is out of the scope of this paper ."}, {"review_id": "B1X0mzZCW-2", "review_text": "The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data. This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable. The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework. The authors evaluate their proposed methods on one toy problem and two real-world problems. The paper is well written, easy to follow, and have good experimental study. My main problem with the paper is the lack of enough motivation and justification for the proposed method; the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work. Here are some questions that comes to my mind: (1) Why first building a student model only using the weak data and why not all the data together to train the student model? To me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data? (2) What are the sensitivity of the procedure to how weakly the weak data are annotated (this could be studied using both toy example and real-world examples)? (3) The authors explicitly suggest using an unsupervised method (check Baseline no.1) to annotate data weakly? Why not learning the representation using an unsupervised learning method (unsupervised pre training)? This should be at least one of the baselines. (4) the idea of using surrogate labels to learn representation is also not new. One example work is \"Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks\". The authors didn't compare their method with this one.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q4- the idea of using surrogate labels to learn representation is also not new . One example work is `` Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks '' . The authors did n't compare their method with this one . A4- Thanks for pointing out this paper . The referred work is based on self-supervised feature learning in which the idea is to exploit different labelings that are freely available besides or within the data by defining a surrogate task which uses the intrinsic signals to learn better ( e.g.generic , robust , descriptive and invariant ) features . The learned features are then transferred to be used for a supervised task ( e.g.object classification or description matching ) . We argue that in FWL we do not learn representation through a proxy task . We learn the representation ( and pretrain the student model ) downstream of the main task , but with pseudo-labels ( noisy labels ) . Nonetheless , we can say that representation learning in step # 1 is solving a surrogate task of approximating the expert knowledge , for which a noisy supervision signal is provided by the weak annotator . In the response to the previous question , we have added a baseline for the sentiment classification task in which a surrogate task is used to learn the representation ( see A3 ) . Furthermore , we discussed the advantage of the current setup of the step # 1 , i.e.learning the representation downstream of a task same as the final target task that we want to solve but with lower accuracy in the labels . Here we again summarize them and add one more point : 1 . Using the main task with weak labels in step # 1 leads to a representation that complies better with the target task . 2.In the current setup of the step # 1 , we also pretrain the student model , so representation learning is actually part of student pre-training in a weakly supervised manner ( in A3 , we explained why this is needed ) . 3.In addition to the above points , the definition of the surrogate task in self-supervision depends on the problem to be solved . For instance , if the surrogate task is defined such that it yields features invariant to color , it can not be used to differentiate objects with different colors . However , in our setup the step # 1 is seen as a surrogate task is inherently in accordance with the main task ( in fact they are the same , but with different accuracy in the label space ) and we do not need to think about the suitable surrogate task for the feature learning phase . Looking to the FWL from the perspective of self-supervised feature learning is pretty interesting and valuable to mention . We have added this point ( Section 2 , where we are explaining step # 1 ) and the related papers ( in related work section ) to the revised version of the submission to include this point of view as well ."}], "0": {"review_id": "B1X0mzZCW-0", "review_text": "This paper suggests a simple yet effective approach for learning with weak supervision. This learning scenario involves two datasets, one with clean data (i.e., labeled by the true function) and one with noisy data, collected using a weak source of supervision. The suggested approach assumes a teacher and student networks, and builds the final representation incrementally, by taking into account the \"fidelity\" of the weak label when training the student at the final step. The fidelity score is given by the teacher, after being trained over the clean data, and it's used to build a cost-sensitive loss function for the students. The suggested method seems to work well on several document classification tasks. Overall, I liked the paper. I would like the authors to consider the following questions - - Over the last 10 years or so, many different frameworks for learning with weak supervision were suggested (e.g., indirect supervision, distant supervision, response-based, constraint-based, to name a few). First, I'd suggest acknowledging these works and discussing the differences to your work. Second - Is your approach applicable to these frameworks? It would be an interesting to compare to one of those methods (e.g., distant supervision for relation extraction using a knowledge base), and see if by incorporating fidelity score, results improve. - Can this approach be applied to semi-supervised learning? Is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self-training framework? - The paper emphasizes that the teacher uses the student's initial representation, when trained over the clean data. Is it clear that this step in needed? Can you add an additional variant of your framework when the fidelity score are computed by the teacher when trained from scratch? using different architecture than the student? - I went over the authors comments and I appreciate their efforts to help clarify the issues raised.", "rating": "7: Good paper, accept", "reply_text": "Q5- The paper emphasizes that the teacher uses the student 's initial representation when trained over the clean data . Is it clear that this step in needed ? Can you add an additional variant of your framework when the fidelity sores are computed by the teacher when trained from scratch ? using a different architecture than the student ? A5- In the current model , first the representation of the data is learned by the student using weakly annotated data , then , using the learned representation , we fit the teacher on the data with strong ( true ) labels . Providing the teacher with the learned representation by the student has three main reasons : First of all , it has been shown that we can learn effective representation of the data if we have a large quantity of data available , this can be either by learning the distribution of the data using unlabeled example , or learning representation of the data downstream of the main task using a large set of weakly labeled data . However , for many tasks using just a small amount of data with true labels , we will not be able to model the underlying distribution of the data . Since in FWL , the teacher is trained only on data with strong labels ( which is a small set ) , sharing the learned representation of the previous step alleviates this problem and the teacher can enjoy the learned knowledge from the large quantity of the weakly annotated data . In our setup , we make use of a Gaussian Process as the teacher . It is an interesting direction to search for meaningful kernels on structured data , i.e strings in our case . There exist some works to define such non-vectorial kernels that are designed by experts and are domain specific [ 2,3 ] . However , our goal here is to learn the representation along with solving the main classification task . Even though sme papers connect Gaussian process and deep neural networks , we are not aware of a reliable method for end to end training to learn the input features of GP better than the features learned by a neural network . So we do not learn the representation of the data as part of the step # 2 , but borrow it from step # 1 . Likewise , we do not learn the kernels of the GPs and only learn the vectorial representation of their inputs . As another possible advantage of the current setup , we let the teacher see the data through the student 's lens . This may in particular help the teacher , in step # 3 , to provide better annotation ( and confidence ) for the training of the student when the teacher is aware of the idea of the student about metric properties of the input space learned in step # 1 . Note that the input representation of the student is trained in the step # 3 and is not fully identical with that of the teacher which is kept fixed . We tested the case where the teacher used the input representation of the student in step # 3 but the accuracy dropped considerably . We ascribe this observation to the covariate shift in the input of a trained GP . We made these points more explicit in the revised version of the submission ( Section 2 , where we are explaining step # 2 ) . A variant of FWL would be to use one ( or more ) neural network ( s ) as the teacher [ 1 ] to be able to learn representation in step # 2 , but we believe it is necessary for the teacher and student to both agree on the metric space they see in the input . [ 1 ] Anonymous , Deep Neural Networks as Gaussian Processes , under submission at ICLR2018 , https : //openreview.net/forum ? id=B1EA-M-0Z [ 2 ] Eskin E , Weston J , Noble WS , Leslie CS . Mismatch string kernels for SVM protein classification . InAdvances in neural information processing systems 2003 ( pp.1441-1448 ) . [ 3 ] G\u00e4rtner T. A survey of kernels for structured data . ACM SIGKDD Explorations Newsletter . 2003 Jul 1 ; 5 ( 1 ) :49-58"}, "1": {"review_id": "B1X0mzZCW-1", "review_text": "The problem of interest is to train deep neural network models with few labelled training samples. The specific assumption is there is a large pool of unlabelled data, and a heuristic function that can provide label annotations, possibly with varying levels of noises, to those unlabelled data. The adopted learning model is of a student/teacher framework as in privileged learning/knowledge distillation/model compression, and also machine teaching. The student (deep neural network) model will learn from both labelled and unlabelled training data with the labels provided by the teacher (Gaussian process) model. The teacher also supplies an uncertainty estimate to each predicted label. How about the heuristic function? This is used for learning initial feature representation of the student model. Crucially, the teacher model will also rely on these learned features. Labelled data and unlabelled data are therefore lie in the same dimensional space. Specific questions to be addressed: 1) Clustering of strongly-labelled data points. Thinking about the statement \u201ceach an expert on this specific region of data space\u201d, if this is the case, I am expecting a clustering for both strongly-labelled data points and weakly-labelled data points. Each teacher model is trained on a portion of strongly-labelled data, and will only predict similar weakly-labelled data. On a related remark, the nice side-effect is not right as it was emphasized that data points with a high-quality label will be limited. As well, GP models, are quite scalable nowadays (experiments with millions to billions of data points are available in recent NIPS/ICML papers, though, they are all rely on low dimensionality of the feature space for optimizing the inducing point locations). It will be informative to provide results with a single GP model. 2) From modifying learning rates to weighting samples. Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate, it is more \u201cintuitive\u201d to use it to modify the sampling procedure of mini-batches (akin to baseline #4); sample with higher probability data points with higher certainty. Here, experimental comparison with, for example, an SVM model that takes into account instance weighting will be informative, and a student model trained with logits (as in knowledge distillation/model compression). ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Regarding the point : \u201c I am expecting a clustering for both strongly-labelled data points and weakly-labelled data points \u201d : In FWL , during training , in step # 2 , we train multiple GPs as the teacher using only the data with \u201c strong labels \u201d which is a rather small set . In step # 3 , we go through both data with strong and weak labels and for each data point , we assign each point to a teacher based on the centroid of the teacher \u2019 s corresponding cluster . Therefore , each teacher predicts the new label in its territory . The predicted labels are almost the same as the original labels for the strongly labeled points and hopefully better labels for the weakly labeled data points . The confidence for the newly labeled point is also reported by its corresponding GP ( teacher ) . Clustering the weakly-labeled data points means having multiple student models as well . However , during the recall , we do not want to have multiple students which is computationally and space-wise prohibitive . Having a separate student corresponding to each teacher prevent the makes each student almost blind with respect to other clusters which is not desirable . The single student defined in our framework enables it to have a holistic view of the entire input space . We want our main task to be solved by a single student which is assumed expressive enough . The entire framework is designed to help this single student to settle on a better local optimum enjoying multiple teachers in the distillation framework . One important point here is that in FWL the teacher can be implemented by any predictor that can provide uncertainty for its prediction [ 1 ] . Even though , we resort to GP and a small strongly labeled dataset to capture this uncertainty , we should argue that the concept is applicable also when the uncertainty signal is provided from outside the dataset . -- -- - [ 1 ] Anonymous , Deep Neural Networks as Gaussian Processes , under submission at ICLR2018 , https : //openreview.net/forum ? id=B1EA-M-0Z Q2- From modifying learning rates to weighting samples . Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate , it is more \u201c intuitive \u201d to use it to modify the sampling procedure of mini-batches ( akin to baseline # 4 ) ; sample with higher probability data points with higher certainty . Here , experimental comparison with , for example , an SVM model that takes into account instance weighting will be informative , and a student model trained with logits ( as in knowledge distillation/model compression ) . A2- We think the suggested comparison is the case mainly when samples are seen by the model with the frequency proportional to the certainty of their label . We designed a new experiment in which , we kept the architectures of the student and the teacher and the procedure of the first two steps of the FWL fixed . We changed the step # 3 as follows : For each sample in D_ { sw } ( dataset consisting of strongly and weakly labeled data points which are relabeled by the teacher and each label is associated with a confidence ) , we normalize the confidence scores for all training samples and set the normalized score of each sample as its probability to be sampled . Afterwards , we train student model by sampling mini-batches from D_ { sw } with respect to the probabilities associated with each sample , without considering their confidence as a multiplicative factor for the learning rate . This means that more confident the teacher is about the generated label for each sample , the more chance that sample has to be seen by the student model . We have added a new subsection , \u201c 4.4 . From modifying the learning rates to weighted sampling \u201d , to the revised manuscript to report our observations . Based on the results , compared to the original FWL , the performance of FWL with sampling increases rapidly in the beginning but it slows down afterward . We have looked into the sampling procedure and noticed that the confidence scores provided by the teacher form a rather skewed distribution and there is a strong bias toward sampling from data points that are either in or closed to the points in the dataset with strong labels , as GP has less uncertainty around these points and the confidence scores are high . We observed that the performance of the FWL with sampling gets closer to the performance of FWL after many epochs , while FWL had already a long convergence . The skewness of the confidence distribution makes FWL with sampling to have a tendency for more exploitation than exploration , however , FWL has more chance to explore the input space , while it controls the effect of updates on the parameters for samples based on their merit . We believe FWL with sampling can be improved by having a better strategy for sampling from a skewed distribution or using approaches for active learning and selective sampling which is out of the scope of this paper ."}, "2": {"review_id": "B1X0mzZCW-2", "review_text": "The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data. This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable. The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework. The authors evaluate their proposed methods on one toy problem and two real-world problems. The paper is well written, easy to follow, and have good experimental study. My main problem with the paper is the lack of enough motivation and justification for the proposed method; the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work. Here are some questions that comes to my mind: (1) Why first building a student model only using the weak data and why not all the data together to train the student model? To me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data? (2) What are the sensitivity of the procedure to how weakly the weak data are annotated (this could be studied using both toy example and real-world examples)? (3) The authors explicitly suggest using an unsupervised method (check Baseline no.1) to annotate data weakly? Why not learning the representation using an unsupervised learning method (unsupervised pre training)? This should be at least one of the baselines. (4) the idea of using surrogate labels to learn representation is also not new. One example work is \"Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks\". The authors didn't compare their method with this one.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q4- the idea of using surrogate labels to learn representation is also not new . One example work is `` Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks '' . The authors did n't compare their method with this one . A4- Thanks for pointing out this paper . The referred work is based on self-supervised feature learning in which the idea is to exploit different labelings that are freely available besides or within the data by defining a surrogate task which uses the intrinsic signals to learn better ( e.g.generic , robust , descriptive and invariant ) features . The learned features are then transferred to be used for a supervised task ( e.g.object classification or description matching ) . We argue that in FWL we do not learn representation through a proxy task . We learn the representation ( and pretrain the student model ) downstream of the main task , but with pseudo-labels ( noisy labels ) . Nonetheless , we can say that representation learning in step # 1 is solving a surrogate task of approximating the expert knowledge , for which a noisy supervision signal is provided by the weak annotator . In the response to the previous question , we have added a baseline for the sentiment classification task in which a surrogate task is used to learn the representation ( see A3 ) . Furthermore , we discussed the advantage of the current setup of the step # 1 , i.e.learning the representation downstream of a task same as the final target task that we want to solve but with lower accuracy in the labels . Here we again summarize them and add one more point : 1 . Using the main task with weak labels in step # 1 leads to a representation that complies better with the target task . 2.In the current setup of the step # 1 , we also pretrain the student model , so representation learning is actually part of student pre-training in a weakly supervised manner ( in A3 , we explained why this is needed ) . 3.In addition to the above points , the definition of the surrogate task in self-supervision depends on the problem to be solved . For instance , if the surrogate task is defined such that it yields features invariant to color , it can not be used to differentiate objects with different colors . However , in our setup the step # 1 is seen as a surrogate task is inherently in accordance with the main task ( in fact they are the same , but with different accuracy in the label space ) and we do not need to think about the suitable surrogate task for the feature learning phase . Looking to the FWL from the perspective of self-supervised feature learning is pretty interesting and valuable to mention . We have added this point ( Section 2 , where we are explaining step # 1 ) and the related papers ( in related work section ) to the revised version of the submission to include this point of view as well ."}}