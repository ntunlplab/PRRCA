{"year": "2019", "forum": "rkVOXhAqY7", "title": "The Conditional Entropy Bottleneck", "decision": "Reject", "meta_review": "This paper proposes a criterion for representation learning, minimum necessary information, which states that for a task defined by some joint probability distribution P(X,Y) and the goal of (for example) predicting Y from X, a learned representation of X, denoted Z, should satisfy the equality I(X;Y) = I(X;Z) = I(Y;Z). The authors then propose an objective function, the conditional entropy bottleneck (CEB), to ensure that a learned representation satisfies the minimum necessary information criterion, and a variational approximation to the conditional entropy bottleneck that can be parameterized using deep networks and optimized with standard methods such as stochastic gradient descent. The authors also relate the conditional entropy bottleneck to the information bottleneck Lagrangian proposed by Tishby, showing that the CEB corresponds to the information bottleneck with \u03b2 = 0.5. An important contribution of this work is that it gives a theoretical justification for selecting a specific value of \u03b2 rather than testing multiple values. Experiments on Fashion-MNIST show that, in comparison to a deterministic classifier and to variational information bottleneck models with \u03b2 in {0.01, 0.1, 0.5}, the CEB model achieves good accuracy and calibration, is competitive at detecting out-of-distribution inputs, and is more resistant to white-box adversarial attacks. Another experiment demonstrates that a model trained with the CEB criterion is *unable* to memorize a randomly labeled version of Fashion-MNIST. There was a strong difference of opinion between the reviewers on this paper. One reviewer (R1) dismissed the work as trivial. The authors rebutted this claim in their response and revision, and R1 failed to participate in the discussion, so the AC strongly discounted this review. The other two reviewers had some concerns about the paper, most of which were addressed by the revision. But, crucially, some concerns still remain. R4 would like more theoretical rigor in the paper, while R2 would like a direct comparison against MINE and CPC. In the end, the AC thinks that this paper needs just a bit more work to address these concerns. The authors are encouraged to revise this work and submit it to another machine learning venue.", "reviews": [{"review_id": "rkVOXhAqY7-0", "review_text": "[UPDATE] I find the revised version of the paper much clearer and streamlined than the originally submitted one, and am mostly content with the authors reply to my comments. However, I still think the the work would highly benefit from a non-heuristic justification of its approach and some theoretic guarantees on the performance of the proposed framework (especially, in which regimes it is beneficial and when it is not). Also, I still find the presentation of experimental results too convoluted to give a clear and comprehensive picture of how this methods compares to the competition, when is it better, when is it worse, do the observations/claim generalize to other task, and which are the right competing methods to be considering. I think the paper can still be improved on this aspect as well. As I find the idea (once it was clarified) generally interesting, I will raise my score to 6. ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ The paper proposes an objective function for learning representations, termed the conditional entropy bottleneck (CEB). Variational bounds on the objective function are derived and used to train classifiers according to the CEB and compare the results to those attained by competing methods. Robustness and adversarial examples detection of CEB are emphasized. My major comments are as follows: 1) The authors base their 'information-theoretic' reasoning on the set-theoretic structure of Shannon\u2019s information measures. It is noteworthy that when dealing with more than 2 random variables, e.g., when going from the twofold I(X;Y) to the threefold I(X;Y;Z), this theory has major issues. In particular, there are simple (and natural) examples for which I(X;Y;Z) is negative. The paper presents an information-theoretic heuristic/intuitive explanation for their CEB construction based on this framework. No proofs backing up any of the claims of performance/robustness in the paper are given. Unfortunately, with such counter-intuitive issues of the underlying theory, a heurisitc explanation that motivates the proposed construction is not convincing. Simulations are presented to justify the construction but whether the claimed properties hold for a wide variety of setups remain unclear. 2) Appendix A is referred to early on for explaining the minimal necessary information (MNI), but it is very unclear. What is the claim of this Appendix? Is there a claim? It's just seems like a convoluted and long explanation of mutual information. Even more so, this explanation is inaccurate. For instance, the authors refer to the mutual information as a 'minimal sufficient statistic' but it is not. For a pair of random variables (X,Y), a sufficient statistic, say, for X given Y is a function f of Y such X-f(Y)-Y forms a Markov chain. Specifically, f(Y) is another random variable. The mutual information I(X;Y) is just a number. I have multiple guesses on what the authors' meaning could be here, but was unable to figure it out from the text. One option, which is a pretty standard way to define sufficient statistic though mutual information is as a function f such that I(X;Y|f(Y))=0. Such an f is a sufficient statistic since the zero mutual information term is equivalent to the Markov chain X-f(Y)-Y from before. Is that what the authors mean..? 3) The Z_X variable introduced in Section 3 in inspired by the IB framework (footnote 2). If I understand correctly, this means that in many applications, Z_X is specified by a classifier of X wrt the label Y. My question is whether for a fixed set of system parameters, Z_X is a deterministic function of X? If this Z_X play the role of the sufficient statistics I've referred to in my previous comment, then it should be just a function of X. However, if Z_X=f(X) for a deterministic function f, then the CEB from Equation (3) is vacuous for many interesting cases of (X,Y). For instance, if X is a continuous random variable and Z_X=f(X) is continuous as well, then I(X;Z_X|Y)=h(Z_X|Y)-h(Z_X|X,Y) where h is the differential entropy and the subtracted terms equals -\\infty by definition (see Section 8.3 of (Cover & Thomas, 2006). Consequently, the mutual information and the CEB objective are infinite. If Z_X=f(X) is a mixed random variable (e.g., can be obtain from a ReLU neural network), then the same happens. Other cases of interest, such as discrete X and f being an injective mapping of the set of X values, are also problematic. For details of such problem associated with IB type terms see: [1] R. A. Amjad and B. C. Geiger 'Learning Representations for Neural Network-Based Classification Using the Information Bottleneck Principle', 2018 (https://arxiv.org/abs/1802.09766). Can the authors account for that? 4) The other two reviews addressed the missing accounts for past literature. I agree on this point and will keep track of the authors' responses. I will not comment on that again. Beyond these specific issue, they text is very wordy and confusing at times. If some mathematical justification/modeling was employed the proposed framework might have been easier to accept. The long heuristic explanations employed at the moment do not suffice for this reviewer. Unless the authors are able to provide clarification of all the above points and properly place their work in relation to past literature I cannot recommend acceptance. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for taking the time to write a detailed review . We will address your concerns in turn . Negativity of I ( X ; Y ; Z ) You are correct that I ( X ; Y ; Z ) can be negative in general . However , it can not be negative in the representation learning setting that we are describing here -- the Markov chain Z < - X - > Y does not permit it . The triplet information , I ( X ; Y ; Z ) may be defined as follows : I ( X ; Y ; Z ) = I ( X ; Z ) - I ( X ; Z|Y ) But we already know that I ( X ; Z|Y ) = I ( X ; Z ) - I ( Y ; Z ) ( Equation 2 ) due to our Markov chain Z < - X < - > Y , so in our case we have : I ( X ; Y ; Z ) = I ( X ; Z ) - I ( X ; Z ) + I ( Y ; Z ) = I ( Y ; Z ) which we also know is non-negative , completing our proof . Minimum Necessary Information Our upcoming revisions clarify the relationship between MNI and minimal sufficient statistics and update the discussion of the MNI with the following : Why MNI ? In this work we do not attempt to give a formal proof that CEB representations learn the optimal information about the observed data ( and certainly the variational form of the objective will prevent that from happening in general cases ) . However , the MNI is motivated by the following simple observations : If I ( X ; Z ) < I ( X ; Y ) , then we have thrown out relevant information in X for predicting Y . If I ( X ; Z ) > I ( X ; Y ) , then we are including information in X that is not useful for predicting Y . Thus targeting I ( X ; Z ) = I ( X ; Y ) is the `` correct '' amount of information , which is one of the equalities required in order to satisfy the MNI criterion . Representations and Finiteness of Mutual Information Z_X is not a deterministic function of X in either IB or CEB , it is a stochastic representation of X given by an encoder e ( z_X|x ) . Using a stochastic encoder means that the stated concerns of infinite entropy are not applicable in either objective -- the conditions for infinite mutual information given in Amjad and Geiger ( 2018 ) do not apply . In practice , ( using continuous representations ) , we did not encounter mutual information terms that diverged to infinity , although certainly it is possible to make modeling and data choices that make it more or less likely that there will be numerical instabilities . This is not a flaw specific to CEB or VIB , however , and we found numerical instability to be almost non-existent across a wide variety of modeling and architectural choices for both variational objectives . [ 1 ] R. A. Amjad and B. C. Geiger 'Learning Representations for Neural Network-Based Classification Using the Information Bottleneck Principle ' , 2018"}, {"review_id": "rkVOXhAqY7-1", "review_text": "Update: see comments \"On revisions\" below. This paper essentially introduces a label-dependent regularization to the VIB framework, matching the encoder distribution of one computed from labels. The authors show good performance in generalization, such that their approach is relatively robust in a number of tasks, such as adversarial defense. The idea I think is generally good, but there are several problems with this work. First, there has been recent advances in mutual information estimation, first found in [1]. This is an important departure from the usual variational approximations used in VIB. You need to compare to this baseline, as it was shown that it outperforms VIB in a similar classification task as presented in your work. Second, far too much space is used to lay out some fairly basic formalism with respect to mutual information, conditional entropy, etc. It would be nice, for example, to have an algorithm to make the learning objective more clear. Overall, I don't feel the content justifies the length. Third, I have some concerns about the significance of this work. They introduce essentially a label-dependent \u201cbackwards encoder\u201d to provide samples for the KL term normally found in VIB. The justification is that we need the bottleneck term to improve generalization and the backwards encoder term is supposed to keep the representation relevant to labels. One could have used an approach like MINE, doing min information for the bottleneck and max info for the labels. In addition, much work has been done on learning representations that generalize using mutual information (maximizing instead of minimizing) [2, 3, 4, 5] along with some sort of term to improve \"relevance\", and this work seems to ignore / not be aware of this work. Overall I could see some potential in this paper being published, as I think the approach is sensible, but it's not presented in the proper context of past work. [1] Belghazi, I., Baratin, A., Rajeswar, S., Courville, A., Bengio, Y., & Hjelm, R. D. (2018). MINE: mutual information neural estimation. International Conference for Machine Learning, 2018. [2] Gomes, R., Krause, A., and Perona, P. Discriminative clustering by regularized information maximization. In NIPS, 2010. [3] Hu, W., Miyato, T., Tokui, S., Matsumoto, E., and Sugiyama, M. Learning discrete representations via information maximizing self-augmented training. In ICML, 2017. [4] Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Trischler, A., & Bengio, Y. (2018). Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670. [5] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" arXiv preprint arXiv:1807.03748 (2018).", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review , and for your thorough search of the literature on uses of the mutual information in objective functions . We were aware of the work you mentioned . The high order bit is that you are correct -- any estimator of the mutual information could be used in conjunction with the CEB objective , assuming the inequalities of the estimator correspond correctly to the direction the mutual information estimate needs to be optimized . We chose to explore variational bounds in this paper because they are simple , tractable , and well-understood , but we mention in Section 3 that other approaches are possible . We will be more specific about that in our revisions . There is quite a bit of care that needs to be taken to understand these mutual information estimators , but both MINE and CPC may be used to optimize the lower bound on I ( Z ; Y ) . For reasons we describe in more detail below , we don \u2019 t think that doing so would substantially change the results in these particular experiments , but when applying CEB to tasks other than classification , both options are worth considering , in addition to the variational approach we present . In other words , the CEB objective involves minimization and maximization of mutual information terms , and any correctly bounded method to estimate mutual information could be used to optimize the components of the general CEB objective . Our focus on the variational approach in the paper is merely convenient , rather than essential . Below we discuss MINE in more detail , but first we would like to clarify that all five of the papers you mention are proposing objectives that are not consistent with the MNI criterion if used by themselves , rather than as part of the optimization approach for an implementation of CEB . They are all focusing on maximizing estimations or lower bounds of either I ( X ; Z ) or I ( Y ; Z ) . As we point out in Section 3 , maximizing I ( Y ; Z ) is necessary but not sufficient for achieving the Minimum Necessary Information , and maximizing I ( X ; Z ) is fundamentally inconsistent with MNI . Additionally , it is worth pointing out that standard maximum likelihood estimation training also maximizes I ( Y ; Z ) in deep networks , where Z can be taken as any intermediate layer of the network -- minimizing the cross entropy is the same as minimizing the H ( Y|Z ) term in the paper , which maximizes a lower bound on I ( Y ; Z ) - H ( Y ) , and H ( Y ) is constant with respect to the parameters . Thus , we expect that all five of these approaches ( when used by themselves ) still suffer from the excess of information in the representation that we hypothesize MLE to suffer from ( whether the representation is explicit , as in VIB , Gomes et al. , 2010 , and Hjelm et al. , 2018 , or implicit , as in the other 3 papers you mention ) . In contrast , CEB is maximizing I ( Y ; Z ) while also minimizing I ( X ; Z|Y ) , which forces optimization to get the trained model as close as it can to the MNI goal state of I ( X ; Z ) = I ( Y ; Z ) = I ( X ; Y ) . To summarize the point we are making here , we do not consider unconstrained maximization of the mutual information between an observed variable and a representation variable to be a desirable property by itself , and we show that doing so is inconsistent with the MNI criterion . Such techniques can only be made compatible with MNI when used to optimize a complete MNI-compatible objective function , such as CEB ."}, {"review_id": "rkVOXhAqY7-2", "review_text": "This paper wants to discuss a new objective function, which the authors dub \"Conditional Entropy Bottleneck\" (CEB), motivated by learning better latent representations. However, as far as I can tell, the objective functions already exists in the one-parameter family of Information Bottleneck (IB) of Tishby, Pereira, and Bialek. The author seems to realize this in Appendix B, but calls it \"a somewhat surprising theoretical result\". However, if we express IB as max I(Z;Y) - beta I(Z;X), see (19), and then flip signs and take the max to the min, we get min beta I(Z;X) - I(Z;Y). Taking beta = 1/2, multiplying through by 2, and writing I(X;Z) - I(Y Z) = I(X;Z|Y), we find CIB. Unfortunately, I fail to see how this is surprising or different. A difference only arises when using a variational approximation to IB. The authors compare to the Variational Information Bottleneck (VIB) of Alemi, Fischer, Dillon, and Murphy (arXiv:1612.00410), which requires a classifier, an encoder, and a marginal posterior over the latents. Here, instead of the marginal posterior, they learn a backwards encoder from labels to latents. This difference arises because the IB objective has two terms of opposite sign, and we can group them into positive definite terms in different ways, creating different bounds. Perhaps this grouping leads to a better variational bound? If so, that's only a point about the variational method employed by Alemi et al., and not a separate objective. As this seems to be the main contribution of the paper, this point needs to be explained more carefully and in more detail. For instance, it seems worth pointing out, in the discrete case, that the marginal posterior |Z| values to estimate, and the backwards encoder has |Z| x |Y| -- suggesting this is a possibly a much harder learning problem. If so, there should be a compelling benefit for using this approximation and not the other one. In summary, the authors are not really clear about what they are doing and how it relates to IB. Furthermore, the need for this specific choice in IB parameter space is not made clear, nor do the experimental results giving a compelling need. (The experimental results are also not at all clearly presented or explained.) Therefore, I don't think this paper satisfies the quality, clarity, originality, or significance criteria for ICLR.", "rating": "2: Strong rejection", "reply_text": "We appreciate that you read our paper closely , and address your concerns in detail below . Surprise The surprise of the result relating CEB to IB so simply comes from two things . First , the fact that there is a single value of beta for IB and VIB that achieves the MNI-optimal information , so long as the model , optimizer , etc , is capable of capturing that amount of information . This is surprising because the analysis in Alemi et al . ( 2018 ) naively applied to the VIB case would assume that sweeping beta would be necessary to find the optimal information even if you knew a priori what that amount of information was . We discuss this point in the appendix to some extent , but will clarify that discussion in our revisions . Note that a similar and more directly-applicable Pareto-optimal frontier is described in Strauss and Schwab ( 2017 ) , and that work also does not point out that beta = 1/2 would result in a learned representation where I ( X ; Z ) = I ( Y ; Z ) = I ( X ; Y ) . Second is a point that we decided not to make in the version we submitted , but that we can add in revision . Tishby et al.say two things in many of the IB papers quite clearly . In Tishby et al . ( 2015 ) , the authors state : \u201c The information bottleneck ( IB ) method was introduced as an information theoretic principle for extracting relevant information that an input random variable X \u2208 X contains about an output random variable Y \u2208 Y . Given their joint distribution p ( X , Y ) , the relevant information is defined as the mutual information I ( X ; Y ) , where we assume statistical dependence between X and Y . In this case , Y implicitly determines the relevant and irrelevant features in X . An optimal representation of X would capture the relevant features , and compress X by dismissing the irrelevant parts which do not contribute to the prediction of Y. \u201d But in Tishby et al . ( 2000 ) , the authors also state : \u201c ... there is a tradeoff between compressing the representation and preserving meaningful information , and there is no single right solution for the tradeoff. \u201d In other words , Tishby et al.recognize that the information measured by I ( X ; Y ) corresponds to the optimal representation , but they do not quite know how to find it . Instead , the IB approach relies on sweeping beta and cross validation . Thus , the surprise we are describing isn \u2019 t the trivial arithmetic relating CEB to IB once you have seen both objectives . Instead , the surprise is that you _can_ learn a representation with the optimal amount of information defined by the observed data , I ( X ; Y ) without having to know I ( X ; Y ) ahead of time . This shows that sweeping beta is unnecessary if you believe ( as Tishby appears to believe , and as we certainly believe ) that I ( X ; Y ) is the correct amount of information to retain in your representation . The information bottleneck was presented 19 years ago , and so far as we know , no-one has previously proposed a way to learn a representation that doesn \u2019 t require you to guess what I ( X ; Y ) might be . Why MNI ? In this work we do not attempt to give a formal proof that CEB representations learn the optimal information about the observed data ( and certainly the variational form of the objective will prevent that from happening in general cases ) . However , the MNI is motivated by the following simple observations : If I ( X ; Z ) < I ( X ; Y ) , then we have thrown out relevant information in X for predicting Y . If I ( X ; Z ) > I ( X ; Y ) , then we are including information in X that is not useful for predicting Y . Thus targeting I ( X ; Z ) = I ( X ; Y ) is the `` correct '' amount of information , which is one of the equalities required in order to satisfy the MNI criterion . Relation between general forms of CEB and IB Yes , CEB is a special case of IB with beta = 1/2 . However , we are the first to show that the IB Lagrangian with beta = 1/2 targets the MNI . This is a significant result that was not previously highlighted in any of the literature on IB ."}], "0": {"review_id": "rkVOXhAqY7-0", "review_text": "[UPDATE] I find the revised version of the paper much clearer and streamlined than the originally submitted one, and am mostly content with the authors reply to my comments. However, I still think the the work would highly benefit from a non-heuristic justification of its approach and some theoretic guarantees on the performance of the proposed framework (especially, in which regimes it is beneficial and when it is not). Also, I still find the presentation of experimental results too convoluted to give a clear and comprehensive picture of how this methods compares to the competition, when is it better, when is it worse, do the observations/claim generalize to other task, and which are the right competing methods to be considering. I think the paper can still be improved on this aspect as well. As I find the idea (once it was clarified) generally interesting, I will raise my score to 6. ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ The paper proposes an objective function for learning representations, termed the conditional entropy bottleneck (CEB). Variational bounds on the objective function are derived and used to train classifiers according to the CEB and compare the results to those attained by competing methods. Robustness and adversarial examples detection of CEB are emphasized. My major comments are as follows: 1) The authors base their 'information-theoretic' reasoning on the set-theoretic structure of Shannon\u2019s information measures. It is noteworthy that when dealing with more than 2 random variables, e.g., when going from the twofold I(X;Y) to the threefold I(X;Y;Z), this theory has major issues. In particular, there are simple (and natural) examples for which I(X;Y;Z) is negative. The paper presents an information-theoretic heuristic/intuitive explanation for their CEB construction based on this framework. No proofs backing up any of the claims of performance/robustness in the paper are given. Unfortunately, with such counter-intuitive issues of the underlying theory, a heurisitc explanation that motivates the proposed construction is not convincing. Simulations are presented to justify the construction but whether the claimed properties hold for a wide variety of setups remain unclear. 2) Appendix A is referred to early on for explaining the minimal necessary information (MNI), but it is very unclear. What is the claim of this Appendix? Is there a claim? It's just seems like a convoluted and long explanation of mutual information. Even more so, this explanation is inaccurate. For instance, the authors refer to the mutual information as a 'minimal sufficient statistic' but it is not. For a pair of random variables (X,Y), a sufficient statistic, say, for X given Y is a function f of Y such X-f(Y)-Y forms a Markov chain. Specifically, f(Y) is another random variable. The mutual information I(X;Y) is just a number. I have multiple guesses on what the authors' meaning could be here, but was unable to figure it out from the text. One option, which is a pretty standard way to define sufficient statistic though mutual information is as a function f such that I(X;Y|f(Y))=0. Such an f is a sufficient statistic since the zero mutual information term is equivalent to the Markov chain X-f(Y)-Y from before. Is that what the authors mean..? 3) The Z_X variable introduced in Section 3 in inspired by the IB framework (footnote 2). If I understand correctly, this means that in many applications, Z_X is specified by a classifier of X wrt the label Y. My question is whether for a fixed set of system parameters, Z_X is a deterministic function of X? If this Z_X play the role of the sufficient statistics I've referred to in my previous comment, then it should be just a function of X. However, if Z_X=f(X) for a deterministic function f, then the CEB from Equation (3) is vacuous for many interesting cases of (X,Y). For instance, if X is a continuous random variable and Z_X=f(X) is continuous as well, then I(X;Z_X|Y)=h(Z_X|Y)-h(Z_X|X,Y) where h is the differential entropy and the subtracted terms equals -\\infty by definition (see Section 8.3 of (Cover & Thomas, 2006). Consequently, the mutual information and the CEB objective are infinite. If Z_X=f(X) is a mixed random variable (e.g., can be obtain from a ReLU neural network), then the same happens. Other cases of interest, such as discrete X and f being an injective mapping of the set of X values, are also problematic. For details of such problem associated with IB type terms see: [1] R. A. Amjad and B. C. Geiger 'Learning Representations for Neural Network-Based Classification Using the Information Bottleneck Principle', 2018 (https://arxiv.org/abs/1802.09766). Can the authors account for that? 4) The other two reviews addressed the missing accounts for past literature. I agree on this point and will keep track of the authors' responses. I will not comment on that again. Beyond these specific issue, they text is very wordy and confusing at times. If some mathematical justification/modeling was employed the proposed framework might have been easier to accept. The long heuristic explanations employed at the moment do not suffice for this reviewer. Unless the authors are able to provide clarification of all the above points and properly place their work in relation to past literature I cannot recommend acceptance. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for taking the time to write a detailed review . We will address your concerns in turn . Negativity of I ( X ; Y ; Z ) You are correct that I ( X ; Y ; Z ) can be negative in general . However , it can not be negative in the representation learning setting that we are describing here -- the Markov chain Z < - X - > Y does not permit it . The triplet information , I ( X ; Y ; Z ) may be defined as follows : I ( X ; Y ; Z ) = I ( X ; Z ) - I ( X ; Z|Y ) But we already know that I ( X ; Z|Y ) = I ( X ; Z ) - I ( Y ; Z ) ( Equation 2 ) due to our Markov chain Z < - X < - > Y , so in our case we have : I ( X ; Y ; Z ) = I ( X ; Z ) - I ( X ; Z ) + I ( Y ; Z ) = I ( Y ; Z ) which we also know is non-negative , completing our proof . Minimum Necessary Information Our upcoming revisions clarify the relationship between MNI and minimal sufficient statistics and update the discussion of the MNI with the following : Why MNI ? In this work we do not attempt to give a formal proof that CEB representations learn the optimal information about the observed data ( and certainly the variational form of the objective will prevent that from happening in general cases ) . However , the MNI is motivated by the following simple observations : If I ( X ; Z ) < I ( X ; Y ) , then we have thrown out relevant information in X for predicting Y . If I ( X ; Z ) > I ( X ; Y ) , then we are including information in X that is not useful for predicting Y . Thus targeting I ( X ; Z ) = I ( X ; Y ) is the `` correct '' amount of information , which is one of the equalities required in order to satisfy the MNI criterion . Representations and Finiteness of Mutual Information Z_X is not a deterministic function of X in either IB or CEB , it is a stochastic representation of X given by an encoder e ( z_X|x ) . Using a stochastic encoder means that the stated concerns of infinite entropy are not applicable in either objective -- the conditions for infinite mutual information given in Amjad and Geiger ( 2018 ) do not apply . In practice , ( using continuous representations ) , we did not encounter mutual information terms that diverged to infinity , although certainly it is possible to make modeling and data choices that make it more or less likely that there will be numerical instabilities . This is not a flaw specific to CEB or VIB , however , and we found numerical instability to be almost non-existent across a wide variety of modeling and architectural choices for both variational objectives . [ 1 ] R. A. Amjad and B. C. Geiger 'Learning Representations for Neural Network-Based Classification Using the Information Bottleneck Principle ' , 2018"}, "1": {"review_id": "rkVOXhAqY7-1", "review_text": "Update: see comments \"On revisions\" below. This paper essentially introduces a label-dependent regularization to the VIB framework, matching the encoder distribution of one computed from labels. The authors show good performance in generalization, such that their approach is relatively robust in a number of tasks, such as adversarial defense. The idea I think is generally good, but there are several problems with this work. First, there has been recent advances in mutual information estimation, first found in [1]. This is an important departure from the usual variational approximations used in VIB. You need to compare to this baseline, as it was shown that it outperforms VIB in a similar classification task as presented in your work. Second, far too much space is used to lay out some fairly basic formalism with respect to mutual information, conditional entropy, etc. It would be nice, for example, to have an algorithm to make the learning objective more clear. Overall, I don't feel the content justifies the length. Third, I have some concerns about the significance of this work. They introduce essentially a label-dependent \u201cbackwards encoder\u201d to provide samples for the KL term normally found in VIB. The justification is that we need the bottleneck term to improve generalization and the backwards encoder term is supposed to keep the representation relevant to labels. One could have used an approach like MINE, doing min information for the bottleneck and max info for the labels. In addition, much work has been done on learning representations that generalize using mutual information (maximizing instead of minimizing) [2, 3, 4, 5] along with some sort of term to improve \"relevance\", and this work seems to ignore / not be aware of this work. Overall I could see some potential in this paper being published, as I think the approach is sensible, but it's not presented in the proper context of past work. [1] Belghazi, I., Baratin, A., Rajeswar, S., Courville, A., Bengio, Y., & Hjelm, R. D. (2018). MINE: mutual information neural estimation. International Conference for Machine Learning, 2018. [2] Gomes, R., Krause, A., and Perona, P. Discriminative clustering by regularized information maximization. In NIPS, 2010. [3] Hu, W., Miyato, T., Tokui, S., Matsumoto, E., and Sugiyama, M. Learning discrete representations via information maximizing self-augmented training. In ICML, 2017. [4] Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Trischler, A., & Bengio, Y. (2018). Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670. [5] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" arXiv preprint arXiv:1807.03748 (2018).", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review , and for your thorough search of the literature on uses of the mutual information in objective functions . We were aware of the work you mentioned . The high order bit is that you are correct -- any estimator of the mutual information could be used in conjunction with the CEB objective , assuming the inequalities of the estimator correspond correctly to the direction the mutual information estimate needs to be optimized . We chose to explore variational bounds in this paper because they are simple , tractable , and well-understood , but we mention in Section 3 that other approaches are possible . We will be more specific about that in our revisions . There is quite a bit of care that needs to be taken to understand these mutual information estimators , but both MINE and CPC may be used to optimize the lower bound on I ( Z ; Y ) . For reasons we describe in more detail below , we don \u2019 t think that doing so would substantially change the results in these particular experiments , but when applying CEB to tasks other than classification , both options are worth considering , in addition to the variational approach we present . In other words , the CEB objective involves minimization and maximization of mutual information terms , and any correctly bounded method to estimate mutual information could be used to optimize the components of the general CEB objective . Our focus on the variational approach in the paper is merely convenient , rather than essential . Below we discuss MINE in more detail , but first we would like to clarify that all five of the papers you mention are proposing objectives that are not consistent with the MNI criterion if used by themselves , rather than as part of the optimization approach for an implementation of CEB . They are all focusing on maximizing estimations or lower bounds of either I ( X ; Z ) or I ( Y ; Z ) . As we point out in Section 3 , maximizing I ( Y ; Z ) is necessary but not sufficient for achieving the Minimum Necessary Information , and maximizing I ( X ; Z ) is fundamentally inconsistent with MNI . Additionally , it is worth pointing out that standard maximum likelihood estimation training also maximizes I ( Y ; Z ) in deep networks , where Z can be taken as any intermediate layer of the network -- minimizing the cross entropy is the same as minimizing the H ( Y|Z ) term in the paper , which maximizes a lower bound on I ( Y ; Z ) - H ( Y ) , and H ( Y ) is constant with respect to the parameters . Thus , we expect that all five of these approaches ( when used by themselves ) still suffer from the excess of information in the representation that we hypothesize MLE to suffer from ( whether the representation is explicit , as in VIB , Gomes et al. , 2010 , and Hjelm et al. , 2018 , or implicit , as in the other 3 papers you mention ) . In contrast , CEB is maximizing I ( Y ; Z ) while also minimizing I ( X ; Z|Y ) , which forces optimization to get the trained model as close as it can to the MNI goal state of I ( X ; Z ) = I ( Y ; Z ) = I ( X ; Y ) . To summarize the point we are making here , we do not consider unconstrained maximization of the mutual information between an observed variable and a representation variable to be a desirable property by itself , and we show that doing so is inconsistent with the MNI criterion . Such techniques can only be made compatible with MNI when used to optimize a complete MNI-compatible objective function , such as CEB ."}, "2": {"review_id": "rkVOXhAqY7-2", "review_text": "This paper wants to discuss a new objective function, which the authors dub \"Conditional Entropy Bottleneck\" (CEB), motivated by learning better latent representations. However, as far as I can tell, the objective functions already exists in the one-parameter family of Information Bottleneck (IB) of Tishby, Pereira, and Bialek. The author seems to realize this in Appendix B, but calls it \"a somewhat surprising theoretical result\". However, if we express IB as max I(Z;Y) - beta I(Z;X), see (19), and then flip signs and take the max to the min, we get min beta I(Z;X) - I(Z;Y). Taking beta = 1/2, multiplying through by 2, and writing I(X;Z) - I(Y Z) = I(X;Z|Y), we find CIB. Unfortunately, I fail to see how this is surprising or different. A difference only arises when using a variational approximation to IB. The authors compare to the Variational Information Bottleneck (VIB) of Alemi, Fischer, Dillon, and Murphy (arXiv:1612.00410), which requires a classifier, an encoder, and a marginal posterior over the latents. Here, instead of the marginal posterior, they learn a backwards encoder from labels to latents. This difference arises because the IB objective has two terms of opposite sign, and we can group them into positive definite terms in different ways, creating different bounds. Perhaps this grouping leads to a better variational bound? If so, that's only a point about the variational method employed by Alemi et al., and not a separate objective. As this seems to be the main contribution of the paper, this point needs to be explained more carefully and in more detail. For instance, it seems worth pointing out, in the discrete case, that the marginal posterior |Z| values to estimate, and the backwards encoder has |Z| x |Y| -- suggesting this is a possibly a much harder learning problem. If so, there should be a compelling benefit for using this approximation and not the other one. In summary, the authors are not really clear about what they are doing and how it relates to IB. Furthermore, the need for this specific choice in IB parameter space is not made clear, nor do the experimental results giving a compelling need. (The experimental results are also not at all clearly presented or explained.) Therefore, I don't think this paper satisfies the quality, clarity, originality, or significance criteria for ICLR.", "rating": "2: Strong rejection", "reply_text": "We appreciate that you read our paper closely , and address your concerns in detail below . Surprise The surprise of the result relating CEB to IB so simply comes from two things . First , the fact that there is a single value of beta for IB and VIB that achieves the MNI-optimal information , so long as the model , optimizer , etc , is capable of capturing that amount of information . This is surprising because the analysis in Alemi et al . ( 2018 ) naively applied to the VIB case would assume that sweeping beta would be necessary to find the optimal information even if you knew a priori what that amount of information was . We discuss this point in the appendix to some extent , but will clarify that discussion in our revisions . Note that a similar and more directly-applicable Pareto-optimal frontier is described in Strauss and Schwab ( 2017 ) , and that work also does not point out that beta = 1/2 would result in a learned representation where I ( X ; Z ) = I ( Y ; Z ) = I ( X ; Y ) . Second is a point that we decided not to make in the version we submitted , but that we can add in revision . Tishby et al.say two things in many of the IB papers quite clearly . In Tishby et al . ( 2015 ) , the authors state : \u201c The information bottleneck ( IB ) method was introduced as an information theoretic principle for extracting relevant information that an input random variable X \u2208 X contains about an output random variable Y \u2208 Y . Given their joint distribution p ( X , Y ) , the relevant information is defined as the mutual information I ( X ; Y ) , where we assume statistical dependence between X and Y . In this case , Y implicitly determines the relevant and irrelevant features in X . An optimal representation of X would capture the relevant features , and compress X by dismissing the irrelevant parts which do not contribute to the prediction of Y. \u201d But in Tishby et al . ( 2000 ) , the authors also state : \u201c ... there is a tradeoff between compressing the representation and preserving meaningful information , and there is no single right solution for the tradeoff. \u201d In other words , Tishby et al.recognize that the information measured by I ( X ; Y ) corresponds to the optimal representation , but they do not quite know how to find it . Instead , the IB approach relies on sweeping beta and cross validation . Thus , the surprise we are describing isn \u2019 t the trivial arithmetic relating CEB to IB once you have seen both objectives . Instead , the surprise is that you _can_ learn a representation with the optimal amount of information defined by the observed data , I ( X ; Y ) without having to know I ( X ; Y ) ahead of time . This shows that sweeping beta is unnecessary if you believe ( as Tishby appears to believe , and as we certainly believe ) that I ( X ; Y ) is the correct amount of information to retain in your representation . The information bottleneck was presented 19 years ago , and so far as we know , no-one has previously proposed a way to learn a representation that doesn \u2019 t require you to guess what I ( X ; Y ) might be . Why MNI ? In this work we do not attempt to give a formal proof that CEB representations learn the optimal information about the observed data ( and certainly the variational form of the objective will prevent that from happening in general cases ) . However , the MNI is motivated by the following simple observations : If I ( X ; Z ) < I ( X ; Y ) , then we have thrown out relevant information in X for predicting Y . If I ( X ; Z ) > I ( X ; Y ) , then we are including information in X that is not useful for predicting Y . Thus targeting I ( X ; Z ) = I ( X ; Y ) is the `` correct '' amount of information , which is one of the equalities required in order to satisfy the MNI criterion . Relation between general forms of CEB and IB Yes , CEB is a special case of IB with beta = 1/2 . However , we are the first to show that the IB Lagrangian with beta = 1/2 targets the MNI . This is a significant result that was not previously highlighted in any of the literature on IB ."}}