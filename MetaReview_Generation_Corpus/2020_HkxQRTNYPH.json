{"year": "2020", "forum": "HkxQRTNYPH", "title": "Mirror-Generative Neural Machine Translation", "decision": "Accept (Talk)", "meta_review": "This paper proposes a novel method for considering translations in both directions within the framework of generative neural machine translation, significantly improving accuracy.\n\nAll three reviewers appreciated the paper, although they noted that the gains were somewhat small for the increased complexity of the model. Nonetheless, the baselines presented are already quite competitive, so improvements on these datasets are likely to never be extremely large.\n\nOverall, I found this to be a quite nice paper, and strongly recommend acceptance, perhaps as an oral presentation.", "reviews": [{"review_id": "HkxQRTNYPH-0", "review_text": "This paper proposes an approach to neural MT in which the joint (source, target) distribution is modeled as an average over two different factorizations: target given source and source given target. This gives rise to four distributions - two language models and two translation models - which are parameterized separately but conditioned on a common variational latent variable. The model is trained on parallel data using a standard VAE approach. It can additionally be trained on non-parallel data in an approach similar to iterated back-translation at sentence-level granularity, but with language and translation model probabilities for observed sentences coupled by the latent variable. Inference iterates between sampling a latent variable given the current best hypothesis, and using beam search plus rescoring to find a new best hypothesis given the current latent variable. The approach is evaluated in several different scenarios (low- and high-resource, domain adaptation - trained on parallel data only or parallel plus monolingual data) and found to generally outperform previous work on generative NMT and iterated back-translation. Strengths: clearly written, well motivated, very comprehensive experiments comparing to relevant baselines. Weaknesses: somewhat incremental relative to Shah and Barber (Neurips 2018), results are only marginally positive, framework is probably too cumbersome to justify widespread adoption based on the results. I think the paper should be accepted. Although it\u2019s not highly original, it ties together three strands of work in a principled way: joint models, variational approaches, and back-translation / dual learning. The increment over Shah and Barber is bolstered by the addition of back-translation, which gives substantial improvements when using non-parallel data; and to a lesser extent by the argument about the advantage of separate models for distant language pairs. Using all possible LMs and TMs coupled with a latent variable feels like an area that was inevitably going to get explored, and this paper does a good job of it. Although the gains over the baselines are not overly compelling, they are quite systematic, indicating that the advantage is probably real, albeit slight. The authors are also to be commended on their use of not just the Shah and Barber baseline, but also the back-translation-based techniques, which are generally stronger competitors when monolingual data is incorporated. Further comments/questions: Why are there no results for Transformer+Dual in table 4? This omission looks odd, since Transformer+Dual was the strongest baseline in table 3. Please add implementation details for the Transformer+{BT,JBT,Dual} baselines. It was surprising not to see robustness experiments like Shah and Barber\u2019s dropped source words, since robustness to source noise could be one of the advantages of having an explicit model of the source sentence. A few additional suggestions for related work: noisy channel approaches (eg, The Neural Noisy Channel, Yu et al, ICLR 2017); decipherment (eg, Beyond parallel data: Joint word alignment and decipherment improves machine translation, EMNLP 2014 - yes, from SMT days, but still); other joint modeling work (KERMIT: Generative Insertion-Based Modeling for Sequences, Chan et al, 2019). Consider dropping the \u201c1+1 > 2\u201d metaphor. It\u2019s not clear to me exactly what it means, or what it adds to the paper. \u201cdeviation\u201d is used in a couple places where you probably meant \u201cderivation\u201d? Line 6 in algorithm 2 should use both forward and backward scores for rescoring. ", "rating": "8: Accept", "reply_text": "Thanks very much for your valuable comments ! Q1 : About missing Transformer+Dual in table 4 A1 : Thanks for your kind notes . We have RNMT+Dual in similar experiments in the Appendix showing that MGNMT ( RNMT-based ) outperforms RNMT+Dual in WMT En-De and NIST Zh-En . We provide results of Transformer+Dual here , which has also included in the updated paper draft : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Model EN-DE DE-EN EN-ZH ZH-EN -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Transformer+Dual +NP 29.6 33.2 42.13 48.60 MGNMT ( Transformer ) + NP 30.3 33.8 42.56 49.05 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * NP : non-parallel As shown in the results , MGNMT is consistently better than dual learning on these high resource tasks . Q2 : Experiments on noisy source sentence A2 : Thank you so much for your nice suggestion ! We conduct experiments on noisy source sentence to investigate the robustness of our models compared with GNMT . The experimental setting is similar to Shah & Barber 's : each word of the source sentence has a 30 % chance of being missing . Given the noisy source sentence , MGNMT first samples a draft target translation as usual . And then , 1 ) we compute the latent variable z given the current ( noisy ) source and target sentences ; 2 ) remember that MGNMT is symmetric , we can easily use TM ( Y- > X|Z ) and LM ( X|Z ) to find a corrected source sentence ( run line 5 , Alg.2 ) .3 ) given the better-corrected source sentence , we can find a better target translation ( run line 4-6 , Alg.2 ) . 4 ) repeat 1 ) - 3 ) until converge . We conduct experiments on WMT En-De . The results are as follow : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Model EN-DE DE-EN EN-DE ( noisy ) DE-EN ( noisy ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- GNMT ( Transformer ) 27.5 31.1 19.4 23.0 MGNMT ( Transformer ) 27.7 31.4 20.3 24.1 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- As shown in the above table , our model is more robust than GNMT with noisy source input . This may be attributed to the unified probabilistic modeling of TMs and LMs in MGNMT , where the backward translation and language models are naturally and directly leveraged to better `` denoise '' the noisy source input . Nevertheless , the missing content in the noisy source input is still very hard to recover , leading to a large drop to all methods . Dealing with noisy input is interesting and we will leave it for future study . Q3 : About missing references A3 : Thanks . We will add content to discuss these related work in the next version . Q4 : About line 6 in Alg 2 A4 : Yes , it should be both forward and backward scores and we actually implemented this exactly as you suggested . We will fix it in the next version . Q5 : About implementation details for the Transformer+ { BT , JBT , Dual } A5 : We have added the implementation details for them in the revised draft ."}, {"review_id": "HkxQRTNYPH-1", "review_text": "This work proposes a new translation model that combines translation models in two directions and language modelss in two languages by sharing a latent semantic representation. The basic idea to joint modeling of translations conditioning on the latent representations and the parameters are learned by generating pseudo translations in two directions. Decoding is also carefully designed by interchanging sampling in two directions in a greedy fashion. Empirical results show consistent gains when compared with heuristic methods to generate pseudo data, e.g., back translation. It is an interesting work on proposing a unified framework to translation by conditioning on a shared latent space in four models. It is not only rivaling heuristic methods to generate pseudo data, but surpassing competitive Transformer baselines. Other comment: - It is a bit confusing that MGNMT was not experimented with Transformer, though the paper and appendix describe that it is easy to use the Transformer in the MGNMT setting. ", "rating": "8: Accept", "reply_text": "Thanks very much for your comments ! Q1 : About experimenting MGNMT with Transformer A1 : All experimental results in the experiment section are implemented based on Transformer . We also give results implemented on RNMT , which are listed in the Appendix . We will make it clearer in the revision . Sorry for the confusion ."}, {"review_id": "HkxQRTNYPH-2", "review_text": "In this paper, the authors propose MGNMT (Mirror Generative NMT) which aims to integrate s2t, t2s, source and target language models in a single framework. They lay out the details of their framework and motivate the need for leveraging monolingual data in both source and target directions. They also talk about related work in this space. Finally, they perform experiments on low and high resource tasks. They also investigate certain specific phenomena like effect of non-parallel data, effect of target LM during decoding, and effect of adding one side monolingual data. Pros: - Overall, the paper was clearly written and well motivated. The authors clearly lay out their new framework and establish it for the reader. - The set of experiments are very detailed and the authors make sure to compare against all semi-supervised works like BT, JBT and Dual learning. - The set of analyses at the end was also interesting and tried to dig deeper in certain phenomena. - All training details and hyperparameters have been laid it in the paper. Cons: - For all the additional complexity, this newly proposed method only slightly outperforms other semi-supervised methods like BT, JBT & Dual learning as seen in Tables 3 and 4. - The authors could have been more upfront about training and inference costs of their proposed framework and compared it to the other setups. For example, decoding costs 2.7x more than a vanilla transformer. A comparison of decoding and training costs of all methods would have provided the right balance between complexity and quality. This additional complexity might outweigh the gains obtained in some cases. Rating Justification: Despite the con of added complexity, I like the formulation of the new joint framework and I think this will serve as a good starting point for others to push in this direction further. Hence, I want to see this paper accepted. Minor comments: last para of section 1: first line is too big. Please break into multiple lines. \"Exploiting non-parallel data for NMT\" - second para, please cite Dong et. al and Johnson et. al who also share al parameters and vocab in a single model. Page 5, section 3.2, second para - line 1 please rephrase.", "rating": "8: Accept", "reply_text": "Thanks very much for your insightful comments ! Q1.About training and decoding costs of MGNMT A1 : Yes , MGNMT introduces extra costs for training and decoding compared to Transformer baseline . When being trained on parallel data , our model only slightly increases the training cost . However , the training cost regarding non-parallel training is larger than vanilla Transformer because of the on-fly sampling of pseudo-translation pairs , which is also the cost for JBT and dual learning . We list the training time of each model on IWSLT task as follow , which has also been added in the revised paper draft : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Model training time to converge ( hrs , on a single 1080ti ) decoding -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Transformer ~17h 1x Transformer+BT +NP ~25h 1x Transformer+JBT +NP ~34h 1x Transformer+Dual +NP ~52h 1x GNMT-M-SSL +NP ~30h 2.1x MGNMT ( Transformer ) ~22h 2.7x MGNMT ( Transformer ) + NP ~45h 2.7x -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * NP : non-parallel We can see that on-fly sampling leads to time-consumption One possible way to improve the efficiency may be to sample and save these pseudo-translation pairs in advance to the next iteration of training . As for inference time , Transformer+ { BT/JBT/Dual } are roughly the same as vanilla Transformer because essentially they are different strategies for training Transformer which do not modify the decoding phase . Meanwhile , MGNMT requires ~2.7x time for decoding because MGNMT needs iterative decoding for several iterations . Q2 : Typos and related works A2 : Thanks . We will fix typos and missing citations in the next version ."}], "0": {"review_id": "HkxQRTNYPH-0", "review_text": "This paper proposes an approach to neural MT in which the joint (source, target) distribution is modeled as an average over two different factorizations: target given source and source given target. This gives rise to four distributions - two language models and two translation models - which are parameterized separately but conditioned on a common variational latent variable. The model is trained on parallel data using a standard VAE approach. It can additionally be trained on non-parallel data in an approach similar to iterated back-translation at sentence-level granularity, but with language and translation model probabilities for observed sentences coupled by the latent variable. Inference iterates between sampling a latent variable given the current best hypothesis, and using beam search plus rescoring to find a new best hypothesis given the current latent variable. The approach is evaluated in several different scenarios (low- and high-resource, domain adaptation - trained on parallel data only or parallel plus monolingual data) and found to generally outperform previous work on generative NMT and iterated back-translation. Strengths: clearly written, well motivated, very comprehensive experiments comparing to relevant baselines. Weaknesses: somewhat incremental relative to Shah and Barber (Neurips 2018), results are only marginally positive, framework is probably too cumbersome to justify widespread adoption based on the results. I think the paper should be accepted. Although it\u2019s not highly original, it ties together three strands of work in a principled way: joint models, variational approaches, and back-translation / dual learning. The increment over Shah and Barber is bolstered by the addition of back-translation, which gives substantial improvements when using non-parallel data; and to a lesser extent by the argument about the advantage of separate models for distant language pairs. Using all possible LMs and TMs coupled with a latent variable feels like an area that was inevitably going to get explored, and this paper does a good job of it. Although the gains over the baselines are not overly compelling, they are quite systematic, indicating that the advantage is probably real, albeit slight. The authors are also to be commended on their use of not just the Shah and Barber baseline, but also the back-translation-based techniques, which are generally stronger competitors when monolingual data is incorporated. Further comments/questions: Why are there no results for Transformer+Dual in table 4? This omission looks odd, since Transformer+Dual was the strongest baseline in table 3. Please add implementation details for the Transformer+{BT,JBT,Dual} baselines. It was surprising not to see robustness experiments like Shah and Barber\u2019s dropped source words, since robustness to source noise could be one of the advantages of having an explicit model of the source sentence. A few additional suggestions for related work: noisy channel approaches (eg, The Neural Noisy Channel, Yu et al, ICLR 2017); decipherment (eg, Beyond parallel data: Joint word alignment and decipherment improves machine translation, EMNLP 2014 - yes, from SMT days, but still); other joint modeling work (KERMIT: Generative Insertion-Based Modeling for Sequences, Chan et al, 2019). Consider dropping the \u201c1+1 > 2\u201d metaphor. It\u2019s not clear to me exactly what it means, or what it adds to the paper. \u201cdeviation\u201d is used in a couple places where you probably meant \u201cderivation\u201d? Line 6 in algorithm 2 should use both forward and backward scores for rescoring. ", "rating": "8: Accept", "reply_text": "Thanks very much for your valuable comments ! Q1 : About missing Transformer+Dual in table 4 A1 : Thanks for your kind notes . We have RNMT+Dual in similar experiments in the Appendix showing that MGNMT ( RNMT-based ) outperforms RNMT+Dual in WMT En-De and NIST Zh-En . We provide results of Transformer+Dual here , which has also included in the updated paper draft : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Model EN-DE DE-EN EN-ZH ZH-EN -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Transformer+Dual +NP 29.6 33.2 42.13 48.60 MGNMT ( Transformer ) + NP 30.3 33.8 42.56 49.05 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * NP : non-parallel As shown in the results , MGNMT is consistently better than dual learning on these high resource tasks . Q2 : Experiments on noisy source sentence A2 : Thank you so much for your nice suggestion ! We conduct experiments on noisy source sentence to investigate the robustness of our models compared with GNMT . The experimental setting is similar to Shah & Barber 's : each word of the source sentence has a 30 % chance of being missing . Given the noisy source sentence , MGNMT first samples a draft target translation as usual . And then , 1 ) we compute the latent variable z given the current ( noisy ) source and target sentences ; 2 ) remember that MGNMT is symmetric , we can easily use TM ( Y- > X|Z ) and LM ( X|Z ) to find a corrected source sentence ( run line 5 , Alg.2 ) .3 ) given the better-corrected source sentence , we can find a better target translation ( run line 4-6 , Alg.2 ) . 4 ) repeat 1 ) - 3 ) until converge . We conduct experiments on WMT En-De . The results are as follow : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Model EN-DE DE-EN EN-DE ( noisy ) DE-EN ( noisy ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- GNMT ( Transformer ) 27.5 31.1 19.4 23.0 MGNMT ( Transformer ) 27.7 31.4 20.3 24.1 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- As shown in the above table , our model is more robust than GNMT with noisy source input . This may be attributed to the unified probabilistic modeling of TMs and LMs in MGNMT , where the backward translation and language models are naturally and directly leveraged to better `` denoise '' the noisy source input . Nevertheless , the missing content in the noisy source input is still very hard to recover , leading to a large drop to all methods . Dealing with noisy input is interesting and we will leave it for future study . Q3 : About missing references A3 : Thanks . We will add content to discuss these related work in the next version . Q4 : About line 6 in Alg 2 A4 : Yes , it should be both forward and backward scores and we actually implemented this exactly as you suggested . We will fix it in the next version . Q5 : About implementation details for the Transformer+ { BT , JBT , Dual } A5 : We have added the implementation details for them in the revised draft ."}, "1": {"review_id": "HkxQRTNYPH-1", "review_text": "This work proposes a new translation model that combines translation models in two directions and language modelss in two languages by sharing a latent semantic representation. The basic idea to joint modeling of translations conditioning on the latent representations and the parameters are learned by generating pseudo translations in two directions. Decoding is also carefully designed by interchanging sampling in two directions in a greedy fashion. Empirical results show consistent gains when compared with heuristic methods to generate pseudo data, e.g., back translation. It is an interesting work on proposing a unified framework to translation by conditioning on a shared latent space in four models. It is not only rivaling heuristic methods to generate pseudo data, but surpassing competitive Transformer baselines. Other comment: - It is a bit confusing that MGNMT was not experimented with Transformer, though the paper and appendix describe that it is easy to use the Transformer in the MGNMT setting. ", "rating": "8: Accept", "reply_text": "Thanks very much for your comments ! Q1 : About experimenting MGNMT with Transformer A1 : All experimental results in the experiment section are implemented based on Transformer . We also give results implemented on RNMT , which are listed in the Appendix . We will make it clearer in the revision . Sorry for the confusion ."}, "2": {"review_id": "HkxQRTNYPH-2", "review_text": "In this paper, the authors propose MGNMT (Mirror Generative NMT) which aims to integrate s2t, t2s, source and target language models in a single framework. They lay out the details of their framework and motivate the need for leveraging monolingual data in both source and target directions. They also talk about related work in this space. Finally, they perform experiments on low and high resource tasks. They also investigate certain specific phenomena like effect of non-parallel data, effect of target LM during decoding, and effect of adding one side monolingual data. Pros: - Overall, the paper was clearly written and well motivated. The authors clearly lay out their new framework and establish it for the reader. - The set of experiments are very detailed and the authors make sure to compare against all semi-supervised works like BT, JBT and Dual learning. - The set of analyses at the end was also interesting and tried to dig deeper in certain phenomena. - All training details and hyperparameters have been laid it in the paper. Cons: - For all the additional complexity, this newly proposed method only slightly outperforms other semi-supervised methods like BT, JBT & Dual learning as seen in Tables 3 and 4. - The authors could have been more upfront about training and inference costs of their proposed framework and compared it to the other setups. For example, decoding costs 2.7x more than a vanilla transformer. A comparison of decoding and training costs of all methods would have provided the right balance between complexity and quality. This additional complexity might outweigh the gains obtained in some cases. Rating Justification: Despite the con of added complexity, I like the formulation of the new joint framework and I think this will serve as a good starting point for others to push in this direction further. Hence, I want to see this paper accepted. Minor comments: last para of section 1: first line is too big. Please break into multiple lines. \"Exploiting non-parallel data for NMT\" - second para, please cite Dong et. al and Johnson et. al who also share al parameters and vocab in a single model. Page 5, section 3.2, second para - line 1 please rephrase.", "rating": "8: Accept", "reply_text": "Thanks very much for your insightful comments ! Q1.About training and decoding costs of MGNMT A1 : Yes , MGNMT introduces extra costs for training and decoding compared to Transformer baseline . When being trained on parallel data , our model only slightly increases the training cost . However , the training cost regarding non-parallel training is larger than vanilla Transformer because of the on-fly sampling of pseudo-translation pairs , which is also the cost for JBT and dual learning . We list the training time of each model on IWSLT task as follow , which has also been added in the revised paper draft : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Model training time to converge ( hrs , on a single 1080ti ) decoding -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Transformer ~17h 1x Transformer+BT +NP ~25h 1x Transformer+JBT +NP ~34h 1x Transformer+Dual +NP ~52h 1x GNMT-M-SSL +NP ~30h 2.1x MGNMT ( Transformer ) ~22h 2.7x MGNMT ( Transformer ) + NP ~45h 2.7x -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- * NP : non-parallel We can see that on-fly sampling leads to time-consumption One possible way to improve the efficiency may be to sample and save these pseudo-translation pairs in advance to the next iteration of training . As for inference time , Transformer+ { BT/JBT/Dual } are roughly the same as vanilla Transformer because essentially they are different strategies for training Transformer which do not modify the decoding phase . Meanwhile , MGNMT requires ~2.7x time for decoding because MGNMT needs iterative decoding for several iterations . Q2 : Typos and related works A2 : Thanks . We will fix typos and missing citations in the next version ."}}