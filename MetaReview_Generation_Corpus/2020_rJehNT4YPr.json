{"year": "2020", "forum": "rJehNT4YPr", "title": "I Am Going MAD: Maximum Discrepancy Competition for Comparing Classifiers Adaptively", "decision": "Accept (Poster)", "meta_review": "This paper proposes a new way of comparing classifiers, which does not use fixed test set and adaptively sample it from an arbitrarily large corpus of unlabeled images, i.e. replacing the conventional test-set-based evaluation methods with a more flexible mechanism. The main proposal is to build a test set adaptively in a manner that captures how classifiers disagree, as measured by the wordnet tree. As noted by R2, this work has the potential to be of interest to a broad audience and can motivate many subsequent works. \n\nWhile the reviewers acknowledged the importance of this work, they raised several concerns: (1) the proposed approach is immature to be considered for benchmarking yet (R1,R4), (2) selecting k and studying its influence on the performance ( R1, R3, R4), (3) the proposed approach requires data annotation which might not be straightforward -- (R3, R4).  The authors provided a detailed rebuttal addressing the reviewer concerns.\n\nThere is reviewer disagreement on this paper. The comments from R3 were valuable for the discussion, but at the same time too brief to be adequately addressed by the authors. The comments from emergency reviewer were helpful in making the decision. AC decided to recommend acceptance of the paper seeing its valuable contributions towards re-thinking the evaluation of current SOTA models.\n", "reviews": [{"review_id": "rJehNT4YPr-0", "review_text": "This paper proposes a new method to compare existing classifiers, which does not use fixed test set and adaptively sample it from an arbitrarily large corpus of unlabeled images. The main idea seems similar to adopting active learning for the test set selection. One of the main advantage is that it can select a sample set from an arbitrarily large unlabeled images. However, to compare different classifiers, the proposed algorithm still needs humans to annotate the selected dataset, which is very expensive compared with traditional methods. Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing. The authors invite five volunteer graduate students to annotate the selected example. However, for many categories, it\u2019s nor easy for normal people to distinguish. So the experiments in this paper is also not convincing. ", "rating": "3: Weak Reject", "reply_text": "With all our due respect to Reviewer # 3 \u2019 s valuable time and effort in reviewing our manuscript , we must admit that we are a bit upset by this last late review , due to the apparent lack of understanding before placing comments , and several factual errors that make the current comments at least poorly grounded . We understand that the idea of \u201c model falsification as model comparison \u201d might not be trivial to understand for people primarily from practical deep learning backgrounds . The idea is deeply rooted in a successful series of studies from image perceptual assessment research : a basic introduction can be found in ( Wang & Simoncelli ( 2008 ) ) . We notice that Reviewer # 2 also kindly points out another interdisciplinary foundation of MAD in software differential testing . We hope Reviewer # 3 can carefully read the below explanation , and reconsider the rating to a more serious and appropriate one . Q1 : One of the main advantages is that it can select a sample set from an arbitrarily large unlabeled images . However , to compare different classifiers , the proposed algorithm still needs humans to annotate the selected dataset , which is very expensive compared with traditional methods . Response : Our method is very efficient in terms of human annotation budget compared with traditional methods , which is one of the main claims we elaborated in our paper . We are disappointed that this major important point was not well understood . In fact , MAD provides the very first and efficient solution ( in the context of image classification ) to exploit a large-scale image set under the constraint of the very limited budget for human labeling . We have noticed that the other two reviewers agree with us and appreciate this point . For example , quote Reviewer # 2 : \u201c Because of the efficacy of such `` worst-case '' comparison , the needed set size is very small and thus minimizes the human annotation workload \u201d . To evaluate the relative performance of two ImageNet classifiers , traditional evaluation methods compute accuracy on a fixed test set . For ImageNet validation set , human annotations for 50,000 images need to be provided . This number is large in terms of human labeling effort , but is extremely small compared to the set of all natural images ( the natural image manifold ) . As also mentioned by the reviewer , annotation for each image is a 1000-class classification task , which makes the labeling task more difficult compared to a binary classification problem . In contrast , rather than comparing fixed test sets which are typically small , the proposed MAD adaptively samples a test set from an arbitrarily large corpus of unlabeled images so as to maximize the discrepancies between the classifiers , measured by the distance over WordNet hierarchy . Human labeling is only required on the resulting small and model-dependent image sets , which contains only k=30 images ( for each pair of classifiers ) on the ImageNet experiment as reported in our paper . Our experiments show that the MAD ranking stabilizes at around k > 15 ( see figure 5 ) and successfully tracks the recent progress in image classification . For comparing 11 classifiers , the total labeled images needed are 1,650 ( see page 6 ) : it is obviously smaller than 50,000 and leaves much room to compare more classifiers ( before it reaches 50 , 000 ) . In conclusion , our method is apparently much more efficient in terms of human annotation budget compared with traditional methods . In addition , despite the fact that the selected set by MAD is small ( as a way of maximizing the efficiency of human labeling ) , it provides the strongest examples to let classifiers compete with one another . Quote Reviewer # 2 : \u201c The proposed MAD competition distinguishes classifiers by finding their respective counterexamples . It is therefore an `` error spotting '' mechanism \u201d . Their respective strengths , weaknesses as well as biases can be most easily revealed ( see figures in the appendix ) , which sheds light on potential ways to improve the classifiers or combine them into a better one . Those gains are way beyond the scope of collecting random image samples ."}, {"review_id": "rJehNT4YPr-1", "review_text": "The paper proposed a novel image classifier comparison approach that went beyond one fixed testing set for all. Instead, for a pair of classifiers to be compared, it advocated to sample their \"most disagreed\" test set from a large corpus of unlabeled images. The level of disagreement was measured by a semantic-aware distance derived from WordNet ontology. Because of the efficacy of such \"worst-case\" comparison, the needed set size is very small and thus minimizes the human annotation workload. The proposed MAD competition distinguishes classifiers by finding their respective counterexamples. It is therefore an \"error spotting\" mechanism, rather than a drop-in replacement of standard test accuracy. I feel the approach to implicitly assume that the classifiers to be compared are already \"reasonably accurate\"; since if not, both classifiers might be easily falsified by certain trivial examples, making the \"disagreed examples\" not as meaningful. If that is true, I would suggest the authors to make this hidden assumption clearer in the paper The idea shows clear liaison to the \"differential testing\" concept in software engineering besides the cited work of perceptual quality assessment. The idea has a cross-disciplinary nature and is fairly interesting to me. I can see the paper to be of interest to a quite broad audience and can motivate many subsequent works. One minor comment: for images in \"Case III\", the authors considered them \"contribute little to performance comparison between the two classifiers\" and therefore did not source labels for them. However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class. ", "rating": "8: Accept", "reply_text": "1.I feel the approach to implicitly assume that the classifiers to be compared are already `` reasonably accurate '' ; since if not , both classifiers might be easily falsified by certain trivial examples , making the `` disagreed examples '' not as meaningful . If that is true , I would suggest the authors to make this hidden assumption clearer in the paper Response : Thanks for the constructive suggestion . We agree with the reviewer and will make this assumption explicit in the revised manuscript . 2.The idea shows clear liaison to the `` differential testing '' concept in software engineering besides the cited work of perceptual quality assessment . The idea has a cross-disciplinary nature and is fairly interesting to me . I can see the paper to be of interest to a quite broad audience and can motivate many subsequent works . Response : Thanks for recognizing the strengths of the paper . We will add the appropriate references regarding the `` differential testing '' concept in software engineering . 3.One minor comment : for images in `` Case III '' , the authors considered them `` contribute little to performance comparison between the two classifiers '' and therefore did not source labels for them . However , since the authors adopted an affinity-aware distance , two incorrect predictions can still be compared based on their semantic tree distances to the true class . Response : Thanks for pointing it out . We agree with the reviewer that images falling into Case III can be used to distinguish the associated two classifiers using the proposed semantic tree distance . We will revise the writing to make it more rigorous . In our current subjective assessment environment , we choose to stop labeling images in Case III because it is difficult for humans to select one among 200 classes , especially when they are unfamiliar with the class ontology ."}, {"review_id": "rJehNT4YPr-2", "review_text": "This paper points out that the traditional way of model selection is flawed due to that the validation/test set is often small. The authors also attribute the existence of adversarial examples to the small validation/test set, which I agree to some degree. Hence, the authors proposed an alternative approach to comparing different classification models by the notion of inter-model discrepancy. The main idea is reasonable, but it requires that the models to compare all perform reasonably well. Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach. Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes. Another question which is not answered in the paper is the number $k$ of images to select for each pair of classifiers. Is this number task-dependent? Is it related to the number of classes? What is a general guideline for one to choose this number $k$ given a new application scenario? The unlabeled set is not \"unlabeled\" in essence. If my understanding was correct, it cannot contain open-set images which do not belong to any of the classes of interest. It is also nontrivial to control that the images contain only one salient object per image. Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet. ", "rating": "3: Weak Reject", "reply_text": "Regarding comment 1 : Thanks for recognizing the merit of our idea . As also mentioned by Reviewer # 2 , the proposed MAD implicitly assumes that classifiers in the competition are reasonably accurate . Otherwise , the selected counterexamples may be less meaningful . We will make this assumption explicit in the revised manuscript . From this perspective ( and other reasons mentioned in the discussion section ) , MAD should be viewed as complementary to , rather than a replacement for , the conventional accuracy comparison for image classification . When two classifiers perform at a reasonable level and achieve very close accuracy numbers ( e.g. , VGG16BN and ResNet34 on ImageNet validation set ) , MAD provides the most efficient way of differentiating the two models by maximizing their discrepancies over a large-scale image set . We want to emphasize that MAD is especially useful on image classification tasks where most cutting-edge classifiers achieve very close performance . In these situations , the MAD competition ranking , which is obtained by evaluating on corner examples searched from web-scale unlabeled dataset , is more convincing than something like 1 % accuracy advantage on the validation set . For problem domains where there are few sufficiently accurate models , we may still apply the underlying principle behind MAD to create adaptive test sets such that the strengths and weaknesses of the models are most easily revealed . In those scenarios , we conjecture that we need increase k to a reasonably larger number , thus at the cost of efficiency . Regarding comment 2 : Thanks for the comment . As long as two ( or multiple ) models differ ( even in slightly different ways ) , MAD provides the highly efficient way of spotting such differences by exploring a large-scale unlabelled dataset . However , these differences are less likely to be revealed using a fixed and small test set ( i.e. , they will probably have the same accuracy numbers as models to be compared are very similar and are biased in similar ways ) . For the extreme case that two models are exactly the same ( i.e , they are biased in identical ways and make identical prediction errors ) , both MAD and traditional accuracy-based methods will draw the same conclusion - the two models have the same performance . Accuracy-based evaluation methods arrive at this conclusion by comparing model predictions with ground truth labels and outputting the same accuracy numbers . In contrast , MAD arrives at the same conclusion without any human labeling since the set S for subjective testing is empty . So in this extreme case , both MAD and accuracy fail to compare those two models . In summary , to reliably compare the relative performance of computational models , all evaluation methodologies ( including MAD ) rely on the assumption that the models to be compared should be diverse to a certain extent , and the proposed MAD makes this assumption more explicit . In fact , MAD makes the best use of model discrepancies ( even if models are biased in very similar but not identical ways ) to rank the model performance . As a matter of fact , based on our experiments , we find that state-of-the-art ImageNet classifiers do have their own biases . ( See figure 8 in the appendix . ) Regarding comment 3 : Thanks for the excellent question . We believe that the parameter k is task-dependent . For problem domains where there are reasonably accurate models ( e.g. , imageNet classification in our example ) , we may obtain a stable ranking with a relative small k ( e.g. , k=15 in the imageNet classification example ) . For problem domains where there are no good models , we may increase k to the limit of human labeling budget in order to obtain reasonable performance comparison . Regarding comment 4 : Thanks for the comment . In our current setting , we restrict the dataset D to the domain of interest that contain natural images of mainly 200 classes . However , as the construction of D is noisy and coarse , D contains plenty of open-set images , which do not belong to any class of interest . Since we do not perform any manually data screening at this stage , some of the open-set images may even be selected to construct the dataset S. This means that although the selected open-set image is out of the domain of interest , the associated two classifiers make different , high-confident ( with threshold set to 0.8 ) , but incorrect predictions ( Case III ) . As a result , we consider it as a strong counterexample of the two classifiers . Note that this situation rarely happens , at least in our experiments because the competing classifiers tend to give open-set images low-confidence , and therefore are automatically filtered out . We agree with the reviewer that selecting images that contain only one salient object requires a lot of human effort . So we did not eliminate Case I . It turns out that keeping Case I does not seem to affect the comparison and analysis of competing models ."}], "0": {"review_id": "rJehNT4YPr-0", "review_text": "This paper proposes a new method to compare existing classifiers, which does not use fixed test set and adaptively sample it from an arbitrarily large corpus of unlabeled images. The main idea seems similar to adopting active learning for the test set selection. One of the main advantage is that it can select a sample set from an arbitrarily large unlabeled images. However, to compare different classifiers, the proposed algorithm still needs humans to annotate the selected dataset, which is very expensive compared with traditional methods. Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing. The authors invite five volunteer graduate students to annotate the selected example. However, for many categories, it\u2019s nor easy for normal people to distinguish. So the experiments in this paper is also not convincing. ", "rating": "3: Weak Reject", "reply_text": "With all our due respect to Reviewer # 3 \u2019 s valuable time and effort in reviewing our manuscript , we must admit that we are a bit upset by this last late review , due to the apparent lack of understanding before placing comments , and several factual errors that make the current comments at least poorly grounded . We understand that the idea of \u201c model falsification as model comparison \u201d might not be trivial to understand for people primarily from practical deep learning backgrounds . The idea is deeply rooted in a successful series of studies from image perceptual assessment research : a basic introduction can be found in ( Wang & Simoncelli ( 2008 ) ) . We notice that Reviewer # 2 also kindly points out another interdisciplinary foundation of MAD in software differential testing . We hope Reviewer # 3 can carefully read the below explanation , and reconsider the rating to a more serious and appropriate one . Q1 : One of the main advantages is that it can select a sample set from an arbitrarily large unlabeled images . However , to compare different classifiers , the proposed algorithm still needs humans to annotate the selected dataset , which is very expensive compared with traditional methods . Response : Our method is very efficient in terms of human annotation budget compared with traditional methods , which is one of the main claims we elaborated in our paper . We are disappointed that this major important point was not well understood . In fact , MAD provides the very first and efficient solution ( in the context of image classification ) to exploit a large-scale image set under the constraint of the very limited budget for human labeling . We have noticed that the other two reviewers agree with us and appreciate this point . For example , quote Reviewer # 2 : \u201c Because of the efficacy of such `` worst-case '' comparison , the needed set size is very small and thus minimizes the human annotation workload \u201d . To evaluate the relative performance of two ImageNet classifiers , traditional evaluation methods compute accuracy on a fixed test set . For ImageNet validation set , human annotations for 50,000 images need to be provided . This number is large in terms of human labeling effort , but is extremely small compared to the set of all natural images ( the natural image manifold ) . As also mentioned by the reviewer , annotation for each image is a 1000-class classification task , which makes the labeling task more difficult compared to a binary classification problem . In contrast , rather than comparing fixed test sets which are typically small , the proposed MAD adaptively samples a test set from an arbitrarily large corpus of unlabeled images so as to maximize the discrepancies between the classifiers , measured by the distance over WordNet hierarchy . Human labeling is only required on the resulting small and model-dependent image sets , which contains only k=30 images ( for each pair of classifiers ) on the ImageNet experiment as reported in our paper . Our experiments show that the MAD ranking stabilizes at around k > 15 ( see figure 5 ) and successfully tracks the recent progress in image classification . For comparing 11 classifiers , the total labeled images needed are 1,650 ( see page 6 ) : it is obviously smaller than 50,000 and leaves much room to compare more classifiers ( before it reaches 50 , 000 ) . In conclusion , our method is apparently much more efficient in terms of human annotation budget compared with traditional methods . In addition , despite the fact that the selected set by MAD is small ( as a way of maximizing the efficiency of human labeling ) , it provides the strongest examples to let classifiers compete with one another . Quote Reviewer # 2 : \u201c The proposed MAD competition distinguishes classifiers by finding their respective counterexamples . It is therefore an `` error spotting '' mechanism \u201d . Their respective strengths , weaknesses as well as biases can be most easily revealed ( see figures in the appendix ) , which sheds light on potential ways to improve the classifiers or combine them into a better one . Those gains are way beyond the scope of collecting random image samples ."}, "1": {"review_id": "rJehNT4YPr-1", "review_text": "The paper proposed a novel image classifier comparison approach that went beyond one fixed testing set for all. Instead, for a pair of classifiers to be compared, it advocated to sample their \"most disagreed\" test set from a large corpus of unlabeled images. The level of disagreement was measured by a semantic-aware distance derived from WordNet ontology. Because of the efficacy of such \"worst-case\" comparison, the needed set size is very small and thus minimizes the human annotation workload. The proposed MAD competition distinguishes classifiers by finding their respective counterexamples. It is therefore an \"error spotting\" mechanism, rather than a drop-in replacement of standard test accuracy. I feel the approach to implicitly assume that the classifiers to be compared are already \"reasonably accurate\"; since if not, both classifiers might be easily falsified by certain trivial examples, making the \"disagreed examples\" not as meaningful. If that is true, I would suggest the authors to make this hidden assumption clearer in the paper The idea shows clear liaison to the \"differential testing\" concept in software engineering besides the cited work of perceptual quality assessment. The idea has a cross-disciplinary nature and is fairly interesting to me. I can see the paper to be of interest to a quite broad audience and can motivate many subsequent works. One minor comment: for images in \"Case III\", the authors considered them \"contribute little to performance comparison between the two classifiers\" and therefore did not source labels for them. However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class. ", "rating": "8: Accept", "reply_text": "1.I feel the approach to implicitly assume that the classifiers to be compared are already `` reasonably accurate '' ; since if not , both classifiers might be easily falsified by certain trivial examples , making the `` disagreed examples '' not as meaningful . If that is true , I would suggest the authors to make this hidden assumption clearer in the paper Response : Thanks for the constructive suggestion . We agree with the reviewer and will make this assumption explicit in the revised manuscript . 2.The idea shows clear liaison to the `` differential testing '' concept in software engineering besides the cited work of perceptual quality assessment . The idea has a cross-disciplinary nature and is fairly interesting to me . I can see the paper to be of interest to a quite broad audience and can motivate many subsequent works . Response : Thanks for recognizing the strengths of the paper . We will add the appropriate references regarding the `` differential testing '' concept in software engineering . 3.One minor comment : for images in `` Case III '' , the authors considered them `` contribute little to performance comparison between the two classifiers '' and therefore did not source labels for them . However , since the authors adopted an affinity-aware distance , two incorrect predictions can still be compared based on their semantic tree distances to the true class . Response : Thanks for pointing it out . We agree with the reviewer that images falling into Case III can be used to distinguish the associated two classifiers using the proposed semantic tree distance . We will revise the writing to make it more rigorous . In our current subjective assessment environment , we choose to stop labeling images in Case III because it is difficult for humans to select one among 200 classes , especially when they are unfamiliar with the class ontology ."}, "2": {"review_id": "rJehNT4YPr-2", "review_text": "This paper points out that the traditional way of model selection is flawed due to that the validation/test set is often small. The authors also attribute the existence of adversarial examples to the small validation/test set, which I agree to some degree. Hence, the authors proposed an alternative approach to comparing different classification models by the notion of inter-model discrepancy. The main idea is reasonable, but it requires that the models to compare all perform reasonably well. Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach. Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes. Another question which is not answered in the paper is the number $k$ of images to select for each pair of classifiers. Is this number task-dependent? Is it related to the number of classes? What is a general guideline for one to choose this number $k$ given a new application scenario? The unlabeled set is not \"unlabeled\" in essence. If my understanding was correct, it cannot contain open-set images which do not belong to any of the classes of interest. It is also nontrivial to control that the images contain only one salient object per image. Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet. ", "rating": "3: Weak Reject", "reply_text": "Regarding comment 1 : Thanks for recognizing the merit of our idea . As also mentioned by Reviewer # 2 , the proposed MAD implicitly assumes that classifiers in the competition are reasonably accurate . Otherwise , the selected counterexamples may be less meaningful . We will make this assumption explicit in the revised manuscript . From this perspective ( and other reasons mentioned in the discussion section ) , MAD should be viewed as complementary to , rather than a replacement for , the conventional accuracy comparison for image classification . When two classifiers perform at a reasonable level and achieve very close accuracy numbers ( e.g. , VGG16BN and ResNet34 on ImageNet validation set ) , MAD provides the most efficient way of differentiating the two models by maximizing their discrepancies over a large-scale image set . We want to emphasize that MAD is especially useful on image classification tasks where most cutting-edge classifiers achieve very close performance . In these situations , the MAD competition ranking , which is obtained by evaluating on corner examples searched from web-scale unlabeled dataset , is more convincing than something like 1 % accuracy advantage on the validation set . For problem domains where there are few sufficiently accurate models , we may still apply the underlying principle behind MAD to create adaptive test sets such that the strengths and weaknesses of the models are most easily revealed . In those scenarios , we conjecture that we need increase k to a reasonably larger number , thus at the cost of efficiency . Regarding comment 2 : Thanks for the comment . As long as two ( or multiple ) models differ ( even in slightly different ways ) , MAD provides the highly efficient way of spotting such differences by exploring a large-scale unlabelled dataset . However , these differences are less likely to be revealed using a fixed and small test set ( i.e. , they will probably have the same accuracy numbers as models to be compared are very similar and are biased in similar ways ) . For the extreme case that two models are exactly the same ( i.e , they are biased in identical ways and make identical prediction errors ) , both MAD and traditional accuracy-based methods will draw the same conclusion - the two models have the same performance . Accuracy-based evaluation methods arrive at this conclusion by comparing model predictions with ground truth labels and outputting the same accuracy numbers . In contrast , MAD arrives at the same conclusion without any human labeling since the set S for subjective testing is empty . So in this extreme case , both MAD and accuracy fail to compare those two models . In summary , to reliably compare the relative performance of computational models , all evaluation methodologies ( including MAD ) rely on the assumption that the models to be compared should be diverse to a certain extent , and the proposed MAD makes this assumption more explicit . In fact , MAD makes the best use of model discrepancies ( even if models are biased in very similar but not identical ways ) to rank the model performance . As a matter of fact , based on our experiments , we find that state-of-the-art ImageNet classifiers do have their own biases . ( See figure 8 in the appendix . ) Regarding comment 3 : Thanks for the excellent question . We believe that the parameter k is task-dependent . For problem domains where there are reasonably accurate models ( e.g. , imageNet classification in our example ) , we may obtain a stable ranking with a relative small k ( e.g. , k=15 in the imageNet classification example ) . For problem domains where there are no good models , we may increase k to the limit of human labeling budget in order to obtain reasonable performance comparison . Regarding comment 4 : Thanks for the comment . In our current setting , we restrict the dataset D to the domain of interest that contain natural images of mainly 200 classes . However , as the construction of D is noisy and coarse , D contains plenty of open-set images , which do not belong to any class of interest . Since we do not perform any manually data screening at this stage , some of the open-set images may even be selected to construct the dataset S. This means that although the selected open-set image is out of the domain of interest , the associated two classifiers make different , high-confident ( with threshold set to 0.8 ) , but incorrect predictions ( Case III ) . As a result , we consider it as a strong counterexample of the two classifiers . Note that this situation rarely happens , at least in our experiments because the competing classifiers tend to give open-set images low-confidence , and therefore are automatically filtered out . We agree with the reviewer that selecting images that contain only one salient object requires a lot of human effort . So we did not eliminate Case I . It turns out that keeping Case I does not seem to affect the comparison and analysis of competing models ."}}