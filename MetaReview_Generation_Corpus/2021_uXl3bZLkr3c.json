{"year": "2021", "forum": "uXl3bZLkr3c", "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization", "decision": "Accept (Spotlight)", "meta_review": "The paper is proposing a test time adaptation method without modifying the training. The proposed idea is simple and effective, adapting the normalization layers using the entropy of the model predictions as a loss function. The paper presents an extensive empirical study. Paper received unanimously accept scores. It also has potential to be impactful as it is easy to apply without any strong assumption/requirement. A clear accept!", "reviews": [{"review_id": "uXl3bZLkr3c-0", "review_text": "$ Paper $ $ Summary $ This paper proposes a method to adapt a pre-trained model to a target domain , without the need to access samples from the source domain - on which the model was originally trained . The idea is to adapt layer normalization parameters at test time , by learning affine transformations . This is applied in tandem with the re-collection of the domain statistics . $ Pros $ - The paper is very well written : it is easy to understand the core idea and its applicability in the context of the broader literature . Figures and Tables are also well designed and placed . - The method is reasonable and simple , and results are strong . As the Authors claim , it is true that the proposed approach has significantly wider applicability than UDA methods and TTT . $ Cons $ - While interesting , I believe Section 4.2 ( 'Target-only Domain Adaptation ' ) would require additional experiments to properly assess how competitive the proposed approach is with respect to the state of the art of UDA . For instance , several better performing methods could be included in Table 3 ( different papers from CVPR/ICLR/etc 2018-2020 achieve significantly higher results on the SVHN - > MNIST split , basically closing the gap with target models - see for example `` A DIRT-T Approach to Unsupervised Domain Adaptation '' [ Shu et al.ICLR 2018 ] ) . The better performing methods still need more data to train ( source + target ) , so higher numbers for the competitors would not undermine the proposed method , but they would provide the reader with a more realistic perspective . - Results associated with more challenging splits should be provided ; for instance , can the proposed method handle MNIST - > SVHN adaptation ? This would also clarify one concern I have , which is < how good the source model should be for the method to be effective > . The MNIST - > SVHN split would help clarifying this point , since MNIST models are severely under-performing on SVHN ( typically accuracy is ~30 % ) . - One important baseline that is missing is `` Domain Adaptation in the Absence of Source Data '' , [ Chidlovskii et al.SIGKDD 2016 ] . Can the authors comment on this ? It seems to me that some of the Algorithms proposed in this related work could be applied here . $ Minor $ $ points $ / $ suggestions $ - Is the performance of UDA-SS in Table 3 correct ? The error is larger than the Source model . I am also checking at the original paper . - The paper `` On Calibration of Modern Neural Networks '' [ Guo et al.ICML 2017 ] shows that deep neural networks are poorly calibrated , as over-confident on samples they are wrong about . Since high-confidence generally implies low-entropy , their results are not aligned with Figure 1 , that seems to suggest proper calibration . It would be nice to comment on this in the manuscript . $ Review $ $ summary $ I believe this is a strong paper , proposing a simple method with large applicability - hence I recommend acceptance . Still , it presents some weaknesses at this stage : I would be happy to read the Author response on these points and iterate the discussion . - Post-rebuttal comments- The rebuttal and the paper revision address my concerns . I fully recommend acceptance .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful and detailed review ! We reply point-by-point here , to begin the discussion , and will post a revised paper tomorrow . `` how competitive the proposed approach is with respect to the state of the art of UDA '' , `` see for example `` A DIRT-T Approach to Unsupervised Domain Adaptation '' [ Shu et al.ICLR 2018 '' Thank you for the pointer to a stronger unsupervised domain adaptation ( UDA ) method . We will cite and discuss it in the revision to give further context to our adaptation results . Our position is that UDA methods serve as context for our work , not competition , since tent makes use of less data and computation . We claim in the abstract and Sec.4.2 that target-only adaptation is feasible , and can rival UDA , but that does not mean it is better : it addresses a different setting . The UDA results help gauge the performance of fully test-time adaptation . Therefore it is valuable to know that the state-of-the-art for UDA is better still , to underline the opportunity for more research on fully test-time adaptation to try and close the gap . While we will note the accuracy of DIRT-T in the text of the revision , it does not seem appropriate to compare with it directly in the tables as the experimental conditions are different . DIRT-T 's impressive accuracy is in part due to more hyperparameters ( three loss coefficients , plus optimization settings ) and cross-validation on labeled target data , while our baselines and method do not depend on labeled target data for tuning , and so are not comparable . `` Domain Adaptation in the Absence of Source Data '' [ Chidlovskii et al.SIGKDD 2016 ] '' , `` algorithms proposed in this related work could be applied here . '' Thank you for this early reference motivating adaptation without source data . We will cite it in the revision : `` To the best of our knowledge , Chidlovski et al . '16 are the first to motivate adaptation without source data due to legal , contractual , or technical limitations . They adapt classifier predictions without adapting the classifier itself by applying denoising auto-encoders , in contrast to end-to-end source-free methods that adapt the model itself . '' Chidlovskii et al.differ in their method and experimental conditions : they restrict their attention to linear classifiers ( on fixed deep features ) and do not consider corruptions . Reproducing their method and extending it into an end-to-end method is out of scope for our work , but we confirm that we will cite it and thank you again for the reference . Thank you again for the two references on unsupervised domain adaptation and source-free adaptation . Please let us know if there are further points we can discuss !"}, {"review_id": "uXl3bZLkr3c-1", "review_text": "Summary : This paper tackles an interesting problem setting \u2014 fully test-time adaptation with only target data . The proposed method is to minimize the test-time entropy , and the loss is used to update the feature modulation layer only . The proposed method compares favorably with the state of the arts , on the ImageNet-C benchmark and unsupervised domain adaptation tasks . Strength : + The problem setting is interesting and meaningful + The proposed method is simple , efficient , network-agnostic , and generally applicable to many tasks + The proposed method achieves competitive performance on the ImageNet-C benchmark and unsupervised domain adaptation tasks Weakness : - When comparing with UDA methods ( i.e. , RG and UDA-SS ) , I am not sure if the current setting is fair or not . I think they can use the target test set instead of the target training set during training ? In this case , it should be a fair comparison with the offline adaptation . - I wonder if the improvement comes from modulating the features to the target domain distribution , or because the network is optimized specifically on the test data . Run online/offline adaptation on the target training set , and then directly apply this model on the target test set without optimization might be interesting . - Figure 6 is a bit not intuitive to me . Are the authors trying to convey that \u201c entropy reduction is correlated with classification loss reduction \u201d ? Probably a better visualization is needed . - In Section 4.3 \u201c Tent needs feature modulation \u201d , I wonder where are the exact numbers that I can refer to . - What is the message the authors try to convey in Figure 7 ? What does \u201c BN brings them back \u201d mean ? I think this study is interesting and important , more discussions and insights are appreciated . Other comments : * I wonder when this method will fail . I guess if the target test set is very small , the proposed method might not improve over the source model too much ? * Neural networks sometimes tend to output overconfident ( but wrong ) predictions even when given out-of-distribution data as input . In this case , I wonder if test entropy is still a good supervision signal to use . - Post-rebuttal comments- Thanks for the response . The rebuttal addresses all my concerns . I am willing to increase my score to 8 and recommend acceptance .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for the thoughtful and detailed review ! We reply point-by-point here , to begin the discussion , and will post a revised paper tomorrow . `` comparing with UDA methods ( i.e. , RG and UDA-SS ) '' , `` can use the target test set instead of the target training set '' Thank you for the attention to detail . As indicated by Tables 2 & 3 , the UDA methods make use of target train while the fully test-time adaptation methods make use of target test . This is by design , as target train may not be available for test-time adaptation , or it may be inefficient for online adaptation . That said , for rebuttal we control for this difference by experimenting with adapting tent to target train then directly evaluating it on target test without further evaluation . ( This is more practical than re-training the UDA methods on target test instead , as they require more computation . ) Please see the next point for the results . `` if the improvement comes from modulating the features to the target domain distribution , or because the network is optimized specifically on the test data '' , `` Run online/offline adaptation on the target training set '' This is an intriguing question ! For rebuttal we experiment with this by adapting tent online to target train for corruption on CIFAR-100 and domain adaptation on SVHN-to-MNIST . Results improve : CIFAR-100-C error is reduced from 37.3 % to 34.2 % and SVHN-to-MNIST error is reduced from 8.2 % to 6.5 % . In this case adaptation is on separate data , but it is also on more data . When target train is subsampled to the size of target test the difference in errors is < 0.1 % . As a further check , we experiment with online adaptation without repeating the forward pass ( Sec.3.3 ) to update predictions . In effect adaptation lags one batch behind , because the predictions are made before the updates . On IN-C , the difference in errors for online tent with and without the repeated forward is negligible at ~0.1 % . In summary , tent does not need to be optimized to the specific test data . Its adapted statistics and transformation parameters generalize across different test samples . We will include these new results and discuss this point in Sec.4.3 of the revision . `` networks sometimes tend to output overconfident ( but wrong ) predictions even when given out-of-distribution data '' , `` I wonder if test entropy is still a good supervision signal '' This does indeed happen , in both the corruption and domain adaptation settings . While Figure 2 shows that entropy generally increases on out-of-distribution corruptions , there are particular images with low-entropy and high-loss predictions . Adaptation with tent can nevertheless improve the model predictions on average in spite of these cases as shown by our results ( Tables 2 & 3 , Figure 5 ) ."}, {"review_id": "uXl3bZLkr3c-2", "review_text": "Presents Test-time Entropy ( TENT ) minimization , an algorithm for adapting deep models at test time to distributionally shifted data , without requiring access to source training data . At test time , the algorithm updates batch-norm parameters ( that control channel-wise normalization and transformation ) to minimize predictive entropy over target data . This simple approach is found to lead to state of the art performance on various corruption benchmarks for image classification , and competitive performance on simple DIGITS recognition-based domain adaptation shifts . Strengths \u2013 The approach appears very simple to implement and seems to work well , particularly on adapting to corruptions \u2013 The paper is well-written , clearly motivated , and very easy to follow . In particular , the source free-assumption is a compelling feature of the method . \u2013 The analysis on rank-correlation b/w change in entropy and loss , and applicability to different architectures , strengthen the claims of the paper Weaknesses \u2013 While it \u2019 s clear that entropy minimization is correlated with correctness , the motivation beyond updating ( only ) batch-norm parameters to minimize it is unclear to me . Sec 3.2 states that the reason is \u201c stability and efficiency \u201d , but I think a more comprehensive ablation study would validate this choice better . \u2013 I appreciate the fact that the method is benchmarked on multiple tasks \u2013 visual corruptions and DA . However the DA experiments/comparisons appear quite cursory . DIGITS is an easy benchmark , and the DA point of comparison ( RevGrad ) is not competitive with the current SoTA . Without results on other more challenging benchmarks ( VisDA , DomainNet , OfficeHome , etc . ) I \u2019 m not convinced of the usefulness of this method as a DA technique . To be clear , it would be completely fine if the method * does not * outperform prior work that uses source data or additional computation , but I think it is important to at least benchmark its performance to understand whether it is a viable DA strategy . The qualitative results on semantic segmentation in supplementary appears promising but a quantitative comparison would have been more convincing . Additional questions / suggestions \u2013 In Fig.5 , how is source performance measured for TENT ? Is this performance on source data after applying TENT , with the updated batch-norm parameters ? If so , how does TENT achieve identical performance on the source test set ? \u2013 It would be interesting to benchmark the performance of TENT for online DA \u2013 beyond not requiring source data , being able to also adapt ( even reasonably well ) online could be very useful . \u2013 A more descriptive caption for Figure 2 would be helpful . Does opacity correspond to the severity of corruption ? \u2013 In Fig.5 , it would be good to break down performance of ANT by corruption type as is done for other methods \u2013 Sec 4.2 : \u201c Tent needs less computation , but still improves with more \u201d : Does this hold indefinitely ? Does performance degrade after a certain number of epochs , or does it remain stable ? Overall comments Interesting paper on test-time-adaptation that proposes a simple entropy-minimization based objective to update batch norm parameters , and works well on robustness benchmarks . I have concerns around the motivation behind the algorithm \u2019 s design , and its viability as a DA method , but would be willing to reevaluate based on the author response . - Post-rebuttal comments- The author rebuttal + revised draft adequately addresses most of my concerns \u2013 in particular , the experiments on online adaptation and semantic segmentation are strong , and the additional context on the DA results is helpful . I would still have liked to see DA results on more challenging benchmarks but nevertheless think that the paper proposes an interesting approach and is worth accepting .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful and detailed review ! We reply point-by-point here , to begin the discussion , and will post a revised paper tomorrow . `` motivation beyond updating ( only ) batch-norm parameters to minimize it is unclear to me '' , `` more comprehensive ablation study '' We motivate and explain our parameterization in Sec.3.2 and ablate it in Sec.4.3 `` Tent needs feature modulation '' . In short , only updating the normalization statistics and affine transformation parameters works while simple alternatives do not . We experiment with optimizing all model parameters , or optimizing the last layer of the network , and both fail for adapting with tent . Our work identifies test entropy and feature modulation as an effective pair for fully test-time adaptation , and we hope more research will identify additional objectives and parameterizations . `` qualitative results on semantic segmentation in supplementary appears promising but a quantitative comparison would have been more convincing '' Note the quantitative results for offline adaptation with the standard metric of intersection-over-union in Sec.4.2 `` Tent scales to semantic segmentation '' with source 28.8 % , BN 31.4 % , and tent 35.8 % ( higher is better ) . For rebuttal , we quantitatively evaluate single-image adaptation , as shown in the supplement , and report 36.4 % for tent with 10 iterations . Thank you for suggesting this quantitative result , which demonstrates a case for which tent can adapt to a single target instance ( one image ) rather than needing to observe an entire domain ( many images ) . `` performance of TENT for online DA '' The results on ImageNet-C are for tent with online optimization ( Figure 5 and Sec.4.1 `` Tent reaches a new state-of-the-art of 44.0 % error by online adaptation and 42.3 % error by offline adaptation '' ) . For rebuttal , we also evaluate SVHN-to-MNIST domain adaptation online ( compared to offline in Table 3 ) . At one epoch online/offline error is 12.3 % vs. 10.0 % , but at 10 epochs errors are closer at 8.4 % vs. 8.2 % . This online capability is a plus for tent , as it is more efficient and lower-latency than offline optimization . Is this what the reviewer is requesting , or is `` online DA '' rather a different setting where domains shift online as the data is encountered ? `` Does performance degrade after a certain number of epochs , or does it remain stable ? '' Tent largely remains stable , but this depends . For domain adaptation on SVHN-to-MNIST , error keeps improving at 10 epochs to 8.2 % ( Table 3 ) and at 100 epochs to 6.5 % ( evaluated for rebuttal ) . For corruption on CIFAR-100 , error worsens slightly from 37.3 % at 1 epoch to 38.0 % at 10 epochs . This is due to our parameterization with feature modulation . With more free parameters , for instance optimizing only the last layer of the network , error first improves and then degrades . If all parameters are optimized , then optimization fails , and error at only 1 epoch is worse than the unadapted source model . `` DIGITS is an easy benchmark '' , `` the DA point of comparison ( RevGrad ) is not competitive with the current SoTA '' For the benchmark , we choose DIGITS as an accessible benchmark that is common to many works ( Ganin et al. , Tzeng et al. , Sun et al. , Li et al. , ... ) . For the baselines , we choose RevGrad as a common adversarial method , and UDA-SS as a self-supervised method for more context w.r.t.self-supervised test-time training ( TTT ) . We include these because ( 1 ) their model selection rules do not require labeled target data and ( 2 ) we were able to reproduce their results . Together these allow for controlled comparison across our baselines and method . To give more domain adaptation context , we will include the stronger accuracy of the DIRT-T UDA method ( pointed out by R1 ) in the revision . `` whether it is a viable DA strategy '' We claim only the feasibility of target-only adaptation ( abstract and Sec.4.2 ) on digits and simulation-to-real semantic segmentation . We certainly agree that state-of-the-art UDA methods can be stronger and larger domain adaptation benchmarks can be harder . Tent is not a replacement , but an alternative for settings where UDA does not apply without source data or when more efficiency is necessary , and UDA methods help to measure how well tent adapts . We hope that our results on digits and semantic segmentation encourage research on fully test-time adaptation to pursue more conditions where there is strong progress by unsupervised domain adaptation methods like VisDA and OfficeHome as suggested in the review ."}], "0": {"review_id": "uXl3bZLkr3c-0", "review_text": "$ Paper $ $ Summary $ This paper proposes a method to adapt a pre-trained model to a target domain , without the need to access samples from the source domain - on which the model was originally trained . The idea is to adapt layer normalization parameters at test time , by learning affine transformations . This is applied in tandem with the re-collection of the domain statistics . $ Pros $ - The paper is very well written : it is easy to understand the core idea and its applicability in the context of the broader literature . Figures and Tables are also well designed and placed . - The method is reasonable and simple , and results are strong . As the Authors claim , it is true that the proposed approach has significantly wider applicability than UDA methods and TTT . $ Cons $ - While interesting , I believe Section 4.2 ( 'Target-only Domain Adaptation ' ) would require additional experiments to properly assess how competitive the proposed approach is with respect to the state of the art of UDA . For instance , several better performing methods could be included in Table 3 ( different papers from CVPR/ICLR/etc 2018-2020 achieve significantly higher results on the SVHN - > MNIST split , basically closing the gap with target models - see for example `` A DIRT-T Approach to Unsupervised Domain Adaptation '' [ Shu et al.ICLR 2018 ] ) . The better performing methods still need more data to train ( source + target ) , so higher numbers for the competitors would not undermine the proposed method , but they would provide the reader with a more realistic perspective . - Results associated with more challenging splits should be provided ; for instance , can the proposed method handle MNIST - > SVHN adaptation ? This would also clarify one concern I have , which is < how good the source model should be for the method to be effective > . The MNIST - > SVHN split would help clarifying this point , since MNIST models are severely under-performing on SVHN ( typically accuracy is ~30 % ) . - One important baseline that is missing is `` Domain Adaptation in the Absence of Source Data '' , [ Chidlovskii et al.SIGKDD 2016 ] . Can the authors comment on this ? It seems to me that some of the Algorithms proposed in this related work could be applied here . $ Minor $ $ points $ / $ suggestions $ - Is the performance of UDA-SS in Table 3 correct ? The error is larger than the Source model . I am also checking at the original paper . - The paper `` On Calibration of Modern Neural Networks '' [ Guo et al.ICML 2017 ] shows that deep neural networks are poorly calibrated , as over-confident on samples they are wrong about . Since high-confidence generally implies low-entropy , their results are not aligned with Figure 1 , that seems to suggest proper calibration . It would be nice to comment on this in the manuscript . $ Review $ $ summary $ I believe this is a strong paper , proposing a simple method with large applicability - hence I recommend acceptance . Still , it presents some weaknesses at this stage : I would be happy to read the Author response on these points and iterate the discussion . - Post-rebuttal comments- The rebuttal and the paper revision address my concerns . I fully recommend acceptance .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful and detailed review ! We reply point-by-point here , to begin the discussion , and will post a revised paper tomorrow . `` how competitive the proposed approach is with respect to the state of the art of UDA '' , `` see for example `` A DIRT-T Approach to Unsupervised Domain Adaptation '' [ Shu et al.ICLR 2018 '' Thank you for the pointer to a stronger unsupervised domain adaptation ( UDA ) method . We will cite and discuss it in the revision to give further context to our adaptation results . Our position is that UDA methods serve as context for our work , not competition , since tent makes use of less data and computation . We claim in the abstract and Sec.4.2 that target-only adaptation is feasible , and can rival UDA , but that does not mean it is better : it addresses a different setting . The UDA results help gauge the performance of fully test-time adaptation . Therefore it is valuable to know that the state-of-the-art for UDA is better still , to underline the opportunity for more research on fully test-time adaptation to try and close the gap . While we will note the accuracy of DIRT-T in the text of the revision , it does not seem appropriate to compare with it directly in the tables as the experimental conditions are different . DIRT-T 's impressive accuracy is in part due to more hyperparameters ( three loss coefficients , plus optimization settings ) and cross-validation on labeled target data , while our baselines and method do not depend on labeled target data for tuning , and so are not comparable . `` Domain Adaptation in the Absence of Source Data '' [ Chidlovskii et al.SIGKDD 2016 ] '' , `` algorithms proposed in this related work could be applied here . '' Thank you for this early reference motivating adaptation without source data . We will cite it in the revision : `` To the best of our knowledge , Chidlovski et al . '16 are the first to motivate adaptation without source data due to legal , contractual , or technical limitations . They adapt classifier predictions without adapting the classifier itself by applying denoising auto-encoders , in contrast to end-to-end source-free methods that adapt the model itself . '' Chidlovskii et al.differ in their method and experimental conditions : they restrict their attention to linear classifiers ( on fixed deep features ) and do not consider corruptions . Reproducing their method and extending it into an end-to-end method is out of scope for our work , but we confirm that we will cite it and thank you again for the reference . Thank you again for the two references on unsupervised domain adaptation and source-free adaptation . Please let us know if there are further points we can discuss !"}, "1": {"review_id": "uXl3bZLkr3c-1", "review_text": "Summary : This paper tackles an interesting problem setting \u2014 fully test-time adaptation with only target data . The proposed method is to minimize the test-time entropy , and the loss is used to update the feature modulation layer only . The proposed method compares favorably with the state of the arts , on the ImageNet-C benchmark and unsupervised domain adaptation tasks . Strength : + The problem setting is interesting and meaningful + The proposed method is simple , efficient , network-agnostic , and generally applicable to many tasks + The proposed method achieves competitive performance on the ImageNet-C benchmark and unsupervised domain adaptation tasks Weakness : - When comparing with UDA methods ( i.e. , RG and UDA-SS ) , I am not sure if the current setting is fair or not . I think they can use the target test set instead of the target training set during training ? In this case , it should be a fair comparison with the offline adaptation . - I wonder if the improvement comes from modulating the features to the target domain distribution , or because the network is optimized specifically on the test data . Run online/offline adaptation on the target training set , and then directly apply this model on the target test set without optimization might be interesting . - Figure 6 is a bit not intuitive to me . Are the authors trying to convey that \u201c entropy reduction is correlated with classification loss reduction \u201d ? Probably a better visualization is needed . - In Section 4.3 \u201c Tent needs feature modulation \u201d , I wonder where are the exact numbers that I can refer to . - What is the message the authors try to convey in Figure 7 ? What does \u201c BN brings them back \u201d mean ? I think this study is interesting and important , more discussions and insights are appreciated . Other comments : * I wonder when this method will fail . I guess if the target test set is very small , the proposed method might not improve over the source model too much ? * Neural networks sometimes tend to output overconfident ( but wrong ) predictions even when given out-of-distribution data as input . In this case , I wonder if test entropy is still a good supervision signal to use . - Post-rebuttal comments- Thanks for the response . The rebuttal addresses all my concerns . I am willing to increase my score to 8 and recommend acceptance .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for the thoughtful and detailed review ! We reply point-by-point here , to begin the discussion , and will post a revised paper tomorrow . `` comparing with UDA methods ( i.e. , RG and UDA-SS ) '' , `` can use the target test set instead of the target training set '' Thank you for the attention to detail . As indicated by Tables 2 & 3 , the UDA methods make use of target train while the fully test-time adaptation methods make use of target test . This is by design , as target train may not be available for test-time adaptation , or it may be inefficient for online adaptation . That said , for rebuttal we control for this difference by experimenting with adapting tent to target train then directly evaluating it on target test without further evaluation . ( This is more practical than re-training the UDA methods on target test instead , as they require more computation . ) Please see the next point for the results . `` if the improvement comes from modulating the features to the target domain distribution , or because the network is optimized specifically on the test data '' , `` Run online/offline adaptation on the target training set '' This is an intriguing question ! For rebuttal we experiment with this by adapting tent online to target train for corruption on CIFAR-100 and domain adaptation on SVHN-to-MNIST . Results improve : CIFAR-100-C error is reduced from 37.3 % to 34.2 % and SVHN-to-MNIST error is reduced from 8.2 % to 6.5 % . In this case adaptation is on separate data , but it is also on more data . When target train is subsampled to the size of target test the difference in errors is < 0.1 % . As a further check , we experiment with online adaptation without repeating the forward pass ( Sec.3.3 ) to update predictions . In effect adaptation lags one batch behind , because the predictions are made before the updates . On IN-C , the difference in errors for online tent with and without the repeated forward is negligible at ~0.1 % . In summary , tent does not need to be optimized to the specific test data . Its adapted statistics and transformation parameters generalize across different test samples . We will include these new results and discuss this point in Sec.4.3 of the revision . `` networks sometimes tend to output overconfident ( but wrong ) predictions even when given out-of-distribution data '' , `` I wonder if test entropy is still a good supervision signal '' This does indeed happen , in both the corruption and domain adaptation settings . While Figure 2 shows that entropy generally increases on out-of-distribution corruptions , there are particular images with low-entropy and high-loss predictions . Adaptation with tent can nevertheless improve the model predictions on average in spite of these cases as shown by our results ( Tables 2 & 3 , Figure 5 ) ."}, "2": {"review_id": "uXl3bZLkr3c-2", "review_text": "Presents Test-time Entropy ( TENT ) minimization , an algorithm for adapting deep models at test time to distributionally shifted data , without requiring access to source training data . At test time , the algorithm updates batch-norm parameters ( that control channel-wise normalization and transformation ) to minimize predictive entropy over target data . This simple approach is found to lead to state of the art performance on various corruption benchmarks for image classification , and competitive performance on simple DIGITS recognition-based domain adaptation shifts . Strengths \u2013 The approach appears very simple to implement and seems to work well , particularly on adapting to corruptions \u2013 The paper is well-written , clearly motivated , and very easy to follow . In particular , the source free-assumption is a compelling feature of the method . \u2013 The analysis on rank-correlation b/w change in entropy and loss , and applicability to different architectures , strengthen the claims of the paper Weaknesses \u2013 While it \u2019 s clear that entropy minimization is correlated with correctness , the motivation beyond updating ( only ) batch-norm parameters to minimize it is unclear to me . Sec 3.2 states that the reason is \u201c stability and efficiency \u201d , but I think a more comprehensive ablation study would validate this choice better . \u2013 I appreciate the fact that the method is benchmarked on multiple tasks \u2013 visual corruptions and DA . However the DA experiments/comparisons appear quite cursory . DIGITS is an easy benchmark , and the DA point of comparison ( RevGrad ) is not competitive with the current SoTA . Without results on other more challenging benchmarks ( VisDA , DomainNet , OfficeHome , etc . ) I \u2019 m not convinced of the usefulness of this method as a DA technique . To be clear , it would be completely fine if the method * does not * outperform prior work that uses source data or additional computation , but I think it is important to at least benchmark its performance to understand whether it is a viable DA strategy . The qualitative results on semantic segmentation in supplementary appears promising but a quantitative comparison would have been more convincing . Additional questions / suggestions \u2013 In Fig.5 , how is source performance measured for TENT ? Is this performance on source data after applying TENT , with the updated batch-norm parameters ? If so , how does TENT achieve identical performance on the source test set ? \u2013 It would be interesting to benchmark the performance of TENT for online DA \u2013 beyond not requiring source data , being able to also adapt ( even reasonably well ) online could be very useful . \u2013 A more descriptive caption for Figure 2 would be helpful . Does opacity correspond to the severity of corruption ? \u2013 In Fig.5 , it would be good to break down performance of ANT by corruption type as is done for other methods \u2013 Sec 4.2 : \u201c Tent needs less computation , but still improves with more \u201d : Does this hold indefinitely ? Does performance degrade after a certain number of epochs , or does it remain stable ? Overall comments Interesting paper on test-time-adaptation that proposes a simple entropy-minimization based objective to update batch norm parameters , and works well on robustness benchmarks . I have concerns around the motivation behind the algorithm \u2019 s design , and its viability as a DA method , but would be willing to reevaluate based on the author response . - Post-rebuttal comments- The author rebuttal + revised draft adequately addresses most of my concerns \u2013 in particular , the experiments on online adaptation and semantic segmentation are strong , and the additional context on the DA results is helpful . I would still have liked to see DA results on more challenging benchmarks but nevertheless think that the paper proposes an interesting approach and is worth accepting .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the thoughtful and detailed review ! We reply point-by-point here , to begin the discussion , and will post a revised paper tomorrow . `` motivation beyond updating ( only ) batch-norm parameters to minimize it is unclear to me '' , `` more comprehensive ablation study '' We motivate and explain our parameterization in Sec.3.2 and ablate it in Sec.4.3 `` Tent needs feature modulation '' . In short , only updating the normalization statistics and affine transformation parameters works while simple alternatives do not . We experiment with optimizing all model parameters , or optimizing the last layer of the network , and both fail for adapting with tent . Our work identifies test entropy and feature modulation as an effective pair for fully test-time adaptation , and we hope more research will identify additional objectives and parameterizations . `` qualitative results on semantic segmentation in supplementary appears promising but a quantitative comparison would have been more convincing '' Note the quantitative results for offline adaptation with the standard metric of intersection-over-union in Sec.4.2 `` Tent scales to semantic segmentation '' with source 28.8 % , BN 31.4 % , and tent 35.8 % ( higher is better ) . For rebuttal , we quantitatively evaluate single-image adaptation , as shown in the supplement , and report 36.4 % for tent with 10 iterations . Thank you for suggesting this quantitative result , which demonstrates a case for which tent can adapt to a single target instance ( one image ) rather than needing to observe an entire domain ( many images ) . `` performance of TENT for online DA '' The results on ImageNet-C are for tent with online optimization ( Figure 5 and Sec.4.1 `` Tent reaches a new state-of-the-art of 44.0 % error by online adaptation and 42.3 % error by offline adaptation '' ) . For rebuttal , we also evaluate SVHN-to-MNIST domain adaptation online ( compared to offline in Table 3 ) . At one epoch online/offline error is 12.3 % vs. 10.0 % , but at 10 epochs errors are closer at 8.4 % vs. 8.2 % . This online capability is a plus for tent , as it is more efficient and lower-latency than offline optimization . Is this what the reviewer is requesting , or is `` online DA '' rather a different setting where domains shift online as the data is encountered ? `` Does performance degrade after a certain number of epochs , or does it remain stable ? '' Tent largely remains stable , but this depends . For domain adaptation on SVHN-to-MNIST , error keeps improving at 10 epochs to 8.2 % ( Table 3 ) and at 100 epochs to 6.5 % ( evaluated for rebuttal ) . For corruption on CIFAR-100 , error worsens slightly from 37.3 % at 1 epoch to 38.0 % at 10 epochs . This is due to our parameterization with feature modulation . With more free parameters , for instance optimizing only the last layer of the network , error first improves and then degrades . If all parameters are optimized , then optimization fails , and error at only 1 epoch is worse than the unadapted source model . `` DIGITS is an easy benchmark '' , `` the DA point of comparison ( RevGrad ) is not competitive with the current SoTA '' For the benchmark , we choose DIGITS as an accessible benchmark that is common to many works ( Ganin et al. , Tzeng et al. , Sun et al. , Li et al. , ... ) . For the baselines , we choose RevGrad as a common adversarial method , and UDA-SS as a self-supervised method for more context w.r.t.self-supervised test-time training ( TTT ) . We include these because ( 1 ) their model selection rules do not require labeled target data and ( 2 ) we were able to reproduce their results . Together these allow for controlled comparison across our baselines and method . To give more domain adaptation context , we will include the stronger accuracy of the DIRT-T UDA method ( pointed out by R1 ) in the revision . `` whether it is a viable DA strategy '' We claim only the feasibility of target-only adaptation ( abstract and Sec.4.2 ) on digits and simulation-to-real semantic segmentation . We certainly agree that state-of-the-art UDA methods can be stronger and larger domain adaptation benchmarks can be harder . Tent is not a replacement , but an alternative for settings where UDA does not apply without source data or when more efficiency is necessary , and UDA methods help to measure how well tent adapts . We hope that our results on digits and semantic segmentation encourage research on fully test-time adaptation to pursue more conditions where there is strong progress by unsupervised domain adaptation methods like VisDA and OfficeHome as suggested in the review ."}}