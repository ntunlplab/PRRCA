{"year": "2020", "forum": "SkxpxJBKwS", "title": "Emergent Tool Use From Multi-Agent Autocurricula", "decision": "Accept (Spotlight)", "meta_review": "This paper describes how multi-agent reinforcement learning at scale leads to the evolution of complex behaviors. Actually, \"at scale\" may be an understatement - a lot of computing power was used here. But the amount of compute used is not the point, rather the point is that complex and fascinating behavior can emerge from a long co-evolutionary process (though gradient-based RL is used here, the principle is the same) where the arms race forms an implicit curriculum. This is the existence proof that people in artificial life and adaptive behavior have been looking for for so long. \n\nTwo reviewers were positive about the paper, with a third being negative because the paper does not give any new insights about how to do RL at scale. But that was not the stated aim of the paper, as the authors clarify in a response.\n\nThis paper will draw quite some attention and deserves an oral presentation.", "reviews": [{"review_id": "SkxpxJBKwS-0", "review_text": "# Review ICLR20, Emergent Tool Use... This review is for the originally uploaded version of this article. Comments from other reviewers and revisions have deliberately not been taken into account. After publishing this review, this reviewer will participate in the forum discussion and help the authors improve the paper. I apologize in advance for being reviewer 2. ## Overall **Summary** The article introduces a new multi-agent physics environment called \"hide-and-seek\". The authors trained agents in this environment and studied the emergence of and changes in strategies. The authors also study the performance of these same agents in new \"targeted intelligence tests\" compared to training from scratch and compared to agents trained with curiosity. **Overall Opinion** I think the environment is very appealing and the paper is overall well-structured and demonstrates novel work. Therefore I'd recommend this paper to be accepted. That being said, there are glaring issues with some of the writing that need to be addressed before I think this work conforms to the standards of ICLR. However, if these issues are addressed, I have no issue increasing my review score. Main problems: - The majority of the paper presents essentially a case study of what happened during a single seed of policy training. For RL literature that's very uncommon and I think it's consensus that DRL is very sensitive to random seeds. I know that you do have additional seeds in the appendix, but why didn't you mention those in the main body of the paper? You seem to have found some robustness against multiple seeds, so why not show it? And also the fact that Figure 1 & 3 only apply to 1 seed is not mentioned. I think this is easy enough to fix - I suggest since you're already at 10 pages, to just bring in the additional seeds from the appendix and average over their performance in Fig.1&3. - The contributions section is overselling the work: (1) states that autocurricula lead to changes in agent strategy - Maybe I'm mistaken here but that sounds like a tautology. In other words, \"a self-generated sequence of challenges\" (\"Autocurricula\", according to [Leibo et al., 2019][1]) lead to changes in strategy. And (3) advertises \"a proposed framework for evaluating agents in open-ended environments\" and also \"a suite of targeted intelligence tests for our domain\". The former of those two is either not in the paper or you mean your section \"6.2 Transfer and Fine-Tuning as Evaluation\", which isn't novel (see e.g. [Alain & Bengio, 2016][2]) - Your acknowledgments should be anonymized until publication. Otherwise, reviewers might draw conclusions which group published this work, thus violating the double-blind review procedure. [1]: https://arxiv.org/pdf/1903.00742.pdf [2]: https://arxiv.org/pdf/1610.01644.pdf Like I mentioned above, I think these are all easy to address, which should allow acceptance of this work. Here are some additional questions, comments, and nitpicks: ## Specific comments and questions ### Abstract - \"evidence that ... competition may scale better with increasing environment complexity\" - that's only shown in the appendix ### Intro - You mention TD-Gammon as a game, but I think it's an algorithm for the game Backgammon, similarly to how \"Go\" is the game and \"AlphaGo\" is an algorithm for playing. ### Rel. Work - all good ### Hide And Seek - Arena boundaries: What's the penalty and what's \" too far outside the play area\"? And in all depictions, it looks like the geometry of the arena is elevated around the edges and the agents don't have a jump action, so how would they ever go out of borders? After watching the videos: Apparently, the jagged-looking arena boundary in the videos is purely cosmetic and agents can still access that space. This is unclear from just the paper and the renderings in Figure 1. ### Policy Optimization - Policy network and fusion are underspecified: How do you deal with the varying number of agents, boxes, obstacles? Do you just set the x/v of the missing pieces to zero or is the observation actually of a different shape in case there are more/fewer objects or agents? How's the embedding done that is depicted in Figure 2? Also, I didn't see the embedding being mentioned in the text - any reason for that? - Figure 2 - This diagram is visually appealing but confusing and needs to be improved. Why does the agent's embedding say \"1\"? Why do the other agents' embeddings have a \"-1\" at the end of the orange box and the others don't? Is the agent's embedding concatenated with the other embeddings? If so, why and how (concat, sum, multiply, conditional batch norm, etc.)? In the center and on the right you use a blue block to indicate the agent's embedding and then at the bottom right you seem to use it as a network component or something (between the \"LSTM\")? If you're trying to signal that this is the agent's perception at different stages in the network, I'd use a different color to separate it from the agent's lidar and pos/vel. You don't mention that \"x,v\" stands for \"position, velocity\". ### Auto-curriculum and Emergent Behavior - Figure 3: \"environment-specific\" (add dash). Draw skill development boundaries like in Fig.1. - How exactly does the \"surfing\" work? The seekers step (not jump, right, since there is no jumping?) onto the boxes and then what? The momentum propels the box forward? Do other seekers push the box? Their movement on top of the box somehow moves the box (this seems to be the case judging by the videos but this is the least physically plausible)? This is a super interesting adaptation but I'd suspect the physics simulation to have a bug/glitch that's being exploited here. - You mention in footnote 3 that the developmental stage and changes in reward aren't necessarily correlated. The same seems to be true for the metrics in Fig.3, which raises the question how did you come up with those boundaries for the different developmental stages in Fig.1? Did someone look at rollouts from the trained policy every couple of million steps? And do all agents learn new skills at the same time or is there a delay? From my understanding, they are all using the same policy and critic networks but maybe dependent on the proximity of an agent to an object/obstacle, it's easier or harder to execute. ### Evaluation - clear and well-written, slightly too much content in the appendix and not enough in the main paper. Weird appendix numbering - A.6 appears in the main paper pages after A.7 ### Discussion and Future Work - all good ### Appendix - I appreciate the TOC. I did not look into Appendix B-D because it's another 10 pages on top of the 10 pages of the article. All in all an interesting work. Good luck with the rebuttal/discussion.", "rating": "6: Weak Accept", "reply_text": "Thank you for the very detailed review and constructive criticisms ! \u2014 \u201c The majority of the paper presents essentially a case study of what happened during a single seed of policy training ... \u201d Great point , we will update Figures 1 and 3 to be the average across the 3 seeds we show in the appendix . We found very little seed dependence throughout the project , which is likely why we made this oversight . \u2014 \u201c The contributions section is overselling the work : ( 1 ) states that autocurricula lead to changes in agent strategy - Maybe I 'm mistaken here but that sounds like a tautology . In other words , `` a self-generated sequence of challenges '' ( `` Autocurricula '' , according to [ Leibo et al. , 2019 ] [ 1 ] ) lead to changes in strategy. \u201d In this sentence we were trying to place emphasis on \u201c distinct and compounding phase shifts \u201d \u2014 we agree with you that autocurricula by definition are causing changes in strategy , but there is no guarantee that they are distinct shifts ( they could just as easily be small changes in strategy ) . Distinct shifts make it easier to see the effects of an autocurriculum , as small shifts can be hard to detect or analyse . We \u2019 ve changed this clause to \u201c clear evidence that multi-agent self-play can lead to emergent autocurricula with many distinct and compounding phase shifts in agent strategy \u201d . \u2014 \u201c And ( 3 ) advertises `` a proposed framework for evaluating agents in open-ended environments '' and also `` a suite of targeted intelligence tests for our domain '' . The former of those two is either not in the paper or you mean your section `` 6.2 Transfer and Fine-Tuning as Evaluation '' , which is n't novel ( see e.g . [ Alain & Bengio , 2016 ] [ 2 ] ) \u201d After re-reviewing this sentence we agree with you in that it was misleading ; we did not intend claim transfer as our idea but rather that we would like to use transfer to evaluate skill progression in open-ended environments . The reason we think it is a contribution is that in most MARL settings , progress is evaluated through play against humans or through metrics like ELO against past versions or other populations . We will modify it to \u201c a proposal to use transfer as a framework for evaluating agents in open-ended environments ... \u201d . \u2014 \u201c Your acknowledgments should be anonymized until publication . Otherwise , reviewers might draw conclusions which group published this work , thus violating the double-blind review procedure. \u201d Thank you for pointing this out ! \u2014 \u201c `` evidence that ... competition may scale better with increasing environment complexity '' - that 's only shown in the appendix \u201d We believe Figure 5 is also evidence for this , as you increase the observation space complexity , meaningful interaction with objects goes down when you use intrinsic motivation methods . \u2014 \u201c You mention TD-Gammon as a game , but I think it 's an algorithm for the game Backgammon , similarly to how `` Go '' is the game and `` AlphaGo '' is an algorithm for playing. \u201d Thank you for catching this ! \u2014 \u201c Arena boundaries : What 's the penalty and what 's `` too far outside the play area '' ? \u201d Great point . We will update the paper with more clear language around this . We give a -10 reward if the agents go outside an 18 meter square ( which is 9 times the area of the quadrant game shown in the appendix ) . \u2014 \u201c Policy network and fusion are underspecified : How do you deal with the varying number of agents , boxes , obstacles ? Do you just set the x/v of the missing pieces to zero or is the observation actually of a different shape in case there are more/fewer objects or agents ? How 's the embedding done that is depicted in Figure 2 ? Also , I did n't see the embedding being mentioned in the text - any reason for that ? \u201d We have more detail in appendix section B.7 , which answers some of your questions , but we will move some of these details to the main text/caption of Figure 2 and add more clarification . The architecture is an attention and pooling based architecture so it naturally deals with varying numbers of objects . We mask out anything not visible to the agent in the attention and pooling operations so that they do not receive privileged information . The embedding weights are shared within object type , e.g.all box entities pass through the same shared embedding function . We currently show in figure 2 that the observations pass through fully connected layers to create these embeddings , but we \u2019 ll add the comment about shared weights to the caption . \u2014 \u201c Why does the agent 's embedding say `` 1 '' ? Why do the other agents ' embeddings have a `` -1 '' at the end of the orange box and the others do n't ? \u201d The agents have an ego-centric architecture , so that \u201c 1 \u201d shows that that entry is the agent \u2019 s observation of itself ( the agent itself is just 1 entity as opposed to boxes or other agents which are many entities ) . There are ( # agents - 1 ) other agents ( from the view of any single agent ) . We \u2019 ll add this clarification to the figure caption ."}, {"review_id": "SkxpxJBKwS-1", "review_text": "Authors in introduce a new competitive/cooperative physics-based environment in which different teams of agents compete in a visual concealment and search task with visibility-based team-based rewards (although There are no explicit incentives for agents to interact with objects in the environment). They show that, complex behaviour emerge as the episode progresses and agents are able to learn 6 emergent skills/(counter-)strategies (including tool use), where agents intentionally change their environment to suit their needs. Agents trained using self-play In my opinion, this is an excellent paper which main contribution is to provide experimental evidence that relevant and complex skills and strategies can emerge from multi-agent RL competing scenarios. Minor comments: - Hide&seek rules and safety issues: is it not supposed that hiders and the seekers could not get together (i.e., hiders cannot push seekers or as we can see in some videos)? Furthermore, it is surprising (one would say worrying) that hiders identified the barriers as an impediment to the seeker (not only as a way to hide). I wouldn\u2019t say that this is a \u201c human-relevant strategies and skills \u201c as the authors claim. Hider agents even double walled seekers! - Have the authors thought about joining the Animal-AI Olympics (http://animalaiolympics.com/) competition? It would be a great opportunity to to test the skills of your agents in a further general testing scenario. They provide an arena (test-bed) which contains 300 different intelligent tests for testing the cognitive abilities of RL agents (https://www.mdcrosby.com/blog/animalaiprizes1.html) which have to interact with the environment. ", "rating": "8: Accept", "reply_text": "Thank you for the review and questions ! \u2014 \u201c Hide & seek rules and safety issues : is it not supposed that hiders and the seekers could not get together ( i.e. , hiders can not push seekers or as we can see in some videos ) ? Furthermore , it is surprising ( one would say worrying ) that hiders identified the barriers as an impediment to the seeker ( not only as a way to hide ) . I wouldn \u2019 t say that this is a \u201c human-relevant strategies and skills \u201c as the authors claim . Hider agents even double walled seekers ! \u201d In the environment as is , the hiders can push the seekers during the preparation phase . It \u2019 s unclear that this is bad , but we agree that we could easily make it not the case , though it likely would not change the skill progression in the main hide-and-seek environment . However , as you note , this can definitely change the resulting skill progression in other game variants ( Figure A.8 ) . We also believe finding methods that can make agents converge on safe outcomes is an important direction for future research ! \u201c Have the authors thought about joining the Animal-AI Olympics ( http : //animalaiolympics.com/ ) competition ? \u201d \u2014 We thought this would be outside the scope of our current work , but we agree this challenge is very interesting . However , from the description it feels slightly different than the transfer tasks we propose . They say \u201c The goal will always be to retrieve the same food items by interacting with previously seen objects , \u201d where in our transfer tests agents are given very different objectives from the original objective of hide-and-seek ."}, {"review_id": "SkxpxJBKwS-2", "review_text": "1. Summary The authors report on an empirical study of emergent behavior of multiple RL agents learning to play hide-and-seek (a sparse reward task). The main point of this paper is that RL agents learning at scale (large number of samples, batch-size 64000). can learn to solve tasks with strategies that are human-interpretable (e.g., using ramps, boxes). Scale also requires various simplifications (e.g., keeping the learning setup as close as possible to a single-agent problem as possible). Agents are grouped in 2 teams (seekers, hiders). Each agent receives a team reward, e.g., it can be punished for events that it did not participate in, e.g., if a team-mate is seen by an opponent. If hiders are hidden, seekers also automatically see reward. The first 40% of the episode there is no reward to let hiders hide. There is one actor model, all agents share weights. Hence this is self-play: hiders and seekers use the same agent model. Also, all agents use a central value function that can see the entire state (decentralized execution, centralized learning). This makes the setting basically a single-agent problem, with the only decentralized aspect being each actor model only receiving its own observation. Note that a large body of multi-agent RL work in fact uses agents that do not share weights, etc. Other features described: - Auto-curricula: e.g. agents find new strategies (using ramps, boxes) that other agents have to counteract. - Human-relevant skills: They report that the agent model learns multiple ways to interact with (objects in) the environment that are semantically interesting (resembles something humans might do). - Authors compare with policies learning via intrinsic motivation. - Evaluation through transfer learning shows some benefit of transfer of hide-seek agents to auxiliary tasks. However, it is not so clear how this evaluation informs future work on transfer learning (e.g., how would you pick evaluation tasks for a given train-task?) 1. Decision (accept or reject) with one or two key reasons for this choice. Reject. The main point of the paper is empirical RL at scale. Although the learned behaviors are human-interpretable, this does not seem surprising given the fact that in many (large-scale) RL applications (Atari games, Go, DotA 2, Starcraft), it has been observed that RL agents can learn to manipulate and use their environment (which includes other agents!) in unexpected ways / find creative ways to exploit the reward function (see e.g. demos in https://www.alexirpan.com/2018/02/14/rl-hard.html). There has also been work on object-level RL [Agnew, Domingos 2018], which involves agents interacting with objects in the environment. Compared to this, the observation that RL agents learn human-interpretable uses of objects does not seem surprising. The paper also does not give new insights in how to make large-scale RL \"'work'. For instance, there are no significant differences in algorithm / model structure from DotA / Starcraft agents that can inform future large-scale experiments. The paper also does not introduce new concrete evaluation metrics that can apply to other tasks / RL problems, skill detection / segmentation methods to learn the structure of auto-curricula. Furthermore, the setup is very close to a single-agent problem (see above), and is far simpler in the multi-agent assumptions from other decentralized multi-agent work (Foerster 2018, Jacques 2019, etc).", "rating": "3: Weak Reject", "reply_text": "Thank you for your review and constructive criticisms ! We \u2019 ll try to address each piece of criticism in turn . \u2014 \u201c The main point of the paper is empirical RL at scale \u201d The main point we hope to convey is that large-scale multi-agent reinforcement learning ( MARL ) can lead to self-supervised autocurricula in which agents learn successively more complex human-relevant skills such as construction and tool use . We absolutely agree that there have been many amazing previous results from MARL at scale , and we acknowledge many of the works you mention and more in our introduction and related work sections . We believe our work differs from these in that our environment is built from very simple components in a physically grounded simulator , making it extremely extensible . It is much more clear how one could add to or modify the hide-and-seek environment to include more human-relevant components than it is how one could modify games like Go , Dota , or Starcraft . \u2014 \u201c There has also been work on object-level RL \u2026 the observation that RL agents learn human-interpretable uses of objects does not seem surprising. \u201d We agree that there has been much work on object-level RL . We didn \u2019 t advertise this as a novel portion of our work , and we \u2019 ve already included many citations that use object-level architectures and attention at the end of Section 4 . We also acknowledge that there have been prior works where RL learns human-interpretable uses of objects , which is why we include a paragraph in Section 2 on prior work in tool-use ; however , our work can be distinguished from these and the work you cite in that we provide no explicit signal for interacting with objects ; the pressure to interact with the objects is solely a result of multi-agent competition . \u2014 \u201c The paper also does not give new insights in how to make large-scale RL work \u201d This paper was not on how to make large-scale RL work , but rather on showing the power of current large-scale RL algorithms in a new setting that is more physically grounded and human-relevant than previous settings like DotA , Starcraft , and Go . The main argument of the paper is that multi-agent autocurricula can lead to agents learning many human-relevant skills like tool-use and construction ; the fact that we required no new significant algorithmic modifications actually strengthens this point in our opinion , as the results can \u2019 t be confused as a pathology of a new specific algorithm . That being said , we agree that it is a great direction for future research to incorporate methods that can learn faster or better in this environment . \u2014 \u201c The paper also does not introduce new concrete evaluation metrics that can apply to other tasks / RL problems ... \u201d It \u2019 s very hard to create transfer tasks that are valid across domains . However , we hope that the tasks we proposed can be used as transfer metrics for any future research within our domain ( both of which we will open source ) . \u2014 \u201c There is one actor model , all agents share weights \u201d Using shared weights , or at least some portion of training data coming from self-play , is very common ( AlphaGo , DotA , Alphastar , Capture-the-Flag , NeuralMMO , etc . ) , and it doesn \u2019 t alter the multi-agent optimization objective . Each agent still takes a greedy gradient and has its own observations and memory state so that at execution they use no privileged information . Shared weights does not mean uni-brain ( one brain many actions ) , which would indeed reduce this to a single agent problem . That being said , we \u2019 ve run the hide-and-seek experiment with separate weights for each agent and as expected have seen no difference in learned strategy . \u2014 \u201c all agents use a central value function that can see the entire state . This makes the setting basically a single-agent problem and is far simpler in the multi-agent assumptions from other decentralized multi-agent work \u201d This is a commonly used method to reduce policy gradient variance in partially observed settings without letting agents cheat at execution time both for MARL ( MADDPG , Counterfactual RL , AlphaStar ) and also single agent RL ( Dactyl , Asymmetric Actor Critic ) . We ablate this choice in the appendix and find that it is important at the given scale of compute but agents still learn without it . As for other MARL algorithms , we cite both of the works you mention in our paper already , and it is an excellent line of future research to incorporate methods like these into setups such as hide-and-seek to see if they bring benefit to learning . However , we don \u2019 t think algorithmic simplicity is a fault of our work but rather a strength . We show that with only standard simple algorithms , multi-agent autocurricula can lead to human-relevant skills like construction and tool-use in physically grounded environments , which we believe provides a good baseline for future algorithmic research ."}], "0": {"review_id": "SkxpxJBKwS-0", "review_text": "# Review ICLR20, Emergent Tool Use... This review is for the originally uploaded version of this article. Comments from other reviewers and revisions have deliberately not been taken into account. After publishing this review, this reviewer will participate in the forum discussion and help the authors improve the paper. I apologize in advance for being reviewer 2. ## Overall **Summary** The article introduces a new multi-agent physics environment called \"hide-and-seek\". The authors trained agents in this environment and studied the emergence of and changes in strategies. The authors also study the performance of these same agents in new \"targeted intelligence tests\" compared to training from scratch and compared to agents trained with curiosity. **Overall Opinion** I think the environment is very appealing and the paper is overall well-structured and demonstrates novel work. Therefore I'd recommend this paper to be accepted. That being said, there are glaring issues with some of the writing that need to be addressed before I think this work conforms to the standards of ICLR. However, if these issues are addressed, I have no issue increasing my review score. Main problems: - The majority of the paper presents essentially a case study of what happened during a single seed of policy training. For RL literature that's very uncommon and I think it's consensus that DRL is very sensitive to random seeds. I know that you do have additional seeds in the appendix, but why didn't you mention those in the main body of the paper? You seem to have found some robustness against multiple seeds, so why not show it? And also the fact that Figure 1 & 3 only apply to 1 seed is not mentioned. I think this is easy enough to fix - I suggest since you're already at 10 pages, to just bring in the additional seeds from the appendix and average over their performance in Fig.1&3. - The contributions section is overselling the work: (1) states that autocurricula lead to changes in agent strategy - Maybe I'm mistaken here but that sounds like a tautology. In other words, \"a self-generated sequence of challenges\" (\"Autocurricula\", according to [Leibo et al., 2019][1]) lead to changes in strategy. And (3) advertises \"a proposed framework for evaluating agents in open-ended environments\" and also \"a suite of targeted intelligence tests for our domain\". The former of those two is either not in the paper or you mean your section \"6.2 Transfer and Fine-Tuning as Evaluation\", which isn't novel (see e.g. [Alain & Bengio, 2016][2]) - Your acknowledgments should be anonymized until publication. Otherwise, reviewers might draw conclusions which group published this work, thus violating the double-blind review procedure. [1]: https://arxiv.org/pdf/1903.00742.pdf [2]: https://arxiv.org/pdf/1610.01644.pdf Like I mentioned above, I think these are all easy to address, which should allow acceptance of this work. Here are some additional questions, comments, and nitpicks: ## Specific comments and questions ### Abstract - \"evidence that ... competition may scale better with increasing environment complexity\" - that's only shown in the appendix ### Intro - You mention TD-Gammon as a game, but I think it's an algorithm for the game Backgammon, similarly to how \"Go\" is the game and \"AlphaGo\" is an algorithm for playing. ### Rel. Work - all good ### Hide And Seek - Arena boundaries: What's the penalty and what's \" too far outside the play area\"? And in all depictions, it looks like the geometry of the arena is elevated around the edges and the agents don't have a jump action, so how would they ever go out of borders? After watching the videos: Apparently, the jagged-looking arena boundary in the videos is purely cosmetic and agents can still access that space. This is unclear from just the paper and the renderings in Figure 1. ### Policy Optimization - Policy network and fusion are underspecified: How do you deal with the varying number of agents, boxes, obstacles? Do you just set the x/v of the missing pieces to zero or is the observation actually of a different shape in case there are more/fewer objects or agents? How's the embedding done that is depicted in Figure 2? Also, I didn't see the embedding being mentioned in the text - any reason for that? - Figure 2 - This diagram is visually appealing but confusing and needs to be improved. Why does the agent's embedding say \"1\"? Why do the other agents' embeddings have a \"-1\" at the end of the orange box and the others don't? Is the agent's embedding concatenated with the other embeddings? If so, why and how (concat, sum, multiply, conditional batch norm, etc.)? In the center and on the right you use a blue block to indicate the agent's embedding and then at the bottom right you seem to use it as a network component or something (between the \"LSTM\")? If you're trying to signal that this is the agent's perception at different stages in the network, I'd use a different color to separate it from the agent's lidar and pos/vel. You don't mention that \"x,v\" stands for \"position, velocity\". ### Auto-curriculum and Emergent Behavior - Figure 3: \"environment-specific\" (add dash). Draw skill development boundaries like in Fig.1. - How exactly does the \"surfing\" work? The seekers step (not jump, right, since there is no jumping?) onto the boxes and then what? The momentum propels the box forward? Do other seekers push the box? Their movement on top of the box somehow moves the box (this seems to be the case judging by the videos but this is the least physically plausible)? This is a super interesting adaptation but I'd suspect the physics simulation to have a bug/glitch that's being exploited here. - You mention in footnote 3 that the developmental stage and changes in reward aren't necessarily correlated. The same seems to be true for the metrics in Fig.3, which raises the question how did you come up with those boundaries for the different developmental stages in Fig.1? Did someone look at rollouts from the trained policy every couple of million steps? And do all agents learn new skills at the same time or is there a delay? From my understanding, they are all using the same policy and critic networks but maybe dependent on the proximity of an agent to an object/obstacle, it's easier or harder to execute. ### Evaluation - clear and well-written, slightly too much content in the appendix and not enough in the main paper. Weird appendix numbering - A.6 appears in the main paper pages after A.7 ### Discussion and Future Work - all good ### Appendix - I appreciate the TOC. I did not look into Appendix B-D because it's another 10 pages on top of the 10 pages of the article. All in all an interesting work. Good luck with the rebuttal/discussion.", "rating": "6: Weak Accept", "reply_text": "Thank you for the very detailed review and constructive criticisms ! \u2014 \u201c The majority of the paper presents essentially a case study of what happened during a single seed of policy training ... \u201d Great point , we will update Figures 1 and 3 to be the average across the 3 seeds we show in the appendix . We found very little seed dependence throughout the project , which is likely why we made this oversight . \u2014 \u201c The contributions section is overselling the work : ( 1 ) states that autocurricula lead to changes in agent strategy - Maybe I 'm mistaken here but that sounds like a tautology . In other words , `` a self-generated sequence of challenges '' ( `` Autocurricula '' , according to [ Leibo et al. , 2019 ] [ 1 ] ) lead to changes in strategy. \u201d In this sentence we were trying to place emphasis on \u201c distinct and compounding phase shifts \u201d \u2014 we agree with you that autocurricula by definition are causing changes in strategy , but there is no guarantee that they are distinct shifts ( they could just as easily be small changes in strategy ) . Distinct shifts make it easier to see the effects of an autocurriculum , as small shifts can be hard to detect or analyse . We \u2019 ve changed this clause to \u201c clear evidence that multi-agent self-play can lead to emergent autocurricula with many distinct and compounding phase shifts in agent strategy \u201d . \u2014 \u201c And ( 3 ) advertises `` a proposed framework for evaluating agents in open-ended environments '' and also `` a suite of targeted intelligence tests for our domain '' . The former of those two is either not in the paper or you mean your section `` 6.2 Transfer and Fine-Tuning as Evaluation '' , which is n't novel ( see e.g . [ Alain & Bengio , 2016 ] [ 2 ] ) \u201d After re-reviewing this sentence we agree with you in that it was misleading ; we did not intend claim transfer as our idea but rather that we would like to use transfer to evaluate skill progression in open-ended environments . The reason we think it is a contribution is that in most MARL settings , progress is evaluated through play against humans or through metrics like ELO against past versions or other populations . We will modify it to \u201c a proposal to use transfer as a framework for evaluating agents in open-ended environments ... \u201d . \u2014 \u201c Your acknowledgments should be anonymized until publication . Otherwise , reviewers might draw conclusions which group published this work , thus violating the double-blind review procedure. \u201d Thank you for pointing this out ! \u2014 \u201c `` evidence that ... competition may scale better with increasing environment complexity '' - that 's only shown in the appendix \u201d We believe Figure 5 is also evidence for this , as you increase the observation space complexity , meaningful interaction with objects goes down when you use intrinsic motivation methods . \u2014 \u201c You mention TD-Gammon as a game , but I think it 's an algorithm for the game Backgammon , similarly to how `` Go '' is the game and `` AlphaGo '' is an algorithm for playing. \u201d Thank you for catching this ! \u2014 \u201c Arena boundaries : What 's the penalty and what 's `` too far outside the play area '' ? \u201d Great point . We will update the paper with more clear language around this . We give a -10 reward if the agents go outside an 18 meter square ( which is 9 times the area of the quadrant game shown in the appendix ) . \u2014 \u201c Policy network and fusion are underspecified : How do you deal with the varying number of agents , boxes , obstacles ? Do you just set the x/v of the missing pieces to zero or is the observation actually of a different shape in case there are more/fewer objects or agents ? How 's the embedding done that is depicted in Figure 2 ? Also , I did n't see the embedding being mentioned in the text - any reason for that ? \u201d We have more detail in appendix section B.7 , which answers some of your questions , but we will move some of these details to the main text/caption of Figure 2 and add more clarification . The architecture is an attention and pooling based architecture so it naturally deals with varying numbers of objects . We mask out anything not visible to the agent in the attention and pooling operations so that they do not receive privileged information . The embedding weights are shared within object type , e.g.all box entities pass through the same shared embedding function . We currently show in figure 2 that the observations pass through fully connected layers to create these embeddings , but we \u2019 ll add the comment about shared weights to the caption . \u2014 \u201c Why does the agent 's embedding say `` 1 '' ? Why do the other agents ' embeddings have a `` -1 '' at the end of the orange box and the others do n't ? \u201d The agents have an ego-centric architecture , so that \u201c 1 \u201d shows that that entry is the agent \u2019 s observation of itself ( the agent itself is just 1 entity as opposed to boxes or other agents which are many entities ) . There are ( # agents - 1 ) other agents ( from the view of any single agent ) . We \u2019 ll add this clarification to the figure caption ."}, "1": {"review_id": "SkxpxJBKwS-1", "review_text": "Authors in introduce a new competitive/cooperative physics-based environment in which different teams of agents compete in a visual concealment and search task with visibility-based team-based rewards (although There are no explicit incentives for agents to interact with objects in the environment). They show that, complex behaviour emerge as the episode progresses and agents are able to learn 6 emergent skills/(counter-)strategies (including tool use), where agents intentionally change their environment to suit their needs. Agents trained using self-play In my opinion, this is an excellent paper which main contribution is to provide experimental evidence that relevant and complex skills and strategies can emerge from multi-agent RL competing scenarios. Minor comments: - Hide&seek rules and safety issues: is it not supposed that hiders and the seekers could not get together (i.e., hiders cannot push seekers or as we can see in some videos)? Furthermore, it is surprising (one would say worrying) that hiders identified the barriers as an impediment to the seeker (not only as a way to hide). I wouldn\u2019t say that this is a \u201c human-relevant strategies and skills \u201c as the authors claim. Hider agents even double walled seekers! - Have the authors thought about joining the Animal-AI Olympics (http://animalaiolympics.com/) competition? It would be a great opportunity to to test the skills of your agents in a further general testing scenario. They provide an arena (test-bed) which contains 300 different intelligent tests for testing the cognitive abilities of RL agents (https://www.mdcrosby.com/blog/animalaiprizes1.html) which have to interact with the environment. ", "rating": "8: Accept", "reply_text": "Thank you for the review and questions ! \u2014 \u201c Hide & seek rules and safety issues : is it not supposed that hiders and the seekers could not get together ( i.e. , hiders can not push seekers or as we can see in some videos ) ? Furthermore , it is surprising ( one would say worrying ) that hiders identified the barriers as an impediment to the seeker ( not only as a way to hide ) . I wouldn \u2019 t say that this is a \u201c human-relevant strategies and skills \u201c as the authors claim . Hider agents even double walled seekers ! \u201d In the environment as is , the hiders can push the seekers during the preparation phase . It \u2019 s unclear that this is bad , but we agree that we could easily make it not the case , though it likely would not change the skill progression in the main hide-and-seek environment . However , as you note , this can definitely change the resulting skill progression in other game variants ( Figure A.8 ) . We also believe finding methods that can make agents converge on safe outcomes is an important direction for future research ! \u201c Have the authors thought about joining the Animal-AI Olympics ( http : //animalaiolympics.com/ ) competition ? \u201d \u2014 We thought this would be outside the scope of our current work , but we agree this challenge is very interesting . However , from the description it feels slightly different than the transfer tasks we propose . They say \u201c The goal will always be to retrieve the same food items by interacting with previously seen objects , \u201d where in our transfer tests agents are given very different objectives from the original objective of hide-and-seek ."}, "2": {"review_id": "SkxpxJBKwS-2", "review_text": "1. Summary The authors report on an empirical study of emergent behavior of multiple RL agents learning to play hide-and-seek (a sparse reward task). The main point of this paper is that RL agents learning at scale (large number of samples, batch-size 64000). can learn to solve tasks with strategies that are human-interpretable (e.g., using ramps, boxes). Scale also requires various simplifications (e.g., keeping the learning setup as close as possible to a single-agent problem as possible). Agents are grouped in 2 teams (seekers, hiders). Each agent receives a team reward, e.g., it can be punished for events that it did not participate in, e.g., if a team-mate is seen by an opponent. If hiders are hidden, seekers also automatically see reward. The first 40% of the episode there is no reward to let hiders hide. There is one actor model, all agents share weights. Hence this is self-play: hiders and seekers use the same agent model. Also, all agents use a central value function that can see the entire state (decentralized execution, centralized learning). This makes the setting basically a single-agent problem, with the only decentralized aspect being each actor model only receiving its own observation. Note that a large body of multi-agent RL work in fact uses agents that do not share weights, etc. Other features described: - Auto-curricula: e.g. agents find new strategies (using ramps, boxes) that other agents have to counteract. - Human-relevant skills: They report that the agent model learns multiple ways to interact with (objects in) the environment that are semantically interesting (resembles something humans might do). - Authors compare with policies learning via intrinsic motivation. - Evaluation through transfer learning shows some benefit of transfer of hide-seek agents to auxiliary tasks. However, it is not so clear how this evaluation informs future work on transfer learning (e.g., how would you pick evaluation tasks for a given train-task?) 1. Decision (accept or reject) with one or two key reasons for this choice. Reject. The main point of the paper is empirical RL at scale. Although the learned behaviors are human-interpretable, this does not seem surprising given the fact that in many (large-scale) RL applications (Atari games, Go, DotA 2, Starcraft), it has been observed that RL agents can learn to manipulate and use their environment (which includes other agents!) in unexpected ways / find creative ways to exploit the reward function (see e.g. demos in https://www.alexirpan.com/2018/02/14/rl-hard.html). There has also been work on object-level RL [Agnew, Domingos 2018], which involves agents interacting with objects in the environment. Compared to this, the observation that RL agents learn human-interpretable uses of objects does not seem surprising. The paper also does not give new insights in how to make large-scale RL \"'work'. For instance, there are no significant differences in algorithm / model structure from DotA / Starcraft agents that can inform future large-scale experiments. The paper also does not introduce new concrete evaluation metrics that can apply to other tasks / RL problems, skill detection / segmentation methods to learn the structure of auto-curricula. Furthermore, the setup is very close to a single-agent problem (see above), and is far simpler in the multi-agent assumptions from other decentralized multi-agent work (Foerster 2018, Jacques 2019, etc).", "rating": "3: Weak Reject", "reply_text": "Thank you for your review and constructive criticisms ! We \u2019 ll try to address each piece of criticism in turn . \u2014 \u201c The main point of the paper is empirical RL at scale \u201d The main point we hope to convey is that large-scale multi-agent reinforcement learning ( MARL ) can lead to self-supervised autocurricula in which agents learn successively more complex human-relevant skills such as construction and tool use . We absolutely agree that there have been many amazing previous results from MARL at scale , and we acknowledge many of the works you mention and more in our introduction and related work sections . We believe our work differs from these in that our environment is built from very simple components in a physically grounded simulator , making it extremely extensible . It is much more clear how one could add to or modify the hide-and-seek environment to include more human-relevant components than it is how one could modify games like Go , Dota , or Starcraft . \u2014 \u201c There has also been work on object-level RL \u2026 the observation that RL agents learn human-interpretable uses of objects does not seem surprising. \u201d We agree that there has been much work on object-level RL . We didn \u2019 t advertise this as a novel portion of our work , and we \u2019 ve already included many citations that use object-level architectures and attention at the end of Section 4 . We also acknowledge that there have been prior works where RL learns human-interpretable uses of objects , which is why we include a paragraph in Section 2 on prior work in tool-use ; however , our work can be distinguished from these and the work you cite in that we provide no explicit signal for interacting with objects ; the pressure to interact with the objects is solely a result of multi-agent competition . \u2014 \u201c The paper also does not give new insights in how to make large-scale RL work \u201d This paper was not on how to make large-scale RL work , but rather on showing the power of current large-scale RL algorithms in a new setting that is more physically grounded and human-relevant than previous settings like DotA , Starcraft , and Go . The main argument of the paper is that multi-agent autocurricula can lead to agents learning many human-relevant skills like tool-use and construction ; the fact that we required no new significant algorithmic modifications actually strengthens this point in our opinion , as the results can \u2019 t be confused as a pathology of a new specific algorithm . That being said , we agree that it is a great direction for future research to incorporate methods that can learn faster or better in this environment . \u2014 \u201c The paper also does not introduce new concrete evaluation metrics that can apply to other tasks / RL problems ... \u201d It \u2019 s very hard to create transfer tasks that are valid across domains . However , we hope that the tasks we proposed can be used as transfer metrics for any future research within our domain ( both of which we will open source ) . \u2014 \u201c There is one actor model , all agents share weights \u201d Using shared weights , or at least some portion of training data coming from self-play , is very common ( AlphaGo , DotA , Alphastar , Capture-the-Flag , NeuralMMO , etc . ) , and it doesn \u2019 t alter the multi-agent optimization objective . Each agent still takes a greedy gradient and has its own observations and memory state so that at execution they use no privileged information . Shared weights does not mean uni-brain ( one brain many actions ) , which would indeed reduce this to a single agent problem . That being said , we \u2019 ve run the hide-and-seek experiment with separate weights for each agent and as expected have seen no difference in learned strategy . \u2014 \u201c all agents use a central value function that can see the entire state . This makes the setting basically a single-agent problem and is far simpler in the multi-agent assumptions from other decentralized multi-agent work \u201d This is a commonly used method to reduce policy gradient variance in partially observed settings without letting agents cheat at execution time both for MARL ( MADDPG , Counterfactual RL , AlphaStar ) and also single agent RL ( Dactyl , Asymmetric Actor Critic ) . We ablate this choice in the appendix and find that it is important at the given scale of compute but agents still learn without it . As for other MARL algorithms , we cite both of the works you mention in our paper already , and it is an excellent line of future research to incorporate methods like these into setups such as hide-and-seek to see if they bring benefit to learning . However , we don \u2019 t think algorithmic simplicity is a fault of our work but rather a strength . We show that with only standard simple algorithms , multi-agent autocurricula can lead to human-relevant skills like construction and tool-use in physically grounded environments , which we believe provides a good baseline for future algorithmic research ."}}