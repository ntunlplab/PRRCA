{"year": "2020", "forum": "B1em9h4KDS", "title": "Generative Imputation and Stochastic Prediction", "decision": "Reject", "meta_review": "The paper proposes a method that does uncertainty modeling over missing data imputation using a framework based on generative adversarial network. While the method shows some empirical improvements over the baselines, reviewers have found the work incremental in terms of technical novelty over the existing GAIN approach which renders it slightly below the acceptance threshold for the main conference, particularly in case of space constraints in the program. ", "reviews": [{"review_id": "B1em9h4KDS-0", "review_text": "This is a nice piece of incremental work on top of previously published GAN imputation methods. It seems to work well in the limited evaluation and is at least claimed to be easier to use for practitioners. This paper could benefit tremendously from both better evaluation and discussion. The paper would be much clearer if GI contextualized itself relative to GAIN on the one hand (which is the most similar GAN method) and multiple imputation on the other hand (of which this is almost, but not quite, an instance of). Suggestions for improving the introduction & discussion: * The purpose of this paper is to model uncertainties about missing values \u2014 you really should say more about probabilistic methods than \"A few exceptions exist such as Bayesian models\u201d. At least give some motivation for why certain imputations problems couldn\u2019t be feasibly solved by modeling the missing values in a probabilistic programming framework. * Other GAN methods for imputation (GAIN and MisGAN) are dismissed as \"often very complicated to be applied in practical setups by practitioners\u201d. Given that the described method resembles GAIN, is it really much simpler? If so, can you be more specific when characterizing related work? * \"This is different from approaches such as multiple imputation where several predictors are trained on different imputed versions of a dataset.\u201d \u2014 the main difference between this approach and MI is that you\u2019re interleaving imputation and training a downstream model. Emphasize this earlier on, since it will make the whole technique easier to understand. Suggestions for improving the evaluation: * You\u2019re imputing missing rectangles from an image dataset \u2014 please show us the resulting images. I would greatly prefer this to Section 4.5 \u2014 which is a very low sample size, low dimensionality example and it\u2019s really unclear how well it generalizes to real data. * \"We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution.\u201d \u2014 I think this is a mistake, likely motivated by the proposed method doing worse under the RMSE metric. Show us several relevant metrics and then discuss their tradeoffs afterward. * \"We run each experiment multiple times (at least 4)\u201d \u2014 please report how often each experiment was run, even better if you standardize this number. * For Table 2, please provide accuracy without missing values as a baseline. * Add MICE or some other \u201cstandard\u201d imputation method as a baseline. Suggestions for improving readability: * Many sentences start with \u201cin this\u201d (e.g. \u201cin this case\u201d, \u201cin this setting\u201d, &c). Sometimes these sentences even co-occur within the same paragraph. Try to switch up the phrasing and move away from repetition. * Not a complete sentence: \"For instance, jointly training multiple generator/discriminator networks, tuning objective functions with multiple hyper-parameters, etc.\" Update: I think the latest draft of the paper is a big improvement, the inclusion of a \"classical\" baseline, improved language and additional appendices are all welcome. I'm leaving the rating as a \"weak accept\" since the paper still feels rough and could use additional editing/streamlining. ", "rating": "6: Weak Accept", "reply_text": "* Comment : \u201c `` We also considered using root means squared error ( RMSE ) ; however , we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution. \u201d \u2014 I think this is a mistake , likely motivated by the proposed method doing worse under the RMSE metric . Show us several relevant metrics and then discuss their tradeoffs afterward. \u201d In appendix F of the revised version , we report RMSE values for all experiments . We also provided a discussion about how RMSE values compare to FID values as well as how different imputation methods perform based on the RMSE measure . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c `` We run each experiment multiple times ( at least 4 ) \u201d \u2014 please report how often each experiment was run , even better if you standardize this number. \u201d To address your comment we revised the sentence to be more precise about the number of runs ( first paragraph of Section 4.3 ) : \u201c We run each experiment multiple times : 4 times for CIFAR-10 and 8 times for tabular datasets . We report the mean and standard deviation of results for each case. \u201c -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c For Table 2 , please provide accuracy without missing values as a baseline. \u201d As suggested , we included baseline accuracies for complete datasets ( 0 % missing rate ) as a footnote to Table 2 . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c Add MICE or some other \u201c standard \u201d imputation method as a baseline. \u201d To address your comment , we included MICE imputation results for all tabular datasets . See the revised Table 2 . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Suggestions for improving readability : * Comment : \u201c Many sentences start with \u201c in this \u201d ( e.g. \u201c in this case \u201d , \u201c in this setting \u201d , & c ) . Sometimes these sentences even co-occur within the same paragraph . Try to switch up the phrasing and move away from repetition. \u201d As suggested , we revised the paper to be more clear and reduced unnecessary repetitions . We modified about 10 instances of \u201c In this \u201d appearing at the beginning of a sentence throughout the paper . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c Not a complete sentence : `` For instance , jointly training multiple generator/discriminator networks , tuning objective functions with multiple hyper-parameters , etc . '' Thank you for pointing this out . In the revised version , we changed the sentence to be more explicit and fixed the grammar issues : \u201c For instance , Yoon et al . ( 2018 ) requires setting hyperparameters to adjust the influence of an MSE loss term as well as the rate of discriminator hint vectors . Also as another example , Li et al . ( 2019 ) uses three generators and three discriminators for the final imputer architecture . \u201d"}, {"review_id": "B1em9h4KDS-1", "review_text": "The uncertainty of having a missing value is investigated on the prediction by not assigning a single imputed value but N different values generated via an imputer network (based on GAIN). Unlike old-fashion multiple imputation techniques, one predictor is trained on different samples and it induces the uncertainty. The experiments on number of datasets show the proposed predictor is capable of having a fairly better performance. Overall, this paper raises an interesting point about missing data imputation via generative models, and well-written; however, there are number of concerns: 1- The predictor is trained on different version of imputed samples (imputed via the generator); this equates to making noisy version of the real samples, where noise is applied to the missing variables. A side effect of this is generalization of the predictor; thus, have you been careful that the improved accuracy is not due to generalization? In other words, if we imposed the generalization via adding gaussian noise to the imputed samples by GAIN for example, would we get improved accuracy too? 2- Your method known as GI is a modified version of GAIN. You could also use MisGAN, and I am wondering if the results would have been different if generator in MisGAN was used in GI in Figure 2 (b), as MisGAN works better than GAIN. 3- I am also wondering how the GAIN imputation changes by removing the MSE term? GI discards the MSE term in GAIN, and it changes the distribution of the imputed variables by GAIN. Could you maybe fit a Normal distribution on a chosen imputed variable (N=128) and visualize how different it is from the distribution of imputed variables with GAIN with MSE term. 4- In justification for claim 1, it is said \u201cThis is equivalent to training models using noisy labels\u201d. This is not accurate: in noisy label prediction, we have one (noisy) y corresponding to each x, in your case there are multiple ys for one sample. 5- In the implementation details, I cannot fully wrap my head around the part \u201cz vector of size 1/8\u201d; how did you choose this 1/8? ", "rating": "6: Weak Accept", "reply_text": "* Comment : \u201c 3- I am also wondering how the GAIN imputation changes by removing the MSE term ? GI discards the MSE term in GAIN , and it changes the distribution of the imputed variables by GAIN . Could you maybe fit a Normal distribution on a chosen imputed variable ( N=128 ) and visualize how different it is from the distribution of imputed variables with GAIN with MSE term. \u201d As suggested , we conducted an experiment to visualize the impact of MSE term on the imputed distribution for a synthesized dataset . The results of this comparison are included in Appendix H of the paper . As can be seen from the comparison , the MSE term induces a bias toward the mean and if we increase the hyper-parameter controlling the MSE term the variance of the generated samples is decreased . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c 4- In justification for claim 1 , it is said \u201c This is equivalent to training models using noisy labels \u201d . This is not accurate : in noisy label prediction , we have one ( noisy ) y corresponding to each x , in your case there are multiple ys for one sample. \u201d We understand the reviewer \u2019 s concern ; the case of having noisy labels and having imputed samples with different real labels are not exactly equivalent in general . However , if we consider the equivalence in terms of gradient-based training ; it can be a valid statement to say that these two cases are very similar . Note that , during the optimization , we are doing an update based on gradients of loss terms that come from different versions of an imputed sample in which a few cases have a different assigned label as the correct underlying label . Intuitively , we are using misleading gradient terms as it happens in gradient-based learning using noisy labels . If we consider the average impact on gradients for batches of samples rather than the individual cases , the overall impact on the training would be very similar . In order to address the reviewer 's concern , we revised the paper to be more precise about this and omitted the term \u201c equivalent \u201d ( see the last paragraph of Claim 1 in Section 3.3 ) . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c 5- In the implementation details , I can not fully wrap my head around the part \u201c z vector of size 1/8 \u201d ; how did you choose this 1/8 ? \u201d We use the z vector as the source of randomness for the generator network . A z vector of very small size would not be effective enough and may result in deterministic imputations given a set of observed features . On the other hand , a very large z vector would unnecessarily increase the size of the input layer for the generator network causing training issues in practice . Regarding the specific choice of 1/8 , we conducted initial experiments using different sizes for the z vector . We found that the final performance is not too sensitive to the choice of z size ( as long as it is not relatively small or large ) . In this paper , we suggest the 1/8 ratio as it is a reasonable choice that does not add a huge overhead and at the same time is strong enough to induce randomness to the generator . For instance , the original implementation of GAN on MNIST ( Goodfellow et al.2014 ) uses a z of size 100 for inputs of size 784 , roughly the same ratio ."}, {"review_id": "B1em9h4KDS-2", "review_text": "This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance. Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN's and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis. There are several parts that are confusing/missing in the paper: - In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this. - On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples? - Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch. - I think the statement in the first paragraph in Section 4.4 that \"MSE loss term would act as a denoising loss smoothing noisy missing pixels\" could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear. I think the paper would benefit if the authors could explain/show: - Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?", "rating": "6: Weak Accept", "reply_text": "* Comment : \u201c Can you explain the difference between the results in Figure-7 and Table-2 ? Results between the two mismatch. \u201d Thank you for pointing out the inconsistency . Table 2 provides final classification accuracies for Landsat . To generate results related Figure 7 , we used less number of training epochs and did not use the optimal hyper-parameter settings . To address this issue , we rerun the ablation experiments and updated Figure 7 in the revised version to use a similar setup as used for Table 2 . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c I think the statement in the first paragraph in Section 4.4 that `` MSE loss term would act as a denoising loss smoothing noisy missing pixels '' could be misleading . MSE is used with mask in GAIN , hence it only applies to the observed features during training . Its effect on smoothing noisy missing pixels is not clear. \u201d We understand the reviewer \u2019 s concern that the MSE loss is not exactly similar to what we have for denoising autoencoders . However , based on our experiments , its overall impact is very similar to autoencoders . Note that the generator network is partially receiving an autoencoder loss term which is well-known to cause over-smoothing . To address the comment and prevent misleading our readers , we revised the sentence to be more scientifically accurate ( first paragraph in Section 4.4 ) : \u201c One possible explanation for this behavior might be the fact that GAIN has an MSE loss term acting similar to an autoencoder loss smoothing noisy missing pixels. \u201d -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c I think the paper would benefit if the authors could explain/show : - Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution . Especially in rectangular generation part where it can remove a complete object . Does stochastic averaging benefit more in this case ? \u201d As suggested by the reviewer , we conducted a new experiment to show the benefit of stochastic averaging over simple classification for rectangular missingness ranging from 20 % to 90 % ( see Table 6 in Appendix D ) . Last paragraph of Appendix D : \u201c From Table 6 , it can be inferred that as the rate of missingness increases , the benefits of the suggested predictor algorithm increase significantly . We hypothesize that at higher rates of missingness , the conditional distribution of missing features becomes multimodal . In such a scenario , the suggested method captures the uncertainties over the target distribution resulting in the predictor to make more reliable class assignments . \u201d"}], "0": {"review_id": "B1em9h4KDS-0", "review_text": "This is a nice piece of incremental work on top of previously published GAN imputation methods. It seems to work well in the limited evaluation and is at least claimed to be easier to use for practitioners. This paper could benefit tremendously from both better evaluation and discussion. The paper would be much clearer if GI contextualized itself relative to GAIN on the one hand (which is the most similar GAN method) and multiple imputation on the other hand (of which this is almost, but not quite, an instance of). Suggestions for improving the introduction & discussion: * The purpose of this paper is to model uncertainties about missing values \u2014 you really should say more about probabilistic methods than \"A few exceptions exist such as Bayesian models\u201d. At least give some motivation for why certain imputations problems couldn\u2019t be feasibly solved by modeling the missing values in a probabilistic programming framework. * Other GAN methods for imputation (GAIN and MisGAN) are dismissed as \"often very complicated to be applied in practical setups by practitioners\u201d. Given that the described method resembles GAIN, is it really much simpler? If so, can you be more specific when characterizing related work? * \"This is different from approaches such as multiple imputation where several predictors are trained on different imputed versions of a dataset.\u201d \u2014 the main difference between this approach and MI is that you\u2019re interleaving imputation and training a downstream model. Emphasize this earlier on, since it will make the whole technique easier to understand. Suggestions for improving the evaluation: * You\u2019re imputing missing rectangles from an image dataset \u2014 please show us the resulting images. I would greatly prefer this to Section 4.5 \u2014 which is a very low sample size, low dimensionality example and it\u2019s really unclear how well it generalizes to real data. * \"We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution.\u201d \u2014 I think this is a mistake, likely motivated by the proposed method doing worse under the RMSE metric. Show us several relevant metrics and then discuss their tradeoffs afterward. * \"We run each experiment multiple times (at least 4)\u201d \u2014 please report how often each experiment was run, even better if you standardize this number. * For Table 2, please provide accuracy without missing values as a baseline. * Add MICE or some other \u201cstandard\u201d imputation method as a baseline. Suggestions for improving readability: * Many sentences start with \u201cin this\u201d (e.g. \u201cin this case\u201d, \u201cin this setting\u201d, &c). Sometimes these sentences even co-occur within the same paragraph. Try to switch up the phrasing and move away from repetition. * Not a complete sentence: \"For instance, jointly training multiple generator/discriminator networks, tuning objective functions with multiple hyper-parameters, etc.\" Update: I think the latest draft of the paper is a big improvement, the inclusion of a \"classical\" baseline, improved language and additional appendices are all welcome. I'm leaving the rating as a \"weak accept\" since the paper still feels rough and could use additional editing/streamlining. ", "rating": "6: Weak Accept", "reply_text": "* Comment : \u201c `` We also considered using root means squared error ( RMSE ) ; however , we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution. \u201d \u2014 I think this is a mistake , likely motivated by the proposed method doing worse under the RMSE metric . Show us several relevant metrics and then discuss their tradeoffs afterward. \u201d In appendix F of the revised version , we report RMSE values for all experiments . We also provided a discussion about how RMSE values compare to FID values as well as how different imputation methods perform based on the RMSE measure . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c `` We run each experiment multiple times ( at least 4 ) \u201d \u2014 please report how often each experiment was run , even better if you standardize this number. \u201d To address your comment we revised the sentence to be more precise about the number of runs ( first paragraph of Section 4.3 ) : \u201c We run each experiment multiple times : 4 times for CIFAR-10 and 8 times for tabular datasets . We report the mean and standard deviation of results for each case. \u201c -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c For Table 2 , please provide accuracy without missing values as a baseline. \u201d As suggested , we included baseline accuracies for complete datasets ( 0 % missing rate ) as a footnote to Table 2 . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c Add MICE or some other \u201c standard \u201d imputation method as a baseline. \u201d To address your comment , we included MICE imputation results for all tabular datasets . See the revised Table 2 . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Suggestions for improving readability : * Comment : \u201c Many sentences start with \u201c in this \u201d ( e.g. \u201c in this case \u201d , \u201c in this setting \u201d , & c ) . Sometimes these sentences even co-occur within the same paragraph . Try to switch up the phrasing and move away from repetition. \u201d As suggested , we revised the paper to be more clear and reduced unnecessary repetitions . We modified about 10 instances of \u201c In this \u201d appearing at the beginning of a sentence throughout the paper . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c Not a complete sentence : `` For instance , jointly training multiple generator/discriminator networks , tuning objective functions with multiple hyper-parameters , etc . '' Thank you for pointing this out . In the revised version , we changed the sentence to be more explicit and fixed the grammar issues : \u201c For instance , Yoon et al . ( 2018 ) requires setting hyperparameters to adjust the influence of an MSE loss term as well as the rate of discriminator hint vectors . Also as another example , Li et al . ( 2019 ) uses three generators and three discriminators for the final imputer architecture . \u201d"}, "1": {"review_id": "B1em9h4KDS-1", "review_text": "The uncertainty of having a missing value is investigated on the prediction by not assigning a single imputed value but N different values generated via an imputer network (based on GAIN). Unlike old-fashion multiple imputation techniques, one predictor is trained on different samples and it induces the uncertainty. The experiments on number of datasets show the proposed predictor is capable of having a fairly better performance. Overall, this paper raises an interesting point about missing data imputation via generative models, and well-written; however, there are number of concerns: 1- The predictor is trained on different version of imputed samples (imputed via the generator); this equates to making noisy version of the real samples, where noise is applied to the missing variables. A side effect of this is generalization of the predictor; thus, have you been careful that the improved accuracy is not due to generalization? In other words, if we imposed the generalization via adding gaussian noise to the imputed samples by GAIN for example, would we get improved accuracy too? 2- Your method known as GI is a modified version of GAIN. You could also use MisGAN, and I am wondering if the results would have been different if generator in MisGAN was used in GI in Figure 2 (b), as MisGAN works better than GAIN. 3- I am also wondering how the GAIN imputation changes by removing the MSE term? GI discards the MSE term in GAIN, and it changes the distribution of the imputed variables by GAIN. Could you maybe fit a Normal distribution on a chosen imputed variable (N=128) and visualize how different it is from the distribution of imputed variables with GAIN with MSE term. 4- In justification for claim 1, it is said \u201cThis is equivalent to training models using noisy labels\u201d. This is not accurate: in noisy label prediction, we have one (noisy) y corresponding to each x, in your case there are multiple ys for one sample. 5- In the implementation details, I cannot fully wrap my head around the part \u201cz vector of size 1/8\u201d; how did you choose this 1/8? ", "rating": "6: Weak Accept", "reply_text": "* Comment : \u201c 3- I am also wondering how the GAIN imputation changes by removing the MSE term ? GI discards the MSE term in GAIN , and it changes the distribution of the imputed variables by GAIN . Could you maybe fit a Normal distribution on a chosen imputed variable ( N=128 ) and visualize how different it is from the distribution of imputed variables with GAIN with MSE term. \u201d As suggested , we conducted an experiment to visualize the impact of MSE term on the imputed distribution for a synthesized dataset . The results of this comparison are included in Appendix H of the paper . As can be seen from the comparison , the MSE term induces a bias toward the mean and if we increase the hyper-parameter controlling the MSE term the variance of the generated samples is decreased . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c 4- In justification for claim 1 , it is said \u201c This is equivalent to training models using noisy labels \u201d . This is not accurate : in noisy label prediction , we have one ( noisy ) y corresponding to each x , in your case there are multiple ys for one sample. \u201d We understand the reviewer \u2019 s concern ; the case of having noisy labels and having imputed samples with different real labels are not exactly equivalent in general . However , if we consider the equivalence in terms of gradient-based training ; it can be a valid statement to say that these two cases are very similar . Note that , during the optimization , we are doing an update based on gradients of loss terms that come from different versions of an imputed sample in which a few cases have a different assigned label as the correct underlying label . Intuitively , we are using misleading gradient terms as it happens in gradient-based learning using noisy labels . If we consider the average impact on gradients for batches of samples rather than the individual cases , the overall impact on the training would be very similar . In order to address the reviewer 's concern , we revised the paper to be more precise about this and omitted the term \u201c equivalent \u201d ( see the last paragraph of Claim 1 in Section 3.3 ) . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c 5- In the implementation details , I can not fully wrap my head around the part \u201c z vector of size 1/8 \u201d ; how did you choose this 1/8 ? \u201d We use the z vector as the source of randomness for the generator network . A z vector of very small size would not be effective enough and may result in deterministic imputations given a set of observed features . On the other hand , a very large z vector would unnecessarily increase the size of the input layer for the generator network causing training issues in practice . Regarding the specific choice of 1/8 , we conducted initial experiments using different sizes for the z vector . We found that the final performance is not too sensitive to the choice of z size ( as long as it is not relatively small or large ) . In this paper , we suggest the 1/8 ratio as it is a reasonable choice that does not add a huge overhead and at the same time is strong enough to induce randomness to the generator . For instance , the original implementation of GAN on MNIST ( Goodfellow et al.2014 ) uses a z of size 100 for inputs of size 784 , roughly the same ratio ."}, "2": {"review_id": "B1em9h4KDS-2", "review_text": "This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance. Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN's and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis. There are several parts that are confusing/missing in the paper: - In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this. - On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples? - Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch. - I think the statement in the first paragraph in Section 4.4 that \"MSE loss term would act as a denoising loss smoothing noisy missing pixels\" could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear. I think the paper would benefit if the authors could explain/show: - Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?", "rating": "6: Weak Accept", "reply_text": "* Comment : \u201c Can you explain the difference between the results in Figure-7 and Table-2 ? Results between the two mismatch. \u201d Thank you for pointing out the inconsistency . Table 2 provides final classification accuracies for Landsat . To generate results related Figure 7 , we used less number of training epochs and did not use the optimal hyper-parameter settings . To address this issue , we rerun the ablation experiments and updated Figure 7 in the revised version to use a similar setup as used for Table 2 . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c I think the statement in the first paragraph in Section 4.4 that `` MSE loss term would act as a denoising loss smoothing noisy missing pixels '' could be misleading . MSE is used with mask in GAIN , hence it only applies to the observed features during training . Its effect on smoothing noisy missing pixels is not clear. \u201d We understand the reviewer \u2019 s concern that the MSE loss is not exactly similar to what we have for denoising autoencoders . However , based on our experiments , its overall impact is very similar to autoencoders . Note that the generator network is partially receiving an autoencoder loss term which is well-known to cause over-smoothing . To address the comment and prevent misleading our readers , we revised the sentence to be more scientifically accurate ( first paragraph in Section 4.4 ) : \u201c One possible explanation for this behavior might be the fact that GAIN has an MSE loss term acting similar to an autoencoder loss smoothing noisy missing pixels. \u201d -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - * Comment : \u201c I think the paper would benefit if the authors could explain/show : - Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution . Especially in rectangular generation part where it can remove a complete object . Does stochastic averaging benefit more in this case ? \u201d As suggested by the reviewer , we conducted a new experiment to show the benefit of stochastic averaging over simple classification for rectangular missingness ranging from 20 % to 90 % ( see Table 6 in Appendix D ) . Last paragraph of Appendix D : \u201c From Table 6 , it can be inferred that as the rate of missingness increases , the benefits of the suggested predictor algorithm increase significantly . We hypothesize that at higher rates of missingness , the conditional distribution of missing features becomes multimodal . In such a scenario , the suggested method captures the uncertainties over the target distribution resulting in the predictor to make more reliable class assignments . \u201d"}}