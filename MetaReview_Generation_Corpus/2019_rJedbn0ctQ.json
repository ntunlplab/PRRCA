{"year": "2019", "forum": "rJedbn0ctQ", "title": "Zero-training Sentence Embedding via Orthogonal Basis", "decision": "Reject", "meta_review": "The paper proposes a simple approach for computing a sentence embedding as a weighted combination of pre-trained word embeddings, which obtains nice results on a number of tasks.  The approach is described as training-free but does require computing principal components of word embedding subspaces on the test set (similarly to some earlier work).  The reviewers are generally in agreement that the approach is interesting, and the results are encouraging.  However, there is some concern about the clarity of the paper and in particular the placement of the work in relation to other methods.  There is also a bit of concern about whether there is sufficient novelty compared to Arora et al. 2017, which also compose sentence embeddings as weighted combinations of word embeddings, and also use a principal subspace of embeddings in the test set.  This AC feels that the method here is sufficiently different from Arora et al., but agrees with the reviewers that the paper clarity needs to be improved, so that the community can appreciate what is gained from the new aspects of the approach and what conclusions should be drawn from each experimental comparison.", "reviews": [{"review_id": "rJedbn0ctQ-0", "review_text": "The paper presented a new training-free way of generating sentence embedding. The proposed work is along the same motivation from Arora et al., 2017. A systematic analysis has been done on a number of tasks to show the strong performance (close or higher than the specifically \"supervised\" strategies). - I suggest the author to re-ward the category terms of the existing methods. Un-supervised and training-free are confusing. Unsupervised and supervised should be all in a group of training-required methods. unsupervised in this paper is more task-agnostic but domain specific and supervised is to extract sentence emb that is prediction task specific. - The evaluation tasks are rich but not clearly stated. For instance, the supervised taske are only discussed at high-level. Not clear what each task is and how one should interpret the results from each experiments. The way author presented it suggests the detail here were not important. It is also good to include discussion on how the baseline algorithms are tuned and/or trained on these tasks. Readers cannot reproduce the same results based on the current paper. - Notation and Math: --r-1 in (4) is not clear as \\mathbf{r} is not defined properly --based on sec 2.2., it is easy to motivate the novelty score from subspace projection rather than QR/GS; -- a_n and a_s are both functions of r_{-1} which is the perp. energy of the words w.r.t. its contexts. Is there a fundamental difference? -- Figure 1 is a little bit confusing. Not clear what is word and what is a sentence/corpus. -- in Eq(8), better not to use r as it confuses with the GS coeffs. -- 2.4.1 is a bit confusing, sentence embeddings c_1, \\ldot, c_N are introduced, but so far no sentence embedding has been formally introduced. Is this initialized from some heuristic? It is confusing in the sense that eq (9) c_s are defined by a_u, but a_u defined in eq (8) depends on sigma_d that relies on X^{c}, which is a funcion of all c_s's. -- there are several parameters for GEM, please add some discussion on how these are selected in each of the evaluated tasks. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Hello AnonReviewer3 , We appreciate your comprehensive review and questions . Please find our response below . ( 1 ) About re-word the categories . Thanks for your suggestion . In the revised version submitted , we categorize sentence embeddings methods into two types , one is non-parameterized methods , including GEM and SIF , that don \u2019 t depend on parameters or need training . The other type is parameterized methods , such as InferSent and QuickThoughts , that need supervised/unsupervised training to update the parameters . ( 2 ) About supervised tasks . We are sorry for the confusion . in section 3.3 , we add a description of supervised tasks ( first paragraph ) and an analysis of results ( the end of second paragraph ) . ( 3 ) On \u201c how the baseline algorithms are tuned and/or trained on these tasks \u201d . On the supervised tasks , the performance of baseline model \u201c GloVe BOW \u201d is extracted from ref [ 1 ] . On STSB dataset , results of baseline model \u201c word2vec skipgram \u201d and \u201c Glove \u201d are extracted from the official website of STSB dataset ( http : //ixa2.si.ehu.es/stswiki/index.php/STSbenchmark ) . \u201c LexVec \u201d , \u201c L.F.P \u201d and \u201c ELMo \u201d are from experiments run by us . As noted in the \u201c experimental settings \u201d section in the appendix , sentences are tokenized using the NLTK wordpunct tokenizer , and then all punctuation is then skipped . Sentence vectors are just mean of word representations , and the similarity score is the cosine similarity of two vectors . ( 4 ) About \\mathbf { r } . In the line under Eq ( 4 ) , we mention that \\mathbf { r } is the last column of R^i . And R^i is defined in Eq ( 3 ) . ( 5 ) About GS and subspace projections . We agree that subspaces projection is more mathematically concise compared with GS . The reason why we still use GS to introduce novelty score is that GEM is motivated by the fact when a sentence is formed , different words bring in different meaning to this sentence one by one , and GS is appropriate to describe this process by yielding the orthogonal basis vectors one by one . ( 6 ) Although a_n and a_s are both functions of r_ { -1 } , they describe different quantities . Note that a_s is initially computed as q_i \u2019 s alignment with the meanings in its context . And Eq ( 6 ) shows that a_s is r_ { -1 } , i.e.the l_2 norm of q_i , divided by a constant . a_s is trying to quantify the absolute significance/magnitude of the new semantic meaning q_i . In contrast , a_n is a function ( exponential ) of r_ { -1 } divided by l_2 norm of r , i.e.a function of the \u201c proportion \u201d of q_i in word w_i . Note that ||r||_2 = ||v_ { w_i } ||_2 , and r_ { -1 } = ||q_i||_2 . Therefore , a_n is quantifying that among all the information that w_i is trying to ship , what \u2019 s proportion of the new meaning q_i ? ( 7 ) On fig 1 . We apologize for the possible ambiguity . The sentence is represented by a sequence of blue block in the top middle , marked as w_1 \u2026 w_ { i-m } \u2026 w_i \u2026 w_ { i+m } \u2026 w_n . And we didn \u2019 t show the corpus in fig 1 , and instead we show the top K principal vectors of X^c as those orange/yellow blocks on the right . And more descriptions are added to the caption of fig 1 . ( 8 ) In eq ( 8 ) , we change the notation \u201c r \u201d to \u201c h \u201d . Thanks for your suggestion . ( 9 ) On \u201c 2.4.1 is a bit confusing \u201d . We think you referred to the matrix in the first paragraph in 2.4.1 . The first paragraph is a revisit of the method in SIF . The formal desription of GEM starts from the second paragraph . We form a matrix X^c and its ith column is given by eq ( 7 ) . Eq ( 7 ) is independent of a_u , a_n and a_s , and it \u2019 s computed using the singular values and singular vectors of the sentence matrix $ \\mS $ . And then we use X^c and q_i to compute a_u . ( 10 ) In the STS benchmark dataset , our hyper-parameters are chosen by conducting parameters search on STSB dev set at m = 7 , h = 17 , K = 45 , and t = 3 . And we use the same values for all supervised tasks . The integer interval of parameters search are m \u2208 [ 5 , 9 ] , h \u2208 [ 8 , 20 ] , L \u2208 [ 35 , 75 ] ( at stride of 5 ) , and t \u2208 [ 1 , 5 ] . And we use the same values for all supervised tasks . We add the discussion to the \u201c experimental settings \u201d section in the appendix . Thanks for your time and we hope that our response has addressed your questions . Look forward to your suggestion and evaluation . Reference : [ 1 ] Conneau , Alexis , et al . `` Supervised learning of universal sentence representations from natural language inference data . '' arXiv preprint arXiv:1705.02364 ( 2017 ) ."}, {"review_id": "rJedbn0ctQ-1", "review_text": "Paper overview: This paper proposes a new geometry-based method for sentence embedding from word embedding vectors, inspired by Arora et al (2017). The idea is to quantify the novelty,significance and corpus-wise uniqueness of each word. In order to do so, they analyze geometrically how the word vector of the target word relates to 1) the subspace created by the word-vectors in its context 2) its alignment with the meanings in its context (using SVD) 3) its presence in the all the corpus. For each of these aspects, they output a score or weight. The final sentence representation is a weighted average, using these scores, of the word vectors of the sentence. Remarks and questions: 1) In table 1, Glove and word2vec are word representations, how is the sentence representation computed here? 2) The authors are not comparing to what is now considered the state of the art methods, such as Quick thoughts vectors (ICLR 2018, 'an efficient framework for learning sentence representations' by Logeswaran et al.), Transformer (Attention is all you need by Vaswani et al.) and ELMo (Deep contextualized word representations, by Peters et al.). Points in favor: 1) Results: The method gives the best performance for non-training methods with an +2 point improvement on average, although it cannot beat training methods (see Table 3, for instance). 2) On the result tables, it should be reported also the std, not just the average, so the reader can evaluate if the difference between the methods is statistically significant. 3) Inference speed: the method is fast (see table 5) 4) stability of the results: The method is robust to slight changes in the hyperparameters such as the size of the window, number of principal components used, etc (see Fig 2) Points against: The methods presented in the paper are not novel. The main novelties are the geometrical analysis on the contribution of each word of the sentence to the sentence overall semantic meaning, and the definition of the scores (eqs 4,6,8) that allow to improve the weighted average sentence representation (eq 9), an idea already present in Arora et al.'s paper. Conclusion: Although the geometric analysis of the paper is interesting, I dont think it is sufficient to justify a paper at ICLR, unless, after comparison with the other methods proposed previously, the proposed model is still competitive and the difference is statistically significant. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Hello AnonReviewer2 , Thank you for the detailed and careful review . We appreciate your points in favor and against . About Remarks and questions : ( 1 ) For rows \u201c Glove \u201d and \u201c word2vec \u201d in table 1 , the sentence embeddings are computed as the simple average of all word embeddings of words in the sentence . ( 2 ) Sorry if we didn \u2019 t make this clearer in the paper , but we \u2019 ve included results from Quick Thoughts and a very recent model using transformer in the first version of our paper . Quick Thoughts is denoted as \u201c QT \u201d , and their results are shown on table 3 . \u201c Reddit + SNLI \u201d in table 1 and table 2 is a very recent and competitive transformer model , introduced in [ 1 ] and [ 2 ] . The model uses the transformer from \u201c attention is all you need \u201d as the encoder . And in the revised version , we include their results on supervised tasks in table 3 , denoted as USE . We also include ELMo \u2019 s performance on STS benchmark in table 1 . The sentence embeddings are computed as the mean of ELMo vector of each word . Besides , we did comparison with other very recent and even more competitive models published around mid 2018 , for example \u201c a lar carte \u201d and STN in table 3 . Comparison with these models mentioned above : On STSB dataset , GEM ( 77.5/82.1 ) clearly outperforms mean of ELMo ( 55.87/64.58 ) , and is very close to the transformer model on test set ( actually better than it on dev set ) . On supervised tasks , GEM \u2019 s performance is definitely better than some parameterized methods ( like SkipThought , Sent2Vec and FastSent ) . And it \u2019 s still very competitive compared with parameterized SOTA models , for example , GEM is better than transformer model USE on SUBJ , MPQA , better than a lar carte on MPQA , TREC . About novelty : ( 1 ) We acknowledge that SIF is the first published work on using weighted sum of word vectors for sentence representation . And representing sentence as a composition ( average , non-linear , p-mean etc . ) of word vectors has been an active research topic before and after SIF ( e.g.ref [ 3 ] [ 4 ] [ 5 ] ) . And we believe there are still much to explore on this direction . ( 2 ) On GEM \u2019 s novelty . Although our model utilizes the idea weighted sum of word vectors , GEM is significantly different from SIF , including following aspects : - To our knowledge , we are the first to adopt well-established numerical linear algebra to quantify the sentence semantic meaning and the importance of words . And this simple method proves to be competitive . - The weights in SIF depend on statistic of vocabularies on very large corpus ( wikipedia ) . In contrast , the weights in GEM are directly computed from the sentence \u201c on the scene \u201d . Given a sentence and its context , GEM is ready to go , independent of prior statistical knowledge of words . - In GEM , the components in weights are all computed from numerical linear algebra . And SIF directly have a hyper-parameter term in the weights , i.e.the smooth term . - As suggested by experiments in table 1 and 3 , GEM outperforms SIF by significant margin . Thanks for your time again . Hope that our response addresses your concern . We kindly ask for your further evaluation and opinions . Reference : [ 1 ] Cer , Daniel , et al . `` Universal sentence encoder . '' arXiv preprint arXiv:1803.11175 ( 2018 ) . [ 2 ] Yang , Yinfei , et al . `` Learning Semantic Textual Similarity from Conversations . '' arXiv preprint arXiv:1804.07754 ( 2018 ) . [ 3 ] Wieting , John , et al . `` Towards universal paraphrastic sentence embeddings . '' arXiv preprint arXiv:1511.08198 ( 2015 ) . [ 4 ] Wieting , John , and Kevin Gimpel . `` Revisiting recurrent networks for paraphrastic sentence embeddings . '' arXiv preprint arXiv:1705.00364 ( 2017 ) . [ 5 ] R\u00fcckl\u00e9 , Andreas , et al . `` Concatenated $ p $ -mean Word Embeddings as Universal Cross-Lingual Sentence Representations . '' arXiv preprint arXiv:1803.01400 ( 2018 ) ."}, {"review_id": "rJedbn0ctQ-2", "review_text": "This is a paper about sentence embedding based on orthogonal decomposition of the spanned space by word embeddings. Via Gram-Schmidt process, the sequence of words in a sentence is regarded as a sequence of incoming vectors to be orthogonalized. Each word is then assigned 3 scores: novelty score, significance score, and uniqueness score. Eventually, the sentence embedding is achieved as weighted average of word embeddings based on those scores. The authors conduct extensive experiments to demonstrate the performance of the proposed embedding. I think the idea of the paper is novel and inspiring. But there are several issues and possible areas to improve: 1. What if the length of the sentence is larger than the dimension of the word embedding? Some of the 3 scores will not be well-defined. 2. Gram-Schmidt process is sensitive to the order of the incoming vectors. A well-defined sentence embedding algorithm should not. I suggest the authors to evaluate whether this is an issue. For example, if by simply removing a non-important stop word at the begging of the sentence and then the sentence embedding changes drastically, then it indicates that the embedding is problematic. 3. I\u2019m confused by the classification between training-free sentence embedding and unsupervised sentence embedding? Don\u2019t both of them require training word2vec-type embedding? 4. The definition of the three scores seems reasonable, but requires further evidence to justify. For example, by the definition of the scores, do we have any proof that the value of \\alpha indeed demonstrated the related importance level?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Hi AnonReviewer1 , Thanks for reviewing the paper and recognizing the novelty in our idea ! Please find our response to the four points as follows . ( 1 ) In the case that the length of the sentence is larger than the dimension of the word embeddings , our algorithm still works fun . Sorry for the possible confusion and here are some clarifications : First , the novelty score and significance score are independent of the length of the sentence , so they are good . For the uniqueness score , the part that depends on the length of the sentence is the coarse embedding in eq ( 7 ) . For the coarse embedding , now we have a sentence matrix S of size d * n , where d is word vector embedding size , n is the length of sentence , and n > d. The thin SVD of S is S = U * Sigma * V , where U is of size d * d , Sigma is of size d * n , and V is of size n * n. And the ( d+1 ) th column to the nth column in Sigma is zeros , this is because S only has number of d singular values . In this case , the upper limit in eq ( 7 ) is n instead of d , and we have the coarse embedding from sentence matrix S. Therefore , in this case our model is fun . We \u2019 ll add explanation on this corner case in the appendix in the revised version ( will submit very soon ) . ( 2 ) First , although we use Gram-Schmidt process ( GS ) , GEM is not that sensitive to the order of words , explained as follows . For GS on n incoming vectors , if the last vector is fixed , the last orthogonal base vector computed is independent of the order of first n vectors . In our case , the word w_i is always shifted to the last column in the context window , and we only utilize the last orthogonal base vector , q_i , generated by GS . Therefore , no matter how the first ( n-1 ) words in the context window are shifted , q_i is always the same . And those three scores stay the same for w_i . Second , as suggested in the review , we do some experiment of removing non-important stop words . S1 : `` The student is reading a physics book '' S2 : `` student is reading a physics textbook '' The cosine similarity between sentence vector of s1 and s2 given by GEM is 0.998 sent1= `` A man walks along walkway to the store '' sent2= `` man walks along walkway to the store '' cosine similarity = 0.984 sent1= `` Someone is sitting on the blanket '' sent2= `` Someone is sitting on blanket '' cosine similarity = 0.981 The similarity scores are all very closed to 1 , suggesting that sentence embeddings barely change . ( 3 ) We are sorry about the confusion . For \u201c training-free \u201d , we are trying to say that the sentence embedding model built upon word2vec-type embedding doesn \u2019 t require training and free of trained parameters , for example , SIF and GEM belongs to this training-free type . And \u201c training-required \u201d means the embeddings model needs training to update its parameters , for example skip-thoughs and InferSent . We plan to rename the two types as parameter-free and parameters-required in the revised version . ( 4 ) We do some experiments to show that the value of \\alpha in GEM shows the relative importance level . First , assume that originally the GEM assigns a weight alpha_i for the word w_i , . On STS benchmark test set , GEM achieves 77.5 ( Pearson \u2019 s r * 100 ) . If now the weight is changed to 1/ ( alpha_i ) , the performance drops to 69.59 . And the performance falls to 32.83 , if the weight is exp ( -alpha_i ) . These results show that if we are trying to assign small weight to words that GEM assign high alpha value , and then sentence embeddings performs very badly . This phenomenon indicates that the alpha value given by GEM reflects the related importance level . And we list some concrete examples of alpha value below : The sentence to encode : \u201c there are two ducks swimming in the river \u201d Alpha values ( sorted ) by GEM are : [ ducks : 4.93198917 , river:4.7221562 , swimming : 4.70170178 , two : 3.87061874 , are : 3.54588148 , there : 3.23038324 , in:3.045169 , the:2.93566744 ] GEM assigns higher weight to informative words like \u201c ducks \u201d and \u201c river \u201d , and downplay stop words like \u201c the \u201d and \u201c there \u201d . The sentence to encode : \u201c The stock market opens low on Friday \u201d Alpha values ( sorted ) by GEM are : [ lower : 4.94505258 , stock : 4.93871886 , closes : 4.78424269 , market : 4.62267853 , Friday : 4.51399687 , the : 3.75456615 , on : 3.70935467 ] GEM emphasizes informative words like \u201c lower \u201d and \u201c closes \u201d , and diminishes stop words like \u201c the \u201d and \u201c there \u201d . We sincerely look forward to your further feedbacks and evaluation ."}], "0": {"review_id": "rJedbn0ctQ-0", "review_text": "The paper presented a new training-free way of generating sentence embedding. The proposed work is along the same motivation from Arora et al., 2017. A systematic analysis has been done on a number of tasks to show the strong performance (close or higher than the specifically \"supervised\" strategies). - I suggest the author to re-ward the category terms of the existing methods. Un-supervised and training-free are confusing. Unsupervised and supervised should be all in a group of training-required methods. unsupervised in this paper is more task-agnostic but domain specific and supervised is to extract sentence emb that is prediction task specific. - The evaluation tasks are rich but not clearly stated. For instance, the supervised taske are only discussed at high-level. Not clear what each task is and how one should interpret the results from each experiments. The way author presented it suggests the detail here were not important. It is also good to include discussion on how the baseline algorithms are tuned and/or trained on these tasks. Readers cannot reproduce the same results based on the current paper. - Notation and Math: --r-1 in (4) is not clear as \\mathbf{r} is not defined properly --based on sec 2.2., it is easy to motivate the novelty score from subspace projection rather than QR/GS; -- a_n and a_s are both functions of r_{-1} which is the perp. energy of the words w.r.t. its contexts. Is there a fundamental difference? -- Figure 1 is a little bit confusing. Not clear what is word and what is a sentence/corpus. -- in Eq(8), better not to use r as it confuses with the GS coeffs. -- 2.4.1 is a bit confusing, sentence embeddings c_1, \\ldot, c_N are introduced, but so far no sentence embedding has been formally introduced. Is this initialized from some heuristic? It is confusing in the sense that eq (9) c_s are defined by a_u, but a_u defined in eq (8) depends on sigma_d that relies on X^{c}, which is a funcion of all c_s's. -- there are several parameters for GEM, please add some discussion on how these are selected in each of the evaluated tasks. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Hello AnonReviewer3 , We appreciate your comprehensive review and questions . Please find our response below . ( 1 ) About re-word the categories . Thanks for your suggestion . In the revised version submitted , we categorize sentence embeddings methods into two types , one is non-parameterized methods , including GEM and SIF , that don \u2019 t depend on parameters or need training . The other type is parameterized methods , such as InferSent and QuickThoughts , that need supervised/unsupervised training to update the parameters . ( 2 ) About supervised tasks . We are sorry for the confusion . in section 3.3 , we add a description of supervised tasks ( first paragraph ) and an analysis of results ( the end of second paragraph ) . ( 3 ) On \u201c how the baseline algorithms are tuned and/or trained on these tasks \u201d . On the supervised tasks , the performance of baseline model \u201c GloVe BOW \u201d is extracted from ref [ 1 ] . On STSB dataset , results of baseline model \u201c word2vec skipgram \u201d and \u201c Glove \u201d are extracted from the official website of STSB dataset ( http : //ixa2.si.ehu.es/stswiki/index.php/STSbenchmark ) . \u201c LexVec \u201d , \u201c L.F.P \u201d and \u201c ELMo \u201d are from experiments run by us . As noted in the \u201c experimental settings \u201d section in the appendix , sentences are tokenized using the NLTK wordpunct tokenizer , and then all punctuation is then skipped . Sentence vectors are just mean of word representations , and the similarity score is the cosine similarity of two vectors . ( 4 ) About \\mathbf { r } . In the line under Eq ( 4 ) , we mention that \\mathbf { r } is the last column of R^i . And R^i is defined in Eq ( 3 ) . ( 5 ) About GS and subspace projections . We agree that subspaces projection is more mathematically concise compared with GS . The reason why we still use GS to introduce novelty score is that GEM is motivated by the fact when a sentence is formed , different words bring in different meaning to this sentence one by one , and GS is appropriate to describe this process by yielding the orthogonal basis vectors one by one . ( 6 ) Although a_n and a_s are both functions of r_ { -1 } , they describe different quantities . Note that a_s is initially computed as q_i \u2019 s alignment with the meanings in its context . And Eq ( 6 ) shows that a_s is r_ { -1 } , i.e.the l_2 norm of q_i , divided by a constant . a_s is trying to quantify the absolute significance/magnitude of the new semantic meaning q_i . In contrast , a_n is a function ( exponential ) of r_ { -1 } divided by l_2 norm of r , i.e.a function of the \u201c proportion \u201d of q_i in word w_i . Note that ||r||_2 = ||v_ { w_i } ||_2 , and r_ { -1 } = ||q_i||_2 . Therefore , a_n is quantifying that among all the information that w_i is trying to ship , what \u2019 s proportion of the new meaning q_i ? ( 7 ) On fig 1 . We apologize for the possible ambiguity . The sentence is represented by a sequence of blue block in the top middle , marked as w_1 \u2026 w_ { i-m } \u2026 w_i \u2026 w_ { i+m } \u2026 w_n . And we didn \u2019 t show the corpus in fig 1 , and instead we show the top K principal vectors of X^c as those orange/yellow blocks on the right . And more descriptions are added to the caption of fig 1 . ( 8 ) In eq ( 8 ) , we change the notation \u201c r \u201d to \u201c h \u201d . Thanks for your suggestion . ( 9 ) On \u201c 2.4.1 is a bit confusing \u201d . We think you referred to the matrix in the first paragraph in 2.4.1 . The first paragraph is a revisit of the method in SIF . The formal desription of GEM starts from the second paragraph . We form a matrix X^c and its ith column is given by eq ( 7 ) . Eq ( 7 ) is independent of a_u , a_n and a_s , and it \u2019 s computed using the singular values and singular vectors of the sentence matrix $ \\mS $ . And then we use X^c and q_i to compute a_u . ( 10 ) In the STS benchmark dataset , our hyper-parameters are chosen by conducting parameters search on STSB dev set at m = 7 , h = 17 , K = 45 , and t = 3 . And we use the same values for all supervised tasks . The integer interval of parameters search are m \u2208 [ 5 , 9 ] , h \u2208 [ 8 , 20 ] , L \u2208 [ 35 , 75 ] ( at stride of 5 ) , and t \u2208 [ 1 , 5 ] . And we use the same values for all supervised tasks . We add the discussion to the \u201c experimental settings \u201d section in the appendix . Thanks for your time and we hope that our response has addressed your questions . Look forward to your suggestion and evaluation . Reference : [ 1 ] Conneau , Alexis , et al . `` Supervised learning of universal sentence representations from natural language inference data . '' arXiv preprint arXiv:1705.02364 ( 2017 ) ."}, "1": {"review_id": "rJedbn0ctQ-1", "review_text": "Paper overview: This paper proposes a new geometry-based method for sentence embedding from word embedding vectors, inspired by Arora et al (2017). The idea is to quantify the novelty,significance and corpus-wise uniqueness of each word. In order to do so, they analyze geometrically how the word vector of the target word relates to 1) the subspace created by the word-vectors in its context 2) its alignment with the meanings in its context (using SVD) 3) its presence in the all the corpus. For each of these aspects, they output a score or weight. The final sentence representation is a weighted average, using these scores, of the word vectors of the sentence. Remarks and questions: 1) In table 1, Glove and word2vec are word representations, how is the sentence representation computed here? 2) The authors are not comparing to what is now considered the state of the art methods, such as Quick thoughts vectors (ICLR 2018, 'an efficient framework for learning sentence representations' by Logeswaran et al.), Transformer (Attention is all you need by Vaswani et al.) and ELMo (Deep contextualized word representations, by Peters et al.). Points in favor: 1) Results: The method gives the best performance for non-training methods with an +2 point improvement on average, although it cannot beat training methods (see Table 3, for instance). 2) On the result tables, it should be reported also the std, not just the average, so the reader can evaluate if the difference between the methods is statistically significant. 3) Inference speed: the method is fast (see table 5) 4) stability of the results: The method is robust to slight changes in the hyperparameters such as the size of the window, number of principal components used, etc (see Fig 2) Points against: The methods presented in the paper are not novel. The main novelties are the geometrical analysis on the contribution of each word of the sentence to the sentence overall semantic meaning, and the definition of the scores (eqs 4,6,8) that allow to improve the weighted average sentence representation (eq 9), an idea already present in Arora et al.'s paper. Conclusion: Although the geometric analysis of the paper is interesting, I dont think it is sufficient to justify a paper at ICLR, unless, after comparison with the other methods proposed previously, the proposed model is still competitive and the difference is statistically significant. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Hello AnonReviewer2 , Thank you for the detailed and careful review . We appreciate your points in favor and against . About Remarks and questions : ( 1 ) For rows \u201c Glove \u201d and \u201c word2vec \u201d in table 1 , the sentence embeddings are computed as the simple average of all word embeddings of words in the sentence . ( 2 ) Sorry if we didn \u2019 t make this clearer in the paper , but we \u2019 ve included results from Quick Thoughts and a very recent model using transformer in the first version of our paper . Quick Thoughts is denoted as \u201c QT \u201d , and their results are shown on table 3 . \u201c Reddit + SNLI \u201d in table 1 and table 2 is a very recent and competitive transformer model , introduced in [ 1 ] and [ 2 ] . The model uses the transformer from \u201c attention is all you need \u201d as the encoder . And in the revised version , we include their results on supervised tasks in table 3 , denoted as USE . We also include ELMo \u2019 s performance on STS benchmark in table 1 . The sentence embeddings are computed as the mean of ELMo vector of each word . Besides , we did comparison with other very recent and even more competitive models published around mid 2018 , for example \u201c a lar carte \u201d and STN in table 3 . Comparison with these models mentioned above : On STSB dataset , GEM ( 77.5/82.1 ) clearly outperforms mean of ELMo ( 55.87/64.58 ) , and is very close to the transformer model on test set ( actually better than it on dev set ) . On supervised tasks , GEM \u2019 s performance is definitely better than some parameterized methods ( like SkipThought , Sent2Vec and FastSent ) . And it \u2019 s still very competitive compared with parameterized SOTA models , for example , GEM is better than transformer model USE on SUBJ , MPQA , better than a lar carte on MPQA , TREC . About novelty : ( 1 ) We acknowledge that SIF is the first published work on using weighted sum of word vectors for sentence representation . And representing sentence as a composition ( average , non-linear , p-mean etc . ) of word vectors has been an active research topic before and after SIF ( e.g.ref [ 3 ] [ 4 ] [ 5 ] ) . And we believe there are still much to explore on this direction . ( 2 ) On GEM \u2019 s novelty . Although our model utilizes the idea weighted sum of word vectors , GEM is significantly different from SIF , including following aspects : - To our knowledge , we are the first to adopt well-established numerical linear algebra to quantify the sentence semantic meaning and the importance of words . And this simple method proves to be competitive . - The weights in SIF depend on statistic of vocabularies on very large corpus ( wikipedia ) . In contrast , the weights in GEM are directly computed from the sentence \u201c on the scene \u201d . Given a sentence and its context , GEM is ready to go , independent of prior statistical knowledge of words . - In GEM , the components in weights are all computed from numerical linear algebra . And SIF directly have a hyper-parameter term in the weights , i.e.the smooth term . - As suggested by experiments in table 1 and 3 , GEM outperforms SIF by significant margin . Thanks for your time again . Hope that our response addresses your concern . We kindly ask for your further evaluation and opinions . Reference : [ 1 ] Cer , Daniel , et al . `` Universal sentence encoder . '' arXiv preprint arXiv:1803.11175 ( 2018 ) . [ 2 ] Yang , Yinfei , et al . `` Learning Semantic Textual Similarity from Conversations . '' arXiv preprint arXiv:1804.07754 ( 2018 ) . [ 3 ] Wieting , John , et al . `` Towards universal paraphrastic sentence embeddings . '' arXiv preprint arXiv:1511.08198 ( 2015 ) . [ 4 ] Wieting , John , and Kevin Gimpel . `` Revisiting recurrent networks for paraphrastic sentence embeddings . '' arXiv preprint arXiv:1705.00364 ( 2017 ) . [ 5 ] R\u00fcckl\u00e9 , Andreas , et al . `` Concatenated $ p $ -mean Word Embeddings as Universal Cross-Lingual Sentence Representations . '' arXiv preprint arXiv:1803.01400 ( 2018 ) ."}, "2": {"review_id": "rJedbn0ctQ-2", "review_text": "This is a paper about sentence embedding based on orthogonal decomposition of the spanned space by word embeddings. Via Gram-Schmidt process, the sequence of words in a sentence is regarded as a sequence of incoming vectors to be orthogonalized. Each word is then assigned 3 scores: novelty score, significance score, and uniqueness score. Eventually, the sentence embedding is achieved as weighted average of word embeddings based on those scores. The authors conduct extensive experiments to demonstrate the performance of the proposed embedding. I think the idea of the paper is novel and inspiring. But there are several issues and possible areas to improve: 1. What if the length of the sentence is larger than the dimension of the word embedding? Some of the 3 scores will not be well-defined. 2. Gram-Schmidt process is sensitive to the order of the incoming vectors. A well-defined sentence embedding algorithm should not. I suggest the authors to evaluate whether this is an issue. For example, if by simply removing a non-important stop word at the begging of the sentence and then the sentence embedding changes drastically, then it indicates that the embedding is problematic. 3. I\u2019m confused by the classification between training-free sentence embedding and unsupervised sentence embedding? Don\u2019t both of them require training word2vec-type embedding? 4. The definition of the three scores seems reasonable, but requires further evidence to justify. For example, by the definition of the scores, do we have any proof that the value of \\alpha indeed demonstrated the related importance level?", "rating": "5: Marginally below acceptance threshold", "reply_text": "Hi AnonReviewer1 , Thanks for reviewing the paper and recognizing the novelty in our idea ! Please find our response to the four points as follows . ( 1 ) In the case that the length of the sentence is larger than the dimension of the word embeddings , our algorithm still works fun . Sorry for the possible confusion and here are some clarifications : First , the novelty score and significance score are independent of the length of the sentence , so they are good . For the uniqueness score , the part that depends on the length of the sentence is the coarse embedding in eq ( 7 ) . For the coarse embedding , now we have a sentence matrix S of size d * n , where d is word vector embedding size , n is the length of sentence , and n > d. The thin SVD of S is S = U * Sigma * V , where U is of size d * d , Sigma is of size d * n , and V is of size n * n. And the ( d+1 ) th column to the nth column in Sigma is zeros , this is because S only has number of d singular values . In this case , the upper limit in eq ( 7 ) is n instead of d , and we have the coarse embedding from sentence matrix S. Therefore , in this case our model is fun . We \u2019 ll add explanation on this corner case in the appendix in the revised version ( will submit very soon ) . ( 2 ) First , although we use Gram-Schmidt process ( GS ) , GEM is not that sensitive to the order of words , explained as follows . For GS on n incoming vectors , if the last vector is fixed , the last orthogonal base vector computed is independent of the order of first n vectors . In our case , the word w_i is always shifted to the last column in the context window , and we only utilize the last orthogonal base vector , q_i , generated by GS . Therefore , no matter how the first ( n-1 ) words in the context window are shifted , q_i is always the same . And those three scores stay the same for w_i . Second , as suggested in the review , we do some experiment of removing non-important stop words . S1 : `` The student is reading a physics book '' S2 : `` student is reading a physics textbook '' The cosine similarity between sentence vector of s1 and s2 given by GEM is 0.998 sent1= `` A man walks along walkway to the store '' sent2= `` man walks along walkway to the store '' cosine similarity = 0.984 sent1= `` Someone is sitting on the blanket '' sent2= `` Someone is sitting on blanket '' cosine similarity = 0.981 The similarity scores are all very closed to 1 , suggesting that sentence embeddings barely change . ( 3 ) We are sorry about the confusion . For \u201c training-free \u201d , we are trying to say that the sentence embedding model built upon word2vec-type embedding doesn \u2019 t require training and free of trained parameters , for example , SIF and GEM belongs to this training-free type . And \u201c training-required \u201d means the embeddings model needs training to update its parameters , for example skip-thoughs and InferSent . We plan to rename the two types as parameter-free and parameters-required in the revised version . ( 4 ) We do some experiments to show that the value of \\alpha in GEM shows the relative importance level . First , assume that originally the GEM assigns a weight alpha_i for the word w_i , . On STS benchmark test set , GEM achieves 77.5 ( Pearson \u2019 s r * 100 ) . If now the weight is changed to 1/ ( alpha_i ) , the performance drops to 69.59 . And the performance falls to 32.83 , if the weight is exp ( -alpha_i ) . These results show that if we are trying to assign small weight to words that GEM assign high alpha value , and then sentence embeddings performs very badly . This phenomenon indicates that the alpha value given by GEM reflects the related importance level . And we list some concrete examples of alpha value below : The sentence to encode : \u201c there are two ducks swimming in the river \u201d Alpha values ( sorted ) by GEM are : [ ducks : 4.93198917 , river:4.7221562 , swimming : 4.70170178 , two : 3.87061874 , are : 3.54588148 , there : 3.23038324 , in:3.045169 , the:2.93566744 ] GEM assigns higher weight to informative words like \u201c ducks \u201d and \u201c river \u201d , and downplay stop words like \u201c the \u201d and \u201c there \u201d . The sentence to encode : \u201c The stock market opens low on Friday \u201d Alpha values ( sorted ) by GEM are : [ lower : 4.94505258 , stock : 4.93871886 , closes : 4.78424269 , market : 4.62267853 , Friday : 4.51399687 , the : 3.75456615 , on : 3.70935467 ] GEM emphasizes informative words like \u201c lower \u201d and \u201c closes \u201d , and diminishes stop words like \u201c the \u201d and \u201c there \u201d . We sincerely look forward to your further feedbacks and evaluation ."}}