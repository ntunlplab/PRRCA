{"year": "2020", "forum": "BklXkCNYDB", "title": "Fast Training of Sparse Graph Neural Networks on Dense Hardware", "decision": "Reject", "meta_review": "While there was some interest in the ideas presented, this paper was on the borderline, and was ultimately not able to be accepted for publication at ICLR.\n\nReviewers raised concerns as to the novelty, generality, and practicality of the approach, which could have been better demonstrated via experiments.", "reviews": [{"review_id": "BklXkCNYDB-0", "review_text": "The authors propose a method to speed-up the time to validation accuracy for a particular class of graph neural networks: Gated graph sequence neural networks (GGSNNs). The paper is interesting in that it describes several operations and engineering considerations to speed up the processing of a GGSNN on TPUs. It is essentially a collection of engineering steps that improve the time to validation accuracy. While I'm not an expert in (G)NN acceleration on TPUs, I have experience with GNNs and approaches to accelerate CNNs in GPUs. My assessment is that the scope of this work is far too narrow. It is specific to GGSNNs which is a small family of GNNs not widely used. It is also specific to TPUs and lacks evaluations of the proposed approach on other type of hardware. It is for these reasons that I think the paper is not appropriate for ICLR. The scope has to be broadened both in terms of the NN models and the hardware types. ", "rating": "3: Weak Reject", "reply_text": "Thank you for taking the time to review our work . We would like to respond to both raised concerns : the method being ( 1 ) specific to a model family that is \u201c not widely used \u201d , and ( 2 ) \u201c specific to TPUs \u201d . ( 1 ) We would like to point out that the claim in the review that our work is specific to \u201c Gated graph sequence neural networks ( GGSNNs ) \u201d is imprecise . The confusion may have arisen because the cited paper [ Li et al. , 2016 ] introduces both Gated Graph Neural Networks ( GGNNs ) as well as GGSNNs , but as explained and described at the beginning of Section 2 , we use GGNNs to demonstrate our method . We are not sure how to best define \u201c widely used \u201d , but GGNNs have been successfully used not only for program understanding understanding and generation [ Allamanis et al. , ICLR 2018 ; Brockschmidt et al. , ICLR 2019 ] , but also in models with graph-structured external memory [ Johnson , ICLR 2017 ] , in computer vision [ Marino et al. , CVPR 2017 ; Li et al. , ICCV 2017 ; Chuang et al. , CVPR 2018 ] , music generation [ Jeong et al. , ICML 2019 ] , and molecule generation [ Liu et al. , NeurIPS 2018 ] . This is just a subset of uses that we were able to find from a brief Google scholar search ; the paper introducing it has now accumulated over 600 citations and has been applied broadly . Further , we focus on the GGNN variant because that was the variant used in the work whose dataset and model we are building on [ Allamanis et al. , ICLR 2018 ] , but the techniques that we present could straightforwardly be adapted to other graph neural network variants , such as those categorized as `` message passing neural networks '' by Gilmer et al . [ ICML 2017 ] . ( 2 ) Apart from the GPU evaluation in Appendix D , we have indeed only evaluated our method on TPUs , which we believe to be representative of the class of \u201c dense hardware \u201d ( for lack of a better term ) as described in the 3rd paragraph of the Introduction . However : a ) The claim that the method is specific to TPUs is imprecise \u2014 by having eliminated the need for any sparse operations , it makes training sparse GGNNs possible on any hardware where sparse operations are unavailable or slow . b ) Our contributions 1-3 , including empirical results on large-batch training for GNNs , and the observation that GNN training is robust to dropping some non-conforming edges during training time , are entirely independent of TPUs and the results would be identical on any hardware ( that permits fast enough training to actually compute these results ) . Please let us know your thoughts ( and any further reconsideration would be appreciated ) ."}, {"review_id": "BklXkCNYDB-1", "review_text": "The authors present a framework to implement graph neural networks training efficiently ---an inherently sparse task--- using \"custom dense hardware\", here Tensor Processing Units (TPUs V2). The key steps are: (1) reordering the labels to reduce the bandwidth of the adjacency matrix, (2) (Sometimes approximate) Decomposition using block matrices for efficient storage and computations, and (3) memory layout optimization. They evaluate their framework on the VarMisuse dataset and compare the performance against a GPU implementation running on Nvidia Tesla V100. In the best configuration, they were able to reach 78% test accuracy in 13 minutes vs 19h for the baseline. The paper is well structured and thorough. I was able to understand the challenges and the solutions proposed, even without prior knowledge in the architecture this work focuses on. I think it is sufficiently detailed to enable an independent implementation without referring to the authors source code. However, I feel that it might lack novelty. Indeed, as described in the Related Work section, each component of the pipeline is well known and used very frequently in the HPC community. Sometimes, knowing how arrange common primitives is very powerful, and looking at the results of the experimental section, this is enough to improve performance by almost two orders of magnitude. However, I think it might be more due to some intrinsic properties of the dataset than the method itself. As made clear by the title of section 5.1, it is the data itself that has low bandwidth. If we consider a dataset that does not satisfy this requirement, stage (1) has no effect, and it is impossible to perform the decomposition done in (2). In that situation, the contributions of this paper would be nullified. It would be perfectly reasonable to think that most datasets have low bandwidth, but this not a claim that made the authors. This work would be considerably more impactful if it measured the bandwidth of more well recognized datasets. My small exposition to these problem does not allow me to make any meaningful suggestion. Finally, I am puzzled by the \"dense hardware\"/GPU distinction. From my experience, GPU devices *are* designed for and extremely efficient at dense linear algebra. Sparse operations are historically performed on CPUs. While they are possible on GPUs, they are usually much slower. For example CUSparse, the sparse matrices library, part of NVidia CUDA toolkit, was only introduced in its version 4. It's a clear indication that sparse operations are not a strength of GPUs. According to my experience writing GPU code, I feel that this approach would actually perform extremely well on GPUs as it does on TPUs. I think it is thus important to compare this framework on GPUs too. Since these optimizations are not TPU specific and have not been applied in the GPU based GNN libraries referenced in this paper reinforce my concerns that they are problem-specific. Even though the performance gains demonstrated are sizeable, the fact the approach does not seem TPU specific and is potentially problem specific makes me lean towards rejection. ", "rating": "6: Weak Accept", "reply_text": "Thank you for reviewing our work , and for praising the clarity and detail of our presentation . We 'd like to start by addressing the review 's final two paragraphs first , as we think we can clear up the puzzlement . > Paragraph starting `` Finally , I am puzzled by the `` dense hardware '' /GPU distinction. `` As we tried to explain in paragraph 3 of the Introduction , GPUs certainly perform well on dense operations , but they are flexible enough to efficiently support sparse operations . The `` dense hardware '' that we are targeting is even more specialized to dense operations than GPUs . For example , hardware based on systolic arrays ( https : //en.wikipedia.org/wiki/Systolic_array ) can lead to even better dense matrix-multiply performance , but at the cost of flexibility in supporting sparse operations . Note that TPUs are mentioned as a prominent example of this kind of hardware in the above wikipedia link . We acknowledge that `` dense hardware '' is not a standardized term , and we 're open to using alternative terminology that would be clearer if any of the reviewers have suggestions . > It 's a clear indication that sparse operations are not a strength of GPUs . While we agree GPUs may not be the optimal hardware for sparse operations , our experience is that it is better to place sparse graph neural network operations on GPU than CPU . It is possible to achieve good GPU utilization and also avoids having to copy activations back and forth from CPU to GPU during GNN propagation . > According to my experience writing GPU code , I feel that this approach would actually perform extremely well on GPUs as it does on TPUs . I think it is thus important to compare this framework on GPUs too . Actually , this experiment is included with the submission in Appendix D. We found that training was slower on a single V100 GPU than on a TPU with the same amount of RAM , which gives additional support to our above claims that what we 're calling `` dense hardware '' is more specialized to dense operations than GPUs . > Even though the performance gains demonstrated are sizeable , the fact the approach does not seem TPU specific and is potentially problem specific makes me lean towards rejection . In light of the above , we 'd ask the reviewer to reconsider their opinion on this . While we agree the approach can be run on GPUs , the benefit only comes when running on hardware that is more specialized to dense ( as opposed to sparse ) operations than GPUs . If we understand correctly , this addresses a major concern of the review . If not , could you please help us understand what we 're still missing ?"}, {"review_id": "BklXkCNYDB-2", "review_text": "This paper proposes a method to train graph neural networks on dense hardware such as TPUs. The method is motivated by an observation that connections in graphs have locality in some datasets. Experiments show significant improvements in training speed compared to single-GPU training. The overall score of this paper is slightly positive. There is a certain demand to perform training on hardware targeted to dense computations. Even though the applications of the proposed method is limited to data with low-bandwidth, the paper shows there are real applications of the method. The effectiveness of the proposed method is well-supported by the experiments. Major comments: Comparisons with single-GPU training can be unfair. The method in Ma et al. (2018) is indeed not easy to scale many GPUs because their target is processing extremely large graphs in parallel. Since the experiments in the submitted paper use relatively small graphs that fit in a single GPU memory, it will not be so challenging to scale many GPUs. At least, it is recommended to compare the results with training on several GPUs using data-parallel execution implemented in TensorFlow (or any other suitable frameworks). If it is difficult, please provide more specific reasons why it is challenging to perform multi-GPU training.", "rating": "6: Weak Accept", "reply_text": "Thanks for your encouraging review ! We do n't claim that it is impossible to match our training speeds using a large number of GPUs , but we are not aware of any work that has successfully done so . Our claim in this regard is simply that we have achieved training speeds that are far better than any existing results . While we agree Ma et al . [ 2018 ] focus on larger graphs , we do not think all the challenges they encounter could be totally avoided on the Allamanis et al . [ 2018 ] dataset that we use . For example , we believe the challenges related to shared PCIe interconnect [ Ma et al.2018 , Sec.6.3 ] would still persist . We reported single GPU training times to establish that training the model to state of the art accuracy takes a reasonable amount of time . This helps contextualize the results we get on multi-TPU training . We 'd like to reiterate that before this work , it was not clear that it would be possible to use dense hardware to train sparse GNNs in any reasonable timeframe at all , because the hardware is very specialized to fixed-size dense computation , and a naive densification of large graphs is n't feasible . Our results showing that it 's possible are valuable because this style of dense hardware is becoming increasingly prevalent as hardware becomes more specialized to matrix multiply-based workloads . Since the presented techniques did allow us to train on TPUs , we exploited the ease of scaling up to 512 cores ( with TPUs it is a matter of changing a single parameter ) in order to report results from large-batch training of sparse GGNNs , and we also pointed out the fast training time one can achieve this way ."}], "0": {"review_id": "BklXkCNYDB-0", "review_text": "The authors propose a method to speed-up the time to validation accuracy for a particular class of graph neural networks: Gated graph sequence neural networks (GGSNNs). The paper is interesting in that it describes several operations and engineering considerations to speed up the processing of a GGSNN on TPUs. It is essentially a collection of engineering steps that improve the time to validation accuracy. While I'm not an expert in (G)NN acceleration on TPUs, I have experience with GNNs and approaches to accelerate CNNs in GPUs. My assessment is that the scope of this work is far too narrow. It is specific to GGSNNs which is a small family of GNNs not widely used. It is also specific to TPUs and lacks evaluations of the proposed approach on other type of hardware. It is for these reasons that I think the paper is not appropriate for ICLR. The scope has to be broadened both in terms of the NN models and the hardware types. ", "rating": "3: Weak Reject", "reply_text": "Thank you for taking the time to review our work . We would like to respond to both raised concerns : the method being ( 1 ) specific to a model family that is \u201c not widely used \u201d , and ( 2 ) \u201c specific to TPUs \u201d . ( 1 ) We would like to point out that the claim in the review that our work is specific to \u201c Gated graph sequence neural networks ( GGSNNs ) \u201d is imprecise . The confusion may have arisen because the cited paper [ Li et al. , 2016 ] introduces both Gated Graph Neural Networks ( GGNNs ) as well as GGSNNs , but as explained and described at the beginning of Section 2 , we use GGNNs to demonstrate our method . We are not sure how to best define \u201c widely used \u201d , but GGNNs have been successfully used not only for program understanding understanding and generation [ Allamanis et al. , ICLR 2018 ; Brockschmidt et al. , ICLR 2019 ] , but also in models with graph-structured external memory [ Johnson , ICLR 2017 ] , in computer vision [ Marino et al. , CVPR 2017 ; Li et al. , ICCV 2017 ; Chuang et al. , CVPR 2018 ] , music generation [ Jeong et al. , ICML 2019 ] , and molecule generation [ Liu et al. , NeurIPS 2018 ] . This is just a subset of uses that we were able to find from a brief Google scholar search ; the paper introducing it has now accumulated over 600 citations and has been applied broadly . Further , we focus on the GGNN variant because that was the variant used in the work whose dataset and model we are building on [ Allamanis et al. , ICLR 2018 ] , but the techniques that we present could straightforwardly be adapted to other graph neural network variants , such as those categorized as `` message passing neural networks '' by Gilmer et al . [ ICML 2017 ] . ( 2 ) Apart from the GPU evaluation in Appendix D , we have indeed only evaluated our method on TPUs , which we believe to be representative of the class of \u201c dense hardware \u201d ( for lack of a better term ) as described in the 3rd paragraph of the Introduction . However : a ) The claim that the method is specific to TPUs is imprecise \u2014 by having eliminated the need for any sparse operations , it makes training sparse GGNNs possible on any hardware where sparse operations are unavailable or slow . b ) Our contributions 1-3 , including empirical results on large-batch training for GNNs , and the observation that GNN training is robust to dropping some non-conforming edges during training time , are entirely independent of TPUs and the results would be identical on any hardware ( that permits fast enough training to actually compute these results ) . Please let us know your thoughts ( and any further reconsideration would be appreciated ) ."}, "1": {"review_id": "BklXkCNYDB-1", "review_text": "The authors present a framework to implement graph neural networks training efficiently ---an inherently sparse task--- using \"custom dense hardware\", here Tensor Processing Units (TPUs V2). The key steps are: (1) reordering the labels to reduce the bandwidth of the adjacency matrix, (2) (Sometimes approximate) Decomposition using block matrices for efficient storage and computations, and (3) memory layout optimization. They evaluate their framework on the VarMisuse dataset and compare the performance against a GPU implementation running on Nvidia Tesla V100. In the best configuration, they were able to reach 78% test accuracy in 13 minutes vs 19h for the baseline. The paper is well structured and thorough. I was able to understand the challenges and the solutions proposed, even without prior knowledge in the architecture this work focuses on. I think it is sufficiently detailed to enable an independent implementation without referring to the authors source code. However, I feel that it might lack novelty. Indeed, as described in the Related Work section, each component of the pipeline is well known and used very frequently in the HPC community. Sometimes, knowing how arrange common primitives is very powerful, and looking at the results of the experimental section, this is enough to improve performance by almost two orders of magnitude. However, I think it might be more due to some intrinsic properties of the dataset than the method itself. As made clear by the title of section 5.1, it is the data itself that has low bandwidth. If we consider a dataset that does not satisfy this requirement, stage (1) has no effect, and it is impossible to perform the decomposition done in (2). In that situation, the contributions of this paper would be nullified. It would be perfectly reasonable to think that most datasets have low bandwidth, but this not a claim that made the authors. This work would be considerably more impactful if it measured the bandwidth of more well recognized datasets. My small exposition to these problem does not allow me to make any meaningful suggestion. Finally, I am puzzled by the \"dense hardware\"/GPU distinction. From my experience, GPU devices *are* designed for and extremely efficient at dense linear algebra. Sparse operations are historically performed on CPUs. While they are possible on GPUs, they are usually much slower. For example CUSparse, the sparse matrices library, part of NVidia CUDA toolkit, was only introduced in its version 4. It's a clear indication that sparse operations are not a strength of GPUs. According to my experience writing GPU code, I feel that this approach would actually perform extremely well on GPUs as it does on TPUs. I think it is thus important to compare this framework on GPUs too. Since these optimizations are not TPU specific and have not been applied in the GPU based GNN libraries referenced in this paper reinforce my concerns that they are problem-specific. Even though the performance gains demonstrated are sizeable, the fact the approach does not seem TPU specific and is potentially problem specific makes me lean towards rejection. ", "rating": "6: Weak Accept", "reply_text": "Thank you for reviewing our work , and for praising the clarity and detail of our presentation . We 'd like to start by addressing the review 's final two paragraphs first , as we think we can clear up the puzzlement . > Paragraph starting `` Finally , I am puzzled by the `` dense hardware '' /GPU distinction. `` As we tried to explain in paragraph 3 of the Introduction , GPUs certainly perform well on dense operations , but they are flexible enough to efficiently support sparse operations . The `` dense hardware '' that we are targeting is even more specialized to dense operations than GPUs . For example , hardware based on systolic arrays ( https : //en.wikipedia.org/wiki/Systolic_array ) can lead to even better dense matrix-multiply performance , but at the cost of flexibility in supporting sparse operations . Note that TPUs are mentioned as a prominent example of this kind of hardware in the above wikipedia link . We acknowledge that `` dense hardware '' is not a standardized term , and we 're open to using alternative terminology that would be clearer if any of the reviewers have suggestions . > It 's a clear indication that sparse operations are not a strength of GPUs . While we agree GPUs may not be the optimal hardware for sparse operations , our experience is that it is better to place sparse graph neural network operations on GPU than CPU . It is possible to achieve good GPU utilization and also avoids having to copy activations back and forth from CPU to GPU during GNN propagation . > According to my experience writing GPU code , I feel that this approach would actually perform extremely well on GPUs as it does on TPUs . I think it is thus important to compare this framework on GPUs too . Actually , this experiment is included with the submission in Appendix D. We found that training was slower on a single V100 GPU than on a TPU with the same amount of RAM , which gives additional support to our above claims that what we 're calling `` dense hardware '' is more specialized to dense operations than GPUs . > Even though the performance gains demonstrated are sizeable , the fact the approach does not seem TPU specific and is potentially problem specific makes me lean towards rejection . In light of the above , we 'd ask the reviewer to reconsider their opinion on this . While we agree the approach can be run on GPUs , the benefit only comes when running on hardware that is more specialized to dense ( as opposed to sparse ) operations than GPUs . If we understand correctly , this addresses a major concern of the review . If not , could you please help us understand what we 're still missing ?"}, "2": {"review_id": "BklXkCNYDB-2", "review_text": "This paper proposes a method to train graph neural networks on dense hardware such as TPUs. The method is motivated by an observation that connections in graphs have locality in some datasets. Experiments show significant improvements in training speed compared to single-GPU training. The overall score of this paper is slightly positive. There is a certain demand to perform training on hardware targeted to dense computations. Even though the applications of the proposed method is limited to data with low-bandwidth, the paper shows there are real applications of the method. The effectiveness of the proposed method is well-supported by the experiments. Major comments: Comparisons with single-GPU training can be unfair. The method in Ma et al. (2018) is indeed not easy to scale many GPUs because their target is processing extremely large graphs in parallel. Since the experiments in the submitted paper use relatively small graphs that fit in a single GPU memory, it will not be so challenging to scale many GPUs. At least, it is recommended to compare the results with training on several GPUs using data-parallel execution implemented in TensorFlow (or any other suitable frameworks). If it is difficult, please provide more specific reasons why it is challenging to perform multi-GPU training.", "rating": "6: Weak Accept", "reply_text": "Thanks for your encouraging review ! We do n't claim that it is impossible to match our training speeds using a large number of GPUs , but we are not aware of any work that has successfully done so . Our claim in this regard is simply that we have achieved training speeds that are far better than any existing results . While we agree Ma et al . [ 2018 ] focus on larger graphs , we do not think all the challenges they encounter could be totally avoided on the Allamanis et al . [ 2018 ] dataset that we use . For example , we believe the challenges related to shared PCIe interconnect [ Ma et al.2018 , Sec.6.3 ] would still persist . We reported single GPU training times to establish that training the model to state of the art accuracy takes a reasonable amount of time . This helps contextualize the results we get on multi-TPU training . We 'd like to reiterate that before this work , it was not clear that it would be possible to use dense hardware to train sparse GNNs in any reasonable timeframe at all , because the hardware is very specialized to fixed-size dense computation , and a naive densification of large graphs is n't feasible . Our results showing that it 's possible are valuable because this style of dense hardware is becoming increasingly prevalent as hardware becomes more specialized to matrix multiply-based workloads . Since the presented techniques did allow us to train on TPUs , we exploited the ease of scaling up to 512 cores ( with TPUs it is a matter of changing a single parameter ) in order to report results from large-batch training of sparse GGNNs , and we also pointed out the fast training time one can achieve this way ."}}