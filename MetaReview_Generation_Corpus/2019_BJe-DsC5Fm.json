{"year": "2019", "forum": "BJe-DsC5Fm", "title": "signSGD via Zeroth-Order Oracle", "decision": "Accept (Poster)", "meta_review": "This is a solid paper that proposes and analyzes a sound approach to zero order optimization, covering a variants of a simple base algorithm.  After resolving some issues during the response period, the reviewers concluded with a unanimous recommendation of acceptance.  Some concerns regarding the necessity for such algorithms persisted, but the connection to adversarial examples provides an interesting motivation.", "reviews": [{"review_id": "BJe-DsC5Fm-0", "review_text": "The authors proposed a zero-order version of the recent signSGD algorithm, by replacing the stochastic gradient with a usual function difference estimate. Similar convergence rates as signSGD were obtained, with an additional sqrt(d) factor which is typical in zero-order methods. Three (typical) gradient estimates based on function values were discussed. Overall, the obtained results are relatively straightforward combination of signSGD with existing zero-order techniques. Quality: The technical part of this paper seems to be solid. The experiments, on the other hand, are quite ambiguous. First off, why do you choose that peculiar least squares binary classification problem on page 7? Is Assumption A2 satisfied for this problem? Why not use logistic regression? The experimental results are also strange: Why would ZO-signSGD converge faster than ZO-SGD or any other ZO variant? Shouldn't they enjoy similar rates of convergence? Why would taking the sign make the algorithm converge faster? Note that the original motivation for signSGD is not for faster convergence but less communication. For the second set of experiment, how do you apply ZO-SGD to generate adversarial examples? Again, why do we expect ZO-signSGD to perform better than ZO-SGD? Clarity: This paper is mostly well-written, but the authors at times largely overclaim their contributions or exaggerate the technical challenges. -- Page 2, 2nd line: the authors claim that \"Our analysis removes the impractical assumption of b = O(T)\", but in the later examples (page 6, top), they require q = O(T). How is this any different than b = O(T)? Even worse, the former case also require b = n, i.e., there is no stochasity at all... -- Assumption A2: how crucial is this assumption for obtaining the convergence results? note that not many functions have Lipschitz continuous bounded gradients... (logistic regression is an example) -- Page 4, top: \"ZO-signSGD has no restriction on the mini-batch size b\"? The rates at the end of page 5 suggests otherwise if we want the bound to go to 0 (due to the term sqrt(d/b)). -- Page 4, top: the last two technical challenges do not make sense: once we replace f by f_mu, these difficulties go away immediately, and it is well-known how to relate f_mu with f. Originality: The originality seems to be limited. Contrary to what the authors claimed, I found the established results to be relatively straightforward combination of signSGD and existing zero-order techniques. Can the authors elaborate on what additional difficulties they need to overcome in order to extend existing zero-order results to the signSGD case? Significance: The proposed zero-order version of signSGD may potentially be significant in applications where gradient information is not available and yet distributed optimization is needed. This, however, is not demonstrated in the paper as the authors never considered distributed optimization. ##### added after author response ##### I appreciate the authors effort in trying to make their contributions precise and appropriate. The connection between ZO-signSGD and adversarial examples is further elaborated, which I agree is an interesting and potentially fruitful direction. I commend the authors for supplying further experiments to explain the pros and cons of the proposed algorithms. Many of the concerns in my original review were largely alleviate/addressed. As such, I have raised my original evaluation.", "rating": "7: Good paper, accept", "reply_text": "Response to Reviewer 3 ( Q : question ; R : response ) : Q : The obtained results are relatively straightforward combination of signSGD with existing zero-order techniques . And question on originality : Can the authors elaborate on what additional difficulties they need to overcome in order to extend existing zero-order results to the signSGD case ? R : We are sorry to learn that the reviewer feels our work is a relatively straightforward combination of signSGD with existing zero-order techniques . Based on the reviewer \u2019 s comments , our paper has been largely improved . In what follows , we clarity our main contributions and 'additional difficulties ' . First , beyond signSGD , our established results apply to the case of mini-batch sampling without replacement . And thus , ZO-signGD can be treated as a special case in our analysis . To derive the variance of ZO gradient estimate , we require careful analysis on the effects of two types of mini-batch sampling as well as random direction sampling , and then link them with statistics of a single random gradient estimate known in the existing ZO results . Second , to derive the eventual convergence error of ZO-signSGD , we require to fill the gap between the L1 geometry of signSGD and the variance of the ZO gradient estimate in terms of squared L2 norm . Moreover , we require to study the effects of different types of ZO gradient estimators on the convergence of ZO-signSGD . In particular , sign-based gradient estimators , ( 11 ) - ( 12 ) in Sec.5 , have not been well studied in the ZO literature . These estimators can be interpreted as the ZO counterparts of first-order gradient estimators with majority vote in the centralized and distributed settings . Last but not the least , our goal is not to 'combine ' ZO and signSGD . As a matter of fact , ZO-signSGD has been well motivated in the design of black-box adversarial examples ( Ilyas et al. , 2018a ) . However , the formal connection between optimization theory and adversarial ML was not fully established . Our work provides a comprehensive study on ZO-signSGD from multiple perspectives including convergence analysis , gradient estimator , and applications . We really hope that the reviewer can recognize the contributions of this work in both theory and practice . Q : The technical part of this paper seems to be solid . The experiments , on the other hand , are quite ambiguous . First off , why do you choose that peculiar least squares binary classification problem on page 7 ? Is Assumption A2 satisfied for this problem ? Why not use logistic regression ? R : The least squared formulation is commonly used for nonconvex machine learning ( Xue et al.2017 ) , given the fact that the standard logistic regression yields a convex problem . Since we study ZO-signSGD in the nonconvex setting , we choose to solve the least squared binary classification problem in order to make empirical studies consistent with theory . And Assumption A2 is indeed satisfied for the proposed problem . This is not difficult to prove by the boundedness of the sigmoid function . We have clarified this point in Sec.6.P . Xu , F. Roosta-Khorasan , and M. W. Mahoney . Second-order optimization for non-convex machine learning : An empirical study . arXiv preprint arXiv:1708.07827 , 2017"}, {"review_id": "BJe-DsC5Fm-1", "review_text": "The paper presents algorithms for optimization using sign-SGD when the access is restricted to a zero order oracle only, and provide detailed analysis and convergence rates. They also run optimization experiments on synthetic data. Additionally, they demonstrate superiority of the algorithm in the number of oracle calls for black box adversarial attacks for MNIST and CIFAR-10. The provided algorithm has optimal iteration complexity from a theoretical viewpoint. The paper was, overall very well written and sufficient experiment were presented. The math also seems correct. However, I think they should have explained the motivation for the need of developing such an algorithm better. Section 3 can be improved. I think this is an important paper because it provides a guaranteed algorithm for zero order sign-gradient descent. However, the ideas and the estimators are not novel. They show applicability of standard gradient estimators for zero order oracles for sign-sgd algorithm. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Response to reviewer 1 ( Q : question ; R : response ) Q : The paper was , overall very well written and sufficient experiment were presented . The math also seems correct . However , I think they should have explained the motivation for the need of developing such an algorithm better . Section 3 can be improved . R : Based on this comment , we have improved Sec.3 and added a subsection \u2018 Motivations of ZO-signSGD \u2019 . Particularly , two concrete motivating examples ( Appendix 1 ) are presented to motivate how ZO-signSGD could outperform ZO-SGD . In Fig.A1 , we show the robustness of ZO-signSGD against sparse noise perturbation through a quadratic optimization problem , first introduced by ( Bernstein et al. , 2018 ) . In Fig.A2 , we show that ZO gradient estimates indeed encounter gradient noise of large variance . Thus , taking the sign of a gradient estimate might scale down the extremely noisy components . Moreover , in Sec.6 , we have added an experiment to compare ZO-signSGD with a benchmark black-box attack generation method ( Ilyas et al. , 2018a ) . As we can see , ZO-signSGD offers fast convergence to the first successful adversarial attack under limited queries . Q : I think this is an important paper because it provides a guaranteed algorithm for zero order sign-gradient descent . However , the ideas and the estimators are not novel . They show applicability of standard gradient estimators for zero order oracles for sign-sgd algorithm . R : We thank R1 for the positive comments on our paper . We would like to point out that sign-based gradient estimators , e.g. , ( 11 ) - ( 12 ) in Sec.5 , have not been well studied in the ZO literature . These estimators can be interpreted as the ZO counterparts of first-order gradient estimators with majority vote in the centralized and distributed settings , respectively . Here the ZO gradient estimator ( 12 ) is newly introduced for ZO distributed optimization . Even the gradient estimators ( 3 ) and ( 10 ) were used by existing ZO methods , how they affect the convergence of ZO-signSGD has not been well studied . Due to their popularity in designing black-box adversarial examples ( Ilyas et al. , 2018a ) , it is important to rigorously analyze the effect of standard gradient estimators on ZO-signSGD , in order to characterize their limitations or possible improvements . Refs : A. Ilyas , L. Engstrom , A. Athalye , and J. Lin . Black-box adversarial attacks with limited queries and information . ICLR 2018 ."}, {"review_id": "BJe-DsC5Fm-2", "review_text": "In this paper, the authors studied zeroth order sign SGD. Sign SGD is commonly used in adversarial example generation. Compared to sign SGD, zeroth-order sign SGD does not require the knowledge of the magnitude of the gradient, which makes it suitable to optimize black-box systems. The authors studied the convergence rate of zeroth-order sign SGD, and showed that under common assumptions, zero-order sign SGD achieves O(sqrt(d/T)) convergence rate, which is slower than sign SGD by a factor of sqrt(d). However, sign SGD requires an unrealisitcally large mini-batch size, which zeroth-order sign SGD does not. The authors demonstrated the performance of zeroth-order sign SGD in numerical experiments. Overall, this is a well written paper. The convergence property of the zeroth-order sign SGD is sufficiently studied. The proposal seems to be useful in real world tasks. Weaknesses: 1) out of curiosity, can we improve the convergence rate of the zeroth-order sign SGD if we assume the mini-batch size is of order O(T)? This could help us better compare zeroth-order sign SGD and sign SGD. 2) Figure 2 is too small to be legible. Also, it seems that the adversarial examples generated by zeroth-order sign SGD have higher distortion than those found by zeroth-order SGD on CIFAR-10 dataset. Is it true? If so, it would be beneficial to have a qualitative explanation of such behavior.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Reviewer # 2 ( Q : question ; R : response ) : We thank the reviewer for the positive comments on our paper . We provide the detailed response to each comment as below . Q : 1 ) out of curiosity , can we improve the convergence rate of the zeroth-order sign SGD if we assume the mini-batch size is of order O ( T ) ? This could help us better compare zeroth-order sign SGD and sign SGD . R : Yes , the large mini-batch size of b = O ( T ) indeed improves the convergence rate of ZO-signSGD . As b = O ( T ) , the convergence rate given in ( 9 ) becomes O ( \\sqrt { d } /\\sqrt { T } + \\alpha_b \\sqrt { d } /\\sqrt { T } + d/\\sqrt { Tq } ) , where the last error term O ( d/\\sqrt { Tq } ) is induced by ZO gradient estimation error . In order to further improve the rate to O ( \\sqrt { d } /\\sqrt { T } ) , it is required to make the number of random direction samples $ q $ proportional to $ d $ . Similar to other ZO methods ( Liu et al.2018 ; Hajinezhad et al.2017 ) , the large q helps to reduce the variance of ZO gradient estimates . On the other hand , the assumption of b = O ( T ) might not be necessary if n < O ( T ) , where n is the total number of individual cost functions . Suppose that b = n and we use mini-batch sampling without replacement , then ZO-signSGD becomes ZO-signGD . This leads to the convergence rate O ( \\sqrt { d } /\\sqrt { T } + d/\\sqrt { nq } ) . In this case , we can improve the rate to recover O ( \\sqrt { d } /\\sqrt { T } ) by only setting the number of random direction vectors induced by ZO gradient estimation , $ q = O ( dT/n ) $ . It is worth mentioning that such an improvement can not be achieved by ZO-signSGD using mini-batch with replacement even if b = n with the same setting of q . We refer reviewer to our detailed analysis in the last paragraph of Sec.4.S . Liu , et al. , Zeroth-order stochastic variance reduction for nonconvex optimization , NIPS , 2018 D. Hajinezhad , et al. , \u201c Zeroth order nonconvex multi-agent optimization over networks , \u201d arXiv preprint arXiv:1710.09997 , 2017 . Q : 2 ) Figure 2 is too small to be legible . Also , it seems that the adversarial examples generated by zeroth-order sign SGD have higher distortion than those found by zeroth-order SGD on CIFAR-10 dataset . Is it true ? If so , it would be beneficial to have a qualitative explanation of such behavior . R : We have enlarged Figure 2 . Yes , Given the first successful adversarial example , we observe that ZO-signSGD yields slightly higher L2 distortion than ZO-SGD . This is not surprising since compared to ZO-SGD , the convergence rate of ZO-signSGD involves an additional error correction term ( relying on b and q in ( 9 ) ) . Accordingly , ZO-signSGD might converge to moderate accuracy ( e.g. , a solution neighborhood ) rather than a very high accuracy . However , the convergence of ZO-signSGD to moderate accuracy could be much faster than ZO-SGD since the former meets a stricter convergence criterion ( L2 norm of gradient ) than that of ZO-SGD ( squared L2 norm of gradient ) . We refer the reviewer to the paragraph after Eq . ( 9 ) for more discussions . In the example of generating black-box adversarial attacks , compared to convergence accuracy ( in terms of attack distortion ) , the effectiveness of a black-box attack is measured by the number of function queries needed to achieve the first successful adversarial attack . Thus , ZO-signSGD is desired in this application due to its fast convergence to moderate accuracy . To further confirm this point , in Sec.6 we have added an experiment to compare ZO-signSGD with a benchmark black-box attack generation method in ( Ilyas et al. , 2018a ) . Indeed , ZO-signSGD offers fast convergence to the first successful adversarial attack under limited queries . A. Ilyas , et al. , Black-box adversarial attacks with limited queries and information . ICLR 2018 ."}], "0": {"review_id": "BJe-DsC5Fm-0", "review_text": "The authors proposed a zero-order version of the recent signSGD algorithm, by replacing the stochastic gradient with a usual function difference estimate. Similar convergence rates as signSGD were obtained, with an additional sqrt(d) factor which is typical in zero-order methods. Three (typical) gradient estimates based on function values were discussed. Overall, the obtained results are relatively straightforward combination of signSGD with existing zero-order techniques. Quality: The technical part of this paper seems to be solid. The experiments, on the other hand, are quite ambiguous. First off, why do you choose that peculiar least squares binary classification problem on page 7? Is Assumption A2 satisfied for this problem? Why not use logistic regression? The experimental results are also strange: Why would ZO-signSGD converge faster than ZO-SGD or any other ZO variant? Shouldn't they enjoy similar rates of convergence? Why would taking the sign make the algorithm converge faster? Note that the original motivation for signSGD is not for faster convergence but less communication. For the second set of experiment, how do you apply ZO-SGD to generate adversarial examples? Again, why do we expect ZO-signSGD to perform better than ZO-SGD? Clarity: This paper is mostly well-written, but the authors at times largely overclaim their contributions or exaggerate the technical challenges. -- Page 2, 2nd line: the authors claim that \"Our analysis removes the impractical assumption of b = O(T)\", but in the later examples (page 6, top), they require q = O(T). How is this any different than b = O(T)? Even worse, the former case also require b = n, i.e., there is no stochasity at all... -- Assumption A2: how crucial is this assumption for obtaining the convergence results? note that not many functions have Lipschitz continuous bounded gradients... (logistic regression is an example) -- Page 4, top: \"ZO-signSGD has no restriction on the mini-batch size b\"? The rates at the end of page 5 suggests otherwise if we want the bound to go to 0 (due to the term sqrt(d/b)). -- Page 4, top: the last two technical challenges do not make sense: once we replace f by f_mu, these difficulties go away immediately, and it is well-known how to relate f_mu with f. Originality: The originality seems to be limited. Contrary to what the authors claimed, I found the established results to be relatively straightforward combination of signSGD and existing zero-order techniques. Can the authors elaborate on what additional difficulties they need to overcome in order to extend existing zero-order results to the signSGD case? Significance: The proposed zero-order version of signSGD may potentially be significant in applications where gradient information is not available and yet distributed optimization is needed. This, however, is not demonstrated in the paper as the authors never considered distributed optimization. ##### added after author response ##### I appreciate the authors effort in trying to make their contributions precise and appropriate. The connection between ZO-signSGD and adversarial examples is further elaborated, which I agree is an interesting and potentially fruitful direction. I commend the authors for supplying further experiments to explain the pros and cons of the proposed algorithms. Many of the concerns in my original review were largely alleviate/addressed. As such, I have raised my original evaluation.", "rating": "7: Good paper, accept", "reply_text": "Response to Reviewer 3 ( Q : question ; R : response ) : Q : The obtained results are relatively straightforward combination of signSGD with existing zero-order techniques . And question on originality : Can the authors elaborate on what additional difficulties they need to overcome in order to extend existing zero-order results to the signSGD case ? R : We are sorry to learn that the reviewer feels our work is a relatively straightforward combination of signSGD with existing zero-order techniques . Based on the reviewer \u2019 s comments , our paper has been largely improved . In what follows , we clarity our main contributions and 'additional difficulties ' . First , beyond signSGD , our established results apply to the case of mini-batch sampling without replacement . And thus , ZO-signGD can be treated as a special case in our analysis . To derive the variance of ZO gradient estimate , we require careful analysis on the effects of two types of mini-batch sampling as well as random direction sampling , and then link them with statistics of a single random gradient estimate known in the existing ZO results . Second , to derive the eventual convergence error of ZO-signSGD , we require to fill the gap between the L1 geometry of signSGD and the variance of the ZO gradient estimate in terms of squared L2 norm . Moreover , we require to study the effects of different types of ZO gradient estimators on the convergence of ZO-signSGD . In particular , sign-based gradient estimators , ( 11 ) - ( 12 ) in Sec.5 , have not been well studied in the ZO literature . These estimators can be interpreted as the ZO counterparts of first-order gradient estimators with majority vote in the centralized and distributed settings . Last but not the least , our goal is not to 'combine ' ZO and signSGD . As a matter of fact , ZO-signSGD has been well motivated in the design of black-box adversarial examples ( Ilyas et al. , 2018a ) . However , the formal connection between optimization theory and adversarial ML was not fully established . Our work provides a comprehensive study on ZO-signSGD from multiple perspectives including convergence analysis , gradient estimator , and applications . We really hope that the reviewer can recognize the contributions of this work in both theory and practice . Q : The technical part of this paper seems to be solid . The experiments , on the other hand , are quite ambiguous . First off , why do you choose that peculiar least squares binary classification problem on page 7 ? Is Assumption A2 satisfied for this problem ? Why not use logistic regression ? R : The least squared formulation is commonly used for nonconvex machine learning ( Xue et al.2017 ) , given the fact that the standard logistic regression yields a convex problem . Since we study ZO-signSGD in the nonconvex setting , we choose to solve the least squared binary classification problem in order to make empirical studies consistent with theory . And Assumption A2 is indeed satisfied for the proposed problem . This is not difficult to prove by the boundedness of the sigmoid function . We have clarified this point in Sec.6.P . Xu , F. Roosta-Khorasan , and M. W. Mahoney . Second-order optimization for non-convex machine learning : An empirical study . arXiv preprint arXiv:1708.07827 , 2017"}, "1": {"review_id": "BJe-DsC5Fm-1", "review_text": "The paper presents algorithms for optimization using sign-SGD when the access is restricted to a zero order oracle only, and provide detailed analysis and convergence rates. They also run optimization experiments on synthetic data. Additionally, they demonstrate superiority of the algorithm in the number of oracle calls for black box adversarial attacks for MNIST and CIFAR-10. The provided algorithm has optimal iteration complexity from a theoretical viewpoint. The paper was, overall very well written and sufficient experiment were presented. The math also seems correct. However, I think they should have explained the motivation for the need of developing such an algorithm better. Section 3 can be improved. I think this is an important paper because it provides a guaranteed algorithm for zero order sign-gradient descent. However, the ideas and the estimators are not novel. They show applicability of standard gradient estimators for zero order oracles for sign-sgd algorithm. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Response to reviewer 1 ( Q : question ; R : response ) Q : The paper was , overall very well written and sufficient experiment were presented . The math also seems correct . However , I think they should have explained the motivation for the need of developing such an algorithm better . Section 3 can be improved . R : Based on this comment , we have improved Sec.3 and added a subsection \u2018 Motivations of ZO-signSGD \u2019 . Particularly , two concrete motivating examples ( Appendix 1 ) are presented to motivate how ZO-signSGD could outperform ZO-SGD . In Fig.A1 , we show the robustness of ZO-signSGD against sparse noise perturbation through a quadratic optimization problem , first introduced by ( Bernstein et al. , 2018 ) . In Fig.A2 , we show that ZO gradient estimates indeed encounter gradient noise of large variance . Thus , taking the sign of a gradient estimate might scale down the extremely noisy components . Moreover , in Sec.6 , we have added an experiment to compare ZO-signSGD with a benchmark black-box attack generation method ( Ilyas et al. , 2018a ) . As we can see , ZO-signSGD offers fast convergence to the first successful adversarial attack under limited queries . Q : I think this is an important paper because it provides a guaranteed algorithm for zero order sign-gradient descent . However , the ideas and the estimators are not novel . They show applicability of standard gradient estimators for zero order oracles for sign-sgd algorithm . R : We thank R1 for the positive comments on our paper . We would like to point out that sign-based gradient estimators , e.g. , ( 11 ) - ( 12 ) in Sec.5 , have not been well studied in the ZO literature . These estimators can be interpreted as the ZO counterparts of first-order gradient estimators with majority vote in the centralized and distributed settings , respectively . Here the ZO gradient estimator ( 12 ) is newly introduced for ZO distributed optimization . Even the gradient estimators ( 3 ) and ( 10 ) were used by existing ZO methods , how they affect the convergence of ZO-signSGD has not been well studied . Due to their popularity in designing black-box adversarial examples ( Ilyas et al. , 2018a ) , it is important to rigorously analyze the effect of standard gradient estimators on ZO-signSGD , in order to characterize their limitations or possible improvements . Refs : A. Ilyas , L. Engstrom , A. Athalye , and J. Lin . Black-box adversarial attacks with limited queries and information . ICLR 2018 ."}, "2": {"review_id": "BJe-DsC5Fm-2", "review_text": "In this paper, the authors studied zeroth order sign SGD. Sign SGD is commonly used in adversarial example generation. Compared to sign SGD, zeroth-order sign SGD does not require the knowledge of the magnitude of the gradient, which makes it suitable to optimize black-box systems. The authors studied the convergence rate of zeroth-order sign SGD, and showed that under common assumptions, zero-order sign SGD achieves O(sqrt(d/T)) convergence rate, which is slower than sign SGD by a factor of sqrt(d). However, sign SGD requires an unrealisitcally large mini-batch size, which zeroth-order sign SGD does not. The authors demonstrated the performance of zeroth-order sign SGD in numerical experiments. Overall, this is a well written paper. The convergence property of the zeroth-order sign SGD is sufficiently studied. The proposal seems to be useful in real world tasks. Weaknesses: 1) out of curiosity, can we improve the convergence rate of the zeroth-order sign SGD if we assume the mini-batch size is of order O(T)? This could help us better compare zeroth-order sign SGD and sign SGD. 2) Figure 2 is too small to be legible. Also, it seems that the adversarial examples generated by zeroth-order sign SGD have higher distortion than those found by zeroth-order SGD on CIFAR-10 dataset. Is it true? If so, it would be beneficial to have a qualitative explanation of such behavior.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Reviewer # 2 ( Q : question ; R : response ) : We thank the reviewer for the positive comments on our paper . We provide the detailed response to each comment as below . Q : 1 ) out of curiosity , can we improve the convergence rate of the zeroth-order sign SGD if we assume the mini-batch size is of order O ( T ) ? This could help us better compare zeroth-order sign SGD and sign SGD . R : Yes , the large mini-batch size of b = O ( T ) indeed improves the convergence rate of ZO-signSGD . As b = O ( T ) , the convergence rate given in ( 9 ) becomes O ( \\sqrt { d } /\\sqrt { T } + \\alpha_b \\sqrt { d } /\\sqrt { T } + d/\\sqrt { Tq } ) , where the last error term O ( d/\\sqrt { Tq } ) is induced by ZO gradient estimation error . In order to further improve the rate to O ( \\sqrt { d } /\\sqrt { T } ) , it is required to make the number of random direction samples $ q $ proportional to $ d $ . Similar to other ZO methods ( Liu et al.2018 ; Hajinezhad et al.2017 ) , the large q helps to reduce the variance of ZO gradient estimates . On the other hand , the assumption of b = O ( T ) might not be necessary if n < O ( T ) , where n is the total number of individual cost functions . Suppose that b = n and we use mini-batch sampling without replacement , then ZO-signSGD becomes ZO-signGD . This leads to the convergence rate O ( \\sqrt { d } /\\sqrt { T } + d/\\sqrt { nq } ) . In this case , we can improve the rate to recover O ( \\sqrt { d } /\\sqrt { T } ) by only setting the number of random direction vectors induced by ZO gradient estimation , $ q = O ( dT/n ) $ . It is worth mentioning that such an improvement can not be achieved by ZO-signSGD using mini-batch with replacement even if b = n with the same setting of q . We refer reviewer to our detailed analysis in the last paragraph of Sec.4.S . Liu , et al. , Zeroth-order stochastic variance reduction for nonconvex optimization , NIPS , 2018 D. Hajinezhad , et al. , \u201c Zeroth order nonconvex multi-agent optimization over networks , \u201d arXiv preprint arXiv:1710.09997 , 2017 . Q : 2 ) Figure 2 is too small to be legible . Also , it seems that the adversarial examples generated by zeroth-order sign SGD have higher distortion than those found by zeroth-order SGD on CIFAR-10 dataset . Is it true ? If so , it would be beneficial to have a qualitative explanation of such behavior . R : We have enlarged Figure 2 . Yes , Given the first successful adversarial example , we observe that ZO-signSGD yields slightly higher L2 distortion than ZO-SGD . This is not surprising since compared to ZO-SGD , the convergence rate of ZO-signSGD involves an additional error correction term ( relying on b and q in ( 9 ) ) . Accordingly , ZO-signSGD might converge to moderate accuracy ( e.g. , a solution neighborhood ) rather than a very high accuracy . However , the convergence of ZO-signSGD to moderate accuracy could be much faster than ZO-SGD since the former meets a stricter convergence criterion ( L2 norm of gradient ) than that of ZO-SGD ( squared L2 norm of gradient ) . We refer the reviewer to the paragraph after Eq . ( 9 ) for more discussions . In the example of generating black-box adversarial attacks , compared to convergence accuracy ( in terms of attack distortion ) , the effectiveness of a black-box attack is measured by the number of function queries needed to achieve the first successful adversarial attack . Thus , ZO-signSGD is desired in this application due to its fast convergence to moderate accuracy . To further confirm this point , in Sec.6 we have added an experiment to compare ZO-signSGD with a benchmark black-box attack generation method in ( Ilyas et al. , 2018a ) . Indeed , ZO-signSGD offers fast convergence to the first successful adversarial attack under limited queries . A. Ilyas , et al. , Black-box adversarial attacks with limited queries and information . ICLR 2018 ."}}