{"year": "2017", "forum": "H1Gq5Q9el", "title": "Unsupervised Pretraining for Sequence to Sequence Learning", "decision": "Reject", "meta_review": "This paper effectively demonstrates that the use of pretraining can improve the performance of seq2seq models for MT and summarization tasks. However, despite these empirical gains, the reviewers were not convinced enough by the novelty of the work itself and did not feel like there were technical contributions to make this a fit for ICLR.\n \n Pros:\n - All reviewers agree that the empirical gains in this paper are convincing and lead to BLEU improvements on a large scale translation and translation like tasks. Reviewer 4 also praises the detailed analysis that demonstrates that these gains come from the pretraining process itself. \n - From an impact perspective, the reviewers found the approach clear and implementable. \n \n Cons:\n - Novelty criticisms are that the method is a \"compilation\" of past approaches (although at a larger scale) and therefore primarily experimental, and that the objectives given are \"highly empirical\" and not yet motivated by theory. The authors did respond, but the reviewer did not change their score.\n - There are suggestions that this type of work would perhaps be more widely impactful in an NLP venue, where a BLEU improvement of this regard is a strong supporting piece on its own.", "reviews": [{"review_id": "H1Gq5Q9el-0", "review_text": "Authors propose the use of layer-wise language model-like pretraining for encoder-decoder models. This allows to leverage separate source and target corpora (in unsupervised manner) without necessity of large amounts of parallel training corpora. The idea is in principle fairly simple, and rely on initial optimising both encoder and decoder with LSTM tasked to perform language modelling. The ideas are not new, and the paper is more like a successful compilation of several approaches that have been around for some time. The experimental validation, though, offers some interesting insights into importance of initialization, and the effectiveness of different initialisations approaches in enc-dec setting. The regulariser you propose to use on page 3, looks like typical multi-task objective function, especially it is used in an alternating manner would be interesting to see whether similar performance might have been obtained starting with this objective, from random initialisation. You should probably give credit for encoder-decoder like-RNN models published in 1990s. Minors: Pg. 2, Sec 2.1 2nd paragraph: can be different sizes -> can be of different sizes", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . * Even though our paper does look like a compilation of previous techniques , it is important to see that it really works . After years of research , it is extremely rare to see unsupervised learning help supervised learning , and thus our paper is an important achievement . * In recent literature , pretraining has been dismissed as being effective only on small datasets . We show that pretraining seq2seq models gives significant improvements even on challenging medium sized datasets . * Furthermore , as noted in the review , we \u2019 re one of the only works that try to deeply understand pretraining , which we tackle with our comprehensive ablation study . * We in fact have an experiment with a randomly initialized model trained with the monolingual objective in our paper . Pretraining increases performance by 2.0 BLEU points over this baseline . * We have added additional references to papers published in the 80s and 90s , and have also corrected the typo in the latest revision of our paper ."}, {"review_id": "H1Gq5Q9el-1", "review_text": "In this paper, the authors propose to pretrain the encoder/decoder of seq2seq models on a large amount of unlabeled data using a LM objective. They obtain improvements using this technique on machine translation and abstractive summarization. While the effectiveness of pretraining seq2seq models has been known among researchers and explored in a few papers (e.g. Zoph et al. 2016, Dai and Le 2015), I believe this is the first paper to pretrain using a LM for both the encoder/decoder. The technique is simple, but the gains are large (e.g. +2.7 BLEU on NMT). In addition, the authors perform extensive ablation studies to analyze where the performance is coming from. Hence, I think this paper should be accepted. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review . We would like to highlight some important differences between our work and the related works that were mentioned : * Dai and Le , 2015 focus on text classification , not harder seq2seq tasks . * It has been well documented in the machine learning literature that pretraining improves performance on tasks with very little data . Zoph et al.2016 , and a few other seq2seq works , demonstrate similar findings on resource poor language pairs . * However , the conventional wisdom is that pretraining is unnecessary , or even harmful , for tasks with medium or large datasets . We believe we are the first to show the contrary by achieving strong results on competitive tasks with medium sized datasets . We were surprised to see the score of 6 given the positive review , because a paper with an averaged score of 6 may not eventually get accepted ( taking into account last year \u2019 s threshold and this year \u2019 s increased number of submissions ) ."}, {"review_id": "H1Gq5Q9el-2", "review_text": "strengths: A method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data. After such pretraining, all weights are jointly fine-tuned with parallel labeled data with an additional language modeling loss. It is shown that pretraining accelerates training and improves generalization of seq2seq models. The main value of the proposed method is to leverage separate source and target corpora, contrasting the common methods of using large amounts of parallel training corpora. weaknesses: The objective function shown in the middle of pg 3 is highly empirical, not directly linked to how non-parallel data helps to improve the final prediction results. The paper should compare with and discuss the objective function based on expectation of cross entropy which is directly linked to improving prediction results as proposed in arXiv:1606.04646, Chen et al.: Unsupervised Learning of Predictors from Unpaired Input-Output Samples, 2016. The pre-training procedure proposed in this paper is also closely connected with the DNN pretraining method presented in Dahl et al. 2011, 2012. Comparisons should be made in the paper, highlighting why the proposed one is conceptually superior if the authors believe so. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review . The method of Chen et al. , 2016 and our method are indeed similar in spirit , and the differences are in the details : * They do not apply their method to seq2seq.They experiment on toy datasets as a proof-of-concept . We demonstrate strong results on real world datasets . * If applied to seq2seq , we can interpret the first term of their objective as making the output sequence more likely , as scored by a pretrained language model . In our method , the language model is folded into the decoder . We believe that using the pretrained language model for scoring is less efficient than using all the pretrained weights . * They have an artificial objective to ensure output sequences depend on input sequences . We rely on labeled examples . We believe that the connection between our algorithm and Chen et al \u2019 s will strengthen our work and make our algorithm more theoretically justified . The method of Dahl et al. , 2012 and our method are also similar in that we both use unsupervised pretraining to help supervised learning , but they are different in many other ways : * They focus on feedfoward networks with DBNs , which are not a natural model for sequences . We focus on seq2seq learning , using easily trained language models . * Pretraining acoustic models is now considered unnecessary , probably because the reconstruction objective used by DBNs and autoencoders is too easy . We show that pretraining by next step prediction improves seq2seq models significantly on realistic and challenging datasets . * In addition to the differences in the algorithms , we perform comprehensive ablation studies to verify that pretraining helps generalization and optimization . They make this argument without substantial supporting evidence from their experiments . We have rewritten the related works section in the latest revision of our paper to include discussions about these two interesting papers ."}], "0": {"review_id": "H1Gq5Q9el-0", "review_text": "Authors propose the use of layer-wise language model-like pretraining for encoder-decoder models. This allows to leverage separate source and target corpora (in unsupervised manner) without necessity of large amounts of parallel training corpora. The idea is in principle fairly simple, and rely on initial optimising both encoder and decoder with LSTM tasked to perform language modelling. The ideas are not new, and the paper is more like a successful compilation of several approaches that have been around for some time. The experimental validation, though, offers some interesting insights into importance of initialization, and the effectiveness of different initialisations approaches in enc-dec setting. The regulariser you propose to use on page 3, looks like typical multi-task objective function, especially it is used in an alternating manner would be interesting to see whether similar performance might have been obtained starting with this objective, from random initialisation. You should probably give credit for encoder-decoder like-RNN models published in 1990s. Minors: Pg. 2, Sec 2.1 2nd paragraph: can be different sizes -> can be of different sizes", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . * Even though our paper does look like a compilation of previous techniques , it is important to see that it really works . After years of research , it is extremely rare to see unsupervised learning help supervised learning , and thus our paper is an important achievement . * In recent literature , pretraining has been dismissed as being effective only on small datasets . We show that pretraining seq2seq models gives significant improvements even on challenging medium sized datasets . * Furthermore , as noted in the review , we \u2019 re one of the only works that try to deeply understand pretraining , which we tackle with our comprehensive ablation study . * We in fact have an experiment with a randomly initialized model trained with the monolingual objective in our paper . Pretraining increases performance by 2.0 BLEU points over this baseline . * We have added additional references to papers published in the 80s and 90s , and have also corrected the typo in the latest revision of our paper ."}, "1": {"review_id": "H1Gq5Q9el-1", "review_text": "In this paper, the authors propose to pretrain the encoder/decoder of seq2seq models on a large amount of unlabeled data using a LM objective. They obtain improvements using this technique on machine translation and abstractive summarization. While the effectiveness of pretraining seq2seq models has been known among researchers and explored in a few papers (e.g. Zoph et al. 2016, Dai and Le 2015), I believe this is the first paper to pretrain using a LM for both the encoder/decoder. The technique is simple, but the gains are large (e.g. +2.7 BLEU on NMT). In addition, the authors perform extensive ablation studies to analyze where the performance is coming from. Hence, I think this paper should be accepted. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the review . We would like to highlight some important differences between our work and the related works that were mentioned : * Dai and Le , 2015 focus on text classification , not harder seq2seq tasks . * It has been well documented in the machine learning literature that pretraining improves performance on tasks with very little data . Zoph et al.2016 , and a few other seq2seq works , demonstrate similar findings on resource poor language pairs . * However , the conventional wisdom is that pretraining is unnecessary , or even harmful , for tasks with medium or large datasets . We believe we are the first to show the contrary by achieving strong results on competitive tasks with medium sized datasets . We were surprised to see the score of 6 given the positive review , because a paper with an averaged score of 6 may not eventually get accepted ( taking into account last year \u2019 s threshold and this year \u2019 s increased number of submissions ) ."}, "2": {"review_id": "H1Gq5Q9el-2", "review_text": "strengths: A method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data. After such pretraining, all weights are jointly fine-tuned with parallel labeled data with an additional language modeling loss. It is shown that pretraining accelerates training and improves generalization of seq2seq models. The main value of the proposed method is to leverage separate source and target corpora, contrasting the common methods of using large amounts of parallel training corpora. weaknesses: The objective function shown in the middle of pg 3 is highly empirical, not directly linked to how non-parallel data helps to improve the final prediction results. The paper should compare with and discuss the objective function based on expectation of cross entropy which is directly linked to improving prediction results as proposed in arXiv:1606.04646, Chen et al.: Unsupervised Learning of Predictors from Unpaired Input-Output Samples, 2016. The pre-training procedure proposed in this paper is also closely connected with the DNN pretraining method presented in Dahl et al. 2011, 2012. Comparisons should be made in the paper, highlighting why the proposed one is conceptually superior if the authors believe so. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review . The method of Chen et al. , 2016 and our method are indeed similar in spirit , and the differences are in the details : * They do not apply their method to seq2seq.They experiment on toy datasets as a proof-of-concept . We demonstrate strong results on real world datasets . * If applied to seq2seq , we can interpret the first term of their objective as making the output sequence more likely , as scored by a pretrained language model . In our method , the language model is folded into the decoder . We believe that using the pretrained language model for scoring is less efficient than using all the pretrained weights . * They have an artificial objective to ensure output sequences depend on input sequences . We rely on labeled examples . We believe that the connection between our algorithm and Chen et al \u2019 s will strengthen our work and make our algorithm more theoretically justified . The method of Dahl et al. , 2012 and our method are also similar in that we both use unsupervised pretraining to help supervised learning , but they are different in many other ways : * They focus on feedfoward networks with DBNs , which are not a natural model for sequences . We focus on seq2seq learning , using easily trained language models . * Pretraining acoustic models is now considered unnecessary , probably because the reconstruction objective used by DBNs and autoencoders is too easy . We show that pretraining by next step prediction improves seq2seq models significantly on realistic and challenging datasets . * In addition to the differences in the algorithms , we perform comprehensive ablation studies to verify that pretraining helps generalization and optimization . They make this argument without substantial supporting evidence from their experiments . We have rewritten the related works section in the latest revision of our paper to include discussions about these two interesting papers ."}}