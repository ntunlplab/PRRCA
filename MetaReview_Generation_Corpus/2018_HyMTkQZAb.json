{"year": "2018", "forum": "HyMTkQZAb", "title": "Kronecker-factored Curvature Approximations for Recurrent Neural Networks", "decision": "Accept (Poster)", "meta_review": "This clearly written paper extends the Kronecker-factored approximate curvature optimizer to recurrent networks.  Experiments on Penn Treebank language modeling and training of differentiable neural computers on a repeated copy task show that the proposed K-FAC optimizers are stronger than SGD, Adam, and Adam with layer normalization. The most negative reviewer objected to a lack of theoretical error bounds on the approximations made, but the authors successfully argue that obtaining such bounds would require making assumptions that are likely to be violated in practice, and that strong empirical performance on real tasks is sufficient justification for the approximations.\n\nPros:\n+ \"Completes\" K-FAC training by extending it to recurrent models.\n+ Experiments show effects of different K-FAC approximations.\n\nCons:\n- The algorithm is rather complex to implement.\n", "reviews": [{"review_id": "HyMTkQZAb-0", "review_text": " Summary of the paper ------------------------------- The authors extend the K-FAC method to RNNs. Due to the nature of BPTT, the approximation that the activations 'a' are independent from the gradients 'Ds' doesn't hold anymore and thus other approximations have to be made. They present 3 ways of approximating F, and show optimization results on 3 datasets, outperforming ADAM in both number of updates and computation time. Clarity, Significance and Correctness -------------------------------------------------- Clarity: Above average. The mathematical notations is overly verbose, so it makes the paper harder to understand. Note that it is not the author's fault only, since they followed the notation of the other K-FAC papers. For instance, the notations goes from 'E[xy^T]' to 'cov(x, y)' to 'V_{x,y}'. I don't think introducing the 'cov' notation helps with the understanding of the paper (unless they explicitly wanted to stress out that the covariance of the gradients of the outputs of the model are centered). Also the 'V' in equation (4) could be confused with the 'V' in the first equation. Moreover, for the gradients with respect to the activations, we go from 'dL/ds' to 'Ds' to 'g', and for the weights we go from 'dL/dW' to 'DW' to 'w'. Why not keeping the 'Ds' and 'Dw' notation throughout the paper, and defining Dx as vec(dL/dx)? Significance: This paper aims at helping with the optimization of RNNs and is thus and important contribution for our community. Correctness: The paper is technically correct. Questions -------------- 1. In figure 1, how does it compare to Adam instead of SGD? I think it would be a more fair comparison since SGD is rarely used to train RNNs (as RMSprop and ADAM might help with the vanishing/exploding gradients problem). Also, does the SGD baseline has momentum (since your method does)? 2. In all experiments, how do the validation / testing curves look like? 3. How does it compare to different reparametrization techniques, such as Layer Normalization or Batch Normalization? Pros ------ 1. This paper completes the K-FAC family. 2. It addresses the optimization of RNNs, which is an important research direction in our field. 3. It shows different levels of approximations of the Fisher, with the corresponding performances. Cons ------- 1. No validation / test curves for any experiments, which makes it hard to asses if one should use this method in practice or not. 2. The notation is a bit verbose and can become confusing. 3. Small experimental setting (only PTB and DNC). Typos -------- 1. Sec 1, par 5: \"a new family curvature\" -> \"a new family of curvature\"", "rating": "7: Good paper, accept", "reply_text": "Thank you for your detailed review . See below for our response to your various questions and concerns . Notation -- -- -- -- -- - The \u201c V \u201d in the first equation is just a free/arbitrary variable to define the D [ ... ] notation . We will use a different symbol to avoid confusion , and clarify that it \u2019 s a free variable in the first equation . Because the zero-centered nature of the second-order statistics are not crucial we will replace the use of covariances with uncentered 2nd-order moments , as you suggest . We never actually use the notation dL/dZ anywhere except to define the DZ notation . Because of the way that gradient quantities appear in complex expressions in our paper ( often multiple times in the same expression ) , this shortened notation seems necessary to avoid producing very long and ugly expressions that are hard to parse . Unfortunately , it is not feasible to define DZ = vec ( dL/DZ ) , since we need to use the non-vectorized version at different points . Questions -- -- -- -- -- -- 1 . SGD has become the goto optimizer for these PTB tasks due to its superior generalization properties ( Merity et al , 2017 ; Wilson et al.2017 ) , which is why we used it in our experiments . But since our paper is not concerned with generalization ( see our answer to your second question below ) there is a good argument for using Adam as a second baseline , so we will include this in an upcoming revision of the manuscript . Also , we would observe that diagonal methods like RMSProp / Adam likely won \u2019 t do anything to address the vanishing or exploding gradients problem in RNNs ( as suggested in your comment ) . This is because the parameters are shared across time , and contributions from all the time-steps are added together before any preconditioning is applied . The same argument also applies to a non-diagonal method like K-FAC . However , if the gradient contributions from different time-steps happen to land inside of distinct subspaces of the parameter space , then a non-diagonal method like K-FAC may still help with vanishing/exploding gradients by individually rescaling each of these contributions . ( See Section 3.3.1 of J Martens \u2019 thesis http : //www.cs.toronto.edu/~jmartens/docs/thesis_phd_martens.pdf ) . Merity , Stephen , Nitish Shirish Keskar , and Richard Socher . `` Regularizing and optimizing LSTM language models . '' arXiv preprint arXiv:1708.02182 ( 2017 ) . Wilson , Ashia C. , et al . `` The Marginal Value of Adaptive Gradient Methods in Machine Learning . '' arXiv preprint arXiv:1705.08292 ( 2017 ) . 2.In our experiments K-FAC did overfit more than SGD . The final perplexity values on the PTB tasks were about 5-8 points higher . Please see Appendix D in the latest revision for the generalization performance . The reasons why we didn \u2019 t present test performance in the existing version of the paper , and why we stand by this decision , are discussed below . The tendency for SGD w/ early-stopping to self-regularize is well-documented , and there are many compelling theories about why this happens ( e.g.Duvenaud et al. , 2016 ; Hardt et al , 2015 ) . It is also well-known that 2nd-order methods , including K-FAC and diagonal methods like Adam/RMSprop , don \u2019 t self-regularize nearly as much ( e.g.Wilson et al , 2017 ; Keskar et al , 2017 ) . But just because a method like K-FAC doesn \u2019 t self-regularize as much as SGD , this doesn \u2019 t mean that it isn \u2019 t of practical utility . ( Otherwise diagonal 2nd-order methods like Adam and RMSprop wouldn \u2019 t be as widely used as they are . ) Implicit self-regularization of the form that SGD has can always be replaced by * explicit * regularization ( i.e.modification of the loss ) and/or model modifications . Moreover , in the online or large-data setting , where each example is processed only once , there is no question of generalization gap because the population loss is directly ( stochastically ) optimized . This online setting is encountered frequently in language modeling tasks , and our K-FAC method is particularly relevant for such tasks . ( continued in next reply )"}, {"review_id": "HyMTkQZAb-1", "review_text": "This paper extends the Kronecker-factor Approximate Curvature (K-FAC) optimization method to the setting of recurrent neural networks. The K-FAC method is an approximate 2nd-order optimization method that builds a block diagonal approximation of the Fisher information matrix, where the block diagonal elements are Kronecker products of smaller matrices. In order to approximate the Fisher information matrix for RNNs, the authors assume that the derivative of the loss function with respect to each weight matrix at each time step is independent of the length of the sequence, that these derivatives are temporally homogeneous, that the input and derivatives of the output are independent across every point in time, and that either the one-step cross-covariance of these derivatives is symmetric or that the training sequences are effectively infinite in length. Based on these assumptions, the authors show that the Fisher information can be reduced into a form in which the derivatives of the weight matrices can be approximated by a linear Gaussian graphical model and in which the approximate 2nd order method can be efficiently carried out. The authors compare their method to SGD on two language modeling tasks and against Adam for learning differentiable neural computers. The paper is relatively clear, and the authors do a reasonable job of introducing related work of the original K-FAC algorithm as well as its extension to CNNs before systematically deriving their method for RNNs. The problem of extending the K-FAC algorithm is natural, and the steps taken in this paper seem natural yet also original and non-trivial. The main issue that I have with this paper is the lack of theoretical justification or even intuition for the many approximations carried out in the course of approximating the Fisher information matrix. In many instances, it seemed like these approximations were made purely for convenience and tractability without much regard for (even approximate) correctness. This quality of this paper would be greatly strengthened if it had some bounds on approximation error or even empirical results testing the validity of the assumptions in the paper. Moreover, the experiments do not demonstrate levels of statistical significance in the results, so it is difficult to assert the practical significance of this work. Specific comments and questions Page 2, \"r is is\". Typo. Page 2, \"DV\". I found the introduction of V without any explanation to be confusing. Page 2, \"P_{y|x}(\\theta)\". The relation between P_{y|x}(\\theta) and f(x,\\theta) is never explained. Page 3, \"common practice of computing the natural gradient as (F + \\lambda I) \\nabla h instead of F^{-1} \\nabla h\". I don't see how the former can serve as a replacement for the latter. Page 3, \"approximate g and a as statistically independent\". Even though K-FAC already exists, it would be good to explain why this assumption is reasonable, since similar assumptions are made for the work presented in this paper. Page 4, \"This new approximation, called \"KFC\", is derived by assuming....\". Same as previous comment. It would be good to briefly discuss why these assumptions are reasonable. Page 5, Independence of T and w_t's, temporal homogeneity of w_t's,, and independence between a_t's and g_t's. I can see why these are convenient assumptions, but why are they reasonable? Moreover, why is it further natural to assume that A and G are temporally homogeneous as well? Page 7, \"But insofar as the w_t's ... encode the relevant information contained in these external variables, they should be approximately Markovian\". I am not sure what this means. Page 7, \"The linear-Gaussian assumption meanwhile is a more severe one to make, but it seems necessary for there to be any hope that the required expectations remain tractable\". I am not sure that this is a good enough justification for such an idea, unless there are compelling approximation error bounds. Page 8, Option 1. In what situations is it reasonable to assume that V_1 is symmetric? Pages 8-9, Option 2. What is a good finite sample size in which the assumption that the training sequences are infinitely long is reasonable in practice? Can the error |\\kappa(x) - \\zeta_T(x)| be translated into a statement on the approximation error? Page 9, \"V_1 = V_{1,0} = ...\". Typos (that appear to have been caught by the authors already). Page 9, \"The 2nd-order statistics ... are accumulated through an exponential moving average during training\". How sensitive is the performance of this method to the decay rate of the exponential moving average? Page 10, \"The additional computations required to get the approximate Fisher inverse from these statistics ... are performed asynchronously on the CPU's\". I find it a bit unfair to compare SGD to K-FAC in terms of wall clock time without also using the extra CPU's for SGD as well (e.g. via Hogwild or synchronous parallel SGD). Page 10, \"The hyperparameters of our approach...\". What is the sensitivity of the experimental results to these hyperparameters? Moreover, how sensitive are the results to initialization? Page 10, \"we found that each parameter update of our method required about 80% more wall-clock time than an SGD update\". How much of this is attributed to the fact that the statistics are computed asynchronously? Pages 10-12, Experiments. There are no error bars in any of the plots, so it is impossible to ascertain the statistical significance of any of these results. Page 11: Figure 2. Where is the Adam batchsize 50 line in the left plot? Why did the Adam batchsize 200 line disappear halfway through the right plot? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed comments . We will address each of your major points in the sections below , followed by your remaining questions/comments . Empirical / theoretical analysis of approximation quality ========================= A detailed discussion , empirical study , and analysis of the main approximation assumption ( independence of the a \u2019 s and g \u2019 s ) used in K-FAC and its derivatives ( including this work ) is contained in the original K-FAC paper . However , these are not approximations bounds in the sense you likely mean . Due to the mathematically intractable nature of neural networks it is almost certainly impossible to provide such theoretical bounds . Moreover , for each of these approximating assumptions , it is quite likely that there exists some artificially constructed model and dataset pair where they would be strongly violated . And they are almost surely violated for real models and datasets as well , just to a lesser degree . An empirical study of each of these approximations would be interesting , but there are very many of them and so this would be a large undertaking . We felt that this was outside of the scope of a conference paper , especially given that our manuscript was already on the long side . Instead , we decided to evaluate the quality of our approximations in the only sense in which matters in practice : how well they translate into optimization performance on real tasks . RE prioritization of tractability over approximation quality ======================== Approximations are often made for the sake of tractability , with little justification beyond how well their associated algorithms perform in practice . For example , the diagonal approximations that drive most practical second-order optimization schemes ( like Adam/RMSprop ) are made purely for practical reasons , with very little theoretical or empirical justification . And as one can see from the figures in the original K-FAC paper ( see Figure 2 of Martens & Grosse , 2015 ) , the true curvature matrix is highly non-diagonal in neural networks , so the diagonal approximation is indeed quite severe/inaccurate . While the approximations proposed in our paper are greater in sheer number , their sum total is still far less severe than a diagonal approximation . ( Diagonal approximations take the components of the gradient to be independent , thereby completely giving up on trying to model its inter-component statistical structure . ) In designing our approximations we looked for the mildest possible ones that preserved the tractability properties we need . Ultimately , tractability has to be the overriding consideration when designing algorithms that can be used in practice . In one of your points you question our use of a linear-Gaussian model to describe the dependencies between the w_t \u2019 s . However , the only obvious alternative to this , which would preserve tractability ( in the context of the other approximations being made ) , is to neglect the dependencies between the w_t \u2019 s , thus treating them as statistically independent . Should we give up on modeling these dependencies simply because the only tractable model we are aware of has no theoretical guarantees in terms of its accuracy ? Perhaps better tractable approximations exist and could be the subject of future research . Indeed , we can \u2019 t prove that they don \u2019 t exist , and would be excited to learn about them . However , we feel that the onus should not be on us to provide such a proof . Our contribution is a constructive existence proof of a non-obvious approximation to the Fisher of an RNN , which is a ) much less severe that existing approaches ( e.g.diagonal approximations ) , is b ) validated on real data , and is c ) useful in practice . It feels like this should be good enough for a conference paper . Another very important point to keep in mind is that the approximations need not be particularly accurate for them to be useful in optimization . Consider the analogy to statistical data modeling . People frequently use primitive statistical models ( e.g.linear-Gaussian ) to describe data distributions that their models can not possibly ever capture faithfully . Nonetheless , these models have some predictive power and utility insofar as there are some aspects of the true underlying data-generated processes that can be described ( however approximately ) by such a simple model . Our situation is analogous . Our approximations , while they are clearly imperfect , allow us to capture enough of the statistical structure of the gradients that the resulting approximate Fisher still has some utility . They could be wildly inaccurate in absolute terms and still be useful for our purposes . ( continued in the next reply )"}, {"review_id": "HyMTkQZAb-2", "review_text": "In this paper, the authors present a second-order method that is specifically designed for RNNs. The paper overall is well-written and I enjoyed reading the paper. The main idea of the paper is to extend the existing kronecker-factored algorithms to RNNs. In order to obtain a tractable formulation, the authors impose certain assumptions and provide detailed derivations. Even though the gain in the convergence speed is not very impressive and the algorithm is quite complicated and possibly not very accessible by deep learning practitioners, I still believe this is a novel and valuable contribution and will be of interest to the community. I only have some minor corrections: 1) Sec 2.1: typo \"is is\" 2) Sec 2.2: typo \"esstentiallybe\" 3) Sec 2.2: (F+lambda I) --> should be inverse 4) The authors should include a proper conclusion", "rating": "7: Good paper, accept", "reply_text": "Thanks for your comments . We have corrected the errors you pointed out and added back in the conclusion , which was originally cut for space considerations . These will appear in our next revision ( to be posted soon ) . With regards to convergence speed , we feel that the gains over SGD/Adam are significant . While wall-clock time wasn \u2019 t improved substantially in the DNC experiment ( Figure 3 ) , it was on the first two experiments on Penn-TreeBank ( Figures 1 and 2 ) . From those latter two figures one can clearly see that SGD/Adam slow down at a significantly higher loss than our method ( almost to the point of plateauing ) . While we agree that the method is challenging to implement , we have a TensorFlow implementation ready for public release . We will make this available as soon as we can while respecting the anonymity of the reviewing process ."}], "0": {"review_id": "HyMTkQZAb-0", "review_text": " Summary of the paper ------------------------------- The authors extend the K-FAC method to RNNs. Due to the nature of BPTT, the approximation that the activations 'a' are independent from the gradients 'Ds' doesn't hold anymore and thus other approximations have to be made. They present 3 ways of approximating F, and show optimization results on 3 datasets, outperforming ADAM in both number of updates and computation time. Clarity, Significance and Correctness -------------------------------------------------- Clarity: Above average. The mathematical notations is overly verbose, so it makes the paper harder to understand. Note that it is not the author's fault only, since they followed the notation of the other K-FAC papers. For instance, the notations goes from 'E[xy^T]' to 'cov(x, y)' to 'V_{x,y}'. I don't think introducing the 'cov' notation helps with the understanding of the paper (unless they explicitly wanted to stress out that the covariance of the gradients of the outputs of the model are centered). Also the 'V' in equation (4) could be confused with the 'V' in the first equation. Moreover, for the gradients with respect to the activations, we go from 'dL/ds' to 'Ds' to 'g', and for the weights we go from 'dL/dW' to 'DW' to 'w'. Why not keeping the 'Ds' and 'Dw' notation throughout the paper, and defining Dx as vec(dL/dx)? Significance: This paper aims at helping with the optimization of RNNs and is thus and important contribution for our community. Correctness: The paper is technically correct. Questions -------------- 1. In figure 1, how does it compare to Adam instead of SGD? I think it would be a more fair comparison since SGD is rarely used to train RNNs (as RMSprop and ADAM might help with the vanishing/exploding gradients problem). Also, does the SGD baseline has momentum (since your method does)? 2. In all experiments, how do the validation / testing curves look like? 3. How does it compare to different reparametrization techniques, such as Layer Normalization or Batch Normalization? Pros ------ 1. This paper completes the K-FAC family. 2. It addresses the optimization of RNNs, which is an important research direction in our field. 3. It shows different levels of approximations of the Fisher, with the corresponding performances. Cons ------- 1. No validation / test curves for any experiments, which makes it hard to asses if one should use this method in practice or not. 2. The notation is a bit verbose and can become confusing. 3. Small experimental setting (only PTB and DNC). Typos -------- 1. Sec 1, par 5: \"a new family curvature\" -> \"a new family of curvature\"", "rating": "7: Good paper, accept", "reply_text": "Thank you for your detailed review . See below for our response to your various questions and concerns . Notation -- -- -- -- -- - The \u201c V \u201d in the first equation is just a free/arbitrary variable to define the D [ ... ] notation . We will use a different symbol to avoid confusion , and clarify that it \u2019 s a free variable in the first equation . Because the zero-centered nature of the second-order statistics are not crucial we will replace the use of covariances with uncentered 2nd-order moments , as you suggest . We never actually use the notation dL/dZ anywhere except to define the DZ notation . Because of the way that gradient quantities appear in complex expressions in our paper ( often multiple times in the same expression ) , this shortened notation seems necessary to avoid producing very long and ugly expressions that are hard to parse . Unfortunately , it is not feasible to define DZ = vec ( dL/DZ ) , since we need to use the non-vectorized version at different points . Questions -- -- -- -- -- -- 1 . SGD has become the goto optimizer for these PTB tasks due to its superior generalization properties ( Merity et al , 2017 ; Wilson et al.2017 ) , which is why we used it in our experiments . But since our paper is not concerned with generalization ( see our answer to your second question below ) there is a good argument for using Adam as a second baseline , so we will include this in an upcoming revision of the manuscript . Also , we would observe that diagonal methods like RMSProp / Adam likely won \u2019 t do anything to address the vanishing or exploding gradients problem in RNNs ( as suggested in your comment ) . This is because the parameters are shared across time , and contributions from all the time-steps are added together before any preconditioning is applied . The same argument also applies to a non-diagonal method like K-FAC . However , if the gradient contributions from different time-steps happen to land inside of distinct subspaces of the parameter space , then a non-diagonal method like K-FAC may still help with vanishing/exploding gradients by individually rescaling each of these contributions . ( See Section 3.3.1 of J Martens \u2019 thesis http : //www.cs.toronto.edu/~jmartens/docs/thesis_phd_martens.pdf ) . Merity , Stephen , Nitish Shirish Keskar , and Richard Socher . `` Regularizing and optimizing LSTM language models . '' arXiv preprint arXiv:1708.02182 ( 2017 ) . Wilson , Ashia C. , et al . `` The Marginal Value of Adaptive Gradient Methods in Machine Learning . '' arXiv preprint arXiv:1705.08292 ( 2017 ) . 2.In our experiments K-FAC did overfit more than SGD . The final perplexity values on the PTB tasks were about 5-8 points higher . Please see Appendix D in the latest revision for the generalization performance . The reasons why we didn \u2019 t present test performance in the existing version of the paper , and why we stand by this decision , are discussed below . The tendency for SGD w/ early-stopping to self-regularize is well-documented , and there are many compelling theories about why this happens ( e.g.Duvenaud et al. , 2016 ; Hardt et al , 2015 ) . It is also well-known that 2nd-order methods , including K-FAC and diagonal methods like Adam/RMSprop , don \u2019 t self-regularize nearly as much ( e.g.Wilson et al , 2017 ; Keskar et al , 2017 ) . But just because a method like K-FAC doesn \u2019 t self-regularize as much as SGD , this doesn \u2019 t mean that it isn \u2019 t of practical utility . ( Otherwise diagonal 2nd-order methods like Adam and RMSprop wouldn \u2019 t be as widely used as they are . ) Implicit self-regularization of the form that SGD has can always be replaced by * explicit * regularization ( i.e.modification of the loss ) and/or model modifications . Moreover , in the online or large-data setting , where each example is processed only once , there is no question of generalization gap because the population loss is directly ( stochastically ) optimized . This online setting is encountered frequently in language modeling tasks , and our K-FAC method is particularly relevant for such tasks . ( continued in next reply )"}, "1": {"review_id": "HyMTkQZAb-1", "review_text": "This paper extends the Kronecker-factor Approximate Curvature (K-FAC) optimization method to the setting of recurrent neural networks. The K-FAC method is an approximate 2nd-order optimization method that builds a block diagonal approximation of the Fisher information matrix, where the block diagonal elements are Kronecker products of smaller matrices. In order to approximate the Fisher information matrix for RNNs, the authors assume that the derivative of the loss function with respect to each weight matrix at each time step is independent of the length of the sequence, that these derivatives are temporally homogeneous, that the input and derivatives of the output are independent across every point in time, and that either the one-step cross-covariance of these derivatives is symmetric or that the training sequences are effectively infinite in length. Based on these assumptions, the authors show that the Fisher information can be reduced into a form in which the derivatives of the weight matrices can be approximated by a linear Gaussian graphical model and in which the approximate 2nd order method can be efficiently carried out. The authors compare their method to SGD on two language modeling tasks and against Adam for learning differentiable neural computers. The paper is relatively clear, and the authors do a reasonable job of introducing related work of the original K-FAC algorithm as well as its extension to CNNs before systematically deriving their method for RNNs. The problem of extending the K-FAC algorithm is natural, and the steps taken in this paper seem natural yet also original and non-trivial. The main issue that I have with this paper is the lack of theoretical justification or even intuition for the many approximations carried out in the course of approximating the Fisher information matrix. In many instances, it seemed like these approximations were made purely for convenience and tractability without much regard for (even approximate) correctness. This quality of this paper would be greatly strengthened if it had some bounds on approximation error or even empirical results testing the validity of the assumptions in the paper. Moreover, the experiments do not demonstrate levels of statistical significance in the results, so it is difficult to assert the practical significance of this work. Specific comments and questions Page 2, \"r is is\". Typo. Page 2, \"DV\". I found the introduction of V without any explanation to be confusing. Page 2, \"P_{y|x}(\\theta)\". The relation between P_{y|x}(\\theta) and f(x,\\theta) is never explained. Page 3, \"common practice of computing the natural gradient as (F + \\lambda I) \\nabla h instead of F^{-1} \\nabla h\". I don't see how the former can serve as a replacement for the latter. Page 3, \"approximate g and a as statistically independent\". Even though K-FAC already exists, it would be good to explain why this assumption is reasonable, since similar assumptions are made for the work presented in this paper. Page 4, \"This new approximation, called \"KFC\", is derived by assuming....\". Same as previous comment. It would be good to briefly discuss why these assumptions are reasonable. Page 5, Independence of T and w_t's, temporal homogeneity of w_t's,, and independence between a_t's and g_t's. I can see why these are convenient assumptions, but why are they reasonable? Moreover, why is it further natural to assume that A and G are temporally homogeneous as well? Page 7, \"But insofar as the w_t's ... encode the relevant information contained in these external variables, they should be approximately Markovian\". I am not sure what this means. Page 7, \"The linear-Gaussian assumption meanwhile is a more severe one to make, but it seems necessary for there to be any hope that the required expectations remain tractable\". I am not sure that this is a good enough justification for such an idea, unless there are compelling approximation error bounds. Page 8, Option 1. In what situations is it reasonable to assume that V_1 is symmetric? Pages 8-9, Option 2. What is a good finite sample size in which the assumption that the training sequences are infinitely long is reasonable in practice? Can the error |\\kappa(x) - \\zeta_T(x)| be translated into a statement on the approximation error? Page 9, \"V_1 = V_{1,0} = ...\". Typos (that appear to have been caught by the authors already). Page 9, \"The 2nd-order statistics ... are accumulated through an exponential moving average during training\". How sensitive is the performance of this method to the decay rate of the exponential moving average? Page 10, \"The additional computations required to get the approximate Fisher inverse from these statistics ... are performed asynchronously on the CPU's\". I find it a bit unfair to compare SGD to K-FAC in terms of wall clock time without also using the extra CPU's for SGD as well (e.g. via Hogwild or synchronous parallel SGD). Page 10, \"The hyperparameters of our approach...\". What is the sensitivity of the experimental results to these hyperparameters? Moreover, how sensitive are the results to initialization? Page 10, \"we found that each parameter update of our method required about 80% more wall-clock time than an SGD update\". How much of this is attributed to the fact that the statistics are computed asynchronously? Pages 10-12, Experiments. There are no error bars in any of the plots, so it is impossible to ascertain the statistical significance of any of these results. Page 11: Figure 2. Where is the Adam batchsize 50 line in the left plot? Why did the Adam batchsize 200 line disappear halfway through the right plot? ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your detailed comments . We will address each of your major points in the sections below , followed by your remaining questions/comments . Empirical / theoretical analysis of approximation quality ========================= A detailed discussion , empirical study , and analysis of the main approximation assumption ( independence of the a \u2019 s and g \u2019 s ) used in K-FAC and its derivatives ( including this work ) is contained in the original K-FAC paper . However , these are not approximations bounds in the sense you likely mean . Due to the mathematically intractable nature of neural networks it is almost certainly impossible to provide such theoretical bounds . Moreover , for each of these approximating assumptions , it is quite likely that there exists some artificially constructed model and dataset pair where they would be strongly violated . And they are almost surely violated for real models and datasets as well , just to a lesser degree . An empirical study of each of these approximations would be interesting , but there are very many of them and so this would be a large undertaking . We felt that this was outside of the scope of a conference paper , especially given that our manuscript was already on the long side . Instead , we decided to evaluate the quality of our approximations in the only sense in which matters in practice : how well they translate into optimization performance on real tasks . RE prioritization of tractability over approximation quality ======================== Approximations are often made for the sake of tractability , with little justification beyond how well their associated algorithms perform in practice . For example , the diagonal approximations that drive most practical second-order optimization schemes ( like Adam/RMSprop ) are made purely for practical reasons , with very little theoretical or empirical justification . And as one can see from the figures in the original K-FAC paper ( see Figure 2 of Martens & Grosse , 2015 ) , the true curvature matrix is highly non-diagonal in neural networks , so the diagonal approximation is indeed quite severe/inaccurate . While the approximations proposed in our paper are greater in sheer number , their sum total is still far less severe than a diagonal approximation . ( Diagonal approximations take the components of the gradient to be independent , thereby completely giving up on trying to model its inter-component statistical structure . ) In designing our approximations we looked for the mildest possible ones that preserved the tractability properties we need . Ultimately , tractability has to be the overriding consideration when designing algorithms that can be used in practice . In one of your points you question our use of a linear-Gaussian model to describe the dependencies between the w_t \u2019 s . However , the only obvious alternative to this , which would preserve tractability ( in the context of the other approximations being made ) , is to neglect the dependencies between the w_t \u2019 s , thus treating them as statistically independent . Should we give up on modeling these dependencies simply because the only tractable model we are aware of has no theoretical guarantees in terms of its accuracy ? Perhaps better tractable approximations exist and could be the subject of future research . Indeed , we can \u2019 t prove that they don \u2019 t exist , and would be excited to learn about them . However , we feel that the onus should not be on us to provide such a proof . Our contribution is a constructive existence proof of a non-obvious approximation to the Fisher of an RNN , which is a ) much less severe that existing approaches ( e.g.diagonal approximations ) , is b ) validated on real data , and is c ) useful in practice . It feels like this should be good enough for a conference paper . Another very important point to keep in mind is that the approximations need not be particularly accurate for them to be useful in optimization . Consider the analogy to statistical data modeling . People frequently use primitive statistical models ( e.g.linear-Gaussian ) to describe data distributions that their models can not possibly ever capture faithfully . Nonetheless , these models have some predictive power and utility insofar as there are some aspects of the true underlying data-generated processes that can be described ( however approximately ) by such a simple model . Our situation is analogous . Our approximations , while they are clearly imperfect , allow us to capture enough of the statistical structure of the gradients that the resulting approximate Fisher still has some utility . They could be wildly inaccurate in absolute terms and still be useful for our purposes . ( continued in the next reply )"}, "2": {"review_id": "HyMTkQZAb-2", "review_text": "In this paper, the authors present a second-order method that is specifically designed for RNNs. The paper overall is well-written and I enjoyed reading the paper. The main idea of the paper is to extend the existing kronecker-factored algorithms to RNNs. In order to obtain a tractable formulation, the authors impose certain assumptions and provide detailed derivations. Even though the gain in the convergence speed is not very impressive and the algorithm is quite complicated and possibly not very accessible by deep learning practitioners, I still believe this is a novel and valuable contribution and will be of interest to the community. I only have some minor corrections: 1) Sec 2.1: typo \"is is\" 2) Sec 2.2: typo \"esstentiallybe\" 3) Sec 2.2: (F+lambda I) --> should be inverse 4) The authors should include a proper conclusion", "rating": "7: Good paper, accept", "reply_text": "Thanks for your comments . We have corrected the errors you pointed out and added back in the conclusion , which was originally cut for space considerations . These will appear in our next revision ( to be posted soon ) . With regards to convergence speed , we feel that the gains over SGD/Adam are significant . While wall-clock time wasn \u2019 t improved substantially in the DNC experiment ( Figure 3 ) , it was on the first two experiments on Penn-TreeBank ( Figures 1 and 2 ) . From those latter two figures one can clearly see that SGD/Adam slow down at a significantly higher loss than our method ( almost to the point of plateauing ) . While we agree that the method is challenging to implement , we have a TensorFlow implementation ready for public release . We will make this available as soon as we can while respecting the anonymity of the reviewing process ."}}