{"year": "2019", "forum": "Byf5-30qFX", "title": "DHER: Hindsight Experience Replay for Dynamic Goals", "decision": "Accept (Poster)", "meta_review": "This work proposes a method for extending hindsight experience replay to the setting where the goal is not fixed, but dynamic or moving. It proceeds by amending failed episodes by searching replay memory for a compatible trajectories from which to construct a trajectory that can be productively learned from.\n\nReviewers were generally positive on the novelty and importance of the contribution. While noting its limitations, it was still felt that the key ideas could be useful and influential. The tasks considered are modifications of OpenAI robotics environments, adapted to the dynamic goal setting, as well as a 2D planar \"snake\" game. There were concerns about the strength of the baselines employed but reviewers seemed happy with the state of these post-revision. There were also concerns regarding clarity of presentation, particularly from AnonReviewer2, but significant progress was made on this front following discussions and revision.\n\nDespite remaining concerns over clarity I am convinced that this is an interesting problem setting worth studying and that the proposed method makes significant progress. The method has limitations with respect to the sorts of environments where we can reasonably expect it to work (where other aspects of the environment are relatively stable both within and across episodes), but there is lots of work in the literature, particularly where robotics is concerned, that focuses on exactly these kinds of environments. This submission is therefore highly relevant to current practice and by reviewers' accounts, generally well-executed in its post-revision form. I therefore recommend acceptance.", "reviews": [{"review_id": "Byf5-30qFX-0", "review_text": "In this paper, the authors extend the HER framework to deal with dynamical goals, i.e. goals that change over time. In order to do so, they first need to learn a model of the dynamics of the goal, and then to select in the replay buffer experience reaching the expected value of the goal at the expected time. Empirical results are based on three (or four, see the appendix) experiments with a Mujoco UR10 simulated environment, and one experiment is successfully transfered to a real robot. Overall, the addressed problem is relevant (the question being how can you efficiently replay experience when the goal is dynamical?), the idea is original and the approach looks sound, but seems to suffer from a fundamental flaw (see below). Despite some merits, the paper mainly suffers from the fact that the implementation of the approach described above is not explained clearly at all. Among other things, after reading the paper twice, it is still unclear to me: - how the agent learns of the goal motion (what substrate for such learning, what architecture, how many repetitions of the goal trajectory, how accurate is the learned model...) - how the output of this model is taken as input to infer the desired values of the goal in the future: shall the agent address the goal at the next time step or later in time, how does it search in practice in its replay buffer, etc. These unclarities are partly due to unsufficient structuring of the \"methodology\" section of the paper, but also to unsufficient mastery of scientific english. At many points it is not easy to get what the authors mean, and the paper would definitely benefit from the help of an experienced scientific writer. Note that Figure 1 helps getting the overall idea, but another Figure showing an architecture diagram with the main model variables would help further. In Figures 3a and 5, we can see that performance decreases. The explanation of the authors just before 4.3.1 seem to imply that there is a fundamental flaw in the algorithm, as this may happen with any other experiment. This is an important weakness of the approach. To me, Section 4.5 about transfer to a real robot does not bring much, as the authors did nothing specific to favor this transfer. They just tried and it happens that it works, but I would like to see a discussion why it works, or that the authors show me with an ablation study that if they change something in their approach, it does not work any more. In Section 4.6, the fact that DHER can outperform HER+ is weird: how can a learn model do better that a model given by hand, unless that given model is wrong? This needs further investigation and discussion. In more details, a few further remarks: In related work, twice: you should not replace an accurate enumeration of papers with \"and so on\". p3: In contrary, => By contrast, which is the same to => same as compare the above with the static goals => please rephrase In Algorithm 1, line 26: this is not the algorithm A that you optimize, this is its critic network. line 15: you search for a trajectory that matches the desired goal. Do you take the first that matches? Do you take all that match, and select the \"best\" one? If yes, what is the criterion for being the best? p5: we find such two failed => two such failed that borrows from the Ej => please rephrase we assign certain rules to the goals so that they accordingly move => very unclear. What rules? Specified how? Please give a formal description. For defining the reward, you use s_{t+1} and g_{t+1}, why not s_t and g_t? p6: the same cell as the food at a certain time step. Which time step? How do you choose? The caption of Fig. 6 needs to be improved to be contratsed with Fig. 7. p8: the performance of DQN and DHER is closed => close? DHER quickly acheive(s) Because the law...environment. => This is not a sentence. Mentioning in the appendix a further experiment (dy-sliding) which is not described in the paper is of little use. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q4 : an architecture diagram A4 : We updated Figure 1 . Q5 : Figures 3a and 5 \u2026 performance decreases ... A5 : One reason may be that it is a temporal drop and will recover later . Another reason may be that the policy trained with assembled experiences becomes overfitting to simple cases as such kind of experiences are assembled a lot . The overfitting to simple cases decreases overall performance . The similar pattern also appeared in other papers . See Pusing task in Fig 2 in HER ( Andrychowicz et al. , 2017 ) . Q6 : To me , Section 4.5 about transfer to a real robot does not bring much \u2026 A6 : The experiments of transferring to a real robot mainly demonstrate dynamic goals are real-world problems and can be solved by our method . At the same time , it shows when DHER uses positions , it is robust to the real-world environment . Q7 : In Section 4.6 , the fact that DHER can outperform HER+ is weird \u2026 A7 : It is indeed a little surprising . It shows DHER is very efficient in some simple environments . In a simple environment , such as Dy-Snake , DHER has better generation than HER+ . The reason may be that HER+ uses only one way to modify a trajectory . However , DHER has different ways to create success trajectories because we can find different matching positions given a trajectory from the past experiences . The Dy-Snake environment is so simple that DHER is able to create a lot of success experience in a short time . Q8 : In more details , a few further remarks ... A8 : We polished the paper . Q9 : in the appendix a further experiment ( dy-sliding ) \u2026 of little use\u2026 A9 : We removed it . We added this before because our open source will contain this environment and our model also works on it successfully . Q10 : In Algorithm 1 , line 26 : this is not the algorithm A that you optimize , this is its critic network . A10 : Line 26 indicates a standard update for the RL algorithm A . It is similar to HER . Please see the last several lines of Algorithm 1 in HER ( Andrychowicz et al. , 2017 ) . The key process of DHER is from lines 13 to 23 . We had added a marker at the end of Line 20 . Q11 : line 15 : you search for a trajectory that matches the desired goal ... A11 : We use a hash table to store trajectories . We search trajectories in the hash table and return the first that matches . Q12 : we assign certain rules to the goals so that they accordingly move = > very unclear ... A12 : The details are given in the next paragraph . See the second paragraph in Section 4.1 . For different environments , the rules are slightly different . Q13 : For defining the reward , you use s_ { t+1 } and g_ { t+1 } , why not s_t and g_t ? A13 : They are the same meaning and just corresponding to different timesteps . At time step t , after taking an action , the state turns to s_ { t+1 } and the goal turns to g_ { t+1 } . Thus the reward is defined based on s_ { t+1 } and g_ { t+1 } . Similarly , if the time step is t - 1 ( t > 1 ) , the reward is defined based on s_ { t } and g_ { t } . Q14 : p6 : the same cell as the food at a certain time step . Which time step ? How do you choose ? A14 : It means if the snake moves to the same cell as the food at any timestep , the game is over . We only set the maximum timestep for each episode ."}, {"review_id": "Byf5-30qFX-1", "review_text": "The authors propose an extension of hindsight replay to settings where the goal is moving. This consists in taking a failed episode and constructing a valid moving goal by searching prior experiences for a compatible goal trajectory. Results are shown on simulated robotic grasping tasks and a toy task introduced by the authors. Authors show improved results compared to other baselines. The authors also show a demonstration of transfering their policies to the real world. The algorithm appears very specific and not applicable to all cases with dynamic goals. It would be good if the authors discussed when it can and cannot be applied. My understanding is it would be hard to apply this when the environment changes across episodes as there needs to be matching trajectories. It would also be hard to apply this for the same reason if there are dynamics chaging the environment (besides the goal). If the goal was following more complex dynamics like teleporting from one place it seems it would again be rather hard to adapt this. I am also wondering if for most practical cases one could construct a heuristic for making the goal trajectory a valid one (not necessarily relying on knowing exact dynamics) thus avoiding the matching step. The literature review and the baselines do not appear to consider any other methods designed for dynamic goals. The paper seems to approach the dynamic goal problem as if it was a fresh problem. It would be good to have a better overview of this field and baselines that address this problem as it has certainly been studied in robotics, computer vision, and reinforcement learning. I find this paper hard to assess without a more appropriate context for this problem besides a recently proposed technique for sparse rewards that the authors might want to adapt to it. I find it difficult to believe that nobody has studied solutions to this problem and solutions specific to that don\u2019t exist. The writing is a bit repetitive at times and I do believe the algorithm can be more tersely summarized earlier in the paper. It\u2019s difficult to get the full idea from the Algorithm block. Overall, I think the paper is borderline. There is several interesting ideas and a new dataset introduced, but I would like to be more convinced that the problems tackled are indeed as hard as the authors claim and to have a better literature review. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the comments , and we would like to clarify a few important misconceptions that the reviewer has regarding our work . 1 ) We position the paper in the context of RL with sparse rewards . We follow the goal setting of UVFA ( Schaul et al. , 2015a ) and HER ( Andrychowicz et al. , 2017 ) . The dynamic goal problem is extended from this setting , not all other cases . Please see paragraph 3 in Section 1 ( Introduction ) and paragraph 1 in Section 3.1 ( Dynamic goals ) for more descriptions . 2 ) We propose a new experience replay method . The proposed algorithm can be combined with any off-policy RL algorithms , similar to HER , as shown in Figure 1 . 3 ) The motivation of developing algorithms which can learn from unshaped reward signals is that it does not need domain-specific knowledge and is applicable in situations where we do not know what admissible behaviour may look like . The similar motivation is also mentioned in HER ( Andrychowicz et al. , 2017 ) . We also added new experimental results about dense rewards . The results show DHER works better . See Figures 3 and 6 . Q1 : The algorithm appears very specific and not applicable to all cases with dynamic goals . \u2026 A1 : Please see 1 ) and 3 ) above . Q2 : I am also wondering if for most practical cases one could construct a heuristic for making the goal trajectory a valid one ( not necessarily relying on knowing exact dynamics ) thus avoiding the matching step . A2 : It is a good idea to take domain heuristics into consideration . However , in our paper , we aim to construct a model-free method for dynamic goals to avoid the complexity of constructing goal trajectories . We agree that your idea worths a try in the future . Q3 : The literature review and the baselines do not appear to consider any other methods designed for dynamic goals . \u2026 A3 : We do not want to claim that the dynamic goal problem is a fresh problem . However , there is little work addressing dynamic goals in the sparse reward setting . As far as we know , there is no open-source RL environments for such problems . ( OpenAI Gym Robotics uses fixed goals . ) Q4 : I find it difficult to believe that nobody has studied solutions to this problem and solutions specific to that don \u2019 t exist . A4 : Our paper focuses on addressing dynamic goals with sparse rewards . This setting has not been addressed probably because it is difficult to learn . For example , the recently developed DDPG and HER failed in our tasks . Moreover , there is no open-source environments for the dynamic goals and sparse rewards , to the best of our knowledge . Q5 : There is several interesting ideas and a new dataset introduced , but I would like to be more convinced that the problems tackled are indeed as hard as the authors claim and to have a better literature review . A5 : Except for sparse rewards , we also added new experimental results about dense rewards for the dynamic goal setting . We have similar results . Similar to DDPG and DDPG+HER , DDPG ( dense ) does not work well in our tasks . For the simple Dy-Snake environment , DQN ( dense ) is better than DQN but not better than DQN+DHER . See Figures 3 and 6 ."}, {"review_id": "Byf5-30qFX-2", "review_text": "This paper proposes a way of extending Hindsight Experience Replay (HER) to dynamic or moving goals. The proposed method (DHER) constructs new successful trajectories from pairs of failed trajectories where the goal accomplished at some point in the first trajectory happens to match the desired goal in the second trajectory. The method is demonstrated to work well in several simulated environments and some qualitative sim2real transfer results to a real robot are also provided. The paper is well written and is mostly easy to follow. I liked the idea of combining parts of two trajectories and to the best of my knowledge it is new. It is a simple idea that seems to work well in practice. While DHER has some limitations I think the key ideas will lead to interesting future work. The main shortcoming of the paper is that it does not consider other relevant baselines. For example, since the position of the goal is known, why not use a shaped reward as opposed to a sparse reward? The HER paper showed that using sparse rewards with HER can work better than shaped rewards. These findings may or may not transfer to the dynamic goal case so including a shaped reward baseline would make the paper stronger. Some questions and suggestions on how to improve the paper: - It would be good to be more upfront about the limitations of the method. For example, the results on a real robot probably require accurate localization of the gripper and cup. Making this work for precise manipulation will probably require end-to-end training from vision where it\u2019s not obvious DHER would apply. - It would be interesting to see quantitative results for the simulated experiments in section 4.5. - The performance of DHER on Dy-Reaching seems to degrade in later stages of training (Figures 3a and 5). Do you know what is causing it? DQN or DHER? Overall, I think this a good paper.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your insightful comments and feedback ! Q1 : baselines \u2026 shaped rewards\u2026 A1 : We added shaped reward baselines . We use a natural distance related ( dense ) reward function to train the agent . Figures 3 and 6 in the paper show that the dense rewards do not work well for dynamic goals , though they help at the beginning of the learning . Q2 : - It would be good to be more upfront about the limitations of the method \u2026 A2 : We agree . In the revised paper , we provided more details about the limitations , including the goal assumption , the transfer requirements and so on . See Section 1 and 4.5 for more details . Q3 : It would be interesting to see quantitative results for the simulated experiments in section 4.5 . A3 : Thanks for your valuable suggestion . In Section 4.5 , with the accurate positions , we have 100 % success rate for 5 trials . Q4 : The performance of DHER on Dy-Reaching seems to degrade in later stages of training ( Figures 3a and 5 ) . Do you know what is causing it ? DQN or DHER ? A4 : One reason may be that it is a temporal drop and will recover later . Another reason may be that the policy trained with assembled experiences becomes overfitting to simple cases as such kind of experiences are assembled a lot . The overfitting to simple cases decreases overall performance . The similar pattern also appeared in other papers . See Pusing task in Fig 2 in HER ( Andrychowicz et al. , 2017 ) ."}], "0": {"review_id": "Byf5-30qFX-0", "review_text": "In this paper, the authors extend the HER framework to deal with dynamical goals, i.e. goals that change over time. In order to do so, they first need to learn a model of the dynamics of the goal, and then to select in the replay buffer experience reaching the expected value of the goal at the expected time. Empirical results are based on three (or four, see the appendix) experiments with a Mujoco UR10 simulated environment, and one experiment is successfully transfered to a real robot. Overall, the addressed problem is relevant (the question being how can you efficiently replay experience when the goal is dynamical?), the idea is original and the approach looks sound, but seems to suffer from a fundamental flaw (see below). Despite some merits, the paper mainly suffers from the fact that the implementation of the approach described above is not explained clearly at all. Among other things, after reading the paper twice, it is still unclear to me: - how the agent learns of the goal motion (what substrate for such learning, what architecture, how many repetitions of the goal trajectory, how accurate is the learned model...) - how the output of this model is taken as input to infer the desired values of the goal in the future: shall the agent address the goal at the next time step or later in time, how does it search in practice in its replay buffer, etc. These unclarities are partly due to unsufficient structuring of the \"methodology\" section of the paper, but also to unsufficient mastery of scientific english. At many points it is not easy to get what the authors mean, and the paper would definitely benefit from the help of an experienced scientific writer. Note that Figure 1 helps getting the overall idea, but another Figure showing an architecture diagram with the main model variables would help further. In Figures 3a and 5, we can see that performance decreases. The explanation of the authors just before 4.3.1 seem to imply that there is a fundamental flaw in the algorithm, as this may happen with any other experiment. This is an important weakness of the approach. To me, Section 4.5 about transfer to a real robot does not bring much, as the authors did nothing specific to favor this transfer. They just tried and it happens that it works, but I would like to see a discussion why it works, or that the authors show me with an ablation study that if they change something in their approach, it does not work any more. In Section 4.6, the fact that DHER can outperform HER+ is weird: how can a learn model do better that a model given by hand, unless that given model is wrong? This needs further investigation and discussion. In more details, a few further remarks: In related work, twice: you should not replace an accurate enumeration of papers with \"and so on\". p3: In contrary, => By contrast, which is the same to => same as compare the above with the static goals => please rephrase In Algorithm 1, line 26: this is not the algorithm A that you optimize, this is its critic network. line 15: you search for a trajectory that matches the desired goal. Do you take the first that matches? Do you take all that match, and select the \"best\" one? If yes, what is the criterion for being the best? p5: we find such two failed => two such failed that borrows from the Ej => please rephrase we assign certain rules to the goals so that they accordingly move => very unclear. What rules? Specified how? Please give a formal description. For defining the reward, you use s_{t+1} and g_{t+1}, why not s_t and g_t? p6: the same cell as the food at a certain time step. Which time step? How do you choose? The caption of Fig. 6 needs to be improved to be contratsed with Fig. 7. p8: the performance of DQN and DHER is closed => close? DHER quickly acheive(s) Because the law...environment. => This is not a sentence. Mentioning in the appendix a further experiment (dy-sliding) which is not described in the paper is of little use. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Q4 : an architecture diagram A4 : We updated Figure 1 . Q5 : Figures 3a and 5 \u2026 performance decreases ... A5 : One reason may be that it is a temporal drop and will recover later . Another reason may be that the policy trained with assembled experiences becomes overfitting to simple cases as such kind of experiences are assembled a lot . The overfitting to simple cases decreases overall performance . The similar pattern also appeared in other papers . See Pusing task in Fig 2 in HER ( Andrychowicz et al. , 2017 ) . Q6 : To me , Section 4.5 about transfer to a real robot does not bring much \u2026 A6 : The experiments of transferring to a real robot mainly demonstrate dynamic goals are real-world problems and can be solved by our method . At the same time , it shows when DHER uses positions , it is robust to the real-world environment . Q7 : In Section 4.6 , the fact that DHER can outperform HER+ is weird \u2026 A7 : It is indeed a little surprising . It shows DHER is very efficient in some simple environments . In a simple environment , such as Dy-Snake , DHER has better generation than HER+ . The reason may be that HER+ uses only one way to modify a trajectory . However , DHER has different ways to create success trajectories because we can find different matching positions given a trajectory from the past experiences . The Dy-Snake environment is so simple that DHER is able to create a lot of success experience in a short time . Q8 : In more details , a few further remarks ... A8 : We polished the paper . Q9 : in the appendix a further experiment ( dy-sliding ) \u2026 of little use\u2026 A9 : We removed it . We added this before because our open source will contain this environment and our model also works on it successfully . Q10 : In Algorithm 1 , line 26 : this is not the algorithm A that you optimize , this is its critic network . A10 : Line 26 indicates a standard update for the RL algorithm A . It is similar to HER . Please see the last several lines of Algorithm 1 in HER ( Andrychowicz et al. , 2017 ) . The key process of DHER is from lines 13 to 23 . We had added a marker at the end of Line 20 . Q11 : line 15 : you search for a trajectory that matches the desired goal ... A11 : We use a hash table to store trajectories . We search trajectories in the hash table and return the first that matches . Q12 : we assign certain rules to the goals so that they accordingly move = > very unclear ... A12 : The details are given in the next paragraph . See the second paragraph in Section 4.1 . For different environments , the rules are slightly different . Q13 : For defining the reward , you use s_ { t+1 } and g_ { t+1 } , why not s_t and g_t ? A13 : They are the same meaning and just corresponding to different timesteps . At time step t , after taking an action , the state turns to s_ { t+1 } and the goal turns to g_ { t+1 } . Thus the reward is defined based on s_ { t+1 } and g_ { t+1 } . Similarly , if the time step is t - 1 ( t > 1 ) , the reward is defined based on s_ { t } and g_ { t } . Q14 : p6 : the same cell as the food at a certain time step . Which time step ? How do you choose ? A14 : It means if the snake moves to the same cell as the food at any timestep , the game is over . We only set the maximum timestep for each episode ."}, "1": {"review_id": "Byf5-30qFX-1", "review_text": "The authors propose an extension of hindsight replay to settings where the goal is moving. This consists in taking a failed episode and constructing a valid moving goal by searching prior experiences for a compatible goal trajectory. Results are shown on simulated robotic grasping tasks and a toy task introduced by the authors. Authors show improved results compared to other baselines. The authors also show a demonstration of transfering their policies to the real world. The algorithm appears very specific and not applicable to all cases with dynamic goals. It would be good if the authors discussed when it can and cannot be applied. My understanding is it would be hard to apply this when the environment changes across episodes as there needs to be matching trajectories. It would also be hard to apply this for the same reason if there are dynamics chaging the environment (besides the goal). If the goal was following more complex dynamics like teleporting from one place it seems it would again be rather hard to adapt this. I am also wondering if for most practical cases one could construct a heuristic for making the goal trajectory a valid one (not necessarily relying on knowing exact dynamics) thus avoiding the matching step. The literature review and the baselines do not appear to consider any other methods designed for dynamic goals. The paper seems to approach the dynamic goal problem as if it was a fresh problem. It would be good to have a better overview of this field and baselines that address this problem as it has certainly been studied in robotics, computer vision, and reinforcement learning. I find this paper hard to assess without a more appropriate context for this problem besides a recently proposed technique for sparse rewards that the authors might want to adapt to it. I find it difficult to believe that nobody has studied solutions to this problem and solutions specific to that don\u2019t exist. The writing is a bit repetitive at times and I do believe the algorithm can be more tersely summarized earlier in the paper. It\u2019s difficult to get the full idea from the Algorithm block. Overall, I think the paper is borderline. There is several interesting ideas and a new dataset introduced, but I would like to be more convinced that the problems tackled are indeed as hard as the authors claim and to have a better literature review. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the comments , and we would like to clarify a few important misconceptions that the reviewer has regarding our work . 1 ) We position the paper in the context of RL with sparse rewards . We follow the goal setting of UVFA ( Schaul et al. , 2015a ) and HER ( Andrychowicz et al. , 2017 ) . The dynamic goal problem is extended from this setting , not all other cases . Please see paragraph 3 in Section 1 ( Introduction ) and paragraph 1 in Section 3.1 ( Dynamic goals ) for more descriptions . 2 ) We propose a new experience replay method . The proposed algorithm can be combined with any off-policy RL algorithms , similar to HER , as shown in Figure 1 . 3 ) The motivation of developing algorithms which can learn from unshaped reward signals is that it does not need domain-specific knowledge and is applicable in situations where we do not know what admissible behaviour may look like . The similar motivation is also mentioned in HER ( Andrychowicz et al. , 2017 ) . We also added new experimental results about dense rewards . The results show DHER works better . See Figures 3 and 6 . Q1 : The algorithm appears very specific and not applicable to all cases with dynamic goals . \u2026 A1 : Please see 1 ) and 3 ) above . Q2 : I am also wondering if for most practical cases one could construct a heuristic for making the goal trajectory a valid one ( not necessarily relying on knowing exact dynamics ) thus avoiding the matching step . A2 : It is a good idea to take domain heuristics into consideration . However , in our paper , we aim to construct a model-free method for dynamic goals to avoid the complexity of constructing goal trajectories . We agree that your idea worths a try in the future . Q3 : The literature review and the baselines do not appear to consider any other methods designed for dynamic goals . \u2026 A3 : We do not want to claim that the dynamic goal problem is a fresh problem . However , there is little work addressing dynamic goals in the sparse reward setting . As far as we know , there is no open-source RL environments for such problems . ( OpenAI Gym Robotics uses fixed goals . ) Q4 : I find it difficult to believe that nobody has studied solutions to this problem and solutions specific to that don \u2019 t exist . A4 : Our paper focuses on addressing dynamic goals with sparse rewards . This setting has not been addressed probably because it is difficult to learn . For example , the recently developed DDPG and HER failed in our tasks . Moreover , there is no open-source environments for the dynamic goals and sparse rewards , to the best of our knowledge . Q5 : There is several interesting ideas and a new dataset introduced , but I would like to be more convinced that the problems tackled are indeed as hard as the authors claim and to have a better literature review . A5 : Except for sparse rewards , we also added new experimental results about dense rewards for the dynamic goal setting . We have similar results . Similar to DDPG and DDPG+HER , DDPG ( dense ) does not work well in our tasks . For the simple Dy-Snake environment , DQN ( dense ) is better than DQN but not better than DQN+DHER . See Figures 3 and 6 ."}, "2": {"review_id": "Byf5-30qFX-2", "review_text": "This paper proposes a way of extending Hindsight Experience Replay (HER) to dynamic or moving goals. The proposed method (DHER) constructs new successful trajectories from pairs of failed trajectories where the goal accomplished at some point in the first trajectory happens to match the desired goal in the second trajectory. The method is demonstrated to work well in several simulated environments and some qualitative sim2real transfer results to a real robot are also provided. The paper is well written and is mostly easy to follow. I liked the idea of combining parts of two trajectories and to the best of my knowledge it is new. It is a simple idea that seems to work well in practice. While DHER has some limitations I think the key ideas will lead to interesting future work. The main shortcoming of the paper is that it does not consider other relevant baselines. For example, since the position of the goal is known, why not use a shaped reward as opposed to a sparse reward? The HER paper showed that using sparse rewards with HER can work better than shaped rewards. These findings may or may not transfer to the dynamic goal case so including a shaped reward baseline would make the paper stronger. Some questions and suggestions on how to improve the paper: - It would be good to be more upfront about the limitations of the method. For example, the results on a real robot probably require accurate localization of the gripper and cup. Making this work for precise manipulation will probably require end-to-end training from vision where it\u2019s not obvious DHER would apply. - It would be interesting to see quantitative results for the simulated experiments in section 4.5. - The performance of DHER on Dy-Reaching seems to degrade in later stages of training (Figures 3a and 5). Do you know what is causing it? DQN or DHER? Overall, I think this a good paper.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your insightful comments and feedback ! Q1 : baselines \u2026 shaped rewards\u2026 A1 : We added shaped reward baselines . We use a natural distance related ( dense ) reward function to train the agent . Figures 3 and 6 in the paper show that the dense rewards do not work well for dynamic goals , though they help at the beginning of the learning . Q2 : - It would be good to be more upfront about the limitations of the method \u2026 A2 : We agree . In the revised paper , we provided more details about the limitations , including the goal assumption , the transfer requirements and so on . See Section 1 and 4.5 for more details . Q3 : It would be interesting to see quantitative results for the simulated experiments in section 4.5 . A3 : Thanks for your valuable suggestion . In Section 4.5 , with the accurate positions , we have 100 % success rate for 5 trials . Q4 : The performance of DHER on Dy-Reaching seems to degrade in later stages of training ( Figures 3a and 5 ) . Do you know what is causing it ? DQN or DHER ? A4 : One reason may be that it is a temporal drop and will recover later . Another reason may be that the policy trained with assembled experiences becomes overfitting to simple cases as such kind of experiences are assembled a lot . The overfitting to simple cases decreases overall performance . The similar pattern also appeared in other papers . See Pusing task in Fig 2 in HER ( Andrychowicz et al. , 2017 ) ."}}