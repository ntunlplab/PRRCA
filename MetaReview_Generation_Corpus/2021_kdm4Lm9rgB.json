{"year": "2021", "forum": "kdm4Lm9rgB", "title": "Monotonic Robust Policy Optimization with Model Discrepancy", "decision": "Reject", "meta_review": "The paper tackles the problem of mitigating the effect of model discrepancies between the learning and deployment environments. In particular, the author focus on the worst-case possible performance. The paper has both an empirical and theoretical flavor. The algorithm they derived is backed by theoretical guarantees. There exists a gap between the theory presented and the final practical algorithm, which generated some elements of concern from the reviewers. Some of these issues (choice and sensitivity of the Lipschitz constant, in what cases can we make that assumption, choice of p_w, discrepancy between the theoretical proposal and the practical algorithm) are well addressed in the rebuttal. However, after careful examination of the reviews, the meta-reviewer is still not convinced that the paper meets the minimum requirements for acceptance, as many of the reviewers' initial concerns still remain.", "reviews": [{"review_id": "kdm4Lm9rgB-0", "review_text": "Motivated by the domain transfer problem in RL where policies are trained on simulators that may not reflect perfectly the reality , this paper propose a new policy optimization algorithm named MRPO that is expected to be robust to changes in the environment 's dynamic . The formal setting and the notations are the same as in EPOpt ( Rajewara 2017 ) : each variant of the environment is an MDP parametrized by a parameter p and the trained policy is expected to be robust to ( adversarial ) changes on p. Instead of focusing on worst cases with a CEM-like procedure on p distribution like in EPOpt , the authors propose to divert the TRPO approximation bound into a safety bound . Theorem 1 gives a TRPLO-like lower bound , Theorem 2 show that optimizing for the LHS of Theorem 1 inequality may not degrade the wort-case reward . The experiments study both the 10 % word-case returns and the avarge returns . They show that MRPO improves clearly from EPOpt ( renamed PW-DR for the occasion ) , the improvement against simple uniform domain randomization is less significant . Probably because this paper relies on notions gathered from both ( Rajewara 2017 ) and ( Schulman et al.2017 ) , I found the 8 pages of the main paper quite dense and hard to follow . The proofs in the appendix are however clearly detailed and easy to read . I checked integrally the proof of Theorem 1/3 without any difficulty . This domain randomization model formally equivalent to a single ( continuous ) MDP where the the environment 's dynamic is parametrized by the initial state distribution ( for instance by enriching the MDP states by the p parameter ) . It is therefore unclear to me that a specific algorithm is required for the specific case of parametrized MDPs . What would be the performance of a generic CVaR algorithm like `` Risk-constrained reinforcement learning with percentile risk criteria '' ( Chow et al.2017 ) on this setting ? I found the idea of diverting the TRPO approximation bound into a safety bound appealing . Applied to a single MDP it could lead to a CVaR variant of TRPO . Minor remarks : p3 detalis - < details I found the \\rho notation for cumulative reward a bit confusing especially when p is involved in the equations , maybe a \\nu instead would improve readability ? Experiments on non-free systems like Mujoco are not easily reproducible . A few experiments on free-to-use environments would improve the reproducibility of the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "First of all , we would like to thank the reviewer for providing the detailed comments . Please see below our detailed responses to these comments , and corresponding revisions in the rebuttal version of our paper . $ \\\\\\\\ $ Comment 1 : `` This domain randomization model formally equivalent to a single ( continuous ) MDP where the the environment 's dynamic is parametrized by the initial state distribution ( for instance by enriching the MDP states by the p parameter ) . It is therefore unclear to me that a specific algorithm is required for the specific case of parametrized MDPs . What would be the performance of a generic CVaR algorithm like `` Risk-constrained reinforcement learning with percentile risk criteria '' ( Chow et al.2017 ) on this setting ? I found the idea of diverting the TRPO approximation bound into a safety bound appealing . Applied to a single MDP it could lead to a CVaR variant of TRPO . '' Response : Please note that as a representative of robust RL algorithms , EPOpt in ( Rajeswaran et al. , 2017 ) was in essence a generic CVaR algorithm in the parameterized MDP case , which aims to maximize the conditional value at risk , i.e. , the expected reward over the subset of environments with the lowest expected reward . Specifically , the optimization problem that EPOpt aims to solve is as follows : \\begin { align * } \\max_ { \\theta , y } \\int_ { \\mathcal { F } ( \\theta ) } \\eta ( \\pi_ { \\theta } \\vert p ) P ( p ) dp \\quad s.t.\\quad Pr ( \\eta ( \\pi_ { \\theta } \\vert p ) \\leq y ) = \\epsilon , \\end { align * } where $ \\mathcal { F } ( \\theta ) = \\ { p\\vert \\eta ( \\pi_ { \\theta } \\vert p ) \\leq y \\ } $ is the set of environment parameters that produce the worst $ \\epsilon $ percentile of expected returns , and $ y $ is the $ \\epsilon $ -quantile of expected return $ \\eta $ . It can be seen that this optimization problem can be viewed as the CVaR optimization under the parametrized MDP . For practical implementation , EPOpt proposed to optimize the policy on the subset of trajectories from the worst $ \\epsilon $ percentile environments , which was essentially an approximation solution to CVaR problem under parametrized MDP . In this paper , the baseline PW-DR was the practical implementation of EPOpt algorithm . Through performance evaluation on five different robot control tasks , we could see that compared to PW-DR , the proposed MRPO improved both the average and worst-case performance in the training environment , and achieved a better generalization performance in the unseen environments . Reference : Aravind Rajeswaran , Sarvjeet Ghotra , Balaraman Ravindran , and Sergey Levine . EPOpt : Learning robust neural network policies using model ensembles . 2017 ."}, {"review_id": "kdm4Lm9rgB-1", "review_text": "This paper focuses on the generalization issue in reinforcemetn leanring , specifically aims to address the problems of domain randomization ( DR ) technique . Different from standard DR which treats all the sample environment as equal , this paper proposed to improve the performance over all possible environments and the worst-case environment concurrently . This paper theoretically derives a lower bound for the worst-case performance of a given policy over all environment , and in practical , the proposed method , monotonic robust policy optimization ( MRPO ) carries out a two-step optimization to imporve the lower bound such as to maximize the averaged and worst-case policy perfomance . This paper is well written and the key concept is clearly introduced . The Theorem.1 makes the connections between the averaged and the worst-case performance , such that maximizing the worst-case performance can be solved by maximizing the averaged performance problem with some trajectories from environments with both poor and good-enough performance . The emprical results also support the theorical analysis . 1.For Lemma 1 : The conclusion is based on the assumption that the the worst case $ \\rho ( \\pi|p_w ) - \\max_p \\rho ( \\pi|\\rho ) $ is bounded ( Proof A.1 ) . However , such equation does not strictly holds without bounded reward function . The author should stated the condition . 2.About the monotonic worst-case performance improvement theorem , the proof says `` ... the approximation is made under the assumption that the worst-case environment between two iterations are similar , which stems from the trust region constraint we impose on the update step between current and new policies ... '' , however , the trust region constraint can only limit the difference between policy updates , the similarity between worst-case environments can not be promised . 3.In theorem 2 , the fomula ( 50 ) and ( 51 ) in the proof , is this approximation reasonable ? Since the policy is updated , the worst-case environment may have changed a lot . Similarly , if the updated policy changes very little , can we make $ \\pi_ { new } =\\pi_ { old } $ ? 4.The experiments are slightly inadequate , the effects of tunable hyperparameter k should be further analyzed ; In unseen environment , the MRPO algorithm is only tested on one environment .", "rating": "7: Good paper, accept", "reply_text": "First of all , would like to thank the reviewer for providing the detailed comments . Please see below our detailed responses to these comments , and corresponding revisions in the rebuttal version of our paper . $ \\\\\\\\ $ Comment 1 : `` For Lemma 1 : The conclusion is based on the assumption that the worst case $ \\rho ( \\pi|p_w ) - \\max_p \\rho ( \\pi|\\rho ) $ is bounded ( Proof A.1 ) . However , such equation does not strictly holds without bounded reward function . The author should stated the condition . '' Response : 1 ) We would like to thank the reviewer for pointing out bounded reward function condition . In Theorem 1 of the rebuttal version , we have stated this bounded reward function condition . 2 ) In Appendix A.7 of the rebuttal version , we have also listed the reward functions of the five robot control tasks evaluated in this paper to support this condition , as follows . Referring to the source code of OpenAI gym , the reward function for the five robot control tasks evaluated in this paper are listed below . Hopper and Walker2d : \\begin { align * } R = x_ { t+1 } - x_t + b - 0.001\\vert a_t \\vert^2 ; \\end { align * } Halfcheetah : \\begin { align * } R = x_ { t+1 } - x_t - 0.001\\vert a_t \\vert^2 ; \\end { align * } Cartpole : \\begin { align * } R = 1 , \\quad \\text { if the pole does not fall down } ; \\end { align * } InvertedDoublePendulum : \\begin { align * } R = b - c_ { dist } - c_ { vel } . \\end { align * } In Hopper , Walker2d and Halfcheetah , $ x_ { t+1 } $ and $ x_ { t } $ denote the positions of the robot at timestep $ t+1 $ and $ t $ , respectively . For Hopper and Walker2d , $ b\\in \\ { 0,1\\ } $ , and $ b $ equals $ 0 $ when the robot falls down or $ 1 $ otherwise . The squared norm of action represents the energy cost of the system . Since the maximum distance that the robot can move in one timestep and the energy cost by taking an action at each timestep are bounded , these three tasks all have the bounded reward function . In Cartpole , the reward is always $ 1 $ . In InvertedDoublePendulum , $ b $ equals $ 0 $ when the pendulum falls down or $ 10 $ otherwise , $ c_ { dist } $ is the distance between the robot and the centre , and $ c_ { vel } $ is the weighted sum of the two pendulum 's angular velocities . Since all the three parameters $ b $ , $ c_ { dist } $ and $ c_ { vel } $ are physically bounded , the reward function , as a linear combination of them , is also bounded ."}, {"review_id": "kdm4Lm9rgB-2", "review_text": "summary : This paper introduces Monotonic Robust Policy Optimization ( MRPO ) , an RL algorithm that aims to jointly optimize policy and domain sampling distribution , with the goal of improving policy performance for both average and worst-case scenarios and addressing the model discrepancy between the training and target environments . They derive a lower bound for the worst-case performance , which comprises the average performance , policy change , and the statistical distance between the worst and average case environments . A TRPO-like monotonic performance improvement guarantee is provided for the worst-case expected return . Finally , a practical approximation to MRPO is proposed , which imposes the assumption on Lipschitz continuity with respect to the environment parameters and circumvents the estimation of total variation distance between the worst-case environment and the sampled environment . Experiments are conducted on three control tasks with diverse transition dynamics parameters , where MRPO could improve both average and worst-case performance in the training environments , and it shows better generalization to the unseen test environments than baseline algorithms . pros : - The theoretical analysis is provided , which shows the relationship between the worst-case and average performance for the first time . - The algorithm is backed by the theoretical guarantee of monotonic worst-case performance improvement . cons : - The assumption that the transition dynamics model is L-Lipschitz with respect to the environment parameter seems to be strong . - Some of the experimental results are not convincing . For example , in Figure 1f , MRPO underperforms DR , even if DR does not consider the worst-case performance during optimization at all . comments and questions : - How natural is the model 's Lipschitz assumption ? Are many real-world problems satisfying this assumption ? - In Figure 1 , what does the shaded-area stand for ? standard deviation ? standard error ? Also , it is not clear that MRPO outperforms other baselines statistically significantly . - It seems that two dense layers are used to construct the policy and value networks in the experiments . Why was the recurrent ( e.g.LSTM ) policy not used ? Since the recurrent policy can implicitly embed system identification , I think the performance of the DR baseline could have been improved with the use of the recurrent policy . It would be great to see the performance comparison when the recurrent policy is used for MRPO and baselines . - For the experiments on generalization to unseen environments , only the results for Hopper is provided , which may not be sufficient to demonstrate the behavior of each algorithm . It would be great to provide the heatmap results for other domains , i.e.Walker and HalfCheetah . - In Theorem 1 , is $ p_w $ is the worst-case parameter for $ \\pi $ ? or for $ \\tilde \\pi $ ? It would be good if notation presents the dependence on the policy of $ p_w $ , e.g. $ p_w^\\pi $ . - In Algorithm 2 , line 6 : how can $ p_w^k $ be found ? ( even before completing sampling the trajectories for each environment )", "rating": "5: Marginally below acceptance threshold", "reply_text": "First of all , we would like to thank the reviewer for providing the detailed comments . Please see below our detailed responses to these comments , and corresponding revisions in the rebuttal version of our paper . $ \\\\\\\\ $ Comment 1 : `` The assumption that the transition dynamics model is L-Lipschitz with respect to the environment parameter seems to be strong . '' `` How natural is the model 's Lipschitz assumption ? Are many real-world problems satisfying this assumption ? '' Response : 1 ) Reason to make the Lipshitz assumption : In robot control tasks , classical optimal control methods commonly utilize the differential equation to formulate the dynamic model , which then indicates that the transition dynamics model is $ L_p $ -Lipschitz and this formulated dynamic function can be used to estimate the Lipschitz constant $ L_p $ . For example , the inverted double pendulum , one of our newly added test environments , can be viewed as a two-link pendulum system ( Chang et al.,2019 ) . To simplify the analysis , we illustrate here a single inverted pendulum , which is the basic unit that forms the inverted double pendulum system . The single inverted pendulum has two state variables $ \\theta $ and $ \\dot { \\theta } $ , and one control input $ u $ , where $ \\theta $ and $ \\dot { \\theta } $ represent the angular position from the inverted position and the angular velocity , respectively , and $ u $ is the torque . The system dynamics can therefore be described as \\begin { align } \\ddot { \\theta } = \\frac { mgl \\sin { \\theta } + u -0.1\\dot { \\theta } } { m l^2 } , \\end { align } where $ m $ is the mass , $ g $ is the Gravitational acceleration , and $ l $ is the length of pendulum . In our setting , we may choose $ m $ as the variable environment parameter $ p $ . Since the above system dynamics are differentiable w.r.t. $ m $ , it can be verified that the maximum value of the first derivative of the system dynamic model can be chosen as the Lipschitz constant $ L_p $ . Reference : Chang , Ya-Chien , Nima Roohi , and Sicun Gao . `` Neural Lyapunov control . '' Advances in Neural Information Processing Systems . 2019.2 ) Relation between the Lipschitz constant and the hyperparameter $ \\kappa $ : From ( 3 ) , it can be seen that the second term of the bound provided in Theorem 1 is not only dependent on the expected distance $ \\epsilon ( p_w \\Vert p ) $ , but also on $ \\frac { 2|r|_ { \\max } \\gamma } { ( 1-\\gamma ) ^2 } $ . Therefore , in the practical implementation ( Algorithm 2 , Line 7 ) , the Lipschitz constant was integrated into the hyperparameter $ \\kappa $ which was the tunable hyperparameter during the experiment . Theoretically , in Algorithm 2 , $ \\kappa $ is a hyperparameter that controls the trade-off between the expected cumulative discounted reward $ \\eta ( \\pi_k|p_i ) $ and distance $ \\Vert p_i - p^k_w \\Vert $ to the worst-case environment . A larger $ \\kappa $ means that the policy cares more about the poorly-performing environments , while a smaller $ \\kappa $ would par more attention to the average performance . As empirical evaluation , we conduct experiment of MRPO on Hopper with different choices of hyperparameter $ \\kappa $ . The training curves of both average return and the 10\\ % worst-case return are shown in Figs . 5 ( a ) and 5 ( b ) of the rebuttal version , respectively . It can be verified that for the fixed value choice of $ \\kappa $ , the curve of $ \\kappa=5 $ outperforms the curves of $ \\kappa=20 , 40 , 60 $ in terms of the average return in Fig.5 ( a ) , while the curve of $ \\kappa=60 $ outperforms the curves of $ \\kappa=5 , 20 , 40 $ in terms of the 10\\ % worst-case return in Fig.5 ( b ) .In practical implementation , we gradually increase $ \\kappa $ to a fixed high value . It can therefore strike a tradeoff between the average return and 10\\ % worst-case return , demonstrating the best performance both in Figs . 5 ( a ) and 5 ( b ) of the rebuttal version . 3 ) Revision in the rebuttal version : We have added Appendix A.9 to analyze the Lipschtz assumption , and Appendix A.10 to study the hayperparameter $ \\kappa $ ."}, {"review_id": "kdm4Lm9rgB-3", "review_text": "In this paper , the authors proposed a more robust policy optimization method for domain randomization , by constraining the gap between the average performance of the whole range of environments and the performance of the worst-case environments . To achieve this , the author provide a lower bound for the worst-case performance , though the lower bound does not take the uncertainty of the finite samples into account . In addition , the algorithm 1 proposed by authors requires to calculate a model discrepancy between $ p_ { w } $ and other environments $ p_ { i } \\sim P $ , which is impractical to estimate by samples if the discrepancy is total variation distance . To achieve this , the authors assumes that the transition is lipschitz , with the requirement of tunning lipschitz constant . For empirical evaluation , the author compare with PPO with DR and PW-DR on three continuous benchmark mujoco task , which demonstrate that MRPO has some advantage over the other two algorithms . The followings are my detailed comments and questions : - I feel that selecting the worst-case environment is one of the key challenging of the proposed algorithm . I did not find the description how to choose the $ p_ { w } $ given a set of environments $ \\ { p_ { i } \\ } _ { i=0 } ^ { M-1 } $ . If the authors means that the expected return of the a single trajectory can be used to select the worst-case environment , then how do your algorithm can guarantee the expected return of the sampled trajectories is the exact performance of the environment ? The author did not give finite sample high confidence upper bound for empirical mc estimation , and the selection of the worst case environment would be hard to implement in practical settings ? - How do you choose or estimate the lipschitz constant ? If the lipschitz constant is not right , then the bound will not given any practical guidence here . - It would be great if the authors can explain the gap between algorithm 2 and your practical implementation of using the 10 % worst-case environments . If so , then the algorithm the authors use in the experiments can be viewed as directly select top performance trajectories to perform policy optimization , which I think the final algorithm is not consistent with your algorithm presented in the methodology part ( please correct me if I am wrong about the final algorithm ) . - The experiments do not give strong empirical support for the new algorithm . The authors only evaluate on three environments , which I think is not enough , can the authors add more mujoco benchmarks ? Also from the current results , I can not conclude that MRPO is better than PPO-DR since the evaluated domain is only three . Further , can the authors run more iterations to make sure the algorithms converge ? The curves now presented in the paper did not converge . Overall I think there is a gap between the methodology presented in the paper and the final practical algorithm , and the lower bound presented in the paper does not take the uncertainty caused by the finite samples into account , which will not give guidance to design empirical algorithms since the variance of the mc return of the policy is large . Finally the evaluation of the algorithms have not been conducted thoroughly .", "rating": "4: Ok but not good enough - rejection", "reply_text": "First of all , we would like to thank the reviewer for providing the detailed comments . Please see below our detailed responses to these comments , and corresponding revisions in the rebuttal version of our paper . $ \\\\\\\\ $ Comment 1 : `` the author provide a lower bound for the worst-case performance , ... , the lower bound presented in the paper does not take the uncertainty caused by the finite samples into account , which will not give guidance to design empirical algorithms since the variance of the mc return of the policy is large . '' `` I feel that selecting the worst-case environment is one of the key challenging of the proposed algorithm . I did not find the description how to choose the $ p_w $ given a set of environments $ { p_i } ^ { M-1 } _ { i=0 } $ . If the authors means that the expected return of the a single trajectory can be used to select the worst-case environment , then how do your algorithm can guarantee the expected return of the sampled trajectories is the exact performance of the environment ? The author did not give finite sample high confidence upper bound for empirical mc estimation , and the selection of the worst case environment would be hard to implement in practical settings ? `` Response : 1 ) Description on selection of $ p_w $ : In Theorem 1 , the worst-case environment parameter $ p_w $ needs to be selected according to the expected cumulative discounted reward $ \\eta ( \\pi\\vert p ) $ of environment $ p $ . Please note that in the rebuttal version , following Reviewer 3 's suggestion , we have changed the notation from $ \\rho ( \\pi\\vert p ) $ in the original submission to $ \\eta ( \\pi\\vert p ) $ to denote this expected cumulative discounted reward , such that possible confusion with the environment parameter $ p $ is avoided . However , $ \\eta ( \\pi\\vert p ) $ is infeasible to get in the practical implementation . Therefore , as a commonly used alternative approach as in ( Rajeswaran et al. , 2017 ) , we used in Algorithms 1 and 2 the mean of the cumulative discounted reward of $ L $ sampled trajectories $ \\sum_ { j=0 } ^ { L-1 } G ( \\tau_ { i , j } |p_i ) /L $ to approximate the expectation $ \\eta ( \\pi| p_i ) =E_ { \\tau } [ G ( \\tau| p_i ) ] $ of any environment $ p_i $ , by using Monte Carlo method . In the original submission , we followed the setting in ( Rajeswaran et al. , 2017 ) and let $ L=1 $ , i.e. , $ G ( \\tau_ { i,1 } \\vert p_i ) $ of a single trajectory $ \\tau_ { i,1 } $ was used to estimate $ \\eta ( \\pi\\vert p_i ) $ . We then determined the worst-case environment $ p_w $ based on $ G ( \\tau_ { i,1 } \\vert p_i ) $ of a given set of environments $ { p_i } ^ { M-1 } _ { i=0 } $ . In the following , we will analyze the impact of $ L $ on the estimation error . Reference : Aravind Rajeswaran , Sarvjeet Ghotra , Balaraman Ravindran , and Sergey Levine . EPOpt : Learning robust neural network policies using model ensembles . 2017.2 ) Theoretical analysis of the impact of $ L $ : Referring to Chebyshev 's inequality , for any environment $ p_i $ and any $ \\varepsilon \\geq 0 $ , with probability of at least $ 1-\\frac { \\sigma^2 } { L\\varepsilon^2 } $ , we have $ \\left\\vert \\frac { \\sum_ { j=0 } ^ { L-1 } G ( \\tau_ { i , j } \\vert p_i ) } { L } -\\frac { \\sum_ { j=0 } ^ { L-1 } E_ { \\tau_ { i , j } } [ G ( \\tau_ { i , j } \\vert p_i ) ] } { L } \\right\\vert = \\left\\vert \\frac { \\sum_ { j=0 } ^ { L-1 } G ( \\tau_ { i , j } \\vert p_i ) } { L } -\\eta ( \\pi\\vert p_i ) \\right\\vert \\leq \\varepsilon , $ where $ \\sigma=Var ( G ( \\tau\\vert p_i ) ) $ is the variance of trajectory $ \\tau $ 's return . From the above equation , we find out that the variance of the return does affect the MC estimation of $ \\eta ( \\pi\\vert p ) $ and a larger $ L $ can guarantee a higher probability for the convergence of $ \\sum_ { j=0 } ^ { L-1 } G ( \\tau_ { i , j } \\vert p_i ) /L $ to $ \\eta ( \\pi\\vert p_i ) $ . 3 ) Empirical evaluation of the impact of $ L $ : In practice , we have conducted experiment of MRPO on Hopper with different choices of $ L $ . We found out that the a larger $ L $ would not greatly affect the performance in terms of average return as shown in Fig.4 ( a ) in the rebuttal version , but would significantly increase the training time as shown in Fig.4 ( b ) in the rebuttal version . In other words , for the same number of training iterations , a larger $ L $ would consume significantly longer running time than a smaller $ L $ , while the performance is similar . Therefore , we set $ L=1 $ in our practical implementation of MRPO to strike a trade-off between the approximation accuracy and time complexity in training . 4 ) Revision in the rebuttal version : We have modified Algorithms 1 and 2 to clarify how to select the worst-case environment $ p_w $ . We have also added Appendix A.8 to analyze the Monte Carlo Estimation of $ \\eta ( \\pi\\vert p ) $ , and the impact of number of sampled trajectories $ L $ both theoretically and empirically ."}], "0": {"review_id": "kdm4Lm9rgB-0", "review_text": "Motivated by the domain transfer problem in RL where policies are trained on simulators that may not reflect perfectly the reality , this paper propose a new policy optimization algorithm named MRPO that is expected to be robust to changes in the environment 's dynamic . The formal setting and the notations are the same as in EPOpt ( Rajewara 2017 ) : each variant of the environment is an MDP parametrized by a parameter p and the trained policy is expected to be robust to ( adversarial ) changes on p. Instead of focusing on worst cases with a CEM-like procedure on p distribution like in EPOpt , the authors propose to divert the TRPO approximation bound into a safety bound . Theorem 1 gives a TRPLO-like lower bound , Theorem 2 show that optimizing for the LHS of Theorem 1 inequality may not degrade the wort-case reward . The experiments study both the 10 % word-case returns and the avarge returns . They show that MRPO improves clearly from EPOpt ( renamed PW-DR for the occasion ) , the improvement against simple uniform domain randomization is less significant . Probably because this paper relies on notions gathered from both ( Rajewara 2017 ) and ( Schulman et al.2017 ) , I found the 8 pages of the main paper quite dense and hard to follow . The proofs in the appendix are however clearly detailed and easy to read . I checked integrally the proof of Theorem 1/3 without any difficulty . This domain randomization model formally equivalent to a single ( continuous ) MDP where the the environment 's dynamic is parametrized by the initial state distribution ( for instance by enriching the MDP states by the p parameter ) . It is therefore unclear to me that a specific algorithm is required for the specific case of parametrized MDPs . What would be the performance of a generic CVaR algorithm like `` Risk-constrained reinforcement learning with percentile risk criteria '' ( Chow et al.2017 ) on this setting ? I found the idea of diverting the TRPO approximation bound into a safety bound appealing . Applied to a single MDP it could lead to a CVaR variant of TRPO . Minor remarks : p3 detalis - < details I found the \\rho notation for cumulative reward a bit confusing especially when p is involved in the equations , maybe a \\nu instead would improve readability ? Experiments on non-free systems like Mujoco are not easily reproducible . A few experiments on free-to-use environments would improve the reproducibility of the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "First of all , we would like to thank the reviewer for providing the detailed comments . Please see below our detailed responses to these comments , and corresponding revisions in the rebuttal version of our paper . $ \\\\\\\\ $ Comment 1 : `` This domain randomization model formally equivalent to a single ( continuous ) MDP where the the environment 's dynamic is parametrized by the initial state distribution ( for instance by enriching the MDP states by the p parameter ) . It is therefore unclear to me that a specific algorithm is required for the specific case of parametrized MDPs . What would be the performance of a generic CVaR algorithm like `` Risk-constrained reinforcement learning with percentile risk criteria '' ( Chow et al.2017 ) on this setting ? I found the idea of diverting the TRPO approximation bound into a safety bound appealing . Applied to a single MDP it could lead to a CVaR variant of TRPO . '' Response : Please note that as a representative of robust RL algorithms , EPOpt in ( Rajeswaran et al. , 2017 ) was in essence a generic CVaR algorithm in the parameterized MDP case , which aims to maximize the conditional value at risk , i.e. , the expected reward over the subset of environments with the lowest expected reward . Specifically , the optimization problem that EPOpt aims to solve is as follows : \\begin { align * } \\max_ { \\theta , y } \\int_ { \\mathcal { F } ( \\theta ) } \\eta ( \\pi_ { \\theta } \\vert p ) P ( p ) dp \\quad s.t.\\quad Pr ( \\eta ( \\pi_ { \\theta } \\vert p ) \\leq y ) = \\epsilon , \\end { align * } where $ \\mathcal { F } ( \\theta ) = \\ { p\\vert \\eta ( \\pi_ { \\theta } \\vert p ) \\leq y \\ } $ is the set of environment parameters that produce the worst $ \\epsilon $ percentile of expected returns , and $ y $ is the $ \\epsilon $ -quantile of expected return $ \\eta $ . It can be seen that this optimization problem can be viewed as the CVaR optimization under the parametrized MDP . For practical implementation , EPOpt proposed to optimize the policy on the subset of trajectories from the worst $ \\epsilon $ percentile environments , which was essentially an approximation solution to CVaR problem under parametrized MDP . In this paper , the baseline PW-DR was the practical implementation of EPOpt algorithm . Through performance evaluation on five different robot control tasks , we could see that compared to PW-DR , the proposed MRPO improved both the average and worst-case performance in the training environment , and achieved a better generalization performance in the unseen environments . Reference : Aravind Rajeswaran , Sarvjeet Ghotra , Balaraman Ravindran , and Sergey Levine . EPOpt : Learning robust neural network policies using model ensembles . 2017 ."}, "1": {"review_id": "kdm4Lm9rgB-1", "review_text": "This paper focuses on the generalization issue in reinforcemetn leanring , specifically aims to address the problems of domain randomization ( DR ) technique . Different from standard DR which treats all the sample environment as equal , this paper proposed to improve the performance over all possible environments and the worst-case environment concurrently . This paper theoretically derives a lower bound for the worst-case performance of a given policy over all environment , and in practical , the proposed method , monotonic robust policy optimization ( MRPO ) carries out a two-step optimization to imporve the lower bound such as to maximize the averaged and worst-case policy perfomance . This paper is well written and the key concept is clearly introduced . The Theorem.1 makes the connections between the averaged and the worst-case performance , such that maximizing the worst-case performance can be solved by maximizing the averaged performance problem with some trajectories from environments with both poor and good-enough performance . The emprical results also support the theorical analysis . 1.For Lemma 1 : The conclusion is based on the assumption that the the worst case $ \\rho ( \\pi|p_w ) - \\max_p \\rho ( \\pi|\\rho ) $ is bounded ( Proof A.1 ) . However , such equation does not strictly holds without bounded reward function . The author should stated the condition . 2.About the monotonic worst-case performance improvement theorem , the proof says `` ... the approximation is made under the assumption that the worst-case environment between two iterations are similar , which stems from the trust region constraint we impose on the update step between current and new policies ... '' , however , the trust region constraint can only limit the difference between policy updates , the similarity between worst-case environments can not be promised . 3.In theorem 2 , the fomula ( 50 ) and ( 51 ) in the proof , is this approximation reasonable ? Since the policy is updated , the worst-case environment may have changed a lot . Similarly , if the updated policy changes very little , can we make $ \\pi_ { new } =\\pi_ { old } $ ? 4.The experiments are slightly inadequate , the effects of tunable hyperparameter k should be further analyzed ; In unseen environment , the MRPO algorithm is only tested on one environment .", "rating": "7: Good paper, accept", "reply_text": "First of all , would like to thank the reviewer for providing the detailed comments . Please see below our detailed responses to these comments , and corresponding revisions in the rebuttal version of our paper . $ \\\\\\\\ $ Comment 1 : `` For Lemma 1 : The conclusion is based on the assumption that the worst case $ \\rho ( \\pi|p_w ) - \\max_p \\rho ( \\pi|\\rho ) $ is bounded ( Proof A.1 ) . However , such equation does not strictly holds without bounded reward function . The author should stated the condition . '' Response : 1 ) We would like to thank the reviewer for pointing out bounded reward function condition . In Theorem 1 of the rebuttal version , we have stated this bounded reward function condition . 2 ) In Appendix A.7 of the rebuttal version , we have also listed the reward functions of the five robot control tasks evaluated in this paper to support this condition , as follows . Referring to the source code of OpenAI gym , the reward function for the five robot control tasks evaluated in this paper are listed below . Hopper and Walker2d : \\begin { align * } R = x_ { t+1 } - x_t + b - 0.001\\vert a_t \\vert^2 ; \\end { align * } Halfcheetah : \\begin { align * } R = x_ { t+1 } - x_t - 0.001\\vert a_t \\vert^2 ; \\end { align * } Cartpole : \\begin { align * } R = 1 , \\quad \\text { if the pole does not fall down } ; \\end { align * } InvertedDoublePendulum : \\begin { align * } R = b - c_ { dist } - c_ { vel } . \\end { align * } In Hopper , Walker2d and Halfcheetah , $ x_ { t+1 } $ and $ x_ { t } $ denote the positions of the robot at timestep $ t+1 $ and $ t $ , respectively . For Hopper and Walker2d , $ b\\in \\ { 0,1\\ } $ , and $ b $ equals $ 0 $ when the robot falls down or $ 1 $ otherwise . The squared norm of action represents the energy cost of the system . Since the maximum distance that the robot can move in one timestep and the energy cost by taking an action at each timestep are bounded , these three tasks all have the bounded reward function . In Cartpole , the reward is always $ 1 $ . In InvertedDoublePendulum , $ b $ equals $ 0 $ when the pendulum falls down or $ 10 $ otherwise , $ c_ { dist } $ is the distance between the robot and the centre , and $ c_ { vel } $ is the weighted sum of the two pendulum 's angular velocities . Since all the three parameters $ b $ , $ c_ { dist } $ and $ c_ { vel } $ are physically bounded , the reward function , as a linear combination of them , is also bounded ."}, "2": {"review_id": "kdm4Lm9rgB-2", "review_text": "summary : This paper introduces Monotonic Robust Policy Optimization ( MRPO ) , an RL algorithm that aims to jointly optimize policy and domain sampling distribution , with the goal of improving policy performance for both average and worst-case scenarios and addressing the model discrepancy between the training and target environments . They derive a lower bound for the worst-case performance , which comprises the average performance , policy change , and the statistical distance between the worst and average case environments . A TRPO-like monotonic performance improvement guarantee is provided for the worst-case expected return . Finally , a practical approximation to MRPO is proposed , which imposes the assumption on Lipschitz continuity with respect to the environment parameters and circumvents the estimation of total variation distance between the worst-case environment and the sampled environment . Experiments are conducted on three control tasks with diverse transition dynamics parameters , where MRPO could improve both average and worst-case performance in the training environments , and it shows better generalization to the unseen test environments than baseline algorithms . pros : - The theoretical analysis is provided , which shows the relationship between the worst-case and average performance for the first time . - The algorithm is backed by the theoretical guarantee of monotonic worst-case performance improvement . cons : - The assumption that the transition dynamics model is L-Lipschitz with respect to the environment parameter seems to be strong . - Some of the experimental results are not convincing . For example , in Figure 1f , MRPO underperforms DR , even if DR does not consider the worst-case performance during optimization at all . comments and questions : - How natural is the model 's Lipschitz assumption ? Are many real-world problems satisfying this assumption ? - In Figure 1 , what does the shaded-area stand for ? standard deviation ? standard error ? Also , it is not clear that MRPO outperforms other baselines statistically significantly . - It seems that two dense layers are used to construct the policy and value networks in the experiments . Why was the recurrent ( e.g.LSTM ) policy not used ? Since the recurrent policy can implicitly embed system identification , I think the performance of the DR baseline could have been improved with the use of the recurrent policy . It would be great to see the performance comparison when the recurrent policy is used for MRPO and baselines . - For the experiments on generalization to unseen environments , only the results for Hopper is provided , which may not be sufficient to demonstrate the behavior of each algorithm . It would be great to provide the heatmap results for other domains , i.e.Walker and HalfCheetah . - In Theorem 1 , is $ p_w $ is the worst-case parameter for $ \\pi $ ? or for $ \\tilde \\pi $ ? It would be good if notation presents the dependence on the policy of $ p_w $ , e.g. $ p_w^\\pi $ . - In Algorithm 2 , line 6 : how can $ p_w^k $ be found ? ( even before completing sampling the trajectories for each environment )", "rating": "5: Marginally below acceptance threshold", "reply_text": "First of all , we would like to thank the reviewer for providing the detailed comments . Please see below our detailed responses to these comments , and corresponding revisions in the rebuttal version of our paper . $ \\\\\\\\ $ Comment 1 : `` The assumption that the transition dynamics model is L-Lipschitz with respect to the environment parameter seems to be strong . '' `` How natural is the model 's Lipschitz assumption ? Are many real-world problems satisfying this assumption ? '' Response : 1 ) Reason to make the Lipshitz assumption : In robot control tasks , classical optimal control methods commonly utilize the differential equation to formulate the dynamic model , which then indicates that the transition dynamics model is $ L_p $ -Lipschitz and this formulated dynamic function can be used to estimate the Lipschitz constant $ L_p $ . For example , the inverted double pendulum , one of our newly added test environments , can be viewed as a two-link pendulum system ( Chang et al.,2019 ) . To simplify the analysis , we illustrate here a single inverted pendulum , which is the basic unit that forms the inverted double pendulum system . The single inverted pendulum has two state variables $ \\theta $ and $ \\dot { \\theta } $ , and one control input $ u $ , where $ \\theta $ and $ \\dot { \\theta } $ represent the angular position from the inverted position and the angular velocity , respectively , and $ u $ is the torque . The system dynamics can therefore be described as \\begin { align } \\ddot { \\theta } = \\frac { mgl \\sin { \\theta } + u -0.1\\dot { \\theta } } { m l^2 } , \\end { align } where $ m $ is the mass , $ g $ is the Gravitational acceleration , and $ l $ is the length of pendulum . In our setting , we may choose $ m $ as the variable environment parameter $ p $ . Since the above system dynamics are differentiable w.r.t. $ m $ , it can be verified that the maximum value of the first derivative of the system dynamic model can be chosen as the Lipschitz constant $ L_p $ . Reference : Chang , Ya-Chien , Nima Roohi , and Sicun Gao . `` Neural Lyapunov control . '' Advances in Neural Information Processing Systems . 2019.2 ) Relation between the Lipschitz constant and the hyperparameter $ \\kappa $ : From ( 3 ) , it can be seen that the second term of the bound provided in Theorem 1 is not only dependent on the expected distance $ \\epsilon ( p_w \\Vert p ) $ , but also on $ \\frac { 2|r|_ { \\max } \\gamma } { ( 1-\\gamma ) ^2 } $ . Therefore , in the practical implementation ( Algorithm 2 , Line 7 ) , the Lipschitz constant was integrated into the hyperparameter $ \\kappa $ which was the tunable hyperparameter during the experiment . Theoretically , in Algorithm 2 , $ \\kappa $ is a hyperparameter that controls the trade-off between the expected cumulative discounted reward $ \\eta ( \\pi_k|p_i ) $ and distance $ \\Vert p_i - p^k_w \\Vert $ to the worst-case environment . A larger $ \\kappa $ means that the policy cares more about the poorly-performing environments , while a smaller $ \\kappa $ would par more attention to the average performance . As empirical evaluation , we conduct experiment of MRPO on Hopper with different choices of hyperparameter $ \\kappa $ . The training curves of both average return and the 10\\ % worst-case return are shown in Figs . 5 ( a ) and 5 ( b ) of the rebuttal version , respectively . It can be verified that for the fixed value choice of $ \\kappa $ , the curve of $ \\kappa=5 $ outperforms the curves of $ \\kappa=20 , 40 , 60 $ in terms of the average return in Fig.5 ( a ) , while the curve of $ \\kappa=60 $ outperforms the curves of $ \\kappa=5 , 20 , 40 $ in terms of the 10\\ % worst-case return in Fig.5 ( b ) .In practical implementation , we gradually increase $ \\kappa $ to a fixed high value . It can therefore strike a tradeoff between the average return and 10\\ % worst-case return , demonstrating the best performance both in Figs . 5 ( a ) and 5 ( b ) of the rebuttal version . 3 ) Revision in the rebuttal version : We have added Appendix A.9 to analyze the Lipschtz assumption , and Appendix A.10 to study the hayperparameter $ \\kappa $ ."}, "3": {"review_id": "kdm4Lm9rgB-3", "review_text": "In this paper , the authors proposed a more robust policy optimization method for domain randomization , by constraining the gap between the average performance of the whole range of environments and the performance of the worst-case environments . To achieve this , the author provide a lower bound for the worst-case performance , though the lower bound does not take the uncertainty of the finite samples into account . In addition , the algorithm 1 proposed by authors requires to calculate a model discrepancy between $ p_ { w } $ and other environments $ p_ { i } \\sim P $ , which is impractical to estimate by samples if the discrepancy is total variation distance . To achieve this , the authors assumes that the transition is lipschitz , with the requirement of tunning lipschitz constant . For empirical evaluation , the author compare with PPO with DR and PW-DR on three continuous benchmark mujoco task , which demonstrate that MRPO has some advantage over the other two algorithms . The followings are my detailed comments and questions : - I feel that selecting the worst-case environment is one of the key challenging of the proposed algorithm . I did not find the description how to choose the $ p_ { w } $ given a set of environments $ \\ { p_ { i } \\ } _ { i=0 } ^ { M-1 } $ . If the authors means that the expected return of the a single trajectory can be used to select the worst-case environment , then how do your algorithm can guarantee the expected return of the sampled trajectories is the exact performance of the environment ? The author did not give finite sample high confidence upper bound for empirical mc estimation , and the selection of the worst case environment would be hard to implement in practical settings ? - How do you choose or estimate the lipschitz constant ? If the lipschitz constant is not right , then the bound will not given any practical guidence here . - It would be great if the authors can explain the gap between algorithm 2 and your practical implementation of using the 10 % worst-case environments . If so , then the algorithm the authors use in the experiments can be viewed as directly select top performance trajectories to perform policy optimization , which I think the final algorithm is not consistent with your algorithm presented in the methodology part ( please correct me if I am wrong about the final algorithm ) . - The experiments do not give strong empirical support for the new algorithm . The authors only evaluate on three environments , which I think is not enough , can the authors add more mujoco benchmarks ? Also from the current results , I can not conclude that MRPO is better than PPO-DR since the evaluated domain is only three . Further , can the authors run more iterations to make sure the algorithms converge ? The curves now presented in the paper did not converge . Overall I think there is a gap between the methodology presented in the paper and the final practical algorithm , and the lower bound presented in the paper does not take the uncertainty caused by the finite samples into account , which will not give guidance to design empirical algorithms since the variance of the mc return of the policy is large . Finally the evaluation of the algorithms have not been conducted thoroughly .", "rating": "4: Ok but not good enough - rejection", "reply_text": "First of all , we would like to thank the reviewer for providing the detailed comments . Please see below our detailed responses to these comments , and corresponding revisions in the rebuttal version of our paper . $ \\\\\\\\ $ Comment 1 : `` the author provide a lower bound for the worst-case performance , ... , the lower bound presented in the paper does not take the uncertainty caused by the finite samples into account , which will not give guidance to design empirical algorithms since the variance of the mc return of the policy is large . '' `` I feel that selecting the worst-case environment is one of the key challenging of the proposed algorithm . I did not find the description how to choose the $ p_w $ given a set of environments $ { p_i } ^ { M-1 } _ { i=0 } $ . If the authors means that the expected return of the a single trajectory can be used to select the worst-case environment , then how do your algorithm can guarantee the expected return of the sampled trajectories is the exact performance of the environment ? The author did not give finite sample high confidence upper bound for empirical mc estimation , and the selection of the worst case environment would be hard to implement in practical settings ? `` Response : 1 ) Description on selection of $ p_w $ : In Theorem 1 , the worst-case environment parameter $ p_w $ needs to be selected according to the expected cumulative discounted reward $ \\eta ( \\pi\\vert p ) $ of environment $ p $ . Please note that in the rebuttal version , following Reviewer 3 's suggestion , we have changed the notation from $ \\rho ( \\pi\\vert p ) $ in the original submission to $ \\eta ( \\pi\\vert p ) $ to denote this expected cumulative discounted reward , such that possible confusion with the environment parameter $ p $ is avoided . However , $ \\eta ( \\pi\\vert p ) $ is infeasible to get in the practical implementation . Therefore , as a commonly used alternative approach as in ( Rajeswaran et al. , 2017 ) , we used in Algorithms 1 and 2 the mean of the cumulative discounted reward of $ L $ sampled trajectories $ \\sum_ { j=0 } ^ { L-1 } G ( \\tau_ { i , j } |p_i ) /L $ to approximate the expectation $ \\eta ( \\pi| p_i ) =E_ { \\tau } [ G ( \\tau| p_i ) ] $ of any environment $ p_i $ , by using Monte Carlo method . In the original submission , we followed the setting in ( Rajeswaran et al. , 2017 ) and let $ L=1 $ , i.e. , $ G ( \\tau_ { i,1 } \\vert p_i ) $ of a single trajectory $ \\tau_ { i,1 } $ was used to estimate $ \\eta ( \\pi\\vert p_i ) $ . We then determined the worst-case environment $ p_w $ based on $ G ( \\tau_ { i,1 } \\vert p_i ) $ of a given set of environments $ { p_i } ^ { M-1 } _ { i=0 } $ . In the following , we will analyze the impact of $ L $ on the estimation error . Reference : Aravind Rajeswaran , Sarvjeet Ghotra , Balaraman Ravindran , and Sergey Levine . EPOpt : Learning robust neural network policies using model ensembles . 2017.2 ) Theoretical analysis of the impact of $ L $ : Referring to Chebyshev 's inequality , for any environment $ p_i $ and any $ \\varepsilon \\geq 0 $ , with probability of at least $ 1-\\frac { \\sigma^2 } { L\\varepsilon^2 } $ , we have $ \\left\\vert \\frac { \\sum_ { j=0 } ^ { L-1 } G ( \\tau_ { i , j } \\vert p_i ) } { L } -\\frac { \\sum_ { j=0 } ^ { L-1 } E_ { \\tau_ { i , j } } [ G ( \\tau_ { i , j } \\vert p_i ) ] } { L } \\right\\vert = \\left\\vert \\frac { \\sum_ { j=0 } ^ { L-1 } G ( \\tau_ { i , j } \\vert p_i ) } { L } -\\eta ( \\pi\\vert p_i ) \\right\\vert \\leq \\varepsilon , $ where $ \\sigma=Var ( G ( \\tau\\vert p_i ) ) $ is the variance of trajectory $ \\tau $ 's return . From the above equation , we find out that the variance of the return does affect the MC estimation of $ \\eta ( \\pi\\vert p ) $ and a larger $ L $ can guarantee a higher probability for the convergence of $ \\sum_ { j=0 } ^ { L-1 } G ( \\tau_ { i , j } \\vert p_i ) /L $ to $ \\eta ( \\pi\\vert p_i ) $ . 3 ) Empirical evaluation of the impact of $ L $ : In practice , we have conducted experiment of MRPO on Hopper with different choices of $ L $ . We found out that the a larger $ L $ would not greatly affect the performance in terms of average return as shown in Fig.4 ( a ) in the rebuttal version , but would significantly increase the training time as shown in Fig.4 ( b ) in the rebuttal version . In other words , for the same number of training iterations , a larger $ L $ would consume significantly longer running time than a smaller $ L $ , while the performance is similar . Therefore , we set $ L=1 $ in our practical implementation of MRPO to strike a trade-off between the approximation accuracy and time complexity in training . 4 ) Revision in the rebuttal version : We have modified Algorithms 1 and 2 to clarify how to select the worst-case environment $ p_w $ . We have also added Appendix A.8 to analyze the Monte Carlo Estimation of $ \\eta ( \\pi\\vert p ) $ , and the impact of number of sampled trajectories $ L $ both theoretically and empirically ."}}