{"year": "2020", "forum": "HygDF6NFPB", "title": "A Fair Comparison of Graph Neural Networks for Graph Classification", "decision": "Accept (Poster)", "meta_review": "The paper provides a careful, reproducible empirical comparison of 5 graph neural network models on 9 datasets for graph classification. The paper shows that baseline methods that use only node features (either counting node types, or summing node features) can be competitive. The authors also provide some guidelines for ways to improve reproducibility in empirical comparisons of graph classification.\n\nThe authors responded well to the issued raised during review, and updated the paper during the discussion period. The reviewers improved their score, and while there were reservations about the comprehensiveness of the set of experiments, they all agreed that the paper provides a solid empirical contribution to the literature.\n\nAs machine learning becomes increasingly popular, papers that perform a careful empirical survey of baselines provide an important sanity check that future work can be built upon. Therefore, this paper, while not covering all possible graph neural network questions, provides an excellent starting point for future work to extend.\n\n", "reviews": [{"review_id": "HygDF6NFPB-0", "review_text": "This paper provides an empirical comparison between several existing graph classification algorithms, aimed at providing a fair comparison among them, as well as proposing a simple baseline that does not take into account graph structural information. Overall, I found the experimental section very thorough and sound, with proper ways of performing parameter tuning and reporting test accuracies. On the other hand, I found this paper to miss some deeper insights into why some models perform better than others, and what are the challenges provided by these graph datasets. Also, there are other interesting dimensions of comparison that are not considered (see details below), as well as insights that could benefit future work in the area. As a disclaimer, I would like to mention that I am more familiar with graph node classification methods, as opposed to whole-graph classification (which is the focus of this paper), so I cannot assess very well the authors\u2019 choice of which models to compare, and which datasets they were tested on. Positive aspects of the paper: a) Extensive experiments, which try to find the best parameter configuration for each of these models. b) I appreciate the addition of the structure-agnostic baselines, although I was missing major details about the particular choice of baselines (see below). c) The writing is very clear and easy to follow. Points where I found the paper lacking: 1. As mentioned above, I believe there are insufficient details about the baselines. For instance what is the global sum pooling over? If it is over neighbors, then this is not exactly \u201cstructure-agnostic\u201d. If it is over features, then why was this architecture chosen as opposed to just a multilayer perceptron, without the Deep Sets component? I see the value in an order-invariant component when aggregating features over a node and its neighbors, but why is this necessary at node level? 2. What is the range of values considered for each parameter in the hyperparameter validation phase? For instance, I found it a bit surprising in table A.3. that all models need 500 patience. 3. I found the discussion over the results to be quite limited. For instance: a) The authors point out that the performance on the NCI1 dataset is different than all the others (it is the only dataset where the baseline is not the best). How is this dataset different? b) How do these different models compare depending on certain dataset features? Perhaps a model is better than the rest if a dataset has some particular properties. c) Are the differences in performance shown in Tables 3-4 indeed significant, given the standard deviations, or these models practically perform the same? d) What makes these datasets challenging, such that the best performing models get up to 70-80%? This could provide interesting insights for future work. If the authors clarify some of the issues above, especially about the results discussion part, I believe this paper may indeed be of value to the graph classification community. ", "rating": "8: Accept", "reply_text": "We thank the reviewer , whose suggestions helped us improve the clarity and quality of the presentation . We are glad they found the experimental section very thorough and sound , which was our major priority . In the following comment , we provide an answer for each clarification asked by the reviewer , and we revised our paper accordingly ."}, {"review_id": "HygDF6NFPB-1", "review_text": " ********** Post-Rebuttal Update ********** I appreciate that authors provided comments on all the raised concerns and updated the paper accordingly. I think the revised paper is of a higher quality. Although the paper's experimental setup could be done in a more proper way -- nested CV; lack of which degrades the conclusiveness of the comparisons, I still think the paper is better to be accepted than not, given that 1) as pointed out by the authors the computational cost of the current experiments was quite high already, 2) the issues of replicability and reproducibility are important and GNNs are quite popular for various applications and 3) the results are interesting and important to be considered for future GNN works. So, I am happy to change my rating to weak accept. However, for the record, I would like to mention the following point: The authors removed the word nested from the paper which is good since I still believe it does not correctly reflect what\u2019s been done in the work. However, in the reply, they imply that several works including Varma & Simon, 2006 [1] would consider their method a nested CV. They attribute this different viewpoint to the reviewer\u2019s misunderstanding of CV for *k-fold* CV. That is not true. CV requires \u201ccross\u201d testing which is absent from the inner loop of this work. I refer the interested readers to actually the mentioned paper by the authors: Varma & Simon, 2006 [1], sectio:background. Page 2 clearly defines cross validation requiring a for-loop for several evaluation across different training/test sets and page 3 defines nested CV as requiring an inner loop and outer loop of this kind. So, as far as I understand, based on [1], the proposed method is not nested CV since the inner (model selection) part uses the classic single train/test split without any loop. ********** Summary ********** The paper conducts an empirical study of 5 recently-proposed graph neural networks (GNN). Three questions are studied: 1) For some selected set of classification tasks (chemistry and social networks) and datasets (9 different datasets), how do the considered GNN models perform, relative to each other, given the same hyperparameter search strategy and a \u201cnested\u201d cross validation scenario. 2) How much does the structure (graph edges) bring on top of a multi-layer perceptron (MLP) operating on merely the node features? 3) Do the considered GNN models exploit structural information of the input graphs beyond that of the node degrees? ********** Strengths and Weaknesses ********** + it is shown that the inclusion of nodes\u2019 degree in its feature representation brings a very large performance boost. This is very interesting. + it is shown that on some datasets, the results of a simple baseline -- only operating on the node features, can achieve similar results to the elaborate GNNs. This result is very informative and suggests that this baseline should always be included in GNN works. + some of the results in table 4 contradicts the corresponding papers which can be informative for the practitioners of the field. + the reproducibility and replicability problems, that is the motivation of this work, are important concerns of the field. - reproducibility and replicability problems is not specific to research done on graph neural network and is a caveat for the general machine learning research as discussed by Lipton and Steinhardt 2018., and in fact for the current scientific practice at large as pointed in the recent NSA report [Reproducibility and Replicability in Science, 2019]. So, it may be important to raise and investigate this in different subfields of machine learning, such as GNN, individually. However, the current abstract and introduction of the paper (before the last sentence of intro\u2019s second paragraph), associates this problem specifically to the research conducted in GNN. I strongly believe the better formulation is to refer to Lipton and Steinhardt 2018 as the troubling trend in ML as well as the NSA report in general and then mention that in the current work the authors focus on the subfield of GNN *classification*. - page 1: \u201cFor instance, it is often unclear how hyper-parameters have been selected or which validation splits have been used.\u201d. These being *often* the case in GNN research is not backed up by statistics and is not shown to be specific to GNN. Even if it\u2019s assumed the 5 GNN methods , under this paper\u2019s scrutiny, are representative of GNN classification, GNNs are used well beyond graph classification. - it should be clearly discussed what are the additional information that this work brings on top of Shchur et al. 2018. Is it only the shift of focus from node classification to graph classification? - the two notions of \u201cfairness in comparison\u201d and \u201cablation studies\u201d are sometimes conflated. For instance, the argument against \u201cfairness in comparison\u201d of using one-hot encoding in GIN is not a matter of fairness (since it\u2019s a novel proposal for node features of GNNs) but rather a matter of \u201cablation studies\u201d (to show the effectiveness of individual components of a new GNN method). - many concerns regarding the nested cross validation: a) it is important to note that the paper (as described in algorithm 2) does not do \u201cnested cross validation\u201d despite the claim in various parts of the work. Algorithm 2 conducts model selection based on a fixed train-val split. As such, there is no \u201ccross\u201d validation even with the minimum of 2 folds. b) the standard deviations reported in table 3 and 4 are unreasonably high. This important observation is not properly discussed. This high std is in contrast to the standard deviation reported in the baseline papers and makes most methods fall into one-std interval of each other. I strongly suspect that this is due to the non-nested cross validation that is done in this work. That is, the hyperparameters are found using a single val set consisting of 9% of all data. A proper nested cross validation would test each set of hyper parameters against k_in validation folds to make a robust inference of best hyperparameters. c) there are 3 runs used \u201cto smooth the effect of unfavourable random weight initialization on test performances\u201d, however the 10-fold outer CV should take care of this to some extent. I would argue it would have been much more helpful to use this budget to do a proper CV for model selection using 3 folds. d) The datasets are quite small (600-5000 samples). That might make the 10-fold cross validation problematic and overly optimistic in its performance report (and high variance). It might have been better to do say 5-fold cross validation but repeat the whole process twice. Of course, this is an old dilemma in the pratice regarding the bias and variance of the estimated CV error but as far as I am aware there is a general agreement that getting \u201ccloser\u201d to leave-one-out setup is not a good idea, Furthermore, in my anecdotal experience 10-fold CV for a dataset with only 600 samples can really be problematic. - would the ranking of GNN methods change with different types of the node features being used? Some GNN setups might be better at representation learning and thus should work better on raw features (as opposed to hand engineered ones). - how is the number of parameters (or capacity) across different GNN models and the baseline? Has this been taken into account for a fair comparison? For instance, could it be that higher/lower number of parameters explain the differences across GNNs and baseline due to over/under fitting? ********** Disclaimer. I did not thoroughly check the paper\u2019s report of the 5 GNN methods (summarized in table 1). ********** Final Decision ********** I believe the paper has merits as well as interesting findings. It is also true that this is an important concern in machine learning research. However, there are many issues that degrades the quality of the paper. Of all those critiques, I suggest \u201cWeak Reject\u201d mainly due to the ones raised regarding a proper nested cross validation; which is the main proposal of the paper. ********** Minor Points ********** - Page 1: \u201cOur results put on a fair and unique reference scale many published results which, as we document, were obtained under unclear experimental settings.\u201c: please rephrase; at least one preposition is missing. - Page 3: Nested Cross Validation: the brief textual explanation makes the simple idea more complicated than it is. Instead, I suggest to use citation as well as a small figure or short algorithm describing it since it\u2019s the focus of the paper. It will also serve pedagogical purposes. - Table 1: precisely define the two mark \u201cA\u201d (ambiguous) and \u201c-\u201d (lack of information). - page 7: \u201chigher or equal than\u201d \u2192 \u201chigher than or equal to\u201d - in page 6, it\u2019s mentioned that some experiments took more than 72 hours. This sounds excessive for such small datasets. My guess is that this is due to the fact that large number of epochs are allowed (e.g. 5000 in table A.3) in conjunction with extremely small learning rate (e.g. 1e-6). The question is if those extreme hyperparameter settings are actually important for this study? - the first paragraph of 6.1 argues that \u201cGNNs are still unable to exploit the structure on such datasets [D&D, PROTEINS, ENZYMES]\u201d. While this might be true, the conclusion is only based on 5 methods and limited by the design choices such as the architecture. It should be toned down. - page 5, \u201cFeatures\u201d paragraph is not very clearly written. For instance, from the sentence: \u201cMore in detail, in the former nodes ...\u201d, it\u2019s hard to understand what \u201cformer\u201d refers to. - mean and standard deviation should have the same number of decimals (table 3,4) - page 4: \u201cMoreover, the authors applied early stopping, which entails the use of a validation set\u201c. Early stopping, in a less common setup, can be used by only looking at the training set and stopping with the same n-patience strategy as used in this work. - page 4: \u201cwe conform to the available code and do not use sampled neighborhood Aggregation.\u201c: needs more explanation. ********** Points of improvements ********** The main point of improvement would be a proper nested cross validation setup. The authors can also consider the ReScience journal: https://rescience.github.io/ ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for writing this detailed review . We understand the reviewer \u2019 s concerns , and we have put our best effort to address them in the paper . Before answering each point in detail , we would like to make an important remark to place our replies into the right perspective . This work \u2019 s major contribution revolves around the re-evaluation of recently published results under a rigorous , reproducible and uniform framework , aimed at clearly separating the role of model selection from model assessment . This is something that is definitely lacking in the field of GNNs for graph classification , as we have shown in Section 4 . The priority of this study is therefore on this specific issue . About the choice and quality of the statistical estimator , we argue that each method of performance assessment by resampling is subjected to trade-offs , which involve bias/variance balancement as well as computational requirements . In this respect , our choice is to keep our methodology as close as possible to the experiments performed by other works in literature . In the following comments , we provide an answer to each question posed by the reviewer ."}, {"review_id": "HygDF6NFPB-2", "review_text": "This type of benchmarking paper is long overdue for graph classification with deep neural networks. The paper would've been strongly if it had the following: 1. Considered more structural features than simple node degree and clustering coefficient. Prior work [1] has looked at such features and answered questions like: How do structural features improve classification performance?And, which structural features are the most useful? 2. Investigated which graph neural network performs better for which graph structures (preferential attachment, small world, regular, etc) and for how much homophily. 3. Investigated the robustness of graph neural networks on classification as the structure of graphs become more random (e.g., by rewiring edges while maintaining degree distribution). [1] B. Gallagher, T. Eliassi-Rad. Leveraging Label-Independent Features for Classification in Sparsely Labeled Networks: An Empirical Study. Lecture Notes in Computer Science: Advances in Social Network Mining and Analysis, Springer, 2009.", "rating": "6: Weak Accept", "reply_text": "We sincerely thank the reviewer for the constructive criticism and for saying that our work has potential value . In this paper , we provide a partial answer to the reviewer \u2019 s question 1 , by investigating the effect of using/not using degree information on social datasets . As regards the other two questions , we think that the proposed ideas could be interesting and valuable follow-ups of this work , whose primary goal is to provide a uniform , rigorous and reproducible re-evaluation of popular benchmarks for GNN models that all researchers can use as a reference for graph classification . Once the evaluation framework is uniform for all models , one can start to reason more effectively on the structural inductive bias of different approaches and how more sophisticated structural features affect performances . For completeness , we have also incorporated [ 1 ] in Section 5 , as it is relevant to this work ."}], "0": {"review_id": "HygDF6NFPB-0", "review_text": "This paper provides an empirical comparison between several existing graph classification algorithms, aimed at providing a fair comparison among them, as well as proposing a simple baseline that does not take into account graph structural information. Overall, I found the experimental section very thorough and sound, with proper ways of performing parameter tuning and reporting test accuracies. On the other hand, I found this paper to miss some deeper insights into why some models perform better than others, and what are the challenges provided by these graph datasets. Also, there are other interesting dimensions of comparison that are not considered (see details below), as well as insights that could benefit future work in the area. As a disclaimer, I would like to mention that I am more familiar with graph node classification methods, as opposed to whole-graph classification (which is the focus of this paper), so I cannot assess very well the authors\u2019 choice of which models to compare, and which datasets they were tested on. Positive aspects of the paper: a) Extensive experiments, which try to find the best parameter configuration for each of these models. b) I appreciate the addition of the structure-agnostic baselines, although I was missing major details about the particular choice of baselines (see below). c) The writing is very clear and easy to follow. Points where I found the paper lacking: 1. As mentioned above, I believe there are insufficient details about the baselines. For instance what is the global sum pooling over? If it is over neighbors, then this is not exactly \u201cstructure-agnostic\u201d. If it is over features, then why was this architecture chosen as opposed to just a multilayer perceptron, without the Deep Sets component? I see the value in an order-invariant component when aggregating features over a node and its neighbors, but why is this necessary at node level? 2. What is the range of values considered for each parameter in the hyperparameter validation phase? For instance, I found it a bit surprising in table A.3. that all models need 500 patience. 3. I found the discussion over the results to be quite limited. For instance: a) The authors point out that the performance on the NCI1 dataset is different than all the others (it is the only dataset where the baseline is not the best). How is this dataset different? b) How do these different models compare depending on certain dataset features? Perhaps a model is better than the rest if a dataset has some particular properties. c) Are the differences in performance shown in Tables 3-4 indeed significant, given the standard deviations, or these models practically perform the same? d) What makes these datasets challenging, such that the best performing models get up to 70-80%? This could provide interesting insights for future work. If the authors clarify some of the issues above, especially about the results discussion part, I believe this paper may indeed be of value to the graph classification community. ", "rating": "8: Accept", "reply_text": "We thank the reviewer , whose suggestions helped us improve the clarity and quality of the presentation . We are glad they found the experimental section very thorough and sound , which was our major priority . In the following comment , we provide an answer for each clarification asked by the reviewer , and we revised our paper accordingly ."}, "1": {"review_id": "HygDF6NFPB-1", "review_text": " ********** Post-Rebuttal Update ********** I appreciate that authors provided comments on all the raised concerns and updated the paper accordingly. I think the revised paper is of a higher quality. Although the paper's experimental setup could be done in a more proper way -- nested CV; lack of which degrades the conclusiveness of the comparisons, I still think the paper is better to be accepted than not, given that 1) as pointed out by the authors the computational cost of the current experiments was quite high already, 2) the issues of replicability and reproducibility are important and GNNs are quite popular for various applications and 3) the results are interesting and important to be considered for future GNN works. So, I am happy to change my rating to weak accept. However, for the record, I would like to mention the following point: The authors removed the word nested from the paper which is good since I still believe it does not correctly reflect what\u2019s been done in the work. However, in the reply, they imply that several works including Varma & Simon, 2006 [1] would consider their method a nested CV. They attribute this different viewpoint to the reviewer\u2019s misunderstanding of CV for *k-fold* CV. That is not true. CV requires \u201ccross\u201d testing which is absent from the inner loop of this work. I refer the interested readers to actually the mentioned paper by the authors: Varma & Simon, 2006 [1], sectio:background. Page 2 clearly defines cross validation requiring a for-loop for several evaluation across different training/test sets and page 3 defines nested CV as requiring an inner loop and outer loop of this kind. So, as far as I understand, based on [1], the proposed method is not nested CV since the inner (model selection) part uses the classic single train/test split without any loop. ********** Summary ********** The paper conducts an empirical study of 5 recently-proposed graph neural networks (GNN). Three questions are studied: 1) For some selected set of classification tasks (chemistry and social networks) and datasets (9 different datasets), how do the considered GNN models perform, relative to each other, given the same hyperparameter search strategy and a \u201cnested\u201d cross validation scenario. 2) How much does the structure (graph edges) bring on top of a multi-layer perceptron (MLP) operating on merely the node features? 3) Do the considered GNN models exploit structural information of the input graphs beyond that of the node degrees? ********** Strengths and Weaknesses ********** + it is shown that the inclusion of nodes\u2019 degree in its feature representation brings a very large performance boost. This is very interesting. + it is shown that on some datasets, the results of a simple baseline -- only operating on the node features, can achieve similar results to the elaborate GNNs. This result is very informative and suggests that this baseline should always be included in GNN works. + some of the results in table 4 contradicts the corresponding papers which can be informative for the practitioners of the field. + the reproducibility and replicability problems, that is the motivation of this work, are important concerns of the field. - reproducibility and replicability problems is not specific to research done on graph neural network and is a caveat for the general machine learning research as discussed by Lipton and Steinhardt 2018., and in fact for the current scientific practice at large as pointed in the recent NSA report [Reproducibility and Replicability in Science, 2019]. So, it may be important to raise and investigate this in different subfields of machine learning, such as GNN, individually. However, the current abstract and introduction of the paper (before the last sentence of intro\u2019s second paragraph), associates this problem specifically to the research conducted in GNN. I strongly believe the better formulation is to refer to Lipton and Steinhardt 2018 as the troubling trend in ML as well as the NSA report in general and then mention that in the current work the authors focus on the subfield of GNN *classification*. - page 1: \u201cFor instance, it is often unclear how hyper-parameters have been selected or which validation splits have been used.\u201d. These being *often* the case in GNN research is not backed up by statistics and is not shown to be specific to GNN. Even if it\u2019s assumed the 5 GNN methods , under this paper\u2019s scrutiny, are representative of GNN classification, GNNs are used well beyond graph classification. - it should be clearly discussed what are the additional information that this work brings on top of Shchur et al. 2018. Is it only the shift of focus from node classification to graph classification? - the two notions of \u201cfairness in comparison\u201d and \u201cablation studies\u201d are sometimes conflated. For instance, the argument against \u201cfairness in comparison\u201d of using one-hot encoding in GIN is not a matter of fairness (since it\u2019s a novel proposal for node features of GNNs) but rather a matter of \u201cablation studies\u201d (to show the effectiveness of individual components of a new GNN method). - many concerns regarding the nested cross validation: a) it is important to note that the paper (as described in algorithm 2) does not do \u201cnested cross validation\u201d despite the claim in various parts of the work. Algorithm 2 conducts model selection based on a fixed train-val split. As such, there is no \u201ccross\u201d validation even with the minimum of 2 folds. b) the standard deviations reported in table 3 and 4 are unreasonably high. This important observation is not properly discussed. This high std is in contrast to the standard deviation reported in the baseline papers and makes most methods fall into one-std interval of each other. I strongly suspect that this is due to the non-nested cross validation that is done in this work. That is, the hyperparameters are found using a single val set consisting of 9% of all data. A proper nested cross validation would test each set of hyper parameters against k_in validation folds to make a robust inference of best hyperparameters. c) there are 3 runs used \u201cto smooth the effect of unfavourable random weight initialization on test performances\u201d, however the 10-fold outer CV should take care of this to some extent. I would argue it would have been much more helpful to use this budget to do a proper CV for model selection using 3 folds. d) The datasets are quite small (600-5000 samples). That might make the 10-fold cross validation problematic and overly optimistic in its performance report (and high variance). It might have been better to do say 5-fold cross validation but repeat the whole process twice. Of course, this is an old dilemma in the pratice regarding the bias and variance of the estimated CV error but as far as I am aware there is a general agreement that getting \u201ccloser\u201d to leave-one-out setup is not a good idea, Furthermore, in my anecdotal experience 10-fold CV for a dataset with only 600 samples can really be problematic. - would the ranking of GNN methods change with different types of the node features being used? Some GNN setups might be better at representation learning and thus should work better on raw features (as opposed to hand engineered ones). - how is the number of parameters (or capacity) across different GNN models and the baseline? Has this been taken into account for a fair comparison? For instance, could it be that higher/lower number of parameters explain the differences across GNNs and baseline due to over/under fitting? ********** Disclaimer. I did not thoroughly check the paper\u2019s report of the 5 GNN methods (summarized in table 1). ********** Final Decision ********** I believe the paper has merits as well as interesting findings. It is also true that this is an important concern in machine learning research. However, there are many issues that degrades the quality of the paper. Of all those critiques, I suggest \u201cWeak Reject\u201d mainly due to the ones raised regarding a proper nested cross validation; which is the main proposal of the paper. ********** Minor Points ********** - Page 1: \u201cOur results put on a fair and unique reference scale many published results which, as we document, were obtained under unclear experimental settings.\u201c: please rephrase; at least one preposition is missing. - Page 3: Nested Cross Validation: the brief textual explanation makes the simple idea more complicated than it is. Instead, I suggest to use citation as well as a small figure or short algorithm describing it since it\u2019s the focus of the paper. It will also serve pedagogical purposes. - Table 1: precisely define the two mark \u201cA\u201d (ambiguous) and \u201c-\u201d (lack of information). - page 7: \u201chigher or equal than\u201d \u2192 \u201chigher than or equal to\u201d - in page 6, it\u2019s mentioned that some experiments took more than 72 hours. This sounds excessive for such small datasets. My guess is that this is due to the fact that large number of epochs are allowed (e.g. 5000 in table A.3) in conjunction with extremely small learning rate (e.g. 1e-6). The question is if those extreme hyperparameter settings are actually important for this study? - the first paragraph of 6.1 argues that \u201cGNNs are still unable to exploit the structure on such datasets [D&D, PROTEINS, ENZYMES]\u201d. While this might be true, the conclusion is only based on 5 methods and limited by the design choices such as the architecture. It should be toned down. - page 5, \u201cFeatures\u201d paragraph is not very clearly written. For instance, from the sentence: \u201cMore in detail, in the former nodes ...\u201d, it\u2019s hard to understand what \u201cformer\u201d refers to. - mean and standard deviation should have the same number of decimals (table 3,4) - page 4: \u201cMoreover, the authors applied early stopping, which entails the use of a validation set\u201c. Early stopping, in a less common setup, can be used by only looking at the training set and stopping with the same n-patience strategy as used in this work. - page 4: \u201cwe conform to the available code and do not use sampled neighborhood Aggregation.\u201c: needs more explanation. ********** Points of improvements ********** The main point of improvement would be a proper nested cross validation setup. The authors can also consider the ReScience journal: https://rescience.github.io/ ", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for writing this detailed review . We understand the reviewer \u2019 s concerns , and we have put our best effort to address them in the paper . Before answering each point in detail , we would like to make an important remark to place our replies into the right perspective . This work \u2019 s major contribution revolves around the re-evaluation of recently published results under a rigorous , reproducible and uniform framework , aimed at clearly separating the role of model selection from model assessment . This is something that is definitely lacking in the field of GNNs for graph classification , as we have shown in Section 4 . The priority of this study is therefore on this specific issue . About the choice and quality of the statistical estimator , we argue that each method of performance assessment by resampling is subjected to trade-offs , which involve bias/variance balancement as well as computational requirements . In this respect , our choice is to keep our methodology as close as possible to the experiments performed by other works in literature . In the following comments , we provide an answer to each question posed by the reviewer ."}, "2": {"review_id": "HygDF6NFPB-2", "review_text": "This type of benchmarking paper is long overdue for graph classification with deep neural networks. The paper would've been strongly if it had the following: 1. Considered more structural features than simple node degree and clustering coefficient. Prior work [1] has looked at such features and answered questions like: How do structural features improve classification performance?And, which structural features are the most useful? 2. Investigated which graph neural network performs better for which graph structures (preferential attachment, small world, regular, etc) and for how much homophily. 3. Investigated the robustness of graph neural networks on classification as the structure of graphs become more random (e.g., by rewiring edges while maintaining degree distribution). [1] B. Gallagher, T. Eliassi-Rad. Leveraging Label-Independent Features for Classification in Sparsely Labeled Networks: An Empirical Study. Lecture Notes in Computer Science: Advances in Social Network Mining and Analysis, Springer, 2009.", "rating": "6: Weak Accept", "reply_text": "We sincerely thank the reviewer for the constructive criticism and for saying that our work has potential value . In this paper , we provide a partial answer to the reviewer \u2019 s question 1 , by investigating the effect of using/not using degree information on social datasets . As regards the other two questions , we think that the proposed ideas could be interesting and valuable follow-ups of this work , whose primary goal is to provide a uniform , rigorous and reproducible re-evaluation of popular benchmarks for GNN models that all researchers can use as a reference for graph classification . Once the evaluation framework is uniform for all models , one can start to reason more effectively on the structural inductive bias of different approaches and how more sophisticated structural features affect performances . For completeness , we have also incorporated [ 1 ] in Section 5 , as it is relevant to this work ."}}