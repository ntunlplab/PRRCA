{"year": "2018", "forum": "SJn0sLgRb", "title": "Data Augmentation by Pairing Samples for Images Classification", "decision": "Reject", "meta_review": "The paper proposes a data augmentation technique for image classification which consists in averaging two input images and using the label of one of them. The method is shown to outperform the baseline on the image classification task, the but evaluation doesn\u2019t extend beyond that (to other tasks or alternative augmentation mechanisms); theoretical justification is also lacking.", "reviews": [{"review_id": "SJn0sLgRb-0", "review_text": "The paper proposes a new data augmentation technique based on picking random image pairs and producing a new average image which is associated with the label of one of the two original samples. The experiments show that this strategy allows to reduce the risk of overfitting especially in the case of a limited amount of training samples or in experimental settings with a small number of categories. + The paper is easy to read: the method and the experiments are explained clearly. - the method is presented as a heuristic technique. 1) The training process has some specific steps with the Sample Pairing intermittently disabled. The number of epochs with enabled or disabled Sample Pairing changes depending on the dataset. How much si the method robust/sensitive to variations on these choices? 2) There is no specific analysis on the results besides showing the validation and training errors: would it be possible to see the results per class? Would the confusion matrices reveal something more about the effect of the method? Does Sample Pairing help to differentiate similar categories even if they are mixed at trainign time? 3) Would it be possible to better control the importance of each sample label rather than always choosing one of the two as ground truth? The paper misses an in-depth analysis of the proposed practical strategy. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you so much for your comments . Please refer the updates in above response on three points you mentioned in the comment . I like to specially thank the advice on confusion matrix . I have never investigated it . On average , SamplePairing gave improvements in classification of similar classes ( e.g.two animals or two vehicles ) or different classes ( e.g.animal and vehicle ) . But I am doing further investigation on the characteristics of SamplePairing using confusion matrices ."}, {"review_id": "SJn0sLgRb-1", "review_text": "The paper investigates a method of data augmentation for image classification, where two images from the training set are averaged together as input, but the label from only one image is used as a target. Since this scheme is asymmetric and uses quite unrealistic input images, a training scheme is used where the technique is only enabled in the middle of training (not very beginning or end), and in an alternating on-off fashion. This improves classification performance nicely on a variety of datasets. This is a simple technique, and the paper is concise and to the point. However, I would have liked to see a few additional comparisons. First, this augmentation technique seems to have two components: One is the mixing of inputs, but another is the effective dropping of labels from one of the two images in the pair. Which of these are more important, and can they be separated? What if some of the images' labels are changed at random, for half the images in a minibatch, for example? This would have the effect of random label changes, but without the input mixing. Likewise, what if both labels in the pair are used as targets (with 0.5 assigned to each in the softmax target)? This would mix the images, but keep targets intact. Second, the bottom of p.3 says that multiple training procedures were evaluated, but I'd be interested to see the results of some of these. In particular, is it important to alternate enabling and disabling SamplePairing, or does it also work to mix samples with and without it in each minibatch (e.g. 3/4 of the minibatch with pairing augmentation, and 1/4 without it)? I liked the experiment mixing images from within a restricted training set composed of a subset of the CIFAR images, compared to mixing these images with CIFAR training set images outside the restricted sample (p.5 and Fig 5). This suggests to me, however, that it's possible the label manipulations may play an important role. Or, is an explanation why this performs not as well that the network will train these mixing images to random targets (that of the training image in the pair), and never see this example again, whereas by using the training set alone, the mixing image is likely to be repeated with its correct label? Some more discussion on this would be nice. Overall, I think this is an interesting technique that appears to achieve nice results. It could be investigated deeper at some key points. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you so much for your comments . Please refer the updates 1 ) and 2 ) in the above response . I am currently implementing SamplePairing in a sub-minibatch granularity . So far , I do not see the significant differences by using smaller granularity of enabling/disabling SamplePairing , e.g.disabling for one mini batch after enabling for four mini batches instead of disabling two epochs after enabling eight epochs . But I am going to add the data with different granularity including the sub-minibatch granularity ."}, {"review_id": "SJn0sLgRb-2", "review_text": "The paper reports that averaging pairs of training images improves image classification generalization in many datasets. This is quite interesting. The paper is also straightforward to read and clear, which is positive. Overall i think the finding is of sufficient interest for acceptance. The paper would benefit from adding some speculation on reasons why this phenomenon occurs. There are a couple of choices that would benefit from more explanation / analysis: a) averaging, then forcing the classifier to pick one of the two classes present; why not pick both? b) the choice of hard-switching between sample pairing and regular training - it would be interesting if sample-pairing as an augmentation meshed better with other augmentations implementation-wise, so that it could be easier to integrate in other frameworks.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you so much for your comments . Please refer the updates 1 ) and 2 ) in above response on two points you mentioned in the comment ( using two labels and switching between SamplePairing and regular training ) . I am adding more experiments on the second point ( switching ) , e.g.using different granularity . I hope I can add more discussion on this point ."}], "0": {"review_id": "SJn0sLgRb-0", "review_text": "The paper proposes a new data augmentation technique based on picking random image pairs and producing a new average image which is associated with the label of one of the two original samples. The experiments show that this strategy allows to reduce the risk of overfitting especially in the case of a limited amount of training samples or in experimental settings with a small number of categories. + The paper is easy to read: the method and the experiments are explained clearly. - the method is presented as a heuristic technique. 1) The training process has some specific steps with the Sample Pairing intermittently disabled. The number of epochs with enabled or disabled Sample Pairing changes depending on the dataset. How much si the method robust/sensitive to variations on these choices? 2) There is no specific analysis on the results besides showing the validation and training errors: would it be possible to see the results per class? Would the confusion matrices reveal something more about the effect of the method? Does Sample Pairing help to differentiate similar categories even if they are mixed at trainign time? 3) Would it be possible to better control the importance of each sample label rather than always choosing one of the two as ground truth? The paper misses an in-depth analysis of the proposed practical strategy. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you so much for your comments . Please refer the updates in above response on three points you mentioned in the comment . I like to specially thank the advice on confusion matrix . I have never investigated it . On average , SamplePairing gave improvements in classification of similar classes ( e.g.two animals or two vehicles ) or different classes ( e.g.animal and vehicle ) . But I am doing further investigation on the characteristics of SamplePairing using confusion matrices ."}, "1": {"review_id": "SJn0sLgRb-1", "review_text": "The paper investigates a method of data augmentation for image classification, where two images from the training set are averaged together as input, but the label from only one image is used as a target. Since this scheme is asymmetric and uses quite unrealistic input images, a training scheme is used where the technique is only enabled in the middle of training (not very beginning or end), and in an alternating on-off fashion. This improves classification performance nicely on a variety of datasets. This is a simple technique, and the paper is concise and to the point. However, I would have liked to see a few additional comparisons. First, this augmentation technique seems to have two components: One is the mixing of inputs, but another is the effective dropping of labels from one of the two images in the pair. Which of these are more important, and can they be separated? What if some of the images' labels are changed at random, for half the images in a minibatch, for example? This would have the effect of random label changes, but without the input mixing. Likewise, what if both labels in the pair are used as targets (with 0.5 assigned to each in the softmax target)? This would mix the images, but keep targets intact. Second, the bottom of p.3 says that multiple training procedures were evaluated, but I'd be interested to see the results of some of these. In particular, is it important to alternate enabling and disabling SamplePairing, or does it also work to mix samples with and without it in each minibatch (e.g. 3/4 of the minibatch with pairing augmentation, and 1/4 without it)? I liked the experiment mixing images from within a restricted training set composed of a subset of the CIFAR images, compared to mixing these images with CIFAR training set images outside the restricted sample (p.5 and Fig 5). This suggests to me, however, that it's possible the label manipulations may play an important role. Or, is an explanation why this performs not as well that the network will train these mixing images to random targets (that of the training image in the pair), and never see this example again, whereas by using the training set alone, the mixing image is likely to be repeated with its correct label? Some more discussion on this would be nice. Overall, I think this is an interesting technique that appears to achieve nice results. It could be investigated deeper at some key points. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you so much for your comments . Please refer the updates 1 ) and 2 ) in the above response . I am currently implementing SamplePairing in a sub-minibatch granularity . So far , I do not see the significant differences by using smaller granularity of enabling/disabling SamplePairing , e.g.disabling for one mini batch after enabling for four mini batches instead of disabling two epochs after enabling eight epochs . But I am going to add the data with different granularity including the sub-minibatch granularity ."}, "2": {"review_id": "SJn0sLgRb-2", "review_text": "The paper reports that averaging pairs of training images improves image classification generalization in many datasets. This is quite interesting. The paper is also straightforward to read and clear, which is positive. Overall i think the finding is of sufficient interest for acceptance. The paper would benefit from adding some speculation on reasons why this phenomenon occurs. There are a couple of choices that would benefit from more explanation / analysis: a) averaging, then forcing the classifier to pick one of the two classes present; why not pick both? b) the choice of hard-switching between sample pairing and regular training - it would be interesting if sample-pairing as an augmentation meshed better with other augmentations implementation-wise, so that it could be easier to integrate in other frameworks.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you so much for your comments . Please refer the updates 1 ) and 2 ) in above response on two points you mentioned in the comment ( using two labels and switching between SamplePairing and regular training ) . I am adding more experiments on the second point ( switching ) , e.g.using different granularity . I hope I can add more discussion on this point ."}}