{"year": "2019", "forum": "Hklc6oAcFX", "title": "Co-manifold learning with missing data", "decision": "Reject", "meta_review": "This manuscript proposes a technique for co-manifold learning that exploits smoothness jointly over the rows and columns of the data. This is an important topic worth further study in the community.\n\nThe reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work or expressing issues about the clarity of the presentation. Further improvement of the clarity -- particularly clarification of the learning goals, combined with additional convincing experiments would significantly strengthen this submission.", "reviews": [{"review_id": "Hklc6oAcFX-0", "review_text": "Review for CO-MANIFOLD LEARNING WITH MISSING DATA Summary: This paper proposes a two-stage method to recovering the underlying structure of a data manifold using both the rows and columns of an incomplete data matrix. In the first stage they impute the missing values using their proposed co-clustering algorithm and in the second stage they propose a new metric for dimension reduction. The overall motivation for how they construct the algorithm and the intuition behind how all the pieces of the algorithm work together are not great. The paper also has significant specific clarity issues (listed below). Currently these issues seem to imply the proposed algorithm has significant logic issues (mainly on the convex/concave confusions); however depending on how they are addressed, this may end up not being an issue. The experimental results for the two simulated datasets look very good. However for the lung dataset, the results are less promising and it is less clear of the advantage of the proposed algorithm to the two competing ones. Novelty/Significance: The overall idea of the algorithm is sufficiently novel. It is very interesting to consider both rows and column correlations. Each piece of the algorithm seems to draw heavily on previous work; bi-clustering, diffusion maps, but overall the idea is novel enough. The algorithm is significant in that it addresses a relatively open problem that currently doesn\u2019t have a well established solution. Questions/Clarity: Smooth is not clearly defined and not an obvious measure for a matrix. Figure 1 shows smooth matrices at various levels, but still doesn\u2019t define explicitly what smoothness is. Does smoothness imply all entries are closer to the same value? \u201cReplacing Jr(U) and Jc(U) by quadratic row and column Laplacian penalties\u201d \u2013 The sentence is kind of strange as Laplacian penalties is not a thing. Graph Laplacian can be used as an empirical estimate for the Laplace Beltrami operator which gives a measure of smoothness in terms of divergence of the gradient of a function on a manifold; however the penalty is one on a function\u2019s complexity in the intrinsic geometry of a manifold. It is not clear how the proposed penalty is an estimator for the intrinsic geometry penalty. It seems like the equation that is listed is just the function map Omega(x) = x^2, which also is not a concave function (it is convex), so it does not fit the requirements of Assumption 2.2. Proposition 1 is kind of strangely presented. At first glance, it is not clear where the proof is, and it takes some looking to figure out it is Appendix B because it is reference before, not after the proposition. Or it might be more helpful if it is clearly stated at the beginning of Appendix B that this is the proof for Proposition 1. The authors write: \u201cMissing values can sabotage efforts to learn the low dimensional manifold underlying the data. \u2026 As the number of missing entries grows, the distances between points are increasingly distorted, resulting in poor representation of the data in the low-dimensional space.\u201d However, they use the observed values to build the knn graph used for the row/column penalties, which is counter-intuitive because this knn graph is essentially estimating a property of a manifold and the distances have the same distortion issue. Why do the author\u2019s want Omega to be concave functions as this makes the objective not convex. Additionally the penalty sqrt(|| ||_2) is approximately doing a square root twice because the l2-norm already is the square root of the sum of squares. Also what is the point of approximating the square root function instead of just using the square root function? It is overall not clear what the nature of the penalty term g2 is; Appendix A, implies it must be overall a convex function because of the upper bound. Equation 5 is not clear that it is the first order taylor approximation. Omega\u2019 is the derivative of the Omega function? Do the other terms cancel out? Also what is the derivative with respect to; each Ui. for all Uj. ? \u201cfirst-order Taylor approximation of a differentiable concave function provides a tight bound on the function\u201d \u2013 Tight bound is not an appropriate term and requires being provable. Unless the function is close to linear, a first order Taylor approximation won\u2019t be anything close to tight. The authors state the objective in 1 is not convex. Do they mean it is not strictly convex? In which case, by stationary points, they are specifically referring to local minima? Otherwise, what benefits does the MM algorithm have on an indefinite objective i.e. couldn\u2019t you end up converging to a saddle point or a local maxima instead of a local minima, as these are all fixed points. It is not clear what the sub/super scripts l, k mean. Maybe with these defined, the proposed multi-scale metric would have obvious advantages, but currently it is not clear what the point of this metric is. Figure 4 appears before it is mentioned and is displayed as part of the previous section. For the Lung data, it does not look like the proposed algorithm is better than the other two. None of the algorithms seem to do great at capturing any of the underlying structure, especially in the rows. It also is not super clear that the normal patients are significantly further from the cancer patients. Additionally are the linkage results from figure 3 from one trial? Without multiple trials it is hard to argue that this not just trial noise. How big are N1 and N2 in the linkage simulations. The Lung dataset is not very large, and it seems like the proposed algorithm has large computation complexity (it is not clear). Will the algorithm work on even medium-large sized matrices (10^4 x 10^4)? ", "rating": "7: Good paper, accept", "reply_text": "1.The overall motivation for how they construct the algorithm and the intuition behind how all the pieces of the algorithm work together are not great . A.Corrected , we have provided more motivation intuition and details on the algorithm . 2.Smooth is not clearly defined and not an obvious measure for a matrix . Figure 1 shows smooth matrices at various levels , but still does n't define explicitly what smoothness is . Does smoothness imply all entries are closer to the same value ? A. Smoothness can be characterized mathematically using a bi-H\u00f6lder condition , which is a common assumption in the matrix organization / biclustering literature . Smoothness implies that under the true row and column geometry of the data , neighboring entries are similar . 3 . `` Replacing Jr ( U ) and Jc ( U ) by quadratic row and column Laplacian penalties '' The sentence is kind of strange as Laplacian penalties is not a thing . Graph Laplacian can be used as an empirical estimate for the Laplace Beltrami operator which gives a measure of smoothness in terms of divergence of the gradient of a function on a manifold ; however the penalty is one on a function 's complexity in the intrinsic geometry of a manifold . It is not clear how the proposed penalty is an estimator for the intrinsic geometry penalty . It seems like the equation that is listed is just the function map $ \\Omega ( x ) = x^2 $ , which also is not a concave function ( it is convex ) , so it does not fit the requirements of Assumption 2.2 . A.Corrected . We have removed the term `` Laplacian penalties . '' The function $ \\Omega ( x ) = x^2 $ is convex , but it is not the $ \\Omega $ function studied in this paper . We included it as part of the literature review as it is a commonly used regularizer that bears some similarity to the one used in this paper , but agree we did not make this clear enough . We have added discussion below Assumption 2.2 , clarifying that the penalties used in this paper ( ones that satisfy Assumption 2.2 ) are different from those like commonly used convex quadratic penalties , previously referred to as `` `` Laplacian penalties . '' Penalties used in this paper can completely eliminate small variations between pairs of similar rows ( columns ) but less aggressively shrink very different pairs of rows ( columns ) towards each other . 4.Proposition 1 is kind of strangely presented . At first glance , it is not clear where the proof is , and it takes some looking to figure out it is Appendix B because it is reference before , not after the proposition . Or it might be more helpful if it is clearly stated at the beginning of Appendix B that this is the proof for Proposition 1 . A.Corrected . We have put the sentence `` The proof of Proposition 1 is in Appendix B '' after the statement of the proposition . We have also changed the title of Appendix B from `` Convergence '' to `` Proof of Proposition 1 . '' 5.The authors write : `` Missing values can sabotage efforts to learn the low dimensional manifold underlying the data . As the number of missing entries grows , the distances between points are increasingly distorted , resulting in poor representation of the data in the low-dimensional space . '' However , they use the observed values to build the knn graph used for the row/column penalties , which is counter-intuitive because this knn graph is essentially estimating a property of a manifold and the distances have the same distortion issue . A.While we are using a knn graph on the rows and columns , note that our method takes into account both rows and columns geometry jointly . Thus we are leveraging information from both domains to fill in the values of the data . In addition the weights of of our knn graph are continuously updated throughout the optimization based on the current smooth estimate $ U $ . Thus the weights are pulling rows and columns together at increasingly coarse scales , going from local geometry to global geometry ."}, {"review_id": "Hklc6oAcFX-1", "review_text": "The manuscript proposes a co-manifold learning approach for missing data. The problem is important, but the method is lack of novelty. Pros: important problem setting, Good experimental results. Cons: the method is lack of novelty. In detail, the method just simply combines a loss for competing missing values, which is not new, and Laplacian losses for rows and columns, which are also not new. I don't see much novelty in the model. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q.In detail , the method just simply combines a loss for competing missing values , which is not new , and Laplacian losses for rows and columns , which are also not new . A.Corrected . We have added a clarification on how our work is different from related work . Specifically , we clarify below Assumption 2.2 , that our penalties are different from commonly used Laplacian losses and the advantage our penalties have over commonly used Laplacian penalties for rows and columns . We also now write that : `` Our formulation ( 1 ) is distinct from related problem formulations in the following ways : 1 . Rows and columns of U are simultaneously shrunk towards each other as the parameters $ \\gamma_r $ and $ \\gamma_c $ increase . Note that this shrinkage procedure is fundamentally different from methods like the clustered dendrogram , which independently cluster the rows and columns as well as alternating partition tree construction procedures ( Gavish & Coifman , 2012 ; Mishne et al. , 2016 ) . 2.Our ultimate goal is not to perform matrix completion ( though this is a by-product of our approach ) but rather to perform joint row and column dimension reduction . 3.Our work generalizes both Shahid et al . ( 2016 ) and Chi et al . ( 2017 ) in that we seek the flexibility of performing non-linear dimension reduction on the rows and columns of the data matrix , i.e.a more general manifold organization than a co-clustered structure . 4.Instead of determining an optimal single scale of the solution as in Shahid et al . ( 2016 ) ; Chi et al . ( 2017 ) , we recognize that the multiple scales of the different solutions can be aggregated to better estimate the underlying geometry , similar to the tree-based Earth mover 's distance proposed in Ankenman ( 2014 ) ; Mishne et al. ( 2017 ) . ''"}, {"review_id": "Hklc6oAcFX-2", "review_text": "This paper presents a joint learning method for filling missing value and bi-clustering. The method extends (Chi et al. 2017), using a penalized matrix approximation. The proposed method is tested on three data sets, where two are synthetic and one small real-world data matrix. The presented method is claimed to be better than two classical approaches Nonlinear PCA and Diffusion Maps. 1) Filling missing values is not new. Even co-clustering with missing values also exists. It is insufficient to defeat two methods which are older than ten years. More extensive comparison is needed but lacking here. Why not first use a dedicated method such as MICE or collaborative filtering, and then run embedding method on rows and columns? 2) The purpose of the learning is unclear. The title does not give any hint about the learning goal. The objective function reads like filling missing values. The subsequent text claims that minimizing such a objective can achieve biclustering. However, in the experiment, the comparison is done via visualization and normal clustering (k-means). 3) The empirical results are not convincing. Two data sets are synthetic. The only real-world data set is very small. Why k-means was used? How to choose k in k-means? 4) The choice Omega function after Proposition 1 needs to be elaborated. A function curve plot could also help. 5) What is Omega' in Eq. 2?", "rating": "4: Ok but not good enough - rejection", "reply_text": "1a ) Filling missing values is not new . Even co-clustering with missing values also exists . A.Corrected . Note that our end-goal is not to fill in the missing data , but rather to reveal low dimensional embeddings for rows and columns of a data matrix in a missing data scenario . In addition , we are not addressing a co-clustering scenario but rather a more general problem in which the rows and columns are not necessarily clustered , but rather lie on a manifold structure . Taking the reviewers comments into consideration we have updated the introduction , now writing that `` ... In certain settings , however , assuming a bi-clustering model is too restrictive and results in breaking up smooth geometries into artificial disjoint clusters that do not match the actual structure of the data . This can occur when the true geometry is one of overlapping rather than disjoint clusters , for example in word-document analysis ( Ahn et al. , 2010 ) , or when the underlying structure is not one of clusters at all but rather a smooth manifold ( Gavish & Coifman , 2012 ) . Thus , we consider a more general viewpoint : data matrices possess geometric relationships between their rows ( features ) and columns ( observations ) such that both modes lie on low-dimensional manifolds . '' We have also added more details on how our formulation differs from related work , now writing that `` Our formulation ( 1 ) is distinct from related problem formulations in the following ways : 1 . Rows and columns of U are simultaneously shrunk towards each other as the parameters $ \\gamma_r $ and $ \\gamma_c $ increase . Note that this shrinkage procedure is fundamentally different from methods like the clustered dendrogram , which independently cluster the rows and columns as well as alternating partition tree construction procedures ( Gavish & Coifman , 2012 ; Mishne et al. , 2016 ) . 2.Our ultimate goal is not to perform matrix completion ( though this is a by-product of our approach ) but rather to perform joint row and column dimension reduction . 3.Our work generalizes both Shahid et al . ( 2016 ) and Chi et al . ( 2017 ) in that we seek the flexibility of performing non-linear dimension reduction on the rows and columns of the data matrix , i.e.a more general manifold organization than a co-clustered structure . 4.Instead of determining an optimal single scale of the solution as in Shahid et al . ( 2016 ) ; Chi et al . ( 2017 ) , we recognize that the multiple scales of the different solutions can be aggregated to better estimate the underlying geometry , similar to the tree-based Earth mover 's distance proposed in Ankenman ( 2014 ) ; Mishne et al. ( 2017 ) . `` 1b ) It is insufficient to defeat two methods which are older than ten years . More extensive comparison is needed but lacking here . Why not first use a dedicated method such as MICE or collaborative filtering , and then run embedding method on rows and columns ? A.Corrected . We have added both qualitative and quantitative comparisons to Fast robust PCA on Graphs [ Shahid2016 ] in the experimental results . We note that we tried to impute the data using MICE as the reviewer suggested but the algorithm failed to converge in reasonable time on either dataset . From our understanding MICE is also unsuitable for filling in the data for high percentage of missing values as we considered in our paper . Comparison to Diffusion Maps is intended to demonstrate the degradation that occurs for both visualization and clustering of data when values are missing . Since we use diffusion maps in our framework , this is a natural comparison . In addition , comparing to Diffusion Maps when the values have been filled in with the mean of all data is s equivalent to applying our approach for only a single scale of the cost parameters : $ \\gamma_r , \\gamma_c \\rightarrow \\infty $ ."}], "0": {"review_id": "Hklc6oAcFX-0", "review_text": "Review for CO-MANIFOLD LEARNING WITH MISSING DATA Summary: This paper proposes a two-stage method to recovering the underlying structure of a data manifold using both the rows and columns of an incomplete data matrix. In the first stage they impute the missing values using their proposed co-clustering algorithm and in the second stage they propose a new metric for dimension reduction. The overall motivation for how they construct the algorithm and the intuition behind how all the pieces of the algorithm work together are not great. The paper also has significant specific clarity issues (listed below). Currently these issues seem to imply the proposed algorithm has significant logic issues (mainly on the convex/concave confusions); however depending on how they are addressed, this may end up not being an issue. The experimental results for the two simulated datasets look very good. However for the lung dataset, the results are less promising and it is less clear of the advantage of the proposed algorithm to the two competing ones. Novelty/Significance: The overall idea of the algorithm is sufficiently novel. It is very interesting to consider both rows and column correlations. Each piece of the algorithm seems to draw heavily on previous work; bi-clustering, diffusion maps, but overall the idea is novel enough. The algorithm is significant in that it addresses a relatively open problem that currently doesn\u2019t have a well established solution. Questions/Clarity: Smooth is not clearly defined and not an obvious measure for a matrix. Figure 1 shows smooth matrices at various levels, but still doesn\u2019t define explicitly what smoothness is. Does smoothness imply all entries are closer to the same value? \u201cReplacing Jr(U) and Jc(U) by quadratic row and column Laplacian penalties\u201d \u2013 The sentence is kind of strange as Laplacian penalties is not a thing. Graph Laplacian can be used as an empirical estimate for the Laplace Beltrami operator which gives a measure of smoothness in terms of divergence of the gradient of a function on a manifold; however the penalty is one on a function\u2019s complexity in the intrinsic geometry of a manifold. It is not clear how the proposed penalty is an estimator for the intrinsic geometry penalty. It seems like the equation that is listed is just the function map Omega(x) = x^2, which also is not a concave function (it is convex), so it does not fit the requirements of Assumption 2.2. Proposition 1 is kind of strangely presented. At first glance, it is not clear where the proof is, and it takes some looking to figure out it is Appendix B because it is reference before, not after the proposition. Or it might be more helpful if it is clearly stated at the beginning of Appendix B that this is the proof for Proposition 1. The authors write: \u201cMissing values can sabotage efforts to learn the low dimensional manifold underlying the data. \u2026 As the number of missing entries grows, the distances between points are increasingly distorted, resulting in poor representation of the data in the low-dimensional space.\u201d However, they use the observed values to build the knn graph used for the row/column penalties, which is counter-intuitive because this knn graph is essentially estimating a property of a manifold and the distances have the same distortion issue. Why do the author\u2019s want Omega to be concave functions as this makes the objective not convex. Additionally the penalty sqrt(|| ||_2) is approximately doing a square root twice because the l2-norm already is the square root of the sum of squares. Also what is the point of approximating the square root function instead of just using the square root function? It is overall not clear what the nature of the penalty term g2 is; Appendix A, implies it must be overall a convex function because of the upper bound. Equation 5 is not clear that it is the first order taylor approximation. Omega\u2019 is the derivative of the Omega function? Do the other terms cancel out? Also what is the derivative with respect to; each Ui. for all Uj. ? \u201cfirst-order Taylor approximation of a differentiable concave function provides a tight bound on the function\u201d \u2013 Tight bound is not an appropriate term and requires being provable. Unless the function is close to linear, a first order Taylor approximation won\u2019t be anything close to tight. The authors state the objective in 1 is not convex. Do they mean it is not strictly convex? In which case, by stationary points, they are specifically referring to local minima? Otherwise, what benefits does the MM algorithm have on an indefinite objective i.e. couldn\u2019t you end up converging to a saddle point or a local maxima instead of a local minima, as these are all fixed points. It is not clear what the sub/super scripts l, k mean. Maybe with these defined, the proposed multi-scale metric would have obvious advantages, but currently it is not clear what the point of this metric is. Figure 4 appears before it is mentioned and is displayed as part of the previous section. For the Lung data, it does not look like the proposed algorithm is better than the other two. None of the algorithms seem to do great at capturing any of the underlying structure, especially in the rows. It also is not super clear that the normal patients are significantly further from the cancer patients. Additionally are the linkage results from figure 3 from one trial? Without multiple trials it is hard to argue that this not just trial noise. How big are N1 and N2 in the linkage simulations. The Lung dataset is not very large, and it seems like the proposed algorithm has large computation complexity (it is not clear). Will the algorithm work on even medium-large sized matrices (10^4 x 10^4)? ", "rating": "7: Good paper, accept", "reply_text": "1.The overall motivation for how they construct the algorithm and the intuition behind how all the pieces of the algorithm work together are not great . A.Corrected , we have provided more motivation intuition and details on the algorithm . 2.Smooth is not clearly defined and not an obvious measure for a matrix . Figure 1 shows smooth matrices at various levels , but still does n't define explicitly what smoothness is . Does smoothness imply all entries are closer to the same value ? A. Smoothness can be characterized mathematically using a bi-H\u00f6lder condition , which is a common assumption in the matrix organization / biclustering literature . Smoothness implies that under the true row and column geometry of the data , neighboring entries are similar . 3 . `` Replacing Jr ( U ) and Jc ( U ) by quadratic row and column Laplacian penalties '' The sentence is kind of strange as Laplacian penalties is not a thing . Graph Laplacian can be used as an empirical estimate for the Laplace Beltrami operator which gives a measure of smoothness in terms of divergence of the gradient of a function on a manifold ; however the penalty is one on a function 's complexity in the intrinsic geometry of a manifold . It is not clear how the proposed penalty is an estimator for the intrinsic geometry penalty . It seems like the equation that is listed is just the function map $ \\Omega ( x ) = x^2 $ , which also is not a concave function ( it is convex ) , so it does not fit the requirements of Assumption 2.2 . A.Corrected . We have removed the term `` Laplacian penalties . '' The function $ \\Omega ( x ) = x^2 $ is convex , but it is not the $ \\Omega $ function studied in this paper . We included it as part of the literature review as it is a commonly used regularizer that bears some similarity to the one used in this paper , but agree we did not make this clear enough . We have added discussion below Assumption 2.2 , clarifying that the penalties used in this paper ( ones that satisfy Assumption 2.2 ) are different from those like commonly used convex quadratic penalties , previously referred to as `` `` Laplacian penalties . '' Penalties used in this paper can completely eliminate small variations between pairs of similar rows ( columns ) but less aggressively shrink very different pairs of rows ( columns ) towards each other . 4.Proposition 1 is kind of strangely presented . At first glance , it is not clear where the proof is , and it takes some looking to figure out it is Appendix B because it is reference before , not after the proposition . Or it might be more helpful if it is clearly stated at the beginning of Appendix B that this is the proof for Proposition 1 . A.Corrected . We have put the sentence `` The proof of Proposition 1 is in Appendix B '' after the statement of the proposition . We have also changed the title of Appendix B from `` Convergence '' to `` Proof of Proposition 1 . '' 5.The authors write : `` Missing values can sabotage efforts to learn the low dimensional manifold underlying the data . As the number of missing entries grows , the distances between points are increasingly distorted , resulting in poor representation of the data in the low-dimensional space . '' However , they use the observed values to build the knn graph used for the row/column penalties , which is counter-intuitive because this knn graph is essentially estimating a property of a manifold and the distances have the same distortion issue . A.While we are using a knn graph on the rows and columns , note that our method takes into account both rows and columns geometry jointly . Thus we are leveraging information from both domains to fill in the values of the data . In addition the weights of of our knn graph are continuously updated throughout the optimization based on the current smooth estimate $ U $ . Thus the weights are pulling rows and columns together at increasingly coarse scales , going from local geometry to global geometry ."}, "1": {"review_id": "Hklc6oAcFX-1", "review_text": "The manuscript proposes a co-manifold learning approach for missing data. The problem is important, but the method is lack of novelty. Pros: important problem setting, Good experimental results. Cons: the method is lack of novelty. In detail, the method just simply combines a loss for competing missing values, which is not new, and Laplacian losses for rows and columns, which are also not new. I don't see much novelty in the model. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q.In detail , the method just simply combines a loss for competing missing values , which is not new , and Laplacian losses for rows and columns , which are also not new . A.Corrected . We have added a clarification on how our work is different from related work . Specifically , we clarify below Assumption 2.2 , that our penalties are different from commonly used Laplacian losses and the advantage our penalties have over commonly used Laplacian penalties for rows and columns . We also now write that : `` Our formulation ( 1 ) is distinct from related problem formulations in the following ways : 1 . Rows and columns of U are simultaneously shrunk towards each other as the parameters $ \\gamma_r $ and $ \\gamma_c $ increase . Note that this shrinkage procedure is fundamentally different from methods like the clustered dendrogram , which independently cluster the rows and columns as well as alternating partition tree construction procedures ( Gavish & Coifman , 2012 ; Mishne et al. , 2016 ) . 2.Our ultimate goal is not to perform matrix completion ( though this is a by-product of our approach ) but rather to perform joint row and column dimension reduction . 3.Our work generalizes both Shahid et al . ( 2016 ) and Chi et al . ( 2017 ) in that we seek the flexibility of performing non-linear dimension reduction on the rows and columns of the data matrix , i.e.a more general manifold organization than a co-clustered structure . 4.Instead of determining an optimal single scale of the solution as in Shahid et al . ( 2016 ) ; Chi et al . ( 2017 ) , we recognize that the multiple scales of the different solutions can be aggregated to better estimate the underlying geometry , similar to the tree-based Earth mover 's distance proposed in Ankenman ( 2014 ) ; Mishne et al. ( 2017 ) . ''"}, "2": {"review_id": "Hklc6oAcFX-2", "review_text": "This paper presents a joint learning method for filling missing value and bi-clustering. The method extends (Chi et al. 2017), using a penalized matrix approximation. The proposed method is tested on three data sets, where two are synthetic and one small real-world data matrix. The presented method is claimed to be better than two classical approaches Nonlinear PCA and Diffusion Maps. 1) Filling missing values is not new. Even co-clustering with missing values also exists. It is insufficient to defeat two methods which are older than ten years. More extensive comparison is needed but lacking here. Why not first use a dedicated method such as MICE or collaborative filtering, and then run embedding method on rows and columns? 2) The purpose of the learning is unclear. The title does not give any hint about the learning goal. The objective function reads like filling missing values. The subsequent text claims that minimizing such a objective can achieve biclustering. However, in the experiment, the comparison is done via visualization and normal clustering (k-means). 3) The empirical results are not convincing. Two data sets are synthetic. The only real-world data set is very small. Why k-means was used? How to choose k in k-means? 4) The choice Omega function after Proposition 1 needs to be elaborated. A function curve plot could also help. 5) What is Omega' in Eq. 2?", "rating": "4: Ok but not good enough - rejection", "reply_text": "1a ) Filling missing values is not new . Even co-clustering with missing values also exists . A.Corrected . Note that our end-goal is not to fill in the missing data , but rather to reveal low dimensional embeddings for rows and columns of a data matrix in a missing data scenario . In addition , we are not addressing a co-clustering scenario but rather a more general problem in which the rows and columns are not necessarily clustered , but rather lie on a manifold structure . Taking the reviewers comments into consideration we have updated the introduction , now writing that `` ... In certain settings , however , assuming a bi-clustering model is too restrictive and results in breaking up smooth geometries into artificial disjoint clusters that do not match the actual structure of the data . This can occur when the true geometry is one of overlapping rather than disjoint clusters , for example in word-document analysis ( Ahn et al. , 2010 ) , or when the underlying structure is not one of clusters at all but rather a smooth manifold ( Gavish & Coifman , 2012 ) . Thus , we consider a more general viewpoint : data matrices possess geometric relationships between their rows ( features ) and columns ( observations ) such that both modes lie on low-dimensional manifolds . '' We have also added more details on how our formulation differs from related work , now writing that `` Our formulation ( 1 ) is distinct from related problem formulations in the following ways : 1 . Rows and columns of U are simultaneously shrunk towards each other as the parameters $ \\gamma_r $ and $ \\gamma_c $ increase . Note that this shrinkage procedure is fundamentally different from methods like the clustered dendrogram , which independently cluster the rows and columns as well as alternating partition tree construction procedures ( Gavish & Coifman , 2012 ; Mishne et al. , 2016 ) . 2.Our ultimate goal is not to perform matrix completion ( though this is a by-product of our approach ) but rather to perform joint row and column dimension reduction . 3.Our work generalizes both Shahid et al . ( 2016 ) and Chi et al . ( 2017 ) in that we seek the flexibility of performing non-linear dimension reduction on the rows and columns of the data matrix , i.e.a more general manifold organization than a co-clustered structure . 4.Instead of determining an optimal single scale of the solution as in Shahid et al . ( 2016 ) ; Chi et al . ( 2017 ) , we recognize that the multiple scales of the different solutions can be aggregated to better estimate the underlying geometry , similar to the tree-based Earth mover 's distance proposed in Ankenman ( 2014 ) ; Mishne et al. ( 2017 ) . `` 1b ) It is insufficient to defeat two methods which are older than ten years . More extensive comparison is needed but lacking here . Why not first use a dedicated method such as MICE or collaborative filtering , and then run embedding method on rows and columns ? A.Corrected . We have added both qualitative and quantitative comparisons to Fast robust PCA on Graphs [ Shahid2016 ] in the experimental results . We note that we tried to impute the data using MICE as the reviewer suggested but the algorithm failed to converge in reasonable time on either dataset . From our understanding MICE is also unsuitable for filling in the data for high percentage of missing values as we considered in our paper . Comparison to Diffusion Maps is intended to demonstrate the degradation that occurs for both visualization and clustering of data when values are missing . Since we use diffusion maps in our framework , this is a natural comparison . In addition , comparing to Diffusion Maps when the values have been filled in with the mean of all data is s equivalent to applying our approach for only a single scale of the cost parameters : $ \\gamma_r , \\gamma_c \\rightarrow \\infty $ ."}}