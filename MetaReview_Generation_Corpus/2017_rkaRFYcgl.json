{"year": "2017", "forum": "rkaRFYcgl", "title": "Low-rank passthrough neural networks", "decision": "Reject", "meta_review": "The reviewers seem to agree that the framework presented is not very novel, something I agree with.\n The experiments show that the low rank + diagonal parameterization can be useful, however. The paper could be improved by making a more tightened message, and clearer arguments. As it currently stands, however it does not seem ready for publication in ICLR.", "reviews": [{"review_id": "rkaRFYcgl-0", "review_text": "The author proposes the use of low-rank matrix in feedfoward and RNNs. In particular, they try their approach in a GRU and a feedforward highway network. Author also presents as a contribution the passthrough framework, which can describe feedforward and recurrent networks. However, this framework seems hardly novel, relatively to the formalism introduced by LSTM or highway networks. An empirical evaluation is performed on different datasets (MNIST, memory/addition tasks, sequential permuted MNIST and character level penntreebank). However, there are few problems with the evaluation: - In the highway network experiment, the author does not compare with a baseline. We can not assess what it the impact of the low-rank parameterization. Also, it would be interesting to compare the result with a highway network that have this capacity bottleneck across layer (first layer of size $n$, second layer of size $d$, third layer of size $n$) and not in the gate functions. Also, how did you select the hyperparameter values?. - It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison. Also the overall bpc perplexity seems relatively high for this dataset. It is therefore not clear how low-rank decomposition would perform on this task applied on a stronger baseline. -Author claims state-of-art in the memory task. However, their approach uses more parameters than the uRNN (41K against 6.5K for the memory) which makes the comparison a little bit unfair toward uRNN. It would be informative to see how low-rank RNN performs using overall 6.5K parameters. Generally, it would be good to see what is the impact of the matrix rank given a fix state size. - It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task. - it is not clear when to use low-rank or low-rank + diagonal from the experiments. Overall, the evaluation in its current form in not really convincing, except for the sequential MNIST dataset.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments . > - In the highway network experiment , the author does not compare with a baseline The highway network experiment was exploratory , and I did n't have time to carry out all the proper evaluation . Therefore I moved it to the appendix . May remove it from the paper if the reviewers agree . > - It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison . I now tried to replicate ( Graves 2013 ) setup , although I still get worse results . Nevertheless , I still get an improvement using the Low-rank plus diagonal parametrization on an LSTM model , indicating that the method is robust w.r.t.different architectures . > -Author claims state-of-art in the memory task . However , their approach uses more parameters than the uRNN ( 41K against 6.5K for the memory ) which makes the comparison a little bit unfair toward uRNN . Fair point , I have now included a direct comparison to a uRNN with a large state size and also some experiments with a variant of the LRD-GRU . > - It would be informative to see how low-rank RNN performs using overall 6.5K parameters . > Overall , the evaluation in its current form in not really convincing , except for the sequential MNIST dataset . Hope that now it 's better . I 've tried in different configurations but it fails . > - It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task . Added the curve for the memory task , although for a different state size than in the uRNN paper . > - it is not clear when to use low-rank or low-rank + diagonal from the experiments . The low-rank is sufficient in most cases but not on the memory task . Since the low-rank plus diagonal adds a very small amount of complexity and parameters , there seems no reasons not to always use it ."}, {"review_id": "rkaRFYcgl-1", "review_text": "The paper proposes a low-rank version of pass-through networks to better control capacity, which can be useful in some cases, as shown in the experiments. That said, I found the results not very convincing overall. Results are overall not as good as state-of-the-art on sequential MNIST or the memory task, but add one more hyper-parameter to tune. As I said, it would help to show in Tables and/or Figures competing approaches like uRNNs.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Results are SOTA on the memory tasks as far as I know ."}, {"review_id": "rkaRFYcgl-2", "review_text": "The authors study the use of low-rank approximation to the matrix-multiply in RNNs. This reduces the number of parameters by a large factor, and with a diagonal addition (called low-rank plus diagonal) it is shown to work as well as a fully-parametrized network on a number of tasks. The paper is solid, the only weakness being some claims about conceptual unification (e.g., the first line of the conclusion -- \"We presented a framework that unifies the description various types of recurrent and feed-forward neural networks as passthrough neural networks.\" -- claiming this framework as a contribution of this paper is untrue, the general framework is well known in the community and RNNs have been presented in this way before.) Aside from the above small point, the true contribution is in making low-rank RNNs work, the results are generally as good as fully-parametrized networks. They are hardly better though, which makes it unclear why low-rank networks should be used. The contribution is thus not very strong in terms of results, but even achieving the same results with fewer parameters is not easy and the studies were well-executed and explained.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments . > the only weakness being some claims about conceptual unification ( e.g. , the first line of the conclusion -- `` We presented a framework that unifies the description various types of recurrent and feed-forward neural networks as passthrough neural networks . '' -- claiming this framework as a contribution of this paper is untrue , the general framework is well known in the community and RNNs have been presented in this way before . ) Fair point , I removed that sentence . > They are hardly better though , which makes it unclear why low-rank networks should be used . For memory , addition and permuted sequential MNIST they work better than full-rank networks , presumably due to a regularization effect . The contribution is thus not very strong in terms of results , but even achieving the same results with fewer parameters is not easy and the studies were well-executed and explained . On language modelling , even with the additional regularization of dropout , they work pretty much as good as full-rank networks for the same state size , and better for the same parameter size , thus at the very least they could be used as a from of model compression ."}], "0": {"review_id": "rkaRFYcgl-0", "review_text": "The author proposes the use of low-rank matrix in feedfoward and RNNs. In particular, they try their approach in a GRU and a feedforward highway network. Author also presents as a contribution the passthrough framework, which can describe feedforward and recurrent networks. However, this framework seems hardly novel, relatively to the formalism introduced by LSTM or highway networks. An empirical evaluation is performed on different datasets (MNIST, memory/addition tasks, sequential permuted MNIST and character level penntreebank). However, there are few problems with the evaluation: - In the highway network experiment, the author does not compare with a baseline. We can not assess what it the impact of the low-rank parameterization. Also, it would be interesting to compare the result with a highway network that have this capacity bottleneck across layer (first layer of size $n$, second layer of size $d$, third layer of size $n$) and not in the gate functions. Also, how did you select the hyperparameter values?. - It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison. Also the overall bpc perplexity seems relatively high for this dataset. It is therefore not clear how low-rank decomposition would perform on this task applied on a stronger baseline. -Author claims state-of-art in the memory task. However, their approach uses more parameters than the uRNN (41K against 6.5K for the memory) which makes the comparison a little bit unfair toward uRNN. It would be informative to see how low-rank RNN performs using overall 6.5K parameters. Generally, it would be good to see what is the impact of the matrix rank given a fix state size. - It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task. - it is not clear when to use low-rank or low-rank + diagonal from the experiments. Overall, the evaluation in its current form in not really convincing, except for the sequential MNIST dataset.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for your comments . > - In the highway network experiment , the author does not compare with a baseline The highway network experiment was exploratory , and I did n't have time to carry out all the proper evaluation . Therefore I moved it to the appendix . May remove it from the paper if the reviewers agree . > - It is unfortunate that the character level penntreebank does not use the same experimental setting than previous works as it prevents from direct comparison . I now tried to replicate ( Graves 2013 ) setup , although I still get worse results . Nevertheless , I still get an improvement using the Low-rank plus diagonal parametrization on an LSTM model , indicating that the method is robust w.r.t.different architectures . > -Author claims state-of-art in the memory task . However , their approach uses more parameters than the uRNN ( 41K against 6.5K for the memory ) which makes the comparison a little bit unfair toward uRNN . Fair point , I have now included a direct comparison to a uRNN with a large state size and also some experiments with a variant of the LRD-GRU . > - It would be informative to see how low-rank RNN performs using overall 6.5K parameters . > Overall , the evaluation in its current form in not really convincing , except for the sequential MNIST dataset . Hope that now it 's better . I 've tried in different configurations but it fails . > - It would be informative as well to have the baseline and the uRNN curve in Figure 2 for the memory/addition task . Added the curve for the memory task , although for a different state size than in the uRNN paper . > - it is not clear when to use low-rank or low-rank + diagonal from the experiments . The low-rank is sufficient in most cases but not on the memory task . Since the low-rank plus diagonal adds a very small amount of complexity and parameters , there seems no reasons not to always use it ."}, "1": {"review_id": "rkaRFYcgl-1", "review_text": "The paper proposes a low-rank version of pass-through networks to better control capacity, which can be useful in some cases, as shown in the experiments. That said, I found the results not very convincing overall. Results are overall not as good as state-of-the-art on sequential MNIST or the memory task, but add one more hyper-parameter to tune. As I said, it would help to show in Tables and/or Figures competing approaches like uRNNs.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Results are SOTA on the memory tasks as far as I know ."}, "2": {"review_id": "rkaRFYcgl-2", "review_text": "The authors study the use of low-rank approximation to the matrix-multiply in RNNs. This reduces the number of parameters by a large factor, and with a diagonal addition (called low-rank plus diagonal) it is shown to work as well as a fully-parametrized network on a number of tasks. The paper is solid, the only weakness being some claims about conceptual unification (e.g., the first line of the conclusion -- \"We presented a framework that unifies the description various types of recurrent and feed-forward neural networks as passthrough neural networks.\" -- claiming this framework as a contribution of this paper is untrue, the general framework is well known in the community and RNNs have been presented in this way before.) Aside from the above small point, the true contribution is in making low-rank RNNs work, the results are generally as good as fully-parametrized networks. They are hardly better though, which makes it unclear why low-rank networks should be used. The contribution is thus not very strong in terms of results, but even achieving the same results with fewer parameters is not easy and the studies were well-executed and explained.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your comments . > the only weakness being some claims about conceptual unification ( e.g. , the first line of the conclusion -- `` We presented a framework that unifies the description various types of recurrent and feed-forward neural networks as passthrough neural networks . '' -- claiming this framework as a contribution of this paper is untrue , the general framework is well known in the community and RNNs have been presented in this way before . ) Fair point , I removed that sentence . > They are hardly better though , which makes it unclear why low-rank networks should be used . For memory , addition and permuted sequential MNIST they work better than full-rank networks , presumably due to a regularization effect . The contribution is thus not very strong in terms of results , but even achieving the same results with fewer parameters is not easy and the studies were well-executed and explained . On language modelling , even with the additional regularization of dropout , they work pretty much as good as full-rank networks for the same state size , and better for the same parameter size , thus at the very least they could be used as a from of model compression ."}}