{"year": "2020", "forum": "HJx8HANFDH", "title": "Four Things Everyone Should Know to Improve Batch Normalization", "decision": "Accept (Poster)", "meta_review": "This paper proposes techniques to improve training with batch normalization. The paper establishes the benefits of these techniques experimentally using ablation studies. The reviewers found the results to be promising and of interest to the community. However, this paper is borderline due in part due to the writing (notation issues) and because it does not discuss related work enough. We encourage the authors to properly address these issues before the camera ready.", "reviews": [{"review_id": "HJx8HANFDH-0", "review_text": "The paper performs an empirical study of four batch-normalization improvements and proposes a new normalization technique for small batch sizes, based on group and batch normalizations. Among others, the authors address the inconsistency between the train and the test stages and the problem of small batch sizes. The authors conducted an empirical ablation study of the four techniques and proposed an intuition when each method should be used. Concerns: (1) The comparison with baselines in section 4.2 seems to be unclear. Fig.5 shows the performance of normalization methods for different batch sizes. Batch Normalization, however, has the same performance for all batch sizes. The authors refer to this baseline as \u201cidealized Batch Normalization\u201d. Additional elaboration on what does this means is required. (2) It would also be beneficial to see the comparison with the original Ghost Batch Normalization in the final evaluation (section 4.2), since this method, according to section 3.2, was capable of the significant improvement for Caltech-256 dataset. (3) In section 3.1, the authors provide an intuition of why can the discrepancy between test and train phases hurts the performance of a model. The empirical evaluation of this effect is needed to justify this intuition. Overall, the newly proposed method is a minor update, and novelty is limited. However, the thorough empirical study of existing improvement techniques would be a good addition to the conference. Minor comments: 1. share the y-axis in Fig.4 between different ghost batch sizes. I would also recommend authors to include the following papers to the related work section: 1. Riemannian approach to batch normalization [https://arxiv.org/abs/1709.09603] ---------- Respond to the rebuttal. Clarification on the concern (3): I agree that, in general, a discrepancy between training and testing can hurt a model. The paper showed that the output of a batch normalization layer is theoretically unbounded during testing. However, it would be beneficial to see numerically if it indeed the case on a real test set (the output range is wider than the one during training). ", "rating": "6: Weak Accept", "reply_text": "Thank you for the thoughtful review ! We appreciate your thorough reading of the paper , and here are responses to your listed concerns : ( 1 ) This was meant to provide an easy comparison to Batch Normalization -- in the ideal case , Batch Normalization would scale perfectly across batch sizes , not changing in performance as the overall batch size changed . Plotting the performance of Batch Normalization this way lets us compare our approach against that ideal . ( 2 ) Thank you for the suggestion , we have now added standalone Ghost Batch Normalization to the analysis in Figure 5 of the updated paper . ( 3 ) We 're a little unsure of the precise meaning of what you 're asking , and would appreciate any clarification you could give . Generally speaking , introducing most types of discrepancies between training and testing should hurt a model , since models perform best when they are trained to directly optimize the task they are evaluated on . While some exceptions exist ( e.g.data augmentation ) , training on a task or distribution slightly different from the task being evaluated on , or training in a slightly different way than the evaluation settings , should hurt performance . Resolving discrepancies introduced by Batch Normalization has been a common theme in prior work ( see references in Sec.3.1 ) Novelty ( same comment as Reviewer 2 ) : We appreciate and agree with the reviewer 's opinion that improving Batch Normalization , such a fundamental component of modern neural networks , is of significant value to the community . We presented multiple ( four ) improvements to Batch Normalization , validated our results extensively , and in our latest revision we have added experiments on another transfer learning dataset , bringing the number of evaluation datasets up to 6 , and added an additional experiment for training on non-i.i.d.distribution . Fig.4 y-axis : Thank you for your suggestion ! We 've looked into this , but found that keeping the y-axis constant across different plots makes many of the plots hard to read -- for example , for Caltech-256 the full y-axis for accuracy would be [ 30 , 60 ] , but Ghost Batch Sizes 4 through 16 only have values in the range [ 53 , 60 ] , which squeezes all of the accuracy curves down to ~25 % of their original size . Reference : Thank you for the reference ! We have added it in our related work . Thank you for the review ! Best , Paper 1113 Authors"}, {"review_id": "HJx8HANFDH-1", "review_text": "The authors discuss four techniques to improve Batch Normalization, including inference example weighing, medium batch size, weight decay, the combination of batch and group normalization. Equipped with the proposed techniques, the authors obtain promising results when training deep models with various batch sizes. However, the novelty of this paper seems very limited and more experiments are required. Please see my detailed comments below. Positive points\uff1a 1. The proposed inference example weighing method yields promising results and does not require any re-training. 2. The combination of batch and group normalization makes it possible to train deep models with very small batch size. 3. By combining all the techniques, the proposed method yields promising performance when training deep models with different batch sizes. Negative points: 1. Some notations are very confusing. For example, the authors use B to represent the size of a minibatch. However, why do the authors only consider B-1 samples in Eq. (2), i.e., selecting the minimum possible output among x_1, \u2026, x_{B-1}? 2. The proposed inference example weighing method seems very similar to Batch Renormalization. Both methods seek to use a linear function to combine the batch statistics and the moving statistics. What is the essential difference between these methods? 3. What model do the authors use in the experiment of Figure 2? Why do the authors conduct experiments on different datasets in Section 3.1 (including ImageNet) and Section 3.2 (excluding ImageNet)? It would be stronger to provide ImageNet results in Section 3.2. 4. The authors draw different conclusions about the usage of weight decay from a recent work (He et al, CVPR2019). The CVPR paper reports that training \\gamma and \\beta without weight decay on ResNet-50 yields significant performance improvement. However, this paper shows that training ResNet-50 with weight decay improves the performance. Please comment on the differences in the conclusions. Reference: \"Bag of tricks for image classification with convolutional neural networks.\" CVPR, 2019. 5. The authors only report ImageNet results of the proposed inference example weighing method. However, all the experiments in Section 4 are performed on three small datasets. It is necessary and important to provide ImageNet results to show the effectiveness of the other three techniques in Section 4. 6. Note that training deep models with non-i.i.d. minibatches is a typical case to evaluate normalization methods, e.g., Batch Renormalization. Specifically, examples in a minibatch are not sampled independently. What would happen if the authors apply the proposed techniques to the non-i.i.d. case? 7. Some closely related work should be discussed in the paper, such as [1] \"Decorrelated Batch Normalization.\" CVPR, 2018. [2] \"Double Forward Propagation for Memorized Batch Normalization.\" AAAI, 2018. [3] \"Differentiable Dynamic Normalization for Learning Deep Representation.\" ICML, 2019. [4] \"Iterative Normalization: Beyond Standardization towards Efficient Whitening.\" CVPR, 2019. Minor issues: 1. In Section 1, the third contribution is not a complete sentence. 2. There are many typos in the paper. (1) In Section 2, \u201cLayer Normalization, which has found use in many natural language processing tasks.\u201d Should \u201cwhich has found use\u201d be \u201cwhich has been used\u201d? (2) In Section 3.1, \u201cBatch Normalization has a disparity in function between training inference\u201d. \u201cbetween training inference\u201d should be \u201cbetween training and inference\u201d. (3) In Section 3.1, \u201cwe need only figure out \u2026\u201d should be \u201cwe only need to figure out \u2026\u201d ", "rating": "3: Weak Reject", "reply_text": "( continued from part 1 ) 7 . Thank you for the references , we have added brief descriptions of each to our section detailing related work . Minor issues : With all due respect , only one of these is actually a typo ( missing the word 'and ' for 2.2 ) : 1 . The contributions are written as a list ( note that each ends in a comma ) . Since contribution 4 is the last item in the list , contribution 3 ends with `` and '' . 2.1.This is correct English grammar . 2.2.Thank you ! This was indeed a typo . 2.3.This is correct English grammar . Thank you for the review ! Best , Paper 1113 Authors"}, {"review_id": "HJx8HANFDH-2", "review_text": "The paper introduces four techniques to improve the deep network model through modifying Batch Normalization (BN). The inspirations are from the gaps between train&test and between batches in multi-gpu training, comparison to other normalization methods, and weight decay in regularizing convolution weights training. The paper studies each techniques with the support from experiments. The paper is easy to follow. The techniques seem effective. The paper mentions \"theory\" multiple times, but lacks sufficient justification to support these \"theories\". So one suggestion is to replace \"theory\" with a soft word. Experimental evidence seems sufficient and there are some theoretical derivations, but it looks incremental that the paper presents some techniques in improving Batch Normalization only. In general, the paper is of values to the community.", "rating": "6: Weak Accept", "reply_text": "Thank you for your review ! We put care into making sure the concepts in the paper were easy to follow , and appreciate that it came through effectively . Theory : While we do have some theory in our paper ( e.g.the proof in Appendix A ) , we have replaced the instances of this word with `` hypothesis '' , which is more clear for the other parts of the paper . Thank you for your suggestion ! Novelty ( same comment as Reviewer 3 ) : We appreciate and agree with the reviewer 's opinion that improving Batch Normalization , such a fundamental component of modern neural networks , is of significant value to the community . We presented multiple ( four ) improvements to Batch Normalization , validated our results extensively , and in our latest revision we have added experiments on another transfer learning dataset , bringing the number of evaluation datasets up to 6 , and added an additional experiment for training on non-i.i.d.distribution . Thank you for the review ! Best , Paper 1113 Authors"}], "0": {"review_id": "HJx8HANFDH-0", "review_text": "The paper performs an empirical study of four batch-normalization improvements and proposes a new normalization technique for small batch sizes, based on group and batch normalizations. Among others, the authors address the inconsistency between the train and the test stages and the problem of small batch sizes. The authors conducted an empirical ablation study of the four techniques and proposed an intuition when each method should be used. Concerns: (1) The comparison with baselines in section 4.2 seems to be unclear. Fig.5 shows the performance of normalization methods for different batch sizes. Batch Normalization, however, has the same performance for all batch sizes. The authors refer to this baseline as \u201cidealized Batch Normalization\u201d. Additional elaboration on what does this means is required. (2) It would also be beneficial to see the comparison with the original Ghost Batch Normalization in the final evaluation (section 4.2), since this method, according to section 3.2, was capable of the significant improvement for Caltech-256 dataset. (3) In section 3.1, the authors provide an intuition of why can the discrepancy between test and train phases hurts the performance of a model. The empirical evaluation of this effect is needed to justify this intuition. Overall, the newly proposed method is a minor update, and novelty is limited. However, the thorough empirical study of existing improvement techniques would be a good addition to the conference. Minor comments: 1. share the y-axis in Fig.4 between different ghost batch sizes. I would also recommend authors to include the following papers to the related work section: 1. Riemannian approach to batch normalization [https://arxiv.org/abs/1709.09603] ---------- Respond to the rebuttal. Clarification on the concern (3): I agree that, in general, a discrepancy between training and testing can hurt a model. The paper showed that the output of a batch normalization layer is theoretically unbounded during testing. However, it would be beneficial to see numerically if it indeed the case on a real test set (the output range is wider than the one during training). ", "rating": "6: Weak Accept", "reply_text": "Thank you for the thoughtful review ! We appreciate your thorough reading of the paper , and here are responses to your listed concerns : ( 1 ) This was meant to provide an easy comparison to Batch Normalization -- in the ideal case , Batch Normalization would scale perfectly across batch sizes , not changing in performance as the overall batch size changed . Plotting the performance of Batch Normalization this way lets us compare our approach against that ideal . ( 2 ) Thank you for the suggestion , we have now added standalone Ghost Batch Normalization to the analysis in Figure 5 of the updated paper . ( 3 ) We 're a little unsure of the precise meaning of what you 're asking , and would appreciate any clarification you could give . Generally speaking , introducing most types of discrepancies between training and testing should hurt a model , since models perform best when they are trained to directly optimize the task they are evaluated on . While some exceptions exist ( e.g.data augmentation ) , training on a task or distribution slightly different from the task being evaluated on , or training in a slightly different way than the evaluation settings , should hurt performance . Resolving discrepancies introduced by Batch Normalization has been a common theme in prior work ( see references in Sec.3.1 ) Novelty ( same comment as Reviewer 2 ) : We appreciate and agree with the reviewer 's opinion that improving Batch Normalization , such a fundamental component of modern neural networks , is of significant value to the community . We presented multiple ( four ) improvements to Batch Normalization , validated our results extensively , and in our latest revision we have added experiments on another transfer learning dataset , bringing the number of evaluation datasets up to 6 , and added an additional experiment for training on non-i.i.d.distribution . Fig.4 y-axis : Thank you for your suggestion ! We 've looked into this , but found that keeping the y-axis constant across different plots makes many of the plots hard to read -- for example , for Caltech-256 the full y-axis for accuracy would be [ 30 , 60 ] , but Ghost Batch Sizes 4 through 16 only have values in the range [ 53 , 60 ] , which squeezes all of the accuracy curves down to ~25 % of their original size . Reference : Thank you for the reference ! We have added it in our related work . Thank you for the review ! Best , Paper 1113 Authors"}, "1": {"review_id": "HJx8HANFDH-1", "review_text": "The authors discuss four techniques to improve Batch Normalization, including inference example weighing, medium batch size, weight decay, the combination of batch and group normalization. Equipped with the proposed techniques, the authors obtain promising results when training deep models with various batch sizes. However, the novelty of this paper seems very limited and more experiments are required. Please see my detailed comments below. Positive points\uff1a 1. The proposed inference example weighing method yields promising results and does not require any re-training. 2. The combination of batch and group normalization makes it possible to train deep models with very small batch size. 3. By combining all the techniques, the proposed method yields promising performance when training deep models with different batch sizes. Negative points: 1. Some notations are very confusing. For example, the authors use B to represent the size of a minibatch. However, why do the authors only consider B-1 samples in Eq. (2), i.e., selecting the minimum possible output among x_1, \u2026, x_{B-1}? 2. The proposed inference example weighing method seems very similar to Batch Renormalization. Both methods seek to use a linear function to combine the batch statistics and the moving statistics. What is the essential difference between these methods? 3. What model do the authors use in the experiment of Figure 2? Why do the authors conduct experiments on different datasets in Section 3.1 (including ImageNet) and Section 3.2 (excluding ImageNet)? It would be stronger to provide ImageNet results in Section 3.2. 4. The authors draw different conclusions about the usage of weight decay from a recent work (He et al, CVPR2019). The CVPR paper reports that training \\gamma and \\beta without weight decay on ResNet-50 yields significant performance improvement. However, this paper shows that training ResNet-50 with weight decay improves the performance. Please comment on the differences in the conclusions. Reference: \"Bag of tricks for image classification with convolutional neural networks.\" CVPR, 2019. 5. The authors only report ImageNet results of the proposed inference example weighing method. However, all the experiments in Section 4 are performed on three small datasets. It is necessary and important to provide ImageNet results to show the effectiveness of the other three techniques in Section 4. 6. Note that training deep models with non-i.i.d. minibatches is a typical case to evaluate normalization methods, e.g., Batch Renormalization. Specifically, examples in a minibatch are not sampled independently. What would happen if the authors apply the proposed techniques to the non-i.i.d. case? 7. Some closely related work should be discussed in the paper, such as [1] \"Decorrelated Batch Normalization.\" CVPR, 2018. [2] \"Double Forward Propagation for Memorized Batch Normalization.\" AAAI, 2018. [3] \"Differentiable Dynamic Normalization for Learning Deep Representation.\" ICML, 2019. [4] \"Iterative Normalization: Beyond Standardization towards Efficient Whitening.\" CVPR, 2019. Minor issues: 1. In Section 1, the third contribution is not a complete sentence. 2. There are many typos in the paper. (1) In Section 2, \u201cLayer Normalization, which has found use in many natural language processing tasks.\u201d Should \u201cwhich has found use\u201d be \u201cwhich has been used\u201d? (2) In Section 3.1, \u201cBatch Normalization has a disparity in function between training inference\u201d. \u201cbetween training inference\u201d should be \u201cbetween training and inference\u201d. (3) In Section 3.1, \u201cwe need only figure out \u2026\u201d should be \u201cwe only need to figure out \u2026\u201d ", "rating": "3: Weak Reject", "reply_text": "( continued from part 1 ) 7 . Thank you for the references , we have added brief descriptions of each to our section detailing related work . Minor issues : With all due respect , only one of these is actually a typo ( missing the word 'and ' for 2.2 ) : 1 . The contributions are written as a list ( note that each ends in a comma ) . Since contribution 4 is the last item in the list , contribution 3 ends with `` and '' . 2.1.This is correct English grammar . 2.2.Thank you ! This was indeed a typo . 2.3.This is correct English grammar . Thank you for the review ! Best , Paper 1113 Authors"}, "2": {"review_id": "HJx8HANFDH-2", "review_text": "The paper introduces four techniques to improve the deep network model through modifying Batch Normalization (BN). The inspirations are from the gaps between train&test and between batches in multi-gpu training, comparison to other normalization methods, and weight decay in regularizing convolution weights training. The paper studies each techniques with the support from experiments. The paper is easy to follow. The techniques seem effective. The paper mentions \"theory\" multiple times, but lacks sufficient justification to support these \"theories\". So one suggestion is to replace \"theory\" with a soft word. Experimental evidence seems sufficient and there are some theoretical derivations, but it looks incremental that the paper presents some techniques in improving Batch Normalization only. In general, the paper is of values to the community.", "rating": "6: Weak Accept", "reply_text": "Thank you for your review ! We put care into making sure the concepts in the paper were easy to follow , and appreciate that it came through effectively . Theory : While we do have some theory in our paper ( e.g.the proof in Appendix A ) , we have replaced the instances of this word with `` hypothesis '' , which is more clear for the other parts of the paper . Thank you for your suggestion ! Novelty ( same comment as Reviewer 3 ) : We appreciate and agree with the reviewer 's opinion that improving Batch Normalization , such a fundamental component of modern neural networks , is of significant value to the community . We presented multiple ( four ) improvements to Batch Normalization , validated our results extensively , and in our latest revision we have added experiments on another transfer learning dataset , bringing the number of evaluation datasets up to 6 , and added an additional experiment for training on non-i.i.d.distribution . Thank you for the review ! Best , Paper 1113 Authors"}}