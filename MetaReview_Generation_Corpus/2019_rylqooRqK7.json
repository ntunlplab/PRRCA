{"year": "2019", "forum": "rylqooRqK7", "title": "SNAS: stochastic neural architecture search", "decision": "Accept (Poster)", "meta_review": "This paper provides an alternative way to enable differentiable optimization to the neural architecture search problem.  Different from DARTS, SNAS reformulates the problem and employs Gumbel random variables to directly optimize the NAS objective. In addition, the resource-constrained regularization is interesting. The major cons of the paper is that the empirical results are not quite impressive, especially when compared to DARTS, in terms of both accuracy and convergence. I think this is a borderline paper but maybe good enough for acceptance.\n\n", "reviews": [{"review_id": "rylqooRqK7-0", "review_text": "This work refines the NAS method for efficient neural architecture search. The paper brings new methods for gradient/reward updates and credit assignment. pros: 1. An improvement on gradient calculation and reward back-propagation mechanism 2. Good experiment results and fair comparisons cons: 1. Missing details on how to use the gradient information to generate child network structures. In eq.2, multiplying each one-hot random variable Zij to each edge (i, j) in the DAG can obtain a child graph whose intermediate nodes are xj. However, it is still unclear how to generate the child graph. More details on generating child network based on gradient information is expected. 2. In SNAS, P(z) is assumed fully factorizable. Factors are parameterized with alpha and learnt along with operation parameters theta. The factorization of p(Z) is based on the observation that NAS is a task with fully delayed rewards in a deterministic environment. That is, the feedback signal is only ready after the whole episode is done and all state transitions distributions are delta functions. In eq. 3, the authors use the training/testing loss directly as reward, while the previous method uses a constant reward from validation accuracy. It is unclear why using the training/testing loss can improve the results? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . 1 ) How to use gradient information to generate child network SNAS does not directly use gradient information to generate child network . Search gradient is naturally applied to update architecture parameters , which are parameters for concrete distribution . Then in the derivation step , operations with largest probability in the concrete distribution are selected . 2 ) Why using training/testing loss as reward can improve the results ? This is introduced with details in Section 2.3 , as well as Appendix D and E , which we believe is one of our contribution . As stated in your comment , we first prove that NAS is a task in deterministic environment with fully delayed reward . Then a proof from [ 1 ] is introduced that TD-learning suffers from delayed bias when delayed reward exists . It is proposed and proved in [ 1 ] that Taylor decomposition of reward could resolve delayed bias because no temporal difference setting exists anymore . We then prove that the delayed reward in NAS could be decomposed and assigned to all structural decisions with gradient back-propagation , when differentiable training/testing loss is used as reward . Therefore , leveraging the proof from [ 1 ] , we prove that SNAS should converge faster than ENAS , which is verified by our experiment as stated in Section 3.1 . Intuitively speaking , we spot the unnecessary temporal difference setting in NAS and solve it by using training/testing loss as reward . [ 1 ] Arjona-Medina et al. , `` Rudder : Return decomposition for delayed rewards '' , arXiv 2018"}, {"review_id": "rylqooRqK7-1", "review_text": "This paper improves upon ENAS and DARTS by taking a differentiable approach to NAS and optimizing the objective across the distribution of child graphs. This technique allows for end-to-end architecture search while constraining resource usage and allowing parameter sharing by generating effective reusable child graphs. SNAS employs Gumbel random variables which gives it better gradients and makes learning more robust compared to ENAS. The use of Gumbel variables also allow SNAS to directly optimize the NAS objective which is an advantage over DARTS. The resource constraint regularization is interesting. Regularizing on the parameters that describe the architecture can help constrain resource usage during the forward pass. The proposed method is novel but the main concern here is that there is no clear win over existing techniques in terms of performance. I can't see anywhere in the tables where you demonstrate a clear improvement over DARTS or ENAS. Furthermore, in your child network evaluation with CIFAR-10, you mention that the comparison is without fine-tuning. Do you think this might be contributing to the performance gap in DARTS? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your review and questions ! 1 ) Clear win over DARTS : As a NAS task , it is believed that the performance of a framework is evaluated with i ) the efficiency and automation in searching process and ii ) the accuracy and complexity of searching result , i.e.child networks . In this metric , SNAS 's advantage over DARTS is three-fold , due to less-biased searching objective and the resource constraint : A . Less computing resources for the whole searching pipeline Child network derived from SNAS without any fine-tuning could maintain the accuracy , thus a ) during searching , the accuracy in SNAS could reflect the performance of child network . DARTS , on the contrast , has to retrain the network for 100 epochs as stated in the caption of figure 3 to track the actual searching progress ; b ) after searching , DARTS has to retrain the child network even if there is no extension on cell number of channel number . SNAS , on the contrast , does not have this requirement . A retraining is only needed when the child network is extended , which in our work is basically for fair comparison . All these retraining will take much longer time when NAS is directly applied to a large dataset . B.Automated sparse network generation Though DARTS takes ZERO op , which represents deleting the edge , into account in the searching process , it is omitted in child network operation selection as discovered by one of our reviewers in this comment [ 1 ] . ( This discovery is very interesting , as this reviewer discovered that ZERO tends to be the op with largest weight in DARTS.That is , in DARTS the `` soft-2nd-max '' is chosen . ) The approach to delete edge is manually designed as `` to choose the top-k incoming edges for each node '' . In SNAS , to keep or delete an edge is automatically learnt . That is to say , the ZERO op is acting its supposed job to engender sparsity . In our updated version , experiment showed that with an aggressive resource constraint , SNAS discovers architecture whose reduction cell has only two edges and two nodes but comparable accuracy with 1st-order DARTS in CIFAR-10 , posing a question for the validity or optimality of manually designed scheme in DARTS . C. Comparable accuracy with less resource in child networks In our updated version , we show that with a moderate resource constraint which plays the role of a regularizer , SNAS discovers architecture with slightly better accuracy and fewer parameters comparing to 1st-order DARTS , which is also comparable to 2nd-order DARTS . Note that in this paper we only show result of 1st-order SNAS due to limited time and extensive experiment required , though the 2nd-order extension is straight-forward [ 2 ] . As shown in DARTS , as well as [ 3 ] , 2nd-order empirically brings better optimality , a fair comparison would be with 1st-order DARTS . Actually in 1st-order SNAS , an accuracy comparable with 1st-order DARTS could be achieved with 1/3 fewer parameters , when an aggressive constraint is applied . As for transferring to ImageNet , there is no theoretical justification in the literature to the best of our knowledge , we provide it mainly for a fair comparison with DARTS . Our next step is to try a direct search on ImageNet leveraging that SNAS does not need retraining on the searching result . 2 ) The effect of fine-tuning in evaluating child networks directly derived from DARTS In our empirical study , a fine-tuning of the derived child networks can improve its performance , but could not remedy the gap completely after 100 epochs . ( 100 epoch is the plateau of fine-tuning , and also a fair comparison with SNAS . ) And there seems always to be a small gap ( -1.0+/-0.7 ) % between the accuracy after fine-tuning and at the end of searching . More importantly , the 'gap ' we want to discuss here is between the performance of a derived child network and the optimization objective in searching . As shown in Figure 3 in our paper , the optimization objective in searching is already converged to some optimum before this derivation , for both architecture parameters and operation parameters . Theoretically speaking , to use this parent network would be a justified result for the optimization problem . But with the absence of a guarantee that softmax weights will become discrete in the end , it would become an attention learning task , rather than NAS task . Though there exist methods like designing prior or extra learning objective to autonomously encourage one-hot-ness , a scheme is manually designed to delete a large portion of operations even though their weights are not 0 . A natural question to ask is , why in this case the architecture parameter is still the optimal , even though the performance of child networks could be boosted with some fine-tuning . [ 1 ] https : //openreview.net/forum ? id=rylqooRqK7 & noteId=rkeruyjYhX [ 2 ] Finn et al. , `` Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks '' , ICML 2017 . [ 3 ] https : //openreview.net/forum ? id=rylqooRqK7 & noteId=BJxaZ7Kojm"}, {"review_id": "rylqooRqK7-2", "review_text": "Summary: This paper proposes Stochastic Neural Architecture Search (SNAS), a method to automatically and efficiently search for neural architectures. It is built upon 2 existing works on these topics, namely ENAS (Pham et al 2018) and DARTS (Liu et al 2018). SNAS provides nice theory and explanation of gradient computations, unites the strengths and avoid the weaknesses of ENAS and DARTS. There are many details in the paper, including the Appendix. The idea is as follows: +------------+---------------------+-------------------------+ | Method | Differentiable | Directly Optimize | | | | NAS reward | +------------+---------------------+-------------------------+ | ENAS | No | Yes | | DARTS | Yes | No | | SNAS | Yes | Yes | +------------+---------------------+-------------------------+ SNAS inherits the idea of ENAS and DARTS by superpositioning all possible architectures into a Directed Acyclic Graph (DAG), effectively sharing the weights among all architectures. However, SNAS improves over ENAS and DARTS as follows (Section 2.2): 1. SNAS improves over ENAS in that it allows independent sampling at edges in the shared DAG, leading to a more tractable gradient at the edges of the DAG, which in turn allows more tractable Monte Carlo estimation of the gradients with respect to the architectural parameters. 2. While DARTS also has the property (1), DARTS implements this by computing the expected value at each node in the DAG, with respect to the joint distribution of the input edges and the operations. This makes DARTS not optimize the direct NAS objective. SNAS, due to their smart manipulation of architectural gradients using Gumbel variables, still optimizes the same objective with NAS and ENAS, but has a smoother gradients. Experimental results in the paper show that SNAS finds architectures on CIFAR-10 that are comparable to those found by ENAS and DARTS, using a reasonable amount of computing resource. These architectures can also be transferred to learn competent models on ImageNet, like those of DARTS. Furthermore, experimental observations (Figure 3) are consistent with the theory above, that is: 1. The search process of SNAS is more stable than that of ENAS (as SNAS samples with a smaller variance). 2. Architectures found by SNAS perform better than those of DARTS, as SNAS searches directly for the NAS reward of the sampled models. Strengths: 1. SNAS unites the strengths and avoids the weaknesses of ENAS and DARTS 2. SNAS provides a nice theory, which is verified through their experimental results. Weaknesses: I don\u2019t really have any complaints about this paper. Some presentations of the paper might have been improved, e.g. the discussion on the ZERO operation in other comments should have been included. ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your positive comments and detailed summary ! We have included experiments to show how the effect of ZERO op differentiates SNAS from DARTS . Please kindly have a check ."}], "0": {"review_id": "rylqooRqK7-0", "review_text": "This work refines the NAS method for efficient neural architecture search. The paper brings new methods for gradient/reward updates and credit assignment. pros: 1. An improvement on gradient calculation and reward back-propagation mechanism 2. Good experiment results and fair comparisons cons: 1. Missing details on how to use the gradient information to generate child network structures. In eq.2, multiplying each one-hot random variable Zij to each edge (i, j) in the DAG can obtain a child graph whose intermediate nodes are xj. However, it is still unclear how to generate the child graph. More details on generating child network based on gradient information is expected. 2. In SNAS, P(z) is assumed fully factorizable. Factors are parameterized with alpha and learnt along with operation parameters theta. The factorization of p(Z) is based on the observation that NAS is a task with fully delayed rewards in a deterministic environment. That is, the feedback signal is only ready after the whole episode is done and all state transitions distributions are delta functions. In eq. 3, the authors use the training/testing loss directly as reward, while the previous method uses a constant reward from validation accuracy. It is unclear why using the training/testing loss can improve the results? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your review . 1 ) How to use gradient information to generate child network SNAS does not directly use gradient information to generate child network . Search gradient is naturally applied to update architecture parameters , which are parameters for concrete distribution . Then in the derivation step , operations with largest probability in the concrete distribution are selected . 2 ) Why using training/testing loss as reward can improve the results ? This is introduced with details in Section 2.3 , as well as Appendix D and E , which we believe is one of our contribution . As stated in your comment , we first prove that NAS is a task in deterministic environment with fully delayed reward . Then a proof from [ 1 ] is introduced that TD-learning suffers from delayed bias when delayed reward exists . It is proposed and proved in [ 1 ] that Taylor decomposition of reward could resolve delayed bias because no temporal difference setting exists anymore . We then prove that the delayed reward in NAS could be decomposed and assigned to all structural decisions with gradient back-propagation , when differentiable training/testing loss is used as reward . Therefore , leveraging the proof from [ 1 ] , we prove that SNAS should converge faster than ENAS , which is verified by our experiment as stated in Section 3.1 . Intuitively speaking , we spot the unnecessary temporal difference setting in NAS and solve it by using training/testing loss as reward . [ 1 ] Arjona-Medina et al. , `` Rudder : Return decomposition for delayed rewards '' , arXiv 2018"}, "1": {"review_id": "rylqooRqK7-1", "review_text": "This paper improves upon ENAS and DARTS by taking a differentiable approach to NAS and optimizing the objective across the distribution of child graphs. This technique allows for end-to-end architecture search while constraining resource usage and allowing parameter sharing by generating effective reusable child graphs. SNAS employs Gumbel random variables which gives it better gradients and makes learning more robust compared to ENAS. The use of Gumbel variables also allow SNAS to directly optimize the NAS objective which is an advantage over DARTS. The resource constraint regularization is interesting. Regularizing on the parameters that describe the architecture can help constrain resource usage during the forward pass. The proposed method is novel but the main concern here is that there is no clear win over existing techniques in terms of performance. I can't see anywhere in the tables where you demonstrate a clear improvement over DARTS or ENAS. Furthermore, in your child network evaluation with CIFAR-10, you mention that the comparison is without fine-tuning. Do you think this might be contributing to the performance gap in DARTS? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for your review and questions ! 1 ) Clear win over DARTS : As a NAS task , it is believed that the performance of a framework is evaluated with i ) the efficiency and automation in searching process and ii ) the accuracy and complexity of searching result , i.e.child networks . In this metric , SNAS 's advantage over DARTS is three-fold , due to less-biased searching objective and the resource constraint : A . Less computing resources for the whole searching pipeline Child network derived from SNAS without any fine-tuning could maintain the accuracy , thus a ) during searching , the accuracy in SNAS could reflect the performance of child network . DARTS , on the contrast , has to retrain the network for 100 epochs as stated in the caption of figure 3 to track the actual searching progress ; b ) after searching , DARTS has to retrain the child network even if there is no extension on cell number of channel number . SNAS , on the contrast , does not have this requirement . A retraining is only needed when the child network is extended , which in our work is basically for fair comparison . All these retraining will take much longer time when NAS is directly applied to a large dataset . B.Automated sparse network generation Though DARTS takes ZERO op , which represents deleting the edge , into account in the searching process , it is omitted in child network operation selection as discovered by one of our reviewers in this comment [ 1 ] . ( This discovery is very interesting , as this reviewer discovered that ZERO tends to be the op with largest weight in DARTS.That is , in DARTS the `` soft-2nd-max '' is chosen . ) The approach to delete edge is manually designed as `` to choose the top-k incoming edges for each node '' . In SNAS , to keep or delete an edge is automatically learnt . That is to say , the ZERO op is acting its supposed job to engender sparsity . In our updated version , experiment showed that with an aggressive resource constraint , SNAS discovers architecture whose reduction cell has only two edges and two nodes but comparable accuracy with 1st-order DARTS in CIFAR-10 , posing a question for the validity or optimality of manually designed scheme in DARTS . C. Comparable accuracy with less resource in child networks In our updated version , we show that with a moderate resource constraint which plays the role of a regularizer , SNAS discovers architecture with slightly better accuracy and fewer parameters comparing to 1st-order DARTS , which is also comparable to 2nd-order DARTS . Note that in this paper we only show result of 1st-order SNAS due to limited time and extensive experiment required , though the 2nd-order extension is straight-forward [ 2 ] . As shown in DARTS , as well as [ 3 ] , 2nd-order empirically brings better optimality , a fair comparison would be with 1st-order DARTS . Actually in 1st-order SNAS , an accuracy comparable with 1st-order DARTS could be achieved with 1/3 fewer parameters , when an aggressive constraint is applied . As for transferring to ImageNet , there is no theoretical justification in the literature to the best of our knowledge , we provide it mainly for a fair comparison with DARTS . Our next step is to try a direct search on ImageNet leveraging that SNAS does not need retraining on the searching result . 2 ) The effect of fine-tuning in evaluating child networks directly derived from DARTS In our empirical study , a fine-tuning of the derived child networks can improve its performance , but could not remedy the gap completely after 100 epochs . ( 100 epoch is the plateau of fine-tuning , and also a fair comparison with SNAS . ) And there seems always to be a small gap ( -1.0+/-0.7 ) % between the accuracy after fine-tuning and at the end of searching . More importantly , the 'gap ' we want to discuss here is between the performance of a derived child network and the optimization objective in searching . As shown in Figure 3 in our paper , the optimization objective in searching is already converged to some optimum before this derivation , for both architecture parameters and operation parameters . Theoretically speaking , to use this parent network would be a justified result for the optimization problem . But with the absence of a guarantee that softmax weights will become discrete in the end , it would become an attention learning task , rather than NAS task . Though there exist methods like designing prior or extra learning objective to autonomously encourage one-hot-ness , a scheme is manually designed to delete a large portion of operations even though their weights are not 0 . A natural question to ask is , why in this case the architecture parameter is still the optimal , even though the performance of child networks could be boosted with some fine-tuning . [ 1 ] https : //openreview.net/forum ? id=rylqooRqK7 & noteId=rkeruyjYhX [ 2 ] Finn et al. , `` Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks '' , ICML 2017 . [ 3 ] https : //openreview.net/forum ? id=rylqooRqK7 & noteId=BJxaZ7Kojm"}, "2": {"review_id": "rylqooRqK7-2", "review_text": "Summary: This paper proposes Stochastic Neural Architecture Search (SNAS), a method to automatically and efficiently search for neural architectures. It is built upon 2 existing works on these topics, namely ENAS (Pham et al 2018) and DARTS (Liu et al 2018). SNAS provides nice theory and explanation of gradient computations, unites the strengths and avoid the weaknesses of ENAS and DARTS. There are many details in the paper, including the Appendix. The idea is as follows: +------------+---------------------+-------------------------+ | Method | Differentiable | Directly Optimize | | | | NAS reward | +------------+---------------------+-------------------------+ | ENAS | No | Yes | | DARTS | Yes | No | | SNAS | Yes | Yes | +------------+---------------------+-------------------------+ SNAS inherits the idea of ENAS and DARTS by superpositioning all possible architectures into a Directed Acyclic Graph (DAG), effectively sharing the weights among all architectures. However, SNAS improves over ENAS and DARTS as follows (Section 2.2): 1. SNAS improves over ENAS in that it allows independent sampling at edges in the shared DAG, leading to a more tractable gradient at the edges of the DAG, which in turn allows more tractable Monte Carlo estimation of the gradients with respect to the architectural parameters. 2. While DARTS also has the property (1), DARTS implements this by computing the expected value at each node in the DAG, with respect to the joint distribution of the input edges and the operations. This makes DARTS not optimize the direct NAS objective. SNAS, due to their smart manipulation of architectural gradients using Gumbel variables, still optimizes the same objective with NAS and ENAS, but has a smoother gradients. Experimental results in the paper show that SNAS finds architectures on CIFAR-10 that are comparable to those found by ENAS and DARTS, using a reasonable amount of computing resource. These architectures can also be transferred to learn competent models on ImageNet, like those of DARTS. Furthermore, experimental observations (Figure 3) are consistent with the theory above, that is: 1. The search process of SNAS is more stable than that of ENAS (as SNAS samples with a smaller variance). 2. Architectures found by SNAS perform better than those of DARTS, as SNAS searches directly for the NAS reward of the sampled models. Strengths: 1. SNAS unites the strengths and avoids the weaknesses of ENAS and DARTS 2. SNAS provides a nice theory, which is verified through their experimental results. Weaknesses: I don\u2019t really have any complaints about this paper. Some presentations of the paper might have been improved, e.g. the discussion on the ZERO operation in other comments should have been included. ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your positive comments and detailed summary ! We have included experiments to show how the effect of ZERO op differentiates SNAS from DARTS . Please kindly have a check ."}}