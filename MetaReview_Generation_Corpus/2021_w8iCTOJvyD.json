{"year": "2021", "forum": "w8iCTOJvyD", "title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time", "decision": "Reject", "meta_review": "This work proposes new learning algorithms that fine-tune (\"tailor\") a model at test-time using unsupervised objectives. This formulation allows for introducing an inductive bias into the model that might improve generalization on unseen data. The proposed algorithm is demonstrated on two example tasks.\n\nThe reviewers like the topic and also find the proposed approach to be interesting. However, they are unconvinced by the current empirical evaluation of the method. Additional experimental evaluation could improve our understanding of the proposed method and help contrast it to previously proposed techniques. Given these reviews I recommend rejecting the paper at this time.", "reviews": [{"review_id": "w8iCTOJvyD-0", "review_text": "The paper proposes tailoring and meta-tailoring , learning processes that fine-tune the model parameters during test-time using unsupervised objectives . This allows for designing and integrating powerful inductive biases into the model , leading to an improved test-time performance in two example tasks . Strengths : * The proposed approaches are well-motivated , and the paper is written clearly . * CNGrad provides an elegant solution to implement tailoring efficiently . * The experimental results show the benefits of the proposed approach convincingly . Weaknesses : * While CNGrad provides an efficient implementation of tailoring , it would be interesting to see how it compares to its \u201c inefficient counterpart \u201d , in which no additional parameters are introduced , but the parameters w are optimized for each individual sample using the supervised and the tailoring objective simultaneously . * It would be nice to include the inductively trained model as a baseline to the experiment in section 5.2 . This could highlight more clearly the benefit of tailoring when applied to adversarial examples . * I find the statement that the paper showed \u201c the applicability of tailoring on three domains \u201d slightly misleading . The paper shows its applicability experimentally in two domains , and shows theoretical results for the third . Additionally , building on these theoretical results , it would be interesting to see how the contrastive loss might be used for tailoring in an experimental setting . Questions : * How is the element-wise normalization in CNGrad performed at test-time ? Do you keep a running average of the batch-statistics as is done in batch normalization ? * Which loss was used for the results in Table 1 ? * What are the run-time implications of tailoring , both at train and test time ? Additional Comments : * Fig 1 : Try to avoid using red and green as distinguishing colors to improve the paper \u2019 s accessibility . * It might be nice to add a paragraph on inductive learning to the intro . Disclaimer : I did not check the provided proofs in detail .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your review . We address your points in order , and note that the answer to all reviewers includes general comments on recurrent topics as well as new experiments . * * Inefficient meta-tailoring vs CNGrad * * Compute-wise the naive implementation of meta-tailoring ( using MAML or similar algorithms ) is significantly slower than CNGrad by a factor between 128 ( batch for ImageNet ) and 6400 ( batch for planet experiments ) , making it far too slow to run ( on the order of weeks to months ) . This is because it has to be run sequentially and the affine layers are very cheap computationally compared to linear layers . Result-wise , our theorem in section 4 guarantees that we have enough capacity to optimize the inner loop with the affine parameters alone . However , CNGrad still needs to optimize all parameters in the outer loop . * * Classical inductive baseline & setting * * Because inductive learning is classic machine learning ( train model , freeze parameters , test ) , section 5.2 is comparing vanilla ( inductive ) RandomizedSmoothing vs. its meta-tailoring counterpart . We have added \u201c Inductive \u201d in the table to make it clearer . This is also relevant to one of your comments , suggesting we add a paragraph on inductive learning in the intro . The paragraph was there ( starting with \u201c In classic supervised learning \u201c ) , but also didn \u2019 t mention inductive learning . We have changed it to start with \u201c In classic inductive supervised learning \u201d to make it clear . * * Contrastive experiments & theoretical guarantees * * In the general response to all reviewers , we address the confusion regarding the contrastive experiments and add more details on the theoretical guarantees . There , we also discuss new results on model-based RL . * * BatchNorm statistics * * \u201c Do you keep a running average of the batch-statistics for CNGrad at test time ? \u201d CNGrad per se does not apply any normalization , only an affine transformation . Therefore , there is no need to keep any statistics . One can combine BatchNorm and CNGrad , handling the batch statistics of BatchNorm as per usual . Table 1 used MSE loss ; we \u2019 ve added that to the description . * * \u201c Run-time implications of tailoring both at training and test-time \u201d * * Good question . The detailed answer can be found on \u201c Computational cost \u201d of appendix D. In short , first , it depends on the number of tailoring steps linearly ( often 1 is enough ) . Then , it depends on which layers are adapted in the inner loop : if we add affine transformations to all the layers in the network and do one tailoring step , performance will be 3x the usual prediction time ( initial forward pass , tailoring adaptation , final forward pass ) . One can speed this up ( which we did in the adversarial experiments ) by only adapting the higher layers ( which are usually the ones needing adaptation ) . This reduces the computational factor to $ 1+\\frac { 2a } { L } $ , where \u2018 a \u2019 is the number of adapted layers and L the number of total layers . For instance , if we tailor the last 5 layers of a resnet-110 then we would get an increase only of $ 5\\cdot2/110 \\approx 9 $ percent , ( 1.09x factor ) . * * Color-blind-friendly plots * * Good point on the colors for the plots , we have changed the red to light orange and made the green darker . We also checked with a color-blind simulator that they are distinguishable . Thanks for the suggestion ."}, {"review_id": "w8iCTOJvyD-1", "review_text": "The authors propose a learning method called tailoring , inspired by transductive learning , which works by fine tuning a model on an unsupervised loss given a test time example input . The benefit of this approach is that arbitrary constraints on predictions can be imposed without a suffering from a generalization gap if the constraints were used as an auxiliary objective during training . The authors also propose meta-tailor , which includes the tailoring process as an inner optimization loop during training . In a sense , meta-tailoring is like meta-learning but with each training example considered a separate task . The authors provided extensive theoretical justification for tailoring and meta tailoring , as well as an efficient means to implement it through conditional normalization parameters . They then perform two experiments , one where ( meta- ) tailoring is used to impose physical constraints on a neural network physics simulator and another where meta-tailoring is used to improve the robustness of an image classifier to adversarial attacks . The idea of ( meta- ) tailoring is quite intriguing and I could see it applied to scenarios in structured prediction or as an alternative to posterior regularization . However , this paper was very theory heavy and I must admit , I struggled to understand the import or necessity of the provided theorems . One of the claims of this paper is that they demonstrate that improving prediction quality with contrastive learning , but this is only done theoretically . As this claim is included in the list of experimental results , I find that somewhat disingenuous . I would have much preferred a contrastive learning experiment . I lean to reject this paper . Miscellaneous note : In the last paragraph of page 4 , it is mentioned that parts of definition 1 and theorem 1 are in bold green , but I see no bold green in this copy of the paper .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comments . \u201c I struggled to understand the importance or necessity of the provided theorems \u201d & \u201c I see no bold green in theorem 1 and definition 1 \u201d * * The theory section formalizes and provides guarantees to the claims and intuitions made in the introduction . * * As you mention , the bold green in the theoretical guarantees is indeed hardly noticeable , we have made it brighter . It was also hard to see because the differences in guarantees between meta-tailoring and classic ( inductive ) learning are very small ( text-wise , not result-wise ! ) , affecting only the subscript of a subscript . In particular , $ f_ { \\theta_ { x , S } } $ turns to $ f_ { \\theta_ { S } } $ for inductive learning guarantees , because meta-tailoring adapts the parameters to the input $ x $ , but regular inductive learning doesn \u2019 t . By having two extremely similar bounds , we can understand the effect of meta-tailoring more clearly . * * Eliminating generalization gap for tailoring loss * * Concretely , we show that we can upper bound the task loss using two terms . The first term is the key to our approach , depending solely on the tailoring loss at the query point . Because we tailor a custom $ \\theta_x $ to minimize the tailoring loss at test time , as well as training time , we can make the term small , eliminating the generalization gap that arises in classic ML training . In contrast , classic ( inductive ) ML can not adapt $ \\theta $ to each $ x $ and thus has a generalization gap . The second term involves a classic stability bound , and we also provide arguments why meta-tailoring can help on that term as well . In the general response to all reviewers , we address the confusion regarding the contrastive experiments and add more details on the theoretical guarantees . There , we also discuss new results on model-based RL . * * Empirial validation * * : please see the general response where we detail new baselines with related work and a totally new application in model-based RL , added on a new section 5.3 in the main text ."}, {"review_id": "w8iCTOJvyD-2", "review_text": "This paper presents tailoring and meta-tailoring to eliminate the generalization gap by optimizing at test time . This paper combines the idea of test time optimization and meta-learning and provides some theoretical analysis of the proposed methods . Experiments show the proposed method is effective in solving the machine learning problem and improving model robustness . Strengths : - The idea of using meta-learning to improve tailoring is interesting . Some theoretical justification for the proposed method is provided . - The explanation of different learning settings is insightful . Weaknesses : - The proposed tailoring method is very close to Test Time Training ( Sun et al. , 2019 ) .However , this paper fails to clearly show the differences between tailoring and TTT . There is also no theoretical or empirical evidence to show that tailoring/meta-tailoring is better than TTT . - The experiments in this paper are quite weak . The descriptions of the experiment settings are unclear . There is no direct comparison with closely related methods like TTT . The results of widely used benchmarks are not provided . Overall , although I think the idea is interesting , the proposed method is not well verified , which makes the effectiveness of the method is still unclear . Some closely related work is not sufficiently discussed . Therefore , I lean to recommend rejection for this paper .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments . * * Standard benchmarks on adversarial examples and model-based RL * * `` The results of widely used benchmarks are not provided '' As you point out , the physics experiment is not a standard benchmark . We chose it for instructive purposes and to show potential useful applications of our method in science . However , the adversarial experiments build on arguably the most standard certifiable defense ( Randomized Smoothing ) and evaluate on the two most important benchmarks for adversarial examples : CIFAR-10 & Imagenet . For conciseness ( and because we believe the value of the paper lies in its novelty and general applicability rather than matching state-of-the-art ) we put the tables including the SOA methods in the appendix . We also mentioned both works in the penultimate paragraph of section 5.2 ( Zhai et al. , Salman et al . ) . There , we detail that our approach matches SOA on ImageNet ( not CIFAR-10 ) despite our framework not being specialized to adversarial examples and that meta-tailoring could also potentially improve the other approaches . In the general response we detail new experiments improving on PDDM ( Nagabandi et al . ) , a popular method , on a complex MuJoCo benchmark , with results added to section 5.3 in the main text . We believe these further show the wide range of applicability of meta-tailoring . * * Extra experiments comparing to TTT * * `` There is no direct comparison with closely related methods like TTT . '' In the general answer to all reviewers we discuss differences from test time training ( Sun et al . ) , and show that it is a sub-optimal approach for same-distribution generalization by providing extra experiments . We hope this addresses your concerns , both in terms of novelty and experimental performance ."}, {"review_id": "w8iCTOJvyD-3", "review_text": "= Summary This paper proposes a meta-tailoring method where the auxiliary tailor loss is used to adapt the model parameters at test time . The paper also provides a theoretical analysis of the advantages of the proposed tailoring/meta-tailoring . The proposed method is evaluated on various application tasks and obtain significant improvements over baselines . = Pros 1.Test-time training is getting increasing attention recently , however , the theoretical analysis of the advantages of test time training is still behind . This paper provides a reasonable theoretical analysis of tailoring and meta-tailoring . 2.The proposed CNGrad approach is simple but effective , which provides strong empirical improvements on various tasks . 3.The paper is well written and clearly organized . == Concerns/confusions 1. what is the formulation of the affine parameters ? Is there any study about adapting the entire model versus the affine parameters only ? 2.Missing contrastive learning experiments on images ? In the last sentence of the abstract , it says `` ... , and using contrastive losses on the query image to improve generalization '' . However , I did n't find a discussion on it in the main paper . Given that previous work ( e.g. , Sun et al . ) has done experiments on image classification tasks , it would be more compelling to provide a comparison in the same setting . In general , this paper proposes an interesting approach and insightful analysis of the proposed method . I 'm willing to upgrade my score with comparable experiments with prior work on the image classification task .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your review . * * \u201c What is the formulation of the affine parameters ? Is there any study about adapting the entire model versus the affine parameters ? \u201d * * - Inspired by conditional normalization , our method CNGrad adds affine layers $ \\vec { y } = \\vec { \\gamma } \\cdot \\vec { x } + \\vec { \\beta } $ and only optimizes these parameters in the inner loop . These layers can be added in every hidden layer of most deep architectures . - The closest method that we know is CAVIA ( Zintgraf et al . ) , which makes a neural network predict these affine parameters from a trainable vector . Then , instead of directly optimizing the affine parameters , they optimize the trainable vector . We show this extra complexity is unnecessary . - In theorem 2 of section 4 we prove that optimizing the affine parameters alone has the same capacity as optimizing the entire network . This theorem holds for a wide variety of losses and realistic network requirements , but requires evaluating the loss only at a finite number of data-points . As a result , the affine parameters have enough capacity for the inner loop ( which uses a finite number of samples ) , but not for the outer loop ( where we optimize all parameters to express arbitrary functions ) . * * Comparison to test-time training , missing contrastive results and new model-based RL results : * * We address these points in the general answer , where we also explain a new application of meta-tailoring to model-based RL , added in section 5.3 in the main text ."}], "0": {"review_id": "w8iCTOJvyD-0", "review_text": "The paper proposes tailoring and meta-tailoring , learning processes that fine-tune the model parameters during test-time using unsupervised objectives . This allows for designing and integrating powerful inductive biases into the model , leading to an improved test-time performance in two example tasks . Strengths : * The proposed approaches are well-motivated , and the paper is written clearly . * CNGrad provides an elegant solution to implement tailoring efficiently . * The experimental results show the benefits of the proposed approach convincingly . Weaknesses : * While CNGrad provides an efficient implementation of tailoring , it would be interesting to see how it compares to its \u201c inefficient counterpart \u201d , in which no additional parameters are introduced , but the parameters w are optimized for each individual sample using the supervised and the tailoring objective simultaneously . * It would be nice to include the inductively trained model as a baseline to the experiment in section 5.2 . This could highlight more clearly the benefit of tailoring when applied to adversarial examples . * I find the statement that the paper showed \u201c the applicability of tailoring on three domains \u201d slightly misleading . The paper shows its applicability experimentally in two domains , and shows theoretical results for the third . Additionally , building on these theoretical results , it would be interesting to see how the contrastive loss might be used for tailoring in an experimental setting . Questions : * How is the element-wise normalization in CNGrad performed at test-time ? Do you keep a running average of the batch-statistics as is done in batch normalization ? * Which loss was used for the results in Table 1 ? * What are the run-time implications of tailoring , both at train and test time ? Additional Comments : * Fig 1 : Try to avoid using red and green as distinguishing colors to improve the paper \u2019 s accessibility . * It might be nice to add a paragraph on inductive learning to the intro . Disclaimer : I did not check the provided proofs in detail .", "rating": "7: Good paper, accept", "reply_text": "Thanks for your review . We address your points in order , and note that the answer to all reviewers includes general comments on recurrent topics as well as new experiments . * * Inefficient meta-tailoring vs CNGrad * * Compute-wise the naive implementation of meta-tailoring ( using MAML or similar algorithms ) is significantly slower than CNGrad by a factor between 128 ( batch for ImageNet ) and 6400 ( batch for planet experiments ) , making it far too slow to run ( on the order of weeks to months ) . This is because it has to be run sequentially and the affine layers are very cheap computationally compared to linear layers . Result-wise , our theorem in section 4 guarantees that we have enough capacity to optimize the inner loop with the affine parameters alone . However , CNGrad still needs to optimize all parameters in the outer loop . * * Classical inductive baseline & setting * * Because inductive learning is classic machine learning ( train model , freeze parameters , test ) , section 5.2 is comparing vanilla ( inductive ) RandomizedSmoothing vs. its meta-tailoring counterpart . We have added \u201c Inductive \u201d in the table to make it clearer . This is also relevant to one of your comments , suggesting we add a paragraph on inductive learning in the intro . The paragraph was there ( starting with \u201c In classic supervised learning \u201c ) , but also didn \u2019 t mention inductive learning . We have changed it to start with \u201c In classic inductive supervised learning \u201d to make it clear . * * Contrastive experiments & theoretical guarantees * * In the general response to all reviewers , we address the confusion regarding the contrastive experiments and add more details on the theoretical guarantees . There , we also discuss new results on model-based RL . * * BatchNorm statistics * * \u201c Do you keep a running average of the batch-statistics for CNGrad at test time ? \u201d CNGrad per se does not apply any normalization , only an affine transformation . Therefore , there is no need to keep any statistics . One can combine BatchNorm and CNGrad , handling the batch statistics of BatchNorm as per usual . Table 1 used MSE loss ; we \u2019 ve added that to the description . * * \u201c Run-time implications of tailoring both at training and test-time \u201d * * Good question . The detailed answer can be found on \u201c Computational cost \u201d of appendix D. In short , first , it depends on the number of tailoring steps linearly ( often 1 is enough ) . Then , it depends on which layers are adapted in the inner loop : if we add affine transformations to all the layers in the network and do one tailoring step , performance will be 3x the usual prediction time ( initial forward pass , tailoring adaptation , final forward pass ) . One can speed this up ( which we did in the adversarial experiments ) by only adapting the higher layers ( which are usually the ones needing adaptation ) . This reduces the computational factor to $ 1+\\frac { 2a } { L } $ , where \u2018 a \u2019 is the number of adapted layers and L the number of total layers . For instance , if we tailor the last 5 layers of a resnet-110 then we would get an increase only of $ 5\\cdot2/110 \\approx 9 $ percent , ( 1.09x factor ) . * * Color-blind-friendly plots * * Good point on the colors for the plots , we have changed the red to light orange and made the green darker . We also checked with a color-blind simulator that they are distinguishable . Thanks for the suggestion ."}, "1": {"review_id": "w8iCTOJvyD-1", "review_text": "The authors propose a learning method called tailoring , inspired by transductive learning , which works by fine tuning a model on an unsupervised loss given a test time example input . The benefit of this approach is that arbitrary constraints on predictions can be imposed without a suffering from a generalization gap if the constraints were used as an auxiliary objective during training . The authors also propose meta-tailor , which includes the tailoring process as an inner optimization loop during training . In a sense , meta-tailoring is like meta-learning but with each training example considered a separate task . The authors provided extensive theoretical justification for tailoring and meta tailoring , as well as an efficient means to implement it through conditional normalization parameters . They then perform two experiments , one where ( meta- ) tailoring is used to impose physical constraints on a neural network physics simulator and another where meta-tailoring is used to improve the robustness of an image classifier to adversarial attacks . The idea of ( meta- ) tailoring is quite intriguing and I could see it applied to scenarios in structured prediction or as an alternative to posterior regularization . However , this paper was very theory heavy and I must admit , I struggled to understand the import or necessity of the provided theorems . One of the claims of this paper is that they demonstrate that improving prediction quality with contrastive learning , but this is only done theoretically . As this claim is included in the list of experimental results , I find that somewhat disingenuous . I would have much preferred a contrastive learning experiment . I lean to reject this paper . Miscellaneous note : In the last paragraph of page 4 , it is mentioned that parts of definition 1 and theorem 1 are in bold green , but I see no bold green in this copy of the paper .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your comments . \u201c I struggled to understand the importance or necessity of the provided theorems \u201d & \u201c I see no bold green in theorem 1 and definition 1 \u201d * * The theory section formalizes and provides guarantees to the claims and intuitions made in the introduction . * * As you mention , the bold green in the theoretical guarantees is indeed hardly noticeable , we have made it brighter . It was also hard to see because the differences in guarantees between meta-tailoring and classic ( inductive ) learning are very small ( text-wise , not result-wise ! ) , affecting only the subscript of a subscript . In particular , $ f_ { \\theta_ { x , S } } $ turns to $ f_ { \\theta_ { S } } $ for inductive learning guarantees , because meta-tailoring adapts the parameters to the input $ x $ , but regular inductive learning doesn \u2019 t . By having two extremely similar bounds , we can understand the effect of meta-tailoring more clearly . * * Eliminating generalization gap for tailoring loss * * Concretely , we show that we can upper bound the task loss using two terms . The first term is the key to our approach , depending solely on the tailoring loss at the query point . Because we tailor a custom $ \\theta_x $ to minimize the tailoring loss at test time , as well as training time , we can make the term small , eliminating the generalization gap that arises in classic ML training . In contrast , classic ( inductive ) ML can not adapt $ \\theta $ to each $ x $ and thus has a generalization gap . The second term involves a classic stability bound , and we also provide arguments why meta-tailoring can help on that term as well . In the general response to all reviewers , we address the confusion regarding the contrastive experiments and add more details on the theoretical guarantees . There , we also discuss new results on model-based RL . * * Empirial validation * * : please see the general response where we detail new baselines with related work and a totally new application in model-based RL , added on a new section 5.3 in the main text ."}, "2": {"review_id": "w8iCTOJvyD-2", "review_text": "This paper presents tailoring and meta-tailoring to eliminate the generalization gap by optimizing at test time . This paper combines the idea of test time optimization and meta-learning and provides some theoretical analysis of the proposed methods . Experiments show the proposed method is effective in solving the machine learning problem and improving model robustness . Strengths : - The idea of using meta-learning to improve tailoring is interesting . Some theoretical justification for the proposed method is provided . - The explanation of different learning settings is insightful . Weaknesses : - The proposed tailoring method is very close to Test Time Training ( Sun et al. , 2019 ) .However , this paper fails to clearly show the differences between tailoring and TTT . There is also no theoretical or empirical evidence to show that tailoring/meta-tailoring is better than TTT . - The experiments in this paper are quite weak . The descriptions of the experiment settings are unclear . There is no direct comparison with closely related methods like TTT . The results of widely used benchmarks are not provided . Overall , although I think the idea is interesting , the proposed method is not well verified , which makes the effectiveness of the method is still unclear . Some closely related work is not sufficiently discussed . Therefore , I lean to recommend rejection for this paper .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments . * * Standard benchmarks on adversarial examples and model-based RL * * `` The results of widely used benchmarks are not provided '' As you point out , the physics experiment is not a standard benchmark . We chose it for instructive purposes and to show potential useful applications of our method in science . However , the adversarial experiments build on arguably the most standard certifiable defense ( Randomized Smoothing ) and evaluate on the two most important benchmarks for adversarial examples : CIFAR-10 & Imagenet . For conciseness ( and because we believe the value of the paper lies in its novelty and general applicability rather than matching state-of-the-art ) we put the tables including the SOA methods in the appendix . We also mentioned both works in the penultimate paragraph of section 5.2 ( Zhai et al. , Salman et al . ) . There , we detail that our approach matches SOA on ImageNet ( not CIFAR-10 ) despite our framework not being specialized to adversarial examples and that meta-tailoring could also potentially improve the other approaches . In the general response we detail new experiments improving on PDDM ( Nagabandi et al . ) , a popular method , on a complex MuJoCo benchmark , with results added to section 5.3 in the main text . We believe these further show the wide range of applicability of meta-tailoring . * * Extra experiments comparing to TTT * * `` There is no direct comparison with closely related methods like TTT . '' In the general answer to all reviewers we discuss differences from test time training ( Sun et al . ) , and show that it is a sub-optimal approach for same-distribution generalization by providing extra experiments . We hope this addresses your concerns , both in terms of novelty and experimental performance ."}, "3": {"review_id": "w8iCTOJvyD-3", "review_text": "= Summary This paper proposes a meta-tailoring method where the auxiliary tailor loss is used to adapt the model parameters at test time . The paper also provides a theoretical analysis of the advantages of the proposed tailoring/meta-tailoring . The proposed method is evaluated on various application tasks and obtain significant improvements over baselines . = Pros 1.Test-time training is getting increasing attention recently , however , the theoretical analysis of the advantages of test time training is still behind . This paper provides a reasonable theoretical analysis of tailoring and meta-tailoring . 2.The proposed CNGrad approach is simple but effective , which provides strong empirical improvements on various tasks . 3.The paper is well written and clearly organized . == Concerns/confusions 1. what is the formulation of the affine parameters ? Is there any study about adapting the entire model versus the affine parameters only ? 2.Missing contrastive learning experiments on images ? In the last sentence of the abstract , it says `` ... , and using contrastive losses on the query image to improve generalization '' . However , I did n't find a discussion on it in the main paper . Given that previous work ( e.g. , Sun et al . ) has done experiments on image classification tasks , it would be more compelling to provide a comparison in the same setting . In general , this paper proposes an interesting approach and insightful analysis of the proposed method . I 'm willing to upgrade my score with comparable experiments with prior work on the image classification task .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your review . * * \u201c What is the formulation of the affine parameters ? Is there any study about adapting the entire model versus the affine parameters ? \u201d * * - Inspired by conditional normalization , our method CNGrad adds affine layers $ \\vec { y } = \\vec { \\gamma } \\cdot \\vec { x } + \\vec { \\beta } $ and only optimizes these parameters in the inner loop . These layers can be added in every hidden layer of most deep architectures . - The closest method that we know is CAVIA ( Zintgraf et al . ) , which makes a neural network predict these affine parameters from a trainable vector . Then , instead of directly optimizing the affine parameters , they optimize the trainable vector . We show this extra complexity is unnecessary . - In theorem 2 of section 4 we prove that optimizing the affine parameters alone has the same capacity as optimizing the entire network . This theorem holds for a wide variety of losses and realistic network requirements , but requires evaluating the loss only at a finite number of data-points . As a result , the affine parameters have enough capacity for the inner loop ( which uses a finite number of samples ) , but not for the outer loop ( where we optimize all parameters to express arbitrary functions ) . * * Comparison to test-time training , missing contrastive results and new model-based RL results : * * We address these points in the general answer , where we also explain a new application of meta-tailoring to model-based RL , added in section 5.3 in the main text ."}}