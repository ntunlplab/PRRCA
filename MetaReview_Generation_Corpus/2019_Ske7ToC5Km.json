{"year": "2019", "forum": "Ske7ToC5Km", "title": "Graph2Seq: Scalable Learning Dynamics for Graphs", "decision": "Reject", "meta_review": "This was an extremely difficult case. There are many positive aspects of Graph2Seq, as detailed by all of the reviewers, however two of the reviewers have issue with the current theory, specifically the definition of k-local-gather and its relation to existing models. The authors  and reviewers have had a detailed and discussion on the issue, however we do not seem to have come to a resolution. I will not wade into the specifics of the argument, however, ultimately, the onus is on the authors to convince the reviewers of the merits/correctness, and in this case two reviewers had the same issue, and their concerns have not been resolved. The best advice I can give is to consider the discussion so far and why this misunderstanding occurred, so that it might lead the best version of this paper possible.", "reviews": [{"review_id": "Ske7ToC5Km-0", "review_text": "Graph representation techniques are important as various applications require learning over graph-structured data. The authors proposed a novel method to embedding a graph as a vector. Compared to Graph Convolutions Neural Networks (GCNN), the proposed are able to handle directed graphs while GCNN can not. Overall the paper is good, the derivation and theory are solid. The authors managed to prove the proposed representation is somehow lossless, which is very nice. The experiment is also convincing. My only concern is as follows. The authors claim that Eq. (1) is able to handle features on vertices or edges. However, in the current formulation, the evolution only depends on vertex features, thus how can it handle edge features?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the helpful comments . Including edge features : There are a few different ways to include edge features . One way would be to include a second term $ \\sum_ { e\\in\\eta ( v ) } y_e ( t ) $ , where $ \\eta ( v ) $ are edges incident to node v and y_e are edge features of edge e , inside the ReLU function of Equation 1 . Another way is to transform the graph with edge features into a new ( larger ) graph where there are no edge features . This is done by converting the original graph into a new bipartite graph where one partite corresponds to vertices of the original graph , and the other partite corresponds to edges of the original graph . Each edge-node in the bipartite graph is connected to the two vertex-nodes that constitute its end points in the original graph . The edge-nodes have the edge features of the original graph , while the vertex-nodes have the vertex features . We will explain this in the revision ."}, {"review_id": "Ske7ToC5Km-1", "review_text": "This paper proposes a new representation learning model for graph optimization, Graph2Seq. The novelty of Graph2Seq lies in utilizing intermediate vector representation of vertices in the final representation. Theoretically, the authors show that an infinite sequence of such intermediate representations is much more powerful than existing models, which do not maintain intermediate representations. Experimentally, Graph2Seq results in greedy heuristics that generalize very well from small training graphs (e.g. 15 nodes) to large testing graphs (e.g. 3200 nodes). Overall, the current version of the paper raises a number of crucial questions that I would like the authors to address before I make my decision. First, some strengths of the paper: - Theory: although I have not reviewed the proofs in details, the theorems are very interesting. If correct, the theorems provide a strong basis for Graph2Seq. In contrast, this aspect is missing from other work on ML for optimization. - Experiments: the experiments are generally thorough and well-presented. The performance of Graph2Seq is remarkable, especially in terms of generalization to significantly larger graphs. - Writing: the paper is very well-written and complex ideas are neatly articulated. I also liked the Appendix trying to interpret the trained model. Good job! That being said, I have some serious concerns. Please clarify if I misunderstood anything and update the paper otherwise. - Graph2Seq at test time: in section Testing, you explain how multiple solutions are output by G2S-RNN at intermediate \"states\" of the model, and the best w.r.t. the objective value is returned. If I understand all this correctly, you take the output of the T-th LSTM unit, run it through the Q-network, then select the next node (e.g. in a vertex cover solution). Then, the complexity should be O((E+V)*T_max*V), since the Graph2Seq operations are linear in the size of the graph O(E+V), a single G2S-RNN(i) takes O(V) times if you want to construct a cover of size O(V), and you repeat that process exactly T_max times, for each i between 1 and T_max. What's wrong in my understanding of G2S-RNN here? Or is your complexity incorrect? - Local-Gather definition: in your definition of the Local-Gather model, do you assume that computations are performed for a single iteration, i.e. a single local step followed by a gather step? If so, then how is Graph2Seq infinity-local-gather? What does that even mean? I understand how some of the other GCNN-based models like Khalil et al.'s is 4-local-gather (assuming 4 embedding iterations of structure2vec), but how is Graph2Seq infinity-local-gather? - Comparison to Structure2Vec: for fair comparison, why not apply Algorithm 2 to that method? Just run more embedding iterations up to T_max, and use the best among the solutions constructed between 1 and T_max. Minor: - Section 4: Vinyals et al. (2015) does not do any RL. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the helpful comments . 1.Time complexity : Thank you for pointing out the complexity . It is as follows : ( a ) The time-complexity for one forward pass of G2S-RNN ( e.g. , to select one vertex in minimum vertex cover ) is O ( ( E + V ) T_max ) . This is because during each step of Graph2Seq , O ( E ) operations are required to update the state at each vertex based on neighbors ' states , and O ( V ) operations are required by the GRU to aggregate the states of all vertices ( Equation 19 in appendix ) . Since these operations have to be repeated at each step , and there are at most T_max steps , the time-complexity is O ( ( E + V ) T_max ) . ( b ) For a fixed number of steps T , the time-complexity to compute a complete solution ( e.g. , to select multiple vertices such that they form a valid vertex cover ) is O ( ( E + V ) T_max * V ) . This is because selecting one vertex has complexity O ( ( E + V ) T_max ) , and we may have to select O ( V ) vertices to obtain a valid solution to the input graph . ( c ) The overall time-complexity is O ( ( E + V ) T_max * V * T_max ) . This is because the final solution is computed by first computing valid solutions for each T=1,2 , .. , T_max , and then picking the best valid solution from among them . Computing a valid solution for a fixed T takes O ( ( E + V ) T_max * V ) as mentioned above , and we have to repeat the process T_max times . Note that aggregating states from all the vertices in the GRU is a hyperparameter choice . If only local neighborhood states are used in the GRU , the time-complexity in step ( a ) above becomes O ( ET_max + V ) . We will clarify the complexity in Section 5 . 2.Local Gather definition : Yes , the definition of local-gather consists of one local step followed by one gather step . An algorithm is k-local-gather if in the local step , each vertex computes an embedding based on the k-hop neighborhood graph around the vertex . Structure2Vec is 4-local-gather because the four embedding iterations cause each node 's embedding to depend on its 4-hop neighborhood . Graph2Seq is infinity-local-gather since the infinite number of embedding iterations cause each node 's embedding to depend on the entire graph -- -not just on vertices a constant number of hops away from the node . Infinity is used to emphasize that the local graph neighborhoods on which the node embeddings depend are not constrained in size . For a specific graph G with diameter dia ( G ) , it is also true that Graph2Seq is dia ( G ) -local-gather . We will include a remark to explain that infinity-local-gather means that a node 's embedding can depend on the entire graph , regardless of the graph size . 3.Comparison to Structure2Vec : We will include an experiment that applies algorithm 2 to Structure2Vec in the paper . Note , however , that applying this procedure to Structure2Vec implicitly uses the sequence of Structure2Vec embeddings as the embedding for each vertex . Therefore , this method is a different instantiation of our idea of using sequences for node embeddings . In particular , like Graph2Seq , this method will also consider neighborhoods of different sizes around each vertex for different graphs . The only difference is that Graph2Seq additionally uses an LSTM to process the sequence . Therefore , we indeed expect that the combination of Algorithm 2 with Structure2Vec will also perform well ."}, {"review_id": "Ske7ToC5Km-2", "review_text": "The authors propose a method for learning vector representations for graphs. The problem is relevant to the ICLR community. The paper has, however, three major problems: The motivation of the paper is somewhat lacking. I agree that learning representations for graphs is a very important research theme. However, the authors miss to motivate their specific approach. They mention the importance of learning on smaller graphs and applying the learned models to larger graphs (i.e., extrapolating better). I would encourage the authors to elaborate on some use cases where this is important. I cannot think of any at the moment. I assume the authors had use cases in combinatorial optimization in mind? Perhaps it might make sense to motivate the use of GNNs to solve vertex cover etc. I\u2019m not sure about the correctness of some of the theorems. For instance, Theorem 2 states \u201cFor any fixed k > 0, there exists a function f(\u00b7) and an input graph instance G such that no k-LOCAL-GATHER algorithm can compute f(G) exactly.\u201d I\u2019m not claiming that this is a false statement. What I am suspecting at the moment is that the proof might not necessarily be correct. For instance, it is known that what you call 1-LOCAL-GATHER can compute the 1-Weisfeiler-Leman partition of the nodes (sometimes also referred to as the 1-WL node coloring). Now consider the chain graph 1 - 2 - 3 - 4 - 5. Here, the partition that puts together 1-WL indistinguishable nodes are {1, 5}, {2, 4} and {3}. Hence, the 1-WL coloring is able to distinguish say nodes 2 and 3 even their 1-neighborhood looks exactly the same. A similar argument might apply to your example pairs of graphs but I haven\u2019t checked it yet in detail. What is for sure though: what you provide in the appendix is not a proper formal proof of Theorem 2. This has to be fixed. The experiments are insufficient. The authors should compare to existing methods on common benchmark problems such as node or graph classification datasets. Comparing to baselines on a new set of task is not enough. Why not compare your method also on existing datasets? If you motivate your method as one that performs well on combinatorial problems (e.g., vertex cover) you should compare to existing deterministic solvers. I assume that these are often much faster at least on smaller graphs. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the helpful comments . 1.Motivation : Our primary motivation is learning algorithms for combinatorial optimization on graphs . In many practical applications , it is desirable to learn an algorithm on smaller graphs that can generalize to larger graphs . For example , Mirrhosseini et al . [ 1 ] consider the problem of deciding how to optimally assign TensorFlow ops to GPUs to minimize runtime . Since directly training placement policies on large TensorFlow graphs can be extremely slow , it would be beneficial if a model can be trained on small TensorFlow graphs in a way that generalizes to large TensorFlow graphs . Another example is query optimization in databases , where the optimal order of join operations in the query plan tree has to be determined [ 2 ] . Since evaluating complex queries with large query plans can be expensive , it is again helpful if the learning algorithm can be trained on simple queries in a way that generalizes to complex queries . We will modify the introduction to emphasize these use cases . 2.Theorem 2 : The chain graph 1-2-3-4-5 results in the partitions { 1 , 5 } , { 2 , 4 } and { 3 } after the 1-hop WL algorithm if the initial node labels are chosen as their respective degrees . Since the degree label already includes one-hop information , this means , overall it is a 2-local-gather algorithm and not a 1-local-gather algorithm . If the initial node labels are chosen identically for all nodes , then the partitions are { 1 , 5 } , { 2 , 3 , 4 } . We would appreciate clarification on what parts of the proof of Theorem 2 are unclear or informal . 3.Baselines : As our focus is on combinatorial optimization problems , comparing on benchmark node or graph classification datasets is outside the scope of this paper and is an important future research direction . We have compared Graph2Seq to existing deterministic solvers ( Gurobi ) , heuristics ( list ) , approximation algorithms ( matching , greedy ) and a range of graph neural networks . Note that the performance plots for the Gurobi solver is implicit in the plots ( e.g. , Figure 2 and 3 ) since the approximation ratio for all other schemes have been computed relative to the Gurobi solver . The plots corresponding to the list heuristic ( brown ) , matching algorithm ( green ) and greedy heuristic ( yellow ) are explicitly shown in Figures 2 and 3 . [ 1 ] Device placement optimization with reinforcement learning , Mirhoseini et al , 2017 [ 2 ] Learning to optimize join queries with deep reinforcement learning , Krishnan et al , 2018"}], "0": {"review_id": "Ske7ToC5Km-0", "review_text": "Graph representation techniques are important as various applications require learning over graph-structured data. The authors proposed a novel method to embedding a graph as a vector. Compared to Graph Convolutions Neural Networks (GCNN), the proposed are able to handle directed graphs while GCNN can not. Overall the paper is good, the derivation and theory are solid. The authors managed to prove the proposed representation is somehow lossless, which is very nice. The experiment is also convincing. My only concern is as follows. The authors claim that Eq. (1) is able to handle features on vertices or edges. However, in the current formulation, the evolution only depends on vertex features, thus how can it handle edge features?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the helpful comments . Including edge features : There are a few different ways to include edge features . One way would be to include a second term $ \\sum_ { e\\in\\eta ( v ) } y_e ( t ) $ , where $ \\eta ( v ) $ are edges incident to node v and y_e are edge features of edge e , inside the ReLU function of Equation 1 . Another way is to transform the graph with edge features into a new ( larger ) graph where there are no edge features . This is done by converting the original graph into a new bipartite graph where one partite corresponds to vertices of the original graph , and the other partite corresponds to edges of the original graph . Each edge-node in the bipartite graph is connected to the two vertex-nodes that constitute its end points in the original graph . The edge-nodes have the edge features of the original graph , while the vertex-nodes have the vertex features . We will explain this in the revision ."}, "1": {"review_id": "Ske7ToC5Km-1", "review_text": "This paper proposes a new representation learning model for graph optimization, Graph2Seq. The novelty of Graph2Seq lies in utilizing intermediate vector representation of vertices in the final representation. Theoretically, the authors show that an infinite sequence of such intermediate representations is much more powerful than existing models, which do not maintain intermediate representations. Experimentally, Graph2Seq results in greedy heuristics that generalize very well from small training graphs (e.g. 15 nodes) to large testing graphs (e.g. 3200 nodes). Overall, the current version of the paper raises a number of crucial questions that I would like the authors to address before I make my decision. First, some strengths of the paper: - Theory: although I have not reviewed the proofs in details, the theorems are very interesting. If correct, the theorems provide a strong basis for Graph2Seq. In contrast, this aspect is missing from other work on ML for optimization. - Experiments: the experiments are generally thorough and well-presented. The performance of Graph2Seq is remarkable, especially in terms of generalization to significantly larger graphs. - Writing: the paper is very well-written and complex ideas are neatly articulated. I also liked the Appendix trying to interpret the trained model. Good job! That being said, I have some serious concerns. Please clarify if I misunderstood anything and update the paper otherwise. - Graph2Seq at test time: in section Testing, you explain how multiple solutions are output by G2S-RNN at intermediate \"states\" of the model, and the best w.r.t. the objective value is returned. If I understand all this correctly, you take the output of the T-th LSTM unit, run it through the Q-network, then select the next node (e.g. in a vertex cover solution). Then, the complexity should be O((E+V)*T_max*V), since the Graph2Seq operations are linear in the size of the graph O(E+V), a single G2S-RNN(i) takes O(V) times if you want to construct a cover of size O(V), and you repeat that process exactly T_max times, for each i between 1 and T_max. What's wrong in my understanding of G2S-RNN here? Or is your complexity incorrect? - Local-Gather definition: in your definition of the Local-Gather model, do you assume that computations are performed for a single iteration, i.e. a single local step followed by a gather step? If so, then how is Graph2Seq infinity-local-gather? What does that even mean? I understand how some of the other GCNN-based models like Khalil et al.'s is 4-local-gather (assuming 4 embedding iterations of structure2vec), but how is Graph2Seq infinity-local-gather? - Comparison to Structure2Vec: for fair comparison, why not apply Algorithm 2 to that method? Just run more embedding iterations up to T_max, and use the best among the solutions constructed between 1 and T_max. Minor: - Section 4: Vinyals et al. (2015) does not do any RL. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the helpful comments . 1.Time complexity : Thank you for pointing out the complexity . It is as follows : ( a ) The time-complexity for one forward pass of G2S-RNN ( e.g. , to select one vertex in minimum vertex cover ) is O ( ( E + V ) T_max ) . This is because during each step of Graph2Seq , O ( E ) operations are required to update the state at each vertex based on neighbors ' states , and O ( V ) operations are required by the GRU to aggregate the states of all vertices ( Equation 19 in appendix ) . Since these operations have to be repeated at each step , and there are at most T_max steps , the time-complexity is O ( ( E + V ) T_max ) . ( b ) For a fixed number of steps T , the time-complexity to compute a complete solution ( e.g. , to select multiple vertices such that they form a valid vertex cover ) is O ( ( E + V ) T_max * V ) . This is because selecting one vertex has complexity O ( ( E + V ) T_max ) , and we may have to select O ( V ) vertices to obtain a valid solution to the input graph . ( c ) The overall time-complexity is O ( ( E + V ) T_max * V * T_max ) . This is because the final solution is computed by first computing valid solutions for each T=1,2 , .. , T_max , and then picking the best valid solution from among them . Computing a valid solution for a fixed T takes O ( ( E + V ) T_max * V ) as mentioned above , and we have to repeat the process T_max times . Note that aggregating states from all the vertices in the GRU is a hyperparameter choice . If only local neighborhood states are used in the GRU , the time-complexity in step ( a ) above becomes O ( ET_max + V ) . We will clarify the complexity in Section 5 . 2.Local Gather definition : Yes , the definition of local-gather consists of one local step followed by one gather step . An algorithm is k-local-gather if in the local step , each vertex computes an embedding based on the k-hop neighborhood graph around the vertex . Structure2Vec is 4-local-gather because the four embedding iterations cause each node 's embedding to depend on its 4-hop neighborhood . Graph2Seq is infinity-local-gather since the infinite number of embedding iterations cause each node 's embedding to depend on the entire graph -- -not just on vertices a constant number of hops away from the node . Infinity is used to emphasize that the local graph neighborhoods on which the node embeddings depend are not constrained in size . For a specific graph G with diameter dia ( G ) , it is also true that Graph2Seq is dia ( G ) -local-gather . We will include a remark to explain that infinity-local-gather means that a node 's embedding can depend on the entire graph , regardless of the graph size . 3.Comparison to Structure2Vec : We will include an experiment that applies algorithm 2 to Structure2Vec in the paper . Note , however , that applying this procedure to Structure2Vec implicitly uses the sequence of Structure2Vec embeddings as the embedding for each vertex . Therefore , this method is a different instantiation of our idea of using sequences for node embeddings . In particular , like Graph2Seq , this method will also consider neighborhoods of different sizes around each vertex for different graphs . The only difference is that Graph2Seq additionally uses an LSTM to process the sequence . Therefore , we indeed expect that the combination of Algorithm 2 with Structure2Vec will also perform well ."}, "2": {"review_id": "Ske7ToC5Km-2", "review_text": "The authors propose a method for learning vector representations for graphs. The problem is relevant to the ICLR community. The paper has, however, three major problems: The motivation of the paper is somewhat lacking. I agree that learning representations for graphs is a very important research theme. However, the authors miss to motivate their specific approach. They mention the importance of learning on smaller graphs and applying the learned models to larger graphs (i.e., extrapolating better). I would encourage the authors to elaborate on some use cases where this is important. I cannot think of any at the moment. I assume the authors had use cases in combinatorial optimization in mind? Perhaps it might make sense to motivate the use of GNNs to solve vertex cover etc. I\u2019m not sure about the correctness of some of the theorems. For instance, Theorem 2 states \u201cFor any fixed k > 0, there exists a function f(\u00b7) and an input graph instance G such that no k-LOCAL-GATHER algorithm can compute f(G) exactly.\u201d I\u2019m not claiming that this is a false statement. What I am suspecting at the moment is that the proof might not necessarily be correct. For instance, it is known that what you call 1-LOCAL-GATHER can compute the 1-Weisfeiler-Leman partition of the nodes (sometimes also referred to as the 1-WL node coloring). Now consider the chain graph 1 - 2 - 3 - 4 - 5. Here, the partition that puts together 1-WL indistinguishable nodes are {1, 5}, {2, 4} and {3}. Hence, the 1-WL coloring is able to distinguish say nodes 2 and 3 even their 1-neighborhood looks exactly the same. A similar argument might apply to your example pairs of graphs but I haven\u2019t checked it yet in detail. What is for sure though: what you provide in the appendix is not a proper formal proof of Theorem 2. This has to be fixed. The experiments are insufficient. The authors should compare to existing methods on common benchmark problems such as node or graph classification datasets. Comparing to baselines on a new set of task is not enough. Why not compare your method also on existing datasets? If you motivate your method as one that performs well on combinatorial problems (e.g., vertex cover) you should compare to existing deterministic solvers. I assume that these are often much faster at least on smaller graphs. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the helpful comments . 1.Motivation : Our primary motivation is learning algorithms for combinatorial optimization on graphs . In many practical applications , it is desirable to learn an algorithm on smaller graphs that can generalize to larger graphs . For example , Mirrhosseini et al . [ 1 ] consider the problem of deciding how to optimally assign TensorFlow ops to GPUs to minimize runtime . Since directly training placement policies on large TensorFlow graphs can be extremely slow , it would be beneficial if a model can be trained on small TensorFlow graphs in a way that generalizes to large TensorFlow graphs . Another example is query optimization in databases , where the optimal order of join operations in the query plan tree has to be determined [ 2 ] . Since evaluating complex queries with large query plans can be expensive , it is again helpful if the learning algorithm can be trained on simple queries in a way that generalizes to complex queries . We will modify the introduction to emphasize these use cases . 2.Theorem 2 : The chain graph 1-2-3-4-5 results in the partitions { 1 , 5 } , { 2 , 4 } and { 3 } after the 1-hop WL algorithm if the initial node labels are chosen as their respective degrees . Since the degree label already includes one-hop information , this means , overall it is a 2-local-gather algorithm and not a 1-local-gather algorithm . If the initial node labels are chosen identically for all nodes , then the partitions are { 1 , 5 } , { 2 , 3 , 4 } . We would appreciate clarification on what parts of the proof of Theorem 2 are unclear or informal . 3.Baselines : As our focus is on combinatorial optimization problems , comparing on benchmark node or graph classification datasets is outside the scope of this paper and is an important future research direction . We have compared Graph2Seq to existing deterministic solvers ( Gurobi ) , heuristics ( list ) , approximation algorithms ( matching , greedy ) and a range of graph neural networks . Note that the performance plots for the Gurobi solver is implicit in the plots ( e.g. , Figure 2 and 3 ) since the approximation ratio for all other schemes have been computed relative to the Gurobi solver . The plots corresponding to the list heuristic ( brown ) , matching algorithm ( green ) and greedy heuristic ( yellow ) are explicitly shown in Figures 2 and 3 . [ 1 ] Device placement optimization with reinforcement learning , Mirhoseini et al , 2017 [ 2 ] Learning to optimize join queries with deep reinforcement learning , Krishnan et al , 2018"}}