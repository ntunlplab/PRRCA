{"year": "2020", "forum": "r1erNxBtwr", "title": "Demystifying Graph Neural Network Via Graph Filter Assessment", "decision": "Reject", "meta_review": "The paper investigates graph convolutional filters, and proposes an adaptation of the Fisher score to assess the quality of a convolutional filter. Formally, the defined Graph Filter Discriminant Score assesses how the filter improves the Fisher score attached to a pair of classes (considering the nodes in each class, and their embedding through the filter and the graph structure, as propositional samples), taking into account the class imbalance.\n\nAn analysis is conducted on synthetic graphs to assess how the hyper-parameters (order, normalization strategy) of the filter rule the GFD score depending on the graph and class features. As could have been expected there no single killer filter.\n\nA finite set of filters, called base filters, being defined by varying the above hyper-parameters, the search space is that of a linear combination of the base filters in each layer. Three losses are considered: with and without graph filter discriminant score, and alternatively optimizing the cross-entropy loss and the GFD; this last option is the best one in the experiments.\n\nAs noted by the reviewers and other public comments, the idea of incorporating LDA ideas into GNN is nice and elegant. The reservations of the reviewers are mostly related to the experimental validation: of course getting the best score on each dataset is not expected; but the set of considered problems is too limited and their diversity is limited too (as demonstrated by the very nice Fig. 5).\n\nThe area chair thus encourages the authors to pursue this very promising line of research and hopes to see a revised version backed up with more experimental evidence. ", "reviews": [{"review_id": "r1erNxBtwr-0", "review_text": "This is a very interesting study about GNN. Authors proposed to extend LDA as a discrimination evaluator for graph filleters. Also authors proposed Adaptive Filter Graph Neural Network to find the optimal filter within a limited family of graph convolutional filters. The whole study is novel, and beneficial for the community of graph neural network study. It provides a new way to understand and evaluate GNN. There are some questions authors should clarify. And some writing errors to correct. Eq (3) defines GFD for a pair of classes i and j. For a graph with more than two classes, the GFD will be the average of all pairs? Will class imbalance will have any impact on this GFD measure? Errors: \u2022 we studies the roles of this two components \u2022 there exist a best choice \u2022 we only consider to to find ", "rating": "8: Accept", "reply_text": "Thank you so much for the positive feedback ! We really appreciate your support for our paper as well as your constructive suggestions . We have improved our paper based on your advice ( we marked the modifications related to your suggestions with blue text , and highlighted the previous version with strikethrough ) : https : //drive.google.com/file/d/1wJYwz1oPDK1-NbpesHUR6ZMCSBRVxdh3/view ? usp=sharing For Eq . ( 3 ) : to get the GFD score for a multi-class setting , we first calculate the Fisher Difference for each possible pair of classes , then normalize them based on class size , and finally sum them together to get GFD score . Based on the normalization , the class imbalance would not be a problem . We have improved our writing to make this part more clear . For the writing errors , thank you for pointing them out . We have corrected all the errors pointed out by you and also have carefully gone through the paper to improve the writing ."}, {"review_id": "r1erNxBtwr-1", "review_text": "In this paper, the authors raise and address three questions about graph neural network: (1) Whether there is a best filter that works for all graphs. (2) Which properties of the graph will influence the performance of the graph filter. (3) How to design a method to adaptively find the optimal filter for a given graph. The paper proposes an assessment method called the Graph Filter Discriminant Score based on the Fisher Score. It measures how well the graph convolutional filter discriminate node representations of different classes in the graph by comparing the Fisher score before and after the filter. Based on the GFD scores of different normalization strategy and different order of the graph filter in the experiments on synthetic data, the authors answer the first two questions: (1) There is no optimal normalization for all graphs. (2) row normalization performs better with lower power-law coefficient, but works worse with imbalanced label classes and large density gap. For the third question, the authors propose a learnable linear combination of a limited family of graph convolutional filters as the layer of model AFGNN, which can learn the optimal arguments of the combination based on the FGD score. The paper focuses on a significant topic and proposes an assessment tool for the graph filters. Based on that, it also introduces a model to choose filters from a family of filters for any specific graph. The description of preliminaries is clear. The observations of the impact of the graph properties on the filter choice are interesting and explanations are provided. The results of the test accuracy on both bench mark and synthetic datasets demonstrate the good performance of the proposed model. It is good that the paper provides proof for the claim that the graph convolutional can help the non-linear separable data to be linearly separable, so it is reasonable to use Fisher score. However, does this claim support the second term in equation (3), where the Fisher score is used to evaluate before the filters applied? The presentation of the last paragraph of \u201cgraph filter discriminant score\u201d in page 4 can be improved. The Figure references seem incorrect and confusing. The analysis of the influence of label ratio seems not accurate enough. For the GFD score comparison in Figure 4, why choose order 1,3,7 for density and different order 2,3,6 for density gap? What is the meaning of the symbol psi(l)? It would be better if the explanation of the raining loss section is more detailed and clear. What is \u201cAFGNN_P\u201d in the experiment analysis? It could be interesting to see the comparison of time between the proposed method and the GAT. For the graph filter discriminate analysis, is it fair to compare the learned layer with the other base filter using the GFD score? Since the learned layer is picked with highest GFD score. Maybe one or two sentences on this will be helpful. The writing of the paper must be improved. Too many typos and grammar problems will impair the presentation and the reader can be distracted. Minor comments: The layout of the sub caption of Figure 1 can be improved. The usage of capital letter in the phrase \u201cdensity gap\u201d is inconsistent. \u201cAs shown in figure\u201d instead of \u201cAs is shown in figure\u201d. Many sentences miss article. There are many typos in the writing. For example, \u201cNote that for given (feature)\u201d, \u201c\u2026make the representation of nodes in different (class) more separable.\u201d, \u201cNoted that there are some other (variant) of GNN filters that (does) not fall into\u2026\u201d in page 4. \u201cHere we give (a) empirical explanation to this phenomenon\u201d, \u201cthis normalization strategy (take) into account\u2026\u201d, \u201cThus even in the case that the other two (doesn\u2019t) perform well\u2026\u201d in page 5. \u201c\u2026a very important factor that (influence) the choice of normalization strategy\u201d, \u201cwhen power-law coefficient (decrease)\u201d, \u201cwhen the (sizes) of each class (become) more imbalanced\u201d, \u201cThis is because column normalization better (leverage) \u2026\u201d , \u201cin a similar manner (with) label ratio\u201d, \u201cwhen the density or density gap (increase)\u201d, \u201chigh-order filters can help gather\u2026 and thus (makes) the representations\u2026\u201d, \u201cwhen the density gap (increase)\u201d in page 6. These can be continued but it is obvious that this paper needs proofreading. ", "rating": "1: Reject", "reply_text": "Thank you for your constructive comments ! We improved our paper based on your advice ( we marked the modifications related to your suggestions with red text , and highlighted the previous version with strikethrough ) : https : //drive.google.com/file/d/1adtmKH61RLyBKzn46DWdEj5JQvu5M5WO/view ? usp=sharing First , we want to emphasize this paper \u2019 s contribution . Our work provides a theoretical understanding to GNNs in a novel way , and we are the first to analyze GNNs for the node classification task from a data perspective . Since rich literature demonstrated that the key of GNNs lies in their graph convolutional filters , we propose a new assessment tool ( GFD ) to evaluate the effectiveness of filters given a specific graph data Further , this tool is applied to analyze existing filters and found some meaningful insights . Finally , we propose the AFGNN model to automatically learn the best filter from the given family ( i.e. , learn the best coefficients for the linear combination of a set of filters ) for the given graph data . Now we address your comments and concerns in detail : Q1 : Is it reasonable to use the Fisher score \u201d to support the second term in equation ( 3 ) , where the Fisher score is used to evaluate before the filters applied ? A1 : We \u2019 d like to clarify our GFD is an assessment tool to see whether a filter is good for a particular graph data . Fisher Score is used to evaluate the separability of data . GFD defined in Eq . ( 3 ) , which is the Difference of Fisher Score before and after the filter , is to evaluate whether a filter can increase the data separability . As we show that a good graph convolutional filter can help the non-linear separable data to be linearly separable , we can expect the GFD score is higher for these filters . Note that not every graph filter has this property for every dataset , and our final model is to learn the best filter that could enhance this property for a given dataset . Experimental results in Figure 5 and Table 2 also support this claim . Q2 : The presentation of the last paragraph of \u201c graph filter discriminant score \u201d in page 4 can be improved . The Figure references seem incorrect and confusing . A2 : As mentioned in Q1 , we use Fisher score to evaluate the separability of two classes , we use Fisher Score Difference to evaluate the power of a filter on two classes , and we use GFD , which is a weighted sum of Fisher Score Difference of each pair of classes , to evaluate the power of a filter on the given graph . We have revised our writing and corrected our Figure reference to make this more clear . Q3 : The analysis of the influence of label ratio seems not accurate enough . A3 : Suppose the density and density gap are fixed , when label ratio drops , which means the two classes become more imbalanced , and nodes in a larger class tend to have more neighbors . Then , with the column normalization strategy that does not have any constraint on the range of representation , those nodes with a larger degree tend to aggregate more information and thus have larger new representations . This would be helpful to differentiate the two classes . Take Figure 6 ( g ) as an example , nodes in the large-size class ( green nodes ) are gathered in the upper right part while nodes in the small-size class ( purple nodes ) are gathered in the lower left part , so the two classes become more separable after applying a column-normalized filter . We revised our writing to make this more clear . Q4 : For the GFD score comparison in Figure 4 , why choose order 1,3,7 for density and different order 2,3,6 for the density gap ? A4 : Thanks for pointing out . Previously we just pick the orders that can show our findings most clearly , but we agree it is important to use consistent choice in two subfigures . We now choose the same set of orders in these two figures . The result remains the same . Q5 : What is the meaning of the symbol psi ( l ) ? A5 : It is a learnable intermediate weight ( before normalization ) for each base filter . We then apply softmax normalization to it to get alpha ( l ) . We revised our writing to make this more clear ."}, {"review_id": "r1erNxBtwr-2", "review_text": "This paper introduces an assessment framework for an in-depth analysis of the effect of graph convolutional filters and proposes a novel graph neural network with adaptable filters based on the analysis. The assessment framework builts on the Fisher discriminant score of features and can also be used as an additional (regularization) term for choosing optimal filters in training. The assessment result shows that there is no single graph filter for all types of graph structures. Experiments on both synthetic and real-world benchmark datasets demonstrate that the proposed adaptive GNN can learn appropriate filters for different graph tasks. The proposed analysis using the Fisher score is reasonable and interesting, giving us an insight into the role of graph filters. Even though the analysis is limited (using simple graph models and filter family) and the result is not surprising (given no free lunch theorem, there is very likely to be no single silver bullet fo graph filters), I appreciate the analysis and the result. But, I have some concerns as follows. 1) The proposed GNN and the optimization process The proposed method is to extend CNN to a simple linear combination of different filter bases with learnable weights, which I don't think is very novel. Adding the GFD score as an additional constraint term is interesting, but the way of optimizing the whole objective function is unclear. (In addition, I think calling it the \"regularization term\" is inadequate since the term actually involves data observation, rather than a prior on parameters only.) In the case of AFGNN_inf, I don't think it is equivalent to applying infinite lamda. If lamda is infinite, L_CE needs to be completely ignored. This needs to be clarified. In the case of AFGNN1, I don't clearly understand how the whole objective function is properly optimized with fixed data representation. Is it also iteratively optimized? I hope this is also clarified in more detail. 2) Unconvincing experiments The results on three real datasets do not show significant gains, and two of them are even worse than those of GAT. Furthermore, inductive learning (e.g., protein-protein interaction (PPI) dataset used in GAT) is not tested, which I think needs to be also evaluated. While two synthetic datasets (SmallGap and SmallRatio) created by the authors show significant improvement, these datasets appear to be extreme and unrealistic and look carefully selected in favor of the proposed method. I recommend the authors use for evaluation more realistic datasets that can be found in related research. ", "rating": "3: Weak Reject", "reply_text": "We appreciate your valuable comments and we are now actively working on the supplementary experiments on real-world datasets !"}], "0": {"review_id": "r1erNxBtwr-0", "review_text": "This is a very interesting study about GNN. Authors proposed to extend LDA as a discrimination evaluator for graph filleters. Also authors proposed Adaptive Filter Graph Neural Network to find the optimal filter within a limited family of graph convolutional filters. The whole study is novel, and beneficial for the community of graph neural network study. It provides a new way to understand and evaluate GNN. There are some questions authors should clarify. And some writing errors to correct. Eq (3) defines GFD for a pair of classes i and j. For a graph with more than two classes, the GFD will be the average of all pairs? Will class imbalance will have any impact on this GFD measure? Errors: \u2022 we studies the roles of this two components \u2022 there exist a best choice \u2022 we only consider to to find ", "rating": "8: Accept", "reply_text": "Thank you so much for the positive feedback ! We really appreciate your support for our paper as well as your constructive suggestions . We have improved our paper based on your advice ( we marked the modifications related to your suggestions with blue text , and highlighted the previous version with strikethrough ) : https : //drive.google.com/file/d/1wJYwz1oPDK1-NbpesHUR6ZMCSBRVxdh3/view ? usp=sharing For Eq . ( 3 ) : to get the GFD score for a multi-class setting , we first calculate the Fisher Difference for each possible pair of classes , then normalize them based on class size , and finally sum them together to get GFD score . Based on the normalization , the class imbalance would not be a problem . We have improved our writing to make this part more clear . For the writing errors , thank you for pointing them out . We have corrected all the errors pointed out by you and also have carefully gone through the paper to improve the writing ."}, "1": {"review_id": "r1erNxBtwr-1", "review_text": "In this paper, the authors raise and address three questions about graph neural network: (1) Whether there is a best filter that works for all graphs. (2) Which properties of the graph will influence the performance of the graph filter. (3) How to design a method to adaptively find the optimal filter for a given graph. The paper proposes an assessment method called the Graph Filter Discriminant Score based on the Fisher Score. It measures how well the graph convolutional filter discriminate node representations of different classes in the graph by comparing the Fisher score before and after the filter. Based on the GFD scores of different normalization strategy and different order of the graph filter in the experiments on synthetic data, the authors answer the first two questions: (1) There is no optimal normalization for all graphs. (2) row normalization performs better with lower power-law coefficient, but works worse with imbalanced label classes and large density gap. For the third question, the authors propose a learnable linear combination of a limited family of graph convolutional filters as the layer of model AFGNN, which can learn the optimal arguments of the combination based on the FGD score. The paper focuses on a significant topic and proposes an assessment tool for the graph filters. Based on that, it also introduces a model to choose filters from a family of filters for any specific graph. The description of preliminaries is clear. The observations of the impact of the graph properties on the filter choice are interesting and explanations are provided. The results of the test accuracy on both bench mark and synthetic datasets demonstrate the good performance of the proposed model. It is good that the paper provides proof for the claim that the graph convolutional can help the non-linear separable data to be linearly separable, so it is reasonable to use Fisher score. However, does this claim support the second term in equation (3), where the Fisher score is used to evaluate before the filters applied? The presentation of the last paragraph of \u201cgraph filter discriminant score\u201d in page 4 can be improved. The Figure references seem incorrect and confusing. The analysis of the influence of label ratio seems not accurate enough. For the GFD score comparison in Figure 4, why choose order 1,3,7 for density and different order 2,3,6 for density gap? What is the meaning of the symbol psi(l)? It would be better if the explanation of the raining loss section is more detailed and clear. What is \u201cAFGNN_P\u201d in the experiment analysis? It could be interesting to see the comparison of time between the proposed method and the GAT. For the graph filter discriminate analysis, is it fair to compare the learned layer with the other base filter using the GFD score? Since the learned layer is picked with highest GFD score. Maybe one or two sentences on this will be helpful. The writing of the paper must be improved. Too many typos and grammar problems will impair the presentation and the reader can be distracted. Minor comments: The layout of the sub caption of Figure 1 can be improved. The usage of capital letter in the phrase \u201cdensity gap\u201d is inconsistent. \u201cAs shown in figure\u201d instead of \u201cAs is shown in figure\u201d. Many sentences miss article. There are many typos in the writing. For example, \u201cNote that for given (feature)\u201d, \u201c\u2026make the representation of nodes in different (class) more separable.\u201d, \u201cNoted that there are some other (variant) of GNN filters that (does) not fall into\u2026\u201d in page 4. \u201cHere we give (a) empirical explanation to this phenomenon\u201d, \u201cthis normalization strategy (take) into account\u2026\u201d, \u201cThus even in the case that the other two (doesn\u2019t) perform well\u2026\u201d in page 5. \u201c\u2026a very important factor that (influence) the choice of normalization strategy\u201d, \u201cwhen power-law coefficient (decrease)\u201d, \u201cwhen the (sizes) of each class (become) more imbalanced\u201d, \u201cThis is because column normalization better (leverage) \u2026\u201d , \u201cin a similar manner (with) label ratio\u201d, \u201cwhen the density or density gap (increase)\u201d, \u201chigh-order filters can help gather\u2026 and thus (makes) the representations\u2026\u201d, \u201cwhen the density gap (increase)\u201d in page 6. These can be continued but it is obvious that this paper needs proofreading. ", "rating": "1: Reject", "reply_text": "Thank you for your constructive comments ! We improved our paper based on your advice ( we marked the modifications related to your suggestions with red text , and highlighted the previous version with strikethrough ) : https : //drive.google.com/file/d/1adtmKH61RLyBKzn46DWdEj5JQvu5M5WO/view ? usp=sharing First , we want to emphasize this paper \u2019 s contribution . Our work provides a theoretical understanding to GNNs in a novel way , and we are the first to analyze GNNs for the node classification task from a data perspective . Since rich literature demonstrated that the key of GNNs lies in their graph convolutional filters , we propose a new assessment tool ( GFD ) to evaluate the effectiveness of filters given a specific graph data Further , this tool is applied to analyze existing filters and found some meaningful insights . Finally , we propose the AFGNN model to automatically learn the best filter from the given family ( i.e. , learn the best coefficients for the linear combination of a set of filters ) for the given graph data . Now we address your comments and concerns in detail : Q1 : Is it reasonable to use the Fisher score \u201d to support the second term in equation ( 3 ) , where the Fisher score is used to evaluate before the filters applied ? A1 : We \u2019 d like to clarify our GFD is an assessment tool to see whether a filter is good for a particular graph data . Fisher Score is used to evaluate the separability of data . GFD defined in Eq . ( 3 ) , which is the Difference of Fisher Score before and after the filter , is to evaluate whether a filter can increase the data separability . As we show that a good graph convolutional filter can help the non-linear separable data to be linearly separable , we can expect the GFD score is higher for these filters . Note that not every graph filter has this property for every dataset , and our final model is to learn the best filter that could enhance this property for a given dataset . Experimental results in Figure 5 and Table 2 also support this claim . Q2 : The presentation of the last paragraph of \u201c graph filter discriminant score \u201d in page 4 can be improved . The Figure references seem incorrect and confusing . A2 : As mentioned in Q1 , we use Fisher score to evaluate the separability of two classes , we use Fisher Score Difference to evaluate the power of a filter on two classes , and we use GFD , which is a weighted sum of Fisher Score Difference of each pair of classes , to evaluate the power of a filter on the given graph . We have revised our writing and corrected our Figure reference to make this more clear . Q3 : The analysis of the influence of label ratio seems not accurate enough . A3 : Suppose the density and density gap are fixed , when label ratio drops , which means the two classes become more imbalanced , and nodes in a larger class tend to have more neighbors . Then , with the column normalization strategy that does not have any constraint on the range of representation , those nodes with a larger degree tend to aggregate more information and thus have larger new representations . This would be helpful to differentiate the two classes . Take Figure 6 ( g ) as an example , nodes in the large-size class ( green nodes ) are gathered in the upper right part while nodes in the small-size class ( purple nodes ) are gathered in the lower left part , so the two classes become more separable after applying a column-normalized filter . We revised our writing to make this more clear . Q4 : For the GFD score comparison in Figure 4 , why choose order 1,3,7 for density and different order 2,3,6 for the density gap ? A4 : Thanks for pointing out . Previously we just pick the orders that can show our findings most clearly , but we agree it is important to use consistent choice in two subfigures . We now choose the same set of orders in these two figures . The result remains the same . Q5 : What is the meaning of the symbol psi ( l ) ? A5 : It is a learnable intermediate weight ( before normalization ) for each base filter . We then apply softmax normalization to it to get alpha ( l ) . We revised our writing to make this more clear ."}, "2": {"review_id": "r1erNxBtwr-2", "review_text": "This paper introduces an assessment framework for an in-depth analysis of the effect of graph convolutional filters and proposes a novel graph neural network with adaptable filters based on the analysis. The assessment framework builts on the Fisher discriminant score of features and can also be used as an additional (regularization) term for choosing optimal filters in training. The assessment result shows that there is no single graph filter for all types of graph structures. Experiments on both synthetic and real-world benchmark datasets demonstrate that the proposed adaptive GNN can learn appropriate filters for different graph tasks. The proposed analysis using the Fisher score is reasonable and interesting, giving us an insight into the role of graph filters. Even though the analysis is limited (using simple graph models and filter family) and the result is not surprising (given no free lunch theorem, there is very likely to be no single silver bullet fo graph filters), I appreciate the analysis and the result. But, I have some concerns as follows. 1) The proposed GNN and the optimization process The proposed method is to extend CNN to a simple linear combination of different filter bases with learnable weights, which I don't think is very novel. Adding the GFD score as an additional constraint term is interesting, but the way of optimizing the whole objective function is unclear. (In addition, I think calling it the \"regularization term\" is inadequate since the term actually involves data observation, rather than a prior on parameters only.) In the case of AFGNN_inf, I don't think it is equivalent to applying infinite lamda. If lamda is infinite, L_CE needs to be completely ignored. This needs to be clarified. In the case of AFGNN1, I don't clearly understand how the whole objective function is properly optimized with fixed data representation. Is it also iteratively optimized? I hope this is also clarified in more detail. 2) Unconvincing experiments The results on three real datasets do not show significant gains, and two of them are even worse than those of GAT. Furthermore, inductive learning (e.g., protein-protein interaction (PPI) dataset used in GAT) is not tested, which I think needs to be also evaluated. While two synthetic datasets (SmallGap and SmallRatio) created by the authors show significant improvement, these datasets appear to be extreme and unrealistic and look carefully selected in favor of the proposed method. I recommend the authors use for evaluation more realistic datasets that can be found in related research. ", "rating": "3: Weak Reject", "reply_text": "We appreciate your valuable comments and we are now actively working on the supplementary experiments on real-world datasets !"}}