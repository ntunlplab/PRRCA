{"year": "2021", "forum": "guEuB3FPcd", "title": "AlgebraNets", "decision": "Reject", "meta_review": "The paper proposes to deep neural network models with elements of the weight from algebras, and considers a wide range of algebras and large scale promising experiments. The paper raised a heated discussion.\n\nPros: \n\n- Using algebras, one can hope for more efficient architectures \n\n- Numerical experiments on a wide range of problems\n\nCons:\n\n - The theoretical grounding provided in the current version of the paper is not sufficient. The study is empirical (nothing wrong about it), but there is no clear understanding/explanation of why particular choice is better than another, and also why it works in the particular setup. \n\n- The title does not reflect the content of the paper. It is too broad, and also in some sense \u201cprovocative\u201d. The reader expects something much more significant from it.\n\n- Experiment setup: the resulting flops/accuracy figure (main result, Figure 1) does not contain error bars.  I.e., the accuracies should be averaged over several random seeds in order to guarantee the resulting metrics. Also, this figure does not show a clear advantage over the ResNet-50 baseline.", "reviews": [{"review_id": "guEuB3FPcd-0", "review_text": "The paper proposes an interesting kind of networks , AlgebraNets , which is a general paradigm of replacing the commonly used real-valued algebra with other associative algebras . This paper considers C , H , M2 ( R ) ( the set of 2 \u00d7 2 real-valued matrices ) , M2 ( C ) , M3 ( R ) , M4 ( R ) , dual numbers , and the R3 cross product , and investigates the sparsity within AlgebraNets . The work in the paper is interesting and this paper is generally written well . However , there are a few issues/comments with the work : 1.The citation of the references in the main body of this paper is not easy to read . It will be better to replace the format \u201c author ( s ) ( year ) \u201d with the format \u201c ( author ( s ) , year ) \u201d ; 2.Some figures and tables do not appear near the discussion , for example , Figure 1 is shown on Page but it is discussed until page 5 , which makes it difficult to read ; 3.In Figure 1 , the subfigure in the second row and first column , it seems that the performance of model with H and whitening the best stable performance . The subfigure in the second row and second column , it can be seen that the model with H is not better than the baseline model ; 4.There are many inconsistencies in the format of the reference , for example , 1 ) In some places the author 's name is abbreviated , while in others it is not . References \u201c C . J. Gaudet and A. S. Maida . Deep quaternion networks . In 2018 International Joint Conference on Neural Networks ( IJCNN ) , pages 1\u20138 , 2018 . \u201d and \u201c Geoffrey E. Hinton , Sara Sabour , and Nicholas Frosst . Matrix capsules with em routing . In ICLR , 2018 . \u201d ; 2 ) In some places the conference \u2019 s name is abbreviated with the link , while in others it is not . References \u201c Siddhant M. Jayakumar , Wojciech M. Czarnecki , Jacob Menick , Jonathan Schwarz , Jack Rae , Simon Osindero , Yee Whye Teh , Tim Harley , and Razvan Pascanu . Multiplicative interactions and where to find them . In International Conference on Learning Representations , 2020 . URL https : //openreview.net/forum ? id=rylnK6VtDH. \u201d and \u201c Geoffrey E. Hinton , Sara Sabour , and Nicholas Frosst . Matrix capsules with em routing . In ICLR , 2018 . \u201d .Please check carefully and correct the inconsistencies . +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ The paper replaces the traditional real-valued algebra with other associative algebras and shows its parameter and FLOP efficiency . In the beginning , `` I think it is an interesting piece of work , and it may be helpful to develop the basic structural design of neural networks . `` .However , after getting the response from the author ( s ) , I more doubt the significance of the work in this paper : although many types of models have been proposed in this paper , the improvement over the baseline models is limited . I did not lower the grade on this paper since I thought it would be interesting and important ( if effective ) to extend the traditional real number field to more complex algebraic structures . +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your careful read of our manuscript . We appreciate the comments very much , and will update the paper with the changes to the references and try to better stagger the introduction and reference to the different figures in the manuscript . Are there technical issues we can address/clarify/improve that would help improve the perception of our work ?"}, {"review_id": "guEuB3FPcd-1", "review_text": "In this paper , the authors propose the usage of complex numbers in deep neural networks . Would be good to know that complex numbers , n x n matrices , quaternions , diagonal matrices , etc . all can be used in neural networks . The authors also claims benchmark performance in large-scale image classification and language modeling . However , this work can not be appreciated due to the following aspects : 1 . A first question is `` Why it is necessary ? '' Interestingly , the authors already included Section 2.1 Why Algebras ? However , I am not convinced at all . A good answer may take either of the two forms : A ) . simply a math step showing great potential behind ; 2 ) large-scale neural networks that have engineering advantages . It seems that the authors took the second approach , however , ImageNet is not that challenging and there may be no clear need to switch to complex numbers . Would the authors be able to justify this ? 2.Then , the authors directly go to evaluations . The figures seem to show good advantages . However , could you please justify your x , y-axis ? The reported results look high biased . As a reviewer , I have to doubt that the authors may have selectively present their results . A good research paper on such a big topic , should give clear methodology first , right ? If the methodology is questionable , such good results may become noise to the community . I would hope the authors clarify their methodology , and then present that advantages obtained in the experiments . 3.As a top AI conference , I believe that we are looking for intellectual contributions . This paper is working on a huge title , which is attractive . However , when I try to identify the intellectual contributions ( can be theory , algorithm , engineering , applications ) , I am not convinced at all . I know such a topic is not easy to handle . I would simple ask the authors to respond to a direct question : how would like the community to appreciate your work ? NOTE : a lot of disputes are around `` the huge title 'AlgebraNets ' '' . However , I did not receive justification response from the authors . A possible reason may be the authors are not aware of how big the topic it is , and were so attractive/confident in the current experimental improvements ( which is also very appreciated ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . Below , we \u2019 ve tried to answer your questions , but firstly here is our motivation for this work , which we hope will help frame both the manuscript and our response . a ) We wanted to search for more efficient alternatives to real numbers to use in neural networks . This was the goal from the beginning . We were especially interested if we could combine the higher compute density of algebras with sparsity . b ) There had been some prior work showing complex numbers and quaternions were more parameter efficient but nothing about FLOPs . FLOPs are correlated with runtime and often at least as important as parameter efficiency , especially in vision models . c ) We noticed the prior work on complex and quaternions used very FLOP expensive whitening and special initialization . d ) We chose to investigate those algebras and many more on both a parameter and FLOP efficiency basis . Furthermore , we made preliminary steps towards testing sparsity inducing techniques and these algebras . In response to your specific queries : * Do you mean why are more efficient neural networks necessary ? Or why are different algebras necessary for more efficient networks ? They are one approach to finding more efficient architectures , but certainly not the only one . We are surprised that you do not think ImageNet is not a suitable task for demonstrating this method : it is a challenging task and is widely used as a way to benchmark state-of-the-art methods . We also have results on enwik8 and wikitext-103 language modeling which , while certainly not large by GPT-3 standards , has been considered a standard language modeling benchmark in the literature . What tasks would you like to see ? * Thanks for commenting that the results look promising . We choose our axes based upon the standards in other work on efficiency in neural networks . For example : EfficientNet ( M. Tan , et al 2019 ) show results as FLOPs/parameters vs top-1 accuracy . Similarly , MobileNet ( A.G. Howard et al 2017 ) and MobileNet v2 ( M. Sandler et al 2018 ) also present the same axes . Many pruning papers also use these same axes , for example , \u201c What is the State of Neural Network Pruning ? \u201d from D. Blalock et al 2020 and \u201c The State of Sparsity in Deep Neural Networks \u201d from T. Gale et al 2019 . We thank the reviewer for commenting that they found some of the methodology unclear -- are there certain aspects that you found to be particularly confusing ? * We feel there are a series of important contributions in the work : a . ) We find some complexities from prior works are not needed . For example , we do not need special initializations for good performance . b . ) Clear demonstration of which algebras are more efficient both in terms of parameters and FLOPs in a modern regime across multiple domains . c. ) We discover that M_2R is better than all algebras that have been previously considered in terms of performance per FLOP while still offering a substantial parameter reduction . d. ) Showing that M_2R networks can be made sparse and will be better than normal sparsity due to the higher compute density of the algebra . We hope that this helps address your concerns . Please do let us know if there is anything more we can clarify ."}, {"review_id": "guEuB3FPcd-2", "review_text": "# # Summary The authors propose AlgebraNets - a previously explored approach to replace real-valued algebra in deep learning models with other associative algebras that include 2x2 matrices over real and complex numbers . They provide a comprehensive overview of prior methods in this direction and motivate their work with potential for both parameter and computational efficiency , and suggest that the latter is typically overlooked in prior literature . The paper is very well-written and follows a nice narrative , and the claims are mostly backed empirically with experimental results . # # Pros * Empirically justified with experiments on state-of-the-art benchmarks in both computer vision and NLP . * Establishes that exploring other algebras is not just an exercise for mathematical curiosity but also practical , and encourages deep learning practitioners to extend the results . * Perhaps the most useful aspect is that the experiments fit well into a standard deep learning framework \u2013 with conventional operations , initialization , etc . That is , the algebras do not require significant custom ops/modifications to achieve state-of-the-art results . * Shows multiplicative efficiency ( parameter count and FLOPs ) in many cases # # Cons * The authors motivate this work with computational efficiency ; however , I did not find any discussion or comments on the total memory footprint . Do any of the algebras require us to keep track of partial computations/intermediate steps - subsequently increasing the total memory footprint ? In the case of vision examples , which are dominated by the activations , what are the implications ? If the memory footprint is indeed not consistent with a real-valued algebra , then are we trading model/input size for fewer parameters/efficient computation ? * An intuitive justification of the algebras used in these experiments , along with insight for future algebras might be a nice addition , although I would n't consider it a con . * Are certain algebras more amenable to specific hardware architectures ? If so , a brief discussion would enhance the paper overall .", "rating": "7: Good paper, accept", "reply_text": "Deep Complex Networks is an interesting paper that highlighted some of the potential of investigating these alternate algebras . However , they only investigate a single algebra ( complex numbers ) and do not recognize the increased compute density of algebra nor explore pruning or sparsity inducing methods that would greatly benefit from this increased compute density on modern hardware . Additionally , while their proposal is parameter efficient , it is not FLOP efficient due to the computationally expensive whitening procedure . We test a large number of algebras and find an algebra ( 2x2 matrix rings ) that actually work better than anything that has previously been looked at , in terms of performance per FLOP . Additionally , we show that we do not need some of the complexities discussed in earlier work exploring these algebras : specific initialisation schemes , for example , do not seem to matter as much . Lastly , we find some crucial differences in terms of the efficacy of these algebras in testing at scale . Using ImageNet instead of CIFAR-10 one does not recover the same performance per-parameter . To further test this regime , we also use the more computationally efficient MobileNet . Finally , we test the most promising algebras on a variety of different domains as well . Deep Complex Networks was an exciting work but we think we have made a series of new contributions that are important to anyone interested in complex networks or other algebras ."}], "0": {"review_id": "guEuB3FPcd-0", "review_text": "The paper proposes an interesting kind of networks , AlgebraNets , which is a general paradigm of replacing the commonly used real-valued algebra with other associative algebras . This paper considers C , H , M2 ( R ) ( the set of 2 \u00d7 2 real-valued matrices ) , M2 ( C ) , M3 ( R ) , M4 ( R ) , dual numbers , and the R3 cross product , and investigates the sparsity within AlgebraNets . The work in the paper is interesting and this paper is generally written well . However , there are a few issues/comments with the work : 1.The citation of the references in the main body of this paper is not easy to read . It will be better to replace the format \u201c author ( s ) ( year ) \u201d with the format \u201c ( author ( s ) , year ) \u201d ; 2.Some figures and tables do not appear near the discussion , for example , Figure 1 is shown on Page but it is discussed until page 5 , which makes it difficult to read ; 3.In Figure 1 , the subfigure in the second row and first column , it seems that the performance of model with H and whitening the best stable performance . The subfigure in the second row and second column , it can be seen that the model with H is not better than the baseline model ; 4.There are many inconsistencies in the format of the reference , for example , 1 ) In some places the author 's name is abbreviated , while in others it is not . References \u201c C . J. Gaudet and A. S. Maida . Deep quaternion networks . In 2018 International Joint Conference on Neural Networks ( IJCNN ) , pages 1\u20138 , 2018 . \u201d and \u201c Geoffrey E. Hinton , Sara Sabour , and Nicholas Frosst . Matrix capsules with em routing . In ICLR , 2018 . \u201d ; 2 ) In some places the conference \u2019 s name is abbreviated with the link , while in others it is not . References \u201c Siddhant M. Jayakumar , Wojciech M. Czarnecki , Jacob Menick , Jonathan Schwarz , Jack Rae , Simon Osindero , Yee Whye Teh , Tim Harley , and Razvan Pascanu . Multiplicative interactions and where to find them . In International Conference on Learning Representations , 2020 . URL https : //openreview.net/forum ? id=rylnK6VtDH. \u201d and \u201c Geoffrey E. Hinton , Sara Sabour , and Nicholas Frosst . Matrix capsules with em routing . In ICLR , 2018 . \u201d .Please check carefully and correct the inconsistencies . +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ The paper replaces the traditional real-valued algebra with other associative algebras and shows its parameter and FLOP efficiency . In the beginning , `` I think it is an interesting piece of work , and it may be helpful to develop the basic structural design of neural networks . `` .However , after getting the response from the author ( s ) , I more doubt the significance of the work in this paper : although many types of models have been proposed in this paper , the improvement over the baseline models is limited . I did not lower the grade on this paper since I thought it would be interesting and important ( if effective ) to extend the traditional real number field to more complex algebraic structures . +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your careful read of our manuscript . We appreciate the comments very much , and will update the paper with the changes to the references and try to better stagger the introduction and reference to the different figures in the manuscript . Are there technical issues we can address/clarify/improve that would help improve the perception of our work ?"}, "1": {"review_id": "guEuB3FPcd-1", "review_text": "In this paper , the authors propose the usage of complex numbers in deep neural networks . Would be good to know that complex numbers , n x n matrices , quaternions , diagonal matrices , etc . all can be used in neural networks . The authors also claims benchmark performance in large-scale image classification and language modeling . However , this work can not be appreciated due to the following aspects : 1 . A first question is `` Why it is necessary ? '' Interestingly , the authors already included Section 2.1 Why Algebras ? However , I am not convinced at all . A good answer may take either of the two forms : A ) . simply a math step showing great potential behind ; 2 ) large-scale neural networks that have engineering advantages . It seems that the authors took the second approach , however , ImageNet is not that challenging and there may be no clear need to switch to complex numbers . Would the authors be able to justify this ? 2.Then , the authors directly go to evaluations . The figures seem to show good advantages . However , could you please justify your x , y-axis ? The reported results look high biased . As a reviewer , I have to doubt that the authors may have selectively present their results . A good research paper on such a big topic , should give clear methodology first , right ? If the methodology is questionable , such good results may become noise to the community . I would hope the authors clarify their methodology , and then present that advantages obtained in the experiments . 3.As a top AI conference , I believe that we are looking for intellectual contributions . This paper is working on a huge title , which is attractive . However , when I try to identify the intellectual contributions ( can be theory , algorithm , engineering , applications ) , I am not convinced at all . I know such a topic is not easy to handle . I would simple ask the authors to respond to a direct question : how would like the community to appreciate your work ? NOTE : a lot of disputes are around `` the huge title 'AlgebraNets ' '' . However , I did not receive justification response from the authors . A possible reason may be the authors are not aware of how big the topic it is , and were so attractive/confident in the current experimental improvements ( which is also very appreciated ) .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review . Below , we \u2019 ve tried to answer your questions , but firstly here is our motivation for this work , which we hope will help frame both the manuscript and our response . a ) We wanted to search for more efficient alternatives to real numbers to use in neural networks . This was the goal from the beginning . We were especially interested if we could combine the higher compute density of algebras with sparsity . b ) There had been some prior work showing complex numbers and quaternions were more parameter efficient but nothing about FLOPs . FLOPs are correlated with runtime and often at least as important as parameter efficiency , especially in vision models . c ) We noticed the prior work on complex and quaternions used very FLOP expensive whitening and special initialization . d ) We chose to investigate those algebras and many more on both a parameter and FLOP efficiency basis . Furthermore , we made preliminary steps towards testing sparsity inducing techniques and these algebras . In response to your specific queries : * Do you mean why are more efficient neural networks necessary ? Or why are different algebras necessary for more efficient networks ? They are one approach to finding more efficient architectures , but certainly not the only one . We are surprised that you do not think ImageNet is not a suitable task for demonstrating this method : it is a challenging task and is widely used as a way to benchmark state-of-the-art methods . We also have results on enwik8 and wikitext-103 language modeling which , while certainly not large by GPT-3 standards , has been considered a standard language modeling benchmark in the literature . What tasks would you like to see ? * Thanks for commenting that the results look promising . We choose our axes based upon the standards in other work on efficiency in neural networks . For example : EfficientNet ( M. Tan , et al 2019 ) show results as FLOPs/parameters vs top-1 accuracy . Similarly , MobileNet ( A.G. Howard et al 2017 ) and MobileNet v2 ( M. Sandler et al 2018 ) also present the same axes . Many pruning papers also use these same axes , for example , \u201c What is the State of Neural Network Pruning ? \u201d from D. Blalock et al 2020 and \u201c The State of Sparsity in Deep Neural Networks \u201d from T. Gale et al 2019 . We thank the reviewer for commenting that they found some of the methodology unclear -- are there certain aspects that you found to be particularly confusing ? * We feel there are a series of important contributions in the work : a . ) We find some complexities from prior works are not needed . For example , we do not need special initializations for good performance . b . ) Clear demonstration of which algebras are more efficient both in terms of parameters and FLOPs in a modern regime across multiple domains . c. ) We discover that M_2R is better than all algebras that have been previously considered in terms of performance per FLOP while still offering a substantial parameter reduction . d. ) Showing that M_2R networks can be made sparse and will be better than normal sparsity due to the higher compute density of the algebra . We hope that this helps address your concerns . Please do let us know if there is anything more we can clarify ."}, "2": {"review_id": "guEuB3FPcd-2", "review_text": "# # Summary The authors propose AlgebraNets - a previously explored approach to replace real-valued algebra in deep learning models with other associative algebras that include 2x2 matrices over real and complex numbers . They provide a comprehensive overview of prior methods in this direction and motivate their work with potential for both parameter and computational efficiency , and suggest that the latter is typically overlooked in prior literature . The paper is very well-written and follows a nice narrative , and the claims are mostly backed empirically with experimental results . # # Pros * Empirically justified with experiments on state-of-the-art benchmarks in both computer vision and NLP . * Establishes that exploring other algebras is not just an exercise for mathematical curiosity but also practical , and encourages deep learning practitioners to extend the results . * Perhaps the most useful aspect is that the experiments fit well into a standard deep learning framework \u2013 with conventional operations , initialization , etc . That is , the algebras do not require significant custom ops/modifications to achieve state-of-the-art results . * Shows multiplicative efficiency ( parameter count and FLOPs ) in many cases # # Cons * The authors motivate this work with computational efficiency ; however , I did not find any discussion or comments on the total memory footprint . Do any of the algebras require us to keep track of partial computations/intermediate steps - subsequently increasing the total memory footprint ? In the case of vision examples , which are dominated by the activations , what are the implications ? If the memory footprint is indeed not consistent with a real-valued algebra , then are we trading model/input size for fewer parameters/efficient computation ? * An intuitive justification of the algebras used in these experiments , along with insight for future algebras might be a nice addition , although I would n't consider it a con . * Are certain algebras more amenable to specific hardware architectures ? If so , a brief discussion would enhance the paper overall .", "rating": "7: Good paper, accept", "reply_text": "Deep Complex Networks is an interesting paper that highlighted some of the potential of investigating these alternate algebras . However , they only investigate a single algebra ( complex numbers ) and do not recognize the increased compute density of algebra nor explore pruning or sparsity inducing methods that would greatly benefit from this increased compute density on modern hardware . Additionally , while their proposal is parameter efficient , it is not FLOP efficient due to the computationally expensive whitening procedure . We test a large number of algebras and find an algebra ( 2x2 matrix rings ) that actually work better than anything that has previously been looked at , in terms of performance per FLOP . Additionally , we show that we do not need some of the complexities discussed in earlier work exploring these algebras : specific initialisation schemes , for example , do not seem to matter as much . Lastly , we find some crucial differences in terms of the efficacy of these algebras in testing at scale . Using ImageNet instead of CIFAR-10 one does not recover the same performance per-parameter . To further test this regime , we also use the more computationally efficient MobileNet . Finally , we test the most promising algebras on a variety of different domains as well . Deep Complex Networks was an exciting work but we think we have made a series of new contributions that are important to anyone interested in complex networks or other algebras ."}}