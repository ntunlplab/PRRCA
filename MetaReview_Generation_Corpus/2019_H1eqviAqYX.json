{"year": "2019", "forum": "H1eqviAqYX", "title": "Why Do Neural Response Generation Models Prefer Universal Replies?", "decision": "Reject", "meta_review": "This paper seeks to shed light on why seq2seq models favor generic replies. The problem is an important one, unfortunately the responses proposed in the paper are not satisfactory. Most reviewers note problems and general lack of rigorousness in the assumptions used to produce the theoretical part of the paper (e.g., strong assumption of independence of generated words). The experiments themselves are not convincing enough to warrant acceptance by themselves.", "reviews": [{"review_id": "H1eqviAqYX-0", "review_text": "The paper investigates the problem of universal replies plaguing the Seq2Seq neural generation models. The problem is indeed quite important because for problems with high entropy solutions the seq2seq models have been shown to struggle in past literature. While the authors do pick a good problem, that's where the quality of the paper ends for me. The paper goes on an endless meandering through a lot of meaningless probabilistic arguments. First of all, factorizing a seq2seq model as done in equation 1 is plain wrong. The model doesn't operate by first selecting a set of words and then ordering them. On top of this wrong factorization, section 2.2 & 2.3 derives a bunch of meaningless lemmas with extremely crude assumptions. For example, for lemma 3, M is supposed to be some universal constant defined to be the frequency of universal replies while all other replies seem to have a frequency of 1. Somehow through this wrong factorization and some probabilistic jugglery, we arrive at section 3 where the takeaway from section 2 is the rather known one that the model promotes universal replies regardless of query. In section 3, the authors then introduce the \"max-marginal regularization\" which is a linear combination of log-likelihood and max-margin (where the score is given by log-likelihood) losses. Firstly, the use of word \"marginal\" instead of \"margin\" seems quite wrong to say the least. Secondly, the stated definition seems to be wrong. In the definition the range of values for \\gamma is not stated. I consider the two mutual exclusive and exhaustive cases (assuming \\gamma not equals 0) below and show that both have issues: (a) \\gamma > 0: This seems to imply that when the log-likelihood of ground-truth is already \\gamma better than the log-likelihood of the random negative, the loss comes to life. Strange! (b) \\gamma < 0: This is again weird and doesn't seem to be the intended behavior from a max-margin{al} loss. I'm assuming the authors swapped y with y^{-} in the \"regularization\" part. Anyways, the loss/regularization doesn't seem to be novel and should have been compared against pure max-margin methods as well. Coming to the results section, figure 3 doesn't inspire much confidence in the results. For the first example in figure 3, the baseline outputs seem much better than the proposed model, even if they follow a trend, it's much better than the ungrammatical and incomprehensible sentences generated by the proposed model. Also there seems to be a discrepancy in figure 3 with the baseline output for first query having two \"Where is your location?\" outputs. The human column of results for Table 3 is calculated over just 100 examples which seems quite low for any meaningful statistical comparison. Moreover, not quite sure why the results used the top-10 results of beam instead of the top-1. A lot of typos/wrong phrasing/wrong claims and here are some of them: (a) Page 1, \"lead to the misrecognition of those common replies as grammatically corrected patterns\"? - No idea what the authors meant. (b) Page 1, \"unconsciously preferred\" - I would avoid attaching consciousness before AGI strikes us. (c) Page 1, \"Above characters\" -> \"Above characteristics\" (d) Page 1, \"most historical\" -> \"most previous\" (e) Page 2, \"rest replies\" -> \"rest of the replies\" (f) Page 3, \"variational upper bound\" -> Not sure what's variational about the bound (g) \"Word Perplexity (PPL) was used to determine the semantic context of phrase-level utterance\"? - No idea what the authors meant.", "rating": "3: Clear rejection", "reply_text": "Thank you for your helpful comments . We will improve our paper accordingly . We would like to clarify some points mentioned in the review comment , as follows : Q1 : \u201c factorizing a seq2seq model as done in equation 1 is plain wrong \u201d First , Equation 1 performs the decomposition on the loss function defined for NRG task , which is in fact widely used in the natural language generation related tasks ( Hashimoto et al.2018 ) , especially in the statistical machine translation ( Qch , et al.2002 , Koehn et al. , 2003 ) , neural machine translation ( Weng et al.2017 ) , as well as previous NRG related works ( Wu et al.2018 ) .It should be noted that , the decomposition in Equation 1 is indeed not about the training procedure of the seq2seq models for NRG . Moreover , from the perspective of probability theory , any event z ( either latent or explicit ) could be involved to replace S ( y ) as -\\log p ( y|x ) = -\\log p ( z|x ) -\\log p ( y|z , x ) in Equation 1 . In this paper , we assign the set of words of response as the event z to perform the analysis . The result of the analysis on -\\log p ( S ( y ) |x ) -\\log p ( y|S ( y ) , x ) also holds for -\\log p ( y|x ) and its decomposition with any other forms of z . Besides , the usages of this notation S ( y ) are consistent throughout Section 2 . Q2 : \u201c \u2026while all other replies seem to have a frequency of 1\u2026 \u201d Actually , the frequencies of non-universal replies should be a positive constant C ( 1 < = C < < M ) . To simplify , as mentioned in Section 2.3.1 , we suppose that C equals to 1 , but it is actually not rigorous . We have fixed it in the revised version . Consequently , Equation 5 based on this declare will be deducted as follows : \\sum_i p ( y^ { ur } _i | S ( y ) ) = \\frac { \\sum_i^m f ( y^ { ur } _i ) } { \\sum_i^m f ( y^ { ur } _i ) + \\sum_i^ { n-m } f ( Y^ { o } _i ) } = \\frac { M * m } { M * m + c * ( n - m ) } = \\frac { M } { M + n / m - c } > \\frac { M } { M + 3 - c } Thus , the conclusion of Lemma 3 still holds . Q3 : \u201c \u2026the range of values for \\gamma is not stated\u2026 \u201d The \\gamma should be larger than 0 and consistent with the standard max-margin loss ( Agarwal & Collins , 2010 ; Hu et al , 2014 ) . We have added its range in the revised manuscript . Besides , the discussion in the review comment about the case of \\gamma > 0 can be established , since the preference of choosing more informative words is exactly what we expect the model to capture . Besides , since in the decoding phase , { y_1 , \u2026 y_ { i-1 } } is given when predicting y_i , the log-likelihood of the ground-truth is not always higher than that of the random negative , in other words , the regularization does not often come to life . However , we believe that comparing our loss against the pure max-margin methods is not reasonable , since the max-margin loss also penalizes the literal expression ( language modeling ) included in negative samples , which makes the model fail to generate grammatical sentences . Based on this consideration , we propose this term as a regularization , which is also mentioned in the last paragraph of Section 3 . Q4 : \u201c \u2026ungrammatical and incomprehensible sentences generated by the proposed model \u2026 duplicated sentences in the output of the baseline model\u2026 \u201d Sorry for the confusion caused by the representation in the original manuscript First , the samples in Figure 3 are the translated version from Chinese . The translation guideline is to keep as many patterns , key words within the Chinese sentence as possible . Therefore , some ungrammatical and incomprehensible sentences appear . Besides , the duplicated sentences are attributed to the translation as well , because there are no duplications in the untranslated group of generated sentences . We will fix this issue and provide the original Chinese sentences . Q5 : \u201c \u2026100 examples which seems quite low for any meaningful statistical comparison ... \u201d In fact , we randomly sample 100 query and generate 10 replies for each query , which produces 1000 examples for each model and 4000 examples for each labeler . It should be noted that , the human evaluation is a widely used measure in NRG to cooperate with the numeric metrics , and the count of manually labeled examples in previous studies always ranges from hundreds to thousands ( Li et al. , 2016a ; Mou et al. , 2016 ; Xu et al.2017 ; Serban et al.2017 ) .Q6 : \u201c Moreover , not quite sure why the results used the top-10 results of beam instead of the top-1 \u201d A chatbot agent which can reply with diverse answers means such agent generates various responses for a fixed query , and these various responses ( generated by beam search ) do not share information or pattern between each other . However , as current NRG models tend to generate a series of universal replies from the same beam , they often fail to reply diverse answers . The objective of our paper is to investigate the reason for NRG models preferring universal replies and to propose a method to promote the diversity of generated responses from NRG models . Correspondingly , it is only reasonable to measure the performance on diversity by evaluating more than one responses for each query ."}, {"review_id": "H1eqviAqYX-1", "review_text": "The paper looks into improving the neural response generation task by deemphasizing the common responses using modification of the loss function and presentation the common/universal responses during the training phase. The authors show that the approach yields better results in the dataset considered using various measures and human evaluation. Improvement Points - the explanation for low ROGUE measure due to the method favoring non-repetitive words sounds like it can be supported using numerical statistics, than hand-waiving argument - for the timing, how much time was taken to tune the additional parameters (how the # of negative responses sampled for each positive response was chosen as four via uniform sampling) - some description about a) how many users are there, what type of conversation/active users/topics etc. b) what time frame was used during data collection (this may have implications for lemma asserting zipf) - it would be interesting to know for the trivial questions if the performance was impacted by the deemphasizing (one that do result in universal replies)", "rating": "7: Good paper, accept", "reply_text": "Thank you for your helpful comments and suggestions . We will add more details about the data and statistics in the appendix according to your suggestions . For some questions mentioned in the review comment , we would like to make the clarification as follows : Q1 : \u201c for the timing , how much time was taken to tune the additional parameters ( how the # of negative responses sampled for each positive response was chosen as four via uniform sampling ) \u201d As mentioned in the question , the encoding-decoding process of our method is around 8 times longer than the standard S2SA for a ground-truth query-reply pair . However , since the encoder modeling the query x in learning each ( x , y , y^ { - } ) pair can be shared , the final training cost is around 6 times of the standard S2SA . Moreover , the proposed max-margin regularization can be used to fine-tune a trained S2SA model , and our experiment shows the model can converge in only 1-2 fine-tuning epochs . Therefore , in practice our proposed method can be more efficient . Q2 : \u201c some description about a ) how many users are there , what type of conversation/active users/topics etc . b ) what time frame was used during data collection ( this may have implications for lemma asserting zipf ) \u201d The dataset is extracted from 1 million conversations within half year . The source of our dataset is Douban group , which is a forum similar to Reddit . We did not restrict the types of communities , thus the collected conversations cover various topics including games , movie , music , sports , etc . The approximated number of users is around 1 million according to the statistics on small samples . Q3 : \u201c it would be interesting to know for the trivial questions if the performance was impacted by the deemphasizing ( one that do result in universal replies ) \u201d It is really an interesting question . For each query , the sampled negative replies are not equivalent to the ground-truth , and we do not expect the model to learn the ground-truth pairs with universal replies . The proposed loss function aims to punish those universal replies from a statistical perspective , by considering the distributions of words and sentences . Therefore , the regularization does not directly influence those trivial questions ."}, {"review_id": "H1eqviAqYX-2", "review_text": "This paper presents a framework for understanding why seq2seq neural response generators prefer \"universal\"/generic replies. To do so the paper breaks down the response generation probability into the probability of selecting the set of tokens (reflecting the topic of the output) and then selecting an ordering of the tokens (reflecting the syntax of the output.) The results presented in this paper are not technically sound. E.g the derivation in Eq(2) derive a meaningless bound. Here is why: 1. The first equality assumes that the words in a set are independent which is not true. 2. In the second equality, the authors incorrectly replace the summation of word probability in each sentence with the summation of word probabilities over all unique words (the set) overall sentences. This is simply not true if there are common words shared between sentences. 3. Perhaps the biggest issue is the incorrect application of Jensen's lemma. JL is often used as log(\\sum_i a_i x_i) > \\sum_i a_i log x_i if \\sum_i a_i = 1. Instead what authors have used is log (\\sum_i x_i) > \\sum_i log(x_i), which is not always true, and is trivially true for all x_i < 1. In fact, this bound is not even tight (unlike Jensen's lemma) and the *worst* part is that the LHS increases if we add more x_i (<1) and the RHS decreases. This means this bound is far from being meaningful and as such should be summarily ignored. Similarly, in section 2.3, the technical content is quite poor. Why is this true -- \"the amount of possible queries M of y... 1 << M \\propto N\"? There are many assumptions in lemma 3 that are quite difficult to unpack to verify the correctness e.g. can the most frequent words not occur at all in \"non-universal\" replies? I am not going more into the details in this section because I think the problems with section 2.2 are themselves dealbreakers. Overall, given the problems this work is not technically sound to be accepted.", "rating": "1: Trivial or wrong", "reply_text": "Thank you for your helpful comments . For some questions mentioned in the review comment , we would like to make the clarification as follows : Q1 : \u201c perhaps the biggest issue is the incorrect application of Jensen \u2019 s lemma \u201d As detailed in another comment given by us , Equation 2 is actually indirectly derived using Jensen \u2019 s Inequality though it still holds . For more details , the size of \\cup_k^K { S ( y_k ) } , noted as L_S , should be included in Equation 2 when performing Jensen \u2019 s Inequality , where L_S < = K * T , and K is the number of responses of x and T is the fixed number of words in a response . Then the last inequality in Equation 2 can be derived as follows . \\sum \\log p ( w|x ) < = L_S \\log \\sum [ p ( w|x ) / L_S ] To simplify , we use \\sum to denote \\sum_ { } ^ { } . Then , the Eq.3 analyzes the last term in the previous inequality : L_ { S } \\log \\sum_ { w \\in \\cup_k^K S ( y_k ) } \\frac { p ( w|x ) } { L_ { S } } = L_ { S } \\log \\frac { E ( w|x ) * T } { K * T * L_ { S } } \\propto \\log \\frac { 1 } { ( K * L_ { S } ) ^ { L_ { S } } } \\leq \\frac { 1 } { K^ { 2K } } which is much smaller than 1 / K , so that the analysis still holds . We have already updated this part to the right version in the revised manuscript . Q2 : \u201c The first equality assumes that the words in a set are independent which is not true. \u201d Actually , the independent hypothesis is often used in the natural language generation related task ( Hashimoto et al.2018 ) especially in the statistical machine translation ( Qch , et al.2002 , Koehn et al. , 2003 ) , neural machine translation ( Weng et al.2017 ) , as well as previous NRG related works ( Wu et al.2018 ) .Such a hypothesis can be taken to simplify and analyze the complicated natural language processing problem , which is similar to some other hypotheses such as the Markov Chain . Besides , the model aims to maximize the generation probability of each { y_k } in the training procedure , but only produces a single one in the inference phase . Considering this fact , most of the words ( appearing in different sentences ) could be taken as independent naturally . Q3 : \u201c \u2026e.g.can the most frequent words not occur at in \u201c non-universal \u201d replies ? \u201d As claimed in the paper , the very first characteristic of a universal reply can be described as \u201c A response is universal if it consists of only top-t ranked words \u201d , which is not equivalent to \u201c top-t ranked words only exist in the universal replies \u201d . It should be noted that Equation 4 is derived based on this characteristic . Moreover , we totally agree with the fact that the \u201c the most frequent words not occur at in \u201c non-universal \u201d replies \u201d is not true . Q4 : \u201d Why is this true \u2013 \u201c The amount of possible queries M of y is directly proportional to the size of query-response pairs N , noted as 1 < < M \\propto N. \u201d \u201d NRG models tend to generate universal replies , such as \u201c I don \u2019 t know \u201d , \u201c I \u2019 m OK \u201d etc. , and this trend is one of the most tough problem faced by current NRG models ( Sordoni et al. , 2015 ; Vinyals and Le , 2015 ; Li et al. , 2016b ; c ; d ; Li & Jurafsky , 2016 ; Mou et al. , 2016 ; Xing et al. , 2017 ; Shao et al. , 2016 ; 2017 ; Serban et al. , 2017 ) . Moreover , there are some other aliases of universal replies including \u201c safe responses \u201d , \u201c common replies \u201d , and \u201c generic responses \u201d , which indicates these responses are mostly high-frequent in the corpus . Especially , as discussed in the previous studies , the universal reply can be the subsequent utterance of a rather large number of varieties of queries ( Li et al. , 2016a , Li et al. , 2016b ) . Thus , given the large-scale corpus of query-reply pairs , the number of universal replies M is apparently much larger than 1 . Meanwhile , with the unchanged data distribution , if we sample a k-times larger corpus from the source , the number of universal replies will be extended to k-times larger , that is the reason we claim \u201c M \\propto N \u201d . Furthermore , \u201c M \\propto N \u201d is not equivalent to claiming that M is as large as N ."}], "0": {"review_id": "H1eqviAqYX-0", "review_text": "The paper investigates the problem of universal replies plaguing the Seq2Seq neural generation models. The problem is indeed quite important because for problems with high entropy solutions the seq2seq models have been shown to struggle in past literature. While the authors do pick a good problem, that's where the quality of the paper ends for me. The paper goes on an endless meandering through a lot of meaningless probabilistic arguments. First of all, factorizing a seq2seq model as done in equation 1 is plain wrong. The model doesn't operate by first selecting a set of words and then ordering them. On top of this wrong factorization, section 2.2 & 2.3 derives a bunch of meaningless lemmas with extremely crude assumptions. For example, for lemma 3, M is supposed to be some universal constant defined to be the frequency of universal replies while all other replies seem to have a frequency of 1. Somehow through this wrong factorization and some probabilistic jugglery, we arrive at section 3 where the takeaway from section 2 is the rather known one that the model promotes universal replies regardless of query. In section 3, the authors then introduce the \"max-marginal regularization\" which is a linear combination of log-likelihood and max-margin (where the score is given by log-likelihood) losses. Firstly, the use of word \"marginal\" instead of \"margin\" seems quite wrong to say the least. Secondly, the stated definition seems to be wrong. In the definition the range of values for \\gamma is not stated. I consider the two mutual exclusive and exhaustive cases (assuming \\gamma not equals 0) below and show that both have issues: (a) \\gamma > 0: This seems to imply that when the log-likelihood of ground-truth is already \\gamma better than the log-likelihood of the random negative, the loss comes to life. Strange! (b) \\gamma < 0: This is again weird and doesn't seem to be the intended behavior from a max-margin{al} loss. I'm assuming the authors swapped y with y^{-} in the \"regularization\" part. Anyways, the loss/regularization doesn't seem to be novel and should have been compared against pure max-margin methods as well. Coming to the results section, figure 3 doesn't inspire much confidence in the results. For the first example in figure 3, the baseline outputs seem much better than the proposed model, even if they follow a trend, it's much better than the ungrammatical and incomprehensible sentences generated by the proposed model. Also there seems to be a discrepancy in figure 3 with the baseline output for first query having two \"Where is your location?\" outputs. The human column of results for Table 3 is calculated over just 100 examples which seems quite low for any meaningful statistical comparison. Moreover, not quite sure why the results used the top-10 results of beam instead of the top-1. A lot of typos/wrong phrasing/wrong claims and here are some of them: (a) Page 1, \"lead to the misrecognition of those common replies as grammatically corrected patterns\"? - No idea what the authors meant. (b) Page 1, \"unconsciously preferred\" - I would avoid attaching consciousness before AGI strikes us. (c) Page 1, \"Above characters\" -> \"Above characteristics\" (d) Page 1, \"most historical\" -> \"most previous\" (e) Page 2, \"rest replies\" -> \"rest of the replies\" (f) Page 3, \"variational upper bound\" -> Not sure what's variational about the bound (g) \"Word Perplexity (PPL) was used to determine the semantic context of phrase-level utterance\"? - No idea what the authors meant.", "rating": "3: Clear rejection", "reply_text": "Thank you for your helpful comments . We will improve our paper accordingly . We would like to clarify some points mentioned in the review comment , as follows : Q1 : \u201c factorizing a seq2seq model as done in equation 1 is plain wrong \u201d First , Equation 1 performs the decomposition on the loss function defined for NRG task , which is in fact widely used in the natural language generation related tasks ( Hashimoto et al.2018 ) , especially in the statistical machine translation ( Qch , et al.2002 , Koehn et al. , 2003 ) , neural machine translation ( Weng et al.2017 ) , as well as previous NRG related works ( Wu et al.2018 ) .It should be noted that , the decomposition in Equation 1 is indeed not about the training procedure of the seq2seq models for NRG . Moreover , from the perspective of probability theory , any event z ( either latent or explicit ) could be involved to replace S ( y ) as -\\log p ( y|x ) = -\\log p ( z|x ) -\\log p ( y|z , x ) in Equation 1 . In this paper , we assign the set of words of response as the event z to perform the analysis . The result of the analysis on -\\log p ( S ( y ) |x ) -\\log p ( y|S ( y ) , x ) also holds for -\\log p ( y|x ) and its decomposition with any other forms of z . Besides , the usages of this notation S ( y ) are consistent throughout Section 2 . Q2 : \u201c \u2026while all other replies seem to have a frequency of 1\u2026 \u201d Actually , the frequencies of non-universal replies should be a positive constant C ( 1 < = C < < M ) . To simplify , as mentioned in Section 2.3.1 , we suppose that C equals to 1 , but it is actually not rigorous . We have fixed it in the revised version . Consequently , Equation 5 based on this declare will be deducted as follows : \\sum_i p ( y^ { ur } _i | S ( y ) ) = \\frac { \\sum_i^m f ( y^ { ur } _i ) } { \\sum_i^m f ( y^ { ur } _i ) + \\sum_i^ { n-m } f ( Y^ { o } _i ) } = \\frac { M * m } { M * m + c * ( n - m ) } = \\frac { M } { M + n / m - c } > \\frac { M } { M + 3 - c } Thus , the conclusion of Lemma 3 still holds . Q3 : \u201c \u2026the range of values for \\gamma is not stated\u2026 \u201d The \\gamma should be larger than 0 and consistent with the standard max-margin loss ( Agarwal & Collins , 2010 ; Hu et al , 2014 ) . We have added its range in the revised manuscript . Besides , the discussion in the review comment about the case of \\gamma > 0 can be established , since the preference of choosing more informative words is exactly what we expect the model to capture . Besides , since in the decoding phase , { y_1 , \u2026 y_ { i-1 } } is given when predicting y_i , the log-likelihood of the ground-truth is not always higher than that of the random negative , in other words , the regularization does not often come to life . However , we believe that comparing our loss against the pure max-margin methods is not reasonable , since the max-margin loss also penalizes the literal expression ( language modeling ) included in negative samples , which makes the model fail to generate grammatical sentences . Based on this consideration , we propose this term as a regularization , which is also mentioned in the last paragraph of Section 3 . Q4 : \u201c \u2026ungrammatical and incomprehensible sentences generated by the proposed model \u2026 duplicated sentences in the output of the baseline model\u2026 \u201d Sorry for the confusion caused by the representation in the original manuscript First , the samples in Figure 3 are the translated version from Chinese . The translation guideline is to keep as many patterns , key words within the Chinese sentence as possible . Therefore , some ungrammatical and incomprehensible sentences appear . Besides , the duplicated sentences are attributed to the translation as well , because there are no duplications in the untranslated group of generated sentences . We will fix this issue and provide the original Chinese sentences . Q5 : \u201c \u2026100 examples which seems quite low for any meaningful statistical comparison ... \u201d In fact , we randomly sample 100 query and generate 10 replies for each query , which produces 1000 examples for each model and 4000 examples for each labeler . It should be noted that , the human evaluation is a widely used measure in NRG to cooperate with the numeric metrics , and the count of manually labeled examples in previous studies always ranges from hundreds to thousands ( Li et al. , 2016a ; Mou et al. , 2016 ; Xu et al.2017 ; Serban et al.2017 ) .Q6 : \u201c Moreover , not quite sure why the results used the top-10 results of beam instead of the top-1 \u201d A chatbot agent which can reply with diverse answers means such agent generates various responses for a fixed query , and these various responses ( generated by beam search ) do not share information or pattern between each other . However , as current NRG models tend to generate a series of universal replies from the same beam , they often fail to reply diverse answers . The objective of our paper is to investigate the reason for NRG models preferring universal replies and to propose a method to promote the diversity of generated responses from NRG models . Correspondingly , it is only reasonable to measure the performance on diversity by evaluating more than one responses for each query ."}, "1": {"review_id": "H1eqviAqYX-1", "review_text": "The paper looks into improving the neural response generation task by deemphasizing the common responses using modification of the loss function and presentation the common/universal responses during the training phase. The authors show that the approach yields better results in the dataset considered using various measures and human evaluation. Improvement Points - the explanation for low ROGUE measure due to the method favoring non-repetitive words sounds like it can be supported using numerical statistics, than hand-waiving argument - for the timing, how much time was taken to tune the additional parameters (how the # of negative responses sampled for each positive response was chosen as four via uniform sampling) - some description about a) how many users are there, what type of conversation/active users/topics etc. b) what time frame was used during data collection (this may have implications for lemma asserting zipf) - it would be interesting to know for the trivial questions if the performance was impacted by the deemphasizing (one that do result in universal replies)", "rating": "7: Good paper, accept", "reply_text": "Thank you for your helpful comments and suggestions . We will add more details about the data and statistics in the appendix according to your suggestions . For some questions mentioned in the review comment , we would like to make the clarification as follows : Q1 : \u201c for the timing , how much time was taken to tune the additional parameters ( how the # of negative responses sampled for each positive response was chosen as four via uniform sampling ) \u201d As mentioned in the question , the encoding-decoding process of our method is around 8 times longer than the standard S2SA for a ground-truth query-reply pair . However , since the encoder modeling the query x in learning each ( x , y , y^ { - } ) pair can be shared , the final training cost is around 6 times of the standard S2SA . Moreover , the proposed max-margin regularization can be used to fine-tune a trained S2SA model , and our experiment shows the model can converge in only 1-2 fine-tuning epochs . Therefore , in practice our proposed method can be more efficient . Q2 : \u201c some description about a ) how many users are there , what type of conversation/active users/topics etc . b ) what time frame was used during data collection ( this may have implications for lemma asserting zipf ) \u201d The dataset is extracted from 1 million conversations within half year . The source of our dataset is Douban group , which is a forum similar to Reddit . We did not restrict the types of communities , thus the collected conversations cover various topics including games , movie , music , sports , etc . The approximated number of users is around 1 million according to the statistics on small samples . Q3 : \u201c it would be interesting to know for the trivial questions if the performance was impacted by the deemphasizing ( one that do result in universal replies ) \u201d It is really an interesting question . For each query , the sampled negative replies are not equivalent to the ground-truth , and we do not expect the model to learn the ground-truth pairs with universal replies . The proposed loss function aims to punish those universal replies from a statistical perspective , by considering the distributions of words and sentences . Therefore , the regularization does not directly influence those trivial questions ."}, "2": {"review_id": "H1eqviAqYX-2", "review_text": "This paper presents a framework for understanding why seq2seq neural response generators prefer \"universal\"/generic replies. To do so the paper breaks down the response generation probability into the probability of selecting the set of tokens (reflecting the topic of the output) and then selecting an ordering of the tokens (reflecting the syntax of the output.) The results presented in this paper are not technically sound. E.g the derivation in Eq(2) derive a meaningless bound. Here is why: 1. The first equality assumes that the words in a set are independent which is not true. 2. In the second equality, the authors incorrectly replace the summation of word probability in each sentence with the summation of word probabilities over all unique words (the set) overall sentences. This is simply not true if there are common words shared between sentences. 3. Perhaps the biggest issue is the incorrect application of Jensen's lemma. JL is often used as log(\\sum_i a_i x_i) > \\sum_i a_i log x_i if \\sum_i a_i = 1. Instead what authors have used is log (\\sum_i x_i) > \\sum_i log(x_i), which is not always true, and is trivially true for all x_i < 1. In fact, this bound is not even tight (unlike Jensen's lemma) and the *worst* part is that the LHS increases if we add more x_i (<1) and the RHS decreases. This means this bound is far from being meaningful and as such should be summarily ignored. Similarly, in section 2.3, the technical content is quite poor. Why is this true -- \"the amount of possible queries M of y... 1 << M \\propto N\"? There are many assumptions in lemma 3 that are quite difficult to unpack to verify the correctness e.g. can the most frequent words not occur at all in \"non-universal\" replies? I am not going more into the details in this section because I think the problems with section 2.2 are themselves dealbreakers. Overall, given the problems this work is not technically sound to be accepted.", "rating": "1: Trivial or wrong", "reply_text": "Thank you for your helpful comments . For some questions mentioned in the review comment , we would like to make the clarification as follows : Q1 : \u201c perhaps the biggest issue is the incorrect application of Jensen \u2019 s lemma \u201d As detailed in another comment given by us , Equation 2 is actually indirectly derived using Jensen \u2019 s Inequality though it still holds . For more details , the size of \\cup_k^K { S ( y_k ) } , noted as L_S , should be included in Equation 2 when performing Jensen \u2019 s Inequality , where L_S < = K * T , and K is the number of responses of x and T is the fixed number of words in a response . Then the last inequality in Equation 2 can be derived as follows . \\sum \\log p ( w|x ) < = L_S \\log \\sum [ p ( w|x ) / L_S ] To simplify , we use \\sum to denote \\sum_ { } ^ { } . Then , the Eq.3 analyzes the last term in the previous inequality : L_ { S } \\log \\sum_ { w \\in \\cup_k^K S ( y_k ) } \\frac { p ( w|x ) } { L_ { S } } = L_ { S } \\log \\frac { E ( w|x ) * T } { K * T * L_ { S } } \\propto \\log \\frac { 1 } { ( K * L_ { S } ) ^ { L_ { S } } } \\leq \\frac { 1 } { K^ { 2K } } which is much smaller than 1 / K , so that the analysis still holds . We have already updated this part to the right version in the revised manuscript . Q2 : \u201c The first equality assumes that the words in a set are independent which is not true. \u201d Actually , the independent hypothesis is often used in the natural language generation related task ( Hashimoto et al.2018 ) especially in the statistical machine translation ( Qch , et al.2002 , Koehn et al. , 2003 ) , neural machine translation ( Weng et al.2017 ) , as well as previous NRG related works ( Wu et al.2018 ) .Such a hypothesis can be taken to simplify and analyze the complicated natural language processing problem , which is similar to some other hypotheses such as the Markov Chain . Besides , the model aims to maximize the generation probability of each { y_k } in the training procedure , but only produces a single one in the inference phase . Considering this fact , most of the words ( appearing in different sentences ) could be taken as independent naturally . Q3 : \u201c \u2026e.g.can the most frequent words not occur at in \u201c non-universal \u201d replies ? \u201d As claimed in the paper , the very first characteristic of a universal reply can be described as \u201c A response is universal if it consists of only top-t ranked words \u201d , which is not equivalent to \u201c top-t ranked words only exist in the universal replies \u201d . It should be noted that Equation 4 is derived based on this characteristic . Moreover , we totally agree with the fact that the \u201c the most frequent words not occur at in \u201c non-universal \u201d replies \u201d is not true . Q4 : \u201d Why is this true \u2013 \u201c The amount of possible queries M of y is directly proportional to the size of query-response pairs N , noted as 1 < < M \\propto N. \u201d \u201d NRG models tend to generate universal replies , such as \u201c I don \u2019 t know \u201d , \u201c I \u2019 m OK \u201d etc. , and this trend is one of the most tough problem faced by current NRG models ( Sordoni et al. , 2015 ; Vinyals and Le , 2015 ; Li et al. , 2016b ; c ; d ; Li & Jurafsky , 2016 ; Mou et al. , 2016 ; Xing et al. , 2017 ; Shao et al. , 2016 ; 2017 ; Serban et al. , 2017 ) . Moreover , there are some other aliases of universal replies including \u201c safe responses \u201d , \u201c common replies \u201d , and \u201c generic responses \u201d , which indicates these responses are mostly high-frequent in the corpus . Especially , as discussed in the previous studies , the universal reply can be the subsequent utterance of a rather large number of varieties of queries ( Li et al. , 2016a , Li et al. , 2016b ) . Thus , given the large-scale corpus of query-reply pairs , the number of universal replies M is apparently much larger than 1 . Meanwhile , with the unchanged data distribution , if we sample a k-times larger corpus from the source , the number of universal replies will be extended to k-times larger , that is the reason we claim \u201c M \\propto N \u201d . Furthermore , \u201c M \\propto N \u201d is not equivalent to claiming that M is as large as N ."}}