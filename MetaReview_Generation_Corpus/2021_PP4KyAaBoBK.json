{"year": "2021", "forum": "PP4KyAaBoBK", "title": "Human Perception-based Evaluation Criterion for Ultra-high Resolution Cell Membrane Segmentation", "decision": "Reject", "meta_review": "This paper focuses on a segmentation of cell imagery (as opposed to the more commonly studied domain of \"natural images\"). Among its contributions are a novel metric for evaluation of results and a novel dataset. These are acknowledged by the reviewers as strengths. Multiple issues raised in the initial reviews were addressed in the revision (the reviewers agree on this and most of them raised their scores). On the other hand, the concerns remaining have to do with significance and impact. The final evaluation ratings are split, with only a single score clearly in favor of acceptance. \n\nI tend to agree that the contributions, while without a doubt valuable, make this less of a fit to ICLR than to a more specialized venue focusing on biomedical data. ", "reviews": [{"review_id": "PP4KyAaBoBK-0", "review_text": "Summary : This work presents two contributions towards cell membrane segmentation . First , it introduces a new labelled database for this purpose . The authors claim that this is the largest labelled database of high resolution Electron-Microscopy images for this purpose . Second , the work tackles the issue that the F1Score , Dice and IoU scores that evaluate segmentation performance by quantifying the overlap of 2 segmentations are not adequately describing quality of a segmentation with respect to what human experts would prefer for the task . This has been evaluated via employing humans ( experts on the task of cell segmentation ) to grade which segmentation they prefer , and analysed how their preference correlates with these scores . As a solution , the authors propose a metric , PHD , that can be described as the average Haussdorf distance between the skeletons of two segmentations , together with a threshold tolerance , which they show correlates better with the preference of experts with respect to segmentation quality on this task . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : I recommend a rejection of this work for the following reasons . On one hand , constructing a database and releasing it to the community is a great contribution . I am sure that this would be very well accepted . But , on the other hand , I don \u2019 t think the article adequately describes the dataset or compare it adequately with existing databases ( which makes less of a \u201c database article \u201d ) . Instead , half the article discusses a metric that is essentially an adaptation of the haussdorf distance ( actually , of the \u201c average symmetric surface distance \u201d ) , adapted in a manner specific to the cell-segmentation task ( applied on skeleton , and with a tolerance , the importance of which is questionable ) . This 2nd contribution has not been accompanied by a literature review on metrics ( e.g.only discusses IoU/F1/Dice , missing related distance based metrics like ASSD completely ) , nor adequately evaluated with such related metrics ( besides IoU/Dice/F1 ) . Finally , the modifications , along with many claims in the article , are only relevant to the specific task of cell segmentation ( and quite subjective ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . Great contribution by releasing publicly a new labelled database of high quality . Seems there was a lot of effort to construct good quality ground truth on a number of images much larger than the existing publically available databases . This is definitely interesting for the community that works in this problem . 2.Interesting human-based evaluation of the usefulness of IoU/Dice/F1 scores for the cell-segmentation problem ( by 20 humans , this is nice ) . It is known in the broader community that overlap metrics ( IoU/Dice etc ) are not perfect , hence there is a lot of work on other metrics [ 1,2 etc ] , but this substantiates/quantifies it very nicely . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . If I would judge the paper focused on the 1st contribution ( releasing a database ) , I would say that it does not contain a sufficient analysis of the database itself , and especially not an adequate description and comparison with other public databases . I think this point could be sufficiently addressed in the rebuttal . 2.From the technical viewpoint , the claimed 2nd contribution is the derivation of a new metric , but the work has not performed any literature review on related work on metrics except IoU/Dice/F1 . In fact , the work produces a metric ( Eq.2 ) that seems to me the same as Average Assymetric Surface Distance ( ASSD , see [ 1 ] ) , applied to the skeleton ( thinned ) segmentation , with a tolerance ( task-specific modifications ) . I note that both skeleton-like operations have been previously performed for computing metrics in the cell-segmentation domain ( e.g.for evaluation of ISBI2012 challenge : http : //brainiac2.mit.edu/isbi_challenge/evaluation , notice the \u201c after thinning \u201d operation ) . Tolerance-based modifications have also been applied to various other metrics ( e.g . [ 2 ] below ) and are task-specific modifications ( and not necessary for the metric to be appropriate in the general sense ) . In my opinion , this makes the value of the 2nd contribution very low . I think this point can not be sufficiently addressed in the rebuttal , as I basically think that the contribution of the derived metric is low in comparison to existing literature . 3.The \u201c skeleton \u201d part of PHD is not individually evaluated whether it actually adds substantially . I note that without it , the metric is essentially ASSD with tolerance ( tolerance being task-specifically motivated in this context ) . 4.Evaluation of the proposed metrics is limited , because it does not contain other metrics except IoU/Dice . E.g. , it should have been compared with Haussdord , ASSD , etc . 5.The scope of the paper is limited to the cell-segmentation problem . 6.The work contains a number of statements that are not true and would need significant text alterations to reduce them . The above Cons are described in detail below , with the detailed comments I raise in the \u201c Questions for rebuttal period \u201d section . References : Paper with some review on related metrics ( there are many more such reviews ) : [ 1 ] Yeghiazaryan and Voiculescu , Family of boundary overlap metrics for the evaluation of medical image segmentation , Journal of Medical Imaging , 2018 Paper implementing tolerance ( which motivates it , and is subjective to the specific task and needs ) : [ 2 ] Nikolov et al , Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy , arxiv 2018 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions/points to address during rebuttal period : The abstract \u201c proposes \u201d a database with \u201c multiple iterative annotations \u201d . However , the actual database released only contains the last 3rd annotation . Please rephrase the abstract so that this is clear to the reader that only 1 annotation should be expected . Similar to the above , Sec 2.1 claims the existence of multiple ( 3 ) annotations as an advantage of the proposed database over other databases ( Sec.2.1 , \u201c Besides that , U-RICS produced 3 sets of annotations\u2026 , all of which can be applied in developing depe learning \u201d ) . But , these 3 annotations are not released , hence their existence is irrelevant to the reader and this claim/advantage simply does not hold . The authors claim that the intermediate annotations are \u201c are very valuable for learning \u201d ( Sec.2 ) and therefore will not be released . In this case , if they are valueable , perhaps consider releasing them ? Otherwise , I would suggest rephrasing the article , reducing the emphasis on these 3 labels across the whole manuscript , as they are of little relevance to the reader . You can spend the saved space to extend on more related points ( e.g.related work etc , see below ) . Sec 1. claims : \u201c We found the human performance is far superior to these methods \u201d . I think this statement is very strong and not supported adequately by the current evaluation . I think the authors refer to experiments in Table 1. , where the labels from the 1st and 2nd iteration where compared with the results in the 3rd iteration . But , naturally , image after 3rd iteration is conditioned ( related ) very closely to those from the 1st and 2nd iterations . Of course they will have very high agreement . Also , we note , the 2nd iteration is not a result of 1 human but of multiple ( 5 experts + annotator ) . Hence , for these 2 reasons , they can not support the statement . For a correct assessment for such a claim , segmentations from a single annotator , who is not the same contributing to making the ground truth for the image , should be evaluated against the ground truth . Additionally , inter-rate segmentation performance ( multiple humans ) could also be evaluated to make such claims . I would recommend this claim to be altered , as well as state explicitly these factors ( conditioning of 1st , 2nd , 3rd iterations ) in sec 4 to explain how come agreement is so high in Table 1 . The work has performed no literature review on related metrics for segmentation . It discusses and evaluates solely overlap based metrics ( Dice/F1 & IoU ) , and then proposes a distance-based measure . The authors should have discussed and evaluated distance based measures too , and especially have a look at the Average Symmetric Surface Distance ( ASSD ) metric , which is very similar to what they propose ( Eq.2 ) . See [ 1 ] as a starting point for related metrics , but there are many more . Many of these papers raise the argument that no metric is enough for all tasks , and that \u201c quality \u201d of a segmentation is subjective to the task . Hence , for each task , one should chose the correct metric , while for \u201c objective \u201d and all-around evaluation of a \u201c general \u201d method ( e.g.an arbitrary segmentation network ) , multiple complimentary metrics should be used . See [ 2 ] also with related discussion . Also see that ISBI2012 challenge itself implemented multiple metrics ( http : //brainiac2.mit.edu/isbi_challenge/evaluation ) that are not duscussed here ( and their paper also discusses appropriateness of metrics ) . Sec.1 \u201c Considering that image compression generally loses many texture details \u201d : Not necessarily . Depends on how much compression , what type of compression , and the actual content . If you would like to claim this , I would suggest you analyse what type of structures disappear if you do a 2x sub-sampling ( based on the fact that ISBI has 2x less resolution ) . Otherwise , I would suggest this is rephrased a bit less strong . Sec.2.1 intends to do a comparison with ISBI database and SNEMI3D databases . However it still does not provide important information about them . For example , there is no mention of the actual resolution of the other databases , although the work emphasizes a lot in explaining it contains more info due to higher resolution . ISBI seems to be 4 nm x 4 nm x 50 nm / pixel . ( from http : //brainiac2.mit.edu/isbi_challenge/home ) , while the introduced one is 2.18 x 2.18 x 70 nm /pixel . Notice that ISBI resolution at z-axis actually is higher than the introduced . This should be made clear in text . Also , please add same information about SNEMI3D . Do you believe this difference in z-axis could make any difference with respect to what structures can be segmented ? ( In fact , in Sec 2.2 , you say that thicker slice affect imaging quality . ) The current database has approximately 2x resolution than ISBI in x , y plane . What type of structures do you believe are not capable of segmenting well in 4x4nm resolution , but capable at 2x2nm resolution ? To support the claim that the higher-resolution is a significant advantage ( and hence the contribution of releasing such a database is strong ) , perhaps the work should have performed an evaluation of how useful this extra resolution is in practice , to support the main contirbution . Please discuss in Sec 2.1 what anatomy are the images of each database coming from . As they are not coming from the same tissue ( e.g.this is from retina , ISBI from Larvae cord etc ) , please discuss if you think this could be a factor for qualitative differences between the databases . If you think it may be , then perhaps claims about what database is more suitable should be adjusted , as perhaps the two have a bit different purpose / characteristics ? Sec.2.1 : \u201c much more challenging \u201d : What evidence is this claim based on ? I could make the argument that ISBI may be more challenging to segment due to the lower resolution ( hence less information ) . Please ensure that you back up all claims with appropriate arguments . As it currently stands , this is an unsupported claim and should be removed . Same comment as above for the \u201c suitable in exploiting cell segmentation algorithms \u201d claim . Why more suitable to exploit algorithms ? I think this needs a rephrase . Sec.3.1 : \u201c may not be consistent with human perception\u2026 tasks \u201d , \u201c an natural first instinct was that \u201d , and in Sec 3.3 : \u201c humans are more sensitive to structure changes , instead of thickness changes. \u201d . I think these statements are not passing the correct meaning . It is not the \u201c human perception \u201d or the \u201c instinct \u201d of the humans that prioritizes thin ( non- ) existence of structures over thickness . In your experiments , the evaluators were clearly trained about what the task is . For example , perhaps they know that in cell-segmentation , where the ultimate goal is creating the connectomic , the connectomic can be created regardless the thickness , but a structure should not be missing . Hence , what wrong structures more important than thickness , is the task . Not human perception or instinct ( in fact , for me , it \u2019 s clearly easier to identify thickness , than locating a small structure missing somewhere in the images ) . I think these statements pass a wrong meaning . I would suggest that they are rephrased , to emphasize that in every task , where the segmentation itself is not the ultimate goal , the quality of a segmentation should be judged with respect to what the actual ultimate goal is ( e.g.here , creating the whole structure of how membranes are connected ? ) . And this should be reflected in the evaluation metrics , where in each task , different metrics , appropriate for the specific one should be used . Please discuss your viewpoint and your recommended amendments . Same point as the above , in Sec 3.3 : \u201c humans are more sensitive to structure changes , instead of thickness changes. \u201d : I don \u2019 t think this statement is in general true . I would say the opposite for me . I can immediately tell that the thickness differs among segmentations , but I have to focus explicitly on certain areas to find whether a specific area has been wrong segmented . I expect the fact that the humans that performed the evaluation were specifically \u201c trained \u201d ( Sec 3.2 ) that they perform * the specific task of cell segmentation * is likely what made them emphasize the actual structure and not care about the thickness . In other words , what criterion/metric is most appropriate has to do with the actual task of interest . Please discuss . I would suggest all related statements about human perception , vision or sensitivity , to be rephrased in a way that is less generic , and instead perhaps passes the message that in each task , quality of segmentation should be judged with respect to the actual ultimate goal . Sec 3.2 does not describe on what data were the segmentation methods trained . Specifically , the paper should state explicitly if the training data were different images from those that were used to create the 200 groups of images that the 20 humans evaluated the results , or were they the same . Can you please clarify this here and in the text ? Sec 3.1 : What is the difference between F1 score and Dice Coefficient ? As far as I know ( I double checked ) , these two scores seem the same to me . Phrasing in sec 3.1 suggests they are different . Am I wrong ? Please clarify . If they are the same , then perhaps one of them should be removed . Related to above : Figure4 c : I think that F1 score really is the same as Dice . After you double check , please check this figure . In Fig 4 c , the values of F1/Dice differ . How come ? Please double check and clarify . Perhaps implementation detail ? Or am I wrong ? You see that in the end of Sec 3.2 , F1-score and Dice also brought \u201c Exactly \u201d same results for correlation with human perception , agreeing with my view that F1-score/dice are the same . On the other hand , in Table 1 , F1 score and Dice differs in * some * methods ( humans , GLNet , Unet ) , but are absolutely the same for other methods ( SENet , CASENet , Unet++ , LinkNet ) . Please double check and clarify . I would suggest you check for a small implementation error ? Sorry if I misunderstand something , I am happy to hear clarifications . Sec.3.3 : I think there is no strong technical argument given for introducing tolerance ? Without the tolerance , for small offsets/differences , the distance metric will simply have low value . Humans dont `` ignore it '' , it 's just small so it does not `` bother them enough to mention '' . Which is exactly what a low value from a distance metric means . From Fig 5 , we can see that even without tolerance ( =0 ) , the metrics ( ASSD on skeleton ) behaves perfectly fine , giving higher PHD for the case ( b ) that has larger distance than case ( a ) . I would recommend adding such explanation and discussion in the paper . What is your view on the above ? Sec 3.3 & Fig 4 : \u201c suggesting human vision does have tolerance \u201d : Sure , but this does not necessarily mean it \u2019 s the right thing , right ? For example , factors for inducing human \u201c tolerance \u2019 can be limited vision capability ( our eyes are not as good as a computer in processing pixel-by-pixel ) or subjectivity with respect to the task ( e.g.if the human annotators know , or they have been trained , that 1-3 pixels is not a important * for the particular task * of cell segmentation ) . But this does not mean that a metric that has no tolerance is a bad thing , perhaps the metric is even more objective . Please discuss . \u201c Can F1-score\u2026 be improved\u2026 refutes this \u201d : This statement is wrong . Skeletonizing * does * improve all these metrics , as shown in Fig 6 . They simply don \u2019 t reach the result of PHD . Please rephrase . The evaluation could/should have included other metrics , such as basic Haussdorf distance , ASSD , or metrics used in related challenges , such as Rand etc ( see http : //brainiac2.mit.edu/isbi_challenge/evaluation ) # # # # # # # # # # # # # # # # # # # Minors , or additional feedback for improving the work in the future ( not subject to rebuttal ) : I would recommend rephrasing the phrase \u201c Surprisingly , we found \u201d in Sec.1 , as it is actually a commonly discussed issue in the literature ( see my previous comments on related work/references ) Sec.2 : \u201c provided by Marc \u2019 s lab ( Anderson et al . ( 2011 ) ) \u201d : I think this could be rephrased to a more canonical way of refering to a wold , and also be more accurately descriptive ? One that clarifies whether the data are exactly those described in Anderson et al 2011 ? Were they made publically available together with the specific paper ( Anderson et al ) ? Or have they been provided by Marc \u2019 s lab ( author of the cited work ) to you personally for this current work ? E.g.a rephrase like \u201c made publically available and described in the work of Anderson et al ( 2011 ) \u201d or something like that is more descriptive . \u201c at an x-y resolution of 2.18 nm/pixel \u201d : is the resolution the same along the 2 axes ? If so , clarify something like \u201c 2.18nm/pixel across both axes , and 70nm\u2026 \u201d \u201c from different layers \u201d : What is a layer in this context ? It has not been defined . Is it a slice ? Remember that you are addressing this to a non-domain-specific audience of ICLR , so ensure to be clear about these terms . Fig 1 : \u201c image number \u201d = > \u201c number of images \u201d reads better . Sec.3.2 : \u201c and 2 segmentation results \u201d = > 2 \u201c automatically generated \u201d segmentations ? Abstract : \u201c proposes \u201d a dataset ? Sounds wrong . I would suggest \u201c introduces \u201d a dataset . Sec 1 : \u201c \u201d how robust if these methods are compared \u201d : grammar * * UPDATE / EDIT AFTER REBUTTAL PERIOD AND UPDATES TO PAPER == * * Summary of improvements during rebuttal and remaining concerns on the updated manuscript : Main improvements : - Improved the comparison with existing EM datasets by extending descriptions and text in Sec 2.1 . - Added an experiment that shows that the same model ( Unet ) performs significantly better on previous databases than on the proposed ( Appendix VIII , IX ) . This acts as a solid empirical evidence that the current database is indeed more challenging than previous ones , which was previously missing . This supports the value of the database . - The authors also clarified that they will release all 3 sets of annotations , which can serve various types of methodological developments , as databases like this are not common . - Added a short discussion of previous work on metrics for segmentation quality , which previously was entirely missing . - Extended the analysis by incorporating multiple more metrics ( ~10 ) that were previously missing , extending significantly over the first version , where only overlap-based metrics were considered ( Dice , IoU ) . The new results support that for the specific problem of cell-segmentaiton , PHD agrees more with human perception than other metrics on this task , including the very related HD and ASSD . - Showed that the Skeletonization process , part of the proposed metric , improves various metrics ( among which the very related HD-based metrics ) , which fulfills the previous gap of empirical evidence to support its incorporation in the proposed metric . - Rephrased most points where the text was ambiguous or incorrect . Summary and Reviewer \u2019 s Score adjustment : Overall , the revision has improved the document significantly . My primary remaining concerns have to do with the actual paper being of interest primarily to the audience interested in the specific task of cell-segmentation , and that the technical value of the PHD metric is relatively limited ( as per my initial comments ) . However , the updated document supports much better the main claims of the paper , and this database could serve as a benchmark for general ML segmentation methods , benefitting the greater ML community . These improvements make me increase my score from a 3 ( Clear Reject ) to a 6 ( Above acceptance threshold ) . -- new minor problems in the updated version -- Some new minor points that I noticed . In case the work is accepted , please try to address them for camera ready : \u201c ASSD\u2026 which is not widely used in deep learning researches \u201d : Not a true statement . HD/ASSD/etc are very popular metrics in segmentation tasks ( including with DL ) and there have been efforts to even turn them into losses . Please rephrase/remove this statement . \u201c the consistency of the F1 score , IoU and Dice with the human choices was calculated \u201d ( Sec.3.2 ) : Wasn \u2019 t this done for the other metrics as well now ? Update the text . Sec.3.2 , \u201c Six popular\u2026 segmentation results \u201d : Refer readers to appendix for details on training/test config.1 ? Sec.4 , \u201c Then four evaluation\u2026 \u201d : \u201c four \u201d is not correct after the updates . Sec.4 , Discussion : \u201c it can be seen\u2026 methods \u201d : This no longer holds after the new metrics . Taking into account all metrics , if we naively count for how many metrics a method ranks 1st ( which is what the paper did in the first place ) , then it seems the best is LinkNet , followed by U-net++ , not CASENet ( which only ranks 1 for PHD-1 and PHD-3 ) .Please update the argument . Appendix V : \u201c texture \u201d = > text", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # Question 5 The scope of the paper is limited to the cell-segmentation problem . # # # Answer 5 Thank you for your concern about the scope of this paper . However , we respectfully disagree with this point . The ISBI2012 and SNEMI3D datasets dominate the evaluation of cell membrane segmentation in EM data , however , the performance of deep learning methods appears to be \u201c saturated \u201d on these datasets ( as pointed out by the reviewer 3 ) . Similar concerns were also addressed by KisukLee and his colleagues ( Kisuk Lee , et al.NeurIPS , 2017 ) that \u201c the SNEMI3D challenge has become obsolete in its present form , and must be modified or replaced by a challenge that is capable of properly evaluating algorithms that are now exceedingly accurate. \u201d In our hand , U-net easily exceeded 95 % on ISBI2012 , but dramatically dropped to about 60 % on U-RISC . Such a big gap in performance can not simply be explained by parameter tuning . In order to improve the performance on U-RISC , more substantial innovations in algorithms are needed . The U-RISC dataset may reveal several classic challenges in the field that haven \u2019 t been solved : One challenge might be the \u201c imbalance problem of samples \u201d ( Alejo R , et al , 2016 ) , ( Li D C , et al,2010 ) , ( ZhangK , et al , 2020 ) . Due to ultra-high resolution images , the pixels of labeled cell membranes only account for 5.64 % of total pixels in training sets , in contrast to 21.96 % in ISBS2012 and 33.23 % in SNEMI3D . The future design of deep learning methods on U-RISC will have to solve this issue . Some other challenges might include , e.g.ultra high-resolution image segmentation ( Ilke Demir , et al , 2018 ) , ( Hengshuang Zhao , et al , 2018 ) , ( Chen W , et al , 2019 ) ) , appropriate loss function design ( ( SudreC H , et al , 2017 ) , ( Spiring F A , et al , 1993 ) , ( Choromanska A , et al , 2015 ) ) , and the issues related to `` unclosed '' edges as suggested by the reviewer 3 . Taken together , we strongly believe that the U-RISC dataset will have great contribution in technique novelty , by revealing defects in the existing popular methods and promoting novel algorithms for solving classic challenges in machine learning or deep learning community . In addition , the design of evaluation criteria has been widely concerned in the field of computer science ( ( Gerl S , et al , 2020 ) , ( Lin P L , etal , 2015 ) , ( Liu S , et al , 2018 ) ) . The PHD we proposed may inspire researchers from a new perspective and further promote the developments of algorithms . The technical novelties of the PHD metric lie in many aspects . To listed a few , ( 1 ) It can be potentially used in other tasks , such as vascular segmentation ( Gerl S , et al , 2020 ) , bone segmentation ( Lin P L , etal , 2015 ) , edge detection ( Liu S , et al , 2018 ) , and other tasks related to structural and shape information . For example , ( Gerl S , et al,2020 ) successfully used a distance-based criterion to improve skin layer segmentation in optoacoustic images . ( 2 ) It can be modified into loss functions which is also part of our on-going work . It is worthy to note that some works have successfully integrated Hausdorff distance into the loss function ( ( Genovese C R , et al , 2012 ) , ( Karimi D , et al , 2019 ) , ( Ribera J , et al,2019 ) ) . Therefore , we think the scope of this paper is not limited to the cell-segmentation problem . # # # Question 6 The work contains a number of statements that are not true and would need significant text alterations to reduce them . # # # Answer 6 We are grateful for your careful and valuable comments . We rephrase the statements as shown in Details part and update the new version correspondingly ."}, {"review_id": "PP4KyAaBoBK-1", "review_text": "# pros : - To the author 's and reviewer 's best knowledge , this paper includes the largest annotated public EM data set for cell membrane segmentation ( in case it is published with this paper ) . Until now , the ISBI 2012 challenge ( http : //brainiac2.mit.edu/isbi_challenge/ ) dominates the evaluation of cell membrane segmentation in EM data , even though the performance is nearly saturated . New datasets can identify potential weaknesses in similar domains , that are not covered in current datasets and by state of the art methods , yet . - The discussion about suitable segmentation metrics for cell membrane segmentation is important and must be continued . - The article is written in a clear and comprehensive manner . # cons : - The discussion about appropriate metrics for cell segmentation , that do not depend on the thickness of the segmented cell membrane , has extensively been elaborated in `` Crowdsourcing the creation of image segmentation algorithms for connectomics '' by Ignacio Arganda-Carreras et al. , Frontiers in Neuroanatomy 2015 ( 9 ) 142 : pp . 1-13.However , this paper is not referenced and the therein proposed metrics are not mentioned or compared . - The evaluated `` state-of-the-art '' methods are not `` state-of-the-art '' . They do not correspond to the top entries of the current ISBI Segmentation Challenge Leaderboard . Additionally , no parameters of the methods were adapted . - It is left unclear how the 20 human raters were instructed to evaluate the segmentation results . For the correct evaluation , not ( only ) intuitive human perception must be taken into account , but also the usability of the resulting segmentation . The segmentation results on high resolution EM data presented in this paper display many `` unclosed '' edges , which lead to severe problems , when using the segmentation as a basis for connectivity analysis . To the reviewer 's understanding , the proposed Perceptual Hausdorff distance will hardly penalize these errors . - The dataset is not published in the format of a challenge , which would allow benchmarking on a private test set . - Spelling should be revised . # Summary The presented new high-quality dataset is highly valuabl to the community in order to improve and develop methods for instance segmentation , specifically cell membrane segmentation . The segmentation of thin cell boundaries imposes different challenges and includes different priors , than in other domains of instance segmentation . The use of appropriate evaluation metrics is crucial to identify suitable und successful methods in experiments and must be critically discussed including domain knowledge . However , in the presented paper , the discussion about suitable metrics is not appropriately linked to the existing literature . A metric is proposed , that is ( more ) consistent with `` human perception '' . This is an interesting aspect , but its contribution to the successful analysis of neuronal connectivity from EM data remains unclear .", "rating": "3: Clear rejection", "reply_text": "# # # Question 3 It is left unclear how the 20 human raters were instructed to evaluate the segmentation results . For the correct evaluation , not ( only ) intuitive human perception must be taken into account , but also the usability of the resulting segmentation . The segmentation results on high resolution EM data presented in this paper display many `` unclosed '' edges , which lead to severe problems , when using the segmentation as a basis for connectivity analysis . To the reviewer 's understanding , the proposed Perceptual Hausdorff distance will hardly penalize these errors . # # # Answer 3 Thank you for raising this question . Firstly , the 20 human raters were introduced the value of cell membrane segmentation to connectivity and the importance of structure before testing . And then we used several simple examples to teach them about the experimental process . During the formal experiment , the distribution and selection of data were random . The subjects only need to choose one of the two images that they think is more similar to the ground truth . A detailed introduction has been added in Appendix IV . Secondly , we are fully aware `` closed edges '' is important for segmentation and connectivity analysis . In fact , during the labeling and check process of U-RISC , whether the cell membranes were labeled as \u201c closed \u201d edges is one of the major inspection points . Therefore , our dataset itself has very few \u201c unclosed \u201d edges , which in theory should have helped training deep learning methods to avoid `` unclosed '' edges if the method itself is proper . However , the current tested methods all showed poor performance on U-RISC dataset , suggesting the displayed \u201c unclosed \u201d edges are more likely due to defects of the methods rather than defects of the U-RISC dataset . With the improvement of the performance for deep learning methods , we believe \u201c unclosed \u201d edges in the segmentation results would be greatly reduced . Although at current stage we didn \u2019 t penalize the errors of \u201c unclosed \u201d edges in the proposed PHD , we agree that such penalties might be critical for improving the overall performance of the deep learning methods . However , the remaining questions are , e.g.how much should we set the penalty ( as it is a hyperparameter ) , or should we also set penalties for other bad misalignment errors ? In future research , we will keep exploring these questions . # # # Question 4 The dataset is not published in the format of a challenge , which would allow benchmarking on a private test set . # # # Answer 4 Thanks for your valuable consideration . Due to the anonymous requirements of ICLR , we can not publish the information in the current version . After the acceptance of the paper , we will publish the dataset in the format of a challenge . # # # Question 5 Spelling should be revised . # # # Answer 5 Thanks for your careful reading and suggestions . We have carefully corrected the spelling ."}, {"review_id": "PP4KyAaBoBK-2", "review_text": "This paper presents a large high-resolution cell membrane segmentation dataset and also proposes a new evaluation metric that is more consistent with human perception . The new metric is called Perceptual Hausdorff Distance ( PHD ) , which first applies thinning to skeletonize the segmentation outcome , then computes the Hausdorff distance between skeletons . PHD has a hyper-parameter , i.e. , the tolerance distance , to represent the human 's tolerance . Overall , I think this is a good paper that addresses how to correctly evaluate cell membrane segmentation , which is essential for evaluation at a fair standard but has not been studied extensively . The authors provide strong reasons to illustrate the limitations of existing evaluation metrics , and present a relatively larger scale high-quality dataset for evaluating different techniques . The pixel number and the number of images are presented to demonstrate the advantages over ISBI 2012 and SNEMI3D . The image collection , annotation , and evaluation seem to be performed very carefully with 20 subjects involved . Some minor additional questions : i ) The measurement seems to be only tailored for cell membrane segmentation . Is it possible to make the criterion more generalized ? ii ) If I am understanding correctly , the measurement seems very hard to be modified into loss functions since it involves thinning and other heuristics . Is it possible to use PHD not only for evaluation purposes but also for improving standard training ?", "rating": "7: Good paper, accept", "reply_text": "Thanks for your positive and constructive feedbacks . We will explain your concerns point by point as follows . The references we used in rebuttal can be found in the overall response . # # # Question 1 The measurement seems to be only tailored for cell membrane segmentation . Is it possible to make the criterion more generalized ? # # # Answer 1 Thank you for your constructive consideration . Although the measurement we proposed is mainly used for cell membrane segmentation , it can be potentially used in other tasks , such as vascular segmentation ( Gerl S , et al , 2020 ) , bone segmentation ( Lin P L , et al , 2015 ) , edge detection ( Liu S , et al , 2018 ) , and other tasks related to structural and shape information . For example , ( Gerl S , et al , 2020 ) successfully used a distance-based criterion to improve skin layer segmentation in optoacoustic images . # # # Question 2 The measurement seems very hard to be modified into loss functions since it involves thinning and other heuristics . Is it possible to use PHD not only for evaluation purposes but also for improving standard training ? # # # Answer 2 Yes . PHD can be modified into loss functions which is also part of our on-going work . It is worthy to note that some works have successfully integrated Hausdorff distance into loss function ( ( Genovese C R , et al , 2012 ) , ( Karimi D , et al , 2019 ) , ( Ribera J , et al , 2019 ) ) . Considering the computational complexity caused by thinning , we can use some alternative methods such as efficient skeleton extraction ( Au O K C , et al , 2008 ) ."}, {"review_id": "PP4KyAaBoBK-3", "review_text": "Strength : ( 1 ) This work proposed an ultra-high-resolution image segmentation dataset for the cell membrane , named U-RISC . The proposed U-RISC is the largest annotated Electron Microscopy ( EM ) dataset for the cell membrane with multiple iterative annotations and uncompressed high-resolution raw data . Given the uniqueness of the proposed dataset , it is likely to contribute to the future EM based research , such as membrane segmentation . ( 2 ) For membrane segmentation in EM images , the authors develop a human-perception based evaluation criterion , called Perceptual Hausdorff Distance ( PHD ) . Based on the experiments on a small-scale dataset . the proposed PHD metric is better consistency with human perception than the traditional ones . Weakness : ( 1 ) The first concern is on the proposed PHD metrics . The reviewer thinks that there is a lack of comparison analysis between the proposed PHD with the traditional Hausdorff distance . According to Equation 2 , the PHD metric is build based on the Hausdorff distance . Therefore , it is necessary to include comparison analysis of using the traditional Hausdorff distance . This will further highlight the novelty of the PHD metric proposed in this paper . ( 2 ) The second concern is the limit technique novelty . The overall contributions of this paper have two parts : establishing a new dataset , and proposing a new PHD metric for the EM membrane segmentation tasks . However , there is no contributions based on the machine learning or deep learning based methods . The reviewer agrees that this manuscript has made some contributions on biomedical image analysis . However , the reviewer thinks this paper can not meet the requirement of the ICLR conference . ( 3 ) The third concern is the limit validation experiments on the PHD metric . Since the proposed PHD metric is particularly for membrane segmentation , it is supposed to be effective on other EM datasets , such as the ISBI2012 and SNEMI3D challenges . However , the authors did not conduct comparison methods on any other EM datasets . It would be more convincing to conduct experimental analysis on various EM membrane segmentation tasks .", "rating": "4: Ok but not good enough - rejection", "reply_text": "# # # Question 3 The third concern is the limit validation experiments on the PHD metric . Since the proposed PHD metric is particularly for membrane segmentation , it is supposed to be effective on other EM datasets , such as the ISBI2012 and SNEMI3D challenges . However , the authors did not conduct comparison methods on any other EM datasets . It would be more convincing to conduct experimental analysis on various EM membrane segmentation tasks . # # # Answer 3 Thank you for the valuable suggestion . We have added experiments on the other two EM segmentation datasets as suggested in Appendix VIII and IX . The results show that evaluation rankings for F1-score , IoU , V-Rand-sk , and V-Info-sk are more consistent with each other , but they are different from PHD-based rankings , which is consistent with previous conclusions ."}], "0": {"review_id": "PP4KyAaBoBK-0", "review_text": "Summary : This work presents two contributions towards cell membrane segmentation . First , it introduces a new labelled database for this purpose . The authors claim that this is the largest labelled database of high resolution Electron-Microscopy images for this purpose . Second , the work tackles the issue that the F1Score , Dice and IoU scores that evaluate segmentation performance by quantifying the overlap of 2 segmentations are not adequately describing quality of a segmentation with respect to what human experts would prefer for the task . This has been evaluated via employing humans ( experts on the task of cell segmentation ) to grade which segmentation they prefer , and analysed how their preference correlates with these scores . As a solution , the authors propose a metric , PHD , that can be described as the average Haussdorf distance between the skeletons of two segmentations , together with a threshold tolerance , which they show correlates better with the preference of experts with respect to segmentation quality on this task . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : I recommend a rejection of this work for the following reasons . On one hand , constructing a database and releasing it to the community is a great contribution . I am sure that this would be very well accepted . But , on the other hand , I don \u2019 t think the article adequately describes the dataset or compare it adequately with existing databases ( which makes less of a \u201c database article \u201d ) . Instead , half the article discusses a metric that is essentially an adaptation of the haussdorf distance ( actually , of the \u201c average symmetric surface distance \u201d ) , adapted in a manner specific to the cell-segmentation task ( applied on skeleton , and with a tolerance , the importance of which is questionable ) . This 2nd contribution has not been accompanied by a literature review on metrics ( e.g.only discusses IoU/F1/Dice , missing related distance based metrics like ASSD completely ) , nor adequately evaluated with such related metrics ( besides IoU/Dice/F1 ) . Finally , the modifications , along with many claims in the article , are only relevant to the specific task of cell segmentation ( and quite subjective ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . Great contribution by releasing publicly a new labelled database of high quality . Seems there was a lot of effort to construct good quality ground truth on a number of images much larger than the existing publically available databases . This is definitely interesting for the community that works in this problem . 2.Interesting human-based evaluation of the usefulness of IoU/Dice/F1 scores for the cell-segmentation problem ( by 20 humans , this is nice ) . It is known in the broader community that overlap metrics ( IoU/Dice etc ) are not perfect , hence there is a lot of work on other metrics [ 1,2 etc ] , but this substantiates/quantifies it very nicely . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : 1 . If I would judge the paper focused on the 1st contribution ( releasing a database ) , I would say that it does not contain a sufficient analysis of the database itself , and especially not an adequate description and comparison with other public databases . I think this point could be sufficiently addressed in the rebuttal . 2.From the technical viewpoint , the claimed 2nd contribution is the derivation of a new metric , but the work has not performed any literature review on related work on metrics except IoU/Dice/F1 . In fact , the work produces a metric ( Eq.2 ) that seems to me the same as Average Assymetric Surface Distance ( ASSD , see [ 1 ] ) , applied to the skeleton ( thinned ) segmentation , with a tolerance ( task-specific modifications ) . I note that both skeleton-like operations have been previously performed for computing metrics in the cell-segmentation domain ( e.g.for evaluation of ISBI2012 challenge : http : //brainiac2.mit.edu/isbi_challenge/evaluation , notice the \u201c after thinning \u201d operation ) . Tolerance-based modifications have also been applied to various other metrics ( e.g . [ 2 ] below ) and are task-specific modifications ( and not necessary for the metric to be appropriate in the general sense ) . In my opinion , this makes the value of the 2nd contribution very low . I think this point can not be sufficiently addressed in the rebuttal , as I basically think that the contribution of the derived metric is low in comparison to existing literature . 3.The \u201c skeleton \u201d part of PHD is not individually evaluated whether it actually adds substantially . I note that without it , the metric is essentially ASSD with tolerance ( tolerance being task-specifically motivated in this context ) . 4.Evaluation of the proposed metrics is limited , because it does not contain other metrics except IoU/Dice . E.g. , it should have been compared with Haussdord , ASSD , etc . 5.The scope of the paper is limited to the cell-segmentation problem . 6.The work contains a number of statements that are not true and would need significant text alterations to reduce them . The above Cons are described in detail below , with the detailed comments I raise in the \u201c Questions for rebuttal period \u201d section . References : Paper with some review on related metrics ( there are many more such reviews ) : [ 1 ] Yeghiazaryan and Voiculescu , Family of boundary overlap metrics for the evaluation of medical image segmentation , Journal of Medical Imaging , 2018 Paper implementing tolerance ( which motivates it , and is subjective to the specific task and needs ) : [ 2 ] Nikolov et al , Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy , arxiv 2018 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions/points to address during rebuttal period : The abstract \u201c proposes \u201d a database with \u201c multiple iterative annotations \u201d . However , the actual database released only contains the last 3rd annotation . Please rephrase the abstract so that this is clear to the reader that only 1 annotation should be expected . Similar to the above , Sec 2.1 claims the existence of multiple ( 3 ) annotations as an advantage of the proposed database over other databases ( Sec.2.1 , \u201c Besides that , U-RICS produced 3 sets of annotations\u2026 , all of which can be applied in developing depe learning \u201d ) . But , these 3 annotations are not released , hence their existence is irrelevant to the reader and this claim/advantage simply does not hold . The authors claim that the intermediate annotations are \u201c are very valuable for learning \u201d ( Sec.2 ) and therefore will not be released . In this case , if they are valueable , perhaps consider releasing them ? Otherwise , I would suggest rephrasing the article , reducing the emphasis on these 3 labels across the whole manuscript , as they are of little relevance to the reader . You can spend the saved space to extend on more related points ( e.g.related work etc , see below ) . Sec 1. claims : \u201c We found the human performance is far superior to these methods \u201d . I think this statement is very strong and not supported adequately by the current evaluation . I think the authors refer to experiments in Table 1. , where the labels from the 1st and 2nd iteration where compared with the results in the 3rd iteration . But , naturally , image after 3rd iteration is conditioned ( related ) very closely to those from the 1st and 2nd iterations . Of course they will have very high agreement . Also , we note , the 2nd iteration is not a result of 1 human but of multiple ( 5 experts + annotator ) . Hence , for these 2 reasons , they can not support the statement . For a correct assessment for such a claim , segmentations from a single annotator , who is not the same contributing to making the ground truth for the image , should be evaluated against the ground truth . Additionally , inter-rate segmentation performance ( multiple humans ) could also be evaluated to make such claims . I would recommend this claim to be altered , as well as state explicitly these factors ( conditioning of 1st , 2nd , 3rd iterations ) in sec 4 to explain how come agreement is so high in Table 1 . The work has performed no literature review on related metrics for segmentation . It discusses and evaluates solely overlap based metrics ( Dice/F1 & IoU ) , and then proposes a distance-based measure . The authors should have discussed and evaluated distance based measures too , and especially have a look at the Average Symmetric Surface Distance ( ASSD ) metric , which is very similar to what they propose ( Eq.2 ) . See [ 1 ] as a starting point for related metrics , but there are many more . Many of these papers raise the argument that no metric is enough for all tasks , and that \u201c quality \u201d of a segmentation is subjective to the task . Hence , for each task , one should chose the correct metric , while for \u201c objective \u201d and all-around evaluation of a \u201c general \u201d method ( e.g.an arbitrary segmentation network ) , multiple complimentary metrics should be used . See [ 2 ] also with related discussion . Also see that ISBI2012 challenge itself implemented multiple metrics ( http : //brainiac2.mit.edu/isbi_challenge/evaluation ) that are not duscussed here ( and their paper also discusses appropriateness of metrics ) . Sec.1 \u201c Considering that image compression generally loses many texture details \u201d : Not necessarily . Depends on how much compression , what type of compression , and the actual content . If you would like to claim this , I would suggest you analyse what type of structures disappear if you do a 2x sub-sampling ( based on the fact that ISBI has 2x less resolution ) . Otherwise , I would suggest this is rephrased a bit less strong . Sec.2.1 intends to do a comparison with ISBI database and SNEMI3D databases . However it still does not provide important information about them . For example , there is no mention of the actual resolution of the other databases , although the work emphasizes a lot in explaining it contains more info due to higher resolution . ISBI seems to be 4 nm x 4 nm x 50 nm / pixel . ( from http : //brainiac2.mit.edu/isbi_challenge/home ) , while the introduced one is 2.18 x 2.18 x 70 nm /pixel . Notice that ISBI resolution at z-axis actually is higher than the introduced . This should be made clear in text . Also , please add same information about SNEMI3D . Do you believe this difference in z-axis could make any difference with respect to what structures can be segmented ? ( In fact , in Sec 2.2 , you say that thicker slice affect imaging quality . ) The current database has approximately 2x resolution than ISBI in x , y plane . What type of structures do you believe are not capable of segmenting well in 4x4nm resolution , but capable at 2x2nm resolution ? To support the claim that the higher-resolution is a significant advantage ( and hence the contribution of releasing such a database is strong ) , perhaps the work should have performed an evaluation of how useful this extra resolution is in practice , to support the main contirbution . Please discuss in Sec 2.1 what anatomy are the images of each database coming from . As they are not coming from the same tissue ( e.g.this is from retina , ISBI from Larvae cord etc ) , please discuss if you think this could be a factor for qualitative differences between the databases . If you think it may be , then perhaps claims about what database is more suitable should be adjusted , as perhaps the two have a bit different purpose / characteristics ? Sec.2.1 : \u201c much more challenging \u201d : What evidence is this claim based on ? I could make the argument that ISBI may be more challenging to segment due to the lower resolution ( hence less information ) . Please ensure that you back up all claims with appropriate arguments . As it currently stands , this is an unsupported claim and should be removed . Same comment as above for the \u201c suitable in exploiting cell segmentation algorithms \u201d claim . Why more suitable to exploit algorithms ? I think this needs a rephrase . Sec.3.1 : \u201c may not be consistent with human perception\u2026 tasks \u201d , \u201c an natural first instinct was that \u201d , and in Sec 3.3 : \u201c humans are more sensitive to structure changes , instead of thickness changes. \u201d . I think these statements are not passing the correct meaning . It is not the \u201c human perception \u201d or the \u201c instinct \u201d of the humans that prioritizes thin ( non- ) existence of structures over thickness . In your experiments , the evaluators were clearly trained about what the task is . For example , perhaps they know that in cell-segmentation , where the ultimate goal is creating the connectomic , the connectomic can be created regardless the thickness , but a structure should not be missing . Hence , what wrong structures more important than thickness , is the task . Not human perception or instinct ( in fact , for me , it \u2019 s clearly easier to identify thickness , than locating a small structure missing somewhere in the images ) . I think these statements pass a wrong meaning . I would suggest that they are rephrased , to emphasize that in every task , where the segmentation itself is not the ultimate goal , the quality of a segmentation should be judged with respect to what the actual ultimate goal is ( e.g.here , creating the whole structure of how membranes are connected ? ) . And this should be reflected in the evaluation metrics , where in each task , different metrics , appropriate for the specific one should be used . Please discuss your viewpoint and your recommended amendments . Same point as the above , in Sec 3.3 : \u201c humans are more sensitive to structure changes , instead of thickness changes. \u201d : I don \u2019 t think this statement is in general true . I would say the opposite for me . I can immediately tell that the thickness differs among segmentations , but I have to focus explicitly on certain areas to find whether a specific area has been wrong segmented . I expect the fact that the humans that performed the evaluation were specifically \u201c trained \u201d ( Sec 3.2 ) that they perform * the specific task of cell segmentation * is likely what made them emphasize the actual structure and not care about the thickness . In other words , what criterion/metric is most appropriate has to do with the actual task of interest . Please discuss . I would suggest all related statements about human perception , vision or sensitivity , to be rephrased in a way that is less generic , and instead perhaps passes the message that in each task , quality of segmentation should be judged with respect to the actual ultimate goal . Sec 3.2 does not describe on what data were the segmentation methods trained . Specifically , the paper should state explicitly if the training data were different images from those that were used to create the 200 groups of images that the 20 humans evaluated the results , or were they the same . Can you please clarify this here and in the text ? Sec 3.1 : What is the difference between F1 score and Dice Coefficient ? As far as I know ( I double checked ) , these two scores seem the same to me . Phrasing in sec 3.1 suggests they are different . Am I wrong ? Please clarify . If they are the same , then perhaps one of them should be removed . Related to above : Figure4 c : I think that F1 score really is the same as Dice . After you double check , please check this figure . In Fig 4 c , the values of F1/Dice differ . How come ? Please double check and clarify . Perhaps implementation detail ? Or am I wrong ? You see that in the end of Sec 3.2 , F1-score and Dice also brought \u201c Exactly \u201d same results for correlation with human perception , agreeing with my view that F1-score/dice are the same . On the other hand , in Table 1 , F1 score and Dice differs in * some * methods ( humans , GLNet , Unet ) , but are absolutely the same for other methods ( SENet , CASENet , Unet++ , LinkNet ) . Please double check and clarify . I would suggest you check for a small implementation error ? Sorry if I misunderstand something , I am happy to hear clarifications . Sec.3.3 : I think there is no strong technical argument given for introducing tolerance ? Without the tolerance , for small offsets/differences , the distance metric will simply have low value . Humans dont `` ignore it '' , it 's just small so it does not `` bother them enough to mention '' . Which is exactly what a low value from a distance metric means . From Fig 5 , we can see that even without tolerance ( =0 ) , the metrics ( ASSD on skeleton ) behaves perfectly fine , giving higher PHD for the case ( b ) that has larger distance than case ( a ) . I would recommend adding such explanation and discussion in the paper . What is your view on the above ? Sec 3.3 & Fig 4 : \u201c suggesting human vision does have tolerance \u201d : Sure , but this does not necessarily mean it \u2019 s the right thing , right ? For example , factors for inducing human \u201c tolerance \u2019 can be limited vision capability ( our eyes are not as good as a computer in processing pixel-by-pixel ) or subjectivity with respect to the task ( e.g.if the human annotators know , or they have been trained , that 1-3 pixels is not a important * for the particular task * of cell segmentation ) . But this does not mean that a metric that has no tolerance is a bad thing , perhaps the metric is even more objective . Please discuss . \u201c Can F1-score\u2026 be improved\u2026 refutes this \u201d : This statement is wrong . Skeletonizing * does * improve all these metrics , as shown in Fig 6 . They simply don \u2019 t reach the result of PHD . Please rephrase . The evaluation could/should have included other metrics , such as basic Haussdorf distance , ASSD , or metrics used in related challenges , such as Rand etc ( see http : //brainiac2.mit.edu/isbi_challenge/evaluation ) # # # # # # # # # # # # # # # # # # # Minors , or additional feedback for improving the work in the future ( not subject to rebuttal ) : I would recommend rephrasing the phrase \u201c Surprisingly , we found \u201d in Sec.1 , as it is actually a commonly discussed issue in the literature ( see my previous comments on related work/references ) Sec.2 : \u201c provided by Marc \u2019 s lab ( Anderson et al . ( 2011 ) ) \u201d : I think this could be rephrased to a more canonical way of refering to a wold , and also be more accurately descriptive ? One that clarifies whether the data are exactly those described in Anderson et al 2011 ? Were they made publically available together with the specific paper ( Anderson et al ) ? Or have they been provided by Marc \u2019 s lab ( author of the cited work ) to you personally for this current work ? E.g.a rephrase like \u201c made publically available and described in the work of Anderson et al ( 2011 ) \u201d or something like that is more descriptive . \u201c at an x-y resolution of 2.18 nm/pixel \u201d : is the resolution the same along the 2 axes ? If so , clarify something like \u201c 2.18nm/pixel across both axes , and 70nm\u2026 \u201d \u201c from different layers \u201d : What is a layer in this context ? It has not been defined . Is it a slice ? Remember that you are addressing this to a non-domain-specific audience of ICLR , so ensure to be clear about these terms . Fig 1 : \u201c image number \u201d = > \u201c number of images \u201d reads better . Sec.3.2 : \u201c and 2 segmentation results \u201d = > 2 \u201c automatically generated \u201d segmentations ? Abstract : \u201c proposes \u201d a dataset ? Sounds wrong . I would suggest \u201c introduces \u201d a dataset . Sec 1 : \u201c \u201d how robust if these methods are compared \u201d : grammar * * UPDATE / EDIT AFTER REBUTTAL PERIOD AND UPDATES TO PAPER == * * Summary of improvements during rebuttal and remaining concerns on the updated manuscript : Main improvements : - Improved the comparison with existing EM datasets by extending descriptions and text in Sec 2.1 . - Added an experiment that shows that the same model ( Unet ) performs significantly better on previous databases than on the proposed ( Appendix VIII , IX ) . This acts as a solid empirical evidence that the current database is indeed more challenging than previous ones , which was previously missing . This supports the value of the database . - The authors also clarified that they will release all 3 sets of annotations , which can serve various types of methodological developments , as databases like this are not common . - Added a short discussion of previous work on metrics for segmentation quality , which previously was entirely missing . - Extended the analysis by incorporating multiple more metrics ( ~10 ) that were previously missing , extending significantly over the first version , where only overlap-based metrics were considered ( Dice , IoU ) . The new results support that for the specific problem of cell-segmentaiton , PHD agrees more with human perception than other metrics on this task , including the very related HD and ASSD . - Showed that the Skeletonization process , part of the proposed metric , improves various metrics ( among which the very related HD-based metrics ) , which fulfills the previous gap of empirical evidence to support its incorporation in the proposed metric . - Rephrased most points where the text was ambiguous or incorrect . Summary and Reviewer \u2019 s Score adjustment : Overall , the revision has improved the document significantly . My primary remaining concerns have to do with the actual paper being of interest primarily to the audience interested in the specific task of cell-segmentation , and that the technical value of the PHD metric is relatively limited ( as per my initial comments ) . However , the updated document supports much better the main claims of the paper , and this database could serve as a benchmark for general ML segmentation methods , benefitting the greater ML community . These improvements make me increase my score from a 3 ( Clear Reject ) to a 6 ( Above acceptance threshold ) . -- new minor problems in the updated version -- Some new minor points that I noticed . In case the work is accepted , please try to address them for camera ready : \u201c ASSD\u2026 which is not widely used in deep learning researches \u201d : Not a true statement . HD/ASSD/etc are very popular metrics in segmentation tasks ( including with DL ) and there have been efforts to even turn them into losses . Please rephrase/remove this statement . \u201c the consistency of the F1 score , IoU and Dice with the human choices was calculated \u201d ( Sec.3.2 ) : Wasn \u2019 t this done for the other metrics as well now ? Update the text . Sec.3.2 , \u201c Six popular\u2026 segmentation results \u201d : Refer readers to appendix for details on training/test config.1 ? Sec.4 , \u201c Then four evaluation\u2026 \u201d : \u201c four \u201d is not correct after the updates . Sec.4 , Discussion : \u201c it can be seen\u2026 methods \u201d : This no longer holds after the new metrics . Taking into account all metrics , if we naively count for how many metrics a method ranks 1st ( which is what the paper did in the first place ) , then it seems the best is LinkNet , followed by U-net++ , not CASENet ( which only ranks 1 for PHD-1 and PHD-3 ) .Please update the argument . Appendix V : \u201c texture \u201d = > text", "rating": "6: Marginally above acceptance threshold", "reply_text": "# # # Question 5 The scope of the paper is limited to the cell-segmentation problem . # # # Answer 5 Thank you for your concern about the scope of this paper . However , we respectfully disagree with this point . The ISBI2012 and SNEMI3D datasets dominate the evaluation of cell membrane segmentation in EM data , however , the performance of deep learning methods appears to be \u201c saturated \u201d on these datasets ( as pointed out by the reviewer 3 ) . Similar concerns were also addressed by KisukLee and his colleagues ( Kisuk Lee , et al.NeurIPS , 2017 ) that \u201c the SNEMI3D challenge has become obsolete in its present form , and must be modified or replaced by a challenge that is capable of properly evaluating algorithms that are now exceedingly accurate. \u201d In our hand , U-net easily exceeded 95 % on ISBI2012 , but dramatically dropped to about 60 % on U-RISC . Such a big gap in performance can not simply be explained by parameter tuning . In order to improve the performance on U-RISC , more substantial innovations in algorithms are needed . The U-RISC dataset may reveal several classic challenges in the field that haven \u2019 t been solved : One challenge might be the \u201c imbalance problem of samples \u201d ( Alejo R , et al , 2016 ) , ( Li D C , et al,2010 ) , ( ZhangK , et al , 2020 ) . Due to ultra-high resolution images , the pixels of labeled cell membranes only account for 5.64 % of total pixels in training sets , in contrast to 21.96 % in ISBS2012 and 33.23 % in SNEMI3D . The future design of deep learning methods on U-RISC will have to solve this issue . Some other challenges might include , e.g.ultra high-resolution image segmentation ( Ilke Demir , et al , 2018 ) , ( Hengshuang Zhao , et al , 2018 ) , ( Chen W , et al , 2019 ) ) , appropriate loss function design ( ( SudreC H , et al , 2017 ) , ( Spiring F A , et al , 1993 ) , ( Choromanska A , et al , 2015 ) ) , and the issues related to `` unclosed '' edges as suggested by the reviewer 3 . Taken together , we strongly believe that the U-RISC dataset will have great contribution in technique novelty , by revealing defects in the existing popular methods and promoting novel algorithms for solving classic challenges in machine learning or deep learning community . In addition , the design of evaluation criteria has been widely concerned in the field of computer science ( ( Gerl S , et al , 2020 ) , ( Lin P L , etal , 2015 ) , ( Liu S , et al , 2018 ) ) . The PHD we proposed may inspire researchers from a new perspective and further promote the developments of algorithms . The technical novelties of the PHD metric lie in many aspects . To listed a few , ( 1 ) It can be potentially used in other tasks , such as vascular segmentation ( Gerl S , et al , 2020 ) , bone segmentation ( Lin P L , etal , 2015 ) , edge detection ( Liu S , et al , 2018 ) , and other tasks related to structural and shape information . For example , ( Gerl S , et al,2020 ) successfully used a distance-based criterion to improve skin layer segmentation in optoacoustic images . ( 2 ) It can be modified into loss functions which is also part of our on-going work . It is worthy to note that some works have successfully integrated Hausdorff distance into the loss function ( ( Genovese C R , et al , 2012 ) , ( Karimi D , et al , 2019 ) , ( Ribera J , et al,2019 ) ) . Therefore , we think the scope of this paper is not limited to the cell-segmentation problem . # # # Question 6 The work contains a number of statements that are not true and would need significant text alterations to reduce them . # # # Answer 6 We are grateful for your careful and valuable comments . We rephrase the statements as shown in Details part and update the new version correspondingly ."}, "1": {"review_id": "PP4KyAaBoBK-1", "review_text": "# pros : - To the author 's and reviewer 's best knowledge , this paper includes the largest annotated public EM data set for cell membrane segmentation ( in case it is published with this paper ) . Until now , the ISBI 2012 challenge ( http : //brainiac2.mit.edu/isbi_challenge/ ) dominates the evaluation of cell membrane segmentation in EM data , even though the performance is nearly saturated . New datasets can identify potential weaknesses in similar domains , that are not covered in current datasets and by state of the art methods , yet . - The discussion about suitable segmentation metrics for cell membrane segmentation is important and must be continued . - The article is written in a clear and comprehensive manner . # cons : - The discussion about appropriate metrics for cell segmentation , that do not depend on the thickness of the segmented cell membrane , has extensively been elaborated in `` Crowdsourcing the creation of image segmentation algorithms for connectomics '' by Ignacio Arganda-Carreras et al. , Frontiers in Neuroanatomy 2015 ( 9 ) 142 : pp . 1-13.However , this paper is not referenced and the therein proposed metrics are not mentioned or compared . - The evaluated `` state-of-the-art '' methods are not `` state-of-the-art '' . They do not correspond to the top entries of the current ISBI Segmentation Challenge Leaderboard . Additionally , no parameters of the methods were adapted . - It is left unclear how the 20 human raters were instructed to evaluate the segmentation results . For the correct evaluation , not ( only ) intuitive human perception must be taken into account , but also the usability of the resulting segmentation . The segmentation results on high resolution EM data presented in this paper display many `` unclosed '' edges , which lead to severe problems , when using the segmentation as a basis for connectivity analysis . To the reviewer 's understanding , the proposed Perceptual Hausdorff distance will hardly penalize these errors . - The dataset is not published in the format of a challenge , which would allow benchmarking on a private test set . - Spelling should be revised . # Summary The presented new high-quality dataset is highly valuabl to the community in order to improve and develop methods for instance segmentation , specifically cell membrane segmentation . The segmentation of thin cell boundaries imposes different challenges and includes different priors , than in other domains of instance segmentation . The use of appropriate evaluation metrics is crucial to identify suitable und successful methods in experiments and must be critically discussed including domain knowledge . However , in the presented paper , the discussion about suitable metrics is not appropriately linked to the existing literature . A metric is proposed , that is ( more ) consistent with `` human perception '' . This is an interesting aspect , but its contribution to the successful analysis of neuronal connectivity from EM data remains unclear .", "rating": "3: Clear rejection", "reply_text": "# # # Question 3 It is left unclear how the 20 human raters were instructed to evaluate the segmentation results . For the correct evaluation , not ( only ) intuitive human perception must be taken into account , but also the usability of the resulting segmentation . The segmentation results on high resolution EM data presented in this paper display many `` unclosed '' edges , which lead to severe problems , when using the segmentation as a basis for connectivity analysis . To the reviewer 's understanding , the proposed Perceptual Hausdorff distance will hardly penalize these errors . # # # Answer 3 Thank you for raising this question . Firstly , the 20 human raters were introduced the value of cell membrane segmentation to connectivity and the importance of structure before testing . And then we used several simple examples to teach them about the experimental process . During the formal experiment , the distribution and selection of data were random . The subjects only need to choose one of the two images that they think is more similar to the ground truth . A detailed introduction has been added in Appendix IV . Secondly , we are fully aware `` closed edges '' is important for segmentation and connectivity analysis . In fact , during the labeling and check process of U-RISC , whether the cell membranes were labeled as \u201c closed \u201d edges is one of the major inspection points . Therefore , our dataset itself has very few \u201c unclosed \u201d edges , which in theory should have helped training deep learning methods to avoid `` unclosed '' edges if the method itself is proper . However , the current tested methods all showed poor performance on U-RISC dataset , suggesting the displayed \u201c unclosed \u201d edges are more likely due to defects of the methods rather than defects of the U-RISC dataset . With the improvement of the performance for deep learning methods , we believe \u201c unclosed \u201d edges in the segmentation results would be greatly reduced . Although at current stage we didn \u2019 t penalize the errors of \u201c unclosed \u201d edges in the proposed PHD , we agree that such penalties might be critical for improving the overall performance of the deep learning methods . However , the remaining questions are , e.g.how much should we set the penalty ( as it is a hyperparameter ) , or should we also set penalties for other bad misalignment errors ? In future research , we will keep exploring these questions . # # # Question 4 The dataset is not published in the format of a challenge , which would allow benchmarking on a private test set . # # # Answer 4 Thanks for your valuable consideration . Due to the anonymous requirements of ICLR , we can not publish the information in the current version . After the acceptance of the paper , we will publish the dataset in the format of a challenge . # # # Question 5 Spelling should be revised . # # # Answer 5 Thanks for your careful reading and suggestions . We have carefully corrected the spelling ."}, "2": {"review_id": "PP4KyAaBoBK-2", "review_text": "This paper presents a large high-resolution cell membrane segmentation dataset and also proposes a new evaluation metric that is more consistent with human perception . The new metric is called Perceptual Hausdorff Distance ( PHD ) , which first applies thinning to skeletonize the segmentation outcome , then computes the Hausdorff distance between skeletons . PHD has a hyper-parameter , i.e. , the tolerance distance , to represent the human 's tolerance . Overall , I think this is a good paper that addresses how to correctly evaluate cell membrane segmentation , which is essential for evaluation at a fair standard but has not been studied extensively . The authors provide strong reasons to illustrate the limitations of existing evaluation metrics , and present a relatively larger scale high-quality dataset for evaluating different techniques . The pixel number and the number of images are presented to demonstrate the advantages over ISBI 2012 and SNEMI3D . The image collection , annotation , and evaluation seem to be performed very carefully with 20 subjects involved . Some minor additional questions : i ) The measurement seems to be only tailored for cell membrane segmentation . Is it possible to make the criterion more generalized ? ii ) If I am understanding correctly , the measurement seems very hard to be modified into loss functions since it involves thinning and other heuristics . Is it possible to use PHD not only for evaluation purposes but also for improving standard training ?", "rating": "7: Good paper, accept", "reply_text": "Thanks for your positive and constructive feedbacks . We will explain your concerns point by point as follows . The references we used in rebuttal can be found in the overall response . # # # Question 1 The measurement seems to be only tailored for cell membrane segmentation . Is it possible to make the criterion more generalized ? # # # Answer 1 Thank you for your constructive consideration . Although the measurement we proposed is mainly used for cell membrane segmentation , it can be potentially used in other tasks , such as vascular segmentation ( Gerl S , et al , 2020 ) , bone segmentation ( Lin P L , et al , 2015 ) , edge detection ( Liu S , et al , 2018 ) , and other tasks related to structural and shape information . For example , ( Gerl S , et al , 2020 ) successfully used a distance-based criterion to improve skin layer segmentation in optoacoustic images . # # # Question 2 The measurement seems very hard to be modified into loss functions since it involves thinning and other heuristics . Is it possible to use PHD not only for evaluation purposes but also for improving standard training ? # # # Answer 2 Yes . PHD can be modified into loss functions which is also part of our on-going work . It is worthy to note that some works have successfully integrated Hausdorff distance into loss function ( ( Genovese C R , et al , 2012 ) , ( Karimi D , et al , 2019 ) , ( Ribera J , et al , 2019 ) ) . Considering the computational complexity caused by thinning , we can use some alternative methods such as efficient skeleton extraction ( Au O K C , et al , 2008 ) ."}, "3": {"review_id": "PP4KyAaBoBK-3", "review_text": "Strength : ( 1 ) This work proposed an ultra-high-resolution image segmentation dataset for the cell membrane , named U-RISC . The proposed U-RISC is the largest annotated Electron Microscopy ( EM ) dataset for the cell membrane with multiple iterative annotations and uncompressed high-resolution raw data . Given the uniqueness of the proposed dataset , it is likely to contribute to the future EM based research , such as membrane segmentation . ( 2 ) For membrane segmentation in EM images , the authors develop a human-perception based evaluation criterion , called Perceptual Hausdorff Distance ( PHD ) . Based on the experiments on a small-scale dataset . the proposed PHD metric is better consistency with human perception than the traditional ones . Weakness : ( 1 ) The first concern is on the proposed PHD metrics . The reviewer thinks that there is a lack of comparison analysis between the proposed PHD with the traditional Hausdorff distance . According to Equation 2 , the PHD metric is build based on the Hausdorff distance . Therefore , it is necessary to include comparison analysis of using the traditional Hausdorff distance . This will further highlight the novelty of the PHD metric proposed in this paper . ( 2 ) The second concern is the limit technique novelty . The overall contributions of this paper have two parts : establishing a new dataset , and proposing a new PHD metric for the EM membrane segmentation tasks . However , there is no contributions based on the machine learning or deep learning based methods . The reviewer agrees that this manuscript has made some contributions on biomedical image analysis . However , the reviewer thinks this paper can not meet the requirement of the ICLR conference . ( 3 ) The third concern is the limit validation experiments on the PHD metric . Since the proposed PHD metric is particularly for membrane segmentation , it is supposed to be effective on other EM datasets , such as the ISBI2012 and SNEMI3D challenges . However , the authors did not conduct comparison methods on any other EM datasets . It would be more convincing to conduct experimental analysis on various EM membrane segmentation tasks .", "rating": "4: Ok but not good enough - rejection", "reply_text": "# # # Question 3 The third concern is the limit validation experiments on the PHD metric . Since the proposed PHD metric is particularly for membrane segmentation , it is supposed to be effective on other EM datasets , such as the ISBI2012 and SNEMI3D challenges . However , the authors did not conduct comparison methods on any other EM datasets . It would be more convincing to conduct experimental analysis on various EM membrane segmentation tasks . # # # Answer 3 Thank you for the valuable suggestion . We have added experiments on the other two EM segmentation datasets as suggested in Appendix VIII and IX . The results show that evaluation rankings for F1-score , IoU , V-Rand-sk , and V-Info-sk are more consistent with each other , but they are different from PHD-based rankings , which is consistent with previous conclusions ."}}