{"year": "2019", "forum": "HkElFj0qYQ", "title": "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning", "decision": "Reject", "meta_review": "This paper presents a new defense against adversarial examples using random permutations and a Fourier transform. The technique is clearly novel, and the paper is clearly written. \n\nHowever, as the reviewers and commenters pointed out, there is a significant degradation in natural accuracy, which does not seem to be easily recoverable. This degradation is due to the random permutation of the images, which effectively disallows the use of convolutions. \n\nFurthermore, Reviewer 1 points out that the baselines are insufficient, as the authors do not explore (a) learning the transformation, or (b) using expectation over transformation to attack the model. \n\nThis concern is further validated by the fact that Black-box attacks are often the best-performing, which is a sign of gradient masking. The authors try to address this by performing an attack against an ensemble of models, and against a substitute model attack. However, attacking an ensemble is not equivalent to optimizing the expectation, which would require sampling a new permutation at each step. \n\nThe paper thus requires significantly stronger baselines and attacks.", "reviews": [{"review_id": "HkElFj0qYQ-0", "review_text": "The Paper is written rather well and addresses relevant research questions. In summary the authors propose a simple and intuitive method to improve the defense on adversarial attacks by combining random permutations and using a 2d DFT. The experiments with regards to robustness to adversarial attacks I find convincing, however the overall performance is not very good (such as the accuracy on Cifar10). My main points of critique are: 1. The test accuracy on Cifar10 seems to be quite low, due to the permutation of the inputs. This makes me question how favorable the trade-off between robustness vs performance is. 2. The authors state \"We believe that better results on clean images automatically translate to better results on adversarial examples\" I am not sure if this is true. One counter argument is that better results on clean images can be obtained by memorizing more structure of the data (see [1]). But if more memorizing (as opposed to generalization) happens, the classifier is more easily fooled (the decision boundary is more complicated and exploitable). [1] Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2016). Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the valuable feedback and address the comments in the following : Reviewer comment : The test accuracy on Cifar10 seems to be quite low , due to the permutation of the inputs . This makes me question how favorable the trade-off between robustness vs performance is . Our response : Training on CIFAR-10 is a much more complicated task compared to MNIST and moving to the permutation-phase domain makes it even more difficult . We do n't know at the moment what type of network structure and learning technique results in the best accuracy in the permutation-phase domain , but our experiments demonstrate that even a simple 3 layer dense neural network that achieves 48\\ % accuracy on clean test images , provides SOTA robustness . We believe that using techniques such as transfer learning helps to improve accuracy on CIFAR-10 data set , but this idea requires more time and resources to evaluate and we leave it for future work . Reviewer comment : The authors state `` We believe that better results on clean images automatically translate to better results on adversarial examples '' . I am not sure if this is true . One counter argument is that better results on clean images can be obtained by memorizing more structure of the data ( see [ 1 ] ) . But if more memorizing ( as opposed to generalization ) happens , the classifier is more easily fooled ( the decision boundary is more complicated and exploitable ) . Our response : By better results on clean images , we mean better results on clean `` test '' images . The paper referred by the reviewer states the ability of neural networks to memorize the entire training data set and reaching 100 % training accuracy while achieving only random guess on testing data set . This is absolutely correct and in fact we have seen it even in the permutation-phase domain that training accuracy can reach as high as 100 % while generalizing poorly on testing data set ( although not reported in the paper ) . However , our claim states that a PPD model that can reach high clean `` test '' accuracy , will not perform poorly on adversarial examples . This is simply because PPD breaks adversarial perturbation of pixel domain to random noise in the permutation-phase domain . Thus , the only goal left for future work is to increase clean test accuracy ."}, {"review_id": "HkElFj0qYQ-1", "review_text": "This paper proposes Permutation Phase Defense (PPD), a novel image hiding method to resist adversarial attacks. PPD relies on safekeeping of the key, specifically the seed used for permuting the image pixels. The paper demonstrated the method on MNIST and CIFAR10, and evaluates it against a number of adversarial attacks. The method appears to be robust across attacks and distortion levels. The idea is clearly presented and evaluated. *Details to Improve* It would be interesting to see how performance degrades if the opponent trains with an ensemble of random keys. It would be great to see this extended to convolutional networks. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the positive feedback . Reviewer comment : It would be interesting to see how performance degrades if the opponent trains with an ensemble of random keys . Our response : Per the reviewer requested , we tested PPD against expectation over transformation ( EoT ) attack ( https : //arxiv.org/abs/1707.07397 ) . EoT uses an ensemble of 30 PPD models to make adversarial examples . Our experiments showed that EoT could not degrade the performance more than an adversary that uses a single PPD model . One possible explanation is that each permutation yields a unique domain . In other words , information gained by other domains does not reveal much about the unknown domains . Reviewer comment : It would be great to see this extended to convolutional networks . Our response : We have observed that PPD shows robustness even in the case that convolutional networks are used . However , the accuracy in the convolutional networks is slightly worse than dense networks . For example , convolutional nets achieved around 97 % ( rather than 98 % ) on MNIST dataset and 44 % ( rather than 48 % ) on CIFAR-10 dataset . One possible explanation for this observation is that the permutation block breaks the local properties of the images exploited by convolutional networks ."}, {"review_id": "HkElFj0qYQ-2", "review_text": "This paper explores the idea of utilizing a secret random permutation in the Fourier phase domain to defense against adversarial examples. The idea is drawn from cryptography, where the random permutation is treated as a secret key that the adversarial does not have access to. This setting has practical limitations, but is plausible in theory. While the defense technique is certainly novel and inspired, its use case seems limited to simple datasets such as MNIST. The permuted phase component does not admit weight sharing and invariances exploited by convolutional networks, which results in severely hindered clean accuracy -- only 96% on MNIST and 45% on CIFAR-10 for a single model. While the security of a model against adversarial attacks is important, a defense should not sacrifice clean accuracy to such an extent. For this weakness, I recommend rejection but encourage the authors to continue exploring in this direction for a more suitable scheme that does not compromise clean accuracy. Pros: - Novel defense technique against very challenging white-box attacks. - Sound threat model drawn from traditional security. - Clearly written. Cons: - Poor clean accuracy makes the technique very impractical. - Insufficient baselines. While the permutation is kept as a secret, it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs. Also, the adversary may attack an ensemble of PPD models for different random permutations (i.e. expectation over random permutations). The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the detailed feedback and address the comments below : Reviewer comment : Poor clean accuracy makes the technique very impractical . Our response : The 48 % accuracy on CIFAR-10 is for a simple 3 layer dense neural network and our goal was to show that even with such a simple network , SOTA robustness can be achieved . We believe that high accuracy combined with adversarial robustness is possible for CIFAR-10 , and transfer learning shows promise in this direction . What we plan to do as future work is to replace the neural network in the PPD pipeline with a pre-trained model on massive datasets such as ImageNet and retrain the final layers to fit the permutation-phase domain . Reviewer comment : Insufficient baselines . While the permutation is kept as a secret , it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs . Our response : Thanks for bringing this attack scenario to our attention . To test PPD against an adversary that tries to learn the transformation , we used Blackbox attack ( https : //arxiv.org/abs/1602.02697 ) . In this attack , adversary probes an ensemble of PPD models as a black box by enough input-output pairs and trains a substitute model . The substitute model is then used to craft adversarial examples . Table 1 is updated with the Blackbox results . Reviewer comment : The adversary may attack an ensemble of PPD models for different random permutations ( i.e. , expectation over random permutations ) . The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model . Our response : Per the reviewer 's request , we tested PPD against expectation over transformation ( EoT ) ( https : //arxiv.org/abs/1707.07397 ) where the permutation is considered as the transformation . 30 PPD models ( with different permutations ) are used for EoT . The adversarial examples are then fed to an ensemble of 10 PPD models ( with different permutations from the 30 models ) . Our experiments show that EoT can not decrease accuracy more than an adversary that attacks with a single model . One possible explanation is that EoT is mostly useful in the case that sampling a few transformations provides a good approximation of the expectation over transformation . For example , two scenarios that EoT is shown to be successful are : ( 1 ) synthesizing adversarial examples that are robust to camera viewpoint shift and ( 2 ) breaking a defense that randomly drops pixels of the image and replaces them with total variance minimization . In both of these two scenarios , sampling a few transformations gives a good idea of the expectation . However , in PPD , each transformation has its own fingerprint which is totally different from others ."}], "0": {"review_id": "HkElFj0qYQ-0", "review_text": "The Paper is written rather well and addresses relevant research questions. In summary the authors propose a simple and intuitive method to improve the defense on adversarial attacks by combining random permutations and using a 2d DFT. The experiments with regards to robustness to adversarial attacks I find convincing, however the overall performance is not very good (such as the accuracy on Cifar10). My main points of critique are: 1. The test accuracy on Cifar10 seems to be quite low, due to the permutation of the inputs. This makes me question how favorable the trade-off between robustness vs performance is. 2. The authors state \"We believe that better results on clean images automatically translate to better results on adversarial examples\" I am not sure if this is true. One counter argument is that better results on clean images can be obtained by memorizing more structure of the data (see [1]). But if more memorizing (as opposed to generalization) happens, the classifier is more easily fooled (the decision boundary is more complicated and exploitable). [1] Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2016). Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the valuable feedback and address the comments in the following : Reviewer comment : The test accuracy on Cifar10 seems to be quite low , due to the permutation of the inputs . This makes me question how favorable the trade-off between robustness vs performance is . Our response : Training on CIFAR-10 is a much more complicated task compared to MNIST and moving to the permutation-phase domain makes it even more difficult . We do n't know at the moment what type of network structure and learning technique results in the best accuracy in the permutation-phase domain , but our experiments demonstrate that even a simple 3 layer dense neural network that achieves 48\\ % accuracy on clean test images , provides SOTA robustness . We believe that using techniques such as transfer learning helps to improve accuracy on CIFAR-10 data set , but this idea requires more time and resources to evaluate and we leave it for future work . Reviewer comment : The authors state `` We believe that better results on clean images automatically translate to better results on adversarial examples '' . I am not sure if this is true . One counter argument is that better results on clean images can be obtained by memorizing more structure of the data ( see [ 1 ] ) . But if more memorizing ( as opposed to generalization ) happens , the classifier is more easily fooled ( the decision boundary is more complicated and exploitable ) . Our response : By better results on clean images , we mean better results on clean `` test '' images . The paper referred by the reviewer states the ability of neural networks to memorize the entire training data set and reaching 100 % training accuracy while achieving only random guess on testing data set . This is absolutely correct and in fact we have seen it even in the permutation-phase domain that training accuracy can reach as high as 100 % while generalizing poorly on testing data set ( although not reported in the paper ) . However , our claim states that a PPD model that can reach high clean `` test '' accuracy , will not perform poorly on adversarial examples . This is simply because PPD breaks adversarial perturbation of pixel domain to random noise in the permutation-phase domain . Thus , the only goal left for future work is to increase clean test accuracy ."}, "1": {"review_id": "HkElFj0qYQ-1", "review_text": "This paper proposes Permutation Phase Defense (PPD), a novel image hiding method to resist adversarial attacks. PPD relies on safekeeping of the key, specifically the seed used for permuting the image pixels. The paper demonstrated the method on MNIST and CIFAR10, and evaluates it against a number of adversarial attacks. The method appears to be robust across attacks and distortion levels. The idea is clearly presented and evaluated. *Details to Improve* It would be interesting to see how performance degrades if the opponent trains with an ensemble of random keys. It would be great to see this extended to convolutional networks. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the positive feedback . Reviewer comment : It would be interesting to see how performance degrades if the opponent trains with an ensemble of random keys . Our response : Per the reviewer requested , we tested PPD against expectation over transformation ( EoT ) attack ( https : //arxiv.org/abs/1707.07397 ) . EoT uses an ensemble of 30 PPD models to make adversarial examples . Our experiments showed that EoT could not degrade the performance more than an adversary that uses a single PPD model . One possible explanation is that each permutation yields a unique domain . In other words , information gained by other domains does not reveal much about the unknown domains . Reviewer comment : It would be great to see this extended to convolutional networks . Our response : We have observed that PPD shows robustness even in the case that convolutional networks are used . However , the accuracy in the convolutional networks is slightly worse than dense networks . For example , convolutional nets achieved around 97 % ( rather than 98 % ) on MNIST dataset and 44 % ( rather than 48 % ) on CIFAR-10 dataset . One possible explanation for this observation is that the permutation block breaks the local properties of the images exploited by convolutional networks ."}, "2": {"review_id": "HkElFj0qYQ-2", "review_text": "This paper explores the idea of utilizing a secret random permutation in the Fourier phase domain to defense against adversarial examples. The idea is drawn from cryptography, where the random permutation is treated as a secret key that the adversarial does not have access to. This setting has practical limitations, but is plausible in theory. While the defense technique is certainly novel and inspired, its use case seems limited to simple datasets such as MNIST. The permuted phase component does not admit weight sharing and invariances exploited by convolutional networks, which results in severely hindered clean accuracy -- only 96% on MNIST and 45% on CIFAR-10 for a single model. While the security of a model against adversarial attacks is important, a defense should not sacrifice clean accuracy to such an extent. For this weakness, I recommend rejection but encourage the authors to continue exploring in this direction for a more suitable scheme that does not compromise clean accuracy. Pros: - Novel defense technique against very challenging white-box attacks. - Sound threat model drawn from traditional security. - Clearly written. Cons: - Poor clean accuracy makes the technique very impractical. - Insufficient baselines. While the permutation is kept as a secret, it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs. Also, the adversary may attack an ensemble of PPD models for different random permutations (i.e. expectation over random permutations). The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the detailed feedback and address the comments below : Reviewer comment : Poor clean accuracy makes the technique very impractical . Our response : The 48 % accuracy on CIFAR-10 is for a simple 3 layer dense neural network and our goal was to show that even with such a simple network , SOTA robustness can be achieved . We believe that high accuracy combined with adversarial robustness is possible for CIFAR-10 , and transfer learning shows promise in this direction . What we plan to do as future work is to replace the neural network in the PPD pipeline with a pre-trained model on massive datasets such as ImageNet and retrain the final layers to fit the permutation-phase domain . Reviewer comment : Insufficient baselines . While the permutation is kept as a secret , it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs . Our response : Thanks for bringing this attack scenario to our attention . To test PPD against an adversary that tries to learn the transformation , we used Blackbox attack ( https : //arxiv.org/abs/1602.02697 ) . In this attack , adversary probes an ensemble of PPD models as a black box by enough input-output pairs and trains a substitute model . The substitute model is then used to craft adversarial examples . Table 1 is updated with the Blackbox results . Reviewer comment : The adversary may attack an ensemble of PPD models for different random permutations ( i.e. , expectation over random permutations ) . The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model . Our response : Per the reviewer 's request , we tested PPD against expectation over transformation ( EoT ) ( https : //arxiv.org/abs/1707.07397 ) where the permutation is considered as the transformation . 30 PPD models ( with different permutations ) are used for EoT . The adversarial examples are then fed to an ensemble of 10 PPD models ( with different permutations from the 30 models ) . Our experiments show that EoT can not decrease accuracy more than an adversary that attacks with a single model . One possible explanation is that EoT is mostly useful in the case that sampling a few transformations provides a good approximation of the expectation over transformation . For example , two scenarios that EoT is shown to be successful are : ( 1 ) synthesizing adversarial examples that are robust to camera viewpoint shift and ( 2 ) breaking a defense that randomly drops pixels of the image and replaces them with total variance minimization . In both of these two scenarios , sampling a few transformations gives a good idea of the expectation . However , in PPD , each transformation has its own fingerprint which is totally different from others ."}}