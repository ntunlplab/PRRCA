{"year": "2019", "forum": "rJg4J3CqFm", "title": "Learning Embeddings into Entropic Wasserstein Spaces", "decision": "Accept (Poster)", "meta_review": "\n+ An interesting and original idea of embedding words into the (very low dimensional) Wasserstein space, i.e. clouds of points in a low-dimensional space\n+ As the space is low-dimensional (2D), it can be directly visualized. \n+ I could imagine the technique to be useful in social / human science for data visualization, the visualization is more faithful to what the model is doing than t-SNE plots of high-dimensional embeddings\n+ Though not the first method to embed words as densities but seemingly the first one which shows that multi-modality  / multiple senses are captured (except for models which capture discrete senses)\n+ The paper is very well written\n\n-  The results are not very convincing but show that embeddings do capture word similarity (even when training the model on a small dataset)\n-  The approach is not very scalable (hence evaluation on 17M corpus)\n-  The method cannot be used to deal with data sparsity, though (very) interesting for visualization\n-  This is mostly an empirical paper (i.e. an interesting application of an existing method)\n\nThe reviewers are split. One reviewer is negative as they are unclear what the technical contribution is (but seems a bit biased against empirical papers). Another two find the paper very interesting. \n\n\n\n\n", "reviews": [{"review_id": "rJg4J3CqFm-0", "review_text": "This paper learns embeddings in a discrete space of probability distributions, endowed with a (regularized) Wasserstein distance. pros: - interesting idea, nice results, mostly readable presentation. - the paper is mostly experimental but the message delivers clearly the paper\u2019s objective - the direct visualisation is interesting - the paper suggests interesting problems related to the technique cons: - to be fair with the technique, the title should mention the fact that the paper minimises a regularised version of Wasserstein distances (Wasserstein -> Sinkhorn ? put \u201cregularised\" ?) - and to be fair, the paper should put some warnings related to regularisation -- this is not a distance anymore, sparsity is affected by regularisation (which may affect visualisation). Put some reminders in the conclusion, reword at least the third paragraph in the introduction. - the paper could have been a little bit more detailed on Section 2.3, in particular for its third paragraph. Even when it is an experimental paper. - the direct visualisation is interesting in the general case but has in fact a problem when distributions are highly multimodal, which can be the case in NLP. This blurs the interpretation. - the paper delivers a superficial message on the representation: I do not consider that nice having modes near physical locations (Paris, France) is wrong. It is also a city. However, it would have been interesting to see the modes of \u201ccity\u201d (or similar) to check whether the system indeed did something semantically wrong. Questions: - beyond that last remark comes the problem as to whether one can ensure that semantic hierarchies appear in the plot: for example if Nice was only a city, would we observe a minimal intersection with the support of word \u201ccity\u201d ? (intersection to be understood at minimal level set, not necessarily 0). ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your helpful comments ! We made several small revisions , according to your suggestions . To make it more clear that we are using regularized transport , we have made four changes : 1 . The title is now \u201c Learning Entropic Wasserstein Embeddings. \u201d 2 . In the introduction , we expanded the existing discussion of the Sinkhorn divergence and moved it to a separate paragraph ( now the fifth paragraph ) . 3.In Section 2.3 , we note that we are using the Sinkhorn divergence , and briefly discuss its theoretical properties . 4.In the conclusion , we state that our empirical results are for the Sinkhorn divergence . We have expanded the third paragraph of Section 2.3 , to provide more detail on embedding capacity results for Wasserstein spaces . Our intent with including \u201c nice \u201d in the visualization section was to show first an error ( Figure 3c ) , which is that \u201c nice \u201d is visibly distant from \u201c kind \u201d ( a synonym ) , then to show the explanation for the error ( Figure 3d ) , which is that the network learned that \u201c nice \u201d is a city in France , while ignoring its second meaning . To make this clearer , we have changed the caption for Figure 3d to \u201c Explaining a failed association. \u201d Please let us know if this addresses your comment . The idea to look at semantic hierarchies is very interesting , and we did investigate this briefly . We observed that the minimal level sets of the parent and child in the hierarchy were often partially overlapping -- for instance the embedding of \u201c city \u201d has two major modes , one of which overlaps with a number of cities , including \u201c nice. \u201d As you point out , however , it is not immediately clear how hierarchies should manifest in this type of embedding , particularly as the parent can have semantics not shared by the children ( e.g. \u201c state , \u201d which has multiple non-geographic meanings ) . Perhaps the parent ( or one of its modes ) should be near to the barycenter of the children ? This would be interesting to investigate further . Thank you again for your feedback !"}, {"review_id": "rJg4J3CqFm-1", "review_text": "The paper \u2018Learning Discrete Wasserstein Embeddings' describes a new embedding method that, contrary to usual embedding approaches, does not try to embed (complex, structured) data into an Hilbertian space where Euclidean distance is used, but rather to the space of probability measures endowed with the Wasserstein distance. As such, data are embed on an empirical distribution supported by Diracs, which locations can be determined by a map that is learnt from data. Interestingly, authors note a 'potential universality' for W_p(R^3) (from a result of Andoni et al., 2015), suggesting that having Diracs in R^3 could embed potentially any kind of metric on symbolic data. Experimental validations are presented on graph and word embedding, and a discussion on visualization of the embedding is also proposed (since the Diracs are located in a low dimensional space). All in all the paper is very clear and interesting. The idea of embedding in a Wasserstein space is original (up to my knowledge) and well described. I definitely believe that this work should be presented at ICLR. I have a couple of questions and remarks for the authors: - It is noted in section 3.2 that both Diracs location and associated weights could be optimized. Yet the authors chose to only optimize locations. Why not only optimizing the weights (as in an Eulerian view of probability distributions) ? The sentence involving works of Brancolini and Claici 2018 is not clear to me. Why weighting does not improve asymptotically the approximation quality ? - Introducing the entropic regularization is mainly done for being able to differentiate the Wasserstein distance. However, few is said on the embeddability of metrics in W^\\lambda_p(R). Is using an entropic version of W moderating the capacity of embedding ? At least experimentally, a discussion could be made on the choice of the regularization parameter, at least in section 4.1. In eq. (9), it seems that it is not the regularized version of W. ? - I assume that the mapping is hard to invert, but did the authors tried to experiment reconstructing an object of interest by following a geodesic in the Wasserstein space ? - It seems to me that authors never give generalization results. What is the performance of the metric approximation when tested on unseen graphs or words ? This point should be clarified in the experiment. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thoughtful comments ! We have made several small edits , according to your suggestions . ( Brancolini 2009 ) and ( Kloeckner 2012 ) show that , when using a weighted point cloud to approximate an absolutely continuous , compactly-supported measure , the order of convergence in p-Wasserstein distance when allowing non-uniform weights is O ( n^ ( -1/d ) ) , which is the same rate as when restricted to uniform weights ( Dudley 1969 ) . Non-uniform weights might buy you an improved constant term , but the rate is the same . We have expanded the description of this fact in Section 3.2 , for clarity . The embedding capacity of W^\\lambda_p ( R^d ) is unknown , so far as we are aware , except in the weak sense that the approximation error with respect to the p-Wasserstein distance vanishes as the regularizer is taken zero ( Carlier 2017 ; Genevay 2018 ) . We have added a discussion of this distinction between W_p and W^\\lambda_p to Section 2.3 . Inverting the mapping and following geodesics in Wasserstein space would definitely be interesting . We have added this to the suggested future work in the conclusion . An approach such as ( Seguy 2015 ) might be useful here . We have updated the notation in eq . ( 9 ) to highlight the fact that entropic regularization is used for learning word embeddings . We have added a comment on generalization performance to Section 4.1 , paragraph 3 . Thank you again for your feedback ! ( Brancolini 2009 ) Alessio Brancolini , Giuseppe Buttazzo , Filippo Santambrogio , Eugene Stepanov . Long-term planning versus short-term planning in the asymptotical location problem . ESAIM : Control , Optimisation , and Calculus of Variations 15 , no . 3 ( 2009 ) . ( Kloeckner 2012 ) Benoit Kloeckner . Approximation by Finitely Supported Measures . ESAIM : Control , Optimisation , and Calculus of Variations 18 , no . 2 ( 2012 ) . ( Dudley 1969 ) Richard Dudley . The Speed of Mean Glivenko-Cantelli Convergence . Annals of Mathematical Statistics 40 , no . 1 ( 1969 ) . ( Carlier 2017 ) Guillaume Carlier , Vincent Duval , Gabriel Peyr\u00e9 , Bernhard Schmitzer . Convergence of Entropic Schemes for Optimal Transport and Gradient Flows . SIAM Journal on Mathematical Analysis 49 , no . 2 ( 2017 ) . ( Genevay 2018 ) Aude Genevay , L\u00e9naic Chizat , Francis Bach , Marco Cuturi , Gabriel Peyr\u00e9 . Sample Complexity of Sinkhorn Divergences . arXiv:1810.02733 ( 2018 ) . ( Seguy 2015 ) Vivien Seguy and Marco Cuturi . Principal Geodesic Analysis for Probability Measures under the Optimal Transport Metric . NIPS 2015 ."}, {"review_id": "rJg4J3CqFm-2", "review_text": "The paper proposes embedding the data into low-dimensional Wasserstein spaces. These spaces are larger and more flexible than Euclidean spaces and thus, can capture the underlying structure of the data more accurately. However, the paper simply uses the automatic differentiation to calculate the gradients. Thus, it offers almost no theoretical contribution (for instance, how to calculate these embeddings more efficiently (e.g. faster or more efficient calculation of the Sinkhorn divergence), how to motivate loss functions than can benefit the structure of the Wasserstein spaces, what the interpretation of phi(x) is for each problem, e. g. word embedding, etc.). Additionally, the experiments are unclear and need further improvement. For instance, which method is used to find the Euclidean embedding of the datasets? Have you tried any alternative loss functions for the discussed problems? Are these embeddings useful (classification accuracy, other distortion measure)? How does the 2-D visualizations compare to DR methods such as t-SNE? What is complexity of the method? How does the runtime compare to similar methods?", "rating": "3: Clear rejection", "reply_text": "Thank you for your comments ! We are confident that the contributions of our work are both novel and interesting to the ICLR community . We are eager to address your concerns , which can be addressed in a minor revision to our text or experiments . Some responses are provided here , and we are happy to continue the conversation and/or provide additional information at your request . Most importantly , although you mention automatic differentiation as a negative , its use for approximating gradients of optimal transport-based divergences is non-obvious and was very recently a primary contribution of the AISTATS paper ( Genevay 2018 ) , which provides a way to differentiate transport for use in a neural network that is both stable and efficient . In a sense , our work is among the first to apply these new developments to a representation learning/embedding problem . More generally , efficient evaluation of optimal transport distances and their derivatives is a well-known and long-standing challenge in optimization . We leverage the current state of the art , in terms of efficiency , by using the Sinkhorn approximation to the Wasserstein distance ( Section 2.2 ) . While we appreciate the suggestion to show t-SNE plots and can add them to the paper if acceptance to ICLR is contingent on this change , it is worth noting that they will communicate different , more limited information in comparison to our visualizations . As is well-known in the data science community , it is easy to ascribe signal to noise when interpreting the locations of the t-SNE points , as they are not intended to capture locations or distances in the original embedding space . Of course , we are happy to generate the figures and see if they are useful , at your request . We respectfully highlight a few instances in which your questions are partially addressed in the existing text . We will gladly revise and/or augment the text for clarity , with guidance on the most effective ways to communicate the ideas below . 1.The interpretation of phi ( x ) for word embeddings is discussed in Section 4.2.1 , where we demonstrate direct visualization of the embedding . 2.The Euclidean and hyperbolic embeddings are computed nearly identically to the Wasserstein embedding -- same loss function , same optimizer ( Adam ) , different learning rate . This was mentioned in Section 4.1 , fifth paragraph ; we have also edited this to clarify that the optimizer is the same , and would be happy to add additional detail to this description . 3.The loss function for the first problem ( complex networks ) is dictated by the problem statement itself : We are establishing that one can learn Wasserstein embeddings that achieve low mean distortion , so our loss is the mean distortion . This is stated in Section 4.1 , paragraph 2 . Note that distortion is the standard criterion for metric embeddings . 4.The utility of the word embeddings for scoring semantic similarity of words is described in Section 4.2 , paragraph 4 , and in Table 2 , where we evaluate the word embeddings on several benchmarks . Note also that we compare to five alternative methods . Thank you again for your feedback ! We appreciate your time and look forward to the possibility of sharing our work in ICLR soon . ( Genevay 2018 ) Aude Genevay , Gabriel Peyr\u00e9 , Marco Cuturi . Learning Generative Models with Sinkhorn Divergences . AISTATS 2018 ."}], "0": {"review_id": "rJg4J3CqFm-0", "review_text": "This paper learns embeddings in a discrete space of probability distributions, endowed with a (regularized) Wasserstein distance. pros: - interesting idea, nice results, mostly readable presentation. - the paper is mostly experimental but the message delivers clearly the paper\u2019s objective - the direct visualisation is interesting - the paper suggests interesting problems related to the technique cons: - to be fair with the technique, the title should mention the fact that the paper minimises a regularised version of Wasserstein distances (Wasserstein -> Sinkhorn ? put \u201cregularised\" ?) - and to be fair, the paper should put some warnings related to regularisation -- this is not a distance anymore, sparsity is affected by regularisation (which may affect visualisation). Put some reminders in the conclusion, reword at least the third paragraph in the introduction. - the paper could have been a little bit more detailed on Section 2.3, in particular for its third paragraph. Even when it is an experimental paper. - the direct visualisation is interesting in the general case but has in fact a problem when distributions are highly multimodal, which can be the case in NLP. This blurs the interpretation. - the paper delivers a superficial message on the representation: I do not consider that nice having modes near physical locations (Paris, France) is wrong. It is also a city. However, it would have been interesting to see the modes of \u201ccity\u201d (or similar) to check whether the system indeed did something semantically wrong. Questions: - beyond that last remark comes the problem as to whether one can ensure that semantic hierarchies appear in the plot: for example if Nice was only a city, would we observe a minimal intersection with the support of word \u201ccity\u201d ? (intersection to be understood at minimal level set, not necessarily 0). ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your helpful comments ! We made several small revisions , according to your suggestions . To make it more clear that we are using regularized transport , we have made four changes : 1 . The title is now \u201c Learning Entropic Wasserstein Embeddings. \u201d 2 . In the introduction , we expanded the existing discussion of the Sinkhorn divergence and moved it to a separate paragraph ( now the fifth paragraph ) . 3.In Section 2.3 , we note that we are using the Sinkhorn divergence , and briefly discuss its theoretical properties . 4.In the conclusion , we state that our empirical results are for the Sinkhorn divergence . We have expanded the third paragraph of Section 2.3 , to provide more detail on embedding capacity results for Wasserstein spaces . Our intent with including \u201c nice \u201d in the visualization section was to show first an error ( Figure 3c ) , which is that \u201c nice \u201d is visibly distant from \u201c kind \u201d ( a synonym ) , then to show the explanation for the error ( Figure 3d ) , which is that the network learned that \u201c nice \u201d is a city in France , while ignoring its second meaning . To make this clearer , we have changed the caption for Figure 3d to \u201c Explaining a failed association. \u201d Please let us know if this addresses your comment . The idea to look at semantic hierarchies is very interesting , and we did investigate this briefly . We observed that the minimal level sets of the parent and child in the hierarchy were often partially overlapping -- for instance the embedding of \u201c city \u201d has two major modes , one of which overlaps with a number of cities , including \u201c nice. \u201d As you point out , however , it is not immediately clear how hierarchies should manifest in this type of embedding , particularly as the parent can have semantics not shared by the children ( e.g. \u201c state , \u201d which has multiple non-geographic meanings ) . Perhaps the parent ( or one of its modes ) should be near to the barycenter of the children ? This would be interesting to investigate further . Thank you again for your feedback !"}, "1": {"review_id": "rJg4J3CqFm-1", "review_text": "The paper \u2018Learning Discrete Wasserstein Embeddings' describes a new embedding method that, contrary to usual embedding approaches, does not try to embed (complex, structured) data into an Hilbertian space where Euclidean distance is used, but rather to the space of probability measures endowed with the Wasserstein distance. As such, data are embed on an empirical distribution supported by Diracs, which locations can be determined by a map that is learnt from data. Interestingly, authors note a 'potential universality' for W_p(R^3) (from a result of Andoni et al., 2015), suggesting that having Diracs in R^3 could embed potentially any kind of metric on symbolic data. Experimental validations are presented on graph and word embedding, and a discussion on visualization of the embedding is also proposed (since the Diracs are located in a low dimensional space). All in all the paper is very clear and interesting. The idea of embedding in a Wasserstein space is original (up to my knowledge) and well described. I definitely believe that this work should be presented at ICLR. I have a couple of questions and remarks for the authors: - It is noted in section 3.2 that both Diracs location and associated weights could be optimized. Yet the authors chose to only optimize locations. Why not only optimizing the weights (as in an Eulerian view of probability distributions) ? The sentence involving works of Brancolini and Claici 2018 is not clear to me. Why weighting does not improve asymptotically the approximation quality ? - Introducing the entropic regularization is mainly done for being able to differentiate the Wasserstein distance. However, few is said on the embeddability of metrics in W^\\lambda_p(R). Is using an entropic version of W moderating the capacity of embedding ? At least experimentally, a discussion could be made on the choice of the regularization parameter, at least in section 4.1. In eq. (9), it seems that it is not the regularized version of W. ? - I assume that the mapping is hard to invert, but did the authors tried to experiment reconstructing an object of interest by following a geodesic in the Wasserstein space ? - It seems to me that authors never give generalization results. What is the performance of the metric approximation when tested on unseen graphs or words ? This point should be clarified in the experiment. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thoughtful comments ! We have made several small edits , according to your suggestions . ( Brancolini 2009 ) and ( Kloeckner 2012 ) show that , when using a weighted point cloud to approximate an absolutely continuous , compactly-supported measure , the order of convergence in p-Wasserstein distance when allowing non-uniform weights is O ( n^ ( -1/d ) ) , which is the same rate as when restricted to uniform weights ( Dudley 1969 ) . Non-uniform weights might buy you an improved constant term , but the rate is the same . We have expanded the description of this fact in Section 3.2 , for clarity . The embedding capacity of W^\\lambda_p ( R^d ) is unknown , so far as we are aware , except in the weak sense that the approximation error with respect to the p-Wasserstein distance vanishes as the regularizer is taken zero ( Carlier 2017 ; Genevay 2018 ) . We have added a discussion of this distinction between W_p and W^\\lambda_p to Section 2.3 . Inverting the mapping and following geodesics in Wasserstein space would definitely be interesting . We have added this to the suggested future work in the conclusion . An approach such as ( Seguy 2015 ) might be useful here . We have updated the notation in eq . ( 9 ) to highlight the fact that entropic regularization is used for learning word embeddings . We have added a comment on generalization performance to Section 4.1 , paragraph 3 . Thank you again for your feedback ! ( Brancolini 2009 ) Alessio Brancolini , Giuseppe Buttazzo , Filippo Santambrogio , Eugene Stepanov . Long-term planning versus short-term planning in the asymptotical location problem . ESAIM : Control , Optimisation , and Calculus of Variations 15 , no . 3 ( 2009 ) . ( Kloeckner 2012 ) Benoit Kloeckner . Approximation by Finitely Supported Measures . ESAIM : Control , Optimisation , and Calculus of Variations 18 , no . 2 ( 2012 ) . ( Dudley 1969 ) Richard Dudley . The Speed of Mean Glivenko-Cantelli Convergence . Annals of Mathematical Statistics 40 , no . 1 ( 1969 ) . ( Carlier 2017 ) Guillaume Carlier , Vincent Duval , Gabriel Peyr\u00e9 , Bernhard Schmitzer . Convergence of Entropic Schemes for Optimal Transport and Gradient Flows . SIAM Journal on Mathematical Analysis 49 , no . 2 ( 2017 ) . ( Genevay 2018 ) Aude Genevay , L\u00e9naic Chizat , Francis Bach , Marco Cuturi , Gabriel Peyr\u00e9 . Sample Complexity of Sinkhorn Divergences . arXiv:1810.02733 ( 2018 ) . ( Seguy 2015 ) Vivien Seguy and Marco Cuturi . Principal Geodesic Analysis for Probability Measures under the Optimal Transport Metric . NIPS 2015 ."}, "2": {"review_id": "rJg4J3CqFm-2", "review_text": "The paper proposes embedding the data into low-dimensional Wasserstein spaces. These spaces are larger and more flexible than Euclidean spaces and thus, can capture the underlying structure of the data more accurately. However, the paper simply uses the automatic differentiation to calculate the gradients. Thus, it offers almost no theoretical contribution (for instance, how to calculate these embeddings more efficiently (e.g. faster or more efficient calculation of the Sinkhorn divergence), how to motivate loss functions than can benefit the structure of the Wasserstein spaces, what the interpretation of phi(x) is for each problem, e. g. word embedding, etc.). Additionally, the experiments are unclear and need further improvement. For instance, which method is used to find the Euclidean embedding of the datasets? Have you tried any alternative loss functions for the discussed problems? Are these embeddings useful (classification accuracy, other distortion measure)? How does the 2-D visualizations compare to DR methods such as t-SNE? What is complexity of the method? How does the runtime compare to similar methods?", "rating": "3: Clear rejection", "reply_text": "Thank you for your comments ! We are confident that the contributions of our work are both novel and interesting to the ICLR community . We are eager to address your concerns , which can be addressed in a minor revision to our text or experiments . Some responses are provided here , and we are happy to continue the conversation and/or provide additional information at your request . Most importantly , although you mention automatic differentiation as a negative , its use for approximating gradients of optimal transport-based divergences is non-obvious and was very recently a primary contribution of the AISTATS paper ( Genevay 2018 ) , which provides a way to differentiate transport for use in a neural network that is both stable and efficient . In a sense , our work is among the first to apply these new developments to a representation learning/embedding problem . More generally , efficient evaluation of optimal transport distances and their derivatives is a well-known and long-standing challenge in optimization . We leverage the current state of the art , in terms of efficiency , by using the Sinkhorn approximation to the Wasserstein distance ( Section 2.2 ) . While we appreciate the suggestion to show t-SNE plots and can add them to the paper if acceptance to ICLR is contingent on this change , it is worth noting that they will communicate different , more limited information in comparison to our visualizations . As is well-known in the data science community , it is easy to ascribe signal to noise when interpreting the locations of the t-SNE points , as they are not intended to capture locations or distances in the original embedding space . Of course , we are happy to generate the figures and see if they are useful , at your request . We respectfully highlight a few instances in which your questions are partially addressed in the existing text . We will gladly revise and/or augment the text for clarity , with guidance on the most effective ways to communicate the ideas below . 1.The interpretation of phi ( x ) for word embeddings is discussed in Section 4.2.1 , where we demonstrate direct visualization of the embedding . 2.The Euclidean and hyperbolic embeddings are computed nearly identically to the Wasserstein embedding -- same loss function , same optimizer ( Adam ) , different learning rate . This was mentioned in Section 4.1 , fifth paragraph ; we have also edited this to clarify that the optimizer is the same , and would be happy to add additional detail to this description . 3.The loss function for the first problem ( complex networks ) is dictated by the problem statement itself : We are establishing that one can learn Wasserstein embeddings that achieve low mean distortion , so our loss is the mean distortion . This is stated in Section 4.1 , paragraph 2 . Note that distortion is the standard criterion for metric embeddings . 4.The utility of the word embeddings for scoring semantic similarity of words is described in Section 4.2 , paragraph 4 , and in Table 2 , where we evaluate the word embeddings on several benchmarks . Note also that we compare to five alternative methods . Thank you again for your feedback ! We appreciate your time and look forward to the possibility of sharing our work in ICLR soon . ( Genevay 2018 ) Aude Genevay , Gabriel Peyr\u00e9 , Marco Cuturi . Learning Generative Models with Sinkhorn Divergences . AISTATS 2018 ."}}