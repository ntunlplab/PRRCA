{"year": "2020", "forum": "rJgffkSFPS", "title": "Multi-objective Neural Architecture Search via Predictive Network Performance Optimization", "decision": "Reject", "meta_review": "This paper proposes to use Graph Convolutional Networks (GCNs) in Bayesian optimization for neural architecture search. While the paper title includes multi-objective, this component appears to only be a posthoc evaluation of the Pareto front of networks evaluated using a single-objective search -- this could be performed for any method that evaluates more than one network. Performance on NAS-Bench-101 appears to be very good. \n\nIn the private discussion of reviewers and AC, several issues were raised, including whether the approach is compared fairly to LaNAS and whether the GCN will predict well for large search spaces. Also, unfortunately, no code is provided, making it unclear whether the work is reproducible. The reviewers unanimously agreed on a weak rejection score.\n\nI concur with this assessment and therefore recommend rejection.\n", "reviews": [{"review_id": "rJgffkSFPS-0", "review_text": "This paper proposed BOGCN-NAS that encodes current architecture with Graph convolutional network (GCN) and uses the feature extracted from GCN as the input to perform a Bayesian regression (predicting bias and variance, See Eqn. 5-6). They use Bayesian Optimization to pick the most promising next model with Expected Improvement, train it and take its resulting accuracy/latency as an additional training sample, and repeat. They tested the framework on both single-objective and multi-objective tasks. On the single-objective (accuracy task). They tested it on NasBench and LSTM-12K, two NAS datasets with pre-trained models and their performance. They obtained very good performance on both, beating LaNAS (previous SoTA) by 7.8x higher sample efficiency. On multiple-objective, they show higher efficiency in finding Pareto frontier models, compared to random search. One main question I have is whether the next model is chosen given the current prediction model? For NasBench, did you run your predictor for all (420k minus explored) models and pick the one that maximizes Expected Improvement? Note that LaNAS is more efficient in that manner by sampling directly on polytopes formed by linear constraints. If so, how do you pick the next candidate models in open domain setting? It looks like Eqn. 9 biases the training heavily towards high accuracy region, which is a hack. Although in the Appendix (Fig. 7) the authors have already perform some analysis on the effect of different weight terms, I wonder whether there is a more problem-independent way of doing it. The MCTS in LaNAS is one way that automatically focuses the search on the important regions. Currently, the proposed approach might limit the usability of the proposed method to other situations when accuracy is no longer that important. The performance is really impressive in the NasBench and LSTM dataset. The paper mentioned that \u201cBO can really predict the precious model to explore next\u201d but didn\u2019t provide an examples in the paper. I would whether the author could visualize it in the next revision, which would be very cool. Do you have open domain search research in single-objective search? Why not use NasNet architecture for a fair comparison with other NAS papers? In Appendix, Fig. 6 shows that even without GCN and BO, a single MLP already achieves global optimal solution in LSTM-12K dataset with ~850 samples, already beating all the previous methods (Random, Reg Evolution, MCTS and LaNAS). If that\u2019s the case, I wonder how much roles the proposed methods (BO + GCN) play during search? Also what do you mean by \u201cwithout BO\u201d? Do you only predict the mean and assume all variance is constant? =====Post Rebuttal====== I have read other reviewers' comments and the rebuttal. One of the main problems in this paper is an unfair comparison against LaNAS. LaNAS only uses a single sample at each leaf, while they sample 100% to 0.1% of the models, evaluate them with the current BO model and find the best. For NasBench-101 with 420K models, even sampling 0.1% each time means ~400 samples and the performance (from the rebuttal) seems to degrade substantially from 100% case (1464.4->4004.4). This means that almost 3x more samples are needed, compared to what they claimed. I agree with the authors that calling BO function is super fast so maybe this is fine. However, on the open domain experiments, their performance is also not better than LaNAS+c/o, which they didn't list in the rebuttal. I listed it here: Model Params Top-1 err No. of samples truly evaluated ----------------------------------------------------------------------------------------------------------------- BOGCN+cutout (V1) 3.1M 2.74 200 BOGCN+cutout (V2) 3.5M 2.61 400 LaNAS+c/o 3.2M 2.53\u00b10.05 803 Overall this paper is on the borderline. I don't mind if the paper gets rejected. For now I lower the score to 3. ", "rating": "3: Weak Reject", "reply_text": "Thanks for your interests and we appreciate your constructive comments . We hope our following clarifications can address your concerns . 1 . `` whether the next model is chosen given the current prediction model , how do you pick the next candidate models in open domain setting ? '' - As mentioned in section 3.4 , we randomly sample a subspace as candidate pool if the search space is huge . For NASBench , since the search space only contains 420K models , which can be inferred easily with very little cost ( less than 0.01 seconds ) , we take the whole search space as our candidate pool . To simulate the situation in a very large search space , we also test our algorithm with different pool sampling ratio ( 1 , 0.1 , 0.01 , 0.001 ) and the result is shown as follows . Even though we select a subspace for prediction , the improvement is still significant . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - ratio NASBench LSTM 1 1465.4 558.8 0.1 1564.6 1483.2 0.01 2078.8 1952.4 0.001 4004.4 2984.0 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 2 . `` It looks like Eqn . 9 biases the training heavily towards high accuracy region , which is a hack '' - The main purpose of NAS is to search an architecture with high accuracy . Similar with MCTS in LaNAS , Our proposed loss is another way that focuses the search on the important regions . Even the weighted loss is removed , our method is still outperform than previous SOTA - LaNAS ( shown in appendices section A.2 ) . 3 . `` I wonder whether there is a more problem-independent way of doing it '' - Our proposed weighted loss is a possible way to solve searching problem with attention , which is problem-independent . Our idea is inspired by Focal Loss [ 1 ] , while Focal Loss is used for classification and our weighted loss is used for regression . 4 . `` Do you have open domain search research in single-objective search ? '' - Yes , the model we find can achieve 0.783 accuracy on ImageNet dataset . The model is the same as M3 , but we can achieve it within less samples ( 400 samples ) for single-objective search . We add experiment in DARTS search space , where each cell contains 4 blocks ( 11 nodes ) and 8 possible operations . After sampling 200 models , we picked out 2 models with best test accuracy 97.26 % , 97.39 % on cifar10 respectively . The experiment details are appended in the updated paper 5 . `` Why not use NasNet architecture for a fair comparison with other NAS papers ? '' - We add experiment in DARTS search space , where each cell contains 4 blocks ( 11 nodes ) and 8 possible operations . We picked out 2 models ( V1 & V2 ) with best test accuracy on cifar10 after sampling 200 and 400 models respectively . The experiment details are appended in the updated paper . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Model Params Top-1 err No . of samples truly evaluated -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - NASNet-A+cutout 3.3 M 2.65 20000 AmoebaNet-B+cutout 2.8 M 2.55 27000 PNASNet-5 3.2 M 3.41 1160 NAO 10.6 M 3.18 1000 ENAS+cutout 4.6 M 2.89 - DARTS+cutout 3.3 M 2.76 - BayesNAS+cutout 3.4 M 2.81 - ASNG-NAS+cutout 3.9 M 2.83 - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- BOGCN+cutout ( V1 ) 3.1M 2.74 200 BOGCN+cutout ( V2 ) 3.5M 2.61 400 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - 6 . `` I wonder how much roles the proposed methods ( BO + GCN ) play during search ? '' - We indeed performed ablation study in appendices section ( A.1 & A.2 ) and the number of samples evaluated is shown below . As can be seen , the improvement of BO and GCN is significant . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- MLP BOMLP GCN BOGCN 4527.0 4042.25 2860.6 1465.4 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- On NasBench-101 , compared to using only the GCN predictor , BOGCN finds global optimal with 1395.2 fewer samples ; compared to using BOMLP , BOGCN finds global optimal with 2576.85 fewer samples , which proves the importance of BO and GCN respectively . 7 . `` Also what do you mean by \u201c without BO \u201d ? Do you only predict the mean and assume all variance is constant ? '' - Yes , we remove BO part and select architectures only based on predictors . In this scenario , we use point estimation to predict the accuracy of each model , thus not taking variance into account . We make it clear in the updated version . [ 1 ] Focal Loss for Dense Object Detection . 2017"}, {"review_id": "rJgffkSFPS-1", "review_text": "This paper provide a NAS algorithm using Bayesian Optimization with Graph Convolutional Network predictor. The method apply GCN as a surrogate model to adaptively discover and incorporate nodes structure to approximate the performance of the architecture. The method further considers an efficient multi-objective search which can be flexibly injected into any sample-based NAS pipelines to efficiently find the best speed/accuracy trade-off. The paper is well-written. The experiments are abundant. However, the paper has following drawbacks that need to be further concerned: 1. In my opinion, the key point of the paper has nothing to do with GCN or multi-objective. The important part is to use BO and EI to sample new architecture. However, no theoretical proof is provided to guarantee that the performance is getting better during while loop in Algorithm 1. 2. Eq.(9) focuses more on models with higher accuracy. However, those models with bad performance will be predicted inaccurately and may have a higher score than good models. For model with ground-truth near 0, arbitrary predicted score results in the same loss. Eq.(9) seems cannot prevent this situation from happening. 3. Table 1 compare different architectures with GCN. However, the LSTM is the worst architecture used among the 3 different architectures in the original paper (Alpha-X), which makes the comparison unfair. 4. Table 2 shows the number of architectures trained. However, the proposed method need to update GCN multiple times during searching, which makes the comparison unfair. 5. Table 2 shows the number of training models before finding the best model. It is meaningless when used in reality, which often contains more than 10^10 different architectures and the best architecture is unknown. In my opinion, the performance of the top1 architecture predicted by the proposed method is much important. 6. Algorithm 1 uses Pareto front, which does not exist when doing experiments on single-objective search. More details should be clarified.", "rating": "3: Weak Reject", "reply_text": "Thank you for your comments . We address your main concerns as follows . 1 . `` the key point of the paper has nothing to do with GCN '' - We believe GCN predictor is one of the key points of our paper . As we mentioned , GCN predictor is proposed as the surrogate model for Bayesian Optimization . Different from popular Gaussian Processes , GCN predictor can obtain the architecture embeddings better as shown in the experiments in Figure 2 . 2 . `` the key point of the paper has nothing to do with multi-objective '' - Multi-objective search is not discussed in some well-known NAS papers such as DARTS , ENAS , NAO , etc . We believe it is not fully explored in the NAS literatures . We have shown that our method is much better than other multi-objective searching algorithms by adding more baseline comparisons in Section 4.3. https : //drive.google.com/file/d/1xs4SNva5hdd7Rhaok15cPP7UXohVi5QR/view ? usp=sharing 3 . `` no theoretical proof is provided to guarantee that the performance is getting better during while loop in Algorithm '' - Bayesian Optimization and Graph Convolutional Network are both well explored theoretically . For BO part , it has theory for balancing exploitation and exploration [ 1 ] ; For GCN part , it has theory for guaranteeing the better embeddings for graph-based data [ 2 ] . 4 . `` those models with bad performance will be predicted inaccurately and may have a higher score than good models . '' - We find that bad models will still have a lower score than good models . For the NAS problem , we only care about those high-accuracy models . Our motivation is to distinguish low-performance model roughly . For example , the model with lowest accuracy in NASBench ( true accuracy=0.0998 ) will be predicted to estimated accuracy=0.2872 , which is still separable from top models ( estimated accuracy=0.94 ) . Adding more weights on models with high performance does n't mean we do n't care models with bad performance . 5 . `` For model with ground-truth near 0 , arbitrary predicted score results in the same loss . '' - Even though the loss w.r.t. $ \\widetilde { y_i } =0 $ wo n't be counted , it wo n't be predicted so high . Furthermore , the ablation study section is shown in Appendices section ( A.2 ) . Empirically , adding the weighted loss will speed up 1.25X for searching the optimal on NASBench . 6 . `` the LSTM is the worst architecture used among the 3 different architectures in the original paper ( Alpha-X ) , which makes the comparison unfair . '' - We did not only compare GCN with LSTM , MLP ( the best predictor in Alpha-X ) is also included in the comparison . Furthermore , we used MLP for all following ablations studies ( Appendices A.1 ) . There are two different architectures in Alpha-X because multi-stage is the ensemble form , which can also be used in GCN . 7 . `` the proposed method needs to update GCN multiple times during searching , which makes the comparison unfair . '' - For NAS problem , the cost bottleneck is the evaluation of the architecture . Compared with evaluation cost , GCN updating cost is negligible . Other baselines have similar steps . For instance , previous SOTA LaNAS [ 3 ] need multiple times for action space updating . 8 . `` It is meaningless when used in reality , which often contains more than 10^10 different architectures and the best architecture is unknown '' - It is one common metric for NAS problem . Previous SOTA LaNAS [ 3 ] and MCTS [ 4 ] both compare this metric . 9 . `` the performance of the top1 architecture predicted by the proposed method is much important . '' - We indeed compared time course performance of different methods ( please see appendices B.1 ) . Our method outperforms other baselines consistently . https : //drive.google.com/file/d/1IOr511FIjCIfIxeqLn1JIZmV0FKuog-K/view ? usp=sharing 10 . `` Algorithm 1 uses Pareto front , which does not exist when doing experiments on single-objective search . '' - Single-objective problem is compatible with our definition of multi-objective problem ( Section 2.1 ) where the Pareto front is only constituted by one architecture . We describe single-objective case specifically in the updated version . [ 1 ] A tutorial on Bayesian optimization of expensive cost functions , with application to active user modeling and hierarchical reinforcement learning . 2010 [ 2 ] Semi-Supervised Classification with Graph Convolutional Networks . 2017 [ 3 ] Sample-Efficient Neural Architecture Search by Learning Action Space . 2019 [ 4 ] AlphaX : eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search . 2019"}, {"review_id": "rJgffkSFPS-2", "review_text": "--Summary-- The authors present an algorithm BOGCN-NAS which combines bayesian optimization and GCNs for searching over NN architectures. The authors emphasize that this method can be used for multi-objective optimization and run experiments over NAS-Bench, LSTM-12K and ResNet models. --Method-- Methodologically, the contribution is somewhat weak. The main technical contribution is to use a GCN to get a global representation of a graph, which can then be used for downstream regression tasks such as predicting accuracy. It\u2019s not clear how much the GCN generalizes in being able to encode arbitrarily large architectures. The two main examples offered are NAS-Bench and LSTM-12K focus on optimizing cell architectures which contain a handful of nodes e.g. (5 in the case of NAS-101). Graph embeddings: The authors have not considered other graph embeddings to use in their Bayesian regression setup. E.g. see https://arxiv.org/pdf/1903.11835.pdf for a list. Bayes-opt: In Algo1, step 5. The authors randomly sample a number of architectures in order to calculate EI scores on them. For large discrete combinatorial search spaces, this approach will not scale. Multi-objective optimization: It\u2019s not clear why GCNs or BO is required for this. Any predictor that generates multiple metrics could substitute in order to create a pareto-curve. Even multi-objective RL based approaches could suffice. Thus multi-objective opt only seems like a minor/tangential contribution. --Experiments-- The main claim of the paper is that this approach works well for the multi-objective case. However, the results only look at two objectives #params vs accuracy. There\u2019s a pretty strong correlation between the two. It\u2019s unclear how the method generalizes when objectives are not correlated. The authors need to thoroughly demonstrate other objectives/find suitable benchmarks for the same as clearly NAS-101 will not suffice. Other concerns: - Table 1 has correlations using 1000 training architectures. Why 1000? Why not 50 (that\u2019s how sec 4.2 is initialized). Also, the correlation results are less impressive in Figure 9. - Table 1 lists the number of params that the predictor uses. Why is this important? How about comparing with a linear regressor? - The results in Sec 4.3 are using random as the only baseline. This is a pretty weak baseline. - In sec 4.4, the authors pick models M1, M2 and M3 as candidate examples.. How were these chosen ? - Sec 4.5 transfer learning results are pretty weak. Transfer across datasets is much more interesting e.g. between ImageNet and Cifar-10. Overall, this paper has some interesting results, which show that GCNs can be useful models to encode graph structured inputs. However, the methodological and experimental results can definitely be strengthened. The authors may consider the following: Address how GCNs can model and scale to general architecture spaces than a small number of nodes in a cell. Address how to sample better over combinatorial search spaces than random in the inner loop of BO. Strengthen MO-opt results. Use better baselines than random and different objectives than accuracy vs #params. ", "rating": "3: Weak Reject", "reply_text": "Other Concerns : 5 . `` Table 1 has correlations using 1000 training architectures . Why 1000 ? Why not 50 ( that 's how sec 4.2 is initialized ) . '' - In table 1 , our purpose was to show that GCN 's prediction accuracy is better comparing to other methods given few data . We have done experiments with 50 , 550 , 1050 ... 7050 training samples to simulate the actual search process . It can be found that our methods also works well in these situations . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - training archs GCN MLP LSTM 50 0.385 0.082 0.209 550 0.570 0.329 0.352 1050 0.597 0.414 0.472 ... 7050 0.692 0.573 0.504 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 6 . `` Table 1 lists the number of params that the predictor uses . Why is this important ? How about comparing with a linear regressor ? '' - We think the number of parameters of predictor is important , and predictors with fewer parameters are more efficient due to the following reasons : - Given fewer training data , predictors with more parameters tend to under-fit . In practice , we can only have very few trained models in the beginning . - The latency caused during prediction is shorter , which allows BOGCN to predict more models each time . - We have followed your advice and done the experiment with a linear regressor . The correlation is only 0.34 , which is uncompetitive becasue it can not handle non-linear features . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - GCN MLP LSTM Linear_Regressor 0.61 0.40 0.46 0.34 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 7 . `` The results in Sec 4.3 are using random as the only baseline . This is a pretty weak baseline . '' - As pointed out by [ 2 , 3 , 4 ] , random search may not be a weak baseline in NAS problem . However , we agree with you about the lack of comparisons in multi-objective tasks . We have performed experiments using other methods and appended them in the paper . 8 . `` In sec 4.4 , the authors pick models M1 , M2 and M3 as candidate examples .. How were these chosen ? '' - We fully train every model on the estimated Pareto front and compare them with ResNets . Then we can pick three models ( M1 , M2 , M3 ) , which can dominate ResNets . We have elaborated more in detail in the paper . 9 . `` Sec 4.5 transfer learning results are pretty weak . Transfer across datasets is much more interesting e.g.between ImageNet and Cifar-10 . '' - The motivation of this experiment is to provide evidence for expanding search space dynamically . We want to show that GCN can adapt to architecture cells with a varying number of nodes . Using this feature , we could progressively search architectures : start with a small number of nodes and gradually grow to larger architectures . We will consider doing transfer learning across different datasets , but this kind of experiment is very time consuming , thus can only be further explored in future works . [ 1 ] Semi-Supervised Classification with Graph Convolutional Networks . 2017 [ 2 ] Evaluating The Search Phase of Neural Architecture Search . 2019 [ 3 ] Random Search and Reproducibility for Neural Architecture Search . 2019 [ 4 ] Exploring Randomly Wired Neural Networks for Image Recognition . 2019"}], "0": {"review_id": "rJgffkSFPS-0", "review_text": "This paper proposed BOGCN-NAS that encodes current architecture with Graph convolutional network (GCN) and uses the feature extracted from GCN as the input to perform a Bayesian regression (predicting bias and variance, See Eqn. 5-6). They use Bayesian Optimization to pick the most promising next model with Expected Improvement, train it and take its resulting accuracy/latency as an additional training sample, and repeat. They tested the framework on both single-objective and multi-objective tasks. On the single-objective (accuracy task). They tested it on NasBench and LSTM-12K, two NAS datasets with pre-trained models and their performance. They obtained very good performance on both, beating LaNAS (previous SoTA) by 7.8x higher sample efficiency. On multiple-objective, they show higher efficiency in finding Pareto frontier models, compared to random search. One main question I have is whether the next model is chosen given the current prediction model? For NasBench, did you run your predictor for all (420k minus explored) models and pick the one that maximizes Expected Improvement? Note that LaNAS is more efficient in that manner by sampling directly on polytopes formed by linear constraints. If so, how do you pick the next candidate models in open domain setting? It looks like Eqn. 9 biases the training heavily towards high accuracy region, which is a hack. Although in the Appendix (Fig. 7) the authors have already perform some analysis on the effect of different weight terms, I wonder whether there is a more problem-independent way of doing it. The MCTS in LaNAS is one way that automatically focuses the search on the important regions. Currently, the proposed approach might limit the usability of the proposed method to other situations when accuracy is no longer that important. The performance is really impressive in the NasBench and LSTM dataset. The paper mentioned that \u201cBO can really predict the precious model to explore next\u201d but didn\u2019t provide an examples in the paper. I would whether the author could visualize it in the next revision, which would be very cool. Do you have open domain search research in single-objective search? Why not use NasNet architecture for a fair comparison with other NAS papers? In Appendix, Fig. 6 shows that even without GCN and BO, a single MLP already achieves global optimal solution in LSTM-12K dataset with ~850 samples, already beating all the previous methods (Random, Reg Evolution, MCTS and LaNAS). If that\u2019s the case, I wonder how much roles the proposed methods (BO + GCN) play during search? Also what do you mean by \u201cwithout BO\u201d? Do you only predict the mean and assume all variance is constant? =====Post Rebuttal====== I have read other reviewers' comments and the rebuttal. One of the main problems in this paper is an unfair comparison against LaNAS. LaNAS only uses a single sample at each leaf, while they sample 100% to 0.1% of the models, evaluate them with the current BO model and find the best. For NasBench-101 with 420K models, even sampling 0.1% each time means ~400 samples and the performance (from the rebuttal) seems to degrade substantially from 100% case (1464.4->4004.4). This means that almost 3x more samples are needed, compared to what they claimed. I agree with the authors that calling BO function is super fast so maybe this is fine. However, on the open domain experiments, their performance is also not better than LaNAS+c/o, which they didn't list in the rebuttal. I listed it here: Model Params Top-1 err No. of samples truly evaluated ----------------------------------------------------------------------------------------------------------------- BOGCN+cutout (V1) 3.1M 2.74 200 BOGCN+cutout (V2) 3.5M 2.61 400 LaNAS+c/o 3.2M 2.53\u00b10.05 803 Overall this paper is on the borderline. I don't mind if the paper gets rejected. For now I lower the score to 3. ", "rating": "3: Weak Reject", "reply_text": "Thanks for your interests and we appreciate your constructive comments . We hope our following clarifications can address your concerns . 1 . `` whether the next model is chosen given the current prediction model , how do you pick the next candidate models in open domain setting ? '' - As mentioned in section 3.4 , we randomly sample a subspace as candidate pool if the search space is huge . For NASBench , since the search space only contains 420K models , which can be inferred easily with very little cost ( less than 0.01 seconds ) , we take the whole search space as our candidate pool . To simulate the situation in a very large search space , we also test our algorithm with different pool sampling ratio ( 1 , 0.1 , 0.01 , 0.001 ) and the result is shown as follows . Even though we select a subspace for prediction , the improvement is still significant . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - ratio NASBench LSTM 1 1465.4 558.8 0.1 1564.6 1483.2 0.01 2078.8 1952.4 0.001 4004.4 2984.0 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 2 . `` It looks like Eqn . 9 biases the training heavily towards high accuracy region , which is a hack '' - The main purpose of NAS is to search an architecture with high accuracy . Similar with MCTS in LaNAS , Our proposed loss is another way that focuses the search on the important regions . Even the weighted loss is removed , our method is still outperform than previous SOTA - LaNAS ( shown in appendices section A.2 ) . 3 . `` I wonder whether there is a more problem-independent way of doing it '' - Our proposed weighted loss is a possible way to solve searching problem with attention , which is problem-independent . Our idea is inspired by Focal Loss [ 1 ] , while Focal Loss is used for classification and our weighted loss is used for regression . 4 . `` Do you have open domain search research in single-objective search ? '' - Yes , the model we find can achieve 0.783 accuracy on ImageNet dataset . The model is the same as M3 , but we can achieve it within less samples ( 400 samples ) for single-objective search . We add experiment in DARTS search space , where each cell contains 4 blocks ( 11 nodes ) and 8 possible operations . After sampling 200 models , we picked out 2 models with best test accuracy 97.26 % , 97.39 % on cifar10 respectively . The experiment details are appended in the updated paper 5 . `` Why not use NasNet architecture for a fair comparison with other NAS papers ? '' - We add experiment in DARTS search space , where each cell contains 4 blocks ( 11 nodes ) and 8 possible operations . We picked out 2 models ( V1 & V2 ) with best test accuracy on cifar10 after sampling 200 and 400 models respectively . The experiment details are appended in the updated paper . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Model Params Top-1 err No . of samples truly evaluated -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - NASNet-A+cutout 3.3 M 2.65 20000 AmoebaNet-B+cutout 2.8 M 2.55 27000 PNASNet-5 3.2 M 3.41 1160 NAO 10.6 M 3.18 1000 ENAS+cutout 4.6 M 2.89 - DARTS+cutout 3.3 M 2.76 - BayesNAS+cutout 3.4 M 2.81 - ASNG-NAS+cutout 3.9 M 2.83 - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- BOGCN+cutout ( V1 ) 3.1M 2.74 200 BOGCN+cutout ( V2 ) 3.5M 2.61 400 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - 6 . `` I wonder how much roles the proposed methods ( BO + GCN ) play during search ? '' - We indeed performed ablation study in appendices section ( A.1 & A.2 ) and the number of samples evaluated is shown below . As can be seen , the improvement of BO and GCN is significant . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- MLP BOMLP GCN BOGCN 4527.0 4042.25 2860.6 1465.4 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- On NasBench-101 , compared to using only the GCN predictor , BOGCN finds global optimal with 1395.2 fewer samples ; compared to using BOMLP , BOGCN finds global optimal with 2576.85 fewer samples , which proves the importance of BO and GCN respectively . 7 . `` Also what do you mean by \u201c without BO \u201d ? Do you only predict the mean and assume all variance is constant ? '' - Yes , we remove BO part and select architectures only based on predictors . In this scenario , we use point estimation to predict the accuracy of each model , thus not taking variance into account . We make it clear in the updated version . [ 1 ] Focal Loss for Dense Object Detection . 2017"}, "1": {"review_id": "rJgffkSFPS-1", "review_text": "This paper provide a NAS algorithm using Bayesian Optimization with Graph Convolutional Network predictor. The method apply GCN as a surrogate model to adaptively discover and incorporate nodes structure to approximate the performance of the architecture. The method further considers an efficient multi-objective search which can be flexibly injected into any sample-based NAS pipelines to efficiently find the best speed/accuracy trade-off. The paper is well-written. The experiments are abundant. However, the paper has following drawbacks that need to be further concerned: 1. In my opinion, the key point of the paper has nothing to do with GCN or multi-objective. The important part is to use BO and EI to sample new architecture. However, no theoretical proof is provided to guarantee that the performance is getting better during while loop in Algorithm 1. 2. Eq.(9) focuses more on models with higher accuracy. However, those models with bad performance will be predicted inaccurately and may have a higher score than good models. For model with ground-truth near 0, arbitrary predicted score results in the same loss. Eq.(9) seems cannot prevent this situation from happening. 3. Table 1 compare different architectures with GCN. However, the LSTM is the worst architecture used among the 3 different architectures in the original paper (Alpha-X), which makes the comparison unfair. 4. Table 2 shows the number of architectures trained. However, the proposed method need to update GCN multiple times during searching, which makes the comparison unfair. 5. Table 2 shows the number of training models before finding the best model. It is meaningless when used in reality, which often contains more than 10^10 different architectures and the best architecture is unknown. In my opinion, the performance of the top1 architecture predicted by the proposed method is much important. 6. Algorithm 1 uses Pareto front, which does not exist when doing experiments on single-objective search. More details should be clarified.", "rating": "3: Weak Reject", "reply_text": "Thank you for your comments . We address your main concerns as follows . 1 . `` the key point of the paper has nothing to do with GCN '' - We believe GCN predictor is one of the key points of our paper . As we mentioned , GCN predictor is proposed as the surrogate model for Bayesian Optimization . Different from popular Gaussian Processes , GCN predictor can obtain the architecture embeddings better as shown in the experiments in Figure 2 . 2 . `` the key point of the paper has nothing to do with multi-objective '' - Multi-objective search is not discussed in some well-known NAS papers such as DARTS , ENAS , NAO , etc . We believe it is not fully explored in the NAS literatures . We have shown that our method is much better than other multi-objective searching algorithms by adding more baseline comparisons in Section 4.3. https : //drive.google.com/file/d/1xs4SNva5hdd7Rhaok15cPP7UXohVi5QR/view ? usp=sharing 3 . `` no theoretical proof is provided to guarantee that the performance is getting better during while loop in Algorithm '' - Bayesian Optimization and Graph Convolutional Network are both well explored theoretically . For BO part , it has theory for balancing exploitation and exploration [ 1 ] ; For GCN part , it has theory for guaranteeing the better embeddings for graph-based data [ 2 ] . 4 . `` those models with bad performance will be predicted inaccurately and may have a higher score than good models . '' - We find that bad models will still have a lower score than good models . For the NAS problem , we only care about those high-accuracy models . Our motivation is to distinguish low-performance model roughly . For example , the model with lowest accuracy in NASBench ( true accuracy=0.0998 ) will be predicted to estimated accuracy=0.2872 , which is still separable from top models ( estimated accuracy=0.94 ) . Adding more weights on models with high performance does n't mean we do n't care models with bad performance . 5 . `` For model with ground-truth near 0 , arbitrary predicted score results in the same loss . '' - Even though the loss w.r.t. $ \\widetilde { y_i } =0 $ wo n't be counted , it wo n't be predicted so high . Furthermore , the ablation study section is shown in Appendices section ( A.2 ) . Empirically , adding the weighted loss will speed up 1.25X for searching the optimal on NASBench . 6 . `` the LSTM is the worst architecture used among the 3 different architectures in the original paper ( Alpha-X ) , which makes the comparison unfair . '' - We did not only compare GCN with LSTM , MLP ( the best predictor in Alpha-X ) is also included in the comparison . Furthermore , we used MLP for all following ablations studies ( Appendices A.1 ) . There are two different architectures in Alpha-X because multi-stage is the ensemble form , which can also be used in GCN . 7 . `` the proposed method needs to update GCN multiple times during searching , which makes the comparison unfair . '' - For NAS problem , the cost bottleneck is the evaluation of the architecture . Compared with evaluation cost , GCN updating cost is negligible . Other baselines have similar steps . For instance , previous SOTA LaNAS [ 3 ] need multiple times for action space updating . 8 . `` It is meaningless when used in reality , which often contains more than 10^10 different architectures and the best architecture is unknown '' - It is one common metric for NAS problem . Previous SOTA LaNAS [ 3 ] and MCTS [ 4 ] both compare this metric . 9 . `` the performance of the top1 architecture predicted by the proposed method is much important . '' - We indeed compared time course performance of different methods ( please see appendices B.1 ) . Our method outperforms other baselines consistently . https : //drive.google.com/file/d/1IOr511FIjCIfIxeqLn1JIZmV0FKuog-K/view ? usp=sharing 10 . `` Algorithm 1 uses Pareto front , which does not exist when doing experiments on single-objective search . '' - Single-objective problem is compatible with our definition of multi-objective problem ( Section 2.1 ) where the Pareto front is only constituted by one architecture . We describe single-objective case specifically in the updated version . [ 1 ] A tutorial on Bayesian optimization of expensive cost functions , with application to active user modeling and hierarchical reinforcement learning . 2010 [ 2 ] Semi-Supervised Classification with Graph Convolutional Networks . 2017 [ 3 ] Sample-Efficient Neural Architecture Search by Learning Action Space . 2019 [ 4 ] AlphaX : eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search . 2019"}, "2": {"review_id": "rJgffkSFPS-2", "review_text": "--Summary-- The authors present an algorithm BOGCN-NAS which combines bayesian optimization and GCNs for searching over NN architectures. The authors emphasize that this method can be used for multi-objective optimization and run experiments over NAS-Bench, LSTM-12K and ResNet models. --Method-- Methodologically, the contribution is somewhat weak. The main technical contribution is to use a GCN to get a global representation of a graph, which can then be used for downstream regression tasks such as predicting accuracy. It\u2019s not clear how much the GCN generalizes in being able to encode arbitrarily large architectures. The two main examples offered are NAS-Bench and LSTM-12K focus on optimizing cell architectures which contain a handful of nodes e.g. (5 in the case of NAS-101). Graph embeddings: The authors have not considered other graph embeddings to use in their Bayesian regression setup. E.g. see https://arxiv.org/pdf/1903.11835.pdf for a list. Bayes-opt: In Algo1, step 5. The authors randomly sample a number of architectures in order to calculate EI scores on them. For large discrete combinatorial search spaces, this approach will not scale. Multi-objective optimization: It\u2019s not clear why GCNs or BO is required for this. Any predictor that generates multiple metrics could substitute in order to create a pareto-curve. Even multi-objective RL based approaches could suffice. Thus multi-objective opt only seems like a minor/tangential contribution. --Experiments-- The main claim of the paper is that this approach works well for the multi-objective case. However, the results only look at two objectives #params vs accuracy. There\u2019s a pretty strong correlation between the two. It\u2019s unclear how the method generalizes when objectives are not correlated. The authors need to thoroughly demonstrate other objectives/find suitable benchmarks for the same as clearly NAS-101 will not suffice. Other concerns: - Table 1 has correlations using 1000 training architectures. Why 1000? Why not 50 (that\u2019s how sec 4.2 is initialized). Also, the correlation results are less impressive in Figure 9. - Table 1 lists the number of params that the predictor uses. Why is this important? How about comparing with a linear regressor? - The results in Sec 4.3 are using random as the only baseline. This is a pretty weak baseline. - In sec 4.4, the authors pick models M1, M2 and M3 as candidate examples.. How were these chosen ? - Sec 4.5 transfer learning results are pretty weak. Transfer across datasets is much more interesting e.g. between ImageNet and Cifar-10. Overall, this paper has some interesting results, which show that GCNs can be useful models to encode graph structured inputs. However, the methodological and experimental results can definitely be strengthened. The authors may consider the following: Address how GCNs can model and scale to general architecture spaces than a small number of nodes in a cell. Address how to sample better over combinatorial search spaces than random in the inner loop of BO. Strengthen MO-opt results. Use better baselines than random and different objectives than accuracy vs #params. ", "rating": "3: Weak Reject", "reply_text": "Other Concerns : 5 . `` Table 1 has correlations using 1000 training architectures . Why 1000 ? Why not 50 ( that 's how sec 4.2 is initialized ) . '' - In table 1 , our purpose was to show that GCN 's prediction accuracy is better comparing to other methods given few data . We have done experiments with 50 , 550 , 1050 ... 7050 training samples to simulate the actual search process . It can be found that our methods also works well in these situations . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - training archs GCN MLP LSTM 50 0.385 0.082 0.209 550 0.570 0.329 0.352 1050 0.597 0.414 0.472 ... 7050 0.692 0.573 0.504 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 6 . `` Table 1 lists the number of params that the predictor uses . Why is this important ? How about comparing with a linear regressor ? '' - We think the number of parameters of predictor is important , and predictors with fewer parameters are more efficient due to the following reasons : - Given fewer training data , predictors with more parameters tend to under-fit . In practice , we can only have very few trained models in the beginning . - The latency caused during prediction is shorter , which allows BOGCN to predict more models each time . - We have followed your advice and done the experiment with a linear regressor . The correlation is only 0.34 , which is uncompetitive becasue it can not handle non-linear features . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - GCN MLP LSTM Linear_Regressor 0.61 0.40 0.46 0.34 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 7 . `` The results in Sec 4.3 are using random as the only baseline . This is a pretty weak baseline . '' - As pointed out by [ 2 , 3 , 4 ] , random search may not be a weak baseline in NAS problem . However , we agree with you about the lack of comparisons in multi-objective tasks . We have performed experiments using other methods and appended them in the paper . 8 . `` In sec 4.4 , the authors pick models M1 , M2 and M3 as candidate examples .. How were these chosen ? '' - We fully train every model on the estimated Pareto front and compare them with ResNets . Then we can pick three models ( M1 , M2 , M3 ) , which can dominate ResNets . We have elaborated more in detail in the paper . 9 . `` Sec 4.5 transfer learning results are pretty weak . Transfer across datasets is much more interesting e.g.between ImageNet and Cifar-10 . '' - The motivation of this experiment is to provide evidence for expanding search space dynamically . We want to show that GCN can adapt to architecture cells with a varying number of nodes . Using this feature , we could progressively search architectures : start with a small number of nodes and gradually grow to larger architectures . We will consider doing transfer learning across different datasets , but this kind of experiment is very time consuming , thus can only be further explored in future works . [ 1 ] Semi-Supervised Classification with Graph Convolutional Networks . 2017 [ 2 ] Evaluating The Search Phase of Neural Architecture Search . 2019 [ 3 ] Random Search and Reproducibility for Neural Architecture Search . 2019 [ 4 ] Exploring Randomly Wired Neural Networks for Image Recognition . 2019"}}