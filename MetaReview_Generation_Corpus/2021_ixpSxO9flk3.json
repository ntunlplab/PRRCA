{"year": "2021", "forum": "ixpSxO9flk3", "title": "No MCMC for me: Amortized sampling for fast and stable training of energy-based models", "decision": "Accept (Poster)", "meta_review": "The authors proposed to train an energy based model with a hierachical\nvariational approximations. The entropy can be tricky in hierarchical\nvariational approximations.  The authors suggest using the auxillary\nsamples to guide an importance samples to compute the gradient of the\nentropy. They evaluate their approach on a slew of models. The idea is\nstraightfoward and could potentially be applied to other hierarchical\nvariational models out side of the energy-based model setting.  The\nauthors were responsive and clarified many agressive questions. I'd\nask the authors to clean up two things\n\n- Equation 8 would be easier to follow if it kept the expectation from\n  equation 6 thereby making z_0 feel like it materialize out of thin\n  air\n\n\n- A more detailed discusion of when the proposal is good and what could\n  be missed out\twhen relying on the generating z to center the proposal", "reviews": [{"review_id": "ixpSxO9flk3-0", "review_text": "This paper proposes an improved algorithm to train EBM-based models , called Variational Entropy Regularized Approximate maximum likelihood . The basic idea is to formulate the intractable partition function as an optimization problem with an additional entropy term . To estimate the gradient of the entropy term , the authors then propose a variational formulation for approximation . The idea is interesting but not very exciting , and there consists of technical details that need to be carefully deal with . One of my biggest concerns is that since the algorithm relies on importance sampling to estimate the gradient of log marginal likelihood , the variance can be arbitrarily large . Specifically , the importance weight is the ration between a joint distribution and a proposed variational distribution \\xi . One can see that a sample from \\xi is a zero-mean Gaussian distribution , whereas q ( z , x ) should not . I suspect this would make the ratios to be vary unstable with high variance , which seems to contradict with the goal of the paper . In other words , I am not convinced why the importance sampling based method can work better than existing methods . In addition , this importance sampling based method is introduced to avoid sampling the posterior distribution with HMC . I think this step can be considered as an efficient approximation to the method , I am expecting HMC should perform at least as good as the proposed method but less efficient . The results in Table 2 seems to agree with this ( although not completely ) . I wonder how is the running time comparison ? Also , since HMC based method is a competitive method , why do n't you consider this in other experiments such as those in Section 6 ? Also , since the proposed method is claimed to outperform the recent SSM method . I think the same experiments as in the SSM paper should be conducted for comparison . BTW , the generative images in Appendix are too small to be informative . After rebuttal After the rebuttal , my main concern remains . Specifically , the paper defines a variational distribution q ( z|x0 via a hierarchical construction : z_0 ~ N ( 0 , 1 ) , z ~ N ( z_0 , \\eta I ) , which is essentially a zero-mean Gaussian . And I suspect the this is a bad variational distribution and it will induce high variance . The author said they did n't the hierarchical construction to define the variational distribution , because they fix z_0 after sampling . I do n't think this is a formal way of defining a variational distribution . One reason is that even if they fix z_0 , the proposal distribution will be a z_0-mean Gaussian , and the mean is randomly drawn from N ( 0 , 1 ) , which will not match the true posterior distribution ( they only optimize the variance parameter ) . I think this should be make clear and investigated in more details . I will keep my initial score .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank you for your time and your thoughtful comments on our work . We will address your concerns in order . Regarding the variance of the importance sampling estimator : These are important concerns you raise . You say in your review that $ \\xi $ is a sample from a zero-mean Gaussian distribution and this would be a bad proposal distribution for importance sampling from $ q ( z|x ) $ . We agree that if we were using such an uninformed proposal distribution for importance sampling , then the variance of the estimator would be very high . However , as described in the text directly following equation 8 , this is not the proposal distribution we use . We construct an informed proposal distribution that closely approximates $ q ( z|x ) $ and this informed proposal is a key contribution of our work . We elaborate more below : When generating a sample $ x \\sim q ( x ) $ , we first sample $ z \\sim q ( z ) $ and then $ x \\sim q ( x|z ) $ . The sample $ z $ was thrown out in most previous approaches , but we do not do so . Knowing the $ z $ which generated x allows us to build a much better proposal distribution . Namely , this $ z $ by construction is a sample from the true posterior $ q ( z|x ) $ and because of the smoothness of the generator network , the mass of $ q ( z|x ) $ should concentrate near $ z $ . This same observation was made in [ 1 ] to motivate their use of only a few HMC samples to regularize the generator \u2019 s entropy . In our work , we let our proposal distribution be a Gaussian centered around $ z $ and we further refine this proposal to maximize the ELBO which allows us to learn the dimension-wise scale of the proposal . Thus our proposal distribution is not a sample from a zero-mean Gaussian . It is a sample from a Gaussian centered around a true posterior sample whose dimension-wise scale is tuned to have minimal KL-divergence from the true posterior . To make this clearer , we have moved the definition of $ \\xi ( z|z_0 ) $ out of the text into its own equation . In practice , we find importance sampling using this proposal performs very well ( Table 2 ) , even ( slightly ) out-performing HMC based entropy regularization . We attribute this improvement to a reduction in bias over the HMC entropy regularizer ( Figure 3 ) . We believe this gap in performance would likely close if more HMC steps were taken at training time but the number of samples used in our experiments ( referring to figure 3 , we used 2 burn-in steps ) resulted in training which was 20x slower than our method ( as mentioned in the paper ) . This massive increase in runtime is why we did not run the HMC baseline during our large-scale JEM experiments . The cost would have been prohibitive with the Resnet generator architectures we used . Despite all of this , we agree that the variance of the estimator is of just as much importance as the bias as analyzed in section 5.2 . For that reason , we have added additional results to this section which analyze the variance of the estimator applied to the Factor Analysis model shown in Figure 3 . We compare this variance with the HMC estimator . We find the stddev of our estimator is approximately 4x higher than the HMC estimator but the bias is approximately \u2155 the size of the HMC estimator ( Figure 3 ) . Empirically we find this increased variance to be alleviated by mini-batch averaging and the decreased bias results in better model performance . We also compute the effective sample size ( ESS ) of the importance sampling estimators . We find that the ESS of VERA is 1.31 for our CIFAR10 model and 1.29 for our MNIST model with the 20 importance samples we use in training , indicating importance sampling is giving informative gradient estimates . Using a $ \\mathcal { N } ( 0 , 1 ) $ proposal gives an ESS of 1.0 ( minimum possible value ) for both MNIST and CIFAR10 models , indicating the uninformed proposal distribution gives uninformative gradient estimates ( as expected ) . Regarding comparisons to experiments in SSM : We are confused by this question . Our experimental setup for section 5.1 exactly follows [ 2 ] . We stated this in Appendix B.3 , but have also updated the paper to state this in the main body as well . The only difference is that we train our models for more epochs because of the reduced learning rates we use . We re-ran some of the SSM experiments as well to follow this number of training epochs but found them to obtain their best performance ( in terms of likelihood ) very early in training , so this change did not impact the results . The focus of this work was on large-scale EBM training so we do not feel that other experiments from the SSM paper are relevant to our method . Image size in the appendix : We have increased the size of these images . We hope this response and the changes we made have cleared up some of your concerns about our work . Thank you for your time . [ 1 ] Dieng , Adji B. , et al . `` Prescribed generative adversarial networks . '' [ 2 ] Song , Yang , et al . `` Sliced score matching : A scalable approach to density and score estimation . ''"}, {"review_id": "ixpSxO9flk3-1", "review_text": "This paper proposes a new method on training energy-based models with maximum likelihood . Instead of using MCMC approaches to sample from the EBM , authors follow previous work on training neural generators for faster sample generation . In particular , authors consider a special generator where the output is convolved with Gaussian noise . The score function of this generator can be estimated with self-normalized importance sampling , which is then used to estimate the entropy term through the reparameterization trick . Authors demonstrate that their new method is able to train EBMs efficiently , and improves the stability and performance of JEMs compared to MCMC-based training approaches . # # # # Pros * The method is more computationally efficient compared to MCMC-based approaches . As the title suggests , no iterative MCMC approaches are needed . Though the technique is inherently similar to Dieng et al . ( 2019 ) , authors replaces the HMC sub-routine with a carefully designed variational approximation . * Experiments on JEMs are particularly interesting . Authors demonstrate that their method can train JEMs in a stable way and outperforms baselines on classification accuracy , sample quality , out-of-distribution detection and semi-supervised learning . * Writing is clear and easy to follow . # # # # Cons * From my perspective , the biggest disadvantage is related to various additional hyper-parameters . In VERA training , $ \\gamma $ controls the gradient penalty and $ \\lambda $ controls the contribution of the estimated entropy gradient . Both requires considerable tuning for optimal performance . However , typical MCMC-based approaches do not require gradient penalty , and tuning $ \\lambda $ is unsatisfying \u2014 should n't $ \\lambda \\equiv 1 $ for real maximum likelihood training ? * The idea of training a neural generator with the dual form of the likelihood objective has been explored before . Authors have compared with MEG , which uses a similar objective . I think adversarial dynamics embedding is also a necessary baseline , since it uses the same objective and has a special design of the neural generator to make entropy computing tractable . It would be better if authors will include it in both NICE and JEM experiments . * Authors cited ( Song & Ermon , 2019a ) when pointing out the difficulty of MI estimation . The reference below should also be included as it actually appeared earlier in 2018 . McAllester , David , and Karl Stratos . `` Formal limitations on the measurement of mutual information . '' International Conference on Artificial Intelligence and Statistics . 2020 . * Authors did not include methods such as MEG in the JEM experiments . Any reason ? - Post-rebuttal I appreciate the authors ' response and additional comparison against previous work . I do think that proper comparison with previous work is important , as it allows us to know better when and where the proposed approach is beneficial .", "rating": "7: Good paper, accept", "reply_text": "We thank you for your time and your kind words about our work . We will address your concerns in order . Regarding hyper-parameters : While we agree our approach introduces a new set of hyper-parameters to be tuned , we disagree that our approach \u201c adds \u201d hyper-parameters . In fact , our approach has fewer hyper-parameters than MCMC-based EBM training methods and these hyper-parameters can be set using common-sense principles . As you point out , the main hyper-parameters to tune are the strength of the entropy regularizer $ \\lambda $ and the strength of the gradient regularizer $ \\gamma $ . We compare this with the PCD training used in the original JEM work [ 1 ] . In PCD training with SGLD you must specify each of : the SGLD step-size , number of MCMC steps , SGLD noise variance , buffer size , and reinitialization frequency . So PCD has 3 more hyper-parameters than VERA . Further , we add that the gradient regularizer strength was chosen early in our experiments and not searched over . We found this choice worked well across multiple data domains and model sizes , so we recommend setting $ \\gamma = .1 $ and ignoring this when searching hyper-parameters . Further still , a simple analysis can verify that the strength of the entropy regularizer $ \\lambda $ has the effect of tempering the energy function . This is analogous to decoupling the step-size in SGLD from the noise variance as is common in EBM training [ 1 , 2 , 3 , 4 ] . This has the effect of increasing the impact of the gradient of the energy function in generator training ( or in sampling for PCD ) which makes learning more efficient at the cost of sample quality . As in PCD , this can be tuned to be as large as possible while keeping training stable . We have added a section ( B.1.1 ) to the appendix to explain this connection . We also find that training with VERA allows us to remove the Gaussian noise typically added to the input data to stabilize EBM training . The scale of this noise is another important hyper-parameter to set [ 1 , 2 ] and it can be removed completely when training with VERA . So to summarize , VERA has fewer hyper-parameters than PCD training , one parameter does not need to be searched over , the other parameter can be tuned in a common-sense fashion , and we can completely remove a pre-preprocessing step ( which has its own hyper-parameters ) that was previously required for stable EBM training . Thus we feel that the number of hyper-parameters of VERA is actually a strength of our method instead of a weakness . Regarding MEG in the JEM experiments : As you can see in section 5.1 the results of MEG are near identical to training with no entropy regularization at all . Further , our experiments on mode-capturing demonstrate that entropy regularization is not necessary to achieve strong performance at this task on MNIST-sized data . Thus , we feel that the MEG entropy regularizer is not responsible for the published performance at this task ( and no baseline without their regularizer was reported in the original work on MEG ) . This , combined with the results from 5.1 led us to believe this approach to entropy regularization would not be competitive in high dimensions so it was left out of our comparisons . Regarding Adversarial Dynamics Embedding in NICE and JEM experiments : We did not compare against this method as it was complex , not fully described in their paper , and the code provided was not easy to use . Regarding the citation for MI estimation : We were not aware of this work . It is quite insightful , and we have added a reference to the updated paper . [ 1 ] Grathwohl , Will , et al . `` Your classifier is secretly an energy based model and you should treat it like one . '' [ 2 ] Nijkamp , Erik , et al . `` Learning non-convergent non-persistent short-run MCMC toward energy-based model . '' [ 3 ] Nijkamp , Erik , et al . `` On the anatomy of mcmc-based maximum likelihood learning of energy-based models . '' [ 4 ] Du , Yilun , and Igor Mordatch . `` Implicit generation and generalization in energy-based models . ''"}, {"review_id": "ixpSxO9flk3-2", "review_text": "* * Summary * * : This paper presents a method for improving training of energy-based models . Rather than drawing samples using persistent contrastive divergence / MCMC , this approach parameterizes a separate model , which is trained to directly output samples . This effectively adds an additional KL divergence to the objective . The authors use a particular form of sampling model ( a latent Gaussian model ) , borrowing a few tricks for getting entropy estimates out of the model . Results are demonstrated on a few qualitative setups , but most of the results are centered on improved JEM on sample quality , out-of-distribution detection , and semi-supervised learning . The main benefit of the approach seems to be speed and stability , however , the authors also claim that minimal tuning is needed . * * Strong Points * * : Overall , the paper is fairly clear in its description of the background , method and experiments . Likewise , the tables , figures , and algorithm box provide clear demonstrations of key aspects of the approach . The descriptions in Eq.6 are also useful for unfamiliar readers . From my understanding , the technical aspects of the paper appear to be largely correct . Like previous papers , the authors use an objective that includes both the energy function and the approximate sampler . This includes the entropy of the sampler ( from the sampler \u2019 s KL ) . The authors opt for a Gaussian latent variable model as the sampler . Evaluating the entropy thus entails an integral over the posterior of this model . The key technical contribution of this paper is to approximate this integral not with MCMC sampling , but instead use an importance-weighted variational approximation . This involves drawing a sample from the prior , then sampling points from a Gaussian around this point . The experimental results in the paper are highly thorough . The authors demonstrate their method in settings where exact log-likelihood estimates are feasible ( a flow-based model and probabilistic PCA ) . This allows the authors to demonstrate both improved samples and log-likelihood performance . In these cases , the authors compare with several relevant baselines . In the main experiments of the paper , the authors incorporate their approximate sampling scheme into JEM , a recent hybrid discriminative/generative model . Experimental results demonstrate that using the approximate sampler generally results in ( slight ) improvements over JEM , with the exception of semi-supervised classification , where the proposed method is substantially better . In all cases , experiments are performed using standard , benchmark datasets , and the authors compare with competitive , recent methods . There are substantial details on the experiments in the appendix . This , combined with the experiments across multiple datasets and settings , suggests that these results are reproducible . However , given the fact that this methods requires training multiple models , exactly reproducing the results will likely require the corresponding code ( which the authors did not provide ) . While the method itself is not highly novel , the thorough experimental results somewhat compensate for this aspect . In particular , at least in the \u201c toy \u201d experiments , the authors compare with MEG , a relevant , related baseline . This demonstrates that , although the proposed method is a natural extension of previous works , the contribution is nevertheless useful . * * Weak Points * * : One weak point of this paper is with regard to novelty . The main contribution of this paper is in estimating the integral of the sampling model \u2019 s entropy with an importance-weighted variational approximation . This is a fairly natural extension of previous works on learning sampling models , simply making the estimation procedure more efficient . On its own , this is not a highly substantial contribution . It \u2019 s also not clear that this is the best set of design choices for this problem . For instance , would the issues with the entropy gradient not be obviated with an exact log-likelihood model , such as a flow-based model / autoregressive model ? Some of the theoretical considerations of the setup appear to be skimmed over . The authors present the approximate sampling method with an equality in Eq.4.However , in practice , the sampling model will not be able to reach the max , due to inherent limitations in the model class . Further , in Algorithm 1 , the energy-based model and the sampling model are trained jointly , rather than training the sampling model inside of an inner loop ( for the max operation ) . This is analogous to amortized inference in VAEs , where training the encoder and decoder jointly typically implies that the encoder can not fully maximize the ELBO for the decoder . This phenomenon is referred to as \u201c lagging inference networks , \u201d ( He et al. , 2019 ) and the performance gap is the \u201c amortization gap \u201d ( Cremer et al. , 2018 ) . Similarly , there will be a \u201c sampling gap \u201d here due to a \u201c lagging sampling model. \u201d While the method still seems to work well in practice , the fact that this is an approximation is almost entirely missing from the paper . Note that , to generate samples , the authors do indeed run additional MCMC steps ( see appendix ) . Finally , although the experiments are comprehensive , it \u2019 s not clear whether the improvements over JEM are highly substantial . In addition , in the case with the largest gap , semi-supervised learning , it \u2019 s unclear why the proposed method , VERA , should dramatically outperform JEM . Given that this is the most substantial result , it seems as though further investigation should be performed to analyze the reason for the boost in performance . Further , the experiments on JEM only compare the baseline models ( which uses MCMC ) with the proposed ( amortized ) method . A more complete set of experiments would also include other learned sampling methods , such as MEG . * * Accept / Reject * * : Although the method is not highly novel , the experiments are fairly comprehensive in demonstrating the proposed method . Analyses are performed in multiple settings , across multiple datasets , comparing with the relevant baselines . At the very least , this paper demonstrates an efficient method that seems to outperform previous methods in simpler tasks and improve JEM on more difficult tasks . For these reasons , I would be in favor of accepting this paper . With a more detailed motivation/explanation of the design choices , discussion of the quality of the approximation , and some additional sampling model baselines on the JEM results , this paper could be further improved . * * Questions * * : Could you elaborate on the issue with the score function estimator in Eq.6 ? Is this simply a matter of having a high-variance estimator ? In the importance weights , should $ q_\\phi ( x ) $ be included in the denominator ? In Algorithm 1 , are you using a mini-batch of data examples but only one sample from the model . If so , why not use a batch of samples ? The MNIST samples don \u2019 t look great , even for VERA . Why is this the case ? Is using NICE in Section 5.1 representative of energy-based models ? Flow-based models tend to have their own issues associated with stability . Why is VERA better at semi-supervised learning as compared with JEM ? * * Additional Feedback * * : Method : I would not compare Eq.5 with a GAN decoder , as GANs define an implicit density . \u201c or simple a diagonal Gaussian \u201d \u2014 > \u201c or simply a diagonal Gaussian \u201d \u201c to the gradient of our model \u2019 s likelihood\u2026 \u201d : should specify that this is the EBM . EBM Training Experiments : I would put zeros in front of all decimals to be consistent . Not sure if it \u2019 s fair to conclude that entropy regularization is unnecessary for preventing mode collapse from this experiment . This seems like a limited setting . Section A.2 : \u201c Equation equation 7 \u201d \u2014 > \u201c Equation 7 \u201d Figure 7 : \u201c Unconditional CIFAR10 Samples \u201d \u2014 > \u201c Unconditional MNIST Samples \u201d", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank you for your very comprehensive , thoughtful review , and for your kind words about our work . We will address your concerns in order : Regarding novelty : We have developed a novel entropy regularizer for latent-variable models which is considerably faster than prior approaches , does not require the training of auxiliary MI or score networks , and outperforms prior approaches for training large scale EBMs . Our proposed training procedure is several times faster than the state-of-the-art EBM training methods and alleviates the notorious stability issues found in these state-of-the-art approaches [ 1 ] . These stability improvements allow EBMs to be applied to domains where MCMC-based training fails ( section 6.1 ) and to perform well at new applications . We believe our work provides a novel contribution to the field . Use of flows or AR models : Autoregressive models are not useful in this setting since sampling comes at an O ( D ) cost and we must sample at every training iteration . The cost of sampling would dominate training . Flows are not favorable since they are much less parameter efficient and produce lower quality samples than the latent-variable models we use . The CIFAR10 Glow [ 2 ] model has approximately 100x as many parameters as our Resnet generator and is much slower to sample from . Despite this , it achieves much lower sample quality ( in terms of FID ) . For these reasons , we felt it valuable to address the difficulties that arise when using latent-variable generators -- which lead to this work . Regarding the approximations : In section 5.1 we explicitly state that our proposed optimization procedure maximizes an upper bound on likelihood . The objective is an upper bound exactly due to the \u201c sampling gap \u201d you bring up . The results in this section were specifically added to the paper to demonstrate that while we are indeed maximizing an upper bound , the bound is tight enough to train high-quality models at scale . To make this more clear we have added a discussion of this under equation 4 in the updated paper . We hope this clarifies your concern . Regarding your point about using MCMC to generate samples : The MCMC refinement procedure is there only to improve sample quality by making the generator samples match the distribution defined by the EBM more closely . This is not necessary and samples taken only from the generator are of high quality and competitive -- but of slightly lower quality . This can be seen in Appendix C.3 . Regarding Improvements over JEM : The main intent of this paper was to present a new method for training EBMs which is faster and more stable than MCMC-based training . We chose to apply this to JEM as JEM is a new and effective application of EBMs . Training JEM as proposed in [ 1 ] is slow to train and unstable ( [ 1 ] , Sections 6 , H.3 ) . The main point of these experiments was to demonstrate that we can match the performance of JEM while training much faster and relieving the instability that is common with PCD training of JEM models . The fact that we improve upon the results is even better , but we feel our point would have been made even if the results were identical to the original work on JEM . PCD training of EBMs has made much progress but almost all research in this area has focused on image models . Little attention has been paid to alternate domains . As we believed our approach is more stable and easier to tune than PCD training we wanted to focus on training models outside of the image domain . This is why we focused on a diverse range of non-image datasets for our SSL experiments . Note that SSL outside of the image domain can be especially challenging because we can \u2019 t rely on strong prior knowledge of useful data augmentations . Our VERA-trained JEM models achieved strong results at SSL whereas our PCD training JEM models all performed poorly despite a considerable hyper-parameter search . We believe VERA \u2019 s improved performance in this setting is due to its stability and ease of tuning . We believe it is likely that a PCD trained model could achieve similar performance but we were not able to train one to convergence . We chose not to compare with MEG in this setting because of our results in section 5.1 which demonstrated that their entropy regularizer had little-to-no effect on MNIST sized data . -continued below"}], "0": {"review_id": "ixpSxO9flk3-0", "review_text": "This paper proposes an improved algorithm to train EBM-based models , called Variational Entropy Regularized Approximate maximum likelihood . The basic idea is to formulate the intractable partition function as an optimization problem with an additional entropy term . To estimate the gradient of the entropy term , the authors then propose a variational formulation for approximation . The idea is interesting but not very exciting , and there consists of technical details that need to be carefully deal with . One of my biggest concerns is that since the algorithm relies on importance sampling to estimate the gradient of log marginal likelihood , the variance can be arbitrarily large . Specifically , the importance weight is the ration between a joint distribution and a proposed variational distribution \\xi . One can see that a sample from \\xi is a zero-mean Gaussian distribution , whereas q ( z , x ) should not . I suspect this would make the ratios to be vary unstable with high variance , which seems to contradict with the goal of the paper . In other words , I am not convinced why the importance sampling based method can work better than existing methods . In addition , this importance sampling based method is introduced to avoid sampling the posterior distribution with HMC . I think this step can be considered as an efficient approximation to the method , I am expecting HMC should perform at least as good as the proposed method but less efficient . The results in Table 2 seems to agree with this ( although not completely ) . I wonder how is the running time comparison ? Also , since HMC based method is a competitive method , why do n't you consider this in other experiments such as those in Section 6 ? Also , since the proposed method is claimed to outperform the recent SSM method . I think the same experiments as in the SSM paper should be conducted for comparison . BTW , the generative images in Appendix are too small to be informative . After rebuttal After the rebuttal , my main concern remains . Specifically , the paper defines a variational distribution q ( z|x0 via a hierarchical construction : z_0 ~ N ( 0 , 1 ) , z ~ N ( z_0 , \\eta I ) , which is essentially a zero-mean Gaussian . And I suspect the this is a bad variational distribution and it will induce high variance . The author said they did n't the hierarchical construction to define the variational distribution , because they fix z_0 after sampling . I do n't think this is a formal way of defining a variational distribution . One reason is that even if they fix z_0 , the proposal distribution will be a z_0-mean Gaussian , and the mean is randomly drawn from N ( 0 , 1 ) , which will not match the true posterior distribution ( they only optimize the variance parameter ) . I think this should be make clear and investigated in more details . I will keep my initial score .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank you for your time and your thoughtful comments on our work . We will address your concerns in order . Regarding the variance of the importance sampling estimator : These are important concerns you raise . You say in your review that $ \\xi $ is a sample from a zero-mean Gaussian distribution and this would be a bad proposal distribution for importance sampling from $ q ( z|x ) $ . We agree that if we were using such an uninformed proposal distribution for importance sampling , then the variance of the estimator would be very high . However , as described in the text directly following equation 8 , this is not the proposal distribution we use . We construct an informed proposal distribution that closely approximates $ q ( z|x ) $ and this informed proposal is a key contribution of our work . We elaborate more below : When generating a sample $ x \\sim q ( x ) $ , we first sample $ z \\sim q ( z ) $ and then $ x \\sim q ( x|z ) $ . The sample $ z $ was thrown out in most previous approaches , but we do not do so . Knowing the $ z $ which generated x allows us to build a much better proposal distribution . Namely , this $ z $ by construction is a sample from the true posterior $ q ( z|x ) $ and because of the smoothness of the generator network , the mass of $ q ( z|x ) $ should concentrate near $ z $ . This same observation was made in [ 1 ] to motivate their use of only a few HMC samples to regularize the generator \u2019 s entropy . In our work , we let our proposal distribution be a Gaussian centered around $ z $ and we further refine this proposal to maximize the ELBO which allows us to learn the dimension-wise scale of the proposal . Thus our proposal distribution is not a sample from a zero-mean Gaussian . It is a sample from a Gaussian centered around a true posterior sample whose dimension-wise scale is tuned to have minimal KL-divergence from the true posterior . To make this clearer , we have moved the definition of $ \\xi ( z|z_0 ) $ out of the text into its own equation . In practice , we find importance sampling using this proposal performs very well ( Table 2 ) , even ( slightly ) out-performing HMC based entropy regularization . We attribute this improvement to a reduction in bias over the HMC entropy regularizer ( Figure 3 ) . We believe this gap in performance would likely close if more HMC steps were taken at training time but the number of samples used in our experiments ( referring to figure 3 , we used 2 burn-in steps ) resulted in training which was 20x slower than our method ( as mentioned in the paper ) . This massive increase in runtime is why we did not run the HMC baseline during our large-scale JEM experiments . The cost would have been prohibitive with the Resnet generator architectures we used . Despite all of this , we agree that the variance of the estimator is of just as much importance as the bias as analyzed in section 5.2 . For that reason , we have added additional results to this section which analyze the variance of the estimator applied to the Factor Analysis model shown in Figure 3 . We compare this variance with the HMC estimator . We find the stddev of our estimator is approximately 4x higher than the HMC estimator but the bias is approximately \u2155 the size of the HMC estimator ( Figure 3 ) . Empirically we find this increased variance to be alleviated by mini-batch averaging and the decreased bias results in better model performance . We also compute the effective sample size ( ESS ) of the importance sampling estimators . We find that the ESS of VERA is 1.31 for our CIFAR10 model and 1.29 for our MNIST model with the 20 importance samples we use in training , indicating importance sampling is giving informative gradient estimates . Using a $ \\mathcal { N } ( 0 , 1 ) $ proposal gives an ESS of 1.0 ( minimum possible value ) for both MNIST and CIFAR10 models , indicating the uninformed proposal distribution gives uninformative gradient estimates ( as expected ) . Regarding comparisons to experiments in SSM : We are confused by this question . Our experimental setup for section 5.1 exactly follows [ 2 ] . We stated this in Appendix B.3 , but have also updated the paper to state this in the main body as well . The only difference is that we train our models for more epochs because of the reduced learning rates we use . We re-ran some of the SSM experiments as well to follow this number of training epochs but found them to obtain their best performance ( in terms of likelihood ) very early in training , so this change did not impact the results . The focus of this work was on large-scale EBM training so we do not feel that other experiments from the SSM paper are relevant to our method . Image size in the appendix : We have increased the size of these images . We hope this response and the changes we made have cleared up some of your concerns about our work . Thank you for your time . [ 1 ] Dieng , Adji B. , et al . `` Prescribed generative adversarial networks . '' [ 2 ] Song , Yang , et al . `` Sliced score matching : A scalable approach to density and score estimation . ''"}, "1": {"review_id": "ixpSxO9flk3-1", "review_text": "This paper proposes a new method on training energy-based models with maximum likelihood . Instead of using MCMC approaches to sample from the EBM , authors follow previous work on training neural generators for faster sample generation . In particular , authors consider a special generator where the output is convolved with Gaussian noise . The score function of this generator can be estimated with self-normalized importance sampling , which is then used to estimate the entropy term through the reparameterization trick . Authors demonstrate that their new method is able to train EBMs efficiently , and improves the stability and performance of JEMs compared to MCMC-based training approaches . # # # # Pros * The method is more computationally efficient compared to MCMC-based approaches . As the title suggests , no iterative MCMC approaches are needed . Though the technique is inherently similar to Dieng et al . ( 2019 ) , authors replaces the HMC sub-routine with a carefully designed variational approximation . * Experiments on JEMs are particularly interesting . Authors demonstrate that their method can train JEMs in a stable way and outperforms baselines on classification accuracy , sample quality , out-of-distribution detection and semi-supervised learning . * Writing is clear and easy to follow . # # # # Cons * From my perspective , the biggest disadvantage is related to various additional hyper-parameters . In VERA training , $ \\gamma $ controls the gradient penalty and $ \\lambda $ controls the contribution of the estimated entropy gradient . Both requires considerable tuning for optimal performance . However , typical MCMC-based approaches do not require gradient penalty , and tuning $ \\lambda $ is unsatisfying \u2014 should n't $ \\lambda \\equiv 1 $ for real maximum likelihood training ? * The idea of training a neural generator with the dual form of the likelihood objective has been explored before . Authors have compared with MEG , which uses a similar objective . I think adversarial dynamics embedding is also a necessary baseline , since it uses the same objective and has a special design of the neural generator to make entropy computing tractable . It would be better if authors will include it in both NICE and JEM experiments . * Authors cited ( Song & Ermon , 2019a ) when pointing out the difficulty of MI estimation . The reference below should also be included as it actually appeared earlier in 2018 . McAllester , David , and Karl Stratos . `` Formal limitations on the measurement of mutual information . '' International Conference on Artificial Intelligence and Statistics . 2020 . * Authors did not include methods such as MEG in the JEM experiments . Any reason ? - Post-rebuttal I appreciate the authors ' response and additional comparison against previous work . I do think that proper comparison with previous work is important , as it allows us to know better when and where the proposed approach is beneficial .", "rating": "7: Good paper, accept", "reply_text": "We thank you for your time and your kind words about our work . We will address your concerns in order . Regarding hyper-parameters : While we agree our approach introduces a new set of hyper-parameters to be tuned , we disagree that our approach \u201c adds \u201d hyper-parameters . In fact , our approach has fewer hyper-parameters than MCMC-based EBM training methods and these hyper-parameters can be set using common-sense principles . As you point out , the main hyper-parameters to tune are the strength of the entropy regularizer $ \\lambda $ and the strength of the gradient regularizer $ \\gamma $ . We compare this with the PCD training used in the original JEM work [ 1 ] . In PCD training with SGLD you must specify each of : the SGLD step-size , number of MCMC steps , SGLD noise variance , buffer size , and reinitialization frequency . So PCD has 3 more hyper-parameters than VERA . Further , we add that the gradient regularizer strength was chosen early in our experiments and not searched over . We found this choice worked well across multiple data domains and model sizes , so we recommend setting $ \\gamma = .1 $ and ignoring this when searching hyper-parameters . Further still , a simple analysis can verify that the strength of the entropy regularizer $ \\lambda $ has the effect of tempering the energy function . This is analogous to decoupling the step-size in SGLD from the noise variance as is common in EBM training [ 1 , 2 , 3 , 4 ] . This has the effect of increasing the impact of the gradient of the energy function in generator training ( or in sampling for PCD ) which makes learning more efficient at the cost of sample quality . As in PCD , this can be tuned to be as large as possible while keeping training stable . We have added a section ( B.1.1 ) to the appendix to explain this connection . We also find that training with VERA allows us to remove the Gaussian noise typically added to the input data to stabilize EBM training . The scale of this noise is another important hyper-parameter to set [ 1 , 2 ] and it can be removed completely when training with VERA . So to summarize , VERA has fewer hyper-parameters than PCD training , one parameter does not need to be searched over , the other parameter can be tuned in a common-sense fashion , and we can completely remove a pre-preprocessing step ( which has its own hyper-parameters ) that was previously required for stable EBM training . Thus we feel that the number of hyper-parameters of VERA is actually a strength of our method instead of a weakness . Regarding MEG in the JEM experiments : As you can see in section 5.1 the results of MEG are near identical to training with no entropy regularization at all . Further , our experiments on mode-capturing demonstrate that entropy regularization is not necessary to achieve strong performance at this task on MNIST-sized data . Thus , we feel that the MEG entropy regularizer is not responsible for the published performance at this task ( and no baseline without their regularizer was reported in the original work on MEG ) . This , combined with the results from 5.1 led us to believe this approach to entropy regularization would not be competitive in high dimensions so it was left out of our comparisons . Regarding Adversarial Dynamics Embedding in NICE and JEM experiments : We did not compare against this method as it was complex , not fully described in their paper , and the code provided was not easy to use . Regarding the citation for MI estimation : We were not aware of this work . It is quite insightful , and we have added a reference to the updated paper . [ 1 ] Grathwohl , Will , et al . `` Your classifier is secretly an energy based model and you should treat it like one . '' [ 2 ] Nijkamp , Erik , et al . `` Learning non-convergent non-persistent short-run MCMC toward energy-based model . '' [ 3 ] Nijkamp , Erik , et al . `` On the anatomy of mcmc-based maximum likelihood learning of energy-based models . '' [ 4 ] Du , Yilun , and Igor Mordatch . `` Implicit generation and generalization in energy-based models . ''"}, "2": {"review_id": "ixpSxO9flk3-2", "review_text": "* * Summary * * : This paper presents a method for improving training of energy-based models . Rather than drawing samples using persistent contrastive divergence / MCMC , this approach parameterizes a separate model , which is trained to directly output samples . This effectively adds an additional KL divergence to the objective . The authors use a particular form of sampling model ( a latent Gaussian model ) , borrowing a few tricks for getting entropy estimates out of the model . Results are demonstrated on a few qualitative setups , but most of the results are centered on improved JEM on sample quality , out-of-distribution detection , and semi-supervised learning . The main benefit of the approach seems to be speed and stability , however , the authors also claim that minimal tuning is needed . * * Strong Points * * : Overall , the paper is fairly clear in its description of the background , method and experiments . Likewise , the tables , figures , and algorithm box provide clear demonstrations of key aspects of the approach . The descriptions in Eq.6 are also useful for unfamiliar readers . From my understanding , the technical aspects of the paper appear to be largely correct . Like previous papers , the authors use an objective that includes both the energy function and the approximate sampler . This includes the entropy of the sampler ( from the sampler \u2019 s KL ) . The authors opt for a Gaussian latent variable model as the sampler . Evaluating the entropy thus entails an integral over the posterior of this model . The key technical contribution of this paper is to approximate this integral not with MCMC sampling , but instead use an importance-weighted variational approximation . This involves drawing a sample from the prior , then sampling points from a Gaussian around this point . The experimental results in the paper are highly thorough . The authors demonstrate their method in settings where exact log-likelihood estimates are feasible ( a flow-based model and probabilistic PCA ) . This allows the authors to demonstrate both improved samples and log-likelihood performance . In these cases , the authors compare with several relevant baselines . In the main experiments of the paper , the authors incorporate their approximate sampling scheme into JEM , a recent hybrid discriminative/generative model . Experimental results demonstrate that using the approximate sampler generally results in ( slight ) improvements over JEM , with the exception of semi-supervised classification , where the proposed method is substantially better . In all cases , experiments are performed using standard , benchmark datasets , and the authors compare with competitive , recent methods . There are substantial details on the experiments in the appendix . This , combined with the experiments across multiple datasets and settings , suggests that these results are reproducible . However , given the fact that this methods requires training multiple models , exactly reproducing the results will likely require the corresponding code ( which the authors did not provide ) . While the method itself is not highly novel , the thorough experimental results somewhat compensate for this aspect . In particular , at least in the \u201c toy \u201d experiments , the authors compare with MEG , a relevant , related baseline . This demonstrates that , although the proposed method is a natural extension of previous works , the contribution is nevertheless useful . * * Weak Points * * : One weak point of this paper is with regard to novelty . The main contribution of this paper is in estimating the integral of the sampling model \u2019 s entropy with an importance-weighted variational approximation . This is a fairly natural extension of previous works on learning sampling models , simply making the estimation procedure more efficient . On its own , this is not a highly substantial contribution . It \u2019 s also not clear that this is the best set of design choices for this problem . For instance , would the issues with the entropy gradient not be obviated with an exact log-likelihood model , such as a flow-based model / autoregressive model ? Some of the theoretical considerations of the setup appear to be skimmed over . The authors present the approximate sampling method with an equality in Eq.4.However , in practice , the sampling model will not be able to reach the max , due to inherent limitations in the model class . Further , in Algorithm 1 , the energy-based model and the sampling model are trained jointly , rather than training the sampling model inside of an inner loop ( for the max operation ) . This is analogous to amortized inference in VAEs , where training the encoder and decoder jointly typically implies that the encoder can not fully maximize the ELBO for the decoder . This phenomenon is referred to as \u201c lagging inference networks , \u201d ( He et al. , 2019 ) and the performance gap is the \u201c amortization gap \u201d ( Cremer et al. , 2018 ) . Similarly , there will be a \u201c sampling gap \u201d here due to a \u201c lagging sampling model. \u201d While the method still seems to work well in practice , the fact that this is an approximation is almost entirely missing from the paper . Note that , to generate samples , the authors do indeed run additional MCMC steps ( see appendix ) . Finally , although the experiments are comprehensive , it \u2019 s not clear whether the improvements over JEM are highly substantial . In addition , in the case with the largest gap , semi-supervised learning , it \u2019 s unclear why the proposed method , VERA , should dramatically outperform JEM . Given that this is the most substantial result , it seems as though further investigation should be performed to analyze the reason for the boost in performance . Further , the experiments on JEM only compare the baseline models ( which uses MCMC ) with the proposed ( amortized ) method . A more complete set of experiments would also include other learned sampling methods , such as MEG . * * Accept / Reject * * : Although the method is not highly novel , the experiments are fairly comprehensive in demonstrating the proposed method . Analyses are performed in multiple settings , across multiple datasets , comparing with the relevant baselines . At the very least , this paper demonstrates an efficient method that seems to outperform previous methods in simpler tasks and improve JEM on more difficult tasks . For these reasons , I would be in favor of accepting this paper . With a more detailed motivation/explanation of the design choices , discussion of the quality of the approximation , and some additional sampling model baselines on the JEM results , this paper could be further improved . * * Questions * * : Could you elaborate on the issue with the score function estimator in Eq.6 ? Is this simply a matter of having a high-variance estimator ? In the importance weights , should $ q_\\phi ( x ) $ be included in the denominator ? In Algorithm 1 , are you using a mini-batch of data examples but only one sample from the model . If so , why not use a batch of samples ? The MNIST samples don \u2019 t look great , even for VERA . Why is this the case ? Is using NICE in Section 5.1 representative of energy-based models ? Flow-based models tend to have their own issues associated with stability . Why is VERA better at semi-supervised learning as compared with JEM ? * * Additional Feedback * * : Method : I would not compare Eq.5 with a GAN decoder , as GANs define an implicit density . \u201c or simple a diagonal Gaussian \u201d \u2014 > \u201c or simply a diagonal Gaussian \u201d \u201c to the gradient of our model \u2019 s likelihood\u2026 \u201d : should specify that this is the EBM . EBM Training Experiments : I would put zeros in front of all decimals to be consistent . Not sure if it \u2019 s fair to conclude that entropy regularization is unnecessary for preventing mode collapse from this experiment . This seems like a limited setting . Section A.2 : \u201c Equation equation 7 \u201d \u2014 > \u201c Equation 7 \u201d Figure 7 : \u201c Unconditional CIFAR10 Samples \u201d \u2014 > \u201c Unconditional MNIST Samples \u201d", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank you for your very comprehensive , thoughtful review , and for your kind words about our work . We will address your concerns in order : Regarding novelty : We have developed a novel entropy regularizer for latent-variable models which is considerably faster than prior approaches , does not require the training of auxiliary MI or score networks , and outperforms prior approaches for training large scale EBMs . Our proposed training procedure is several times faster than the state-of-the-art EBM training methods and alleviates the notorious stability issues found in these state-of-the-art approaches [ 1 ] . These stability improvements allow EBMs to be applied to domains where MCMC-based training fails ( section 6.1 ) and to perform well at new applications . We believe our work provides a novel contribution to the field . Use of flows or AR models : Autoregressive models are not useful in this setting since sampling comes at an O ( D ) cost and we must sample at every training iteration . The cost of sampling would dominate training . Flows are not favorable since they are much less parameter efficient and produce lower quality samples than the latent-variable models we use . The CIFAR10 Glow [ 2 ] model has approximately 100x as many parameters as our Resnet generator and is much slower to sample from . Despite this , it achieves much lower sample quality ( in terms of FID ) . For these reasons , we felt it valuable to address the difficulties that arise when using latent-variable generators -- which lead to this work . Regarding the approximations : In section 5.1 we explicitly state that our proposed optimization procedure maximizes an upper bound on likelihood . The objective is an upper bound exactly due to the \u201c sampling gap \u201d you bring up . The results in this section were specifically added to the paper to demonstrate that while we are indeed maximizing an upper bound , the bound is tight enough to train high-quality models at scale . To make this more clear we have added a discussion of this under equation 4 in the updated paper . We hope this clarifies your concern . Regarding your point about using MCMC to generate samples : The MCMC refinement procedure is there only to improve sample quality by making the generator samples match the distribution defined by the EBM more closely . This is not necessary and samples taken only from the generator are of high quality and competitive -- but of slightly lower quality . This can be seen in Appendix C.3 . Regarding Improvements over JEM : The main intent of this paper was to present a new method for training EBMs which is faster and more stable than MCMC-based training . We chose to apply this to JEM as JEM is a new and effective application of EBMs . Training JEM as proposed in [ 1 ] is slow to train and unstable ( [ 1 ] , Sections 6 , H.3 ) . The main point of these experiments was to demonstrate that we can match the performance of JEM while training much faster and relieving the instability that is common with PCD training of JEM models . The fact that we improve upon the results is even better , but we feel our point would have been made even if the results were identical to the original work on JEM . PCD training of EBMs has made much progress but almost all research in this area has focused on image models . Little attention has been paid to alternate domains . As we believed our approach is more stable and easier to tune than PCD training we wanted to focus on training models outside of the image domain . This is why we focused on a diverse range of non-image datasets for our SSL experiments . Note that SSL outside of the image domain can be especially challenging because we can \u2019 t rely on strong prior knowledge of useful data augmentations . Our VERA-trained JEM models achieved strong results at SSL whereas our PCD training JEM models all performed poorly despite a considerable hyper-parameter search . We believe VERA \u2019 s improved performance in this setting is due to its stability and ease of tuning . We believe it is likely that a PCD trained model could achieve similar performance but we were not able to train one to convergence . We chose not to compare with MEG in this setting because of our results in section 5.1 which demonstrated that their entropy regularizer had little-to-no effect on MNIST sized data . -continued below"}}