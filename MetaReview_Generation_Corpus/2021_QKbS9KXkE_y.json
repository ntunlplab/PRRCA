{"year": "2021", "forum": "QKbS9KXkE_y", "title": "Data-efficient Hindsight Off-policy Option Learning", "decision": "Reject", "meta_review": "There was a fair amount of discussion about the paper.  Several reviewers felt that the paper would have been stronger if it tried to do less but better.  The reviews describe in detail what the reviewers would have found compelling, but the key suggestion is to remove the complexity that is not essential for the approach to provide consistent improvements.  Doing this requires a better understanding of the algorithm's behavior and a valid ablation study, a new concern raised during the discussion with the authors. \n\nThe reviewers felt that the proposed approach is potentially interesting and would like to see this paper done well.", "reviews": [{"review_id": "QKbS9KXkE_y-0", "review_text": "# # Summary This paper introduces a novel option-learning policy gradient method , HO2 . The method learns a parameterized joint distribution over options and actions and uses a soft-continuation based approach to interrupt or `` switch '' between options before option termination . The method introduces a new meta-parameter which enforces a hard limit on the number of `` switches '' that can occur , significantly reducing the variance of the option-learning method and replacing softer loss penalization based approaches . The paper demonstrates the performance of the proposed algorithm on a handful of 3D virtualized environments as well as on robotic simulation tasks . # # Review # # # Summary I am currently leaning towards recommending reject for this paper . While the approach and algorithm are novel , they also appear to be highly complex , do n't provide a noticeable or consistent improvement over much simpler benchmarks , and I fear that the improvements that _are_ seen are likely due to variance in the results or are hidden behind the additional machinery in the algorithm . I remain slightly skeptical of the utility of the proposed meta-parameter $ n $ for setting a hard limit on the number of `` switches '' between active options , with my skepticism primarily due to concerns on the difficulty of tuning this parameter and the domain-specificity of the parameter . # # # Details I 'm curious about some of the hidden complexities in the algorithm . In the problem setup , a policy is defined on an MDP as possibly being $ \\pi ( a | h ) $ where $ h=\\ { s_t , a_ { t-1 } , s_ { t-1 } , a_ { t-2 } , \\ldots , s_0\\ } $ , that is a full history of interactions . First , I suppose this means we are no longer dealing with the original MDP and are working in a modified MDP where the `` Markov '' state is a full history ; already leading towards an exponential growth of the state-space . The algorithm itself depends on a recursive product of distributions for the entire length of a sampled trajectory as a result of this adapted Markov state . I 'm initially worried at how difficult it is to keep this probability from decaying towards 0 rapidly . There appear to be multiple partial marginalizations ( e.g. $ \\sum_ { i = 0 } ^M p ( X | Y = y_i ) p ( Y=y_i ) $ ) which require rescaling the final product to stay within the standard simplex , perhaps this is used to prevent the product of distributions from decaying towards 0 ? One of the primary motivations of the hard switching limit is that an auxiliary penalization on the objective is hard to tune . However , it is n't clear to me that the hard limit parameter $ n $ would be any easier to tune . In fact , because the inclusion of $ n $ seems to require more partial marginalizations , it almost seems as if this would cause additional complexity in the optimization problem . Did you find this meta-parameter easy to tune ? What are the effects of choosing it to be $ n=5 $ for the experiments instead of ( say ) 10 ? It appears to play a very mild variance reduction role in the results ( though with only 5 seeds , we _really_ ca n't say much about variance since this is severely underestimating variance ) . If the switching limit prefers to stay small , would this suggest that the best form of the algorithm is one without the options framework at all ( e.g.the best version of the proposed algorithm is an unaltered actor-critic algorithm ) ? The experiments in the paper start with deep neural networks with all of the necessary machinery to make Deep RL run at the moment , including experience replay , target networks , ADAM optimizer , layer norms , mini-batches , various types of activations on each layer , neural networks of different architectures for each of the three sets of weights , different stepsizes for each of the networks , etc . I fail to see why this approach could n't have been studied in a much simpler linear function approximation setting where statistically significant results with fewer confounding variables could have been achieved . As it stands , it is entirely unclear to me if the proposed algorithm actually provides any benefit when , in the midst of all of the machinery , the modifications above the benchmark algorithms are modest . Given this , there certainly is something to be said for a novel algorithm that does perform favorably when included in the machinery of a Deep RL feat of engineering . I , however , am unsure if the proposed algorithm does perform favorably . From the results , it appears that in most cases RHPO performs equivalently to the proposed ; certainly not statistically significantly different . With only 5 random seeds , it would be very hard to make sound claims ; especially considering the known variance issues in Deep RL ( take Henderson et al.2018 for a deeper discussion ) . The one place where the proposed algorithm _does_ outperform RHPO is in the robot simulator ( though again with only 5 seeds , heavy skepticism is called for ) . I found this fascinating and am curious if there is some structure that the proposed algorithm is able to take advantage of in this domain that RHPO is unable to replicate . # After discussion period : I have read all other reviews , resulting conversation , and have read the edits to the paper . After extended conversation with the authors and a deeper investigation into the empirical components of the paper , I find I have further concerns than originally realized in my original review and that many of my original concerns remain . I am lowering my recommendation from a 5 - > 3 to reflect the new concerns ; namely the validity of the ablation study as detailed in-depth below .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for their detailed and constructive feedback . In this post , we will focus on reviewer specific feedback . We will additionally provide an overview post describing the general feedback and corresponding changes . - Working with an extended MDP Yes , it is correct that the options framework in general ( not just our method ) extends the MDP . Our paper uses the usual semi-MDP model [ 3 ] which is common across option frameworks . Although our notation in Equation 3 remains general in that the policy at time t is dependent on the entire history , this dependence is later refined to the kind of dependence that is described in the semi-MDP framework ( i.e.the dependence is mediated by the option which is described in Equation 4 ) . We do not discuss semi-MDP framework in detail in the paper , but there are relevant discussions to be found for example in [ 1 , 2 , 3 ] . In addition , we have clarified this aspect in the paper . - Details for the switch constrained extension The problem of controlling when to switch between options is a known challenge ( e.g . [ 1 ] ) .For most domains , it is usually not known a priori what would be appropriate switching behaviour . The possibility to explicitly optimise for temporal consistency via the switch constraints , which can further improve performance , is an additional feature of HO2 ( as shown in Figure 6 ) . Our presented approach aims to simplify tuning this aspect . It is easier to tune than another weighted cost term ( as e.g.given in [ 1 ] ) since it can be chosen independently of the reward scale and does not directly cause a potential conflict with other reward terms . In addition , setting how many times the agent should maximally switch between options along a trajectory can be more intuitive than an additional cost term , which is missing a similarly easy semantic interpretation . We will emphasise this further in the paper . If the switching limit prefers to stay small , would this suggest that the best form of the algorithm is one without the options framework at all ( e.g.the best version of the proposed algorithm is an unaltered actor-critic algorithm ) ? The algorithm often converges to a low rate of switching but never ( in our experiments ) degrades to the single option case ( which would be equivalent to a non-hierarchical policy ) . This shows that even when the agent would be able to change its behaviour to represent the \u2018 unaltered actor-critic algorithm \u2019 , there are benefits in using multiple options . Further , note that MPO trains a flat single actor-critic model , and yields poorer performance . We have clarified this in the experiments section . - Simpler setting e.g.linear function approximation setting Simplifying evaluation to obtain more general insights is an important point . However , designing toy domains that still include the relevant aspects which render the real-world problems hard is challenging on its own . We investigate option learning in particular in the context of deep models as these can represent solutions to more complex tasks which share more aspects with real-world control problems . While sharing some of the challenges with deep models , the optimisation of linear models is commonly affected by different aspects of the optimization problem . While clearly multiple factors affect performance ( as the reviewer suggests ) , we carefully ablate over these in the experiments to identify their effect . These experiments include : 1 ) Comparison to current on-policy option algorithms to estimate the importance of off-policy learning in HO2 . 2 ) Comparison to flat and mixture policies ( MPO and RHPO ) with equivalent underlying policy optimization to independently evaluate the benefits of temporal and action abstraction . 3 ) Ablation over the use of switching constraints , action-conditioning , off-policyness , and robustness via trust region constraints . 4 ) Analysis and interpretation of option decompositions for simpler tasks . - Performance differences on the simpler benchmarks and in general We have changed the paper to be more specific with respect to domains where HO2 provides the strongest performance gains . However , across all domains , the proposed method performs either better than existing methods or at least on par . - Variance of results and number of seeds We agree that variance and significance of results in reinforcement represent a critical point . Commonly , authors have to trade-off between increasing accuracy of estimates and acceptable computational cost . We run 5 seeds per algorithm per task ( a number common across RL papers and also used in [ 4 ] for most experiments ) . We see consistent results across many tasks with stronger benefits dominant in more complex tasks . [ Feedback continues in the next post ]"}, {"review_id": "QKbS9KXkE_y-1", "review_text": "The paper introduces a reinforcement learning algorithm with temporal abstraction using the options framework . It provides empirical results in a variety of domains , demonstrating that the algorithm can improve data efficiency . The paper is well motivated . Data efficiency is an important concern in applications of reinforcement learning . The approach is sufficiently novel . The empirical results are positive , showing performance improvements in a variety of domains . Results in simulated robotic manipulation tasks are particularly positive , as measured by average return . The paper can be improved by providing additional analysis to better understand the behaviour of the algorithm and the conditions under which it improves performance . For instance , it would be useful to see what type of options are being learned in the various domains and with different limits on the number of switches allowed . I had difficulty in evaluating the importance of the performance differences in Figure 5 . In the main text of the paper , there is no information on the reward structure of the task . Additional information is present in the appendix but I could not easily locate the relevant information ( if it is indeed there ) . For instance , it would be useful to know what the behavioural difference is between average returns of 60 and 100 . In figure 3 , the number of switches are listed as 5 . Some experimentation with various different values would be informative . In figure 5 , seeing a longer training period would be informative . MPO and RHPO are still improving at the end of the learning curve . Please explain Equation 4 in some detail . I could not follow . In particular , I do not follow why the $ \\pi^ { L } $ term is there . The paper is not easy to read . Many sentences do not communicate the intended meaning clearly . As an example , the first paragraph of the introduction would be difficult to understand by readers who are not already familiar with hierarchical reinforcement learning , the options framework , and the papers cited . And some of the writing is not clear regardless of the background of the reader . For instance , the first paragraph ends with `` Overall , the interaction of algorithm and environment can become increasingly difficult , especially in an off-policy setting ( Precup et al. , 2006 ) . '' It is not clear what is meant by a `` difficult '' interaction here . The writing could be more nuanced in the discussion of results presented in Figure 3 . The authors write that \u201c off-policy learning alone [ .. ] improves data-efficiency and can suffice to outperform on-policy option algorithms such as DAC , IOPG and Option-Critic. \u201d This is not true in every domain . Similarly , the authors write that they \u201c achieve improvements when training mixture policies via RHPO \u201d . Again , this is not true in all four domains . For instance , in Hopper-v2 , RHPO lags behind MPO . In the pseudo code for Algorithm 1 on page 5 , please specify inputs to the algorithm . And please do not use any undefined symbols ( e.g. , $ \\pi \u2019 $ , $ Q \u2019 $ ) . There is no reference to Figure 6 in the text . In Figure 3 , please include DAC , OC , and IOPG in the figure legend . In Figure 3 , the way the performance of DAC , OC , and IOPG are presented on the plots is misleading . For each algorithm , a constant value is shown from step 0 until step $ 2 \\times 10^6 $ although these constants correspond only to average return obtained after $ 2 \\times 10^6 $ steps . Furthermore , my understanding is that these numbers have been taken from Zhang & Whiteson ( 2019 ) . For best experimental practice , these algorithms should be tested by the authors themselves along with the other algorithms shown in the plot ( e.g. , HO2 ) . This would ensure that the performances are truly comparable and that there are differences in relevant experimental settings . In addition , it would allow the reader to compare the algorithms along the entirety of their learning curves . Doina Precup 's dissertation is listed twice in the references , with different publications years . Misuse of the comma is prevalent throughout the paper . The author response answered some of my questions . But I can not say that I now better understand the behaviour of the algorithm and the conditions under which it improves performance . I agree with reviewer 2 that analysing behaviour and performance in a simpler setting would be informative . While the writing has improved , it stills lacks the clarity and nuance one would wish to see in a paper at this conference .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their detailed and constructive feedback . In this post , we will focus on reviewer specific feedback . We will additionally provide an overview post describing the general feedback and corresponding changes . - Additional analysis of the behaviour of the algorithm ; conditions under which it improves performance ; types of emerging options We agree that these aspects are important to understand . We have clarified and extended existing analysis sections . Section 4.2 focuses on differences to a simple Gaussian policy and a mixture of Gaussians policy ( which can be understood as a single-step options model ) . In these sections , we analyse the impact of action abstraction ( the ability to control by choosing from a set of individual skills/options ) and temporal abstraction ( the ability to model consistent behaviour enabled via the termination conditions ) . Furthermore , Section 4.3 includes the investigation of the impact of off-policy training , trust-region constraints and different aspects affecting the types of option decompositions . For improved understanding , we will include further details in the main paper regarding the properties of learned options based on conditions such as switch constraints and information asymmetry ( information only provided as input to a part of the agent ) between high- and low-level controller . - Clarity of writing Thanks for pointing this out . We have gone through the paper carefully and changed any sentences that may have been unclear . In particular , we adapted the introduction to render the paper more accessible and improve overall readability . Please feel free to use the openreview diff function to trace these changes . - Gym experiments : On-policy and off-policy option learning ; comparing mixture and option policies In general , off-policy algorithms have a considerable advantage over on-policy methods as they enable multiple updates for the same data samples . This advantage grows when training with stochastic gradient descent and only small adjustments per update . With respect to our experiments , we have explicitly clarified on which domains off-policy learning ( including non-hierarchical policies ) outperformed existing work on on-policy option learning . Additionally , we have emphasised the equivalent details in the RHPO and HO2 comparison . - Gym experiments : results from DAC , OC , IOPG We agree that this is an important figure and there is no perfect way of doing this comparison . We 've attempted to do it in a fair manner and will clarify the motivation behind this type of comparison below . As described in the caption , we use previously obtained results in the gym domains from prior work and have additionally contacted the authors to ensure that the environments and experiment setting are exactly the same . We chose to follow this direction to ensure that we take the best known results in this benchmark ( a common practice in many fields e.g.computer vision , NLP and partially used in RL as well ) , instead of using a sub-optimal reimplementation of the algorithms which would potentially underperform existing results . Using straight lines to indicate final results after the maximum training time of 2x10^6 steps instead of complete learning curves has two reasons . First , to prevent additional clutter in the graphs . Second , the learning curve comparison between on-policy and off-policy learning is only meaningful within limits . While we can align the number of actor steps , we can not do so for learner steps as the ratio can be independently chosen in off-policy learning . We will further emphasise the description of the lines in the caption ( which we use instead of an additional legend ) to ensure the reader is aware of this setting . - Details for the switch constrained extension The problem of controlling when to switch between options is a known challenge ( e.g . [ 1 ] ) .For most domains , it is usually not known a priori what would be appropriate switching behaviour . The possibility to explicitly optimise for temporal consistency via the switch constraints , which can further improve performance , is an additional feature of HO2 ( as shown in Figure 6 ) . Our presented approach aims to simplify tuning this property . It is easier to tune than another weighted cost term ( as e.g.given in [ 1 ] ) since it can be chosen independently of the reward scale and does not directly cause a potential conflict with other reward terms . In addition , setting how many times the agent should maximally switch between options along a trajectory can be more intuitive than an additional cost term , which is missing a similar semantic interpretation . Empirically , we have found performance in the learning from scratch results particularly robust with respect to different values of the constraint . We have emphasised this further in the paper . [ Feedback continues in the next post ]"}, {"review_id": "QKbS9KXkE_y-2", "review_text": "The paper considers the Hierarchical Reinforcement Learning setting , Options in particular , and proposes an algorithm that allows to learn both the high-level and low-level ( option ) policies at once , from off-policy samples . An original aspect of the algorithm is that it is easy to constrain the learned policies on how often they terminate an option and start a new one . This prevents the agent from learning tiny options that immediately terminate . It is unclear whether it can also be used to prevent the agent from learning a single big option that does everything . The paper is very interesting and has a high educational value . It combines many different approaches and is mathematically sound . The empirical evaluation shows encouraging results . However , the significance of the empirical results is difficult to measure , due to a few cons of this paper : - The paper is quite difficult to follow , and requires several attentive reads to be understood ( by someone having a deep knowledge about options , intra-option learning and the Option-Critic architecture ) . I believe that the lack of clarity comes from the brevity of the paper , that has to fit in the page limit . I would suggest the authors to remove Figures 1 and 2 , that take place while still being very difficult to understand . The text helps understand the figures , the figures do not help understand the text , so I would remove the figures . `` Problem setup '' in the preliminaries could also be removed , and replaced with an introductory paragraph in `` Method '' , that clearly states what the contribution will be , and what are its main components/properties . - `` multi-task learning '' , just before `` Experiments '' , is then used in the experiments to increase the sample-efficiency of all the algorithms . Because it is used for all the algorithms and not ablated , I do n't see how it contributes to the paper . I think that the paper already proposes many ideas , and that multi-task learning could be omitted and left for a future paper . - In the experiments , comparing MPO with HO2 allows to see a benefit from the use of options , with the proposed learning algorithm . This is a good point . However , MPO is not a well-known baseline , and a quick glance at the plots does not allow the reader to see that MPO does not use options . I would suggest either to add a little `` no options '' next to MPO in the figures , or to replace it ( or add to it ) PPO , ACKTR , ACER or the Soft Actor-Critic ( SAC ) . These algorithms are well-known baselines . Not all of them are off-policy , but I believe that comparing HO2 to state-of-the-art algorithms , without restriction to off-policy ones , would better allow to illustrate all the gains to be obtained by HO2 . - A good argument for the use of options is to use them for explainability : telling an observer what is the current option , and what it tries to achieve . Is it possible to have a small discussion of what do the options learned by HO2 do ? Do they aim at goals , or do they perform small bits of trajectories ? In summary , I like the proposed algorithm , the core contribution of the paper , the contents of Section 3 . However , the clarity of this paper is to me too low , and prevents fully grasping the impact of the proposed research . I therefore recommend against acceptance , and invite the authors to remove parts on multi-task , figures that do not help , and use well-known baselines in their evaluation . With the space gained with these changes , more text can be spent summarizing what the contribution will be , and motivating the use of options . Author response : the authors answered my question about the absence of learning curves , and provided extra details . However , I still think that the paper could be clearer and more focused , a sentiment that I think I share with the other reviewers . Given my hesitation , I would therefore not vote for accepting this paper , but I acknowledge that the proposed method is original and interesting , so I would not mind if this paper were to be accepted .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their detailed and constructive feedback . In this post , we will focus on reviewer specific feedback . We will additionally provide an overview post describing the general feedback and corresponding changes . - Motivating the use of options We thank the reviewer for raising this point . With the additional page available to address such concerns , we have extended the introduction to more clearly motivate options and discuss the merits of such an approach before delving into the existing options literature . - Lack of clarity ( figures , contributions , and multi-task learning ) Figures 1 and 2 are used to support understanding of the option policies , mixture policies and temporal consistency parts in Section 3 by providing the corresponding graphical models . We believe that this will help understanding for readers who are more familiar with graphical models , sequence modelling , and related fields , while those more familiar with the option literature may benefit more from the derivations in Section 3 . We have added additional references in the text when introducing the corresponding equations , improved the captions , and added some brief intuition to the start of each subsection . Our contributions are explicitly stated in bullet form at the end of Section 1 . We additionally separate the method section with respect to flat Gaussians and mixture policies which have been trained via critic-weighted maximum likelihood in prior work and the proposed extension to train option policies . The current writing aims to find a compromise between clear separation of methods and clarity and self-containment of the method description . We have furthermore emphasized the use of MPO and RHPO to train Gaussian and mixture policies in prior work . Finally , since options represent reusable behaviour abstractions , they are known to be beneficial in a multi-task setting where they can be shared across tasks ( see e.g . [ 1 ] ) .We additionally use multitask learning with related tasks to enable us to address more complex domains with acceptable data requirements . The described methods for multi-task learning have been proposed and tested in prior work and we merely use them here to accelerate learning . We have integrated the corresponding description into the experimental section and adapted it for clarification . - Clarity regarding the use of MPO We use MPO ( using flat Gaussian policies ) and RHPO ( using mixture policies ) as the baseline methods for comparison with HO2 , because they use an equivalent underlying optimization procedure based on critic-reweighted likelihood estimation . The fact that the base algorithm is equivalent makes it easy to compare the three approaches and to isolate the effects of action abstraction ( which MPO does not have ) and temporal abstraction ( which both MPO and RHPO miss ) . We have extended the text to make this clear , and have also clarified that MPO trains a flat policy in the experiments , as suggested by the reviewer . - On learning a single degenerate option that solves everything The reviewer is right to point out that in certain cases , hierarchical approaches can discover degenerate solutions such as a single option . In the case of HO2 , the experiments found that with enough structure in the problem setup , a diverse set of options was learned and used - this is also supported and explained by the \u201c Further analysis \u201d experiments in Section 4.3 . [ Feedback continues in the next post ]"}, {"review_id": "QKbS9KXkE_y-3", "review_text": "This paper studies an important area in RL , hierarchical RL , which improves data efficiency by incorporating abstractions . In this paper , the authors proposes an efficient option learning algorithm , which utilizes a TD ( 0 ) type objective and constrains the learned policy being not too far away from the past policy . In terms of different abstractions , the paper studies action abstraction through a mixture policy , and temporal abstraction through explicitly limiting the maximum number of switches between options . The method is well-motived in general , however , I feel the notation in Alg 1 is a little bit unclear , what is \\pi ' and Q ' ? The ablation study is well-done , to separate the effects of different types , and gives practitioners some useful guidelines . There is one thing I am curious about , do you try the methods using some online data ? Since the paper argues the improvement compared with online option learning , it would be great to also have some experiments using online data for a fair comparison . I am not that familiar with hierarchical RL , so I could not give a fair judgement of the novelty compared with previous option learning literature . In terms of quality , it is a well-motivated work , clearly written in most part of the paper and gives a method with reasonably good empirical performance . I feel the off-policy argument in this paper is less clear , is it just achieved by using a Q-learning based method ? This can also be used online , and how is the online-version compared with the actor-critic option learning method ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their detailed and constructive feedback . In this post , we will focus on reviewer specific feedback . We will additionally provide an overview post describing the general feedback and corresponding changes . - Online/offline versus on-policy/off-policy learning It is unclear if we correctly understand the comment regarding the use of online data as off-policy methods in general still use online data and will try to clarify in the following paragraphs . In general , this paper only focuses on the online reinforcement learning setting , and we do not address the offline RL problem ( where the policy must be learned entirely from existing data , without interaction ) . However , HO2 is an off-policy reinforcement learning algorithm , meaning that it can learn from interactions that do not ( only ) come from the current policy itself . The experiments show that ( a ) common existing approaches to hierarchical RL ( which are on-policy ) can be outperformed by strong non-hierarchical off-policy methods like MPO , thus showing the criticality of learning off-policy ; and ( b ) our proposed off-policy hierarchical approach outperforms other methods with the same underlying optimization algorithm , such as RHPO and MPO , showing the benefit of a hierarchical off-policy method . We have added more discussion to clarify some of these nuances . - Unclear notation in Algorithm 1 and what are \\pi ' and Q ' We addressed the following points directly in the paper . \\pi ' and Q ' respectively represent the target policy and target Q-function as previously indicated via the corresponding comment in the algorithm . We have clarified this aspect and provided additional information for other algorithm parameters . If not addressed explicitly in the author feedback , we will address the remaining minor comments directly in the paper . Please do not hesitate to emphasise remaining questions or concerns ."}], "0": {"review_id": "QKbS9KXkE_y-0", "review_text": "# # Summary This paper introduces a novel option-learning policy gradient method , HO2 . The method learns a parameterized joint distribution over options and actions and uses a soft-continuation based approach to interrupt or `` switch '' between options before option termination . The method introduces a new meta-parameter which enforces a hard limit on the number of `` switches '' that can occur , significantly reducing the variance of the option-learning method and replacing softer loss penalization based approaches . The paper demonstrates the performance of the proposed algorithm on a handful of 3D virtualized environments as well as on robotic simulation tasks . # # Review # # # Summary I am currently leaning towards recommending reject for this paper . While the approach and algorithm are novel , they also appear to be highly complex , do n't provide a noticeable or consistent improvement over much simpler benchmarks , and I fear that the improvements that _are_ seen are likely due to variance in the results or are hidden behind the additional machinery in the algorithm . I remain slightly skeptical of the utility of the proposed meta-parameter $ n $ for setting a hard limit on the number of `` switches '' between active options , with my skepticism primarily due to concerns on the difficulty of tuning this parameter and the domain-specificity of the parameter . # # # Details I 'm curious about some of the hidden complexities in the algorithm . In the problem setup , a policy is defined on an MDP as possibly being $ \\pi ( a | h ) $ where $ h=\\ { s_t , a_ { t-1 } , s_ { t-1 } , a_ { t-2 } , \\ldots , s_0\\ } $ , that is a full history of interactions . First , I suppose this means we are no longer dealing with the original MDP and are working in a modified MDP where the `` Markov '' state is a full history ; already leading towards an exponential growth of the state-space . The algorithm itself depends on a recursive product of distributions for the entire length of a sampled trajectory as a result of this adapted Markov state . I 'm initially worried at how difficult it is to keep this probability from decaying towards 0 rapidly . There appear to be multiple partial marginalizations ( e.g. $ \\sum_ { i = 0 } ^M p ( X | Y = y_i ) p ( Y=y_i ) $ ) which require rescaling the final product to stay within the standard simplex , perhaps this is used to prevent the product of distributions from decaying towards 0 ? One of the primary motivations of the hard switching limit is that an auxiliary penalization on the objective is hard to tune . However , it is n't clear to me that the hard limit parameter $ n $ would be any easier to tune . In fact , because the inclusion of $ n $ seems to require more partial marginalizations , it almost seems as if this would cause additional complexity in the optimization problem . Did you find this meta-parameter easy to tune ? What are the effects of choosing it to be $ n=5 $ for the experiments instead of ( say ) 10 ? It appears to play a very mild variance reduction role in the results ( though with only 5 seeds , we _really_ ca n't say much about variance since this is severely underestimating variance ) . If the switching limit prefers to stay small , would this suggest that the best form of the algorithm is one without the options framework at all ( e.g.the best version of the proposed algorithm is an unaltered actor-critic algorithm ) ? The experiments in the paper start with deep neural networks with all of the necessary machinery to make Deep RL run at the moment , including experience replay , target networks , ADAM optimizer , layer norms , mini-batches , various types of activations on each layer , neural networks of different architectures for each of the three sets of weights , different stepsizes for each of the networks , etc . I fail to see why this approach could n't have been studied in a much simpler linear function approximation setting where statistically significant results with fewer confounding variables could have been achieved . As it stands , it is entirely unclear to me if the proposed algorithm actually provides any benefit when , in the midst of all of the machinery , the modifications above the benchmark algorithms are modest . Given this , there certainly is something to be said for a novel algorithm that does perform favorably when included in the machinery of a Deep RL feat of engineering . I , however , am unsure if the proposed algorithm does perform favorably . From the results , it appears that in most cases RHPO performs equivalently to the proposed ; certainly not statistically significantly different . With only 5 random seeds , it would be very hard to make sound claims ; especially considering the known variance issues in Deep RL ( take Henderson et al.2018 for a deeper discussion ) . The one place where the proposed algorithm _does_ outperform RHPO is in the robot simulator ( though again with only 5 seeds , heavy skepticism is called for ) . I found this fascinating and am curious if there is some structure that the proposed algorithm is able to take advantage of in this domain that RHPO is unable to replicate . # After discussion period : I have read all other reviews , resulting conversation , and have read the edits to the paper . After extended conversation with the authors and a deeper investigation into the empirical components of the paper , I find I have further concerns than originally realized in my original review and that many of my original concerns remain . I am lowering my recommendation from a 5 - > 3 to reflect the new concerns ; namely the validity of the ablation study as detailed in-depth below .", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for their detailed and constructive feedback . In this post , we will focus on reviewer specific feedback . We will additionally provide an overview post describing the general feedback and corresponding changes . - Working with an extended MDP Yes , it is correct that the options framework in general ( not just our method ) extends the MDP . Our paper uses the usual semi-MDP model [ 3 ] which is common across option frameworks . Although our notation in Equation 3 remains general in that the policy at time t is dependent on the entire history , this dependence is later refined to the kind of dependence that is described in the semi-MDP framework ( i.e.the dependence is mediated by the option which is described in Equation 4 ) . We do not discuss semi-MDP framework in detail in the paper , but there are relevant discussions to be found for example in [ 1 , 2 , 3 ] . In addition , we have clarified this aspect in the paper . - Details for the switch constrained extension The problem of controlling when to switch between options is a known challenge ( e.g . [ 1 ] ) .For most domains , it is usually not known a priori what would be appropriate switching behaviour . The possibility to explicitly optimise for temporal consistency via the switch constraints , which can further improve performance , is an additional feature of HO2 ( as shown in Figure 6 ) . Our presented approach aims to simplify tuning this aspect . It is easier to tune than another weighted cost term ( as e.g.given in [ 1 ] ) since it can be chosen independently of the reward scale and does not directly cause a potential conflict with other reward terms . In addition , setting how many times the agent should maximally switch between options along a trajectory can be more intuitive than an additional cost term , which is missing a similarly easy semantic interpretation . We will emphasise this further in the paper . If the switching limit prefers to stay small , would this suggest that the best form of the algorithm is one without the options framework at all ( e.g.the best version of the proposed algorithm is an unaltered actor-critic algorithm ) ? The algorithm often converges to a low rate of switching but never ( in our experiments ) degrades to the single option case ( which would be equivalent to a non-hierarchical policy ) . This shows that even when the agent would be able to change its behaviour to represent the \u2018 unaltered actor-critic algorithm \u2019 , there are benefits in using multiple options . Further , note that MPO trains a flat single actor-critic model , and yields poorer performance . We have clarified this in the experiments section . - Simpler setting e.g.linear function approximation setting Simplifying evaluation to obtain more general insights is an important point . However , designing toy domains that still include the relevant aspects which render the real-world problems hard is challenging on its own . We investigate option learning in particular in the context of deep models as these can represent solutions to more complex tasks which share more aspects with real-world control problems . While sharing some of the challenges with deep models , the optimisation of linear models is commonly affected by different aspects of the optimization problem . While clearly multiple factors affect performance ( as the reviewer suggests ) , we carefully ablate over these in the experiments to identify their effect . These experiments include : 1 ) Comparison to current on-policy option algorithms to estimate the importance of off-policy learning in HO2 . 2 ) Comparison to flat and mixture policies ( MPO and RHPO ) with equivalent underlying policy optimization to independently evaluate the benefits of temporal and action abstraction . 3 ) Ablation over the use of switching constraints , action-conditioning , off-policyness , and robustness via trust region constraints . 4 ) Analysis and interpretation of option decompositions for simpler tasks . - Performance differences on the simpler benchmarks and in general We have changed the paper to be more specific with respect to domains where HO2 provides the strongest performance gains . However , across all domains , the proposed method performs either better than existing methods or at least on par . - Variance of results and number of seeds We agree that variance and significance of results in reinforcement represent a critical point . Commonly , authors have to trade-off between increasing accuracy of estimates and acceptable computational cost . We run 5 seeds per algorithm per task ( a number common across RL papers and also used in [ 4 ] for most experiments ) . We see consistent results across many tasks with stronger benefits dominant in more complex tasks . [ Feedback continues in the next post ]"}, "1": {"review_id": "QKbS9KXkE_y-1", "review_text": "The paper introduces a reinforcement learning algorithm with temporal abstraction using the options framework . It provides empirical results in a variety of domains , demonstrating that the algorithm can improve data efficiency . The paper is well motivated . Data efficiency is an important concern in applications of reinforcement learning . The approach is sufficiently novel . The empirical results are positive , showing performance improvements in a variety of domains . Results in simulated robotic manipulation tasks are particularly positive , as measured by average return . The paper can be improved by providing additional analysis to better understand the behaviour of the algorithm and the conditions under which it improves performance . For instance , it would be useful to see what type of options are being learned in the various domains and with different limits on the number of switches allowed . I had difficulty in evaluating the importance of the performance differences in Figure 5 . In the main text of the paper , there is no information on the reward structure of the task . Additional information is present in the appendix but I could not easily locate the relevant information ( if it is indeed there ) . For instance , it would be useful to know what the behavioural difference is between average returns of 60 and 100 . In figure 3 , the number of switches are listed as 5 . Some experimentation with various different values would be informative . In figure 5 , seeing a longer training period would be informative . MPO and RHPO are still improving at the end of the learning curve . Please explain Equation 4 in some detail . I could not follow . In particular , I do not follow why the $ \\pi^ { L } $ term is there . The paper is not easy to read . Many sentences do not communicate the intended meaning clearly . As an example , the first paragraph of the introduction would be difficult to understand by readers who are not already familiar with hierarchical reinforcement learning , the options framework , and the papers cited . And some of the writing is not clear regardless of the background of the reader . For instance , the first paragraph ends with `` Overall , the interaction of algorithm and environment can become increasingly difficult , especially in an off-policy setting ( Precup et al. , 2006 ) . '' It is not clear what is meant by a `` difficult '' interaction here . The writing could be more nuanced in the discussion of results presented in Figure 3 . The authors write that \u201c off-policy learning alone [ .. ] improves data-efficiency and can suffice to outperform on-policy option algorithms such as DAC , IOPG and Option-Critic. \u201d This is not true in every domain . Similarly , the authors write that they \u201c achieve improvements when training mixture policies via RHPO \u201d . Again , this is not true in all four domains . For instance , in Hopper-v2 , RHPO lags behind MPO . In the pseudo code for Algorithm 1 on page 5 , please specify inputs to the algorithm . And please do not use any undefined symbols ( e.g. , $ \\pi \u2019 $ , $ Q \u2019 $ ) . There is no reference to Figure 6 in the text . In Figure 3 , please include DAC , OC , and IOPG in the figure legend . In Figure 3 , the way the performance of DAC , OC , and IOPG are presented on the plots is misleading . For each algorithm , a constant value is shown from step 0 until step $ 2 \\times 10^6 $ although these constants correspond only to average return obtained after $ 2 \\times 10^6 $ steps . Furthermore , my understanding is that these numbers have been taken from Zhang & Whiteson ( 2019 ) . For best experimental practice , these algorithms should be tested by the authors themselves along with the other algorithms shown in the plot ( e.g. , HO2 ) . This would ensure that the performances are truly comparable and that there are differences in relevant experimental settings . In addition , it would allow the reader to compare the algorithms along the entirety of their learning curves . Doina Precup 's dissertation is listed twice in the references , with different publications years . Misuse of the comma is prevalent throughout the paper . The author response answered some of my questions . But I can not say that I now better understand the behaviour of the algorithm and the conditions under which it improves performance . I agree with reviewer 2 that analysing behaviour and performance in a simpler setting would be informative . While the writing has improved , it stills lacks the clarity and nuance one would wish to see in a paper at this conference .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their detailed and constructive feedback . In this post , we will focus on reviewer specific feedback . We will additionally provide an overview post describing the general feedback and corresponding changes . - Additional analysis of the behaviour of the algorithm ; conditions under which it improves performance ; types of emerging options We agree that these aspects are important to understand . We have clarified and extended existing analysis sections . Section 4.2 focuses on differences to a simple Gaussian policy and a mixture of Gaussians policy ( which can be understood as a single-step options model ) . In these sections , we analyse the impact of action abstraction ( the ability to control by choosing from a set of individual skills/options ) and temporal abstraction ( the ability to model consistent behaviour enabled via the termination conditions ) . Furthermore , Section 4.3 includes the investigation of the impact of off-policy training , trust-region constraints and different aspects affecting the types of option decompositions . For improved understanding , we will include further details in the main paper regarding the properties of learned options based on conditions such as switch constraints and information asymmetry ( information only provided as input to a part of the agent ) between high- and low-level controller . - Clarity of writing Thanks for pointing this out . We have gone through the paper carefully and changed any sentences that may have been unclear . In particular , we adapted the introduction to render the paper more accessible and improve overall readability . Please feel free to use the openreview diff function to trace these changes . - Gym experiments : On-policy and off-policy option learning ; comparing mixture and option policies In general , off-policy algorithms have a considerable advantage over on-policy methods as they enable multiple updates for the same data samples . This advantage grows when training with stochastic gradient descent and only small adjustments per update . With respect to our experiments , we have explicitly clarified on which domains off-policy learning ( including non-hierarchical policies ) outperformed existing work on on-policy option learning . Additionally , we have emphasised the equivalent details in the RHPO and HO2 comparison . - Gym experiments : results from DAC , OC , IOPG We agree that this is an important figure and there is no perfect way of doing this comparison . We 've attempted to do it in a fair manner and will clarify the motivation behind this type of comparison below . As described in the caption , we use previously obtained results in the gym domains from prior work and have additionally contacted the authors to ensure that the environments and experiment setting are exactly the same . We chose to follow this direction to ensure that we take the best known results in this benchmark ( a common practice in many fields e.g.computer vision , NLP and partially used in RL as well ) , instead of using a sub-optimal reimplementation of the algorithms which would potentially underperform existing results . Using straight lines to indicate final results after the maximum training time of 2x10^6 steps instead of complete learning curves has two reasons . First , to prevent additional clutter in the graphs . Second , the learning curve comparison between on-policy and off-policy learning is only meaningful within limits . While we can align the number of actor steps , we can not do so for learner steps as the ratio can be independently chosen in off-policy learning . We will further emphasise the description of the lines in the caption ( which we use instead of an additional legend ) to ensure the reader is aware of this setting . - Details for the switch constrained extension The problem of controlling when to switch between options is a known challenge ( e.g . [ 1 ] ) .For most domains , it is usually not known a priori what would be appropriate switching behaviour . The possibility to explicitly optimise for temporal consistency via the switch constraints , which can further improve performance , is an additional feature of HO2 ( as shown in Figure 6 ) . Our presented approach aims to simplify tuning this property . It is easier to tune than another weighted cost term ( as e.g.given in [ 1 ] ) since it can be chosen independently of the reward scale and does not directly cause a potential conflict with other reward terms . In addition , setting how many times the agent should maximally switch between options along a trajectory can be more intuitive than an additional cost term , which is missing a similar semantic interpretation . Empirically , we have found performance in the learning from scratch results particularly robust with respect to different values of the constraint . We have emphasised this further in the paper . [ Feedback continues in the next post ]"}, "2": {"review_id": "QKbS9KXkE_y-2", "review_text": "The paper considers the Hierarchical Reinforcement Learning setting , Options in particular , and proposes an algorithm that allows to learn both the high-level and low-level ( option ) policies at once , from off-policy samples . An original aspect of the algorithm is that it is easy to constrain the learned policies on how often they terminate an option and start a new one . This prevents the agent from learning tiny options that immediately terminate . It is unclear whether it can also be used to prevent the agent from learning a single big option that does everything . The paper is very interesting and has a high educational value . It combines many different approaches and is mathematically sound . The empirical evaluation shows encouraging results . However , the significance of the empirical results is difficult to measure , due to a few cons of this paper : - The paper is quite difficult to follow , and requires several attentive reads to be understood ( by someone having a deep knowledge about options , intra-option learning and the Option-Critic architecture ) . I believe that the lack of clarity comes from the brevity of the paper , that has to fit in the page limit . I would suggest the authors to remove Figures 1 and 2 , that take place while still being very difficult to understand . The text helps understand the figures , the figures do not help understand the text , so I would remove the figures . `` Problem setup '' in the preliminaries could also be removed , and replaced with an introductory paragraph in `` Method '' , that clearly states what the contribution will be , and what are its main components/properties . - `` multi-task learning '' , just before `` Experiments '' , is then used in the experiments to increase the sample-efficiency of all the algorithms . Because it is used for all the algorithms and not ablated , I do n't see how it contributes to the paper . I think that the paper already proposes many ideas , and that multi-task learning could be omitted and left for a future paper . - In the experiments , comparing MPO with HO2 allows to see a benefit from the use of options , with the proposed learning algorithm . This is a good point . However , MPO is not a well-known baseline , and a quick glance at the plots does not allow the reader to see that MPO does not use options . I would suggest either to add a little `` no options '' next to MPO in the figures , or to replace it ( or add to it ) PPO , ACKTR , ACER or the Soft Actor-Critic ( SAC ) . These algorithms are well-known baselines . Not all of them are off-policy , but I believe that comparing HO2 to state-of-the-art algorithms , without restriction to off-policy ones , would better allow to illustrate all the gains to be obtained by HO2 . - A good argument for the use of options is to use them for explainability : telling an observer what is the current option , and what it tries to achieve . Is it possible to have a small discussion of what do the options learned by HO2 do ? Do they aim at goals , or do they perform small bits of trajectories ? In summary , I like the proposed algorithm , the core contribution of the paper , the contents of Section 3 . However , the clarity of this paper is to me too low , and prevents fully grasping the impact of the proposed research . I therefore recommend against acceptance , and invite the authors to remove parts on multi-task , figures that do not help , and use well-known baselines in their evaluation . With the space gained with these changes , more text can be spent summarizing what the contribution will be , and motivating the use of options . Author response : the authors answered my question about the absence of learning curves , and provided extra details . However , I still think that the paper could be clearer and more focused , a sentiment that I think I share with the other reviewers . Given my hesitation , I would therefore not vote for accepting this paper , but I acknowledge that the proposed method is original and interesting , so I would not mind if this paper were to be accepted .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their detailed and constructive feedback . In this post , we will focus on reviewer specific feedback . We will additionally provide an overview post describing the general feedback and corresponding changes . - Motivating the use of options We thank the reviewer for raising this point . With the additional page available to address such concerns , we have extended the introduction to more clearly motivate options and discuss the merits of such an approach before delving into the existing options literature . - Lack of clarity ( figures , contributions , and multi-task learning ) Figures 1 and 2 are used to support understanding of the option policies , mixture policies and temporal consistency parts in Section 3 by providing the corresponding graphical models . We believe that this will help understanding for readers who are more familiar with graphical models , sequence modelling , and related fields , while those more familiar with the option literature may benefit more from the derivations in Section 3 . We have added additional references in the text when introducing the corresponding equations , improved the captions , and added some brief intuition to the start of each subsection . Our contributions are explicitly stated in bullet form at the end of Section 1 . We additionally separate the method section with respect to flat Gaussians and mixture policies which have been trained via critic-weighted maximum likelihood in prior work and the proposed extension to train option policies . The current writing aims to find a compromise between clear separation of methods and clarity and self-containment of the method description . We have furthermore emphasized the use of MPO and RHPO to train Gaussian and mixture policies in prior work . Finally , since options represent reusable behaviour abstractions , they are known to be beneficial in a multi-task setting where they can be shared across tasks ( see e.g . [ 1 ] ) .We additionally use multitask learning with related tasks to enable us to address more complex domains with acceptable data requirements . The described methods for multi-task learning have been proposed and tested in prior work and we merely use them here to accelerate learning . We have integrated the corresponding description into the experimental section and adapted it for clarification . - Clarity regarding the use of MPO We use MPO ( using flat Gaussian policies ) and RHPO ( using mixture policies ) as the baseline methods for comparison with HO2 , because they use an equivalent underlying optimization procedure based on critic-reweighted likelihood estimation . The fact that the base algorithm is equivalent makes it easy to compare the three approaches and to isolate the effects of action abstraction ( which MPO does not have ) and temporal abstraction ( which both MPO and RHPO miss ) . We have extended the text to make this clear , and have also clarified that MPO trains a flat policy in the experiments , as suggested by the reviewer . - On learning a single degenerate option that solves everything The reviewer is right to point out that in certain cases , hierarchical approaches can discover degenerate solutions such as a single option . In the case of HO2 , the experiments found that with enough structure in the problem setup , a diverse set of options was learned and used - this is also supported and explained by the \u201c Further analysis \u201d experiments in Section 4.3 . [ Feedback continues in the next post ]"}, "3": {"review_id": "QKbS9KXkE_y-3", "review_text": "This paper studies an important area in RL , hierarchical RL , which improves data efficiency by incorporating abstractions . In this paper , the authors proposes an efficient option learning algorithm , which utilizes a TD ( 0 ) type objective and constrains the learned policy being not too far away from the past policy . In terms of different abstractions , the paper studies action abstraction through a mixture policy , and temporal abstraction through explicitly limiting the maximum number of switches between options . The method is well-motived in general , however , I feel the notation in Alg 1 is a little bit unclear , what is \\pi ' and Q ' ? The ablation study is well-done , to separate the effects of different types , and gives practitioners some useful guidelines . There is one thing I am curious about , do you try the methods using some online data ? Since the paper argues the improvement compared with online option learning , it would be great to also have some experiments using online data for a fair comparison . I am not that familiar with hierarchical RL , so I could not give a fair judgement of the novelty compared with previous option learning literature . In terms of quality , it is a well-motivated work , clearly written in most part of the paper and gives a method with reasonably good empirical performance . I feel the off-policy argument in this paper is less clear , is it just achieved by using a Q-learning based method ? This can also be used online , and how is the online-version compared with the actor-critic option learning method ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their detailed and constructive feedback . In this post , we will focus on reviewer specific feedback . We will additionally provide an overview post describing the general feedback and corresponding changes . - Online/offline versus on-policy/off-policy learning It is unclear if we correctly understand the comment regarding the use of online data as off-policy methods in general still use online data and will try to clarify in the following paragraphs . In general , this paper only focuses on the online reinforcement learning setting , and we do not address the offline RL problem ( where the policy must be learned entirely from existing data , without interaction ) . However , HO2 is an off-policy reinforcement learning algorithm , meaning that it can learn from interactions that do not ( only ) come from the current policy itself . The experiments show that ( a ) common existing approaches to hierarchical RL ( which are on-policy ) can be outperformed by strong non-hierarchical off-policy methods like MPO , thus showing the criticality of learning off-policy ; and ( b ) our proposed off-policy hierarchical approach outperforms other methods with the same underlying optimization algorithm , such as RHPO and MPO , showing the benefit of a hierarchical off-policy method . We have added more discussion to clarify some of these nuances . - Unclear notation in Algorithm 1 and what are \\pi ' and Q ' We addressed the following points directly in the paper . \\pi ' and Q ' respectively represent the target policy and target Q-function as previously indicated via the corresponding comment in the algorithm . We have clarified this aspect and provided additional information for other algorithm parameters . If not addressed explicitly in the author feedback , we will address the remaining minor comments directly in the paper . Please do not hesitate to emphasise remaining questions or concerns ."}}