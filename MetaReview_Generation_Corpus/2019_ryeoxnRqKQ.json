{"year": "2019", "forum": "ryeoxnRqKQ", "title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "decision": "Reject", "meta_review": "Although one review is favorable, it does not make a strong enough case for accepting this paper. Thus there is not sufficient support in the reviews to accept this paper.\n\nI am recommending rejecting this submission for multiple reasons.\n\nGiven that this is a \"black box\" attack formalized as an optimization problem, the method must be compared to other approaches in the large field of derivative-free optimization. There are many techniques including: Bayesian optimization, (other) evolutionary algorithms, simulated annealing, Nelder-Mead, coordinate descent, etc. Since the method of the paper does not use anything about the structure of the problem it can be applied to other derivative-free optimization problems that had the same search constraint. However, the paper does not provide evidence that it has advanced the state of the art in derivative-free optimization.\n\nThe method the paper describes does not need a new name and is an obvious variation of existing evolutionary algorithms. Someone facing the same problem could easily reinvent the exact method of the paper without reading it and this limits the value of the contribution.\n\nFinally, this paper amounts to breaking already broken defenses, which is not an activity of high value to the community at this stage and also limits the contribution of this work.\n", "reviews": [{"review_id": "ryeoxnRqKQ-0", "review_text": "In this paper, authors propose a \"universal\" Gaussian balck-box adversarial attack. Original and well-written (although there are a few grammar mistakes that would require some revision) and structured. Having followed the comments and discussion I am convinced that the proposed method is state of the art and interesting enough fro ICLR. To the best of my knowledge, the study is technically sound. It fairly accounts for recent literature in the field. Experiments are convincing. One thing I am not so convinced about is the naming of the evaluation curve as \"a new ROC curve\". I understand the appeal of pairing the proposed evaluation curve with the ROC curve but, beyond an arguable resemblance, they have no much in common, really.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the encouraging comments ! Regarding the name of the curve , we have removed \u201c ROC \u201d and now simply call it the curve of success rate vs. number of evolution iterations . We will continue to polish the text ."}, {"review_id": "ryeoxnRqKQ-1", "review_text": "Summary: In this paper the authors discuss a black-box method to learn adversarial inputs to DNNs which are \"close\" to some nominal example but nevertheless get misclassified. The algorithm essentially tries to learn the mean of a joint Gaussian distribution over image perturbations so that the perturbed image has high likelihood of being misclassified. The method takes the form of zero-th order gradient updates on an objective measuring to what degree the perturbed example is misclassified. The authors test their method against 10 recent DNN defense mechanisms, which showed higher attack-success rates than other methods. Additionally the authors looked at transferrability of the learned adversarial examples. Feedback: As noted before, this paper shares many similarities with [1] \"Black-box Adversarial Attacks with Limited Queries and Information\" (https://arxiv.org/abs/1804.08598) and the authors have responded to those similarities in two follow-ups. I have reviewed these results and their method does appear to improve over [1]. However, I am still reluctant to admit these additions to the original submission, mainly because dropping [1] in the original submission seems to be a fairly major omission of one of the most relevant competitors out there. In its current form, the apparent redundancies distract significantly from the paper, and to remedy this, the paper would have to change significantly in order to relate it properly to [1] clear is needed. I'd be curious on the ACs thoughts on this. I appreciate the authors' claim that their method can breach many of the popular defense methods out there, but we also see that many of the percentages in Figure 1 converge to 1. On the one hand this suggests that all defense methods are in some sense equally bad, but on the other, it could also just reflect on the fact that the thresholds are chosen \"too large\". I understand that many of the thresholds were inherited from previous work, but it would nevertheless help if the authors showed some example adversarial images to help baseline this Figure. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q : The paper would have to change significantly in order to relate it properly to ( Ilyas et al. , 2018 ) . A : Denote by QL ( Ilyas et al. , 2018 ) \u2019 s approach . We have added to the revised PDF + a new paragraph ( in the Introduction section ) to draw readers \u2019 attention to QL upfront , + new results of QL on attacking the 10 defense methods ( Table 1 ) , and + a new section ( Section 3.1.3 ) to carefully investigate the factors that contribute to the inferior performance of QL algorithm . The results reveal that , in order to improve its attack success rates , it is vital to get rid of PGD ( projection and the sign of the gradients ) , which is the foundation upon which QL is built , and meanwhile to couple the $ \\ell_infty $ clip with the tanh transformation . Given the above changes , it seems like feasible to extensively discuss QL and yet not completely re-write the paper . Additionally , we wanted to emphasize that the Gaussian mean is more important than the gradients in our approach . Whereas ES is a natural choice to search for the Gaussian mean , some derivative-free methods [ 2 ] are also good afternatives . In sharp contrast , QL is hinged on the white-box PGD attack -- - in terms of the methodology , it is actually closer to [ 1 ] than ours because both QL and [ 1 ] essentially approximate the gradients for PGD . As a result , the quality of the estimated gradients in QL is a big deal . Unfortunately , ES does not give rise to stable gradients due to the sampling step and PGD \u2019 s projection and sign functions . Indeed , after we remove the PGD step in QL , there is a significant performance boost ( cf.Table 2 ) . [ 1 ] Anish Athalye , Nicholas Carlini , and David Wagner . Obfuscated gradients give a false sense of security : Circumventing defenses to adversarial examples . arXiv preprint arXiv:1802.00420 , 2018 . [ 2 ] Luis Miguel Rios and Nikolaos V Sahinidis . Derivative-free optimization : a review of algorithms and comparison of software implementations . Journal of Global Optimization , 56 ( 3 ) :1247\u20131293,2013 . Q : Example adversarial examples to baseline the figure : A : Sorry for the confusion about Figure 1 . First of all , we did not include all the defense methods in Figure 1 due to the heavy run time on ImageNet . Besides , for each attack method , we had removed all the examples of which it failed to change the labels . Our intention was to compare the relative convergences when their last steps are aligned . Upon reading your comments , however , we think this alignment is actually unnecessary and should be removed . In the revised PDF submission , you can see that some of the attack methods fail to reach 100 % success rate . We will add some example adversarial examples in the appendix , but the adversarial examples in $ \\ell_\\infty = 0.031 $ are hardly differentiable from the benign ones ."}, {"review_id": "ryeoxnRqKQ-2", "review_text": "In this work the authors use a score-based adversarial attack (based on the natural evolution strategy (NES)) to successfully attack a multitude of defended networks, with success rates rivalling the best gradient-based attacks. As confirmed by the authors in a detailed and very open response to a question of mine, the attack introduced here is actually equivalent to [1]. While the attack itself is not novel (which will require a major revision of the manuscript), the authors point out the following contributions over [1]: * Attack experiments here go way beyond Ilyas et al. in terms of Lp metrics, different defense models, different datasets and transferability. * Different motivation/derivation of NES. * Concept of adversarial distributions. * Regression network for good initialization. * Introduction of accuracy-iterations plots. My main concerns are as follows: * The review of the prior literature, in particular on score-based and decision-based defences (the latter of which are not even mentioned), is very limited and is framed wrongly. In particular, the statement \u201cHowever, existing black-box attacks are weaker than their white-box counterparts\u201d is simply not true: as an example, the most prominent decision-based attack [2] rivals white-box attacks on vanilla DNNs as well as defended networks [3]. * The concept of adversarial distributions is not new but is common in the literature of real-world adversarials that are robust to transformations and perturbations (like gaussian noise), check for example [4]. In [4] the concept of _Expectation Over Transformation (EOT)_ is introduced, which is basically the generalised concept of the expectation over gaussian perturbations introduced in this work. * While I like the idea of accuracy-iterations plots, the idea is not new, see e.g. the accuracy-iterations plot in [2] (sample-based, Figure 6), the loss-iterations plot in [5] or the accuracy-distortion plots in [3]. However, I agree that these type of visualisation or metric is not as widespread as it should be. Hence, in summary the main contribution of the paper is the application of NES against different defence models, datasets and Lp metrics as well as the use of a regression network for initialisation. Along this second point it would be great if the authors would be able to demonstrate substantial gains in the accuracy-query metric. In any case, in the light of previous literature a major revision of the manuscript will be necessary. [1] Ilyas et al. (2018) \u201cBlack-box Adversarial Attacks with Limited Queries and Information\u201d (https://arxiv.org/abs/1804.08598) [2] Brendel et al. (2018) \u201cDecision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models\u201d (https://arxiv.org/abs/1712.04248) [3] Schott et al. (2018) \u201cTowards the first adversarially robust neural network model on MNIST\u201d (https://arxiv.org/abs/1805.09190) [4] Athalye et al. (2017) \u201cSynthesizing Robust Adversarial Examples\u201d (https://arxiv.org/pdf/1707.07397.pdf) [5] Madry et al. {2017) \u201cTowards Deep Learning Models Resistant to Adversarial Attacks\u201d (https://arxiv.org/pdf/1706.06083.pdf)", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q : the attack introduced here is actually equivalent to ( Ilyas et al. , 2018 ) . A : We believe the above is a mis-interpretation of our responses . Denote by QL ( Ilyas et al. , 2018 ) . QL is hinged on the white-box PGD attack -- - in terms of the methodology , it is actually closer to [ 1 ] than ours because both QL and [ 1 ] essentially approximate the gradients for PGD . As a result , the quality of the estimated gradients in QL is a big deal . Unfortunately , ES does not give rise to stable gradients due to the sampling step and PGD \u2019 s projection and sign functions . Indeed , after we remove the PGD step in QL , there is a significant performance boost ( cf.Table 2 in the revised PDF ) . On the contrary , we do not employ any white-box attack methods at all in developing our algorithm . The Gaussian mean is more important than the gradients in our approach . Whereas ES is a natural choice to search for the Gaussian mean , some derivative-free methods [ 2 ] are also good alternatives . Please see Section 3.1.3 for a more detailed investigation about QL . Q : `` existing black-box attacks are weaker than their white-box counterparts \u201d is simply not true . A : It is actually unclear how strong the existing black-box methods are on attacking the defended neural networks -- - most experiments reported in the original publications are conducted on vanilla neural networks . Our own experiments do show that ZOO [ 1 ] and the decision-based attack [ 2 ] fail to perform well on attacking all the 10 defense methods ( cf.Table 1 in the PDF ) -- - as the decision-based attack consumes a lot of run time , we have to include the complete results later . We have toned down the description about prior black-box attack in the introduction ( cf.the text highlighted in the blue color ) . [ 1 ] Chen , Pin-Yu , et al . `` Zoo : Zeroth order optimization based black-box attacks to deep neural networks without training substitute models . '' Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security . ACM , 2017 . [ 2 ] Wieland Brendel , Jonas Rauber , and Matthias Bethge . Decision-based adversarial attacks : Reliable attacks against black-box machine learning models . arXiv preprint arXiv:1712.04248 , 2017 . Q : The concept of adversarial distributions is not new A : We have to point out the distribution over adversarial examples per image is different from the distribution over the transformations for a physical adversarial in the real world . In order to photograph a real-world adversarial , it is natural to consider all the conditions ( location , background , lighting , etc . ) as a distribution of transformations . In contrast , it is not so obvious to model by a distribution the adversarial examples for every single image . To the best of our knowledge , this work is the first to capture the whole population of adversarial examples per image ."}], "0": {"review_id": "ryeoxnRqKQ-0", "review_text": "In this paper, authors propose a \"universal\" Gaussian balck-box adversarial attack. Original and well-written (although there are a few grammar mistakes that would require some revision) and structured. Having followed the comments and discussion I am convinced that the proposed method is state of the art and interesting enough fro ICLR. To the best of my knowledge, the study is technically sound. It fairly accounts for recent literature in the field. Experiments are convincing. One thing I am not so convinced about is the naming of the evaluation curve as \"a new ROC curve\". I understand the appeal of pairing the proposed evaluation curve with the ROC curve but, beyond an arguable resemblance, they have no much in common, really.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the encouraging comments ! Regarding the name of the curve , we have removed \u201c ROC \u201d and now simply call it the curve of success rate vs. number of evolution iterations . We will continue to polish the text ."}, "1": {"review_id": "ryeoxnRqKQ-1", "review_text": "Summary: In this paper the authors discuss a black-box method to learn adversarial inputs to DNNs which are \"close\" to some nominal example but nevertheless get misclassified. The algorithm essentially tries to learn the mean of a joint Gaussian distribution over image perturbations so that the perturbed image has high likelihood of being misclassified. The method takes the form of zero-th order gradient updates on an objective measuring to what degree the perturbed example is misclassified. The authors test their method against 10 recent DNN defense mechanisms, which showed higher attack-success rates than other methods. Additionally the authors looked at transferrability of the learned adversarial examples. Feedback: As noted before, this paper shares many similarities with [1] \"Black-box Adversarial Attacks with Limited Queries and Information\" (https://arxiv.org/abs/1804.08598) and the authors have responded to those similarities in two follow-ups. I have reviewed these results and their method does appear to improve over [1]. However, I am still reluctant to admit these additions to the original submission, mainly because dropping [1] in the original submission seems to be a fairly major omission of one of the most relevant competitors out there. In its current form, the apparent redundancies distract significantly from the paper, and to remedy this, the paper would have to change significantly in order to relate it properly to [1] clear is needed. I'd be curious on the ACs thoughts on this. I appreciate the authors' claim that their method can breach many of the popular defense methods out there, but we also see that many of the percentages in Figure 1 converge to 1. On the one hand this suggests that all defense methods are in some sense equally bad, but on the other, it could also just reflect on the fact that the thresholds are chosen \"too large\". I understand that many of the thresholds were inherited from previous work, but it would nevertheless help if the authors showed some example adversarial images to help baseline this Figure. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q : The paper would have to change significantly in order to relate it properly to ( Ilyas et al. , 2018 ) . A : Denote by QL ( Ilyas et al. , 2018 ) \u2019 s approach . We have added to the revised PDF + a new paragraph ( in the Introduction section ) to draw readers \u2019 attention to QL upfront , + new results of QL on attacking the 10 defense methods ( Table 1 ) , and + a new section ( Section 3.1.3 ) to carefully investigate the factors that contribute to the inferior performance of QL algorithm . The results reveal that , in order to improve its attack success rates , it is vital to get rid of PGD ( projection and the sign of the gradients ) , which is the foundation upon which QL is built , and meanwhile to couple the $ \\ell_infty $ clip with the tanh transformation . Given the above changes , it seems like feasible to extensively discuss QL and yet not completely re-write the paper . Additionally , we wanted to emphasize that the Gaussian mean is more important than the gradients in our approach . Whereas ES is a natural choice to search for the Gaussian mean , some derivative-free methods [ 2 ] are also good afternatives . In sharp contrast , QL is hinged on the white-box PGD attack -- - in terms of the methodology , it is actually closer to [ 1 ] than ours because both QL and [ 1 ] essentially approximate the gradients for PGD . As a result , the quality of the estimated gradients in QL is a big deal . Unfortunately , ES does not give rise to stable gradients due to the sampling step and PGD \u2019 s projection and sign functions . Indeed , after we remove the PGD step in QL , there is a significant performance boost ( cf.Table 2 ) . [ 1 ] Anish Athalye , Nicholas Carlini , and David Wagner . Obfuscated gradients give a false sense of security : Circumventing defenses to adversarial examples . arXiv preprint arXiv:1802.00420 , 2018 . [ 2 ] Luis Miguel Rios and Nikolaos V Sahinidis . Derivative-free optimization : a review of algorithms and comparison of software implementations . Journal of Global Optimization , 56 ( 3 ) :1247\u20131293,2013 . Q : Example adversarial examples to baseline the figure : A : Sorry for the confusion about Figure 1 . First of all , we did not include all the defense methods in Figure 1 due to the heavy run time on ImageNet . Besides , for each attack method , we had removed all the examples of which it failed to change the labels . Our intention was to compare the relative convergences when their last steps are aligned . Upon reading your comments , however , we think this alignment is actually unnecessary and should be removed . In the revised PDF submission , you can see that some of the attack methods fail to reach 100 % success rate . We will add some example adversarial examples in the appendix , but the adversarial examples in $ \\ell_\\infty = 0.031 $ are hardly differentiable from the benign ones ."}, "2": {"review_id": "ryeoxnRqKQ-2", "review_text": "In this work the authors use a score-based adversarial attack (based on the natural evolution strategy (NES)) to successfully attack a multitude of defended networks, with success rates rivalling the best gradient-based attacks. As confirmed by the authors in a detailed and very open response to a question of mine, the attack introduced here is actually equivalent to [1]. While the attack itself is not novel (which will require a major revision of the manuscript), the authors point out the following contributions over [1]: * Attack experiments here go way beyond Ilyas et al. in terms of Lp metrics, different defense models, different datasets and transferability. * Different motivation/derivation of NES. * Concept of adversarial distributions. * Regression network for good initialization. * Introduction of accuracy-iterations plots. My main concerns are as follows: * The review of the prior literature, in particular on score-based and decision-based defences (the latter of which are not even mentioned), is very limited and is framed wrongly. In particular, the statement \u201cHowever, existing black-box attacks are weaker than their white-box counterparts\u201d is simply not true: as an example, the most prominent decision-based attack [2] rivals white-box attacks on vanilla DNNs as well as defended networks [3]. * The concept of adversarial distributions is not new but is common in the literature of real-world adversarials that are robust to transformations and perturbations (like gaussian noise), check for example [4]. In [4] the concept of _Expectation Over Transformation (EOT)_ is introduced, which is basically the generalised concept of the expectation over gaussian perturbations introduced in this work. * While I like the idea of accuracy-iterations plots, the idea is not new, see e.g. the accuracy-iterations plot in [2] (sample-based, Figure 6), the loss-iterations plot in [5] or the accuracy-distortion plots in [3]. However, I agree that these type of visualisation or metric is not as widespread as it should be. Hence, in summary the main contribution of the paper is the application of NES against different defence models, datasets and Lp metrics as well as the use of a regression network for initialisation. Along this second point it would be great if the authors would be able to demonstrate substantial gains in the accuracy-query metric. In any case, in the light of previous literature a major revision of the manuscript will be necessary. [1] Ilyas et al. (2018) \u201cBlack-box Adversarial Attacks with Limited Queries and Information\u201d (https://arxiv.org/abs/1804.08598) [2] Brendel et al. (2018) \u201cDecision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models\u201d (https://arxiv.org/abs/1712.04248) [3] Schott et al. (2018) \u201cTowards the first adversarially robust neural network model on MNIST\u201d (https://arxiv.org/abs/1805.09190) [4] Athalye et al. (2017) \u201cSynthesizing Robust Adversarial Examples\u201d (https://arxiv.org/pdf/1707.07397.pdf) [5] Madry et al. {2017) \u201cTowards Deep Learning Models Resistant to Adversarial Attacks\u201d (https://arxiv.org/pdf/1706.06083.pdf)", "rating": "4: Ok but not good enough - rejection", "reply_text": "Q : the attack introduced here is actually equivalent to ( Ilyas et al. , 2018 ) . A : We believe the above is a mis-interpretation of our responses . Denote by QL ( Ilyas et al. , 2018 ) . QL is hinged on the white-box PGD attack -- - in terms of the methodology , it is actually closer to [ 1 ] than ours because both QL and [ 1 ] essentially approximate the gradients for PGD . As a result , the quality of the estimated gradients in QL is a big deal . Unfortunately , ES does not give rise to stable gradients due to the sampling step and PGD \u2019 s projection and sign functions . Indeed , after we remove the PGD step in QL , there is a significant performance boost ( cf.Table 2 in the revised PDF ) . On the contrary , we do not employ any white-box attack methods at all in developing our algorithm . The Gaussian mean is more important than the gradients in our approach . Whereas ES is a natural choice to search for the Gaussian mean , some derivative-free methods [ 2 ] are also good alternatives . Please see Section 3.1.3 for a more detailed investigation about QL . Q : `` existing black-box attacks are weaker than their white-box counterparts \u201d is simply not true . A : It is actually unclear how strong the existing black-box methods are on attacking the defended neural networks -- - most experiments reported in the original publications are conducted on vanilla neural networks . Our own experiments do show that ZOO [ 1 ] and the decision-based attack [ 2 ] fail to perform well on attacking all the 10 defense methods ( cf.Table 1 in the PDF ) -- - as the decision-based attack consumes a lot of run time , we have to include the complete results later . We have toned down the description about prior black-box attack in the introduction ( cf.the text highlighted in the blue color ) . [ 1 ] Chen , Pin-Yu , et al . `` Zoo : Zeroth order optimization based black-box attacks to deep neural networks without training substitute models . '' Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security . ACM , 2017 . [ 2 ] Wieland Brendel , Jonas Rauber , and Matthias Bethge . Decision-based adversarial attacks : Reliable attacks against black-box machine learning models . arXiv preprint arXiv:1712.04248 , 2017 . Q : The concept of adversarial distributions is not new A : We have to point out the distribution over adversarial examples per image is different from the distribution over the transformations for a physical adversarial in the real world . In order to photograph a real-world adversarial , it is natural to consider all the conditions ( location , background , lighting , etc . ) as a distribution of transformations . In contrast , it is not so obvious to model by a distribution the adversarial examples for every single image . To the best of our knowledge , this work is the first to capture the whole population of adversarial examples per image ."}}