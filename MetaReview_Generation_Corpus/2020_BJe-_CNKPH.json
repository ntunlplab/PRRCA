{"year": "2020", "forum": "BJe-_CNKPH", "title": "Attention Interpretability Across NLP Tasks", "decision": "Reject", "meta_review": "This paper investigates the degree to which we might view attention weights as explanatory across NLP tasks and architectures. Notably, the authors distinguish between single and \"pair\" sequence tasks, the latter including NLI, and generation tasks (e.g., translation). The argument here is that attention weights do not provide explanatory power for single sequence tasks like classification, but do for NLI and generation. Another notable distinction from most (although not all; see the references below) prior work on the explainability of attention mechanisms in NLP is the inclusion of transformer/self-attentive architectures. \n\nUnfortunately, the paper needs work in presentation (in particular, in Section 3) before it is ready to be published.", "reviews": [{"review_id": "BJe-_CNKPH-0", "review_text": "Motivated by an existing paper, the paper analyzes the interpretability of attention mechanism over three NLP tasks. The previous paper claims that attention mechanism is not interpratable. The paper makes incremental contribution by showing that attentions are not interpratable when they mimic gating units (single sequence task) but are interpretable for two sequence and generation tasks. Experimental results are given to support the claims, which can also help readers to gain insights into the attention mechanism. The paper is well written and all claims are supported. I also have some questions below for clarification. If I get reasonable answers to these questions, I tend to accept the paper. 1. Jain & Wallace (2019) show that for the SNLI problem, attentions weakly correlate with the output based on Kendall's correlation and JSD which contradicts to your observation. Could you explain how this happens? are there any model settings different? 2. in figure 5, for the single sequence (original), most of attentions leading to correct predictions are labeled meaningful. Does this mean that even the attention doesn't necessarily contribute too much to the prediction correctness but they are also interpretable if they are allowed to be trained. Also, in Table 1, with modified attentions, all scores go down a little. This means that attention still can contribute to the final prediction but not significant enough (some like yelp are significant). I am gussing that the sequence encoding already present useful features for the final prediction. Did you check the distance of different word encodings? Are these encodings all very similar?", "rating": "6: Weak Accept", "reply_text": "Thank you for the constructive comments . We are glad that the reviewer liked our work . Below we provide clarification for the reviewer \u2019 s queries . 1.In comparison to Jain & Wallace ( 2019 ) , we use the exact same model architecture , i.e. , Bi-LSTM over word embedding of tokens followed by attention . Although for deciding hyperparameters we use validation data , unlike Jain & Wallace ( 2019 ) who use test data for that . In our experiments , we did not analyze how attention correlates with output based on Kendall and JSD but permutation scatterplots presented in Figure 2 are very similar to what has been reported by Jain & Wallace ( 2019 ) although they do not highlight them much in their paper . The complete set of results of Jain & Wallace ( 2019 ) are provided at this link : https : //successar.github.io/AttentionExplanation/docs/ . Here , we find that Permutation Scatterplots on Babi and SNLI datasets are very similar to what we have reported . 2.For manual evaluation , an annotator was asked to give a binary yes/no based on whether attention weights are meaningful or not . For that , an annotator is shown top three tokens with highest attention weights and if at least of them supports the prediction then \u201c yes \u201d was assigned otherwise \u201c no \u201d . For the case of single sequence tasks with original attention weights , we find that the gating mechanism during training learns to give high scores to tokens which refect some sentiment but does not reflect the reasoning behind the model \u2019 s prediction . An example of the same is also presented in Section 4.4 . Since a given review has few tokens which directly highlight the sentiment so , with our evaluation setting at least one such token is highly likely to appear in the top three . But we can see that the weights do not influence the model output as changing them does not affect the prediction considerably as shown in Figure 5 ( a ) Permuted and by the series of papers [ 1 , 2 ] prior to our work . Yes , attention even in the case of single sequence tasks is helpful as it is adding more parameters to the model and thus allowing it to learn more complex relationships between the input and the output . The size of datasets like YELP and IMDB is considerably large compared to other datasets , therefore providing more parameters allows the model to learn more from training data and perform better . This is reflected through significant changes in the performance on them . Although we have not explicitly performed such analysis but based on the variation in attention weights over tokens ( see x-axis Figure 3 ( a ) ) we can say that the words encoding are not very similar . The attention weights in single sequence tasks for each token are computed using the same W and c parameters . Now , since the difference between a randomly selected attention weight and the highest attention weight is going up to 0.6 on IMDB , where average text length is around 282 words , based on this we believe that the word encoding should not be very similar . [ 1 ] Jain , Sarthak and Byron C. Wallace . \u201c Attention is not Explanation. \u201d NAACL-HLT ( 2019 ) . [ 2 ] Serrano , Sof\u00eda and Noah A. Smith . \u201c Is Attention Interpretable ? \u201d ACL ( 2019 ) ."}, {"review_id": "BJe-_CNKPH-1", "review_text": "This paper investigates the degree to which we might view attention weights as explanatory across NLP tasks and architectures. Notably, the authors distinguish between single and \"pair\" sequence tasks, the latter including NLI, and generation tasks (e.g., translation). The argument here is that attention weights do not provide explanatory power for single sequence tasks like classification, but do for NLI and generation. Another notable distinction from most (although not all; see the references below) prior work on the explainability of attention mechanisms in NLP is the inclusion of transformer/self-attentive architectures. Overall, this is a nice contribution that unifies some parallel lines of inquiry concerning explainability, and attention mechanism for different NLP tasks. The contrast in findings between single sequence (classification) and other NLP tasks and the implications for explainability is interesting, and provides a piece missing from the analyses conducted so far. The main issue I have with the paper is that I'm not sure I follow the rationale in Section 4 arguing that because attention \"works as a gating unit\", it follows that we cannot view single sequence tasks \"as attention\". Can the authors elaborate on why the conclusion follows from the premise? In other words, why is attention inherently *not gating*? This seems like an interesting connection, but again I do not see why attention acting as a gate implies that we should not view it as attention at all. Perhaps the authors could elaborate on their reasoning. Some additional references the authors might consider. Note that the latter two papers, I think, would seem to broadly disagree regarding self-attention and tasks aside from single sequence (classification): - \"Fine-grained Sentiment Analysis with Faithful Attention\": https://arxiv.org/abs/1908.06870 - \"Interrogating the Explanatory Power of Attention in Neural Machine Translation\": https://arxiv.org/abs/1910.00139 - \"On Identifiability in Transformers\": https://arxiv.org/abs/1908.04211", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for constructive feedback . In Section 4 , unlike the prior works [ 1,2 ] on attention interpretability who have simply reported the observation that attention weights are not interpretable , we are providing an explanation behind that observation . By showing attention in single sequence tasks as a gating unit we want to distinguish the attention mechanism used for text classification from attention used in tasks like NLI and NMT for which it was originally proposed [ 3,4 ] . The main distinguishing factor is that in the former the attention weight for a given token is computed only based on its own representation whereas in the latter case the attention weights are based on another part of the input . We want to highlight that this simple difference is the reason behind the different nature of attention weights in both cases . In single sequence tasks , for each token attention mechanism is learning scalar to weight its representation whereas for pair sequence and generation tasks the weights are being learned with respect to some other part of the input for deciding the importance of the token . Therefore , in the latter case , the weights actually represent the reasoning behind the model \u2019 s prediction and are more critical for its performance . We will be happy to elaborate further on this in case it is not clear . Thank you for pointing out additional references . We will surely include a discussion on them in the updated version of our paper . `` Interrogating the Explanatory Power of Attention in Neural Machine Translation '' investigates several ways of perturbing attention weights in the NMT model of which Permute and Uniform are common with us . Please note that the authors have reported that with Permute and Uniform only 6 % and 11 % words are retained from the original output of the model which is quite low thus our results are consistent with what they have reported . In `` On Identifiability in Transformers '' , authors have demonstrated that multiple attention distribution exists for the same output . However , in our work , we are analyzing that how randomly permuting attention weights in self-attention based models affect the overall prediction of the model . [ 1 ] Jain , Sarthak and Byron C. Wallace . \u201c Attention is not Explanation. \u201d NAACL-HLT ( 2019 ) . [ 2 ] Serrano , Sof\u00eda and Noah A. Smith . \u201c Is Attention Interpretable ? \u201d ACL ( 2019 ) . [ 3 ] Rockt\u00e4schel , Tim , Edward Grefenstette , Karl Moritz Hermann , Tom\u00e1s Kocisk\u00fd and Phil Blunsom . \u201c Reasoning about Entailment with Neural Attention. \u201d CoRR abs/1509.06664 ( 2015 ) : n. Pag . [ 4 ] Bahdanau , Dzmitry , Kyunghyun Cho and Yoshua Bengio . \u201c Neural Machine Translation by Jointly Learning to Align and Translate. \u201d CoRR abs/1409.0473 ( 2014 ) : n. pag ."}, {"review_id": "BJe-_CNKPH-2", "review_text": "CONTRIBUTIONS: I use (unqualified) \u201cself-attention\u201d to refer to attention of tokens in a sequence to other tokens in the same sequence, as described by [some corrected version of] Eq (1) and the paragraph following it (citing Bahdanau et al. 2015). This contrasts with \u201cTransformer self-attention\u201d and \u201ccross-sequence attention\u201d. C1. Self-attention is gating. (Prop. 4.1, Sec. 4) C2. Gating cannot provide explanations in the way that attention is alleged to do. C3. Single input sequence models deploy only self-attention, so by C1-C2, attention cannot be used for explanation of these models; attention in two input sequence models is not limited to self-attention and is not equivalent to gating, so attention can provide explanations in these models. C4. Outputs are sensitive to alteration of attention weights for two-sequence but not one-sequence models. (Tables 2-3 vs. 1; Figs. 2-3) C5. Human judgments (\u201cmanual evaluation\u201d) of the intuitive importance, for the output, of items with highest attention weights show that such intuitive importance is found for both one- and two-sequence models. (Fig. 5) RATING: Reject REASONS FOR RATING (SUMMARY). Because the modeling experiments closely follow previous work, the primary contribution rests on the account provided of why explanation-by-attention works only sometimes --- on the basis of the Proposition identifying attention with gating. But the reasoning and the math as presented are problematic. There is a worthwhile contribution from the human judgment experiment, but this is not sufficient to overcome the weakness with the main argument of the paper. (I also have reasons to question whether one- vs two-sequence inputs is the right distinction that needs to be accounted for.) REVIEW C1. If this point were made in plain English, \u2018attention weights and gating units both multiply activation values\u2019, I would say, \u201cyes, obviously\u201d. But the point is stated as a Proposition, with a Proof, so the math given should be correct. I don\u2019t see a way to make it correct, and that shouldn\u2019t be the job of the reader anyway. There are several errors in the dimensions of the matrices which make the equations incoherent. Eq. (1) contains W*h, a matrix product, where the dimensions stated are W in R^{d x d\u2019} and h in R{T x m}; these cannot be multiplied. This unclarity about matrix dimensions propagates into Prop. 4.1. In the definition of g(X), we have WX + b, where b is presumably a vector. Addition then requires that WX also be a vector, but X is stated to be in R^{n x d}, so WX cannot be a vector. Whether WX + b is actually a vector or a matrix, g(X) = c^T tanh(WX + b) is not a matrix: it is either a scalar or a vector. But this can\u2019t be. The definition of h uses the elementwise product, which requires that both arguments have the same dimensions, so g(X) must have the same dimensions as f(X). We\u2019re told f is the identity function, so f(X), like X, must be a matrix. Furthermore, the statement of Prop. 4.1 says that self-attention can be interpreted as *a* gating unit. By the standard definition of \u2018unit\u2019, this should mean that self-attention is a scalar. Throughout the paper, we are never told what kind of \u201cRNN\u201d is being assumed. If the RNN unit contains gates, as in an LSTM or a GRU, I can imagine that the intention is for Prop. 4.1 to say that (*) \u201cthe effect of self-attention can be reproduced without attention by adjusting the weights in the gating mechanism already present in the RNN\u201d, so that attention doesn\u2019t increase the capacity of the model. But what I see in the paper does not convince me that (*) is true. (Because of the kind of global normalization required by the softmax, I actually suspect it is not.) C2. I don\u2019t see why (formally or intuitively) gating is not a legitimate candidate for explaining the behavior of networks containing gates; I would assume just the opposite, actually. How can it *not* be part of a satisfactory explanation? And why should changing a name from \u201cattention\u201d to \u201cgating\u201d have any bearing on whether it (whatever it is called) can potentially serve for explanation of network behavior? C3. Leaving the formalism aside, I don\u2019t see intuitively why, whatever an analysis of self-attention might entail about explanation, the same implication shouldn\u2019t apply to straightforward (not Rocktaeschel) attention when two sequences are present. Why can\u2019t we just treat the concatenation of the input sequences as a single input sequence, as standardly done for example for the Transformer? If the formal content of Prop. 4.1 were clear, perhaps this could be justified, but it is simply asserted without justification in the proof that \u201cthe same reduction does not hold in the case of pair sequence\u201d. C4. Claims C1-C3 attempt to give an account for why various tests of the explanatory role of attention turn out positive for two-sequence but not one-sequence tasks, a pattern previously reported and verified with new results in the present paper. I fear however that one- vs two- is not the correct empirical generalization about attention that one should try to account for. Messing about with attention weights would not be expected to alter outputs if the output is determined by the input considered as a bag of words. And there is a troubling confound in the tasks compared: the one-sequence tasks are sentiment and topic classification, where a BOW model should do very well \u2013 and I suspect that is the real reason why these tasks don\u2019t show strong sensitivity to attention weight distribution. But the two-sequence tasks are NLI and QA, where (ideally) BOW models should not do nearly so well: paying attention to the right tokens should be important. The same is true of translation. So the confound in the tasks examined leaves it undetermined whether the crucial factor to account for is one- vs two-sequence or BOW-friendly vs BOW-unfriendly tasks. C5. Put together, the human judgments and the sensitivity-to-altering-attention-weights seem to indicate that attention tends always to be allocated to intuitively important tokens, and that matters for the output of the two-sequence models but not for the one-sequence models. This is what we\u2019d expect if attention is always being allocated appropriately, but for BOW-friendly tasks that doesn\u2019t make much difference. ", "rating": "1: Reject", "reply_text": "We thank the reviewer for constructive feedback . We firmly believe that the reviewer \u2019 s concerns can be easily handled through additional clarifications in the updated version of the paper . We hope the reviewer will agree . We provide further details below . C1 : We apologize for the inconsistency in the dimensions in Equation 1 . Please note that the mistake is in stating the dimensions of W which should be R^ { d \u2019 x m } . With this change , the entire Equation ( 1 ) becomes consistent . We again apologize for the confusion in Prop . 1.We have corrected the propagated inconsistency in the updated version of the paper and have explicitly mentioned all the dimensions . We clarify that the gating unit is similar to attention as it also computes scalar weights over tokens in the sentence . g ( x ) is a scalar weight for a token and its dimension is not the same as x ( which is a vector ) . Throughout the paper , we use LSTM in all our models . Adding a gating unit to models for single sequence tasks does increase the number of parameters in the model and thus help to improve the performance especially for large datasets like YELP and IMDB . However , unlike the attention weights in other kinds of tasks , they do not represent reasoning behind the model \u2019 s prediction and thus are not critical for the model \u2019 s performance . C2 : Unlike the prior works [ 1,2 ] on attention interpretability who have simply reported the observation that attention weights are not interpretable , we are providing an explanation behind that observation . By showing attention in single sequence tasks as a gating unit we want to distinguish the attention mechanism used for text classification from attention used in tasks like NLI and NMT for which it was originally proposed [ 3,4 ] . The main distinguishing factor is that in the former the attention weight for a given token is computed only based on its own representation whereas in the latter case the attention weights are based on another part of the input . We want to highlight that this simple difference is the reason behind the different nature of attention weights in both cases . In single sequence tasks , for each token attention mechanism is learning a scalar to decide its importance independent of other tokens whereas for pair sequence and generation tasks the weights are being learned with respect to some other part of the input for deciding the importance of the token . Therefore , in the latter case , the weights actually represent the reasoning behind the model \u2019 s prediction and are more critical for its performance . C3 : Please note that in our Bahdanau et al.attention-based model for pair sequence tasks the proposition 4.1 does not hold because there the attention weight for a token in the premise also depends on the encoding of the hypothesis given by Bi-RNN based encoder . h ( x ) = f ( x ) . \\sigma ( g ( x ) ) does not hold because the second term is a function of the hypothesis as well . So , in pair sequence and generation tasks the equation becomes h ( x , y ) = f ( x ) . \\sigma ( g ( x , y ) ) which is not the gating unit proposed by Dauphin et al . ( 2017 ) .We apologize for not explicitly specifying this in the paper . C4 and C5 : BOW-friendly and BOW-unfriendly tasks can be another possible explanation behind the different nature of attention weights . However , it is not very clear how different tasks can be categorized into these two classes as even NLI can be solved using a model which utilizes BOW features from the premise and hypothesis for identifying entailment . In our work , we have given another explanation , basic idea behind which is that if the attention weights are computed with respect to another part of the input then they reflect the model \u2019 s reasoning and are critical for the model \u2019 s output otherwise they are not . In the case of single-sequence tasks , that does not hold , therefore , there attention mechanism can be reduced to gating and altering weights doesn \u2019 t have a substantial impact on the output . However , in the case of pair sequence tasks say NLI , attention weights over premise are computed based on the hypothesis and over the source sentence in NMT based on the current hidden state . Therefore , attention weights in those tasks are more meaningful and critical . [ 1 ] Jain , Sarthak and Byron C. Wallace . \u201c Attention is not Explanation. \u201d NAACL-HLT ( 2019 ) . [ 2 ] Serrano , Sof\u00eda and Noah A. Smith . \u201c Is Attention Interpretable ? \u201d ACL ( 2019 ) . [ 4 ] Rockt\u00e4schel , Tim , Edward Grefenstette , Karl Moritz Hermann , Tom\u00e1s Kocisk\u00fd and Phil Blunsom . \u201c Reasoning about Entailment with Neural Attention. \u201d CoRR abs/1509.06664 ( 2015 ) : n. Pag . [ 5 ] Bahdanau , Dzmitry , Kyunghyun Cho and Yoshua Bengio . \u201c Neural Machine Translation by Jointly Learning to Align and Translate. \u201d CoRR abs/1409.0473 ( 2014 ) : n. pag ."}], "0": {"review_id": "BJe-_CNKPH-0", "review_text": "Motivated by an existing paper, the paper analyzes the interpretability of attention mechanism over three NLP tasks. The previous paper claims that attention mechanism is not interpratable. The paper makes incremental contribution by showing that attentions are not interpratable when they mimic gating units (single sequence task) but are interpretable for two sequence and generation tasks. Experimental results are given to support the claims, which can also help readers to gain insights into the attention mechanism. The paper is well written and all claims are supported. I also have some questions below for clarification. If I get reasonable answers to these questions, I tend to accept the paper. 1. Jain & Wallace (2019) show that for the SNLI problem, attentions weakly correlate with the output based on Kendall's correlation and JSD which contradicts to your observation. Could you explain how this happens? are there any model settings different? 2. in figure 5, for the single sequence (original), most of attentions leading to correct predictions are labeled meaningful. Does this mean that even the attention doesn't necessarily contribute too much to the prediction correctness but they are also interpretable if they are allowed to be trained. Also, in Table 1, with modified attentions, all scores go down a little. This means that attention still can contribute to the final prediction but not significant enough (some like yelp are significant). I am gussing that the sequence encoding already present useful features for the final prediction. Did you check the distance of different word encodings? Are these encodings all very similar?", "rating": "6: Weak Accept", "reply_text": "Thank you for the constructive comments . We are glad that the reviewer liked our work . Below we provide clarification for the reviewer \u2019 s queries . 1.In comparison to Jain & Wallace ( 2019 ) , we use the exact same model architecture , i.e. , Bi-LSTM over word embedding of tokens followed by attention . Although for deciding hyperparameters we use validation data , unlike Jain & Wallace ( 2019 ) who use test data for that . In our experiments , we did not analyze how attention correlates with output based on Kendall and JSD but permutation scatterplots presented in Figure 2 are very similar to what has been reported by Jain & Wallace ( 2019 ) although they do not highlight them much in their paper . The complete set of results of Jain & Wallace ( 2019 ) are provided at this link : https : //successar.github.io/AttentionExplanation/docs/ . Here , we find that Permutation Scatterplots on Babi and SNLI datasets are very similar to what we have reported . 2.For manual evaluation , an annotator was asked to give a binary yes/no based on whether attention weights are meaningful or not . For that , an annotator is shown top three tokens with highest attention weights and if at least of them supports the prediction then \u201c yes \u201d was assigned otherwise \u201c no \u201d . For the case of single sequence tasks with original attention weights , we find that the gating mechanism during training learns to give high scores to tokens which refect some sentiment but does not reflect the reasoning behind the model \u2019 s prediction . An example of the same is also presented in Section 4.4 . Since a given review has few tokens which directly highlight the sentiment so , with our evaluation setting at least one such token is highly likely to appear in the top three . But we can see that the weights do not influence the model output as changing them does not affect the prediction considerably as shown in Figure 5 ( a ) Permuted and by the series of papers [ 1 , 2 ] prior to our work . Yes , attention even in the case of single sequence tasks is helpful as it is adding more parameters to the model and thus allowing it to learn more complex relationships between the input and the output . The size of datasets like YELP and IMDB is considerably large compared to other datasets , therefore providing more parameters allows the model to learn more from training data and perform better . This is reflected through significant changes in the performance on them . Although we have not explicitly performed such analysis but based on the variation in attention weights over tokens ( see x-axis Figure 3 ( a ) ) we can say that the words encoding are not very similar . The attention weights in single sequence tasks for each token are computed using the same W and c parameters . Now , since the difference between a randomly selected attention weight and the highest attention weight is going up to 0.6 on IMDB , where average text length is around 282 words , based on this we believe that the word encoding should not be very similar . [ 1 ] Jain , Sarthak and Byron C. Wallace . \u201c Attention is not Explanation. \u201d NAACL-HLT ( 2019 ) . [ 2 ] Serrano , Sof\u00eda and Noah A. Smith . \u201c Is Attention Interpretable ? \u201d ACL ( 2019 ) ."}, "1": {"review_id": "BJe-_CNKPH-1", "review_text": "This paper investigates the degree to which we might view attention weights as explanatory across NLP tasks and architectures. Notably, the authors distinguish between single and \"pair\" sequence tasks, the latter including NLI, and generation tasks (e.g., translation). The argument here is that attention weights do not provide explanatory power for single sequence tasks like classification, but do for NLI and generation. Another notable distinction from most (although not all; see the references below) prior work on the explainability of attention mechanisms in NLP is the inclusion of transformer/self-attentive architectures. Overall, this is a nice contribution that unifies some parallel lines of inquiry concerning explainability, and attention mechanism for different NLP tasks. The contrast in findings between single sequence (classification) and other NLP tasks and the implications for explainability is interesting, and provides a piece missing from the analyses conducted so far. The main issue I have with the paper is that I'm not sure I follow the rationale in Section 4 arguing that because attention \"works as a gating unit\", it follows that we cannot view single sequence tasks \"as attention\". Can the authors elaborate on why the conclusion follows from the premise? In other words, why is attention inherently *not gating*? This seems like an interesting connection, but again I do not see why attention acting as a gate implies that we should not view it as attention at all. Perhaps the authors could elaborate on their reasoning. Some additional references the authors might consider. Note that the latter two papers, I think, would seem to broadly disagree regarding self-attention and tasks aside from single sequence (classification): - \"Fine-grained Sentiment Analysis with Faithful Attention\": https://arxiv.org/abs/1908.06870 - \"Interrogating the Explanatory Power of Attention in Neural Machine Translation\": https://arxiv.org/abs/1910.00139 - \"On Identifiability in Transformers\": https://arxiv.org/abs/1908.04211", "rating": "6: Weak Accept", "reply_text": "We thank the reviewer for constructive feedback . In Section 4 , unlike the prior works [ 1,2 ] on attention interpretability who have simply reported the observation that attention weights are not interpretable , we are providing an explanation behind that observation . By showing attention in single sequence tasks as a gating unit we want to distinguish the attention mechanism used for text classification from attention used in tasks like NLI and NMT for which it was originally proposed [ 3,4 ] . The main distinguishing factor is that in the former the attention weight for a given token is computed only based on its own representation whereas in the latter case the attention weights are based on another part of the input . We want to highlight that this simple difference is the reason behind the different nature of attention weights in both cases . In single sequence tasks , for each token attention mechanism is learning scalar to weight its representation whereas for pair sequence and generation tasks the weights are being learned with respect to some other part of the input for deciding the importance of the token . Therefore , in the latter case , the weights actually represent the reasoning behind the model \u2019 s prediction and are more critical for its performance . We will be happy to elaborate further on this in case it is not clear . Thank you for pointing out additional references . We will surely include a discussion on them in the updated version of our paper . `` Interrogating the Explanatory Power of Attention in Neural Machine Translation '' investigates several ways of perturbing attention weights in the NMT model of which Permute and Uniform are common with us . Please note that the authors have reported that with Permute and Uniform only 6 % and 11 % words are retained from the original output of the model which is quite low thus our results are consistent with what they have reported . In `` On Identifiability in Transformers '' , authors have demonstrated that multiple attention distribution exists for the same output . However , in our work , we are analyzing that how randomly permuting attention weights in self-attention based models affect the overall prediction of the model . [ 1 ] Jain , Sarthak and Byron C. Wallace . \u201c Attention is not Explanation. \u201d NAACL-HLT ( 2019 ) . [ 2 ] Serrano , Sof\u00eda and Noah A. Smith . \u201c Is Attention Interpretable ? \u201d ACL ( 2019 ) . [ 3 ] Rockt\u00e4schel , Tim , Edward Grefenstette , Karl Moritz Hermann , Tom\u00e1s Kocisk\u00fd and Phil Blunsom . \u201c Reasoning about Entailment with Neural Attention. \u201d CoRR abs/1509.06664 ( 2015 ) : n. Pag . [ 4 ] Bahdanau , Dzmitry , Kyunghyun Cho and Yoshua Bengio . \u201c Neural Machine Translation by Jointly Learning to Align and Translate. \u201d CoRR abs/1409.0473 ( 2014 ) : n. pag ."}, "2": {"review_id": "BJe-_CNKPH-2", "review_text": "CONTRIBUTIONS: I use (unqualified) \u201cself-attention\u201d to refer to attention of tokens in a sequence to other tokens in the same sequence, as described by [some corrected version of] Eq (1) and the paragraph following it (citing Bahdanau et al. 2015). This contrasts with \u201cTransformer self-attention\u201d and \u201ccross-sequence attention\u201d. C1. Self-attention is gating. (Prop. 4.1, Sec. 4) C2. Gating cannot provide explanations in the way that attention is alleged to do. C3. Single input sequence models deploy only self-attention, so by C1-C2, attention cannot be used for explanation of these models; attention in two input sequence models is not limited to self-attention and is not equivalent to gating, so attention can provide explanations in these models. C4. Outputs are sensitive to alteration of attention weights for two-sequence but not one-sequence models. (Tables 2-3 vs. 1; Figs. 2-3) C5. Human judgments (\u201cmanual evaluation\u201d) of the intuitive importance, for the output, of items with highest attention weights show that such intuitive importance is found for both one- and two-sequence models. (Fig. 5) RATING: Reject REASONS FOR RATING (SUMMARY). Because the modeling experiments closely follow previous work, the primary contribution rests on the account provided of why explanation-by-attention works only sometimes --- on the basis of the Proposition identifying attention with gating. But the reasoning and the math as presented are problematic. There is a worthwhile contribution from the human judgment experiment, but this is not sufficient to overcome the weakness with the main argument of the paper. (I also have reasons to question whether one- vs two-sequence inputs is the right distinction that needs to be accounted for.) REVIEW C1. If this point were made in plain English, \u2018attention weights and gating units both multiply activation values\u2019, I would say, \u201cyes, obviously\u201d. But the point is stated as a Proposition, with a Proof, so the math given should be correct. I don\u2019t see a way to make it correct, and that shouldn\u2019t be the job of the reader anyway. There are several errors in the dimensions of the matrices which make the equations incoherent. Eq. (1) contains W*h, a matrix product, where the dimensions stated are W in R^{d x d\u2019} and h in R{T x m}; these cannot be multiplied. This unclarity about matrix dimensions propagates into Prop. 4.1. In the definition of g(X), we have WX + b, where b is presumably a vector. Addition then requires that WX also be a vector, but X is stated to be in R^{n x d}, so WX cannot be a vector. Whether WX + b is actually a vector or a matrix, g(X) = c^T tanh(WX + b) is not a matrix: it is either a scalar or a vector. But this can\u2019t be. The definition of h uses the elementwise product, which requires that both arguments have the same dimensions, so g(X) must have the same dimensions as f(X). We\u2019re told f is the identity function, so f(X), like X, must be a matrix. Furthermore, the statement of Prop. 4.1 says that self-attention can be interpreted as *a* gating unit. By the standard definition of \u2018unit\u2019, this should mean that self-attention is a scalar. Throughout the paper, we are never told what kind of \u201cRNN\u201d is being assumed. If the RNN unit contains gates, as in an LSTM or a GRU, I can imagine that the intention is for Prop. 4.1 to say that (*) \u201cthe effect of self-attention can be reproduced without attention by adjusting the weights in the gating mechanism already present in the RNN\u201d, so that attention doesn\u2019t increase the capacity of the model. But what I see in the paper does not convince me that (*) is true. (Because of the kind of global normalization required by the softmax, I actually suspect it is not.) C2. I don\u2019t see why (formally or intuitively) gating is not a legitimate candidate for explaining the behavior of networks containing gates; I would assume just the opposite, actually. How can it *not* be part of a satisfactory explanation? And why should changing a name from \u201cattention\u201d to \u201cgating\u201d have any bearing on whether it (whatever it is called) can potentially serve for explanation of network behavior? C3. Leaving the formalism aside, I don\u2019t see intuitively why, whatever an analysis of self-attention might entail about explanation, the same implication shouldn\u2019t apply to straightforward (not Rocktaeschel) attention when two sequences are present. Why can\u2019t we just treat the concatenation of the input sequences as a single input sequence, as standardly done for example for the Transformer? If the formal content of Prop. 4.1 were clear, perhaps this could be justified, but it is simply asserted without justification in the proof that \u201cthe same reduction does not hold in the case of pair sequence\u201d. C4. Claims C1-C3 attempt to give an account for why various tests of the explanatory role of attention turn out positive for two-sequence but not one-sequence tasks, a pattern previously reported and verified with new results in the present paper. I fear however that one- vs two- is not the correct empirical generalization about attention that one should try to account for. Messing about with attention weights would not be expected to alter outputs if the output is determined by the input considered as a bag of words. And there is a troubling confound in the tasks compared: the one-sequence tasks are sentiment and topic classification, where a BOW model should do very well \u2013 and I suspect that is the real reason why these tasks don\u2019t show strong sensitivity to attention weight distribution. But the two-sequence tasks are NLI and QA, where (ideally) BOW models should not do nearly so well: paying attention to the right tokens should be important. The same is true of translation. So the confound in the tasks examined leaves it undetermined whether the crucial factor to account for is one- vs two-sequence or BOW-friendly vs BOW-unfriendly tasks. C5. Put together, the human judgments and the sensitivity-to-altering-attention-weights seem to indicate that attention tends always to be allocated to intuitively important tokens, and that matters for the output of the two-sequence models but not for the one-sequence models. This is what we\u2019d expect if attention is always being allocated appropriately, but for BOW-friendly tasks that doesn\u2019t make much difference. ", "rating": "1: Reject", "reply_text": "We thank the reviewer for constructive feedback . We firmly believe that the reviewer \u2019 s concerns can be easily handled through additional clarifications in the updated version of the paper . We hope the reviewer will agree . We provide further details below . C1 : We apologize for the inconsistency in the dimensions in Equation 1 . Please note that the mistake is in stating the dimensions of W which should be R^ { d \u2019 x m } . With this change , the entire Equation ( 1 ) becomes consistent . We again apologize for the confusion in Prop . 1.We have corrected the propagated inconsistency in the updated version of the paper and have explicitly mentioned all the dimensions . We clarify that the gating unit is similar to attention as it also computes scalar weights over tokens in the sentence . g ( x ) is a scalar weight for a token and its dimension is not the same as x ( which is a vector ) . Throughout the paper , we use LSTM in all our models . Adding a gating unit to models for single sequence tasks does increase the number of parameters in the model and thus help to improve the performance especially for large datasets like YELP and IMDB . However , unlike the attention weights in other kinds of tasks , they do not represent reasoning behind the model \u2019 s prediction and thus are not critical for the model \u2019 s performance . C2 : Unlike the prior works [ 1,2 ] on attention interpretability who have simply reported the observation that attention weights are not interpretable , we are providing an explanation behind that observation . By showing attention in single sequence tasks as a gating unit we want to distinguish the attention mechanism used for text classification from attention used in tasks like NLI and NMT for which it was originally proposed [ 3,4 ] . The main distinguishing factor is that in the former the attention weight for a given token is computed only based on its own representation whereas in the latter case the attention weights are based on another part of the input . We want to highlight that this simple difference is the reason behind the different nature of attention weights in both cases . In single sequence tasks , for each token attention mechanism is learning a scalar to decide its importance independent of other tokens whereas for pair sequence and generation tasks the weights are being learned with respect to some other part of the input for deciding the importance of the token . Therefore , in the latter case , the weights actually represent the reasoning behind the model \u2019 s prediction and are more critical for its performance . C3 : Please note that in our Bahdanau et al.attention-based model for pair sequence tasks the proposition 4.1 does not hold because there the attention weight for a token in the premise also depends on the encoding of the hypothesis given by Bi-RNN based encoder . h ( x ) = f ( x ) . \\sigma ( g ( x ) ) does not hold because the second term is a function of the hypothesis as well . So , in pair sequence and generation tasks the equation becomes h ( x , y ) = f ( x ) . \\sigma ( g ( x , y ) ) which is not the gating unit proposed by Dauphin et al . ( 2017 ) .We apologize for not explicitly specifying this in the paper . C4 and C5 : BOW-friendly and BOW-unfriendly tasks can be another possible explanation behind the different nature of attention weights . However , it is not very clear how different tasks can be categorized into these two classes as even NLI can be solved using a model which utilizes BOW features from the premise and hypothesis for identifying entailment . In our work , we have given another explanation , basic idea behind which is that if the attention weights are computed with respect to another part of the input then they reflect the model \u2019 s reasoning and are critical for the model \u2019 s output otherwise they are not . In the case of single-sequence tasks , that does not hold , therefore , there attention mechanism can be reduced to gating and altering weights doesn \u2019 t have a substantial impact on the output . However , in the case of pair sequence tasks say NLI , attention weights over premise are computed based on the hypothesis and over the source sentence in NMT based on the current hidden state . Therefore , attention weights in those tasks are more meaningful and critical . [ 1 ] Jain , Sarthak and Byron C. Wallace . \u201c Attention is not Explanation. \u201d NAACL-HLT ( 2019 ) . [ 2 ] Serrano , Sof\u00eda and Noah A. Smith . \u201c Is Attention Interpretable ? \u201d ACL ( 2019 ) . [ 4 ] Rockt\u00e4schel , Tim , Edward Grefenstette , Karl Moritz Hermann , Tom\u00e1s Kocisk\u00fd and Phil Blunsom . \u201c Reasoning about Entailment with Neural Attention. \u201d CoRR abs/1509.06664 ( 2015 ) : n. Pag . [ 5 ] Bahdanau , Dzmitry , Kyunghyun Cho and Yoshua Bengio . \u201c Neural Machine Translation by Jointly Learning to Align and Translate. \u201d CoRR abs/1409.0473 ( 2014 ) : n. pag ."}}