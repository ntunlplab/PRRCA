{"year": "2020", "forum": "SkeYUkStPr", "title": "Deep Lifetime Clustering", "decision": "Reject", "meta_review": "The authors propose a clustering algorithm for users in a system based on their lifetime distribution. The reviewers acknowledge the novelty of the proposed clustering algorithm, but one concern left unresolved is how the results of the analysis can be of use in the real world examples used. ", "reviews": [{"review_id": "SkeYUkStPr-0", "review_text": "This paper proposes a method to cluster subjects based on the latent lifetime distribution. The proposed model clusters by maximizing the empirical divergence between different clusters. The problem setting of this paper is not clear to me. I am not sure which variables are observed and which are not. For example, in the Friendster experiment, the data of 5 months from joining is used for clustering. However, the termination windows are chosen to be 10 months. Therefore, it is clear that the observed data will not contain the termination signals, and I do not believe the training of the model is possible, without observing any termination signals. In the paper, do we consider only one type or multiple types of events? Is $M_{k, i}$ a vector that represents the attributes or properties of an event? Some details of the model are not clear to me. In Equation (2), the input of the neural network differs in length across different subject $u$, because the number of observed events for each subject is different. How does the proposed neural network take inputs of different lengths? How the non-decreasing function $\\xi^{(u)}$ is defined in Section 3.2? Is it a function of the observed data for each subject? How the empirical distribution $\\hat{S}_i$ in Equation (4) is computed is also not clear to me. How $\\hat{S}_i$ is a vector? Is it constructed by concatenating $\\hat{S}_k(W_1, W_2; D)[t] $ with different $t$? How to normalize $\\hat{S}_i$ such that it is a valid probabilistic distribution? Since $\\hat{S}_i$ is high dimensional, it looks very challenging to estimate the joint distribution. The overall objective function is given by Equation (4), is it correct? In Equation (4), why should we compute the minimum values across all possible pairs of clusters rather than the summation of all pairs? If Equation (4) is the overall objective function, then it looks like the model does not contain a component that maximizes a likelihood function. How is it guaranteed that the model will fit the data? It looks like the model will converge to a trivial solution that $\\beta$ is a constant such that $\\beta = 1$ for one cluster and $\\beta = 0$ for another cluster, if the likelihood function is not involved. This will give a maximum divergence between distributions. In summary, it seems that numerous technical details are missing and the paper might contain technical flaws. I do not suggest the acceptance of this paper. ", "rating": "6: Weak Accept", "reply_text": "Thank you for the detailed review . We would like to clarify that the paper has absolutely no technical flaws , as we detail next . Q1 . `` The problem setting of this paper is not clear to me . I am not sure which variables are observed and which are not . '' To help the reader , we have moved the Table of notations from the Supplementary Material to the main paper ( now Table 1 ) . Since the clusters are unobserved , all variables with subscript $ k $ are unobserved . In Definition 1 , we use variables with subscript $ k $ to indicate a random variable indexed by a random subject of cluster $ k $ . The definition describes only the underlying ( hidden ) generative process for the observed training data . Consider only the Friendster social network where $ u $ denotes a user in the system . Then we consider the following variables : - $ X^ { ( u ) } $ ( observed ) is a vector of user covariates ( e.g. , age , gender ) . - $ Y^ { ( u ) } _i $ is the inter-event time between activity events ( e.g. , sending comments ) and $ M^ { ( u ) } _i $ is a vector of event covariates . - For training users , the activity events are observed for a long time ( within $ [ 0 , t_m ] $ ) to compute observed lifetime $ H^ { ( u ) } $ and period of inactivity $ \\chi^ { ( u ) } $ ( both of these variables are tied to $ t_m $ ) . - For test users , the activity events are observed only till time $ \\tau = 5 $ months from their joining . We wish to cluster the test users using their covariates and the initial activity events . We have added a paragraph in the paper to clarify this distinction between training and test users . - $ \\theta^ { ( u ) } $ ( observed ) is the joining time . - $ A^ { ( u ) } _i $ is the termination signal ( observed in healthcare datasets , unobserved in social networks ) . - $ T_k $ ( unobserved ) is the lifetime of a random subject in cluster $ k $ . - $ S_k $ ( unobserved ) is the lifetime distribution ( CCDF ) $ P [ T_k > t ] $ of cluster $ k $ . Q2 . `` In the paper , do we consider only one type or multiple types of events ? '' We consider different types of activity events ( e.g. , login , send/receive comments ) . Terminal event ( e.g. , quitting social media ) is one that ends all activity events . Terminal event can have different causes ; but lacking additional information about these causes , we do not model them separately in this paper . Q3 . `` Is $ M_ { k , i } $ a vector that represents the attributes or properties of an event ? '' Yes , $ M_ { k , i } $ is a random vector representing the attributes of the $ i $ -th activity event of a random user in cluster $ k $ . $ M^ { ( u ) } _i $ is the same for a particular user $ u $ . Q4 . `` In the Friendster experiment , the data of 5 months from joining is used for clustering ... termination windows are ... 10 months . Therefore , it is clear that the observed data will not contain the termination signals ... I do not believe the training of the model is possible without observing termination signals . '' Unobservability of termination signals is a property of the domain/dataset ( healthcare or social networks ) and not due to the choice of timeout window . For example , in Friendster , we do not know if or when the users deactivate their account ; we only have information regarding their activities ( i.e. , that they have been inactive for say 15 months ) . In contrast , time of death ( if occurred ) is available in healthcare datasets . A `` timeout '' window of 10 months is an artificial way of providing these termination signals : if a user is inactive for greater than 10 months , then consider the user `` terminated '' ( left the social network ) . Note that we use artificial termination signals only for baselines and to compute performance metrics , and not for training our model . We train our model by jointly learning a smooth `` timeout '' window in the form of $ \\beta^ { ( u ) } $ . Whereas we see only $ \\tau=5 $ months of activities of test subjects , we observe training subjects for longer times ( e.g. , $ [ 0 , t_m ] $ is 6 years long in Friendster experiments ) , thus facilitating model training . Q5 . `` How the non-decreasing function $ \\xi^ { ( u ) } $ is defined in Section 3.2 ? Is it a function of the observed data for each subject ? '' As described in Section 3.2 , we use $ \\beta^ { ( u ) } : = 1 - e^ { -\\xi^ { ( u ) } \\chi^ { ( u ) } } $ with a shared scalar rate parameter $ \\xi^ { ( u ) } = W_2 $ . This makes sure that $ \\beta^ { ( u ) } $ is a non-decreasing function of inactivity ; longer the inactivity , higher the probability of termination . To avoid confusion , we have replaced $ \\xi^ { ( u ) } $ with $ W_2 $ in the paper ."}, {"review_id": "SkeYUkStPr-1", "review_text": "This paper proposes a deep learning method for clustering subjects (patients, users of social networks) into clusters of different lifetime (survival time) distributions. Inputs for a subject include covariates (age, gender, etc) and an event sequence. A key difficulty in the problem is that the lifetime of a subject is often unobserved. The proposed method is to learn a termination function that is non-decreasing with time, which essentially treats prolonged inactivity as termination in a probabilistic way. Clustering is done in a discriminative manner where the objective function considers cluster sizes besides between-cluster differences in lifetime distribution. (The inclusion of cluster size information in the objective function avoids degenerate clustering results.) The paper is strong in technical contents. The objective function is well-motivated and placed on solid theoretical foundation. Empirically, the proposed method performed significantly better than baselines. The weakness of the paper is that the utility of the work has not been demonstrated. For example, the Friendster dataset (1.1 million social network users) is partitioned into 2-5 clusters. However, are there clusters that a service provider can use to improve their service? It is doubtful whether such \u201cuseful\u201d clusters can be found using the proposed algorithm. One might need to obtain a fairly large number of clusters, via conditioning perhaps, before finding some really useful ones. \u2022 The paper is strong in technical contents. The objective function is well-motivated and placed on solid theoretical foundation. \u2022 Empirically, the proposed method performed significantly better than baselines. \u2022 The utility of the work has not been demonstrated. Potential impact is an outstanding issue here because the proposed method is very special-purpose. Question to the authors: It is observed that the numbers of friends and comments are strongly correlated with the clustering. Were those two covariates included in the analysis? If not, why? ", "rating": "6: Weak Accept", "reply_text": "Thank you for your positive comments and feedback . Q1 . `` Potential impact is an outstanding issue here because the proposed method is very special-purpose . Are there clusters that a service provider can use to improve their service ? One might need to obtain a large number of clusters , via conditioning perhaps , before finding some really useful ones . '' Lifetime clustering is a very important task , with impactful applications in multiple fields . 1.There are numerous biomedical applications for this approach for example , clustering gene profiles to identify cancer subtypes [ 1,2,3 ] , identifying subgroups of patients with different clinical characteristics [ 4 ] . Concurrent to our work , Chen et al . [ 1 ] ( October 2019 ) propose a supervised deep learning approach to identify cancer subtypes with different lifetime distributions . However , their approach requires at least a few ground truth lifetime clusters for supervision , whereas our approach does not require such supervision . 2.Similarly , identifying user subgroups in a social network can be a very useful tool . For example , a service provider might wish to obtain lifetime clusters of users based on their usage pattern alone ( e.g. , a high-risk cluster might only include people that use features X , Y and Z of the service ) . Identifying such groups can provide insights about the best/worst workflows in the service ( different from predicting the lifetime of individual users ) . Note that the service provider need not obtain a large number of clusters to find these groups ; she simply needs to remove all other covariates ( like age , number of friends , etc . ) whose analysis is not required . Moreover , of independent interest , the Kuiper loss provided by our paper ( through our cheap-to-compute bound ) is shown to be superior to MMD in our task . MMD finds a wide range of applications in machine learning . One could dedicate an entire paper just to the Kuiper bound ; in our paper , it is just one of the contributions ( albeit important ) . Q2 . `` Were number of friends and comments included in the analysis ? '' Yes , the number of friends and comments were used as covariates for lifetime clustering ( along with 58 other covariates ) . Matching our intuition , number of friends and comments were correlated with the cluster assignments , thus qualitatively validating the clusters . References : [ 1 ] Chen , Runpu , et al . `` Deep learning approach to identifying cancer subtypes using high-dimensional genomic data . '' Bioinformatics ( 2019 ) . [ 2 ] Jiang , Limin , et al . `` Discovering cancer subtypes via an accurate fusion strategy on multiple profile data . '' Frontiers in genetics 10 ( 2019 ) : 20 . [ 3 ] Guo , Yang , Xuequn Shang , and Zhanhuai Li . `` Identification of cancer subtypes by integrating multiple types of transcriptomics data with deep learning in breast cancer . '' Neurocomputing 324 ( 2019 ) : 20-30 . [ 4 ] Hastie , Barbara A. , et al . `` Cluster analysis of multiple experimental pain modalities . '' Pain 116.3 ( 2005 ) : 227-237 ."}, {"review_id": "SkeYUkStPr-2", "review_text": "The authors propose an approach to cluster subjects into K clusters according to their underlying, often unknown, life distribution. The loss function to the neural-network-based approach is based on a tight upper bound to the two-sample Kuiper test. Experiments are conducted on artificial and two real life datasets. I enjoyed reading the paper, the authors propose an approach to address an important yet relatively under-explored problem in survival analysis. It is not entirely clear how to handle the case when after the model is trained, H^(u) for a new subject u is larger than t_max when the model is trained. In such case, \\Chi^(u) will be negative, thus what happens with \\beta^(u)(W_2)? When the termination signals are not available (friendster data) termination signals are set artificially when training and evaluation, thus, is the model learning artificially set termination signals and thus artificial survival functions? what is the point of doing this? Moreover, if the termination signals are artificial, what is the point of calculating C-Index and Integrated Brier score? I understand the motivation to compare to existing methods, it is really not clear how meaningful they are at evaluating performance. In general, using C-Index and Brier scores in the context of the presented application may be misleading (or non-applicable) because these metrics are usually applied to survival analysis scenarios where one seeks to estimate likelihood of survival or time to event (over a usually infinite time horizon). Here, \\beta^(u)(W_2) is a function of \\Chi^(u) which is known (also not comparable to survival analysis), artificially tied to a pre-specified time-horizon t_max, and not dependent on covariates as in survival analysis. Further, SSC-Bair and SSC-Gaynor are clustering models, how are survival estimates obtained for these? In practice, how is K selected? based on the friendster results for K=2,3,4,5 they all seem to produce distinct clusters, so which one should be used? This raises a question: how are the clusters, their members or the number of clusters informing the use case? In the MIMIC III experiment, where events are observed, the model finds two clusters without strikingly different survival functions (relative to friendster). Can one really consider S_2 as the high-risk group? Can one get higher risk clusters with larger K? Minor: - A_i^(u) is missing from the definition in the training data. - In Section 3.2, if \\xi^(u) is shared, why the superscript indicating that is subject specific?", "rating": "3: Weak Reject", "reply_text": "Thank you for your positive comments and feedback . Q1 . `` It is not entirely clear how to handle the case when after the model is trained , ... what happens with $ \\beta^ { ( u ) } ( W_2 ) $ ? '' $ \\beta^ { ( u ) } $ are only computed and used during training . For a new test subject , the model outputs cluster assignments $ \\alpha^ { ( u ) } $ using only the covariates $ X^ { ( u ) } $ and the subject 's events till time $ \\tau $ as described in Equation ( 2 ) . We do not need to calculate $ \\beta^ { ( u ) } $ , $ H^ { ( u ) } $ , $ \\chi^ { ( u ) } $ for a test subject ; indeed we will not be able to calculate them since they are all tied to $ t_m $ . $ H^ { ( u ) } $ and $ \\chi^ { ( u ) } $ are only used during training to obtain a probabilistic proxy for true lifetime of $ u $ via $ \\beta^ { ( u ) } $ . We have added a paragraph in Section 2 to clarify this distinction between training and test subjects . Q2 . `` When the termination signals are not available ( friendster data ) ... set artificially when training and evaluation , ... what is the point of doing this ? ... what is the point of calculating C-Index and Integrated Brier score ? I understand the motivation to compare to existing methods , it is really not clear how meaningful they are at evaluating performance . '' The paragraph titled `` Termination signals for evaluation and baselines '' ( page 7 in the paper ) clarifies this . The termination signals are artificially set only for the baselines ( since they can not handle unobservability ) , and during evaluation ( since * all * the performance metrics including Logrank score require termination signals ) . Our approach is trained without any termination signals ; they are learnt using the smooth `` timeout '' window $ \\beta^ { ( u ) } $ as part of the optimization . This unfairly helps the competing methods since they are trained and evaluated using termination signals , whereas our approach is not trained with these termination signals . Q3 . `` In general , using C-Index and Brier scores ... may be misleading ( or non-applicable ) ... horizon . Here , $ \\beta^ { ( u ) } ( W_2 ) $ is a function of $ \\chi^ { ( u ) } $ which is known ... and not dependent on covariates as in survival analysis . Further , SSC-Bair and SSC-Gaynor are clustering models , how are survival estimates obtained for these ? '' We used all metrics available in the literature , Logrank , C-index and Brier score to further validate the discriminative power of the clusters . This is a standard quantitative evaluation procedure used in unsupervised learning , common , for instance , in learning unsupervised image representations ( e.g. , [ 1 ] ) . The answer to ( 1 ) above should clarify that $ \\beta^ { ( u ) } ( W_2 ) $ and $ \\chi^ { ( u ) } $ ( both tied to $ t_m $ ) can not be computed for test subjects . The clusters from all the methods ( SSC-Bair , SSC-Gaynor , DeepHit+GMM , DeepCLife ) are evaluated the same way . Given the cluster assignments $ \\kappa ( u ' ) \\in \\ { 1 , \\ldots , K\\ } $ and the termination signals ( possibly using a `` timeout '' window as described in answer to ( 2 ) above ) for all the users $ u ' $ in the test data , we can obtain the empirical lifetime distribution of all the clusters $ \\hat { S } _k , \\ : \\forall k \\in \\ { 1 , \\ldots , K\\ } $ using the Kaplan-Meier estimates ( over the test data alone ) . Then , the empirical lifetime distribution of a user $ u ' $ is given by that of her assigned cluster , i.e. , $ \\hat { S } ^ { ( u ' ) } : = \\hat { S } _ { \\kappa ( u ' ) } $ ( shared for all users in the cluster ) . Integrated Brier score can be computed as usual using $ \\hat { S } ^ { ( u ' ) } $ ( Equation 14 ) . C-index is computed by using the expected lifetime obtained from the lifetime distribution $ \\hat { S } ^ { ( u ' ) } $ as the predicted lifetime of $ u ' $ . We have updated the Metrics subsection in the paper to include the above discussion . References : [ 1 ] Bengio , Yoshua . `` Deep learning of representations for unsupervised and transfer learning . '' Proceedings of ICML workshop on unsupervised and transfer learning . 2012 ."}], "0": {"review_id": "SkeYUkStPr-0", "review_text": "This paper proposes a method to cluster subjects based on the latent lifetime distribution. The proposed model clusters by maximizing the empirical divergence between different clusters. The problem setting of this paper is not clear to me. I am not sure which variables are observed and which are not. For example, in the Friendster experiment, the data of 5 months from joining is used for clustering. However, the termination windows are chosen to be 10 months. Therefore, it is clear that the observed data will not contain the termination signals, and I do not believe the training of the model is possible, without observing any termination signals. In the paper, do we consider only one type or multiple types of events? Is $M_{k, i}$ a vector that represents the attributes or properties of an event? Some details of the model are not clear to me. In Equation (2), the input of the neural network differs in length across different subject $u$, because the number of observed events for each subject is different. How does the proposed neural network take inputs of different lengths? How the non-decreasing function $\\xi^{(u)}$ is defined in Section 3.2? Is it a function of the observed data for each subject? How the empirical distribution $\\hat{S}_i$ in Equation (4) is computed is also not clear to me. How $\\hat{S}_i$ is a vector? Is it constructed by concatenating $\\hat{S}_k(W_1, W_2; D)[t] $ with different $t$? How to normalize $\\hat{S}_i$ such that it is a valid probabilistic distribution? Since $\\hat{S}_i$ is high dimensional, it looks very challenging to estimate the joint distribution. The overall objective function is given by Equation (4), is it correct? In Equation (4), why should we compute the minimum values across all possible pairs of clusters rather than the summation of all pairs? If Equation (4) is the overall objective function, then it looks like the model does not contain a component that maximizes a likelihood function. How is it guaranteed that the model will fit the data? It looks like the model will converge to a trivial solution that $\\beta$ is a constant such that $\\beta = 1$ for one cluster and $\\beta = 0$ for another cluster, if the likelihood function is not involved. This will give a maximum divergence between distributions. In summary, it seems that numerous technical details are missing and the paper might contain technical flaws. I do not suggest the acceptance of this paper. ", "rating": "6: Weak Accept", "reply_text": "Thank you for the detailed review . We would like to clarify that the paper has absolutely no technical flaws , as we detail next . Q1 . `` The problem setting of this paper is not clear to me . I am not sure which variables are observed and which are not . '' To help the reader , we have moved the Table of notations from the Supplementary Material to the main paper ( now Table 1 ) . Since the clusters are unobserved , all variables with subscript $ k $ are unobserved . In Definition 1 , we use variables with subscript $ k $ to indicate a random variable indexed by a random subject of cluster $ k $ . The definition describes only the underlying ( hidden ) generative process for the observed training data . Consider only the Friendster social network where $ u $ denotes a user in the system . Then we consider the following variables : - $ X^ { ( u ) } $ ( observed ) is a vector of user covariates ( e.g. , age , gender ) . - $ Y^ { ( u ) } _i $ is the inter-event time between activity events ( e.g. , sending comments ) and $ M^ { ( u ) } _i $ is a vector of event covariates . - For training users , the activity events are observed for a long time ( within $ [ 0 , t_m ] $ ) to compute observed lifetime $ H^ { ( u ) } $ and period of inactivity $ \\chi^ { ( u ) } $ ( both of these variables are tied to $ t_m $ ) . - For test users , the activity events are observed only till time $ \\tau = 5 $ months from their joining . We wish to cluster the test users using their covariates and the initial activity events . We have added a paragraph in the paper to clarify this distinction between training and test users . - $ \\theta^ { ( u ) } $ ( observed ) is the joining time . - $ A^ { ( u ) } _i $ is the termination signal ( observed in healthcare datasets , unobserved in social networks ) . - $ T_k $ ( unobserved ) is the lifetime of a random subject in cluster $ k $ . - $ S_k $ ( unobserved ) is the lifetime distribution ( CCDF ) $ P [ T_k > t ] $ of cluster $ k $ . Q2 . `` In the paper , do we consider only one type or multiple types of events ? '' We consider different types of activity events ( e.g. , login , send/receive comments ) . Terminal event ( e.g. , quitting social media ) is one that ends all activity events . Terminal event can have different causes ; but lacking additional information about these causes , we do not model them separately in this paper . Q3 . `` Is $ M_ { k , i } $ a vector that represents the attributes or properties of an event ? '' Yes , $ M_ { k , i } $ is a random vector representing the attributes of the $ i $ -th activity event of a random user in cluster $ k $ . $ M^ { ( u ) } _i $ is the same for a particular user $ u $ . Q4 . `` In the Friendster experiment , the data of 5 months from joining is used for clustering ... termination windows are ... 10 months . Therefore , it is clear that the observed data will not contain the termination signals ... I do not believe the training of the model is possible without observing termination signals . '' Unobservability of termination signals is a property of the domain/dataset ( healthcare or social networks ) and not due to the choice of timeout window . For example , in Friendster , we do not know if or when the users deactivate their account ; we only have information regarding their activities ( i.e. , that they have been inactive for say 15 months ) . In contrast , time of death ( if occurred ) is available in healthcare datasets . A `` timeout '' window of 10 months is an artificial way of providing these termination signals : if a user is inactive for greater than 10 months , then consider the user `` terminated '' ( left the social network ) . Note that we use artificial termination signals only for baselines and to compute performance metrics , and not for training our model . We train our model by jointly learning a smooth `` timeout '' window in the form of $ \\beta^ { ( u ) } $ . Whereas we see only $ \\tau=5 $ months of activities of test subjects , we observe training subjects for longer times ( e.g. , $ [ 0 , t_m ] $ is 6 years long in Friendster experiments ) , thus facilitating model training . Q5 . `` How the non-decreasing function $ \\xi^ { ( u ) } $ is defined in Section 3.2 ? Is it a function of the observed data for each subject ? '' As described in Section 3.2 , we use $ \\beta^ { ( u ) } : = 1 - e^ { -\\xi^ { ( u ) } \\chi^ { ( u ) } } $ with a shared scalar rate parameter $ \\xi^ { ( u ) } = W_2 $ . This makes sure that $ \\beta^ { ( u ) } $ is a non-decreasing function of inactivity ; longer the inactivity , higher the probability of termination . To avoid confusion , we have replaced $ \\xi^ { ( u ) } $ with $ W_2 $ in the paper ."}, "1": {"review_id": "SkeYUkStPr-1", "review_text": "This paper proposes a deep learning method for clustering subjects (patients, users of social networks) into clusters of different lifetime (survival time) distributions. Inputs for a subject include covariates (age, gender, etc) and an event sequence. A key difficulty in the problem is that the lifetime of a subject is often unobserved. The proposed method is to learn a termination function that is non-decreasing with time, which essentially treats prolonged inactivity as termination in a probabilistic way. Clustering is done in a discriminative manner where the objective function considers cluster sizes besides between-cluster differences in lifetime distribution. (The inclusion of cluster size information in the objective function avoids degenerate clustering results.) The paper is strong in technical contents. The objective function is well-motivated and placed on solid theoretical foundation. Empirically, the proposed method performed significantly better than baselines. The weakness of the paper is that the utility of the work has not been demonstrated. For example, the Friendster dataset (1.1 million social network users) is partitioned into 2-5 clusters. However, are there clusters that a service provider can use to improve their service? It is doubtful whether such \u201cuseful\u201d clusters can be found using the proposed algorithm. One might need to obtain a fairly large number of clusters, via conditioning perhaps, before finding some really useful ones. \u2022 The paper is strong in technical contents. The objective function is well-motivated and placed on solid theoretical foundation. \u2022 Empirically, the proposed method performed significantly better than baselines. \u2022 The utility of the work has not been demonstrated. Potential impact is an outstanding issue here because the proposed method is very special-purpose. Question to the authors: It is observed that the numbers of friends and comments are strongly correlated with the clustering. Were those two covariates included in the analysis? If not, why? ", "rating": "6: Weak Accept", "reply_text": "Thank you for your positive comments and feedback . Q1 . `` Potential impact is an outstanding issue here because the proposed method is very special-purpose . Are there clusters that a service provider can use to improve their service ? One might need to obtain a large number of clusters , via conditioning perhaps , before finding some really useful ones . '' Lifetime clustering is a very important task , with impactful applications in multiple fields . 1.There are numerous biomedical applications for this approach for example , clustering gene profiles to identify cancer subtypes [ 1,2,3 ] , identifying subgroups of patients with different clinical characteristics [ 4 ] . Concurrent to our work , Chen et al . [ 1 ] ( October 2019 ) propose a supervised deep learning approach to identify cancer subtypes with different lifetime distributions . However , their approach requires at least a few ground truth lifetime clusters for supervision , whereas our approach does not require such supervision . 2.Similarly , identifying user subgroups in a social network can be a very useful tool . For example , a service provider might wish to obtain lifetime clusters of users based on their usage pattern alone ( e.g. , a high-risk cluster might only include people that use features X , Y and Z of the service ) . Identifying such groups can provide insights about the best/worst workflows in the service ( different from predicting the lifetime of individual users ) . Note that the service provider need not obtain a large number of clusters to find these groups ; she simply needs to remove all other covariates ( like age , number of friends , etc . ) whose analysis is not required . Moreover , of independent interest , the Kuiper loss provided by our paper ( through our cheap-to-compute bound ) is shown to be superior to MMD in our task . MMD finds a wide range of applications in machine learning . One could dedicate an entire paper just to the Kuiper bound ; in our paper , it is just one of the contributions ( albeit important ) . Q2 . `` Were number of friends and comments included in the analysis ? '' Yes , the number of friends and comments were used as covariates for lifetime clustering ( along with 58 other covariates ) . Matching our intuition , number of friends and comments were correlated with the cluster assignments , thus qualitatively validating the clusters . References : [ 1 ] Chen , Runpu , et al . `` Deep learning approach to identifying cancer subtypes using high-dimensional genomic data . '' Bioinformatics ( 2019 ) . [ 2 ] Jiang , Limin , et al . `` Discovering cancer subtypes via an accurate fusion strategy on multiple profile data . '' Frontiers in genetics 10 ( 2019 ) : 20 . [ 3 ] Guo , Yang , Xuequn Shang , and Zhanhuai Li . `` Identification of cancer subtypes by integrating multiple types of transcriptomics data with deep learning in breast cancer . '' Neurocomputing 324 ( 2019 ) : 20-30 . [ 4 ] Hastie , Barbara A. , et al . `` Cluster analysis of multiple experimental pain modalities . '' Pain 116.3 ( 2005 ) : 227-237 ."}, "2": {"review_id": "SkeYUkStPr-2", "review_text": "The authors propose an approach to cluster subjects into K clusters according to their underlying, often unknown, life distribution. The loss function to the neural-network-based approach is based on a tight upper bound to the two-sample Kuiper test. Experiments are conducted on artificial and two real life datasets. I enjoyed reading the paper, the authors propose an approach to address an important yet relatively under-explored problem in survival analysis. It is not entirely clear how to handle the case when after the model is trained, H^(u) for a new subject u is larger than t_max when the model is trained. In such case, \\Chi^(u) will be negative, thus what happens with \\beta^(u)(W_2)? When the termination signals are not available (friendster data) termination signals are set artificially when training and evaluation, thus, is the model learning artificially set termination signals and thus artificial survival functions? what is the point of doing this? Moreover, if the termination signals are artificial, what is the point of calculating C-Index and Integrated Brier score? I understand the motivation to compare to existing methods, it is really not clear how meaningful they are at evaluating performance. In general, using C-Index and Brier scores in the context of the presented application may be misleading (or non-applicable) because these metrics are usually applied to survival analysis scenarios where one seeks to estimate likelihood of survival or time to event (over a usually infinite time horizon). Here, \\beta^(u)(W_2) is a function of \\Chi^(u) which is known (also not comparable to survival analysis), artificially tied to a pre-specified time-horizon t_max, and not dependent on covariates as in survival analysis. Further, SSC-Bair and SSC-Gaynor are clustering models, how are survival estimates obtained for these? In practice, how is K selected? based on the friendster results for K=2,3,4,5 they all seem to produce distinct clusters, so which one should be used? This raises a question: how are the clusters, their members or the number of clusters informing the use case? In the MIMIC III experiment, where events are observed, the model finds two clusters without strikingly different survival functions (relative to friendster). Can one really consider S_2 as the high-risk group? Can one get higher risk clusters with larger K? Minor: - A_i^(u) is missing from the definition in the training data. - In Section 3.2, if \\xi^(u) is shared, why the superscript indicating that is subject specific?", "rating": "3: Weak Reject", "reply_text": "Thank you for your positive comments and feedback . Q1 . `` It is not entirely clear how to handle the case when after the model is trained , ... what happens with $ \\beta^ { ( u ) } ( W_2 ) $ ? '' $ \\beta^ { ( u ) } $ are only computed and used during training . For a new test subject , the model outputs cluster assignments $ \\alpha^ { ( u ) } $ using only the covariates $ X^ { ( u ) } $ and the subject 's events till time $ \\tau $ as described in Equation ( 2 ) . We do not need to calculate $ \\beta^ { ( u ) } $ , $ H^ { ( u ) } $ , $ \\chi^ { ( u ) } $ for a test subject ; indeed we will not be able to calculate them since they are all tied to $ t_m $ . $ H^ { ( u ) } $ and $ \\chi^ { ( u ) } $ are only used during training to obtain a probabilistic proxy for true lifetime of $ u $ via $ \\beta^ { ( u ) } $ . We have added a paragraph in Section 2 to clarify this distinction between training and test subjects . Q2 . `` When the termination signals are not available ( friendster data ) ... set artificially when training and evaluation , ... what is the point of doing this ? ... what is the point of calculating C-Index and Integrated Brier score ? I understand the motivation to compare to existing methods , it is really not clear how meaningful they are at evaluating performance . '' The paragraph titled `` Termination signals for evaluation and baselines '' ( page 7 in the paper ) clarifies this . The termination signals are artificially set only for the baselines ( since they can not handle unobservability ) , and during evaluation ( since * all * the performance metrics including Logrank score require termination signals ) . Our approach is trained without any termination signals ; they are learnt using the smooth `` timeout '' window $ \\beta^ { ( u ) } $ as part of the optimization . This unfairly helps the competing methods since they are trained and evaluated using termination signals , whereas our approach is not trained with these termination signals . Q3 . `` In general , using C-Index and Brier scores ... may be misleading ( or non-applicable ) ... horizon . Here , $ \\beta^ { ( u ) } ( W_2 ) $ is a function of $ \\chi^ { ( u ) } $ which is known ... and not dependent on covariates as in survival analysis . Further , SSC-Bair and SSC-Gaynor are clustering models , how are survival estimates obtained for these ? '' We used all metrics available in the literature , Logrank , C-index and Brier score to further validate the discriminative power of the clusters . This is a standard quantitative evaluation procedure used in unsupervised learning , common , for instance , in learning unsupervised image representations ( e.g. , [ 1 ] ) . The answer to ( 1 ) above should clarify that $ \\beta^ { ( u ) } ( W_2 ) $ and $ \\chi^ { ( u ) } $ ( both tied to $ t_m $ ) can not be computed for test subjects . The clusters from all the methods ( SSC-Bair , SSC-Gaynor , DeepHit+GMM , DeepCLife ) are evaluated the same way . Given the cluster assignments $ \\kappa ( u ' ) \\in \\ { 1 , \\ldots , K\\ } $ and the termination signals ( possibly using a `` timeout '' window as described in answer to ( 2 ) above ) for all the users $ u ' $ in the test data , we can obtain the empirical lifetime distribution of all the clusters $ \\hat { S } _k , \\ : \\forall k \\in \\ { 1 , \\ldots , K\\ } $ using the Kaplan-Meier estimates ( over the test data alone ) . Then , the empirical lifetime distribution of a user $ u ' $ is given by that of her assigned cluster , i.e. , $ \\hat { S } ^ { ( u ' ) } : = \\hat { S } _ { \\kappa ( u ' ) } $ ( shared for all users in the cluster ) . Integrated Brier score can be computed as usual using $ \\hat { S } ^ { ( u ' ) } $ ( Equation 14 ) . C-index is computed by using the expected lifetime obtained from the lifetime distribution $ \\hat { S } ^ { ( u ' ) } $ as the predicted lifetime of $ u ' $ . We have updated the Metrics subsection in the paper to include the above discussion . References : [ 1 ] Bengio , Yoshua . `` Deep learning of representations for unsupervised and transfer learning . '' Proceedings of ICML workshop on unsupervised and transfer learning . 2012 ."}}