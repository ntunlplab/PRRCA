{"year": "2021", "forum": "8wqCDnBmnrT", "title": "Zero-shot Synthesis with Group-Supervised Learning", "decision": "Accept (Poster)", "meta_review": "This paper presents a zero-shot generation approach by disentangling representations into swappable components (each component corresponding to an attribute) and then conditioning on any desired combination of attributes to do zero-shot synthesis of samples containing those attributes.\n\nThere were some concerns raised in the original reviews which the authors have addressed in the rebuttal and the revised submission. Post the discussion phase, all reviewers see merit in the proposed ideas and unanimously recommend acceptance. Based on my own reading of the paper and the reviews/author responses, I agree with the assessment. ", "reviews": [{"review_id": "8wqCDnBmnrT-0", "review_text": "Summary : The paper proposed a new training framework , namely GSL , for novel content synthesis . And GSL enables learning of disentangled representations of tangible attributes and achieve novel image synthesis by recombining those swappable components under a zero-shot setting . The framework leverages the underlying semantic links across samples which could be instantiated as a multigraph . Cycle-consistent reconstruction loss as well as reconstruction loss are computed on synthetic samples from swapped latent representations . Reasons for score : I vote for accepting . Overall , I think the paper is well written and the idea of GSL leveraging the underlying semantic relationships between samples to learn disentangled representations of tangible attributes is pretty novel and interesting . Experimental result on several real and synthetic datasets are also promising . Concerns : 1 ) How are hyperparameters { d_j } ^m_ { j=1 } selected ? It would be great if the authors could elaborate more about this . 2 ) Concerning the cycle attribute swap , how to guarantee the intermediate generated image to be reasonable instead of a trivial solution . Seems like no losses ( possibly like adversarial loss ) is directly added on the intermediate result .", "rating": "7: Good paper, accept", "reply_text": "1.Per-attribute latent dimensions were selected by some preliminary experiments , but they were mostly guided by intuition : for instance , we feel that the identity attribute for faces/vehicles should carry the most information and therefore be devoted the highest capacity . Nonetheless , this is an interesting question , so we have started running some experiments to change the dimension which we will add to supplementary material . 2.We ran ablation experiments for removing the cycle swap loss term ( please refer to the comment , addressing all reviewers , on top ) . Indeed , results are downgraded when the loss is removed . As we write in the text , the two swaps should guarantee that there is no information loss . Our intuition tells us that these swaps are not _trivial_ : if they were trivial ( e.g. , swapping latent values carrying no information ) , then we should not be able to reconstruct the inputs , especially that * * all * * latent values are eventually swapped ( for different example pairs ) -- therefore , no attribute latents carry zero information . We did not add adversarial loss on the intermediate result , however , it seems like a good idea and might improve the performance , especially for a small dataset . Nonetheless our method is currently fully supervised : it reduces the choices of hyperparameters , uses feed-forward auto-encoders ."}, {"review_id": "8wqCDnBmnrT-1", "review_text": "This paper proposes a Group-Supervised learning method for zero-shot synthesis . This method can generate multi-attributes images through the GZS-Net . However , the authors may pay attention to the following questions : 1 . The authors should check for grammar mistakes , e.g. , \u201d Fonts is a ... \u201d - > \u201d Fonts are ... \u201d . 2.From the description , the Group-Supervised learning method is just a special data sampling manner used in training . Besides , the total framework is commonly used in GANs for attribute control generation . 3.The contrast methods are too simple . The reviewer thinks the authors should compare their method with some new works , e.g. , \u201c ELEGANT : Exchanging Latent Encodings with GAN for Transferring Multiple Face \u201d . 4.In Section 5.3 , the reasons for the results in Table 1 are not analyzed . Why is the accuracy of contrast method AE-DS higher than GZS-Net ? 5.The author should add more quantitative analysis . Such as comparing the FID/IS values of generated results using different methods .", "rating": "6: Marginally above acceptance threshold", "reply_text": "1 . * * Grammar * * : * Fonts * is a name of our dataset . We made it italic . We also corrected the other grammar mistakes that we observed . 2 . * * Our method is a Sampling method * * : You are right GSL samples examples in a special way ( that is aware of , which examples share attribute values ) , but * additionally and crucially * , the model is aware of these attributes ( the latent space is partitioned among attributes ) and training algorithm is aware of the group nature ( semantic relations ) within the data , as it swap values in the latent space , as summarized by the training algorithm listed in the paper . Ours , has merits over the GAN-based approaches : Our method admits a simpler training pipeline : one learning rate , auto-encoder architecture with latent swap -- versus GAN , that require carefully choosing 2 learning rates : one for G one for D ; gradient clipping ; and overall provide no guarantees for convergence . Nonetheless , we show results for starGAN ( see comment on top , addressing all reviewers ) . 3.We are now setting up to run ELEGANT , but we wanted to respond sooner ( and we will respond again , once we have results on ELEGANT ) . However , we feel that ELEGANT will suffer from the same issue as starGAN , in that it will be good at local image transformations but not global ones ( i.e.might also fail in rotating an object , while maintaining semantic correctness ) . This is supported by their architecture ( see Figure 6 of ELEGANT ) : their attribute transfer takes the form of * * element-wise addition * * in the pixel space ( which works well for local edits ) . Nonetheless , it is * possible * for someone to train a GAN-based approach using GSL , however , we leave this as future work . As an initial contribution , we want to show that even simple NN architectures , such as autoencoders , suffice for our tasks when using GSL . 4.Table1 shows one way to evaluate disentanglement in the latent space . Good performance result in table1 shows that the information in the latent space is good enough information for discrimination across different attribute values : this does not imply good and realistic to image synthesis ( shown in figure 4,5 and 6 ) . To comment on this , we added the sentence in the text when we mention Table 1 : _Directly supervising an autoencoder on attributes helps in classifying them , but shows inferior synthesis performance_ . The reason why AE+DS performs well on table1 , because this exact descrimination is used in its training objective . 5.We now have conducted quantitative experiments , please see the comment addressing all reviewers , on top ."}, {"review_id": "8wqCDnBmnrT-2", "review_text": "I.Summary of paper . The authors propose GZS-Net , a model based on an auto-encoder that synthesizes realistic images by exploring the semantic relationships between group images . The model is trained with a family of objective functions expressed on groups of examples . It first maps training images into disentangled latent representations , which can be decomposed into multiple components , which in the specific implementation correspond to semantic attributes . Components are then recombined to form novel images that can correspond to previously unseen combinations of attributes . The main experimental setup considered is zero-shot image synthesis : having seen different combinations of attributes , the model relies on disentangling attribute representations to synthesize novel combinations of previously seen attributes . Good performance is achieved on the Fonts , ilab-20M and RaFD datasets . In addition , the authors perform a more detailed analysis of their proposed approach and show that it results in good downstream performance on an object recognition task . II.Strong and weak points . Positive : - The proposed task , zero-shot synthesis , is interesting . - Overall , the paper is well-written and clear - Good results against the selected baselines - The novelty of the approach itself . Negative : - The following justifies some of the comments below . The authors claim GAN-related training is unstable with significant mode collapse . This is simply not the case currently , and in particular in the relatively small datasets considered . A comparison with e.g.cycle-GAN is fully warranted . - The qualitative comparisons are limited . Yes , the model can generate good-looking images on what are mostly toy tasks . But these generations are commonly compared only to a relatively weak baseline ( AE + DS ) in fig 6 , 5 . Cycle GANs would work in this setting , and I supposed might be very competitive . - The paper suffers from a general lack of ( serious ) quantitative comparisons . The authors provide generated images but the single results table on disentanglement does not contain models that would fare well in this setting . III.Recommendation and justification . I feel the paper is borderline , with a slight inclination to reject . On one hand , an interesting task is presented and the writing/clarity/experiments make this submission an interesting submission from a scientific standpoint . On the other hand , a lot of experimental work is needed to show this approach offers a benefit compared to serious baselines . IV.Advice for author response . I like this paper conceptually and would be prepared to raise my score . I 'm interested in the following , if possible : - Is there a way for the authors to provide an indication of sample quality versus adversarial baselines ( cycleGAN , starGAN or some related method e.g . ) to see if results hold ? - On the disentanglement side , is there a way to extend Table 1 to show whether the model performs well when compared to models that are good at disentangling , e.g.beta-VAE family . If not feasible to include new experiments , please if possible at least discuss the issue wrt . baselines . V. Improving the paper . On the link between zero-shot learning , disentanglement , and generative models , I would suggest including [ 1 , 2 ] . To be clear , these are just suggestions , choosing to include them or not will have no impact on my rating . References [ 1 ] Sylvain , Tristan , Linda Petrini , and Devon Hjelm . `` Locality and Compositionality in Zero-Shot Learning . '' International Conference on Learning Representations . 2019 . [ 2 ] Higgins , Irina , et al . `` SCAN : Learning Hierarchical Compositional Visual Concepts . '' International Conference on Learning Representations . 2018.Edit : I have read the author 's response . They have responded to the questions I had , and they address many of the concerns I had . I am now raising Edit : updating score , and recommending acceptance as per my response to the author 's rebuttal .", "rating": "7: Good paper, accept", "reply_text": "# # Experiments We understand that your biggest concern , that you feel our work falls short , in terms of experimentation . Therefore , we ran more experiments : please view the comment addressing all reviewers . * Specifically , rather than cycleGAN which work well for transferring * one attribute * value : in our setting , we would have to train a pair of generator and discriminator for each attribute value pairs ( E.g. , we have 111 background attribute values for vehicles , it would require us to train thousands of GAN models ) . Therefore , we chose starGAN , its their authors show that it can be used to edit * multiple attributes * ( i.e. , more similar to ours ) . * We also ran Quantitative results as you requested . * We have extended table 1 For all of these results , please refer to the comment above , addressing all reviewers . # # References Thank you for the references , we will be adding them to the manuscript ."}, {"review_id": "8wqCDnBmnrT-3", "review_text": "Summary : This paper proposes Group-Supervised Learning ( GSL ) that can learn disentangled representations by swapping components in latent space , and enable one-shot novel view synthesis . They created large dataset to evaluate their method , and demonstrate its effectiveness in controllable image synthesis , and disentanglement , outperforming existing baselines . Pros : 1.This paper is good-written and clear to follow . The authors can demonstrate their idea well in Section 3 and 4 . 2.Swapping attributes is an interesting and novel idea to encourage disentangled representation learning , which are easy to implement and could have wide applications in many fields . 3.The experiments are extensive , and the results are able to support their claims . Cons : 1.In Equation ( 4 ) , can you explain the defination of separate losses , L_r , L_sr and L_csr along with the equation ? They are not mentioned above , but simply in the algorithm 1 . 2.If the cycle loss is removed , will the performance degrade dramatically ? Can you please offer ablations for each loss ?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "# # Cons * We now have clarified the meaning of those terms in the writeup , as some readers might prefer to not pay as much attention to the algorithm listing . After the equation defining $ \\mathcal { L } $ , we added the text : > where $ L_\\textrm { r } $ , $ L_\\textrm { sr } $ and $ L_\\textrm { csr } $ , respectively are the reconstruction , swap-reconstruction , and cycle swap reconstruction losses . * We are now conducting ablation experiments , thank you for the suggestion ! We have results for removing the cycle loss ( the term $ L_\\textrm { csr } $ ) . Removing the term , worsens quality ( photos look a little more blurry ) and achieves worse quantitative metrics -- in addition to the tables above , we have created this quantitative comparison on this ablation : | ilab-20M | GZS-Net ( without cycle loss ) $ \\ \\ $ | GZS-Net ( with cycle loss ) $ \\ \\ $ | | : | : :| : :| | Average MSE $ \\ \\ $ | 0.0073 |0.0046 | | Average PSNR $ \\ \\ $ | 21.55 | 23.55 | Training with only swap reconstruction loss shows much worse qualitative results , and we will update the table ( and finally , merge all tables into the paper , either in the appendix or in the main text , if we get a 9th page allowance with your acceptance recommendation ) . Note that removing _both_ swap reconstruction and cycle-swap reconstruction , would cast our method becomes to an autoencoder , and we have experiments on autoencoder in the paper ."}], "0": {"review_id": "8wqCDnBmnrT-0", "review_text": "Summary : The paper proposed a new training framework , namely GSL , for novel content synthesis . And GSL enables learning of disentangled representations of tangible attributes and achieve novel image synthesis by recombining those swappable components under a zero-shot setting . The framework leverages the underlying semantic links across samples which could be instantiated as a multigraph . Cycle-consistent reconstruction loss as well as reconstruction loss are computed on synthetic samples from swapped latent representations . Reasons for score : I vote for accepting . Overall , I think the paper is well written and the idea of GSL leveraging the underlying semantic relationships between samples to learn disentangled representations of tangible attributes is pretty novel and interesting . Experimental result on several real and synthetic datasets are also promising . Concerns : 1 ) How are hyperparameters { d_j } ^m_ { j=1 } selected ? It would be great if the authors could elaborate more about this . 2 ) Concerning the cycle attribute swap , how to guarantee the intermediate generated image to be reasonable instead of a trivial solution . Seems like no losses ( possibly like adversarial loss ) is directly added on the intermediate result .", "rating": "7: Good paper, accept", "reply_text": "1.Per-attribute latent dimensions were selected by some preliminary experiments , but they were mostly guided by intuition : for instance , we feel that the identity attribute for faces/vehicles should carry the most information and therefore be devoted the highest capacity . Nonetheless , this is an interesting question , so we have started running some experiments to change the dimension which we will add to supplementary material . 2.We ran ablation experiments for removing the cycle swap loss term ( please refer to the comment , addressing all reviewers , on top ) . Indeed , results are downgraded when the loss is removed . As we write in the text , the two swaps should guarantee that there is no information loss . Our intuition tells us that these swaps are not _trivial_ : if they were trivial ( e.g. , swapping latent values carrying no information ) , then we should not be able to reconstruct the inputs , especially that * * all * * latent values are eventually swapped ( for different example pairs ) -- therefore , no attribute latents carry zero information . We did not add adversarial loss on the intermediate result , however , it seems like a good idea and might improve the performance , especially for a small dataset . Nonetheless our method is currently fully supervised : it reduces the choices of hyperparameters , uses feed-forward auto-encoders ."}, "1": {"review_id": "8wqCDnBmnrT-1", "review_text": "This paper proposes a Group-Supervised learning method for zero-shot synthesis . This method can generate multi-attributes images through the GZS-Net . However , the authors may pay attention to the following questions : 1 . The authors should check for grammar mistakes , e.g. , \u201d Fonts is a ... \u201d - > \u201d Fonts are ... \u201d . 2.From the description , the Group-Supervised learning method is just a special data sampling manner used in training . Besides , the total framework is commonly used in GANs for attribute control generation . 3.The contrast methods are too simple . The reviewer thinks the authors should compare their method with some new works , e.g. , \u201c ELEGANT : Exchanging Latent Encodings with GAN for Transferring Multiple Face \u201d . 4.In Section 5.3 , the reasons for the results in Table 1 are not analyzed . Why is the accuracy of contrast method AE-DS higher than GZS-Net ? 5.The author should add more quantitative analysis . Such as comparing the FID/IS values of generated results using different methods .", "rating": "6: Marginally above acceptance threshold", "reply_text": "1 . * * Grammar * * : * Fonts * is a name of our dataset . We made it italic . We also corrected the other grammar mistakes that we observed . 2 . * * Our method is a Sampling method * * : You are right GSL samples examples in a special way ( that is aware of , which examples share attribute values ) , but * additionally and crucially * , the model is aware of these attributes ( the latent space is partitioned among attributes ) and training algorithm is aware of the group nature ( semantic relations ) within the data , as it swap values in the latent space , as summarized by the training algorithm listed in the paper . Ours , has merits over the GAN-based approaches : Our method admits a simpler training pipeline : one learning rate , auto-encoder architecture with latent swap -- versus GAN , that require carefully choosing 2 learning rates : one for G one for D ; gradient clipping ; and overall provide no guarantees for convergence . Nonetheless , we show results for starGAN ( see comment on top , addressing all reviewers ) . 3.We are now setting up to run ELEGANT , but we wanted to respond sooner ( and we will respond again , once we have results on ELEGANT ) . However , we feel that ELEGANT will suffer from the same issue as starGAN , in that it will be good at local image transformations but not global ones ( i.e.might also fail in rotating an object , while maintaining semantic correctness ) . This is supported by their architecture ( see Figure 6 of ELEGANT ) : their attribute transfer takes the form of * * element-wise addition * * in the pixel space ( which works well for local edits ) . Nonetheless , it is * possible * for someone to train a GAN-based approach using GSL , however , we leave this as future work . As an initial contribution , we want to show that even simple NN architectures , such as autoencoders , suffice for our tasks when using GSL . 4.Table1 shows one way to evaluate disentanglement in the latent space . Good performance result in table1 shows that the information in the latent space is good enough information for discrimination across different attribute values : this does not imply good and realistic to image synthesis ( shown in figure 4,5 and 6 ) . To comment on this , we added the sentence in the text when we mention Table 1 : _Directly supervising an autoencoder on attributes helps in classifying them , but shows inferior synthesis performance_ . The reason why AE+DS performs well on table1 , because this exact descrimination is used in its training objective . 5.We now have conducted quantitative experiments , please see the comment addressing all reviewers , on top ."}, "2": {"review_id": "8wqCDnBmnrT-2", "review_text": "I.Summary of paper . The authors propose GZS-Net , a model based on an auto-encoder that synthesizes realistic images by exploring the semantic relationships between group images . The model is trained with a family of objective functions expressed on groups of examples . It first maps training images into disentangled latent representations , which can be decomposed into multiple components , which in the specific implementation correspond to semantic attributes . Components are then recombined to form novel images that can correspond to previously unseen combinations of attributes . The main experimental setup considered is zero-shot image synthesis : having seen different combinations of attributes , the model relies on disentangling attribute representations to synthesize novel combinations of previously seen attributes . Good performance is achieved on the Fonts , ilab-20M and RaFD datasets . In addition , the authors perform a more detailed analysis of their proposed approach and show that it results in good downstream performance on an object recognition task . II.Strong and weak points . Positive : - The proposed task , zero-shot synthesis , is interesting . - Overall , the paper is well-written and clear - Good results against the selected baselines - The novelty of the approach itself . Negative : - The following justifies some of the comments below . The authors claim GAN-related training is unstable with significant mode collapse . This is simply not the case currently , and in particular in the relatively small datasets considered . A comparison with e.g.cycle-GAN is fully warranted . - The qualitative comparisons are limited . Yes , the model can generate good-looking images on what are mostly toy tasks . But these generations are commonly compared only to a relatively weak baseline ( AE + DS ) in fig 6 , 5 . Cycle GANs would work in this setting , and I supposed might be very competitive . - The paper suffers from a general lack of ( serious ) quantitative comparisons . The authors provide generated images but the single results table on disentanglement does not contain models that would fare well in this setting . III.Recommendation and justification . I feel the paper is borderline , with a slight inclination to reject . On one hand , an interesting task is presented and the writing/clarity/experiments make this submission an interesting submission from a scientific standpoint . On the other hand , a lot of experimental work is needed to show this approach offers a benefit compared to serious baselines . IV.Advice for author response . I like this paper conceptually and would be prepared to raise my score . I 'm interested in the following , if possible : - Is there a way for the authors to provide an indication of sample quality versus adversarial baselines ( cycleGAN , starGAN or some related method e.g . ) to see if results hold ? - On the disentanglement side , is there a way to extend Table 1 to show whether the model performs well when compared to models that are good at disentangling , e.g.beta-VAE family . If not feasible to include new experiments , please if possible at least discuss the issue wrt . baselines . V. Improving the paper . On the link between zero-shot learning , disentanglement , and generative models , I would suggest including [ 1 , 2 ] . To be clear , these are just suggestions , choosing to include them or not will have no impact on my rating . References [ 1 ] Sylvain , Tristan , Linda Petrini , and Devon Hjelm . `` Locality and Compositionality in Zero-Shot Learning . '' International Conference on Learning Representations . 2019 . [ 2 ] Higgins , Irina , et al . `` SCAN : Learning Hierarchical Compositional Visual Concepts . '' International Conference on Learning Representations . 2018.Edit : I have read the author 's response . They have responded to the questions I had , and they address many of the concerns I had . I am now raising Edit : updating score , and recommending acceptance as per my response to the author 's rebuttal .", "rating": "7: Good paper, accept", "reply_text": "# # Experiments We understand that your biggest concern , that you feel our work falls short , in terms of experimentation . Therefore , we ran more experiments : please view the comment addressing all reviewers . * Specifically , rather than cycleGAN which work well for transferring * one attribute * value : in our setting , we would have to train a pair of generator and discriminator for each attribute value pairs ( E.g. , we have 111 background attribute values for vehicles , it would require us to train thousands of GAN models ) . Therefore , we chose starGAN , its their authors show that it can be used to edit * multiple attributes * ( i.e. , more similar to ours ) . * We also ran Quantitative results as you requested . * We have extended table 1 For all of these results , please refer to the comment above , addressing all reviewers . # # References Thank you for the references , we will be adding them to the manuscript ."}, "3": {"review_id": "8wqCDnBmnrT-3", "review_text": "Summary : This paper proposes Group-Supervised Learning ( GSL ) that can learn disentangled representations by swapping components in latent space , and enable one-shot novel view synthesis . They created large dataset to evaluate their method , and demonstrate its effectiveness in controllable image synthesis , and disentanglement , outperforming existing baselines . Pros : 1.This paper is good-written and clear to follow . The authors can demonstrate their idea well in Section 3 and 4 . 2.Swapping attributes is an interesting and novel idea to encourage disentangled representation learning , which are easy to implement and could have wide applications in many fields . 3.The experiments are extensive , and the results are able to support their claims . Cons : 1.In Equation ( 4 ) , can you explain the defination of separate losses , L_r , L_sr and L_csr along with the equation ? They are not mentioned above , but simply in the algorithm 1 . 2.If the cycle loss is removed , will the performance degrade dramatically ? Can you please offer ablations for each loss ?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "# # Cons * We now have clarified the meaning of those terms in the writeup , as some readers might prefer to not pay as much attention to the algorithm listing . After the equation defining $ \\mathcal { L } $ , we added the text : > where $ L_\\textrm { r } $ , $ L_\\textrm { sr } $ and $ L_\\textrm { csr } $ , respectively are the reconstruction , swap-reconstruction , and cycle swap reconstruction losses . * We are now conducting ablation experiments , thank you for the suggestion ! We have results for removing the cycle loss ( the term $ L_\\textrm { csr } $ ) . Removing the term , worsens quality ( photos look a little more blurry ) and achieves worse quantitative metrics -- in addition to the tables above , we have created this quantitative comparison on this ablation : | ilab-20M | GZS-Net ( without cycle loss ) $ \\ \\ $ | GZS-Net ( with cycle loss ) $ \\ \\ $ | | : | : :| : :| | Average MSE $ \\ \\ $ | 0.0073 |0.0046 | | Average PSNR $ \\ \\ $ | 21.55 | 23.55 | Training with only swap reconstruction loss shows much worse qualitative results , and we will update the table ( and finally , merge all tables into the paper , either in the appendix or in the main text , if we get a 9th page allowance with your acceptance recommendation ) . Note that removing _both_ swap reconstruction and cycle-swap reconstruction , would cast our method becomes to an autoencoder , and we have experiments on autoencoder in the paper ."}}