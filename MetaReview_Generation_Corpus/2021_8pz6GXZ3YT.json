{"year": "2021", "forum": "8pz6GXZ3YT", "title": "Why Lottery Ticket Wins? A Theoretical Perspective of Sample Complexity on Sparse Neural Networks", "decision": "Reject", "meta_review": "Even though the authors revised the problem formulation, the paper seems not ready for publication. The assumptions are still too strong (The learning algorithm assumes knowledge of the sparsity mask). The proof technique also heavily relies on Zhong et al'17 without properly highlighting the difference. ", "reviews": [{"review_id": "8pz6GXZ3YT-0", "review_text": "Summary : This paper investigates the theoretical evidence behind improved generalization of the winning lottery tickets . More precisely authors characterize the testing error of a pruned network that is then trained using AGD . Under relatively reasonable assumptions , they manage to show an improved generalization bound for a properly pruned network over the full network . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # pros : This is one of the first works to provide theoretical guarantees for winning lottery tickets under the one-hidden-layer sparse neural network model . The objective function being highly non-convex in general , authors rely on local convexity of the latter objective function around the ground truth . Hence Given a good initialization , they managed to show that AGD achieves a good performance after a certain number of steps . Numerical experiments are interesting and complement pretty well the theory of the paper . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # cons/questions : * Line 6 page 5 , you say that the radius of the convex ball is $ \\Theta ( 1 ) - \\Theta ( \\sqrt { r } ) $ . How is that even positive as $ r $ grows ? The same comment also holds for your comment after Lemma 1 . * Using local convexity , which is a uniform property , you should be able to prove your main result based on the whole training data instead of sample splitting . I do n't see why you have only managed to show your result under sample splitting which is less interesting in my opinion . * The hypothesis of a sparse ground model where you are given the true corresponding support is too restrictive . It would have been very interesting to apply a variant of IHT to your proposed algorithm . I believe your results should hold under relatively similar conditions . I strongly think proving your result without prior knowledge of the true support would make the present paper stronger and would avoid having trivial follow-up works that may take more credit than yours . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Score : This paper is well written and the proofs seem sound to me . Overall , I think the present paper is marginally above the acceptance threshold because of the reasons I explain above . I am willing to revise my score if the authors give constructive feedback to my concerns . Comments : * In Theorem 7 , it would be good to precise that your initialization is independent from the data you use for your AGD .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the comments and suggestions . Here are some major revisions in the papers : $ \\bullet $ We have revised the problem formulation to clarify our setup . Our setup can be interpreted by a `` teacher-student '' mode , where the training data are generated from a teacher network , and we learn on a student network with the same number of neurons as the teacher network . We do not require the student network to have the same architecture as the teacher network . As long as the teacher network can be viewed as a sub-network of the student network by pruning weights , our method finds the ground-truth model with a sufficient number of samples . We study how the architecture of the student network affects the learning rate and sample complexity in this paper . $ \\bullet $ Following the reviewers comments , we added one section ( Section 4.1 ) to discuss the guarantee of obtaining a proper student network architecture from a fully connected dense network by magnitude pruning . The main idea is that after some iterations of learning on the dense network using Algorithm 1 , the weights that correspond to a non-existing edge in the teacher network becomes sufficient small ( in magnitude ) , then magnitude pruning removes edges correctly ."}, {"review_id": "8pz6GXZ3YT-1", "review_text": "The paper analyzes the geometric structure of the objective function for a sparse one hidden layer neural net , and has a novel theorem on the convergence rate of the algorithm that recovers the weights in the one hidden layer neural net . From the perspective of sample complexity , namely how many samples are needed to have the whole recovery of the weights in the neural net , it is established that a sparse one hidden layer neural net usually requires fewer samples than a fully connected counterpart . The paper uses many simulations to support the theoretical results . The sparse neural net can represent the winning ticket for the lottery ticket hypothesis , and the sample complexity explains why the winning ticket has better performance . The paper seems the first work to focus on the weights recovery of sparse neural networks and provides useful guarantees with important insights , and the paper should be distributed to other researchers . Although the paper has many contributions and should be worth distributed to other researchers , the paper appears to be just below the threshold for acceptance to the ICLR conference . Therefore , I am afraid I might not recommend to accept the paper in this current form . 1.Frankly , the technical novelty over the reference Zhong 2017 ( Recovery guarantees for one-hidden-layer neural networks ) seems not fully clarified . While it is accepted that this work has the sparsity $ r $ and proves a tighter bound than Zhong 2017 , it seems not fully clarified what technical novelty or key breakthrough are directly due to the sparsity setting in this paper , compared with the approach that Zhong 2017 took . The authors should highlight key technical differences in the proof due to the sparsity setting , to make it more convincing about the novelty of this paper . 2.The paper only explains why a sparse neural net should train well with fewer examples than fully connected counterpart . The observation has its own merit . To my understanding , the paper starts from the assumption that the neural net to be learned is sparse \u2013 however , this could not be always correct for the lottery ticket problem . Thus , the assumption seems not always plausible for real applications of the lottery ticket hypothesis , and it would be useful if there can be examples of sparse neural networks that come naturally for some problem . 3.Here is a question for the authors to confirm . Although the work is on sparse neural networks , the sparsity $ r < < d $ is not assumed in Theorem 1 and 2 , correct ? Even if $ r=d $ , we still have the theorems valid ? Please feel free to let me know if I do not understand well .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the comments and suggestions . Here are some major revisions in the papers : $ \\bullet $ We have revised the problem formulation to clarify our setup . Our setup can be interpreted by a `` teacher-student '' mode , where the training data are generated from a teacher network , and we learn on a student network with the same number of neurons as the teacher network . We do not require the student network to have the same architecture as the teacher network . As long as the teacher network can be viewed as a sub-network of the student network by pruning weights , our method finds the ground-truth model with a sufficient number of samples . We study how the architecture of the student network affects the learning rate and sample complexity in this paper . $ \\bullet $ Following the reviewers comments , we added one section ( Section 4.1 ) to discuss the guarantee of obtaining a proper student network architecture from a fully connected dense network by magnitude pruning . The main idea is that after some iterations of learning on the dense network using Algorithm 1 , the weights that correspond to a non-existing edge in the teacher network becomes sufficient small ( in magnitude ) , then magnitude pruning removes edges correctly . The point to point responses to the comments are summarized in the following : $ \\bullet $ Q1 : The technical novelty over the reference Zhong 2017 ( Recovery guarantees for one-hidden-layer neural networks ) seems not fully clarified . $ \\bullet $ A1 : Thanks for the comments , and we are sorry that the differences with Zhong etal'17 were not well presented . The main differences of our proof from Zhong et al'17 lie in two aspects : First , each neuron weights $ w_j $ connects to different subset of $ x $ instead of fixed $ x $ . Hence , the concentration theorem can not be directly applied in this paper to bound the distance between population and empirical risk function . Second , we need to replace $ x $ with $ \\frac { 1 } { \\sqrt { K } } \\sum_ { j=1 } ^K x_j $ for the tensor initialization since $ w_j $ is no longer respect with $ x $ . Although the follow the road map of the proof in Zhong et al'17 , the proof techniques to guarantee the lemmas still hold are totally different because of the difference we described above . We have added a separate paragraph after Theorem 2 to clarify the differences with Zhong et al'17 . $ \\bullet $ Q2 : To my understanding , the paper starts from the assumption that the neural net to be learned is sparse - however , this could not be always correct for the lottery ticket problem . Thus , the assumption seems not always plausible for real applications of the lottery ticket hypothesis , and it would be useful if there can be examples of sparse neural networks that come naturally for some problem . $ \\bullet $ A2 : Thanks for the comments . We do not require the learned neural net to be sparse , and we are sorry our original statement was confusing . First , we need to clarify that we consider a teacher-student setup where the data are generated from a teacher network with $ r^ * $ weights per neuron . We learn on a student network with $ r $ weights per neuron . We only assume $ r^ * \\leq r \\leq d $ throughout the paper , and $ r^ * $ and $ r $ can take the entire spectrum of weight density , from sparse to dense network . Thus , our formulation is associated with a quite general case . We removed the statement about sparsity that could lead to confusion . Second , the student network does not need to be the same as the teacher network . As long as the teacher network is a subnetwork of the student network , our results in Theorems 1 and 2 applies . In other words , we do not assume knowing the mask of the teacher network exactly . Third , our work provides an explicit characterization of the impact of pruning on the sample complexity and the learning rate . The sample complexity decreases , and the learning becomes faster when $ r $ in the student network decreases . If the teacher network has a small $ r^ * $ , one can find a proper student network ( using our approach in the new added Section 4.1 ) with a small $ r $ such that the learning performance on the student network is much improved over training on a fully connected network . $ \\bullet $ Q3 : Although the work is on sparse neural networks , the sparsity is not assumed in Theorem 1 and 2 , correct ? $ \\bullet $ A3 : Thanks for bringing up the confusion . As our response in Q2 , the sparsity is not required throughout the paper , and the theorems hold for any $ r\\le d $ . We apologize for the misunderstanding sentence in the original paper ."}, {"review_id": "8pz6GXZ3YT-2", "review_text": "Summary of review : This paper provides recovery guarantees for learning one-hidden-layer neural networks with sparse ground truth weights , given an isotropic Gaussian input distribution . The main result shows local convexity guarantees near the ground truth . Provided that a mask of the sparsity pattern is already known , this paper extends the tensor initialization approach of Zhong et al'17 to show a convergence guarantee for learning the sparse neural network . Simulations validate the local Setting : This paper focuses on learning a one-hidden-layer neural network , where the weight matrix of the hidden layer is sparse , given input samples from an isotropic Gaussian distribution . Results : ( i ) The first result is that within a small vicinity of the ground truth weight matrix , the standard mean squared loss for learning the neural network is convex . ( ii ) The second result shows how to learn the ground truth weight matrix , by extending the tensor initialization approach in Zhong et al'17 . ( iii ) Numerical results are provided to validate the above two theoretical results . Pros : - The sample size requirement of both result ( i ) and ( ii ) growly proportionally to the sparsity of the ground truth matrix , as opposed to the size of the matrix . This result is particularly interesting in light of recent empirical results about network pruning and learning sparse ConvNets ( Neyshabur'20 ) . Cons : - The authors prove the above results by adapting the proof of Zhong et al'17 . In fact , since the input distribution is isotropic , standard concentration results apply whether or not the ground truth matrix is sparse . Therefore , it is unclear to the reviewer whether this result is as novel as the authors claim in the introduction . - The learning algorithm assumes knowledge of the sparsity mask . This seems like a strong assumption . Is n't the point of IMP to find this sparsity mask ? Understanding how to find this sparsity mask seems like a more important question , but this is not discussed at all in this paper . Writing : Overall , this paper is easy to follow . The quality of writing is marginally acceptable . Please find several detailed comments below . - P1 , `` the theoretical justification of winning tickets are remains elusive expect for a few recent works '' - > remove `` are '' , replace `` expect '' with except - P4 : `` an one-hidden-layer neural network '' - > a one-hidden-layer neural network - P5 : here you say that $ \\varepsilon_1 = \\Theta ( \\sqrt r ) $ , but $ r > 1 $ and $ \\varepsilon_1 < 1 $ . Please clarify . - P5 : regarding the convergence for the vanilla GD algorithm . Please add a reference to this claim . - P5 : `` accurate estimate '' - > replace `` estimate '' with estimation", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the comments and suggestions . Here are some major revisions in the papers : $ \\bullet $ We have revised the problem formulation to clarify our setup . Our setup can be interpreted by a `` teacher-student '' mode , where the training data are generated from a teacher network , and we learn on a student network with the same number of neurons as the teacher network . We do not require the student network to have the same architecture as the teacher network . As long as the teacher network can be viewed as a sub-network of the student network by pruning weights , our method finds the ground-truth model with a sufficient number of samples . We study how the architecture of the student network affects the learning rate and sample complexity in this paper . $ \\bullet $ Following the reviewers comments , we added one section ( Section 4.1 ) to discuss the guarantee of obtaining a proper student network architecture from a fully connected dense network by magnitude pruning . The main idea is that after some iterations of learning on the dense network using Algorithm 1 , the weights that correspond to a non-existing edge in the teacher network becomes sufficient small ( in magnitude ) , then magnitude pruning removes edges correctly . The point to point responses to the comments are summarized in the following : $ \\bullet $ Q1 : The authors prove the above results by adapting the proof of Zhong et al'17 . In fact , since the input distribution is isotropic , standard concentration results apply whether or not the ground truth matrix is sparse . Therefore , it is unclear to the reviewer whether this result is as novel as the authors claim in the introduction . $ \\bullet $ A1 : Thanks for the comments , and we are sorry that the differences with Zhong etal'17 were not well presented . However , we do not think the statement `` since the input distribution is isotropic , standard concentration results apply whether or not the ground truth matrix is sparse '' is correct . The main differences of our proof from Zhong et al'17 lie in two aspects : First , each neuron weights $ w_j $ connects to different subset of $ x $ instead of fixed $ x $ . Hence , the concentration theorem can not be directly applied in this paper to bound the distance between population and empirical risk function . Second , we need to replace $ x $ with $ \\frac { 1 } { \\sqrt { K } } \\sum_ { j=1 } ^K x_j $ for the tensor initialization since $ w_j $ is no longer respect with $ x $ . % We do not deny that the proof shares similarity with Zhong et al 's in presenting theorems and lemmas because the roadmap of this paper follows exactly the same as that in Zhong et al 's . However , Although the follow the road map of the proof in Zhong et al'17 , the proof techniques to guarantee the lemmas still hold are totally different because of the difference we described above . We have added a separate paragraph after Theorem 2 to clarify the differences with Zhong et al'17 . $ \\bullet $ Q2 : The learning algorithm assumes knowledge of the sparsity mask . This seems like a strong assumption . Is n't the point of IMP to find this sparsity mask ? Understanding how to find this sparsity mask seems like a more important question , but this is not discussed at all in this paper . $ \\bullet $ A2 : Thanks for bringing the question . We realize that our setup was not discussed accurately and thus may lead to confusions . We need to clarify that our algorithm does not need to know the mask accurately . Moreover , we added a subsection 4.1 to discuss the guarantee to find a mask that works for our algorithm using magnitude pruning from a dense networks . Details as as follows . First , we need to clarify that we consider a teacher-student setup where the data are generated from a teacher network with $ r^ * $ weights per neuron . We learn on a student network with $ r $ weights per neuron . We only assume $ r^ * \\leq r \\leq d $ throughout the paper , and $ r^ * $ and $ r $ can take the entire spectrum of weight density , from sparse to dense network . Thus , our formulation is associated with a quite general case . We removed the statement about sparsity that could lead to confusion . Second , the student network does not need to be the same as the teacher network . As long as the teacher network is a subnetwork of the student network , our results in Theorems 1 and 2 applies . In other words , we do not assume knowing the mask of the teacher network exactly . Third , we added a subsection 4.1 to discuss how to obtain a proper student network from a fully connected neural network using our Algorithm 1 and magnitude pruning . We can train on a fully connected neural network using tensor initialization and iterate with $ T_1 $ iterations , then apply magnitude pruning to prune the network . We show that the teacher network is indeed a subnetwork of the resulting pruned network . Then this pruned network can be used as the student network discussed in this paper . Please see section 4.1 in the paper for details ."}, {"review_id": "8pz6GXZ3YT-3", "review_text": "This paper studies the lottery ticket hypothesis , which says that there is an underlying sub-network ( lottery ticket ) in a neural network such that if we train it , we will obtain a better test accuracy compared to the original network . The authors develop a theoretical validation of the improved generalization error of the lottery ticket . Similar to many theoretical results for neural networks , several assumptions have been made . For example , it is assumed that the underlying function , which we try to learn , can be entirely captured by a sparse neural network . This assumption indicates that for some underlying sub-graph , we can learn with zero generalization error . They provide empirical validation for their results as well . The authors consider a significant problem . Pruning techniques allow us to enjoy high accuracy while reducing neural networks ' memory and computational running time . These techniques have been studied for a long time , but mostly from an empirical perspective . I believe understanding the pruning problem 's theoretical aspects will help us develop better algorithms for pruning as well . The results are clearly explained , and the paper is well-written . Comments : In empirical cases , the pruned network 's initialization is often set to the weights obtained in the training of the more extensive network . Is this technique used in your comparisons in the numerical evaluation ? It is assumed that the covariates x_i 's are coming from a Gaussian distribution , implying that the label y_i is a mixture of truncated Gaussians . ( before adding noise ) . While this assumption has been made in previous works , the role of this assumption is not well-explained . I am not sure whether the underlying assumption trivialized the problem . It is assumed that the function f , which we try to learn , can be captured by a sparse one-hidden-layer neural network . This sparse sub-network would essentially be the lottery-ticket of the fully connected network . Then it is claimed that if we know the structure of this sparse sub-network ( i.e. , known mask matrix M ) , then it means we are learning in a lower dimension of parameters compared to the case of a complete network . Thus , it is not surprising to see that the SGD algorithm converges faster . In other words , by knowing M , we already set a large portion of parameters to their optimal values . Thus , it is easier to find the optimal solution to the minimization problem . The ultimate goal of the pruning problem is to get the sub-structure and train it efficiently . Does your result affect how one could potentially improve the current pruning techniques ( such as IMP ) ? Or can we say anything about the algorithm 's performance when a large fraction of the sub-network is picked correctly ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the comments and suggestions . Here are some major revisions in the papers : $ \\bullet $ We have revised the problem formulation to clarify our setup . Our setup can be interpreted by a `` teacher-student '' mode , where the training data are generated from a teacher network , and we learn on a student network with the same number of neurons as the teacher network . We do not require the student network to have the same architecture as the teacher network . As long as the teacher network can be viewed as a sub-network of the student network by pruning weights , our method finds the ground-truth model with a sufficient number of samples . We study how the architecture of the student network affects the learning rate and sample complexity in this paper . $ \\bullet $ Following the reviewers comments , we added one section ( Section 4.1 ) to discuss the guarantee of obtaining a proper student network architecture from a fully connected dense network by magnitude pruning . The main idea is that after some iterations of learning on the dense network using Algorithm 1 , the weights that correspond to a non-existing edge in the teacher network becomes sufficient small ( in magnitude ) , then magnitude pruning removes edges correctly . The pointwise responses to the comments are summarized in the following : $ \\bullet $ Q1 : In empirical cases , the pruned network 's initialization is often set to the weights obtained in the training of the more extensive network . Is this technique used in your comparisons in the numerical evaluation ? $ \\bullet $ A1 : In our numerical evaluation , we set the pruned network 's initialization in the following two methods . One way is that the initial point is obtained by truncating the iterate trained in a more extensive network as you said . The other way is to set it as a random point near the ground truth . To be specific , when conducting the experiments as shown in Figures 6 and 7 , the initialization is set by truncating the smallest entries of the weight matrix that obtained in training the more extensive network , and the numerical results suggest that our algorithm works properly . For Figures 3-5 we directly initialize randomly in a region around the ground truth . This reduces the computational time . For example , to compute the sample complexity in Figure 4 , we need to test many pairs of $ n $ and $ d $ . The convergence rate in Figure 2 and the estimated model error in Figure 4 do not depend on the exact initiation as long as it is in the local convex region . $ \\bullet $ Q2 : It is assumed that the covariates $ x_i $ 's are coming from a Gaussian distribution , implying that the label $ y_i $ is a mixture of truncated Gaussians . ( before adding noise ) . While this assumption has been made in previous works , the role of this assumption is not well-explained . $ \\bullet $ A2 : Thanks for your comments , and we apologize that the rules of the assumption were missed in the original version . Overall , the assumption is critical in most part of the theoretical analysis . A major part of the proof is to bound the population risk function , which is an expectation over the distribution of input data . Hence , to guarantee the population risk function analyzable , the distribution of input can not be arbitrary . In addition , Gaussian distribution guarantee a nice landscape of the population risk function , i.e. , the local convexity region is large enough . Another part is for tensor initialization . The major idea for tensor initialization is to utilize the high-order momentum to characterize the angle and the magnitude of the weights vector $ w_j $ 's . Gaussian distribution or at least rotation invariant distribution is necessary to separate the information of the angle and the magnitude , which simplifies the calculation . The analysis might be extended to other distributions such as non-standard Gaussian or Gaussian mixture models , and we will leave that for future work . In the paper , we have added some intuitive discussion for the necessity of Gaussian assumption in the footnote 3 of page 4 ."}], "0": {"review_id": "8pz6GXZ3YT-0", "review_text": "Summary : This paper investigates the theoretical evidence behind improved generalization of the winning lottery tickets . More precisely authors characterize the testing error of a pruned network that is then trained using AGD . Under relatively reasonable assumptions , they manage to show an improved generalization bound for a properly pruned network over the full network . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # pros : This is one of the first works to provide theoretical guarantees for winning lottery tickets under the one-hidden-layer sparse neural network model . The objective function being highly non-convex in general , authors rely on local convexity of the latter objective function around the ground truth . Hence Given a good initialization , they managed to show that AGD achieves a good performance after a certain number of steps . Numerical experiments are interesting and complement pretty well the theory of the paper . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # cons/questions : * Line 6 page 5 , you say that the radius of the convex ball is $ \\Theta ( 1 ) - \\Theta ( \\sqrt { r } ) $ . How is that even positive as $ r $ grows ? The same comment also holds for your comment after Lemma 1 . * Using local convexity , which is a uniform property , you should be able to prove your main result based on the whole training data instead of sample splitting . I do n't see why you have only managed to show your result under sample splitting which is less interesting in my opinion . * The hypothesis of a sparse ground model where you are given the true corresponding support is too restrictive . It would have been very interesting to apply a variant of IHT to your proposed algorithm . I believe your results should hold under relatively similar conditions . I strongly think proving your result without prior knowledge of the true support would make the present paper stronger and would avoid having trivial follow-up works that may take more credit than yours . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Score : This paper is well written and the proofs seem sound to me . Overall , I think the present paper is marginally above the acceptance threshold because of the reasons I explain above . I am willing to revise my score if the authors give constructive feedback to my concerns . Comments : * In Theorem 7 , it would be good to precise that your initialization is independent from the data you use for your AGD .", "rating": "7: Good paper, accept", "reply_text": "Thank you for the comments and suggestions . Here are some major revisions in the papers : $ \\bullet $ We have revised the problem formulation to clarify our setup . Our setup can be interpreted by a `` teacher-student '' mode , where the training data are generated from a teacher network , and we learn on a student network with the same number of neurons as the teacher network . We do not require the student network to have the same architecture as the teacher network . As long as the teacher network can be viewed as a sub-network of the student network by pruning weights , our method finds the ground-truth model with a sufficient number of samples . We study how the architecture of the student network affects the learning rate and sample complexity in this paper . $ \\bullet $ Following the reviewers comments , we added one section ( Section 4.1 ) to discuss the guarantee of obtaining a proper student network architecture from a fully connected dense network by magnitude pruning . The main idea is that after some iterations of learning on the dense network using Algorithm 1 , the weights that correspond to a non-existing edge in the teacher network becomes sufficient small ( in magnitude ) , then magnitude pruning removes edges correctly ."}, "1": {"review_id": "8pz6GXZ3YT-1", "review_text": "The paper analyzes the geometric structure of the objective function for a sparse one hidden layer neural net , and has a novel theorem on the convergence rate of the algorithm that recovers the weights in the one hidden layer neural net . From the perspective of sample complexity , namely how many samples are needed to have the whole recovery of the weights in the neural net , it is established that a sparse one hidden layer neural net usually requires fewer samples than a fully connected counterpart . The paper uses many simulations to support the theoretical results . The sparse neural net can represent the winning ticket for the lottery ticket hypothesis , and the sample complexity explains why the winning ticket has better performance . The paper seems the first work to focus on the weights recovery of sparse neural networks and provides useful guarantees with important insights , and the paper should be distributed to other researchers . Although the paper has many contributions and should be worth distributed to other researchers , the paper appears to be just below the threshold for acceptance to the ICLR conference . Therefore , I am afraid I might not recommend to accept the paper in this current form . 1.Frankly , the technical novelty over the reference Zhong 2017 ( Recovery guarantees for one-hidden-layer neural networks ) seems not fully clarified . While it is accepted that this work has the sparsity $ r $ and proves a tighter bound than Zhong 2017 , it seems not fully clarified what technical novelty or key breakthrough are directly due to the sparsity setting in this paper , compared with the approach that Zhong 2017 took . The authors should highlight key technical differences in the proof due to the sparsity setting , to make it more convincing about the novelty of this paper . 2.The paper only explains why a sparse neural net should train well with fewer examples than fully connected counterpart . The observation has its own merit . To my understanding , the paper starts from the assumption that the neural net to be learned is sparse \u2013 however , this could not be always correct for the lottery ticket problem . Thus , the assumption seems not always plausible for real applications of the lottery ticket hypothesis , and it would be useful if there can be examples of sparse neural networks that come naturally for some problem . 3.Here is a question for the authors to confirm . Although the work is on sparse neural networks , the sparsity $ r < < d $ is not assumed in Theorem 1 and 2 , correct ? Even if $ r=d $ , we still have the theorems valid ? Please feel free to let me know if I do not understand well .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the comments and suggestions . Here are some major revisions in the papers : $ \\bullet $ We have revised the problem formulation to clarify our setup . Our setup can be interpreted by a `` teacher-student '' mode , where the training data are generated from a teacher network , and we learn on a student network with the same number of neurons as the teacher network . We do not require the student network to have the same architecture as the teacher network . As long as the teacher network can be viewed as a sub-network of the student network by pruning weights , our method finds the ground-truth model with a sufficient number of samples . We study how the architecture of the student network affects the learning rate and sample complexity in this paper . $ \\bullet $ Following the reviewers comments , we added one section ( Section 4.1 ) to discuss the guarantee of obtaining a proper student network architecture from a fully connected dense network by magnitude pruning . The main idea is that after some iterations of learning on the dense network using Algorithm 1 , the weights that correspond to a non-existing edge in the teacher network becomes sufficient small ( in magnitude ) , then magnitude pruning removes edges correctly . The point to point responses to the comments are summarized in the following : $ \\bullet $ Q1 : The technical novelty over the reference Zhong 2017 ( Recovery guarantees for one-hidden-layer neural networks ) seems not fully clarified . $ \\bullet $ A1 : Thanks for the comments , and we are sorry that the differences with Zhong etal'17 were not well presented . The main differences of our proof from Zhong et al'17 lie in two aspects : First , each neuron weights $ w_j $ connects to different subset of $ x $ instead of fixed $ x $ . Hence , the concentration theorem can not be directly applied in this paper to bound the distance between population and empirical risk function . Second , we need to replace $ x $ with $ \\frac { 1 } { \\sqrt { K } } \\sum_ { j=1 } ^K x_j $ for the tensor initialization since $ w_j $ is no longer respect with $ x $ . Although the follow the road map of the proof in Zhong et al'17 , the proof techniques to guarantee the lemmas still hold are totally different because of the difference we described above . We have added a separate paragraph after Theorem 2 to clarify the differences with Zhong et al'17 . $ \\bullet $ Q2 : To my understanding , the paper starts from the assumption that the neural net to be learned is sparse - however , this could not be always correct for the lottery ticket problem . Thus , the assumption seems not always plausible for real applications of the lottery ticket hypothesis , and it would be useful if there can be examples of sparse neural networks that come naturally for some problem . $ \\bullet $ A2 : Thanks for the comments . We do not require the learned neural net to be sparse , and we are sorry our original statement was confusing . First , we need to clarify that we consider a teacher-student setup where the data are generated from a teacher network with $ r^ * $ weights per neuron . We learn on a student network with $ r $ weights per neuron . We only assume $ r^ * \\leq r \\leq d $ throughout the paper , and $ r^ * $ and $ r $ can take the entire spectrum of weight density , from sparse to dense network . Thus , our formulation is associated with a quite general case . We removed the statement about sparsity that could lead to confusion . Second , the student network does not need to be the same as the teacher network . As long as the teacher network is a subnetwork of the student network , our results in Theorems 1 and 2 applies . In other words , we do not assume knowing the mask of the teacher network exactly . Third , our work provides an explicit characterization of the impact of pruning on the sample complexity and the learning rate . The sample complexity decreases , and the learning becomes faster when $ r $ in the student network decreases . If the teacher network has a small $ r^ * $ , one can find a proper student network ( using our approach in the new added Section 4.1 ) with a small $ r $ such that the learning performance on the student network is much improved over training on a fully connected network . $ \\bullet $ Q3 : Although the work is on sparse neural networks , the sparsity is not assumed in Theorem 1 and 2 , correct ? $ \\bullet $ A3 : Thanks for bringing up the confusion . As our response in Q2 , the sparsity is not required throughout the paper , and the theorems hold for any $ r\\le d $ . We apologize for the misunderstanding sentence in the original paper ."}, "2": {"review_id": "8pz6GXZ3YT-2", "review_text": "Summary of review : This paper provides recovery guarantees for learning one-hidden-layer neural networks with sparse ground truth weights , given an isotropic Gaussian input distribution . The main result shows local convexity guarantees near the ground truth . Provided that a mask of the sparsity pattern is already known , this paper extends the tensor initialization approach of Zhong et al'17 to show a convergence guarantee for learning the sparse neural network . Simulations validate the local Setting : This paper focuses on learning a one-hidden-layer neural network , where the weight matrix of the hidden layer is sparse , given input samples from an isotropic Gaussian distribution . Results : ( i ) The first result is that within a small vicinity of the ground truth weight matrix , the standard mean squared loss for learning the neural network is convex . ( ii ) The second result shows how to learn the ground truth weight matrix , by extending the tensor initialization approach in Zhong et al'17 . ( iii ) Numerical results are provided to validate the above two theoretical results . Pros : - The sample size requirement of both result ( i ) and ( ii ) growly proportionally to the sparsity of the ground truth matrix , as opposed to the size of the matrix . This result is particularly interesting in light of recent empirical results about network pruning and learning sparse ConvNets ( Neyshabur'20 ) . Cons : - The authors prove the above results by adapting the proof of Zhong et al'17 . In fact , since the input distribution is isotropic , standard concentration results apply whether or not the ground truth matrix is sparse . Therefore , it is unclear to the reviewer whether this result is as novel as the authors claim in the introduction . - The learning algorithm assumes knowledge of the sparsity mask . This seems like a strong assumption . Is n't the point of IMP to find this sparsity mask ? Understanding how to find this sparsity mask seems like a more important question , but this is not discussed at all in this paper . Writing : Overall , this paper is easy to follow . The quality of writing is marginally acceptable . Please find several detailed comments below . - P1 , `` the theoretical justification of winning tickets are remains elusive expect for a few recent works '' - > remove `` are '' , replace `` expect '' with except - P4 : `` an one-hidden-layer neural network '' - > a one-hidden-layer neural network - P5 : here you say that $ \\varepsilon_1 = \\Theta ( \\sqrt r ) $ , but $ r > 1 $ and $ \\varepsilon_1 < 1 $ . Please clarify . - P5 : regarding the convergence for the vanilla GD algorithm . Please add a reference to this claim . - P5 : `` accurate estimate '' - > replace `` estimate '' with estimation", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the comments and suggestions . Here are some major revisions in the papers : $ \\bullet $ We have revised the problem formulation to clarify our setup . Our setup can be interpreted by a `` teacher-student '' mode , where the training data are generated from a teacher network , and we learn on a student network with the same number of neurons as the teacher network . We do not require the student network to have the same architecture as the teacher network . As long as the teacher network can be viewed as a sub-network of the student network by pruning weights , our method finds the ground-truth model with a sufficient number of samples . We study how the architecture of the student network affects the learning rate and sample complexity in this paper . $ \\bullet $ Following the reviewers comments , we added one section ( Section 4.1 ) to discuss the guarantee of obtaining a proper student network architecture from a fully connected dense network by magnitude pruning . The main idea is that after some iterations of learning on the dense network using Algorithm 1 , the weights that correspond to a non-existing edge in the teacher network becomes sufficient small ( in magnitude ) , then magnitude pruning removes edges correctly . The point to point responses to the comments are summarized in the following : $ \\bullet $ Q1 : The authors prove the above results by adapting the proof of Zhong et al'17 . In fact , since the input distribution is isotropic , standard concentration results apply whether or not the ground truth matrix is sparse . Therefore , it is unclear to the reviewer whether this result is as novel as the authors claim in the introduction . $ \\bullet $ A1 : Thanks for the comments , and we are sorry that the differences with Zhong etal'17 were not well presented . However , we do not think the statement `` since the input distribution is isotropic , standard concentration results apply whether or not the ground truth matrix is sparse '' is correct . The main differences of our proof from Zhong et al'17 lie in two aspects : First , each neuron weights $ w_j $ connects to different subset of $ x $ instead of fixed $ x $ . Hence , the concentration theorem can not be directly applied in this paper to bound the distance between population and empirical risk function . Second , we need to replace $ x $ with $ \\frac { 1 } { \\sqrt { K } } \\sum_ { j=1 } ^K x_j $ for the tensor initialization since $ w_j $ is no longer respect with $ x $ . % We do not deny that the proof shares similarity with Zhong et al 's in presenting theorems and lemmas because the roadmap of this paper follows exactly the same as that in Zhong et al 's . However , Although the follow the road map of the proof in Zhong et al'17 , the proof techniques to guarantee the lemmas still hold are totally different because of the difference we described above . We have added a separate paragraph after Theorem 2 to clarify the differences with Zhong et al'17 . $ \\bullet $ Q2 : The learning algorithm assumes knowledge of the sparsity mask . This seems like a strong assumption . Is n't the point of IMP to find this sparsity mask ? Understanding how to find this sparsity mask seems like a more important question , but this is not discussed at all in this paper . $ \\bullet $ A2 : Thanks for bringing the question . We realize that our setup was not discussed accurately and thus may lead to confusions . We need to clarify that our algorithm does not need to know the mask accurately . Moreover , we added a subsection 4.1 to discuss the guarantee to find a mask that works for our algorithm using magnitude pruning from a dense networks . Details as as follows . First , we need to clarify that we consider a teacher-student setup where the data are generated from a teacher network with $ r^ * $ weights per neuron . We learn on a student network with $ r $ weights per neuron . We only assume $ r^ * \\leq r \\leq d $ throughout the paper , and $ r^ * $ and $ r $ can take the entire spectrum of weight density , from sparse to dense network . Thus , our formulation is associated with a quite general case . We removed the statement about sparsity that could lead to confusion . Second , the student network does not need to be the same as the teacher network . As long as the teacher network is a subnetwork of the student network , our results in Theorems 1 and 2 applies . In other words , we do not assume knowing the mask of the teacher network exactly . Third , we added a subsection 4.1 to discuss how to obtain a proper student network from a fully connected neural network using our Algorithm 1 and magnitude pruning . We can train on a fully connected neural network using tensor initialization and iterate with $ T_1 $ iterations , then apply magnitude pruning to prune the network . We show that the teacher network is indeed a subnetwork of the resulting pruned network . Then this pruned network can be used as the student network discussed in this paper . Please see section 4.1 in the paper for details ."}, "3": {"review_id": "8pz6GXZ3YT-3", "review_text": "This paper studies the lottery ticket hypothesis , which says that there is an underlying sub-network ( lottery ticket ) in a neural network such that if we train it , we will obtain a better test accuracy compared to the original network . The authors develop a theoretical validation of the improved generalization error of the lottery ticket . Similar to many theoretical results for neural networks , several assumptions have been made . For example , it is assumed that the underlying function , which we try to learn , can be entirely captured by a sparse neural network . This assumption indicates that for some underlying sub-graph , we can learn with zero generalization error . They provide empirical validation for their results as well . The authors consider a significant problem . Pruning techniques allow us to enjoy high accuracy while reducing neural networks ' memory and computational running time . These techniques have been studied for a long time , but mostly from an empirical perspective . I believe understanding the pruning problem 's theoretical aspects will help us develop better algorithms for pruning as well . The results are clearly explained , and the paper is well-written . Comments : In empirical cases , the pruned network 's initialization is often set to the weights obtained in the training of the more extensive network . Is this technique used in your comparisons in the numerical evaluation ? It is assumed that the covariates x_i 's are coming from a Gaussian distribution , implying that the label y_i is a mixture of truncated Gaussians . ( before adding noise ) . While this assumption has been made in previous works , the role of this assumption is not well-explained . I am not sure whether the underlying assumption trivialized the problem . It is assumed that the function f , which we try to learn , can be captured by a sparse one-hidden-layer neural network . This sparse sub-network would essentially be the lottery-ticket of the fully connected network . Then it is claimed that if we know the structure of this sparse sub-network ( i.e. , known mask matrix M ) , then it means we are learning in a lower dimension of parameters compared to the case of a complete network . Thus , it is not surprising to see that the SGD algorithm converges faster . In other words , by knowing M , we already set a large portion of parameters to their optimal values . Thus , it is easier to find the optimal solution to the minimization problem . The ultimate goal of the pruning problem is to get the sub-structure and train it efficiently . Does your result affect how one could potentially improve the current pruning techniques ( such as IMP ) ? Or can we say anything about the algorithm 's performance when a large fraction of the sub-network is picked correctly ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the comments and suggestions . Here are some major revisions in the papers : $ \\bullet $ We have revised the problem formulation to clarify our setup . Our setup can be interpreted by a `` teacher-student '' mode , where the training data are generated from a teacher network , and we learn on a student network with the same number of neurons as the teacher network . We do not require the student network to have the same architecture as the teacher network . As long as the teacher network can be viewed as a sub-network of the student network by pruning weights , our method finds the ground-truth model with a sufficient number of samples . We study how the architecture of the student network affects the learning rate and sample complexity in this paper . $ \\bullet $ Following the reviewers comments , we added one section ( Section 4.1 ) to discuss the guarantee of obtaining a proper student network architecture from a fully connected dense network by magnitude pruning . The main idea is that after some iterations of learning on the dense network using Algorithm 1 , the weights that correspond to a non-existing edge in the teacher network becomes sufficient small ( in magnitude ) , then magnitude pruning removes edges correctly . The pointwise responses to the comments are summarized in the following : $ \\bullet $ Q1 : In empirical cases , the pruned network 's initialization is often set to the weights obtained in the training of the more extensive network . Is this technique used in your comparisons in the numerical evaluation ? $ \\bullet $ A1 : In our numerical evaluation , we set the pruned network 's initialization in the following two methods . One way is that the initial point is obtained by truncating the iterate trained in a more extensive network as you said . The other way is to set it as a random point near the ground truth . To be specific , when conducting the experiments as shown in Figures 6 and 7 , the initialization is set by truncating the smallest entries of the weight matrix that obtained in training the more extensive network , and the numerical results suggest that our algorithm works properly . For Figures 3-5 we directly initialize randomly in a region around the ground truth . This reduces the computational time . For example , to compute the sample complexity in Figure 4 , we need to test many pairs of $ n $ and $ d $ . The convergence rate in Figure 2 and the estimated model error in Figure 4 do not depend on the exact initiation as long as it is in the local convex region . $ \\bullet $ Q2 : It is assumed that the covariates $ x_i $ 's are coming from a Gaussian distribution , implying that the label $ y_i $ is a mixture of truncated Gaussians . ( before adding noise ) . While this assumption has been made in previous works , the role of this assumption is not well-explained . $ \\bullet $ A2 : Thanks for your comments , and we apologize that the rules of the assumption were missed in the original version . Overall , the assumption is critical in most part of the theoretical analysis . A major part of the proof is to bound the population risk function , which is an expectation over the distribution of input data . Hence , to guarantee the population risk function analyzable , the distribution of input can not be arbitrary . In addition , Gaussian distribution guarantee a nice landscape of the population risk function , i.e. , the local convexity region is large enough . Another part is for tensor initialization . The major idea for tensor initialization is to utilize the high-order momentum to characterize the angle and the magnitude of the weights vector $ w_j $ 's . Gaussian distribution or at least rotation invariant distribution is necessary to separate the information of the angle and the magnitude , which simplifies the calculation . The analysis might be extended to other distributions such as non-standard Gaussian or Gaussian mixture models , and we will leave that for future work . In the paper , we have added some intuitive discussion for the necessity of Gaussian assumption in the footnote 3 of page 4 ."}}