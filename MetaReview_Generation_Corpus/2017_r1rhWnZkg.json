{"year": "2017", "forum": "r1rhWnZkg", "title": "Hadamard Product for Low-rank Bilinear Pooling", "decision": "Accept (Poster)", "meta_review": "The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns with the paper that the authors are strongly encouraged to address for the final version of the paper, overall, the work has contributions that are worth presenting at ICLR.", "reviews": [{"review_id": "r1rhWnZkg-0", "review_text": "Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built. Strengths: 1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. 2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below). 3. The various design choices made in model development have been experimentally verified. Weaknesses/Suggestions: 1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA). 2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? 3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art. 4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper. 5. In the caption for Table 1, fix the following: \u201chave not\u201d -> \u201chave no\u201d Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. ", "rating": "7: Good paper, accept", "reply_text": "1.Maybe you misunderstand our recent comment . We found that compact bilinear pooling performs worse , not low-rank bilinear pooling , keeping rest of the model architecture of ours . ( We updated the previous comment to avoid confusing ) . If it affects your rating , please reconsider it . 2.Major parameter reduction comes from the difference between the last fully-connected layer as described in Section 7.1 . Fukui et al . ( 2016 ) should use 16,000-dimension in the middle of networks for their method , compact bilinear pooling . 3.The VQA dataset has 360K questions for training/validation , and 240K for test . Moreover , for test-standard , we should upload our prediction result to the designated evaluation server with up to five submissions ( though we used only two submissions for our paper , one for single model , and the other one for ensemble model ) . The accuracy variance is empirically around 0.05 % . Hence , we believe 0.42 % gain is a significant improvement . Notice that we attribute a relative huge improvement of the ensemble model from Fukui et al . ( 2016 ) in Table 3 to both answer sampling ( in our experiment , +0.27 % ) and data augmentation using visual-genome dataset ( in Fukui et al . ( 2016 ) +0.70 % ) , which is not used in previous methods . We also use both methods in our ensemble model to compare with Fukui et al . ( 2016 ) .4.MRN is from Kim et al . ( 2016b ) , MARN is an experimental model which have an attention mechanism in MRN , and MLB , our proposed model , is a simpler model of MARN , which have no shortcut connection ."}, {"review_id": "r1rhWnZkg-1", "review_text": "This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product). This formulation is evaluated on the visual question answering (VQA) task together with several other model variants. Strength: 1. The paper discusses how the Hadamard product can be used to approximate the full outer product. 2. The paper provides an extensive experimental evaluation of other model aspect for VQA. 3. The full model archives a slight improvement over prior state-of-the-art on the challenging and large scale VQA challenge. Weaknesses: 1. Novelty: The paper presents only a new \u201cinterpretation\u201d of the Hadamard product which has previously been widely used for pooling, including for VQA. 2. Experimental evaluation: 2.1. An experimental direct comparison with MCB missing. Although the evaluated model is similar to Fukui et al. several other changes have been made, including question encoding (GRU vs. LSTM), normalization (tanh vs. L2 vs. none). The small difference in performance (0.44% om Table 1) could easily be attributed to these differences. 2.2. An experimental comparison to the full outer product (e.g. for a lower dimension) is missing. It remains unclear how good the proposed approximation for the full outer product is. While a comparison to MCB is presented this seems insufficient as MCB is a very different model. 2.3. One of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have? 2.4. Comparison with other pooling strategies, e.g. elementwise sum instead of elementwise product. 3. No theoretical analysis or properties of the approximation are presented. 4. The paper seems to be general at the beginning, but the claim of the benefit of the Hadamard product is only shown experimentally on the VQA dataset. 5. Related work: The comparison to the related works in the appendix should at least be mentioned in the main paper, even if the details are the supplemental. Minor - It is not clear why the Lu et al, 2015 is cited rather than the published paper from Antol et al. - Sect 2, first sentence: \u201cevery pairs\u201d -> \u201cevery pair\u201d Summary: While the paper provides a new best performance and an interesting interpretation of Hadamard product, to be a strong paper, either a more theoretical analysis of the properties of this approximation is required or a corresponding experimental evaluation. It is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the Hadamard product) but of unrelated aspects which are important to achieve high performance in the VQA challenge. To be more convincing I would like to see the following experiments - Comparison with Outer product in the identical model - Comparison with MCB in the identical model - Comparison with elementwise sum instead of elementwise product - One of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have? ", "rating": "7: Good paper, accept", "reply_text": "2.1 Please refer to our recent comment . 2.2 Equivalent full outer product can not be done even for modern GPUs . Note that Fukui et al . ( 2016 ) did an experiment with fairly small dimensions , which shows that their approximation is better for generalization . 2.4 With the idea of residual learning , element-wise sum can be interpreted as additive complementary learning with two different models . However , in multimodal learning , two complementary models compete against with each other to fit the desired function , not considering interactive characteristics of multimodality . 5.You 're right , we \u2019 ll update that . Minor.Early version of Antol et al . ( 2015 ) didn \u2019 t include Lu et al . ( 2015 ) \u2019 s model . We \u2019 ll update the cite . Thanks !"}, {"review_id": "r1rhWnZkg-2", "review_text": "Results on the VQA task are good for this simple model, the ablation study of table 1 gives some insights as to what is important. Missing are some explanations about the language embedding and the importance in deciding embedding dimension and final output dimension, equivalent to deciding the projected dimension in the compact bilinear model. Since the main contribution of the paper seems to be slightly better performance with fairly large reduction in parameters vs. compact bilinear something should be said about choice of those hyper parameters. If you increase embedded and output dimensions to equalize parameters to the compact bilinear model are further gains possible? How is the question encoded? Is word order preserved in this encoding, the compact bilinear model compared to in table 1 mentions glove, the proposed model is using this as well? The meaning of visual attention in this model along with the number of glimpses should be tied to the sentence embedding, so now we are looking at particular spatial components when that part of the sentence is encoded, then we stack according to your equation 9? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Explanation about the question embedding can be found in Appendix A.1.1 , and please refer to our recent comment on that . The output dimension ( the number of candidate answers ) and joint embedding size is fixed according to Kim et al. , ( 2016b ) . Our preliminary study shows that the choice of the number of joint embedding size is difficult , since the variance of performance is relatively high , but around 1,000 is fairly good . For the multiple-glimpse attention mechanism , we get two different attention probability distributions over 14x14 grids using attention mechanism , and concatenating the two different weighted-sum visual features for each distribution , respectively ."}], "0": {"review_id": "r1rhWnZkg-0", "review_text": "Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built. Strengths: 1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. 2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below). 3. The various design choices made in model development have been experimentally verified. Weaknesses/Suggestions: 1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA). 2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? 3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art. 4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper. 5. In the caption for Table 1, fix the following: \u201chave not\u201d -> \u201chave no\u201d Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling. ", "rating": "7: Good paper, accept", "reply_text": "1.Maybe you misunderstand our recent comment . We found that compact bilinear pooling performs worse , not low-rank bilinear pooling , keeping rest of the model architecture of ours . ( We updated the previous comment to avoid confusing ) . If it affects your rating , please reconsider it . 2.Major parameter reduction comes from the difference between the last fully-connected layer as described in Section 7.1 . Fukui et al . ( 2016 ) should use 16,000-dimension in the middle of networks for their method , compact bilinear pooling . 3.The VQA dataset has 360K questions for training/validation , and 240K for test . Moreover , for test-standard , we should upload our prediction result to the designated evaluation server with up to five submissions ( though we used only two submissions for our paper , one for single model , and the other one for ensemble model ) . The accuracy variance is empirically around 0.05 % . Hence , we believe 0.42 % gain is a significant improvement . Notice that we attribute a relative huge improvement of the ensemble model from Fukui et al . ( 2016 ) in Table 3 to both answer sampling ( in our experiment , +0.27 % ) and data augmentation using visual-genome dataset ( in Fukui et al . ( 2016 ) +0.70 % ) , which is not used in previous methods . We also use both methods in our ensemble model to compare with Fukui et al . ( 2016 ) .4.MRN is from Kim et al . ( 2016b ) , MARN is an experimental model which have an attention mechanism in MRN , and MLB , our proposed model , is a simpler model of MARN , which have no shortcut connection ."}, "1": {"review_id": "r1rhWnZkg-1", "review_text": "This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product). This formulation is evaluated on the visual question answering (VQA) task together with several other model variants. Strength: 1. The paper discusses how the Hadamard product can be used to approximate the full outer product. 2. The paper provides an extensive experimental evaluation of other model aspect for VQA. 3. The full model archives a slight improvement over prior state-of-the-art on the challenging and large scale VQA challenge. Weaknesses: 1. Novelty: The paper presents only a new \u201cinterpretation\u201d of the Hadamard product which has previously been widely used for pooling, including for VQA. 2. Experimental evaluation: 2.1. An experimental direct comparison with MCB missing. Although the evaluated model is similar to Fukui et al. several other changes have been made, including question encoding (GRU vs. LSTM), normalization (tanh vs. L2 vs. none). The small difference in performance (0.44% om Table 1) could easily be attributed to these differences. 2.2. An experimental comparison to the full outer product (e.g. for a lower dimension) is missing. It remains unclear how good the proposed approximation for the full outer product is. While a comparison to MCB is presented this seems insufficient as MCB is a very different model. 2.3. One of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have? 2.4. Comparison with other pooling strategies, e.g. elementwise sum instead of elementwise product. 3. No theoretical analysis or properties of the approximation are presented. 4. The paper seems to be general at the beginning, but the claim of the benefit of the Hadamard product is only shown experimentally on the VQA dataset. 5. Related work: The comparison to the related works in the appendix should at least be mentioned in the main paper, even if the details are the supplemental. Minor - It is not clear why the Lu et al, 2015 is cited rather than the published paper from Antol et al. - Sect 2, first sentence: \u201cevery pairs\u201d -> \u201cevery pair\u201d Summary: While the paper provides a new best performance and an interesting interpretation of Hadamard product, to be a strong paper, either a more theoretical analysis of the properties of this approximation is required or a corresponding experimental evaluation. It is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the Hadamard product) but of unrelated aspects which are important to achieve high performance in the VQA challenge. To be more convincing I would like to see the following experiments - Comparison with Outer product in the identical model - Comparison with MCB in the identical model - Comparison with elementwise sum instead of elementwise product - One of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have? ", "rating": "7: Good paper, accept", "reply_text": "2.1 Please refer to our recent comment . 2.2 Equivalent full outer product can not be done even for modern GPUs . Note that Fukui et al . ( 2016 ) did an experiment with fairly small dimensions , which shows that their approximation is better for generalization . 2.4 With the idea of residual learning , element-wise sum can be interpreted as additive complementary learning with two different models . However , in multimodal learning , two complementary models compete against with each other to fit the desired function , not considering interactive characteristics of multimodality . 5.You 're right , we \u2019 ll update that . Minor.Early version of Antol et al . ( 2015 ) didn \u2019 t include Lu et al . ( 2015 ) \u2019 s model . We \u2019 ll update the cite . Thanks !"}, "2": {"review_id": "r1rhWnZkg-2", "review_text": "Results on the VQA task are good for this simple model, the ablation study of table 1 gives some insights as to what is important. Missing are some explanations about the language embedding and the importance in deciding embedding dimension and final output dimension, equivalent to deciding the projected dimension in the compact bilinear model. Since the main contribution of the paper seems to be slightly better performance with fairly large reduction in parameters vs. compact bilinear something should be said about choice of those hyper parameters. If you increase embedded and output dimensions to equalize parameters to the compact bilinear model are further gains possible? How is the question encoded? Is word order preserved in this encoding, the compact bilinear model compared to in table 1 mentions glove, the proposed model is using this as well? The meaning of visual attention in this model along with the number of glimpses should be tied to the sentence embedding, so now we are looking at particular spatial components when that part of the sentence is encoded, then we stack according to your equation 9? ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Explanation about the question embedding can be found in Appendix A.1.1 , and please refer to our recent comment on that . The output dimension ( the number of candidate answers ) and joint embedding size is fixed according to Kim et al. , ( 2016b ) . Our preliminary study shows that the choice of the number of joint embedding size is difficult , since the variance of performance is relatively high , but around 1,000 is fairly good . For the multiple-glimpse attention mechanism , we get two different attention probability distributions over 14x14 grids using attention mechanism , and concatenating the two different weighted-sum visual features for each distribution , respectively ."}}