{"year": "2017", "forum": "BysvGP5ee", "title": "Variational Lossy Autoencoder", "decision": "Accept (Poster)", "meta_review": "The reviewers agree that this is a well executed paper, and should be accepted and will make a positive contribution to the conference. In any final version please try to make a connection to the other paper at this conference with the same aims and execution.", "reviews": [{"review_id": "BysvGP5ee-0", "review_text": "This paper proposes a Variational Autoencoder model that can discard information found irrelevant, in order to learn interesting global representations of the data. This can be seen as a lossy compression algorithm, hence the name Variational Lossy Autoencoder. To achieve such model, the authors combine VAEs with neural autoregressive models resulting in a model that has both a latent variable structure and a powerful recurrence structure. The authors first present an insightful Bits-Back interpretation of VAE to show when and how the latent code is ignored. As it was also mentioned in the literature, they say that the autoregressive part of the model ends up explaining all structure in the data, while the latent variables are not used. Then, they propose two complementary approaches to force the latent variables to be used by the decoder. The first one is to make sure the autoregressive decoder only uses small local receptive field so the model has to use the latent code to learn long-range dependency. The second is to parametrize the prior distribution over the latent code with an autoregressive model. They also report new state-of-the-art results on binarized MNIST (both dynamical and statically binarization), OMNIGLOT and Caltech-101 Silhouettes. Review: The bits-Back interpretation of VAE is a nice contribution to the community. Having novel interpretations for a model helps to better understand it and sometimes, like in this paper, highlights how it can be improved. Having a fine-grained control over the kind of information that gets included in the learned representation can be useful for a lot of applications. For instance, in image retrieval, such learned representation could be used to retrieve objects that have similar shape no matter what texture they have. However, the authors say they propose two complementary classes of improvements to VAE, that is the lossy code via explicit information placement (Section 3.1) and learning the prior with autoregressive flow (Section 3.2). However, they never actually showed how a VAE without AF prior but that has a PixelCNN decoder performs. What would be the impact on the latent code is no AF prior is used? Also, it is not clear if WindowAround(i) represents only a subset of x_{<i} or it can contain any data other than x_i. The authors mentioned the window can be represented as a small rectangle adjacent to a pixel x_i, must it only contains pixels above and to the left of x_i (similar to PixelCNN) Minor: In Equation 8, should there be an expectation over the data distribution?", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer2 , Thank you for your thoughtful review and we are glad that you think learning lossy features that are relevant is important\uff01Below are our answers to your questions : - `` What would be the impact on the latent code is no AF prior is used ? '' We addressed this question indirectly in text `` A similar gain carries over when an autoregressive decoder is used : on statically binarized MNIST , using AF prior instead of IAF posterior reduces train NLL by $ 0.8 $ nat and test NLL by $ 0.6 $ nat . '' And the effect of introducing IAF posterior to a VAE that uses PixelCNN decoder is similar to that of a VAE that uses factorized decoder ( Table 1 in [ 1 ] ) . We will address this more directly in next revision by giving direct ablation experiments to show the complementary benefits of both modifications . - `` The authors mentioned the window can be represented as a small rectangle adjacent to a pixel x_i , must it only contains pixels above and to the left of x_i ( similar to PixelCNN ) '' The small window will be exacly the local receptive field of a depth-limited PixelCNN . We will include visualizations to make the exact definition used more explicit . - `` In Equation 8 , should there be an expectation over the data distribution ? '' Yes , you are right ! It will be fixed in next revision . Thanks again for taking time to review . [ 1 ] : http : //papers.nips.cc/paper/6581-improving-variational-autoencoders-with-inverse-autoregressive-flow"}, {"review_id": "BysvGP5ee-1", "review_text": "This paper introduces the notion of a \"variational lossy autoencoder\", where a powerful autoregressive conditional distribution on the inputs x given the latent code z is crippled in a way that forces it to use z in a meaningful way. Its three main contributions are: (1) It gives an interesting information-theoretical insight as to why VAE-type models don't tend to take advantage of their latent representation when the conditional distribution on x given z is powerful enough. (2) It shows that this insight can be used to efficiently train VAEs with powerful autoregressive conditional distributions such that they make use of the latent code. (3) It presents a powerful way to parametrize the prior in the form of an autoregressive flow transformation which is equivalent to using an inverse autoregressive flow transformation on the approximate posterior. By itself, I think the information-theoretical explanation of why VAEs do not use their latent code when the conditional distribution on x given z is powerful enough constitutes an excellent addition to our understanding of VAE-related approaches. However, the way this intuition is empirically evaluated is a bit weak. The \"crippling\" method used feels hand-crafted and very task-dependent, and the qualitative evaluation of the \"lossyness\" of the learned representation is carried out on three datasets (MNIST, OMNIGLOT and Caltech-101 Silhouettes) which feature black-and-white images with little-to-no texture. Figures 1a and 2a do show that reconstructions discard low-level information, as observed in the slight variations in strokes between the input and the reconstruction, but such an analysis would have been more compelling with more complex image datasets. Have the authors tried applying VLAE to such datasets? I think the Caltech101 Silhouettes benchmark should be treated with caution, as no comparison is made against other competitive approaches like IAF VAE, PixelRNN and Conv DRAW. This means that VLAE significantly outperforms the state-of-the-art in only one of the four settings examined. A question which is very relevant to this paper is \"Does a latent representation on top of an autoregressive model help improve the density modeling performance?\" The paper touches this question, but very briefly: the only setting in which VLAE is compared against recent autoregressive approaches shows that it wins against PixelRNN by a small margin. The proposal to transform the latent code with an autoregressive flow which is equivalent to parametrizing the approximate posterior with an inverse autoregressive flow transformation is also interesting. There is, however, one important distinction to be made between the two approaches: in the former, the prior over the latent code can potentially be very complex whereas in the latter the prior is limited to be a simple, factorized distribution. It is not clear to me that having a very powerful prior is necessarily a good thing from a representation learning point of view: oftentimes we are interested in learning a representation of the data distribution which is untangled and composed of roughly independent factors of variation. The degree to which this can be achieved using something as simple as a spherical gaussian prior is up for discussion, but finding a good balance between the ability of the prior to fit the data and its usefulness as a high-level representation certainly warrants some thought. I would be interested in hearing the authors' opinion on this. Overall, the paper introduces interesting ideas despite the flaws outlined above, but weaknesses in the empirical evaluation prevent me from recommending its acceptance. UPDATE: The rating has been revised to a 7 following the authors' reply.", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer3 , Thank you for reading our manuscript carefully ! Below are some answers to questions in your review . I 'd like to also ask a few clarification questions that can help us better prepare next revision : - `` However , the way this intuition is empirically evaluated is a bit weak .... but such an analysis would have been more compelling with more complex image datasets '' Do you want to see qualitative results on colored datasets that have richer variations ? We do have some initial results on CIFAR10 that show characteristics qualitatively similar to current binary experiments : global shape being preserved whilst local color patterns and exact details change from one decompression to another . We can also run similar experiments on higher-resolution datasets if this is what you are looking for . - `` This means that VLAE significantly outperforms the state-of-the-art in only one of the four settings examined . '' We 'd like to reiterate that on three out of four settings , we kept the hyperparameters fixed for VLAE and we kept VLAE at a disadvantage to assess its ability to generalize across datasets . We do out-perform previous SOTA ( ConvDraw ) on OMNIGLOT , for instance , more significantly when we tune hyperparameters specifically for this dataset . Do you want to see this kind of evaluation ? - `` '' Does a latent representation on top of an autoregressive model help improve the density modeling performance ? '' The paper touches this question , but very briefly : the only setting in which VLAE is compared against recent autoregressive approaches shows that it wins against PixelRNN by a small margin . '' I apologize for the confusion as the question that we try to ask and answer is `` Does an autoregressive model as decoder help improve latent-code models ' density estimation performance '' . Since a latent-code model like VAE suffers some penalty in code length for using approximate posterior , PixelCNN/RNN seems to be a better fit for just getting good logprob . Nevertheless , it 's usually hard to get good representations from this kind of autoregressive models , unlike latent-code models . We believe part of this work 's value is improving density estimation performance for latent-code models specifically . - `` It is not clear to me that having a very powerful prior is necessarily a good thing ..... '' A prior that 's transformed by an autoregressive flow can indeed become quite complicated and less useful for learning disentangled representations . If we are interested in obtaining a latent code that has factorized distribution , we can treat the noise source \\epsilon in ( 14 ) as the true latent code . - `` weaknesses in the empirical evaluation prevent me from recommending its acceptance . '' We thought the current set of experiments , though on smaller scale datasets , do validate the 3 main contributions that you summarized excellently , but we are definitely open to suggestions . Is there any additional evaluation not covered in questions above that you think is important ? Thanks again for your time . I hope that , through more discussion enabled an open review format , we can improve the manuscript to make it more useful to our community ."}, {"review_id": "BysvGP5ee-2", "review_text": "This paper motivates the combination of autoregressive models with Variational Auto-Encoders and how to control the amount the amount of information stored in the latent code. The authors provide state-of-the-art results on MNIST, OMNIGLOT and Caltech-101. I find that the insights provided in the paper, e.g. with respect to the effect of having a more powerful decoder on learning the latent code, the bit-back coding, and the lossy decoding are well-written but are not novel. The difference between an auto-regressive prior and the inverse auto-regressive posterior is new and interesting though. The model presented combines the recent technique of PixelRNN/PixelCNN and Variational Auto-Encoders with Inverse Auto-Regressive Flows, which enables the authors to obtain state-of-the-art results on MNIST, OMNIGLOT and Caltech-101. Given the insights provided in the paper, the authors are also able to control the amount of information contained in the latent code to an extent. This paper gather several insight on Variational Auto-Encoders scattered through several publications in a well-written way. From these, the authors are able to obtain state-of-the-art models on small complexity datasets. Larger scale experiments will be necessary.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer1 , Thank you for your thoughtful review ! I 'd like to ask a few clarification questions that can help us better prepare next revision : - `` I find that the insights provided in the paper , e.g.with respect to the effect of having a more powerful decoder on learning the latent code , the bit-back coding , and the lossy decoding are well-written but are not novel. `` I 'd like to agree with you that the connection between bits-back coding and variational inference is widely known , as acknowledged in original submission . Nevertheless , to the best of my knowledge , there is no prior work that discusses lossy decoding and information placement . If you could point us to related work , we would include proper citations and discussions . - `` Larger scale experiments will be necessary . '' Is this the main concern you have about the current revision ? If so , what kind of evaluations do you want to see on datasets with higher complexity ? Thanks again for your time . I hope that , through more discussion enabled an open review format , we can improve the manuscript to make it more useful to our community ."}], "0": {"review_id": "BysvGP5ee-0", "review_text": "This paper proposes a Variational Autoencoder model that can discard information found irrelevant, in order to learn interesting global representations of the data. This can be seen as a lossy compression algorithm, hence the name Variational Lossy Autoencoder. To achieve such model, the authors combine VAEs with neural autoregressive models resulting in a model that has both a latent variable structure and a powerful recurrence structure. The authors first present an insightful Bits-Back interpretation of VAE to show when and how the latent code is ignored. As it was also mentioned in the literature, they say that the autoregressive part of the model ends up explaining all structure in the data, while the latent variables are not used. Then, they propose two complementary approaches to force the latent variables to be used by the decoder. The first one is to make sure the autoregressive decoder only uses small local receptive field so the model has to use the latent code to learn long-range dependency. The second is to parametrize the prior distribution over the latent code with an autoregressive model. They also report new state-of-the-art results on binarized MNIST (both dynamical and statically binarization), OMNIGLOT and Caltech-101 Silhouettes. Review: The bits-Back interpretation of VAE is a nice contribution to the community. Having novel interpretations for a model helps to better understand it and sometimes, like in this paper, highlights how it can be improved. Having a fine-grained control over the kind of information that gets included in the learned representation can be useful for a lot of applications. For instance, in image retrieval, such learned representation could be used to retrieve objects that have similar shape no matter what texture they have. However, the authors say they propose two complementary classes of improvements to VAE, that is the lossy code via explicit information placement (Section 3.1) and learning the prior with autoregressive flow (Section 3.2). However, they never actually showed how a VAE without AF prior but that has a PixelCNN decoder performs. What would be the impact on the latent code is no AF prior is used? Also, it is not clear if WindowAround(i) represents only a subset of x_{<i} or it can contain any data other than x_i. The authors mentioned the window can be represented as a small rectangle adjacent to a pixel x_i, must it only contains pixels above and to the left of x_i (similar to PixelCNN) Minor: In Equation 8, should there be an expectation over the data distribution?", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer2 , Thank you for your thoughtful review and we are glad that you think learning lossy features that are relevant is important\uff01Below are our answers to your questions : - `` What would be the impact on the latent code is no AF prior is used ? '' We addressed this question indirectly in text `` A similar gain carries over when an autoregressive decoder is used : on statically binarized MNIST , using AF prior instead of IAF posterior reduces train NLL by $ 0.8 $ nat and test NLL by $ 0.6 $ nat . '' And the effect of introducing IAF posterior to a VAE that uses PixelCNN decoder is similar to that of a VAE that uses factorized decoder ( Table 1 in [ 1 ] ) . We will address this more directly in next revision by giving direct ablation experiments to show the complementary benefits of both modifications . - `` The authors mentioned the window can be represented as a small rectangle adjacent to a pixel x_i , must it only contains pixels above and to the left of x_i ( similar to PixelCNN ) '' The small window will be exacly the local receptive field of a depth-limited PixelCNN . We will include visualizations to make the exact definition used more explicit . - `` In Equation 8 , should there be an expectation over the data distribution ? '' Yes , you are right ! It will be fixed in next revision . Thanks again for taking time to review . [ 1 ] : http : //papers.nips.cc/paper/6581-improving-variational-autoencoders-with-inverse-autoregressive-flow"}, "1": {"review_id": "BysvGP5ee-1", "review_text": "This paper introduces the notion of a \"variational lossy autoencoder\", where a powerful autoregressive conditional distribution on the inputs x given the latent code z is crippled in a way that forces it to use z in a meaningful way. Its three main contributions are: (1) It gives an interesting information-theoretical insight as to why VAE-type models don't tend to take advantage of their latent representation when the conditional distribution on x given z is powerful enough. (2) It shows that this insight can be used to efficiently train VAEs with powerful autoregressive conditional distributions such that they make use of the latent code. (3) It presents a powerful way to parametrize the prior in the form of an autoregressive flow transformation which is equivalent to using an inverse autoregressive flow transformation on the approximate posterior. By itself, I think the information-theoretical explanation of why VAEs do not use their latent code when the conditional distribution on x given z is powerful enough constitutes an excellent addition to our understanding of VAE-related approaches. However, the way this intuition is empirically evaluated is a bit weak. The \"crippling\" method used feels hand-crafted and very task-dependent, and the qualitative evaluation of the \"lossyness\" of the learned representation is carried out on three datasets (MNIST, OMNIGLOT and Caltech-101 Silhouettes) which feature black-and-white images with little-to-no texture. Figures 1a and 2a do show that reconstructions discard low-level information, as observed in the slight variations in strokes between the input and the reconstruction, but such an analysis would have been more compelling with more complex image datasets. Have the authors tried applying VLAE to such datasets? I think the Caltech101 Silhouettes benchmark should be treated with caution, as no comparison is made against other competitive approaches like IAF VAE, PixelRNN and Conv DRAW. This means that VLAE significantly outperforms the state-of-the-art in only one of the four settings examined. A question which is very relevant to this paper is \"Does a latent representation on top of an autoregressive model help improve the density modeling performance?\" The paper touches this question, but very briefly: the only setting in which VLAE is compared against recent autoregressive approaches shows that it wins against PixelRNN by a small margin. The proposal to transform the latent code with an autoregressive flow which is equivalent to parametrizing the approximate posterior with an inverse autoregressive flow transformation is also interesting. There is, however, one important distinction to be made between the two approaches: in the former, the prior over the latent code can potentially be very complex whereas in the latter the prior is limited to be a simple, factorized distribution. It is not clear to me that having a very powerful prior is necessarily a good thing from a representation learning point of view: oftentimes we are interested in learning a representation of the data distribution which is untangled and composed of roughly independent factors of variation. The degree to which this can be achieved using something as simple as a spherical gaussian prior is up for discussion, but finding a good balance between the ability of the prior to fit the data and its usefulness as a high-level representation certainly warrants some thought. I would be interested in hearing the authors' opinion on this. Overall, the paper introduces interesting ideas despite the flaws outlined above, but weaknesses in the empirical evaluation prevent me from recommending its acceptance. UPDATE: The rating has been revised to a 7 following the authors' reply.", "rating": "7: Good paper, accept", "reply_text": "Dear AnonReviewer3 , Thank you for reading our manuscript carefully ! Below are some answers to questions in your review . I 'd like to also ask a few clarification questions that can help us better prepare next revision : - `` However , the way this intuition is empirically evaluated is a bit weak .... but such an analysis would have been more compelling with more complex image datasets '' Do you want to see qualitative results on colored datasets that have richer variations ? We do have some initial results on CIFAR10 that show characteristics qualitatively similar to current binary experiments : global shape being preserved whilst local color patterns and exact details change from one decompression to another . We can also run similar experiments on higher-resolution datasets if this is what you are looking for . - `` This means that VLAE significantly outperforms the state-of-the-art in only one of the four settings examined . '' We 'd like to reiterate that on three out of four settings , we kept the hyperparameters fixed for VLAE and we kept VLAE at a disadvantage to assess its ability to generalize across datasets . We do out-perform previous SOTA ( ConvDraw ) on OMNIGLOT , for instance , more significantly when we tune hyperparameters specifically for this dataset . Do you want to see this kind of evaluation ? - `` '' Does a latent representation on top of an autoregressive model help improve the density modeling performance ? '' The paper touches this question , but very briefly : the only setting in which VLAE is compared against recent autoregressive approaches shows that it wins against PixelRNN by a small margin . '' I apologize for the confusion as the question that we try to ask and answer is `` Does an autoregressive model as decoder help improve latent-code models ' density estimation performance '' . Since a latent-code model like VAE suffers some penalty in code length for using approximate posterior , PixelCNN/RNN seems to be a better fit for just getting good logprob . Nevertheless , it 's usually hard to get good representations from this kind of autoregressive models , unlike latent-code models . We believe part of this work 's value is improving density estimation performance for latent-code models specifically . - `` It is not clear to me that having a very powerful prior is necessarily a good thing ..... '' A prior that 's transformed by an autoregressive flow can indeed become quite complicated and less useful for learning disentangled representations . If we are interested in obtaining a latent code that has factorized distribution , we can treat the noise source \\epsilon in ( 14 ) as the true latent code . - `` weaknesses in the empirical evaluation prevent me from recommending its acceptance . '' We thought the current set of experiments , though on smaller scale datasets , do validate the 3 main contributions that you summarized excellently , but we are definitely open to suggestions . Is there any additional evaluation not covered in questions above that you think is important ? Thanks again for your time . I hope that , through more discussion enabled an open review format , we can improve the manuscript to make it more useful to our community ."}, "2": {"review_id": "BysvGP5ee-2", "review_text": "This paper motivates the combination of autoregressive models with Variational Auto-Encoders and how to control the amount the amount of information stored in the latent code. The authors provide state-of-the-art results on MNIST, OMNIGLOT and Caltech-101. I find that the insights provided in the paper, e.g. with respect to the effect of having a more powerful decoder on learning the latent code, the bit-back coding, and the lossy decoding are well-written but are not novel. The difference between an auto-regressive prior and the inverse auto-regressive posterior is new and interesting though. The model presented combines the recent technique of PixelRNN/PixelCNN and Variational Auto-Encoders with Inverse Auto-Regressive Flows, which enables the authors to obtain state-of-the-art results on MNIST, OMNIGLOT and Caltech-101. Given the insights provided in the paper, the authors are also able to control the amount of information contained in the latent code to an extent. This paper gather several insight on Variational Auto-Encoders scattered through several publications in a well-written way. From these, the authors are able to obtain state-of-the-art models on small complexity datasets. Larger scale experiments will be necessary.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear AnonReviewer1 , Thank you for your thoughtful review ! I 'd like to ask a few clarification questions that can help us better prepare next revision : - `` I find that the insights provided in the paper , e.g.with respect to the effect of having a more powerful decoder on learning the latent code , the bit-back coding , and the lossy decoding are well-written but are not novel. `` I 'd like to agree with you that the connection between bits-back coding and variational inference is widely known , as acknowledged in original submission . Nevertheless , to the best of my knowledge , there is no prior work that discusses lossy decoding and information placement . If you could point us to related work , we would include proper citations and discussions . - `` Larger scale experiments will be necessary . '' Is this the main concern you have about the current revision ? If so , what kind of evaluations do you want to see on datasets with higher complexity ? Thanks again for your time . I hope that , through more discussion enabled an open review format , we can improve the manuscript to make it more useful to our community ."}}