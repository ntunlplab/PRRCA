{"year": "2020", "forum": "SyxL2TNtvr", "title": "Unsupervised Model Selection for Variational Disentangled Representation Learning", "decision": "Accept (Poster)", "meta_review": "The authors address the important and understudied problem of tuning of unsupervised models, in particular variational models for learning disentangled representations.  They propose an unsupervised measure for model selection that correlates well with performance on multiple tasks.  After significant fruitful discussion with the reviewers and resulting revisions, many reviewer concerns have been addressed.  There are some remaining concerns that there may still be a gap in the theoretical basis for the application of the proposed measure to some models, that for different downstream tasks the best model selection criteria may vary, and that the method might be too cumbersome and not quite reliable enough for practitioners to use it broadly.  All of that being said, the reviewers (and I) agree that the approach is sufficiently interesting, and the empirical results sufficiently convincing, to make the paper a good contribution and hopefully motivation for additional methods addressing this problem.", "reviews": [{"review_id": "SyxL2TNtvr-0", "review_text": "The paper proposes a metric for unsupervised model (and hyperparameter) selection for VAE-based models. The essential basis for the metric is to rank the models based on how much disentanglement they provide. This method relies on a key observation from this paper [A] viz., disentangled representations by any VAE-based model are likely to be similar (upto permutation and sign). I am inclined to accept the paper for the following reasons: 1. The proposed approach is clear and easy enough to understand and well motivated 2. The paper has clearly outlined the assumptions and limitations of their work 3. The reported result show that models ranked by disentanglement correlate well with the supervised metrics for the various VAE models. 4. This metric is unsupervised and thus can utilize far more data than the supervised metric methods and can be useful even when the dataset has no labels. 5. The supplementary material also shows that the metric correlates well with the task performance. [A] Variational Autoencoders Pursue PCA Directions (by Accident), CVPR 2019 --- Update: Thanks for the thoughtful rebuttal by the authors to all the reviewers' feedback. Based on the several discussions by the other reviewers and the discussion that happened, I am inclined to lower my scores to a weak accept. ", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer , thank you for your feedback ."}, {"review_id": "SyxL2TNtvr-1", "review_text": "This paper proposes a criterion called Unsupervised Disentanglement Ranking (UDR) score. The score is computed based on the assumption that good disentangled representations are alike, while the representations can be entangled in multiple possible ways. The UDR score can be used for unsupervised hyperparameter tuning and model selection for variational disentangled method. The problem this paper focuses on is essential because we usually apply unsupervised disentangled methods to analyze the data when the labels are unavailable. However, existing metrics for hyperparameter tuning and model selection requires ground-truth labels. This paper allows measuring model performance without supervision, making the hyperparameter tuning and model selection possible in practice. It looks like some parts of this paper need rewriting. In the abstract, it is not mentioned at all what is the proposed approach. Most paragraphs in the introduction section review the related work and background but do not introduce what assumption and strategy the proposed method adopted. It looks like the proposed UDR is theoretically supported by Rolinek et al. (2019). However, the proof given by Rolinek et al. (2019) is for $\\beta$-VAE, where the regularization can be turned into the constraint on KL divergence. I do not think the \"polarised regime\" holds for other disentangled model, for example, TCVAE, where a biased estimation of total correlation is introduced in the objective function. Therefore, I am not convinced that I should trust the results of the UDR, which combines multiple disentangled models. The computational process of UDR is heuristic and somewhat arbitrary. There is no theoretical guarantee that UDR should be a useful disentanglement metric. Although the UDR is supported by some experiments, I am not convinced that it is trustworthy for more complex real-world datasets. Equation (3) looks problematic. Note that it is possible to train a Bidirectional Generative Adversarial Network (BiGAN) that can generate complex images based on a uniform distribution (Donahue et al., 2016). The encoder of the BiGAN can be considered as the inverse of the generator, which maps images back to the uniform distribution. This suggests that under the encoder-decoder framework, it is possible that latent variables can be informative even the posterior distribution matches the prior distribution. Although VAEs are trained using a different strategy, I do not see why the posterior needs to diverge from the prior distribution for informative latent representations. The encoder might simply be the inverse of the decoder under a certain scenario. In summary, this paper focuses on solving an important problem. However, the proposed method is not well supported by theorems as it seems. The paper also appears to contain minor technical issues. Therefore, I am inclined to reject this paper. References Donahue, Jeff, Philipp Kr\u00e4henb\u00fchl, and Trevor Darrell. \"Adversarial feature learning.\" arXiv preprint arXiv:1605.09782 (2016).", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer , Thank you for your thoughtful comments . In your feedback you have brought up three major points : 1 ) you have expressed doubt whether the \u201c polarised regime \u201d holds for other disentangling methods other than \\beta-VAE ; 2 ) you were wondering what underlies the thinking behind the choice of our computational process ; and 3 ) you were wondering about our choice of using per latent KL divergence to identify which latents are informative . We will address these points below . 1 ) You have expressed doubt whether the \u201c polarised regime \u201d holds for other disentangling methods other than \\beta-VAE , which was used as the example model in Rolinek et al.First , even the vanilla VAEs ( Kingma and Welling , 2014 ) are known to enter the \u201c polarised regime \u201d , which is often cited as one of their shortcomings ( e.g.see Rezende and Viola , 2018 ) . All of the disentangling VAEs considered in the paper , including TC-VAE , augment the original ELBO objective with extra terms . Hence , all of them still contain the \\beta KL term of the ELBO with \\beta = > 1 . This means that in theory all of them inherit the property of the original VAEs of operating in the \u201c polarised regime \u201d . We have tested this empirically by counting the number of latents that are \u201c switched off \u201d in each of the 5400 models considered in our paper . We found that all models apart from DIP-VAE-I entered the polarised regime , having on average 2.95/10 latents \u201c switched off \u201d ( with a standard deviation of 1.97 ) . Note that DIP-VAE-I is the only model with an objective that explicitly penalises \u201c switching off \u201d latent dimensions , which means that it is less suitable for disentangled representation learning in the common regime where the number of generative factors is smaller than the number of latents ( as discussed in the original paper by Kumar et al , 2018 ) . Despite DIP-VAE-I never entering the polarised regime , the results reported in our paper ( e.g.in Fig.2 or Fig.6 in Supplementary Materials ) suggest that our proposed UDR still performs well and correlates highly with the supervised metrics for this model class . 2 ) The computational process proposed in our paper is motivated by the theoretical results presented in Rolinek et al.Unfortunately there is no computationally feasible way to calculate directly whether the SVD decomposition J=U\u03a3V of the Jacobian J of the decoder results in a V , which is a signed permutation matrix . Our approach uses a simple process to approximate this in a computationally feasible way . We have applied our proposed method to a number of datasets commonly used in the literature and have demonstrated that it performs well across 5400 models . Please let us know if you have a particular suggestion for a more complex dataset that you would like to test our metric on . 3 ) In terms of Equation 3 , the GAN objective in the BiGAN paper implicitly minimises the KL divergence between the prior p ( z ) and the marginalised posterior q ( z ) . However , in Eq.3 we measure the KL divergence between the prior and the conditional posterior q ( z|x ) . Hence , the two are not directly comparable . Eq.3 is a way to quantify which latent dimensions are used by the network that has entered the \u201c polarised regime \u201d . Rolinek et al define a model to be in the \u201c polarised regime \u201d if its latents can be split into two disjoint sets of \u201c used \u201d and \u201c unused \u201d dimensions . \u201c Used \u201d dimensions are defined as those which have inferred \\sigma^2 < < 1 , and the \u201c unused \u201d dimensions are defined as those which have \\mu^2 < < 1 and \\sigma^2 \\approx 1 ( see Sec 3.2 , Definition 1 in Rolinek et al ) . Note that the latter would result in a small KL from a unit Gaussian prior as per Eq.3 in our paper , thus justifying our choice to find the \u201c used \u201d and \u201c unused \u201d latents . Finally , we have modified the abstract and introduction to mention our proposed method as per your suggestion ."}, {"review_id": "SyxL2TNtvr-2", "review_text": "This paper addresses the problem of unsupervised model selection for disentangled representation learning. Based on the understanding of \u201cwhy VAEs disentangle\u201d [Burgess et al. 2017, Locatello et al. 2018, Mathieu et al. 2019, Rolinek et al. 2019], the authors adopt the assumption that disentangled representations are all alike (up to permutation and sign inverse) while entangled representations are different, and propose UDR method and its variants. Experimental results clearly show that UDR is a good approach for hyperparameter/model selection. Overall, I think a reliable metric for model selection/evaluation is needed for the VAE-based disentangled representation learning. According to comprehensive experimental studies performed in this paper, UDR seems to be a potentially good choice. However, I am not sure if very good disentangled representations must benefit (general) subsequent tasks, though the authors provide experimental evidence on fairness classification and data efficiency tasks. Actually, the data generation process in the real-world may consist of different generative factors that are not independent of each other. Though good disentangled representation provides good interpretability, it needs not to be better than entangled representation for concrete tasks. Specifically, for concrete supervised classification tasks, VAE with beta smaller than 1 (not towards disentanglement) might be the best (Alexander A. Alemi et al. 2017, Deep VIB). Another concern is about the choice of some key \u201chyperparameters\u201d. For the KL divergence threshold in equation 3, you set it to be 0.01. It looks like the choice would control how much the UDR favors a \u201csparse representation map\u201d. The larger the value, the few \u201cinformative dimensions\u201d would be considered. In supplementary material, you say that \u201cuninformative latents typically have KL<<0.01 while informative latents have KL >>0.01\u201d. Is this judgment based on \u201cqualitative feeling\u201d? For me, as you are contributing a \"quantitative measurement\u201d, it is interesting and important to see how this threshold would generally affect UDR\u2019s behavior in one (or more) datasets you have tried. Another hyperparameter I cared is P (number of models for pairwise comparison). In the paper, you validate the effect of P in the range [5,45]. How would P smaller than 5 affect UDR? According to Table 1, if I was using UDR, I\u2019d rather using P>=20 (or at least 10) rather than 5. Also, it seems to me P would grow up due to the size of factors that generate the data. Thus, I also have a little concern about the computation cost of the proposed metric (as also mentioned by the authors). Others concerns: -- As a heavy experimental paper, most experimental results are in supplementary material, while the authors spent a lot of time in the main text explaining the conclusions found in other papers. -- To validate the fundamental assumption of UDR, the authors might consider to quantitatively validate that, disentangled representations learned by those approaches you used in the paper are almost the same (up to permutation and sign inverse). ", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer , Thank you for your thoughtful comments . In your feedback you have brought up two points : 1 ) you were wondering whether disentangled representations would benefit subsequent tasks ; 2 ) you were wondering about our choice of certain hyperparameters . We will address these questions below : 1 ) In terms of whether disentangled representations would benefit subsequent tasks , we believe that it is important to consider the nature of the task , and whether it implicitly assumes any of the properties that disentangled representations possess . For example , if one is trying to solve a binary classification task based on the value of a single pixel in a high-dimensional image , it is unlikely that a disentangled representation will be useful . Indeed , a disentangled representation will most likely learn to ignore this pixel , since it doesn \u2019 t contribute much to the quality of the reconstruction . On the other hand , an entangled representation learnt implicitly through a supervised objective aiming to solve the task will throw away all information apart from the value of the relevant pixel and hence will be much more informative for that particular task . On the other hand , if one is interested in solving a large number of natural tasks in a single environment ( e.g.learning to achieve different values of the score in an Atari game , generalising policies to variations in the game colour schemes , fast language binding problems , data efficient classification of object identities , colours , sizes or relations ) , then a disentangled representation may be of more relevance , since it will produce the semantically meaningful equivariant compositional representation that will support many variations of these tasks . Hence , we believe that disentangled representations will be useful for those tasks that require compositionality , generalisation , data efficiency or generalisation/transfer . You were also wondering whether the real world data generative process follows the independence assumption presumed by disentangled representations . To answer this question we would like to refer you to the recently proposed alternative view on disentangled representations that moves away from considering independent generative factors ( Higgins et al , 2018 ) . The new definition suggests that disentangled representations instead reflect the compositional natural symmetry transformations . The implication of this definition is that one can move away from assuming IID training data generated by independent generative factors , and instead think about which aspects of the world can be transformed independently of each other , and how these transformations can be discovered through embodied active learning ( see Caselles-Dupre et al , 2019 for a first effort in that direction ) . 2 ) You were wondering about the choice of the 0.01 threshold in Eq.3 , and whether it was set using a \u201c qualitative feeling \u201d . The answer is no . This equation quantifies which latent dimensions are used by the network that has entered a \u201c polarised regime \u201d . Rolinek et al define a model to be in a \u201c polarised regime \u201d if its latents can be split into two disjoint sets of \u201c used \u201d and \u201c unused \u201d dimensions . \u201c Used \u201d dimensions are defined as those which have inferred \\sigma^2 < < 1 , and the \u201c unused \u201d dimensions are defined as those which have \\mu^2 < < 1 and \\sigma^2 \\approx 1 ( see Sec 3.2 , Definition 1 ) . Note that the latter would result in a small KL from a unit Gaussian prior as per Eq.3.Empirically we found that 0.01 was a good threshold , since the KL values have a bimodal distribution , with on average around 97 % of all \u201c small kl \u201d values lying below this threshold . In terms of P , we do not suggest using P < 5 . Training P seeds per hyperparameter setting is the largest computational overhead of the UDR , however it is subsumed by the largely accepted good research practice of training a number of seeds per hyperparameter setting anyway . The rest of the UDR computations are very fast . To give you an idea , running a single pairwise comparison using UDR Lasso takes around 4 seconds on a standard CPU , and it takes around 1600 seconds on average to compute UDR Lasso with P=50 for 300 models within a hyperparameter sweep , when we parallelise the pairwise comparisons per model . Finally , how would you propose that we quantitatively evaluate whether the representations learnt by the models are the same up to the UDR assumptions apart from running the UDR itself ? Maybe we misunderstood your question ..."}], "0": {"review_id": "SyxL2TNtvr-0", "review_text": "The paper proposes a metric for unsupervised model (and hyperparameter) selection for VAE-based models. The essential basis for the metric is to rank the models based on how much disentanglement they provide. This method relies on a key observation from this paper [A] viz., disentangled representations by any VAE-based model are likely to be similar (upto permutation and sign). I am inclined to accept the paper for the following reasons: 1. The proposed approach is clear and easy enough to understand and well motivated 2. The paper has clearly outlined the assumptions and limitations of their work 3. The reported result show that models ranked by disentanglement correlate well with the supervised metrics for the various VAE models. 4. This metric is unsupervised and thus can utilize far more data than the supervised metric methods and can be useful even when the dataset has no labels. 5. The supplementary material also shows that the metric correlates well with the task performance. [A] Variational Autoencoders Pursue PCA Directions (by Accident), CVPR 2019 --- Update: Thanks for the thoughtful rebuttal by the authors to all the reviewers' feedback. Based on the several discussions by the other reviewers and the discussion that happened, I am inclined to lower my scores to a weak accept. ", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer , thank you for your feedback ."}, "1": {"review_id": "SyxL2TNtvr-1", "review_text": "This paper proposes a criterion called Unsupervised Disentanglement Ranking (UDR) score. The score is computed based on the assumption that good disentangled representations are alike, while the representations can be entangled in multiple possible ways. The UDR score can be used for unsupervised hyperparameter tuning and model selection for variational disentangled method. The problem this paper focuses on is essential because we usually apply unsupervised disentangled methods to analyze the data when the labels are unavailable. However, existing metrics for hyperparameter tuning and model selection requires ground-truth labels. This paper allows measuring model performance without supervision, making the hyperparameter tuning and model selection possible in practice. It looks like some parts of this paper need rewriting. In the abstract, it is not mentioned at all what is the proposed approach. Most paragraphs in the introduction section review the related work and background but do not introduce what assumption and strategy the proposed method adopted. It looks like the proposed UDR is theoretically supported by Rolinek et al. (2019). However, the proof given by Rolinek et al. (2019) is for $\\beta$-VAE, where the regularization can be turned into the constraint on KL divergence. I do not think the \"polarised regime\" holds for other disentangled model, for example, TCVAE, where a biased estimation of total correlation is introduced in the objective function. Therefore, I am not convinced that I should trust the results of the UDR, which combines multiple disentangled models. The computational process of UDR is heuristic and somewhat arbitrary. There is no theoretical guarantee that UDR should be a useful disentanglement metric. Although the UDR is supported by some experiments, I am not convinced that it is trustworthy for more complex real-world datasets. Equation (3) looks problematic. Note that it is possible to train a Bidirectional Generative Adversarial Network (BiGAN) that can generate complex images based on a uniform distribution (Donahue et al., 2016). The encoder of the BiGAN can be considered as the inverse of the generator, which maps images back to the uniform distribution. This suggests that under the encoder-decoder framework, it is possible that latent variables can be informative even the posterior distribution matches the prior distribution. Although VAEs are trained using a different strategy, I do not see why the posterior needs to diverge from the prior distribution for informative latent representations. The encoder might simply be the inverse of the decoder under a certain scenario. In summary, this paper focuses on solving an important problem. However, the proposed method is not well supported by theorems as it seems. The paper also appears to contain minor technical issues. Therefore, I am inclined to reject this paper. References Donahue, Jeff, Philipp Kr\u00e4henb\u00fchl, and Trevor Darrell. \"Adversarial feature learning.\" arXiv preprint arXiv:1605.09782 (2016).", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer , Thank you for your thoughtful comments . In your feedback you have brought up three major points : 1 ) you have expressed doubt whether the \u201c polarised regime \u201d holds for other disentangling methods other than \\beta-VAE ; 2 ) you were wondering what underlies the thinking behind the choice of our computational process ; and 3 ) you were wondering about our choice of using per latent KL divergence to identify which latents are informative . We will address these points below . 1 ) You have expressed doubt whether the \u201c polarised regime \u201d holds for other disentangling methods other than \\beta-VAE , which was used as the example model in Rolinek et al.First , even the vanilla VAEs ( Kingma and Welling , 2014 ) are known to enter the \u201c polarised regime \u201d , which is often cited as one of their shortcomings ( e.g.see Rezende and Viola , 2018 ) . All of the disentangling VAEs considered in the paper , including TC-VAE , augment the original ELBO objective with extra terms . Hence , all of them still contain the \\beta KL term of the ELBO with \\beta = > 1 . This means that in theory all of them inherit the property of the original VAEs of operating in the \u201c polarised regime \u201d . We have tested this empirically by counting the number of latents that are \u201c switched off \u201d in each of the 5400 models considered in our paper . We found that all models apart from DIP-VAE-I entered the polarised regime , having on average 2.95/10 latents \u201c switched off \u201d ( with a standard deviation of 1.97 ) . Note that DIP-VAE-I is the only model with an objective that explicitly penalises \u201c switching off \u201d latent dimensions , which means that it is less suitable for disentangled representation learning in the common regime where the number of generative factors is smaller than the number of latents ( as discussed in the original paper by Kumar et al , 2018 ) . Despite DIP-VAE-I never entering the polarised regime , the results reported in our paper ( e.g.in Fig.2 or Fig.6 in Supplementary Materials ) suggest that our proposed UDR still performs well and correlates highly with the supervised metrics for this model class . 2 ) The computational process proposed in our paper is motivated by the theoretical results presented in Rolinek et al.Unfortunately there is no computationally feasible way to calculate directly whether the SVD decomposition J=U\u03a3V of the Jacobian J of the decoder results in a V , which is a signed permutation matrix . Our approach uses a simple process to approximate this in a computationally feasible way . We have applied our proposed method to a number of datasets commonly used in the literature and have demonstrated that it performs well across 5400 models . Please let us know if you have a particular suggestion for a more complex dataset that you would like to test our metric on . 3 ) In terms of Equation 3 , the GAN objective in the BiGAN paper implicitly minimises the KL divergence between the prior p ( z ) and the marginalised posterior q ( z ) . However , in Eq.3 we measure the KL divergence between the prior and the conditional posterior q ( z|x ) . Hence , the two are not directly comparable . Eq.3 is a way to quantify which latent dimensions are used by the network that has entered the \u201c polarised regime \u201d . Rolinek et al define a model to be in the \u201c polarised regime \u201d if its latents can be split into two disjoint sets of \u201c used \u201d and \u201c unused \u201d dimensions . \u201c Used \u201d dimensions are defined as those which have inferred \\sigma^2 < < 1 , and the \u201c unused \u201d dimensions are defined as those which have \\mu^2 < < 1 and \\sigma^2 \\approx 1 ( see Sec 3.2 , Definition 1 in Rolinek et al ) . Note that the latter would result in a small KL from a unit Gaussian prior as per Eq.3 in our paper , thus justifying our choice to find the \u201c used \u201d and \u201c unused \u201d latents . Finally , we have modified the abstract and introduction to mention our proposed method as per your suggestion ."}, "2": {"review_id": "SyxL2TNtvr-2", "review_text": "This paper addresses the problem of unsupervised model selection for disentangled representation learning. Based on the understanding of \u201cwhy VAEs disentangle\u201d [Burgess et al. 2017, Locatello et al. 2018, Mathieu et al. 2019, Rolinek et al. 2019], the authors adopt the assumption that disentangled representations are all alike (up to permutation and sign inverse) while entangled representations are different, and propose UDR method and its variants. Experimental results clearly show that UDR is a good approach for hyperparameter/model selection. Overall, I think a reliable metric for model selection/evaluation is needed for the VAE-based disentangled representation learning. According to comprehensive experimental studies performed in this paper, UDR seems to be a potentially good choice. However, I am not sure if very good disentangled representations must benefit (general) subsequent tasks, though the authors provide experimental evidence on fairness classification and data efficiency tasks. Actually, the data generation process in the real-world may consist of different generative factors that are not independent of each other. Though good disentangled representation provides good interpretability, it needs not to be better than entangled representation for concrete tasks. Specifically, for concrete supervised classification tasks, VAE with beta smaller than 1 (not towards disentanglement) might be the best (Alexander A. Alemi et al. 2017, Deep VIB). Another concern is about the choice of some key \u201chyperparameters\u201d. For the KL divergence threshold in equation 3, you set it to be 0.01. It looks like the choice would control how much the UDR favors a \u201csparse representation map\u201d. The larger the value, the few \u201cinformative dimensions\u201d would be considered. In supplementary material, you say that \u201cuninformative latents typically have KL<<0.01 while informative latents have KL >>0.01\u201d. Is this judgment based on \u201cqualitative feeling\u201d? For me, as you are contributing a \"quantitative measurement\u201d, it is interesting and important to see how this threshold would generally affect UDR\u2019s behavior in one (or more) datasets you have tried. Another hyperparameter I cared is P (number of models for pairwise comparison). In the paper, you validate the effect of P in the range [5,45]. How would P smaller than 5 affect UDR? According to Table 1, if I was using UDR, I\u2019d rather using P>=20 (or at least 10) rather than 5. Also, it seems to me P would grow up due to the size of factors that generate the data. Thus, I also have a little concern about the computation cost of the proposed metric (as also mentioned by the authors). Others concerns: -- As a heavy experimental paper, most experimental results are in supplementary material, while the authors spent a lot of time in the main text explaining the conclusions found in other papers. -- To validate the fundamental assumption of UDR, the authors might consider to quantitatively validate that, disentangled representations learned by those approaches you used in the paper are almost the same (up to permutation and sign inverse). ", "rating": "6: Weak Accept", "reply_text": "Dear Reviewer , Thank you for your thoughtful comments . In your feedback you have brought up two points : 1 ) you were wondering whether disentangled representations would benefit subsequent tasks ; 2 ) you were wondering about our choice of certain hyperparameters . We will address these questions below : 1 ) In terms of whether disentangled representations would benefit subsequent tasks , we believe that it is important to consider the nature of the task , and whether it implicitly assumes any of the properties that disentangled representations possess . For example , if one is trying to solve a binary classification task based on the value of a single pixel in a high-dimensional image , it is unlikely that a disentangled representation will be useful . Indeed , a disentangled representation will most likely learn to ignore this pixel , since it doesn \u2019 t contribute much to the quality of the reconstruction . On the other hand , an entangled representation learnt implicitly through a supervised objective aiming to solve the task will throw away all information apart from the value of the relevant pixel and hence will be much more informative for that particular task . On the other hand , if one is interested in solving a large number of natural tasks in a single environment ( e.g.learning to achieve different values of the score in an Atari game , generalising policies to variations in the game colour schemes , fast language binding problems , data efficient classification of object identities , colours , sizes or relations ) , then a disentangled representation may be of more relevance , since it will produce the semantically meaningful equivariant compositional representation that will support many variations of these tasks . Hence , we believe that disentangled representations will be useful for those tasks that require compositionality , generalisation , data efficiency or generalisation/transfer . You were also wondering whether the real world data generative process follows the independence assumption presumed by disentangled representations . To answer this question we would like to refer you to the recently proposed alternative view on disentangled representations that moves away from considering independent generative factors ( Higgins et al , 2018 ) . The new definition suggests that disentangled representations instead reflect the compositional natural symmetry transformations . The implication of this definition is that one can move away from assuming IID training data generated by independent generative factors , and instead think about which aspects of the world can be transformed independently of each other , and how these transformations can be discovered through embodied active learning ( see Caselles-Dupre et al , 2019 for a first effort in that direction ) . 2 ) You were wondering about the choice of the 0.01 threshold in Eq.3 , and whether it was set using a \u201c qualitative feeling \u201d . The answer is no . This equation quantifies which latent dimensions are used by the network that has entered a \u201c polarised regime \u201d . Rolinek et al define a model to be in a \u201c polarised regime \u201d if its latents can be split into two disjoint sets of \u201c used \u201d and \u201c unused \u201d dimensions . \u201c Used \u201d dimensions are defined as those which have inferred \\sigma^2 < < 1 , and the \u201c unused \u201d dimensions are defined as those which have \\mu^2 < < 1 and \\sigma^2 \\approx 1 ( see Sec 3.2 , Definition 1 ) . Note that the latter would result in a small KL from a unit Gaussian prior as per Eq.3.Empirically we found that 0.01 was a good threshold , since the KL values have a bimodal distribution , with on average around 97 % of all \u201c small kl \u201d values lying below this threshold . In terms of P , we do not suggest using P < 5 . Training P seeds per hyperparameter setting is the largest computational overhead of the UDR , however it is subsumed by the largely accepted good research practice of training a number of seeds per hyperparameter setting anyway . The rest of the UDR computations are very fast . To give you an idea , running a single pairwise comparison using UDR Lasso takes around 4 seconds on a standard CPU , and it takes around 1600 seconds on average to compute UDR Lasso with P=50 for 300 models within a hyperparameter sweep , when we parallelise the pairwise comparisons per model . Finally , how would you propose that we quantitatively evaluate whether the representations learnt by the models are the same up to the UDR assumptions apart from running the UDR itself ? Maybe we misunderstood your question ..."}}