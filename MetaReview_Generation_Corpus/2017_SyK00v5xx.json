{"year": "2017", "forum": "SyK00v5xx", "title": "A Simple but Tough-to-Beat Baseline for Sentence Embeddings", "decision": "Accept (Poster)", "meta_review": "A new method for sentence embedding that is simple and performs well. Important contribution that will attract attention and help move the field forward.", "reviews": [{"review_id": "SyK00v5xx-0", "review_text": "This is a good paper with an interesting probabilistic motivation for weighted bag of words models. The (hopefully soon) added comparison to Wang and Manning will make it stronger. Though it is sad that for sufficiently large datasets, NB-SVM still works better. In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f. Minor comments: \"The capturing the similarities\" -- typo in line 2 of intro. \"Recently, (Wieting et al.,2016) learned\" -- use citet instead of parenthesized citation ", "rating": "7: Good paper, accept", "reply_text": "Thanks for the positive review ! We will fix the typos pointed out and add the comparisons with NB-SVM ."}, {"review_id": "SyK00v5xx-1", "review_text": "This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work. Here are some comments on technical details: - The word \"discourse\" is confusing. I am not sure whether the words \"discourse\" in \"discourse vector c_s\" and the one in \"most frequent discourse\" have the same meaning. - Is there any justification about $c_0$ related to syntac? - Not sure what thie line means: \"In fact the new model was discovered by our detecting the common component c0 in existing embeddings.\" in section \"Computing the sentence embedding\" - Is there any explanation about the results on sentiment in Table 2?", "rating": "7: Good paper, accept", "reply_text": "Thanks for the positive review ! - The `` discourse '' in `` discourse vector c_s '' and the one in `` most frequent discourse '' have the same meaning . - The top singular vector $ c_0 $ of the datasets seems to roughly correspond to the syntactic information or common words . Closest words ( by cosine similarity ) to $ c_0 $ in the SICK dataset are : `` just '' `` when '' `` even '' `` one '' `` up '' `` little '' `` way '' `` there '' `` while '' `` but '' . - Sorry for the clumsy phrasing . We meant to say that we empirically discovered the significant common component $ c_0 $ in word vectors built by existing methods , which inspired us to propose our theoretical model of this paper . - We speculate that our method does n't outperform RNNs and LSTMs for sentiment tasks because ( a ) the word vectors -- -and more generally the distributional hypothesis of meaning -- -has known limitations for capturing sentiment due to the `` antonym problem '' , ( b ) also in our weighted average scheme , words like `` not '' that may be important for sentiment analysis are downweighted a lot ."}, {"review_id": "SyK00v5xx-2", "review_text": "This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too. Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the positive review ! We agree that our results suggest the benchmarks need to be rethought . At the same time , we did a simple experiment suggesting that word order does play some role in the benchmarks . We trained and tested RNN/LSTM on the supervised tasks where the words in each sentence are randomly shuffled . Performance drops noticeably . Thus our method -- -which does ignore word order -- -must be much better at exploiting the semantics than RNN/LSTM . We will further explore if some ensemble idea can combine the advantages of both approaches . 1 ) Pearson \u2019 s r x 100 on similarity task : rnn lstm ( no ) lstm ( o.g . ) random : 54.50 77.24 79.39 normal : 73.13 85.45 83.41 2 ) accuracy on entailment task : rnn lstm ( no ) lstm ( o.g . ) random : 61.7 78.2 81.0 normal : 76.4 83.2 82.0 3 ) accuracy on sentiment task : rnn lstm ( no ) lstm ( o.g . ) random : 84.2 82.9 84.1 normal : 86.5 86.6 89.2 Technical detail : on the random order datasets , the hyperparameters are enumerated as in the paper , and the best results are reported ."}], "0": {"review_id": "SyK00v5xx-0", "review_text": "This is a good paper with an interesting probabilistic motivation for weighted bag of words models. The (hopefully soon) added comparison to Wang and Manning will make it stronger. Though it is sad that for sufficiently large datasets, NB-SVM still works better. In the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the Glove embeddings with their weighting function f. Minor comments: \"The capturing the similarities\" -- typo in line 2 of intro. \"Recently, (Wieting et al.,2016) learned\" -- use citet instead of parenthesized citation ", "rating": "7: Good paper, accept", "reply_text": "Thanks for the positive review ! We will fix the typos pointed out and add the comparisons with NB-SVM ."}, "1": {"review_id": "SyK00v5xx-1", "review_text": "This paper proposes a simple way to reweight the word embedding in the simple composition function for sentence representation. This paper also shows the connection between this new weighting scheme and some previous work. Here are some comments on technical details: - The word \"discourse\" is confusing. I am not sure whether the words \"discourse\" in \"discourse vector c_s\" and the one in \"most frequent discourse\" have the same meaning. - Is there any justification about $c_0$ related to syntac? - Not sure what thie line means: \"In fact the new model was discovered by our detecting the common component c0 in existing embeddings.\" in section \"Computing the sentence embedding\" - Is there any explanation about the results on sentiment in Table 2?", "rating": "7: Good paper, accept", "reply_text": "Thanks for the positive review ! - The `` discourse '' in `` discourse vector c_s '' and the one in `` most frequent discourse '' have the same meaning . - The top singular vector $ c_0 $ of the datasets seems to roughly correspond to the syntactic information or common words . Closest words ( by cosine similarity ) to $ c_0 $ in the SICK dataset are : `` just '' `` when '' `` even '' `` one '' `` up '' `` little '' `` way '' `` there '' `` while '' `` but '' . - Sorry for the clumsy phrasing . We meant to say that we empirically discovered the significant common component $ c_0 $ in word vectors built by existing methods , which inspired us to propose our theoretical model of this paper . - We speculate that our method does n't outperform RNNs and LSTMs for sentiment tasks because ( a ) the word vectors -- -and more generally the distributional hypothesis of meaning -- -has known limitations for capturing sentiment due to the `` antonym problem '' , ( b ) also in our weighted average scheme , words like `` not '' that may be important for sentiment analysis are downweighted a lot ."}, "2": {"review_id": "SyK00v5xx-2", "review_text": "This paper presents a new theoretically-principled method of representing sentences as vectors. The experiments show that vectors produced by this method perform well on similarity and entailment benchmarks, surpassing some RNN-based methods too. Overall, this is an interesting empirical result, especially since the model is not order-sensitive (as far as I can tell). I would like to see some more discussion on why such a simple model does better than LSTMs at capturing similarity and entailment. Could this be an artifact of these benchmarks?", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the positive review ! We agree that our results suggest the benchmarks need to be rethought . At the same time , we did a simple experiment suggesting that word order does play some role in the benchmarks . We trained and tested RNN/LSTM on the supervised tasks where the words in each sentence are randomly shuffled . Performance drops noticeably . Thus our method -- -which does ignore word order -- -must be much better at exploiting the semantics than RNN/LSTM . We will further explore if some ensemble idea can combine the advantages of both approaches . 1 ) Pearson \u2019 s r x 100 on similarity task : rnn lstm ( no ) lstm ( o.g . ) random : 54.50 77.24 79.39 normal : 73.13 85.45 83.41 2 ) accuracy on entailment task : rnn lstm ( no ) lstm ( o.g . ) random : 61.7 78.2 81.0 normal : 76.4 83.2 82.0 3 ) accuracy on sentiment task : rnn lstm ( no ) lstm ( o.g . ) random : 84.2 82.9 84.1 normal : 86.5 86.6 89.2 Technical detail : on the random order datasets , the hyperparameters are enumerated as in the paper , and the best results are reported ."}}