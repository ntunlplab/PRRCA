{"year": "2021", "forum": "eU776ZYxEpz", "title": "Learning to live with Dale's principle: ANNs with separate excitatory and inhibitory units", "decision": "Accept (Poster)", "meta_review": "This paper was unanimously rated above the acceptance threshold by the\nreviewers.  While all reviewers agree it is worth accepting, they\ndiffered in their enthusiasm.  Most reviewers agree that  major\nlimitations of the paper include that the paper provides no insight into why\nDale's principle exists and the actual results are not truly\nstate-of-the-art.  Nevertheless there is agreement that the paper\npresents results worth publicizing to the ICLR audience.  The comparison\nof the inhibitory network to normalization schemes is interesting.\nAlso, please reference the Neural Abstraction Pyramid work.\n\n", "reviews": [{"review_id": "eU776ZYxEpz-0", "review_text": "Most neurons in the brains are either excitatory ( E ) or inhibitory ( I ) - sometimes referred to as Dale \u2019 s law . Practically Dale \u2019 s principle is often left out of Artificial Neural Networks ( ANNs ) because having the E and I separation often impairs learning , although this has not been well documented in the literature ( probably due to that this is also interpreted as a negative result ) . In this paper , the authors propose a new scheme to construct and train the feedforward E/I network by incorporating several ingredients , including feedforward inhibition and E/I balance among others . It is shown that this particular kind of E/I networks ( DANNs ) trained on MNIST and variations of MNIST could achieve a level of performance that is comparable to those without E/I separation . Quality : I think this is an interesting submission of good quality , with some novel ideas and promising preliminary results . Clarity : The writing is generally clear . Originality : As far as I can tell , the results are original . Significance : Although the results are promising , I have reservations about the significance of these results as the performance of the models are still worst than the standard ANNs . Pros : 1.To my knowledge , this is the first E/I network that could achieve comparable performance with the standard ANN model on MNIST task ( although at the same time , I have to say that not too many papers have studied and reported this issue ) . 2.The ingredients in the proposed model is well motivated in neuroscience , such as the feedforward inhibition , and E/I balance , as well as no connections between I neurons across the different layers . 3.The results on the MNIST and its variations look promising . 4.The paper is fairly well written and the basic ideas are clear . Cons : 1.The role of the subtractive and divisive components need to be better explained . Are both of them necessary for getting the results shown later ? 2.The authors assume the number of E neurons is far larger than that of the I neurons . This is not quite true in physiology . The E/I ratio reported is often around 4:1 . The authors assumed 10 % of neurons are I neurons- this is on the smaller end . Another related concern is that , in cortex , despite of a smaller number , I neurons are often responsible for controlling the dynamics/computation due to the dense connectivity from I to E neurons . I am a little bit worried that the paper is studying a quite different regime , in which the E neurons are dominating . Also , would adding more I neurons decrease the performance of the network ? If that is the case , that would be concerning . 3.The initialization of E/I network has been carefully studied previously in the context of training balanced E/I recurrent neural networks ( e.g. , Ingrosso & Abbott , 2019 , which the authors cited ) . How does the authors scheme different from the previous work ? 4.The method assumes inhibitory units are linear units . Several questions arise . First , is this a mathematical issue or a numerical issues ? Second , does this imply the firing rate of inhibitory neuron can be both positive and negative ? 5.In fig4 , DANN performs significantly worse than LayerNorm and BathNorm . 6.The algorithms is not tested on slightly more challenging benchmark datasets such as CIFAR10 or ImageNet . Relatedly , would DANN scale up to larger networks ? Questions to be clarified : * Are their connections between the I neurons within the same layers ? * page 4 , \u201c Unlike a column constrained network , a layer in a DANN is not restricted in its potential function space . \u201c - It is unclear what this sentence means\u2026 * Between Eq 4 and Eq 5 , the authors mentioned the exponential family . What particular distribution was used ? Gaussian or any exponential family distribution would produce similar results ? * The authors wrote : \u201c As a result , inhibitory unit parameters updates are scaled down relative to excitatory parameter updates . This is intriguing given the differences between inhibitory and excitatory neuron plasticity\u2026including the relative extent of weight changes in excitatory and inhibitory neurons ( McBain et al. , 1999 ) . \u201d I think these comparisons to the neuroscience literature are too vague and potentially mis-leading . To make this useful , it would helpful to make the comparison more specific and clear . * I am worried that the experiments for the ColumnEi model was not treated fairly . In section 5.1 , it is mentioned that 50 columns are negative . Did the authors try to make increase this number to see if the performance would be improved for the ColumnEi model ? * * * * * * * * * updated after rebuttal period I still consider this as an interesting contribution , and stand with my original rating . It would be useful if the discrepancies and similarity between the connectivity structures in the model and the anatomy could be more carefully discussed in the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* 5 ) In fig4 , DANN performs significantly worse than LayerNorm and BathNorm . * We would argue that figure 4 shows equivalent performance on K-MNIST . But , the reviewer is correct that the DANN performance is not quite as good as LayerNorm and BatchNorm on Fashion-MNIST . However , we would note that they are actually quite close . For example , if the reviewer looks at Table 3 , they will see that on the test set DANNs achieved an error rate of 10.962 +/- 0.365 , compared to 10.445 +/- 0.455 for LayerNorm and 9.992 +/- 0.218 for BatchNorm , which is a real difference , but arguably not huge . For comparison , the column constrained ANN achieved only 14.986 +/- 0.674 . Moreover , we would note that the DANN performance was within the standard deviation of the MLP performance . * 6 ) The algorithms is not tested on slightly more challenging benchmark datasets such as CIFAR10 or ImageNet . Relatedly , would DANN scale up to larger networks ? * While , to our knowledge , this is the first paper to show ANNs that obey Dale \u2019 s principle learning as well as standard ANNs on simple tasks , this is a very important question . In-line with our response to Reviewer 3 , comment 3 , we note that the corrections derived in the paper apply to convolutional networks , and we have been running experiments on deep convolutional DANNs trained on CIFAR-10 . We have preliminary data ( see attached figure 2 in https : //pdfhost.io/v/AFBEMscCX_DANN_Preliminary_Responsepdf.pdf ) suggesting that DANN convnets have performance approximately equal to that of standard convnets on this dataset . We intend on running a more thorough set of experiments following on this preliminary data ( with full hyperparameter tuning ) , though this may take some time . We propose to add discussion of how to apply the DANN formalism to convnets , and if the reviewers feel that it is important , we can include the results of these experiments in the camera ready version of the manuscript . * 7 ) Are their connections between the I neurons within the same layers ? * No , there are not . We will clarify this in the paper . * 8 ) page 4 , \u201c Unlike a column constrained network , a layer in a DANN is not restricted in its potential function space . \u201c - It is unclear what this sentence means\u2026 * We can see how this sentence is unclear . What we mean by this is that in a column constrained network there are literally many functions that a single layer can not approximate , because the linear operation is constrained to matrices with columns that have only positive or negative signs . In contrast , in DANNs , the initial linear integration in the excitatory units can match any linear function . We propose expanding this sentence to clarify this point . * 9 ) Between Eq 4 and Eq 5 , the authors mentioned the exponential family . What particular distribution was used ? Gaussian or any exponential family distribution would produce similar results ? * Our mathematical analysis only assumes any distribution from the natural exponential family ( the exponential family with T ( y ) = y , see footnote 1 ) . So , it applies equally to any such distribution . This group of distributions includes the Gaussian , Poisson , gamma , and binomial distributions . We propose to clarify this point in the footnote . * 10 ) The authors wrote : \u201c As a result , inhibitory unit parameters updates are scaled down relative to excitatory parameter updates . This is intriguing given the differences between inhibitory and excitatory neuron plasticity\u2026including the relative extent of weight changes in excitatory and inhibitory neurons ( McBain et al. , 1999 ) . \u201d I think these comparisons to the neuroscience literature are too vague and potentially mis-leading . To make this useful , it would helpful to make the comparison more specific and clear . * This is a fair point . What we were referring to , ultimately , was the fact that inhibitory plasticity is fairly difficult to achieve experimentally . Indeed , in the past , many neuroscientists thought that it did not exist ( such as McBain et al , 1999 ) . Thus , all we intended to refer to here was the apparent reduced plasticity at inhibitory synapses in the brain . We will clarify this in this section . * 11 ) I am worried that the experiments for the ColumnEi model was not treated fairly . In section 5.1 , it is mentioned that 50 columns are negative . Did the authors try to make increase this number to see if the performance would be improved for the ColumnEi model ? * This is an important point to clarify . To test this question , we ran additional experiments with ColumnEi models that contain 100 negative columns . We find that these models learn just as poorly as the other ColumnEi models . Please see the attached figure 1 , table 1 in https : //pdfhost.io/v/AFBEMscCX_DANN_Preliminary_Responsepdf.pdf . We will include these results in the revised paper ."}, {"review_id": "eU776ZYxEpz-1", "review_text": "Summary : It is shown that Dale \u2019 s principle can be observed in feedfoward ANNs if one uses inhibitory neurons in the form of feedforward inhibition , while the other neurons are purely excitatory . Pros : This is a nice and new insight . It appears to be useful for understanding the design of biological neural networks , and at least one type of uses of inhibitory neurons in them . Cons : Apparently this insight provides no benefit for designing ANN . Furthermore the biological insight is rather limited because biological neural networks are not feedforward networks . Also , the chosen tasks ( 3 variations of MNIST ) are relatively simple , and are solved with relatively shallow networks , with just 4 hidden layers . In my view this evaluation does not support the much more general claim in the Abstract that \u201e ANN \u2019 s that respect Dale \u2019 s principle can be built without sacrificing learning performance \u201c .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are very happy that the reviewer found our paper insightful , and we thank them for their constructive critiques . Our responses are as follows : * 1 ) Apparently this insight provides no benefit for designing ANN . * Indeed , as the reviewer notes here , we did not observe better performance with DANNs than with standard ANNs . Of course , the goal of this paper was to close the gap in learning performance between standard ANNs and ANNs that obey Dale \u2019 s principle . One reason that this is important is simply that ANNs that obey Dale \u2019 s principle , but which are not impaired at learning relative to normal ANNs , will be a useful tool for neuroscience research . However , we also wonder about potential computational benefits to Dale \u2019 s principle , and hope in future work to explore this possibility . We see this paper , which closes the learning gap , as a key initial step towards these future investigations . Please see also our reply to Reviewer 2 , comment 2 for more discussion on this matter . * 2 ) Furthermore the biological insight is rather limited because biological neural networks are not feedforward networks . * The reviewer is correct that real neural circuits are typically recurrent , and thus , it would be beneficial to also consider how DANNs can operate within the recurrent context . However , thanks to the mathematical similarity between a multilayer feedforward neural network and a recurrent neural rolled out through time ( see e.g.Liao and Poggio , 2016 , https : //arxiv.org/abs/1604.03640 ) , making this connection is relatively straightforward . In fact , our formulation of DANNs is fully applicable to recurrent neural networks , thanks to these connections . We propose to add a section to the appendix describing how our formulations can be ported to the case of recurrent neural networks . If the reviewers agree that this is a good idea , we will include this in our revised manuscript . Please see also our response to Reviewer 2 , comment 1 . * 3 ) Also , the chosen tasks ( 3 variations of MNIST ) are relatively simple , and are solved with relatively shallow networks , with just 4 hidden layers . In my view this evaluation does not support the much more general claim in the Abstract that \u201e ANN \u2019 s that respect Dale \u2019 s principle can be built without sacrificing learning performance \u201c . * We agree that the tasks we explored here were relatively simple . However , as noted by Reviewer 4 , despite these tasks being simple this is , to our knowledge , the first paper to show ANNs that obey Dale \u2019 s principle that can learn on these tasks as well as standard ANNs . Nonetheless , to expand on our results , we have been exploring the use of DANN style architectures in deep convolutional networks . First we note that the response of a convolutional network can be expressed as a normal matrix multiplication where the rows of the weight matrix correspond to convolutional filters , and the columns of the input matrix correspond to the different filter locations . As such , we can readily express the same DANN formulation for convolutional networks . Second , we have preliminary results showing that learning in DANN convnets is approximately as good as learning in regular convnets ( see the attached figure 2 in https : //pdfhost.io/v/AFBEMscCX_DANN_Preliminary_Responsepdf.pdf ) . We propose to add discussion of how to apply the DANN formalism to convnets , and if the reviewers feels it is important , we can include a more thorough version of this data ( after appropriate hyperparameter optimization ) in the final version of the paper . Though , we note that full hyperparameter optimization and experimentation will take some time , so the final results of these experiments may not be ready by next week ."}, {"review_id": "eU776ZYxEpz-2", "review_text": "This is a great investigation on how to scale the gain of the inhibitory weights to balance the impact that the changes that the excitatory and inhibitory connections have on the layer \u2019 s output . I think using the KL distance that naturally connects with the Fisher Information is neat . I appreciate the effort that the authors make to connect the manner neural circuits are designed and connect it with ANN . You never know when the breakthrough can arise . I love the experiments that the authors present illustrating with clarity the impact that having the proper gain modulation of the inhibitory changes have in the speed of convergence . My single constructive criticism is that the inspiration in cortical circuits do not prevent the authors to get inspiration from smaller neural circuits like in insects for example . The Mushroom Bodies of the insects are the equivalent of the cortex and present feedforward inhibition . The number of layers is much smaller but the neural principles that operate are fairly consistent across multiple animal species . Drawing from that experience , the mutual inhibition within layer may provide a natural mechanism to keep balance in the output distribution as shown for example in mean field models that investigate the regulation of activity in a dynamical neural layer ( see for example https : //journals.plos.org/ploscompbiol/article ? id=10.1371/journal.pcbi.1003133 ) . Other that this comment I learn and enjoy from reading this paper . I think it should be accepted .", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank the reviewer for their kind and constructive comments . We fully agree that non-cortical circuits can provide equal inspiration for these investigations and we will specifically add statements and references to that effect , e.g.noting the mushroom body of insects . Moreover , we will discuss the interesting observation that there may be very general principles at play with respect to maintaining balanced output via mutual inhibition , reference the paper the reviewer noted , and propose this as a future extension of our work ."}, {"review_id": "eU776ZYxEpz-3", "review_text": "Inspired by the observations of feedforward inhibition in the brain , the authors propose a novel ANN architecture that respects Dale \u2019 s rule ( DANN ) . They provide two improvements for training DANNs : better initialization and update scaling for synaptic weights . As a result , they empirically demonstrate that DANNs perform no worse than the ANNs that do not respect Dale \u2019 s rule . Although , I find the contribution interesting , my enthusiasm is tempered by the following two issues : 1 . Although feedforward inhibition has its place in the brain , most connections of inhibitory interneurons with excitatory neurons are reciprocal , resulting in feedback inhibition . Therefore , feedforward inhibition seems like a secondary factor here . 2.The DANNs are shown to be just no worse than ANNs that do not respect Dale \u2019 s rule . If biology \u201c invested the effort \u201d to evolve inhibitory interneurons respecting Dale \u2019 s rule , this is probably because they confer a computational advantage , not just lack of disadavantage . The formulation of Dale \u2019 s rule on page 1 is not consistent with the current biological knowledge . A better version would be : \u201c A neuron releases the same fast neurotransmitter at each of its pre-synaptic terminals \u201d . Note that this does not mean that the action of a neuron is always excitatory or always inhibitory on all of its post-synaptic partners . It is possible , as often the case in invertebrates , that different post-synaptic partners have different receptors resulting in de- or hyper-polarization in different post-synaptic neurons . Although the paper is generally well written , the authors could make it clearer . In particular , it would help if they defined symbols such as the circled dot or variables such as y when they are first used .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their comments , and are happy that they found our paper interesting . The reviewer \u2019 s comments raise important questions . Our responses to these points are as follows : * 1 ) Although feedforward inhibition has its place in the brain , most connections of inhibitory interneurons with excitatory neurons are reciprocal , resulting in feedback inhibition . Therefore , feedforward inhibition seems like a secondary factor here . * The reviewer is correct that reciprocal/feedback inhibition is an important component of inhibition in the brain . We are not sure that it is fair to say that feedforward inhibition is a secondary factor , as there is ample evidence showing that feedforward inhibition is a critical , and plastic , regulator of responses in numerous circuits across the brain ( see e.g.Pouille et al.2009 , Nature Neuroscience , 12:1577 , 2009 or Hennequin et al.2017 , Annual Review of Neuroscience , 40:557-579 for a review related to plasticity ) . Indeed , the evidence suggests that feedforward inhibition can be the critical factor for determining early responses in neural circuits ( Pouille & Scanziani , 2001 , Science , 293 : 1159\u20131163 ) . Of course , learning of feedback inhibition is also important , particularly for maintaining dynamic balance and for shaping responses over time ( as also explained in Hennequin et al.2017 ) .Thus , the reviewer is correct that including feedback inhibition would be ideal . Importantly , though , we note though that our formulation for DANNs can still be applied to feedback inhibition . Recurrent neural networks obey many of the same mathematical principles as multi-layer feedforward neural networks ( see e.g.Liao and Poggio , 2016 , https : //arxiv.org/abs/1604.03640 ) . If we imagine unrolling a recurrent neural network with separate excitatory and inhibitory populations , then the feedback inhibition could be treated exactly like feedforward inhibition , but with \u201c layers \u201d corresponding to timesteps . Thus , all of our mathematical formulations and analyses would still hold for the unrolled recurrent network . Given this important point , if the reviewer agrees , we will add a section in a revised version of the Appendix explaining how our formulation of DANNs can be used to model feedback inhibition . * 2 ) The DANNs are shown to be just no worse than ANNs that do not respect Dale \u2019 s rule . If biology \u201c invested the effort \u201d to evolve inhibitory interneurons respecting Dale \u2019 s rule , this is probably because they confer a computational advantage , not just lack of disadavantage . * This is a very interesting issue that the reviewer raised , and it generated a lot of discussion amongst the authors . After discussing the matter , what we would say is that it is unclear whether Dale \u2019 s principle represents an \u201c investment of effort \u201d by biology or not . Though it is easy to think about possible ways to avoid Dale \u2019 s principle using known physiological mechanisms , it may also represent an evolutionary local minima , whereby early phylogenetic choices led to constraints on the system that were difficult to evolve away . This is the opinion of some of the authors . However , the reviewer may also be right that Dale \u2019 s principle does confer a computational advantage to real brains , which is why evolution kept it around . This is , in fact , the opinion of the majority of the authors . We think that future work should investigate potential advantages to Dale \u2019 s principle more thoroughly . However , this was not the goal of this study , which was instead to solve the problem of ANNs with separate excitatory and inhibitory units performing worse when trained with gradient descent . Indeed , it is hard to see how we can understand the potential computational advantages of ANNs that obey Dale \u2019 s principle if they are actually poor at learning relative to normal ANNs . Thus , we see our work as a necessary first step to future studies that could more thoroughly explore potential advantages to Dale \u2019 s principle . If the reviewer thinks it is important to include in the paper , we would add discussion of this matter to a revised version . * 3 ) The formulation of Dale \u2019 s rule on page 1 is not consistent with the current biological knowledge . A better version would be : \u201c A neuron releases the same fast neurotransmitter at each of its pre-synaptic terminals \u201d . Note that this does not mean that the action of a neuron is always excitatory or always inhibitory on all of its post-synaptic partners . It is possible , as often the case in invertebrates , that different post-synaptic partners have different receptors resulting in de- or hyper-polarization in different post-synaptic neurons . * We agree with the reviewer , thank you for noting this point . We will adjust the language in the introduction to recognize the fact that there are neural circuits where the same neurotransmitters can affect different postsynaptic neurons differently ."}], "0": {"review_id": "eU776ZYxEpz-0", "review_text": "Most neurons in the brains are either excitatory ( E ) or inhibitory ( I ) - sometimes referred to as Dale \u2019 s law . Practically Dale \u2019 s principle is often left out of Artificial Neural Networks ( ANNs ) because having the E and I separation often impairs learning , although this has not been well documented in the literature ( probably due to that this is also interpreted as a negative result ) . In this paper , the authors propose a new scheme to construct and train the feedforward E/I network by incorporating several ingredients , including feedforward inhibition and E/I balance among others . It is shown that this particular kind of E/I networks ( DANNs ) trained on MNIST and variations of MNIST could achieve a level of performance that is comparable to those without E/I separation . Quality : I think this is an interesting submission of good quality , with some novel ideas and promising preliminary results . Clarity : The writing is generally clear . Originality : As far as I can tell , the results are original . Significance : Although the results are promising , I have reservations about the significance of these results as the performance of the models are still worst than the standard ANNs . Pros : 1.To my knowledge , this is the first E/I network that could achieve comparable performance with the standard ANN model on MNIST task ( although at the same time , I have to say that not too many papers have studied and reported this issue ) . 2.The ingredients in the proposed model is well motivated in neuroscience , such as the feedforward inhibition , and E/I balance , as well as no connections between I neurons across the different layers . 3.The results on the MNIST and its variations look promising . 4.The paper is fairly well written and the basic ideas are clear . Cons : 1.The role of the subtractive and divisive components need to be better explained . Are both of them necessary for getting the results shown later ? 2.The authors assume the number of E neurons is far larger than that of the I neurons . This is not quite true in physiology . The E/I ratio reported is often around 4:1 . The authors assumed 10 % of neurons are I neurons- this is on the smaller end . Another related concern is that , in cortex , despite of a smaller number , I neurons are often responsible for controlling the dynamics/computation due to the dense connectivity from I to E neurons . I am a little bit worried that the paper is studying a quite different regime , in which the E neurons are dominating . Also , would adding more I neurons decrease the performance of the network ? If that is the case , that would be concerning . 3.The initialization of E/I network has been carefully studied previously in the context of training balanced E/I recurrent neural networks ( e.g. , Ingrosso & Abbott , 2019 , which the authors cited ) . How does the authors scheme different from the previous work ? 4.The method assumes inhibitory units are linear units . Several questions arise . First , is this a mathematical issue or a numerical issues ? Second , does this imply the firing rate of inhibitory neuron can be both positive and negative ? 5.In fig4 , DANN performs significantly worse than LayerNorm and BathNorm . 6.The algorithms is not tested on slightly more challenging benchmark datasets such as CIFAR10 or ImageNet . Relatedly , would DANN scale up to larger networks ? Questions to be clarified : * Are their connections between the I neurons within the same layers ? * page 4 , \u201c Unlike a column constrained network , a layer in a DANN is not restricted in its potential function space . \u201c - It is unclear what this sentence means\u2026 * Between Eq 4 and Eq 5 , the authors mentioned the exponential family . What particular distribution was used ? Gaussian or any exponential family distribution would produce similar results ? * The authors wrote : \u201c As a result , inhibitory unit parameters updates are scaled down relative to excitatory parameter updates . This is intriguing given the differences between inhibitory and excitatory neuron plasticity\u2026including the relative extent of weight changes in excitatory and inhibitory neurons ( McBain et al. , 1999 ) . \u201d I think these comparisons to the neuroscience literature are too vague and potentially mis-leading . To make this useful , it would helpful to make the comparison more specific and clear . * I am worried that the experiments for the ColumnEi model was not treated fairly . In section 5.1 , it is mentioned that 50 columns are negative . Did the authors try to make increase this number to see if the performance would be improved for the ColumnEi model ? * * * * * * * * * updated after rebuttal period I still consider this as an interesting contribution , and stand with my original rating . It would be useful if the discrepancies and similarity between the connectivity structures in the model and the anatomy could be more carefully discussed in the paper .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* 5 ) In fig4 , DANN performs significantly worse than LayerNorm and BathNorm . * We would argue that figure 4 shows equivalent performance on K-MNIST . But , the reviewer is correct that the DANN performance is not quite as good as LayerNorm and BatchNorm on Fashion-MNIST . However , we would note that they are actually quite close . For example , if the reviewer looks at Table 3 , they will see that on the test set DANNs achieved an error rate of 10.962 +/- 0.365 , compared to 10.445 +/- 0.455 for LayerNorm and 9.992 +/- 0.218 for BatchNorm , which is a real difference , but arguably not huge . For comparison , the column constrained ANN achieved only 14.986 +/- 0.674 . Moreover , we would note that the DANN performance was within the standard deviation of the MLP performance . * 6 ) The algorithms is not tested on slightly more challenging benchmark datasets such as CIFAR10 or ImageNet . Relatedly , would DANN scale up to larger networks ? * While , to our knowledge , this is the first paper to show ANNs that obey Dale \u2019 s principle learning as well as standard ANNs on simple tasks , this is a very important question . In-line with our response to Reviewer 3 , comment 3 , we note that the corrections derived in the paper apply to convolutional networks , and we have been running experiments on deep convolutional DANNs trained on CIFAR-10 . We have preliminary data ( see attached figure 2 in https : //pdfhost.io/v/AFBEMscCX_DANN_Preliminary_Responsepdf.pdf ) suggesting that DANN convnets have performance approximately equal to that of standard convnets on this dataset . We intend on running a more thorough set of experiments following on this preliminary data ( with full hyperparameter tuning ) , though this may take some time . We propose to add discussion of how to apply the DANN formalism to convnets , and if the reviewers feel that it is important , we can include the results of these experiments in the camera ready version of the manuscript . * 7 ) Are their connections between the I neurons within the same layers ? * No , there are not . We will clarify this in the paper . * 8 ) page 4 , \u201c Unlike a column constrained network , a layer in a DANN is not restricted in its potential function space . \u201c - It is unclear what this sentence means\u2026 * We can see how this sentence is unclear . What we mean by this is that in a column constrained network there are literally many functions that a single layer can not approximate , because the linear operation is constrained to matrices with columns that have only positive or negative signs . In contrast , in DANNs , the initial linear integration in the excitatory units can match any linear function . We propose expanding this sentence to clarify this point . * 9 ) Between Eq 4 and Eq 5 , the authors mentioned the exponential family . What particular distribution was used ? Gaussian or any exponential family distribution would produce similar results ? * Our mathematical analysis only assumes any distribution from the natural exponential family ( the exponential family with T ( y ) = y , see footnote 1 ) . So , it applies equally to any such distribution . This group of distributions includes the Gaussian , Poisson , gamma , and binomial distributions . We propose to clarify this point in the footnote . * 10 ) The authors wrote : \u201c As a result , inhibitory unit parameters updates are scaled down relative to excitatory parameter updates . This is intriguing given the differences between inhibitory and excitatory neuron plasticity\u2026including the relative extent of weight changes in excitatory and inhibitory neurons ( McBain et al. , 1999 ) . \u201d I think these comparisons to the neuroscience literature are too vague and potentially mis-leading . To make this useful , it would helpful to make the comparison more specific and clear . * This is a fair point . What we were referring to , ultimately , was the fact that inhibitory plasticity is fairly difficult to achieve experimentally . Indeed , in the past , many neuroscientists thought that it did not exist ( such as McBain et al , 1999 ) . Thus , all we intended to refer to here was the apparent reduced plasticity at inhibitory synapses in the brain . We will clarify this in this section . * 11 ) I am worried that the experiments for the ColumnEi model was not treated fairly . In section 5.1 , it is mentioned that 50 columns are negative . Did the authors try to make increase this number to see if the performance would be improved for the ColumnEi model ? * This is an important point to clarify . To test this question , we ran additional experiments with ColumnEi models that contain 100 negative columns . We find that these models learn just as poorly as the other ColumnEi models . Please see the attached figure 1 , table 1 in https : //pdfhost.io/v/AFBEMscCX_DANN_Preliminary_Responsepdf.pdf . We will include these results in the revised paper ."}, "1": {"review_id": "eU776ZYxEpz-1", "review_text": "Summary : It is shown that Dale \u2019 s principle can be observed in feedfoward ANNs if one uses inhibitory neurons in the form of feedforward inhibition , while the other neurons are purely excitatory . Pros : This is a nice and new insight . It appears to be useful for understanding the design of biological neural networks , and at least one type of uses of inhibitory neurons in them . Cons : Apparently this insight provides no benefit for designing ANN . Furthermore the biological insight is rather limited because biological neural networks are not feedforward networks . Also , the chosen tasks ( 3 variations of MNIST ) are relatively simple , and are solved with relatively shallow networks , with just 4 hidden layers . In my view this evaluation does not support the much more general claim in the Abstract that \u201e ANN \u2019 s that respect Dale \u2019 s principle can be built without sacrificing learning performance \u201c .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We are very happy that the reviewer found our paper insightful , and we thank them for their constructive critiques . Our responses are as follows : * 1 ) Apparently this insight provides no benefit for designing ANN . * Indeed , as the reviewer notes here , we did not observe better performance with DANNs than with standard ANNs . Of course , the goal of this paper was to close the gap in learning performance between standard ANNs and ANNs that obey Dale \u2019 s principle . One reason that this is important is simply that ANNs that obey Dale \u2019 s principle , but which are not impaired at learning relative to normal ANNs , will be a useful tool for neuroscience research . However , we also wonder about potential computational benefits to Dale \u2019 s principle , and hope in future work to explore this possibility . We see this paper , which closes the learning gap , as a key initial step towards these future investigations . Please see also our reply to Reviewer 2 , comment 2 for more discussion on this matter . * 2 ) Furthermore the biological insight is rather limited because biological neural networks are not feedforward networks . * The reviewer is correct that real neural circuits are typically recurrent , and thus , it would be beneficial to also consider how DANNs can operate within the recurrent context . However , thanks to the mathematical similarity between a multilayer feedforward neural network and a recurrent neural rolled out through time ( see e.g.Liao and Poggio , 2016 , https : //arxiv.org/abs/1604.03640 ) , making this connection is relatively straightforward . In fact , our formulation of DANNs is fully applicable to recurrent neural networks , thanks to these connections . We propose to add a section to the appendix describing how our formulations can be ported to the case of recurrent neural networks . If the reviewers agree that this is a good idea , we will include this in our revised manuscript . Please see also our response to Reviewer 2 , comment 1 . * 3 ) Also , the chosen tasks ( 3 variations of MNIST ) are relatively simple , and are solved with relatively shallow networks , with just 4 hidden layers . In my view this evaluation does not support the much more general claim in the Abstract that \u201e ANN \u2019 s that respect Dale \u2019 s principle can be built without sacrificing learning performance \u201c . * We agree that the tasks we explored here were relatively simple . However , as noted by Reviewer 4 , despite these tasks being simple this is , to our knowledge , the first paper to show ANNs that obey Dale \u2019 s principle that can learn on these tasks as well as standard ANNs . Nonetheless , to expand on our results , we have been exploring the use of DANN style architectures in deep convolutional networks . First we note that the response of a convolutional network can be expressed as a normal matrix multiplication where the rows of the weight matrix correspond to convolutional filters , and the columns of the input matrix correspond to the different filter locations . As such , we can readily express the same DANN formulation for convolutional networks . Second , we have preliminary results showing that learning in DANN convnets is approximately as good as learning in regular convnets ( see the attached figure 2 in https : //pdfhost.io/v/AFBEMscCX_DANN_Preliminary_Responsepdf.pdf ) . We propose to add discussion of how to apply the DANN formalism to convnets , and if the reviewers feels it is important , we can include a more thorough version of this data ( after appropriate hyperparameter optimization ) in the final version of the paper . Though , we note that full hyperparameter optimization and experimentation will take some time , so the final results of these experiments may not be ready by next week ."}, "2": {"review_id": "eU776ZYxEpz-2", "review_text": "This is a great investigation on how to scale the gain of the inhibitory weights to balance the impact that the changes that the excitatory and inhibitory connections have on the layer \u2019 s output . I think using the KL distance that naturally connects with the Fisher Information is neat . I appreciate the effort that the authors make to connect the manner neural circuits are designed and connect it with ANN . You never know when the breakthrough can arise . I love the experiments that the authors present illustrating with clarity the impact that having the proper gain modulation of the inhibitory changes have in the speed of convergence . My single constructive criticism is that the inspiration in cortical circuits do not prevent the authors to get inspiration from smaller neural circuits like in insects for example . The Mushroom Bodies of the insects are the equivalent of the cortex and present feedforward inhibition . The number of layers is much smaller but the neural principles that operate are fairly consistent across multiple animal species . Drawing from that experience , the mutual inhibition within layer may provide a natural mechanism to keep balance in the output distribution as shown for example in mean field models that investigate the regulation of activity in a dynamical neural layer ( see for example https : //journals.plos.org/ploscompbiol/article ? id=10.1371/journal.pcbi.1003133 ) . Other that this comment I learn and enjoy from reading this paper . I think it should be accepted .", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "We thank the reviewer for their kind and constructive comments . We fully agree that non-cortical circuits can provide equal inspiration for these investigations and we will specifically add statements and references to that effect , e.g.noting the mushroom body of insects . Moreover , we will discuss the interesting observation that there may be very general principles at play with respect to maintaining balanced output via mutual inhibition , reference the paper the reviewer noted , and propose this as a future extension of our work ."}, "3": {"review_id": "eU776ZYxEpz-3", "review_text": "Inspired by the observations of feedforward inhibition in the brain , the authors propose a novel ANN architecture that respects Dale \u2019 s rule ( DANN ) . They provide two improvements for training DANNs : better initialization and update scaling for synaptic weights . As a result , they empirically demonstrate that DANNs perform no worse than the ANNs that do not respect Dale \u2019 s rule . Although , I find the contribution interesting , my enthusiasm is tempered by the following two issues : 1 . Although feedforward inhibition has its place in the brain , most connections of inhibitory interneurons with excitatory neurons are reciprocal , resulting in feedback inhibition . Therefore , feedforward inhibition seems like a secondary factor here . 2.The DANNs are shown to be just no worse than ANNs that do not respect Dale \u2019 s rule . If biology \u201c invested the effort \u201d to evolve inhibitory interneurons respecting Dale \u2019 s rule , this is probably because they confer a computational advantage , not just lack of disadavantage . The formulation of Dale \u2019 s rule on page 1 is not consistent with the current biological knowledge . A better version would be : \u201c A neuron releases the same fast neurotransmitter at each of its pre-synaptic terminals \u201d . Note that this does not mean that the action of a neuron is always excitatory or always inhibitory on all of its post-synaptic partners . It is possible , as often the case in invertebrates , that different post-synaptic partners have different receptors resulting in de- or hyper-polarization in different post-synaptic neurons . Although the paper is generally well written , the authors could make it clearer . In particular , it would help if they defined symbols such as the circled dot or variables such as y when they are first used .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their comments , and are happy that they found our paper interesting . The reviewer \u2019 s comments raise important questions . Our responses to these points are as follows : * 1 ) Although feedforward inhibition has its place in the brain , most connections of inhibitory interneurons with excitatory neurons are reciprocal , resulting in feedback inhibition . Therefore , feedforward inhibition seems like a secondary factor here . * The reviewer is correct that reciprocal/feedback inhibition is an important component of inhibition in the brain . We are not sure that it is fair to say that feedforward inhibition is a secondary factor , as there is ample evidence showing that feedforward inhibition is a critical , and plastic , regulator of responses in numerous circuits across the brain ( see e.g.Pouille et al.2009 , Nature Neuroscience , 12:1577 , 2009 or Hennequin et al.2017 , Annual Review of Neuroscience , 40:557-579 for a review related to plasticity ) . Indeed , the evidence suggests that feedforward inhibition can be the critical factor for determining early responses in neural circuits ( Pouille & Scanziani , 2001 , Science , 293 : 1159\u20131163 ) . Of course , learning of feedback inhibition is also important , particularly for maintaining dynamic balance and for shaping responses over time ( as also explained in Hennequin et al.2017 ) .Thus , the reviewer is correct that including feedback inhibition would be ideal . Importantly , though , we note though that our formulation for DANNs can still be applied to feedback inhibition . Recurrent neural networks obey many of the same mathematical principles as multi-layer feedforward neural networks ( see e.g.Liao and Poggio , 2016 , https : //arxiv.org/abs/1604.03640 ) . If we imagine unrolling a recurrent neural network with separate excitatory and inhibitory populations , then the feedback inhibition could be treated exactly like feedforward inhibition , but with \u201c layers \u201d corresponding to timesteps . Thus , all of our mathematical formulations and analyses would still hold for the unrolled recurrent network . Given this important point , if the reviewer agrees , we will add a section in a revised version of the Appendix explaining how our formulation of DANNs can be used to model feedback inhibition . * 2 ) The DANNs are shown to be just no worse than ANNs that do not respect Dale \u2019 s rule . If biology \u201c invested the effort \u201d to evolve inhibitory interneurons respecting Dale \u2019 s rule , this is probably because they confer a computational advantage , not just lack of disadavantage . * This is a very interesting issue that the reviewer raised , and it generated a lot of discussion amongst the authors . After discussing the matter , what we would say is that it is unclear whether Dale \u2019 s principle represents an \u201c investment of effort \u201d by biology or not . Though it is easy to think about possible ways to avoid Dale \u2019 s principle using known physiological mechanisms , it may also represent an evolutionary local minima , whereby early phylogenetic choices led to constraints on the system that were difficult to evolve away . This is the opinion of some of the authors . However , the reviewer may also be right that Dale \u2019 s principle does confer a computational advantage to real brains , which is why evolution kept it around . This is , in fact , the opinion of the majority of the authors . We think that future work should investigate potential advantages to Dale \u2019 s principle more thoroughly . However , this was not the goal of this study , which was instead to solve the problem of ANNs with separate excitatory and inhibitory units performing worse when trained with gradient descent . Indeed , it is hard to see how we can understand the potential computational advantages of ANNs that obey Dale \u2019 s principle if they are actually poor at learning relative to normal ANNs . Thus , we see our work as a necessary first step to future studies that could more thoroughly explore potential advantages to Dale \u2019 s principle . If the reviewer thinks it is important to include in the paper , we would add discussion of this matter to a revised version . * 3 ) The formulation of Dale \u2019 s rule on page 1 is not consistent with the current biological knowledge . A better version would be : \u201c A neuron releases the same fast neurotransmitter at each of its pre-synaptic terminals \u201d . Note that this does not mean that the action of a neuron is always excitatory or always inhibitory on all of its post-synaptic partners . It is possible , as often the case in invertebrates , that different post-synaptic partners have different receptors resulting in de- or hyper-polarization in different post-synaptic neurons . * We agree with the reviewer , thank you for noting this point . We will adjust the language in the introduction to recognize the fact that there are neural circuits where the same neurotransmitters can affect different postsynaptic neurons differently ."}}