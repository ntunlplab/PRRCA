{"year": "2017", "forum": "B1kJ6H9ex", "title": "Combining policy gradient and Q-learning", "decision": "Accept (Poster)", "meta_review": "Reviewers agree that this a high-quality and interesting paper exploring important connections between widely used RL algorithms. It has the potential to be an impactful paper, with the most positive comment noting that it \"will likely influence a broad swath of RL\". \n \n Pros:\n - The main concepts of the paper came through clearly, and all reviewers felt the idea was interesting and novel.\n - The empirical part of the paper was convincing and \"empirical section is very well explained\" \n \n Cons:\n - There and some concerns about the writing and notation. None of these were too critical though, and the authors responded \n - Reviewers asked for some more comparisons with alternative formulations.", "reviews": [{"review_id": "B1kJ6H9ex-0", "review_text": "This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done. One minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper. Overall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thankyou for your comments . You are totally correct that the use of the stationary ( i.e.non-discounted ) policy was glossed over originally . We have updated the paper to make the distinction more explicit ."}, {"review_id": "B1kJ6H9ex-1", "review_text": "Nice paper, exploring the connection between value-based methods and policy gradients, formalizing the relation between the softmax-like policy induced by the Q-values and a regularized form of PG. Presentation: Although that seems to be the flow in the first part of the paper, I think it could be cast as a extension/ generalization of the dueling Q-network \u2013 for me that would be a more intuitive exposition of the new algorithm and findings. Small concern in general case derivation: Section 3.2: Eq. (7) the expectation (s,a) is wrt to \\pi, which is a function of \\theta -- that dependency seems to be ignored, although it is key to the PG update derivation. If these policies(the sampling policy for the expectation and \\pi) are close enough it's usually okay -- but except for particular cases (trust-region methods & co), that's generally not true. Thus, you might end up solving a very different problem than the one you actually care solving. Results: A comparison with the dueling architecture could be added as that would be the closest method (it would be nice to see if and in which game you get an improvement) Overall: strong paper, good theoretical insights. ", "rating": "7: Good paper, accept", "reply_text": "Thankyou for your comments and suggestions . - We have made more explicit the differences and similarities between our method and the dueling architecture in the prior work section . You are correct that policy gradient methods can be interpreted as a generalization of the dueling architecture combined with a SARSA style update . - In that equation we 're saying to consider the euclidean projection problem of the log-\\pi s onto a set of values under some arbitrary measure , the KKT conditions of that problem is the next equation ( along with the \\pi s being probability measures ) . If we compare that equation to eq.3 we see that they match up if the measure is the same and q = Q , so we can interpret eq.3 as a projection . This is not the same as solving the optimization problem in eq.7 with the measure \\pi and Q^\\pi , since they are both functions of \\pi and thus we would need to consider how changing \\pi affects them which would change the KKT conditions . However we are not claiming to solve that problem , which is more complicated than a simple projection . - The Q-learning example we compare to in the results is using the same network and parameterizing the architecture as in eq.10 , and so provides a point of comparison between PGQ and the closest pure Q-learning method , that naturally includes dueling ."}, {"review_id": "B1kJ6H9ex-2", "review_text": "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied. I think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful. That being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular: - The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation) - It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations. - The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods - Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere. I hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states). Minor remarks: - The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined) - The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\" - I believe 4.3 is for the tabular case but this is not clearly stated - Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case. Typos: - \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"? - \"Online policy gradient typically require an estimate of the action-values function\" => requires & value - \"the agent generates experience from interacting the environment\" => with the environment - in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi - \"allowing us the spread the influence of a reward\" => to spread - \"in the off-policy case tabular case\" => remove the first case - \"The green line is Q-learning where at the step an update is performed\" => at each step - In Fig. 2 it says A2C instead of A3C NB: I did not have time to carefully read Appendix A", "rating": "7: Good paper, accept", "reply_text": "Thankyou for your detailed and insightful comments . - \\tilde Q is always to be interpreted as an estimate of the Q-values , eq.5 holds exactly because it is how we define \\tilde Q , as a function of the policy . We are basically saying we are going to use the policy as an estimate of the Q-values . - We feel section 3.2 is important to bridge the gap between the tabular case , where we can show that \\tilde Q is equal to Q exactly at the fixed point , to the general case where we ca n't show that but at least we can say that the error between the two is minimized in an l2 sense , so that \\tilde Q is a valid estimate of the Q-values still . - We have made the connections and differences between dueling and our method more explicit in the prior work section . Yes , the Q-learning variant we use in the examples is very similar to dueling , we made that more explicit . We added the interpretation of PGQ as a combination of expected SARSA and Q-learning to sect 4.1 . - We have made an effort to make sect 3.3 clearer . Minor : - Updated the expectation to depend on r. - Clarified the definition of the Boltzmann policy . - Stated in sect 4.3 that it is only for the tabular case . - The grid world example has been updated ( there was a bug where the learning rates were too small originally ) and they now converge to the same policy , in the original draft they all converged to same policy performance given enough time ( roughly double the iterations was sufficient ) . Typos : All fixed ! Thankyou !"}], "0": {"review_id": "B1kJ6H9ex-0", "review_text": "This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done. One minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper. Overall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thankyou for your comments . You are totally correct that the use of the stationary ( i.e.non-discounted ) policy was glossed over originally . We have updated the paper to make the distinction more explicit ."}, "1": {"review_id": "B1kJ6H9ex-1", "review_text": "Nice paper, exploring the connection between value-based methods and policy gradients, formalizing the relation between the softmax-like policy induced by the Q-values and a regularized form of PG. Presentation: Although that seems to be the flow in the first part of the paper, I think it could be cast as a extension/ generalization of the dueling Q-network \u2013 for me that would be a more intuitive exposition of the new algorithm and findings. Small concern in general case derivation: Section 3.2: Eq. (7) the expectation (s,a) is wrt to \\pi, which is a function of \\theta -- that dependency seems to be ignored, although it is key to the PG update derivation. If these policies(the sampling policy for the expectation and \\pi) are close enough it's usually okay -- but except for particular cases (trust-region methods & co), that's generally not true. Thus, you might end up solving a very different problem than the one you actually care solving. Results: A comparison with the dueling architecture could be added as that would be the closest method (it would be nice to see if and in which game you get an improvement) Overall: strong paper, good theoretical insights. ", "rating": "7: Good paper, accept", "reply_text": "Thankyou for your comments and suggestions . - We have made more explicit the differences and similarities between our method and the dueling architecture in the prior work section . You are correct that policy gradient methods can be interpreted as a generalization of the dueling architecture combined with a SARSA style update . - In that equation we 're saying to consider the euclidean projection problem of the log-\\pi s onto a set of values under some arbitrary measure , the KKT conditions of that problem is the next equation ( along with the \\pi s being probability measures ) . If we compare that equation to eq.3 we see that they match up if the measure is the same and q = Q , so we can interpret eq.3 as a projection . This is not the same as solving the optimization problem in eq.7 with the measure \\pi and Q^\\pi , since they are both functions of \\pi and thus we would need to consider how changing \\pi affects them which would change the KKT conditions . However we are not claiming to solve that problem , which is more complicated than a simple projection . - The Q-learning example we compare to in the results is using the same network and parameterizing the architecture as in eq.10 , and so provides a point of comparison between PGQ and the closest pure Q-learning method , that naturally includes dueling ."}, "2": {"review_id": "B1kJ6H9ex-2", "review_text": "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied. I think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful. That being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular: - The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation) - It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations. - The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods - Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere. I hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states). Minor remarks: - The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined) - The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\" - I believe 4.3 is for the tabular case but this is not clearly stated - Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case. Typos: - \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"? - \"Online policy gradient typically require an estimate of the action-values function\" => requires & value - \"the agent generates experience from interacting the environment\" => with the environment - in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi - \"allowing us the spread the influence of a reward\" => to spread - \"in the off-policy case tabular case\" => remove the first case - \"The green line is Q-learning where at the step an update is performed\" => at each step - In Fig. 2 it says A2C instead of A3C NB: I did not have time to carefully read Appendix A", "rating": "7: Good paper, accept", "reply_text": "Thankyou for your detailed and insightful comments . - \\tilde Q is always to be interpreted as an estimate of the Q-values , eq.5 holds exactly because it is how we define \\tilde Q , as a function of the policy . We are basically saying we are going to use the policy as an estimate of the Q-values . - We feel section 3.2 is important to bridge the gap between the tabular case , where we can show that \\tilde Q is equal to Q exactly at the fixed point , to the general case where we ca n't show that but at least we can say that the error between the two is minimized in an l2 sense , so that \\tilde Q is a valid estimate of the Q-values still . - We have made the connections and differences between dueling and our method more explicit in the prior work section . Yes , the Q-learning variant we use in the examples is very similar to dueling , we made that more explicit . We added the interpretation of PGQ as a combination of expected SARSA and Q-learning to sect 4.1 . - We have made an effort to make sect 3.3 clearer . Minor : - Updated the expectation to depend on r. - Clarified the definition of the Boltzmann policy . - Stated in sect 4.3 that it is only for the tabular case . - The grid world example has been updated ( there was a bug where the learning rates were too small originally ) and they now converge to the same policy , in the original draft they all converged to same policy performance given enough time ( roughly double the iterations was sufficient ) . Typos : All fixed ! Thankyou !"}}