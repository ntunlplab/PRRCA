{"year": "2017", "forum": "rJM69B5xx", "title": "Finding a Jack-of-All-Trades: An Examination of Semi-supervised Learning in Reading Comprehension", "decision": "Reject", "meta_review": "The area chair agrees with reviewers 1 and 3 that the paper does not meet the bar for ICLR. Reviewer 3 in particular points out how the paper can be strengthened for future revisions.", "reviews": [{"review_id": "rJM69B5xx-0", "review_text": "First I would like to apologize for the delay in reviewing. summary : This work explores several experiments to transfer training a specific model of reading comprehension ( AS Reader), in an artificial and well populated dataset in order to perform in another target dataset. Here is what I understand are their several experiments to transfer learning, but I am not 100% sure. 1. The model is trained on the big artificial dataset and tested on the small target datasets (section 4.1) 2. The model is pre-trained on the big artificial dataset like before, then fine-tuned on a few examples from the target dataset and tested on the remaining target examples. Several such models are trained using different sub-sets of fine-tuning examples. The results are tested against the performance of randomly intialized then fine-tuned models (section 4.2). 3. The model is pre-trained on the big artificial dataset like before. The model is made of an embedding component and an encoder component. Alternatively, each component is reset to a random initialization, to test the importance of the pre-training in each component. Then the model is fine-tuned on a few examples from the target dataset and tested on the remaining target examples. (section 4.3) I think what makes things difficult to follow is the fact that the test set is composed by several sub tasks, and sometimes what is reported is the mean performance across the tasks, sometimes the performance on a few tasks. Sometimes what we see is the mean performance of several models? You should report standard deviations also. Could you better explain what you mean by best validation ? Interesting and unpretentious work. The clarity of the presentation could be improved maybe by simplifying the experimental setup? The interesting conclusion I think is reported at the end of the section 4.1, when the nuanced difference between the datasets are exposed. Minor: unexplained acronyms: GRU, BT, CBT. benfits p. 2 subsubset p. 6", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . Since generalization is one of the central topics of the article , we wanted to perform the experiments on a variety of tasks . Since behaviour was different on each task , we decided to provide plots of a few selected tasks in the main body of the article , leaving the rest for the appendix . To capture the remainder of the experiments in the main body in a compact form , we decided to provide a plot of the means across all tasks . We are sorry to hear that this had an impact on clarity and will bear this problem in mind when designing and describing future experiments . As you requested , we have now added a table describing the standard deviations for each of the experiments , and perhaps more interestingly , added a table with significance levels of whether the pre-trained model was better than the randomly initialized one . By best validation model we mean the model with the best accuracy on the validation dataset for each task . In the case of bAbI this was a set of 100 examples taken aside from the original training set . In the case of SQuAD this was the SQuAD development dataset filtered in the same way as the training dataset . We have also corrected the typos and the use of acronyms , thank you for pointing them out ."}, {"review_id": "rJM69B5xx-1", "review_text": "This paper proposes a study of transfer learning in the context of QA from stories. A system is presented with a a short story and has to answer a question about it. This paper studies how a system trained to answer questions on a dataset can eventually be used to answer questions from another dataset. The results are mostly negative: transfer seems almost non-existant. This paper is centered around presenting negative results. Indeed the main hypothesis of transferring between QA datasets with the attention sum reader turns out impossible and one needs a small portion of labeled data from the target dataset to get meaningful performance. Having only negative results could be fine if the paper was bringing some value with a sharp analysis of the failure modes and of the reasons behind it. Because this might indicate some research directions to follow. However, there is not much of that. The answers to the pre-review questions actually start to give some insights: typing seems to be transferred for instance. How about the impact of syntax (very different between bAbI, Gutenberg books, and CNN news articles)? And the word/entity/ngrams distributions overlap between the 3 datasets? Unfortunately, there is not much to take-away from this paper. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . The paper presents three kinds of experiments . The results of the first experiment ( transfer without training on the target dataset ) are indeed negative - almost no transfer . However this is not the case for the other two experiments . The results of the second experiment show that pre-training can significantly improve the model 's performance if it is shown at least a few examples from the target domain . On bAbI we see this only on some of the examined tasks ; however on SQuAD we see a jump from 31 % of accuracy with a randomly initialized model to 48 % with a pre-trained one ( Table 2 ) . We do see this as a positive result . Also the third experiment does bring another positive observation - this improvement is not just thanks to pre-trained word embeddings ( whose usefulness is well established ) , but also thanks to the pre-trained recurrent encoder . And where we see the take-away from the paper ? We see the negative result as a warning that what models are learning on some of the recently popular tasks may not be easily transferable to other settings . Since practical usefulness of machine learning models largely relies on this transferability , we consider this a prompt for our community , which often does n't address this point at all in research papers , that it can not be taken for granted and we may need to start paying more attention to this aspect . The second experiment then shows that while transfer can not be taken for granted it is possible under certain conditions , and the third experiment further examines the nature of this transfer . A more specific take-away is that it is possible to transfer knowledge via pre-training a recurrent encoder in a way similar to re-using word-embeddings . This principle may be relevant to wide variety of models since some form of recurrent encoder is being used by almost all reading-comprehension models ."}, {"review_id": "rJM69B5xx-2", "review_text": "This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However, when the model is exposed to few training instances some improvements are observed. The claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [1,2]. More importantly, the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3], these claims bring a marginal novelty and still \"how and why\" should be central in this work. [1] http://www.msmarco.org/dataset.aspx [2] https://datasets.maluuba.com/NewsQA [3] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.8551&rep=rep1&type=pdf", "rating": "3: Clear rejection", "reply_text": "Thank you for the review , however we find that some of its points are not entirely appropriate . Firstly the review criticizes us for using bAbI as a low-resource real-world scenario while they were designed as unit tests . However we do explicitly refer to the tasks as an artificial toy dataset ( top of page 3 ) which we put into contrast with the `` real-world '' SQuAD . We do use them as unit tests for specific reasoning skills - this purpose is made clear already in point 1. on page 2 and , as the review says , this is what they were designed for . Next the review recommends using two real-world scenarios , none of which was however available at the time of submission . Instead we use SQuAD which we evaluated as the best `` real-world '' task at the time of performing the experiments . Even now SQuAD seems like a reasonable alternative to the suggested datasets . Why would you highly recommend using the two new datasets as opposed to SQuAD ? ( We have actually tried running the experiments on MS Marco and observed behaviour very similar to the one on SQuAD . ) Finally the `` how and why '' is certainly an interesting question . We did try to partly address this in our last experiment . We did not just `` hypothesize '' that the knowledge is not only in the embeddings but also in the encoder - we performed an experiment and did verify this hypothesis . We certainly admit that this is only a small part of the why and how - however tackling this question more generally opens the whole topic of model interpretability , which is one of the biggest current challenges in Machine Learning and addressing it more deeply is beyond the scope of this work . One difference between our work and [ 3 ] is that we pre-train both word embeddings and document/question encoder whereas [ 3 ] used just shared word embeddings . See discussion in thread `` multi-task learning '' on this page ."}], "0": {"review_id": "rJM69B5xx-0", "review_text": "First I would like to apologize for the delay in reviewing. summary : This work explores several experiments to transfer training a specific model of reading comprehension ( AS Reader), in an artificial and well populated dataset in order to perform in another target dataset. Here is what I understand are their several experiments to transfer learning, but I am not 100% sure. 1. The model is trained on the big artificial dataset and tested on the small target datasets (section 4.1) 2. The model is pre-trained on the big artificial dataset like before, then fine-tuned on a few examples from the target dataset and tested on the remaining target examples. Several such models are trained using different sub-sets of fine-tuning examples. The results are tested against the performance of randomly intialized then fine-tuned models (section 4.2). 3. The model is pre-trained on the big artificial dataset like before. The model is made of an embedding component and an encoder component. Alternatively, each component is reset to a random initialization, to test the importance of the pre-training in each component. Then the model is fine-tuned on a few examples from the target dataset and tested on the remaining target examples. (section 4.3) I think what makes things difficult to follow is the fact that the test set is composed by several sub tasks, and sometimes what is reported is the mean performance across the tasks, sometimes the performance on a few tasks. Sometimes what we see is the mean performance of several models? You should report standard deviations also. Could you better explain what you mean by best validation ? Interesting and unpretentious work. The clarity of the presentation could be improved maybe by simplifying the experimental setup? The interesting conclusion I think is reported at the end of the section 4.1, when the nuanced difference between the datasets are exposed. Minor: unexplained acronyms: GRU, BT, CBT. benfits p. 2 subsubset p. 6", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the review . Since generalization is one of the central topics of the article , we wanted to perform the experiments on a variety of tasks . Since behaviour was different on each task , we decided to provide plots of a few selected tasks in the main body of the article , leaving the rest for the appendix . To capture the remainder of the experiments in the main body in a compact form , we decided to provide a plot of the means across all tasks . We are sorry to hear that this had an impact on clarity and will bear this problem in mind when designing and describing future experiments . As you requested , we have now added a table describing the standard deviations for each of the experiments , and perhaps more interestingly , added a table with significance levels of whether the pre-trained model was better than the randomly initialized one . By best validation model we mean the model with the best accuracy on the validation dataset for each task . In the case of bAbI this was a set of 100 examples taken aside from the original training set . In the case of SQuAD this was the SQuAD development dataset filtered in the same way as the training dataset . We have also corrected the typos and the use of acronyms , thank you for pointing them out ."}, "1": {"review_id": "rJM69B5xx-1", "review_text": "This paper proposes a study of transfer learning in the context of QA from stories. A system is presented with a a short story and has to answer a question about it. This paper studies how a system trained to answer questions on a dataset can eventually be used to answer questions from another dataset. The results are mostly negative: transfer seems almost non-existant. This paper is centered around presenting negative results. Indeed the main hypothesis of transferring between QA datasets with the attention sum reader turns out impossible and one needs a small portion of labeled data from the target dataset to get meaningful performance. Having only negative results could be fine if the paper was bringing some value with a sharp analysis of the failure modes and of the reasons behind it. Because this might indicate some research directions to follow. However, there is not much of that. The answers to the pre-review questions actually start to give some insights: typing seems to be transferred for instance. How about the impact of syntax (very different between bAbI, Gutenberg books, and CNN news articles)? And the word/entity/ngrams distributions overlap between the 3 datasets? Unfortunately, there is not much to take-away from this paper. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . The paper presents three kinds of experiments . The results of the first experiment ( transfer without training on the target dataset ) are indeed negative - almost no transfer . However this is not the case for the other two experiments . The results of the second experiment show that pre-training can significantly improve the model 's performance if it is shown at least a few examples from the target domain . On bAbI we see this only on some of the examined tasks ; however on SQuAD we see a jump from 31 % of accuracy with a randomly initialized model to 48 % with a pre-trained one ( Table 2 ) . We do see this as a positive result . Also the third experiment does bring another positive observation - this improvement is not just thanks to pre-trained word embeddings ( whose usefulness is well established ) , but also thanks to the pre-trained recurrent encoder . And where we see the take-away from the paper ? We see the negative result as a warning that what models are learning on some of the recently popular tasks may not be easily transferable to other settings . Since practical usefulness of machine learning models largely relies on this transferability , we consider this a prompt for our community , which often does n't address this point at all in research papers , that it can not be taken for granted and we may need to start paying more attention to this aspect . The second experiment then shows that while transfer can not be taken for granted it is possible under certain conditions , and the third experiment further examines the nature of this transfer . A more specific take-away is that it is possible to transfer knowledge via pre-training a recurrent encoder in a way similar to re-using word-embeddings . This principle may be relevant to wide variety of models since some form of recurrent encoder is being used by almost all reading-comprehension models ."}, "2": {"review_id": "rJM69B5xx-2", "review_text": "This work investigates the performance of transfer learning from resource-rich setup (BookTest, CNN/Daily Mail corpora) to low-resource (bAbI, SQuAD benchmarks) settings. Experiments show poor improvements in 0-shot learning. However, when the model is exposed to few training instances some improvements are observed. The claims made here require a more comprehensive analysis. I criticize the use of bAbI as a low-resource real-world scenario. bAbI is designed as a unit test and is far from representing many natural language phenomena. Thus, the claims related to bAbI can only be weak evidence for questioning transfer learning high-resource to low-resource in real-world scenarios. I highly recommend using recently proposed real-world scenarios [1,2]. More importantly, the work does not explain why and how do we get improvement using transfer learning. They remotely address this by hypothesizing the knowledge of transfer is not just encoded in embeddings but also in the model. Considering the related work [3], these claims bring a marginal novelty and still \"how and why\" should be central in this work. [1] http://www.msmarco.org/dataset.aspx [2] https://datasets.maluuba.com/NewsQA [3] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.8551&rep=rep1&type=pdf", "rating": "3: Clear rejection", "reply_text": "Thank you for the review , however we find that some of its points are not entirely appropriate . Firstly the review criticizes us for using bAbI as a low-resource real-world scenario while they were designed as unit tests . However we do explicitly refer to the tasks as an artificial toy dataset ( top of page 3 ) which we put into contrast with the `` real-world '' SQuAD . We do use them as unit tests for specific reasoning skills - this purpose is made clear already in point 1. on page 2 and , as the review says , this is what they were designed for . Next the review recommends using two real-world scenarios , none of which was however available at the time of submission . Instead we use SQuAD which we evaluated as the best `` real-world '' task at the time of performing the experiments . Even now SQuAD seems like a reasonable alternative to the suggested datasets . Why would you highly recommend using the two new datasets as opposed to SQuAD ? ( We have actually tried running the experiments on MS Marco and observed behaviour very similar to the one on SQuAD . ) Finally the `` how and why '' is certainly an interesting question . We did try to partly address this in our last experiment . We did not just `` hypothesize '' that the knowledge is not only in the embeddings but also in the encoder - we performed an experiment and did verify this hypothesis . We certainly admit that this is only a small part of the why and how - however tackling this question more generally opens the whole topic of model interpretability , which is one of the biggest current challenges in Machine Learning and addressing it more deeply is beyond the scope of this work . One difference between our work and [ 3 ] is that we pre-train both word embeddings and document/question encoder whereas [ 3 ] used just shared word embeddings . See discussion in thread `` multi-task learning '' on this page ."}}