{"year": "2020", "forum": "SyeZIkrKwS", "title": "DyNet: Dynamic Convolution for Accelerating Convolution Neural Networks", "decision": "Reject", "meta_review": "The paper proposed the use of dynamic convolutional kernels as a way to reduce inference computation cost, which is a linear combination of static kernels and fused after training for inference to reduce computation cost. The authors evaluated the proposed methods on a variety models and shown good FLOPS reduction while maintaining accuracy. \n\nThe main concern for this paper is the limited novelty. There have been many works use dynamic convolutions as pointed out by all the reviewers. The most similar ones are SENet and soft conditional computation. Although the authors claim that soft conditional computation \"focus on using more parameters to make models to be more expressive while we focus on reducing redundant calculations\", the methods are pretty the same and moreover in the abstract of soft conditional computation they have \"CondConv improves the performance and inference cost trade-off\".", "reviews": [{"review_id": "SyeZIkrKwS-0", "review_text": "=== Summary === The authors propose to use dynamic convolutional kernels as a means to reduce the computation cost in static CNNs while maintaining their performance. The dynamic kernels are obtained by a linear combination of static kernels where the weights of the linear combination are input-dependent (they are obtained similarly to the coefficients in squeeze-and-excite). The authors also include a theoretical and experimental study of the correlation. The authors conduct extensive experiments on image classification and segmentation and show that dynamic convolutional kernels with reduced number of channels lead to significant reduction in FLOPS and increase in inference speed (for batch size 1) compared to their static counterparts with higher number of channels. === Recommendation === The experimental setup is rigorous but the current draft lacks some metrics that should be reported (as training times, parameter counts, memory requirements at training/inference) since the focus is on making CNNs more efficient. The presented experimental results are satisfactory but the studied networks are not quite SOTA: they are much more competitive alternatives to ResNet and MobileNetv2. The correlation study is interesting. My main issue with the paper is the lack of novelty. The use of dynamic convolutions is by no means a novel idea and has been studied in multiple previous works in vision (mixture of experts, soft conditional computation, pay less attention with dynamic convolutions, ...) which the authors fail to cite/compare against. However, most previous work focuses on leveraging dynamic kernels to use more parameters so the focus on accelerating CNNs is novel. Overall, I am on the fence with this paper but slightly leaning towards rejecting it for the above reasons. === Questions/Comments === - Figure 5: how are the models constrained to have same FLOPS? Is it by changing the number of channels? - Consider adding training times for more transparency - Consider adding parameter counts in experiment tables - The related work subsection 2.3 is rather poor compared to existing work. - 'While model compression based methods' -> 'On the other hand, model compression based methods' - 'computing efficient' -> 'compute efficient' - 'values distribute' -> 'values distributed' - 'DETAIL ANALYSIS OF OUR MOTIVATION' -> 'Detailed analysis of our motivation'", "rating": "6: Weak Accept", "reply_text": "> > > Response to \u201c Figure 5 : how are the models constrained to have same FLOPS ? \u201d : Thanks for your valuable question which help us realize some unclear statements . By changing the number of channels of MobileNetV2 , we can get MobileNetV2 ( 0.35 ) , MobileNetV2 ( 0.5 ) , MobileNetV2 ( 0.75 ) , MobileNetV2 ( 1.0 ) and MobileNetV2 ( 1.4 ) . To constrain our models to have the same FLOPS with them , we keep the channels of each layer the same as the original one , while replacing the conventional convolution with dynamic convolution . If we ignore the additional FLOPs of coefficient prediction module and dynamic generation module , which is negligible , the FLOPs will stay the same . This result shows that our proposed dynamic convolution can be deployed as a plug-and-play unit to replace conventional convolution . We realize that it is not proper to call these models \u201c Dy-mobile \u201d because they have different structures . In the updated version we will call them as Dy-MobileNetV2 ( 0.35 ) , Dy-MobileNetV2 ( 0.5 ) , Dy-MobileNetV2 ( 0.75 ) , Dy-MobileNetV2 ( 1.0 ) and Dy-MobileNetV2 ( 1.4 ) . > > > Response to \u201c Consider adding training times and parameter counts. \u201d : Thanks for your suggestion ! We will add parameter counts in experiment tables and discuss the training time in Sec 4.3 in the revision . > > > Response to \u201c Lack of novelty. \u201d : For previous works on dynamic convolution , they all directly generate convolution kernels via a linear layer ( including \u201c pay less attention with dynamic convolutions \u201d ) . In computer vision tasks , the parameter counts of convolution is large , which makes the number of parameters of the linear layer unbearable . In our proposed method , the linear layer is merely used to predict the coefficients for linearly combining static kernels . It can solve this problem and thus be used to achieve real speed up for CNN on hardware . The most related work is \u201c soft conditional computation \u201d . They focus on using more parameters to make models to be more expressive while we focus on reducing redundant calculations in convolution . According to the theoretical analysis in appendix A and correlation study in Figure 6 , we find that correlations among convolutional kernels can be reduced via dynamically fusing several kernels . Thus different from \u201c soft conditional computation \u201d which replaces the conventional convolution directly , we recommend to reduce the channel numbers ( we reduce half of the channels for convolution layers in Dy-ResNet18/50 and Dy-shuffle ) and then replace conventional convolution with dynamic one . We think this may bring a larger improvement . For example , compared with directly replace the conventional convolution in ResNet18 , Dy-ResNet50 reduce 37.9 % FLOPs while improves 2.03 % top-1 accuracy on ImageNet . Moreover , since this paper is submitted to NeurIPS2019 this May as well , it may be regarded as a concurrent work We will add a comparison against existing work in Sec 2.3 ."}, {"review_id": "SyeZIkrKwS-1", "review_text": " Main contribution of the paper - The paper proposes a dynamic convolution selection method can be applied to arbitrary classification networks based on the global average pooled (GAP) feature map info. - The method obtained improvements over various networks (SuffleNet v2, MobileNet v2, ResNet 18) on ImageNet. Methods - Given the set of fixed convolutional filters, the method dynamically selects the (weighted sum) kernels by given a kind of channel attention. - The GAP of the features gives the channel attention on each stage, and the method applies the dynamic selection of the kernels. - The number of channels in skip-connection shluld be the same because it should be elementwise multiplied with the channel attention acquired from GAP. - The author slightly revises the baseline networks to set the networks integrated with the proposed method to have smaller Flops. Questions - According to Figure 4, it seems that the proposed add-on requires many parameters because it would include a FC layer for each block. But we cannot find the number of parameters in this paper. - The parameter $g_t$ is defined as 6. The experiment shows the ablation to the case $g_t$ =1, but what if we set the parameters to other numbers? Strong points - The proposed model achieved improvement with fewer Flops on large scale image classification dataset. - The method shows effectiveness when it is attached to various classification networks. Concerns - The main concern of the reviewer is that the model shares the core contribution to the existing method; squeeze-and-excitation network (SEnet, Hu et.al.). The method also proposes the attention-based scaling of channels, where the attention comes from GAP, so the reviewer thinks that it is possible to explain this work as some variation of SEnet. The author should clarify the difference and the strong points of the proposed block compared to SEnet. - Also, the reviewer cannot guarantee that the networks trained by the proposed method can transfer the knowledge to other tasks such as detection. The reviewer thinks that it is a critical part because one of the primal reasons for training the network is to use them as the pre-trained backbone for the other tasks. Regarding this, the baseline methods (MobileNet V2, Shufflenet v2, ResNet) are widely used as a pre-trained backbone for object detection, and the papers mention the CoCo object detection results using the pre-trained backbones from their method. The reviewer thinks that the experiment regarding this should be included. - The other thing is that the parameter increases. As in the question, the reviewer thinks that the number of parameters would be increased. The reviewer agrees that some recent works focus more on Flops, but the number of parameters is also discussed in general, when telling about the 'model size'. Conclusion - The author proposed a dynamic kernel selection method (add-on), which can enhance the classification accuracy of the baseline network. - However, the reviewer cannot convince the novelty of the proposed approach and usefulness of the pre-trained backbone network from the proposed method when applying it to the other tasks (Object detection). Inquiries - Clarifying the difference between SEnet. - Testing the ImageNet trained network of the proposed method into an object detection task (as the pre-trained backbone). - Discussing the number of the parameter as well. ", "rating": "3: Weak Reject", "reply_text": "> > > Response to \u201c The proposed add-on requires many parameters but the number of parameters is not shown in this paper \u201d : Thanks for your suggestion , we will add the number of parameters in experiment tables in the revision . > > > Response to \u201c The experiment shows the ablation to the case g_t=1 , but what if we set the parameters to other numbers ? \u201d : Thanks for your comments ! Actually , we have shown it in Table 5 . The value of g_t does not change the computation cost of convolution but affects the performance of networks . The performance will become better when g_t gets larger . > > > Response to \u201c The author should clarify the difference and the strong points of the proposed block compared to SEnet. \u201d Thanks for your meaningful question ! From the method 's point of view , we fuse feature maps instead of fusing kernels in the training stage , it can be done via the combination of group point-wise convolution and SE mechanism . However we fuse kernels in the inference stage , it can ` t be done via this combination . This is the core difference with SEnet and reduces the computation cost by gt times compared with fusing feature maps or the combination of group point-wise convolution and SE mechanism . From the experiments ' point of view , as discussed in Sec 4.5 , our proposed method will be the same as SENet when g_t=1 . However , compared with g_t=6 , its performance drops a lot while the computation cost of convolution stay the same as shown in Table 6 . > > > Response to \u201c Testing the ImageNet trained network of the proposed method into an object detection task ( as the pre-trained backbone ) . \u201d Thanks for your suggestion . We are trying to implement the baseline and add experiments for detection in the revision before the deadline ."}, {"review_id": "SyeZIkrKwS-2", "review_text": "This paper proposed dynamic convolution (DyNet) to accelerating convolution networks. The new method is tested on the ImageNet dataset with three different backbones. It reduces the computation flops by a large margin while keeps similar classification accuracy. The additional segmentation experiment on the Cityscapes dataset also shows the new module can save computation a lot while maintaining similar segmentation accuracy. Clarity: The novelty of the paper is limited and the experimental results are weird for me. 1. The proposed module named dynamic convolution is detailed in Sec 3.2. As far as I can see, it is very similar to the former SENet especially in Figure (3) and Equation (2). The only difference is the introduction of g_t where the output dimension is much larger than SENet. 2. As shown in Equation (2), the proposed method contains the normal computation of fixed kernels. How can this method save computations compared to classical convolution? Is the computation flops calculated in the right way? 3. The results in Table 5 are strange to me. Larger g_t will increase the flops absolutely according to Equation (2). 4. The author may need to show the comparisons of the number of parameters. In my opinion, the new module will increase the parameters a lot (the output dimension of the fully connected layer is as large as C_cout*g_t). ", "rating": "3: Weak Reject", "reply_text": "> > > Response to \u201c The only difference with SENet is the introduction of g_t where the output dimension is much larger than SENet. \u201d : Thanks for your meaningful question ! The motivation of our proposed approach is different from SENet . We focus on reducing the computation cost during inference stage , while SENet learns channel-wise importance to recalibrates feature maps . From the methods point of view , during training , indeed the only difference is that the output dimension is much larger than SENet . However , during inference , SENet recalibrate feature maps by learned weights , while we fuse kernels instead of fusing feature maps , which can reduce the output dimension and computation cost of convolution kernels . Moreover , as discussed in Sec 4.5 , our proposed method will be the same as SENet when g_t=1 . However , compared with g_t=6 , its performance drops a lot while the computation cost of convolution stay the same as shown in Table 6 . > > > Response to \u201c How can this method save computations compared to classical convolution ? \u201d : Thanks for your valuable comments ! The goal of our proposed approach is to reduce the computation cost in the inference stage . Equation ( 2 ) only shows that fusing feature maps is mathematically equivalent with fusing kernels . During training , we firstly generate C_cout * g_t output feature maps and then get C_out feature maps by fusing each g_t ones . Thus we can not save computation cost in training procedure . However , we fuse kernels during inference stage instead of fusing feature maps . After fusing , the weight for one convolution layer with shape [ C_out * g_t , C_in , k , k ] will become [ C_cout , C_in , k , k ] , thus the FLOPs is reduced by g_t times . > > > Response to \u201c Why larger g_t doesn ` 't increase the flops in Table 5 ? \u201d : Compared with g_t=1 , the computation cost of convolution is 6 times larger when g_t=6 during training . However , we will fuse each g_t kernels to get dynamic convolutional weight with shape [ C_cout , C_in , k , k ] during inference as Equation ( 1 ) , it makes the FLOPs of convolution invariable with g_t . As for the additional FLOPs of fusing kernels , it varies with g_t but is negligible . For Dy-mobile ( 1.0 ) , the FLOPs of fusing kernels is merely 0.459 * g_t M , when g_t=6 it is 2.75M while the FLOPs of convolution is 129 M. > > > Response to \u201c The author may need to show the comparisons of the number of parameters. \u201d : Thanks for the suggestion ! The new module indeed increases the parameters a lot , because we mainly focus on the computation cost rather than the number of parameters . We will add parameter counts in experiment table in our updated version ."}], "0": {"review_id": "SyeZIkrKwS-0", "review_text": "=== Summary === The authors propose to use dynamic convolutional kernels as a means to reduce the computation cost in static CNNs while maintaining their performance. The dynamic kernels are obtained by a linear combination of static kernels where the weights of the linear combination are input-dependent (they are obtained similarly to the coefficients in squeeze-and-excite). The authors also include a theoretical and experimental study of the correlation. The authors conduct extensive experiments on image classification and segmentation and show that dynamic convolutional kernels with reduced number of channels lead to significant reduction in FLOPS and increase in inference speed (for batch size 1) compared to their static counterparts with higher number of channels. === Recommendation === The experimental setup is rigorous but the current draft lacks some metrics that should be reported (as training times, parameter counts, memory requirements at training/inference) since the focus is on making CNNs more efficient. The presented experimental results are satisfactory but the studied networks are not quite SOTA: they are much more competitive alternatives to ResNet and MobileNetv2. The correlation study is interesting. My main issue with the paper is the lack of novelty. The use of dynamic convolutions is by no means a novel idea and has been studied in multiple previous works in vision (mixture of experts, soft conditional computation, pay less attention with dynamic convolutions, ...) which the authors fail to cite/compare against. However, most previous work focuses on leveraging dynamic kernels to use more parameters so the focus on accelerating CNNs is novel. Overall, I am on the fence with this paper but slightly leaning towards rejecting it for the above reasons. === Questions/Comments === - Figure 5: how are the models constrained to have same FLOPS? Is it by changing the number of channels? - Consider adding training times for more transparency - Consider adding parameter counts in experiment tables - The related work subsection 2.3 is rather poor compared to existing work. - 'While model compression based methods' -> 'On the other hand, model compression based methods' - 'computing efficient' -> 'compute efficient' - 'values distribute' -> 'values distributed' - 'DETAIL ANALYSIS OF OUR MOTIVATION' -> 'Detailed analysis of our motivation'", "rating": "6: Weak Accept", "reply_text": "> > > Response to \u201c Figure 5 : how are the models constrained to have same FLOPS ? \u201d : Thanks for your valuable question which help us realize some unclear statements . By changing the number of channels of MobileNetV2 , we can get MobileNetV2 ( 0.35 ) , MobileNetV2 ( 0.5 ) , MobileNetV2 ( 0.75 ) , MobileNetV2 ( 1.0 ) and MobileNetV2 ( 1.4 ) . To constrain our models to have the same FLOPS with them , we keep the channels of each layer the same as the original one , while replacing the conventional convolution with dynamic convolution . If we ignore the additional FLOPs of coefficient prediction module and dynamic generation module , which is negligible , the FLOPs will stay the same . This result shows that our proposed dynamic convolution can be deployed as a plug-and-play unit to replace conventional convolution . We realize that it is not proper to call these models \u201c Dy-mobile \u201d because they have different structures . In the updated version we will call them as Dy-MobileNetV2 ( 0.35 ) , Dy-MobileNetV2 ( 0.5 ) , Dy-MobileNetV2 ( 0.75 ) , Dy-MobileNetV2 ( 1.0 ) and Dy-MobileNetV2 ( 1.4 ) . > > > Response to \u201c Consider adding training times and parameter counts. \u201d : Thanks for your suggestion ! We will add parameter counts in experiment tables and discuss the training time in Sec 4.3 in the revision . > > > Response to \u201c Lack of novelty. \u201d : For previous works on dynamic convolution , they all directly generate convolution kernels via a linear layer ( including \u201c pay less attention with dynamic convolutions \u201d ) . In computer vision tasks , the parameter counts of convolution is large , which makes the number of parameters of the linear layer unbearable . In our proposed method , the linear layer is merely used to predict the coefficients for linearly combining static kernels . It can solve this problem and thus be used to achieve real speed up for CNN on hardware . The most related work is \u201c soft conditional computation \u201d . They focus on using more parameters to make models to be more expressive while we focus on reducing redundant calculations in convolution . According to the theoretical analysis in appendix A and correlation study in Figure 6 , we find that correlations among convolutional kernels can be reduced via dynamically fusing several kernels . Thus different from \u201c soft conditional computation \u201d which replaces the conventional convolution directly , we recommend to reduce the channel numbers ( we reduce half of the channels for convolution layers in Dy-ResNet18/50 and Dy-shuffle ) and then replace conventional convolution with dynamic one . We think this may bring a larger improvement . For example , compared with directly replace the conventional convolution in ResNet18 , Dy-ResNet50 reduce 37.9 % FLOPs while improves 2.03 % top-1 accuracy on ImageNet . Moreover , since this paper is submitted to NeurIPS2019 this May as well , it may be regarded as a concurrent work We will add a comparison against existing work in Sec 2.3 ."}, "1": {"review_id": "SyeZIkrKwS-1", "review_text": " Main contribution of the paper - The paper proposes a dynamic convolution selection method can be applied to arbitrary classification networks based on the global average pooled (GAP) feature map info. - The method obtained improvements over various networks (SuffleNet v2, MobileNet v2, ResNet 18) on ImageNet. Methods - Given the set of fixed convolutional filters, the method dynamically selects the (weighted sum) kernels by given a kind of channel attention. - The GAP of the features gives the channel attention on each stage, and the method applies the dynamic selection of the kernels. - The number of channels in skip-connection shluld be the same because it should be elementwise multiplied with the channel attention acquired from GAP. - The author slightly revises the baseline networks to set the networks integrated with the proposed method to have smaller Flops. Questions - According to Figure 4, it seems that the proposed add-on requires many parameters because it would include a FC layer for each block. But we cannot find the number of parameters in this paper. - The parameter $g_t$ is defined as 6. The experiment shows the ablation to the case $g_t$ =1, but what if we set the parameters to other numbers? Strong points - The proposed model achieved improvement with fewer Flops on large scale image classification dataset. - The method shows effectiveness when it is attached to various classification networks. Concerns - The main concern of the reviewer is that the model shares the core contribution to the existing method; squeeze-and-excitation network (SEnet, Hu et.al.). The method also proposes the attention-based scaling of channels, where the attention comes from GAP, so the reviewer thinks that it is possible to explain this work as some variation of SEnet. The author should clarify the difference and the strong points of the proposed block compared to SEnet. - Also, the reviewer cannot guarantee that the networks trained by the proposed method can transfer the knowledge to other tasks such as detection. The reviewer thinks that it is a critical part because one of the primal reasons for training the network is to use them as the pre-trained backbone for the other tasks. Regarding this, the baseline methods (MobileNet V2, Shufflenet v2, ResNet) are widely used as a pre-trained backbone for object detection, and the papers mention the CoCo object detection results using the pre-trained backbones from their method. The reviewer thinks that the experiment regarding this should be included. - The other thing is that the parameter increases. As in the question, the reviewer thinks that the number of parameters would be increased. The reviewer agrees that some recent works focus more on Flops, but the number of parameters is also discussed in general, when telling about the 'model size'. Conclusion - The author proposed a dynamic kernel selection method (add-on), which can enhance the classification accuracy of the baseline network. - However, the reviewer cannot convince the novelty of the proposed approach and usefulness of the pre-trained backbone network from the proposed method when applying it to the other tasks (Object detection). Inquiries - Clarifying the difference between SEnet. - Testing the ImageNet trained network of the proposed method into an object detection task (as the pre-trained backbone). - Discussing the number of the parameter as well. ", "rating": "3: Weak Reject", "reply_text": "> > > Response to \u201c The proposed add-on requires many parameters but the number of parameters is not shown in this paper \u201d : Thanks for your suggestion , we will add the number of parameters in experiment tables in the revision . > > > Response to \u201c The experiment shows the ablation to the case g_t=1 , but what if we set the parameters to other numbers ? \u201d : Thanks for your comments ! Actually , we have shown it in Table 5 . The value of g_t does not change the computation cost of convolution but affects the performance of networks . The performance will become better when g_t gets larger . > > > Response to \u201c The author should clarify the difference and the strong points of the proposed block compared to SEnet. \u201d Thanks for your meaningful question ! From the method 's point of view , we fuse feature maps instead of fusing kernels in the training stage , it can be done via the combination of group point-wise convolution and SE mechanism . However we fuse kernels in the inference stage , it can ` t be done via this combination . This is the core difference with SEnet and reduces the computation cost by gt times compared with fusing feature maps or the combination of group point-wise convolution and SE mechanism . From the experiments ' point of view , as discussed in Sec 4.5 , our proposed method will be the same as SENet when g_t=1 . However , compared with g_t=6 , its performance drops a lot while the computation cost of convolution stay the same as shown in Table 6 . > > > Response to \u201c Testing the ImageNet trained network of the proposed method into an object detection task ( as the pre-trained backbone ) . \u201d Thanks for your suggestion . We are trying to implement the baseline and add experiments for detection in the revision before the deadline ."}, "2": {"review_id": "SyeZIkrKwS-2", "review_text": "This paper proposed dynamic convolution (DyNet) to accelerating convolution networks. The new method is tested on the ImageNet dataset with three different backbones. It reduces the computation flops by a large margin while keeps similar classification accuracy. The additional segmentation experiment on the Cityscapes dataset also shows the new module can save computation a lot while maintaining similar segmentation accuracy. Clarity: The novelty of the paper is limited and the experimental results are weird for me. 1. The proposed module named dynamic convolution is detailed in Sec 3.2. As far as I can see, it is very similar to the former SENet especially in Figure (3) and Equation (2). The only difference is the introduction of g_t where the output dimension is much larger than SENet. 2. As shown in Equation (2), the proposed method contains the normal computation of fixed kernels. How can this method save computations compared to classical convolution? Is the computation flops calculated in the right way? 3. The results in Table 5 are strange to me. Larger g_t will increase the flops absolutely according to Equation (2). 4. The author may need to show the comparisons of the number of parameters. In my opinion, the new module will increase the parameters a lot (the output dimension of the fully connected layer is as large as C_cout*g_t). ", "rating": "3: Weak Reject", "reply_text": "> > > Response to \u201c The only difference with SENet is the introduction of g_t where the output dimension is much larger than SENet. \u201d : Thanks for your meaningful question ! The motivation of our proposed approach is different from SENet . We focus on reducing the computation cost during inference stage , while SENet learns channel-wise importance to recalibrates feature maps . From the methods point of view , during training , indeed the only difference is that the output dimension is much larger than SENet . However , during inference , SENet recalibrate feature maps by learned weights , while we fuse kernels instead of fusing feature maps , which can reduce the output dimension and computation cost of convolution kernels . Moreover , as discussed in Sec 4.5 , our proposed method will be the same as SENet when g_t=1 . However , compared with g_t=6 , its performance drops a lot while the computation cost of convolution stay the same as shown in Table 6 . > > > Response to \u201c How can this method save computations compared to classical convolution ? \u201d : Thanks for your valuable comments ! The goal of our proposed approach is to reduce the computation cost in the inference stage . Equation ( 2 ) only shows that fusing feature maps is mathematically equivalent with fusing kernels . During training , we firstly generate C_cout * g_t output feature maps and then get C_out feature maps by fusing each g_t ones . Thus we can not save computation cost in training procedure . However , we fuse kernels during inference stage instead of fusing feature maps . After fusing , the weight for one convolution layer with shape [ C_out * g_t , C_in , k , k ] will become [ C_cout , C_in , k , k ] , thus the FLOPs is reduced by g_t times . > > > Response to \u201c Why larger g_t doesn ` 't increase the flops in Table 5 ? \u201d : Compared with g_t=1 , the computation cost of convolution is 6 times larger when g_t=6 during training . However , we will fuse each g_t kernels to get dynamic convolutional weight with shape [ C_cout , C_in , k , k ] during inference as Equation ( 1 ) , it makes the FLOPs of convolution invariable with g_t . As for the additional FLOPs of fusing kernels , it varies with g_t but is negligible . For Dy-mobile ( 1.0 ) , the FLOPs of fusing kernels is merely 0.459 * g_t M , when g_t=6 it is 2.75M while the FLOPs of convolution is 129 M. > > > Response to \u201c The author may need to show the comparisons of the number of parameters. \u201d : Thanks for the suggestion ! The new module indeed increases the parameters a lot , because we mainly focus on the computation cost rather than the number of parameters . We will add parameter counts in experiment table in our updated version ."}}