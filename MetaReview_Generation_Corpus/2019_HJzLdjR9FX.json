{"year": "2019", "forum": "HJzLdjR9FX", "title": "DeepTwist: Learning Model Compression via Occasional Weight Distortion", "decision": "Reject", "meta_review": "The authors propose a framework for compressing neural network models which involves applying a weight distortion function periodically as part of training. The proposed approach is relatively simple to implement, and is shown to work for weight pruning, low-rank compression and quantization, without sacrificing accuracy. \nHowever, the reviewers had a number of concerns about the work. Broadly, the reviewers felt that the work was incremental. Further, if the proposed techniques are important to get the approach to work well in practice, then the paper would be significantly strengthened by further analyses. Finally, the reviewers noted that the paper does not consider whether the specific weight pruning strategies result in a reduction of computational resources beyond potential storage savings, which would be important if this method is to be used in practice.\n\nOverall, the AC tends to agree with the reviewers criticisms. The authors are encouraged to address some of these issues in future revisions of the work.\n", "reviews": [{"review_id": "HJzLdjR9FX-0", "review_text": "This paper proposed a general framework, DeepTwist, for model compression. The so-called weight distortion procedure is added into the training every several epochs. Three applications are shown to demonstrate the usage of the proposed approach. Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent. See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference. Specifically, the proposed framework can be easily reformulated as a loss function plus a regularizer for proximal gradient. Using gradient descent (GD), there will be two steps: (1) finding a new solution using GD, and (2) project the new solution using proximal function. Now in deep learning, since SGD is used for optimization, several steps are need to locate reasonable solutions, i.e. the Distortion Step in the framework. Then proximal function can be applied directly after Distortion Step to project the solutions. In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent. Since SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1. PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the review . While formulating a proximal function for model compression might be an interesting idea ( if search space is highly limited ) as the reviewer suggested , we believe that our proposed method is fundamentally different from proximal gradient descent approaches due to the following reasons : 1 ) Proximal gradient descent is meant to solve a convex optimization problem while our aim is to solve a non-convex problem in which each local minimum exhibits vastly different test accuracy after compression . Jumping to another local minimum from a certain minimum would not be easily achieved by convex optimization methods . 2 ) Finding a particular flat minimum is the key to obtaining good model compression ( and good generalization as well ) . Such an exploration , however , can not be obtained by a proximal function since we need to investigate lots of different local minima with different amount of flatness in loss surface . 3 ) While proximal gradient descent can be useful to find a certain local minimum close to the starting point given a convex constraint , wide exploration ( associated with possibly transient accuracy loss in the initial training as shown in Figure 2 . ( b ) ) is necessary to escape from a point with sharp loss surface . Investigating many different local minima would be only available with large learning rate ( as we have chosen for our experiments ) and/or large amount of weight distortion . 4 ) Our effort to introduce optimal distortion step size and learning rate for a given compression problems is connected to exploration , not exploitation ( which potentially supported by proximal functions where convergence matters ) . Even though proximal gradient descent selects step size only considering convergence , Figure 1 can lead to the results such as Figure 2 ( b ) which can not be obtained if only local exploitation is employed . Finding a flat minimum has been known to be a difficult work as shown in the paper \u201c On large-batch training for deep learning : generalization gap and sharp minima \u201d , ICLR 2016 . We firmly believe that our search space exploration method based on optimal distortion step size and amount of weight distortion enable us to produce better local minima well-suited to various model compression techniques . In short , unfortunately , we have failed to understand how you could connect our technique to proximal functions and proximal gradient descent . We strongly hope that you reconsider your decision ."}, {"review_id": "HJzLdjR9FX-1", "review_text": "The paper does not really propose a new way of compressing the model weights, but rather a way of applying existing weight compression techniques. Specifically, the proposed solution is to repeatedly apply weight compression and fine-tuning over the entire training process. Unlike the existing work, weight compression is applied as a form of weight distortion, i.e. the model has the full degree of freedom during fine-tuning (to recover potential compression errors). Pros: - The proposed method is shown to work with existing methods like weight pruning, low-rank compression and quantization. Cons: - The idea is a simple extension of existing work. - In Table 4, it is hard to compare DeepTwist with the other methods because activation quantization is not used. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review . While the weight formats after model compression follow well known ones , our model compression method is significantly different from the existing ones . Let us discuss some parts of reasons . - Training models after compression in order to recover accuracy is as important ( if not more ) as compressing weights . We have found that occasional distortions ( not compressing weights for every mini-batch like previous techniques ) , relatively large learning rate , and training batches in full-precision ( unlike previous ones which store compressed weights during entire training ) would be the key to recovering or even increasing the accuracy . - Exploring large search space in much wider area is suggested in this paper through large distortion step and large learning rate ( note that many compression-aware techniques perform compression at every batch has distortion step of \u201c 1 \u201d while much smaller learning rate for retraining that normal training is chosen ) . As we discussed in the paper , investigating various local minima is crucial for good model compression . - Our pruning method is fundamentally different from the previous ones because we do not incorporate a masking layer . While previous pruning ideas keep zero weights during training , we do not have any zero weights at any moment except at the weight distortion step . - Our low-rank approximation is also unique one since 1 ) we do not alter the structure for training even after performing SVD , 2 ) very high learning rate associated with transient accuracy loss is allowed for DeepTwist , and 3 ) we change SV spectrum continuously while the previous ones perform SVD only once ( in practice , retraining low-rank approximated model has been considered to be very difficult , if not impossible ) . - Even though our pruning method is even simpler compared to the previous ones , compression rate is significantly better or very close to the one based on sophisticated Bayesian inference model . - Low-rank approximation results on PTB ( Figure 2 ) shows even higher compression rate compared with weight pruning ( Table 3 ) , which is surprising to us because pruning has been known to show much higher compression ratio compared with SVD ( fine-grain vs. coarse-grain or structured ) . - Quantization is performed also in a very different way . Unlike previous ones , we do not consider quatization during training . \u201c Do not perform quantization at every batch , but instead recover accuracy through full-precision training , high learning rate , and occasional quantization \u201d is the key message . - Overall , our occasional compression is a significant one since we can greatly reduce amount of computation overhead from compression . If our technique is a simple extension from the previous ones , we could not obtain such impressive results with high compression rate and improved accuracy . We believe that our paper suggests a wide view on how model compression should be performed ."}, {"review_id": "HJzLdjR9FX-2", "review_text": "A model compression framework, DeepTwist, was proposed which makes the weights zero if they are small in magnitude. They used different model compression techniques in this framework to show the effectiveness of the proposed method. This paper proposes a framework intending to use fewer hardware resources without compromising the model accuracy. However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node. Therefore, it is not clear how the proposed framework is helping the model compression techniques. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the review . First , we want to mention that DeepTwist is proposed not only for weight pruning , but also for other compression techniques , such as quantization and low-rank approximation , as we discussed in Section 4.2 and 4.3 After weight pruning is performed and zero weights are removed , we usually obtain a sparse matrix to represent non-zero weights . There are lots of existing sparse matrix computation libraries to support SpMV ( sparse matrix-vector multiplication ) and so on . If a matrix is highly sparse , then we would reduce memory footprint and amount of computations ( for example , we can skip zero weights during computation ) significantly . There have been extensive studies of efficient hardware implementation after weight pruning , and we want you to refer to the paper \u201c EIE : efficient inference engine on compressed deep neural network \u201d or \u201c Deep compression : compressing deep neural networks with pruning , trained quatization and Huffman coding. \u201d In this paper , we have not discussed particular sparse matrix implementation methods which are not our focus in this paper . We would greatly appreciate if you can reconsider your decision based on our comments and other methods we also discussed ( i.e. , quantization and low-rank approximation ) ."}], "0": {"review_id": "HJzLdjR9FX-0", "review_text": "This paper proposed a general framework, DeepTwist, for model compression. The so-called weight distortion procedure is added into the training every several epochs. Three applications are shown to demonstrate the usage of the proposed approach. Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent. See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference. Specifically, the proposed framework can be easily reformulated as a loss function plus a regularizer for proximal gradient. Using gradient descent (GD), there will be two steps: (1) finding a new solution using GD, and (2) project the new solution using proximal function. Now in deep learning, since SGD is used for optimization, several steps are need to locate reasonable solutions, i.e. the Distortion Step in the framework. Then proximal function can be applied directly after Distortion Step to project the solutions. In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent. Since SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1. PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the review . While formulating a proximal function for model compression might be an interesting idea ( if search space is highly limited ) as the reviewer suggested , we believe that our proposed method is fundamentally different from proximal gradient descent approaches due to the following reasons : 1 ) Proximal gradient descent is meant to solve a convex optimization problem while our aim is to solve a non-convex problem in which each local minimum exhibits vastly different test accuracy after compression . Jumping to another local minimum from a certain minimum would not be easily achieved by convex optimization methods . 2 ) Finding a particular flat minimum is the key to obtaining good model compression ( and good generalization as well ) . Such an exploration , however , can not be obtained by a proximal function since we need to investigate lots of different local minima with different amount of flatness in loss surface . 3 ) While proximal gradient descent can be useful to find a certain local minimum close to the starting point given a convex constraint , wide exploration ( associated with possibly transient accuracy loss in the initial training as shown in Figure 2 . ( b ) ) is necessary to escape from a point with sharp loss surface . Investigating many different local minima would be only available with large learning rate ( as we have chosen for our experiments ) and/or large amount of weight distortion . 4 ) Our effort to introduce optimal distortion step size and learning rate for a given compression problems is connected to exploration , not exploitation ( which potentially supported by proximal functions where convergence matters ) . Even though proximal gradient descent selects step size only considering convergence , Figure 1 can lead to the results such as Figure 2 ( b ) which can not be obtained if only local exploitation is employed . Finding a flat minimum has been known to be a difficult work as shown in the paper \u201c On large-batch training for deep learning : generalization gap and sharp minima \u201d , ICLR 2016 . We firmly believe that our search space exploration method based on optimal distortion step size and amount of weight distortion enable us to produce better local minima well-suited to various model compression techniques . In short , unfortunately , we have failed to understand how you could connect our technique to proximal functions and proximal gradient descent . We strongly hope that you reconsider your decision ."}, "1": {"review_id": "HJzLdjR9FX-1", "review_text": "The paper does not really propose a new way of compressing the model weights, but rather a way of applying existing weight compression techniques. Specifically, the proposed solution is to repeatedly apply weight compression and fine-tuning over the entire training process. Unlike the existing work, weight compression is applied as a form of weight distortion, i.e. the model has the full degree of freedom during fine-tuning (to recover potential compression errors). Pros: - The proposed method is shown to work with existing methods like weight pruning, low-rank compression and quantization. Cons: - The idea is a simple extension of existing work. - In Table 4, it is hard to compare DeepTwist with the other methods because activation quantization is not used. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review . While the weight formats after model compression follow well known ones , our model compression method is significantly different from the existing ones . Let us discuss some parts of reasons . - Training models after compression in order to recover accuracy is as important ( if not more ) as compressing weights . We have found that occasional distortions ( not compressing weights for every mini-batch like previous techniques ) , relatively large learning rate , and training batches in full-precision ( unlike previous ones which store compressed weights during entire training ) would be the key to recovering or even increasing the accuracy . - Exploring large search space in much wider area is suggested in this paper through large distortion step and large learning rate ( note that many compression-aware techniques perform compression at every batch has distortion step of \u201c 1 \u201d while much smaller learning rate for retraining that normal training is chosen ) . As we discussed in the paper , investigating various local minima is crucial for good model compression . - Our pruning method is fundamentally different from the previous ones because we do not incorporate a masking layer . While previous pruning ideas keep zero weights during training , we do not have any zero weights at any moment except at the weight distortion step . - Our low-rank approximation is also unique one since 1 ) we do not alter the structure for training even after performing SVD , 2 ) very high learning rate associated with transient accuracy loss is allowed for DeepTwist , and 3 ) we change SV spectrum continuously while the previous ones perform SVD only once ( in practice , retraining low-rank approximated model has been considered to be very difficult , if not impossible ) . - Even though our pruning method is even simpler compared to the previous ones , compression rate is significantly better or very close to the one based on sophisticated Bayesian inference model . - Low-rank approximation results on PTB ( Figure 2 ) shows even higher compression rate compared with weight pruning ( Table 3 ) , which is surprising to us because pruning has been known to show much higher compression ratio compared with SVD ( fine-grain vs. coarse-grain or structured ) . - Quantization is performed also in a very different way . Unlike previous ones , we do not consider quatization during training . \u201c Do not perform quantization at every batch , but instead recover accuracy through full-precision training , high learning rate , and occasional quantization \u201d is the key message . - Overall , our occasional compression is a significant one since we can greatly reduce amount of computation overhead from compression . If our technique is a simple extension from the previous ones , we could not obtain such impressive results with high compression rate and improved accuracy . We believe that our paper suggests a wide view on how model compression should be performed ."}, "2": {"review_id": "HJzLdjR9FX-2", "review_text": "A model compression framework, DeepTwist, was proposed which makes the weights zero if they are small in magnitude. They used different model compression techniques in this framework to show the effectiveness of the proposed method. This paper proposes a framework intending to use fewer hardware resources without compromising the model accuracy. However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node. Therefore, it is not clear how the proposed framework is helping the model compression techniques. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the review . First , we want to mention that DeepTwist is proposed not only for weight pruning , but also for other compression techniques , such as quantization and low-rank approximation , as we discussed in Section 4.2 and 4.3 After weight pruning is performed and zero weights are removed , we usually obtain a sparse matrix to represent non-zero weights . There are lots of existing sparse matrix computation libraries to support SpMV ( sparse matrix-vector multiplication ) and so on . If a matrix is highly sparse , then we would reduce memory footprint and amount of computations ( for example , we can skip zero weights during computation ) significantly . There have been extensive studies of efficient hardware implementation after weight pruning , and we want you to refer to the paper \u201c EIE : efficient inference engine on compressed deep neural network \u201d or \u201c Deep compression : compressing deep neural networks with pruning , trained quatization and Huffman coding. \u201d In this paper , we have not discussed particular sparse matrix implementation methods which are not our focus in this paper . We would greatly appreciate if you can reconsider your decision based on our comments and other methods we also discussed ( i.e. , quantization and low-rank approximation ) ."}}