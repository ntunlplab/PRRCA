{"year": "2018", "forum": "SkBYYyZRZ", "title": "Searching for Activation Functions", "decision": "Invite to Workshop Track", "meta_review": "The author's propose to use swish and show that it performs significantly better than Relus on sota vision models. Reviewers and anonymous ones counter that PRelus should be doing quite well too. Unfortunately, the paper falls in the category where it is hard to prove the utility of the method through one paper alone, and broader consensus relies on reproduction by the community. As a results, I'm going to recommend publishing to a workshop for now.", "reviews": [{"review_id": "SkBYYyZRZ-0", "review_text": "Authors propose a reinforcement learning based approach for finding a non-linearity by searching through combinations from a set of unary and binary operators. The best one found is termed Swish unit; x * sigmoid(b*x). The properties of Swish like allowing information flow on the negative side and linear nature on the positive have been proven to be important for better optimization in the past by other functions like LReLU, PLReLU etc. As pointed out by the authors themselves for b=1 Swish is equivalent to SiL proposed in Elfwing et. al. (2017). In terms of experimental validation, in most cases the increase is performance when using Swish as compared to other models are very small fractions. Again, the authors do state that \"our results may not be directly comparable to the results in the corresponding works due to differences in our training steps.\" Based on the Figure 6 authors claim that the non-monotonic bump of Swish on the negative side is very important aspect. More explanation is required on why is it important and how does it help optimization. Distribution of learned b in Swish for different layers of a network can interesting to observe.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We don \u2019 t completely understand the reviewer \u2019 s rationale for rejection . Is it because of the lack of novelty , the inconsistent gains , or the work being insignificant ? First , in terms of the work being significant , we want to emphasize that ReLU is the cornerstone of deep learning models . Being able to replace ReLU is extremely impactful because it produces a gain across a large number of models . So in terms of impact , we believe that our work is significant . Secondly , in terms of inconsistent gains , the signed tests already confirm that the gains are statistically significant in our experiments . These results suggest that switching to Swish is an easy and consistent way of getting an improvement regardless of which baseline activation function is used . Unlike previous studies , the baselines in our work are extremely strong : they are state-of-the-art models where the models are built with ReLUs as the default activation . Furthermore , the same amount of tuning was used for every activation function , and in fact , many non-Swish activation functions actually got more tuning . Thus , it is unreasonable to expect a huge improvement . That said , in some cases , Swish on Imagenet makes a more than 1 % top-1 improvement . For context , the gap between Inception-v3 and Inception-v4 ( a year of work ) is only 1.2 % . Finally , in terms of novelty , our work differs from Elfwing et al . ( 2017 ) in a number of significant ways . They just propose a single activation function , whereas our work searches over a vast space of activation functions to find the best empirically performing activation function . The search component is important because we save researchers from the painful process of manually trying out a number of individual activation functions in order to find one that outperforms ReLU ( i.e. , graduate student descent ) . The activation function found by this search , Swish , is more general than the other proposed by Elfwing et al . ( 2017 ) .Another key contribution is our thorough empirical study . Their activation function was tested only on relatively shallow reinforcement learning models . We performed a thorough experimental evaluation on many challenging , deep , large-scale supervised models with extremely strong baselines . We believe these differences are significant enough to differentiate us . The non-monotonic bump , which is controlled by beta , has gradients for negative preactivations ( unlike ReLU ) . We have plotted the beta distribution over the each layer Swish here : https : //imgur.com/a/AIbS2 . Note this is on the Mobile NASNet-A model , which has many layers composed in parallel ( similar to Inception and unlike ResNet ) . The plot suggests that the tuneable beta is flexibly used . Early layers use large values of beta , which corresponds to ReLU-like behavior , whereas later layers tend to stay around the [ 0 , 1.5 ] range , corresponding to a more linear-like behavior ."}, {"review_id": "SkBYYyZRZ-1", "review_text": "This paper is utilizing reinforcement learning to search new activation function. The search space is combination of a set of unary and binary functions. The search result is a new activation function named Swish function. The authors also run a number of ImageNet experiments, and one NTM experiment. Comments: 1. The search function set and method is not novel. 2. There is no theoretical depth in the searched activation about why it is better. 3. For leaky ReLU, use larger alpha will lead better result, eg, alpha = 0.3 or 0.5. I suggest to add experiment to leak ReLU with larger alpha. This result has been shown in previous work. Overall, I think this paper is not meeting ICLR novelty standard. I recommend to submit this paper to ICLR workshop track. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "1.Can the reviewer explain further why our work is not novel ? Our activation function and the method to find it have not been explored before , and our work holds the promise of improving representation learning across many models . Furthermore , no previous work has come close to our level of thorough empirical evaluation . This type of contribution is as important as novelty -- it can be argued that the resurgence of CNNs is primarily due to conceptually simple empirical studies demonstrating their effectiveness on new datasets . 2.We respectfully disagree with the reviewer that theoretical depth is necessary to be accepted . Following this argument , we can also argue that many extremely useful techniques in representation / deep learning , such as word2vec , ReLU , BatchNorm , etc , should not be accepted to ICLR because the original papers did not supply theoretical results about why they worked . Our community has typically followed that paradigm of discovering techniques experimentally and further work analyzing the technique . We believe our thorough and fair empirical evaluation provides a solid foundation for further work analyzing the theoretical properties of Swish . 3.We experimented with the leaky ReLU using alpha = 0.5 on Inception-ResNet-v2 using the same hyperparameter sweep , and and did not find any improvement over the alpha used in our work ( which was suggested by the original paper that proposed leaky ReLUs ) ."}, {"review_id": "SkBYYyZRZ-2", "review_text": "The author uses reinforcement learning to find new potential activation functions from a rich set of possible candidates. The search is performed by maximizing the validation performance on CIFAR-10 for a given network architecture. One candidate stood out and is thoroughly analyze in the reste of the paper. The analysis is conducted across images datasets and one translation dataset on different architectures and numerous baselines, including recent ones such as SELU. The improvement is marginal compared to some baselines but systematic. Signed test shows that the improvement is statistically significant. Overall the paper is well written and the lack of theoretical grounding is compensated by a reliable and thorough benchmark. While a new activation function is not exiting, improving basic building blocks is still important for the community. Since the paper is fairly experimental, providing code for reproducibility would be appreciated.", "rating": "7: Good paper, accept", "reply_text": "The reviewer suggested \u201c Since the paper is fairly experimental , providing code for reproducibility would be appreciated \u201d . We agree , and we will open source some of the experiments around the time of acceptance ."}], "0": {"review_id": "SkBYYyZRZ-0", "review_text": "Authors propose a reinforcement learning based approach for finding a non-linearity by searching through combinations from a set of unary and binary operators. The best one found is termed Swish unit; x * sigmoid(b*x). The properties of Swish like allowing information flow on the negative side and linear nature on the positive have been proven to be important for better optimization in the past by other functions like LReLU, PLReLU etc. As pointed out by the authors themselves for b=1 Swish is equivalent to SiL proposed in Elfwing et. al. (2017). In terms of experimental validation, in most cases the increase is performance when using Swish as compared to other models are very small fractions. Again, the authors do state that \"our results may not be directly comparable to the results in the corresponding works due to differences in our training steps.\" Based on the Figure 6 authors claim that the non-monotonic bump of Swish on the negative side is very important aspect. More explanation is required on why is it important and how does it help optimization. Distribution of learned b in Swish for different layers of a network can interesting to observe.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We don \u2019 t completely understand the reviewer \u2019 s rationale for rejection . Is it because of the lack of novelty , the inconsistent gains , or the work being insignificant ? First , in terms of the work being significant , we want to emphasize that ReLU is the cornerstone of deep learning models . Being able to replace ReLU is extremely impactful because it produces a gain across a large number of models . So in terms of impact , we believe that our work is significant . Secondly , in terms of inconsistent gains , the signed tests already confirm that the gains are statistically significant in our experiments . These results suggest that switching to Swish is an easy and consistent way of getting an improvement regardless of which baseline activation function is used . Unlike previous studies , the baselines in our work are extremely strong : they are state-of-the-art models where the models are built with ReLUs as the default activation . Furthermore , the same amount of tuning was used for every activation function , and in fact , many non-Swish activation functions actually got more tuning . Thus , it is unreasonable to expect a huge improvement . That said , in some cases , Swish on Imagenet makes a more than 1 % top-1 improvement . For context , the gap between Inception-v3 and Inception-v4 ( a year of work ) is only 1.2 % . Finally , in terms of novelty , our work differs from Elfwing et al . ( 2017 ) in a number of significant ways . They just propose a single activation function , whereas our work searches over a vast space of activation functions to find the best empirically performing activation function . The search component is important because we save researchers from the painful process of manually trying out a number of individual activation functions in order to find one that outperforms ReLU ( i.e. , graduate student descent ) . The activation function found by this search , Swish , is more general than the other proposed by Elfwing et al . ( 2017 ) .Another key contribution is our thorough empirical study . Their activation function was tested only on relatively shallow reinforcement learning models . We performed a thorough experimental evaluation on many challenging , deep , large-scale supervised models with extremely strong baselines . We believe these differences are significant enough to differentiate us . The non-monotonic bump , which is controlled by beta , has gradients for negative preactivations ( unlike ReLU ) . We have plotted the beta distribution over the each layer Swish here : https : //imgur.com/a/AIbS2 . Note this is on the Mobile NASNet-A model , which has many layers composed in parallel ( similar to Inception and unlike ResNet ) . The plot suggests that the tuneable beta is flexibly used . Early layers use large values of beta , which corresponds to ReLU-like behavior , whereas later layers tend to stay around the [ 0 , 1.5 ] range , corresponding to a more linear-like behavior ."}, "1": {"review_id": "SkBYYyZRZ-1", "review_text": "This paper is utilizing reinforcement learning to search new activation function. The search space is combination of a set of unary and binary functions. The search result is a new activation function named Swish function. The authors also run a number of ImageNet experiments, and one NTM experiment. Comments: 1. The search function set and method is not novel. 2. There is no theoretical depth in the searched activation about why it is better. 3. For leaky ReLU, use larger alpha will lead better result, eg, alpha = 0.3 or 0.5. I suggest to add experiment to leak ReLU with larger alpha. This result has been shown in previous work. Overall, I think this paper is not meeting ICLR novelty standard. I recommend to submit this paper to ICLR workshop track. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "1.Can the reviewer explain further why our work is not novel ? Our activation function and the method to find it have not been explored before , and our work holds the promise of improving representation learning across many models . Furthermore , no previous work has come close to our level of thorough empirical evaluation . This type of contribution is as important as novelty -- it can be argued that the resurgence of CNNs is primarily due to conceptually simple empirical studies demonstrating their effectiveness on new datasets . 2.We respectfully disagree with the reviewer that theoretical depth is necessary to be accepted . Following this argument , we can also argue that many extremely useful techniques in representation / deep learning , such as word2vec , ReLU , BatchNorm , etc , should not be accepted to ICLR because the original papers did not supply theoretical results about why they worked . Our community has typically followed that paradigm of discovering techniques experimentally and further work analyzing the technique . We believe our thorough and fair empirical evaluation provides a solid foundation for further work analyzing the theoretical properties of Swish . 3.We experimented with the leaky ReLU using alpha = 0.5 on Inception-ResNet-v2 using the same hyperparameter sweep , and and did not find any improvement over the alpha used in our work ( which was suggested by the original paper that proposed leaky ReLUs ) ."}, "2": {"review_id": "SkBYYyZRZ-2", "review_text": "The author uses reinforcement learning to find new potential activation functions from a rich set of possible candidates. The search is performed by maximizing the validation performance on CIFAR-10 for a given network architecture. One candidate stood out and is thoroughly analyze in the reste of the paper. The analysis is conducted across images datasets and one translation dataset on different architectures and numerous baselines, including recent ones such as SELU. The improvement is marginal compared to some baselines but systematic. Signed test shows that the improvement is statistically significant. Overall the paper is well written and the lack of theoretical grounding is compensated by a reliable and thorough benchmark. While a new activation function is not exiting, improving basic building blocks is still important for the community. Since the paper is fairly experimental, providing code for reproducibility would be appreciated.", "rating": "7: Good paper, accept", "reply_text": "The reviewer suggested \u201c Since the paper is fairly experimental , providing code for reproducibility would be appreciated \u201d . We agree , and we will open source some of the experiments around the time of acceptance ."}}