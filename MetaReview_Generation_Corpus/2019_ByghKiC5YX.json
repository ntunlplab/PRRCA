{"year": "2019", "forum": "ByghKiC5YX", "title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "decision": "Reject", "meta_review": "I appreciate the willingness of the authors to engage in vigorous discussion about their paper. Although several reviewers support accepting this submission, I do not find their arguments for acceptance convincing. The paper considers automated methods for finding errors in text classification models. I believe it is valuable to study the errors our models make in order to understand when they work well and how to improve them. Crucially, in the later case, we should demonstrate how to use the errors we find to close the loop and create better models.\n\nA paper about techniques to find errors for text models should make a sufficiently large contribution to be accepted. I view the following hypothetical contributions as the most salient in this specific case thus my decision reduces to determining if any of these conditions have been met. A paper need not achieve all of these things, any one of them would suffice:\n\n1. Show that the errors found can be used to meaningfully improve the models. \n\nThis requires building a better model than the one probed by the method and convincingly demonstrating that it is superior in an important way that is relevant to the original goals of the application. Ideally it would also consider alternative, simpler ways to improve the models (e.g. making them larger).\n\n2. Show that errors are difficult to find, but that the proposed method is nonetheless capable of finding errors and that the method is non-obvious to a researcher in the field.\n\nThis is not applicable here because errors are extremely easy to find on the test set and from labeling more data. If we demand an automated method, then the greedy algorithm does not qualify as sufficiently non-obvious and it seems to work fine, making the Gumbel method unnecessary.\n\n3. Show that the particular specific errors found are qualitatively different from other errors in their implications and that they provide a unique and important insight.\n\nI do not believe this submission attempts to show this type of contribution. One example of this type of paper would be a paper that does a comparative study of the errors that different models make and finds something interesting (potentially yielding a path to improved models).\n\n4. Generate a new, more difficult/interesting, dataset by finding errors of one or more trained models\n\nGiven that the authors use human labelers to validate examples this is potentially another path. Here is an example of a paper using adversarial techniques in this way: https://arxiv.org/abs/1808.05326\nHowever, I believe the paper would need to be rethought and rewritten to make this sort of contribution.\n\n\nUltimately, the authors and reviews supporting acceptance must explain the contribution succinctly and convincingly. The reviewers most strongly advocating for accepting this submission seem to be saying that there is a valuable new method and probabilistic framework proposed here for finding model errors. I believe researchers in the field could have easily come up with the greedy algorithm (a standard approach to discrete optimization problems) proposed here without needing to read the paper. Furthermore, I believe the other more complicated Gumbel algorithm proposed is not necessary given the similarly effective and simpler greedy algorithm. If the authors believe that the Gumbel algorithm provides application-relevant advantages over the greedy algorithm, then they should specify how these errors will be used and rewrite the paper to make the greedy algorithm a baseline. However, I do not believe the experimental results support this idea.\n", "reviews": [{"review_id": "ByghKiC5YX-0", "review_text": "This paper introduces two new methods for generating adversarial examples for text classification models. The paper is well written, the introduced algorithms and experiments are easy to understand. However, I do not believe that these two methods are sufficiently significant. First of all, I am not convinced that the attacks can be classified as \u201cadversarial examples\u201d, especially the ones on the word-based models. The community originally got interested in adversarial examples because while they can easily be classified correctly by humans, they seemed to fool machine learning models with high efficiency. For example, the PGD attack by Madry et al. can reduce the accuracy of a CIFAR-10 model to 0% by using distortions that are not at all noticeable to humans. In the case of the word-based task studied here, human accuracy drops by 8-11%. While the question of whether adversarial examples are actually a security threat is under debate, the attacks on the word-based models here do not even classify as adversarial examples. Of course, it is interesting that the ML models are much less robust to these distortions than humans are, however, this is a well known problem. This paper did not perform comprehensive experiments to investigate this phenomenon. For example, they could have evaluated a wide range of distortions (including random distortions), and then check if training with all of these distortions makes the network more robust \u2026 etc (for example, see [1]). The attacks on character-based models are closer to adversarial examples from this perspective. However, the performance of the Gumbel Attack is significantly worse on character-based models than an attack as simple as the Delete-1 attack. The Greedy attack is more successful than the Delete-1 attack, however it is a straight-forward application of greedy optimization on discrete data and is not very novel or interesting. [1] Generalisation in humans and deep neural networks, arXiv:1808.08750 ", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for reading our paper and giving detailed comments on our paper . However , we observe the review is posted after the deadline of paper modification has passed , and hope to address a few points of this review . In short , 1 . We think the comparison between adversarial attack on images and on texts is unfair . 2.Some of the questions in the review are highly correlated with AC \u2019 s questions and we have answered in our previous rebuttal . We now address them in details . First , we agree with the reviewer that adversarial attack on texts is at a relatively new stage compared to the counterpart on images . However , to evaluate our work , we think it makes more sense to compare our methods with the best methods in this area , instead of with methods on a different data set . As an example , it does not make sense to reject a paper because their model achieves a lower accuracy on ImageNet than the accuracy of a very simple model on MNIST . We have shown that our method outperforms previous text adversarial attack algorithms in Figure 3 , and we have even compared all methods under human evaluation in Appendix B , which indicates that humans are least sensitive to adversarial examples generated by our algorithm . So we believe our method can advance state-of-the-art in attacks on texts . \u201c The attacks on character-based models are closer to adversarial examples from this perspective. \u201d We aim to propose a general mathematical framework to generate adversarial examples for models with discrete input . Thus , the same algorithm works for both character-based and word-based models , and could be potentially useful for other NLP models such as word-piece models . We are happy to see that the reviewer consider character-based adversarial examples more interesting . \u201c The Greedy attack is a straightforward application of greedy optimization on discrete data and is not very novel or interesting. \u201d See our rebuttal to AC ( point 2 ) in \u201c Efficiency of Gumbel Attack ; Difference and Connections in Discrete Optimization and Adversarial Attack. \u201d We will also elaborate this in our final version . The reviewer also proposed to include an experiment on how our attacks perform on models trained with data augmentation techniques . We agree with the reviewer that the proposed experiment can be interesting . However , given the timeline , we are not able to update the paper now . We are willing to add it in our final version ."}, {"review_id": "ByghKiC5YX-1", "review_text": "This paper addresses the problem of generating adversarial examples for discrete domains like text. They propose two simple techniques: 1) Greedy: two stage process- first stage involves finding the k words in the sentence/paragraph to perturb and second step changes the word in the positions identified in step 1. 2) Gumbel: first approach amortized over datasets where first and second steps are parametrized and learned over the dataset with the loss being the probability of flipping the decision. Specifically, for the Gumbel approach, the authors use the non-differentiable top-k-argmax output to train the module in the second step which is not ideal and it would be better to train both first and second steps jointly in an end-to-end differentiable manner. The results show that Greedy approach is able to significantly affect the accuracy of the systems compared to other adversarial baselines. Mturk evaluation shows that for tasks like sentiment analysis, humans weren't as confused as the systems were when the selected words were changed which is encouraging. However, the Gumbel method performs poorly compared to other baselines. Moreover, a thorough analysis of why Greedy is doing better than some gradient based adversarial attacks is needed in the paper because it is unclear what is causing their greedy approach to perform well; is it the two-stage nature of the process? My major gripe with the paper is that it is egregiously difficult to read in parts and is poorly written. There are dangling conditional bars in many equations (5, 7, Greedy attack etc.), unclear \"expectation (E)\" signs and many other confusing notational choices which make the math difficult to parse. I am not even sure if those equations are correctly conveying the idea they are meant to convey. I found the algorithms to be more clearly written and realize that the text in the models and equations is unnecessarily complicated. The argument about approximation to the objective by considering the i positions independently is not convincing and their is nothing in the paper to show if the assumption is reasonable.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Area Chair and Anonymous Reader : Thanks for your questions on the motivation of adversarial attack for discrete data . Below we briefly explain the motivation , followed by the evidence that simple random perturbation does not work . In summary , the area chair and another reader posed the following questions\uff1a 1 . Why does one need to study the phenomenon of adversarial examples on discrete data ? 2.Why is this paper worth reading ? 3.Do simple methods like random perturbation work on text data ? In short , our reply is 1 . Robustness is an important criterion for models on discrete data . The generation of adversarial examples can be used to evaluate robustness or even improve robustness . 2.In this paper , our goal is to propose methods with better performance ( Greedy attack ) or with higher efficiency ( Gumbel attack ) . 3.We provide evidence that simple methods like random perturbation do not work . Below are concrete details : Robustness is an important criterion for the application of machine learning models in critical areas such as medicine , financial markets , recommendation systems , and criminal justice . Adversarial examples have been used to evaluate the ( adversarial ) robustness of models ( e.g. , [ 1 , 2 , 5 ] ) and have also been applied to train robust models ( e.g. , [ 3 , 4 ] ) . The phenomenon of adversarial examples was first found in state-of-the-art deep neural network models for classifying images ( e.g. , [ 5 , 6 , 2 ] ) , where small perturbations unobservable by human can easily fool neural networks . Similar to image data , the problem of adversarial perturbation on discrete data can be defined as altering the prediction of a model via minimal perturbation to an original sample ( e.g. , [ 7-14 ] ) . While there have been many pioneered and interesting papers in this area ( e.g. , [ 7-14 ] ) , we proposed Greedy attack , a method to increase the misclassification rate of a model with a comparable scale of perturbation , and Gumbel attack , a method to improve the efficiency of generating adversarial examples , ( It just happens to be fashionable : ) ) . It is natural to ask how the simplest algorithm , random perturbation , works before one is persuaded to read our paper . We compare our methods with random perturbation on the test set of the IMDB movie review dataset used in our paper . For each instance , we randomly sample k positions in the sentence , and replace them with randomly sampled words . The average consistency of the predictions of the model from the perturbed and the original instances is 99.9 % after k = 10 words are changed , and 92 % , 90.4 % after k = 50 , 100 words are changed respectively . See the following link for a plot of comparison : https : //drive.google.com/file/d/1T6UJQPz4iDFqsK9XQZ0nYv-bBcYxWraP/view ? usp=sharing . We conclude that random perturbation does not work . [ 1 ] Carlini , Nicholas , and David Wagner . `` Towards evaluating the robustness of neural networks . '' 2017 IEEE Symposium on Security and Privacy ( SP ) . IEEE , 2017 . [ 2 ] Agarwal , Chirag , et al . `` An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks . '' arXiv preprint arXiv:1806.01477 ( 2018 ) . [ 3 ] Aleksander Madry , Aleksandar Makelov , Ludwig Schmidt , Dimitris Tsipras , and Adrian Vladu . Towards deep learning models resistant to adversarial attacks . ICLR ( 2018 ) . [ 4 ] Alex Kurakin , Ian Goodfellow , Samy Bengio . Adversarial machine learning at scale . ICLR 2017 . [ 5 ] Ian J Goodfellow , Jonathon Shlens , and Christian Szegedy . Explaining and harnessing adversarial examples . ICLR , 2015 . [ 6 ] Moosavi-Dezfooli , Seyed-Mohsen , Alhussein Fawzi , and Pascal Frossard . `` Deepfool : a simple and accurate method to fool deep neural networks . '' CVPR , 2016 . [ 7 ] Ji Gao , Jack Lanchantin , Mary Lou Soffa , and Yanjun Qi . Black-box generation of adversarial text sequences to evade deep learning classifiers . IEEE Security and Privacy Workshops ( SPW ) , 2018 . [ 8 ] Robin Jia and Percy Liang . Adversarial examples for evaluating reading comprehension systems . In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pp . 2021\u20132031 , 2017 . [ 9 ] Bin Liang , Hongcheng Li , Miaoqiang Su , Pan Bian , Xirong Li , and Wenchang Shi . IJCAI , 2018 . [ 10 ] Nicolas Papernot , Patrick McDaniel , Ananthram Swami , and Richard Harang . Crafting adversarial input sequences for recurrent neural networks . In Military Communications Conference , MILCOM 2016-2016 IEEE , 2016 . [ 11 ] Suranjana Samanta and Sameep Mehta . Towards crafting text adversarial samples . arXiv preprint arXiv:1707.02812 , 2017 . [ 12 ] Minhao Cheng , Jinfeng Yi , Huan Zhang , Pin-Yu Chen , and Cho-Jui Hsieh . Seq2sick : Evaluating the robustness of sequence-to-sequence models with adversarial examples . arXiv preprint arXiv:1803.01128 , 2018 . [ 13 ] Javid Ebrahimi , Anyi Rao , Daniel Lowd , Dejing Dou . Hotflip : White-box adversarial examples for text classification . ACL , 2018 . [ 14 ] Jiwei Li , Will Monroe , Dan Jurafsky . Understanding neural networks through representation erasure . arXiv preprint arXiv:1612.08220 , 2016 ."}, {"review_id": "ByghKiC5YX-2", "review_text": "In this work the authors introduce two new state-of-the-art adversarial attacks on discrete data based on a two-stage probabilistic process: the first step identifies key features which are then replaced in the second step through choices from a dictionary. Overall the manuscript is very well written and easy to follow. The evaluation is extensive and contains all previous attacks I am aware of. The greedy attack outperforms all prior work by a large margin while the Gumbel attack works on par with the previous state-of-the-art while being significantly faster. I only have a few questions and remarks: * What\u2019s the \u201crandom attack\u201d baseline in these tasks? In computer vision it\u2019s often sufficient to add a little bit of salt-and-pepper noise or Gaussian noise to change the model decision. * Another thing I am wondering is what the human evaluation scores would be on adversarials from other adversarial attacks? Adversarial attacks in general (e.g. in computer vision) can work in two ways: one being actually changing the semantic content (thus also \u201cfooling humans) while the other changes background features / add noise to which humans are pretty insensitive (unless you add too much of it). The greedy attack does seem to change some semantics as can be seen in the increased error rate of humans (which is pretty rare for computer vision adversarials). It might be that other attacks are rather changing words or characters which are not as semantically meaningful, as would be revealed by the accompanying human scores. * Are you planning to release the code? Will it be part of CleverHans or Foolbox? Overall, I find this work to be a really exciting advance on discrete adversarial attacks.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the encouraging comments and the help in addressing the importance of the task ! What \u2019 s the \u201c random attack \u201d baseline in these tasks ? In computer vision it \u2019 s often sufficient to add a little bit of salt-and-pepper noise or Gaussian noise to change the model decision . We define \u201c random attack \u201d as randomly sample k positions in the sentence , and replace them with randomly sampled words . We run random perturbation on the test set of the IMDB movie review dataset used in our paper . The average consistency of the predictions of the model from the perturbed and the original instances is 99.9 % after k = 10 words are changed , and 92 % , 90.4 % after k = 50 , 100 words are changed respectively . See the following link for a plot of comparison with our algorithms ( on the first five words ) : https : //drive.google.com/file/d/1T6UJQPz4iDFqsK9XQZ0nYv-bBcYxWraP/view ? usp=sharing . We conclude that random perturbation does not work . \u201c What the human evaluation scores would be on adversarials from other adversarial attacks ? \u201d We have added another experiment to compare various algorithms with human evaluation on the IMDB movie review data set . On each instance , we increase the number of words to be perturbed until the prediction of the model changes . Then we ask humans to label original texts and perturbed texts . Greedy attack yields the best performance in the experiment . Please see Appendix B of the updated version for details . \u201c Are you planning to release the code ? Will it be part of CleverHans or Foolbox ? \u201d Yes , we plan to release the code . We will either release the code in a stand-alone github repository or merge it into CleverHans ."}, {"review_id": "ByghKiC5YX-3", "review_text": "The authors proposed a novel probabilistic framework to model adversarial attacks on deep networks with discrete inputs such as text. The proposed framework assumes a two step construction of an adversarial perturbation: 1) finding relevant features (or dimensions) to perturb (Eq. 3); 2) finding values to replace the features that are selected in step 1 (Eq. 4). The authors approximate some terms in these two equations to make the optimization easier. For example, it is *implicitly* assumed that given the i-th feature is removed from consideration, the probability of attack success does not change *on average* under probabilistic *adversarial* attack on other features (Eq. 5). It is not clear why that should hold and under what conditions that assumption would be reasonable (given that the attacks on other features are adversarial, although being probabilistic). The proposed framework allows one to solve the computation vs. success rate trade-off by either estimating the best attack from the network (called greedy attack Eq. 6) or using a parametric estimation that does not require model evaluation (called Gumbel attack). Experimental results suggest that Gumbel attack has better or competitive attack rate on models developed for text classification while having the most computationally efficiency among other methods. It is also noticeable that the greedy attack achieves the best success rate with a large margin among all the tested methods. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the detailed and encouraging comments ! To address the reviewer \u2019 s concern on Equation 5 , we have added a more rigorous and detailed explanation of the approximation . Roughly , when one assumes other features are perturbed adversarially , the Greedy Attack can be interpreted as maximizing a lower bound of the original objectives . Details can be found in Section 3.1 of the updated version ."}], "0": {"review_id": "ByghKiC5YX-0", "review_text": "This paper introduces two new methods for generating adversarial examples for text classification models. The paper is well written, the introduced algorithms and experiments are easy to understand. However, I do not believe that these two methods are sufficiently significant. First of all, I am not convinced that the attacks can be classified as \u201cadversarial examples\u201d, especially the ones on the word-based models. The community originally got interested in adversarial examples because while they can easily be classified correctly by humans, they seemed to fool machine learning models with high efficiency. For example, the PGD attack by Madry et al. can reduce the accuracy of a CIFAR-10 model to 0% by using distortions that are not at all noticeable to humans. In the case of the word-based task studied here, human accuracy drops by 8-11%. While the question of whether adversarial examples are actually a security threat is under debate, the attacks on the word-based models here do not even classify as adversarial examples. Of course, it is interesting that the ML models are much less robust to these distortions than humans are, however, this is a well known problem. This paper did not perform comprehensive experiments to investigate this phenomenon. For example, they could have evaluated a wide range of distortions (including random distortions), and then check if training with all of these distortions makes the network more robust \u2026 etc (for example, see [1]). The attacks on character-based models are closer to adversarial examples from this perspective. However, the performance of the Gumbel Attack is significantly worse on character-based models than an attack as simple as the Delete-1 attack. The Greedy attack is more successful than the Delete-1 attack, however it is a straight-forward application of greedy optimization on discrete data and is not very novel or interesting. [1] Generalisation in humans and deep neural networks, arXiv:1808.08750 ", "rating": "3: Clear rejection", "reply_text": "We thank the reviewer for reading our paper and giving detailed comments on our paper . However , we observe the review is posted after the deadline of paper modification has passed , and hope to address a few points of this review . In short , 1 . We think the comparison between adversarial attack on images and on texts is unfair . 2.Some of the questions in the review are highly correlated with AC \u2019 s questions and we have answered in our previous rebuttal . We now address them in details . First , we agree with the reviewer that adversarial attack on texts is at a relatively new stage compared to the counterpart on images . However , to evaluate our work , we think it makes more sense to compare our methods with the best methods in this area , instead of with methods on a different data set . As an example , it does not make sense to reject a paper because their model achieves a lower accuracy on ImageNet than the accuracy of a very simple model on MNIST . We have shown that our method outperforms previous text adversarial attack algorithms in Figure 3 , and we have even compared all methods under human evaluation in Appendix B , which indicates that humans are least sensitive to adversarial examples generated by our algorithm . So we believe our method can advance state-of-the-art in attacks on texts . \u201c The attacks on character-based models are closer to adversarial examples from this perspective. \u201d We aim to propose a general mathematical framework to generate adversarial examples for models with discrete input . Thus , the same algorithm works for both character-based and word-based models , and could be potentially useful for other NLP models such as word-piece models . We are happy to see that the reviewer consider character-based adversarial examples more interesting . \u201c The Greedy attack is a straightforward application of greedy optimization on discrete data and is not very novel or interesting. \u201d See our rebuttal to AC ( point 2 ) in \u201c Efficiency of Gumbel Attack ; Difference and Connections in Discrete Optimization and Adversarial Attack. \u201d We will also elaborate this in our final version . The reviewer also proposed to include an experiment on how our attacks perform on models trained with data augmentation techniques . We agree with the reviewer that the proposed experiment can be interesting . However , given the timeline , we are not able to update the paper now . We are willing to add it in our final version ."}, "1": {"review_id": "ByghKiC5YX-1", "review_text": "This paper addresses the problem of generating adversarial examples for discrete domains like text. They propose two simple techniques: 1) Greedy: two stage process- first stage involves finding the k words in the sentence/paragraph to perturb and second step changes the word in the positions identified in step 1. 2) Gumbel: first approach amortized over datasets where first and second steps are parametrized and learned over the dataset with the loss being the probability of flipping the decision. Specifically, for the Gumbel approach, the authors use the non-differentiable top-k-argmax output to train the module in the second step which is not ideal and it would be better to train both first and second steps jointly in an end-to-end differentiable manner. The results show that Greedy approach is able to significantly affect the accuracy of the systems compared to other adversarial baselines. Mturk evaluation shows that for tasks like sentiment analysis, humans weren't as confused as the systems were when the selected words were changed which is encouraging. However, the Gumbel method performs poorly compared to other baselines. Moreover, a thorough analysis of why Greedy is doing better than some gradient based adversarial attacks is needed in the paper because it is unclear what is causing their greedy approach to perform well; is it the two-stage nature of the process? My major gripe with the paper is that it is egregiously difficult to read in parts and is poorly written. There are dangling conditional bars in many equations (5, 7, Greedy attack etc.), unclear \"expectation (E)\" signs and many other confusing notational choices which make the math difficult to parse. I am not even sure if those equations are correctly conveying the idea they are meant to convey. I found the algorithms to be more clearly written and realize that the text in the models and equations is unnecessarily complicated. The argument about approximation to the objective by considering the i positions independently is not convincing and their is nothing in the paper to show if the assumption is reasonable.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear Area Chair and Anonymous Reader : Thanks for your questions on the motivation of adversarial attack for discrete data . Below we briefly explain the motivation , followed by the evidence that simple random perturbation does not work . In summary , the area chair and another reader posed the following questions\uff1a 1 . Why does one need to study the phenomenon of adversarial examples on discrete data ? 2.Why is this paper worth reading ? 3.Do simple methods like random perturbation work on text data ? In short , our reply is 1 . Robustness is an important criterion for models on discrete data . The generation of adversarial examples can be used to evaluate robustness or even improve robustness . 2.In this paper , our goal is to propose methods with better performance ( Greedy attack ) or with higher efficiency ( Gumbel attack ) . 3.We provide evidence that simple methods like random perturbation do not work . Below are concrete details : Robustness is an important criterion for the application of machine learning models in critical areas such as medicine , financial markets , recommendation systems , and criminal justice . Adversarial examples have been used to evaluate the ( adversarial ) robustness of models ( e.g. , [ 1 , 2 , 5 ] ) and have also been applied to train robust models ( e.g. , [ 3 , 4 ] ) . The phenomenon of adversarial examples was first found in state-of-the-art deep neural network models for classifying images ( e.g. , [ 5 , 6 , 2 ] ) , where small perturbations unobservable by human can easily fool neural networks . Similar to image data , the problem of adversarial perturbation on discrete data can be defined as altering the prediction of a model via minimal perturbation to an original sample ( e.g. , [ 7-14 ] ) . While there have been many pioneered and interesting papers in this area ( e.g. , [ 7-14 ] ) , we proposed Greedy attack , a method to increase the misclassification rate of a model with a comparable scale of perturbation , and Gumbel attack , a method to improve the efficiency of generating adversarial examples , ( It just happens to be fashionable : ) ) . It is natural to ask how the simplest algorithm , random perturbation , works before one is persuaded to read our paper . We compare our methods with random perturbation on the test set of the IMDB movie review dataset used in our paper . For each instance , we randomly sample k positions in the sentence , and replace them with randomly sampled words . The average consistency of the predictions of the model from the perturbed and the original instances is 99.9 % after k = 10 words are changed , and 92 % , 90.4 % after k = 50 , 100 words are changed respectively . See the following link for a plot of comparison : https : //drive.google.com/file/d/1T6UJQPz4iDFqsK9XQZ0nYv-bBcYxWraP/view ? usp=sharing . We conclude that random perturbation does not work . [ 1 ] Carlini , Nicholas , and David Wagner . `` Towards evaluating the robustness of neural networks . '' 2017 IEEE Symposium on Security and Privacy ( SP ) . IEEE , 2017 . [ 2 ] Agarwal , Chirag , et al . `` An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks . '' arXiv preprint arXiv:1806.01477 ( 2018 ) . [ 3 ] Aleksander Madry , Aleksandar Makelov , Ludwig Schmidt , Dimitris Tsipras , and Adrian Vladu . Towards deep learning models resistant to adversarial attacks . ICLR ( 2018 ) . [ 4 ] Alex Kurakin , Ian Goodfellow , Samy Bengio . Adversarial machine learning at scale . ICLR 2017 . [ 5 ] Ian J Goodfellow , Jonathon Shlens , and Christian Szegedy . Explaining and harnessing adversarial examples . ICLR , 2015 . [ 6 ] Moosavi-Dezfooli , Seyed-Mohsen , Alhussein Fawzi , and Pascal Frossard . `` Deepfool : a simple and accurate method to fool deep neural networks . '' CVPR , 2016 . [ 7 ] Ji Gao , Jack Lanchantin , Mary Lou Soffa , and Yanjun Qi . Black-box generation of adversarial text sequences to evade deep learning classifiers . IEEE Security and Privacy Workshops ( SPW ) , 2018 . [ 8 ] Robin Jia and Percy Liang . Adversarial examples for evaluating reading comprehension systems . In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pp . 2021\u20132031 , 2017 . [ 9 ] Bin Liang , Hongcheng Li , Miaoqiang Su , Pan Bian , Xirong Li , and Wenchang Shi . IJCAI , 2018 . [ 10 ] Nicolas Papernot , Patrick McDaniel , Ananthram Swami , and Richard Harang . Crafting adversarial input sequences for recurrent neural networks . In Military Communications Conference , MILCOM 2016-2016 IEEE , 2016 . [ 11 ] Suranjana Samanta and Sameep Mehta . Towards crafting text adversarial samples . arXiv preprint arXiv:1707.02812 , 2017 . [ 12 ] Minhao Cheng , Jinfeng Yi , Huan Zhang , Pin-Yu Chen , and Cho-Jui Hsieh . Seq2sick : Evaluating the robustness of sequence-to-sequence models with adversarial examples . arXiv preprint arXiv:1803.01128 , 2018 . [ 13 ] Javid Ebrahimi , Anyi Rao , Daniel Lowd , Dejing Dou . Hotflip : White-box adversarial examples for text classification . ACL , 2018 . [ 14 ] Jiwei Li , Will Monroe , Dan Jurafsky . Understanding neural networks through representation erasure . arXiv preprint arXiv:1612.08220 , 2016 ."}, "2": {"review_id": "ByghKiC5YX-2", "review_text": "In this work the authors introduce two new state-of-the-art adversarial attacks on discrete data based on a two-stage probabilistic process: the first step identifies key features which are then replaced in the second step through choices from a dictionary. Overall the manuscript is very well written and easy to follow. The evaluation is extensive and contains all previous attacks I am aware of. The greedy attack outperforms all prior work by a large margin while the Gumbel attack works on par with the previous state-of-the-art while being significantly faster. I only have a few questions and remarks: * What\u2019s the \u201crandom attack\u201d baseline in these tasks? In computer vision it\u2019s often sufficient to add a little bit of salt-and-pepper noise or Gaussian noise to change the model decision. * Another thing I am wondering is what the human evaluation scores would be on adversarials from other adversarial attacks? Adversarial attacks in general (e.g. in computer vision) can work in two ways: one being actually changing the semantic content (thus also \u201cfooling humans) while the other changes background features / add noise to which humans are pretty insensitive (unless you add too much of it). The greedy attack does seem to change some semantics as can be seen in the increased error rate of humans (which is pretty rare for computer vision adversarials). It might be that other attacks are rather changing words or characters which are not as semantically meaningful, as would be revealed by the accompanying human scores. * Are you planning to release the code? Will it be part of CleverHans or Foolbox? Overall, I find this work to be a really exciting advance on discrete adversarial attacks.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for the encouraging comments and the help in addressing the importance of the task ! What \u2019 s the \u201c random attack \u201d baseline in these tasks ? In computer vision it \u2019 s often sufficient to add a little bit of salt-and-pepper noise or Gaussian noise to change the model decision . We define \u201c random attack \u201d as randomly sample k positions in the sentence , and replace them with randomly sampled words . We run random perturbation on the test set of the IMDB movie review dataset used in our paper . The average consistency of the predictions of the model from the perturbed and the original instances is 99.9 % after k = 10 words are changed , and 92 % , 90.4 % after k = 50 , 100 words are changed respectively . See the following link for a plot of comparison with our algorithms ( on the first five words ) : https : //drive.google.com/file/d/1T6UJQPz4iDFqsK9XQZ0nYv-bBcYxWraP/view ? usp=sharing . We conclude that random perturbation does not work . \u201c What the human evaluation scores would be on adversarials from other adversarial attacks ? \u201d We have added another experiment to compare various algorithms with human evaluation on the IMDB movie review data set . On each instance , we increase the number of words to be perturbed until the prediction of the model changes . Then we ask humans to label original texts and perturbed texts . Greedy attack yields the best performance in the experiment . Please see Appendix B of the updated version for details . \u201c Are you planning to release the code ? Will it be part of CleverHans or Foolbox ? \u201d Yes , we plan to release the code . We will either release the code in a stand-alone github repository or merge it into CleverHans ."}, "3": {"review_id": "ByghKiC5YX-3", "review_text": "The authors proposed a novel probabilistic framework to model adversarial attacks on deep networks with discrete inputs such as text. The proposed framework assumes a two step construction of an adversarial perturbation: 1) finding relevant features (or dimensions) to perturb (Eq. 3); 2) finding values to replace the features that are selected in step 1 (Eq. 4). The authors approximate some terms in these two equations to make the optimization easier. For example, it is *implicitly* assumed that given the i-th feature is removed from consideration, the probability of attack success does not change *on average* under probabilistic *adversarial* attack on other features (Eq. 5). It is not clear why that should hold and under what conditions that assumption would be reasonable (given that the attacks on other features are adversarial, although being probabilistic). The proposed framework allows one to solve the computation vs. success rate trade-off by either estimating the best attack from the network (called greedy attack Eq. 6) or using a parametric estimation that does not require model evaluation (called Gumbel attack). Experimental results suggest that Gumbel attack has better or competitive attack rate on models developed for text classification while having the most computationally efficiency among other methods. It is also noticeable that the greedy attack achieves the best success rate with a large margin among all the tested methods. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the detailed and encouraging comments ! To address the reviewer \u2019 s concern on Equation 5 , we have added a more rigorous and detailed explanation of the approximation . Roughly , when one assumes other features are perturbed adversarially , the Greedy Attack can be interpreted as maximizing a lower bound of the original objectives . Details can be found in Section 3.1 of the updated version ."}}