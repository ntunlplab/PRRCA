{"year": "2020", "forum": "Bke8764twr", "title": "Bias-Resilient Neural Network", "decision": "Reject", "meta_review": "The paper addressed the problem of machine bias when training machine learning models. The authors propose an approach based on representation learning with adversarial training. As opposed to the majority of previous works that trying to create a representation from which it is not possible to predict the sensitive feature (bias), the authors propose to minimize the dependency between the learned features and the sensitive feature with adversarial training. While acknowledging that the proposed model is addressing an important problem and is potentially useful, the reviewers and AC note the following potential weaknesses:  \n(1) limited technical contribution -- the proposed approach is similar to a number of works published in machine learning and computer vision before the submission deadline that were overlooked by the authors. Specifically: i) adversarial training for learning fair representations [Edwards and Storkey, Censoring Representations with an Adversary, ICLR 2016], [Beutel, et al 2017, Data decisions and theoretical implications when adversarially learning fair representations], ii) learning fair representation by minimizing the dependency between the latent representation and the sensitive attributes [The variational fair autoencoder, ICLR 2016 by Louizos et al.; Fairness Constraints: Mechanisms for Fair Classification, by Zafar et al, 2015] or by minimizing the mutual information between feature embedding and bias [Learning Not to Learn: Training Deep Neural Networks with Biased Data, CVPR 2019]. \n(2) Limited empirical evidence -- the baseline methods used in the evaluation are not sufficient to assess the benefits of the proposed approach over the existing SOTA methods mentioned above. In fact, none of the baseline methods used in the evaluation tackle machine bias (via adversarial training or minimizing statistical dependence). \n(3) It would be beneficial to also report fairness metrics, e.g. equality of opportunity, statistical parity, to assess the effectiveness of bias removal. R1 has raised some concerns regarding empirical evidence -- see the point about mixed results. Also R2 has reported concerns regarding controversial results in experiment 4.2 and suggested ways to justify when and why the results of the CNNs baseline are close to the BR-Net. Addressing these concerns would strengthen the contributions of the proposed method.    \n\nAmong these, (3) did not have a decisive impact on the decision, but would be helpful to address in a subsequent revision. However, (1) and (2) make it very difficult to assess the benefits of the proposed approach, and were viewed by AC as critical issues. AC suggests, in its current state the manuscript is not ready for a publication. We hope the reviews are useful for improving and revising the paper.\n", "reviews": [{"review_id": "Bke8764twr-0", "review_text": "This paper presents Bias-Resilient neural network (BR-Net) that is designed to learn representations that can accurately predict the desired target while being invariant to the confounding covariates in the data. The proposed method is based on domain adversarial training strategies, especially that of (Ganin et al., 2016), where the adversarial component is modified from \u201closs of distinguishing between the source and target domains\u201d to \u201cthe squared Pearson correlation between the ground truth bias covariate and its estimation from the learned representation\u201d. This design is based on the argument that the ultimate goal of bias control here is removing statistical association with respect to the bias variables, as opposed to maximizing the prediction error of them. Things to improve the paper that did not impact the score: - Introduction, line 4: Wrong citation format: use of \\citet instead of \\citep. Correct for all citations throughout the paper. References: - Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., ... & Lempitsky, V. (2016). Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1), 2096-2030. ", "rating": "8: Accept", "reply_text": "> > > Comment : This paper presents Bias-Resilient neural network ( BR-Net ) that is designed to learn representations that can accurately predict the desired target while being invariant to the confounding covariates in the data . The proposed method is based on domain adversarial training strategies , especially that of ( Ganin et al. , 2016 ) , where the adversarial component is modified from \u201c loss of distinguishing between the source and target domains \u201d to \u201c the squared Pearson correlation between the ground truth bias covariate and its estimation from the learned representation \u201d . This design is based on the argument that the ultimate goal of bias control here is removing statistical association with respect to the bias variables , as opposed to maximizing the prediction error of them . References : - Ganin , Y. , Ustinova , E. , Ajakan , H. , Germain , P. , Larochelle , H. , Laviolette , F. , ... & Lempitsky , V. ( 2016 ) . Domain-adversarial training of neural networks . The Journal of Machine Learning Research , 17 ( 1 ) , 2096-2030 . > Response : We appreciate the positive comments from this reviewer . We agree that this is a very important problem that needs to be addressed for deep learning models that has been neglected for long , especially for medical applications . \u2014\u2014\u2014\u2014- > > > Comment : Things to improve the paper that did not impact the score : - Introduction , line 4 : Wrong citation format : use of \\citet instead of \\citep . Correct for all citations throughout the paper . > Response : Thanks for pointing this out . We have fixed the citation formatting ."}, {"review_id": "Bke8764twr-1", "review_text": "The authors propose a method based on GAN to classify data while automatically removing confounding effects during training, in order to obtain a classifier whose features are not biased by any confounding effect. The proposed idea is based on the extension of classical classification architectures to account for bias prediction. In practice, the parameters of the feature extraction component are updated to solve both bias prediction and the desired classification problem in adversarial fashion. Pearson\u2019s correlation was chosen as a metric for bias. The paper is interesting and addresses an important problem for the application of machine learning methods in several real context. The feeling is however that the paper should have better explored the implication of the proposed model of bias, and better investigated the relationship with simpler approaches relying on similar hypothesis. Here are my main comments for this work: - Why Pearson\u2019s correlation should be a reliable metric to quantify bias? This metric is insensitive to affine scaling of the data, which is a quite common form of bias (for example in medical images). - The authors should have investigated the relationship between the proposed method and bias removal through canonical correlation analysis (CCA), and perhaps its non-linear variants. At the end this is what their network is doing, although in an end-to-end fashion. Using the CCA projections in the latent space for classification would be the closest approach to the state of the art for bias removal in statistical analysis (residual analysis). - The experimental setting illustrated in 4.1 is not clear. In which sense \\sigma_A is a common factor for the two groups? Why the theoretical maximum classification accuracy is 90%? Figure 2 is not clear either and doesn\u2019t help understanding the structure of the generated data (e.g. axis labels missing, colorbars units not specified). - It is not clear why authors quantify the correlation in the latent space with tSNE projections. tSNE is highly sensitive to the choice of parameters and it would be important to ensure that it was a \u201cfair competition\u201d between all the methods, when showing the results of the dimensionality reduction. This is another modelling step relying on specific assumptions which decreases interpretability of the findings. The authors also proposed to assess the decorrelation of the estimated features throughout the different methods by measuring the squared distance correlation. Naturally, their method is the one which exhibits the best performances using this metric. However, this way of assessing the decorrelation of the features with the biases is unfair to the other methods, as in their case they specifically built their model to avoid statistical correlation between features and biases. - Experiment 4.2 has some controversial aspects, as the bias correction is performed on the control population only, while the model is trained on the entire population. I understand the fact that confounding can be estimated only on healthy conditions, however in this case the network is going to be biased by the control group by construction. The effect of such a choice in the end-to-end optimisation scheme is really not clear. - We also observe that the results of the baseline CNN are very close to the BR-Net. The main difference lies in the fact that the CNN tends to have an unbalanced classification between true negatives and true positives. However, what would happen if we corrected for age before applying the CNN ? - In the case where the performances of the CNN would be improved, this last question would raise another one. Indeed, if I already know the confounding effects I want to correct for, why wouldn\u2019t I correct them beforehand in order to avoid to train a complex GAN, which leads to more instability during training. This aspect points to the limit of having an online bias-correction (at least for the medical data case). ", "rating": "1: Reject", "reply_text": "We thank the reviewer for the constructive and positive comments . Please see our detailed response to each of the points below . \u2014\u2014\u2014\u2014- > > > Comment : Here are my main comments for this work : - Why Pearson \u2019 s correlation should be a reliable metric to quantify bias ? This metric is insensitive to affine scaling of the data , which is a quite common form of bias ( for example in medical images ) . > Response : In the revised manuscript , we added a rigorous proof of the statistical property of using Pearson \u2019 s correlation as the adversarial training objective ( Section 3.1 ) . We showed that the learned features and bias would achieve \u2018 mean independence \u2019 , a very strong type of statistical independence , thanks to the adversarial network being able to remove non-linear association ( not just confined to affine ) between variables . \u2014\u2014\u2014\u2014- > > > Comment : - The authors should have investigated the relationship between the proposed method and bias removal through canonical correlation analysis ( CCA ) , and perhaps its non-linear variants . At the end this is what their network is doing , although in an end-to-end fashion . Using the CCA projections in the latent space for classification would be the closest approach to the state of the art for bias removal in statistical analysis ( residual analysis ) . > Response : We have not figured out which specific state-of-the-art CCA bias-removal work the reviewer is referring to . In this work , we aim to equip the end-to-end convolution training with a rigorous way of removing bias/confounder . That was the main reason we did not consider applying traditional methods ( linear or non-linear ) to raw images , which are the inputs to deep learning and convolutional methods . However , we have also compared with a widely-used traditional method in all medical studies ( Residualization ) in Table 1 . Our method outperforms it by a large margin . Note that the adversarial component essentially maps the learned features to the 1D space of bias , from which the correlation is computed . This echoes the concept of CCA , only in a non-linear manner . Therefore , our method is very similar to what the reviewer mentioned above . Please also note that we can not directly use CCA in the context of convolutional networks unless we use its variations ( such as Andrew et al.ICML 2013 ) . In order to learn bias-invariant features we had to adversarially inject resilience into the convolutional part ( Feature Extract , FE , component ) in an end-to-end fashion . This can be a future research direction . We added this point in the Conclusion section , thanks to the reviewer . \u2014\u2014\u2014\u2014- > > > Comment : - The experimental setting illustrated in 4.1 is not clear . In which sense \\sigma_A is a common factor for the two groups ? Why the theoretical maximum classification accuracy is 90 % ? Figure 2 is not clear either and doesn \u2019 t help understanding the structure of the generated data ( e.g.axis labels missing , colorbars units not specified ) . > Response : We assume the difference in \\sigma_A ( U ( 1,4 ) and U ( 3,6 ) ) between the two groups represents true discriminative cues , and the difference in \\sigma_B represents a confounding effect . We have reworded this in the text accordingly to avoid confusion . Since \\sigma_A is the only discriminative cue in the dataset and is sampled from two overlapping intervals ( U ( 1,4 ) and U ( 3,6 ) ) for the two groups respectively , the theoretical maximum classification accuracy is 90 % , i.e. , 50 % accuracy when \\sigma_A is sampled from U ( 3,4 ) which is common between the two classes , and 100 % accuracy for the remaining intervals . We have also clarified this point in the revised manuscript . The axis-label and coordinates of 2D t-SNE do not have any meaning , as it only shows relative local neighborhoods among instances . This is a very common practice for visualizing high dimensional data in lower dimensions in which the relative neighborhood of the high dimensional data is kept . As for the colorbar label in Figure 2 , it was explained in the text that it is the value of \\sigma_b , but according to this reviewer \u2019 s comment , we have added this information in the caption as well ."}, {"review_id": "Bke8764twr-2", "review_text": "This paper proposes an adversarial approach toward debiasing neural network representations w.r.t protected attributes. The core idea is balance task loss with an adversarial loss from which protected attributes have low correlation with the feature representation used for the end task. The paper provides some synthetic experiments, and evaluates on HIV data (the bias variable being age) and gender classification in images, stratified by skin shade (the bias variable being skin shade). Results demonstrate improved balanced accuracy across all three experiments. Overall the direction is interesting and the methodology is intuitive and sound. Unfortunately, this paper seems unaware of extremely related works: 1. (AAAI 2018) http://www.m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf 2. (ACL 2018) https://arxiv.org/abs/1808.06640 3. (ICCV 2019) https://arxiv.org/abs/1811.08489 4. (ICCV 2019) http://hal.cse.msu.edu/assets/pdfs/papers/2019-iccv-kernel-adversarial-representation-learning.pdf Beyond experiments on different datasets, and slight modification of the adversarial loss for correlation, I am unsure what this paper contributes beyond these works. While some some of these papers can potentially be considered contemporary, authors must at least address these issues. Furthermore, modification of the objective seems to have mixed results (Table 2 CatGAN vs. BR-NET, although I'm not sure whats going on with vgg vs resnet), where the baseline would correspond more closely to the setup in https://arxiv.org/abs/1811.08489 (3. above) . Overall I am positive about the direction, but I am unsure this paper represents a significant contribution over existing work. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the constructive and positive comments . Please see our detailed response to each of the points below . \u2014\u2014\u2014\u2014- > > > Comment : Overall the direction is interesting and the methodology is intuitive and sound . Unfortunately , this paper seems unaware of extremely related works : 1 . ( AAAI 2018 ) http : //www.m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf 2 . ( ACL 2018 ) https : //arxiv.org/abs/1808.06640 3 . ( ICCV 2019 ) https : //arxiv.org/abs/1811.08489 4 . ( ICCV 2019 ) http : //hal.cse.msu.edu/assets/pdfs/papers/2019-iccv-kernel-adversarial-representation-learning.pdf > Response : Thank you for pointing us to these prior works . We have added them in literature review accordingly and have discussed their differences to our work . Please note that we have already cited some of these articles ( e.g. , Xieet al.NeurIPS 2017 and Akuzawa et al.EMCL 2019 ) . As we were already aware of several of these works , we regarded our work quite distinct from existing state-of-the-art in the sense that our work could handle continuous bias/confounder variables as opposed to many works focusing only on binary/categorical variables . Please also consider that two of the above works ( 3 and 4 ) are ICCV 2019 papers , which were published online on arxiv and CVF OpenAccess in October 2019 , which was after the deadline of ICLR submission . As opposed to other works , we also explicitly modeled the statistical dependency between features and bias . To make clear the difference from prior arts , please refer to Section 3.1 in the updated manuscript describing the statistical property of our method . We theoretically showed that modifying the adversarial loss as the \u2018 Pearson correlation \u2019 actually guarantees an important statistical property of the model : the learned features and bias would achieve \u2018 mean independence \u2019 , a much stronger type of statistical independence than linear independence ( Pearson correlation=0 ) . Such property can only be achieved by our strategy of adversarially optimizing the Pearson-correlation of predicted and true bias . This distinguishes our work from several others , in which adversarial object is solely defined w.r.t.maximizing bias prediction loss ( only aims at predicting the exact value of the loss neglecting proper statistical ( in ) dependence ) . \u2014\u2014\u2014\u2014- > > > Comment : Beyond experiments on different datasets , and slight modification of the adversarial loss for correlation , I am unsure what this paper contributes beyond these works . While some of these papers can potentially be considered contemporary , authors must at least address these issues . Furthermore , modification of the objective seems to have mixed results ( Table 2 CatGAN vs. BR-NET , although I 'm not sure whats going on with vgg vs resnet ) , where the baseline would correspond more closely to the setup in https : //arxiv.org/abs/1811.08489 ( 3. above ) . > Response : As for the novelties of the paper , please see the above text . Our experiments show that strategies taken by other methods do not work for continuous and ordinal variables . These types of bias/confounder variables are very common in real-world computer vision and medical applications . CatGan ( the same idea of the baseline suggested by the reviewer ) also worked reasonably well with VGG backbone but was only applicable when bias took discrete categorical values . Even in that case , the results are BR-Net are significantly superior for some shade categories ( shade 6 , see Fig.7 ) .Please note that in the HIV experiment CatGan is not applicable as the bias variable ( age ) is a continuous variable . \u2014\u2014\u2014\u2014- > > > Comment : Overall I am positive about the direction , but I am unsure this paper represents a significant contribution over existing work . > Response : Thank you very much for the positive and constructive feedback . Please also refer to the repnoses above . In summary , we would like to again mention that our proposed method is not just proposing a new loss function . We explicitly modeled the statistical dependency between features and bias . We added theoretical properties of the proposed method in the updated manuscript ( Section 3.1 ) . We theoretically showed that using \u2018 Pearson correlation \u2019 as the adversarial loss actually guarantees that the learned features and bias would achieve \u2018 mean independence. \u2019 Such property can only be achieved by our strategy of adversarially optimizing the Pearson-correlation of predicted and true bias . This distinguishes our work from the previous work . The experimental evaluations are also designed to show these claims and that is why we selected [ BR-Net with categorical adversarial loss ( CatGAN ) ] , [ BR-Net with MSE adversarial loss ] , and [ Multi-Task ] and direct baselines for our method . This way , we have directly implemented and compared with all previous strategies ( including 1 , 2 , 3 , and 4 referred to by the reviewer above ) ."}], "0": {"review_id": "Bke8764twr-0", "review_text": "This paper presents Bias-Resilient neural network (BR-Net) that is designed to learn representations that can accurately predict the desired target while being invariant to the confounding covariates in the data. The proposed method is based on domain adversarial training strategies, especially that of (Ganin et al., 2016), where the adversarial component is modified from \u201closs of distinguishing between the source and target domains\u201d to \u201cthe squared Pearson correlation between the ground truth bias covariate and its estimation from the learned representation\u201d. This design is based on the argument that the ultimate goal of bias control here is removing statistical association with respect to the bias variables, as opposed to maximizing the prediction error of them. Things to improve the paper that did not impact the score: - Introduction, line 4: Wrong citation format: use of \\citet instead of \\citep. Correct for all citations throughout the paper. References: - Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., ... & Lempitsky, V. (2016). Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1), 2096-2030. ", "rating": "8: Accept", "reply_text": "> > > Comment : This paper presents Bias-Resilient neural network ( BR-Net ) that is designed to learn representations that can accurately predict the desired target while being invariant to the confounding covariates in the data . The proposed method is based on domain adversarial training strategies , especially that of ( Ganin et al. , 2016 ) , where the adversarial component is modified from \u201c loss of distinguishing between the source and target domains \u201d to \u201c the squared Pearson correlation between the ground truth bias covariate and its estimation from the learned representation \u201d . This design is based on the argument that the ultimate goal of bias control here is removing statistical association with respect to the bias variables , as opposed to maximizing the prediction error of them . References : - Ganin , Y. , Ustinova , E. , Ajakan , H. , Germain , P. , Larochelle , H. , Laviolette , F. , ... & Lempitsky , V. ( 2016 ) . Domain-adversarial training of neural networks . The Journal of Machine Learning Research , 17 ( 1 ) , 2096-2030 . > Response : We appreciate the positive comments from this reviewer . We agree that this is a very important problem that needs to be addressed for deep learning models that has been neglected for long , especially for medical applications . \u2014\u2014\u2014\u2014- > > > Comment : Things to improve the paper that did not impact the score : - Introduction , line 4 : Wrong citation format : use of \\citet instead of \\citep . Correct for all citations throughout the paper . > Response : Thanks for pointing this out . We have fixed the citation formatting ."}, "1": {"review_id": "Bke8764twr-1", "review_text": "The authors propose a method based on GAN to classify data while automatically removing confounding effects during training, in order to obtain a classifier whose features are not biased by any confounding effect. The proposed idea is based on the extension of classical classification architectures to account for bias prediction. In practice, the parameters of the feature extraction component are updated to solve both bias prediction and the desired classification problem in adversarial fashion. Pearson\u2019s correlation was chosen as a metric for bias. The paper is interesting and addresses an important problem for the application of machine learning methods in several real context. The feeling is however that the paper should have better explored the implication of the proposed model of bias, and better investigated the relationship with simpler approaches relying on similar hypothesis. Here are my main comments for this work: - Why Pearson\u2019s correlation should be a reliable metric to quantify bias? This metric is insensitive to affine scaling of the data, which is a quite common form of bias (for example in medical images). - The authors should have investigated the relationship between the proposed method and bias removal through canonical correlation analysis (CCA), and perhaps its non-linear variants. At the end this is what their network is doing, although in an end-to-end fashion. Using the CCA projections in the latent space for classification would be the closest approach to the state of the art for bias removal in statistical analysis (residual analysis). - The experimental setting illustrated in 4.1 is not clear. In which sense \\sigma_A is a common factor for the two groups? Why the theoretical maximum classification accuracy is 90%? Figure 2 is not clear either and doesn\u2019t help understanding the structure of the generated data (e.g. axis labels missing, colorbars units not specified). - It is not clear why authors quantify the correlation in the latent space with tSNE projections. tSNE is highly sensitive to the choice of parameters and it would be important to ensure that it was a \u201cfair competition\u201d between all the methods, when showing the results of the dimensionality reduction. This is another modelling step relying on specific assumptions which decreases interpretability of the findings. The authors also proposed to assess the decorrelation of the estimated features throughout the different methods by measuring the squared distance correlation. Naturally, their method is the one which exhibits the best performances using this metric. However, this way of assessing the decorrelation of the features with the biases is unfair to the other methods, as in their case they specifically built their model to avoid statistical correlation between features and biases. - Experiment 4.2 has some controversial aspects, as the bias correction is performed on the control population only, while the model is trained on the entire population. I understand the fact that confounding can be estimated only on healthy conditions, however in this case the network is going to be biased by the control group by construction. The effect of such a choice in the end-to-end optimisation scheme is really not clear. - We also observe that the results of the baseline CNN are very close to the BR-Net. The main difference lies in the fact that the CNN tends to have an unbalanced classification between true negatives and true positives. However, what would happen if we corrected for age before applying the CNN ? - In the case where the performances of the CNN would be improved, this last question would raise another one. Indeed, if I already know the confounding effects I want to correct for, why wouldn\u2019t I correct them beforehand in order to avoid to train a complex GAN, which leads to more instability during training. This aspect points to the limit of having an online bias-correction (at least for the medical data case). ", "rating": "1: Reject", "reply_text": "We thank the reviewer for the constructive and positive comments . Please see our detailed response to each of the points below . \u2014\u2014\u2014\u2014- > > > Comment : Here are my main comments for this work : - Why Pearson \u2019 s correlation should be a reliable metric to quantify bias ? This metric is insensitive to affine scaling of the data , which is a quite common form of bias ( for example in medical images ) . > Response : In the revised manuscript , we added a rigorous proof of the statistical property of using Pearson \u2019 s correlation as the adversarial training objective ( Section 3.1 ) . We showed that the learned features and bias would achieve \u2018 mean independence \u2019 , a very strong type of statistical independence , thanks to the adversarial network being able to remove non-linear association ( not just confined to affine ) between variables . \u2014\u2014\u2014\u2014- > > > Comment : - The authors should have investigated the relationship between the proposed method and bias removal through canonical correlation analysis ( CCA ) , and perhaps its non-linear variants . At the end this is what their network is doing , although in an end-to-end fashion . Using the CCA projections in the latent space for classification would be the closest approach to the state of the art for bias removal in statistical analysis ( residual analysis ) . > Response : We have not figured out which specific state-of-the-art CCA bias-removal work the reviewer is referring to . In this work , we aim to equip the end-to-end convolution training with a rigorous way of removing bias/confounder . That was the main reason we did not consider applying traditional methods ( linear or non-linear ) to raw images , which are the inputs to deep learning and convolutional methods . However , we have also compared with a widely-used traditional method in all medical studies ( Residualization ) in Table 1 . Our method outperforms it by a large margin . Note that the adversarial component essentially maps the learned features to the 1D space of bias , from which the correlation is computed . This echoes the concept of CCA , only in a non-linear manner . Therefore , our method is very similar to what the reviewer mentioned above . Please also note that we can not directly use CCA in the context of convolutional networks unless we use its variations ( such as Andrew et al.ICML 2013 ) . In order to learn bias-invariant features we had to adversarially inject resilience into the convolutional part ( Feature Extract , FE , component ) in an end-to-end fashion . This can be a future research direction . We added this point in the Conclusion section , thanks to the reviewer . \u2014\u2014\u2014\u2014- > > > Comment : - The experimental setting illustrated in 4.1 is not clear . In which sense \\sigma_A is a common factor for the two groups ? Why the theoretical maximum classification accuracy is 90 % ? Figure 2 is not clear either and doesn \u2019 t help understanding the structure of the generated data ( e.g.axis labels missing , colorbars units not specified ) . > Response : We assume the difference in \\sigma_A ( U ( 1,4 ) and U ( 3,6 ) ) between the two groups represents true discriminative cues , and the difference in \\sigma_B represents a confounding effect . We have reworded this in the text accordingly to avoid confusion . Since \\sigma_A is the only discriminative cue in the dataset and is sampled from two overlapping intervals ( U ( 1,4 ) and U ( 3,6 ) ) for the two groups respectively , the theoretical maximum classification accuracy is 90 % , i.e. , 50 % accuracy when \\sigma_A is sampled from U ( 3,4 ) which is common between the two classes , and 100 % accuracy for the remaining intervals . We have also clarified this point in the revised manuscript . The axis-label and coordinates of 2D t-SNE do not have any meaning , as it only shows relative local neighborhoods among instances . This is a very common practice for visualizing high dimensional data in lower dimensions in which the relative neighborhood of the high dimensional data is kept . As for the colorbar label in Figure 2 , it was explained in the text that it is the value of \\sigma_b , but according to this reviewer \u2019 s comment , we have added this information in the caption as well ."}, "2": {"review_id": "Bke8764twr-2", "review_text": "This paper proposes an adversarial approach toward debiasing neural network representations w.r.t protected attributes. The core idea is balance task loss with an adversarial loss from which protected attributes have low correlation with the feature representation used for the end task. The paper provides some synthetic experiments, and evaluates on HIV data (the bias variable being age) and gender classification in images, stratified by skin shade (the bias variable being skin shade). Results demonstrate improved balanced accuracy across all three experiments. Overall the direction is interesting and the methodology is intuitive and sound. Unfortunately, this paper seems unaware of extremely related works: 1. (AAAI 2018) http://www.m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf 2. (ACL 2018) https://arxiv.org/abs/1808.06640 3. (ICCV 2019) https://arxiv.org/abs/1811.08489 4. (ICCV 2019) http://hal.cse.msu.edu/assets/pdfs/papers/2019-iccv-kernel-adversarial-representation-learning.pdf Beyond experiments on different datasets, and slight modification of the adversarial loss for correlation, I am unsure what this paper contributes beyond these works. While some some of these papers can potentially be considered contemporary, authors must at least address these issues. Furthermore, modification of the objective seems to have mixed results (Table 2 CatGAN vs. BR-NET, although I'm not sure whats going on with vgg vs resnet), where the baseline would correspond more closely to the setup in https://arxiv.org/abs/1811.08489 (3. above) . Overall I am positive about the direction, but I am unsure this paper represents a significant contribution over existing work. ", "rating": "3: Weak Reject", "reply_text": "We thank the reviewer for the constructive and positive comments . Please see our detailed response to each of the points below . \u2014\u2014\u2014\u2014- > > > Comment : Overall the direction is interesting and the methodology is intuitive and sound . Unfortunately , this paper seems unaware of extremely related works : 1 . ( AAAI 2018 ) http : //www.m-mitchell.com/papers/Adversarial_Bias_Mitigation.pdf 2 . ( ACL 2018 ) https : //arxiv.org/abs/1808.06640 3 . ( ICCV 2019 ) https : //arxiv.org/abs/1811.08489 4 . ( ICCV 2019 ) http : //hal.cse.msu.edu/assets/pdfs/papers/2019-iccv-kernel-adversarial-representation-learning.pdf > Response : Thank you for pointing us to these prior works . We have added them in literature review accordingly and have discussed their differences to our work . Please note that we have already cited some of these articles ( e.g. , Xieet al.NeurIPS 2017 and Akuzawa et al.EMCL 2019 ) . As we were already aware of several of these works , we regarded our work quite distinct from existing state-of-the-art in the sense that our work could handle continuous bias/confounder variables as opposed to many works focusing only on binary/categorical variables . Please also consider that two of the above works ( 3 and 4 ) are ICCV 2019 papers , which were published online on arxiv and CVF OpenAccess in October 2019 , which was after the deadline of ICLR submission . As opposed to other works , we also explicitly modeled the statistical dependency between features and bias . To make clear the difference from prior arts , please refer to Section 3.1 in the updated manuscript describing the statistical property of our method . We theoretically showed that modifying the adversarial loss as the \u2018 Pearson correlation \u2019 actually guarantees an important statistical property of the model : the learned features and bias would achieve \u2018 mean independence \u2019 , a much stronger type of statistical independence than linear independence ( Pearson correlation=0 ) . Such property can only be achieved by our strategy of adversarially optimizing the Pearson-correlation of predicted and true bias . This distinguishes our work from several others , in which adversarial object is solely defined w.r.t.maximizing bias prediction loss ( only aims at predicting the exact value of the loss neglecting proper statistical ( in ) dependence ) . \u2014\u2014\u2014\u2014- > > > Comment : Beyond experiments on different datasets , and slight modification of the adversarial loss for correlation , I am unsure what this paper contributes beyond these works . While some of these papers can potentially be considered contemporary , authors must at least address these issues . Furthermore , modification of the objective seems to have mixed results ( Table 2 CatGAN vs. BR-NET , although I 'm not sure whats going on with vgg vs resnet ) , where the baseline would correspond more closely to the setup in https : //arxiv.org/abs/1811.08489 ( 3. above ) . > Response : As for the novelties of the paper , please see the above text . Our experiments show that strategies taken by other methods do not work for continuous and ordinal variables . These types of bias/confounder variables are very common in real-world computer vision and medical applications . CatGan ( the same idea of the baseline suggested by the reviewer ) also worked reasonably well with VGG backbone but was only applicable when bias took discrete categorical values . Even in that case , the results are BR-Net are significantly superior for some shade categories ( shade 6 , see Fig.7 ) .Please note that in the HIV experiment CatGan is not applicable as the bias variable ( age ) is a continuous variable . \u2014\u2014\u2014\u2014- > > > Comment : Overall I am positive about the direction , but I am unsure this paper represents a significant contribution over existing work . > Response : Thank you very much for the positive and constructive feedback . Please also refer to the repnoses above . In summary , we would like to again mention that our proposed method is not just proposing a new loss function . We explicitly modeled the statistical dependency between features and bias . We added theoretical properties of the proposed method in the updated manuscript ( Section 3.1 ) . We theoretically showed that using \u2018 Pearson correlation \u2019 as the adversarial loss actually guarantees that the learned features and bias would achieve \u2018 mean independence. \u2019 Such property can only be achieved by our strategy of adversarially optimizing the Pearson-correlation of predicted and true bias . This distinguishes our work from the previous work . The experimental evaluations are also designed to show these claims and that is why we selected [ BR-Net with categorical adversarial loss ( CatGAN ) ] , [ BR-Net with MSE adversarial loss ] , and [ Multi-Task ] and direct baselines for our method . This way , we have directly implemented and compared with all previous strategies ( including 1 , 2 , 3 , and 4 referred to by the reviewer above ) ."}}