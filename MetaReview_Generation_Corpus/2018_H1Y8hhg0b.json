{"year": "2018", "forum": "H1Y8hhg0b", "title": "Learning Sparse Neural Networks through L_0 Regularization", "decision": "Accept (Poster)", "meta_review": "The results in the paper are interesting, and the modifications improve the paper further. Reviewers found teh paper interesting and potentailly applicable to many models.", "reviews": [{"review_id": "H1Y8hhg0b-0", "review_text": "This paper presents a continuous surrogate for the ell_0 norm and focuses on its applications in regularized empirical regularized minimization. The proposed continuous relaxation scheme allows for gradient based-stochastic optimization for binary discrete variables under the reparameterization trick, and extends the original binary concrete distribution by allowing the parameter taking values of exact zeros and ones, with additional stretching and thresholding operations. Under a compound construction of sparsity, the proposed approach can easily incorporate group sparsity by sharing supports among the grouped variables, or be combined with other types of regularizations on the magnitude of non-zero components. The efficacy of the proposed method in sparsification and speedup is demonstrated in two experiments with comparisons against a few baseline methods. Pros: - The paper is clearly written, self-contained and a pleasure to read. - Based on the evidence provided, the procedure seems to be a useful continuous relaxation scheme to consider in handling optimization with spike and slab regularization Cons: - It would be interesting to see how the induced penalty behaves in terms shrinkage comparing against ell_0 and other ell_p choices - It is unclear what properties does the proposed hard-concrete distribution have, e.g., closed-form density, convexity, etc. - If the authors can offer a rigorous analysis on the influence of base concrete distribution and provide more guidance on how to choose the stretching parameters in practice, this paper would be more significant ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would first like to thank you for taking the time to review our submission ; we will now address your comments : - We agree that comparing other L_p choices with the L_0 norm is beneficial ; we would like to point out that the GL ( Group Lasso ) baseline method for the LeNet5-Caffe experiment employs L_1 regularization for pruning neurons and convolutional filters so we believe that it can serve as a way to measure differences between the L_0 norm and the most popular L_p alternative . - Due to lack of space we provided a bit of more information about the hard concrete distribution at the appendix ; it has a closed-form density that involves the CDF and PDF of the concrete distribution . - The stretching parameters were initially chosen heuristically and kept fixed for all of the experiments . The heuristic was to approximately aim for clipping to zero if the value of the random variable is less than 0.1 or rounding to 1 if the value of the random variable is larger than 0.9 . It should be noted that their choice is not particularly important due to their interplay with the temperature parameter of the concrete distribution ; they collectively determine the probabilities of the endpoints { 0 , 1 } , i.e.p ( z=0 ) and p ( z=1 ) . As a result we believe that the choice of the stretching parameters is not very important , given the fact that the temperature of the concrete distribution is tuned appropriately ."}, {"review_id": "H1Y8hhg0b-1", "review_text": "Learning sparse neural networks through L0 regularisation Summary: The authors introduce a gradient-based approach to minimise an objective function with an L0 sparse penalty. The problem is relaxed onto a continuous optimisation by changing an expectation over discrete variables (representing whether a variable is present or not) to an expectation over continuous variables, inspired by earlier work from Maddison et al (ICLR 2017) where a similar transformation was used to learn over discrete variable prediction tasks with neural networks. Here the application is to learn sparse feedforward networks in standard classification tasks, although the framework described is quite general and could be used to impose L0 sparsity to any objective function in principal. The method provides equivalent accuracy and sparsity to published state-of-the-art results on these datasets but it is argue that learning sparsity during the training process will lead to significant speed-ups - this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations. Pros: The paper is well written and the derivation of the method is easy to follow with a good explanation of the underlying theory. Optimisation under L0 regularisation is a difficult and generally important topic and certainly has advantages over other sparse inference objective functions that impose shrinkage on non-sparse parameters. The work is put in context and related to some previous relaxation approaches to sparsity. The method allows for sparsity to be learned during training rather than after training (as in standard dropout approaches) and this allows the algorithm to obtain significant per-iteration speed-ups, which improves through training. Cons: The method is applied to standard neural network architectures and performance in terms of accuracy and final achieved sparsity is comparable to the state-of-the-art methods. Therefore the main advance is in terms of learning speed to obtain this similar performance. However, the learning speed-up is presented against a theoretical FLOPs estimate per iteration for a similar network with dropout. It would be useful to know whether the number of iterations to achieve a particular performance is equivalent for all the different architectures considered, e.g. does the proposed sparse learning method converge at the same rate as the others? I felt a more thorough experimental section would have greatly improved the work, focussing on this learning speed aspect. It was unclear how much tuning of the lambda hyper-parameter, which tunes the sparsity, would be required in a practical application since tuning this parameter would increase computation time. It might be useful to provide a full Bayesian treatment so that the optimal sparsity can be chosen through hyper-parameter learning. Minor point: it wasn\u2019t completely clear to me why the fact (3) is a variational approximation to a spike-and-slab is important (Appendix). I don\u2019t see why the spike-and-slab is any more fundamental than the L0 norm prior in (2), it is just more convenient in Bayesian inference because it is an iid prior and potentially allows an informative prior over each parameter. In the context here this didn\u2019t seem a particularly relevant addition to the paper. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would first like to thank you for the thorough and extensive review . Regarding whether the method converges in a similar way to standard networks ; indeed this is the case . On the CIFAR task with WRNs the L0 regularized networks had similar learning curves with the dropout equivalent networks . We have updated the paper with an example plot on CIFAR 10 . Regarding the lambda hyperparameter ; this is true . Empirically , we didn \u2019 t have to tune this parameter a lot and considered a small set of values . Treating this parameter in a Bayesian way would indeed be a fruitful direction for future research . As for the spike-and-slab connection ; we agree that it is a minor point ( hence it is in the appendix ) , but we still believe that it is a relevant addition to the paper . It provides an interpretation to the L0 objective that also allows for the incorporation of prior knowledge about the behaviour of the sparity in the form of a prior over the gates . This could then potentially allow for better regularization of the gating mechanism ."}, {"review_id": "H1Y8hhg0b-2", "review_text": "The paper introduces a technique for optimizing an L0 penalty on the weights of a neural network. The basic problem is empirical risk minimization with a incremental penalty for each non zero weight. To tackle this problem, this paper proposes an expected surrogate loss that is then relaxed using a method related to recently introduced relaxations of discrete random variables. The authors note that this loss can also be seen as a specific variational bound of a Bayesian model over the weights. The key advantage of this method is that it gives a training time technique for sparsifying neural network computation, leading to potential wins in computation time during training. The results presented in the paper are convincing. They achieve results competitive with previous methods, with the additional advantage that their sparse models are available during training time. They show order of magnitude reductions in computation time for small models, and more modest constant improvements for large models. The hard concrete distribution is a small but nice contribution on its own. My only concern is the lack of discussion on the relationship between this method and Concrete Dropout (https://arxiv.org/abs/1705.07832). Although the focus is apparently different, these methods are clearly closely related. A discussion of this relationship seems really important. Specific comments/questions: - The reduction of computation time is the key advantage, and it would have been nice to see a more thorough investigation of this. For example, it would have been interesting to see whether this method would work with structured L0 penalties that removed entire units (as opposed to single weights) or other subsets of the computation. This would give a stronger sense of the kind of wins that are possible in this framework. - Hard concrete is a nice contribution, but there are clearly many possibilities for these relaxations. Extra evaluations of different relaxations would be appreciated. At the very least a comparison to concrete would be nice. - In equation 2, the equality of the L0 norm with the sum of z assumes that tilde{theta} is not 0.", "rating": "7: Good paper, accept", "reply_text": "We would first like to thank you for the constructive review ; we revised the paper and now it contains a discussion of concrete Dropout . The main difference is that concrete dropout does not allow for values of exact zero ( and one ) thus precluding the benefits of sparsity during training time . One potential way to employ concrete Dropout in this case would be to use it as a biased surrogate for the optimization of eq.3 ; this could still allow for potential sparsity at test time by pruning according to thresholds , but nevertheless would require evaluating the full original model during training . As for your other comments : - Perhaps it 's not very prominent but all of our results employ structured penalties , i.e.we are removing either entire convolutional feature maps or entire hidden units . - For the reasons we previously mentioned , we believe that comparing with concrete dropout will not provide much extra information as the sparsity could only be achieved at test time and not during training ( which was one of the main objectives of this work ) . An alternative that maintains the sparsity during training time , and we experimented with in a pilot study , was a smoothing mechanism that involved the hard-sigmoid of a Gaussian r.v .. This turned out to be worse than the hard concrete procedure , and we attribute this to the unimodality of the underlying Gaussian distribution ( which can not accurately capture the behaviour of a Bernoulli r.v . ) . We mentioned a couple of sentences about this in the related work section . We also included a comparison against \u201c Generalized Dropout \u201d ( GD ) that utilizes the straight-through estimator for the same LeNet-5 task we considered in the experiments ; this can serve as a comparison against the proposed hard concrete smoothing procedure . - This is indeed true and we have updated the text accordingly ."}], "0": {"review_id": "H1Y8hhg0b-0", "review_text": "This paper presents a continuous surrogate for the ell_0 norm and focuses on its applications in regularized empirical regularized minimization. The proposed continuous relaxation scheme allows for gradient based-stochastic optimization for binary discrete variables under the reparameterization trick, and extends the original binary concrete distribution by allowing the parameter taking values of exact zeros and ones, with additional stretching and thresholding operations. Under a compound construction of sparsity, the proposed approach can easily incorporate group sparsity by sharing supports among the grouped variables, or be combined with other types of regularizations on the magnitude of non-zero components. The efficacy of the proposed method in sparsification and speedup is demonstrated in two experiments with comparisons against a few baseline methods. Pros: - The paper is clearly written, self-contained and a pleasure to read. - Based on the evidence provided, the procedure seems to be a useful continuous relaxation scheme to consider in handling optimization with spike and slab regularization Cons: - It would be interesting to see how the induced penalty behaves in terms shrinkage comparing against ell_0 and other ell_p choices - It is unclear what properties does the proposed hard-concrete distribution have, e.g., closed-form density, convexity, etc. - If the authors can offer a rigorous analysis on the influence of base concrete distribution and provide more guidance on how to choose the stretching parameters in practice, this paper would be more significant ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would first like to thank you for taking the time to review our submission ; we will now address your comments : - We agree that comparing other L_p choices with the L_0 norm is beneficial ; we would like to point out that the GL ( Group Lasso ) baseline method for the LeNet5-Caffe experiment employs L_1 regularization for pruning neurons and convolutional filters so we believe that it can serve as a way to measure differences between the L_0 norm and the most popular L_p alternative . - Due to lack of space we provided a bit of more information about the hard concrete distribution at the appendix ; it has a closed-form density that involves the CDF and PDF of the concrete distribution . - The stretching parameters were initially chosen heuristically and kept fixed for all of the experiments . The heuristic was to approximately aim for clipping to zero if the value of the random variable is less than 0.1 or rounding to 1 if the value of the random variable is larger than 0.9 . It should be noted that their choice is not particularly important due to their interplay with the temperature parameter of the concrete distribution ; they collectively determine the probabilities of the endpoints { 0 , 1 } , i.e.p ( z=0 ) and p ( z=1 ) . As a result we believe that the choice of the stretching parameters is not very important , given the fact that the temperature of the concrete distribution is tuned appropriately ."}, "1": {"review_id": "H1Y8hhg0b-1", "review_text": "Learning sparse neural networks through L0 regularisation Summary: The authors introduce a gradient-based approach to minimise an objective function with an L0 sparse penalty. The problem is relaxed onto a continuous optimisation by changing an expectation over discrete variables (representing whether a variable is present or not) to an expectation over continuous variables, inspired by earlier work from Maddison et al (ICLR 2017) where a similar transformation was used to learn over discrete variable prediction tasks with neural networks. Here the application is to learn sparse feedforward networks in standard classification tasks, although the framework described is quite general and could be used to impose L0 sparsity to any objective function in principal. The method provides equivalent accuracy and sparsity to published state-of-the-art results on these datasets but it is argue that learning sparsity during the training process will lead to significant speed-ups - this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations. Pros: The paper is well written and the derivation of the method is easy to follow with a good explanation of the underlying theory. Optimisation under L0 regularisation is a difficult and generally important topic and certainly has advantages over other sparse inference objective functions that impose shrinkage on non-sparse parameters. The work is put in context and related to some previous relaxation approaches to sparsity. The method allows for sparsity to be learned during training rather than after training (as in standard dropout approaches) and this allows the algorithm to obtain significant per-iteration speed-ups, which improves through training. Cons: The method is applied to standard neural network architectures and performance in terms of accuracy and final achieved sparsity is comparable to the state-of-the-art methods. Therefore the main advance is in terms of learning speed to obtain this similar performance. However, the learning speed-up is presented against a theoretical FLOPs estimate per iteration for a similar network with dropout. It would be useful to know whether the number of iterations to achieve a particular performance is equivalent for all the different architectures considered, e.g. does the proposed sparse learning method converge at the same rate as the others? I felt a more thorough experimental section would have greatly improved the work, focussing on this learning speed aspect. It was unclear how much tuning of the lambda hyper-parameter, which tunes the sparsity, would be required in a practical application since tuning this parameter would increase computation time. It might be useful to provide a full Bayesian treatment so that the optimal sparsity can be chosen through hyper-parameter learning. Minor point: it wasn\u2019t completely clear to me why the fact (3) is a variational approximation to a spike-and-slab is important (Appendix). I don\u2019t see why the spike-and-slab is any more fundamental than the L0 norm prior in (2), it is just more convenient in Bayesian inference because it is an iid prior and potentially allows an informative prior over each parameter. In the context here this didn\u2019t seem a particularly relevant addition to the paper. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would first like to thank you for the thorough and extensive review . Regarding whether the method converges in a similar way to standard networks ; indeed this is the case . On the CIFAR task with WRNs the L0 regularized networks had similar learning curves with the dropout equivalent networks . We have updated the paper with an example plot on CIFAR 10 . Regarding the lambda hyperparameter ; this is true . Empirically , we didn \u2019 t have to tune this parameter a lot and considered a small set of values . Treating this parameter in a Bayesian way would indeed be a fruitful direction for future research . As for the spike-and-slab connection ; we agree that it is a minor point ( hence it is in the appendix ) , but we still believe that it is a relevant addition to the paper . It provides an interpretation to the L0 objective that also allows for the incorporation of prior knowledge about the behaviour of the sparity in the form of a prior over the gates . This could then potentially allow for better regularization of the gating mechanism ."}, "2": {"review_id": "H1Y8hhg0b-2", "review_text": "The paper introduces a technique for optimizing an L0 penalty on the weights of a neural network. The basic problem is empirical risk minimization with a incremental penalty for each non zero weight. To tackle this problem, this paper proposes an expected surrogate loss that is then relaxed using a method related to recently introduced relaxations of discrete random variables. The authors note that this loss can also be seen as a specific variational bound of a Bayesian model over the weights. The key advantage of this method is that it gives a training time technique for sparsifying neural network computation, leading to potential wins in computation time during training. The results presented in the paper are convincing. They achieve results competitive with previous methods, with the additional advantage that their sparse models are available during training time. They show order of magnitude reductions in computation time for small models, and more modest constant improvements for large models. The hard concrete distribution is a small but nice contribution on its own. My only concern is the lack of discussion on the relationship between this method and Concrete Dropout (https://arxiv.org/abs/1705.07832). Although the focus is apparently different, these methods are clearly closely related. A discussion of this relationship seems really important. Specific comments/questions: - The reduction of computation time is the key advantage, and it would have been nice to see a more thorough investigation of this. For example, it would have been interesting to see whether this method would work with structured L0 penalties that removed entire units (as opposed to single weights) or other subsets of the computation. This would give a stronger sense of the kind of wins that are possible in this framework. - Hard concrete is a nice contribution, but there are clearly many possibilities for these relaxations. Extra evaluations of different relaxations would be appreciated. At the very least a comparison to concrete would be nice. - In equation 2, the equality of the L0 norm with the sum of z assumes that tilde{theta} is not 0.", "rating": "7: Good paper, accept", "reply_text": "We would first like to thank you for the constructive review ; we revised the paper and now it contains a discussion of concrete Dropout . The main difference is that concrete dropout does not allow for values of exact zero ( and one ) thus precluding the benefits of sparsity during training time . One potential way to employ concrete Dropout in this case would be to use it as a biased surrogate for the optimization of eq.3 ; this could still allow for potential sparsity at test time by pruning according to thresholds , but nevertheless would require evaluating the full original model during training . As for your other comments : - Perhaps it 's not very prominent but all of our results employ structured penalties , i.e.we are removing either entire convolutional feature maps or entire hidden units . - For the reasons we previously mentioned , we believe that comparing with concrete dropout will not provide much extra information as the sparsity could only be achieved at test time and not during training ( which was one of the main objectives of this work ) . An alternative that maintains the sparsity during training time , and we experimented with in a pilot study , was a smoothing mechanism that involved the hard-sigmoid of a Gaussian r.v .. This turned out to be worse than the hard concrete procedure , and we attribute this to the unimodality of the underlying Gaussian distribution ( which can not accurately capture the behaviour of a Bernoulli r.v . ) . We mentioned a couple of sentences about this in the related work section . We also included a comparison against \u201c Generalized Dropout \u201d ( GD ) that utilizes the straight-through estimator for the same LeNet-5 task we considered in the experiments ; this can serve as a comparison against the proposed hard concrete smoothing procedure . - This is indeed true and we have updated the text accordingly ."}}