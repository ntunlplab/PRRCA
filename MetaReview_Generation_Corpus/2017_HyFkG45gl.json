{"year": "2017", "forum": "HyFkG45gl", "title": "Machine Solver for Physics Word Problems", "decision": "Reject", "meta_review": "The paper explores a model for solving simple physics problems (posed in automatically generated natural language). Whilst this an interesting problem, the reviewers worry that the problem is too simple because all problems are automatically generated. The paper should at least incorporate some reasonable baseline models and/or apply the proposed methodology on real physics problems.", "reviews": [{"review_id": "HyFkG45gl-0", "review_text": "The authors describe a system for solving physics word problems. The system consists of two neural networks: a labeler and a classifier, followed by a numerical integrator. On the dataset that the authors synthesize, the full system attains near full performance. Outside of the pipeline, the authors also provide some network activation visualizations. The paper is clear, and the data generation procedure/grammar is rich and interesting. However, overall the system is not well motivated. Why did they consider this particular problem domain, and what challenges did they specifically hope to address? Is it the ability to label sequences using LSTM networks, or the ability to classify what is being asked for in the question? This has already been illustrated, for example, by work on POS tagging and by memory networks for the babi tasks. A couple of standard architectural modifications, i.e. bi-directionality and a content-based attention mechanism, were also not considered.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the valuable feedback ! The motivation for the system is to explore new approaches to developing question answering systems . One use case of a question answering system that could solve word problems would be a personal educational assistant . Our overall goal is to develop a way for computers to `` understand '' word problems . We chose the domain of physics to constrain the task as a starting point for this challenge , as the neural network outputs could be readily mapped to a computer-solvable form ( dynamic system ) for numerical integration . We drew inspiration from previous work on natural language processing , such as named entity recognition , in order to apply the tasks to solving physics word problems . The reviewer is absolutely correct that two tasks must occur here : the sequence needs to be labeled and the question classified . We wanted to see if previous successes of neural networks on these tasks could be applied specifically to the problem of reasoning about word problems . Thank you for the suggestions of further architectural modifications that we can incorporate into future work !"}, {"review_id": "HyFkG45gl-1", "review_text": "This paper build a language-based solver for simple physics problems (a free falling object under constant velocity). Given a natural language query sampled from a fixed grammar, the system uses two LSTM models to extract key components, e.g., physical parameters and the type of questions being asked, which are then sent to a numerical integrator for the answer. The overall performance in the test set is almost perfect (99.8%). Overall I found this paper quite interesting to read (and it is well written). However, it is not clear how hard the problem is and how much this approach could generalize over more realistic (and complicated) situations. The dataset are a bit small and might not cover the query space. It might be better to ask AMT workers to come up with more complicated queries/answers. The physics itself is also quite easy. What happens if we apply the same idea on billiards? In this case, even we have a perfect physics simulator, the question to be asked could be very deep and requires multi-hop reasoning. Finally, given the same problem setting (physics solver), in my opinion, a more interesting direction is to study how DNN can take the place of numerical integrator and gives rough answers to the question (i.e., intuitive physics). It is a bit disappointing to see that DNN is only used to extract the parameters while still a traditional approach is used for core reasoning part. It would be more interesting to see the other way round. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the useful feedback ! We are glad you enjoyed reading the paper ! Although our problem generator was based off of physics problems extracted from textbooks , word problems from AMT workers would likely introduce more complexity into our problems . This may also help us better identify the difficulty of the task , particularly if these problems were incorporated into our test sets . Thank you for the suggestion ! As of now we use two neural networks to be able to perform multiple levels of reasoning as required by the word problem , but the reviewer is correct in that some problems may require more levels of reasoning than seen in these problems . While this is true , a question answering system that can solve problems with only several levels of reasoning would still be highly useful for solving simple problems for K-12 students . Finally , while it would be an interesting problem to replace the numerical integrator , we chose to solve the first part of the challenge ( to understand the word problem ) , as there are not established existing tools to do this , but there are many existing numerical integration tools for computers ."}, {"review_id": "HyFkG45gl-2", "review_text": "The paper uses neural networks to answer falling body physics questions by 1. Resolving the parameters of the problem, and 2. Figure out which quantity is in question, compute it using a numerical integrator and return it as an answer. Learning and inference are performed on artificially generated questions using a probabilistic grammar. Overall, the paper is clearly written and seems to be novel in its approach. The main problems I see with this work are: 1. The task is artificial, and it's not clear how hard it is. The authors provide no baseline nor do they compare it to any real world problem. Without some measure of difficulty it's hard to tell if a much simple approach will do better, or if the task even makes sense. 2. The labler LSTM uses only 10 hidden units. This is remarkably small for language modeling problems, and makes one further wonder about the difficulty of the task. The authors provide no reasoning for this choice. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We want to thank the reviewer for the important feedback . We are in the process of assembling a collection of real world problems from published tests and hope to include results from running them through our system in future work . As for the fact that we use 10 hidden units in the labeler LSTM , that number was chose after experimenting with different sizes using a grid search . Some of the experiments are reported in our Appendix B ."}], "0": {"review_id": "HyFkG45gl-0", "review_text": "The authors describe a system for solving physics word problems. The system consists of two neural networks: a labeler and a classifier, followed by a numerical integrator. On the dataset that the authors synthesize, the full system attains near full performance. Outside of the pipeline, the authors also provide some network activation visualizations. The paper is clear, and the data generation procedure/grammar is rich and interesting. However, overall the system is not well motivated. Why did they consider this particular problem domain, and what challenges did they specifically hope to address? Is it the ability to label sequences using LSTM networks, or the ability to classify what is being asked for in the question? This has already been illustrated, for example, by work on POS tagging and by memory networks for the babi tasks. A couple of standard architectural modifications, i.e. bi-directionality and a content-based attention mechanism, were also not considered.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the valuable feedback ! The motivation for the system is to explore new approaches to developing question answering systems . One use case of a question answering system that could solve word problems would be a personal educational assistant . Our overall goal is to develop a way for computers to `` understand '' word problems . We chose the domain of physics to constrain the task as a starting point for this challenge , as the neural network outputs could be readily mapped to a computer-solvable form ( dynamic system ) for numerical integration . We drew inspiration from previous work on natural language processing , such as named entity recognition , in order to apply the tasks to solving physics word problems . The reviewer is absolutely correct that two tasks must occur here : the sequence needs to be labeled and the question classified . We wanted to see if previous successes of neural networks on these tasks could be applied specifically to the problem of reasoning about word problems . Thank you for the suggestions of further architectural modifications that we can incorporate into future work !"}, "1": {"review_id": "HyFkG45gl-1", "review_text": "This paper build a language-based solver for simple physics problems (a free falling object under constant velocity). Given a natural language query sampled from a fixed grammar, the system uses two LSTM models to extract key components, e.g., physical parameters and the type of questions being asked, which are then sent to a numerical integrator for the answer. The overall performance in the test set is almost perfect (99.8%). Overall I found this paper quite interesting to read (and it is well written). However, it is not clear how hard the problem is and how much this approach could generalize over more realistic (and complicated) situations. The dataset are a bit small and might not cover the query space. It might be better to ask AMT workers to come up with more complicated queries/answers. The physics itself is also quite easy. What happens if we apply the same idea on billiards? In this case, even we have a perfect physics simulator, the question to be asked could be very deep and requires multi-hop reasoning. Finally, given the same problem setting (physics solver), in my opinion, a more interesting direction is to study how DNN can take the place of numerical integrator and gives rough answers to the question (i.e., intuitive physics). It is a bit disappointing to see that DNN is only used to extract the parameters while still a traditional approach is used for core reasoning part. It would be more interesting to see the other way round. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the useful feedback ! We are glad you enjoyed reading the paper ! Although our problem generator was based off of physics problems extracted from textbooks , word problems from AMT workers would likely introduce more complexity into our problems . This may also help us better identify the difficulty of the task , particularly if these problems were incorporated into our test sets . Thank you for the suggestion ! As of now we use two neural networks to be able to perform multiple levels of reasoning as required by the word problem , but the reviewer is correct in that some problems may require more levels of reasoning than seen in these problems . While this is true , a question answering system that can solve problems with only several levels of reasoning would still be highly useful for solving simple problems for K-12 students . Finally , while it would be an interesting problem to replace the numerical integrator , we chose to solve the first part of the challenge ( to understand the word problem ) , as there are not established existing tools to do this , but there are many existing numerical integration tools for computers ."}, "2": {"review_id": "HyFkG45gl-2", "review_text": "The paper uses neural networks to answer falling body physics questions by 1. Resolving the parameters of the problem, and 2. Figure out which quantity is in question, compute it using a numerical integrator and return it as an answer. Learning and inference are performed on artificially generated questions using a probabilistic grammar. Overall, the paper is clearly written and seems to be novel in its approach. The main problems I see with this work are: 1. The task is artificial, and it's not clear how hard it is. The authors provide no baseline nor do they compare it to any real world problem. Without some measure of difficulty it's hard to tell if a much simple approach will do better, or if the task even makes sense. 2. The labler LSTM uses only 10 hidden units. This is remarkably small for language modeling problems, and makes one further wonder about the difficulty of the task. The authors provide no reasoning for this choice. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We want to thank the reviewer for the important feedback . We are in the process of assembling a collection of real world problems from published tests and hope to include results from running them through our system in future work . As for the fact that we use 10 hidden units in the labeler LSTM , that number was chose after experimenting with different sizes using a grid search . Some of the experiments are reported in our Appendix B ."}}