{"year": "2017", "forum": "rJqBEPcxe", "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "decision": "Accept (Poster)", "meta_review": "Very nice paper, with simple, intuitive idea that works quite well, solving the problem of how to do recurrent dropout.\n \n Pros:\n - Improved results\n - Very simple method\n \n Cons:\n - Almost the best results (aside from Variational Dropout)", "reviews": [{"review_id": "rJqBEPcxe-0", "review_text": "The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability. Overall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs. The experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case. Overall, the paper bears great potential. However, I do see some points. 1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available. I want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here. Zoneout does not seem to improve that much in the other tasks. 2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K\u2019 at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal\u2019s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis. 3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away. An extreme amount of \u201ctricks\u201d is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2) Consequently, the paper reduces to a \u201cepsilon improvement, great text, mediocre experimental evaluation, little theoretical insight\u201d. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thoughtful and detailed review . Your comments raise some important concerns which we address individually below . Q : I want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set . Thus , overfitting the model selection process is a serious concern here . A : Thank you for pointing out this issue . Our initial table was not clear or representative of our results . We have reformatted the table to clarify both that our test-validation gaps are very consistent within our experiments , and that zoneout does achieve the best validation performance within our experiments . These experiments include a search over regularization techniques and strengths , as detailed below . We believe this addresses your concerns about model selection . ( To fit everything in one table ( Table 1 ) , we had included a mix of results from other papers and from our own experiments . Semeniuta et al. \u2019 s recurrent dropout got worse validation performance than zoneout in our experiments , but we then reported results from their paper . This was confusing , and has been corrected . ) We have also corrected the validation and test values of recurrent dropout on Text8 . They were incorrectly reported , and we apologize for any confusion caused by this error . For the purposes of early stopping and model selection , we measured validation BPC on sequences of length 100 as in training ( as opposed to computing it over the entire sequence , which is what we do at test time ) . This is why we have a larger gap between validation and test BPC than previous works . In order to update Table 1 , we reran the best performing recurrent dropout ( p=0.25 ) and zoneout ( p_cells=0.5 and p_hiddens = 0.05 ) models * with * sequence overlap . In doing so , we also noted that neither of these best-performing models had entirely overfit , so we ran both for longer . Our implementation of recurrent dropout achieves better performance ( 1.286 BPC test ) than reported by Semeniuta et al . ( 2016 ) , and zoneout achieves near state of the art 1.252 BPC . Q : 1 ) As raised during the pre-review questions , I would like to see the results of experiments that feature a complete hyper parameter search . I.e.a proper model selection process , as it should be standard in the community . I do not see why this was not done , especially as the author count seems to indicate that the necessary resources are available . A : We had previously run a suite of experiments searching over regularization strength for zoneout , recurrent dropout , and recurrent stochastic depth , but without sequence overlap . We show this search for zoneout in Figure 3 , and report the best results achieved with all models in Table 1 , and as noted , zoneout achieves the lowest validation score . Given our updates to Table 1 , if you feel that additional hyperparameter search is necessary , please clarify which hyperparameters you think are most relevant to compare . We can probably run 20-60 experiments within a few days given our available computing resources . Q : Zoneout does not seem to improve that much in the other tasks . A : We respectfully disagree . Zoneout achieves a consistent improvement of more than 4 % , relative to a vanilla LSTM baseline , on all datasets ( cf Table 1 and Table 2 ) , just by plug-and-play with existing models . Q : 3 ) The data sets used are only symbolic . It would have been great if more ground was covered , i.e.continuous data such as from dynamical systems . To me it is not obvious whether it will transfer right away . A : These datasets are standard benchmarks for RNNs and allow us to compare to previously published results . Permuted sequential MNIST does take continuous pixel values as input , and we don \u2019 t see any reason to believe that zoneout would not improve performance on tasks with continuous outputs . Q : 2 ) Zoneout is not investigated well mathematically . A : We propose a novel technique and perform a thorough empirical evaluation to demonstrate the benefit of our proposal . We investigate gradient propagation and relate zoneout to other techniques and theory ( pseudoensembles , stochastic depth , residual networks , etc . ) . While it would be great to see further work and theoretically grounded analysis , we think that our technique and results are already of interest to the community ."}, {"review_id": "rJqBEPcxe-1", "review_text": "Paper Summary This paper proposes a variant of dropout, applicable to RNNs, in which the state of a unit is randomly retained, as opposed to being set to zero. This provides noise which gives the regularization effect, but also prevents loss of information over time, in fact making it easier to send gradients back because they can flow right through the identity connections without attenuation. Experiments show that this model works quite well. It is still worse that variational dropout on Penn Tree bank language modeling task, but given the simplicity of the idea it is likely to become widely useful. Strengths - Simple idea that works well. - Detailed experiments help understand the effects of the zoneout probabilities and validate its applicability to different tasks/domains. Weaknesses - Does not beat variational dropout (but maybe better hyper-parameter tuning will help). Quality The experimental design and writeup is high quality. Clarity The paper clear and well written, experimental details seem adequate. Originality The proposed idea is novel. Significance This paper will be of interest to anyone working with RNNs (which is a large group of people!). Minor suggestion- - As the authors mention - Zoneout has two things working for it - the noise and the ability to pass gradients back without decay. It might help to tease apart the contribution from these two factors. For example, if we use a fixed mask over the unrolled network (different at each time step) instead of resampling it again for every training case, it would tell us how much help comes from the identity connections alone.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your detailed review and thoughtful suggestions . -- -- -- Zoneout has two things working for it - the noise and the ability to pass gradients back without decay . It might help to tease apart the contribution from these two factors . For example , if we use a fixed mask over the unrolled network ( different at each time step ) instead of resampling it again for every training case , it would tell us how much help comes from the identity connections alone . -- -- -- This seemed like a pretty cool experiment ; we have run it and added it in the Appendix , Figure 7 . We 're grateful for this interesting contribution to the paper !"}, {"review_id": "rJqBEPcxe-2", "review_text": "This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios. While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections. The continued analysis of this aspect, including analyzing how the gradient flow improves the given tasks, is of great interest and helps show it as an inherent property of zoneout. This is a well written paper with a variety of experiments that support the claims. I have also previously used this technique in a recurrent setting and am confident on the positive impact it can have upon tasks. This is likely to become a standard technique used within RNNs across various frameworks.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review , and great to hear that zoneout has helped in your experiments !"}], "0": {"review_id": "rJqBEPcxe-0", "review_text": "The authors propose a conceptually simple method for regularisation of recurrent neural networks. The idea is related to dropout, but instead of zeroing out units, they are instead set to their respective values at the preceding time step element-wise with a certain probability. Overall, the paper is well written. The method is clearly represented up to issues raised by reviewers during the pre-review question phase. The related work is complete and probably the best currently available on the matter of regularising RNNs. The experimental section focuses on comparing the method with the current SOTA on a set of NLP benchmarks and a synthetic problem. All of the experiments focus on sequences over discrete values. An additional experiment also shows that the sequential Jacobian is far higher for long-term dependencies than in the dropout case. Overall, the paper bears great potential. However, I do see some points. 1) As raised during the pre-review questions, I would like to see the results of experiments that feature a complete hyper parameter search. I.e. a proper model selection process,as it should be standard in the community. I do not see why this was not done, especially as the author count seems to indicate that the necessary resources are available. I want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set. Thus, overfitting the model selection process is a serious concern here. Zoneout does not seem to improve that much in the other tasks. 2) Zoneout is not investigated well mathematically. E.g. an analysis of the of the form of gradients from unit K at time step T to unit K\u2019 at time step T-R would have been interesting, especially as these are not necessarily non-zero for dropout. Also, the question whether zoneout has a variational interpretation in the spirit of Yarin Gal\u2019s work is an obvious one. I can see that it is if we treat zoneout in a resnet framework and dropout on the incremental parts. Overall, little effort is done answering the question *why* zoneout works well, even though the literature bears plenty of starting points for such analysis. 3) The data sets used are only symbolic. It would have been great if more ground was covered, i.e. continuous data such as from dynamical systems. To me it is not obvious whether it will transfer right away. An extreme amount of \u201ctricks\u201d is being published currently for improved RNN training. How does zoneout stand out? It is a nice idea, and simple to implement. However, the paper under delivers: the experiments do not convince me (see 1) and 3)). There authors do not provide convincing theoretical insights either. (2) Consequently, the paper reduces to a \u201cepsilon improvement, great text, mediocre experimental evaluation, little theoretical insight\u201d. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your thoughtful and detailed review . Your comments raise some important concerns which we address individually below . Q : I want to repeat at this point that Table 2 of the paper shows that validation error is not a reliable estimator for testing error in the respective data set . Thus , overfitting the model selection process is a serious concern here . A : Thank you for pointing out this issue . Our initial table was not clear or representative of our results . We have reformatted the table to clarify both that our test-validation gaps are very consistent within our experiments , and that zoneout does achieve the best validation performance within our experiments . These experiments include a search over regularization techniques and strengths , as detailed below . We believe this addresses your concerns about model selection . ( To fit everything in one table ( Table 1 ) , we had included a mix of results from other papers and from our own experiments . Semeniuta et al. \u2019 s recurrent dropout got worse validation performance than zoneout in our experiments , but we then reported results from their paper . This was confusing , and has been corrected . ) We have also corrected the validation and test values of recurrent dropout on Text8 . They were incorrectly reported , and we apologize for any confusion caused by this error . For the purposes of early stopping and model selection , we measured validation BPC on sequences of length 100 as in training ( as opposed to computing it over the entire sequence , which is what we do at test time ) . This is why we have a larger gap between validation and test BPC than previous works . In order to update Table 1 , we reran the best performing recurrent dropout ( p=0.25 ) and zoneout ( p_cells=0.5 and p_hiddens = 0.05 ) models * with * sequence overlap . In doing so , we also noted that neither of these best-performing models had entirely overfit , so we ran both for longer . Our implementation of recurrent dropout achieves better performance ( 1.286 BPC test ) than reported by Semeniuta et al . ( 2016 ) , and zoneout achieves near state of the art 1.252 BPC . Q : 1 ) As raised during the pre-review questions , I would like to see the results of experiments that feature a complete hyper parameter search . I.e.a proper model selection process , as it should be standard in the community . I do not see why this was not done , especially as the author count seems to indicate that the necessary resources are available . A : We had previously run a suite of experiments searching over regularization strength for zoneout , recurrent dropout , and recurrent stochastic depth , but without sequence overlap . We show this search for zoneout in Figure 3 , and report the best results achieved with all models in Table 1 , and as noted , zoneout achieves the lowest validation score . Given our updates to Table 1 , if you feel that additional hyperparameter search is necessary , please clarify which hyperparameters you think are most relevant to compare . We can probably run 20-60 experiments within a few days given our available computing resources . Q : Zoneout does not seem to improve that much in the other tasks . A : We respectfully disagree . Zoneout achieves a consistent improvement of more than 4 % , relative to a vanilla LSTM baseline , on all datasets ( cf Table 1 and Table 2 ) , just by plug-and-play with existing models . Q : 3 ) The data sets used are only symbolic . It would have been great if more ground was covered , i.e.continuous data such as from dynamical systems . To me it is not obvious whether it will transfer right away . A : These datasets are standard benchmarks for RNNs and allow us to compare to previously published results . Permuted sequential MNIST does take continuous pixel values as input , and we don \u2019 t see any reason to believe that zoneout would not improve performance on tasks with continuous outputs . Q : 2 ) Zoneout is not investigated well mathematically . A : We propose a novel technique and perform a thorough empirical evaluation to demonstrate the benefit of our proposal . We investigate gradient propagation and relate zoneout to other techniques and theory ( pseudoensembles , stochastic depth , residual networks , etc . ) . While it would be great to see further work and theoretically grounded analysis , we think that our technique and results are already of interest to the community ."}, "1": {"review_id": "rJqBEPcxe-1", "review_text": "Paper Summary This paper proposes a variant of dropout, applicable to RNNs, in which the state of a unit is randomly retained, as opposed to being set to zero. This provides noise which gives the regularization effect, but also prevents loss of information over time, in fact making it easier to send gradients back because they can flow right through the identity connections without attenuation. Experiments show that this model works quite well. It is still worse that variational dropout on Penn Tree bank language modeling task, but given the simplicity of the idea it is likely to become widely useful. Strengths - Simple idea that works well. - Detailed experiments help understand the effects of the zoneout probabilities and validate its applicability to different tasks/domains. Weaknesses - Does not beat variational dropout (but maybe better hyper-parameter tuning will help). Quality The experimental design and writeup is high quality. Clarity The paper clear and well written, experimental details seem adequate. Originality The proposed idea is novel. Significance This paper will be of interest to anyone working with RNNs (which is a large group of people!). Minor suggestion- - As the authors mention - Zoneout has two things working for it - the noise and the ability to pass gradients back without decay. It might help to tease apart the contribution from these two factors. For example, if we use a fixed mask over the unrolled network (different at each time step) instead of resampling it again for every training case, it would tell us how much help comes from the identity connections alone.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your detailed review and thoughtful suggestions . -- -- -- Zoneout has two things working for it - the noise and the ability to pass gradients back without decay . It might help to tease apart the contribution from these two factors . For example , if we use a fixed mask over the unrolled network ( different at each time step ) instead of resampling it again for every training case , it would tell us how much help comes from the identity connections alone . -- -- -- This seemed like a pretty cool experiment ; we have run it and added it in the Appendix , Figure 7 . We 're grateful for this interesting contribution to the paper !"}, "2": {"review_id": "rJqBEPcxe-2", "review_text": "This paper tests zoneout against a variety of datasets - character level, word level, and pMNIST classification - showing applicability in a wide range of scenarios. While zoneout acts as a regularizer to prevent overfitting, it also has similarities to residual connections. The continued analysis of this aspect, including analyzing how the gradient flow improves the given tasks, is of great interest and helps show it as an inherent property of zoneout. This is a well written paper with a variety of experiments that support the claims. I have also previously used this technique in a recurrent setting and am confident on the positive impact it can have upon tasks. This is likely to become a standard technique used within RNNs across various frameworks.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your review , and great to hear that zoneout has helped in your experiments !"}}