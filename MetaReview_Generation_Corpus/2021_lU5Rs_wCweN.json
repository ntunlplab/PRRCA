{"year": "2021", "forum": "lU5Rs_wCweN", "title": "Taking Notes on the Fly Helps Language Pre-Training", "decision": "Accept (Poster)", "meta_review": "The authors propose an approach for pre-training that involves \"taking notes on the fly\" for rare words. The paper stirred a lively discussion on the reasons for the reported results, which the authors followed-up with new experiments and findings that convinced the reviewers that indeed their approach is valid and interesting. Thus, I am recommending acceptance.", "reviews": [{"review_id": "lU5Rs_wCweN-0", "review_text": "This paper proposes Taking Notes on the Fly , a technique to improve the training efficiency of language-modeling style pretraining . It works by identifying rare words in the pre-training and adding a \u201c note-taking \u201d component to the masked language model which augments these words with an extra \u201c note \u201d embedding at the input layer . The note embedding is constructed from an exponential moving average of mean-pooled contextualized representations of context windows in which that word was previously seen during training . The notes are dropped in fine-tuning . Experiments find that this pre-training method improves fine-tuning results on English NLP tasks in the GLUE benchmark when used in the original BERT pre-training setup . In particular , the model can achieve similar performance to the original BERT model with less than 40 % of the training steps , and similarly for ELECTRA . # # # Strengths This paper is clearly written , the proposed technique is simple , and the results seem strong . It is laudable that the authors give experiments in the appendix to give a sense of hyperparameter sensitivity . The paper has a strong backbone and it seems that the proposed technique or something similar may serve as the basis for solid future work . # # # Weaknesses While the backbone of the paper is strong , I think it could be improved in its head ( motivation ) and legs ( experimental studies ) . First , motivation . While the framing around rare words with the COVID-19 example is interesting , I think it has gaps . The introduction argues that since \u201c COVID-19 \u201d is a rare word , in the course of training the model may lack the necessary signal to predict the masked word \u201c lives. \u201d But isn \u2019 t this fact exactly what should lead the model to improve its embedding of \u201c COVID-19 \u201d ? Because gradients flow into the embeddings both through the softmax layer and the input layer . So while adding to the context may help the model get a foothold with more effective training signal for the masked token , it seems to me that the note could also \u201c explain away \u201d the rare word \u2019 s embedding in the input layer , reducing the learning signal on it . If that \u2019 s the case , then to the extent that TNF works , it would be by the tradeoff between improving the learning signal at the output layer for all words ( and in contextualization ) and degrading it at the input layer ( for rare words ) . As a broader example , see https : //openreview.net/pdf ? id=3Aoft6NWFej . That paper argues for a masking scheme which eliminates easy shortcuts from the prediction problem to increase learning efficiency , whereas this paper argues essentially the opposite\u2014that shortcuts must be added to hard cases in order to facilitate learning . It seems that there may be a line to walk here between a task being too hard to learn from and too easy to be useful . Because it \u2019 s not clear where that line is , I think it \u2019 s not enough to motivate TNF from only one direction . It would be better to also have an explanation of why the note-taking approach does not also make things \u201c too easy. \u201d It \u2019 s not obvious to me how to best make this argument , though results from some of the ablations I will suggest below might help . This brings me to my second point : Ablation experiments . If the motivation is to improve the representations of rare words in the input , then there are even simpler ways to do this . Experiments with simple baselines and ablations are important for figuring out why exactly TNF works . First , if the note is such a useful addition to the word embedding , why not just use it to update the embeddings directly ? At that rate , the method for constructing the note embeddings looks quite similar to word embedding training objectives like word2vec and GloVe . This suggests a critical ablation : * Initialize the word embeddings with word2vec , GloVe , or similar run over the wordpieces in the pretraining corpus . ( Weirdly , I can \u2019 t find an example of this in the literature.It seems like an obvious thing to try . I may have just missed it . ) Indeed , it seems to me that the framing in the paper could just as easily motivate this ( much simpler ) technique than TNF . If TNF outperforms the critical ablation , that implies that its gains are coming from some of the other particulars of the technique , such as 1 ) the extra degree of freedom provided by decoupling the note embeddings from the wordpiece embeddings , or 2 ) the use of contextualized vectors for note embeddings ( rather than the non-contextualized ones in the word embedding objectives ) . To investigate these issues , I would suggest three more ancillary ablations on TNF : * Directly update the rare word \u2019 s embedding with a version of Eq.5 rather than keeping a separate note dictionary . * Update the note embeddings via backprop instead of Eq.5.This would amount to \u201c partially tying \u201d the input and output embeddings , giving more freedom to the input layer , which is partly what \u2019 s happening in TNF . * Pool over non-contextualized instead of contextualized representations in Eq.4.Finally , to address the \u201c too easy \u201d vs \u201c too hard \u201d distinction , two more ablations that might help would be : * Instead of using an exponential moving average for the note embedding update , just use the pooled context vectors from the last instance of the rare word ( i.e. , set $ \\gamma $ to 1 in Eq.5 ) . * instead of using an explicit note dictionary , augment the input context with retrieved text containing the rare word . See TEK-enriched representations ( https : //arxiv.org/pdf/2004.12006.pdf ) for an example of this . For consistency , the exact last-seen context of the rare word could be used . The first will help identify to what extent aggregating over many multiple inputs to get a high quality representation is necessary for TNF . This could then serve as a reference point for the second ablation , which may help determine whether the fixed embedding size and pooling operation helps by creating a bottleneck for the retrieved information and preventing things from getting \u201c too easy. \u201d ( although context window sizes might also be a confound here , that could also be controlled carefully . ) All together I think these ablations would shed a lot of light on why TNF works , and make this work much more useful to researchers who wish to build on it in the future . However , I know I \u2019 ve suggested a lot of crazy experiments here . I would not expect all of this necessarily to be done and I leave it up to the discretion of the researchers which are most important . I am also sure the authors could come up with better ablations than these as well . But my sticking point is the first ablation \u2014 initializing with non-contextualized embeddings \u2014 which I think is critical . And I think it behooves the authors to address some of the lingering questions ( including more written below ) , even if not all of them . # # # Recommendation Unfortunately , reject . The technique is simple and the results seem good , but the paper does not provide empirically-justified insight on why TNF works . I think ablations and investigation into the \u201c why \u201d aspect is the most important part of this kind of model engineering research . # # # More comments & questions I am left with some more questions about how TNF works : * How does the quality of the representations of rare words specifically compare in your approach ? Does it improve the representations of common words and contextualization at the expense of rare words ? While it may be tricky to try to directly assess embedding or contextualization quality , breaking down the MLM perplexities by word frequency ( or presence of rare words in the context ) after removing the note dictionary might be informative . I admit this might also be tricky because I imagine the model would have to be fine-tuned without the notes for a bit before doing such an experiment . But any insight into this issue would be appreciated . * If this method indeed works by more narrowly refocusing the training signal on the masked token than the context tokens , then would you be able to further increase the learning efficiency by oversampling rare words when determining the masks in training ? I am not aware of anyone showing such a thing to work , though I might have missed it . Just a thought . While the pretraining corpus is huge , 100 occurrences still seems like a pretty high threshold for rare words given the justification provided in the paper . Questions : * What do the even rarer words look like ? Are they just a source of noise ? e.g. , because they are components of names or don \u2019 t have clear and consistent semantic content ? * What proportion of contexts contain words appearing less than 100 times ? It seems that the 20 % figure in the paper is meant to apply to your definition of rare words , which appear between 100 and 500 times . * What is the word vocabulary size ? i.e. , how many words appear more than 500 times , and less than 100 ? * Did you do any preliminary experiments with other thresholds ? Would you expect this to work with more common words as well ? Why or why not ? ( This may also relate to the \u201c too easy \u201d vs \u201c too hard \u201d issue . ) On pre-training efficiency results : I think Figs 3a and 3b need to be explicitly qualified a little better . AFAICT , having lower loss here doesn \u2019 t necessarily mean the model ( modulo the note dictionary ) is learning better , because it sees the notes in the input . So we \u2019 re looking at the loss in a different setting than we intend to fine-tune in . It \u2019 s still interesting to see , but I think it 's best to include an explicit caveat . What about training the models for more steps ? Will the trend hold and performance improve overall , or will the gains eventually level off as the representations of rare words get better ? Especially for pretrained models , since they are used as the starting point for many models , it is often worthwhile to train them longer ( as in the RoBERTa paper ) , so it \u2019 s important to understand the usefulness of this method in that regime . # # # Typos etc . : * P.3 : neglectable - > negligible * P.3 : Representation - > Representations ( in BERT acronym ) * P.6 Sec.4.1 : after \u201c MNLI \u201d there is a space missing after the period . * P.6 : \u201c FULL-SENTENCES \u201d would look better & be consistent with Liu et al if it were in small caps . * Please cite the individual dataset creators for the datasets in the GLUE benchmark . Update : upped score from 4 to 5 ; see comment thread . Update again : score further updated from 5 to 6 with GloVe context ablations and perplexity results on sentences with rare words .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We notice that Reviewer 4 's concern about why TNF works firstly lies in why rare word embeddings are poorly trained in BERT and if pre-pretraining word embedding methods can solve this problem . We will start from there to explain our analysis about why TNF works and present our empirical evidence . 1.Regarding the comments `` the gradient flow in BERT can already lead to good rare word embeddings. `` ( Second paragraph in `` Weakness '' ) We respectfully disagree with the reviewer 's claim that `` the gradient flow in BERT can already lead to good rare word embeddings. `` The learning of rare word embeddings has been a challenging problem since the first day of deep learning for NLP . Even when the input embedding and the output embedding ( you mentioned softmax layer ) are tied , the embedding of rare words are still very poorly optimized , see [ 1 ] . There are even recent works suggesting that rare word embeddings act as noise [ 2 ] . We have also listed several related and latest references in the related work section . 2.Regarding why TNF works . We think TNF works because it can benefit the BERT model as a whole by providing a more effective data utilization . The reasons behind are as follows . According to many previous works ( see the related work section ) , rare words ' embeddings are usually poorly optimized . There are even recent works suggesting that rare word embeddings act as noise [ 2 ] . Training a model from noisy inputs is less effective in general ( see [ 3 ] ( Figure 1a ) for more observations about how noisy inputs hurt the optimization and generalization of a machine learning model ) . For language pre-training tasks specifically , the input sequences with noisy rare word embeddings contain less semantically meaningful information for the Transformer to learn , making the whole model training ineffective . Given the above facts , we aim to reduce the noise from rare word embeddings in a sentence , to improve the pre-training of the whole model . Specifically , we achieve this by providing more precise semantics of rare words in a sentence using surrounding contexts from other sentences ( contextualized information saved in notes ) . For the original method , Transformer receives thousands of noisy embeddings like 'Covid-19 ' containing little semantic meanings . Suppose we equip 'Covid-19 ' with a note that contains previous surrounding contextual information such as 'pandemic ' and 'global crisis ' ( which are popular words ) . One could imagine that the sentence 'Covid-19 ( +pandemic + global crisis ) causes thousand of lives ' could have more precise semantics , which makes the training of the Transformer model more effective . 3.Empirical evidences for supporting 2 . To support such a claim , we conduct a new experiment as below : we calculate MLM loss on sentences in the validation set that do not have rare words for both BERT and BERT-TNF at different checkpoints . The total number of sampled sentences that satisfy the condition is roughly 20k . As those testing sentences do n't contain rare words , the note dictionary will not be called , and we can purely compare the performance between the two Transformer models in BERT and BERT-TNF . From the table below , we can see that BERT-TNF 's loss is consistently lower than BERT at all checkpoints , which suggests the entire Transformer model was improved using our method . # Iter 20k 50k 100k 200k 400k 600k 800k 1000k BERT 2.709 2.180 1.947 1.788 1.667 1.602 1.556 1.522 TNF 2.692 2.145 1.902 1.736 1.619 1.558 1.513 1.479 In the submission , we also showed that TNF without notes works well on finetuning downstream tasks , indicating that TNF can provide a better pre-trained model itself ( without notes saved in downstream tasks ) . Moreover , even for sub-tasks with almost no rare words occurring in the training set ( 0.47 % rare word coverage in CoLA ) , BERT-TNF can still outperform BERT on it . All empirical results above support our motivation and indicate that the model as a whole ( including the Transformer layers and token embeddings ) is better pre-trained with TNF . 4.Regarding the comment `` the notes of rare words could 'explain away ' rare word embeddings . '' ( Third paragraph in `` Weakness '' ) We respectfully disagree with Reviewer 4 that the notes of rare words could 'explain away ' rare word embeddings . Given that rare-word embeddings are already poorly-trained and act as noise , their quality has little margins of being hurt . That being said , adding notes for rare words does not 'explain away ' from rare word but 'explains ' the rare word better . It is because as illustrated above , after adding notes , the input sentence with this rare word contains more accurate semantics about the rare word for the model to learn ."}, {"review_id": "lU5Rs_wCweN-1", "review_text": "The paper proposes an external memory architecture . When encountering the rare words ( with a frequency between 100-500 ) , the method will store the average contextualized word embedding of nearby words into a dictionary . Next time it encounters the same rare word , it will retrieve the average embedding and input it into BERT encoder . The experiment results show that given the same number of training steps , adding the external memory improves the MLM loss and significantly improves the results on RTE ( Recognizing Textual Entailment ) dataset , which leads to a slightly better GLUE score . The experiment also shows that keeping the external memory during the fine-tuning stage slightly degrades the performance . Pros : 1.The method is simple and easy to understand 2 . The experimental results on GLUE are quite surprising . It shows that we should take note when training BERT but throw away the note dictionary when fine-tuning the model . Cons : 1.Missing an important citation [ 1 ] 2 . The paper does not well explain the surprising results on GLUE . This is a crucial weakness . The comparison of the MLM loss is not very fair because the proposed method has a large external memory . The benefit of the proposed method relies on the improvement of the average GLUE score . However , Table 2 shows the most of the improvement of GLUE actually comes from the improvement of a single dataset , RTE . Without understanding why it improves RTE , the readers do not know when they want to adopt the proposed method for their downstream applications . Clarity : The text is fluent , but the main story is not well supported by the experiment results . The story is that using an external dictionary could accelerate the training , but the main experiment finding actually says that using an external dictionary can very significantly improve the results on RTE dataset while performing similarly on other datasets in GLUE . Originality : There has been some effort of using an external dictionary to help the training of BERT [ 1 ] , but I am not aware of existing papers that apply the dictionary to only the rare words . I also do not know any other work that shows the external dictionary could improve the GLUE scores . Significance of this work : If the authors could well explain the experimental results on GLUE and justify the explanation using some analysis , this might lead to more important findings . Figure 3c seems to contradict with Table 1 and 2 because in Table 1 and 2 , the GLUE score of BERT ( ours ) is 83.1 but all the points in the BERT curve in Figure 3c is below 83 . Usually , when a study tries to sell its method as a way to accelerate the training , it means the method reaches some performance faster but the method will converge the same performance eventually . However , Figure 3 does not show that they will converge the same value , so selling the method as a way to accelerate the training is weird . Furthermore , I think the lower MLM loss is due to the extra parameters in the note dictionary rather than the note dictionary accelerates the training . It is not surprising that taking notes for rare words could achieve lower loss/perplexity because the note dictionary gives the extra memory capacity [ 1 ] . It is also not surprising that it can achieve better performance on GLEU if using the note dictionary during the fine-tuning stage due to the extra parameters . The really interesting results are that the authors report that the model could very significantly improve the RTE task and mildly improve CoLA without using the note during the fine-tuning stage . Intuitively , the proposed model stores lots of knowledge about the rare words into the note dictionary . Does the fact that the note is not needed in the fine-tuning stage imply that the knowledge about rare words is actually not needed ? Does it mean the RTE or CoLA do not contain many rare words or does it mean the rare words do not affect the decision of BERT and ELECTRA in RTE or CoLA ? Is the reason of improvement that we could store more interactions between popular words in the parameters of BERT itself because the information of rare words has been stored in the note ( maybe you can test this by reporting the MLM loss on the sentences without any rare words ) ? If that is the case , why do we only stably improve RTE and CoLA ? If the authors can show the above hypothesis is true , I think this is a significant contribution because that means this paper provides a way to control what LM should learn when there is a mismatch between MLM training corpus and downstream applications ( e.g. , MLM training corpus contains many rare words but we should ignore the rare words in the downstream applications ) . This paper lacks a good explanation of the above weird result ( in my opinion , the most valuable finding in this paper ) and lacks the analysis that supports the explanation . The main paper says that taking notes improves the tasks with the small datasets the most . The STS-b ( 7k ) and MRPC ( 3.7k ) have smaller training datasets than CoLA ( 8.5k ) . Why are the results of STS-b and MRPC can not be stably improved ? If the authors really want to explain the performance improvement using the training dataset size , the authors can just randomly sample several small subsets of training data from each dataset and show that the GLUE score improves a lot in that setting . In the appendix A.4 , the authors hypothesize that the small proportion of rare words in each dataset of GLUE ( from 0.47 % to 2.31 % ) might be the reason that we can ignore the note dictionary during the fine-tuning stage . This also did not explain why most of the improvement of the GLUE score comes from RTE . Moreover , if the rare words are not important in the testing datasets , why do we want to take notes in the first place ? I will vote for acceptance if the authors could answer these critical questions I raise above strongly . Minor : 1.Although the chance is not high , I think it is possible that parts of MLM improvement could be achieved by simply sampling the sentences containing the rare words more ( This is a minor concern.If you do not have time to finish the experiments for this baseline , you can choose not to do it or compare the results after training fewer steps ) . 2.I guess the dictionary overhead is small but it should be measured and reported because you say the method accelerates the training . [ 1 ] Lample , Guillaume , et al . `` Large memory layers with product keys . '' Advances in Neural Information Processing Systems . 2019 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank Reviewer 1 for the constructive comments and careful reading . From your comments , we realize that we did n't describe our motivation clearly . It leads to some difficulties in reading and further leads to some critical misunderstandings of our work . We first describe why our method works and present more empirical evidence , which we think can largely help better understand our paper . Then we answer each question separately . # # The motivation of TNF First of all , we would like to emphasize that poor rare word embeddings will hurt the training of all model parameters ( such as the Transformer layers ) . The reasons behind are as follows . According to many previous works ( see the related work section ) , rare words ' embeddings are usually poorly optimized . There are even recent works suggesting that rare word embeddings act as noise [ 1 ] . Training a model from noisy inputs is less effective in general ( see [ 2 ] ( Figure 1a ) for more observations about how noisy inputs hurt the optimization and generalization of a machine learning model ) . For language pre-training tasks specifically , the input sequences with noisy rare word embeddings contain less semantically meaningful information for the Transformer to learn , making the whole model training ineffective . Given the above facts , we aim to reduce the noise from rare word embeddings in a sentence , to improve the pre-training of the whole model . Specifically , we achieve this by providing more precise semantics of rare words in a sentence using surrounding contexts from other sentences ( contextualized information saved in notes ) . For the original method , Transformer receives thousands of noisy embeddings like 'Covid-19 ' containing little semantic meanings . Suppose we equip 'Covid-19 ' with a note that contains previous surrounding contextual information such as 'pandemic ' and 'global crisis ' ( which are popular words ) . One could imagine that the sentences 'Covid-19 ( +pandemic + global crisis ) causes thousand of lives ' could have more precise semantics , which makes the training of the Transformer model more effective . We notice that Reviewer 1 has also reached a similar understanding ( `` we could store more interactions between popular words in the parameters of BERT itself '' ) and ask for more supporting empirical evidence . We follow your advice to conduct the experiment below . We calculate MLM loss on sentences in the validation set that do not have rare words for both BERT and BERT-TNF at different checkpoints . The total number of sampled sentences that satisfy the condition is roughly 20k . As those testing sentences do n't contain rare words , the note dictionary will not be called , and we can purely compare the performance between the two Transformer models in BERT and BERT-TNF . From the table below , we can see that BERT-TNF 's loss is consistently lower than BERT at all checkpoints , which suggests the entire Transformer model was improved using our method . # Iter 20k 50k 100k 200k 400k 600k 800k 1000k BERT 2.709 2.180 1.947 1.788 1.667 1.602 1.556 1.522 TNF 2.692 2.145 1.902 1.736 1.619 1.558 1.513 1.479 In the submission , we also showed that TNF without notes works well on finetuning downstream tasks . Moreover , even for sub-tasks with almost no rare words occurring in the training set ( 0.47 % rare word coverage in CoLA ) , BERT-TNF can still outperform BERT on it . All empirical results above support our motivation and indicate that the entire model is better pre-trained with TNF . # # Response to other questions * Regarding the imbalanced performance improvements of TNF Thanks for the careful checking . We think the reason for this imbalanced improvement gain is that sub-tasks in GLUE have different margins for improvements in general . For example , BERT-Large is three times larger than BERT-Base . Its improvements over BERT-Base on RTE and CoLA is more than 3 points . While for the rest of tasks like MRPC and STS-B , their performance gaps are relatively small , e.g. , 0.4 and 0.7 . This indicates that some tasks , like RTE and CoLA , have a larger improvement space when the model is more powerful . While for other tasks , the improvement space may be limited . Similar trends can also be found in other language pre-training methods such as SpanBERT [ 4 ] and ELECTRA [ 5 ] . We understand that imbalanced performance improvements look weird when readers have concerns about why TNF works . While given that it is a common trend of a lot of other language pre-training methods , we think this phenomenon is orthogonal to TNF ."}, {"review_id": "lU5Rs_wCweN-2", "review_text": "* Summary * : This paper proposes a method for improving pretraining convergence speed by augmenting the representations of rare words with the mean-pooled representations from their previously-occuring contexts ( \u201c notes \u201d , stored in a \u201c note dictionary \u201d ) . The method considerably speeds up the convergence of pretraining BERT and ELECTRA , and the authors furthermore show that these models perform better when fine-tuning on downstream GLUE tasks ( likely because the models were undertrained to begin with , so converging faster alleviates this issue ) . * Strengths * : The method is surprisingly simple and empirically quite effective . It 's especially interesting to see that BERT + TNF at 400K steps has better GLUE performance than BERT at 1M steps . * Weaknesses * : the paper does not do a convincing job of arguing that the reasons for the faster convergence comes from better modeling of rare wordsI \u2019 m still not entirely sure why this works so well . Do these rare words commonly show up in GLUE ( and thus , the method is helping because your representations of rare words are better ) ? It seems like TNF is actually improving the representations of more-common words as well . * Recommendation * : 7 Despite the lack of clarity around why exactly this method works so well , the method seems empirically useful and straightforward to apply . I expect that this will be useful to practitioners interested in applying BERT and similar pretraining strategies to new corpora and domains . * Questions * : It \u2019 s a bit unclear to me that note-taking itself is required for this to work well ... in the COVID example presented in the introduction , if you see the sentence \u201c The COVID-19 pandemic is an ongoing global crisis \u201d , isn \u2019 t it possible that MLM itself is sufficient to associate the embedding of \u201c COVID-19 \u201d with \u201c pandemic \u201d and \u201c global crisis \u201d ? Do you have further evidence to show that note-taking is actually improving the representations of rare words , besides GLUE score ( which might not be very indicative , since the rare words might not show up in GLUE ) . The Construction of Note Dictionary : Does 3.47B refer to the number of types or the number of tokens ? Why not define keys with frequencies less than 100 in the dictionary as well ( since you only use types that show up between 100 to 500 times ) ? \u201c It means that to reach the same performance , TNF can save 60 % of pre-training time . If models are trained on 16 NVIDIA Tesla V100 GPUs , BERT-TNF can reach BERT \u2019 s final performance within 2 days while it takes BERT 5.7 days. \u201d : Is the 2 days vs 5.7 days an actual wallclock measurement ? Or , are you hypothesizing this based off of the loss curves ? * Missing / Erroneous Citations : * \u201c It is well-known that in a natural language data corpus , words follow a heavy-tail distribution ( Larson , 2010 ) \u201d This is more-commonly known in the NLP community as Zipf \u2019 s law . Better cites would be : - Zipf G. The Psychobiology of Language . London : Routledge ; 1936 . - Zipf G. Human Behavior and the Principle of Least Effort . New York : Addison-Wesley ; 1949 . * Miscellaneous comments : * \u201c Moreover , completely removing those sentences with rare words is not an applicable choice either since it will significantly reduce the size of the training data and hurt the final model performance. \u201d : I agree that it \u2019 s a bad idea to remove sentences with rare words , but I disagree that the issue is reducing the size of the datayou can always go collect more data and filter it to not include rare words . It \u2019 s more likely that the issue is that removing sentences with rare words would reduce the diversity of the pretraining data , which would be harmful \u201c Our method to solve this problem is inspired by how humans manage information. \u201d : I think the connection to human note-taking is tenuous at best , and would omit it ; the motivation remains clear without this .", "rating": "7: Good paper, accept", "reply_text": "We would like to thank Reviewer 3 \u2019 s support and constructive comments . We notice that similar to Reviewer 1 , Reviewer 3 also has concerns about our motivation and analysis about why TNF works . Therefore , we first describe why our method works and present empirical evidence , which we think can largely help the reviewer better understand our paper . Then we answer each question . * The motivation of TNF First of all , we would like to emphasize that poor rare word embeddings will hurt the training of all model parameters ( such as the Transformer layers ) . The reasons behind are as follows . According to many previous works ( see the related work section ) , rare words ' embeddings are usually poorly optimized . There are even recent works suggesting that rare word embeddings act as noise [ 1 ] . Training a model from noisy inputs is less effective in general ( see [ 2 ] ( Figure 1a ) for more observations about how noisy inputs hurt the optimization and generalization of a machine learning model ) . For language pre-training tasks specifically , the input sequences with noisy rare word embeddings contain less semantically meaningful information for the Transformer to learn , making the whole model training ineffective . Given the above facts , we aim to reduce the noise from rare word embeddings in a sentence , to improve the pre-training of the whole model . Specifically , we achieve this by providing more precise semantics of rare words in a sentence using surrounding contexts from other sentences ( contextualized information saved in notes ) . For the original method , Transformer receives thousands of noisy embeddings like 'Covid-19 ' containing little semantic meanings . Suppose we equip 'Covid-19 ' with a note that contains previous surrounding contextual information such as 'pandemic ' and 'global crisis ' ( which are popular words ) . One could imagine that the sentence ` Covid-19 ( +pandemic + global crisis ) causes thousand of lives ' could have more precise semantics , which makes the training of the Transformer model more effective . We conduct additional experiments to check whether the Transformer model is better trained in BERT-TNF . To show this , we calculate MLM loss on sentences in the validation set that do not have rare words for both BERT and BERT-TNF at different checkpoints . The total number of sampled sentences that satisfy the condition is roughly 20k . As those testing sentences do n't contain rare words , the note dictionary will not be called , and we can purely compare the performance between the two Transformer models in BERT and BERT-TNF . From the table below , we can see that BERT-TNF 's loss is consistently lower than BERT at all checkpoints , which suggests the entire Transformer model was improved using our method . # Iter 20k 50k 100k 200k 400k 600k 800k 1000k BERT 2.709 2.180 1.947 1.788 1.667 1.602 1.556 1.522 TNF 2.692 2.145 1.902 1.736 1.619 1.558 1.513 1.479 In the submission , we also showed that TNF without notes works well on finetuning downstream tasks . Moreover , even for sub-tasks with almost no rare words occurring in the training set ( 0.47 % rare word coverage in CoLA ) , BERT-TNF can still outperform BERT on it . All empirical results above support our motivation and indicate that the entire model is better pre-trained with TNF . * Regarding the frequency range of words in the note dictionary . Thanks for this question . We have tuned this frequency range to build the note dictionary before the submission . We apologize for not having put the results into the paper . Specifically , we have tried setting the range to be 50-100 and 100-1000 . Range 50-100 performs almost identical to the BERT baseline . The reason is that rare words with numbers of occurrences between 50-100 can only cover 0.4 % of the whole training corpora . With such low coverage , very limited training sentences would be enhanced by rare word notes . As a comparison , words whose occurrences between 100-500 cover 2 % of the training corpora , which is shown empirically that could impact the pre-training . We can not further include lower-frequency ( e.g. , < 50 ) words in the dictionary as the number of such words would exponentially increase due to the long-tail problem of language corpora , occupying a massive amount of GPU memory . Range 100-1000 performs similar to our main results from 100-500 , which suggests the rare word range in TNF is robust to some extent . The ' 3.4B ' refers to the total number of words in the 16G training corpora . * Regarding the measurement of acceleration . Yes , the 2 days vs 5.7 days is actual wallclock measurement . - We hope the above responses can address your questions about the design choices . Please let us know if you have further questions ! [ 1 ] Li , Yangming , et al . `` Handling Rare Entities for Neural Sequence Labeling . `` , ( ACL 2020 ) . [ 2 ] Zhang , Chiyuan , et al . `` Understanding deep learning requires rethinking generalization . `` ( ICLR 2017 ) ."}, {"review_id": "lU5Rs_wCweN-3", "review_text": "This work aims at accelerating pre-training by leveraging the contextual embeddings for the rare words . It is argued that the inadequate training of rare words slows down the pre-training . The authors then proposed to keep a moving average of the contextual embeddings for the rare words and use it to augment the input embeddings of the rare words . This technique is applied to BERT and ELECTRA and is shown to improve over the baseline . Strength : 1 . This work proposes a simple approach to accelerate the pre-training , with only a small memory and compute cost during training . The empirical study on BERT and ELECTRA supports the claimed improvements . 2.It provides an interesting view towards the rare words problem that the rare word not only has worse embeddings but also slows down training of the whole model . Weakness : 1 . It is argued that the proposed approach helps with rare words problem . But it will help to add more experiments to see how much more benefit we can get from it . For example , maybe the use of contextual embeddings are actually helpful for all the words or sub-words instead of just the rare words . Specifically , regarding `` we define keys as those words with occurrences between 100 and 500 in the data corpus '' , How are the range 100 to 500 chosen ? Have you tried it on words appearing lower than 100 or higher than 500 ? As mentioned above , it would be interesting to see if this approach can be applied to more words or subwords to get even more gains . 2.Some design choices needs more details or explanations . For example , why does the NoteDictionary use `` words '' instead of `` sub-words '' as keys ? It seems using `` sub-words '' could cover a broader range of sentences with a NoteDictionary of the same size . It will also be easier to use during pre-training , for example , you could use the contextual embeddings to improve the word embeddings of the sub-words directly to avoid having an extra NoteDictionary . Another example is how the window size is chosen , since it seems an important new hyperparameter .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank Reviewer 2 for appreciating our work and providing insightful comments . We appreciate Reviewer 2 's highlight of our view that the poor embeddings of rare words could slow down the training of all model parameters . We find that you have concerns about several design choices of our work . We try to address them below . * Regarding using words or sub-words as keys in the note dictionary This is a good question . Sub-word tokens are indeed easier to be applied as keys for the note dictionary and could potentially cover more of the corpora . However , we find that quite a few rare sub-word tokens , either generated by BPE or in Google 's word piece , do n't have specific understandable semantic meanings to humans . Here we list a few `` 195 @ @ , \u2153 , canter , elids , al , ch , di '' , separated by commas . For such sub-words , their contexts can be very diverse due to their vague semantic meanings . Therefore , saving notes for those sub-words would potentially bring irrelevant context into the current sentence and even add noise into the learning process , which we found not improving the pre-training . While if we save notes for words instead of sub-words , given that most words have concrete semantics , their notes can act as effective auxiliary semantics to enhance the current input sentence . * Regarding using different occurrence range other than ( 100,500 ) Thanks for this question . We have tuned this frequency range to build the note dictionary before the submission . We apologize for not having put the results into the paper . Specifically , we have tried setting the range to be 50-100 and 100-1000 . Range 50-100 performs almost identical to the BERT baseline . The reason is that rare words with numbers of occurrences between 50-100 can only cover 0.3 % of the whole training corpora . With such low coverage , very limited training sentences would be enhanced by rare word notes . As a comparison , words whose occurrences between 100-500 cover 2 % of the training corpora , which is shown that could impact the pre-training empirically . We can not further include lower-frequency ( e.g. , < 50 ) words in the dictionary as the number of such words would exponentially increase due to the long-tail problem of language corpora , occupying a massive amount of GPU memory . Range 100-1000 performs similar to 100-500 , which suggests the rare word range in TNF is robust to some extent . Although TNF targets rare words , we do agree with the reviewer that it is worth trying to apply TNF to common words ( or even all words ) and check the performance . However , we may not be able to finish the experiment within the rebuttal period ( due to the long pre-training and hyper-parameter tuning time ) . We will explore it as future work . * Regarding using different window size Window size is a hyper-parameter in our method . We have provided the ablation study for this hyper-parameter ( and other hyper-parameters ) in the appendix . See Page 12 . The experimental results show that a larger window size usually leads to better performances . We hope the above responses can address your questions about the design choices . Please let us know if you have further questions !"}], "0": {"review_id": "lU5Rs_wCweN-0", "review_text": "This paper proposes Taking Notes on the Fly , a technique to improve the training efficiency of language-modeling style pretraining . It works by identifying rare words in the pre-training and adding a \u201c note-taking \u201d component to the masked language model which augments these words with an extra \u201c note \u201d embedding at the input layer . The note embedding is constructed from an exponential moving average of mean-pooled contextualized representations of context windows in which that word was previously seen during training . The notes are dropped in fine-tuning . Experiments find that this pre-training method improves fine-tuning results on English NLP tasks in the GLUE benchmark when used in the original BERT pre-training setup . In particular , the model can achieve similar performance to the original BERT model with less than 40 % of the training steps , and similarly for ELECTRA . # # # Strengths This paper is clearly written , the proposed technique is simple , and the results seem strong . It is laudable that the authors give experiments in the appendix to give a sense of hyperparameter sensitivity . The paper has a strong backbone and it seems that the proposed technique or something similar may serve as the basis for solid future work . # # # Weaknesses While the backbone of the paper is strong , I think it could be improved in its head ( motivation ) and legs ( experimental studies ) . First , motivation . While the framing around rare words with the COVID-19 example is interesting , I think it has gaps . The introduction argues that since \u201c COVID-19 \u201d is a rare word , in the course of training the model may lack the necessary signal to predict the masked word \u201c lives. \u201d But isn \u2019 t this fact exactly what should lead the model to improve its embedding of \u201c COVID-19 \u201d ? Because gradients flow into the embeddings both through the softmax layer and the input layer . So while adding to the context may help the model get a foothold with more effective training signal for the masked token , it seems to me that the note could also \u201c explain away \u201d the rare word \u2019 s embedding in the input layer , reducing the learning signal on it . If that \u2019 s the case , then to the extent that TNF works , it would be by the tradeoff between improving the learning signal at the output layer for all words ( and in contextualization ) and degrading it at the input layer ( for rare words ) . As a broader example , see https : //openreview.net/pdf ? id=3Aoft6NWFej . That paper argues for a masking scheme which eliminates easy shortcuts from the prediction problem to increase learning efficiency , whereas this paper argues essentially the opposite\u2014that shortcuts must be added to hard cases in order to facilitate learning . It seems that there may be a line to walk here between a task being too hard to learn from and too easy to be useful . Because it \u2019 s not clear where that line is , I think it \u2019 s not enough to motivate TNF from only one direction . It would be better to also have an explanation of why the note-taking approach does not also make things \u201c too easy. \u201d It \u2019 s not obvious to me how to best make this argument , though results from some of the ablations I will suggest below might help . This brings me to my second point : Ablation experiments . If the motivation is to improve the representations of rare words in the input , then there are even simpler ways to do this . Experiments with simple baselines and ablations are important for figuring out why exactly TNF works . First , if the note is such a useful addition to the word embedding , why not just use it to update the embeddings directly ? At that rate , the method for constructing the note embeddings looks quite similar to word embedding training objectives like word2vec and GloVe . This suggests a critical ablation : * Initialize the word embeddings with word2vec , GloVe , or similar run over the wordpieces in the pretraining corpus . ( Weirdly , I can \u2019 t find an example of this in the literature.It seems like an obvious thing to try . I may have just missed it . ) Indeed , it seems to me that the framing in the paper could just as easily motivate this ( much simpler ) technique than TNF . If TNF outperforms the critical ablation , that implies that its gains are coming from some of the other particulars of the technique , such as 1 ) the extra degree of freedom provided by decoupling the note embeddings from the wordpiece embeddings , or 2 ) the use of contextualized vectors for note embeddings ( rather than the non-contextualized ones in the word embedding objectives ) . To investigate these issues , I would suggest three more ancillary ablations on TNF : * Directly update the rare word \u2019 s embedding with a version of Eq.5 rather than keeping a separate note dictionary . * Update the note embeddings via backprop instead of Eq.5.This would amount to \u201c partially tying \u201d the input and output embeddings , giving more freedom to the input layer , which is partly what \u2019 s happening in TNF . * Pool over non-contextualized instead of contextualized representations in Eq.4.Finally , to address the \u201c too easy \u201d vs \u201c too hard \u201d distinction , two more ablations that might help would be : * Instead of using an exponential moving average for the note embedding update , just use the pooled context vectors from the last instance of the rare word ( i.e. , set $ \\gamma $ to 1 in Eq.5 ) . * instead of using an explicit note dictionary , augment the input context with retrieved text containing the rare word . See TEK-enriched representations ( https : //arxiv.org/pdf/2004.12006.pdf ) for an example of this . For consistency , the exact last-seen context of the rare word could be used . The first will help identify to what extent aggregating over many multiple inputs to get a high quality representation is necessary for TNF . This could then serve as a reference point for the second ablation , which may help determine whether the fixed embedding size and pooling operation helps by creating a bottleneck for the retrieved information and preventing things from getting \u201c too easy. \u201d ( although context window sizes might also be a confound here , that could also be controlled carefully . ) All together I think these ablations would shed a lot of light on why TNF works , and make this work much more useful to researchers who wish to build on it in the future . However , I know I \u2019 ve suggested a lot of crazy experiments here . I would not expect all of this necessarily to be done and I leave it up to the discretion of the researchers which are most important . I am also sure the authors could come up with better ablations than these as well . But my sticking point is the first ablation \u2014 initializing with non-contextualized embeddings \u2014 which I think is critical . And I think it behooves the authors to address some of the lingering questions ( including more written below ) , even if not all of them . # # # Recommendation Unfortunately , reject . The technique is simple and the results seem good , but the paper does not provide empirically-justified insight on why TNF works . I think ablations and investigation into the \u201c why \u201d aspect is the most important part of this kind of model engineering research . # # # More comments & questions I am left with some more questions about how TNF works : * How does the quality of the representations of rare words specifically compare in your approach ? Does it improve the representations of common words and contextualization at the expense of rare words ? While it may be tricky to try to directly assess embedding or contextualization quality , breaking down the MLM perplexities by word frequency ( or presence of rare words in the context ) after removing the note dictionary might be informative . I admit this might also be tricky because I imagine the model would have to be fine-tuned without the notes for a bit before doing such an experiment . But any insight into this issue would be appreciated . * If this method indeed works by more narrowly refocusing the training signal on the masked token than the context tokens , then would you be able to further increase the learning efficiency by oversampling rare words when determining the masks in training ? I am not aware of anyone showing such a thing to work , though I might have missed it . Just a thought . While the pretraining corpus is huge , 100 occurrences still seems like a pretty high threshold for rare words given the justification provided in the paper . Questions : * What do the even rarer words look like ? Are they just a source of noise ? e.g. , because they are components of names or don \u2019 t have clear and consistent semantic content ? * What proportion of contexts contain words appearing less than 100 times ? It seems that the 20 % figure in the paper is meant to apply to your definition of rare words , which appear between 100 and 500 times . * What is the word vocabulary size ? i.e. , how many words appear more than 500 times , and less than 100 ? * Did you do any preliminary experiments with other thresholds ? Would you expect this to work with more common words as well ? Why or why not ? ( This may also relate to the \u201c too easy \u201d vs \u201c too hard \u201d issue . ) On pre-training efficiency results : I think Figs 3a and 3b need to be explicitly qualified a little better . AFAICT , having lower loss here doesn \u2019 t necessarily mean the model ( modulo the note dictionary ) is learning better , because it sees the notes in the input . So we \u2019 re looking at the loss in a different setting than we intend to fine-tune in . It \u2019 s still interesting to see , but I think it 's best to include an explicit caveat . What about training the models for more steps ? Will the trend hold and performance improve overall , or will the gains eventually level off as the representations of rare words get better ? Especially for pretrained models , since they are used as the starting point for many models , it is often worthwhile to train them longer ( as in the RoBERTa paper ) , so it \u2019 s important to understand the usefulness of this method in that regime . # # # Typos etc . : * P.3 : neglectable - > negligible * P.3 : Representation - > Representations ( in BERT acronym ) * P.6 Sec.4.1 : after \u201c MNLI \u201d there is a space missing after the period . * P.6 : \u201c FULL-SENTENCES \u201d would look better & be consistent with Liu et al if it were in small caps . * Please cite the individual dataset creators for the datasets in the GLUE benchmark . Update : upped score from 4 to 5 ; see comment thread . Update again : score further updated from 5 to 6 with GloVe context ablations and perplexity results on sentences with rare words .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We notice that Reviewer 4 's concern about why TNF works firstly lies in why rare word embeddings are poorly trained in BERT and if pre-pretraining word embedding methods can solve this problem . We will start from there to explain our analysis about why TNF works and present our empirical evidence . 1.Regarding the comments `` the gradient flow in BERT can already lead to good rare word embeddings. `` ( Second paragraph in `` Weakness '' ) We respectfully disagree with the reviewer 's claim that `` the gradient flow in BERT can already lead to good rare word embeddings. `` The learning of rare word embeddings has been a challenging problem since the first day of deep learning for NLP . Even when the input embedding and the output embedding ( you mentioned softmax layer ) are tied , the embedding of rare words are still very poorly optimized , see [ 1 ] . There are even recent works suggesting that rare word embeddings act as noise [ 2 ] . We have also listed several related and latest references in the related work section . 2.Regarding why TNF works . We think TNF works because it can benefit the BERT model as a whole by providing a more effective data utilization . The reasons behind are as follows . According to many previous works ( see the related work section ) , rare words ' embeddings are usually poorly optimized . There are even recent works suggesting that rare word embeddings act as noise [ 2 ] . Training a model from noisy inputs is less effective in general ( see [ 3 ] ( Figure 1a ) for more observations about how noisy inputs hurt the optimization and generalization of a machine learning model ) . For language pre-training tasks specifically , the input sequences with noisy rare word embeddings contain less semantically meaningful information for the Transformer to learn , making the whole model training ineffective . Given the above facts , we aim to reduce the noise from rare word embeddings in a sentence , to improve the pre-training of the whole model . Specifically , we achieve this by providing more precise semantics of rare words in a sentence using surrounding contexts from other sentences ( contextualized information saved in notes ) . For the original method , Transformer receives thousands of noisy embeddings like 'Covid-19 ' containing little semantic meanings . Suppose we equip 'Covid-19 ' with a note that contains previous surrounding contextual information such as 'pandemic ' and 'global crisis ' ( which are popular words ) . One could imagine that the sentence 'Covid-19 ( +pandemic + global crisis ) causes thousand of lives ' could have more precise semantics , which makes the training of the Transformer model more effective . 3.Empirical evidences for supporting 2 . To support such a claim , we conduct a new experiment as below : we calculate MLM loss on sentences in the validation set that do not have rare words for both BERT and BERT-TNF at different checkpoints . The total number of sampled sentences that satisfy the condition is roughly 20k . As those testing sentences do n't contain rare words , the note dictionary will not be called , and we can purely compare the performance between the two Transformer models in BERT and BERT-TNF . From the table below , we can see that BERT-TNF 's loss is consistently lower than BERT at all checkpoints , which suggests the entire Transformer model was improved using our method . # Iter 20k 50k 100k 200k 400k 600k 800k 1000k BERT 2.709 2.180 1.947 1.788 1.667 1.602 1.556 1.522 TNF 2.692 2.145 1.902 1.736 1.619 1.558 1.513 1.479 In the submission , we also showed that TNF without notes works well on finetuning downstream tasks , indicating that TNF can provide a better pre-trained model itself ( without notes saved in downstream tasks ) . Moreover , even for sub-tasks with almost no rare words occurring in the training set ( 0.47 % rare word coverage in CoLA ) , BERT-TNF can still outperform BERT on it . All empirical results above support our motivation and indicate that the model as a whole ( including the Transformer layers and token embeddings ) is better pre-trained with TNF . 4.Regarding the comment `` the notes of rare words could 'explain away ' rare word embeddings . '' ( Third paragraph in `` Weakness '' ) We respectfully disagree with Reviewer 4 that the notes of rare words could 'explain away ' rare word embeddings . Given that rare-word embeddings are already poorly-trained and act as noise , their quality has little margins of being hurt . That being said , adding notes for rare words does not 'explain away ' from rare word but 'explains ' the rare word better . It is because as illustrated above , after adding notes , the input sentence with this rare word contains more accurate semantics about the rare word for the model to learn ."}, "1": {"review_id": "lU5Rs_wCweN-1", "review_text": "The paper proposes an external memory architecture . When encountering the rare words ( with a frequency between 100-500 ) , the method will store the average contextualized word embedding of nearby words into a dictionary . Next time it encounters the same rare word , it will retrieve the average embedding and input it into BERT encoder . The experiment results show that given the same number of training steps , adding the external memory improves the MLM loss and significantly improves the results on RTE ( Recognizing Textual Entailment ) dataset , which leads to a slightly better GLUE score . The experiment also shows that keeping the external memory during the fine-tuning stage slightly degrades the performance . Pros : 1.The method is simple and easy to understand 2 . The experimental results on GLUE are quite surprising . It shows that we should take note when training BERT but throw away the note dictionary when fine-tuning the model . Cons : 1.Missing an important citation [ 1 ] 2 . The paper does not well explain the surprising results on GLUE . This is a crucial weakness . The comparison of the MLM loss is not very fair because the proposed method has a large external memory . The benefit of the proposed method relies on the improvement of the average GLUE score . However , Table 2 shows the most of the improvement of GLUE actually comes from the improvement of a single dataset , RTE . Without understanding why it improves RTE , the readers do not know when they want to adopt the proposed method for their downstream applications . Clarity : The text is fluent , but the main story is not well supported by the experiment results . The story is that using an external dictionary could accelerate the training , but the main experiment finding actually says that using an external dictionary can very significantly improve the results on RTE dataset while performing similarly on other datasets in GLUE . Originality : There has been some effort of using an external dictionary to help the training of BERT [ 1 ] , but I am not aware of existing papers that apply the dictionary to only the rare words . I also do not know any other work that shows the external dictionary could improve the GLUE scores . Significance of this work : If the authors could well explain the experimental results on GLUE and justify the explanation using some analysis , this might lead to more important findings . Figure 3c seems to contradict with Table 1 and 2 because in Table 1 and 2 , the GLUE score of BERT ( ours ) is 83.1 but all the points in the BERT curve in Figure 3c is below 83 . Usually , when a study tries to sell its method as a way to accelerate the training , it means the method reaches some performance faster but the method will converge the same performance eventually . However , Figure 3 does not show that they will converge the same value , so selling the method as a way to accelerate the training is weird . Furthermore , I think the lower MLM loss is due to the extra parameters in the note dictionary rather than the note dictionary accelerates the training . It is not surprising that taking notes for rare words could achieve lower loss/perplexity because the note dictionary gives the extra memory capacity [ 1 ] . It is also not surprising that it can achieve better performance on GLEU if using the note dictionary during the fine-tuning stage due to the extra parameters . The really interesting results are that the authors report that the model could very significantly improve the RTE task and mildly improve CoLA without using the note during the fine-tuning stage . Intuitively , the proposed model stores lots of knowledge about the rare words into the note dictionary . Does the fact that the note is not needed in the fine-tuning stage imply that the knowledge about rare words is actually not needed ? Does it mean the RTE or CoLA do not contain many rare words or does it mean the rare words do not affect the decision of BERT and ELECTRA in RTE or CoLA ? Is the reason of improvement that we could store more interactions between popular words in the parameters of BERT itself because the information of rare words has been stored in the note ( maybe you can test this by reporting the MLM loss on the sentences without any rare words ) ? If that is the case , why do we only stably improve RTE and CoLA ? If the authors can show the above hypothesis is true , I think this is a significant contribution because that means this paper provides a way to control what LM should learn when there is a mismatch between MLM training corpus and downstream applications ( e.g. , MLM training corpus contains many rare words but we should ignore the rare words in the downstream applications ) . This paper lacks a good explanation of the above weird result ( in my opinion , the most valuable finding in this paper ) and lacks the analysis that supports the explanation . The main paper says that taking notes improves the tasks with the small datasets the most . The STS-b ( 7k ) and MRPC ( 3.7k ) have smaller training datasets than CoLA ( 8.5k ) . Why are the results of STS-b and MRPC can not be stably improved ? If the authors really want to explain the performance improvement using the training dataset size , the authors can just randomly sample several small subsets of training data from each dataset and show that the GLUE score improves a lot in that setting . In the appendix A.4 , the authors hypothesize that the small proportion of rare words in each dataset of GLUE ( from 0.47 % to 2.31 % ) might be the reason that we can ignore the note dictionary during the fine-tuning stage . This also did not explain why most of the improvement of the GLUE score comes from RTE . Moreover , if the rare words are not important in the testing datasets , why do we want to take notes in the first place ? I will vote for acceptance if the authors could answer these critical questions I raise above strongly . Minor : 1.Although the chance is not high , I think it is possible that parts of MLM improvement could be achieved by simply sampling the sentences containing the rare words more ( This is a minor concern.If you do not have time to finish the experiments for this baseline , you can choose not to do it or compare the results after training fewer steps ) . 2.I guess the dictionary overhead is small but it should be measured and reported because you say the method accelerates the training . [ 1 ] Lample , Guillaume , et al . `` Large memory layers with product keys . '' Advances in Neural Information Processing Systems . 2019 .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We would like to thank Reviewer 1 for the constructive comments and careful reading . From your comments , we realize that we did n't describe our motivation clearly . It leads to some difficulties in reading and further leads to some critical misunderstandings of our work . We first describe why our method works and present more empirical evidence , which we think can largely help better understand our paper . Then we answer each question separately . # # The motivation of TNF First of all , we would like to emphasize that poor rare word embeddings will hurt the training of all model parameters ( such as the Transformer layers ) . The reasons behind are as follows . According to many previous works ( see the related work section ) , rare words ' embeddings are usually poorly optimized . There are even recent works suggesting that rare word embeddings act as noise [ 1 ] . Training a model from noisy inputs is less effective in general ( see [ 2 ] ( Figure 1a ) for more observations about how noisy inputs hurt the optimization and generalization of a machine learning model ) . For language pre-training tasks specifically , the input sequences with noisy rare word embeddings contain less semantically meaningful information for the Transformer to learn , making the whole model training ineffective . Given the above facts , we aim to reduce the noise from rare word embeddings in a sentence , to improve the pre-training of the whole model . Specifically , we achieve this by providing more precise semantics of rare words in a sentence using surrounding contexts from other sentences ( contextualized information saved in notes ) . For the original method , Transformer receives thousands of noisy embeddings like 'Covid-19 ' containing little semantic meanings . Suppose we equip 'Covid-19 ' with a note that contains previous surrounding contextual information such as 'pandemic ' and 'global crisis ' ( which are popular words ) . One could imagine that the sentences 'Covid-19 ( +pandemic + global crisis ) causes thousand of lives ' could have more precise semantics , which makes the training of the Transformer model more effective . We notice that Reviewer 1 has also reached a similar understanding ( `` we could store more interactions between popular words in the parameters of BERT itself '' ) and ask for more supporting empirical evidence . We follow your advice to conduct the experiment below . We calculate MLM loss on sentences in the validation set that do not have rare words for both BERT and BERT-TNF at different checkpoints . The total number of sampled sentences that satisfy the condition is roughly 20k . As those testing sentences do n't contain rare words , the note dictionary will not be called , and we can purely compare the performance between the two Transformer models in BERT and BERT-TNF . From the table below , we can see that BERT-TNF 's loss is consistently lower than BERT at all checkpoints , which suggests the entire Transformer model was improved using our method . # Iter 20k 50k 100k 200k 400k 600k 800k 1000k BERT 2.709 2.180 1.947 1.788 1.667 1.602 1.556 1.522 TNF 2.692 2.145 1.902 1.736 1.619 1.558 1.513 1.479 In the submission , we also showed that TNF without notes works well on finetuning downstream tasks . Moreover , even for sub-tasks with almost no rare words occurring in the training set ( 0.47 % rare word coverage in CoLA ) , BERT-TNF can still outperform BERT on it . All empirical results above support our motivation and indicate that the entire model is better pre-trained with TNF . # # Response to other questions * Regarding the imbalanced performance improvements of TNF Thanks for the careful checking . We think the reason for this imbalanced improvement gain is that sub-tasks in GLUE have different margins for improvements in general . For example , BERT-Large is three times larger than BERT-Base . Its improvements over BERT-Base on RTE and CoLA is more than 3 points . While for the rest of tasks like MRPC and STS-B , their performance gaps are relatively small , e.g. , 0.4 and 0.7 . This indicates that some tasks , like RTE and CoLA , have a larger improvement space when the model is more powerful . While for other tasks , the improvement space may be limited . Similar trends can also be found in other language pre-training methods such as SpanBERT [ 4 ] and ELECTRA [ 5 ] . We understand that imbalanced performance improvements look weird when readers have concerns about why TNF works . While given that it is a common trend of a lot of other language pre-training methods , we think this phenomenon is orthogonal to TNF ."}, "2": {"review_id": "lU5Rs_wCweN-2", "review_text": "* Summary * : This paper proposes a method for improving pretraining convergence speed by augmenting the representations of rare words with the mean-pooled representations from their previously-occuring contexts ( \u201c notes \u201d , stored in a \u201c note dictionary \u201d ) . The method considerably speeds up the convergence of pretraining BERT and ELECTRA , and the authors furthermore show that these models perform better when fine-tuning on downstream GLUE tasks ( likely because the models were undertrained to begin with , so converging faster alleviates this issue ) . * Strengths * : The method is surprisingly simple and empirically quite effective . It 's especially interesting to see that BERT + TNF at 400K steps has better GLUE performance than BERT at 1M steps . * Weaknesses * : the paper does not do a convincing job of arguing that the reasons for the faster convergence comes from better modeling of rare wordsI \u2019 m still not entirely sure why this works so well . Do these rare words commonly show up in GLUE ( and thus , the method is helping because your representations of rare words are better ) ? It seems like TNF is actually improving the representations of more-common words as well . * Recommendation * : 7 Despite the lack of clarity around why exactly this method works so well , the method seems empirically useful and straightforward to apply . I expect that this will be useful to practitioners interested in applying BERT and similar pretraining strategies to new corpora and domains . * Questions * : It \u2019 s a bit unclear to me that note-taking itself is required for this to work well ... in the COVID example presented in the introduction , if you see the sentence \u201c The COVID-19 pandemic is an ongoing global crisis \u201d , isn \u2019 t it possible that MLM itself is sufficient to associate the embedding of \u201c COVID-19 \u201d with \u201c pandemic \u201d and \u201c global crisis \u201d ? Do you have further evidence to show that note-taking is actually improving the representations of rare words , besides GLUE score ( which might not be very indicative , since the rare words might not show up in GLUE ) . The Construction of Note Dictionary : Does 3.47B refer to the number of types or the number of tokens ? Why not define keys with frequencies less than 100 in the dictionary as well ( since you only use types that show up between 100 to 500 times ) ? \u201c It means that to reach the same performance , TNF can save 60 % of pre-training time . If models are trained on 16 NVIDIA Tesla V100 GPUs , BERT-TNF can reach BERT \u2019 s final performance within 2 days while it takes BERT 5.7 days. \u201d : Is the 2 days vs 5.7 days an actual wallclock measurement ? Or , are you hypothesizing this based off of the loss curves ? * Missing / Erroneous Citations : * \u201c It is well-known that in a natural language data corpus , words follow a heavy-tail distribution ( Larson , 2010 ) \u201d This is more-commonly known in the NLP community as Zipf \u2019 s law . Better cites would be : - Zipf G. The Psychobiology of Language . London : Routledge ; 1936 . - Zipf G. Human Behavior and the Principle of Least Effort . New York : Addison-Wesley ; 1949 . * Miscellaneous comments : * \u201c Moreover , completely removing those sentences with rare words is not an applicable choice either since it will significantly reduce the size of the training data and hurt the final model performance. \u201d : I agree that it \u2019 s a bad idea to remove sentences with rare words , but I disagree that the issue is reducing the size of the datayou can always go collect more data and filter it to not include rare words . It \u2019 s more likely that the issue is that removing sentences with rare words would reduce the diversity of the pretraining data , which would be harmful \u201c Our method to solve this problem is inspired by how humans manage information. \u201d : I think the connection to human note-taking is tenuous at best , and would omit it ; the motivation remains clear without this .", "rating": "7: Good paper, accept", "reply_text": "We would like to thank Reviewer 3 \u2019 s support and constructive comments . We notice that similar to Reviewer 1 , Reviewer 3 also has concerns about our motivation and analysis about why TNF works . Therefore , we first describe why our method works and present empirical evidence , which we think can largely help the reviewer better understand our paper . Then we answer each question . * The motivation of TNF First of all , we would like to emphasize that poor rare word embeddings will hurt the training of all model parameters ( such as the Transformer layers ) . The reasons behind are as follows . According to many previous works ( see the related work section ) , rare words ' embeddings are usually poorly optimized . There are even recent works suggesting that rare word embeddings act as noise [ 1 ] . Training a model from noisy inputs is less effective in general ( see [ 2 ] ( Figure 1a ) for more observations about how noisy inputs hurt the optimization and generalization of a machine learning model ) . For language pre-training tasks specifically , the input sequences with noisy rare word embeddings contain less semantically meaningful information for the Transformer to learn , making the whole model training ineffective . Given the above facts , we aim to reduce the noise from rare word embeddings in a sentence , to improve the pre-training of the whole model . Specifically , we achieve this by providing more precise semantics of rare words in a sentence using surrounding contexts from other sentences ( contextualized information saved in notes ) . For the original method , Transformer receives thousands of noisy embeddings like 'Covid-19 ' containing little semantic meanings . Suppose we equip 'Covid-19 ' with a note that contains previous surrounding contextual information such as 'pandemic ' and 'global crisis ' ( which are popular words ) . One could imagine that the sentence ` Covid-19 ( +pandemic + global crisis ) causes thousand of lives ' could have more precise semantics , which makes the training of the Transformer model more effective . We conduct additional experiments to check whether the Transformer model is better trained in BERT-TNF . To show this , we calculate MLM loss on sentences in the validation set that do not have rare words for both BERT and BERT-TNF at different checkpoints . The total number of sampled sentences that satisfy the condition is roughly 20k . As those testing sentences do n't contain rare words , the note dictionary will not be called , and we can purely compare the performance between the two Transformer models in BERT and BERT-TNF . From the table below , we can see that BERT-TNF 's loss is consistently lower than BERT at all checkpoints , which suggests the entire Transformer model was improved using our method . # Iter 20k 50k 100k 200k 400k 600k 800k 1000k BERT 2.709 2.180 1.947 1.788 1.667 1.602 1.556 1.522 TNF 2.692 2.145 1.902 1.736 1.619 1.558 1.513 1.479 In the submission , we also showed that TNF without notes works well on finetuning downstream tasks . Moreover , even for sub-tasks with almost no rare words occurring in the training set ( 0.47 % rare word coverage in CoLA ) , BERT-TNF can still outperform BERT on it . All empirical results above support our motivation and indicate that the entire model is better pre-trained with TNF . * Regarding the frequency range of words in the note dictionary . Thanks for this question . We have tuned this frequency range to build the note dictionary before the submission . We apologize for not having put the results into the paper . Specifically , we have tried setting the range to be 50-100 and 100-1000 . Range 50-100 performs almost identical to the BERT baseline . The reason is that rare words with numbers of occurrences between 50-100 can only cover 0.4 % of the whole training corpora . With such low coverage , very limited training sentences would be enhanced by rare word notes . As a comparison , words whose occurrences between 100-500 cover 2 % of the training corpora , which is shown empirically that could impact the pre-training . We can not further include lower-frequency ( e.g. , < 50 ) words in the dictionary as the number of such words would exponentially increase due to the long-tail problem of language corpora , occupying a massive amount of GPU memory . Range 100-1000 performs similar to our main results from 100-500 , which suggests the rare word range in TNF is robust to some extent . The ' 3.4B ' refers to the total number of words in the 16G training corpora . * Regarding the measurement of acceleration . Yes , the 2 days vs 5.7 days is actual wallclock measurement . - We hope the above responses can address your questions about the design choices . Please let us know if you have further questions ! [ 1 ] Li , Yangming , et al . `` Handling Rare Entities for Neural Sequence Labeling . `` , ( ACL 2020 ) . [ 2 ] Zhang , Chiyuan , et al . `` Understanding deep learning requires rethinking generalization . `` ( ICLR 2017 ) ."}, "3": {"review_id": "lU5Rs_wCweN-3", "review_text": "This work aims at accelerating pre-training by leveraging the contextual embeddings for the rare words . It is argued that the inadequate training of rare words slows down the pre-training . The authors then proposed to keep a moving average of the contextual embeddings for the rare words and use it to augment the input embeddings of the rare words . This technique is applied to BERT and ELECTRA and is shown to improve over the baseline . Strength : 1 . This work proposes a simple approach to accelerate the pre-training , with only a small memory and compute cost during training . The empirical study on BERT and ELECTRA supports the claimed improvements . 2.It provides an interesting view towards the rare words problem that the rare word not only has worse embeddings but also slows down training of the whole model . Weakness : 1 . It is argued that the proposed approach helps with rare words problem . But it will help to add more experiments to see how much more benefit we can get from it . For example , maybe the use of contextual embeddings are actually helpful for all the words or sub-words instead of just the rare words . Specifically , regarding `` we define keys as those words with occurrences between 100 and 500 in the data corpus '' , How are the range 100 to 500 chosen ? Have you tried it on words appearing lower than 100 or higher than 500 ? As mentioned above , it would be interesting to see if this approach can be applied to more words or subwords to get even more gains . 2.Some design choices needs more details or explanations . For example , why does the NoteDictionary use `` words '' instead of `` sub-words '' as keys ? It seems using `` sub-words '' could cover a broader range of sentences with a NoteDictionary of the same size . It will also be easier to use during pre-training , for example , you could use the contextual embeddings to improve the word embeddings of the sub-words directly to avoid having an extra NoteDictionary . Another example is how the window size is chosen , since it seems an important new hyperparameter .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank Reviewer 2 for appreciating our work and providing insightful comments . We appreciate Reviewer 2 's highlight of our view that the poor embeddings of rare words could slow down the training of all model parameters . We find that you have concerns about several design choices of our work . We try to address them below . * Regarding using words or sub-words as keys in the note dictionary This is a good question . Sub-word tokens are indeed easier to be applied as keys for the note dictionary and could potentially cover more of the corpora . However , we find that quite a few rare sub-word tokens , either generated by BPE or in Google 's word piece , do n't have specific understandable semantic meanings to humans . Here we list a few `` 195 @ @ , \u2153 , canter , elids , al , ch , di '' , separated by commas . For such sub-words , their contexts can be very diverse due to their vague semantic meanings . Therefore , saving notes for those sub-words would potentially bring irrelevant context into the current sentence and even add noise into the learning process , which we found not improving the pre-training . While if we save notes for words instead of sub-words , given that most words have concrete semantics , their notes can act as effective auxiliary semantics to enhance the current input sentence . * Regarding using different occurrence range other than ( 100,500 ) Thanks for this question . We have tuned this frequency range to build the note dictionary before the submission . We apologize for not having put the results into the paper . Specifically , we have tried setting the range to be 50-100 and 100-1000 . Range 50-100 performs almost identical to the BERT baseline . The reason is that rare words with numbers of occurrences between 50-100 can only cover 0.3 % of the whole training corpora . With such low coverage , very limited training sentences would be enhanced by rare word notes . As a comparison , words whose occurrences between 100-500 cover 2 % of the training corpora , which is shown that could impact the pre-training empirically . We can not further include lower-frequency ( e.g. , < 50 ) words in the dictionary as the number of such words would exponentially increase due to the long-tail problem of language corpora , occupying a massive amount of GPU memory . Range 100-1000 performs similar to 100-500 , which suggests the rare word range in TNF is robust to some extent . Although TNF targets rare words , we do agree with the reviewer that it is worth trying to apply TNF to common words ( or even all words ) and check the performance . However , we may not be able to finish the experiment within the rebuttal period ( due to the long pre-training and hyper-parameter tuning time ) . We will explore it as future work . * Regarding using different window size Window size is a hyper-parameter in our method . We have provided the ablation study for this hyper-parameter ( and other hyper-parameters ) in the appendix . See Page 12 . The experimental results show that a larger window size usually leads to better performances . We hope the above responses can address your questions about the design choices . Please let us know if you have further questions !"}}