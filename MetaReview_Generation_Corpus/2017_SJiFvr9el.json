{"year": "2017", "forum": "SJiFvr9el", "title": "Linear Time Complexity Deep Fourier Scattering Network and Extension to Nonlinear Invariants", "decision": "Reject", "meta_review": "This paper proposes a to use squared modulus nonlinearities within convolutional architectures. Because point-wise squaring can be written as a convolution in the Fourier domain, when doing all the operations in the Fourier this architecture becomes 'dual': convolutions become pointwise operations, and pointwise square-nonlinearities become convolutions. \n The authors study this architecture in the context of scattering transforms and produce a complexity analysis that exploits the previous property, along with preliminary numerical experiments. \n \n All reviewers agreed that, while this is an interesting paper with potentially useful outcomes, its exposition and current experimental section are insufficient. The AC agrees with this assessment, and therefore recommends rejection. \n I agree that the main unanswered question and a 'show-stopper' is the lack of comparisons with its most immediate baseline, scattering using complex modulus, both in terms of accuracy and computational complexity.", "reviews": [{"review_id": "SJiFvr9el-0", "review_text": "I find the general direction of the work is promising but, in my opinion, the paper has three main drawback. While the motivation and overall idea seem very reasonable, the derivation is not convincing mathematically. The experiments are limited and the presentation needs significant improvement. The writing and wording are in general poorly structured to the point that it is sometimes difficult to follow the proposed ideas. The overall organization needs improvement and the connection between sections is not properly established. The paper could be significantly improved by simply re-writing it. I'm not fully convinced by the motivation for the proposed non-linearity (|c|^2), as described on page 5. The authors argue that (Waldspurger, 2016) suggests that higher order nonlinearities might be beneficial for sparsity. But unless I'm missing something, that work seems to suggest that in the general case higher order nonlinearities can be neglected. Could you please comment on this? On the other hand, adding a second order term to the descriptor seems an interesting direction, as long as stability to small variations is preserved (which should be shown experimentally) The experimental section is rather limited. The paper would be stronger with a thorough numerical evaluation. The presented results, in my opinion, do not show convincingly a clear advantage of the proposed method over a standard implementation of the scattering transform. In order to show the merits of the proposed approach, it would be really helpful to directly compare running times and compression rates. Questions: - Can you show empirically that the proposed higher order nonlinearity produces sparser representations than the complex modulus? Other minor issues: - The proof of Section 2.1, should be preceded by a clear statement in the form of a proposition - \"Hadamart\" -> Hadamard - \"Valid set\" -> Validation set - \"nonzeros coefficients\" -> nonzero coefficients - Figure 3 is difficult to understand. Please provide more details. - Figure 5 is supposed to show a comparison to a standard implementation of the Scattering network, but it doesn't seem to be such comparison in that figure. Please explain. - Please verify the references. The first reference states \"MALLAT\". ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments and ideas . The main motivation for the higher order nonlinearity comes from the induced operation inf the Fourier domain which leads to its computation in the Fourier domain very effectively . Also , with the right input signal normalization , this will collapse all the coefficients to 0 at a quicker rate than in the standard scattering transform meaning that one needs a reduced number of layers to capture all the energy in the scattering coefficients . All the writing issues have been changed in the new updated version of the paper . Adding new descriptors as the one proposed is only intended to show that at the final layer , if not all the energy has been captured yet in the scattering coefficients because the signal was very chaotic , then the remaining energy can be encoded in it since by definition it corresponds to the aggregation of all the scattering coefficients from the given layer to `` infinity '' . Please let me know if anything needs more clarification . Regards"}, {"review_id": "SJiFvr9el-1", "review_text": "This work proposes 3 improvements to scattering networks: (1) a non-linearity that allows Fourier-domain computation, (2) compact-supported (in the Fourier domain) representations, and (3) computing additional variance features. The technical contributions seem worthwhile, since #1 and #2 may result in better speed, while #3 may improve accuracy. Unfortunately, they are poorly described and evaluated. If the writing was clear and the evaluation more broad, I would have recommended acceptance since the ideas have merit. One of the biggest faults of the presentation is that many sentences are overly long and full of unnecessary obfuscating language, e.g. the last paragraph of Section 1 (though unfortunately this permeates the whole paper). Likewise, most equations are made unnecessarily complicated. For example, Eq. 5 does not need 4 lines and so many indexes, but just 2: X_0 = x X_l = |X_{l-1} * Psi_l| with the |.| operator being element-wise. Most of the hyperparameter dependencies and indexes are not necessary, as well as the repetition of iterations. The same reasoning can be applied to most Equations 5 to 13. The argument of cardinality (Eq. 14) does not really help prove that variance is more informative. In fact, we could just as easily write that the cardinality of S concatenated with any (!) other quantity is >= the cardinality of S. Another argument from machine learning theory would be better. The authors should strive to make the arguments in the paper less hyperbolic and better substantiated. The claims about finding invariants of any input (Abstract) and fundamental structures (last paragraph of Section 1.2.1) are not really backed up by any math. How can we have any guarantees about singling out, for example, semantically relevant representations? The learning procedures in machine learning give at least some guarantees, while here the feature building seems a bit more heuristic. This does not take away from the main idea, but this part needs to be better researched. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for you consideration . Concerning the equation , they indeed could be 1 line shorter ( not 2 since this would lead to margin problems though ) yet I think for clarification purposes , the extra line presents the fact the at each level a new \\lambda parameter is introduced which would not be clear otherwise . You are indeed right on the fact that whatever other concatenated feature by definition could lead to better discrimination . However the main argument actually comes from the fact that the `` variance '' feature actually contains the aggregated energy of all the scattering coefficients from the current layer till `` infinity '' this means that when used at the last layer , it retrieves the remaining energy that otherwise would required more depth in the scattering network . Please feel free to ask any clarification . Regards"}, {"review_id": "SJiFvr9el-2", "review_text": "Overview: This work seems very promising, but I believe it should be compared with more baselines, and more precisely described and explained, from a signal processing point of view. Pros: New descriptor Fast implementation Cons: a) Lack of rigor b) Too long accordingly to the content c) The computational gain of the algorithm is not clear d) The work is not compared with its most obvious baseline: a scattering transform I will detail each cons. a) Section 1: The author motivates the use of scattering transform because it defines a contraction of the space that relies on geometric features. \" The nonlinearity used in the scattering network is the complex modulus which is piecewise linear.\" A real modulus is piecewise linear. A complex modulus has a shape of bell when interpreting C as R^2. Could you clarify? \\Omega is not introduced. Could you give a precise reference (page+paper) of this claim: \u201cHigher order nonlinearity refers to |x|^2 instead of |x| as it is usually done in the scattering network.\u201d ? Section 2: The motivation of the non-linearity is not clear. First, this non-linearity might potentially increase a lot the variance of your architecture since it depends on higher moments(up to 4). I think a fair analysis would be to compute numerically the normalized variance (e.g. divided by the averaged l^2 norm), as a sanity check. Besides, one should prove that the energy is decreasing. It is not possible to argue that this architecture is similar to a scattering transform which has precise mathematical foundations and those results are required, since the setting is different. Permutation is not a relevant variability. The notion of sparsity during the whole paper sometimes refers to the number of 0 value, either the l^1 norm. Mathematically, a small value, even 10^-1000 is still a non 0 value. Did you compute the graph of the figure 4 on the bird dataset? You might use a ratio instead for clarity. The wavelet that is defined is not a morlet wavelet ( https://en.wikipedia.org/wiki/Morlet_wavelet ). It is close to be a gabor wavelet, and actually it has not a 0 averaging. The measure of the \"sparsity of the filters\" is extremely unclear, is it the ratio between the support of the filter and its size? A good criteria might be for instance to understand the amount of the energy that has been neglected. Besides, a filter with compact support has a bad localisation property. However, this topic is not reached in the paper. For instance, Cauchy wavelets are not used in many applications.(However in mathematical proofs, they often are) In Subsection 3.4 you write that V^(l)(x)=Sum_{j>l} S^(j)(x), but also that you do compute only the 2 first order coefficients because they can be neglected. Besides, you specifically write that adding the variance coefficients improve the representation, whereas they can be obtained as linear combination of S. You claim you apply only one FFT, whereas you apply several FFTs. b) From \"One of the great...\" to \"iof the input.\" section 3.1, the text is not clear. The motivation is that a convolution in space is slower that performing a convolution in the Fourier domain. This whole paragraph can be summarised in few sentences. The section 3.4 is long and corresponds to implementation details. Maybe it could be removed. c) The table 2 seems to indicate that the generation of the filters is one of the bottleneck of your software. Is this really true? One of the main claim of the paper is that sparse filters and staying in Fourier domain speed up the computations. Let us compare the computation of the first order scattering coefficients at scale j with this setting. One has to compare the complexity to compute sum |x*psi_j| and sum |x*psi_j|^2 A downsampling is always performed with wavelets, yet it bears approximation. In a Fourier resolution implementation, one adjust the degree of approximation and speed of computation with the over sampling parameters. Assume a FFT of size N costs C*N*log N, then, Computing \\hat x costs in both case C*N*log N. ST: Then, the signal is multiplied with the fourier transform of the filter, which has a cost of N. In a fourier multi resolution implementation, one periodises the signal by a factor j, such that its size is N/2^j. Then, the FFT has cost C*N/2^j*log(N/2^j), and modulus has cost say N. Then, one applied the averaging that has complexity N/2^j. Here: The signal is multiplied with the fourier transport of the filter that has a support of N/2^j. Then, you convolve it with itself, that has thanks to the padding and the FFT a cost of C*N/2^(j-1)*log(N/2^(j-1))+N/2^(j-1). And you take the 0 frequency. I might be wrong, since this it not my work to do those calculus, but if you claim that your implementation is theoretically faster, you need to prove it, since I do not know any papers where scattering transform claims to be fastly implemented. Here, one sees that the difference is not that significant. Please correct me if I did a mistake. d) It is essential to compare your work with the representation of a scattering transform. First, in term of speed of computation, with a fair implementation (e.g. not MATLAB) and secondly in term of accuracy on the dataset you did use: it is a natural baseline.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments and review . During the whole paper , the sparsity always define the norm of the representation , it goes from the l1 norm to the 0 norm ( number of nonzero coefficients ) depending on the context , either the qualitative aspect of the representation or the the Fourier domain sparsity leading to sparse computation and storage as it is for sparse matrices : exactly 0 coefficients . The whole aim of the paper is to introduce a way to perform computations in a large scale setting by deriving the whole model in the Fourier domain , and not to present a `` better '' scattering transform . The `` variance '' feature presented as explained in my other answers can mostly be seen as a way to retrieve the remaining energy at any given layer and is interesting by its simplicity to be computed still in the Fourier domain , it is not the milestone of the approach though . The wavelet presented is indeed not an analytical wavelet since it does not fulfill mathematically the admissibility criteria . However in practice and as well for you 10^-1000 example , the value of its mean is negligible . In fact , the whole paper brings a approximation/speed approach where one is willing to sacrifice some exact modeling and computation in order to be able to compute all the transformation of a deep scattering network on large dataset . In addition , for the 10^-1000 example , this would be effectively 0 to a computer which makes it irrelevant in this context of sparse approximation . We indeed apply multiple FFT however they are all on a support of much smaller size than the input signal ( they are applied only on the support of the wavelet for each wavelet which by definition are extremely small compared to the input signal , making the M log ( M ) complexity smaller than N the size of the input signal . Simply by the fact that we do not have to inverse the Fourier transform to compute the |.| nonlinearity and apply the Fourier transform again to convolve at the next layer means that the presented approach is faster . Also do not forget than in the scattering transform the number of FFT you have to perform for this grows exponentially with each of the added \\lambda_l for each layer . Also , the presented computation gains seem to be more important than you suggest . In the case of large scale problems , the presented gains can help save tremendous days of computations , for only one signal of course all of this is irrelevant . The variance coefficients can indeed be retrieve through a linear computation of the following S as mentioned in the paper yet it means that you need many layers of the scattering network to encoded all of them . Whereas if you stop at layer L you can still retrieve the sum of all the next S by computing this feature , which helps to reduce the number of required layer to capture all the signal energy . You complexity computation is correct . And in fact , our approach has an asymptotic time complexity equivalent to the scattering transform which is N log ( N ) . However , in practice , the observed gain is much more important and this is mainly due to the fact that you do not just have less computation but more efficient memory access . Whatever operation applied in our setting access contiguous memory blocks whereas the periodization of the signal after element wise multiplication with the wavelet is quite harmful in practice , just as an example . So we indeed do not reduce the asymptotic complexity yet the practical results are more than encouraging for large scale datasets . Finally , when looking at the gains layer per layer , the most gain is obtained at the first layers . Finally , in Table 2 we show that the construction of the filter-bank grows linearly with the signal size which is thus satisfactory especially since ( and as for the scattering network ) usually it is created only one time and then applied on all the signals . Please feel free to ask any clarification . Also what has not been mentioned here has been changed in the new uploaded version of the paper ! Regards"}], "0": {"review_id": "SJiFvr9el-0", "review_text": "I find the general direction of the work is promising but, in my opinion, the paper has three main drawback. While the motivation and overall idea seem very reasonable, the derivation is not convincing mathematically. The experiments are limited and the presentation needs significant improvement. The writing and wording are in general poorly structured to the point that it is sometimes difficult to follow the proposed ideas. The overall organization needs improvement and the connection between sections is not properly established. The paper could be significantly improved by simply re-writing it. I'm not fully convinced by the motivation for the proposed non-linearity (|c|^2), as described on page 5. The authors argue that (Waldspurger, 2016) suggests that higher order nonlinearities might be beneficial for sparsity. But unless I'm missing something, that work seems to suggest that in the general case higher order nonlinearities can be neglected. Could you please comment on this? On the other hand, adding a second order term to the descriptor seems an interesting direction, as long as stability to small variations is preserved (which should be shown experimentally) The experimental section is rather limited. The paper would be stronger with a thorough numerical evaluation. The presented results, in my opinion, do not show convincingly a clear advantage of the proposed method over a standard implementation of the scattering transform. In order to show the merits of the proposed approach, it would be really helpful to directly compare running times and compression rates. Questions: - Can you show empirically that the proposed higher order nonlinearity produces sparser representations than the complex modulus? Other minor issues: - The proof of Section 2.1, should be preceded by a clear statement in the form of a proposition - \"Hadamart\" -> Hadamard - \"Valid set\" -> Validation set - \"nonzeros coefficients\" -> nonzero coefficients - Figure 3 is difficult to understand. Please provide more details. - Figure 5 is supposed to show a comparison to a standard implementation of the Scattering network, but it doesn't seem to be such comparison in that figure. Please explain. - Please verify the references. The first reference states \"MALLAT\". ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments and ideas . The main motivation for the higher order nonlinearity comes from the induced operation inf the Fourier domain which leads to its computation in the Fourier domain very effectively . Also , with the right input signal normalization , this will collapse all the coefficients to 0 at a quicker rate than in the standard scattering transform meaning that one needs a reduced number of layers to capture all the energy in the scattering coefficients . All the writing issues have been changed in the new updated version of the paper . Adding new descriptors as the one proposed is only intended to show that at the final layer , if not all the energy has been captured yet in the scattering coefficients because the signal was very chaotic , then the remaining energy can be encoded in it since by definition it corresponds to the aggregation of all the scattering coefficients from the given layer to `` infinity '' . Please let me know if anything needs more clarification . Regards"}, "1": {"review_id": "SJiFvr9el-1", "review_text": "This work proposes 3 improvements to scattering networks: (1) a non-linearity that allows Fourier-domain computation, (2) compact-supported (in the Fourier domain) representations, and (3) computing additional variance features. The technical contributions seem worthwhile, since #1 and #2 may result in better speed, while #3 may improve accuracy. Unfortunately, they are poorly described and evaluated. If the writing was clear and the evaluation more broad, I would have recommended acceptance since the ideas have merit. One of the biggest faults of the presentation is that many sentences are overly long and full of unnecessary obfuscating language, e.g. the last paragraph of Section 1 (though unfortunately this permeates the whole paper). Likewise, most equations are made unnecessarily complicated. For example, Eq. 5 does not need 4 lines and so many indexes, but just 2: X_0 = x X_l = |X_{l-1} * Psi_l| with the |.| operator being element-wise. Most of the hyperparameter dependencies and indexes are not necessary, as well as the repetition of iterations. The same reasoning can be applied to most Equations 5 to 13. The argument of cardinality (Eq. 14) does not really help prove that variance is more informative. In fact, we could just as easily write that the cardinality of S concatenated with any (!) other quantity is >= the cardinality of S. Another argument from machine learning theory would be better. The authors should strive to make the arguments in the paper less hyperbolic and better substantiated. The claims about finding invariants of any input (Abstract) and fundamental structures (last paragraph of Section 1.2.1) are not really backed up by any math. How can we have any guarantees about singling out, for example, semantically relevant representations? The learning procedures in machine learning give at least some guarantees, while here the feature building seems a bit more heuristic. This does not take away from the main idea, but this part needs to be better researched. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for you consideration . Concerning the equation , they indeed could be 1 line shorter ( not 2 since this would lead to margin problems though ) yet I think for clarification purposes , the extra line presents the fact the at each level a new \\lambda parameter is introduced which would not be clear otherwise . You are indeed right on the fact that whatever other concatenated feature by definition could lead to better discrimination . However the main argument actually comes from the fact that the `` variance '' feature actually contains the aggregated energy of all the scattering coefficients from the current layer till `` infinity '' this means that when used at the last layer , it retrieves the remaining energy that otherwise would required more depth in the scattering network . Please feel free to ask any clarification . Regards"}, "2": {"review_id": "SJiFvr9el-2", "review_text": "Overview: This work seems very promising, but I believe it should be compared with more baselines, and more precisely described and explained, from a signal processing point of view. Pros: New descriptor Fast implementation Cons: a) Lack of rigor b) Too long accordingly to the content c) The computational gain of the algorithm is not clear d) The work is not compared with its most obvious baseline: a scattering transform I will detail each cons. a) Section 1: The author motivates the use of scattering transform because it defines a contraction of the space that relies on geometric features. \" The nonlinearity used in the scattering network is the complex modulus which is piecewise linear.\" A real modulus is piecewise linear. A complex modulus has a shape of bell when interpreting C as R^2. Could you clarify? \\Omega is not introduced. Could you give a precise reference (page+paper) of this claim: \u201cHigher order nonlinearity refers to |x|^2 instead of |x| as it is usually done in the scattering network.\u201d ? Section 2: The motivation of the non-linearity is not clear. First, this non-linearity might potentially increase a lot the variance of your architecture since it depends on higher moments(up to 4). I think a fair analysis would be to compute numerically the normalized variance (e.g. divided by the averaged l^2 norm), as a sanity check. Besides, one should prove that the energy is decreasing. It is not possible to argue that this architecture is similar to a scattering transform which has precise mathematical foundations and those results are required, since the setting is different. Permutation is not a relevant variability. The notion of sparsity during the whole paper sometimes refers to the number of 0 value, either the l^1 norm. Mathematically, a small value, even 10^-1000 is still a non 0 value. Did you compute the graph of the figure 4 on the bird dataset? You might use a ratio instead for clarity. The wavelet that is defined is not a morlet wavelet ( https://en.wikipedia.org/wiki/Morlet_wavelet ). It is close to be a gabor wavelet, and actually it has not a 0 averaging. The measure of the \"sparsity of the filters\" is extremely unclear, is it the ratio between the support of the filter and its size? A good criteria might be for instance to understand the amount of the energy that has been neglected. Besides, a filter with compact support has a bad localisation property. However, this topic is not reached in the paper. For instance, Cauchy wavelets are not used in many applications.(However in mathematical proofs, they often are) In Subsection 3.4 you write that V^(l)(x)=Sum_{j>l} S^(j)(x), but also that you do compute only the 2 first order coefficients because they can be neglected. Besides, you specifically write that adding the variance coefficients improve the representation, whereas they can be obtained as linear combination of S. You claim you apply only one FFT, whereas you apply several FFTs. b) From \"One of the great...\" to \"iof the input.\" section 3.1, the text is not clear. The motivation is that a convolution in space is slower that performing a convolution in the Fourier domain. This whole paragraph can be summarised in few sentences. The section 3.4 is long and corresponds to implementation details. Maybe it could be removed. c) The table 2 seems to indicate that the generation of the filters is one of the bottleneck of your software. Is this really true? One of the main claim of the paper is that sparse filters and staying in Fourier domain speed up the computations. Let us compare the computation of the first order scattering coefficients at scale j with this setting. One has to compare the complexity to compute sum |x*psi_j| and sum |x*psi_j|^2 A downsampling is always performed with wavelets, yet it bears approximation. In a Fourier resolution implementation, one adjust the degree of approximation and speed of computation with the over sampling parameters. Assume a FFT of size N costs C*N*log N, then, Computing \\hat x costs in both case C*N*log N. ST: Then, the signal is multiplied with the fourier transform of the filter, which has a cost of N. In a fourier multi resolution implementation, one periodises the signal by a factor j, such that its size is N/2^j. Then, the FFT has cost C*N/2^j*log(N/2^j), and modulus has cost say N. Then, one applied the averaging that has complexity N/2^j. Here: The signal is multiplied with the fourier transport of the filter that has a support of N/2^j. Then, you convolve it with itself, that has thanks to the padding and the FFT a cost of C*N/2^(j-1)*log(N/2^(j-1))+N/2^(j-1). And you take the 0 frequency. I might be wrong, since this it not my work to do those calculus, but if you claim that your implementation is theoretically faster, you need to prove it, since I do not know any papers where scattering transform claims to be fastly implemented. Here, one sees that the difference is not that significant. Please correct me if I did a mistake. d) It is essential to compare your work with the representation of a scattering transform. First, in term of speed of computation, with a fair implementation (e.g. not MATLAB) and secondly in term of accuracy on the dataset you did use: it is a natural baseline.", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your comments and review . During the whole paper , the sparsity always define the norm of the representation , it goes from the l1 norm to the 0 norm ( number of nonzero coefficients ) depending on the context , either the qualitative aspect of the representation or the the Fourier domain sparsity leading to sparse computation and storage as it is for sparse matrices : exactly 0 coefficients . The whole aim of the paper is to introduce a way to perform computations in a large scale setting by deriving the whole model in the Fourier domain , and not to present a `` better '' scattering transform . The `` variance '' feature presented as explained in my other answers can mostly be seen as a way to retrieve the remaining energy at any given layer and is interesting by its simplicity to be computed still in the Fourier domain , it is not the milestone of the approach though . The wavelet presented is indeed not an analytical wavelet since it does not fulfill mathematically the admissibility criteria . However in practice and as well for you 10^-1000 example , the value of its mean is negligible . In fact , the whole paper brings a approximation/speed approach where one is willing to sacrifice some exact modeling and computation in order to be able to compute all the transformation of a deep scattering network on large dataset . In addition , for the 10^-1000 example , this would be effectively 0 to a computer which makes it irrelevant in this context of sparse approximation . We indeed apply multiple FFT however they are all on a support of much smaller size than the input signal ( they are applied only on the support of the wavelet for each wavelet which by definition are extremely small compared to the input signal , making the M log ( M ) complexity smaller than N the size of the input signal . Simply by the fact that we do not have to inverse the Fourier transform to compute the |.| nonlinearity and apply the Fourier transform again to convolve at the next layer means that the presented approach is faster . Also do not forget than in the scattering transform the number of FFT you have to perform for this grows exponentially with each of the added \\lambda_l for each layer . Also , the presented computation gains seem to be more important than you suggest . In the case of large scale problems , the presented gains can help save tremendous days of computations , for only one signal of course all of this is irrelevant . The variance coefficients can indeed be retrieve through a linear computation of the following S as mentioned in the paper yet it means that you need many layers of the scattering network to encoded all of them . Whereas if you stop at layer L you can still retrieve the sum of all the next S by computing this feature , which helps to reduce the number of required layer to capture all the signal energy . You complexity computation is correct . And in fact , our approach has an asymptotic time complexity equivalent to the scattering transform which is N log ( N ) . However , in practice , the observed gain is much more important and this is mainly due to the fact that you do not just have less computation but more efficient memory access . Whatever operation applied in our setting access contiguous memory blocks whereas the periodization of the signal after element wise multiplication with the wavelet is quite harmful in practice , just as an example . So we indeed do not reduce the asymptotic complexity yet the practical results are more than encouraging for large scale datasets . Finally , when looking at the gains layer per layer , the most gain is obtained at the first layers . Finally , in Table 2 we show that the construction of the filter-bank grows linearly with the signal size which is thus satisfactory especially since ( and as for the scattering network ) usually it is created only one time and then applied on all the signals . Please feel free to ask any clarification . Also what has not been mentioned here has been changed in the new uploaded version of the paper ! Regards"}}