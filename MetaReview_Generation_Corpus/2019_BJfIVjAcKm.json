{"year": "2019", "forum": "BJfIVjAcKm", "title": "Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability", "decision": "Accept (Poster)", "meta_review": "This paper introduced a concept called ReLU stability to motivate regularization and enable fast verification. Most of the analysis was presented empirically on two simple datasets and with low-performing models. I feel theoretical analysis and more comprehensive and realistic empirical studies would make the paper stronger. In general, the contribution of this paper is original and interesting. \n", "reviews": [{"review_id": "BJfIVjAcKm-0", "review_text": "The paper presents several ways to regularize plain ReLU networks to optimize 3 things - the adversarial robustness, defined as the fraction of examples for which adversarial perturbation exists - the provable adversarial robustness, defined as the fraction of examples for which some method can show that there exists no adversarial example within a certain time budget - the verification speed, i.e. the amount of time it takes some method to verify whether there is an adversarial example or not Overall, the ideas are sound and the analysis is solid. My main concern is the comparison between the authors method and the 'certification' methods, both conceptually and regarding performance. The authors note that their method falls under 'verification', whereas many competing methods fall under 'certification'. They point to two advantages of verification over certification: (1) the ability to provide true negatives, i.e. prove that an adversarial example exists when it does, and (2) certification requires that 'models must be trained and optimized for a specific certification method'. However, neither argument convinces me regarding the utility of the authors method. Regarding (2): The authors method also requires training the network in a specific way (with RS loss), and it is only compatible with verifiers that care about ReLU stability. Regarding (1): It is not clear that this would be helpful at all. Is it really that much better if method A has 80% proven robustness and 20% proven non-robustness versus method B that has 80% proven robustness and 20% unknown? One could make the case that method B is actually even better. So overall, I think one has to compare the authors method and the certification methods head-to-head. And in table 3, where this is done, Dvijotham comes out on top 2 out of 2 times and Wong comes out on top 2 out of 4 times. That does not seem convincing. Also, what about the performance numbers form other papers discussed in section 2? ------- Other issues: At first glance, the fact that the paper only deals with (small) plain ReLU networks seems to be a huge downside. While I'm not familiar with the verification / certification literature, from reading the paper, I suspect that all the other verification / certification methods also only deal with that or highly similar architectures. However, I will defer to the other reviewers if this is not the case. To expand upon my comment above, I think the paper should discuss true adversarial accuracy on top of provable adversarial robustness. Looking at table 1, for instance, for rows 2, 3 and 4, it seems that the verifier used much less than 120 seconds on average. Does that mean the verifier finished for all test examples? And wouldn't that mean that the verifier determined for each test example exactly whether an adversarial example existed or not? In that case, I would write \"true adversarial accuracy\" instead of \"provable adversarial accuracy\" as column header. If the verifiers did not finish, I would include in the paper for how many examples the result was \"adverarial example exists\" and for how many the result was \"timeout\". I would also include that information in table 3, and I would also include proving / certification times there. Based on the paper, I'm not quite sure whether the idea of training with L1 regularization and/or small weight pruning and/or ReLU pruning for the purpose of improving robustness / verifiability was an original idea of this paper. In either case, this should be made clear. Also, the paper seems to use networks with adversarial training, small weight pruning, L1 and ReLU pruning as its baseline in most cases (all figures except table 1). If some of these techniques are original contributions, this might not be an appropriate baseline to use, even if it is a strong baselines. Why are most experiments presented outside of the \"experiments\" section? This seems to be bad presentation. I would include all test set accuracy values instead of writing \"its almost as high\". Also, in table 3, it appears as if using RS loss DOES in fact reduce test error significantly, at least for CIFAR. Why is that? While, again, I'm not familiar with the background work on verification / certification, it appears to me from reading this paper that all known verification algorithms perform terribly and are restricted to a narrow range of network architectures. If that is the case, one has to wonder whether that line of research should be encouraged to continue. -------- Minor issues: - \"our focus will be on the most common architecture for state-of-the-art models: k-layer fully-connected feed-forward DNN classifiers\" Citation needed. Otherwise, I would suggest removing this statement. - \"such models can be viewed as a function f(.,W)\" - you also need to include the bias in the formula I think - \"convolutional layers can be represented as fully-connected layers\". I think what you mean is \"convolutional layers can be represented as matrix multiplication\" - could you make the difference between co-design and co-training more clear? - The paper could include in the appendix a section outlining the verification method of Tjeng", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their detailed comments ; they will be helpful in revising our manuscript . We appreciate the point about the importance of \u201c verification \u201d compared to \u201c certification \u201d as it is indeed a great question ! Now , one should note that , in this context , formal verification is the ultimate end goal we strive for ; certification is just a fast \u201c shortcut \u201d that can get us closer to this goal , at the expense of sacrificing part of the robustness guarantee . Of course , if certification already gives us a satisfactory level of robustness , this tradeoff can be beneficial . However , it is unclear if the current state-of-the-art ( SOTA ) certification methods like Wong et . al 2018 [ 1 ] are at this point able to deliver such robustness once we move beyond the smallest perturbation sizes ( see the MNIST , eps=0.3 case in Table 3 of our manuscript ) . Additionally , when applied to neural networks not specifically trained for that certification method , they give vacuous bounds [ 2 ] . Thus , as of now , using and improving formal verification methods should still be our focus . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Now , we would like to address your high-level comments in more detail , followed by your specific comments : > Regarding ( 2 ) : The authors method also requires training the network in a specific way ( with RS loss ) , and it is only compatible with verifiers that care about ReLU stability . We view RS Loss as a regularization method , similar to L1 regularization . It can be added to any training procedure , and it is designed for a natural goal - encouraging stable ReLUs . Even if one could , in principle , imagine a verification approach that does not benefit from the natural goal of ReLU stability , all effective verification methods that we are aware of , either falling under the broader class of SMT-based verifiers [ 3,4 ] or MIP-based verifiers [ 5,6,7 ] , can benefit from ReLU stability . For example , [ 3 ] states that \u201c When tighter bounds are derived for ReLU variables , these variables can sometimes be eliminated , i.e. , fixed to the active or inactive state , without splitting. \u201d [ 6 ] writes : \u201c we conjecture that the large increase of binary variables in the problem caused by the binary constraints on the input creates the large performance gap between the Reuters dataset and MNIST. \u201d ( the Reuters dataset had more binary variables and took much longer to verify ) > Regarding ( 1 ) : While the authors method can provide true negatives , they are not discussed in the paper at all . > I would include in the paper for how many examples the result was `` adverarial example exists '' and for how many the result was `` timeout '' . We do discuss upper bounds for the true robustness of all of our models , as well as the number of timeouts , in Appendix E. We appreciate that you bring up this point though , and we will work to point readers to these relevant details in Appendix E from the main body of the paper when revising . In Appendix E , the column labeled \u201c Verifier Upper Bound \u201d is simply 100 % minus the number of true negatives ( \u201c adversarial examples exists \u201d cases ) - it describes the maximum possible value for the true adversarial accuracy . The difference between the upper bound ( \u201c Verifier Upper Bound \u201d ) and the lower bound ( \u201c Provable Adversarial Accuracy \u201d ) on the true adversarial accuracy equals how many examples reached their \u201c timeout , \u201d as we can not determine which category they belong to . > At first glance , the fact that the paper only deals with ( small ) plain ReLU networks seems to be a huge downside . > While , again , I 'm not familiar with the background work on verification / certification , it appears to me from reading this paper that all known verification algorithms perform terribly and are restricted to a narrow range of network architectures . Indeed , we purposely chose our architectures to match those of prior works in certification literature for the fairest possible comparison . We agree that expanding beyond our current capabilities for verification and certification is an important direction for further research . Our contribution in this manuscript is to show that training for ease-of-verification via inducing weight sparsity and ReLU stability can help scale verification . Prior to our work , verification methods struggled for neural networks with just a few hundreds ReLUs in total . Using our methods , networks as large as our \u201c large \u201d convolutional CIFAR network , which have over 60000 ReLUs ( most of which can be made stable ) , can be verified ."}, {"review_id": "BJfIVjAcKm-1", "review_text": "Training for Faster Adversarial Robustness verification via inducing RELU stability As I am familiar yet not an expert on adversarial training and robustess, my review will focus mainly on the overall soundness of the manuscript. I also only went superficially into the quantitative results. Summary: The authors are interested in the problem of verifying neural networks models trained to be robust against adversarial attacks. The focus is on networks with relu activations and adversarial perturbations within an epsilon l1-ball around each input, and the verification problem consists in proving the network performs as intended for all possible perturbations (infinitely many) The review on verification is clear. Elements that affect verification time are introduced and well explained in main text or appendix from both intuitive and theoretical perspective: l1 penalty, weight pruning, relu stability. These can be summ\\arized as : you want few neurons, and you want them to operate in the same regime for all inputs, both to avoid branching. Relu stability is apparently a new concept and the proposed regularization approximately enforces relu stability. The approximation [itself using the novel improved interval arithmetic] based bounds on unit activations propagated through the network seems not to scale well with depths (more units are mis-labelled as relu unstable, hence wrongly regularized if I understand correctly). The authors acknowledge and document this fact but I would like to hear more discussion on this feature and on the trade-off that still make this approach worthwhile for deeper networks. This regularization does not help performance but only paves the way for a faster verification, for this reason the term co-design is used. The rest of the manuscript is a thorough empirical analysis of the effect of the penalties/regularizations on the network and ultimately on the verification time, keeping an eye on not deteriorating the performance of the network. How much regularization can be added seems to be indeed an empirical question since networks are \u2018over-parametrized in the first place\u2019 with no clear way to a priori quantify task or model complexity. The devil is in the details and in practice implementation seems not straightforward with a complex optimization with varying learning rates and different regularizations applied at different time along the way. But this seems to be the case for most deep learning paper. The authors claim and provide evidence to be able to verify network well beyond the scope of what was achievable before due to the obtained speed-ups, which is a notable feature. Overall, this manuscript is well structured, thorough and pleasant to read, and I recommend it to be accepted for publication at ICLR ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for their helpful comments . We are glad you found the paper pleasant to read ! We agree that labeling unstable ReLUs properly is an important aspect of our technique . The upper and lower bounds we compute on each ReLU are conservative - thus , every unstable ReLU will always be correctly labeled as unstable , while stable ReLUs can be labeled as either stable or unstable . Importantly , every unstable ReLU is correctly labeled and penalized by the RS Loss we propose . The tradeoff is that stable ReLUs mislabeled as unstable will also be penalized , which can be an unnecessary regularization of the model . We showed empirically that we could achieve the following two objectives at once using RS Loss 1 ) Reduce the number of ReLUs labeled as unstable , which is an upper bound on the true number of unstable ReLUs 2 ) Achieve similarly good test set accuracy and PGD-adversarial accuracy as a model trained without RS Loss For example , when comparing the Control and \u201c +RS \u201d networks for MNIST and eps=0.1 , we decreased the average number of ReLUs labeled as unstable ( using bounds from Improved Interval Arithmetic ) from 290.5 to 105.4 with just a 0.26 % loss in test set accuracy ( cf.Appendix C.3 , Appendix E ) . The same trends hold for deeper networks ( we only showed results for a 3-layer network in Appendix C.3 , but we will include details about a 6-layer network in the revision ) . For the deeper 6-layer \u201c +RS \u201d network for MNIST and eps=0.1 that we presented , it had a test set accuracy of 98.93 % and just 184.6 ReLUs labeled as unstable at the end of training [ * ] . Training the exact same network without the RS Loss penalty had a slightly higher test set accuracy ( 99.09 % ) but also had far more ReLUs labeled as unstable ( 1028.3 ) . Thus , we could effectively reduce the number of ReLUs labeled as unstable without significantly degrading test set accuracy . We will clarify these points better in Appendix C where we discuss ReLU bounds when revising the paper . [ * ] Edit made : Previously , we wrote test set accuracy of 98.95 % and 150.3 ReLUs labeled as unstable , which matches Appendix E. Those are the correct numbers after post-processing ( weight pruning and ReLU pruning ) is applied , whereas the updated numbers we write here are before post-processing , to match the fact that the 99.09 % /1028.3 unstable ReLUs numbers are also computed before post-processing ."}, {"review_id": "BJfIVjAcKm-2", "review_text": "This paper proposes methods to train robust neural networks that can also be verified faster. Specifically, it uses pruning methods to encourage weight sparsity and uses regularization to encourage ReLU stability. Both weight sparsity and ReLU stability reduces time needed for verification. The verified robust accuracy reported in this paper is close to previous SOTA certified robust accuracy, although not beating SOTA. The paper is clearly written and easy to follow. The reviewer is familiar with literatures on certifiable robust network literature, but not familiar with verification literature. To the best knowledge of the reviewer, the proposed method is well motivated and novel, and provides a scalable method for verifying (instead of lower bounding) robustness. Other comments: I think there should be some discussions on applicability on different robustness measures. The paper focus on L_\\infty norm bounded attack, is this method extendable to other norms? Re: robust accuracy comparison, I found some previous SOTA results missing from Table 3. For example, Mirman et al., 2018 (Appendix Table 6) reached 82% (higher than 80.68% achieved in this paper) provable robust accuracy for MNIST eps=0.3 case. and this is not reported in Table 3. The CIFAR10 results in Mirman et al., 2018 is also better than the best SOTA accuracy in Table 3. Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably robust neural networks. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3575\u20133583, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/mirman18b.html. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their useful comments . Your comments will help us in revising this paper . We agree that addressing norms other than L_\\infty is an important direction . The techniques explored in our paper are , in general , applicable to other L_p norms ( as well as more broader sets of perturbations ) . Inducing sparsity via L1-regularization and/or weight pruning will still reduce the number of variables in the formulation of verification problems and should improve verification speed . ReLU stability will also help and can still be encouraged via our proposed RS Loss . We do acknowledge that the L_\\infty norm will give the tightest bounds on the input layer , which could mean that ReLU stability is easier to optimize for in the L_\\infty case . To clarify with a quick example - if we have a 784-dimensional MNIST input ( x1 , x2 , \u2026 x784 ) with values in the range [ 0 , 1 ] , a reasonable L_\\infty norm bound on allowed perturbations may be eps=0.3 . On the other hand , a reasonable L_2 norm bound on allowed perturbations may be eps=3 . This means that for the L_\\infty case , a perturbed input x \u2019 with first dimension x1 \u2019 is bounded by x1 - 0.3 < x1 \u2019 < x1 + 0.3 , while for the L_2 case , the tightest bounds on x1 \u2019 are 0 < x1 \u2019 < 1 . Even though these bounds are looser , encouraging ReLU stability will still improve verification speed . Finally , as of now , most literature in verification and certification that we are aware of has also focused on the L_\\infty norm . Therefore , we similarly chose to focus on it as the most common and natural benchmark . We will be sure to discuss addressing other L_p norms and input constraints in more detail in a revised version of this paper . Additionally , thank you for pointing out the Mirman et . al 2018 paper - we will absolutely add those relevant results to our comparison tables and references section ."}], "0": {"review_id": "BJfIVjAcKm-0", "review_text": "The paper presents several ways to regularize plain ReLU networks to optimize 3 things - the adversarial robustness, defined as the fraction of examples for which adversarial perturbation exists - the provable adversarial robustness, defined as the fraction of examples for which some method can show that there exists no adversarial example within a certain time budget - the verification speed, i.e. the amount of time it takes some method to verify whether there is an adversarial example or not Overall, the ideas are sound and the analysis is solid. My main concern is the comparison between the authors method and the 'certification' methods, both conceptually and regarding performance. The authors note that their method falls under 'verification', whereas many competing methods fall under 'certification'. They point to two advantages of verification over certification: (1) the ability to provide true negatives, i.e. prove that an adversarial example exists when it does, and (2) certification requires that 'models must be trained and optimized for a specific certification method'. However, neither argument convinces me regarding the utility of the authors method. Regarding (2): The authors method also requires training the network in a specific way (with RS loss), and it is only compatible with verifiers that care about ReLU stability. Regarding (1): It is not clear that this would be helpful at all. Is it really that much better if method A has 80% proven robustness and 20% proven non-robustness versus method B that has 80% proven robustness and 20% unknown? One could make the case that method B is actually even better. So overall, I think one has to compare the authors method and the certification methods head-to-head. And in table 3, where this is done, Dvijotham comes out on top 2 out of 2 times and Wong comes out on top 2 out of 4 times. That does not seem convincing. Also, what about the performance numbers form other papers discussed in section 2? ------- Other issues: At first glance, the fact that the paper only deals with (small) plain ReLU networks seems to be a huge downside. While I'm not familiar with the verification / certification literature, from reading the paper, I suspect that all the other verification / certification methods also only deal with that or highly similar architectures. However, I will defer to the other reviewers if this is not the case. To expand upon my comment above, I think the paper should discuss true adversarial accuracy on top of provable adversarial robustness. Looking at table 1, for instance, for rows 2, 3 and 4, it seems that the verifier used much less than 120 seconds on average. Does that mean the verifier finished for all test examples? And wouldn't that mean that the verifier determined for each test example exactly whether an adversarial example existed or not? In that case, I would write \"true adversarial accuracy\" instead of \"provable adversarial accuracy\" as column header. If the verifiers did not finish, I would include in the paper for how many examples the result was \"adverarial example exists\" and for how many the result was \"timeout\". I would also include that information in table 3, and I would also include proving / certification times there. Based on the paper, I'm not quite sure whether the idea of training with L1 regularization and/or small weight pruning and/or ReLU pruning for the purpose of improving robustness / verifiability was an original idea of this paper. In either case, this should be made clear. Also, the paper seems to use networks with adversarial training, small weight pruning, L1 and ReLU pruning as its baseline in most cases (all figures except table 1). If some of these techniques are original contributions, this might not be an appropriate baseline to use, even if it is a strong baselines. Why are most experiments presented outside of the \"experiments\" section? This seems to be bad presentation. I would include all test set accuracy values instead of writing \"its almost as high\". Also, in table 3, it appears as if using RS loss DOES in fact reduce test error significantly, at least for CIFAR. Why is that? While, again, I'm not familiar with the background work on verification / certification, it appears to me from reading this paper that all known verification algorithms perform terribly and are restricted to a narrow range of network architectures. If that is the case, one has to wonder whether that line of research should be encouraged to continue. -------- Minor issues: - \"our focus will be on the most common architecture for state-of-the-art models: k-layer fully-connected feed-forward DNN classifiers\" Citation needed. Otherwise, I would suggest removing this statement. - \"such models can be viewed as a function f(.,W)\" - you also need to include the bias in the formula I think - \"convolutional layers can be represented as fully-connected layers\". I think what you mean is \"convolutional layers can be represented as matrix multiplication\" - could you make the difference between co-design and co-training more clear? - The paper could include in the appendix a section outlining the verification method of Tjeng", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their detailed comments ; they will be helpful in revising our manuscript . We appreciate the point about the importance of \u201c verification \u201d compared to \u201c certification \u201d as it is indeed a great question ! Now , one should note that , in this context , formal verification is the ultimate end goal we strive for ; certification is just a fast \u201c shortcut \u201d that can get us closer to this goal , at the expense of sacrificing part of the robustness guarantee . Of course , if certification already gives us a satisfactory level of robustness , this tradeoff can be beneficial . However , it is unclear if the current state-of-the-art ( SOTA ) certification methods like Wong et . al 2018 [ 1 ] are at this point able to deliver such robustness once we move beyond the smallest perturbation sizes ( see the MNIST , eps=0.3 case in Table 3 of our manuscript ) . Additionally , when applied to neural networks not specifically trained for that certification method , they give vacuous bounds [ 2 ] . Thus , as of now , using and improving formal verification methods should still be our focus . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Now , we would like to address your high-level comments in more detail , followed by your specific comments : > Regarding ( 2 ) : The authors method also requires training the network in a specific way ( with RS loss ) , and it is only compatible with verifiers that care about ReLU stability . We view RS Loss as a regularization method , similar to L1 regularization . It can be added to any training procedure , and it is designed for a natural goal - encouraging stable ReLUs . Even if one could , in principle , imagine a verification approach that does not benefit from the natural goal of ReLU stability , all effective verification methods that we are aware of , either falling under the broader class of SMT-based verifiers [ 3,4 ] or MIP-based verifiers [ 5,6,7 ] , can benefit from ReLU stability . For example , [ 3 ] states that \u201c When tighter bounds are derived for ReLU variables , these variables can sometimes be eliminated , i.e. , fixed to the active or inactive state , without splitting. \u201d [ 6 ] writes : \u201c we conjecture that the large increase of binary variables in the problem caused by the binary constraints on the input creates the large performance gap between the Reuters dataset and MNIST. \u201d ( the Reuters dataset had more binary variables and took much longer to verify ) > Regarding ( 1 ) : While the authors method can provide true negatives , they are not discussed in the paper at all . > I would include in the paper for how many examples the result was `` adverarial example exists '' and for how many the result was `` timeout '' . We do discuss upper bounds for the true robustness of all of our models , as well as the number of timeouts , in Appendix E. We appreciate that you bring up this point though , and we will work to point readers to these relevant details in Appendix E from the main body of the paper when revising . In Appendix E , the column labeled \u201c Verifier Upper Bound \u201d is simply 100 % minus the number of true negatives ( \u201c adversarial examples exists \u201d cases ) - it describes the maximum possible value for the true adversarial accuracy . The difference between the upper bound ( \u201c Verifier Upper Bound \u201d ) and the lower bound ( \u201c Provable Adversarial Accuracy \u201d ) on the true adversarial accuracy equals how many examples reached their \u201c timeout , \u201d as we can not determine which category they belong to . > At first glance , the fact that the paper only deals with ( small ) plain ReLU networks seems to be a huge downside . > While , again , I 'm not familiar with the background work on verification / certification , it appears to me from reading this paper that all known verification algorithms perform terribly and are restricted to a narrow range of network architectures . Indeed , we purposely chose our architectures to match those of prior works in certification literature for the fairest possible comparison . We agree that expanding beyond our current capabilities for verification and certification is an important direction for further research . Our contribution in this manuscript is to show that training for ease-of-verification via inducing weight sparsity and ReLU stability can help scale verification . Prior to our work , verification methods struggled for neural networks with just a few hundreds ReLUs in total . Using our methods , networks as large as our \u201c large \u201d convolutional CIFAR network , which have over 60000 ReLUs ( most of which can be made stable ) , can be verified ."}, "1": {"review_id": "BJfIVjAcKm-1", "review_text": "Training for Faster Adversarial Robustness verification via inducing RELU stability As I am familiar yet not an expert on adversarial training and robustess, my review will focus mainly on the overall soundness of the manuscript. I also only went superficially into the quantitative results. Summary: The authors are interested in the problem of verifying neural networks models trained to be robust against adversarial attacks. The focus is on networks with relu activations and adversarial perturbations within an epsilon l1-ball around each input, and the verification problem consists in proving the network performs as intended for all possible perturbations (infinitely many) The review on verification is clear. Elements that affect verification time are introduced and well explained in main text or appendix from both intuitive and theoretical perspective: l1 penalty, weight pruning, relu stability. These can be summ\\arized as : you want few neurons, and you want them to operate in the same regime for all inputs, both to avoid branching. Relu stability is apparently a new concept and the proposed regularization approximately enforces relu stability. The approximation [itself using the novel improved interval arithmetic] based bounds on unit activations propagated through the network seems not to scale well with depths (more units are mis-labelled as relu unstable, hence wrongly regularized if I understand correctly). The authors acknowledge and document this fact but I would like to hear more discussion on this feature and on the trade-off that still make this approach worthwhile for deeper networks. This regularization does not help performance but only paves the way for a faster verification, for this reason the term co-design is used. The rest of the manuscript is a thorough empirical analysis of the effect of the penalties/regularizations on the network and ultimately on the verification time, keeping an eye on not deteriorating the performance of the network. How much regularization can be added seems to be indeed an empirical question since networks are \u2018over-parametrized in the first place\u2019 with no clear way to a priori quantify task or model complexity. The devil is in the details and in practice implementation seems not straightforward with a complex optimization with varying learning rates and different regularizations applied at different time along the way. But this seems to be the case for most deep learning paper. The authors claim and provide evidence to be able to verify network well beyond the scope of what was achievable before due to the obtained speed-ups, which is a notable feature. Overall, this manuscript is well structured, thorough and pleasant to read, and I recommend it to be accepted for publication at ICLR ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank the reviewer for their helpful comments . We are glad you found the paper pleasant to read ! We agree that labeling unstable ReLUs properly is an important aspect of our technique . The upper and lower bounds we compute on each ReLU are conservative - thus , every unstable ReLU will always be correctly labeled as unstable , while stable ReLUs can be labeled as either stable or unstable . Importantly , every unstable ReLU is correctly labeled and penalized by the RS Loss we propose . The tradeoff is that stable ReLUs mislabeled as unstable will also be penalized , which can be an unnecessary regularization of the model . We showed empirically that we could achieve the following two objectives at once using RS Loss 1 ) Reduce the number of ReLUs labeled as unstable , which is an upper bound on the true number of unstable ReLUs 2 ) Achieve similarly good test set accuracy and PGD-adversarial accuracy as a model trained without RS Loss For example , when comparing the Control and \u201c +RS \u201d networks for MNIST and eps=0.1 , we decreased the average number of ReLUs labeled as unstable ( using bounds from Improved Interval Arithmetic ) from 290.5 to 105.4 with just a 0.26 % loss in test set accuracy ( cf.Appendix C.3 , Appendix E ) . The same trends hold for deeper networks ( we only showed results for a 3-layer network in Appendix C.3 , but we will include details about a 6-layer network in the revision ) . For the deeper 6-layer \u201c +RS \u201d network for MNIST and eps=0.1 that we presented , it had a test set accuracy of 98.93 % and just 184.6 ReLUs labeled as unstable at the end of training [ * ] . Training the exact same network without the RS Loss penalty had a slightly higher test set accuracy ( 99.09 % ) but also had far more ReLUs labeled as unstable ( 1028.3 ) . Thus , we could effectively reduce the number of ReLUs labeled as unstable without significantly degrading test set accuracy . We will clarify these points better in Appendix C where we discuss ReLU bounds when revising the paper . [ * ] Edit made : Previously , we wrote test set accuracy of 98.95 % and 150.3 ReLUs labeled as unstable , which matches Appendix E. Those are the correct numbers after post-processing ( weight pruning and ReLU pruning ) is applied , whereas the updated numbers we write here are before post-processing , to match the fact that the 99.09 % /1028.3 unstable ReLUs numbers are also computed before post-processing ."}, "2": {"review_id": "BJfIVjAcKm-2", "review_text": "This paper proposes methods to train robust neural networks that can also be verified faster. Specifically, it uses pruning methods to encourage weight sparsity and uses regularization to encourage ReLU stability. Both weight sparsity and ReLU stability reduces time needed for verification. The verified robust accuracy reported in this paper is close to previous SOTA certified robust accuracy, although not beating SOTA. The paper is clearly written and easy to follow. The reviewer is familiar with literatures on certifiable robust network literature, but not familiar with verification literature. To the best knowledge of the reviewer, the proposed method is well motivated and novel, and provides a scalable method for verifying (instead of lower bounding) robustness. Other comments: I think there should be some discussions on applicability on different robustness measures. The paper focus on L_\\infty norm bounded attack, is this method extendable to other norms? Re: robust accuracy comparison, I found some previous SOTA results missing from Table 3. For example, Mirman et al., 2018 (Appendix Table 6) reached 82% (higher than 80.68% achieved in this paper) provable robust accuracy for MNIST eps=0.3 case. and this is not reported in Table 3. The CIFAR10 results in Mirman et al., 2018 is also better than the best SOTA accuracy in Table 3. Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably robust neural networks. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3575\u20133583, Stockholmsmssan, Stockholm Sweden, 10\u201315 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/mirman18b.html. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their useful comments . Your comments will help us in revising this paper . We agree that addressing norms other than L_\\infty is an important direction . The techniques explored in our paper are , in general , applicable to other L_p norms ( as well as more broader sets of perturbations ) . Inducing sparsity via L1-regularization and/or weight pruning will still reduce the number of variables in the formulation of verification problems and should improve verification speed . ReLU stability will also help and can still be encouraged via our proposed RS Loss . We do acknowledge that the L_\\infty norm will give the tightest bounds on the input layer , which could mean that ReLU stability is easier to optimize for in the L_\\infty case . To clarify with a quick example - if we have a 784-dimensional MNIST input ( x1 , x2 , \u2026 x784 ) with values in the range [ 0 , 1 ] , a reasonable L_\\infty norm bound on allowed perturbations may be eps=0.3 . On the other hand , a reasonable L_2 norm bound on allowed perturbations may be eps=3 . This means that for the L_\\infty case , a perturbed input x \u2019 with first dimension x1 \u2019 is bounded by x1 - 0.3 < x1 \u2019 < x1 + 0.3 , while for the L_2 case , the tightest bounds on x1 \u2019 are 0 < x1 \u2019 < 1 . Even though these bounds are looser , encouraging ReLU stability will still improve verification speed . Finally , as of now , most literature in verification and certification that we are aware of has also focused on the L_\\infty norm . Therefore , we similarly chose to focus on it as the most common and natural benchmark . We will be sure to discuss addressing other L_p norms and input constraints in more detail in a revised version of this paper . Additionally , thank you for pointing out the Mirman et . al 2018 paper - we will absolutely add those relevant results to our comparison tables and references section ."}}