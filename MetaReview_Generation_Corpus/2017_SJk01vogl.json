{"year": "2017", "forum": "SJk01vogl", "title": "Adversarial examples for generative models", "decision": "Reject", "meta_review": "The main idea in this paper is interesting, of considering various forms of adversarial examples in generative models. The paper has been considerably revised since the original submission. The results showing the susceptibility of generative models to attacks are interesting, but the demonstrations need to be more solid and on other datasets to be convincing.", "reviews": [{"review_id": "SJk01vogl-0", "review_text": " After the rebuttal: The paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal. For me now the biggest problem now is that the title and the content do not correspond. The authors clearly attack deterministic encoder-decoder models (as described in 3.2), which are not at all the same as generative models, even though many generative models make use of this architecture. A small experiment with sampling is interesting, but does not change the overall focus of the paper. This inconsistency in not acceptable. The whole issue could be resolved for example by simply replacing \"generative models\" by \"encoder-decoder networks\" in the title. Then I would tend towards recommending acceptance. ------ Initial review: The paper describes three approaches to generating adversarial examples for deep encoder-decoder generative networks (trained as VAE or VAE-GAN), and shows a comparative analysis of these. While the phenomenon of adversarial examples in discriminative models is widely known and relatively well studied, I am not aware of previous work on adversarial examples for generative networks, so this work is novel (there is a concurrent work by Tabacof et al. which should be cited, though). The paper has significantly improved since the initial submission; still, I have a number of remarks on presentation and experimental evaluation. I am in the borderline mode, and may change my rating during the discussion phase. Detailed comments: 1) The paper is 13 pages long - significantly over the recommended page limit of 8 pages. Reviewers have to read multiple papers, multiple versions of each, it is a lot of work. Large portions of the paper should be shortened and/or moved to the appendix. It is job of the authors to make the paper concise and readable. \"in our attempts to be thorough, we have had a hard time keeping the length down\" is a bad excuse - it may be hard, but has to be done. 2) I intentionally avoided term \"generative model\" above because it is not obvious to me if the attacks described by the authors indeed attack generative models. To clarify, the authors train encoder-decoders as generative models (VAE or VAE-GAN), but then remove all stochasticity (sampling) and prior on the latent variables: that is, they treat the models as deterministic encoders-decoders. It is not a big surprise that a deterministic deep network can be easily tricked; it would be much more interesting to see if the probabilistic aspect of generative models makes them more robust to such attacks. Am I missing something? I would like the authors to clarify their view on this and adjust the claims in the paper if necessary. 3) The paper is motivated by possible attacks on a data channel which uses a generative network for compressing information. Description of the attack scenario in 3.1 does not look convincing to me. It takes a huge amount of space and I do not think it adds much to the paper. First, experiments on natural images are necessary to judge if the proposed attack could succeed in a realistic scenario and second, I am not aware of any existing practical applications of VAEs to image compression: attacking JPEG would be much more practical. 4) Experiments are limited to MNIST and, in the latest version, SVHN (which is very nice). While no good generative models of general natural images exist, it is common to evaluate generative models on datasets of faces, so this would be another very natural domain for testing the proposed approach. Smaller remarks: 1) Usage of \"Oracle\" in 3.2 does not look appropriate - oracle typically has access to (part of) ground truth, which is not the case here as far as I understand. 2) Beginning of section 4: \"All three methods work for any generative architecture that relies on a learned latent representation z\" - \"are technically applicable to\" would be more correct than \"work for\". 3) 4.1: \"confidentally\" ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the review ! The new draft is shorter , reducing it to 11 pages while also adding some new results . In this draft we have added experimental results using stochastic sampling . They are referenced in the second paragraph of Section 5 with a figure in the appendix . The results show that for most examples , sampling doesn \u2019 t meaningfully change the reconstructed adversarial examples , and the attack is similarly successful even when sampling is being used . We have also added some citations to clarify where the attack scenario sits in the current literature . There are a few recent publications exploring using deep networks as compression models , similar to what we describe in Section 3.1 , for example : https : //arxiv.org/abs/1511.06085 , and https : //arxiv.org/abs/1608.05148 . We are not attempting to directly attack these networks in this work , however -- we are instead motivating why this type of attack is relevant to current work . Thanks for the suggestion to use faces ! We have added experiments on CelebA in the latest draft . The results in Section 5.3 show that the attacks are equally effective even in the more complex domain . We have also addressed your other comments in this draft ."}, {"review_id": "SJk01vogl-1", "review_text": "Comments: \"This contrasts to adversarial attacks on classifiers, where any inspection of the inputs will reveal the original bytes the adversary supplied, which often have telltale noise\" Is this really true? If it were the case, wouldn't it imply that training \"against\" adversarial examples should easily make a classifier robust to adversarial examples (if they all have a telltale noise)? Pros: -The question of whether adversarial examples exist in generative models, and indeed how the definition of \"adversarial example\" carries over is an interesting one. -Finding that a certain type of generative model *doesn't have* adversarial examples would be a really significant result, finding that generative models have adversarial examples would also be a worth negative result. -The adversarial examples in figures 5 and 6 seem convincing, though they seem much more overt and noisy than the adversarial examples on MNIST shown in (Szegedy 2014). Is this because it's actually harder to find adversarial examples in these types of generative models? Issues: -Paper is significantly over length at 13 pages. -The beginning of the paper should more clearly motivate its purpose. -Paper has \"generative models\" in the title but as far as I can tell the whole paper is concerned with autoencoder-type models. This is kind of annoying because if someone wanted to consider adversarial attacks on, say, autoregressive models, they might be unreasonably burdened by having to explain how they're distinct from a paper called \"adversarial examples for generative models\". -I think that the introduction contains too much background information - it could be tightened. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "What you say about the telltale noise is correct -- as Goodfellow et al.2014 show , it is possible to train a network using the adversarial examples and substantially reduce ( but not eliminate ) the network \u2019 s vulnerability to adversarial examples . Additionally , another ICLR submission this year showed a way to train a separate network to classify images as adversarial or non-adversarial with high accuracy . All that being said , that paragraph is one of the paragraphs that we \u2019 ve removed from the new version of the paper in order to reduce the length . The paper is now 11 pages plus references and includes some new results . Since we are the first to study adversarial examples on generative models in depth , we think it is appropriate to use \u201c generative models \u201d in the title . The reason for focusing on VAE-like models is that in the paper we propose a plausible scenario how such a model can be used in a compression scheme , which can then be attacked by an adversary . To the best of our knowledge , other types of generative models , such as autoregressive models , do not offer such a clear attack scenario , where adversarial examples may be used by an attacker ."}, {"review_id": "SJk01vogl-2", "review_text": "This paper considers different methods of producing adversarial examples for generative models such as VAE and VAEGAN. Specifically, three methods are considered: classification-based adversaries which uses a classifier on top of the hidden code, VAE loss which directly uses the VAE loss and the \"latent attack\" which finds adversarial perturbation in the input so as to match the latent representation of a target input. I think the problem that this paper considers is potentially useful and interesting to the community. To the best of my knowledge this is the first paper that considers adversarial examples for generative models. As I pointed out in my pre-review comments, there is also a concurrent work of \"Adversarial Images for Variational Autoencoders\" that essentially proposes the same \"latent attack\" idea of this paper with both L2 distance and KL divergence. Novelty/originality: I didn't find the ideas of this paper very original. All the proposed three attacks are well-known and standard methods that here are applied to a new problem and this paper does not develop *novel* algorithms for attacking specifically *generative models*. However I still find it interesting to see how these standard methods compare in this new problem domain. The clarity and presentation of the paper is very unsatisfying. The first version of the paper proposes the \"classification-based adversaries\" and reports only negative results. In the second set of revisions, the core idea of the paper changes and almost an entirely new paper with a new co-author is submitted and the idea of \"latent attack\" is proposed which works much better than the \"classification-based adversaries\". However, the authors try to keep around the materials of the first version, which results in a 13 page long paper, with different claims and unrelated set of experiments. \"in our attempts to be thorough, we have had a hard time keeping the length down\" is not a valid excuse. In short, the paper is investigating an interesting problem and apply and compare standard adversarial methods to this domain, but the novelty and the presentation of the paper is limited.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the review , and for pointing out the NIPS workshop paper . That work was not known at the time of our submission to ICLR , as their paper was published online one month after our initial ICLR submission , and a couple of weeks after our first major revision . We have added a paragraph explaining the differences with their paper . It appears that their paper comes to a different conclusion regarding the L_vae attack -- we find it to be effective on MNIST , SVHN and CelebA , while they say that they couldn \u2019 t get it to work . We also present a more in-depth study of the topic , which considers more attacks , more generative model architectures , and more datasets ( with our addition of CelebA results in the latest draft ) . We agree that ultimately we were able to determine that existing attacks could be modified to apply to the new domain of generative models , and so it may feel self-evident in retrospect . However , prior to our work , it was unknown in the literature whether the stochasticity of generative models would confer robustness , or what an attack on a generative model would look like , or even under what scenarios an attacker might want to attack a generative model . With our work , the research community knows that generative models are effectively as vulnerable to adversarial examples as deterministic classifiers , and that there are realistic scenarios where such attacks could matter . In our opinion , this is an important result , even though it didn \u2019 t require discovering substantially different mathematical approaches ( and perhaps it is even more important because the attacks transferred so naturally , underscoring the vulnerability of current architectures ) . It \u2019 s true that we did a lot of work on this paper after the initial deadline . ICLR \u2019 s unusual submission and review process permits and encourages authors to continue revising their work after submission , which clearly has some advantages and disadvantages for all parties involved . Presumably norms and expectations around how much papers change after submission will become more settled over time . Thanks for your patience through the various revisions and additions we have made ! In this version , we have shortened and streamlined the paper , reducing it to 11 pages while also adding some new results ."}], "0": {"review_id": "SJk01vogl-0", "review_text": " After the rebuttal: The paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal. For me now the biggest problem now is that the title and the content do not correspond. The authors clearly attack deterministic encoder-decoder models (as described in 3.2), which are not at all the same as generative models, even though many generative models make use of this architecture. A small experiment with sampling is interesting, but does not change the overall focus of the paper. This inconsistency in not acceptable. The whole issue could be resolved for example by simply replacing \"generative models\" by \"encoder-decoder networks\" in the title. Then I would tend towards recommending acceptance. ------ Initial review: The paper describes three approaches to generating adversarial examples for deep encoder-decoder generative networks (trained as VAE or VAE-GAN), and shows a comparative analysis of these. While the phenomenon of adversarial examples in discriminative models is widely known and relatively well studied, I am not aware of previous work on adversarial examples for generative networks, so this work is novel (there is a concurrent work by Tabacof et al. which should be cited, though). The paper has significantly improved since the initial submission; still, I have a number of remarks on presentation and experimental evaluation. I am in the borderline mode, and may change my rating during the discussion phase. Detailed comments: 1) The paper is 13 pages long - significantly over the recommended page limit of 8 pages. Reviewers have to read multiple papers, multiple versions of each, it is a lot of work. Large portions of the paper should be shortened and/or moved to the appendix. It is job of the authors to make the paper concise and readable. \"in our attempts to be thorough, we have had a hard time keeping the length down\" is a bad excuse - it may be hard, but has to be done. 2) I intentionally avoided term \"generative model\" above because it is not obvious to me if the attacks described by the authors indeed attack generative models. To clarify, the authors train encoder-decoders as generative models (VAE or VAE-GAN), but then remove all stochasticity (sampling) and prior on the latent variables: that is, they treat the models as deterministic encoders-decoders. It is not a big surprise that a deterministic deep network can be easily tricked; it would be much more interesting to see if the probabilistic aspect of generative models makes them more robust to such attacks. Am I missing something? I would like the authors to clarify their view on this and adjust the claims in the paper if necessary. 3) The paper is motivated by possible attacks on a data channel which uses a generative network for compressing information. Description of the attack scenario in 3.1 does not look convincing to me. It takes a huge amount of space and I do not think it adds much to the paper. First, experiments on natural images are necessary to judge if the proposed attack could succeed in a realistic scenario and second, I am not aware of any existing practical applications of VAEs to image compression: attacking JPEG would be much more practical. 4) Experiments are limited to MNIST and, in the latest version, SVHN (which is very nice). While no good generative models of general natural images exist, it is common to evaluate generative models on datasets of faces, so this would be another very natural domain for testing the proposed approach. Smaller remarks: 1) Usage of \"Oracle\" in 3.2 does not look appropriate - oracle typically has access to (part of) ground truth, which is not the case here as far as I understand. 2) Beginning of section 4: \"All three methods work for any generative architecture that relies on a learned latent representation z\" - \"are technically applicable to\" would be more correct than \"work for\". 3) 4.1: \"confidentally\" ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the review ! The new draft is shorter , reducing it to 11 pages while also adding some new results . In this draft we have added experimental results using stochastic sampling . They are referenced in the second paragraph of Section 5 with a figure in the appendix . The results show that for most examples , sampling doesn \u2019 t meaningfully change the reconstructed adversarial examples , and the attack is similarly successful even when sampling is being used . We have also added some citations to clarify where the attack scenario sits in the current literature . There are a few recent publications exploring using deep networks as compression models , similar to what we describe in Section 3.1 , for example : https : //arxiv.org/abs/1511.06085 , and https : //arxiv.org/abs/1608.05148 . We are not attempting to directly attack these networks in this work , however -- we are instead motivating why this type of attack is relevant to current work . Thanks for the suggestion to use faces ! We have added experiments on CelebA in the latest draft . The results in Section 5.3 show that the attacks are equally effective even in the more complex domain . We have also addressed your other comments in this draft ."}, "1": {"review_id": "SJk01vogl-1", "review_text": "Comments: \"This contrasts to adversarial attacks on classifiers, where any inspection of the inputs will reveal the original bytes the adversary supplied, which often have telltale noise\" Is this really true? If it were the case, wouldn't it imply that training \"against\" adversarial examples should easily make a classifier robust to adversarial examples (if they all have a telltale noise)? Pros: -The question of whether adversarial examples exist in generative models, and indeed how the definition of \"adversarial example\" carries over is an interesting one. -Finding that a certain type of generative model *doesn't have* adversarial examples would be a really significant result, finding that generative models have adversarial examples would also be a worth negative result. -The adversarial examples in figures 5 and 6 seem convincing, though they seem much more overt and noisy than the adversarial examples on MNIST shown in (Szegedy 2014). Is this because it's actually harder to find adversarial examples in these types of generative models? Issues: -Paper is significantly over length at 13 pages. -The beginning of the paper should more clearly motivate its purpose. -Paper has \"generative models\" in the title but as far as I can tell the whole paper is concerned with autoencoder-type models. This is kind of annoying because if someone wanted to consider adversarial attacks on, say, autoregressive models, they might be unreasonably burdened by having to explain how they're distinct from a paper called \"adversarial examples for generative models\". -I think that the introduction contains too much background information - it could be tightened. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "What you say about the telltale noise is correct -- as Goodfellow et al.2014 show , it is possible to train a network using the adversarial examples and substantially reduce ( but not eliminate ) the network \u2019 s vulnerability to adversarial examples . Additionally , another ICLR submission this year showed a way to train a separate network to classify images as adversarial or non-adversarial with high accuracy . All that being said , that paragraph is one of the paragraphs that we \u2019 ve removed from the new version of the paper in order to reduce the length . The paper is now 11 pages plus references and includes some new results . Since we are the first to study adversarial examples on generative models in depth , we think it is appropriate to use \u201c generative models \u201d in the title . The reason for focusing on VAE-like models is that in the paper we propose a plausible scenario how such a model can be used in a compression scheme , which can then be attacked by an adversary . To the best of our knowledge , other types of generative models , such as autoregressive models , do not offer such a clear attack scenario , where adversarial examples may be used by an attacker ."}, "2": {"review_id": "SJk01vogl-2", "review_text": "This paper considers different methods of producing adversarial examples for generative models such as VAE and VAEGAN. Specifically, three methods are considered: classification-based adversaries which uses a classifier on top of the hidden code, VAE loss which directly uses the VAE loss and the \"latent attack\" which finds adversarial perturbation in the input so as to match the latent representation of a target input. I think the problem that this paper considers is potentially useful and interesting to the community. To the best of my knowledge this is the first paper that considers adversarial examples for generative models. As I pointed out in my pre-review comments, there is also a concurrent work of \"Adversarial Images for Variational Autoencoders\" that essentially proposes the same \"latent attack\" idea of this paper with both L2 distance and KL divergence. Novelty/originality: I didn't find the ideas of this paper very original. All the proposed three attacks are well-known and standard methods that here are applied to a new problem and this paper does not develop *novel* algorithms for attacking specifically *generative models*. However I still find it interesting to see how these standard methods compare in this new problem domain. The clarity and presentation of the paper is very unsatisfying. The first version of the paper proposes the \"classification-based adversaries\" and reports only negative results. In the second set of revisions, the core idea of the paper changes and almost an entirely new paper with a new co-author is submitted and the idea of \"latent attack\" is proposed which works much better than the \"classification-based adversaries\". However, the authors try to keep around the materials of the first version, which results in a 13 page long paper, with different claims and unrelated set of experiments. \"in our attempts to be thorough, we have had a hard time keeping the length down\" is not a valid excuse. In short, the paper is investigating an interesting problem and apply and compare standard adversarial methods to this domain, but the novelty and the presentation of the paper is limited.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for the review , and for pointing out the NIPS workshop paper . That work was not known at the time of our submission to ICLR , as their paper was published online one month after our initial ICLR submission , and a couple of weeks after our first major revision . We have added a paragraph explaining the differences with their paper . It appears that their paper comes to a different conclusion regarding the L_vae attack -- we find it to be effective on MNIST , SVHN and CelebA , while they say that they couldn \u2019 t get it to work . We also present a more in-depth study of the topic , which considers more attacks , more generative model architectures , and more datasets ( with our addition of CelebA results in the latest draft ) . We agree that ultimately we were able to determine that existing attacks could be modified to apply to the new domain of generative models , and so it may feel self-evident in retrospect . However , prior to our work , it was unknown in the literature whether the stochasticity of generative models would confer robustness , or what an attack on a generative model would look like , or even under what scenarios an attacker might want to attack a generative model . With our work , the research community knows that generative models are effectively as vulnerable to adversarial examples as deterministic classifiers , and that there are realistic scenarios where such attacks could matter . In our opinion , this is an important result , even though it didn \u2019 t require discovering substantially different mathematical approaches ( and perhaps it is even more important because the attacks transferred so naturally , underscoring the vulnerability of current architectures ) . It \u2019 s true that we did a lot of work on this paper after the initial deadline . ICLR \u2019 s unusual submission and review process permits and encourages authors to continue revising their work after submission , which clearly has some advantages and disadvantages for all parties involved . Presumably norms and expectations around how much papers change after submission will become more settled over time . Thanks for your patience through the various revisions and additions we have made ! In this version , we have shortened and streamlined the paper , reducing it to 11 pages while also adding some new results ."}}