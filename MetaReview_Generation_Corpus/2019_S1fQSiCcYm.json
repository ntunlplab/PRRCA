{"year": "2019", "forum": "S1fQSiCcYm", "title": "Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer", "decision": "Accept (Poster)", "meta_review": "The reviewers have reached a consensus that this paper is very interesting and add insights into interpolation in autoencoders.", "reviews": [{"review_id": "S1fQSiCcYm-0", "review_text": "Main idea: This paper investigates the desiderata for a successful interpolation: 1) Interpolation looks realistic; 2) The interpolation path is semantically smooth. An adversarial regularizer is proposed to achieve 1), and in practice 2) may also satisfied. To evaluate the method, they introduce a synthetic dataset with line images and compare with different autoencoder methods without the interpolation regularization. For real data, they show that the interpolation regularized autoencoder (i.e. ACAI) leads to a better unsupervised representation. Questions: 1. Do we really need every interpolated point to be realistic (i.e. similar to a data point in the train-set)? I believe that there exists an interpolation between two totally different objects can never be observed. 2. Do we need interpolation points to form a semantically smooth morphing? I guess this is a desired property for continuous generators, but it seems not necessary in general. 3. The gamma in the 2nd term in (1) is confusing. If gamma = 1, I understand it forces to predict alpha = 0 since x is real. But if gamma < 1, the average in data space may be very blurry thus not realistic at all. How does gamma affect the optimization? 4. ACAI looks very similar to LSGAN: by giving \"0\" label to real data and \"alpha\" label to fake data; in LSGAN, alpha = 1. Have you tested a LSGAN like regularizer? 5. The baselines are not representative: since ACAI introduces an adversarial regularizer, you should compare with other GAN techniques induced regularizers, such as WGAN regularized autoencoder. After rebuttal: See the long discussion below. I tend to believe that a good interpolation is not only a way to do sanity check but also a nice property to explicitly control in representation learning.", "rating": "7: Good paper, accept", "reply_text": "Thanks for your thorough review and questions . We 've answered your questions below and have updated our draft to clarify . > Do we really need every interpolated point to be realistic ( i.e.similar to a data point in the train-set ) ? I believe that there exists an interpolation between two totally different objects can never be observed . We are interested in latent spaces where interpolations produce realistic outputs across the entirety of the interpolation because this suggests some form of continuity in the latent space ( as illustrated in FIgure 1 ) . Our paper asks whether this property also results in an improved representation for downstream tasks . If an intermediate point was not realistic , the latent space might not have this property . Thanks for pointing out that in some cases it 's not obvious that there is a smooth and realistic path between two datapoints . We think two good examples of this are in Figure 6 , bottom , where we interpolate between different MNIST digits . We find that even though there is no real digit which is at the midpoint of , for example , a 2 and a 9 , the midpoint of ACAI 's interpolation still appears realistic . We have added a note about this to our paper . > Do we need interpolation points to form a semantically smooth morphing ? I guess this is a desired property for continuous generators , but it seems not necessary in general . We agree that smoothness is not required for high-quality learned features -- for example , the Denoising Autoencoder fared well on our classification experiments despite producing poor interpolations . However , we are interested in the opposite , namely whether the ability to perform latent-space manipulations like interpolation suggest a better learned representation . We have added some clarification of this point in our paper . > The gamma in the 2nd term in ( 1 ) is confusing . If gamma = 1 , I understand it forces to predict alpha = 0 since x is real . But if gamma < 1 , the average in data space may be very blurry thus not realistic at all . How does gamma affect the optimization ? Note \\hat { x } is a reconstruction of x , so in practice \\gamma * x + ( 1 \u2212 \\gamma ) * \\hat { x } will be quite similar to x as long as \\hat { x } is a reasonable reconstruction . In other words , we are not interpolating between two totally different points , so typically the blurriness you might expect from pixel-space mixing wo n't be present . We have added some additional discussion of gamma and this term to our paper . > ACAI looks very similar to LSGAN : by giving `` 0 '' label to real data and `` alpha '' label to fake data ; in LSGAN , alpha = 1 . Have you tested a LSGAN like regularizer ? You 're right that the LSGAN loss function and our regularization term are similar in the sense that both measure a squared error between the critic 's output and a scalar . The difference is that the LSGAN is designed for use on a GAN-based generative model ; our regularizer is designed as a regularizer for an autoencoder . As a result , the scalar in the LSGAN objective is a fixed hyperparameter whereas we regress the interpolation amount \\alpha . We added some discussion of the LSGAN objective to our paper . > The baselines are not representative : since ACAI introduces an adversarial regularizer , you should compare with other GAN techniques induced regularizers , such as WGAN regularized autoencoder . Note that the Wasserstein Autoencoder ( WAE ) is actually equivalent to an adversarial autoencoder when using a GAN loss ; in the WAE paper [ 1 ] they write `` When c is the squared cost and D_Z is the GAN objective , WAE coincides with adversarial auto-encoders '' . Our paper includes the adversarial autoencoder as a baseline ( labeled AAE in tables and described in Section 3.2 , paragraph 4 ) . We added a citation to [ 1 ] to clarify this . [ 1 ] Ilya Tolstikhin , Olivier Bousquet , Sylvain Gelly and Bernhard Schoelkopf . `` Wasserstein Auto-Encoders '' , ICLR 2017 ."}, {"review_id": "S1fQSiCcYm-1", "review_text": "Summary: The authors propose a new approach to encourage valid interpolation in Auto-Encoders (AE). It is based on a regularization procedure involving a critic network judging the realistic nature of reconstructed data point from its mixed latent representations by recovering the mixing coefficient. The authors show that this approach does indeed improve the quality of interpolated samples on few tasks. A synthetic tasks of lines interpolation (proposing new Mean Distance and Smoothness metric for this task), classification task (with a single-layer classifier) from the latent space representation and finally a clustering accuracy on the latent space. On the proposed regularization method seems to help significantly compared to commonly used AE architectures (Basic AE, Denoising AE, Variational AE, Adversarial AE and VQ-VAE). This paper was a very interesting read, and the work seems to be of significance for the unsupervised learning community. It was clearly written and conveys the contributions clearly and the experimental results and their interpretations seem valid. The proposed approach of a critic based regularizer is a simple but seemingly important addition that contributes to improving interpolation in AE significantly and even show impact \"downstream tasks\" as the authors put it. Few comments/questions come to mind: - For the critic Loss L_d in equation (1) , the authors mention that the \\gamma based second term (that should ensure that the critic outputs 0 for non-interpolated inputs and expose the critic to realistic data even if the AE reconstruction is poor) does not seem to be crucial in your approach but stabilized the adversarial training. Could you somehow quantify this. It seems like stability of the adversarial training should be paramount to your method to make sure the AE learns a better latent representation. This comment, even though I assume it well-founded, seems a bit of a contradiction. - For the Lines synthetic data. It was chosen to use a 32x32 image size with 16 points length lines. This configuration does quantize directly the angles your measures can distinguish. Below a certain angle differences (or delta), 2 angles must have the same pixel representation, i.e. exact overlapping lines. My question is simple: What is the smallest angle you can use/distinguish or, how many exact unique lines can you have? Overall this is a good paper that deserves publications.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your review , we are glad you found the paper interesting and significant . To address your questions and comments : > For the critic Loss L_d in equation ( 1 ) , the authors mention that the \\gamma based second term ( that should ensure that the critic outputs 0 for non-interpolated inputs and expose the critic to realistic data even if the AE reconstruction is poor ) does not seem to be crucial in your approach but stabilized the adversarial training . Could you somehow quantify this . It seems like stability of the adversarial training should be paramount to your method to make sure the AE learns a better latent representation . This comment , even though I assume it well-founded , seems a bit of a contradiction . We agree that this comment should be expanded on , and we have done so in our updated draft . To clarify , when we say it `` helped stabilize the adversarial learning process '' , we mean that a ) it allowed us to use the same value of \\lambda across all of our experiments and still achieve good results and b ) it resulted in smooth convergence of the autoencoder 's loss . We note that stability of the adversarial learning process was not an issue in general , in the sense that stability across runs was not an issue and our model never `` collapsed '' to a bad solution . > For the Lines synthetic data . It was chosen to use a 32x32 image size with 16 points length lines . This configuration does quantize directly the angles your measures can distinguish . Below a certain angle differences ( or delta ) , 2 angles must have the same pixel representation , i.e.exact overlapping lines . My question is simple : What is the smallest angle you can use/distinguish or , how many exact unique lines can you have ? Our code for synthesizing line images uses anti-aliasing , so for example a line with angle 0.3 and another with angle 0.300001 will be rendered differently . As a result , the number of unique lines is actually up to floating point precision . We think some confusion about this probably stems from the fact that we referred to the line images as `` black-and-white '' ; we have updated the language in the paper to say `` grayscale '' ."}, {"review_id": "S1fQSiCcYm-2", "review_text": "This paper proposed an adversarially regularized AE algorithm that improve interpolation in latent space. Specifically, a critic is used to predict the interpolation weight \\alpha and encourage the interpolated images to be more realistic. The paper verified the method on a newly proposed synthetic line benchmark and on downstream classification and clustering tasks. Pros: 1. A novel algorithm that promotes the interpolation ability of AE 2. A new synthesized line benchmark to verify the interpolation ability of different AE variants 3. Strong results on downstream classification and clustering tasks Cons: 1. The interplay of the adversarial network (between AE and critic) isn\u2019t very clear and can be improved 2. Eq. 1, should x be x_1 or a new data other than x1 and x2? 3. The paper states that the 2nd term of Eq. 1 isn\u2019t crucial. If x is a new data (other than x1 or x2), how can the critic infer \\alpha without a reference to x1 or x2? 4. The paper states that \u201cencouraging this behavior also produce semantically smooth interpolation \u2026\u201d. Besides the empirical evidences from data, it would be better to any some theoretical justifications. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thanks for your review and thoughtful analysis . To address each of your cons in turn : > The interplay of the adversarial network ( between AE and critic ) isn \u2019 t very clear and can be improved . The goal of the critic is to predict the interpolation mixing coefficient \\alpha ; the goal of the autoencoder is to `` fool '' the critic into outputting \\alpha = 0 . It can be useful to think of the critic as estimating a divergence between real and interpolated datapoints , and the autoencoder is trying to minimize this divergence . We have added some discussion of this to our paper . > Eq.1 , should x be x_1 or a new data other than x1 and x2 ? It actually can be any real datapoint x - the second term can be computed separately from the first . We have clarified this in our updated draft . > The paper states that the 2nd term of Eq.1 isn \u2019 t crucial . If x is a new data ( other than x1 or x2 ) , how can the critic infer \\alpha without a reference to x1 or x2 ? The critic must infer \\alpha from common artifacts of interpolated datapoints alone . This is best illustrated in Figure 3 ( a ) - note that as the interpolation morphs from one endpoint to the other , the image becomes dimmer and closer to a `` dot '' in the middle of the image . In this case , it is easy to infer \\alpha based on the length and brightness of the line . This is exactly the kind of behavior that ACAI seeks to discourage , and we find it 's effective in practice . We have added some additional discussion of this point to our paper . > The paper states that \u201c encouraging this behavior also produce semantically smooth interpolation \u2026 \u201d . Besides the empirical evidences from data , it would be better to any some theoretical justifications . Our approach can be viewed in the framework of adversarial divergences , where the critic network is being used to estimate a divergence . Of course , the exact form of this divergence is not clear , but it does provide a connection to the GAN theory literature . We have made this connection explicit in our updated draft ."}], "0": {"review_id": "S1fQSiCcYm-0", "review_text": "Main idea: This paper investigates the desiderata for a successful interpolation: 1) Interpolation looks realistic; 2) The interpolation path is semantically smooth. An adversarial regularizer is proposed to achieve 1), and in practice 2) may also satisfied. To evaluate the method, they introduce a synthetic dataset with line images and compare with different autoencoder methods without the interpolation regularization. For real data, they show that the interpolation regularized autoencoder (i.e. ACAI) leads to a better unsupervised representation. Questions: 1. Do we really need every interpolated point to be realistic (i.e. similar to a data point in the train-set)? I believe that there exists an interpolation between two totally different objects can never be observed. 2. Do we need interpolation points to form a semantically smooth morphing? I guess this is a desired property for continuous generators, but it seems not necessary in general. 3. The gamma in the 2nd term in (1) is confusing. If gamma = 1, I understand it forces to predict alpha = 0 since x is real. But if gamma < 1, the average in data space may be very blurry thus not realistic at all. How does gamma affect the optimization? 4. ACAI looks very similar to LSGAN: by giving \"0\" label to real data and \"alpha\" label to fake data; in LSGAN, alpha = 1. Have you tested a LSGAN like regularizer? 5. The baselines are not representative: since ACAI introduces an adversarial regularizer, you should compare with other GAN techniques induced regularizers, such as WGAN regularized autoencoder. After rebuttal: See the long discussion below. I tend to believe that a good interpolation is not only a way to do sanity check but also a nice property to explicitly control in representation learning.", "rating": "7: Good paper, accept", "reply_text": "Thanks for your thorough review and questions . We 've answered your questions below and have updated our draft to clarify . > Do we really need every interpolated point to be realistic ( i.e.similar to a data point in the train-set ) ? I believe that there exists an interpolation between two totally different objects can never be observed . We are interested in latent spaces where interpolations produce realistic outputs across the entirety of the interpolation because this suggests some form of continuity in the latent space ( as illustrated in FIgure 1 ) . Our paper asks whether this property also results in an improved representation for downstream tasks . If an intermediate point was not realistic , the latent space might not have this property . Thanks for pointing out that in some cases it 's not obvious that there is a smooth and realistic path between two datapoints . We think two good examples of this are in Figure 6 , bottom , where we interpolate between different MNIST digits . We find that even though there is no real digit which is at the midpoint of , for example , a 2 and a 9 , the midpoint of ACAI 's interpolation still appears realistic . We have added a note about this to our paper . > Do we need interpolation points to form a semantically smooth morphing ? I guess this is a desired property for continuous generators , but it seems not necessary in general . We agree that smoothness is not required for high-quality learned features -- for example , the Denoising Autoencoder fared well on our classification experiments despite producing poor interpolations . However , we are interested in the opposite , namely whether the ability to perform latent-space manipulations like interpolation suggest a better learned representation . We have added some clarification of this point in our paper . > The gamma in the 2nd term in ( 1 ) is confusing . If gamma = 1 , I understand it forces to predict alpha = 0 since x is real . But if gamma < 1 , the average in data space may be very blurry thus not realistic at all . How does gamma affect the optimization ? Note \\hat { x } is a reconstruction of x , so in practice \\gamma * x + ( 1 \u2212 \\gamma ) * \\hat { x } will be quite similar to x as long as \\hat { x } is a reasonable reconstruction . In other words , we are not interpolating between two totally different points , so typically the blurriness you might expect from pixel-space mixing wo n't be present . We have added some additional discussion of gamma and this term to our paper . > ACAI looks very similar to LSGAN : by giving `` 0 '' label to real data and `` alpha '' label to fake data ; in LSGAN , alpha = 1 . Have you tested a LSGAN like regularizer ? You 're right that the LSGAN loss function and our regularization term are similar in the sense that both measure a squared error between the critic 's output and a scalar . The difference is that the LSGAN is designed for use on a GAN-based generative model ; our regularizer is designed as a regularizer for an autoencoder . As a result , the scalar in the LSGAN objective is a fixed hyperparameter whereas we regress the interpolation amount \\alpha . We added some discussion of the LSGAN objective to our paper . > The baselines are not representative : since ACAI introduces an adversarial regularizer , you should compare with other GAN techniques induced regularizers , such as WGAN regularized autoencoder . Note that the Wasserstein Autoencoder ( WAE ) is actually equivalent to an adversarial autoencoder when using a GAN loss ; in the WAE paper [ 1 ] they write `` When c is the squared cost and D_Z is the GAN objective , WAE coincides with adversarial auto-encoders '' . Our paper includes the adversarial autoencoder as a baseline ( labeled AAE in tables and described in Section 3.2 , paragraph 4 ) . We added a citation to [ 1 ] to clarify this . [ 1 ] Ilya Tolstikhin , Olivier Bousquet , Sylvain Gelly and Bernhard Schoelkopf . `` Wasserstein Auto-Encoders '' , ICLR 2017 ."}, "1": {"review_id": "S1fQSiCcYm-1", "review_text": "Summary: The authors propose a new approach to encourage valid interpolation in Auto-Encoders (AE). It is based on a regularization procedure involving a critic network judging the realistic nature of reconstructed data point from its mixed latent representations by recovering the mixing coefficient. The authors show that this approach does indeed improve the quality of interpolated samples on few tasks. A synthetic tasks of lines interpolation (proposing new Mean Distance and Smoothness metric for this task), classification task (with a single-layer classifier) from the latent space representation and finally a clustering accuracy on the latent space. On the proposed regularization method seems to help significantly compared to commonly used AE architectures (Basic AE, Denoising AE, Variational AE, Adversarial AE and VQ-VAE). This paper was a very interesting read, and the work seems to be of significance for the unsupervised learning community. It was clearly written and conveys the contributions clearly and the experimental results and their interpretations seem valid. The proposed approach of a critic based regularizer is a simple but seemingly important addition that contributes to improving interpolation in AE significantly and even show impact \"downstream tasks\" as the authors put it. Few comments/questions come to mind: - For the critic Loss L_d in equation (1) , the authors mention that the \\gamma based second term (that should ensure that the critic outputs 0 for non-interpolated inputs and expose the critic to realistic data even if the AE reconstruction is poor) does not seem to be crucial in your approach but stabilized the adversarial training. Could you somehow quantify this. It seems like stability of the adversarial training should be paramount to your method to make sure the AE learns a better latent representation. This comment, even though I assume it well-founded, seems a bit of a contradiction. - For the Lines synthetic data. It was chosen to use a 32x32 image size with 16 points length lines. This configuration does quantize directly the angles your measures can distinguish. Below a certain angle differences (or delta), 2 angles must have the same pixel representation, i.e. exact overlapping lines. My question is simple: What is the smallest angle you can use/distinguish or, how many exact unique lines can you have? Overall this is a good paper that deserves publications.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for your review , we are glad you found the paper interesting and significant . To address your questions and comments : > For the critic Loss L_d in equation ( 1 ) , the authors mention that the \\gamma based second term ( that should ensure that the critic outputs 0 for non-interpolated inputs and expose the critic to realistic data even if the AE reconstruction is poor ) does not seem to be crucial in your approach but stabilized the adversarial training . Could you somehow quantify this . It seems like stability of the adversarial training should be paramount to your method to make sure the AE learns a better latent representation . This comment , even though I assume it well-founded , seems a bit of a contradiction . We agree that this comment should be expanded on , and we have done so in our updated draft . To clarify , when we say it `` helped stabilize the adversarial learning process '' , we mean that a ) it allowed us to use the same value of \\lambda across all of our experiments and still achieve good results and b ) it resulted in smooth convergence of the autoencoder 's loss . We note that stability of the adversarial learning process was not an issue in general , in the sense that stability across runs was not an issue and our model never `` collapsed '' to a bad solution . > For the Lines synthetic data . It was chosen to use a 32x32 image size with 16 points length lines . This configuration does quantize directly the angles your measures can distinguish . Below a certain angle differences ( or delta ) , 2 angles must have the same pixel representation , i.e.exact overlapping lines . My question is simple : What is the smallest angle you can use/distinguish or , how many exact unique lines can you have ? Our code for synthesizing line images uses anti-aliasing , so for example a line with angle 0.3 and another with angle 0.300001 will be rendered differently . As a result , the number of unique lines is actually up to floating point precision . We think some confusion about this probably stems from the fact that we referred to the line images as `` black-and-white '' ; we have updated the language in the paper to say `` grayscale '' ."}, "2": {"review_id": "S1fQSiCcYm-2", "review_text": "This paper proposed an adversarially regularized AE algorithm that improve interpolation in latent space. Specifically, a critic is used to predict the interpolation weight \\alpha and encourage the interpolated images to be more realistic. The paper verified the method on a newly proposed synthetic line benchmark and on downstream classification and clustering tasks. Pros: 1. A novel algorithm that promotes the interpolation ability of AE 2. A new synthesized line benchmark to verify the interpolation ability of different AE variants 3. Strong results on downstream classification and clustering tasks Cons: 1. The interplay of the adversarial network (between AE and critic) isn\u2019t very clear and can be improved 2. Eq. 1, should x be x_1 or a new data other than x1 and x2? 3. The paper states that the 2nd term of Eq. 1 isn\u2019t crucial. If x is a new data (other than x1 or x2), how can the critic infer \\alpha without a reference to x1 or x2? 4. The paper states that \u201cencouraging this behavior also produce semantically smooth interpolation \u2026\u201d. Besides the empirical evidences from data, it would be better to any some theoretical justifications. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thanks for your review and thoughtful analysis . To address each of your cons in turn : > The interplay of the adversarial network ( between AE and critic ) isn \u2019 t very clear and can be improved . The goal of the critic is to predict the interpolation mixing coefficient \\alpha ; the goal of the autoencoder is to `` fool '' the critic into outputting \\alpha = 0 . It can be useful to think of the critic as estimating a divergence between real and interpolated datapoints , and the autoencoder is trying to minimize this divergence . We have added some discussion of this to our paper . > Eq.1 , should x be x_1 or a new data other than x1 and x2 ? It actually can be any real datapoint x - the second term can be computed separately from the first . We have clarified this in our updated draft . > The paper states that the 2nd term of Eq.1 isn \u2019 t crucial . If x is a new data ( other than x1 or x2 ) , how can the critic infer \\alpha without a reference to x1 or x2 ? The critic must infer \\alpha from common artifacts of interpolated datapoints alone . This is best illustrated in Figure 3 ( a ) - note that as the interpolation morphs from one endpoint to the other , the image becomes dimmer and closer to a `` dot '' in the middle of the image . In this case , it is easy to infer \\alpha based on the length and brightness of the line . This is exactly the kind of behavior that ACAI seeks to discourage , and we find it 's effective in practice . We have added some additional discussion of this point to our paper . > The paper states that \u201c encouraging this behavior also produce semantically smooth interpolation \u2026 \u201d . Besides the empirical evidences from data , it would be better to any some theoretical justifications . Our approach can be viewed in the framework of adversarial divergences , where the critic network is being used to estimate a divergence . Of course , the exact form of this divergence is not clear , but it does provide a connection to the GAN theory literature . We have made this connection explicit in our updated draft ."}}