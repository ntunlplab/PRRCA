{"year": "2017", "forum": "HJ1JBJ5gl", "title": "Representing inferential uncertainty in deep neural networks through sampling", "decision": "Reject", "meta_review": "The reviewers unanimously recommend rejecting this paper.", "reviews": [{"review_id": "HJ1JBJ5gl-0", "review_text": "This paper investigates whether the variational inference interpretation of dropout, as introduced in [Gal & Ghahramani (2016), and Kingma et al (2015)], can lead to good estimates of mode uncertainty outside of the training distribution. This is an area of research that indeed warrants more experimental investigation. One very interesting finding is that MC integration leads to much calibration, thus probably much better out-of-sample prediction, than the more usual. Critique: - As explained in Kingma et al (2015), when using continuous posterior distributions over the weights, the dropout rate can be optimized, leading to better regularization. While the paper is cited in the introduction, this adaptive form of dropout is missing from experiments, without clarification. - Only the dropout rate p=0.5 was used across experiments, while the optimal rate is problem dependent, as found by earlier published work. - No new ideas are presented, and the analysis in the paper is quite limited. As it stands, this would be more appropriate for a workshop.", "rating": "4: Ok but not good enough - rejection", "reply_text": "-Only the dropout rate p=0.5 was used across experiments , while the optimal rate is problem dependent , as found by earlier published work . We agree that optimizing the dropout/dropconnect parameters , either as hyperparmeters or as learnable parameters during training , can lead to improved performance . We decided to use p = 0.5 for all experiments since it : ( 1 ) corresponds to maximum regularization in linear layers ( Baldi and Sadowski , 2013 ) , ( 2 ) it is the most commonly used dropout/dropconnect parameter value , and ( 3 ) it allows us to easily compare the different distributions without the added complication of vastly different dropout/dropconnect probabilities . - As explained in Kingma et al ( 2015 ) , when using continuous posterior distributions over the weights , the dropout rate can be optimized , leading to better regularization.While the paper is cited in the introduction , this adaptive form of dropout is missing from experiments , without clarification . We now discuss adaptive dropout/dropconnect parameter learning ( Kingma et al. , 2015 ; Louizos , 2015 ; Gal 2016 ) in both the introduction and methods sections ."}, {"review_id": "HJ1JBJ5gl-1", "review_text": "This paper presents an empirical evaluation of the effect of training with noise by dropout or dropconnect, on the predictive uncertainty of neural networks and convnets. The conclusion seems to be that applying both training and inference with noise can help the network produce better uncertainty estimates in terms of better calibrated predictions. Although the experiments were thorough, the issue with an empirical paper like this is that it is very difficult to ensure that the lessons learned will generalize across problems and domains. This paper only investigated two simple image datasets and two neural network architectures on the task of classification. There are several ways in which I think this paper could be made stronger. First, the problem is not well motivated: why do we care about uncertainty (although I believe we do)? A good application, or a motivation section would be beneficial here. Next, what about investigating a different domain such as text or speech recognition? Or other problems such as regression? Do the results hold across different domains? Finally, if we really care about uncertainty, there are a number of other techniques for inference in Bayesian neural networks such as stochastic variational inference using the reparameterization trick, or MCMC methods like stochastic gradient Langevin dynamics. The advantage of dropout is that it\u2019s simple and fast, but do we lose anything by doing this in terms of calibration? Other than the empirical comparison, there is little novelty to this paper, and therefore I think the conclusions drawn need to be more general or the motivation more compelling. Even addressing a subset of these suggestions would make the paper stronger in my opinion. In Figure 4 the calibration MSE does not look so robust when MC sampling is not used. Do you have any hypothesis on why MC sampling is so important here? In Figure 5, the legend says \u201cCNN\u201d when it should just be \u201cNN\u201d. Also the caption says convolutional, when it should say fully connected. In Table 3, why are the results so poor for non-MC-based dropout? Although this agrees with Gal and Ghahramani, this seems to be in direct contradiction to Table 4 of the original JMLR dropout paper by Srivastava et al from 2014. How is the noise in the test set applied for Figures 5-10? Is it Gaussian noise applied to the pixels? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "-In Table 3 , why are the results so poor for non-MC-based dropout ? Although this agrees with Gal and Ghahramani , this seems to be in direct contradiction to Table 4 of the original JMLR dropout paper by Srivastava et al from 2014 . In our experience , MC dropout is much more robust than non-MC dropout to changes in the dropout probability and architecture . Particularly for deeper networks with Bernoulli dropout , higher regularization ( i.e.dropout probabilities closer to 0.5 ) results in non-MC dropout inference performing significantly worse than MC dropout for the same training error . The CIFAR-10 network used by Srivastava et al . ( 2014 ) contained three convolutional layers followed by two fully connected layers . Input units were kept with 0.9 probability , units in the first two convolutional layers were kept with 0.75 probability , and units in the third convolutional and two fully connected layers were kept with a probability of 0.5 . The architecture that we used had thirteen convolutional layers and a fully connected layer with every hidden layer unit being kept with a 0.5 probability . This setup works well when MC sampling is used at inference , but leads to decreased accuracy for non-sampling dropout inference . We now mention this in the CIFAR-10 results section . -For different dropout probabilities , the calibration MSE does not look so robust when MC sampling is not used . Do you have any hypothesis on why MC sampling is so important here ? One potential reason is that each dropout/dropconnect sampled network is trained to make good probabilistic predictions . MC sampling at inference may take advantage of this by averaging the outputs of dropout/dropconnect sampled networks during inference instead of approximating the ensemble of the trained networks by using the expectation of each layer as done in traditional dropout inference . -How is the noise in the test set applied for Figures 5-10 ? Is it Gaussian noise applied to the pixels ? Yes , Gaussian noise was added to the image pixels in z-score space . We have clarified this in the revision ."}, {"review_id": "HJ1JBJ5gl-2", "review_text": "The authors study the calibration of neural networks using different variants of dropout and weight noise. They find that sampling during training and testing improves calibration. Most of the results are not novel and have been discussed previously by Yarin Gal and other authors. What this paper adds is a more systematic evaluation of multiple different variants of dropout. - A comparison against bootstrapped uncertainty estimates would be useful - Ian Osband had a paper at NIPS investigating exactly this kind of uncertainty representation in neural nets. He found that dropout represents the risk of the model, but not really the uncertainty. This difference becomes apparent when e.g. the target of the model is bi-modal.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for pointing us to Ian Osband \u2019 s NIPS paper , which is very interesting and relevant . A key difference between the previous ( e.g.Yarin Gal \u2019 s and Ian Osband \u2019 s ) work and ours is that they are attempting to estimate the uncertainty for the learned parameters given the training data . However , we focus on evaluating how accurately a network distributes the probability mass in its output probability distributions , particularly for new examples . This distribution of probability mass is often characterized by entropy , which measures the amount of uncertainty in a probability distribution . Analyzing the distribution of probability mass in the networks \u2019 outputs instead of attempting to estimate the confidence in the parameters learned during training , as done by previous work , is one the contributions of our work . We will discuss the relationship of our results to those of Ian Osband \u2019 s paper in the Introduction section of our paper ."}], "0": {"review_id": "HJ1JBJ5gl-0", "review_text": "This paper investigates whether the variational inference interpretation of dropout, as introduced in [Gal & Ghahramani (2016), and Kingma et al (2015)], can lead to good estimates of mode uncertainty outside of the training distribution. This is an area of research that indeed warrants more experimental investigation. One very interesting finding is that MC integration leads to much calibration, thus probably much better out-of-sample prediction, than the more usual. Critique: - As explained in Kingma et al (2015), when using continuous posterior distributions over the weights, the dropout rate can be optimized, leading to better regularization. While the paper is cited in the introduction, this adaptive form of dropout is missing from experiments, without clarification. - Only the dropout rate p=0.5 was used across experiments, while the optimal rate is problem dependent, as found by earlier published work. - No new ideas are presented, and the analysis in the paper is quite limited. As it stands, this would be more appropriate for a workshop.", "rating": "4: Ok but not good enough - rejection", "reply_text": "-Only the dropout rate p=0.5 was used across experiments , while the optimal rate is problem dependent , as found by earlier published work . We agree that optimizing the dropout/dropconnect parameters , either as hyperparmeters or as learnable parameters during training , can lead to improved performance . We decided to use p = 0.5 for all experiments since it : ( 1 ) corresponds to maximum regularization in linear layers ( Baldi and Sadowski , 2013 ) , ( 2 ) it is the most commonly used dropout/dropconnect parameter value , and ( 3 ) it allows us to easily compare the different distributions without the added complication of vastly different dropout/dropconnect probabilities . - As explained in Kingma et al ( 2015 ) , when using continuous posterior distributions over the weights , the dropout rate can be optimized , leading to better regularization.While the paper is cited in the introduction , this adaptive form of dropout is missing from experiments , without clarification . We now discuss adaptive dropout/dropconnect parameter learning ( Kingma et al. , 2015 ; Louizos , 2015 ; Gal 2016 ) in both the introduction and methods sections ."}, "1": {"review_id": "HJ1JBJ5gl-1", "review_text": "This paper presents an empirical evaluation of the effect of training with noise by dropout or dropconnect, on the predictive uncertainty of neural networks and convnets. The conclusion seems to be that applying both training and inference with noise can help the network produce better uncertainty estimates in terms of better calibrated predictions. Although the experiments were thorough, the issue with an empirical paper like this is that it is very difficult to ensure that the lessons learned will generalize across problems and domains. This paper only investigated two simple image datasets and two neural network architectures on the task of classification. There are several ways in which I think this paper could be made stronger. First, the problem is not well motivated: why do we care about uncertainty (although I believe we do)? A good application, or a motivation section would be beneficial here. Next, what about investigating a different domain such as text or speech recognition? Or other problems such as regression? Do the results hold across different domains? Finally, if we really care about uncertainty, there are a number of other techniques for inference in Bayesian neural networks such as stochastic variational inference using the reparameterization trick, or MCMC methods like stochastic gradient Langevin dynamics. The advantage of dropout is that it\u2019s simple and fast, but do we lose anything by doing this in terms of calibration? Other than the empirical comparison, there is little novelty to this paper, and therefore I think the conclusions drawn need to be more general or the motivation more compelling. Even addressing a subset of these suggestions would make the paper stronger in my opinion. In Figure 4 the calibration MSE does not look so robust when MC sampling is not used. Do you have any hypothesis on why MC sampling is so important here? In Figure 5, the legend says \u201cCNN\u201d when it should just be \u201cNN\u201d. Also the caption says convolutional, when it should say fully connected. In Table 3, why are the results so poor for non-MC-based dropout? Although this agrees with Gal and Ghahramani, this seems to be in direct contradiction to Table 4 of the original JMLR dropout paper by Srivastava et al from 2014. How is the noise in the test set applied for Figures 5-10? Is it Gaussian noise applied to the pixels? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "-In Table 3 , why are the results so poor for non-MC-based dropout ? Although this agrees with Gal and Ghahramani , this seems to be in direct contradiction to Table 4 of the original JMLR dropout paper by Srivastava et al from 2014 . In our experience , MC dropout is much more robust than non-MC dropout to changes in the dropout probability and architecture . Particularly for deeper networks with Bernoulli dropout , higher regularization ( i.e.dropout probabilities closer to 0.5 ) results in non-MC dropout inference performing significantly worse than MC dropout for the same training error . The CIFAR-10 network used by Srivastava et al . ( 2014 ) contained three convolutional layers followed by two fully connected layers . Input units were kept with 0.9 probability , units in the first two convolutional layers were kept with 0.75 probability , and units in the third convolutional and two fully connected layers were kept with a probability of 0.5 . The architecture that we used had thirteen convolutional layers and a fully connected layer with every hidden layer unit being kept with a 0.5 probability . This setup works well when MC sampling is used at inference , but leads to decreased accuracy for non-sampling dropout inference . We now mention this in the CIFAR-10 results section . -For different dropout probabilities , the calibration MSE does not look so robust when MC sampling is not used . Do you have any hypothesis on why MC sampling is so important here ? One potential reason is that each dropout/dropconnect sampled network is trained to make good probabilistic predictions . MC sampling at inference may take advantage of this by averaging the outputs of dropout/dropconnect sampled networks during inference instead of approximating the ensemble of the trained networks by using the expectation of each layer as done in traditional dropout inference . -How is the noise in the test set applied for Figures 5-10 ? Is it Gaussian noise applied to the pixels ? Yes , Gaussian noise was added to the image pixels in z-score space . We have clarified this in the revision ."}, "2": {"review_id": "HJ1JBJ5gl-2", "review_text": "The authors study the calibration of neural networks using different variants of dropout and weight noise. They find that sampling during training and testing improves calibration. Most of the results are not novel and have been discussed previously by Yarin Gal and other authors. What this paper adds is a more systematic evaluation of multiple different variants of dropout. - A comparison against bootstrapped uncertainty estimates would be useful - Ian Osband had a paper at NIPS investigating exactly this kind of uncertainty representation in neural nets. He found that dropout represents the risk of the model, but not really the uncertainty. This difference becomes apparent when e.g. the target of the model is bi-modal.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for pointing us to Ian Osband \u2019 s NIPS paper , which is very interesting and relevant . A key difference between the previous ( e.g.Yarin Gal \u2019 s and Ian Osband \u2019 s ) work and ours is that they are attempting to estimate the uncertainty for the learned parameters given the training data . However , we focus on evaluating how accurately a network distributes the probability mass in its output probability distributions , particularly for new examples . This distribution of probability mass is often characterized by entropy , which measures the amount of uncertainty in a probability distribution . Analyzing the distribution of probability mass in the networks \u2019 outputs instead of attempting to estimate the confidence in the parameters learned during training , as done by previous work , is one the contributions of our work . We will discuss the relationship of our results to those of Ian Osband \u2019 s paper in the Introduction section of our paper ."}}