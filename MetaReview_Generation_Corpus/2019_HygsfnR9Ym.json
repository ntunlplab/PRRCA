{"year": "2019", "forum": "HygsfnR9Ym", "title": "Recall Traces: Backtracking Models for Efficient Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "The paper presents \"recall traces\", a model based approach designed to improve reinforcement learning in sparse reward settings. The approach learns a generative model of trajectories leading to high-reward states, and is subsequently used to augment the real experience collected by the agent. This novel take on combining model-based and model-free learning is conceptually well motivated and is empirically shown to improve sample efficiency on several benchmark tasks.\n\nThe reviewers noted the following potential weaknesses in their initial reviews: the paper could provide a clearer motivation of why the proposed approach is expected to lead to performance improvements, and how it relates to learning (and uses of) a forward model. Details of the method, e.g., model parameterization is unclear, and the effect of hyperparameter choices is not fully evaluated.\n\nThe authors provided detailed replies to all reviewer suggestions, and ran extensive new experiments, including experiments to address questions about hyperparameter settings, and an entirely new use of the proposed model in a learning from demonstration setting. The authors also clarified the paper as requested by the reviewers. The reviewers have not responded to the rebuttal, but in the AC's assessment their concerns have been adequately addressed. The reviewers have updated their scores in response to the rebuttal, and the consensus is to accept the paper.\n\nThe AC notes that the authors seem unaware of related work by Oh et al. \"Self Imitation Learning\" which was published at ICML 2018. The paper is based on a similar conceptual motivation but imitates high-value traces directly, instead of using a generative model. The authors should include a discussion of how their paper relates to this earlier work in their camera ready version.", "reviews": [{"review_id": "HygsfnR9Ym-0", "review_text": "Revision: The authors have thoroughly addressed my review and I have consequently updated my rating accordingly. Summary: Model-free reinforcement learning is inefficient at exploration if rewards are sparse / low probability. The paper proposes a variational model for online learning to backtrack state / action traces that lead to high reward states based on best previous samples. The backtracking models' generated recall traces are then used to augment policy training by imitation learning, i.e. by optimizing policy to take actions that are taken from the current states in generated recall traces. Overall, the methodology seems akin to an adaptive importance sampling approach for reinforcement learning. Evaluation: The paper gives a clear (at least mathematically) presentation of the core idea but it some details about modeling choices seem to be missing. The experimental evaluation seems preliminary and it is not fully evident when and how the proposed method will be practically relevant (and not relevant). My knowledgable of the previous literature is not sufficient to validate the claimed novelty of the approach. Details: The paper is well written and easy to follow in general. I'm not familiar enough with reinforcment learning benchmarks to judge the quality of the experiments compared to the literature as a whole. Although there are quite a few experiments they seem rather preliminary. It is not clear whether enough work was done to understand the effect of the many different hyperparameters that the proposed method surely must have. The authors claim to show empirically that their method can improve sample efficiency. This is not necessarily a strong claim as such and could be achieved on relatively simple tests. In the discussion the authors claim their results indicate that their approach is able to accelearte learning on a variety of tasks, also not a strong claim. The paper could be improved by adding a more clear explanation of the exact way by which the method helps with exploration and how it affects finding sparse rewards (based on e.g. Figure 1). It seems that since only knowledge of seen trajectories can be used to generate paths to high reward states it only works for generating new trajectories through previously visited states. Questions that could be clarified: - It is not entirely obvious to me what parametric models are used for the backtracking distributions. - Does this method not also potentially hinder exploration by making the agent learn to go after the same high rewards / Does the direction of the variational problem guarantee coverage of the support of the R > L distribution by samples? - What would be the effect of a hyperparameter that balances learning the recall traces and learning the true environment? - Are there also reinforcement learning tasks where the proposed methods' improvement is marginal and the extra modeling effort is not justified (e.g. due to increase complexity). Page 1: iwth (Typo) Page 2: r(s_t) -> r(s_t, a_t) Page 6: Prioritize d (Typo) ", "rating": "7: Good paper, accept", "reply_text": "Thanks for the very thorough feedback . We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below . We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have . `` I 'm not familiar enough with reinforcement learning benchmarks to judge the quality of the experiments compared to the literature as a whole . '' The goal of our experimental evaluation is to demonstrate the effectiveness of the proposed algorithm . We demonstrate that the effectiveness by comparing the proposed algorithm in case when the true backtracking env . was avaliable , as well as when we learned the backtracking model too . We compare our methods to the state-of-the-art SAC algorithm on MuJoCo tasks in OpenAI gym ( Brockman et al. , 2016 ) and in rllab ( Duan et al. , 2016 ) . We use SAC as a baseline as it notably outperforms other existing methods like DDPG , Soft-Q Learning and TD3 . The results show that our method outperform on par with SAC in simple domains like swimmer , walker etc . They also provide evidence that the proposed method outperform SAC in challenging high dimensional domains like humanoid and Ant ( Figure 7 , Main Paper ) . `` It is not entirely obvious to me what parametric models are used for the backtracking distributions . '' The backtracking model we used for all the experiments consisted of two multi-layer perceptrons : one for the backward action predictor Q ( a_t | s_t+1 ) and one for the backward state predictor Q ( s_t | a_t , s_t+1 ) . Both MLPs had two hidden layers of 128 units . The action predictor used hyperbolic tangent units while the inverse state predictor used ReLU units . Each network produced as output the mean and variance parameters of a Gaussian distribution . For the action predictor the output variance was fixed to 1 . For the state predictor this value was learned for each dimension . We have also mentioned this in the appendix ."}, {"review_id": "HygsfnR9Ym-1", "review_text": "This paper nicely proposes a back-tracking model that predicts the trajectories that may lead to high-value states. The proposed approach was shown to be effective in improving sample efficiency for a number of environments and tasks. This paper looks solid to me, well-written motivation with theoretical interpretations, although I am not an expert in RL. Comments / questions: - how does the backtracking model correspond to a forward-model? And it doesn't seem to be contradictory to me that the two can work together. - could the authors give a bit more explanation on why the backtracking model and the policy are trained jointly? Would it still work if to train the backtracking model offline by, say, watching demonstration? Overall this looks like a nice paper. ", "rating": "7: Good paper, accept", "reply_text": "The authors thank the reviewer for the positive and constructive feedback . We appreciate that the reviewer finds that our method is clearly explained . `` how does the backtracking model correspond to a forward-model ? And it does n't seem to be contradictory to me that the two can work together . '' The reviewer raises a good point . This is indeed very useful . The Dyna algorithm uses a forward model to generate simulated experience that could be included in a model-free algorithm . This method was used to work with deep neural network policies , but performed best with models which are not neural networks ( Gu et al. , 2016a ) . Our intuition ( and as we empirically show , Figure 19 , Section H of Appendix ) says that it might be better to generate simulated experience from a backtracking model ( starting from a high value state ) as compared to forward model , just because we know that traces from the backtracking model are good traces , as they lead to high value state , which is not necessarily the case for the simulated experience from a forward model . We have added Figure 16 in Appendix ( Section G ) where we evaluate the Forward model with On-Policy TRPO on Ant and Humanoid Mujoco tasks . We were not able to get any better results on with forward model as compared to the Baseline TRPO , which is consistent with the findings from ( Gu et al. , 2016a ) . In essence , building the backward model is necessarily neither harder nor easier . Realistically , building any kind of model and having it be accurate for more than , say , 10 time steps is pretty hard . But if we only have 10 time steps of accurate transitions , it is probably better to take them backward model from different states as compared to from forward model from the same initial state . ( as corroborated by the findings in Fig 16 of Appendix G , and Figure 19 of Appendix H ) . Something which remains as a part of future investigation is to train the forward model and backtracking model jointly . As the backtracking model is tied to high value states , the forward model could extract the intended goal value from the high value state . When trained jointly , this should help the forward model learn some reduced representation of the state that is necessary to evaluate the reward . Ultimately , when planning , we want the model to predict the goal accurately , which helps to optimize for this \u201d goal-oriented \u201d behaviour directly . This also avoids the need to model irrelevant aspects of the environment . We also mention this in Appendix ( Section G ) . [ 1 ] ( Gu et al , 2016 ) Continuous Deep Q-Learning with Model-based Acceleration http : //proceedings.mlr.press/v48/gu16.html"}, {"review_id": "HygsfnR9Ym-2", "review_text": "The authors propose a bidirectional model for learning a policy. In particular, a backtracking model was proposed to start from a high-value state and sample back the sequence of actions and states that could lead to the current high-value state. These traces can be used later for learning a good policy. The experiments show the effectiveness of the model in terms of increase the expected rewards in different tasks. However, learning the backtracking model would add some computational efforts to the entire learning phase. I would like to see experiments to show the computational time for these components. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the positive and constructive feedback . `` I would like to see experiments to show the computational time for these components . '' If a backtracking model model is available ( like in the maze example ) , then there is no extra computation time , but in the case where we have to learn a bw model , learning a bw model requires more updates compared to only earning a policy ( but a similar number of updates as compared to learning a forward model , i.e. , dynamics model of the environment ) . Please let us know if anything is unclear here , or if there is any other comparison that would be helpful in clarifying things more ."}], "0": {"review_id": "HygsfnR9Ym-0", "review_text": "Revision: The authors have thoroughly addressed my review and I have consequently updated my rating accordingly. Summary: Model-free reinforcement learning is inefficient at exploration if rewards are sparse / low probability. The paper proposes a variational model for online learning to backtrack state / action traces that lead to high reward states based on best previous samples. The backtracking models' generated recall traces are then used to augment policy training by imitation learning, i.e. by optimizing policy to take actions that are taken from the current states in generated recall traces. Overall, the methodology seems akin to an adaptive importance sampling approach for reinforcement learning. Evaluation: The paper gives a clear (at least mathematically) presentation of the core idea but it some details about modeling choices seem to be missing. The experimental evaluation seems preliminary and it is not fully evident when and how the proposed method will be practically relevant (and not relevant). My knowledgable of the previous literature is not sufficient to validate the claimed novelty of the approach. Details: The paper is well written and easy to follow in general. I'm not familiar enough with reinforcment learning benchmarks to judge the quality of the experiments compared to the literature as a whole. Although there are quite a few experiments they seem rather preliminary. It is not clear whether enough work was done to understand the effect of the many different hyperparameters that the proposed method surely must have. The authors claim to show empirically that their method can improve sample efficiency. This is not necessarily a strong claim as such and could be achieved on relatively simple tests. In the discussion the authors claim their results indicate that their approach is able to accelearte learning on a variety of tasks, also not a strong claim. The paper could be improved by adding a more clear explanation of the exact way by which the method helps with exploration and how it affects finding sparse rewards (based on e.g. Figure 1). It seems that since only knowledge of seen trajectories can be used to generate paths to high reward states it only works for generating new trajectories through previously visited states. Questions that could be clarified: - It is not entirely obvious to me what parametric models are used for the backtracking distributions. - Does this method not also potentially hinder exploration by making the agent learn to go after the same high rewards / Does the direction of the variational problem guarantee coverage of the support of the R > L distribution by samples? - What would be the effect of a hyperparameter that balances learning the recall traces and learning the true environment? - Are there also reinforcement learning tasks where the proposed methods' improvement is marginal and the extra modeling effort is not justified (e.g. due to increase complexity). Page 1: iwth (Typo) Page 2: r(s_t) -> r(s_t, a_t) Page 6: Prioritize d (Typo) ", "rating": "7: Good paper, accept", "reply_text": "Thanks for the very thorough feedback . We have conducted additional experiments to address the concerns raised about the evaluation , and we clarify specific points below . We believe that these additions address all of your concerns about the work , though we would appreciate any additional comments or feedback that you might have . `` I 'm not familiar enough with reinforcement learning benchmarks to judge the quality of the experiments compared to the literature as a whole . '' The goal of our experimental evaluation is to demonstrate the effectiveness of the proposed algorithm . We demonstrate that the effectiveness by comparing the proposed algorithm in case when the true backtracking env . was avaliable , as well as when we learned the backtracking model too . We compare our methods to the state-of-the-art SAC algorithm on MuJoCo tasks in OpenAI gym ( Brockman et al. , 2016 ) and in rllab ( Duan et al. , 2016 ) . We use SAC as a baseline as it notably outperforms other existing methods like DDPG , Soft-Q Learning and TD3 . The results show that our method outperform on par with SAC in simple domains like swimmer , walker etc . They also provide evidence that the proposed method outperform SAC in challenging high dimensional domains like humanoid and Ant ( Figure 7 , Main Paper ) . `` It is not entirely obvious to me what parametric models are used for the backtracking distributions . '' The backtracking model we used for all the experiments consisted of two multi-layer perceptrons : one for the backward action predictor Q ( a_t | s_t+1 ) and one for the backward state predictor Q ( s_t | a_t , s_t+1 ) . Both MLPs had two hidden layers of 128 units . The action predictor used hyperbolic tangent units while the inverse state predictor used ReLU units . Each network produced as output the mean and variance parameters of a Gaussian distribution . For the action predictor the output variance was fixed to 1 . For the state predictor this value was learned for each dimension . We have also mentioned this in the appendix ."}, "1": {"review_id": "HygsfnR9Ym-1", "review_text": "This paper nicely proposes a back-tracking model that predicts the trajectories that may lead to high-value states. The proposed approach was shown to be effective in improving sample efficiency for a number of environments and tasks. This paper looks solid to me, well-written motivation with theoretical interpretations, although I am not an expert in RL. Comments / questions: - how does the backtracking model correspond to a forward-model? And it doesn't seem to be contradictory to me that the two can work together. - could the authors give a bit more explanation on why the backtracking model and the policy are trained jointly? Would it still work if to train the backtracking model offline by, say, watching demonstration? Overall this looks like a nice paper. ", "rating": "7: Good paper, accept", "reply_text": "The authors thank the reviewer for the positive and constructive feedback . We appreciate that the reviewer finds that our method is clearly explained . `` how does the backtracking model correspond to a forward-model ? And it does n't seem to be contradictory to me that the two can work together . '' The reviewer raises a good point . This is indeed very useful . The Dyna algorithm uses a forward model to generate simulated experience that could be included in a model-free algorithm . This method was used to work with deep neural network policies , but performed best with models which are not neural networks ( Gu et al. , 2016a ) . Our intuition ( and as we empirically show , Figure 19 , Section H of Appendix ) says that it might be better to generate simulated experience from a backtracking model ( starting from a high value state ) as compared to forward model , just because we know that traces from the backtracking model are good traces , as they lead to high value state , which is not necessarily the case for the simulated experience from a forward model . We have added Figure 16 in Appendix ( Section G ) where we evaluate the Forward model with On-Policy TRPO on Ant and Humanoid Mujoco tasks . We were not able to get any better results on with forward model as compared to the Baseline TRPO , which is consistent with the findings from ( Gu et al. , 2016a ) . In essence , building the backward model is necessarily neither harder nor easier . Realistically , building any kind of model and having it be accurate for more than , say , 10 time steps is pretty hard . But if we only have 10 time steps of accurate transitions , it is probably better to take them backward model from different states as compared to from forward model from the same initial state . ( as corroborated by the findings in Fig 16 of Appendix G , and Figure 19 of Appendix H ) . Something which remains as a part of future investigation is to train the forward model and backtracking model jointly . As the backtracking model is tied to high value states , the forward model could extract the intended goal value from the high value state . When trained jointly , this should help the forward model learn some reduced representation of the state that is necessary to evaluate the reward . Ultimately , when planning , we want the model to predict the goal accurately , which helps to optimize for this \u201d goal-oriented \u201d behaviour directly . This also avoids the need to model irrelevant aspects of the environment . We also mention this in Appendix ( Section G ) . [ 1 ] ( Gu et al , 2016 ) Continuous Deep Q-Learning with Model-based Acceleration http : //proceedings.mlr.press/v48/gu16.html"}, "2": {"review_id": "HygsfnR9Ym-2", "review_text": "The authors propose a bidirectional model for learning a policy. In particular, a backtracking model was proposed to start from a high-value state and sample back the sequence of actions and states that could lead to the current high-value state. These traces can be used later for learning a good policy. The experiments show the effectiveness of the model in terms of increase the expected rewards in different tasks. However, learning the backtracking model would add some computational efforts to the entire learning phase. I would like to see experiments to show the computational time for these components. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the positive and constructive feedback . `` I would like to see experiments to show the computational time for these components . '' If a backtracking model model is available ( like in the maze example ) , then there is no extra computation time , but in the case where we have to learn a bw model , learning a bw model requires more updates compared to only earning a policy ( but a similar number of updates as compared to learning a forward model , i.e. , dynamics model of the environment ) . Please let us know if anything is unclear here , or if there is any other comparison that would be helpful in clarifying things more ."}}