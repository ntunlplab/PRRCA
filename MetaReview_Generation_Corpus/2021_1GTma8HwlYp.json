{"year": "2021", "forum": "1GTma8HwlYp", "title": "AUXILIARY TASK UPDATE DECOMPOSITION: THE GOOD, THE BAD AND THE NEUTRAL", "decision": "Accept (Poster)", "meta_review": "After engaging in some good interactive discussions all but one reviewer settled on a rating of marginal accept. The most negative reviewer didn't really provide a clear enough explanation of what was lacking in the work. The other reviewers felt that the observed gains for this multi-task learning framework were clear enough that the work is worthy of some attention by the community. The AC recommends acceptance, but one may consider this recommendation as a just past the line for acceptance recommendation.", "reviews": [{"review_id": "1GTma8HwlYp-0", "review_text": "The work studies the auxiliary task selection in deep learning to resolve the burden of selecting relevant tasks for pre-training or the multitask learning . By decomposing the auxiliary updates , one can reweight separately the beneficial and harmful directions so that the net contribution to the update of the primary task is always positive . The efficient implementation is experimented in text classification , image classification , and medical imaging transfer tasks . The first contribution is the decomposition algorithm and reweighting of the auxiliary updates . It is a simple idea with a nice insight of treating the primary task and the auxiliary tasks in different manners . The decomposition allows a reweighting on the updates to optimize the primary task as much as possible while keeping the auxiliary tasks providing improvable directions . The second contribution is an efficient mechanism to approximate and calculate the SVD of the Jacobian of the primary task . The mechanism is implemented from an existing randomized approximation method . The third contribution is a set of experiments verifying the proposed method . The experiments include text classification , image classification , and medical imaging transfer tasks . The most salient result is the 99 % data efficiency to achieve improvable performance in the medical imaging transfer task . Concerns Besides the above positive contributions , following are some concerns from the observations : 1 . The relative improvements comparing to the baselines in Table 1 and Table 2 do not seem as much as that in ( Gururangan et al.2020 ) and ( Yu et al. , 2020 ) , respectively . 2.The weights reported in the experiments are 1 or -1 in the experiments . For instance , \\eta_aux = ( 1 , 1 , -1 ) is reported in the image classification task . The reader would expect much better improvements when given the freedom to reassign the weights on the decomposed directions , especially when the harmful part has a negated weight . Moreover , why are the values chosen in \\eta 1 or -1 ? Would there be a nicer balance between , say , the beneficial and the harmful parts ? For instance , would \\eta = ( 1 , 0.8 , -0.9 ) be a better choice ? It would be crucial that the authors can explain furthermore or support further experiments to confirm whether the potential of this decomposition algorithm is fully demonstrated or not . Post Rebuttal I have read the authors ' response . All my concerns are addressed properly . However , I still doubt that even the corner cases of \\eta have a better performance , would there be a systematic way to find the optimal parameters reflecting the true potential of this method . Thus , I will keep my score unchanged .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your helpful feedback on our paper . We would like to address your concerns below . \u201c The relative improvements comparing to the baselines in Table 1 and Table 2 do not seem as much as that in ( Gururangan et al.2020 ) and ( Yu et al. , 2020 ) , respectively \u201d - > Please see the Comparison to ( Gururanga 2020 ) and ( Yue 2020 ) under Common Concerns . We address this there but please let us know if you would like further clarification \u201c The weights reported in the experiments are 1 or -1 in the experiments . For instance , \\eta_aux = ( 1 , 1 , -1 ) is reported in the image classification task \u2026 For instance , would \\eta = ( 1 , 0.8 , -0.9 ) be a better choice ? \u201d - > Please see the Hyperparameter Settings under Common Concerns , where we address this . We note that we are happy to include a more granular exploration of \\eta_ { aux } in the final version of the paper but as noted widening the search space would mean we allocate a much higher search budget to ATTITUD as opposed to the methods we compare to . Please let us know if we can provide further clarification ."}, {"review_id": "1GTma8HwlYp-1", "review_text": "[ Summary ] This paper studies auxiliary learning , and propose a decomposition method on the auxiliary gradient into several components , and to select relevant/useful decomposed gradients to maximize the assistance of the primary task . [ Strength ] How to adjust auxiliary tasks in a beneficial way is always a challenging problem in multi-task learning . The proposed solution on gradient decomposition is novel and interesting . The improved performance based on the proposed method seems to be non-trivial . [ Weakness ] This paper however has some significant weaknesses which I will outline below . -- Missing details . The subspace of the primary task gradient is composed of all training samples in the primary task , based on the definition of $ \\mathcal { S } $ . However , for any reasonable-size training dataset which contains at least 10k training samples , this process seems to be extremely expensive to compute ? The authors have introduced randomised approximation on decomposition step , but accumulate gradients seem to be already taking a lot of computation . I hope the authors could justify this . -- Unfair experimental setting . My biggest concern is on hyper-parameter tuning on each component of the decomposed auxiliary task gradient . In Eq.1 , the authors decompose each auxiliary task gradient into three components : i ) one is lying in the subspace of primary task gradient , ii ) one is orthogonal to the primary task gradient ii ) the final one is in conflict to the primary task gradient . By selecting different weightings on each component , the authors claim that this formulation can be degraded into prior auxiliary learning methods . I agree that this formulation is general . However , all of these component weightings are selected by hand , and seem to be very different across different datasets and tasks . This gives a very unfair comparison to previous methods , simply because these methods are included in one of the hyper-parameter sets of corresponding weighting values . I have noticed that the authors have constrained the search space into 4 sets of weighting , but this does not justify this problem . Ideally , these weighting should be automatically computed during training , and varied based on the dataset . In addition , the weighting for the primary task also varies across different datasets . So I am wondering whether these task weightings are consistent in baselines , and did authors perform a similar hyper-parameter search on baseline methods as well ? Other minor issues : This formulation * can not * be used as pre-training with only auxiliary gradients which are helpful to the primary task , since we do not know the primary task gradient beforehand . Even we know the primary task gradient , ( 0 , 1 , 0 ) is simply putting more weighting on the existing primary task gradient , so it 's the same as to tune up the learning rate . == Rated up after authors ' clarification .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We express our gratitude for providing feedback on our paper . We address your comments below . \u201c This gives a very unfair comparison to previous methods , simply because these methods are included in one of the hyper-parameter sets of corresponding weighting values \u201d - > This is inaccurate because we never report weight value configurations corresponding to baselines as our method in the paper . Note that for MultiCifar100 and Cat-vs-Dog , ( where we compare against PCGrad ) we report in Section 6 the best performing configurations as ( 1 , 1 , -1 ) and ( 1 , 0 , 0 ) respectively . These configurations are novel under our formulation and so we are not simply reporting a setting of PCGrad as our optimal value . Sorry for the confusion , we have updated Section 5 to make this clearer . \u201c The subspace of the primary task gradient is composed of all training samples in the primary task , based on the definition of S \u201d - > We do not use all the available examples to estimate the subspace but rather a mini-batch of samples , since as you rightly pointed out , this would present a significant computational burden . We mention this in Section 4 under implementation \u201c As stochastic optimization is prevalent in this setting , we construct subspace S from a mini-batch of primary task data \u201d \u201c Ideally , these weighting should be automatically computed during training , and varied based on the dataset. \u201d - > We agree that this would be the ideal setting . We decide to leave this as future work because automatic hyper-parameter selection is a challenging problem on its own . We however provide 4 hyper-parameter configurations that perform well across a reasonable set of tasks as guidance for practitioners . \u201c This formulation can not be used as pre-training with only auxiliary gradients which are helpful to the primary task , since we do not know the primary task gradient beforehand . Even we know the primary task gradient , ( 0 , 1 , 0 ) is simply putting more weighting on the existing primary task gradient , so it 's the same as tuning up the learning rate \u201d - > We would like to request further clarification on this point . Please note that our formulation relies on the fact that the primary task is known beforehand and thus cases where the primary task is unknown are outside the scope of our work"}, {"review_id": "1GTma8HwlYp-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : Leveraging the power of the data-rich related tasks have been studied ( e.g. , pre-training and multitask learning ) . This paper points out that careful utilization of auxiliary task is required to gain enhanced performance in primary tasks . In order to prevent harming the performance of primary tasks , they suggest the method to decompose auxiliary updates into three directions which have positive , negative and neutral impact on the primary task . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : In this paper , it is highly interesting to see how to use a decomposition from the span of the primary task Jacobian to adapt auxiliary gradients and validate the proposed methodology on image and textual data . Even though this is an interesting setting and the technical solutions presented in the paper look reasonable , the idea seems to be pretty incremental as it stacks multiple existing techniques without many innovations . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . The proposed methodology utilizes automatic differentiation procedures and randomized singular value decomposition for efficient scalability . 2.The proposed framework allows the model to treat each auxiliary update independently by its impact on the task of interest , which seems to be interesting . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : Authors need to perform more qualitative and quantitative analysis on the datasets to vilify the effectiveness of the proposed methodology . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your feedback . You mention that \u201c Authors need to perform more qualitative and quantitative analysis on the datasets to vilify the effectiveness of the proposed methodology. \u201d - > We have a hard time acting on this comment specifically . We would like to highlight that the experimental section already includes results testing how performance degrades in the scenario when the number of samples or the size of the subspace are very small . We also consider how that is affected by being in a low-resource and high-resource regime . We are happy to improve it . It would be helpful if you point at specific weaknesses or aspects we could address or improve ."}, {"review_id": "1GTma8HwlYp-3", "review_text": "Summary : The authors present a general formulation of different settings in multitask learning ( including pretraining regimes ) , in a setting where the goal is to get best performance for a pre-specified primary task and additional auxiliary tasks . The main idea is to divide the gradients on the auxiliary task into 2 subspaces : a subspace where the gradients influence performance of the primary task and a subspace where they only influence the auxiliary task without changing the loss on the primary task . Within the subspace that does have influence on the primary task , it is easy to compute directions that have a positive or negative effect on the primary task , which allows to create different learning schemes given the gradients that point toward : i ) auxiliary influence only , ii ) positive influence on auxiliary tass , iii ) negative influence on primary task . Experimental results show improvements over previously identified meta learning methods on 2 natural language datasets and 3 image datasets . Strengths : The authors present a general framework for an important problem . It has many applications in a wide variety of fields and contributes to thinking about meta learning and pretraining in a more general way . Explanations , illustrations and mathematical derivations are clear and easy to understand . The authors show that with a careful choice of hyperparameters , their approach can improve performance , especially in settings with limited data on the primary task . The results on natural language datasets are also interesting , showing that they achieve a higher performance when the auxiliary task doesn \u2019 t exactly match the primary task data . Weaknesses : Some of the explanations and especially the proof fall apart , when considering the k-largest principal vector of J * . Since k < < D , the sum of all gradients in g_aux will clearly still have a big influence on the performance of the primary task . The proposed algorithm introduces a lot of additional hyperparameters and not all of those hyper parameters are properly discussed . Eta_aux and eta_prim are properly discussed and the authors convincingly show that these parameters are implicitly set by other methods as well . Figure 2 shows an ablation study but does not help practitioners to set the discussed values , given the large variance over 5 runs . The choice of the subspace for g_aux seems very critical and the provided experimental results and discussion are somewhat lacking . How can a random choice of subspace basis improve the results ? Calculating the randomized_lowrank_approx is only done every n steps , but there is no mention of n later . Some experimental results should be provided to convince the reader that the basis does not change too much given 2 different batches from the primary task . Other remarks : The result in Table 2 is much better than the best result in Table 4 on the Cat-vs-Dogs experiment . What is the difference ? Can the experiments in Table 4 be repeated to be more comparable to Table 2 ? PCGrad most closely resembles this work . What type of subspace basis is used in that work ? It would be interesting to see a direct comparison between PCGrad and the proposed method with eta_aux = ( alpha_aux , alpha_aux , - alpha_aux ) and using the same basis . Early stopping after 10 epochs seems quite short and might explain some of the large variance in the results . Minor remarks : k is introduced without much explanation , which was a bit confusing on first reading . It should be clearly stated that it is a hyper parameter on first mention .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate the time and effort you took to provide us with feedback . We address your comments one by one below . \u201c Since k < < D , the sum of all gradients in g_aux will clearly still have a big influence on the performance of the primary task \u201d - > The choice of a lower dimensional space ( k ) is a computational necessity and indeed the decomposition is not perfect . However , our strategy performs well despite its low dimension : figure 3 shows that with k = 5 and D = O ( 1M ) , we are able to capture up to 20 % of the gradient norm . Thus , even though we do n't capture all of the norm if k is small , we capture a comparably large fraction with very few components . With a large enough choice of k , but still with k < < D , we can reduce the norm of the out-of-span component significantly so as to mitigate its influence on the primary task . Note that increasing k creates a tradeoff between computational efficiency and performance - we are encouraged that we observe significant improvement with relatively small values of k. \u201c Not all of those hyper parameters are properly discussed \u201d , \u201c Calculating the randomized_lowrank_approx is only done every n steps but there is no mention of n later \u201d - > We address this concern under the Hyperparameter Settings section of Common Concerns . Please let us know if you would like any further clarification . \u201c Figure 2 shows an ablation study but does not help practitioners to set the discussed values , given the large variance over 5 runs \u201d - > We attribute the higher variance as an unfortunate side effect of the limited data regime . The settings we explore , even when dubbed \u2018 high-resource \u2019 , involve training a relatively large model on much smaller datasets than typical . Moving to typical levels of data availability would distract from the low-resource setting we are most interested in . \u201c Some experimental results should be provided to convince the reader that the basis does not change too much given 2 different batches from the primary task \u201d - > We acknowledge that this would be a good ablation to include . We plan on including this experiment in the final version of the paper but we are still brainstorming an effective way to showcase this . This is especially challenging because we use a randomized algorithm for computing the subspace , so we necessarily get a different subspace from batch to batch and measuring the difference between 2 subspaces is itself a non-trivial problem \u201c How can a random choice of subspace basis improve the results \u201d - > For a \u2018 random \u2019 basis , since very little of the gradient norm falls within the subspace , the auxiliary task gradient does not change much after gradient decomposition and this essentially reduces to normal end-task agnostic pre-training . \u201c The result in Table 2 is much better than the best result in Table 4 on the Cat-vs-Dogs experiment . What is the difference ? \u201d - > For our ablation studies , we dedicated a limited computational budget for hyper-parameter selection for the methods explored in Table 4 . Table 2 reflects a more expanded set of hyper-parameter configurations to reflect the strength of the baselines being compared to . Note that the baselines in Table 2 also enjoy expanded resources for hyper-parameter search \u201c Early stopping after 10 epochs seems quite short and might explain some of the large variance in the results. \u201d - > We note that we do not early stop after just 10 epochs . Rather , we early stop if there has been no improvement on the end-task validation loss after 10 epochs . We run all experiments for a maximum of 150 pre-training epochs and 500 fine-tuning epochs . We updated the appendix to make this clearer ."}], "0": {"review_id": "1GTma8HwlYp-0", "review_text": "The work studies the auxiliary task selection in deep learning to resolve the burden of selecting relevant tasks for pre-training or the multitask learning . By decomposing the auxiliary updates , one can reweight separately the beneficial and harmful directions so that the net contribution to the update of the primary task is always positive . The efficient implementation is experimented in text classification , image classification , and medical imaging transfer tasks . The first contribution is the decomposition algorithm and reweighting of the auxiliary updates . It is a simple idea with a nice insight of treating the primary task and the auxiliary tasks in different manners . The decomposition allows a reweighting on the updates to optimize the primary task as much as possible while keeping the auxiliary tasks providing improvable directions . The second contribution is an efficient mechanism to approximate and calculate the SVD of the Jacobian of the primary task . The mechanism is implemented from an existing randomized approximation method . The third contribution is a set of experiments verifying the proposed method . The experiments include text classification , image classification , and medical imaging transfer tasks . The most salient result is the 99 % data efficiency to achieve improvable performance in the medical imaging transfer task . Concerns Besides the above positive contributions , following are some concerns from the observations : 1 . The relative improvements comparing to the baselines in Table 1 and Table 2 do not seem as much as that in ( Gururangan et al.2020 ) and ( Yu et al. , 2020 ) , respectively . 2.The weights reported in the experiments are 1 or -1 in the experiments . For instance , \\eta_aux = ( 1 , 1 , -1 ) is reported in the image classification task . The reader would expect much better improvements when given the freedom to reassign the weights on the decomposed directions , especially when the harmful part has a negated weight . Moreover , why are the values chosen in \\eta 1 or -1 ? Would there be a nicer balance between , say , the beneficial and the harmful parts ? For instance , would \\eta = ( 1 , 0.8 , -0.9 ) be a better choice ? It would be crucial that the authors can explain furthermore or support further experiments to confirm whether the potential of this decomposition algorithm is fully demonstrated or not . Post Rebuttal I have read the authors ' response . All my concerns are addressed properly . However , I still doubt that even the corner cases of \\eta have a better performance , would there be a systematic way to find the optimal parameters reflecting the true potential of this method . Thus , I will keep my score unchanged .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your helpful feedback on our paper . We would like to address your concerns below . \u201c The relative improvements comparing to the baselines in Table 1 and Table 2 do not seem as much as that in ( Gururangan et al.2020 ) and ( Yu et al. , 2020 ) , respectively \u201d - > Please see the Comparison to ( Gururanga 2020 ) and ( Yue 2020 ) under Common Concerns . We address this there but please let us know if you would like further clarification \u201c The weights reported in the experiments are 1 or -1 in the experiments . For instance , \\eta_aux = ( 1 , 1 , -1 ) is reported in the image classification task \u2026 For instance , would \\eta = ( 1 , 0.8 , -0.9 ) be a better choice ? \u201d - > Please see the Hyperparameter Settings under Common Concerns , where we address this . We note that we are happy to include a more granular exploration of \\eta_ { aux } in the final version of the paper but as noted widening the search space would mean we allocate a much higher search budget to ATTITUD as opposed to the methods we compare to . Please let us know if we can provide further clarification ."}, "1": {"review_id": "1GTma8HwlYp-1", "review_text": "[ Summary ] This paper studies auxiliary learning , and propose a decomposition method on the auxiliary gradient into several components , and to select relevant/useful decomposed gradients to maximize the assistance of the primary task . [ Strength ] How to adjust auxiliary tasks in a beneficial way is always a challenging problem in multi-task learning . The proposed solution on gradient decomposition is novel and interesting . The improved performance based on the proposed method seems to be non-trivial . [ Weakness ] This paper however has some significant weaknesses which I will outline below . -- Missing details . The subspace of the primary task gradient is composed of all training samples in the primary task , based on the definition of $ \\mathcal { S } $ . However , for any reasonable-size training dataset which contains at least 10k training samples , this process seems to be extremely expensive to compute ? The authors have introduced randomised approximation on decomposition step , but accumulate gradients seem to be already taking a lot of computation . I hope the authors could justify this . -- Unfair experimental setting . My biggest concern is on hyper-parameter tuning on each component of the decomposed auxiliary task gradient . In Eq.1 , the authors decompose each auxiliary task gradient into three components : i ) one is lying in the subspace of primary task gradient , ii ) one is orthogonal to the primary task gradient ii ) the final one is in conflict to the primary task gradient . By selecting different weightings on each component , the authors claim that this formulation can be degraded into prior auxiliary learning methods . I agree that this formulation is general . However , all of these component weightings are selected by hand , and seem to be very different across different datasets and tasks . This gives a very unfair comparison to previous methods , simply because these methods are included in one of the hyper-parameter sets of corresponding weighting values . I have noticed that the authors have constrained the search space into 4 sets of weighting , but this does not justify this problem . Ideally , these weighting should be automatically computed during training , and varied based on the dataset . In addition , the weighting for the primary task also varies across different datasets . So I am wondering whether these task weightings are consistent in baselines , and did authors perform a similar hyper-parameter search on baseline methods as well ? Other minor issues : This formulation * can not * be used as pre-training with only auxiliary gradients which are helpful to the primary task , since we do not know the primary task gradient beforehand . Even we know the primary task gradient , ( 0 , 1 , 0 ) is simply putting more weighting on the existing primary task gradient , so it 's the same as to tune up the learning rate . == Rated up after authors ' clarification .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We express our gratitude for providing feedback on our paper . We address your comments below . \u201c This gives a very unfair comparison to previous methods , simply because these methods are included in one of the hyper-parameter sets of corresponding weighting values \u201d - > This is inaccurate because we never report weight value configurations corresponding to baselines as our method in the paper . Note that for MultiCifar100 and Cat-vs-Dog , ( where we compare against PCGrad ) we report in Section 6 the best performing configurations as ( 1 , 1 , -1 ) and ( 1 , 0 , 0 ) respectively . These configurations are novel under our formulation and so we are not simply reporting a setting of PCGrad as our optimal value . Sorry for the confusion , we have updated Section 5 to make this clearer . \u201c The subspace of the primary task gradient is composed of all training samples in the primary task , based on the definition of S \u201d - > We do not use all the available examples to estimate the subspace but rather a mini-batch of samples , since as you rightly pointed out , this would present a significant computational burden . We mention this in Section 4 under implementation \u201c As stochastic optimization is prevalent in this setting , we construct subspace S from a mini-batch of primary task data \u201d \u201c Ideally , these weighting should be automatically computed during training , and varied based on the dataset. \u201d - > We agree that this would be the ideal setting . We decide to leave this as future work because automatic hyper-parameter selection is a challenging problem on its own . We however provide 4 hyper-parameter configurations that perform well across a reasonable set of tasks as guidance for practitioners . \u201c This formulation can not be used as pre-training with only auxiliary gradients which are helpful to the primary task , since we do not know the primary task gradient beforehand . Even we know the primary task gradient , ( 0 , 1 , 0 ) is simply putting more weighting on the existing primary task gradient , so it 's the same as tuning up the learning rate \u201d - > We would like to request further clarification on this point . Please note that our formulation relies on the fact that the primary task is known beforehand and thus cases where the primary task is unknown are outside the scope of our work"}, "2": {"review_id": "1GTma8HwlYp-2", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : Leveraging the power of the data-rich related tasks have been studied ( e.g. , pre-training and multitask learning ) . This paper points out that careful utilization of auxiliary task is required to gain enhanced performance in primary tasks . In order to prevent harming the performance of primary tasks , they suggest the method to decompose auxiliary updates into three directions which have positive , negative and neutral impact on the primary task . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : In this paper , it is highly interesting to see how to use a decomposition from the span of the primary task Jacobian to adapt auxiliary gradients and validate the proposed methodology on image and textual data . Even though this is an interesting setting and the technical solutions presented in the paper look reasonable , the idea seems to be pretty incremental as it stacks multiple existing techniques without many innovations . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : 1 . The proposed methodology utilizes automatic differentiation procedures and randomized singular value decomposition for efficient scalability . 2.The proposed framework allows the model to treat each auxiliary update independently by its impact on the task of interest , which seems to be interesting . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : Authors need to perform more qualitative and quantitative analysis on the datasets to vilify the effectiveness of the proposed methodology . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your feedback . You mention that \u201c Authors need to perform more qualitative and quantitative analysis on the datasets to vilify the effectiveness of the proposed methodology. \u201d - > We have a hard time acting on this comment specifically . We would like to highlight that the experimental section already includes results testing how performance degrades in the scenario when the number of samples or the size of the subspace are very small . We also consider how that is affected by being in a low-resource and high-resource regime . We are happy to improve it . It would be helpful if you point at specific weaknesses or aspects we could address or improve ."}, "3": {"review_id": "1GTma8HwlYp-3", "review_text": "Summary : The authors present a general formulation of different settings in multitask learning ( including pretraining regimes ) , in a setting where the goal is to get best performance for a pre-specified primary task and additional auxiliary tasks . The main idea is to divide the gradients on the auxiliary task into 2 subspaces : a subspace where the gradients influence performance of the primary task and a subspace where they only influence the auxiliary task without changing the loss on the primary task . Within the subspace that does have influence on the primary task , it is easy to compute directions that have a positive or negative effect on the primary task , which allows to create different learning schemes given the gradients that point toward : i ) auxiliary influence only , ii ) positive influence on auxiliary tass , iii ) negative influence on primary task . Experimental results show improvements over previously identified meta learning methods on 2 natural language datasets and 3 image datasets . Strengths : The authors present a general framework for an important problem . It has many applications in a wide variety of fields and contributes to thinking about meta learning and pretraining in a more general way . Explanations , illustrations and mathematical derivations are clear and easy to understand . The authors show that with a careful choice of hyperparameters , their approach can improve performance , especially in settings with limited data on the primary task . The results on natural language datasets are also interesting , showing that they achieve a higher performance when the auxiliary task doesn \u2019 t exactly match the primary task data . Weaknesses : Some of the explanations and especially the proof fall apart , when considering the k-largest principal vector of J * . Since k < < D , the sum of all gradients in g_aux will clearly still have a big influence on the performance of the primary task . The proposed algorithm introduces a lot of additional hyperparameters and not all of those hyper parameters are properly discussed . Eta_aux and eta_prim are properly discussed and the authors convincingly show that these parameters are implicitly set by other methods as well . Figure 2 shows an ablation study but does not help practitioners to set the discussed values , given the large variance over 5 runs . The choice of the subspace for g_aux seems very critical and the provided experimental results and discussion are somewhat lacking . How can a random choice of subspace basis improve the results ? Calculating the randomized_lowrank_approx is only done every n steps , but there is no mention of n later . Some experimental results should be provided to convince the reader that the basis does not change too much given 2 different batches from the primary task . Other remarks : The result in Table 2 is much better than the best result in Table 4 on the Cat-vs-Dogs experiment . What is the difference ? Can the experiments in Table 4 be repeated to be more comparable to Table 2 ? PCGrad most closely resembles this work . What type of subspace basis is used in that work ? It would be interesting to see a direct comparison between PCGrad and the proposed method with eta_aux = ( alpha_aux , alpha_aux , - alpha_aux ) and using the same basis . Early stopping after 10 epochs seems quite short and might explain some of the large variance in the results . Minor remarks : k is introduced without much explanation , which was a bit confusing on first reading . It should be clearly stated that it is a hyper parameter on first mention .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We appreciate the time and effort you took to provide us with feedback . We address your comments one by one below . \u201c Since k < < D , the sum of all gradients in g_aux will clearly still have a big influence on the performance of the primary task \u201d - > The choice of a lower dimensional space ( k ) is a computational necessity and indeed the decomposition is not perfect . However , our strategy performs well despite its low dimension : figure 3 shows that with k = 5 and D = O ( 1M ) , we are able to capture up to 20 % of the gradient norm . Thus , even though we do n't capture all of the norm if k is small , we capture a comparably large fraction with very few components . With a large enough choice of k , but still with k < < D , we can reduce the norm of the out-of-span component significantly so as to mitigate its influence on the primary task . Note that increasing k creates a tradeoff between computational efficiency and performance - we are encouraged that we observe significant improvement with relatively small values of k. \u201c Not all of those hyper parameters are properly discussed \u201d , \u201c Calculating the randomized_lowrank_approx is only done every n steps but there is no mention of n later \u201d - > We address this concern under the Hyperparameter Settings section of Common Concerns . Please let us know if you would like any further clarification . \u201c Figure 2 shows an ablation study but does not help practitioners to set the discussed values , given the large variance over 5 runs \u201d - > We attribute the higher variance as an unfortunate side effect of the limited data regime . The settings we explore , even when dubbed \u2018 high-resource \u2019 , involve training a relatively large model on much smaller datasets than typical . Moving to typical levels of data availability would distract from the low-resource setting we are most interested in . \u201c Some experimental results should be provided to convince the reader that the basis does not change too much given 2 different batches from the primary task \u201d - > We acknowledge that this would be a good ablation to include . We plan on including this experiment in the final version of the paper but we are still brainstorming an effective way to showcase this . This is especially challenging because we use a randomized algorithm for computing the subspace , so we necessarily get a different subspace from batch to batch and measuring the difference between 2 subspaces is itself a non-trivial problem \u201c How can a random choice of subspace basis improve the results \u201d - > For a \u2018 random \u2019 basis , since very little of the gradient norm falls within the subspace , the auxiliary task gradient does not change much after gradient decomposition and this essentially reduces to normal end-task agnostic pre-training . \u201c The result in Table 2 is much better than the best result in Table 4 on the Cat-vs-Dogs experiment . What is the difference ? \u201d - > For our ablation studies , we dedicated a limited computational budget for hyper-parameter selection for the methods explored in Table 4 . Table 2 reflects a more expanded set of hyper-parameter configurations to reflect the strength of the baselines being compared to . Note that the baselines in Table 2 also enjoy expanded resources for hyper-parameter search \u201c Early stopping after 10 epochs seems quite short and might explain some of the large variance in the results. \u201d - > We note that we do not early stop after just 10 epochs . Rather , we early stop if there has been no improvement on the end-task validation loss after 10 epochs . We run all experiments for a maximum of 150 pre-training epochs and 500 fine-tuning epochs . We updated the appendix to make this clearer ."}}