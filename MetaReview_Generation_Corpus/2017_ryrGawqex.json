{"year": "2017", "forum": "ryrGawqex", "title": "Deep Learning with Dynamic Computation Graphs", "decision": "Accept (Poster)", "meta_review": "All reviewers viewed the paper favorably as a nice/helpful contribution to the implementation of this important class of methods.", "reviews": [{"review_id": "ryrGawqex-0", "review_text": "The paper presents a novel strategy to deal with dynamic computation graphs. They arise, when the computation is dynamically influenced by the input data, such as in LSTMs. The authors propose an `unrolling' strategy over the operations done at every step, which allows a new kind of batching of inputs. The presented idea is novel and the results clearly indicate the potential of the approach. For the sake of clarity of the presentation I would drop parts of Section 3 (\"A combinator library for neural networks\") which presents technical details that are in general interesting, but do not help the understanding of the core idea of the paper. The presented experimental results on the \"Stanford Sentiment Treebank\" are in my opinion not supporting the claim of the paper, which is towards speed, than a little bit confusing. It is important to point out that even though the presented ensemble \"[...] variant sets a new state-of-the-art on both subtasks\" [p. 8], this is not due to the framework, not even due to the model (comp. lines 4 and 2 of Tab. 2), but probably, and this can only be speculated about, due to the ensemble averaging. I would appreciate a clearer argumentation in this respect. Update on Jan. 17th: after the authors update for their newest revision, I increase my rating to 8 due to the again improved, now very clear argumentation.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the kind review . You are right that some of sec.3 and especially the SST experimental results are a bit of a tangent . My motivation for including them was that , while the core of the paper is certainly dynamic batching and the speed results , we are also claiming that dynamic batching enables implementing deep learning models ( which are growing ever more complex ) at a higher level of abstraction ( vs. manual batching ) and thus a more rapid feedback loop for trying out novel model variants ( and thus obtaining superior results ) . Besides the SST experiments , are there any parts of Section 3 in particular that you 'd suggest dropping ?"}, {"review_id": "ryrGawqex-1", "review_text": "Authors describe implementation of TensorFlow Fold which allows one to run various computations without modifying computation graph. They achieve this by creating a generic scheduler as a TensorFlow computation graph, which can accept graph description as input and execute it. They show clear benefits to this approach for tasks where computation changes for each datapoint, such as the case with TreeRNN. In the experiments, they compare against having static batch (same graph structure repeated many times) and batch size 1. The reason my score is 7 and not higher is because they do not provide comparison to the main alternative of their method -- someone could create a new TensorFlow graph for each dynamic batch. In other words, instead of using their graph as the scheduling algorithm, one could explicitly create each non-uniform batch as a TensorFlow graph, and run that using standard TensorFlow.", "rating": "7: Good paper, accept", "reply_text": "Thanks for the kind review . The reason that we do n't compare to explicitly creating TF graphs for non-uniform batches is that this would entail modifying the graph in the course of training . There are two problems with this : 1 . TF graph creation/modification currently happens in pure Python , and is thus super slow 2 . In distributed training , TF does not allow the graph to be modified once training has commenced ( so even if # 1 were resolved in the future and graph creation were fast , it still would n't work at scale ) . Because of # 1 we were careful not to measure the cost of graph creation in any of our experiments . If/when # 1 is resolved ( i.e.with a C/C++ API for graph creation ) then it will be feasible to perform the experiments you suggest within TF ( and it might be possible to perform them today using some other framework ) , but I do n't expect # 2 to change anytime soon ; the assumption of an entirely static graph for distributed training seems to be baked in pretty deeply to the framework ."}, {"review_id": "ryrGawqex-2", "review_text": "The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training. Pros: - significant speed improvements through dynamic batching - source code provided Cons: - the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context - presentation/vizualisation can be improved ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the kind review . In terms of improving the presentation / visualization , did you have anything specific in mind ? One thing that we could add would be an illustration showing the unrolled wiring diagram that you get for a batch of inputs with different shapes - we 've put one together at https : //goo.gl/chKVtZ . My original thought was that it had too many boxes and arrows and would be more confusing than enlightening , but if you or the other reviewers think that it is useful then we could update the paper to include it . Regarding showing the effect of dynamic batching on a large real-world problem , I agree that this would help put the improvements in context . One caveat is that I do n't think that dynamic batching has much to offer for classic seq-to-seq models where the computation graph is essentially a linear chain with no variation in structure from one example to the next . Were there particular ASR/SMT models that you had in mind where the shape of the computation graph varies from example to example ? ( parenthetically , applying dynamic batching and the TensorFlow Fold library to large real-world problem is * exactly * what our team is most focused on right now , although not currently ASR or SMT ... and not quite ready for publication yet , unfortunately )"}], "0": {"review_id": "ryrGawqex-0", "review_text": "The paper presents a novel strategy to deal with dynamic computation graphs. They arise, when the computation is dynamically influenced by the input data, such as in LSTMs. The authors propose an `unrolling' strategy over the operations done at every step, which allows a new kind of batching of inputs. The presented idea is novel and the results clearly indicate the potential of the approach. For the sake of clarity of the presentation I would drop parts of Section 3 (\"A combinator library for neural networks\") which presents technical details that are in general interesting, but do not help the understanding of the core idea of the paper. The presented experimental results on the \"Stanford Sentiment Treebank\" are in my opinion not supporting the claim of the paper, which is towards speed, than a little bit confusing. It is important to point out that even though the presented ensemble \"[...] variant sets a new state-of-the-art on both subtasks\" [p. 8], this is not due to the framework, not even due to the model (comp. lines 4 and 2 of Tab. 2), but probably, and this can only be speculated about, due to the ensemble averaging. I would appreciate a clearer argumentation in this respect. Update on Jan. 17th: after the authors update for their newest revision, I increase my rating to 8 due to the again improved, now very clear argumentation.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the kind review . You are right that some of sec.3 and especially the SST experimental results are a bit of a tangent . My motivation for including them was that , while the core of the paper is certainly dynamic batching and the speed results , we are also claiming that dynamic batching enables implementing deep learning models ( which are growing ever more complex ) at a higher level of abstraction ( vs. manual batching ) and thus a more rapid feedback loop for trying out novel model variants ( and thus obtaining superior results ) . Besides the SST experiments , are there any parts of Section 3 in particular that you 'd suggest dropping ?"}, "1": {"review_id": "ryrGawqex-1", "review_text": "Authors describe implementation of TensorFlow Fold which allows one to run various computations without modifying computation graph. They achieve this by creating a generic scheduler as a TensorFlow computation graph, which can accept graph description as input and execute it. They show clear benefits to this approach for tasks where computation changes for each datapoint, such as the case with TreeRNN. In the experiments, they compare against having static batch (same graph structure repeated many times) and batch size 1. The reason my score is 7 and not higher is because they do not provide comparison to the main alternative of their method -- someone could create a new TensorFlow graph for each dynamic batch. In other words, instead of using their graph as the scheduling algorithm, one could explicitly create each non-uniform batch as a TensorFlow graph, and run that using standard TensorFlow.", "rating": "7: Good paper, accept", "reply_text": "Thanks for the kind review . The reason that we do n't compare to explicitly creating TF graphs for non-uniform batches is that this would entail modifying the graph in the course of training . There are two problems with this : 1 . TF graph creation/modification currently happens in pure Python , and is thus super slow 2 . In distributed training , TF does not allow the graph to be modified once training has commenced ( so even if # 1 were resolved in the future and graph creation were fast , it still would n't work at scale ) . Because of # 1 we were careful not to measure the cost of graph creation in any of our experiments . If/when # 1 is resolved ( i.e.with a C/C++ API for graph creation ) then it will be feasible to perform the experiments you suggest within TF ( and it might be possible to perform them today using some other framework ) , but I do n't expect # 2 to change anytime soon ; the assumption of an entirely static graph for distributed training seems to be baked in pretty deeply to the framework ."}, "2": {"review_id": "ryrGawqex-2", "review_text": "The paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks. An impressive speedup can be observed in their implementation within TensorFlow. The content is presented with sufficient clarity, although some more graphical illustrations could be useful. This work is relevant in order to achieve highest performance in neural network training. Pros: - significant speed improvements through dynamic batching - source code provided Cons: - the effect on a large real-world (ASR, SMT) would allow the reader to put the improvements better into context - presentation/vizualisation can be improved ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the kind review . In terms of improving the presentation / visualization , did you have anything specific in mind ? One thing that we could add would be an illustration showing the unrolled wiring diagram that you get for a batch of inputs with different shapes - we 've put one together at https : //goo.gl/chKVtZ . My original thought was that it had too many boxes and arrows and would be more confusing than enlightening , but if you or the other reviewers think that it is useful then we could update the paper to include it . Regarding showing the effect of dynamic batching on a large real-world problem , I agree that this would help put the improvements in context . One caveat is that I do n't think that dynamic batching has much to offer for classic seq-to-seq models where the computation graph is essentially a linear chain with no variation in structure from one example to the next . Were there particular ASR/SMT models that you had in mind where the shape of the computation graph varies from example to example ? ( parenthetically , applying dynamic batching and the TensorFlow Fold library to large real-world problem is * exactly * what our team is most focused on right now , although not currently ASR or SMT ... and not quite ready for publication yet , unfortunately )"}}