{"year": "2020", "forum": "B1gXR3NtwS", "title": "Deep Bayesian Structure Networks", "decision": "Reject", "meta_review": "The authors develop stochastic variational approaches to learn Bayesian \"structure distributions\" for neural networks. While the reviewers appreciated the updates to the paper made by the authors, there will still a number of remaining concerns. There were particularly concerns about the clarity of the paper (remarking on informality of language and lack of changes in the revision with respect to comments in the original review), and the fairness of comparisons. Regarding comparisons, one reviewer comments: \"I do not agree that the comparison with DARTS is fair because the authors remove the options for retraining in both DARTS and DBSN. The reason DARTS trains using one half of the data and validate on the other is that it includes a retraining phase where all data is used. Therefore fair comparison should use the same procedure as DARTS (including a retraining phrase). At the very least, to compare methods without retraining, results of DARTS with more data (e.g., 80%) for training should be reported.\" The authors are encouraged to continue with this work, carefully accounting for reviewer comments in future revisions.", "reviews": [{"review_id": "B1gXR3NtwS-0", "review_text": "This paper proposed deep Bayesian structure networks (DBSN) to model weights, \\alpha, of the redundant operations in cell-based differentiable NAS. The authors claim that DBSN can achieve better performance (accuracy) than the state of the art. One of my concerns is the Bayesian formulation introduced in Eq. (4) seems problematic. It is not clear what priors are placed on alpha. In the case of Bayes by BP (BBB), which is cited as Blundell et al. 2015 in the paper, a Gaussian prior (with zero mean) is used. Therefore there is a KL term between the variational distribution q(w) and the prior distribution p(w) to regularize q(w). In DBSN, q(\\alpha) is parameterized by \\theta and \\epsilon, and so is p(\\alpha), meaning that the KL term is effectively zero. This is very different from what is done in BBB. The second major concern is on the experiments. (1) The authors use DARTS as a main baseline and show that DBSN significantly outperforms DARTS. However, looking at the DARTS paper, the test error on CIFAR-10 is around 3% for both the first-order and second-order versions. The test error in Table 1 is around 9%, which is a lot lower. I notice that the DARTS paper has a parameter number of 3.3M, while in the current paper it set to 1M. Given that DARTS is the main baseline method and the same dataset (CIFAR-10) is used, it would make much more sense to use exactly the same architecture for comparison. The current results is hardly convincing. (2) Besides, note that in the DARTS paper, DenseNet-BC has test error of 3.46%, much higher than DARTS (~3%). In Table 2 of this paper however, DARTS is significantly worse than DenseNet-BC (8.91% versus 4.51%). These results are highly inconsistent with previous work. As mentioned in the paper, Dikov & Bayer 2019 has a very similar idea to perform NAS from a Bayesian perspective. It would be best (and would definitely make the paper stronger) to include some comparison. Even if Dikov & Bayer 2019 is not very scalable, it is at least possible to compare them in smaller network size. Otherwise it is hard to evaluate the contribution of DBSN given this highly similar work. The authors mentioned in the introduction that DBSN \u2018yields more diverse prediction\u2019 and therefore brings more calibrated uncertainty comparing to ensembling different architectures. This is not verified in the experiment section. Table 3 only reports the ECE for one instance of trained networks. For example, it would be interesting to sample different architecture from the alpha learned in DARTS and DBSN, train several networks, ensemble them, and use the variance of the ensemble to compute ECE. This would verify the claim mentioned above. Do you retrain the network from scratch after the architecture search (which is done in DARTS) for DARTS and DBSN? I am not convinced by the claim that BNN usually achieve compromising performance. Essentially, BNN, if trained well, is a generalization of deterministic NN. If very flat priors and highly confident variational distributions are used, BNN essentially reduces to deterministic NN. Missing references on Bayesian deep learning and BNN: Bayesian Dark Knowledge Towards Bayesian Deep Learning: A Survey Natural-Parameter Networks: A Class of Probabilistic Neural Networks", "rating": "3: Weak Reject", "reply_text": "Q5 : About \u201c re-train the network from scratch after the architecture search \u201d : A : This is not the case for DBSN . As stated above in our response to Q2 and the last parts of Section 2.2 & 3.3 , unlike existing meta-learning approaches in NAS , DBSN does not need to re-train the network after the architecture search . We perform training for once and then can draw multiple samples ( i.e. , networks with different structures but shared weights ) for making predictions . This is exactly one advantage of DBSN over existing NAS solutions . Q6 : The claim that BNN usually achieves compromising performance : A : Theoretically , considering the over-parameterization nature of modern networks , the data we want to model is always relatively \u201c small \u201d ( See e.g . [ 4 ] for an informational theoretical analysis of the amount of \u2018 information \u2019 in a dataset ) . Hence , the posterior inference in BNNs can hardly reduce to MLE , leading to the performance divergence between BNNs and deterministic NNs . Empirically , in general , we deploy BNNs to seek for uncertainty estimation . On top of this , we want the model to be accurate . Therefore , it does not make sense to adopt \u201c very flat priors and highly confident variational distributions \u201d to achieve high performance while discarding the estimation of predictive uncertainty . Q7 : References . A : Thanks for the suggestions . We checked these works and found that they are not so closely related to DBSN , which performs Bayesian structure learning with variational inference techniques . Anyway , we cited them in the related work as general Bayesian deep learning methods . [ 1 ] Georgi Dikov and Justin Bayer . Bayesian learning of neural network architectures . AISTATS 2019 . [ 2 ] Hanxiao Liu , Karen Simonyan , and Yiming Yang . DARTS : Differentiable architecture search . ICLR 2019 . [ 3 ] Sirui Xie , Hehui Zheng , Chunxiao Liu , and Liang Lin . SNAS : stochastic neural architecture search . ICLR 2019 . [ 4 ] William Bialek , Ilya Nemenman and Naftali Tishby . Predictability , Complexity , and Learning , Neural Computation 13 , 2409\u20132463 , 2001 ."}, {"review_id": "B1gXR3NtwS-1", "review_text": "This paper proposes to do approximate Bayesian inference in neural networks by treating the neural network structure as a random variable (RV), while inferring the parameters with point estimates. While performing Bayesian inference for the neural network structure is sensible, I am not convinced by the approach taken in this work. The biggest problem is that the model uses a point estimate of the same weights for different, random network structures. Major problems: - In the motivation the authors write \u201cDBSN places distributions on the network structure, introducing more global randomness, thus is probable to yield more diverse predictions, and ensembling them brings more calibrated uncertainty\u201d. What is \u201cmore global randomness\u201d? This is used multiple times. Does it refer to the hierarchy in the graphical model? Please be precise here and point that out in your model by using an equation or graphical model. Or is it just an intuition? - Generally, I would agree that integrating out multiple network structures provides better calibrated uncertainty. However, given that the authors use point estimates for the weights, it is not clear if that is still true, especially since the number of different architectures used in practice is small. - What\u2019s more, the approach uses *the same* point estimates for different structures. This leads to a graphical model, where the weights are not conditioned on the architecture/structure. This modeling choice could be a big limitation, because the weights now have to fit multiple different architectures; it may thus defeat the calibration completely. One can easily imagine that only a single random architecture works well with the learned point estimates, thus resulting in an (almost) deterministic model. I assume that this modeling choice was made for practical reasons, but could you expand on its implications / interpretation / limitations? Does the posterior of such a constrained model not quickly converge to an \u201calmost\u201d dirac, effectively just one network structure? - Sec. 3.1. presents the above problem resulting from a modeling decision as a \u201ctraining challenge\u201d. To counter this problem, the authors propose to reduce the variance of the structure distribution. By doing so, the approach becomes even less Bayesian and the predictive uncertainty becomes even less reliable. - \u201cWe only learn the connections between the B internal nodes, as shown in Appendix D\u201d. All deterministic weights are learned, but the structure only for some parts of the model? If this is the case, then approach becomes again less probabilistic. Regarding the experiments, the stddevs are calculated from 3(!) independent runs and thus completely misleading (imagine the stddev of the stddev estimate). In summary, the model choice of point estimates for the weights, which are not conditioned on the architecture, leads to various problems. The authors have to introduce tricks such as reducing the variance of the random network structures or learning only a part of the whole structure to make the approach converge. The resulting probabilistic model and its predictive uncertainty is questionable. For this reason, this paper should be rejected. Minor problems - Sec. 3.2. \u201cImprovements of the structure learning space\u201d. What is a \u201cstructure learning space\u201d? - Section 3 introduces the ELBO in Eq. (4) before the complete model is specified. Please specify the whole model first. How do w and alpha depend on each other in your model? - The prior for the weights is omitted; at the same time it is mentioned in the experiments (Sec.5.1.) that weight decay is applied. Why not just be explicit about it and say that a Gaussian prior is used? - Background Sec. 2.2. is not clear. what is a cell? some deterministic transformation in general? bunch of neural network layers? What are the operation (last term in Eq. (2)) doing? This is not detailed and abstract to me. Are the alphas probabilities? Is Eq. (2) consequently a mixture model of different architectures? Or is this here just a weighted sum, where the weights take arbitrary values? A small visualization (additionally) might help here, but can probably be rectified by better explanation. - Bayesian reasoning on the structure. Inference? - Writing that you propose a new \u201cframework\u201d is a bit grandiose for what is actually proposed. There has been previous work in which the architecture is inferred as well and these approaches would certainly be part of the same framework. Please just say model/algorithm/approach, whatever is applicable. - new paragraph starting at \u201cTo empirically validate\u201d in the intro. - Before (4): \u201cThen we rewrite the approximation error\u201d. Eq. (4) is the ELBO, this is not an approximation error. ", "rating": "3: Weak Reject", "reply_text": "Q7 : Concerns on minor problems : Thank you for your kind suggestions ! We have updated the paper accordingly . Below are some selected responses : Q7.1 : What is a \u201c structure learning space \u201d : A : The structure learning space means the support of the structure distribution . It is a set containing all the possible network structures , whose size grows exponentially with the number of paths with redundant operations . Q7.2 : Model specification : A : We apologize for these points . We revised Section 3 and Fig.1 to provide model specifications in detail . As shown in the updated Fig.1 , $ w $ and $ \\alpha $ are independent variables . Q7.3 : About weight decay on $ w $ : A : Thank you . We agree with the reviewer that the weight decay used to regularize $ w $ essentially implies a Gaussian prior on the weights . Then , in practice , DBSN performs maximum a posteriori ( MAP ) estimation of $ w $ , namely , estimating the mode of $ w $ \u2019 s posterior distribution $ p ( w|D ) $ . Therefore , we can regard DBSN as doing an approximation to Bayesian inference on $ w $ , which is much more practical for the model \u2019 s training . Q7.4 : Background Sec.2.2 . is not clear : A : We apologize for that . We updated the paper to make cell-based NAS clearer to understand . A cell is a network module containing several computational nodes , i.e.tensors , which can also be viewed as a bunch of layers . The last term $ o^ { ( i , j ) } _k ( N^i ; w ) $ in Eq . ( 2 ) is the output through the $ k $ -th operation ( e.g. , convolution , skip connection , etc . ) on $ N^i $ .The operation is equipped with a subset of $ w $ as parameters , thus $ w $ is added as a condition in the notation . As we have stated , the $ \\alpha^ { ( i , j ) } $ are the gating weights of the different $ K $ operations and are in a K-dimensional simplex . Regarding the visualization , we kindly remind that it has already been provided in Fig.1.Q7.5 : Other minor problems : A : We addressed them in the revised paper . We hope the new version is clearer and easier to follow . Feel free to let us know if there is still anything unclear . [ 1 ] Yarin Gal and Zoubin Ghahramani . Dropout as a bayesian approximation : Representing model uncertainty in deep learning . ICML 2016 . [ 2 ] Yarin Gal , Jiri Hron , and Alex Kendall : Concrete Dropout . NIPS 2017 . [ 3 ] Gao Huang , Yu Sun , Zhuang Liu , Daniel Sedra , and Kilian Q Weinberger . Deep networks with stochastic depth . ECCV 2016 . [ 4 ] Matthew Mackay , Paul Vicol , Jonathan Lorraine , David Duvenaud , and Roger Grosse . Self-tuning networks : Bilevel optimization of hyperparameters using structured best-response functions . ICLR 2019 . [ 5 ] Hanxiao Liu , Karen Simonyan , and Yiming Yang . DARTS : Differentiable architecture search . ICLR 2019 . [ 6 ] Ziyu Wang , Tongzheng Ren , Jun Zhu , and Bo Zhang . Function space particle optimization for bayesian neural networks . ICLR 2019 ."}, {"review_id": "B1gXR3NtwS-2", "review_text": "The paper combines ideas from neural architecture search (NAS) and Bayesian neural networks. Instead of maintaining uncertainty in network weights, the authors propose to retain uncertainty in the network structure. In particular, building on cell-based differentiable NAS, the authors infer a distribution over the gating weights of different cells incident onto a tensor while relying on point estimates for the weights inside each cell. Overall, I liked the paper and vote for accepting it. The notion of maintaining uncertainty about the network structure is a sensible one, and the paper explores an as yet under-explored area at the intersection of state-of-the-art network architecture search algorithms and Bayesian neural networks. Moreover, this is accompanied by compelling empirics \u2014 results demonstrate gains in both predictive performance and calibration across diverse tasks and careful comparisons to sensible baselines are presented to evaluate various aspects of the proposed approach (Table 1). Detailed Comments: + One issue the experiments fail to adequately disentangle is the effect of weight uncertainty vs structure uncertainty. Are the observed gains in accuracy and calibration simply a product of better structure learning? In particular, I would love to see a baseline where point estimates of \\alpha are learned but posterior distribution over weights is inferred. I realize NEK-FAC was an attempt at providing such a comparison, but since it uses a different structure, it remains unclear whether it\u2019s poor performance stems from the fundamental difficulty of learning posteriors over high dimensional weights or simply a sub-optimal network structure. + In a similar spirit, one can imagine a fully Bayesian DBSN where one infers posterior distributions overbite \\alpha and w. Presumably, this would close the OOD entropy gap between random \\alpha and DBSN. + How many Monte Carlo samples were used to evaluate Equation 8. In variational BNNs one often finds that using more MC samples doesn\u2019t necessarily improve predictive accuracy over using the most likely sample (the mean if using a Gaussian variational family). It would be interesting to see predictive performance as a function of the number of MC samples for DBSN. + Clarity: While I am mostly upbeat about this paper, the writing could be significantly improved. While the overall ideas come across, there are several instances where the text appears muddled and needs a few more polishing passes. ", "rating": "6: Weak Accept", "reply_text": "We realized the BBB method used for modeling weight uncertainty in BNN-LS & Fully Bayesian DBSN may be restrictive , resulting in such weakness . Therefore , we further implemented these two baselines with a most-recently proposed mean-field natural-gradient variational inference method , called Variational Online Gauss-Newton ( VOGN ) ( Khan et al. , 2018 ; Osawa et al. , 2019 ) . VOGN is known to work well with advanced techniques , e.g. , momentum , batch normalisation , data augmentation . As claimed by Osawa et al . ( 2019 ) , VOGN demonstrates comparable results to Adam . Then , we replaced the used BBB in BNN-LS and Fully Bayesian DBSN with VOGN , based on VOGN \u2019 s official repository ( https : //github.com/team-approx-bayes/dl-with-bayes ) . With the original network size ( B=7 , 12 cells ) , the baselines trained with VOGN needed more than one hour for one epoch . Thus we adopted smaller networks ( B=4 , 3 cells ) , which have almost 41K parameters , for the two baselines . We also trained a DBSN in the same setting . The detailed parameters to initialize VOGN are here ( https : //github.com/anonymousest/DBSN/blob/master/dbsn/train_bnn_torchsso.py # L220 ) . The experiments were conducted on CIFAR-10 and the results are provided in Table 5 of Appendix C. The predictive performance and uncertainty gaps between DBSN and the two baselines are very huge , which possibly results from the under-fitting of the high-dim weight distributions in BNN-LS and Fully Bayesian DBSN . We believe that our implementation is correct because our results are consistent with the original results in Table 1 of [ 5 ] ( VOGN has 75.48 % and 84.27 % validation accuracy even with even larger 2.5M AlexNet and 11.1M ResNet-18 architectures ) . Further , DBSN is much more efficient than the two baselines . These comparisons strongly reveal the benefits of modeling structure uncertainty over modeling weight uncertainty , highlighting the practical value of DBSN . Q3 : The number of Monte Carlo ( MC ) samples : A : Thanks for the suggestion . We plotted the changes of test loss , test error rate , and test ECE w.r.t.the number of MC samples used for testing DBSN in Fig.6 ( CIFAR-10 ) and Fig.7 ( CIFAR-100 ) in Appendix B . It is clear that ensembling the predictions from models with various sampled network structures improves the final predictive performance and calibration significantly . This is in marked contrast to the situation of classic variational BNNs , where using more MC samples does not necessarily bring improvement over using the most likely sample . As shown in the plots , we could better utilize 20+ MC samples to predict the unseen data , in order to adequately exploit the learned structure distribution . Indeed , we used 100 MC samples in all the experiments , except the adversarial attack experiments where we used 30 MC samples for attacking and evaluation . Q4 : Writing : A : Thanks for the kind suggestions . We have tried our best to improve the writing in the revised version . Please feel free to comment if there are still misleading or confusing parts in the paper . [ 1 ] Charles Blundell , Julien Cornebise , Koray Kavukcuoglu , and Daan Wierstra . Weight uncertainty in neural network . ICML 2015 . [ 2 ] Christos Louizos and Max Welling . Multiplicative normalizing flows for variational bayesian neural networks . ICML 2017 . [ 3 ] Jiaxin Shi , Shengyang Sun , and Jun Zhu . Kernel implicit variational inference . ICLR 2018 . [ 4 ] Mohammad Emtiyaz Khan , Didrik Nielsen , Voot Tangkaratt , Wu Lin , Yarin Gal , and Akash Srivas-tava . Fast and scalable bayesian deep learning by weight-perturbation in adam . ICML 2018 . [ 5 ] Kazuki Osawa , Siddharth Swaroop , Anirudh Jain , Runa Eschenhagen , Richard E Turner , RioYokota , and Mohammad Emtiyaz Khan . Practical deep learning with Bayesian principles . NeurIPS 2019 ."}], "0": {"review_id": "B1gXR3NtwS-0", "review_text": "This paper proposed deep Bayesian structure networks (DBSN) to model weights, \\alpha, of the redundant operations in cell-based differentiable NAS. The authors claim that DBSN can achieve better performance (accuracy) than the state of the art. One of my concerns is the Bayesian formulation introduced in Eq. (4) seems problematic. It is not clear what priors are placed on alpha. In the case of Bayes by BP (BBB), which is cited as Blundell et al. 2015 in the paper, a Gaussian prior (with zero mean) is used. Therefore there is a KL term between the variational distribution q(w) and the prior distribution p(w) to regularize q(w). In DBSN, q(\\alpha) is parameterized by \\theta and \\epsilon, and so is p(\\alpha), meaning that the KL term is effectively zero. This is very different from what is done in BBB. The second major concern is on the experiments. (1) The authors use DARTS as a main baseline and show that DBSN significantly outperforms DARTS. However, looking at the DARTS paper, the test error on CIFAR-10 is around 3% for both the first-order and second-order versions. The test error in Table 1 is around 9%, which is a lot lower. I notice that the DARTS paper has a parameter number of 3.3M, while in the current paper it set to 1M. Given that DARTS is the main baseline method and the same dataset (CIFAR-10) is used, it would make much more sense to use exactly the same architecture for comparison. The current results is hardly convincing. (2) Besides, note that in the DARTS paper, DenseNet-BC has test error of 3.46%, much higher than DARTS (~3%). In Table 2 of this paper however, DARTS is significantly worse than DenseNet-BC (8.91% versus 4.51%). These results are highly inconsistent with previous work. As mentioned in the paper, Dikov & Bayer 2019 has a very similar idea to perform NAS from a Bayesian perspective. It would be best (and would definitely make the paper stronger) to include some comparison. Even if Dikov & Bayer 2019 is not very scalable, it is at least possible to compare them in smaller network size. Otherwise it is hard to evaluate the contribution of DBSN given this highly similar work. The authors mentioned in the introduction that DBSN \u2018yields more diverse prediction\u2019 and therefore brings more calibrated uncertainty comparing to ensembling different architectures. This is not verified in the experiment section. Table 3 only reports the ECE for one instance of trained networks. For example, it would be interesting to sample different architecture from the alpha learned in DARTS and DBSN, train several networks, ensemble them, and use the variance of the ensemble to compute ECE. This would verify the claim mentioned above. Do you retrain the network from scratch after the architecture search (which is done in DARTS) for DARTS and DBSN? I am not convinced by the claim that BNN usually achieve compromising performance. Essentially, BNN, if trained well, is a generalization of deterministic NN. If very flat priors and highly confident variational distributions are used, BNN essentially reduces to deterministic NN. Missing references on Bayesian deep learning and BNN: Bayesian Dark Knowledge Towards Bayesian Deep Learning: A Survey Natural-Parameter Networks: A Class of Probabilistic Neural Networks", "rating": "3: Weak Reject", "reply_text": "Q5 : About \u201c re-train the network from scratch after the architecture search \u201d : A : This is not the case for DBSN . As stated above in our response to Q2 and the last parts of Section 2.2 & 3.3 , unlike existing meta-learning approaches in NAS , DBSN does not need to re-train the network after the architecture search . We perform training for once and then can draw multiple samples ( i.e. , networks with different structures but shared weights ) for making predictions . This is exactly one advantage of DBSN over existing NAS solutions . Q6 : The claim that BNN usually achieves compromising performance : A : Theoretically , considering the over-parameterization nature of modern networks , the data we want to model is always relatively \u201c small \u201d ( See e.g . [ 4 ] for an informational theoretical analysis of the amount of \u2018 information \u2019 in a dataset ) . Hence , the posterior inference in BNNs can hardly reduce to MLE , leading to the performance divergence between BNNs and deterministic NNs . Empirically , in general , we deploy BNNs to seek for uncertainty estimation . On top of this , we want the model to be accurate . Therefore , it does not make sense to adopt \u201c very flat priors and highly confident variational distributions \u201d to achieve high performance while discarding the estimation of predictive uncertainty . Q7 : References . A : Thanks for the suggestions . We checked these works and found that they are not so closely related to DBSN , which performs Bayesian structure learning with variational inference techniques . Anyway , we cited them in the related work as general Bayesian deep learning methods . [ 1 ] Georgi Dikov and Justin Bayer . Bayesian learning of neural network architectures . AISTATS 2019 . [ 2 ] Hanxiao Liu , Karen Simonyan , and Yiming Yang . DARTS : Differentiable architecture search . ICLR 2019 . [ 3 ] Sirui Xie , Hehui Zheng , Chunxiao Liu , and Liang Lin . SNAS : stochastic neural architecture search . ICLR 2019 . [ 4 ] William Bialek , Ilya Nemenman and Naftali Tishby . Predictability , Complexity , and Learning , Neural Computation 13 , 2409\u20132463 , 2001 ."}, "1": {"review_id": "B1gXR3NtwS-1", "review_text": "This paper proposes to do approximate Bayesian inference in neural networks by treating the neural network structure as a random variable (RV), while inferring the parameters with point estimates. While performing Bayesian inference for the neural network structure is sensible, I am not convinced by the approach taken in this work. The biggest problem is that the model uses a point estimate of the same weights for different, random network structures. Major problems: - In the motivation the authors write \u201cDBSN places distributions on the network structure, introducing more global randomness, thus is probable to yield more diverse predictions, and ensembling them brings more calibrated uncertainty\u201d. What is \u201cmore global randomness\u201d? This is used multiple times. Does it refer to the hierarchy in the graphical model? Please be precise here and point that out in your model by using an equation or graphical model. Or is it just an intuition? - Generally, I would agree that integrating out multiple network structures provides better calibrated uncertainty. However, given that the authors use point estimates for the weights, it is not clear if that is still true, especially since the number of different architectures used in practice is small. - What\u2019s more, the approach uses *the same* point estimates for different structures. This leads to a graphical model, where the weights are not conditioned on the architecture/structure. This modeling choice could be a big limitation, because the weights now have to fit multiple different architectures; it may thus defeat the calibration completely. One can easily imagine that only a single random architecture works well with the learned point estimates, thus resulting in an (almost) deterministic model. I assume that this modeling choice was made for practical reasons, but could you expand on its implications / interpretation / limitations? Does the posterior of such a constrained model not quickly converge to an \u201calmost\u201d dirac, effectively just one network structure? - Sec. 3.1. presents the above problem resulting from a modeling decision as a \u201ctraining challenge\u201d. To counter this problem, the authors propose to reduce the variance of the structure distribution. By doing so, the approach becomes even less Bayesian and the predictive uncertainty becomes even less reliable. - \u201cWe only learn the connections between the B internal nodes, as shown in Appendix D\u201d. All deterministic weights are learned, but the structure only for some parts of the model? If this is the case, then approach becomes again less probabilistic. Regarding the experiments, the stddevs are calculated from 3(!) independent runs and thus completely misleading (imagine the stddev of the stddev estimate). In summary, the model choice of point estimates for the weights, which are not conditioned on the architecture, leads to various problems. The authors have to introduce tricks such as reducing the variance of the random network structures or learning only a part of the whole structure to make the approach converge. The resulting probabilistic model and its predictive uncertainty is questionable. For this reason, this paper should be rejected. Minor problems - Sec. 3.2. \u201cImprovements of the structure learning space\u201d. What is a \u201cstructure learning space\u201d? - Section 3 introduces the ELBO in Eq. (4) before the complete model is specified. Please specify the whole model first. How do w and alpha depend on each other in your model? - The prior for the weights is omitted; at the same time it is mentioned in the experiments (Sec.5.1.) that weight decay is applied. Why not just be explicit about it and say that a Gaussian prior is used? - Background Sec. 2.2. is not clear. what is a cell? some deterministic transformation in general? bunch of neural network layers? What are the operation (last term in Eq. (2)) doing? This is not detailed and abstract to me. Are the alphas probabilities? Is Eq. (2) consequently a mixture model of different architectures? Or is this here just a weighted sum, where the weights take arbitrary values? A small visualization (additionally) might help here, but can probably be rectified by better explanation. - Bayesian reasoning on the structure. Inference? - Writing that you propose a new \u201cframework\u201d is a bit grandiose for what is actually proposed. There has been previous work in which the architecture is inferred as well and these approaches would certainly be part of the same framework. Please just say model/algorithm/approach, whatever is applicable. - new paragraph starting at \u201cTo empirically validate\u201d in the intro. - Before (4): \u201cThen we rewrite the approximation error\u201d. Eq. (4) is the ELBO, this is not an approximation error. ", "rating": "3: Weak Reject", "reply_text": "Q7 : Concerns on minor problems : Thank you for your kind suggestions ! We have updated the paper accordingly . Below are some selected responses : Q7.1 : What is a \u201c structure learning space \u201d : A : The structure learning space means the support of the structure distribution . It is a set containing all the possible network structures , whose size grows exponentially with the number of paths with redundant operations . Q7.2 : Model specification : A : We apologize for these points . We revised Section 3 and Fig.1 to provide model specifications in detail . As shown in the updated Fig.1 , $ w $ and $ \\alpha $ are independent variables . Q7.3 : About weight decay on $ w $ : A : Thank you . We agree with the reviewer that the weight decay used to regularize $ w $ essentially implies a Gaussian prior on the weights . Then , in practice , DBSN performs maximum a posteriori ( MAP ) estimation of $ w $ , namely , estimating the mode of $ w $ \u2019 s posterior distribution $ p ( w|D ) $ . Therefore , we can regard DBSN as doing an approximation to Bayesian inference on $ w $ , which is much more practical for the model \u2019 s training . Q7.4 : Background Sec.2.2 . is not clear : A : We apologize for that . We updated the paper to make cell-based NAS clearer to understand . A cell is a network module containing several computational nodes , i.e.tensors , which can also be viewed as a bunch of layers . The last term $ o^ { ( i , j ) } _k ( N^i ; w ) $ in Eq . ( 2 ) is the output through the $ k $ -th operation ( e.g. , convolution , skip connection , etc . ) on $ N^i $ .The operation is equipped with a subset of $ w $ as parameters , thus $ w $ is added as a condition in the notation . As we have stated , the $ \\alpha^ { ( i , j ) } $ are the gating weights of the different $ K $ operations and are in a K-dimensional simplex . Regarding the visualization , we kindly remind that it has already been provided in Fig.1.Q7.5 : Other minor problems : A : We addressed them in the revised paper . We hope the new version is clearer and easier to follow . Feel free to let us know if there is still anything unclear . [ 1 ] Yarin Gal and Zoubin Ghahramani . Dropout as a bayesian approximation : Representing model uncertainty in deep learning . ICML 2016 . [ 2 ] Yarin Gal , Jiri Hron , and Alex Kendall : Concrete Dropout . NIPS 2017 . [ 3 ] Gao Huang , Yu Sun , Zhuang Liu , Daniel Sedra , and Kilian Q Weinberger . Deep networks with stochastic depth . ECCV 2016 . [ 4 ] Matthew Mackay , Paul Vicol , Jonathan Lorraine , David Duvenaud , and Roger Grosse . Self-tuning networks : Bilevel optimization of hyperparameters using structured best-response functions . ICLR 2019 . [ 5 ] Hanxiao Liu , Karen Simonyan , and Yiming Yang . DARTS : Differentiable architecture search . ICLR 2019 . [ 6 ] Ziyu Wang , Tongzheng Ren , Jun Zhu , and Bo Zhang . Function space particle optimization for bayesian neural networks . ICLR 2019 ."}, "2": {"review_id": "B1gXR3NtwS-2", "review_text": "The paper combines ideas from neural architecture search (NAS) and Bayesian neural networks. Instead of maintaining uncertainty in network weights, the authors propose to retain uncertainty in the network structure. In particular, building on cell-based differentiable NAS, the authors infer a distribution over the gating weights of different cells incident onto a tensor while relying on point estimates for the weights inside each cell. Overall, I liked the paper and vote for accepting it. The notion of maintaining uncertainty about the network structure is a sensible one, and the paper explores an as yet under-explored area at the intersection of state-of-the-art network architecture search algorithms and Bayesian neural networks. Moreover, this is accompanied by compelling empirics \u2014 results demonstrate gains in both predictive performance and calibration across diverse tasks and careful comparisons to sensible baselines are presented to evaluate various aspects of the proposed approach (Table 1). Detailed Comments: + One issue the experiments fail to adequately disentangle is the effect of weight uncertainty vs structure uncertainty. Are the observed gains in accuracy and calibration simply a product of better structure learning? In particular, I would love to see a baseline where point estimates of \\alpha are learned but posterior distribution over weights is inferred. I realize NEK-FAC was an attempt at providing such a comparison, but since it uses a different structure, it remains unclear whether it\u2019s poor performance stems from the fundamental difficulty of learning posteriors over high dimensional weights or simply a sub-optimal network structure. + In a similar spirit, one can imagine a fully Bayesian DBSN where one infers posterior distributions overbite \\alpha and w. Presumably, this would close the OOD entropy gap between random \\alpha and DBSN. + How many Monte Carlo samples were used to evaluate Equation 8. In variational BNNs one often finds that using more MC samples doesn\u2019t necessarily improve predictive accuracy over using the most likely sample (the mean if using a Gaussian variational family). It would be interesting to see predictive performance as a function of the number of MC samples for DBSN. + Clarity: While I am mostly upbeat about this paper, the writing could be significantly improved. While the overall ideas come across, there are several instances where the text appears muddled and needs a few more polishing passes. ", "rating": "6: Weak Accept", "reply_text": "We realized the BBB method used for modeling weight uncertainty in BNN-LS & Fully Bayesian DBSN may be restrictive , resulting in such weakness . Therefore , we further implemented these two baselines with a most-recently proposed mean-field natural-gradient variational inference method , called Variational Online Gauss-Newton ( VOGN ) ( Khan et al. , 2018 ; Osawa et al. , 2019 ) . VOGN is known to work well with advanced techniques , e.g. , momentum , batch normalisation , data augmentation . As claimed by Osawa et al . ( 2019 ) , VOGN demonstrates comparable results to Adam . Then , we replaced the used BBB in BNN-LS and Fully Bayesian DBSN with VOGN , based on VOGN \u2019 s official repository ( https : //github.com/team-approx-bayes/dl-with-bayes ) . With the original network size ( B=7 , 12 cells ) , the baselines trained with VOGN needed more than one hour for one epoch . Thus we adopted smaller networks ( B=4 , 3 cells ) , which have almost 41K parameters , for the two baselines . We also trained a DBSN in the same setting . The detailed parameters to initialize VOGN are here ( https : //github.com/anonymousest/DBSN/blob/master/dbsn/train_bnn_torchsso.py # L220 ) . The experiments were conducted on CIFAR-10 and the results are provided in Table 5 of Appendix C. The predictive performance and uncertainty gaps between DBSN and the two baselines are very huge , which possibly results from the under-fitting of the high-dim weight distributions in BNN-LS and Fully Bayesian DBSN . We believe that our implementation is correct because our results are consistent with the original results in Table 1 of [ 5 ] ( VOGN has 75.48 % and 84.27 % validation accuracy even with even larger 2.5M AlexNet and 11.1M ResNet-18 architectures ) . Further , DBSN is much more efficient than the two baselines . These comparisons strongly reveal the benefits of modeling structure uncertainty over modeling weight uncertainty , highlighting the practical value of DBSN . Q3 : The number of Monte Carlo ( MC ) samples : A : Thanks for the suggestion . We plotted the changes of test loss , test error rate , and test ECE w.r.t.the number of MC samples used for testing DBSN in Fig.6 ( CIFAR-10 ) and Fig.7 ( CIFAR-100 ) in Appendix B . It is clear that ensembling the predictions from models with various sampled network structures improves the final predictive performance and calibration significantly . This is in marked contrast to the situation of classic variational BNNs , where using more MC samples does not necessarily bring improvement over using the most likely sample . As shown in the plots , we could better utilize 20+ MC samples to predict the unseen data , in order to adequately exploit the learned structure distribution . Indeed , we used 100 MC samples in all the experiments , except the adversarial attack experiments where we used 30 MC samples for attacking and evaluation . Q4 : Writing : A : Thanks for the kind suggestions . We have tried our best to improve the writing in the revised version . Please feel free to comment if there are still misleading or confusing parts in the paper . [ 1 ] Charles Blundell , Julien Cornebise , Koray Kavukcuoglu , and Daan Wierstra . Weight uncertainty in neural network . ICML 2015 . [ 2 ] Christos Louizos and Max Welling . Multiplicative normalizing flows for variational bayesian neural networks . ICML 2017 . [ 3 ] Jiaxin Shi , Shengyang Sun , and Jun Zhu . Kernel implicit variational inference . ICLR 2018 . [ 4 ] Mohammad Emtiyaz Khan , Didrik Nielsen , Voot Tangkaratt , Wu Lin , Yarin Gal , and Akash Srivas-tava . Fast and scalable bayesian deep learning by weight-perturbation in adam . ICML 2018 . [ 5 ] Kazuki Osawa , Siddharth Swaroop , Anirudh Jain , Runa Eschenhagen , Richard E Turner , RioYokota , and Mohammad Emtiyaz Khan . Practical deep learning with Bayesian principles . NeurIPS 2019 ."}}