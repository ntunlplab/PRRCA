{"year": "2020", "forum": "S1xJFREKvB", "title": "Amortized Nesterov's Momentum: Robust and Lightweight  Momentum for Deep Learning", "decision": "Reject", "meta_review": "This paper introduces a variant of Nesterov momentum which saves computation by only periodically recomputing certain quantities, and which is claimed to be more robust in the stochastic setting. The method seems easy to use, so there's probably no harm in trying it. However, the reviewers and I don't find the benefits persuasive. While there is theoretical analysis, its role is to show that the algorithm maintains the convergence properties while having other benefits. However, the computations saved by amortization seem like a small fraction of the total cost, and I'm having trouble seeing how the increased \"robustness\" is justified. (It's possible I missed something, but clarity of exposition is another area the paper could use some improvement in.) Overall, this submission seems promising, but probably needs to be cleaned up before publication at ICLR.\n", "reviews": [{"review_id": "S1xJFREKvB-0", "review_text": "%% Post Author Response comments %% Thank you for your detailed response/revision. 1 - Introducing \u201cm-times\u201d larger momentum: Somehow, this is not a particularly intuitive statement or one that reflects clearly in a theoretical bound. Since we are getting to issues surrounding the use of momentum with stochastic optimization, I would like to make a note that the performance of these algorithms more broadly aren't quite sketched out for their use in broader stochastic optimization. In particular, despite broad use in practice, it is unclear if standard variants of Nesterov acceleration/Heavy Ball method achieve \"acceleration\" in stochastic optimization. See for e.g., the work of Kidambi et al ICLR 2018 (\u201cOn the insufficiency of existing momentum schemes for stochastic optimization\u201d) - where, the argument was that these methods were designed for deterministic optimization (where we get exact gradients) - in fact, that paper empirically as well as theoretically shows that these schemes do not offer acceleration in a precise sense compared to specialized algorithms for stochastic optimization. It is unclear if the proposed algorithms can offer a similar improvement over SGD in a provable sense, even for the specific examples described in their paper. 2 - The point about theory (just as you mention) is that it doesn\u2019t directly apply towards the simulations, nor, do they improve on already known algorithms - so I am unable to see the point that these results present broader implications that can guide practice. 3 - The response doesn\u2019t address the fact that for the theory bounds presented in the paper to hold (even in the convex settings described), one requires knowledge of parameters that are not known a-priori, and are often fairly difficult to estimate. So the performance of the algorithm in practice may quite significantly be away from the bounds described in the paper. While I appreciate the points and revision made by the authors, I still believe the paper requires some rethinking to present their results (and this includes more detailed comparisons to existing works) in order to make a case towards broader practical applications. %%. %% This paper considers robustness issues faced by Nesterov\u2019s Acceleration used with mini-batch stochastic gradients for training Deep Models. In particular, the paper proposes amortized momentum, an algorithm that offers a way to handle these issues. The paper in general is well written and easy to follow. The paper proposes algorithms AM-SGD1 and AM-SGD2 and presents extensive results regarding their complexity analysis on convex problems and their performance when training neural networks. The algorithms require storing one more model\u2019s worth of storage compared to standard momentum based methods (which can be viewed as a drawback in certain cases). Comments: [1] I am concerned about the motivation behind this paper - which, according to the paper is that Nesterov\u2019s accelerated gradient method with stochastic gradients has huge initial fluctuations. The issue with regards to more fluctuations of the initial performance is natural given how aggressive these accelerated methods work. As long as this is not a reason/cause for worse terminal performance (which doesn\u2019t seem to be the case), I am unable to see why large initial fluctuations are concerning. [2] Theory: The theory bounds for this problem setting do not appear to improve over known bounds in the literature. As a side note, the work of Hu et al. \u201cAccelerated Gradient Methods for Stochastic Optimization and Online Learning\u201d is highly related to this paper\u2019s theoretical aspects, setup and bounds. Furthermore, this bounded variance noise model for stochastic gradients, while being theoretically useful (and important), is often very detached from practice (as this implies that the domain is bounded and we perform projections of iterates whenever they go outside the set - such aspects hardly reflect on practical SGD implementations). Using this as a means to reason about robustness of the proposed algorithm (for e.g. remarks for theorem 1a. and in conclusions) appears to be a big leap that may lead to potentially misleading conclusions. [3] In order to run the algorithm to achieve the theoretical bounds claimed (in theorems 1 and 2), it appears that the stepsize \\alpha_s depends on unknown quantities such as initial distance to opt, noise variance etc. [4] The claim in page 2 about comparing SGD and M-SGD says that the stepsize in deterministic and stochastic optimization is constrained to be O(1/L) is rather misleading. In realistic practical implementation of SGD with a multiplicative noise oracle, one really has to use a much smaller stepsize than 1/L. This in a sense leads back to point[2] about the unrealistic nature of bounded variance assumptions for understanding SGD based methods used in the context of Machine Learning. They are better suited for understanding stochastic methods in black-box optimization (as opposed to considering Machine Learning problems). My take is that even if the authors justify novelty in terms of theory results (which, to my knowledge is limited compared to existing literature), rewriting the paper by considering its theoretical merit and presenting empirical results (even as considered in this paper) of this algorithm (without attempting to make very strong connections to explain issues experienced in non-convex training of neural networks, since the theory works in vastly different settings under restrictive assumptions) can be appreciated by appropriate sections of audience (both in theory as well as optimization for deep learning communities).", "rating": "3: Weak Reject", "reply_text": "Thanks for carefully reading our paper and your thoughtful comments . [ 1 , Motivation ] We admit that we should have made the motivation clearer in the introduction . Our motivation is to design a favorable alternative for M-SGD . We have listed all the pros and cons of AM1-SGD in the introduction and emphasized the \u201c easy-to-use \u201d feature in the revision . In the original submission , we tried to point out the robustness of M-SGD , for which the amortization technique has the most obvious effect : the amortized momentum is aggressive ( intuitively , AM1-SGD is injecting $ m $ -times larger momentum ( compared with M-SGD ) into plain SGD every $ m $ iterations ) , but it is able to reduce the fluctuations in the initial stage ( comparing SGD and AM1-SGD with Option I in Figure 3 ) . We agree that the terminal performance of M-SGD is not affected by the initial fluctuations , which makes the motivation unclear in the introduction . Thus , we have removed the related paragraph in the introduction in the revision . [ 2 , Theory ] - The theory bounds for this problem setting do not appear to improve . The theory bounds are already optimal , and actually the novelty of this paper is about how we modified M-SGD ( AC-SA ) to fit our high level idea while maintaining the optimality . Our purpose of the theory section is mainly to understand what has changed when amortizing Nesterov \u2019 s momentum ( the effect of $ m $ ) . While these results do not hold for deep learning problems in general , they shed light on the tuning of $ m $ in deep learning applications . We have revised the first paragraph in Section 4 to make it clearer . - The work of Hu et al . [ Hu et al. , 2009 ] and the work [ Ghadimi & Lan , 2012 ] both focus on extending AC-SA [ Lan , 2012 ] to strongly convex setting and refining its parameter choices . In comparison , the assumptions in [ Ghadimi & Lan , 2012 ] are more general and more reasonable ( [ Hu et al. , 2009 ] considers the unconstrained case and assumes that an algorithm-generated sequence is bounded ) . We added [ Hu et al. , 2009 ] to the reference in the revision . - The bounded variance noise model for stochastic gradients . We chose the stochastic bounded variance model since we want to understand why the amortization trick can reduce fluctuations . It actually turns out that in the deterministic setting ( $ \\sigma=0 $ ) , amortization ( increasing $ m $ ) is meaningless ( Theorem 1a and 2 ) . - The domain is bounded and we perform projections of iterates . Compactness is assumed only in Theorem 1b . Theorem 1a and 2 hold in unconstrained setting ( $ X = \\mathbb { R } ^d , h \\equiv 0 $ ) . - Using this as a means to reason \u2026 is a big leap . We admit that the statements about using the theory to reason the empirical results in the original submission are misleading , and we have revised the statements to say that the theory can provide a better understanding of the empirical results . [ 3 , $ \\alpha_s $ depends on unknown quantities ] We did not follow [ Ghadimi & Lan , 2012 ] and [ Hu et al. , 2009 ] to derive a better parameter setting because it does not contribute to the understanding of $ m $ but complicates the analysis . We will add some comments about this in the next revision . [ 4 , $ 1/L $ ] Thanks for pointing this out . We have modified all the $ O ( 1/L ) $ to strict upper bounds . About using intuitions from convex analysis : Nesterov \u2019 s momentum was migrated from convex optimization to deep learning by [ Sutskever et al. , 2013 ] and it turned out to be quite successful . We actually followed [ Sutskever et al. , 2013 ] to design tricks in convex analysis and explore their benefits in deep learning applications . [ Hu et al. , 2009 ] , Accelerated Gradient Methods for Stochastic Optimization and Online Learning , 2009 [ Ghadimi & Lan , 2012 ] . Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i : A generic algorithmic framework , 2012 [ Lan , 2012 ] . An optimal method for stochastic composite optimization , 2012 [ Sutskever et al. , 2013 ] . On the importance of initialization and momentum in deep learning , 2013"}, {"review_id": "S1xJFREKvB-1", "review_text": "This paper provides a new simple method to incorporate Nesterov momentum into standard SGD for deep learning, with good empirical and theoretical results. Overall I think this paper should be accepted, some minor comments follow. At no point does Polyak's heavy ball method get mentioned, even though the variant of Nesterov acceleration you are considering is very similar to it (since the momentum parameter is fixed, which is not the usual form of Nesterov except in the strongly convex case). It would be beneficial to delineate how this is or isn't related to heavy ball. The experiments would benefit from a wall-clock time comparison too, rather than just epochs since these new methods would be slower (but presumably not by much). The appendix is huge with most of the technical details relegated there which I did not read fully. I think this impacts the readability significantly, though not grounds for rejection. Perhaps it suggests that a conference with a small page limit is not the best venue? It seems that SGD still has better convergence early on. The authors suggest their method fixes this (relative to standard nesterov SGD) but it doesn't seem to be quite as good as SGD. Can you explain or discuss why this is still the case? The assumptions require some explanation, they are just listed with no context. What are they and why are they useful? Step size \"should be constrained to O(1/L)\" is misleading, you should say explicitly that step-size <= 1/L (or whatever it is depending on the algorithm). Some of the writing is a bit strange / sloppy, e.g.: \"AM2-SGD is a bit tricky in randomness\" \"However, full-batch loss is way too expensive to evaluate.\" In Algorithm 1 AM1-SGD: \"xk+1 \u2190 xk+1 + \u03b2 \u00b7 (\u02dcx+ \u2212 x\u02dc)\" doesn't parse because x_{k+1} appears twice. Missing references: Accelerated proximal algorithms: *) Beck and Teboulle: A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems *) Nesterov: Gradient Methods for Minimizing Composite Objective Function, Restarting (slightly different to your approach but still relevant): *) O'Donoghue: Adaptive Restart for Accelerated Gradient Schemes ", "rating": "8: Accept", "reply_text": "Thank you for your positive feedbacks . We have added more explanations to the assumptions , changed $ O ( 1/L ) $ to strict upper bounds , improved the writing and the presentation of Algorithm 1 , made the main text more independent from the appendices , and cited missing references in the revision . [ Compare with Polyak \u2019 s heavy ball ( HB ) ] If $ m=1 $ , AM1-SGD reduces to M-SGD and its difference from HB has been carefully studied in [ 2 ] . Here we consider a simple case $ m=2 $ . We only compare 2 iterations $ x_0\\rightarrow x_1\\rightarrow x_2 $ of AM1-SGD ( Option I ) and HB . $ g_0 $ and $ g_1 $ denote the gradients evaluated at $ x_0 $ and $ x_1 $ and we mark the learning rate and momentum parameter ( $ > 0 $ ) for AM1-SGD and HB as $ ( \\eta , \\beta ) $ and $ ( \\alpha , \\gamma ) $ , respectively . Since the first iteration of both AM1-SGD and HB is SGD , if $ \\alpha = \\eta $ , they generate the same $ x_1 $ ( and thus the same $ g_1 $ ) . After the second iteration , AM1-SGD produces $ $ x_2 = x_0 - \\eta ( 1 + \\beta ) \\cdot g_0 - \\eta ( 1 + \\beta/2 ) \\cdot g_1 , $ $ and HB produces $ $ x_2 = x_0 - \\alpha ( 1 + \\gamma ) \\cdot g_0 - \\alpha \\cdot g_1 . $ $ It is unlikely for them to produce the same $ x_2 $ . [ Wall-clock time ] We recorded all the wall-clock time in the CIFAR experiments . However , we observed that even on the same type of GPUs , the running times fluctuate a lot and do not exhibit a clear trend with increasing $ m $ ( For AM1-SGD , a larger $ m $ leads to a lower amortized cost ) . Thus , we did not report them in the paper . The running time is improved by about 2 % -3 % for AM1-SGD ( $ m=5 $ ) compared with M-SGD ( measured on the same GPU and using the same random batches ) . AM2-SGD ( $ m=5 $ ) is slightly slower than M-SGD . [ SGD still has better convergence early on ] Here we provide some possible explanations in the ( strongly ) convex setting ( The objective is $ L $ -smooth and $ \\mu $ -strongly convex , we denote $ \\kappa = L/\\mu $ as the condition number ) : In theory , accelerated algorithms ( e.g. , NAG , Katyusha ) are only faster in the ill-conditioned case ( when $ \\kappa $ is very large ) . When $ \\kappa $ is small ( the well-conditioned case ) , their convergence rates are the same as non-accelerated algorithms ( e.g. , GD , SVRG ) . In practice , it is always observed that in the well-conditioned case , non-accelerated algorithms perform better than accelerated algorithms and increasing momentum only deteriorates the performance . In the ill-conditioned case , it is frequently observed that non-accelerated algorithms have a faster early convergence but are surpassed by accelerated algorithms later on , just like the results of SGD and M-SGD given in the paper . Our conjecture is that in the early stage of training , the problem is locally well-conditioned and momentum hurts the performance . When the iterate enters an ill-conditioned region , the momentum becomes effective and provides acceleration . This negative effect is like an intrinsic disadvantage of momentum that is \u201c transferred \u201d to deep learning training . The amortization technique eases this effect by trading acceleration with variance control ( using the intuition of Section 4 ) . A larger $ m $ can produce a faster early convergence than SGD ( Figure 6 ) but reduces more final performance . Perhaps using an adaptive choice of $ m $ is better . [ 2 ] Ilya Sutskever , James Martens , George Dahl , Geoffrey Hinton . On the importance of initialization and momentum in deep learning , ICML 2013 ."}, {"review_id": "S1xJFREKvB-2", "review_text": "The authors proposed Amortized Nesterov\u2019s Momentum, a variant of Nesterov\u2019s momentum that utilizes several past iterates, instead of one iterate, to provide the momentum. The goal is to have more robust iterates, faster convergence in the early stage and higher efficiency. The authors designed two different realizations, AM1-SGD and AM2-SGD. Comments: My major concern for this paper is its unconvincing motivation and experiment results, especially when the approach is designed for deep learning. The motivation for the proposed approach is not quite convincing. The authors said that \u201cdue to the large stochasticity, SGD with Nesterov\u2019s momentum is not robust...This increased uncertainty slows down its convergence especially in the early stage of training and makes its single run less trustworthy\u201d For image classification, Nesterov momentum is very popular and the final convergence values of different trails seems to be similar. It would be more convincing if the authors can provide practical evidence for supporting this claim. It was claimed that Amortized Nesterov\u2019s Momentum has \u201chigher efficiency and faster convergence in the early stage without losing the good generalization performance\u201d. What is the benefit or advantage for having faster early convergence without improving the final generalization performance? The authors claim that \u201cM-SGD2 is more robust and achieves slightly better performance\u201d, in Figure 1a, however, it is really hard to tell the difference between M-SGD2 and M-SGD from Figure 1a. The efficiency improvement in page 4 is really hard to follow for comparison with Algorithm 1 in page 3. Though m > 2 could reduce the number of operations in step 5 and 6, I don\u2019t think this is a computational bottleneck. I believe these updates should be very fast in comparison with forward and gradient calculation. Making the 1% computation 50% faster does not mean more efficient. I would like to know how much computation cost can be saved with this modification. On the other hand, adding one more auxiliary buffer (scaled momentum) could significantly impact the training as the memory is often the limit. In section 3.1, what is \u201cidentical iteration\u201d? It is hard to compare AM2-SGD with AM1-SGD. It would be easier to follow if the side-by-side algorithm comparison can be shown early. The section 4\u2019s theoretical analysis based on the convex composite problem is not quite convincing. I am not sure how these results are related with the deep learning applications. In the experiment section, the comparison of AM1/2-SGD with other baselines seems not quite consistent. The authors first state that they use all 0.1 learning rate and 0.9 momentum for all methods, however, the setting for M-SGD is using 0.99 momentum and different learning rate schedule. This makes the comparison not very meaningful, while AM1-SGD and AM2-SGD do not use learning rate restart. With so many differences, the advantage of AM1-SGD and AM2-SGD are not that different with M-SGD. In the task of ImageNet-152, M-SGD even is better than AM1-SGD. This makes the conclusion that \u201cAM1-SGD has a lightweight design, which can serve as an excellent replacement for M-SGD in large-scale deep learning tasks\u201d not quite valid. Minor: The author may assume readers maybe familiar Katyusha momentum, I think there may need more background about it. ", "rating": "3: Weak Reject", "reply_text": "[ Experiment section ] We have revised Section 5 to clarify that : For all the experiments in the paper , we aligned $ ( \\eta , \\beta ) $ for AM1/2-SGD and M-SGD and thus they used the same learning rate schedulers . Our goal is to show their potentials of serving as alternatives to M-SGD . The restart ( which is simply setting the buffers to the current iterate ) is not performed on the LSTM experiment since the decay factor is small ( i.e. , 2 ) and the \u201c decay on plateau \u201d scheduler changes the learning rate frequently when converging . Based on the observations in Appendix A.4 , the restart makes a difference only when the hyper-parameter setting is too aggressive . [ Minor ] We have shortened the \u201c Connections with Katyusha \u201d part and moved the details to the appendices . [ 1 ] Zeyuan Allen-Zhu . Katyusha : The first direct acceleration of stochastic gradient methods , JMLR 2018 . [ 2 ] Ilya Sutskever , James Martens , George Dahl , Geoffrey Hinton . On the importance of initialization and momentum in deep learning , ICML 2013 ."}, {"review_id": "S1xJFREKvB-3", "review_text": " This paper proposes two variants of Nesterov momentum that maintains a buffer of recent updates. The paper proves optimal convergence in the convex setting and makes nice connections to mirror descent and Katyusha. I vote to reject the submission, with my main concerns being with the experimental results. I would consider raising my score if my concerns are addressed. Concerns -The learning rate schedule on the CIFAR experiments is unconventional. The original ResNet paper trains for 64k iterations (roughly 160 epochs). It\u2019s standard to train for at least 200 epochs (see schedule from Smith et al.). Do the results hold under the standard schedule with careful tuning for the baselines? -The discussion on \u201cTrain-batch loss vs. Full-batch loss\u201d in Section 2 is unclear. On smaller datasets, it is feasible to perform evaluation at the end of the epoch on the entire batch. Furthermore, reporting test accuracy couples optimization and generalization, and I am not sure what is meant by the statement \"test accuracy is too informative.\u201d -Reporting the peak test accuracy is strange. In general, we do not have access to the test set. It\u2019s more natural to report the final test accuracy or have a holdout set to determine an iteration for evaluation. -It\u2019s not clear how significant the results on ImageNet and PTB are. Namely, a comparison to AggMo and/or QHM would be good, since the flavor of these algorithms is quite similar. Experiments in the AggMo paper suggest that AggMo performs better on PTB. In the comparison given in Appendix A3, it seems like AggMo performs slightly better throughout training. SGD should also attain better performance with learning rate tuning on ImageNet. -I\u2019m not sure how useful \u201cTest Accuracy STD%\u201d is useful as a metric since it is influenced heavily by the learning rate and its schedule. Tail averaging schemes in general seem like they would increase \u201crobustness.\u201d In addition, there seem to be situations where a higher final variance is beneficial (just run the method for longer and you can find a better solution). It would be nice to expand the discussion on the notion of robustness defined in the paper. Minor Comments -The dashed line in figure 1b) is hard to read. I would recommend removing the grid lines and make the colors more differentiable. -Algorithm 1: use of both assignment and equality operator? Whereas other boxes use equality -Spacing looks a bit off in parts of the paper. 1) after the first sentence in the introduction 2) \u201cgeneric optimization layer (Defazio, 2018) . However\u201d) -M-SGD and M-SGD2 can be potentially confusing and are not too informative as acronyms. -Remark on Theorem 1b: depicts does not seem like the right word Smith, S. L., Kindermans, P. J., Ying, C., & Le, Q. V. (2017). Don't decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489.", "rating": "1: Reject", "reply_text": "[ Robustness ] We think that to evaluate and to understand the behavior of an optimizer , it is important to measure the uncertainty in its convergence . The STD is indicating how aggressive a hyper-parameter setting is for an optimizer . An example is the comparison of SGD and M-SGD ( Appendix C.1 ) : when we align the effective learning rate of SGD and M-SGD , they produce similar performance ( training ResNet34 on CIFAR-10 ) . This phenomenon is used to question the effect of momentum in [ 1 ] . However , by measuring the STD , we see that the difference is that SGD has an average STD at 2.205 % while M-SGD only has 1.040 % , which indicates that this learning rate is too large for SGD or their settings are not at the same level of \u201c aggressiveness \u201d . This observation is suggesting that M-SGD still has room for more aggressive hyper-parameter settings , which can potentially increase the performance . In this sense , AM1-SGD has more room for \u201c aggressiveness \u201d , i.e. , by grid-searching for larger $ \\eta $ or $ \\beta $ that are different from those of M-SGD . We didn \u2019 t do so since the current set-ups already achieve comparable final performance as M-SGD and are easy to use . We will include more discussion on robustness in the next revision . Tail-averaging does improve robustness , which is intuitively the difference between Option II and Option I in Table 1 . What we emphasized is that the amortized momentum ( Option I , no tail-averaging ) also increases robustness . [ Higher final variance is beneficial ] From the intuition of the theory parts , $ m $ is trading acceleration for variance control . In a concrete situation , users can determine the amount of variance control they need . Perhaps an adaptive choice of $ m $ can be better . [ Minor Comments ] Thanks for pointing out those issues . We have fixed them in the revision . [ 1 ] Jerry Ma and Denis Yarats . Quasi-hyperbolic momentum and adam for deep learning , 2019 [ 2 ] Liangchen Luo , et al.Adaptive gradient methods with dynamic bound of learning rate , 2019"}], "0": {"review_id": "S1xJFREKvB-0", "review_text": "%% Post Author Response comments %% Thank you for your detailed response/revision. 1 - Introducing \u201cm-times\u201d larger momentum: Somehow, this is not a particularly intuitive statement or one that reflects clearly in a theoretical bound. Since we are getting to issues surrounding the use of momentum with stochastic optimization, I would like to make a note that the performance of these algorithms more broadly aren't quite sketched out for their use in broader stochastic optimization. In particular, despite broad use in practice, it is unclear if standard variants of Nesterov acceleration/Heavy Ball method achieve \"acceleration\" in stochastic optimization. See for e.g., the work of Kidambi et al ICLR 2018 (\u201cOn the insufficiency of existing momentum schemes for stochastic optimization\u201d) - where, the argument was that these methods were designed for deterministic optimization (where we get exact gradients) - in fact, that paper empirically as well as theoretically shows that these schemes do not offer acceleration in a precise sense compared to specialized algorithms for stochastic optimization. It is unclear if the proposed algorithms can offer a similar improvement over SGD in a provable sense, even for the specific examples described in their paper. 2 - The point about theory (just as you mention) is that it doesn\u2019t directly apply towards the simulations, nor, do they improve on already known algorithms - so I am unable to see the point that these results present broader implications that can guide practice. 3 - The response doesn\u2019t address the fact that for the theory bounds presented in the paper to hold (even in the convex settings described), one requires knowledge of parameters that are not known a-priori, and are often fairly difficult to estimate. So the performance of the algorithm in practice may quite significantly be away from the bounds described in the paper. While I appreciate the points and revision made by the authors, I still believe the paper requires some rethinking to present their results (and this includes more detailed comparisons to existing works) in order to make a case towards broader practical applications. %%. %% This paper considers robustness issues faced by Nesterov\u2019s Acceleration used with mini-batch stochastic gradients for training Deep Models. In particular, the paper proposes amortized momentum, an algorithm that offers a way to handle these issues. The paper in general is well written and easy to follow. The paper proposes algorithms AM-SGD1 and AM-SGD2 and presents extensive results regarding their complexity analysis on convex problems and their performance when training neural networks. The algorithms require storing one more model\u2019s worth of storage compared to standard momentum based methods (which can be viewed as a drawback in certain cases). Comments: [1] I am concerned about the motivation behind this paper - which, according to the paper is that Nesterov\u2019s accelerated gradient method with stochastic gradients has huge initial fluctuations. The issue with regards to more fluctuations of the initial performance is natural given how aggressive these accelerated methods work. As long as this is not a reason/cause for worse terminal performance (which doesn\u2019t seem to be the case), I am unable to see why large initial fluctuations are concerning. [2] Theory: The theory bounds for this problem setting do not appear to improve over known bounds in the literature. As a side note, the work of Hu et al. \u201cAccelerated Gradient Methods for Stochastic Optimization and Online Learning\u201d is highly related to this paper\u2019s theoretical aspects, setup and bounds. Furthermore, this bounded variance noise model for stochastic gradients, while being theoretically useful (and important), is often very detached from practice (as this implies that the domain is bounded and we perform projections of iterates whenever they go outside the set - such aspects hardly reflect on practical SGD implementations). Using this as a means to reason about robustness of the proposed algorithm (for e.g. remarks for theorem 1a. and in conclusions) appears to be a big leap that may lead to potentially misleading conclusions. [3] In order to run the algorithm to achieve the theoretical bounds claimed (in theorems 1 and 2), it appears that the stepsize \\alpha_s depends on unknown quantities such as initial distance to opt, noise variance etc. [4] The claim in page 2 about comparing SGD and M-SGD says that the stepsize in deterministic and stochastic optimization is constrained to be O(1/L) is rather misleading. In realistic practical implementation of SGD with a multiplicative noise oracle, one really has to use a much smaller stepsize than 1/L. This in a sense leads back to point[2] about the unrealistic nature of bounded variance assumptions for understanding SGD based methods used in the context of Machine Learning. They are better suited for understanding stochastic methods in black-box optimization (as opposed to considering Machine Learning problems). My take is that even if the authors justify novelty in terms of theory results (which, to my knowledge is limited compared to existing literature), rewriting the paper by considering its theoretical merit and presenting empirical results (even as considered in this paper) of this algorithm (without attempting to make very strong connections to explain issues experienced in non-convex training of neural networks, since the theory works in vastly different settings under restrictive assumptions) can be appreciated by appropriate sections of audience (both in theory as well as optimization for deep learning communities).", "rating": "3: Weak Reject", "reply_text": "Thanks for carefully reading our paper and your thoughtful comments . [ 1 , Motivation ] We admit that we should have made the motivation clearer in the introduction . Our motivation is to design a favorable alternative for M-SGD . We have listed all the pros and cons of AM1-SGD in the introduction and emphasized the \u201c easy-to-use \u201d feature in the revision . In the original submission , we tried to point out the robustness of M-SGD , for which the amortization technique has the most obvious effect : the amortized momentum is aggressive ( intuitively , AM1-SGD is injecting $ m $ -times larger momentum ( compared with M-SGD ) into plain SGD every $ m $ iterations ) , but it is able to reduce the fluctuations in the initial stage ( comparing SGD and AM1-SGD with Option I in Figure 3 ) . We agree that the terminal performance of M-SGD is not affected by the initial fluctuations , which makes the motivation unclear in the introduction . Thus , we have removed the related paragraph in the introduction in the revision . [ 2 , Theory ] - The theory bounds for this problem setting do not appear to improve . The theory bounds are already optimal , and actually the novelty of this paper is about how we modified M-SGD ( AC-SA ) to fit our high level idea while maintaining the optimality . Our purpose of the theory section is mainly to understand what has changed when amortizing Nesterov \u2019 s momentum ( the effect of $ m $ ) . While these results do not hold for deep learning problems in general , they shed light on the tuning of $ m $ in deep learning applications . We have revised the first paragraph in Section 4 to make it clearer . - The work of Hu et al . [ Hu et al. , 2009 ] and the work [ Ghadimi & Lan , 2012 ] both focus on extending AC-SA [ Lan , 2012 ] to strongly convex setting and refining its parameter choices . In comparison , the assumptions in [ Ghadimi & Lan , 2012 ] are more general and more reasonable ( [ Hu et al. , 2009 ] considers the unconstrained case and assumes that an algorithm-generated sequence is bounded ) . We added [ Hu et al. , 2009 ] to the reference in the revision . - The bounded variance noise model for stochastic gradients . We chose the stochastic bounded variance model since we want to understand why the amortization trick can reduce fluctuations . It actually turns out that in the deterministic setting ( $ \\sigma=0 $ ) , amortization ( increasing $ m $ ) is meaningless ( Theorem 1a and 2 ) . - The domain is bounded and we perform projections of iterates . Compactness is assumed only in Theorem 1b . Theorem 1a and 2 hold in unconstrained setting ( $ X = \\mathbb { R } ^d , h \\equiv 0 $ ) . - Using this as a means to reason \u2026 is a big leap . We admit that the statements about using the theory to reason the empirical results in the original submission are misleading , and we have revised the statements to say that the theory can provide a better understanding of the empirical results . [ 3 , $ \\alpha_s $ depends on unknown quantities ] We did not follow [ Ghadimi & Lan , 2012 ] and [ Hu et al. , 2009 ] to derive a better parameter setting because it does not contribute to the understanding of $ m $ but complicates the analysis . We will add some comments about this in the next revision . [ 4 , $ 1/L $ ] Thanks for pointing this out . We have modified all the $ O ( 1/L ) $ to strict upper bounds . About using intuitions from convex analysis : Nesterov \u2019 s momentum was migrated from convex optimization to deep learning by [ Sutskever et al. , 2013 ] and it turned out to be quite successful . We actually followed [ Sutskever et al. , 2013 ] to design tricks in convex analysis and explore their benefits in deep learning applications . [ Hu et al. , 2009 ] , Accelerated Gradient Methods for Stochastic Optimization and Online Learning , 2009 [ Ghadimi & Lan , 2012 ] . Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i : A generic algorithmic framework , 2012 [ Lan , 2012 ] . An optimal method for stochastic composite optimization , 2012 [ Sutskever et al. , 2013 ] . On the importance of initialization and momentum in deep learning , 2013"}, "1": {"review_id": "S1xJFREKvB-1", "review_text": "This paper provides a new simple method to incorporate Nesterov momentum into standard SGD for deep learning, with good empirical and theoretical results. Overall I think this paper should be accepted, some minor comments follow. At no point does Polyak's heavy ball method get mentioned, even though the variant of Nesterov acceleration you are considering is very similar to it (since the momentum parameter is fixed, which is not the usual form of Nesterov except in the strongly convex case). It would be beneficial to delineate how this is or isn't related to heavy ball. The experiments would benefit from a wall-clock time comparison too, rather than just epochs since these new methods would be slower (but presumably not by much). The appendix is huge with most of the technical details relegated there which I did not read fully. I think this impacts the readability significantly, though not grounds for rejection. Perhaps it suggests that a conference with a small page limit is not the best venue? It seems that SGD still has better convergence early on. The authors suggest their method fixes this (relative to standard nesterov SGD) but it doesn't seem to be quite as good as SGD. Can you explain or discuss why this is still the case? The assumptions require some explanation, they are just listed with no context. What are they and why are they useful? Step size \"should be constrained to O(1/L)\" is misleading, you should say explicitly that step-size <= 1/L (or whatever it is depending on the algorithm). Some of the writing is a bit strange / sloppy, e.g.: \"AM2-SGD is a bit tricky in randomness\" \"However, full-batch loss is way too expensive to evaluate.\" In Algorithm 1 AM1-SGD: \"xk+1 \u2190 xk+1 + \u03b2 \u00b7 (\u02dcx+ \u2212 x\u02dc)\" doesn't parse because x_{k+1} appears twice. Missing references: Accelerated proximal algorithms: *) Beck and Teboulle: A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems *) Nesterov: Gradient Methods for Minimizing Composite Objective Function, Restarting (slightly different to your approach but still relevant): *) O'Donoghue: Adaptive Restart for Accelerated Gradient Schemes ", "rating": "8: Accept", "reply_text": "Thank you for your positive feedbacks . We have added more explanations to the assumptions , changed $ O ( 1/L ) $ to strict upper bounds , improved the writing and the presentation of Algorithm 1 , made the main text more independent from the appendices , and cited missing references in the revision . [ Compare with Polyak \u2019 s heavy ball ( HB ) ] If $ m=1 $ , AM1-SGD reduces to M-SGD and its difference from HB has been carefully studied in [ 2 ] . Here we consider a simple case $ m=2 $ . We only compare 2 iterations $ x_0\\rightarrow x_1\\rightarrow x_2 $ of AM1-SGD ( Option I ) and HB . $ g_0 $ and $ g_1 $ denote the gradients evaluated at $ x_0 $ and $ x_1 $ and we mark the learning rate and momentum parameter ( $ > 0 $ ) for AM1-SGD and HB as $ ( \\eta , \\beta ) $ and $ ( \\alpha , \\gamma ) $ , respectively . Since the first iteration of both AM1-SGD and HB is SGD , if $ \\alpha = \\eta $ , they generate the same $ x_1 $ ( and thus the same $ g_1 $ ) . After the second iteration , AM1-SGD produces $ $ x_2 = x_0 - \\eta ( 1 + \\beta ) \\cdot g_0 - \\eta ( 1 + \\beta/2 ) \\cdot g_1 , $ $ and HB produces $ $ x_2 = x_0 - \\alpha ( 1 + \\gamma ) \\cdot g_0 - \\alpha \\cdot g_1 . $ $ It is unlikely for them to produce the same $ x_2 $ . [ Wall-clock time ] We recorded all the wall-clock time in the CIFAR experiments . However , we observed that even on the same type of GPUs , the running times fluctuate a lot and do not exhibit a clear trend with increasing $ m $ ( For AM1-SGD , a larger $ m $ leads to a lower amortized cost ) . Thus , we did not report them in the paper . The running time is improved by about 2 % -3 % for AM1-SGD ( $ m=5 $ ) compared with M-SGD ( measured on the same GPU and using the same random batches ) . AM2-SGD ( $ m=5 $ ) is slightly slower than M-SGD . [ SGD still has better convergence early on ] Here we provide some possible explanations in the ( strongly ) convex setting ( The objective is $ L $ -smooth and $ \\mu $ -strongly convex , we denote $ \\kappa = L/\\mu $ as the condition number ) : In theory , accelerated algorithms ( e.g. , NAG , Katyusha ) are only faster in the ill-conditioned case ( when $ \\kappa $ is very large ) . When $ \\kappa $ is small ( the well-conditioned case ) , their convergence rates are the same as non-accelerated algorithms ( e.g. , GD , SVRG ) . In practice , it is always observed that in the well-conditioned case , non-accelerated algorithms perform better than accelerated algorithms and increasing momentum only deteriorates the performance . In the ill-conditioned case , it is frequently observed that non-accelerated algorithms have a faster early convergence but are surpassed by accelerated algorithms later on , just like the results of SGD and M-SGD given in the paper . Our conjecture is that in the early stage of training , the problem is locally well-conditioned and momentum hurts the performance . When the iterate enters an ill-conditioned region , the momentum becomes effective and provides acceleration . This negative effect is like an intrinsic disadvantage of momentum that is \u201c transferred \u201d to deep learning training . The amortization technique eases this effect by trading acceleration with variance control ( using the intuition of Section 4 ) . A larger $ m $ can produce a faster early convergence than SGD ( Figure 6 ) but reduces more final performance . Perhaps using an adaptive choice of $ m $ is better . [ 2 ] Ilya Sutskever , James Martens , George Dahl , Geoffrey Hinton . On the importance of initialization and momentum in deep learning , ICML 2013 ."}, "2": {"review_id": "S1xJFREKvB-2", "review_text": "The authors proposed Amortized Nesterov\u2019s Momentum, a variant of Nesterov\u2019s momentum that utilizes several past iterates, instead of one iterate, to provide the momentum. The goal is to have more robust iterates, faster convergence in the early stage and higher efficiency. The authors designed two different realizations, AM1-SGD and AM2-SGD. Comments: My major concern for this paper is its unconvincing motivation and experiment results, especially when the approach is designed for deep learning. The motivation for the proposed approach is not quite convincing. The authors said that \u201cdue to the large stochasticity, SGD with Nesterov\u2019s momentum is not robust...This increased uncertainty slows down its convergence especially in the early stage of training and makes its single run less trustworthy\u201d For image classification, Nesterov momentum is very popular and the final convergence values of different trails seems to be similar. It would be more convincing if the authors can provide practical evidence for supporting this claim. It was claimed that Amortized Nesterov\u2019s Momentum has \u201chigher efficiency and faster convergence in the early stage without losing the good generalization performance\u201d. What is the benefit or advantage for having faster early convergence without improving the final generalization performance? The authors claim that \u201cM-SGD2 is more robust and achieves slightly better performance\u201d, in Figure 1a, however, it is really hard to tell the difference between M-SGD2 and M-SGD from Figure 1a. The efficiency improvement in page 4 is really hard to follow for comparison with Algorithm 1 in page 3. Though m > 2 could reduce the number of operations in step 5 and 6, I don\u2019t think this is a computational bottleneck. I believe these updates should be very fast in comparison with forward and gradient calculation. Making the 1% computation 50% faster does not mean more efficient. I would like to know how much computation cost can be saved with this modification. On the other hand, adding one more auxiliary buffer (scaled momentum) could significantly impact the training as the memory is often the limit. In section 3.1, what is \u201cidentical iteration\u201d? It is hard to compare AM2-SGD with AM1-SGD. It would be easier to follow if the side-by-side algorithm comparison can be shown early. The section 4\u2019s theoretical analysis based on the convex composite problem is not quite convincing. I am not sure how these results are related with the deep learning applications. In the experiment section, the comparison of AM1/2-SGD with other baselines seems not quite consistent. The authors first state that they use all 0.1 learning rate and 0.9 momentum for all methods, however, the setting for M-SGD is using 0.99 momentum and different learning rate schedule. This makes the comparison not very meaningful, while AM1-SGD and AM2-SGD do not use learning rate restart. With so many differences, the advantage of AM1-SGD and AM2-SGD are not that different with M-SGD. In the task of ImageNet-152, M-SGD even is better than AM1-SGD. This makes the conclusion that \u201cAM1-SGD has a lightweight design, which can serve as an excellent replacement for M-SGD in large-scale deep learning tasks\u201d not quite valid. Minor: The author may assume readers maybe familiar Katyusha momentum, I think there may need more background about it. ", "rating": "3: Weak Reject", "reply_text": "[ Experiment section ] We have revised Section 5 to clarify that : For all the experiments in the paper , we aligned $ ( \\eta , \\beta ) $ for AM1/2-SGD and M-SGD and thus they used the same learning rate schedulers . Our goal is to show their potentials of serving as alternatives to M-SGD . The restart ( which is simply setting the buffers to the current iterate ) is not performed on the LSTM experiment since the decay factor is small ( i.e. , 2 ) and the \u201c decay on plateau \u201d scheduler changes the learning rate frequently when converging . Based on the observations in Appendix A.4 , the restart makes a difference only when the hyper-parameter setting is too aggressive . [ Minor ] We have shortened the \u201c Connections with Katyusha \u201d part and moved the details to the appendices . [ 1 ] Zeyuan Allen-Zhu . Katyusha : The first direct acceleration of stochastic gradient methods , JMLR 2018 . [ 2 ] Ilya Sutskever , James Martens , George Dahl , Geoffrey Hinton . On the importance of initialization and momentum in deep learning , ICML 2013 ."}, "3": {"review_id": "S1xJFREKvB-3", "review_text": " This paper proposes two variants of Nesterov momentum that maintains a buffer of recent updates. The paper proves optimal convergence in the convex setting and makes nice connections to mirror descent and Katyusha. I vote to reject the submission, with my main concerns being with the experimental results. I would consider raising my score if my concerns are addressed. Concerns -The learning rate schedule on the CIFAR experiments is unconventional. The original ResNet paper trains for 64k iterations (roughly 160 epochs). It\u2019s standard to train for at least 200 epochs (see schedule from Smith et al.). Do the results hold under the standard schedule with careful tuning for the baselines? -The discussion on \u201cTrain-batch loss vs. Full-batch loss\u201d in Section 2 is unclear. On smaller datasets, it is feasible to perform evaluation at the end of the epoch on the entire batch. Furthermore, reporting test accuracy couples optimization and generalization, and I am not sure what is meant by the statement \"test accuracy is too informative.\u201d -Reporting the peak test accuracy is strange. In general, we do not have access to the test set. It\u2019s more natural to report the final test accuracy or have a holdout set to determine an iteration for evaluation. -It\u2019s not clear how significant the results on ImageNet and PTB are. Namely, a comparison to AggMo and/or QHM would be good, since the flavor of these algorithms is quite similar. Experiments in the AggMo paper suggest that AggMo performs better on PTB. In the comparison given in Appendix A3, it seems like AggMo performs slightly better throughout training. SGD should also attain better performance with learning rate tuning on ImageNet. -I\u2019m not sure how useful \u201cTest Accuracy STD%\u201d is useful as a metric since it is influenced heavily by the learning rate and its schedule. Tail averaging schemes in general seem like they would increase \u201crobustness.\u201d In addition, there seem to be situations where a higher final variance is beneficial (just run the method for longer and you can find a better solution). It would be nice to expand the discussion on the notion of robustness defined in the paper. Minor Comments -The dashed line in figure 1b) is hard to read. I would recommend removing the grid lines and make the colors more differentiable. -Algorithm 1: use of both assignment and equality operator? Whereas other boxes use equality -Spacing looks a bit off in parts of the paper. 1) after the first sentence in the introduction 2) \u201cgeneric optimization layer (Defazio, 2018) . However\u201d) -M-SGD and M-SGD2 can be potentially confusing and are not too informative as acronyms. -Remark on Theorem 1b: depicts does not seem like the right word Smith, S. L., Kindermans, P. J., Ying, C., & Le, Q. V. (2017). Don't decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489.", "rating": "1: Reject", "reply_text": "[ Robustness ] We think that to evaluate and to understand the behavior of an optimizer , it is important to measure the uncertainty in its convergence . The STD is indicating how aggressive a hyper-parameter setting is for an optimizer . An example is the comparison of SGD and M-SGD ( Appendix C.1 ) : when we align the effective learning rate of SGD and M-SGD , they produce similar performance ( training ResNet34 on CIFAR-10 ) . This phenomenon is used to question the effect of momentum in [ 1 ] . However , by measuring the STD , we see that the difference is that SGD has an average STD at 2.205 % while M-SGD only has 1.040 % , which indicates that this learning rate is too large for SGD or their settings are not at the same level of \u201c aggressiveness \u201d . This observation is suggesting that M-SGD still has room for more aggressive hyper-parameter settings , which can potentially increase the performance . In this sense , AM1-SGD has more room for \u201c aggressiveness \u201d , i.e. , by grid-searching for larger $ \\eta $ or $ \\beta $ that are different from those of M-SGD . We didn \u2019 t do so since the current set-ups already achieve comparable final performance as M-SGD and are easy to use . We will include more discussion on robustness in the next revision . Tail-averaging does improve robustness , which is intuitively the difference between Option II and Option I in Table 1 . What we emphasized is that the amortized momentum ( Option I , no tail-averaging ) also increases robustness . [ Higher final variance is beneficial ] From the intuition of the theory parts , $ m $ is trading acceleration for variance control . In a concrete situation , users can determine the amount of variance control they need . Perhaps an adaptive choice of $ m $ can be better . [ Minor Comments ] Thanks for pointing out those issues . We have fixed them in the revision . [ 1 ] Jerry Ma and Denis Yarats . Quasi-hyperbolic momentum and adam for deep learning , 2019 [ 2 ] Liangchen Luo , et al.Adaptive gradient methods with dynamic bound of learning rate , 2019"}}