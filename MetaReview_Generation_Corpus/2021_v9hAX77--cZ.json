{"year": "2021", "forum": "v9hAX77--cZ", "title": "Learning Structural Edits via Incremental Tree Transformations", "decision": "Accept (Poster)", "meta_review": "This paper proposes a model for predicting edits to trees given an edit specification that comes either from the ground truth before-after state (\u201cgold\u201d setting, like reconstruction error of auto-encoder) or from the before-after state of an analogous edit. The problem setting follows mostly from Yin et al (2019). \n\nThere are several shortcomings of this paper:\n\n1. The technical novelty of the model is somewhat limited, as it\u2019s an assembly of components that have been used in related work. Authors insist in the discussion on the novelty of the tree edit encoder (Sec 3.2), but I think this is overstated. The related tree-edit models (e.g., Tarlow et al (2019)) perform a very similar encoding *in the decoder* when training with teacher-forcing. While it\u2019s true that decoders are typically thought of as monolithic entities that generate a sequence of edits from a state, inside the teacher-forced training, the models are computing a representation of a prefix of ground truth edits, which are then repeatedly used to predict next edits. AFAIU, the proposal is basically to use this hidden representation as the edit encoder. \n\n2. The claim that the approach is more language agnostic than Dinella et al (2020) also seems shaky, as the authors admit in their response that language-specific grammars need to be handled specially. E.g., I expect that the authors of Dinella et al would find it easier to extend their existing code to use a new language than to adapt this approach.\n\n3. The submission relies too heavily on the \u201cgold\u201d setting (where the target output is fed as an input), and I\u2019m skeptical of their characterization of Yin et al\u2019s intentions when the authors say in comments, \u201cBecause of this, models that are able to reproduce the desired output effectively have a demonstrably better inductive bias that allows them to do so efficiently. This was the original motivation expressed by Yin et al. (2019).\u201d  I don\u2019t see this stated in the Yin et al paper. I see Yin et al. characterizing this setting as an upper bound and saying \u201cbetter performance with the gold-standard edit does not necessarily imply better (more generalizable) edit representation.\u201d (Yin et al., 2019). It\u2019s worrying that the proposed model only seems to do better in this setting, which would be very easy to game if one were aiming to directly optimize for it. \n\nHaving said this, (1) is not a standard way to think about encoding edits, (2) is debatable, and we can hope that future work does not treat improvements in the \u201cgold\u201d setting as a valid research goal. Further, there is another contribution around imitation learning that the reviewers appreciate. In total, reviewers did an excellent job and generally believe the paper should be accepted. I won\u2019t go against that recommendation.", "reviews": [{"review_id": "v9hAX77--cZ-0", "review_text": "The paper presents a neural autoregressive model that learns to incrementally or iteratively perform edit actions on structured data . The authors focus specifically on abstract syntax tree representation of programs ( e.g. , C # ) . The model has two main parts : ( 1 ) Neural editor models p ( a| ) that iteratively performs tree edit actions such as sub-tree deleting or adding ; ( 2 ) Edit encoder learns edit representations f_Delta by encoding the sequence of ground truth tree edit actions . The authors also propose an imitation learning algorithm to train the editor and evaluate the model on source code edit datasets . The idea of incremental tree transformation pretty much follows that of Hoppity by Dinella et al , ICLR 2020 for bug fixing of Javascript programs . Unlike Hoppity where each transformation step is done on a single node , this work extends to support sub-tree operations . However , adding a subtree is nothing but performing a sequence of single-node actions . The difference is that to ensure the syntactic validity of the tree at any point , the authors use a grammar specified a priori . This mechanism was proposed in semantic parsing by Yin et al. , ACL 2017 . Compared to Hoppity , this model has another SubTreeCopy operation , but it is a somewhat straightforward extension of the copy mechanism from Yin et al.Although the whole idea is not new , I think the paper presents a valued extension to existing work . The running example is intuitive , making the paper easy to follow . However , there are a number of parts that are unclear and need more clarity . I hope the authors address these during the rebuttal phase . 1.The authors should be clear ( e.g. , before Equation 1 and in Section 3 ) about where the sequence actions { a_t } comes from and how sub-tree actions are represented ( e.g. , decomposed into single-node actions ) . Later in this experiment , the authors mention dynamic programming to compute the shortest tree edit sequence , but I feel that substantial discussion or an algorithmic description is needed . 2.It is not clear to me whether f_Delta is learned jointly or separately from the parameters of p ( a| ) . In Figure 1 and Equation 1 , f_Delta does not depend on t and seems to be fixed . 3.In the last paragraph of page 3 , this sentence \u201c the operator selects a dummy node ( e.g.node Dummy ) and replaces it with the added node \u201d is not clear . Is there always at least one dummy node at any time ? What is \u201c the added node \u201d ? 4.Another question about Add operation : if an Add operation is given and there is currently no dummy node , would it make more sense to specify the added location with respect to a parent node and its children ? 5.How does the model know when to stop adding a right sibling for constructor fields with * sequential * cardinality ? Similarly , for an * optional * cardinality field , what does the model do on the attached dummy node if the field is indeed optional ? 6.It may be more natural to predict the node location n_t before the operation op_t . Equation 2 does the other way . Does this change the model in any way ? 7.To what extent this framework is language-agnostic ? Despite the ASDL , even for the same language , different parsers can have different grammar specifications , so how easy is it to apply this framework for other languages ? On the same note , since Hoppity is directly related , I am curious how this work compares to it on a same bug fixing task . Finally about the experiments , the authors compare to Graph2Tree and CopySpan . But it does not look like the source code for the baselines are available . Releasing the source code with that of the baselines would be helpful . Minor typos \u201c Arbitary \u201d : Section 3.1.1 paragraph 2 . = After discussion = I have increased my score from 6 to 7 .", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 2 for recognizing the value of our work and giving the detailed comments ! * * Answers to reviewer \u2019 s questions * * 1 . Thanks for your advice ! We will revise our draft to make clear the actions \u2019 definitions and where they come from . To clarify , the CopySubTree operator copies a _complete_ subtree from the initial input tree ( g_1 ) to the current tree ( g_t ) in one single step , rather than decomposing it into multiple steps of adding tree nodes . An example is shown in Fig 1 ( b ) at t=4 , where the whole subtree \u201c Expr \u2192 i + 1 \u201d is copied from the input tree g_1 in Fig 1 ( a ) . The dynamic programming algorithm mentioned in Sec 3.3 is used only for calculating gold-standard edit sequences in our training data . We will add a pseudo-code of the algorithm in the revised version . 2.As we mentioned at the beginning of Sec 3.3 , the edit encoder ( which produces the edit representation f_delta ) is jointly trained with the editor . It is correct that f_delta does not depend on the time step t , but it is a non-fixed , learnable vector representation . 3.A dummy node denotes a vacant position that is syntactically valid to accept a tree node ( see node \u201c Dummy \u201d in Fig 1 ( b ) at t=1 for example ) . It is automatically added following the underlying grammar to ensure no missing child for each tree node . From another perspective , this also means that it is possible to _not_ have dummy nodes , e.g. , when all tree nodes \u2019 syntactically valid child nodes have been fulfilled . * * Note that this will never happen if any tree node has a _sequential_ field , because we always append one dummy node to a sequential field and a sequential field can have a new child inserted to every possible child position in any time . * * For our editor , * * in the case of single/optional fields , * * an Add operator has to be applied to a dummy node . This is intuitive since the Add operation means to add a tree node to a certain syntactically valid position in the current tree , and all such positions * * for single/optional fields * * have been held by dummy nodes . The \u201c added node \u201d in the sentence refers to a _non-terminal_ node such as node \u201c ElementAccess \u201d in Fig 1 ( b ) at t=2 . Predicting a non-terminal to-be-added node is equivalent to selecting a production rule ( e.g. , AssignStmt \u2192 ElementAccess ) for the node \u2019 s parent ( e.g. , AssignStmt ) . Note that the Add operator can also be used to populate an empty _terminal_ node with a literal value ( e.g. , token \u201c list \u201d at t=3 ) . In this case , deciding the terminal node to be added is equivalent to predicting a token in place . 4.Please refer to our answer \u201c 3. \u201d for an explanation about the dummy node mechanism . To this question : since all available positions that are syntactically allowed to accept tree nodes * * for single/optional fields * * have been replaced by dummy nodes in our mechanism , having no dummy node means having no valid position for adding nodes * * to single/optional fields * * in the current tree . In this case , an Add operator is illegitimate * * for such fields * * and will be eliminated from the operator candidates . * * For nodes with sequential fields , there will always be a dummy node appended to it ( e.g. , [ A , B , Dummy ] ) . To add a new node ( e.g. , node C ) to a certain position of a sequential field , our model first selects the \u201c right sibling node of the target position \u201d , and then inserts the new node to its left . For example , adding C before A is done by selecting A and inserting C before it ; adding C as the actual rightmost child is done by selecting Dummy and inserting C before it . * * 5.Due to space constraints , we discuss details about ASDL implementation in Appendix A.1 . For fields with sequential cardinality , there is always one extra dummy node attached as its rightmost child . For example , the child list [ A , B ] is extended to [ A , B , Dummy ] . Therefore , our editor always has the option to add a right sibling to the child list of a sequential-cardinality field . This is implemented by selecting Dummy and replacing it with the to-be-added tree node , which will extend the child list into [ A , B , C , Dummy ] ( with C being the added node ) . * * Note that this is the same as first selecting Dummy and then inserting node C to its left , which we described in the edited answer \u201c 4 \u201d . * * \u201c When to stop adding a right sibling \u201d is left as a learning problem to the neural editor itself . For optional-cardinality fields , a dummy node is attached when their child position is not taken . To add a child node to the field , our editor similarly replaces the dummy node with the node to be added . When the field has already gotten a non-Dummy child , no more dummy child node is attached to ensure its grammatical correctness . 6.We decide the operator prior to the node location simply because for the operator Stop , selecting whatever node location is meaningless . Following our current design , in inference time , once the editor chooses the Stop operator , it has no need to pick the node location , which is more natural than the other way . ..."}, {"review_id": "v9hAX77--cZ-1", "review_text": "# # Summary The paper proposes a general model for incremental editing of tree-structured data such as abstract syntax trees . The editing operations include adding a node , deleting a subtree , or copying a subtree . They also propose a novel edit encoder to learn to represent edits , and an imitation learning method to make the model more robust . # # Pros - The work has several interesting and valuable contributions : + Compared to previous work , the model is much more general : it supports general tree edits , is language agnostic , and can handle much longer edit sequences . + The novel edit encoder which directly encodes the edit actions is more intuitively correct and also performs better than previous approaches . + Imitation learning to make the model more robust is a natural idea that suits incremental edits very well . - The source code will be released which -- given the general nature of the model -- could enable further interesting research . # # Concerns - The explanation for the results in Table 1 could be improved upon . + I do n't think the results support the claim that Seq Edit Encoder memorizes specific patterns with the baselines as the micro average of Graph2Tree for Fixers-one shot is very close to Graph2Edit . + Even if we accept the claim , that would not explain the CopySpan > Graph2Tree > Graph2Edit results on GHE-gold and Fixers-gold . The authors state that Seq Edit memorizes specific patterns , TreeDiff Edit learns more generalizable information , and Graph2Edit makes Seq Edit learn more generalizable information . But they state that in order to solve GHE-gold , we need specific patterns instead of generalization . So why does increasing generalization by switching Seq Edit to TreeDiff Edit improve performance of Graph2Edit on GHE-gold and Fixers-gold ( which does n't need generalization ) , and decrease performance on Fixers-one shot ( which needs generalization ) ? - Some of the contributions are not demonstrated clearly : + The method 's applicability to much longer edit sequences is not clearly demonstrated , although the length of the edit sequences is mentioned briefly in the Appendix . + The imitation learning method is demonstrated only on `` Graph2Edit with Seq Edit Encoder '' , which is the less interesting case compared to `` Graph2Edit with TreeDiff Edit Encoder '' . From the previous experiment it 's clear that the TreeDiff encoder version is the practically relevant one and the one the paper 's about . As the Seq Edit encoder makes more mistakes and so it 's easier to improve , we do n't know whether imitation learning helps in the relevant case of the TreeDiff encoder . In my opinion this makes imitation learning more of a digression and a less of an organic part of the paper . - The writing could be more precise at times . For example , the authors state that they are adding subtrees as an edit operation , but as far as I understand , they are adding individual nodes . # # Reasons for ranking I believe that the model is an important step in learning to represent edits . However there are some problems with the experiments : some of the claims are not adequately supported and the explanations could be improved upon . # # Minor comments - I found Figure 1 confusing at first , because there is essentially no caption and the description of the figure comes much later in parts . It would be good to either have a more substantial caption or to move the figure closer to the explanations . - Table 2 precedes Table 1 , which is confusing - It should be `` general '' , not `` generic '' , like `` general model for incremental editing '' - Page 3 : Delete operators take a tree node ... and remove - The algorithms in the Appendix should be DaggerSampling and PostRefineSampling ( missing `` l '' ) .", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 3 for recognizing our contributions as \u201c interesting and valuable \u201d ! * * Explanations for Table 1 * * 1 . Clarification of \u201c Seq Edit Encoder memorizes specific patterns ( about C+ ) with the baselines \u201d : We make this conjecture when observing the two baselines , CopySpan and Graph2Tree , show superior performance on GHE/Fixers-gold but worse performance on Fixers-one shot . Intuitively , this is because the target output for the two models , the ground-truth edited code C+ , has been exposed to the Seq Edit Encoder . However , due to their different architectures , the two models still show different behavior . For Graph2Tree , since it generates C+ _in the form of an AST tree_ , it suffers less from the \u201c memorization \u201d or overfitting of the Seq Edit Encoder ; this explains its better accuracy on Fixers-one shot . In contrast , since CopySpan generates C+ _in the form of a token sequence , which is exactly the same as how C+ is encoded by the Seq Edit Encoder_ , it suffers the most from the \u201c memorization \u201d problem and can not generalize well to the Fixers-one shot setting . 2.Clarification of the performance of \u201c Graph2Edit + TreeDiff Edit Encoder \u201d : We first clarify that the TreeDiff Edit Encoder has the advantages of being both _expressive_ and _generalizable_ . The \u201c expressive \u201d explains the improvement of Graph2Tree and Graph2Edit when they are paired with TreeDiff Edit . Second , as we discussed in the end of Sec 5.2 , the decreased performance of Graph2Edit on Fixers-one shot is likely because it has overfitted to the specific edit representations during training . Note that all models ( including Graph2Edit ) are trained on the GHE-gold training set and are only tested on Fixers-one shot . This overfitting is caused by a similar \u201c memorization \u201d issue that CopySpan has , when the target output of the model ( ground-truth tree edits for Graph2Edit and ground-truth code tokens for CopySpan ) has been exposed to the edit encoder ( TreeDiff Edit for Graph2Edit and Seq Edit for CopySpan ) in exactly the same format . * * Response regarding some of the contributions not being demonstrated clearly * * 1 . About \u201c longer edit sequences \u201d : please see * * \u201c To All Reviewers \u201d * * for clarification , as well as other discussions about the overall contribution/novelty of our work . 2.About the imitation learning experimental setting : We agree with the reviewer that experimenting with `` Graph2Edit with TreeDiff Edit Encoder '' would definitely be more interesting and more convincing in supporting our claim . We apologize for not being able to explain this in our initial submission . As suggested , we experimented with the setting using \u201c Graph2Edit with Treediff Edit Encoder , \u201d and found that , interestingly , it seems that the model does sufficiently well on the GHE dataset already without imitation learning that further gains through imitation learning were hard to obtain . ( In fact , 80 % of the remaining errors were due to issues such as unknown tokens , which can not be fixed with better training algorithms , as they are outside of the search space of our current model . ) Note that we do _not_ believe that this is necessarily a result discounting the utility of imitation learning in general , but rather just a result of the GHE-gold setting being relatively easy ( compared to , for example Fixers-one shot ) . However , we do not immediately have training data to use for imitation learning in harder settings ( Fixers is test data only ) , so as a proxy for testing on a harder dataset we believe the experiments with the weaker \u201c Graph2Edit with Seq Edit Encoder \u201d model provide a good proxy demonstrating imitation learning \u2019 s potential . Experimenting with imitation learning on harder datasets is definitely very high on our list of things to do -- we were not able to do it in the short time span for author response , but if you think it would contribute significantly to the paper we can try to do it for the final version . * * Regarding preciseness of writing * * Thank you for your advice ! We will revise Sec 3.1.1 to describe the tree edit actions more precisely . To clarify , the CopySubTree operator copies the whole subtree from g_1 ( the initial tree ) to g_t ( the current tree at time step t ) in one single step , rather than \u201c adding individual nodes \u201d in multiple steps . An example is shown in Fig 1 ( b ) at t=4 , where the whole subtree \u201c Expr \u2192 i + 1 \u201d is copied from the input tree g_1 in Fig 1 ( a ) . In our model architecture , the tree edit decoder decides which subtree to copy from g_1 , by using the learned representation of the root node of this subtree ( i.e. , the node representation of \u201c Expr \u201d in the running example ) as a feature . Due to space constraints , we have only briefly mentioned this in the last paragraph of Sec 3.1.3 , but more details can be found in Appendix A.2 . We also thank you for the minor comments ! We will address them in the revised draft ."}, {"review_id": "v9hAX77--cZ-2", "review_text": "# # # Summary # # # The paper presents an approach for predicting edits in programs , by modeling the programs as trees . The approach is mainly an extension of Yin et al . ( 2019 ) , with the main difference that the model is required to predict only the output * * actions * * , instead of generating the entire output tree as in Yin et al . ( 2019 ) .This difference of predicting only output actions is shared with other previous work though . The most interesting part in my opinion is the `` imitation learning '' improvement : during training , the model is trained to correct its own mistakes by `` imitating '' an expert that fixes the incorrect predictions . Overall , I vote for acceptance . Although the technical contribution is limited , the paper presents strong empirical results and a combination of interesting ideas . I think that the paper could be easily further improved , as detailed below . # # # Strengths # # # 1 . The paper presents improved results over the Graph2Tree model ( Yin et al.2019 ) and over CopySpan ( Panthaplackel et al. , 2020a ) . 2.The imitation learning part is very interesting , and its applicability for programs is novel as far as I know . I feel like maybe this should have been the main focus of the paper . # # # Weaknesses # # # 1 . Limited novelty - the encoder , as far as I understand , is identical to the edit encoder of Graph2Tree ( Yin et al.2019 ) .The decoder ( `` editor '' ) is better , empirically and conceptually , than the decoder of Graph2Tree , but its main novelty is the prediction of the edit action itself , rather than generating the entire output tree . To me , this idea is not novel , as it was used in Tarlow et al . ( 2019 ) , Dinella et al . ( 2020 ) , and Brody et al . ( 2020 ) .2.A conceptual comparison with previous work is missing . First , the work of Tarlow et al . ( 2019 ) is not cited at all ( although their application is different , the approach is very similar ) . Second , a comparison to Hoppity ( Dinella et al. , 2020 ) and to C3PO ( Brody et al. , 2020 ) is presented in a single paragraph and contains the following arguments : ( a ) \u201c While some recent works have examined models that make changes to trees for specific applications such as program bug fixing ( Dinella et al. , 2020 ) or edit completion ( Brody et al. , 2020 ) , our method is designed to be generic and flexible in nature \u201d -- I do n't think that this is a fair argument . These previous works are as general as this paper , they were just demonstrated on slightly different datasets . ( b ) \u201c it supports general tree edits including adding new tree nodes or copying a subtree , which are not fully allowed by previous work \u201d -- Ca n't Hoppity add new tree nodes ? Can you clarify the classes of edits that previous works could not express and that this paper can express ? ( c ) \u201c all tree edit operations are language-agnostic owing to the adoption of Abstract Syntax Description Language ( ASDL ; Wang et al . ( 1997 ) ) , which allows us to process arbitrary tree-based languages \u201d -- I do n't think that this is a fair argument . These previous works are language-agnostic as well , they just use a different AST `` format '' . ( d ) \u201c unlike the short edit sequences handled in previous work ( e.g.up to three edits in Dinella et al . ( 2020 ) ) , we demonstrate our method \u2019 s applicability to much longer edit sequences \u201d -- The fact that Hoppity was evaluated with 3 edit actions does not mean that it is not applicable for longer action sequences . This argument would have been valid if it was demonstrated empirically that Hoppity 's accuracy decreases as the length of the sequence increases . In that case , the imitation learning part might be a very natural fix ( which was good ! but not shown ) . 3.Evaluation - it is unclear which datasets should we _really_ care about , and which are the main results . It seems that the proposed Graph2Edit model outperforms the Graph2Tree model ( Yin et al.2019 ) only in the `` gold '' datasets ( GHE-gold and Fixers-gold ) , which expose ( indirectly ) the labels to the input . That is , these serve as `` intrinsic '' tasks that can not really be compared across models . As far as I understand , Yin et al . ( 2019 ) argued that the `` Fixers-one shot '' is the dataset that really matters , and that GHE-gold and Fixers-gold are just `` intermediary '' / '' intrinsic '' training objectives . In the `` intermediary '' datasets , Graph2Edit outperforms Graph2Tree , but it is not compared to `` Seq2seq encoder+editor '' which performed best in these datasets in Yin et al. , 2019 ( Table 4 in Yin et al. , 2019 ) . In the one-shot dataset ( Fixers one-shot ) - Graph2Tree performs better than the proposed Graph2Edit model . So , I am not sure what are the main results that the readers should focus on and what is the correct baseline . See question 1 below . # # # Questions for Authors # # # 1 . Is the accuracy on the `` gold '' datasets ( GHE-gold and Fixers-gold ) really meaningful ? Is n't this accuracy just an intermediary accuracy ? As far as I understand , when trained on these gold datasets : $ f_ { \\Delta } $ depend on $ C_ { + } $ , and then $ f_ { \\Delta } $ is * used * in the prediction of $ C_ { + } $ . I saw the footnote that says that $ f_ { \\Delta } $ does not * directly * expose $ C_ { + } $ . So it means that it * indirectly * expose $ C_ { + } $ , right ? Section 3.3 explicitly says that `` given an input tree $ C_ { - } $ and an edit representation $ f_ { \\Delta } $ ( calculated either from $ < C_ { - } , C_ { + } > $ or another edit pair $ < C ' _ { - } , C ' _ { + } > $ ) , we generate one tree edit at a time step ... '' . So , since in the gold dataset $ < C_ { - } , C_ { + } > $ = $ < C ' _ { - } , C ' _ { + } > $ , the authors model the `` actions leading to $ C_ { + } $ '' given `` an encoding of $ C_ { + } $ '' ? I.e. , are the output labels ( indirectly ) encoded in the input ? This is fine if we consider the gold datasets as intermediary/intrinsic objectives , and consider Fixers one-shot as the `` important '' , downstream task . If so , Graph2Tree ( Yin et al. , 2019 ) performs best on the Fixers one-shot dataset ( which is the `` important '' dataset ) . If not , and the gold datasets are meaningful on their own , then why there is no comparison to the seq2seq editor+decoder that Yin et al. , 2019 found to perform best on the gold datasets ? 2.The paper states that the graph edit encoder of Yin et al . ( 2019 ) does `` not explicitly express the differences between the input and the output trees '' . As it looks in Yin 2019 ( Section 3.2 ) , it seems that they represent the difference between the trees pretty explicitly , using edges such as `` Removed '' , `` Added '' and `` Replaced '' between the old and the new tree . So , what is the main novelty compared to Yin et al . ( 2019 ) in this area ? 3.The main novelty in this paper compared to previous work , in my opinion , is the imitation learning training ( Section 4 ) . I wish the authors elaborated more on this , give examples , show how the parameter values affect the performance , etc . 4.How long are the edit sequences , average , in the Fixers dataset ? ( in comparison to Hoppity 's 3 edits per sequence ) # # # Improving the Paper # # # The paper could be improved in the following ways : 1 . A conceptual discussion of the differences from previous work ( Tarlow , Dinella , Brody ) . 2.Is it possible to perform an empirical comparison with Hoppity ? 3.Other strong baselines would be Transformer+copy and bidirectional LSTM seq2seq+attention+copy mechanism , that copy individual tokens rather than spans as in CopySpan ( Panthaplackel et al. , 2020a ) . Even Section 5.2 of the paper says that the CopySpan baseline , which can copy large spans , memorizes too much . Another straightforward baseline is a sequential * * tagger * * , that tags each individual token in $ C_ { - } $ with tags like KEEP/SWAP/DELETE , as in Malmi et al . ( 2019 ) , and as Brody et al . ( 2020 ) adapted to code as their baseline . 4.Adding more interesting examples in which the model succeeds ( and fails ) . The paper claims `` applicability to much longer edit sequences '' ( Section 1 ) , but the example in Figure 1 converts ` list.ElementAt ( i+1 ) ` to ` list [ i+1 ] ` ( which might be syntactically-long , but not that `` difficult '' ) and the example in Table 2 only converts a ` VAR2.ToString ( ) ` to ` VAR2 ` ( which is also not very `` difficult '' or `` interesting '' , because a simple sequence-tagger that is trained to delete tokens can produce such examples ) . Ideally , the appendix could contain many additional examples like Table 2 ( but also with longer edit sequences ) . # # # Minor questions and comments # # # 5 . The paper 's title contains the phrase `` * Incremental * Tree Transformations '' . What is `` incremental '' about it ? Does n't the model mainly model a single `` action '' or `` transformation '' at a time ? Does n't the `` incrementality '' come from an LSTM `` decoder '' that sequentially predicts these actions ? 6.In Section 3.2 - Why is $ a_ { Stop } $ computed as $ W_ { Stop } emb ( Stop ) +b_ { Stop } $ , instead of just learning a single embedding vector , if all its components are trainable and used only in $ a_ { Stop } $ ? 7.Typo in Section 4 , Algorithm 1 and Algorithm 2 : `` Samping '' - > `` Sampling ''", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank Reviewer 1 for recognizing our work as presenting \u201c strong empirical results and a combination of interesting ideas \u201d ! We also feel excited that Reviewer 1 finds our imitation learning idea interesting and novel ! * * The overall novelty of our work * * We would like to emphasize that our work has made technical contributions from several angles . Please refer to * * \u201c To All Reviewers \u201d * * for a detailed discussion , including a conceptual comparison with previous work . * * The novelty of our proposed edit encoder compared with Yin et al . ( 2019 ) * * We clarify that our proposed edit encoder is a brand new one and has a _completely different model architecture_ compared with existing tree edit encoders , including the graph edit encoder proposed by Yin et al . ( 2019 ) .Specifically , as the reviewer mentioned , Yin et al . ( 2019 ) calculated the edit representation by connecting the old and the new tree via \u201c Removed/Added/Replaced \u201d -labeled edges and then running a graph neural network over the two trees jointly . From a \u201c model learning \u201d perspective , we argue this way of expressing tree differences is still implicit , as the edit encoder has simply treated the labeled edges as \u201c yet another feature \u201d in addition to the nodes/edges separately provided by the two trees . Our proposed edit encoder instead has completely shifted the modeling focus to _the targeted edit actions themselves_ ; it directly ( 1 ) learns a vector representation for each edit action in the targeted tree edit sequence and ( 2 ) calculates the edit representation as an association of the learned targeted edit action representations . The difference could also be told from our empirical results . Comparing \u201c Graph2Tree + TreeDiff Edit \u201d in our Tab . 1 with \u201c Graph2Tree + Graph Edit Encoder \u201d in Tab . 4 of Yin et al . ( 2019 ) , it is obvious that our proposed TreeDiff Edit Encoder gives much better editing accuracy ( 68.09 % vs. 48.05 % on GHE-gold ; Yin et al.did not provide results of Graph Edit Encoder on Fixers settings for comparison , although we believe our edit encoder will still surpass it ) . We will revise our writing to make the comparison and the description more precise . * * Evaluation : which datasets should we really care about and which are the main results * * We clarify that * * all datasets have their values and one should interpret a model from its empirical results on all settings * * . _First_ , admittedly Fixers-one shot is a more realistic benchmark ; however , we \u2019 d like to note that GHE/Fixers-gold has its own merits , as it is a more _controllable_ setting towards answering the question of \u201c how well an editor could performance given a _gold_ edit representation f_\\delta ( C- , C+ ) \u201d ( Yin et al. , 2019 ) . Evaluating on GHE-gold also facilitates direct comparison with prior work in this line . _Second_ , we note that the models are simply _tested_ on Fixers-one shot ; all models need to be first _pretrained_ on GHE-gold . Arguably a more reasonable experiment should have both _trained_ and _tested_ a model on the Fixers-one shot setting . However , we are unable to do this because the dataset is too small ( with only 2,878 examples ) . _Finally_ , Fixers only covers a restricted set of 16 code edit categories . Although those edits are the most common ones ( since they are supported by existing commercial fixers ) , they certainly could not cover all the real-world diverse patterns of code edits on the GitHub commit stream . In this sense , the GHE benchmark is still important as a dataset with _much higher coverage_ of realistic edit patterns . We believe collecting a larger Fixers dataset for end-to-end evaluation of neural editors would be an important future avenue . Based on the above discussion , we clarify the * * empirical comparison of Graph2Edit vs. Graph2Tree * * . We note three crucial observations : ( 1 ) Graph2Edit _substantially outperforms_ Graph2Tree on Fixers-one shot when they are paired with Seq Edit Encoder . This implies that our proposed Graph2Edit editor can drive the edit encoder to learn more generalizable edit semantics . ( 2 ) When the two editors are paired with TreeDiff Edit Encoder , their performance on Fixers-one shot is basically _comparable_ . It is true that Graph2Tree is slightly better . However , as we discussed , the models are actually trained on the GHE-gold training set . Under this restricted setting , Graph2Edit may have overfitted to the specific edit representation when TreeDiff Edit Encoder directly encodes the target edit sequence in training . We hypothesize that when all models are also trained on the realistic one-shot setting , our Graph2Edit model will outperform Graph2Tree . ( 3 ) Note that on the \u201c gold \u201d edit representation settings ( especially Fixers-gold ) , Graph2Edit+TreeDiff clearly outperforms others . ... ( see our follow-up comment )"}, {"review_id": "v9hAX77--cZ-3", "review_text": "# # Summary This paper presents an approach to learn a model over incremental structural tree edits , that takes as input a partially formed tree and applies a sequence of transformations that edit the tree into a final form . They focus on abstract syntax trees as used to represent expressions in programming languages , where the grammar is specified using the ASDL formalism . They demonstrate their approach on two datasets : the GitHubEdits dataset and C # Fixers , showing improved performance relative to some baselines . # # Overview The motivation for this work is weakly specified . Analogies are made to the fact that humans seem to edit objects in sequence , but not much more than that is provided . The authors would do well to specify , or at least hypothesize as to what the benefits of sequences of edits over the alternative . Writing-wise , it is hard to tell what this paper actually contributes from the abstract or introduction . Now , I would characterize it as presenting a template in which neural networks can be composed such that graph transformation sequences can be learned from data . The claimed advantages over existing approaches are : - Supports general tree edits , whereas previous approaches are somehow more restricted - Language agnostic due to use of ASDL - Supports longer sequences of edits in practice A major problem is that these advantages are only weakly demonstrated , if at all : - What is a general tree edit ? One answer could be that any graph transformation can be encoded as an action in theri framework , or is it that it can be achieved through a sequence of steps ? The paper focuses on a very particular set of tree edits , so it is very unclear what is being claimed here . - Longer edit sequences . No evidence is provided to support this claim . - Language agnosticism through ASDL is a valid claim , and useful . It 's not clear that generalizing any of the existing work to work with a general grammar would require much work though . Moreover , the work closest to this work ( Dinella et al ) is not compared against , neither experimentally , nor are the differences in the approaches detailed . Why ? Overall while I think a useful software package could be made based on this work ( which the authors suggest is forthcoming and which itself could be presented as a publication of a much different kind ) , the delta over existing work has not been demonstrated enough for me to recommend publication . # # Questions - In the LSTM encoding of an edit sequence , you have these four transformations to produce a_stop , a_Delete , a_Add , and a_CopySubTree . What is the advantage of doing these transformations prior to having the LSTM read the result ? - I can not see where the embedding function $ \\text { emb } $ is defined . Is this learned , or fixed ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank Reviewer 4 for the detailed comments ! * * Motivation for this work ; the benefits of sequences of edits over the alternative * * We appreciate the reviewer for recognizing the analogy between the modeling of sequential editing and how humans repetitively revise their writings in real life . Other than that , we have also identified many applications that would benefit from the modeling of sequential editing . For sequential data such as sentences , Malmi et al . ( 2019 ) and Dong et al . ( 2019 ) have demonstrated that training models to produce a sequence of edits ( rather than the edited sentence ) leads to better performance on tasks such as sentence simplification and rephrasing . In addition , Welleck et al . ( 2019 ) and Gu et al . ( 2019a , b ) have shown that modeling sequential editing enables text generation in arbitrary orders , so that a generative model can explore the most effective generation order on its own . In this paper , our focus is modeling sequential edits on _tree-structured data_ . Similarly as sequential data , there are also applications where directly editing a tree-structured data is needed , e.g. , editing the AST of computer programs given an edit specification ( e.g. , a code refactoring rule ) . Through experiments on two program editing datasets , we have shown that training a model to generate sequences of edits is advantageous than training it to generate the edited tree ; it allows the model to learn more generalizable edit semantics than the alternative . * * Novelty/contribution of our work * * For a detailed discussion about the novelty/contribution of our work , please refer to * * \u201c To All Reviewers \u201d * * . In particular , while we define only four types of operator in this paper , the operators have been able to fulfill a broad range of editing requirements . Note that the operators are not fully allowed by other approaches ( e.g. , Brody et al . ( 2019 ) does not allow adding a new tree node ) . * * Other questions * * 1 . \u201c What is the advantage of doing these transformations prior to having the LSTM read the result ? \u201d : Since each type of operator involves different numbers of variables to characterize its information , we perform a transformation to project each of them into the same dimension so that they can be fed into one LSTM encoder . 2.We \u2019 re sorry for the oversight in not defining the \u201c emb ( ) \u201d function -- to clarify : all embeddings are learnable in our model ."}], "0": {"review_id": "v9hAX77--cZ-0", "review_text": "The paper presents a neural autoregressive model that learns to incrementally or iteratively perform edit actions on structured data . The authors focus specifically on abstract syntax tree representation of programs ( e.g. , C # ) . The model has two main parts : ( 1 ) Neural editor models p ( a| ) that iteratively performs tree edit actions such as sub-tree deleting or adding ; ( 2 ) Edit encoder learns edit representations f_Delta by encoding the sequence of ground truth tree edit actions . The authors also propose an imitation learning algorithm to train the editor and evaluate the model on source code edit datasets . The idea of incremental tree transformation pretty much follows that of Hoppity by Dinella et al , ICLR 2020 for bug fixing of Javascript programs . Unlike Hoppity where each transformation step is done on a single node , this work extends to support sub-tree operations . However , adding a subtree is nothing but performing a sequence of single-node actions . The difference is that to ensure the syntactic validity of the tree at any point , the authors use a grammar specified a priori . This mechanism was proposed in semantic parsing by Yin et al. , ACL 2017 . Compared to Hoppity , this model has another SubTreeCopy operation , but it is a somewhat straightforward extension of the copy mechanism from Yin et al.Although the whole idea is not new , I think the paper presents a valued extension to existing work . The running example is intuitive , making the paper easy to follow . However , there are a number of parts that are unclear and need more clarity . I hope the authors address these during the rebuttal phase . 1.The authors should be clear ( e.g. , before Equation 1 and in Section 3 ) about where the sequence actions { a_t } comes from and how sub-tree actions are represented ( e.g. , decomposed into single-node actions ) . Later in this experiment , the authors mention dynamic programming to compute the shortest tree edit sequence , but I feel that substantial discussion or an algorithmic description is needed . 2.It is not clear to me whether f_Delta is learned jointly or separately from the parameters of p ( a| ) . In Figure 1 and Equation 1 , f_Delta does not depend on t and seems to be fixed . 3.In the last paragraph of page 3 , this sentence \u201c the operator selects a dummy node ( e.g.node Dummy ) and replaces it with the added node \u201d is not clear . Is there always at least one dummy node at any time ? What is \u201c the added node \u201d ? 4.Another question about Add operation : if an Add operation is given and there is currently no dummy node , would it make more sense to specify the added location with respect to a parent node and its children ? 5.How does the model know when to stop adding a right sibling for constructor fields with * sequential * cardinality ? Similarly , for an * optional * cardinality field , what does the model do on the attached dummy node if the field is indeed optional ? 6.It may be more natural to predict the node location n_t before the operation op_t . Equation 2 does the other way . Does this change the model in any way ? 7.To what extent this framework is language-agnostic ? Despite the ASDL , even for the same language , different parsers can have different grammar specifications , so how easy is it to apply this framework for other languages ? On the same note , since Hoppity is directly related , I am curious how this work compares to it on a same bug fixing task . Finally about the experiments , the authors compare to Graph2Tree and CopySpan . But it does not look like the source code for the baselines are available . Releasing the source code with that of the baselines would be helpful . Minor typos \u201c Arbitary \u201d : Section 3.1.1 paragraph 2 . = After discussion = I have increased my score from 6 to 7 .", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 2 for recognizing the value of our work and giving the detailed comments ! * * Answers to reviewer \u2019 s questions * * 1 . Thanks for your advice ! We will revise our draft to make clear the actions \u2019 definitions and where they come from . To clarify , the CopySubTree operator copies a _complete_ subtree from the initial input tree ( g_1 ) to the current tree ( g_t ) in one single step , rather than decomposing it into multiple steps of adding tree nodes . An example is shown in Fig 1 ( b ) at t=4 , where the whole subtree \u201c Expr \u2192 i + 1 \u201d is copied from the input tree g_1 in Fig 1 ( a ) . The dynamic programming algorithm mentioned in Sec 3.3 is used only for calculating gold-standard edit sequences in our training data . We will add a pseudo-code of the algorithm in the revised version . 2.As we mentioned at the beginning of Sec 3.3 , the edit encoder ( which produces the edit representation f_delta ) is jointly trained with the editor . It is correct that f_delta does not depend on the time step t , but it is a non-fixed , learnable vector representation . 3.A dummy node denotes a vacant position that is syntactically valid to accept a tree node ( see node \u201c Dummy \u201d in Fig 1 ( b ) at t=1 for example ) . It is automatically added following the underlying grammar to ensure no missing child for each tree node . From another perspective , this also means that it is possible to _not_ have dummy nodes , e.g. , when all tree nodes \u2019 syntactically valid child nodes have been fulfilled . * * Note that this will never happen if any tree node has a _sequential_ field , because we always append one dummy node to a sequential field and a sequential field can have a new child inserted to every possible child position in any time . * * For our editor , * * in the case of single/optional fields , * * an Add operator has to be applied to a dummy node . This is intuitive since the Add operation means to add a tree node to a certain syntactically valid position in the current tree , and all such positions * * for single/optional fields * * have been held by dummy nodes . The \u201c added node \u201d in the sentence refers to a _non-terminal_ node such as node \u201c ElementAccess \u201d in Fig 1 ( b ) at t=2 . Predicting a non-terminal to-be-added node is equivalent to selecting a production rule ( e.g. , AssignStmt \u2192 ElementAccess ) for the node \u2019 s parent ( e.g. , AssignStmt ) . Note that the Add operator can also be used to populate an empty _terminal_ node with a literal value ( e.g. , token \u201c list \u201d at t=3 ) . In this case , deciding the terminal node to be added is equivalent to predicting a token in place . 4.Please refer to our answer \u201c 3. \u201d for an explanation about the dummy node mechanism . To this question : since all available positions that are syntactically allowed to accept tree nodes * * for single/optional fields * * have been replaced by dummy nodes in our mechanism , having no dummy node means having no valid position for adding nodes * * to single/optional fields * * in the current tree . In this case , an Add operator is illegitimate * * for such fields * * and will be eliminated from the operator candidates . * * For nodes with sequential fields , there will always be a dummy node appended to it ( e.g. , [ A , B , Dummy ] ) . To add a new node ( e.g. , node C ) to a certain position of a sequential field , our model first selects the \u201c right sibling node of the target position \u201d , and then inserts the new node to its left . For example , adding C before A is done by selecting A and inserting C before it ; adding C as the actual rightmost child is done by selecting Dummy and inserting C before it . * * 5.Due to space constraints , we discuss details about ASDL implementation in Appendix A.1 . For fields with sequential cardinality , there is always one extra dummy node attached as its rightmost child . For example , the child list [ A , B ] is extended to [ A , B , Dummy ] . Therefore , our editor always has the option to add a right sibling to the child list of a sequential-cardinality field . This is implemented by selecting Dummy and replacing it with the to-be-added tree node , which will extend the child list into [ A , B , C , Dummy ] ( with C being the added node ) . * * Note that this is the same as first selecting Dummy and then inserting node C to its left , which we described in the edited answer \u201c 4 \u201d . * * \u201c When to stop adding a right sibling \u201d is left as a learning problem to the neural editor itself . For optional-cardinality fields , a dummy node is attached when their child position is not taken . To add a child node to the field , our editor similarly replaces the dummy node with the node to be added . When the field has already gotten a non-Dummy child , no more dummy child node is attached to ensure its grammatical correctness . 6.We decide the operator prior to the node location simply because for the operator Stop , selecting whatever node location is meaningless . Following our current design , in inference time , once the editor chooses the Stop operator , it has no need to pick the node location , which is more natural than the other way . ..."}, "1": {"review_id": "v9hAX77--cZ-1", "review_text": "# # Summary The paper proposes a general model for incremental editing of tree-structured data such as abstract syntax trees . The editing operations include adding a node , deleting a subtree , or copying a subtree . They also propose a novel edit encoder to learn to represent edits , and an imitation learning method to make the model more robust . # # Pros - The work has several interesting and valuable contributions : + Compared to previous work , the model is much more general : it supports general tree edits , is language agnostic , and can handle much longer edit sequences . + The novel edit encoder which directly encodes the edit actions is more intuitively correct and also performs better than previous approaches . + Imitation learning to make the model more robust is a natural idea that suits incremental edits very well . - The source code will be released which -- given the general nature of the model -- could enable further interesting research . # # Concerns - The explanation for the results in Table 1 could be improved upon . + I do n't think the results support the claim that Seq Edit Encoder memorizes specific patterns with the baselines as the micro average of Graph2Tree for Fixers-one shot is very close to Graph2Edit . + Even if we accept the claim , that would not explain the CopySpan > Graph2Tree > Graph2Edit results on GHE-gold and Fixers-gold . The authors state that Seq Edit memorizes specific patterns , TreeDiff Edit learns more generalizable information , and Graph2Edit makes Seq Edit learn more generalizable information . But they state that in order to solve GHE-gold , we need specific patterns instead of generalization . So why does increasing generalization by switching Seq Edit to TreeDiff Edit improve performance of Graph2Edit on GHE-gold and Fixers-gold ( which does n't need generalization ) , and decrease performance on Fixers-one shot ( which needs generalization ) ? - Some of the contributions are not demonstrated clearly : + The method 's applicability to much longer edit sequences is not clearly demonstrated , although the length of the edit sequences is mentioned briefly in the Appendix . + The imitation learning method is demonstrated only on `` Graph2Edit with Seq Edit Encoder '' , which is the less interesting case compared to `` Graph2Edit with TreeDiff Edit Encoder '' . From the previous experiment it 's clear that the TreeDiff encoder version is the practically relevant one and the one the paper 's about . As the Seq Edit encoder makes more mistakes and so it 's easier to improve , we do n't know whether imitation learning helps in the relevant case of the TreeDiff encoder . In my opinion this makes imitation learning more of a digression and a less of an organic part of the paper . - The writing could be more precise at times . For example , the authors state that they are adding subtrees as an edit operation , but as far as I understand , they are adding individual nodes . # # Reasons for ranking I believe that the model is an important step in learning to represent edits . However there are some problems with the experiments : some of the claims are not adequately supported and the explanations could be improved upon . # # Minor comments - I found Figure 1 confusing at first , because there is essentially no caption and the description of the figure comes much later in parts . It would be good to either have a more substantial caption or to move the figure closer to the explanations . - Table 2 precedes Table 1 , which is confusing - It should be `` general '' , not `` generic '' , like `` general model for incremental editing '' - Page 3 : Delete operators take a tree node ... and remove - The algorithms in the Appendix should be DaggerSampling and PostRefineSampling ( missing `` l '' ) .", "rating": "7: Good paper, accept", "reply_text": "We thank Reviewer 3 for recognizing our contributions as \u201c interesting and valuable \u201d ! * * Explanations for Table 1 * * 1 . Clarification of \u201c Seq Edit Encoder memorizes specific patterns ( about C+ ) with the baselines \u201d : We make this conjecture when observing the two baselines , CopySpan and Graph2Tree , show superior performance on GHE/Fixers-gold but worse performance on Fixers-one shot . Intuitively , this is because the target output for the two models , the ground-truth edited code C+ , has been exposed to the Seq Edit Encoder . However , due to their different architectures , the two models still show different behavior . For Graph2Tree , since it generates C+ _in the form of an AST tree_ , it suffers less from the \u201c memorization \u201d or overfitting of the Seq Edit Encoder ; this explains its better accuracy on Fixers-one shot . In contrast , since CopySpan generates C+ _in the form of a token sequence , which is exactly the same as how C+ is encoded by the Seq Edit Encoder_ , it suffers the most from the \u201c memorization \u201d problem and can not generalize well to the Fixers-one shot setting . 2.Clarification of the performance of \u201c Graph2Edit + TreeDiff Edit Encoder \u201d : We first clarify that the TreeDiff Edit Encoder has the advantages of being both _expressive_ and _generalizable_ . The \u201c expressive \u201d explains the improvement of Graph2Tree and Graph2Edit when they are paired with TreeDiff Edit . Second , as we discussed in the end of Sec 5.2 , the decreased performance of Graph2Edit on Fixers-one shot is likely because it has overfitted to the specific edit representations during training . Note that all models ( including Graph2Edit ) are trained on the GHE-gold training set and are only tested on Fixers-one shot . This overfitting is caused by a similar \u201c memorization \u201d issue that CopySpan has , when the target output of the model ( ground-truth tree edits for Graph2Edit and ground-truth code tokens for CopySpan ) has been exposed to the edit encoder ( TreeDiff Edit for Graph2Edit and Seq Edit for CopySpan ) in exactly the same format . * * Response regarding some of the contributions not being demonstrated clearly * * 1 . About \u201c longer edit sequences \u201d : please see * * \u201c To All Reviewers \u201d * * for clarification , as well as other discussions about the overall contribution/novelty of our work . 2.About the imitation learning experimental setting : We agree with the reviewer that experimenting with `` Graph2Edit with TreeDiff Edit Encoder '' would definitely be more interesting and more convincing in supporting our claim . We apologize for not being able to explain this in our initial submission . As suggested , we experimented with the setting using \u201c Graph2Edit with Treediff Edit Encoder , \u201d and found that , interestingly , it seems that the model does sufficiently well on the GHE dataset already without imitation learning that further gains through imitation learning were hard to obtain . ( In fact , 80 % of the remaining errors were due to issues such as unknown tokens , which can not be fixed with better training algorithms , as they are outside of the search space of our current model . ) Note that we do _not_ believe that this is necessarily a result discounting the utility of imitation learning in general , but rather just a result of the GHE-gold setting being relatively easy ( compared to , for example Fixers-one shot ) . However , we do not immediately have training data to use for imitation learning in harder settings ( Fixers is test data only ) , so as a proxy for testing on a harder dataset we believe the experiments with the weaker \u201c Graph2Edit with Seq Edit Encoder \u201d model provide a good proxy demonstrating imitation learning \u2019 s potential . Experimenting with imitation learning on harder datasets is definitely very high on our list of things to do -- we were not able to do it in the short time span for author response , but if you think it would contribute significantly to the paper we can try to do it for the final version . * * Regarding preciseness of writing * * Thank you for your advice ! We will revise Sec 3.1.1 to describe the tree edit actions more precisely . To clarify , the CopySubTree operator copies the whole subtree from g_1 ( the initial tree ) to g_t ( the current tree at time step t ) in one single step , rather than \u201c adding individual nodes \u201d in multiple steps . An example is shown in Fig 1 ( b ) at t=4 , where the whole subtree \u201c Expr \u2192 i + 1 \u201d is copied from the input tree g_1 in Fig 1 ( a ) . In our model architecture , the tree edit decoder decides which subtree to copy from g_1 , by using the learned representation of the root node of this subtree ( i.e. , the node representation of \u201c Expr \u201d in the running example ) as a feature . Due to space constraints , we have only briefly mentioned this in the last paragraph of Sec 3.1.3 , but more details can be found in Appendix A.2 . We also thank you for the minor comments ! We will address them in the revised draft ."}, "2": {"review_id": "v9hAX77--cZ-2", "review_text": "# # # Summary # # # The paper presents an approach for predicting edits in programs , by modeling the programs as trees . The approach is mainly an extension of Yin et al . ( 2019 ) , with the main difference that the model is required to predict only the output * * actions * * , instead of generating the entire output tree as in Yin et al . ( 2019 ) .This difference of predicting only output actions is shared with other previous work though . The most interesting part in my opinion is the `` imitation learning '' improvement : during training , the model is trained to correct its own mistakes by `` imitating '' an expert that fixes the incorrect predictions . Overall , I vote for acceptance . Although the technical contribution is limited , the paper presents strong empirical results and a combination of interesting ideas . I think that the paper could be easily further improved , as detailed below . # # # Strengths # # # 1 . The paper presents improved results over the Graph2Tree model ( Yin et al.2019 ) and over CopySpan ( Panthaplackel et al. , 2020a ) . 2.The imitation learning part is very interesting , and its applicability for programs is novel as far as I know . I feel like maybe this should have been the main focus of the paper . # # # Weaknesses # # # 1 . Limited novelty - the encoder , as far as I understand , is identical to the edit encoder of Graph2Tree ( Yin et al.2019 ) .The decoder ( `` editor '' ) is better , empirically and conceptually , than the decoder of Graph2Tree , but its main novelty is the prediction of the edit action itself , rather than generating the entire output tree . To me , this idea is not novel , as it was used in Tarlow et al . ( 2019 ) , Dinella et al . ( 2020 ) , and Brody et al . ( 2020 ) .2.A conceptual comparison with previous work is missing . First , the work of Tarlow et al . ( 2019 ) is not cited at all ( although their application is different , the approach is very similar ) . Second , a comparison to Hoppity ( Dinella et al. , 2020 ) and to C3PO ( Brody et al. , 2020 ) is presented in a single paragraph and contains the following arguments : ( a ) \u201c While some recent works have examined models that make changes to trees for specific applications such as program bug fixing ( Dinella et al. , 2020 ) or edit completion ( Brody et al. , 2020 ) , our method is designed to be generic and flexible in nature \u201d -- I do n't think that this is a fair argument . These previous works are as general as this paper , they were just demonstrated on slightly different datasets . ( b ) \u201c it supports general tree edits including adding new tree nodes or copying a subtree , which are not fully allowed by previous work \u201d -- Ca n't Hoppity add new tree nodes ? Can you clarify the classes of edits that previous works could not express and that this paper can express ? ( c ) \u201c all tree edit operations are language-agnostic owing to the adoption of Abstract Syntax Description Language ( ASDL ; Wang et al . ( 1997 ) ) , which allows us to process arbitrary tree-based languages \u201d -- I do n't think that this is a fair argument . These previous works are language-agnostic as well , they just use a different AST `` format '' . ( d ) \u201c unlike the short edit sequences handled in previous work ( e.g.up to three edits in Dinella et al . ( 2020 ) ) , we demonstrate our method \u2019 s applicability to much longer edit sequences \u201d -- The fact that Hoppity was evaluated with 3 edit actions does not mean that it is not applicable for longer action sequences . This argument would have been valid if it was demonstrated empirically that Hoppity 's accuracy decreases as the length of the sequence increases . In that case , the imitation learning part might be a very natural fix ( which was good ! but not shown ) . 3.Evaluation - it is unclear which datasets should we _really_ care about , and which are the main results . It seems that the proposed Graph2Edit model outperforms the Graph2Tree model ( Yin et al.2019 ) only in the `` gold '' datasets ( GHE-gold and Fixers-gold ) , which expose ( indirectly ) the labels to the input . That is , these serve as `` intrinsic '' tasks that can not really be compared across models . As far as I understand , Yin et al . ( 2019 ) argued that the `` Fixers-one shot '' is the dataset that really matters , and that GHE-gold and Fixers-gold are just `` intermediary '' / '' intrinsic '' training objectives . In the `` intermediary '' datasets , Graph2Edit outperforms Graph2Tree , but it is not compared to `` Seq2seq encoder+editor '' which performed best in these datasets in Yin et al. , 2019 ( Table 4 in Yin et al. , 2019 ) . In the one-shot dataset ( Fixers one-shot ) - Graph2Tree performs better than the proposed Graph2Edit model . So , I am not sure what are the main results that the readers should focus on and what is the correct baseline . See question 1 below . # # # Questions for Authors # # # 1 . Is the accuracy on the `` gold '' datasets ( GHE-gold and Fixers-gold ) really meaningful ? Is n't this accuracy just an intermediary accuracy ? As far as I understand , when trained on these gold datasets : $ f_ { \\Delta } $ depend on $ C_ { + } $ , and then $ f_ { \\Delta } $ is * used * in the prediction of $ C_ { + } $ . I saw the footnote that says that $ f_ { \\Delta } $ does not * directly * expose $ C_ { + } $ . So it means that it * indirectly * expose $ C_ { + } $ , right ? Section 3.3 explicitly says that `` given an input tree $ C_ { - } $ and an edit representation $ f_ { \\Delta } $ ( calculated either from $ < C_ { - } , C_ { + } > $ or another edit pair $ < C ' _ { - } , C ' _ { + } > $ ) , we generate one tree edit at a time step ... '' . So , since in the gold dataset $ < C_ { - } , C_ { + } > $ = $ < C ' _ { - } , C ' _ { + } > $ , the authors model the `` actions leading to $ C_ { + } $ '' given `` an encoding of $ C_ { + } $ '' ? I.e. , are the output labels ( indirectly ) encoded in the input ? This is fine if we consider the gold datasets as intermediary/intrinsic objectives , and consider Fixers one-shot as the `` important '' , downstream task . If so , Graph2Tree ( Yin et al. , 2019 ) performs best on the Fixers one-shot dataset ( which is the `` important '' dataset ) . If not , and the gold datasets are meaningful on their own , then why there is no comparison to the seq2seq editor+decoder that Yin et al. , 2019 found to perform best on the gold datasets ? 2.The paper states that the graph edit encoder of Yin et al . ( 2019 ) does `` not explicitly express the differences between the input and the output trees '' . As it looks in Yin 2019 ( Section 3.2 ) , it seems that they represent the difference between the trees pretty explicitly , using edges such as `` Removed '' , `` Added '' and `` Replaced '' between the old and the new tree . So , what is the main novelty compared to Yin et al . ( 2019 ) in this area ? 3.The main novelty in this paper compared to previous work , in my opinion , is the imitation learning training ( Section 4 ) . I wish the authors elaborated more on this , give examples , show how the parameter values affect the performance , etc . 4.How long are the edit sequences , average , in the Fixers dataset ? ( in comparison to Hoppity 's 3 edits per sequence ) # # # Improving the Paper # # # The paper could be improved in the following ways : 1 . A conceptual discussion of the differences from previous work ( Tarlow , Dinella , Brody ) . 2.Is it possible to perform an empirical comparison with Hoppity ? 3.Other strong baselines would be Transformer+copy and bidirectional LSTM seq2seq+attention+copy mechanism , that copy individual tokens rather than spans as in CopySpan ( Panthaplackel et al. , 2020a ) . Even Section 5.2 of the paper says that the CopySpan baseline , which can copy large spans , memorizes too much . Another straightforward baseline is a sequential * * tagger * * , that tags each individual token in $ C_ { - } $ with tags like KEEP/SWAP/DELETE , as in Malmi et al . ( 2019 ) , and as Brody et al . ( 2020 ) adapted to code as their baseline . 4.Adding more interesting examples in which the model succeeds ( and fails ) . The paper claims `` applicability to much longer edit sequences '' ( Section 1 ) , but the example in Figure 1 converts ` list.ElementAt ( i+1 ) ` to ` list [ i+1 ] ` ( which might be syntactically-long , but not that `` difficult '' ) and the example in Table 2 only converts a ` VAR2.ToString ( ) ` to ` VAR2 ` ( which is also not very `` difficult '' or `` interesting '' , because a simple sequence-tagger that is trained to delete tokens can produce such examples ) . Ideally , the appendix could contain many additional examples like Table 2 ( but also with longer edit sequences ) . # # # Minor questions and comments # # # 5 . The paper 's title contains the phrase `` * Incremental * Tree Transformations '' . What is `` incremental '' about it ? Does n't the model mainly model a single `` action '' or `` transformation '' at a time ? Does n't the `` incrementality '' come from an LSTM `` decoder '' that sequentially predicts these actions ? 6.In Section 3.2 - Why is $ a_ { Stop } $ computed as $ W_ { Stop } emb ( Stop ) +b_ { Stop } $ , instead of just learning a single embedding vector , if all its components are trainable and used only in $ a_ { Stop } $ ? 7.Typo in Section 4 , Algorithm 1 and Algorithm 2 : `` Samping '' - > `` Sampling ''", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank Reviewer 1 for recognizing our work as presenting \u201c strong empirical results and a combination of interesting ideas \u201d ! We also feel excited that Reviewer 1 finds our imitation learning idea interesting and novel ! * * The overall novelty of our work * * We would like to emphasize that our work has made technical contributions from several angles . Please refer to * * \u201c To All Reviewers \u201d * * for a detailed discussion , including a conceptual comparison with previous work . * * The novelty of our proposed edit encoder compared with Yin et al . ( 2019 ) * * We clarify that our proposed edit encoder is a brand new one and has a _completely different model architecture_ compared with existing tree edit encoders , including the graph edit encoder proposed by Yin et al . ( 2019 ) .Specifically , as the reviewer mentioned , Yin et al . ( 2019 ) calculated the edit representation by connecting the old and the new tree via \u201c Removed/Added/Replaced \u201d -labeled edges and then running a graph neural network over the two trees jointly . From a \u201c model learning \u201d perspective , we argue this way of expressing tree differences is still implicit , as the edit encoder has simply treated the labeled edges as \u201c yet another feature \u201d in addition to the nodes/edges separately provided by the two trees . Our proposed edit encoder instead has completely shifted the modeling focus to _the targeted edit actions themselves_ ; it directly ( 1 ) learns a vector representation for each edit action in the targeted tree edit sequence and ( 2 ) calculates the edit representation as an association of the learned targeted edit action representations . The difference could also be told from our empirical results . Comparing \u201c Graph2Tree + TreeDiff Edit \u201d in our Tab . 1 with \u201c Graph2Tree + Graph Edit Encoder \u201d in Tab . 4 of Yin et al . ( 2019 ) , it is obvious that our proposed TreeDiff Edit Encoder gives much better editing accuracy ( 68.09 % vs. 48.05 % on GHE-gold ; Yin et al.did not provide results of Graph Edit Encoder on Fixers settings for comparison , although we believe our edit encoder will still surpass it ) . We will revise our writing to make the comparison and the description more precise . * * Evaluation : which datasets should we really care about and which are the main results * * We clarify that * * all datasets have their values and one should interpret a model from its empirical results on all settings * * . _First_ , admittedly Fixers-one shot is a more realistic benchmark ; however , we \u2019 d like to note that GHE/Fixers-gold has its own merits , as it is a more _controllable_ setting towards answering the question of \u201c how well an editor could performance given a _gold_ edit representation f_\\delta ( C- , C+ ) \u201d ( Yin et al. , 2019 ) . Evaluating on GHE-gold also facilitates direct comparison with prior work in this line . _Second_ , we note that the models are simply _tested_ on Fixers-one shot ; all models need to be first _pretrained_ on GHE-gold . Arguably a more reasonable experiment should have both _trained_ and _tested_ a model on the Fixers-one shot setting . However , we are unable to do this because the dataset is too small ( with only 2,878 examples ) . _Finally_ , Fixers only covers a restricted set of 16 code edit categories . Although those edits are the most common ones ( since they are supported by existing commercial fixers ) , they certainly could not cover all the real-world diverse patterns of code edits on the GitHub commit stream . In this sense , the GHE benchmark is still important as a dataset with _much higher coverage_ of realistic edit patterns . We believe collecting a larger Fixers dataset for end-to-end evaluation of neural editors would be an important future avenue . Based on the above discussion , we clarify the * * empirical comparison of Graph2Edit vs. Graph2Tree * * . We note three crucial observations : ( 1 ) Graph2Edit _substantially outperforms_ Graph2Tree on Fixers-one shot when they are paired with Seq Edit Encoder . This implies that our proposed Graph2Edit editor can drive the edit encoder to learn more generalizable edit semantics . ( 2 ) When the two editors are paired with TreeDiff Edit Encoder , their performance on Fixers-one shot is basically _comparable_ . It is true that Graph2Tree is slightly better . However , as we discussed , the models are actually trained on the GHE-gold training set . Under this restricted setting , Graph2Edit may have overfitted to the specific edit representation when TreeDiff Edit Encoder directly encodes the target edit sequence in training . We hypothesize that when all models are also trained on the realistic one-shot setting , our Graph2Edit model will outperform Graph2Tree . ( 3 ) Note that on the \u201c gold \u201d edit representation settings ( especially Fixers-gold ) , Graph2Edit+TreeDiff clearly outperforms others . ... ( see our follow-up comment )"}, "3": {"review_id": "v9hAX77--cZ-3", "review_text": "# # Summary This paper presents an approach to learn a model over incremental structural tree edits , that takes as input a partially formed tree and applies a sequence of transformations that edit the tree into a final form . They focus on abstract syntax trees as used to represent expressions in programming languages , where the grammar is specified using the ASDL formalism . They demonstrate their approach on two datasets : the GitHubEdits dataset and C # Fixers , showing improved performance relative to some baselines . # # Overview The motivation for this work is weakly specified . Analogies are made to the fact that humans seem to edit objects in sequence , but not much more than that is provided . The authors would do well to specify , or at least hypothesize as to what the benefits of sequences of edits over the alternative . Writing-wise , it is hard to tell what this paper actually contributes from the abstract or introduction . Now , I would characterize it as presenting a template in which neural networks can be composed such that graph transformation sequences can be learned from data . The claimed advantages over existing approaches are : - Supports general tree edits , whereas previous approaches are somehow more restricted - Language agnostic due to use of ASDL - Supports longer sequences of edits in practice A major problem is that these advantages are only weakly demonstrated , if at all : - What is a general tree edit ? One answer could be that any graph transformation can be encoded as an action in theri framework , or is it that it can be achieved through a sequence of steps ? The paper focuses on a very particular set of tree edits , so it is very unclear what is being claimed here . - Longer edit sequences . No evidence is provided to support this claim . - Language agnosticism through ASDL is a valid claim , and useful . It 's not clear that generalizing any of the existing work to work with a general grammar would require much work though . Moreover , the work closest to this work ( Dinella et al ) is not compared against , neither experimentally , nor are the differences in the approaches detailed . Why ? Overall while I think a useful software package could be made based on this work ( which the authors suggest is forthcoming and which itself could be presented as a publication of a much different kind ) , the delta over existing work has not been demonstrated enough for me to recommend publication . # # Questions - In the LSTM encoding of an edit sequence , you have these four transformations to produce a_stop , a_Delete , a_Add , and a_CopySubTree . What is the advantage of doing these transformations prior to having the LSTM read the result ? - I can not see where the embedding function $ \\text { emb } $ is defined . Is this learned , or fixed ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank Reviewer 4 for the detailed comments ! * * Motivation for this work ; the benefits of sequences of edits over the alternative * * We appreciate the reviewer for recognizing the analogy between the modeling of sequential editing and how humans repetitively revise their writings in real life . Other than that , we have also identified many applications that would benefit from the modeling of sequential editing . For sequential data such as sentences , Malmi et al . ( 2019 ) and Dong et al . ( 2019 ) have demonstrated that training models to produce a sequence of edits ( rather than the edited sentence ) leads to better performance on tasks such as sentence simplification and rephrasing . In addition , Welleck et al . ( 2019 ) and Gu et al . ( 2019a , b ) have shown that modeling sequential editing enables text generation in arbitrary orders , so that a generative model can explore the most effective generation order on its own . In this paper , our focus is modeling sequential edits on _tree-structured data_ . Similarly as sequential data , there are also applications where directly editing a tree-structured data is needed , e.g. , editing the AST of computer programs given an edit specification ( e.g. , a code refactoring rule ) . Through experiments on two program editing datasets , we have shown that training a model to generate sequences of edits is advantageous than training it to generate the edited tree ; it allows the model to learn more generalizable edit semantics than the alternative . * * Novelty/contribution of our work * * For a detailed discussion about the novelty/contribution of our work , please refer to * * \u201c To All Reviewers \u201d * * . In particular , while we define only four types of operator in this paper , the operators have been able to fulfill a broad range of editing requirements . Note that the operators are not fully allowed by other approaches ( e.g. , Brody et al . ( 2019 ) does not allow adding a new tree node ) . * * Other questions * * 1 . \u201c What is the advantage of doing these transformations prior to having the LSTM read the result ? \u201d : Since each type of operator involves different numbers of variables to characterize its information , we perform a transformation to project each of them into the same dimension so that they can be fed into one LSTM encoder . 2.We \u2019 re sorry for the oversight in not defining the \u201c emb ( ) \u201d function -- to clarify : all embeddings are learnable in our model ."}}