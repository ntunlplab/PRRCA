{"year": "2020", "forum": "H1ezFREtwH", "title": "Composing Task-Agnostic Policies with Deep Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "This paper considers deep reinforcement learning skill transfer and composition, through an attention model that weighs the contributions of several base policies conditioned on the task and state, and uses this to output an action. The method is evaluated on several Mujoco tasks.\n\nThere were two main areas of concern. The first was around issues with using equivalent primitives and training times for comparison methods. The second was around the general motivation of the paper, and also the motivation for using a BiRNN. These issues were resolved in a comprehensive discussion, leaving this as an interesting paper that should be accepted.", "reviews": [{"review_id": "H1ezFREtwH-0", "review_text": "This paper presents an approach in which new tasks can be solved by an attention model that can weigh the contribution of different base policies conditioned on the current state of the environment and task-specific goals. The authors demonstrate their method on a selection of RL tasks, such as an ant maze navigation task and a more complicated \u201cant fall\u201d task, which requires the agent to first move a block to fill a gap in the environment before it is able to reach the goal. I found the paper interesting and well written. My primary concern is that the primitive policies are learned independently of the composite policies, which might limit the application of this approach to more complex problems. Additionally, it would be great to also see the concurrent and sequential form of skill combination for the more complex tasks, and not just the point navigation task shown in Figure 7. Standard errors on Figures 5 and 6 seem to be missing. Additionally, I was curious that in Figure 4a and Figure 6a, the composite\u2019s performance is already a lot better than the other methods after 0 training steps. Maybe the authors can elaborate on that. Maybe the performance at step 0 is just hard to make out in the graphs? I would suggest accepting the paper but there could be a more detailed analysis of how the pre-trained sub-modules are used and learning both composite and sub-policies together would make the paper stronger. Additional comments: - Where is the training graph for the Composite-SAV applied to the \u201cant fall\u201d task? Maybe I missed it? - Algorithm 1 should probably be moved to the main text. ####After rebuttal#### The authors' response and the revised paper address my already minor concerns. ", "rating": "6: Weak Accept", "reply_text": "RC : reviewer comment ; AR : author response RC : I found the paper interesting and well written . My primary concern is that the primitive policies are learned independently of the composite policies , which might limit the application of this approach to more complex problems . AR : We think it is one of the salient features of our method that it can take task-agnostic skills and can compose them for solving new transfer-learning problems . However , to address reviewer concern , we also include new skill acquisition experiments ( Appendix B ) in our paper . These experiments show that our method can acquire missing skills ( task-dependent ) and can compose them together with the existing task-agnostic skill set to solve the given problem in an end-to-end learning manner . RC : Additionally , it would be great to also see the concurrent and sequential form of skill combination for the more complex tasks , and not just the point navigation task shown in Figure 6 . AR : We will add the depiction of attention weights for the other complicated tasks in a couple of days . RC : Standard errors on Figures 4 and 5 seem to be missing . AR : We have updated the figures to include standard errors . Please refer to the revised paper . RC : Additionally , I was curious that in Figure 4a and Figure 6a , the composite \u2019 s performance is already a lot better than the other methods after 0 training steps . Maybe the authors can elaborate on that . Maybe the performance at step 0 is just hard to make out in the graphs ? AR : It is because other methods sometimes push the object away from the target position at early evaluation steps , leading to an increase in distance of the object from the given target than its initial distance . RC : I would suggest accepting the paper but there could be a more detailed analysis of how the pre-trained sub-modules are used and learning both composite and sub-policies together would make the paper stronger . AR : We will include the attention weights depiction soon for the other complicated tasks . Furthermore , we have added new results to show that it is possible to learn both composite and sub-level policies together in end-to-end learning using our framework . RC : Where is the training graph for the Composite-SAV applied to the \u201c ant fall \u201d task ? Maybe I missed it ? AR : We do not include as composite-SAC for ant-maze , ant-push , and ant-fall . These problems require complex task planning ( sub-goal generation ) , and therefore , like other non-hierarchical RL methods ( SAC , TRPO , PPO ) , the composition-SAC also fails to perform in these problems . RC : Algorithm 1 should probably be moved to the main text . AR : Due to limited space , it might not be possible to move the algorithm 1 to the main text ."}, {"review_id": "H1ezFREtwH-1", "review_text": "What is the specific question/problem tackled by the paper? This paper addresses the hierarchical RL problem of combining multiple primitive policies (pi_1, \u2026, pi_K) into policies for more complex tasks. Given a number of primitive skills and a new task within an environment, the paper aims to learn to pick and combine the primitives as needed to solve the new task. This problem statement is interesting and the method performs well on difficult tasks. However, I argue for rejecting this paper because it lacks meaningful contributions to the field. I do not see how the method presented in the paper is more than RL over hand engineered action spaces that are better for the tasks. While this improves results, we already know that for any task, there is some best action space for performing that task. This is why most HRL work aims to also find the primitive policies in additional to composing them. Is this any better than option-critic if the options are hardcoded to be the primitives? The experiment state that the option-critic method did not work, but did you give it access to the same primitives? Summary: The method presented in the paper is as follows: at each state s_t, the K primitives are queried for their action a_k ~ pi_k(s_t). Then, a biRNN reads in the actions in order from 1 to k. In parallel, the state and a goal are encoded by a network named \u201cDecoder\u201d. The encoded state and the hidden states of the RNN are used to output an attention weight over each primitive. Finally, the output action is the weighted combination of all the actions. The encoders and attention weights are trained with RL. This method is evaluated on several mujoco tasks, such as making a cheetah jump hurdles by combining \u201cforward\u201d and \u201cjump\u201d primitives. Each environment has predefined primitives such as \u201cforward\u201d \u201cleft\u201d \u201cright\u201d etc. This method is compared against HIRO, which does not have access to the primitive policies. It is not surprising that hand engineering primitives helps performance. Is the approach well motivated? The general idea behind the approach is well motivated: using primitive skills to learn complex skills is a useful goal. The details of the method are strange. I would like to see a better motivation and empirical justification for the biRNN. Why should the primitive\u2019s action be encode in order? The ordering of the primitives is arbitrary and constant: a fully connected network could be used, or the attentions could be output entirely independently per primitive. In fact, I do see not why the primitives\u2019 actions need to be encoded at all. It would be much simpler for the encoder to look at (s_t, g_t) and output a discrete probability over the K primitives. The ablations in 5.2 are for outputting actions directly rather than mixture weights. The paper would benefit from ablations where mixture weights are output but without the biRNN or without passing in the primitive\u2019s actions. Is the method well placed in the literature? The main idea of predicting weights over multiple experts is not novel (see \"Adaptive mixtures of local experts\u201d from 1991). In the context of RL literature, we can interpret the primitive skills as actions directly, and then the method is performing basic RL over a better action space (the better actions being \u201cgo left\u201d, \u201cjump\u201d etc. We can also interpret these as options, but unlike options a single primitive is not followed for multiple time steps with a termination condition. Functionally, this is equivalent to regular RL using domain knowledge to engineer the action space. ", "rating": "6: Weak Accept", "reply_text": "RC : Reviewer Comment ; AR : Authors Response RC : Is this any better than option-critic if the options are hardcoded to be the primitives ? The experiment state that the option-critic method did not work , but did you give it access to the same primitives ? AR : Option-critic method executes the options sequentially , and there is no concurrent composition . Furthermore , option-critic learns/needs tasks specific primitive , whereas our method can compose task-agnostic skills , both sequentially and concurrently , to solve new problems . RC : Why should the primitive \u2019 s action be encode in order ? AR : We use bidirectional RNN instead of uni-directional RNN to avoid ordering issues . It is also evident from the results that ordering is not a problem for our method . For instance , in cross-maze-ant , the composition has access to four primitive policies ( up , down , left , and right ) , and still , our method learns to not use downward policy ( irrespective of order ) at all as it is not needed to solve the given task . RC : The ablations in 5.2 are for outputting actions directly rather than mixture weights . The paper would benefit from ablations where mixture weights are output but without the biRNN or without passing in the primitive \u2019 s actions . AR : We have included the suggested ablation . Please refer to Fig 5 . Composition-without-BRNN takes the current state and outputs the mixture weights directly , which are then used to compose primitive actions . The composition-without-BRNN indeed perform better than other ablated models but still gives inferior performance than our proposed composition model . We argue that the composition model without BRNN performs poorly compared to the proposed method because it is entirely unaware of low-level continuous actions . Therefore , learning the latent encoding of primitive actions and making them a part of state-space is crucial for learning an effective composite policy ."}, {"review_id": "H1ezFREtwH-2", "review_text": "Overall, I think the method has some great merit but I am not overly confident in the reproducibility of the method. Some of the comparisons (HIRO) do not agree with the results in the HIRO paper. Also, more description is needed to describe how the baselines were used in the analysis. Were they also given the pre-trained sub-policies? A more fair comparison might be to give those baseline methods no sub-policies but let them pre-train for an equivalent amount of time as the sub-policies are trained. Here are some more detailed comments: - Figure 1 is not very clear and does not appear to add much to the explanation of the method. More detail should be included in the caption. - In the paper, it is noted that HRL has high sample complexity and needs lots of training data. I find the comment about how HRL requires many more training steps than regular RL very odd. The purpose of HRL is to have better sample efficiency and learn strong polices faster. Has the author observed different? The purpose of HRL is to reduce sample complexity and search in a well suited and structured way. - The assumption that the sub-policies solve the underlying MDP is rather strong. How are these policies going to be trained to guarantee this? - I like the idea of using an attention model to help pick learn a weighting for the combination of a number of sub-policies. I am not sure if using a bi-directional LSTM is the best or simplest method to accomplish this. The authors can look at \"MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\" NeurIPS 2019 for a recent work that is similar to theirs. - For Figure 4: Do these methods also get to use the sub-policies that have pre-trained some version of tasks? Also, how are these component policies trained? Over what tasks are they trained? This information is very important to make sure the method is not overly biased to the composition of them. - I find the results in Figure 5 very odd. The baseline shows that HIRO does not learn to perform well on these tasks even though these are the tasks from the HIRO paper that it learned to solve rather well. Can this contradiction be explained? - For the HIRO comparison was the system also using the composite policies there were pretrained? HIRO is designed to learn the sub-policies concurrently but it seems in this case the authors are using the outputs of the composition policies as input to the HIRO low policy. - I do not understand the visualization in Figure 7. How to the colored paths for the agent represent the weights for the compose policy?", "rating": "6: Weak Accept", "reply_text": "RC : reviewer comment ; AR : author response We provide the source code to ensure reproducibility with trained polices . Also , we were able to reproduce HIRO results presented in their paper . In their paper , they use low-torque-ant ( 30 Units ) that limits the action space . In contrast , in our work , we use standard-ant ( 150 units ) , so the action-space is large and makes the learning problem significantly harder as the Ant is now more prone to instability . For completeness , we now update for plots ( Fig.4 ) to include HIRO results with low-toque-ant ( conforming with the system settings proposed in their paper ) . RC : Also , more description is needed to describe how the baselines were used in the analysis . Were they also given the pre-trained sub-policies ? A more fair comparison might be to give those baseline methods no sub-policies but let them pre-train for an equivalent amount of time as the sub-policies are trained . AR : We have included more descriptions on the baselines \u2019 analysis in Appendix C. Furthermore , we now include HIRO baselines that were pretrained on low-torque-ant ( as in HIRO paper ) for the equal amount of time as the sub-policies for the composite model were trained . The new results are presented in Fig 4 . The pretraining of HIRO on standard-Ant does not lead to any improvement . Therefore , to avoid clutter , we do not include those plots in Fig 4 . Furthermore , we would like to highlight that , in the case of Ant , our composition model uses the same primitive skills across all presented environments , which in some cases is not equivalent to pretraining other benchmark models every time . RC : Figure 1 is not very clear and does not appear to add much to the explanation of the method . AR : We exclude figure 1 as other reviewers think it \u2019 s unnecessary . RC : In the paper , it is noted that HRL has high sample complexity and needs lots of training data . I find the comment about how HRL requires many more training steps than regular RL very odd . The purpose of HRL is to have better sample efficiency and learn strong polices faster . Has the author observed different ? The purpose of HRL is to reduce sample complexity and search in a well suited and structured way . AR : We agree with the reviewer . Our statement was misleading . HRL methods perform indeed better than regular RL , and we have removed that statement . As per the author \u2019 s understanding , the HRL , especially HIRO , is for complex tasks with weak reward signals that require both task and motion planning . And , due to weak reward signals and complex decision-making , these methods take a huge amount of interactive experience with the environment , which , of course , is still much less than regular RL methods . And in this aspect , our proposed composition framework further improves data-efficiency of HIRO by effectively exploiting the primitive skills . RC : The assumption that the sub-policies solve the underlying MDP is rather strong . How are these policies going to be trained to guarantee this ? AR : We assume standard RL sub-level-policies policies that generally solve underlying MDPs . RC : The authors can look at \u201c MCP : Learning Composable Hierarchical Control with Multiplicative Compositional Policies \u201d NeurIPS 2019 for a recent work that is similar to theirs . AR : Thanks for pointing us to MCP framework , we have included it in our related work section . RC : For Figure 3 : Do these methods also get to use the sub-policies that have pre-trained some version of tasks ? Also , how are these component policies trained ? Over what tasks are they trained ? AR : We have included the details of training sub-level policies in Appendix C. RC : I find the results in Figure 4 very odd . The baseline shows that HIRO does not learn to perform well on these tasks even though these are the tasks from the HIRO paper that it learned to solve rather well . Can this contradiction be explained ? AR : We were able to reproduce HIRO results using their system settings ( low-torque-ant ) , please refer to Fig 4 for results . RC : For the HIRO comparison was the system also using the composite policies there were pretrained ? HIRO is designed to learn the sub-policies concurrently but it seems in this case the authors are using the outputs of the composition policies as input to the HIRO low policy . AR : In our proposed work , HIRO concurrently learns the high-level policy and a composite policy ( given primitive skills ) . RC : I do not understand the visualization in Figure 6 ? AR : The \u201c color \u201d of paths has nothing to do with the attention weights . The agent starts from the center ( origin ) and moves towards one of the given goals . Since primitive polices were to move straight up , down , left , and right , the diagonal motion requires the concurrent composition of low-level skills , which is indicated by the attention weights . On attention maps , the vertical axis reads from top to down , where zero means the starting position of the agent , and it takes about 30 steps to reach the target ."}], "0": {"review_id": "H1ezFREtwH-0", "review_text": "This paper presents an approach in which new tasks can be solved by an attention model that can weigh the contribution of different base policies conditioned on the current state of the environment and task-specific goals. The authors demonstrate their method on a selection of RL tasks, such as an ant maze navigation task and a more complicated \u201cant fall\u201d task, which requires the agent to first move a block to fill a gap in the environment before it is able to reach the goal. I found the paper interesting and well written. My primary concern is that the primitive policies are learned independently of the composite policies, which might limit the application of this approach to more complex problems. Additionally, it would be great to also see the concurrent and sequential form of skill combination for the more complex tasks, and not just the point navigation task shown in Figure 7. Standard errors on Figures 5 and 6 seem to be missing. Additionally, I was curious that in Figure 4a and Figure 6a, the composite\u2019s performance is already a lot better than the other methods after 0 training steps. Maybe the authors can elaborate on that. Maybe the performance at step 0 is just hard to make out in the graphs? I would suggest accepting the paper but there could be a more detailed analysis of how the pre-trained sub-modules are used and learning both composite and sub-policies together would make the paper stronger. Additional comments: - Where is the training graph for the Composite-SAV applied to the \u201cant fall\u201d task? Maybe I missed it? - Algorithm 1 should probably be moved to the main text. ####After rebuttal#### The authors' response and the revised paper address my already minor concerns. ", "rating": "6: Weak Accept", "reply_text": "RC : reviewer comment ; AR : author response RC : I found the paper interesting and well written . My primary concern is that the primitive policies are learned independently of the composite policies , which might limit the application of this approach to more complex problems . AR : We think it is one of the salient features of our method that it can take task-agnostic skills and can compose them for solving new transfer-learning problems . However , to address reviewer concern , we also include new skill acquisition experiments ( Appendix B ) in our paper . These experiments show that our method can acquire missing skills ( task-dependent ) and can compose them together with the existing task-agnostic skill set to solve the given problem in an end-to-end learning manner . RC : Additionally , it would be great to also see the concurrent and sequential form of skill combination for the more complex tasks , and not just the point navigation task shown in Figure 6 . AR : We will add the depiction of attention weights for the other complicated tasks in a couple of days . RC : Standard errors on Figures 4 and 5 seem to be missing . AR : We have updated the figures to include standard errors . Please refer to the revised paper . RC : Additionally , I was curious that in Figure 4a and Figure 6a , the composite \u2019 s performance is already a lot better than the other methods after 0 training steps . Maybe the authors can elaborate on that . Maybe the performance at step 0 is just hard to make out in the graphs ? AR : It is because other methods sometimes push the object away from the target position at early evaluation steps , leading to an increase in distance of the object from the given target than its initial distance . RC : I would suggest accepting the paper but there could be a more detailed analysis of how the pre-trained sub-modules are used and learning both composite and sub-policies together would make the paper stronger . AR : We will include the attention weights depiction soon for the other complicated tasks . Furthermore , we have added new results to show that it is possible to learn both composite and sub-level policies together in end-to-end learning using our framework . RC : Where is the training graph for the Composite-SAV applied to the \u201c ant fall \u201d task ? Maybe I missed it ? AR : We do not include as composite-SAC for ant-maze , ant-push , and ant-fall . These problems require complex task planning ( sub-goal generation ) , and therefore , like other non-hierarchical RL methods ( SAC , TRPO , PPO ) , the composition-SAC also fails to perform in these problems . RC : Algorithm 1 should probably be moved to the main text . AR : Due to limited space , it might not be possible to move the algorithm 1 to the main text ."}, "1": {"review_id": "H1ezFREtwH-1", "review_text": "What is the specific question/problem tackled by the paper? This paper addresses the hierarchical RL problem of combining multiple primitive policies (pi_1, \u2026, pi_K) into policies for more complex tasks. Given a number of primitive skills and a new task within an environment, the paper aims to learn to pick and combine the primitives as needed to solve the new task. This problem statement is interesting and the method performs well on difficult tasks. However, I argue for rejecting this paper because it lacks meaningful contributions to the field. I do not see how the method presented in the paper is more than RL over hand engineered action spaces that are better for the tasks. While this improves results, we already know that for any task, there is some best action space for performing that task. This is why most HRL work aims to also find the primitive policies in additional to composing them. Is this any better than option-critic if the options are hardcoded to be the primitives? The experiment state that the option-critic method did not work, but did you give it access to the same primitives? Summary: The method presented in the paper is as follows: at each state s_t, the K primitives are queried for their action a_k ~ pi_k(s_t). Then, a biRNN reads in the actions in order from 1 to k. In parallel, the state and a goal are encoded by a network named \u201cDecoder\u201d. The encoded state and the hidden states of the RNN are used to output an attention weight over each primitive. Finally, the output action is the weighted combination of all the actions. The encoders and attention weights are trained with RL. This method is evaluated on several mujoco tasks, such as making a cheetah jump hurdles by combining \u201cforward\u201d and \u201cjump\u201d primitives. Each environment has predefined primitives such as \u201cforward\u201d \u201cleft\u201d \u201cright\u201d etc. This method is compared against HIRO, which does not have access to the primitive policies. It is not surprising that hand engineering primitives helps performance. Is the approach well motivated? The general idea behind the approach is well motivated: using primitive skills to learn complex skills is a useful goal. The details of the method are strange. I would like to see a better motivation and empirical justification for the biRNN. Why should the primitive\u2019s action be encode in order? The ordering of the primitives is arbitrary and constant: a fully connected network could be used, or the attentions could be output entirely independently per primitive. In fact, I do see not why the primitives\u2019 actions need to be encoded at all. It would be much simpler for the encoder to look at (s_t, g_t) and output a discrete probability over the K primitives. The ablations in 5.2 are for outputting actions directly rather than mixture weights. The paper would benefit from ablations where mixture weights are output but without the biRNN or without passing in the primitive\u2019s actions. Is the method well placed in the literature? The main idea of predicting weights over multiple experts is not novel (see \"Adaptive mixtures of local experts\u201d from 1991). In the context of RL literature, we can interpret the primitive skills as actions directly, and then the method is performing basic RL over a better action space (the better actions being \u201cgo left\u201d, \u201cjump\u201d etc. We can also interpret these as options, but unlike options a single primitive is not followed for multiple time steps with a termination condition. Functionally, this is equivalent to regular RL using domain knowledge to engineer the action space. ", "rating": "6: Weak Accept", "reply_text": "RC : Reviewer Comment ; AR : Authors Response RC : Is this any better than option-critic if the options are hardcoded to be the primitives ? The experiment state that the option-critic method did not work , but did you give it access to the same primitives ? AR : Option-critic method executes the options sequentially , and there is no concurrent composition . Furthermore , option-critic learns/needs tasks specific primitive , whereas our method can compose task-agnostic skills , both sequentially and concurrently , to solve new problems . RC : Why should the primitive \u2019 s action be encode in order ? AR : We use bidirectional RNN instead of uni-directional RNN to avoid ordering issues . It is also evident from the results that ordering is not a problem for our method . For instance , in cross-maze-ant , the composition has access to four primitive policies ( up , down , left , and right ) , and still , our method learns to not use downward policy ( irrespective of order ) at all as it is not needed to solve the given task . RC : The ablations in 5.2 are for outputting actions directly rather than mixture weights . The paper would benefit from ablations where mixture weights are output but without the biRNN or without passing in the primitive \u2019 s actions . AR : We have included the suggested ablation . Please refer to Fig 5 . Composition-without-BRNN takes the current state and outputs the mixture weights directly , which are then used to compose primitive actions . The composition-without-BRNN indeed perform better than other ablated models but still gives inferior performance than our proposed composition model . We argue that the composition model without BRNN performs poorly compared to the proposed method because it is entirely unaware of low-level continuous actions . Therefore , learning the latent encoding of primitive actions and making them a part of state-space is crucial for learning an effective composite policy ."}, "2": {"review_id": "H1ezFREtwH-2", "review_text": "Overall, I think the method has some great merit but I am not overly confident in the reproducibility of the method. Some of the comparisons (HIRO) do not agree with the results in the HIRO paper. Also, more description is needed to describe how the baselines were used in the analysis. Were they also given the pre-trained sub-policies? A more fair comparison might be to give those baseline methods no sub-policies but let them pre-train for an equivalent amount of time as the sub-policies are trained. Here are some more detailed comments: - Figure 1 is not very clear and does not appear to add much to the explanation of the method. More detail should be included in the caption. - In the paper, it is noted that HRL has high sample complexity and needs lots of training data. I find the comment about how HRL requires many more training steps than regular RL very odd. The purpose of HRL is to have better sample efficiency and learn strong polices faster. Has the author observed different? The purpose of HRL is to reduce sample complexity and search in a well suited and structured way. - The assumption that the sub-policies solve the underlying MDP is rather strong. How are these policies going to be trained to guarantee this? - I like the idea of using an attention model to help pick learn a weighting for the combination of a number of sub-policies. I am not sure if using a bi-directional LSTM is the best or simplest method to accomplish this. The authors can look at \"MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\" NeurIPS 2019 for a recent work that is similar to theirs. - For Figure 4: Do these methods also get to use the sub-policies that have pre-trained some version of tasks? Also, how are these component policies trained? Over what tasks are they trained? This information is very important to make sure the method is not overly biased to the composition of them. - I find the results in Figure 5 very odd. The baseline shows that HIRO does not learn to perform well on these tasks even though these are the tasks from the HIRO paper that it learned to solve rather well. Can this contradiction be explained? - For the HIRO comparison was the system also using the composite policies there were pretrained? HIRO is designed to learn the sub-policies concurrently but it seems in this case the authors are using the outputs of the composition policies as input to the HIRO low policy. - I do not understand the visualization in Figure 7. How to the colored paths for the agent represent the weights for the compose policy?", "rating": "6: Weak Accept", "reply_text": "RC : reviewer comment ; AR : author response We provide the source code to ensure reproducibility with trained polices . Also , we were able to reproduce HIRO results presented in their paper . In their paper , they use low-torque-ant ( 30 Units ) that limits the action space . In contrast , in our work , we use standard-ant ( 150 units ) , so the action-space is large and makes the learning problem significantly harder as the Ant is now more prone to instability . For completeness , we now update for plots ( Fig.4 ) to include HIRO results with low-toque-ant ( conforming with the system settings proposed in their paper ) . RC : Also , more description is needed to describe how the baselines were used in the analysis . Were they also given the pre-trained sub-policies ? A more fair comparison might be to give those baseline methods no sub-policies but let them pre-train for an equivalent amount of time as the sub-policies are trained . AR : We have included more descriptions on the baselines \u2019 analysis in Appendix C. Furthermore , we now include HIRO baselines that were pretrained on low-torque-ant ( as in HIRO paper ) for the equal amount of time as the sub-policies for the composite model were trained . The new results are presented in Fig 4 . The pretraining of HIRO on standard-Ant does not lead to any improvement . Therefore , to avoid clutter , we do not include those plots in Fig 4 . Furthermore , we would like to highlight that , in the case of Ant , our composition model uses the same primitive skills across all presented environments , which in some cases is not equivalent to pretraining other benchmark models every time . RC : Figure 1 is not very clear and does not appear to add much to the explanation of the method . AR : We exclude figure 1 as other reviewers think it \u2019 s unnecessary . RC : In the paper , it is noted that HRL has high sample complexity and needs lots of training data . I find the comment about how HRL requires many more training steps than regular RL very odd . The purpose of HRL is to have better sample efficiency and learn strong polices faster . Has the author observed different ? The purpose of HRL is to reduce sample complexity and search in a well suited and structured way . AR : We agree with the reviewer . Our statement was misleading . HRL methods perform indeed better than regular RL , and we have removed that statement . As per the author \u2019 s understanding , the HRL , especially HIRO , is for complex tasks with weak reward signals that require both task and motion planning . And , due to weak reward signals and complex decision-making , these methods take a huge amount of interactive experience with the environment , which , of course , is still much less than regular RL methods . And in this aspect , our proposed composition framework further improves data-efficiency of HIRO by effectively exploiting the primitive skills . RC : The assumption that the sub-policies solve the underlying MDP is rather strong . How are these policies going to be trained to guarantee this ? AR : We assume standard RL sub-level-policies policies that generally solve underlying MDPs . RC : The authors can look at \u201c MCP : Learning Composable Hierarchical Control with Multiplicative Compositional Policies \u201d NeurIPS 2019 for a recent work that is similar to theirs . AR : Thanks for pointing us to MCP framework , we have included it in our related work section . RC : For Figure 3 : Do these methods also get to use the sub-policies that have pre-trained some version of tasks ? Also , how are these component policies trained ? Over what tasks are they trained ? AR : We have included the details of training sub-level policies in Appendix C. RC : I find the results in Figure 4 very odd . The baseline shows that HIRO does not learn to perform well on these tasks even though these are the tasks from the HIRO paper that it learned to solve rather well . Can this contradiction be explained ? AR : We were able to reproduce HIRO results using their system settings ( low-torque-ant ) , please refer to Fig 4 for results . RC : For the HIRO comparison was the system also using the composite policies there were pretrained ? HIRO is designed to learn the sub-policies concurrently but it seems in this case the authors are using the outputs of the composition policies as input to the HIRO low policy . AR : In our proposed work , HIRO concurrently learns the high-level policy and a composite policy ( given primitive skills ) . RC : I do not understand the visualization in Figure 6 ? AR : The \u201c color \u201d of paths has nothing to do with the attention weights . The agent starts from the center ( origin ) and moves towards one of the given goals . Since primitive polices were to move straight up , down , left , and right , the diagonal motion requires the concurrent composition of low-level skills , which is indicated by the attention weights . On attention maps , the vertical axis reads from top to down , where zero means the starting position of the agent , and it takes about 30 steps to reach the target ."}}