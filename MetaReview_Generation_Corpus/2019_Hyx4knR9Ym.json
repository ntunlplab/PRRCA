{"year": "2019", "forum": "Hyx4knR9Ym", "title": "Generalizable Adversarial Training via Spectral Normalization", "decision": "Accept (Poster)", "meta_review": "Adversarial training has quickly become important for training robust neural networks.  However this training generally results in poor generalization behavior. This paper proposes using margin loss with adversarial training for better generalization. The paper provides generalization bounds for this adversarial training setup motivating the use of spectral regularization. The experimental results using the spectral regularization with adversarial training are very promising and all the reviewers agree that they show non-trivial improvement. Even though the spectral regularization techniques have been tried in different settings, hence of limited novelty, the experimental results in the paper are encouraging and I believe will motivate further study on this topic. Reviewers also opined that the writing in the paper is currently not that great with limited explanation of the theoretical results. More discussions interpreting the theoretical results and their significance can help the readers appreciate the paper better.", "reviews": [{"review_id": "Hyx4knR9Ym-0", "review_text": "This paper is well set-up to target the interesting problem of degraded generalisation after adversarial training. The proposal of applying spectral normalisation (SN) is well motivated, and is supported by margin-based bounds. However, the experimental results are weak in justifying the paper's claims. Pros: * The problem is interesting and well explained * The proposed method is clearly motivated * The proposal looks theoretically solid Cons: * It is unclear to me whether the \"efficient method for SN in convolutional nets\" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides. There is no direct comparison of performance. * Fig. 3 needs more explanation. The horizontal axes are unlabelled, and \"margin normalization\" is confusing when shown together with SN without an explanation. Perhaps it's helpful to briefly introduce it in addition to citing Bartlett et al. 2017. * The epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why? * Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem. However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal. Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm). It is thus unclear whether the advantage can be maintained after applying these standard regularsisers. A typo in page 6, last line: wth -> with", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank Reviewer 2 for the constructive feedback . Here is our point-to-point response to the comments and questions raised in the review : 1 . \u201c It is unclear to me whether the `` efficient method for SN in convolutional nets '' is more efficient than the power iteration algorithm employed in previous work , such as Miyato et al.2018 , which also used SN in conv nets with different strides . There is no direct comparison of performance. \u201d We do not claim that our method is more efficient than Miyato et al. \u2019 s method , which uses the spectral norm of the convolution kernel matrix to approximate the spectral norm of the convolution operation . In fact , our proposed method is computationally more expensive than their approximate scheme because each power iteration in our method requires a conv/deconv operation rather than a simple division used by Miyato et al. \u2019 s . We introduce our new spectral normalization scheme for convolutional layers because there exist examples where the true spectral norm of a convolution operation can be arbitrarily larger than Miyato et al. \u2019 s approximation . Therefore , Miyato et al. \u2019 s normalization scheme is not guaranteed to control the spectral norm of convolutional layers which is critical for controlling a DNN \u2019 s generalization performance ( please see our generalization bounds in Section 3 ) . To further support our argument , we performed additional experiments demonstrating how our proposed method better controls the spectral norm of convolution layers , resulting in better generalization and test performance . The results are presented in Appendix A.1 . Furthermore , we run several experiments to show that our method is not significantly slower than Miyato et al. \u2019 s method , and we report the results in Appendix A.1 , Table 3 . 2. \u201c Fig.3 needs more explanation . The horizontal axes are unlabelled , and `` margin normalization '' is confusing \u201d We relabel the axes and add a more thorough explanation in the caption . We note that the text explaining Figure 3 mentions how the margin normalization is performed ( paragraph 3 in section 5.1 ) : the margin normalization factor is exactly the capacity norm \\Phi described in Theorems 1-4 . We clarify that we divide the obtained margins by the values of \\Phi estimated on the dataset . 3. \u201c The epsilons in Fig.5 have very different scales ( 0 - 0.5 vs. 0 - 5 ) . Are these relevant to the specific algorithms and why ? \u201d Yes , the epsilons are chosen to be different depending on whether we are looking at norm_inf attacks or norm_2 attacks . This is because the two norms can behave very differently in adversarial attack experiments . For example , a norm_inf attack of 0.5 implies that all pixels can be changed by 0.5 . On the other hand , a norm_2 attack of 0.5 means the overall Euclidean norm of perturbation across all pixels is bounded by 0.5 , resulting in a much less powerful attack . Based on this comment , we update the plots with the same attack-norm to have the same scale . 4 . `` Section 5.3 ( Fig.6 ) is the part most relevant to the generalisation problem . However , the results are unconvincing : only the results for epsilon = 0.1 are shown , and even so the advantage is marginal . '' We redo the visualization in Figure 6 to make the gains provided by SN clearer . We see that using SN can improve the test performance by over 12 % for some FGM , PGM , and WRM cases . 5 . `` The baseline models did not use other almost standard regularisation techniques ( weight decay , dropout , batch-norm ) . It is thus unclear whether the advantage can be maintained after applying these standard regularisers . '' We did not originally discuss weight decay , dropout , and batch normalization as none of these methods were motivated by the theory we introduced in section 3 . However , due to the reviewers \u2019 concern in the updated draft we compare spectrally-normalized networks to networks with the same architecture except with weight decay , dropout , or batch norm in Appendix A.2 . In our experiments , the SN-regularized network still performs better in terms of test accuracy ."}, {"review_id": "Hyx4knR9Ym-1", "review_text": "The paper first provides a generalization bounds for adversarial training, showing that the error bound depends on Lipschitz constant. This motivates the use of spectral regularization (similar to Miyato et al 2018) in adversarial training. Using spectral regularization to improve robustness is not new, but it's interesting to combine spectral regularization and adversarial training. Experimental results show significant improvement over vanilla adversarial training. The paper is nicely written and the experimental results are quite strong and comprehensive. I really like the paper but I have two questions about the results: 1. The numbers reported in Figure 5 do not match with the performance of adversarial training in previous paper. In PGM L_inf adversarial training/attack (column 3 of Figure 5), the prediction accuracy is roughly 50% under 0.1 infinity norm perturbation. However, previous papers (e.g., \"Obfuscated Gradients Give a False Sense of Security\") reported 55% accuracy under 0.031 infinity norm perturbation. I wonder why the numbers are so different. Maybe it's because of different scales? Previous works usually scale each pixel to [0,1] or [-1,1], maybe the authors use the [0, 255] scale? But 0.1/255 will be much smaller than 0.031. Another factor might be the model structure. If Alexnet has much lower accuracy, it's probably worthwhile to conduct experiments on the same structure with previous works (Madry et al and Athalye et al) to make the conclusion more clear. 2. What's the training time of the proposed method compared with vanilla adversarial training? 3. The idea of using SN to improve robustness has been introduced in the following paper: \"Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks\" (but this paper did not combine it with adv training). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank Reviewer 1 for the constructive feedback . Here is our point-to-point response to the comments and questions raised in the review : 1 . \u201c The numbers reported in Figure 5 do not match with the performance of adversarial training in previous paper\u2026 I wonder why the numbers are so different. \u201d Table 1 of `` Obfuscated Gradients Give a False Sense of Security '' reports an accuracy of 47 % under 0.031 norm-inf perturbation for the CIFAR10 dataset ( 55 % is reported for the MNIST dataset ) , approximately the same as the 44 % accuracy in our Figure 5 . The difference in performance stems from how we preprocessed the CIFAR10 images : exactly in the manner described by ( Zhang et al. , 2017 ) \u2019 s ICLR paper \u201c Understanding deep learning requires rethinking generalization \u201d ( we whiten and crop each image ) . 2. \u201c What 's the training time of the proposed method compared with vanilla adversarial training ? \u201d We have added Table 2 to the Appendix which reports the increase in runtime for each of the 42 experiments discussed in Table 1 after introducing spectral normalization . For 39 of the cases , our TensorFlow implementation of the proposed method results in longer training times ( from 1.02 to 1.84 times longer ) . In the 3 cases of iterative adversarial attacks with the Inception architecture , the proposed method actually results in faster training time . This is likely due to how TensorFlow handles training in the backend . We provide the code for full transparency . 3. \u201c The idea of using SN to improve robustness has been introduced in the following paper : `` Lipschitz-Margin Training : Scalable Certification of Perturbation Invariance for Deep Neural Networks '' ( but this paper did not combine it with adv training ) . \u201d Thank you for bringing this recent work to our attention . We cite and discuss this NIPS paper in our updated draft ."}, {"review_id": "Hyx4knR9Ym-2", "review_text": "This paper proposes using spectral normalization (SN) as a regularization for adversarial training, which is based on [Miyato et. al., ICLR 2018], where the original paper used SN for GAN training. The paper also uses the results from [Neyshabur et. al., ICLR 2018], where the original paper provided generalization bounds that depends on spectral norm of each layer. The paper is well written in general, the experiments are extensive. The idea of studying based on the combination of the results from two previous papers is quite natural, since one uses spectral normalization in practice for GAN training, and the other provides generalization bound that depends on spectral norm. The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily. The experimental result itself is quite comprehensive. On the other hand, this paper provides specific generalization bounds under three adversarial attack methods, which explains the power of SN under those settings. However, it is not clear to me that these are some novel results that can better help adversarial training. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank Reviewer 3 for the constructive feedback . Here is our point-to-point response to the comments and questions raised in this review : 1 . \u201c The novelty of the algorithm itself is limited , since GAN and adversarial training are both minmax problems , and the original algorithm can be carried over easily \u201d GAN inference and adversarial training seek different goals . Adversarial training addresses a supervised learning task while GAN inference focuses on an unsupervised learning problem . Due to the inherent difference between supervised and unsupervised learning problems , the notion of generalization is defined differently between them . Arora et al . ( 2017 ) provide the standard definition of generalization error for GANs which is very different from the standard generalization error considered in supervised learning . Furthermore , no work in the literature theoretically guarantees that spectral normalization closes the generalization gap for either adversarial supervised learning or GAN unsupervised learning . 2. \u201c It is not clear to me that these are some novel results that can better help adversarial training \u201d Our work \u2019 s main contribution is the theoretical generalization guarantees for spectrally-normalized adversarially-trained DNNs . Introducing the adversary can significantly grow the capacity of a DNN . Therefore , existing DNN generalization bounds are not applicable to adversarial training settings . Our work , to our best knowledge , is the first to show that the adversarial learning capacity of a DNN for FGM , PGM , WRM training schemes can be effectively controlled by regularizing the spectral norm of the DNN \u2019 s weight matrices . Our numerical results further support our theoretical contribution ."}], "0": {"review_id": "Hyx4knR9Ym-0", "review_text": "This paper is well set-up to target the interesting problem of degraded generalisation after adversarial training. The proposal of applying spectral normalisation (SN) is well motivated, and is supported by margin-based bounds. However, the experimental results are weak in justifying the paper's claims. Pros: * The problem is interesting and well explained * The proposed method is clearly motivated * The proposal looks theoretically solid Cons: * It is unclear to me whether the \"efficient method for SN in convolutional nets\" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides. There is no direct comparison of performance. * Fig. 3 needs more explanation. The horizontal axes are unlabelled, and \"margin normalization\" is confusing when shown together with SN without an explanation. Perhaps it's helpful to briefly introduce it in addition to citing Bartlett et al. 2017. * The epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why? * Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem. However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal. Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm). It is thus unclear whether the advantage can be maintained after applying these standard regularsisers. A typo in page 6, last line: wth -> with", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank Reviewer 2 for the constructive feedback . Here is our point-to-point response to the comments and questions raised in the review : 1 . \u201c It is unclear to me whether the `` efficient method for SN in convolutional nets '' is more efficient than the power iteration algorithm employed in previous work , such as Miyato et al.2018 , which also used SN in conv nets with different strides . There is no direct comparison of performance. \u201d We do not claim that our method is more efficient than Miyato et al. \u2019 s method , which uses the spectral norm of the convolution kernel matrix to approximate the spectral norm of the convolution operation . In fact , our proposed method is computationally more expensive than their approximate scheme because each power iteration in our method requires a conv/deconv operation rather than a simple division used by Miyato et al. \u2019 s . We introduce our new spectral normalization scheme for convolutional layers because there exist examples where the true spectral norm of a convolution operation can be arbitrarily larger than Miyato et al. \u2019 s approximation . Therefore , Miyato et al. \u2019 s normalization scheme is not guaranteed to control the spectral norm of convolutional layers which is critical for controlling a DNN \u2019 s generalization performance ( please see our generalization bounds in Section 3 ) . To further support our argument , we performed additional experiments demonstrating how our proposed method better controls the spectral norm of convolution layers , resulting in better generalization and test performance . The results are presented in Appendix A.1 . Furthermore , we run several experiments to show that our method is not significantly slower than Miyato et al. \u2019 s method , and we report the results in Appendix A.1 , Table 3 . 2. \u201c Fig.3 needs more explanation . The horizontal axes are unlabelled , and `` margin normalization '' is confusing \u201d We relabel the axes and add a more thorough explanation in the caption . We note that the text explaining Figure 3 mentions how the margin normalization is performed ( paragraph 3 in section 5.1 ) : the margin normalization factor is exactly the capacity norm \\Phi described in Theorems 1-4 . We clarify that we divide the obtained margins by the values of \\Phi estimated on the dataset . 3. \u201c The epsilons in Fig.5 have very different scales ( 0 - 0.5 vs. 0 - 5 ) . Are these relevant to the specific algorithms and why ? \u201d Yes , the epsilons are chosen to be different depending on whether we are looking at norm_inf attacks or norm_2 attacks . This is because the two norms can behave very differently in adversarial attack experiments . For example , a norm_inf attack of 0.5 implies that all pixels can be changed by 0.5 . On the other hand , a norm_2 attack of 0.5 means the overall Euclidean norm of perturbation across all pixels is bounded by 0.5 , resulting in a much less powerful attack . Based on this comment , we update the plots with the same attack-norm to have the same scale . 4 . `` Section 5.3 ( Fig.6 ) is the part most relevant to the generalisation problem . However , the results are unconvincing : only the results for epsilon = 0.1 are shown , and even so the advantage is marginal . '' We redo the visualization in Figure 6 to make the gains provided by SN clearer . We see that using SN can improve the test performance by over 12 % for some FGM , PGM , and WRM cases . 5 . `` The baseline models did not use other almost standard regularisation techniques ( weight decay , dropout , batch-norm ) . It is thus unclear whether the advantage can be maintained after applying these standard regularisers . '' We did not originally discuss weight decay , dropout , and batch normalization as none of these methods were motivated by the theory we introduced in section 3 . However , due to the reviewers \u2019 concern in the updated draft we compare spectrally-normalized networks to networks with the same architecture except with weight decay , dropout , or batch norm in Appendix A.2 . In our experiments , the SN-regularized network still performs better in terms of test accuracy ."}, "1": {"review_id": "Hyx4knR9Ym-1", "review_text": "The paper first provides a generalization bounds for adversarial training, showing that the error bound depends on Lipschitz constant. This motivates the use of spectral regularization (similar to Miyato et al 2018) in adversarial training. Using spectral regularization to improve robustness is not new, but it's interesting to combine spectral regularization and adversarial training. Experimental results show significant improvement over vanilla adversarial training. The paper is nicely written and the experimental results are quite strong and comprehensive. I really like the paper but I have two questions about the results: 1. The numbers reported in Figure 5 do not match with the performance of adversarial training in previous paper. In PGM L_inf adversarial training/attack (column 3 of Figure 5), the prediction accuracy is roughly 50% under 0.1 infinity norm perturbation. However, previous papers (e.g., \"Obfuscated Gradients Give a False Sense of Security\") reported 55% accuracy under 0.031 infinity norm perturbation. I wonder why the numbers are so different. Maybe it's because of different scales? Previous works usually scale each pixel to [0,1] or [-1,1], maybe the authors use the [0, 255] scale? But 0.1/255 will be much smaller than 0.031. Another factor might be the model structure. If Alexnet has much lower accuracy, it's probably worthwhile to conduct experiments on the same structure with previous works (Madry et al and Athalye et al) to make the conclusion more clear. 2. What's the training time of the proposed method compared with vanilla adversarial training? 3. The idea of using SN to improve robustness has been introduced in the following paper: \"Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks\" (but this paper did not combine it with adv training). ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank Reviewer 1 for the constructive feedback . Here is our point-to-point response to the comments and questions raised in the review : 1 . \u201c The numbers reported in Figure 5 do not match with the performance of adversarial training in previous paper\u2026 I wonder why the numbers are so different. \u201d Table 1 of `` Obfuscated Gradients Give a False Sense of Security '' reports an accuracy of 47 % under 0.031 norm-inf perturbation for the CIFAR10 dataset ( 55 % is reported for the MNIST dataset ) , approximately the same as the 44 % accuracy in our Figure 5 . The difference in performance stems from how we preprocessed the CIFAR10 images : exactly in the manner described by ( Zhang et al. , 2017 ) \u2019 s ICLR paper \u201c Understanding deep learning requires rethinking generalization \u201d ( we whiten and crop each image ) . 2. \u201c What 's the training time of the proposed method compared with vanilla adversarial training ? \u201d We have added Table 2 to the Appendix which reports the increase in runtime for each of the 42 experiments discussed in Table 1 after introducing spectral normalization . For 39 of the cases , our TensorFlow implementation of the proposed method results in longer training times ( from 1.02 to 1.84 times longer ) . In the 3 cases of iterative adversarial attacks with the Inception architecture , the proposed method actually results in faster training time . This is likely due to how TensorFlow handles training in the backend . We provide the code for full transparency . 3. \u201c The idea of using SN to improve robustness has been introduced in the following paper : `` Lipschitz-Margin Training : Scalable Certification of Perturbation Invariance for Deep Neural Networks '' ( but this paper did not combine it with adv training ) . \u201d Thank you for bringing this recent work to our attention . We cite and discuss this NIPS paper in our updated draft ."}, "2": {"review_id": "Hyx4knR9Ym-2", "review_text": "This paper proposes using spectral normalization (SN) as a regularization for adversarial training, which is based on [Miyato et. al., ICLR 2018], where the original paper used SN for GAN training. The paper also uses the results from [Neyshabur et. al., ICLR 2018], where the original paper provided generalization bounds that depends on spectral norm of each layer. The paper is well written in general, the experiments are extensive. The idea of studying based on the combination of the results from two previous papers is quite natural, since one uses spectral normalization in practice for GAN training, and the other provides generalization bound that depends on spectral norm. The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily. The experimental result itself is quite comprehensive. On the other hand, this paper provides specific generalization bounds under three adversarial attack methods, which explains the power of SN under those settings. However, it is not clear to me that these are some novel results that can better help adversarial training. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank Reviewer 3 for the constructive feedback . Here is our point-to-point response to the comments and questions raised in this review : 1 . \u201c The novelty of the algorithm itself is limited , since GAN and adversarial training are both minmax problems , and the original algorithm can be carried over easily \u201d GAN inference and adversarial training seek different goals . Adversarial training addresses a supervised learning task while GAN inference focuses on an unsupervised learning problem . Due to the inherent difference between supervised and unsupervised learning problems , the notion of generalization is defined differently between them . Arora et al . ( 2017 ) provide the standard definition of generalization error for GANs which is very different from the standard generalization error considered in supervised learning . Furthermore , no work in the literature theoretically guarantees that spectral normalization closes the generalization gap for either adversarial supervised learning or GAN unsupervised learning . 2. \u201c It is not clear to me that these are some novel results that can better help adversarial training \u201d Our work \u2019 s main contribution is the theoretical generalization guarantees for spectrally-normalized adversarially-trained DNNs . Introducing the adversary can significantly grow the capacity of a DNN . Therefore , existing DNN generalization bounds are not applicable to adversarial training settings . Our work , to our best knowledge , is the first to show that the adversarial learning capacity of a DNN for FGM , PGM , WRM training schemes can be effectively controlled by regularizing the spectral norm of the DNN \u2019 s weight matrices . Our numerical results further support our theoretical contribution ."}}