{"year": "2019", "forum": "HkNGYjR9FX", "title": "Learning Recurrent Binary/Ternary Weights", "decision": "Accept (Poster)", "meta_review": "This work proposes a simple but useful way to train RNN with binary / ternary weights for improving memory and power efficiency. The paper presented a sequence of experiments on various benchmarks and demonstrated significant improvement on memory size  with only minor decrease of accuracy. Authors' rebuttal addressed the reviewers' concern nicely. \n\n", "reviews": [{"review_id": "HkNGYjR9FX-0", "review_text": "* Summary This paper proposes batch normalization for learning RNNs with binary or ternary weights instead of full-precision weights. Experiments are carried out on character-level and word-level language modeling, as well as sequential MNIST and question answering. * Strengths - I liked the variety of tasks used evaluations (sequential MNIST, language modeling, question answering). - Encouraging results on specialized hardware implementation. * Weaknesses - Using batch normalization on existing binarization/ternarization techniques is a bit of an incremental contribution. - All test perplexities for word-level language models in table 3 underperform compared to current vanilla LSTMs for that task (see Table 4 in https://arxiv.org/pdf/1707.05589.pdf), suggesting that the baseline LSTM used in this paper is not strong enough. - Results on question answering are not convincing -- BinaryConnect has the same size while achieving substantially higher accuracy (94.66% vs 40.78%). This is nowhere discussed and the paper's major claims \"binaryconnect method fails\" and \"our method [...] outperforms all the existing quantization methods\" seem unfounded (Section 5.5). - In the introduction, I am lacking a distinction between improvements w.r.t. training vs inference time. As far as I understand, quantization methods only help at reducing memory footprint or computation time during inference/test but not during training. This should be clarified. - In the introduction on page 2 is argued that the proposed method \"eliminates the need for multiplications\" -- I do not see how this is possible. Maybe what you meant is that it eliminates the need for full-precision multiplications by replacing them with multiplications with binary/ternary matrices? - The notation is quite confusing. For starters, in Section 2 you mention \"a fixed scaling factor A\" and I would encourage you to indicate scalars by lower-case letters, vectors by boldface lower-case letters and matrices by boldface upper-case letters. Moreover, it is unclear when calculations are approximate. For instance, in Eq. 1 I believe you need to replace \"=\" with \"\\approx\". Likewise for the equation in the next to last line on page 2. Lastly, while Eq. 2 seems to be a common way to write down LSTM equations, it is abusive notation. * Minor Comments - Abstract: What is ASIC? It is not referenced in Section 6. - Introduction: What is the justification for calling RNNs over-parameterized? This seems to depend on the task. - Introduction; contributions: Here, I would like to see a distinction between gains during training vs test time. - Section 3.2 comes out of nowhere. You might want to already mention why are introducing batch normalization at this point. - The boldfacing in Table 1, 2 and 3 is misleading. I understand this is done to highlight the proposed method, but I think commonly boldfacing is used to highlight the best results. - Figure 2b. What is your hypothesis why BPC actually goes down the longer the sequence is? - Algorithm 1, line 14: Using the cross-entropy is a specific choice dependent on the task. My understanding is your approach can work with any differentiable downstream loss?", "rating": "7: Good paper, accept", "reply_text": "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Minor Comments : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Abstract : What is ASIC ? It is not referenced in Section 6 . Our response : Application-Specific Integrated Circuit ( ASIC ) is a common term used as an integrated circuit customized for a particular use . We have now defined this abbreviation in Section 6 of the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Introduction : What is the justification for calling RNNs over-parameterized ? This seems to depend on the task . Our response : We agree with the reviewer that calling all RNNs over-parameterized is not precise , as it highly depends on the task and dimensions of inputs/outputs/state vectors of RNNs . Based on your comment , we have changed the sentence to \u201c ... , RNNs are typically over-parameterized ... \u201d in the introduction section of the revised manuscript . It is worth mentioning that it has been shown in literature that most networks \u2019 parameters can be pruned or quantized without any performance degradation , suggesting that neural networks are typically over-parameterized . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Introduction ; contributions : Here , I would like to see a distinction between gains during training vs test time . Our response : Based on your comment , we have clearly mentioned in the revised manuscript ( see the first bullet point of Section 1 and the last sentence of Section 4 ) that using binarized/ternarized weights is only beneficial for inference . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Section 3.2 comes out of nowhere . You might want to already mention why are introducing batch normalization at this point . Our response : We completely agree with the reviewer on this point . Based on your comment , we have merged Section 3.2 of the submitted version with Section 4 . We now introduce batch normalization right after explaining why we are motivated to use it in the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : The boldfacing in Table 1 , 2 and 3 is misleading . I understand this is done to highlight the proposed method , but I think commonly boldfacing is used to highlight the best results . Our response : Based on your comment , we have only highlighted the best results in all the tables of the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Figure 2b . What is your hypothesis why BPC actually goes down the longer the sequence is ? Our response : We believe that the models learn to focus only on information relevant to the generation of the next target character . The prediction accuracy of the models improves as the sequence length increases since longer sequences provide more information from the past to generate the next target character . We have added the above discussion to Section 5.5 of the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Algorithm 1 , line 14 : Using the cross-entropy is a specific choice dependent on the task . My understanding is your approach can work with any differentiable downstream loss ? Our response : Thank you for raising this . We have also used our method with other loss functions , and it is not limited to only cross entropy . We have addressed the issue in the revised manuscript ."}, {"review_id": "HkNGYjR9FX-1", "review_text": "The paper proposes a method to achieve binary and ternary quantization for recurrent networks. The key contribution is applying batch normalization to both input matrix vector and hidden matrix vector products within recurrent layers in order to preserve accuracy. The authors demonstrate accuracy benefits on a variety of datasets including language modeling (character and word level), MNIST sequence, and question answering. A hardware implementation based on DaDianNao is provided as well. Strengths - The authors propose a relatively simple and easy to understand methodology for achieving aggressive binary and ternary quantization. - The authors present compelling accuracy benefits on a range of datasets. Weaknesses / Questions - While the application of batch normalization demonstrates good results, having more compelling results on why covariate shift is such a problem in LSTMs would be helpful. Is this methodology applicable to other recurrent layers like RNNs and GRUs? - Does applying batch normalization across layer boundaries or at the end of each time-step help? This may incur lower overhead during inference and training time compared to applying batch normalization to the output of each matrix vector product (inputs and hidden-states). - Does training with batch-normalization add additional complexity to the training process? I imagine current DL framework do not efficiently parallelize applying batch normalization on both input and hidden matrix vector products. - It would be nice to have more intuition on what execution time overheads batch-normalization applies during inference on a CPU or GPU. That is, without a hardware accelerator what are the run-time costs, if any. - The hardware implementation could have much more detail. First, where are the area and power savings coming from. It would be nice to have a breakdown of on-chip SRAM for weights and activations vs. required DRAM memory. Similarly having a breakdown of power in terms of on-chip memory, off-chip memory, and compute would be helpful. - The hardware accelerator baseline assumes a 12-bit weight and activation quantization. Is this the best that can be achieved without sacrificing accuracy compared to floating point representation? Does adding batch normalization to intermediate matrix-vector products increase the required bit width for activations to preserve accuracy? Other comments - Preceding section 3.2 there no real discussion on batch normalization and covariate shift which are central to the work\u2019s contribution. It would be nice to include this in the introduction to guide the reader. - It is unclear why DaDianNao was chosen as the baseline hardware implementation as opposed to other hardware accelerator implementations such as TPU like dataflows or the open-source NVDLA. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Reviewer comment : The hardware accelerator baseline assumes a 12-bit weight and activation quantization . Is this the best that can be achieved without sacrificing accuracy compared to floating point representation ? Does adding batch normalization to intermediate matrix-vector products increase the required bit width for activations to preserve accuracy ? Our response : According to our simulation results , both the baseline and the proposed models require 12 bits for activations without incurring any accuracy degradation . The weights of the baseline model also require 12 bits for a fixed-point representation without incurring any performance degradation . Similar results have also been reported in ESE paper ( see https : //dl.acm.org/citation.cfm ? id=3021745 ) . Based on your comment , we have added the above discussion to Section 6 of the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Other comments -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Preceding section 3.2 there no real discussion on batch normalization and covariate shift which are central to the work \u2019 s contribution . It would be nice to include this in the introduction to guide the reader . Our response : We completely agree with the reviewer . We have now merged Section 3.2 with Section 4 in the revised manuscript to make a more coherent statement . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : It is unclear why DaDianNao was chosen as the baseline hardware implementation as opposed to other hardware accelerator implementations such as TPU like dataflows or the open-source NVDLA . Our response : We agree with the reviewers that TPU and NVDLA are among the best accelerators reported to-date . However , we believe that DaDianNao is also one of the best accelerators and the reason for that is twofold . First , DaDianNao is designed for energy-efficiency : it can process neural computations ~656x faster and is ~184x more energy efficient than GPUs ( see https : //ieeexplore.ieee.org/document/7480791 ) . Second , some hardware techniques can be adopted on top of DaDianNao to further speed up the computations . For instance , in Cambricon-X paper ( see https : //ieeexplore.ieee.org/document/7783723 ) , it was shown that sparsity among both activations and weights can be exploited on top of the DaDianNao \u2019 s dataflow to skip the noncontributory computations with zeros and speed up the process . Similarly , Cnvlutin \u2019 s paper ( see https : //ieeexplore.ieee.org/document/7551378 ) uses the DaDianNao \u2019 s architecture to skip the noncontributory computations of zero-valued activations . We believe that similar techniques can be also exploited to skip the noncontributory computations of zero-valued weights of ternarized RNNs as a future work . Based on the reviewer \u2019 s comment , we have added the above discussion to the revised manuscript ."}, {"review_id": "HkNGYjR9FX-2", "review_text": "This work proposes a method for reducing memory requirements in RNN models via binary / ternary quantisation. The authors argue that binarising RNNs is due to a covariate shift, and address it with stochastic quantised weights and batch normalisation. The proposed RNN is tested on 6 sequence modelling tasks/datasets and shows drastic memory improvements compared to full-precision RNNs, with almost no loss in test performance. Based on the more efficient RNN cell, the authors furthermore describe a more efficient hardware implementation, compared to an implementation of the full-precision RNN. The core message I took away from this work is: \u201cOne can get away with stochastic binarised weights in a forward pass by compensating for it with batch normalisation\u201d. Strengths: - substantial number of experiments (6 datasets), different domains - surprisingly simple methodological fix - substantial literature review - it has been argued that char-level / pixel-level RNNs present somewhat artificial tasks \u2014 even better that the authors test for a more realistic RNN application (Reading Comprehension) with an actually previously published model. Weaknesses: - little understanding is provided into _why_ covariance shift occurs/ why batch normalisation is so useful. The method works, but the authors could elaborate more on this, given that this is the core argument motivating the chosen method. - some statements are too bold/vague , e.g. page 3: \u201ca binary/ternary model that can perform all temporal tasks\u201d - unclear: by adapting a probabilistic formulation / sampling quantised weights, some variance is introduced. Does it matter for predictions (which should now also be stochastic)? How large is this variance? Even if negligible, it is not obvious and should be addressed. Other Questions / Comments - How dependent is the method on the batch size chosen? This is in particular relevant as smaller batches might yield poor empirical estimates for mean/var. What happens at batch size 1? Are predictions of poorer for smaller batches? - Section 2, second line \u2014 detail: case w_{i,j}=0 is not covered - equation (5): total probability mass does not add up to 1 - a direct comparison with models from previous work would have been interesting, where these previous methods also rely on batch normalisation - as I understand, the main contribution is in the inference (forward pass), not in training. It is somewhat misleading when the authors speak about \u201cthe proposed training algorithm\u201d or \u201cwe introduced a training algorithm\u201d - unclear: last sentence before section 6. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Other Questions / Comments -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : How dependent is the method on the batch size chosen ? This is in particular relevant as smaller batches might yield poor empirical estimates for mean/var . What happens at batch size 1 ? Are predictions of poorer for smaller batches ? Our response : Based on your comment , we have investigated the effect of using different batch sizes on the prediction accuracy of our binarized/ternarized models ( see Section 5.5 of the revised manuscript ) . To this end , we trained an LSTM of size 1000 over a sequence length of 100 and different batch sizes to perform the character-level language modeling task on the Penn Treebank corpus . The simulation results show that batch normalization can not be used with batch size of 1 , as the output vector will be all zeros . Moreover , using batch sizes slightly larger than 1 lead to a high variance in the estimations of the statistics of the unnormalized vector , resulting in a lower prediction accuracy than the baseline model ( without batch normalization ) as shown in Figure 3 of the revised manuscript . On the other hand , the prediction accuracy of our binarized/ternarized models improves as the batch size increases while the prediction accuracy of the baseline model decreases . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Section 2 , second line \u2014 detail : case w_ { i , j } =0 is not covered Our response : Thank you for raising this . We have fixed the typo in the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : equation ( 5 ) : total probability mass does not add up to 1 Our response : For the stochastic ternarization process , we sample from [ 0 , 1 ] interval depending on the weight sign . In case of having a positive sign , the probability of getting +1 is equal to the absolute value of the normalized weight and the probability of getting 0 is 1-P ( w = 1 ) which adds up to 1 . Similarly , in case of having a negative sign , the probability of getting -1 is equal to the absolute value of the normalized weight and the probability of getting 0 is 1-P ( w = -1 ) which also adds up to 1 . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : a direct comparison with models from previous work would have been interesting , where these previous methods also rely on batch normalisation Our response : Unfortunately , we could not find any other methods that rely on batch normalization to the best of our knowledge . However , we tried our best to compare our method with other existing quantization methods . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : as I understand , the main contribution is in the inference ( forward pass ) , not in training . It is somewhat misleading when the authors speak about \u201c the proposed training algorithm \u201d or \u201c we introduced a training algorithm \u201d Our response : We completely agree with the reviewer . We have revised the misleading statements in the manuscript based on your comment . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : unclear : last sentence before section 6 . Our response : Thank you for raising this . We have rephrased the sentence in the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -"}], "0": {"review_id": "HkNGYjR9FX-0", "review_text": "* Summary This paper proposes batch normalization for learning RNNs with binary or ternary weights instead of full-precision weights. Experiments are carried out on character-level and word-level language modeling, as well as sequential MNIST and question answering. * Strengths - I liked the variety of tasks used evaluations (sequential MNIST, language modeling, question answering). - Encouraging results on specialized hardware implementation. * Weaknesses - Using batch normalization on existing binarization/ternarization techniques is a bit of an incremental contribution. - All test perplexities for word-level language models in table 3 underperform compared to current vanilla LSTMs for that task (see Table 4 in https://arxiv.org/pdf/1707.05589.pdf), suggesting that the baseline LSTM used in this paper is not strong enough. - Results on question answering are not convincing -- BinaryConnect has the same size while achieving substantially higher accuracy (94.66% vs 40.78%). This is nowhere discussed and the paper's major claims \"binaryconnect method fails\" and \"our method [...] outperforms all the existing quantization methods\" seem unfounded (Section 5.5). - In the introduction, I am lacking a distinction between improvements w.r.t. training vs inference time. As far as I understand, quantization methods only help at reducing memory footprint or computation time during inference/test but not during training. This should be clarified. - In the introduction on page 2 is argued that the proposed method \"eliminates the need for multiplications\" -- I do not see how this is possible. Maybe what you meant is that it eliminates the need for full-precision multiplications by replacing them with multiplications with binary/ternary matrices? - The notation is quite confusing. For starters, in Section 2 you mention \"a fixed scaling factor A\" and I would encourage you to indicate scalars by lower-case letters, vectors by boldface lower-case letters and matrices by boldface upper-case letters. Moreover, it is unclear when calculations are approximate. For instance, in Eq. 1 I believe you need to replace \"=\" with \"\\approx\". Likewise for the equation in the next to last line on page 2. Lastly, while Eq. 2 seems to be a common way to write down LSTM equations, it is abusive notation. * Minor Comments - Abstract: What is ASIC? It is not referenced in Section 6. - Introduction: What is the justification for calling RNNs over-parameterized? This seems to depend on the task. - Introduction; contributions: Here, I would like to see a distinction between gains during training vs test time. - Section 3.2 comes out of nowhere. You might want to already mention why are introducing batch normalization at this point. - The boldfacing in Table 1, 2 and 3 is misleading. I understand this is done to highlight the proposed method, but I think commonly boldfacing is used to highlight the best results. - Figure 2b. What is your hypothesis why BPC actually goes down the longer the sequence is? - Algorithm 1, line 14: Using the cross-entropy is a specific choice dependent on the task. My understanding is your approach can work with any differentiable downstream loss?", "rating": "7: Good paper, accept", "reply_text": "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Minor Comments : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Abstract : What is ASIC ? It is not referenced in Section 6 . Our response : Application-Specific Integrated Circuit ( ASIC ) is a common term used as an integrated circuit customized for a particular use . We have now defined this abbreviation in Section 6 of the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Introduction : What is the justification for calling RNNs over-parameterized ? This seems to depend on the task . Our response : We agree with the reviewer that calling all RNNs over-parameterized is not precise , as it highly depends on the task and dimensions of inputs/outputs/state vectors of RNNs . Based on your comment , we have changed the sentence to \u201c ... , RNNs are typically over-parameterized ... \u201d in the introduction section of the revised manuscript . It is worth mentioning that it has been shown in literature that most networks \u2019 parameters can be pruned or quantized without any performance degradation , suggesting that neural networks are typically over-parameterized . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Introduction ; contributions : Here , I would like to see a distinction between gains during training vs test time . Our response : Based on your comment , we have clearly mentioned in the revised manuscript ( see the first bullet point of Section 1 and the last sentence of Section 4 ) that using binarized/ternarized weights is only beneficial for inference . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Section 3.2 comes out of nowhere . You might want to already mention why are introducing batch normalization at this point . Our response : We completely agree with the reviewer on this point . Based on your comment , we have merged Section 3.2 of the submitted version with Section 4 . We now introduce batch normalization right after explaining why we are motivated to use it in the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : The boldfacing in Table 1 , 2 and 3 is misleading . I understand this is done to highlight the proposed method , but I think commonly boldfacing is used to highlight the best results . Our response : Based on your comment , we have only highlighted the best results in all the tables of the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Figure 2b . What is your hypothesis why BPC actually goes down the longer the sequence is ? Our response : We believe that the models learn to focus only on information relevant to the generation of the next target character . The prediction accuracy of the models improves as the sequence length increases since longer sequences provide more information from the past to generate the next target character . We have added the above discussion to Section 5.5 of the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Algorithm 1 , line 14 : Using the cross-entropy is a specific choice dependent on the task . My understanding is your approach can work with any differentiable downstream loss ? Our response : Thank you for raising this . We have also used our method with other loss functions , and it is not limited to only cross entropy . We have addressed the issue in the revised manuscript ."}, "1": {"review_id": "HkNGYjR9FX-1", "review_text": "The paper proposes a method to achieve binary and ternary quantization for recurrent networks. The key contribution is applying batch normalization to both input matrix vector and hidden matrix vector products within recurrent layers in order to preserve accuracy. The authors demonstrate accuracy benefits on a variety of datasets including language modeling (character and word level), MNIST sequence, and question answering. A hardware implementation based on DaDianNao is provided as well. Strengths - The authors propose a relatively simple and easy to understand methodology for achieving aggressive binary and ternary quantization. - The authors present compelling accuracy benefits on a range of datasets. Weaknesses / Questions - While the application of batch normalization demonstrates good results, having more compelling results on why covariate shift is such a problem in LSTMs would be helpful. Is this methodology applicable to other recurrent layers like RNNs and GRUs? - Does applying batch normalization across layer boundaries or at the end of each time-step help? This may incur lower overhead during inference and training time compared to applying batch normalization to the output of each matrix vector product (inputs and hidden-states). - Does training with batch-normalization add additional complexity to the training process? I imagine current DL framework do not efficiently parallelize applying batch normalization on both input and hidden matrix vector products. - It would be nice to have more intuition on what execution time overheads batch-normalization applies during inference on a CPU or GPU. That is, without a hardware accelerator what are the run-time costs, if any. - The hardware implementation could have much more detail. First, where are the area and power savings coming from. It would be nice to have a breakdown of on-chip SRAM for weights and activations vs. required DRAM memory. Similarly having a breakdown of power in terms of on-chip memory, off-chip memory, and compute would be helpful. - The hardware accelerator baseline assumes a 12-bit weight and activation quantization. Is this the best that can be achieved without sacrificing accuracy compared to floating point representation? Does adding batch normalization to intermediate matrix-vector products increase the required bit width for activations to preserve accuracy? Other comments - Preceding section 3.2 there no real discussion on batch normalization and covariate shift which are central to the work\u2019s contribution. It would be nice to include this in the introduction to guide the reader. - It is unclear why DaDianNao was chosen as the baseline hardware implementation as opposed to other hardware accelerator implementations such as TPU like dataflows or the open-source NVDLA. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Reviewer comment : The hardware accelerator baseline assumes a 12-bit weight and activation quantization . Is this the best that can be achieved without sacrificing accuracy compared to floating point representation ? Does adding batch normalization to intermediate matrix-vector products increase the required bit width for activations to preserve accuracy ? Our response : According to our simulation results , both the baseline and the proposed models require 12 bits for activations without incurring any accuracy degradation . The weights of the baseline model also require 12 bits for a fixed-point representation without incurring any performance degradation . Similar results have also been reported in ESE paper ( see https : //dl.acm.org/citation.cfm ? id=3021745 ) . Based on your comment , we have added the above discussion to Section 6 of the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Other comments -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Preceding section 3.2 there no real discussion on batch normalization and covariate shift which are central to the work \u2019 s contribution . It would be nice to include this in the introduction to guide the reader . Our response : We completely agree with the reviewer . We have now merged Section 3.2 with Section 4 in the revised manuscript to make a more coherent statement . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : It is unclear why DaDianNao was chosen as the baseline hardware implementation as opposed to other hardware accelerator implementations such as TPU like dataflows or the open-source NVDLA . Our response : We agree with the reviewers that TPU and NVDLA are among the best accelerators reported to-date . However , we believe that DaDianNao is also one of the best accelerators and the reason for that is twofold . First , DaDianNao is designed for energy-efficiency : it can process neural computations ~656x faster and is ~184x more energy efficient than GPUs ( see https : //ieeexplore.ieee.org/document/7480791 ) . Second , some hardware techniques can be adopted on top of DaDianNao to further speed up the computations . For instance , in Cambricon-X paper ( see https : //ieeexplore.ieee.org/document/7783723 ) , it was shown that sparsity among both activations and weights can be exploited on top of the DaDianNao \u2019 s dataflow to skip the noncontributory computations with zeros and speed up the process . Similarly , Cnvlutin \u2019 s paper ( see https : //ieeexplore.ieee.org/document/7551378 ) uses the DaDianNao \u2019 s architecture to skip the noncontributory computations of zero-valued activations . We believe that similar techniques can be also exploited to skip the noncontributory computations of zero-valued weights of ternarized RNNs as a future work . Based on the reviewer \u2019 s comment , we have added the above discussion to the revised manuscript ."}, "2": {"review_id": "HkNGYjR9FX-2", "review_text": "This work proposes a method for reducing memory requirements in RNN models via binary / ternary quantisation. The authors argue that binarising RNNs is due to a covariate shift, and address it with stochastic quantised weights and batch normalisation. The proposed RNN is tested on 6 sequence modelling tasks/datasets and shows drastic memory improvements compared to full-precision RNNs, with almost no loss in test performance. Based on the more efficient RNN cell, the authors furthermore describe a more efficient hardware implementation, compared to an implementation of the full-precision RNN. The core message I took away from this work is: \u201cOne can get away with stochastic binarised weights in a forward pass by compensating for it with batch normalisation\u201d. Strengths: - substantial number of experiments (6 datasets), different domains - surprisingly simple methodological fix - substantial literature review - it has been argued that char-level / pixel-level RNNs present somewhat artificial tasks \u2014 even better that the authors test for a more realistic RNN application (Reading Comprehension) with an actually previously published model. Weaknesses: - little understanding is provided into _why_ covariance shift occurs/ why batch normalisation is so useful. The method works, but the authors could elaborate more on this, given that this is the core argument motivating the chosen method. - some statements are too bold/vague , e.g. page 3: \u201ca binary/ternary model that can perform all temporal tasks\u201d - unclear: by adapting a probabilistic formulation / sampling quantised weights, some variance is introduced. Does it matter for predictions (which should now also be stochastic)? How large is this variance? Even if negligible, it is not obvious and should be addressed. Other Questions / Comments - How dependent is the method on the batch size chosen? This is in particular relevant as smaller batches might yield poor empirical estimates for mean/var. What happens at batch size 1? Are predictions of poorer for smaller batches? - Section 2, second line \u2014 detail: case w_{i,j}=0 is not covered - equation (5): total probability mass does not add up to 1 - a direct comparison with models from previous work would have been interesting, where these previous methods also rely on batch normalisation - as I understand, the main contribution is in the inference (forward pass), not in training. It is somewhat misleading when the authors speak about \u201cthe proposed training algorithm\u201d or \u201cwe introduced a training algorithm\u201d - unclear: last sentence before section 6. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Other Questions / Comments -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : How dependent is the method on the batch size chosen ? This is in particular relevant as smaller batches might yield poor empirical estimates for mean/var . What happens at batch size 1 ? Are predictions of poorer for smaller batches ? Our response : Based on your comment , we have investigated the effect of using different batch sizes on the prediction accuracy of our binarized/ternarized models ( see Section 5.5 of the revised manuscript ) . To this end , we trained an LSTM of size 1000 over a sequence length of 100 and different batch sizes to perform the character-level language modeling task on the Penn Treebank corpus . The simulation results show that batch normalization can not be used with batch size of 1 , as the output vector will be all zeros . Moreover , using batch sizes slightly larger than 1 lead to a high variance in the estimations of the statistics of the unnormalized vector , resulting in a lower prediction accuracy than the baseline model ( without batch normalization ) as shown in Figure 3 of the revised manuscript . On the other hand , the prediction accuracy of our binarized/ternarized models improves as the batch size increases while the prediction accuracy of the baseline model decreases . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : Section 2 , second line \u2014 detail : case w_ { i , j } =0 is not covered Our response : Thank you for raising this . We have fixed the typo in the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : equation ( 5 ) : total probability mass does not add up to 1 Our response : For the stochastic ternarization process , we sample from [ 0 , 1 ] interval depending on the weight sign . In case of having a positive sign , the probability of getting +1 is equal to the absolute value of the normalized weight and the probability of getting 0 is 1-P ( w = 1 ) which adds up to 1 . Similarly , in case of having a negative sign , the probability of getting -1 is equal to the absolute value of the normalized weight and the probability of getting 0 is 1-P ( w = -1 ) which also adds up to 1 . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : a direct comparison with models from previous work would have been interesting , where these previous methods also rely on batch normalisation Our response : Unfortunately , we could not find any other methods that rely on batch normalization to the best of our knowledge . However , we tried our best to compare our method with other existing quantization methods . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : as I understand , the main contribution is in the inference ( forward pass ) , not in training . It is somewhat misleading when the authors speak about \u201c the proposed training algorithm \u201d or \u201c we introduced a training algorithm \u201d Our response : We completely agree with the reviewer . We have revised the misleading statements in the manuscript based on your comment . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Reviewer comment : unclear : last sentence before section 6 . Our response : Thank you for raising this . We have rephrased the sentence in the revised manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -"}}