{"year": "2020", "forum": "BkePHaVKwS", "title": "Learning Surrogate Losses", "decision": "Reject", "meta_review": "Unfortunately, this was a borderline paper that generated disagreement among the reviewers.  After high level round of additional deliberation it was decided that this paper does not yet meet the standard for acceptance.  The paper proposes a potentially interesting approach to learning surrogates for non-differentiable and non-decomposable loss functions.  However, the work is a bit shallow technically, as any supporting theoretical justification is supplied by pointing to other work.  The paper would be stronger with a more serious and comprehensive analysis.  The reviewers criticized the lack of clarity in the technical exposition, which the authors attempted to mitigate in the rebuttal/revision process.  The paper would benefit from additional clarity and systematic presentation of complete details to allow reproduction.", "reviews": [{"review_id": "BkePHaVKwS-0", "review_text": "In this paper authors propose to jointly optimize a critic, that estimates some non-differentiable objective (or an objective with intractable derivatives and/or derivatives that are 0 almost everywhere). Authors conduct numerous experiments, and show improvements over some sota methods, and maybe more importantly - provide a unified way of achieving these across multiple metrics. Paper is well written, very easy to understand and in reviewers opinion, a nice, simple story worth sharing. Major concerns: The theoretical result provided is just a citation of an existing proof, rather than a new contribution. More importantly however, the setup considered strongly violates the assumptions of the theorem, for example assumption (b) does not hold for the neural networks, as there are continuum many global minima (imagine having a unit in a neural net, that has a huge negative bias, which causes it to always be \"turned off\" by relu, then any weight on top of it will have the same final loss value; or simply notice that relu(a*x)=a*relu(x) if x>0, and so you can always \"push\" the norm of weights from one layer to the next one without affecting the outcome etc.), and so argmin is not a singleton, but the set of exactly same power as the whole parameter space (since for each k there exists a bijection from R^k to R, and R is of continuum power). This property is not a mild assumption, but rather a critical element guaranteeing convergence of such systems, with powerful universal approximators, one cannot hope for such strong convergence results without actually analysing the approximators family. Reviewer strongly believes this should be either removed completely, or just briefly mentioned, rather than made a strong statement in the paper. It is an empirical work, and stands strong on its own rights, there is no need to add theorems with assumptions that are never satisfied in the proposed scenario. Results provided use somewhat non-standard datasets for deep learning, and as such it is hard to asses statistical significance of the differences reported; while the test datasets sizes are big enough to trust error to 1e-3 level, they are heavily imbalanced and relatively low dimensional problems, which have proven to be hard for neural network many times in the past, consequently I would expect to see confidence intervals or stds of each result, at least for Table 3. The problem becomes even more severe for metrics such as F1 which do not decompose additively and so can be very sensitive to the (false) positive rates values - reported improvements might disappear once these are introduced, but even if one does not outperform hand-crafted proxies for specific objectives, having a unified method that is on-part with those in black-box scenario is a good result (I would argue that even if all the results are slightly worse, it is still worth publishing). I find it a bit disappointing, that authors did not try to analyse trained models, it would be invaluable to see what kind of aggregation g and h came up with, that is well aligned with losses such as F1 or JAC. Minor concerns: The work resembles closely methods of error critic learning - where one uses an update direction coming from a model, that regresses towards the loss itself, or the gradient of the loss (is available), see: - Neural Network Design for J Function Approximation in Dynamic Programming (Pang and Werbos paper from '98) - Sobolev Training of Neural Networks (from NIPS; more precisely \"Critic\" baseline, which has the same functional form as the loss presented in the paper, if applied at the very top of the network only) It might be worth discussing in the paper. Overall I recommend weak acceptance, and encourage authors to address some of the concerns raised above. **Update** Based on discussion, corrections and new additions, I recommend acceptance of the above work.", "rating": "8: Accept", "reply_text": "Thanks a lot for reading the paper in-depth and providing valuable criticism . Regarding the major concerns : * We agree with your reasoning that the singleton assumption is too strong , therefore we accept the criticism . Your suggestion to shorten the convergence note will be taken into consideration . As a result , we will reduce that section to a few sentences in the rebuttal to make room for addressing the comments of the reviewers . * Related to the choice of datasets , we tried to blend a mix of UCI datasets with more modern datasets from Kaggle . Please notice that the imbalance of the datasets is actually a desirable property rather than a bottleneck because in such datasets it more suitable to assess the performance of metrics such as AUC , F1 , etc .. On the other hand , the imbalance is obviously not a facultative option , but rather a real-life challenge . Since the experimental protocol was conducted on a random 80 % -20 % split , the confidence intervals are not defined . Yet , we agree that the size of the test sets is large enough to offer trustable empirical evidence . * On the other hand , you are right in stating that optimizing directly for F1 or AUC in such imbalanced datasets gives a lift if we would have compared against balance-sensitive decomposable losses such as cross-entropy . However , please notice that we actually compared and outperformed the state-of-the-art baselines of Section 4.3 which also directly optimize for these imbalance-handling losses ( such as AUC , F1 , Jaccard ) . * Yet again , we agree that the interpretability of the surrogate losses would be nice to have . Unfortunately , it is not trivial to analyze how the Q=30 latent per-instance error components of function g directly relate to , for example , the four confusion matrix measures of F1 . Even if we force Q=4 there is no guarantee that the latent error components would directly match the confusion matrix , since the multi-layered aggregation network h combines the components of g in a black-box ( a.k.a.neural network-ish ) manner , and not in the same way that common losses ( such as F1 ) do . Perhaps one can extend this work by a multi-target surrogate optimization where we set Q=4 and at jointly i ) train the output of g to estimate the confusion matrix terms , and ii ) train the output of h to match the final metric ( e.g.F1 , Jaccard ) . The disadvantage of this approach is that it would not be directly applicable to AUC , because we have multiple confusion matrices for the various thresholds used to binarize the estimated target . As a result , it actually seems not straightforward to analyze the black-box surrogates and interpret their neural networks . Regarding Minor concerns : The relation you pointed out to error critic learning seems relevant and we will incorporate it into the related work . However , I failed to see the relation to Sobolev training , In our case , there is no information on the gradients of the target loss with respect to the activations of the different layers of the surrogate model , since in contrast to the assumptions of the NIPS paper our target loss function has no differentiable form ."}, {"review_id": "BkePHaVKwS-1", "review_text": "This paper proposes a method of learning loss functions in addition to the learning of predictors. Since it's not easy to optimize loss functions that evaluate the accuracy, surrogate loss functions have been widely employed. The design of the surrogate loss is problem-dependent, and handcraft is required. This paper tries to tackle this problem from the viewpoint of meta-learning, i.e., the surrogate loss learning. Typically, deep neural networks (DNN) are used to design a surrogate loss that approximates the original loss while maintaining the tractability of the optimization. Some convergence properties of the proposed method are analyzed. Some empirical studies showed the efficiency of the proposed method to the state-of-the-art baselines. The design of the surrogate loss is important for machine learning problems. However, the proposed method in this paper seems an ad-hoc approach rather. For example, the 0-1 loss is often replaced with convex loss functions such as the hinge loss or logistic loss. Using these surrogate loss functions, the statistical properties of the predictors obtained from 0-1 loss are maintained. See the following paper for details. P. L. Bartlett, et al., (2006), Convexity, Classification, and Risk Bounds, Journal of the American Statistical Association March , Vol. 101, No. 473. On the other hand, the current approach does not have such a theoretical guarantee for each learning problems. Though certainly, the proposed method is widely applicable to many problems, there is no theoretical guarantee. Theorem 1 in page 5 shows the convergence property. However, the number of iterations, K_beta, should tend to infinity. This is not a practical operation in the learning algorithm", "rating": "3: Weak Reject", "reply_text": "Thanks for the important point you raised . We would like to address both aspects of your review : a ) The proposed surrogate loss model is ad-hoc b ) There is no theoretic guarantee of the surrogate loss in approximating the true loss Regarding point a ) Please let me remind that given the true targets $ y \\in \\mathbb { R } ^N $ of a batch having $ N $ instances and the estimated targets $ \\hat y \\in \\mathbb { R } ^N $ of that batch , then a loss function is defined as the true loss $ \\ell ( y , \\hat y ) : \\mathbb { R } ^ { 2N } \\rightarrow \\mathbb { R } $ . The purpose of a surrogate $ \\hat \\ell ( y , \\hat y ) $ is , therefore , to approximate well such a true loss $ \\ell ( y , \\hat y ) $ . The first naive intuition would be to define $ \\hat \\ell ( y , \\hat y ) $ as a plain MLP neural network , which by virtue of the universal approximation theorem is guaranteed to estimate $ \\ell ( y , \\hat y ) $ . However , further thinking reveals that $ \\hat \\ell $ can not be a plain neural network with $ 2N $ inputs and $ 1 $ output , because of the permutation-invariant treat of the true losses . Notice that the loss output should remain the same even if the order of the instances within a batch is shuffled ( a.k.a.permutation-invariant ) . A neural network , on the other hand , does not ensure that the output remains the same if the order of features is swapped . Given that loss functions are permutation-invariant , we came up with the intuition that a loss is actually a mathematical function that operates over sets . In this case the sets of $ ( y , \\hat y ) \\in \\mathbb { R } ^2 $ values for a set of batch instances . The set formulation of the loss handles the permutation-invariant nature of randomly shuffling instances within a batch . A recent work [ 1 ] has actually proven that functions expressed via the Kolmogorov-Arnold representation theorem are permutation-invariant . As a result , the surrogate loss we propose in Equation 2 is actually the methodologically-correct surrogate model to approximate the permutation-invariant true loss functions . Regarding point b ) We would like to clarify that the Kolmogorov-Arnold representation is well-known to offer theoretical guarantees in terms of approximating the target function [ 3 ] . Even in the concrete case where the functions $ g $ and $ h $ of Equation 2 are ReLU networks ( as in our paper ) , the respective error bounds exist and are derived by prior literature [ 2 ] . As a result , since our surrogate network is an instance of the standard Kolmogorov-Arnold representation , it automatically benefits from the known aforementioned theoretical guarantees of the Kolmogorov-Arnold representation theorem . We mentioned this aspect briefly in Section 3 , however , thanks to your comment we will expand the clarification with a paragraph on its own for the rebuttal . -- -- -- -- -- - Regarding the comment on $ K_\\beta $ tending to infinity in the optimization algorithm , please notice that in practice the models converge with a small $ K_\\beta=10 $ ( Section 4.1 ) . We can actually observe the convergence in Figure 3 of Section 4.2 , and the fact that the converged models outperform the state-of-the-art in Section 4.3 . We would like to point out that these types of bi-level optimization algorithms are known to converge and also applied to other domains , such as Hyper-parameter optimization [ 4 ] . -- -- -- -- -- - To recap : * the proposed surrogate loss model is the methodologically-correct approach to handle the permutation-invariant nature of the true loss , * theoretical guarantees are inherited by the properties of the well-studied Kolmogorov-Arnold representation theorem . Please let us know if our clarification does not fully address your concerns ? [ 1 ] Zaheer et al. , Deep Sets , NIPS 2017 [ 2 ] Montanelli et al. , Error bounds for deep ReLU networks using the Kolmogorov\u2013Arnold superposition theorem , ArXiv 2019 , https : //arxiv.org/pdf/1906.11945.pdf [ 3 ] Zhang , Kolmogorov \u2019 s Superposition Theorem , 2006 , https : //www.maths.ed.ac.uk/~xzhang/files/oct_2016_xiling.pdf [ 4 ] Franceschi et al. , Bilevel Programming for Hyperparameter Optimization and Meta-Learning , ICML 2018"}, {"review_id": "BkePHaVKwS-2", "review_text": "In this paper, the authors propose to learn surrogate loss functions for non-differentiable and non-decomposable loss. An alternative minimization method is used for training the surrogate network and prediction model. Learning surrogate loss functions for different tasks is somewhat novel, although there are some prior works on learning the loss, e.g., [1]. Pros. 1. The paper is well written and easy to follow. 2. Learning a unified loss for different tasks is interesting. It has the potential to reduce human efforts to design losses. Cons. 1. Some important details of learning the surrogate loss are missing. The function g for extracting the latent error component is not clear. The composition function h is not provided either. 2. The details of the experiments are not clear. Are the results on the test set? How many independent runs are performed? 3. Learning surrogate loss incurs additional approximation error, time complexity, and model complexity. The benefit of the trade-off is not systematically evaluated. Questions 1. The function g for extracting the latent error component is not clear. Is it required to design for different tasks by an expert specifically? Or, does it have the same differentiable form for different tasks? Please provide the details of the function g for the different losses in the experiments. 2. Can the proposed loss work well for multi-class classification tasks? In the experiments, only binary classification is evaluated. Multi-class classification will increase the number of classes, thus increasing the difficulty of approximation. It is better to provide MCR compared with CE on the CIFAR100 dataset for evaluation. Also, please provide the running time of CE and SL-R in the same running environment. 3. Is the results in Table 3 on the test set? CE has fewer parameters compared with the proposed loss, why does SL-R have better generalization performance compared with CE? How many independent runs performed in experiments? 4. CE does not need training the loss compared with SL-R. Please provide the running time of CE in the same environment for a fair comparison. 5. The time complexity analysis treats extracting function g as a black-box function. However, the complexity of function g depends on the tasks. Please provide a detailed discussion about time complexity for different tasks (e.g., AUC, F1, MCR for multi-class classification, and ranking tasks). [1] Learning Loss Functions for Semi-supervised Learning via Discriminative Adversarial Network, 2017 ", "rating": "3: Weak Reject", "reply_text": "Thanks a lot for your constructive comments . I would like to address each concern in the same order . First of all thanks for reference [ 1 ] , we will cite and address it . Regarding Cons . : 1.The function g and h are specified in Section 3 , the fourth paragraph of page 3 to be deep-forward networks ( MLP ) . The architectures for g and h are detailed in Section 4.1 . Notice that the architecture of $ g $ & $ h $ is the same for all losses , but the weights of $ g $ & $ h $ are trained specifically for each loss $ \\ell $ . The function $ g $ and $ h $ are represented by the parameters $ \\beta $ in Equations 4-5 and Algorithm 1 . Our method automatically fits the surrogate to each task via an end-to-end optimization . 2.The reported results are measured on the test set following the protocol of Section 4.1 . 3.The benefit of our method is an end-to-end surrogate loss , whose optimization improves the state-of-the-art in terms of multiple losses ( Section 4.3 ) . The only disadvantage is an added run-time component in computing the gradient of the loss with respect to the estimated target . Please refer to Section 4.4. for additional run-time analysis . Regarding Questions : 1 . The details for the functions g and h are already provided in Section 3 and Section 4.1 as MLP networks . No expert hand-crafting of those networks are needed , on the contrary , the process is an end-to-end learning approach conducted via Algorithm 1 . We showed in Section 4 that a common architecture design for g and h is sufficient to advance the state-of-the-art in terms of popular losses , such as AUC , F1 , Jaccard , etc . 2.Please notice that we already experimented with 7 different non-differentiable measures and 9 datasets . In our review , no other published paper has actually experimented with a larger number of non-differentiable losses . On the other hand , we wanted to exclusively focus on binary classification first in order to systematically and exhaustively analyze this setup , then afterwards extend the work with future papers in different areas , e.g.losses for recommender systems ( MAP , Hit-rate @ K ) , computer vision ( top-k multi-class accuracy , IoU bounding box ) , or language translation ( BLEU ) . However , we believe binary classification losses are valuable to the Machine Learning community with implications in imbalanced classification ( AUC , F1 ) , biometric verification ( Equal Error Rate ) , etc , which justifies the experimental setup . 3.The results are presented on the test set following the protocol of Section 4.1 . The advantage of our approach compared to surrogates like CE is that a parametric surrogate fits the true loss better in the loss regions dictated by the current prediction model . In other words , the surrogate loss params beta are fitted for the actual alpha in a dataset-specific manner ( see Section 3.2 , Equation 5 ) , while non-parametric surrogates are not tailored to dataset-specific relevant regions of the loss surface . 4.The question on the runtime figures is answered in a separate message . 5.Please notice that function g is not treated as a black-box in the runtime analysis . The weights of g and h are represented by the parameters vector $ \\beta $ whose dimensionality is defined as $ Q_\\beta $ and is used in the asymptotic order . On the other hand , we agree that the complexity of a surrogate is dependent on the target loss . In fact , we used the same architecture of g and h for all target losses ( AUC , F1 , MCR , etc . ) and did not search the architectures on a per-loss basis , in order to keep the experiments feasible under our computational resources . However , it turned out that a common surrogate network architecture was actually sufficient to outperform the state-of-the-art surrogates of all target losses ( Section 4.3 ) . After further thinking we concluded that it causes no harm to have a common deep surrogate loss network with a sufficiently high capacity for all target losses , since in the worst case , a very large capacity surrogate will fit the non-differential losses better and might have steeper loss surfaces than a small capacity surrogate around the step-like surfaces of the true loss ( see Figure 1 for the step-like loss surfaces ) . However , if one uses gradient clipping as we do , the gradients of the surrogate loss wrt the estimated target variable will not explode at the steep loss surface regions of the high capacity surrogate loss surface . In the end , the very good empirical results compared to the state-of-the-art supported our hypothesis . Therefore , given a common g and h architecture , the time complexity in our setup remains the same independent of the target loss . As a result of your helpful comments , we will update the paper accordingly to emphasize the answers to the raised questions . Please , let me know if something is not clear , or any further input is needed ."}], "0": {"review_id": "BkePHaVKwS-0", "review_text": "In this paper authors propose to jointly optimize a critic, that estimates some non-differentiable objective (or an objective with intractable derivatives and/or derivatives that are 0 almost everywhere). Authors conduct numerous experiments, and show improvements over some sota methods, and maybe more importantly - provide a unified way of achieving these across multiple metrics. Paper is well written, very easy to understand and in reviewers opinion, a nice, simple story worth sharing. Major concerns: The theoretical result provided is just a citation of an existing proof, rather than a new contribution. More importantly however, the setup considered strongly violates the assumptions of the theorem, for example assumption (b) does not hold for the neural networks, as there are continuum many global minima (imagine having a unit in a neural net, that has a huge negative bias, which causes it to always be \"turned off\" by relu, then any weight on top of it will have the same final loss value; or simply notice that relu(a*x)=a*relu(x) if x>0, and so you can always \"push\" the norm of weights from one layer to the next one without affecting the outcome etc.), and so argmin is not a singleton, but the set of exactly same power as the whole parameter space (since for each k there exists a bijection from R^k to R, and R is of continuum power). This property is not a mild assumption, but rather a critical element guaranteeing convergence of such systems, with powerful universal approximators, one cannot hope for such strong convergence results without actually analysing the approximators family. Reviewer strongly believes this should be either removed completely, or just briefly mentioned, rather than made a strong statement in the paper. It is an empirical work, and stands strong on its own rights, there is no need to add theorems with assumptions that are never satisfied in the proposed scenario. Results provided use somewhat non-standard datasets for deep learning, and as such it is hard to asses statistical significance of the differences reported; while the test datasets sizes are big enough to trust error to 1e-3 level, they are heavily imbalanced and relatively low dimensional problems, which have proven to be hard for neural network many times in the past, consequently I would expect to see confidence intervals or stds of each result, at least for Table 3. The problem becomes even more severe for metrics such as F1 which do not decompose additively and so can be very sensitive to the (false) positive rates values - reported improvements might disappear once these are introduced, but even if one does not outperform hand-crafted proxies for specific objectives, having a unified method that is on-part with those in black-box scenario is a good result (I would argue that even if all the results are slightly worse, it is still worth publishing). I find it a bit disappointing, that authors did not try to analyse trained models, it would be invaluable to see what kind of aggregation g and h came up with, that is well aligned with losses such as F1 or JAC. Minor concerns: The work resembles closely methods of error critic learning - where one uses an update direction coming from a model, that regresses towards the loss itself, or the gradient of the loss (is available), see: - Neural Network Design for J Function Approximation in Dynamic Programming (Pang and Werbos paper from '98) - Sobolev Training of Neural Networks (from NIPS; more precisely \"Critic\" baseline, which has the same functional form as the loss presented in the paper, if applied at the very top of the network only) It might be worth discussing in the paper. Overall I recommend weak acceptance, and encourage authors to address some of the concerns raised above. **Update** Based on discussion, corrections and new additions, I recommend acceptance of the above work.", "rating": "8: Accept", "reply_text": "Thanks a lot for reading the paper in-depth and providing valuable criticism . Regarding the major concerns : * We agree with your reasoning that the singleton assumption is too strong , therefore we accept the criticism . Your suggestion to shorten the convergence note will be taken into consideration . As a result , we will reduce that section to a few sentences in the rebuttal to make room for addressing the comments of the reviewers . * Related to the choice of datasets , we tried to blend a mix of UCI datasets with more modern datasets from Kaggle . Please notice that the imbalance of the datasets is actually a desirable property rather than a bottleneck because in such datasets it more suitable to assess the performance of metrics such as AUC , F1 , etc .. On the other hand , the imbalance is obviously not a facultative option , but rather a real-life challenge . Since the experimental protocol was conducted on a random 80 % -20 % split , the confidence intervals are not defined . Yet , we agree that the size of the test sets is large enough to offer trustable empirical evidence . * On the other hand , you are right in stating that optimizing directly for F1 or AUC in such imbalanced datasets gives a lift if we would have compared against balance-sensitive decomposable losses such as cross-entropy . However , please notice that we actually compared and outperformed the state-of-the-art baselines of Section 4.3 which also directly optimize for these imbalance-handling losses ( such as AUC , F1 , Jaccard ) . * Yet again , we agree that the interpretability of the surrogate losses would be nice to have . Unfortunately , it is not trivial to analyze how the Q=30 latent per-instance error components of function g directly relate to , for example , the four confusion matrix measures of F1 . Even if we force Q=4 there is no guarantee that the latent error components would directly match the confusion matrix , since the multi-layered aggregation network h combines the components of g in a black-box ( a.k.a.neural network-ish ) manner , and not in the same way that common losses ( such as F1 ) do . Perhaps one can extend this work by a multi-target surrogate optimization where we set Q=4 and at jointly i ) train the output of g to estimate the confusion matrix terms , and ii ) train the output of h to match the final metric ( e.g.F1 , Jaccard ) . The disadvantage of this approach is that it would not be directly applicable to AUC , because we have multiple confusion matrices for the various thresholds used to binarize the estimated target . As a result , it actually seems not straightforward to analyze the black-box surrogates and interpret their neural networks . Regarding Minor concerns : The relation you pointed out to error critic learning seems relevant and we will incorporate it into the related work . However , I failed to see the relation to Sobolev training , In our case , there is no information on the gradients of the target loss with respect to the activations of the different layers of the surrogate model , since in contrast to the assumptions of the NIPS paper our target loss function has no differentiable form ."}, "1": {"review_id": "BkePHaVKwS-1", "review_text": "This paper proposes a method of learning loss functions in addition to the learning of predictors. Since it's not easy to optimize loss functions that evaluate the accuracy, surrogate loss functions have been widely employed. The design of the surrogate loss is problem-dependent, and handcraft is required. This paper tries to tackle this problem from the viewpoint of meta-learning, i.e., the surrogate loss learning. Typically, deep neural networks (DNN) are used to design a surrogate loss that approximates the original loss while maintaining the tractability of the optimization. Some convergence properties of the proposed method are analyzed. Some empirical studies showed the efficiency of the proposed method to the state-of-the-art baselines. The design of the surrogate loss is important for machine learning problems. However, the proposed method in this paper seems an ad-hoc approach rather. For example, the 0-1 loss is often replaced with convex loss functions such as the hinge loss or logistic loss. Using these surrogate loss functions, the statistical properties of the predictors obtained from 0-1 loss are maintained. See the following paper for details. P. L. Bartlett, et al., (2006), Convexity, Classification, and Risk Bounds, Journal of the American Statistical Association March , Vol. 101, No. 473. On the other hand, the current approach does not have such a theoretical guarantee for each learning problems. Though certainly, the proposed method is widely applicable to many problems, there is no theoretical guarantee. Theorem 1 in page 5 shows the convergence property. However, the number of iterations, K_beta, should tend to infinity. This is not a practical operation in the learning algorithm", "rating": "3: Weak Reject", "reply_text": "Thanks for the important point you raised . We would like to address both aspects of your review : a ) The proposed surrogate loss model is ad-hoc b ) There is no theoretic guarantee of the surrogate loss in approximating the true loss Regarding point a ) Please let me remind that given the true targets $ y \\in \\mathbb { R } ^N $ of a batch having $ N $ instances and the estimated targets $ \\hat y \\in \\mathbb { R } ^N $ of that batch , then a loss function is defined as the true loss $ \\ell ( y , \\hat y ) : \\mathbb { R } ^ { 2N } \\rightarrow \\mathbb { R } $ . The purpose of a surrogate $ \\hat \\ell ( y , \\hat y ) $ is , therefore , to approximate well such a true loss $ \\ell ( y , \\hat y ) $ . The first naive intuition would be to define $ \\hat \\ell ( y , \\hat y ) $ as a plain MLP neural network , which by virtue of the universal approximation theorem is guaranteed to estimate $ \\ell ( y , \\hat y ) $ . However , further thinking reveals that $ \\hat \\ell $ can not be a plain neural network with $ 2N $ inputs and $ 1 $ output , because of the permutation-invariant treat of the true losses . Notice that the loss output should remain the same even if the order of the instances within a batch is shuffled ( a.k.a.permutation-invariant ) . A neural network , on the other hand , does not ensure that the output remains the same if the order of features is swapped . Given that loss functions are permutation-invariant , we came up with the intuition that a loss is actually a mathematical function that operates over sets . In this case the sets of $ ( y , \\hat y ) \\in \\mathbb { R } ^2 $ values for a set of batch instances . The set formulation of the loss handles the permutation-invariant nature of randomly shuffling instances within a batch . A recent work [ 1 ] has actually proven that functions expressed via the Kolmogorov-Arnold representation theorem are permutation-invariant . As a result , the surrogate loss we propose in Equation 2 is actually the methodologically-correct surrogate model to approximate the permutation-invariant true loss functions . Regarding point b ) We would like to clarify that the Kolmogorov-Arnold representation is well-known to offer theoretical guarantees in terms of approximating the target function [ 3 ] . Even in the concrete case where the functions $ g $ and $ h $ of Equation 2 are ReLU networks ( as in our paper ) , the respective error bounds exist and are derived by prior literature [ 2 ] . As a result , since our surrogate network is an instance of the standard Kolmogorov-Arnold representation , it automatically benefits from the known aforementioned theoretical guarantees of the Kolmogorov-Arnold representation theorem . We mentioned this aspect briefly in Section 3 , however , thanks to your comment we will expand the clarification with a paragraph on its own for the rebuttal . -- -- -- -- -- - Regarding the comment on $ K_\\beta $ tending to infinity in the optimization algorithm , please notice that in practice the models converge with a small $ K_\\beta=10 $ ( Section 4.1 ) . We can actually observe the convergence in Figure 3 of Section 4.2 , and the fact that the converged models outperform the state-of-the-art in Section 4.3 . We would like to point out that these types of bi-level optimization algorithms are known to converge and also applied to other domains , such as Hyper-parameter optimization [ 4 ] . -- -- -- -- -- - To recap : * the proposed surrogate loss model is the methodologically-correct approach to handle the permutation-invariant nature of the true loss , * theoretical guarantees are inherited by the properties of the well-studied Kolmogorov-Arnold representation theorem . Please let us know if our clarification does not fully address your concerns ? [ 1 ] Zaheer et al. , Deep Sets , NIPS 2017 [ 2 ] Montanelli et al. , Error bounds for deep ReLU networks using the Kolmogorov\u2013Arnold superposition theorem , ArXiv 2019 , https : //arxiv.org/pdf/1906.11945.pdf [ 3 ] Zhang , Kolmogorov \u2019 s Superposition Theorem , 2006 , https : //www.maths.ed.ac.uk/~xzhang/files/oct_2016_xiling.pdf [ 4 ] Franceschi et al. , Bilevel Programming for Hyperparameter Optimization and Meta-Learning , ICML 2018"}, "2": {"review_id": "BkePHaVKwS-2", "review_text": "In this paper, the authors propose to learn surrogate loss functions for non-differentiable and non-decomposable loss. An alternative minimization method is used for training the surrogate network and prediction model. Learning surrogate loss functions for different tasks is somewhat novel, although there are some prior works on learning the loss, e.g., [1]. Pros. 1. The paper is well written and easy to follow. 2. Learning a unified loss for different tasks is interesting. It has the potential to reduce human efforts to design losses. Cons. 1. Some important details of learning the surrogate loss are missing. The function g for extracting the latent error component is not clear. The composition function h is not provided either. 2. The details of the experiments are not clear. Are the results on the test set? How many independent runs are performed? 3. Learning surrogate loss incurs additional approximation error, time complexity, and model complexity. The benefit of the trade-off is not systematically evaluated. Questions 1. The function g for extracting the latent error component is not clear. Is it required to design for different tasks by an expert specifically? Or, does it have the same differentiable form for different tasks? Please provide the details of the function g for the different losses in the experiments. 2. Can the proposed loss work well for multi-class classification tasks? In the experiments, only binary classification is evaluated. Multi-class classification will increase the number of classes, thus increasing the difficulty of approximation. It is better to provide MCR compared with CE on the CIFAR100 dataset for evaluation. Also, please provide the running time of CE and SL-R in the same running environment. 3. Is the results in Table 3 on the test set? CE has fewer parameters compared with the proposed loss, why does SL-R have better generalization performance compared with CE? How many independent runs performed in experiments? 4. CE does not need training the loss compared with SL-R. Please provide the running time of CE in the same environment for a fair comparison. 5. The time complexity analysis treats extracting function g as a black-box function. However, the complexity of function g depends on the tasks. Please provide a detailed discussion about time complexity for different tasks (e.g., AUC, F1, MCR for multi-class classification, and ranking tasks). [1] Learning Loss Functions for Semi-supervised Learning via Discriminative Adversarial Network, 2017 ", "rating": "3: Weak Reject", "reply_text": "Thanks a lot for your constructive comments . I would like to address each concern in the same order . First of all thanks for reference [ 1 ] , we will cite and address it . Regarding Cons . : 1.The function g and h are specified in Section 3 , the fourth paragraph of page 3 to be deep-forward networks ( MLP ) . The architectures for g and h are detailed in Section 4.1 . Notice that the architecture of $ g $ & $ h $ is the same for all losses , but the weights of $ g $ & $ h $ are trained specifically for each loss $ \\ell $ . The function $ g $ and $ h $ are represented by the parameters $ \\beta $ in Equations 4-5 and Algorithm 1 . Our method automatically fits the surrogate to each task via an end-to-end optimization . 2.The reported results are measured on the test set following the protocol of Section 4.1 . 3.The benefit of our method is an end-to-end surrogate loss , whose optimization improves the state-of-the-art in terms of multiple losses ( Section 4.3 ) . The only disadvantage is an added run-time component in computing the gradient of the loss with respect to the estimated target . Please refer to Section 4.4. for additional run-time analysis . Regarding Questions : 1 . The details for the functions g and h are already provided in Section 3 and Section 4.1 as MLP networks . No expert hand-crafting of those networks are needed , on the contrary , the process is an end-to-end learning approach conducted via Algorithm 1 . We showed in Section 4 that a common architecture design for g and h is sufficient to advance the state-of-the-art in terms of popular losses , such as AUC , F1 , Jaccard , etc . 2.Please notice that we already experimented with 7 different non-differentiable measures and 9 datasets . In our review , no other published paper has actually experimented with a larger number of non-differentiable losses . On the other hand , we wanted to exclusively focus on binary classification first in order to systematically and exhaustively analyze this setup , then afterwards extend the work with future papers in different areas , e.g.losses for recommender systems ( MAP , Hit-rate @ K ) , computer vision ( top-k multi-class accuracy , IoU bounding box ) , or language translation ( BLEU ) . However , we believe binary classification losses are valuable to the Machine Learning community with implications in imbalanced classification ( AUC , F1 ) , biometric verification ( Equal Error Rate ) , etc , which justifies the experimental setup . 3.The results are presented on the test set following the protocol of Section 4.1 . The advantage of our approach compared to surrogates like CE is that a parametric surrogate fits the true loss better in the loss regions dictated by the current prediction model . In other words , the surrogate loss params beta are fitted for the actual alpha in a dataset-specific manner ( see Section 3.2 , Equation 5 ) , while non-parametric surrogates are not tailored to dataset-specific relevant regions of the loss surface . 4.The question on the runtime figures is answered in a separate message . 5.Please notice that function g is not treated as a black-box in the runtime analysis . The weights of g and h are represented by the parameters vector $ \\beta $ whose dimensionality is defined as $ Q_\\beta $ and is used in the asymptotic order . On the other hand , we agree that the complexity of a surrogate is dependent on the target loss . In fact , we used the same architecture of g and h for all target losses ( AUC , F1 , MCR , etc . ) and did not search the architectures on a per-loss basis , in order to keep the experiments feasible under our computational resources . However , it turned out that a common surrogate network architecture was actually sufficient to outperform the state-of-the-art surrogates of all target losses ( Section 4.3 ) . After further thinking we concluded that it causes no harm to have a common deep surrogate loss network with a sufficiently high capacity for all target losses , since in the worst case , a very large capacity surrogate will fit the non-differential losses better and might have steeper loss surfaces than a small capacity surrogate around the step-like surfaces of the true loss ( see Figure 1 for the step-like loss surfaces ) . However , if one uses gradient clipping as we do , the gradients of the surrogate loss wrt the estimated target variable will not explode at the steep loss surface regions of the high capacity surrogate loss surface . In the end , the very good empirical results compared to the state-of-the-art supported our hypothesis . Therefore , given a common g and h architecture , the time complexity in our setup remains the same independent of the target loss . As a result of your helpful comments , we will update the paper accordingly to emphasize the answers to the raised questions . Please , let me know if something is not clear , or any further input is needed ."}}