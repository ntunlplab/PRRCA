{"year": "2021", "forum": "1OCTOShAmqB", "title": "On the Dynamics of Training Attention Models", "decision": "Accept (Poster)", "meta_review": "This paper investigates the training dynamics of simple neural attention\nmechanisms, in a controlled setting with clear (but rather strict) assumptions. Some reviewers\nexpressed caution about the applicability of the assumptions in practice,\nbut nevertheless there is agreement that the results deepen our understanding and enrich\nour toolkit for reasoning about attention.\nIn support of this, in the discussion period, it was emphasized that the work uses different techniques than\nmost current work in this direction. I am therefore confident that the paper will be useful, and recommend acceptance.\n\nI strongly encourage the authors to improve the clarity of the work and thorough\ncitation, as suggested by the reviewers.", "reviews": [{"review_id": "1OCTOShAmqB-0", "review_text": "This paper provides theoretical insight into the mechanisms by which a simplified attention model trained with gradient descent learns to allocate more mass to relevant words in the input . In a limited toy setting , the authors derive a closed form relationship between word-score and word embedding norm in a simplified , one layer attention model . The theoretical findings are verified empirically both on the toy task and on a more realistic sentiment classification benchmark . Due to the extreme simplicity of the setting considered , as well as the number of assumptions made , it is unclear to me what to make of these results . In particular , it seems that the setting considered ( fixed query attention over bag of word embeddings ) is very different from real use cases of attention . * * Pros * * - The closed-form relationship between attention score and embedding norm during SGD training is novel as far as I know - The theoretical results are well justified in experiments : in particular the predicted `` SEN '' relationship seems to match the prediction . * * Cons * * - Large number of assumptions , the validity of which is unclear in practice : in particular 1 . The assumption that the query vector is ( 1 ) a parameter and not a function of the inputs ( as in self attention or cross-sentence attention ) and ( 2 ) is fixed . I do n't know of many `` real world '' attention networks that work this way , after all one of the main appeals of attention is its `` content-based '' nature 2 . Assumption 1 that the score and embeddings of non-topic words do n't change during training . First , this seems like something that could be proven from the earlier assumption that the topic words are updated more frequently . And second it is unclear if it holds for a real task ( and a different model where eg.the attention layer attends to higher layers rather than just the embeddings ) - Confusing notation makes the paper hard to follow ( see remarks for examples ) - Unclear takeaway : what does this paper tell us about attention as it is used in practice ? * * Remarks * * - 5.1 : `` The \u201c negative effect \u201d can not last long because the topic word embedding moves along the gradient much faster than the non-topic words due to its high occurrence rate '' : This is true in the toy example in the paper , but is this the case in practice ? For instance in sentiment classifications there are many words to describe sentiment that are infrequent ( cue Zipf 's law ) . Moreover , in realistic settings there will be non-topic words which appear very frequently ( stop words such as `` the '' , `` a '' in English ) . - Lemma 1 : while it is true that fixing q does n't change the capacity of the model , it will definitely change its training dynamics ( which is very much the theme of the paper as per the title ) . How important is it to fix q from this perspective ? - A lot of the math would be easier to read if the dependence of some variables ( \\hat v , Z , ... ) on a specific sentence was marked explicitly ( eg.Z_S instead of Z ) - The notation in Lemma 2 was extremely confusing to me , due to the sudden introduction of the bracket notation and the awkward spacing with both equations on the same line . I would recommend at least putting both on separate line , and also reorganizing so that the LHS of the second equation is only ds_i/dt ( move the mean to the other side ) - In 3 . I think using `` \\mathbf R '' for the dictionary is unfortunate ( too similar to \\mathbb R ) . Overall I found the separation between topic and non-topic words dictionaries confusing . Why not have a global Vocabulary V , a set of topic words T and refer to the remaining words as V\\T ? - In 2 . `` [ Hahn and Brunner ] show theoretically that the self-attention blocks are severely limited in their computational capabilities when the input sequence is long , which implies that self-attention may not provide the interpretability that one expects . `` : can you clarify this sentence ? Limitation in computational capabilities does not seem to entail limited interpretability in general ( see linear models for instance ) . - Typo in citations in 2. : `` Hahn ( Hahn , 2020 ) and Brunner ( Brunner et al. , 2019 ) '' - > `` Hahn ( 2020 ) and Brunner et al . ( 2019 ) '' - Typo in 3 . `` The training set [ ... ] are '' - > `` The training set [ ... ] is '' * * Post Rebuttal * * In my review , the main concerns were ( 1 ) validity of assumptions , ( 2 ) confusing writing/notation and ( 3 ) unclear takeaway . The rebuttal appropriately addressed ( 1 ) , although I am looking forward to the revision to see how this is discussed in the paper itself . I can not really say anything about any improvements on the writing ( 2 ) without seeing the revision , but I am confident that the authors can address most of the issues pointed out by myself and other reviewers . Regarding ( 3 ) , unclear takeaway , after reading the authors ' response as well as the other reviews , my concerns are somewhat assuaged ( partly because the assumptions were addressed better ) , although I am still unsure how or if the results in this paper could be expanded to realistic attention models . There are additional issues I raised during the discussion ( general lack of citations in particular ) , however this can be fixed fairly easily for the camera ready so I am willing to give the benefit of doubt and raise my score to 6 ( borderline accept )", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your reviews ! We have carefully gone through them , and here are our responses , explanations and additional results . - * Large number of assumptions , the validity of which is unclear in practice : in particular * 1 . * The assumption that the query vector * ( 1 ) * is a parameter and not a function of the inputs ( as in self attention or cross-sentence attention ) and ( 2 ) is fixed . * * * Response * * : Regarding your complaint that the query in our setting is not a function of the input , we agree that in many modern attention-based models ( e.g , transformers ) , this is not the case . But in the simple setting of our studied task , there is no benefit to consider the query dependent of the input sentence . This is because this task boils down extracting topic words from each sentence and such extraction requires no context information due to the simplicity of the problem set up . We agree that the next step of this research should scale up the task complexity so that the tasks are closer to real-world applications . Having said this , in earlier attention-based models , indeed there exist abundant models ( mostly for simpler tasks like text classification ) in which query is chosen as a parameter independent of the input sentence , see , e.g. , Wang et al , \u201c Attention-based LSTM for Aspect-level Sentiment Classification \u201d , ACL 2016 . ( Google Scholar Citations 830 ) . Regarding your complaint that the query is made fixed and non-trainable , we first note that this choice allows the derivation of a clean closed-form `` SEN '' relation . But it is worth emphasizing that the positive relationship between the score and the embedding norm in fact exists regardless of the trainability of the query . This can be seen as follows . There are two correlated sources of gradient signals , one back-propagates from the score to update key and query , the other backpropagates from the classifier loss to update word embedding . This correlation governs the positive relationship between score and embedding norm . Whether the query is trainable does not alter the existence of this correlation , although trainable query makes the analysis more difficult . In particular , when the query norm is large , the update of query is relatively negligible ; as a consequence , the training behavior is similar to having a fixed query . To support our claims , we have provided extra experimental results in Section C.1 . In summary , the conclusion of this paper is valid even for the case when the query is trainable . 2 . * Assumption 1 that the score and embeddings of non-topic words do n't change during training . * - * First , this seems like something that could be proven from the earlier assumption that the topic words are updated more frequently . * * * Response * * : Yes . It is possible to show analytically that the score and embedding for topic words are NEARLY unchanged during training . This justifies Assumption 1 . But in Assumption 1 , the word \u201c nearly \u201d is removed , hence we call it an assumption . - And second it is unclear if it holds for a real task ( and a different model where eg.the attention layer attends to higher layers rather than just the embeddings ) * * * Response * * : There should be a much larger family of models and tasks in which the assumption holds . To support this claim , we have provided more experimental results in Section C.2 . In particular , we have discussed a variant of the Attn-TC model ( named Attn-TC-CNN ) that contains a CNN layer to preprocess the word embeddings and the keys before feeding them into the attention block . The empirical results show that Assumption 1 still largely holds in this case . Regarding the assumption 's validity in a real task , it is very hard to check it directly as the true topic words are unknown and the tasks may possess a more complex structure . Nonetheless , as we have presented in Section 5.2 , the theoretical SEN curves on real datasets largely coincide with the empirical counterparts . This to an extent supports the assumption , which we used to derive the expression of the theoretical SEN curve ."}, {"review_id": "1OCTOShAmqB-1", "review_text": "Summary : This paper aims to prove and illustrate that attention components are defined during training by gradients that mutually amplify the embedding and score associated with crucial features . In particular , a word embedding with a high magnitude increases the gradient following the attention score for the same word , while a high attention score increases the gradient directed at the word 's embedding . In addition to a proof that treats behavior during training as a dynamical system under a large suite of assumptions , they test the analytic predictions on a synthetic dataset following the same suite of assumptions . They then test on a natural language data set and discuss where it diverges from the analytic and synthetic findings , concluding that the difference is a result of competition between different words associated with a label . Pros : 1.We currently lack any substantial theory about attention modules and why they work . Although their model is simplistic , it could provide essential groundwork for analytic understanding of these popular systems . I would even consider it fairly realistic relative to a lot of the assumptions required for theoretical results in training dynamics research . Currently theory of attention is grounded in infinite-width networks , an assumption this paper does not make . 2.The synthetic results appear to substantiate this theoretical result . 3.They find an interesting result that , in more realistic settings , the learning dynamics follow particular patterns on the words that are paired together with more versus less predictive words . The framing of these effects in terms of competition between possible topic words is clearly inspired by considering which assumptions behind their proof have failed , which is evidence that the thinking behind their proof is potentially valuable . Cons : 1.It 's not clear how dynamics like these would generalize to multilayer attention networks like BERT . 2.The assumptions behind the theoretical and synthetic empirical results are simplistic : The existence of a large vocabulary of `` non-topic '' words required to keep the variance of embedding negligible in out of focus words , the presence of only one topic word . There is also the very common assumption of Lipschltz continuity . 3.The natural language experiments make a specific claim about the different dynamics for competing words of different topic purity , but only presents an example of two words as evidence . I want to see quantitative evidence of the pattern . 4.The synthetic results would be strengthened by including multiple runs with different initializations so they can include confidence intervals . Questions : 1 . Does this mutual amplification effect have any ramifications for the debate over whether attention weights can be used as a proxy for saliency ? 2.In Lemma 1 , there is a reference to the attention block 's capacity which is difficult to decipher . What do you mean here by capacity ? 3.The assumption that word embeddings are sampled from a distribution with small variance seems likely to apply early in training , but not later . Have you checked the actual variance that would be associated with word embeddings late in training ? 4.What is actually meant by a word being `` paired '' with another word in the natural language experiments ? 5.Did I misunderstand something in interpreting gradients amplifying the embedding and score as directed towards v and k respectively in this simplified model ? Minor : 1.Notation is difficult to follow at times because several unrelated concepts use almost the same symbols : $ s_i $ indicates score , but $ S_i $ indicates a sentence ; $ \\tau $ indicates learning rate , but $ \\mathcal { T } $ ( which looks identical as a subscript ) indicates a set of sentences . 2.In discussing early alignment of attention to syntax , Clark et al.2019 was concurrent with https : //www.aclweb.org/anthology/P19-1580/", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank you for your time and constructive suggestions . We are particularly pleased that you appreciate research of such a style . We have carefully gone through all your comments and suggestions and have the following responses , explanations and modifications . - * It 's not clear how dynamics like these would generalize to multilayer attention networks like BERT . * * * Response * * : Our results are unlikely to generalize to BERT directly . The transformer model in BERT not only has multiple attention layers , but also having multiple queries in each layer . We are still in the process of understanding such single-layer multi-query self attention models . In fact , we have noticed some interesting behaviour of the model , which appears to depend on how word embedding dimension relates to the number of labels in the classification tasks . We are in the process of formulating this behaviour , in a separate work . - * The assumptions behind the theoretical and synthetic empirical results are simplistic : The existence of a large vocabulary of `` non-topic '' words required to keep the variance of embedding negligible in out of focus words , the presence of only one topic word . There is also the very common assumption of Lipschltz continuity . * * * Response * * : We agree that our setting is simple . Nonetheless , we consider the insights obtained in this work valuable . - * The natural language experiments make a specific claim about the different dynamics for competing words of different topic purity , but only presents an example of two words as evidence . I want to see quantitative evidence of the pattern . * * * Response * * : In fact , our paper has also provided the quantitative evidence in a macro-scale . Fig 6 demonstrates the changes in the topic purity and the occurrence rate of the most attended words . In this way , we see how the most attended words interact with the other less attended ones in a sentence and how the attended words are gradually replaced by those with higher topic purity but less occurrence rate . Considering that Fig 6 focuses on the sentence set that contains a specific word , we also provide extra results in Section C.3 , focusing on all the training sentences . We hope this addresses your concerns , and we are glad to offer extra results if needed . - * The synthetic results would be strengthened by including multiple runs with different initializations so they can include confidence intervals . * * * Response * * : We agree with your comments and have added a modified Fig 1 in Section C.4 . We decided not to add confidence intervals in Fig 2 because , in different experiments , neither the topic word embedding norms nor the scores have the same values , which makes the confidence interval not applicable . Instead , for each model , we repeated the experiments five times and plotted the experimental SEN curves of the same topic word ( see Fig 12 in Section C.4 . ) . We will use the modified figures to replace Fig 1 and Fig 2 in the revised paper . - * Does this mutual amplification effect have any ramifications for the debate over whether attention weights can be used as a proxy for saliency ? * * * Response * * : Yes . The mutual amplification effect implies that the word with a faster embedding elongation will be assigned with a higher weight . Also , a fast word embedding extension hints the training of the word accounts for most of the training loss drop in comparison to the others . Thus , if we think a word saliency is related to its capability of causing a faster training loss drop , then the attention assigns the weights based on the word 's importance and can be used as a proxy for saliency . - * In Lemma 1 , there is a reference to the attention block 's capacity which is difficult to decipher . What do you mean here by capacity ? * * * Response * * : When two models contain precisely the same family of hypotheses , we say they have the same capacity . - * The assumption that word embeddings are sampled from a distribution with small variance seems likely to apply early in training , but not later . Have you checked the actual variance that would be associated with word embeddings late in training ? * * * Response * * : Our analysis only requires that the non-topic word maintains the word embeddings of a small variance . In the experiment on the artificial data introduced in Section 5.1 , if the embedding is initialized with variance 0.01 , then the sample variances of all random word embeddings on a randomly selected dimension are : 0.01005027 , 0.01005502 , 0.01007887 , 0.01008423 , 0.01008448 at the 1K-th , 2K-th , ... , 5K-th epochs . We can observe that the variance increases at a very low and decreasing speed . So , the assumption is well held ."}, {"review_id": "1OCTOShAmqB-2", "review_text": "( Summary ) The paper investigates the dynamics of attention mechanism by configurating a controlled experiment on a simple topic classification task and training via gradient descent . Each random sentence in the training data is synthesized to include only one topic word among many . Then the authors try to find an intrinsic mechanism that triggers the attention model to discover the topic word and accelerates training via mutual promotion . They further experiment the evolution of models during optimization when no clear distinction between topic and non-topic words exist like in real data . ( Originality and Contribution ) The paper proposes an artificial topic classification task and shows a positive score-and-embedding-norm relationship for the topic words to which the model must attend to . The authors also show that attention mechanism is highly helpful when the classifier has only limited capacity . They also demonstrate mutual promotion effect that leads a faster dropping of training loss than the fixed score and fixed embeddings . This discovery sounds to be original and relevant contribution to the field . ( Strength and Weakness ) - Strength : Design and run novel controlled experiment . Extensive analysis . - Weakness : Too much notational overloading . Writing quality . ( Concerns , Questions , and Suggestions ) 1 ) It is unclear why $ M > > N $ implies that a topic word appears more frequently in the sentences than a non-topic word . Section 3 describes that each sentence consists of only one topic word , then combining with $ m $ non-topic words drawn uniformly at random . When the total number of topics $ N $ is much smaller than the size of non-topic word dictionary $ M $ , what increases frequency of topic word ? 2 ) Overall simplifying some notations and avoiding notational overloading would greatly increase the readability of the paper . 3 ) To reduce confusion , it would be great to change the iterator of summation for the partition function Z into $ \\sum_ { w \u2019 \\in S_k } $ rather than using the same $ w $ . 4 ) In Lemma 1 , assume $ q \\neq 0 $ - > $ q \\neq \\vec { 0 } $ .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your reviews ! We believe you have precisely captured the main concepts of our paper . We have carefully gone through them , and here are our responses and explanations . - It is unclear why $ M > > N $ implies that a topic word appears more frequently in the sentences than a non-topic word . Section 3 describes that each sentence consists of only one topic word , then combining with $ m $ non-topic words drawn uniformly at random . When the total number of topics $ N $ is much smaller than the size of non-topic word dictionary $ M $ , what increases frequency of topic word ? * * Response * * : We will use an example to clarify this point . For simplicity , assume we have $ N $ topics and each topic only has one topic word ( so we have $ N $ topic words in total ) . Consider the sentence generating process introduced in the paper . For a given topic word , the probability that the topic word appears in a sentence is $ 1/N $ , while the probability of a given non-topic word appearing in a sentence of length $ m $ is $ 1- ( ( M-1 ) /M ) ^m $ . As a result , for a fixed $ m $ and $ N $ , if $ M > > N $ , $ 1- ( ( M-1 ) /M ) ^m $ approaches to zero . Thus a topic word appears more frequently in a sentence than a non-topic word . - Other problems regarding notations and writing quality . * * Response * * : We will simplify our notations and make the manuscript flow better in the revised version ."}, {"review_id": "1OCTOShAmqB-3", "review_text": "This paper studies the dynamics of attention in a task of simplified topic modeling , over the course of training for a specific model , where the context vector is the sum over words in a sentence of their embedding weighted by the exponential of the dot-product their key embedding with a global query vector , normalized . Due to the simplification of the topic modeling problem ( two null-intersect sets of words : topic vs. non-topic ) , they consider the embeddings of the non-topic words to be fixed over the course of training for their theoretical analysis . The applicability of the theoretical result is close to zero , and a somewhat known property ( e.g.in word2vec , Mikolov et al.2013 ) .The experimental results include two parts . One on a tiny synthetic dataset that matches the simplified topic modeling problem and serves as illustration . The other is on SST2 and SST5 ( movie comments and ratings , sentiment analysis ) , where the results are poor ( obviously , as the model is simple ) , e.g.yielding 79.59 % on SST while the SOTA is 97.4 , and BERT base is at 91.2 . The analysis is interesting , but does not lead to new insights . For a simple analysis , the paper is at times hard to follow , and could benefit from more structure ( presenting `` what '' before `` how '' ) and better notation ( e.g.\\nu , v , $ v $ all attached to ( forms of the ) the context vector ) . Overall , the contribution does not seem sufficient enough for inclusion at ICLR . The paper could be a good fit for a workshop on topic modeling or attention-based models .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review ! We have carefully read them and here are our responses and explanations . - * The applicability of the theoretical result is close to zero , and a somewhat known property ( e.g.in word2vec , Mikolov et al.2013 ) .The experimental results include two parts . One on a tiny synthetic dataset that matches the simplified topic modeling problem and serves as illustration . The other is on SST2 and SST5 ( movie comments and ratings , sentiment analysis ) , where the results are poor ( obviously , as the model is simple ) , e.g.yielding 79.59 % on SST while the SOTA is 97.4 , and BERT base is at 91.2 . The analysis is interesting , but does not lead to new insights . * * * Response * * : We wish to note that the purpose of this work is not to produce SOTA results for specific NLP tasks . Rather our sole interest is to understand how attention works . Despite the nearly universal applicability of the attention module in neural net based language models , its working mechanism is poorly understood to date . In this work , our hope is to characterize the dynamics of training attention models , using a simplified task . Although research at such a fundamental level may appear somewhat distant from immediate applications , it contributes to developing concrete insights and deep understanding in neural networks , and potentially has a long term impact . Without such understanding and insights , neural networks or deep learning may remain as black boxes , and practitioners can only rely on heuristics in model design and alchemy-like hyper-parameter tuning in model training . For this reason , we can not agree that the applicability of this research is zero . It is unfortunate that our disagreement on this matter perhaps has merely reflected our distinct research taste from yours . - * For a simple analysis , the paper is at times hard to follow , and could benefit from more structure ( presenting `` what '' before `` how '' ) and better notation . * * * Response * * : We will make an effort to simplify our notations and make the manuscript flow better in the revised version ."}], "0": {"review_id": "1OCTOShAmqB-0", "review_text": "This paper provides theoretical insight into the mechanisms by which a simplified attention model trained with gradient descent learns to allocate more mass to relevant words in the input . In a limited toy setting , the authors derive a closed form relationship between word-score and word embedding norm in a simplified , one layer attention model . The theoretical findings are verified empirically both on the toy task and on a more realistic sentiment classification benchmark . Due to the extreme simplicity of the setting considered , as well as the number of assumptions made , it is unclear to me what to make of these results . In particular , it seems that the setting considered ( fixed query attention over bag of word embeddings ) is very different from real use cases of attention . * * Pros * * - The closed-form relationship between attention score and embedding norm during SGD training is novel as far as I know - The theoretical results are well justified in experiments : in particular the predicted `` SEN '' relationship seems to match the prediction . * * Cons * * - Large number of assumptions , the validity of which is unclear in practice : in particular 1 . The assumption that the query vector is ( 1 ) a parameter and not a function of the inputs ( as in self attention or cross-sentence attention ) and ( 2 ) is fixed . I do n't know of many `` real world '' attention networks that work this way , after all one of the main appeals of attention is its `` content-based '' nature 2 . Assumption 1 that the score and embeddings of non-topic words do n't change during training . First , this seems like something that could be proven from the earlier assumption that the topic words are updated more frequently . And second it is unclear if it holds for a real task ( and a different model where eg.the attention layer attends to higher layers rather than just the embeddings ) - Confusing notation makes the paper hard to follow ( see remarks for examples ) - Unclear takeaway : what does this paper tell us about attention as it is used in practice ? * * Remarks * * - 5.1 : `` The \u201c negative effect \u201d can not last long because the topic word embedding moves along the gradient much faster than the non-topic words due to its high occurrence rate '' : This is true in the toy example in the paper , but is this the case in practice ? For instance in sentiment classifications there are many words to describe sentiment that are infrequent ( cue Zipf 's law ) . Moreover , in realistic settings there will be non-topic words which appear very frequently ( stop words such as `` the '' , `` a '' in English ) . - Lemma 1 : while it is true that fixing q does n't change the capacity of the model , it will definitely change its training dynamics ( which is very much the theme of the paper as per the title ) . How important is it to fix q from this perspective ? - A lot of the math would be easier to read if the dependence of some variables ( \\hat v , Z , ... ) on a specific sentence was marked explicitly ( eg.Z_S instead of Z ) - The notation in Lemma 2 was extremely confusing to me , due to the sudden introduction of the bracket notation and the awkward spacing with both equations on the same line . I would recommend at least putting both on separate line , and also reorganizing so that the LHS of the second equation is only ds_i/dt ( move the mean to the other side ) - In 3 . I think using `` \\mathbf R '' for the dictionary is unfortunate ( too similar to \\mathbb R ) . Overall I found the separation between topic and non-topic words dictionaries confusing . Why not have a global Vocabulary V , a set of topic words T and refer to the remaining words as V\\T ? - In 2 . `` [ Hahn and Brunner ] show theoretically that the self-attention blocks are severely limited in their computational capabilities when the input sequence is long , which implies that self-attention may not provide the interpretability that one expects . `` : can you clarify this sentence ? Limitation in computational capabilities does not seem to entail limited interpretability in general ( see linear models for instance ) . - Typo in citations in 2. : `` Hahn ( Hahn , 2020 ) and Brunner ( Brunner et al. , 2019 ) '' - > `` Hahn ( 2020 ) and Brunner et al . ( 2019 ) '' - Typo in 3 . `` The training set [ ... ] are '' - > `` The training set [ ... ] is '' * * Post Rebuttal * * In my review , the main concerns were ( 1 ) validity of assumptions , ( 2 ) confusing writing/notation and ( 3 ) unclear takeaway . The rebuttal appropriately addressed ( 1 ) , although I am looking forward to the revision to see how this is discussed in the paper itself . I can not really say anything about any improvements on the writing ( 2 ) without seeing the revision , but I am confident that the authors can address most of the issues pointed out by myself and other reviewers . Regarding ( 3 ) , unclear takeaway , after reading the authors ' response as well as the other reviews , my concerns are somewhat assuaged ( partly because the assumptions were addressed better ) , although I am still unsure how or if the results in this paper could be expanded to realistic attention models . There are additional issues I raised during the discussion ( general lack of citations in particular ) , however this can be fixed fairly easily for the camera ready so I am willing to give the benefit of doubt and raise my score to 6 ( borderline accept )", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your reviews ! We have carefully gone through them , and here are our responses , explanations and additional results . - * Large number of assumptions , the validity of which is unclear in practice : in particular * 1 . * The assumption that the query vector * ( 1 ) * is a parameter and not a function of the inputs ( as in self attention or cross-sentence attention ) and ( 2 ) is fixed . * * * Response * * : Regarding your complaint that the query in our setting is not a function of the input , we agree that in many modern attention-based models ( e.g , transformers ) , this is not the case . But in the simple setting of our studied task , there is no benefit to consider the query dependent of the input sentence . This is because this task boils down extracting topic words from each sentence and such extraction requires no context information due to the simplicity of the problem set up . We agree that the next step of this research should scale up the task complexity so that the tasks are closer to real-world applications . Having said this , in earlier attention-based models , indeed there exist abundant models ( mostly for simpler tasks like text classification ) in which query is chosen as a parameter independent of the input sentence , see , e.g. , Wang et al , \u201c Attention-based LSTM for Aspect-level Sentiment Classification \u201d , ACL 2016 . ( Google Scholar Citations 830 ) . Regarding your complaint that the query is made fixed and non-trainable , we first note that this choice allows the derivation of a clean closed-form `` SEN '' relation . But it is worth emphasizing that the positive relationship between the score and the embedding norm in fact exists regardless of the trainability of the query . This can be seen as follows . There are two correlated sources of gradient signals , one back-propagates from the score to update key and query , the other backpropagates from the classifier loss to update word embedding . This correlation governs the positive relationship between score and embedding norm . Whether the query is trainable does not alter the existence of this correlation , although trainable query makes the analysis more difficult . In particular , when the query norm is large , the update of query is relatively negligible ; as a consequence , the training behavior is similar to having a fixed query . To support our claims , we have provided extra experimental results in Section C.1 . In summary , the conclusion of this paper is valid even for the case when the query is trainable . 2 . * Assumption 1 that the score and embeddings of non-topic words do n't change during training . * - * First , this seems like something that could be proven from the earlier assumption that the topic words are updated more frequently . * * * Response * * : Yes . It is possible to show analytically that the score and embedding for topic words are NEARLY unchanged during training . This justifies Assumption 1 . But in Assumption 1 , the word \u201c nearly \u201d is removed , hence we call it an assumption . - And second it is unclear if it holds for a real task ( and a different model where eg.the attention layer attends to higher layers rather than just the embeddings ) * * * Response * * : There should be a much larger family of models and tasks in which the assumption holds . To support this claim , we have provided more experimental results in Section C.2 . In particular , we have discussed a variant of the Attn-TC model ( named Attn-TC-CNN ) that contains a CNN layer to preprocess the word embeddings and the keys before feeding them into the attention block . The empirical results show that Assumption 1 still largely holds in this case . Regarding the assumption 's validity in a real task , it is very hard to check it directly as the true topic words are unknown and the tasks may possess a more complex structure . Nonetheless , as we have presented in Section 5.2 , the theoretical SEN curves on real datasets largely coincide with the empirical counterparts . This to an extent supports the assumption , which we used to derive the expression of the theoretical SEN curve ."}, "1": {"review_id": "1OCTOShAmqB-1", "review_text": "Summary : This paper aims to prove and illustrate that attention components are defined during training by gradients that mutually amplify the embedding and score associated with crucial features . In particular , a word embedding with a high magnitude increases the gradient following the attention score for the same word , while a high attention score increases the gradient directed at the word 's embedding . In addition to a proof that treats behavior during training as a dynamical system under a large suite of assumptions , they test the analytic predictions on a synthetic dataset following the same suite of assumptions . They then test on a natural language data set and discuss where it diverges from the analytic and synthetic findings , concluding that the difference is a result of competition between different words associated with a label . Pros : 1.We currently lack any substantial theory about attention modules and why they work . Although their model is simplistic , it could provide essential groundwork for analytic understanding of these popular systems . I would even consider it fairly realistic relative to a lot of the assumptions required for theoretical results in training dynamics research . Currently theory of attention is grounded in infinite-width networks , an assumption this paper does not make . 2.The synthetic results appear to substantiate this theoretical result . 3.They find an interesting result that , in more realistic settings , the learning dynamics follow particular patterns on the words that are paired together with more versus less predictive words . The framing of these effects in terms of competition between possible topic words is clearly inspired by considering which assumptions behind their proof have failed , which is evidence that the thinking behind their proof is potentially valuable . Cons : 1.It 's not clear how dynamics like these would generalize to multilayer attention networks like BERT . 2.The assumptions behind the theoretical and synthetic empirical results are simplistic : The existence of a large vocabulary of `` non-topic '' words required to keep the variance of embedding negligible in out of focus words , the presence of only one topic word . There is also the very common assumption of Lipschltz continuity . 3.The natural language experiments make a specific claim about the different dynamics for competing words of different topic purity , but only presents an example of two words as evidence . I want to see quantitative evidence of the pattern . 4.The synthetic results would be strengthened by including multiple runs with different initializations so they can include confidence intervals . Questions : 1 . Does this mutual amplification effect have any ramifications for the debate over whether attention weights can be used as a proxy for saliency ? 2.In Lemma 1 , there is a reference to the attention block 's capacity which is difficult to decipher . What do you mean here by capacity ? 3.The assumption that word embeddings are sampled from a distribution with small variance seems likely to apply early in training , but not later . Have you checked the actual variance that would be associated with word embeddings late in training ? 4.What is actually meant by a word being `` paired '' with another word in the natural language experiments ? 5.Did I misunderstand something in interpreting gradients amplifying the embedding and score as directed towards v and k respectively in this simplified model ? Minor : 1.Notation is difficult to follow at times because several unrelated concepts use almost the same symbols : $ s_i $ indicates score , but $ S_i $ indicates a sentence ; $ \\tau $ indicates learning rate , but $ \\mathcal { T } $ ( which looks identical as a subscript ) indicates a set of sentences . 2.In discussing early alignment of attention to syntax , Clark et al.2019 was concurrent with https : //www.aclweb.org/anthology/P19-1580/", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank you for your time and constructive suggestions . We are particularly pleased that you appreciate research of such a style . We have carefully gone through all your comments and suggestions and have the following responses , explanations and modifications . - * It 's not clear how dynamics like these would generalize to multilayer attention networks like BERT . * * * Response * * : Our results are unlikely to generalize to BERT directly . The transformer model in BERT not only has multiple attention layers , but also having multiple queries in each layer . We are still in the process of understanding such single-layer multi-query self attention models . In fact , we have noticed some interesting behaviour of the model , which appears to depend on how word embedding dimension relates to the number of labels in the classification tasks . We are in the process of formulating this behaviour , in a separate work . - * The assumptions behind the theoretical and synthetic empirical results are simplistic : The existence of a large vocabulary of `` non-topic '' words required to keep the variance of embedding negligible in out of focus words , the presence of only one topic word . There is also the very common assumption of Lipschltz continuity . * * * Response * * : We agree that our setting is simple . Nonetheless , we consider the insights obtained in this work valuable . - * The natural language experiments make a specific claim about the different dynamics for competing words of different topic purity , but only presents an example of two words as evidence . I want to see quantitative evidence of the pattern . * * * Response * * : In fact , our paper has also provided the quantitative evidence in a macro-scale . Fig 6 demonstrates the changes in the topic purity and the occurrence rate of the most attended words . In this way , we see how the most attended words interact with the other less attended ones in a sentence and how the attended words are gradually replaced by those with higher topic purity but less occurrence rate . Considering that Fig 6 focuses on the sentence set that contains a specific word , we also provide extra results in Section C.3 , focusing on all the training sentences . We hope this addresses your concerns , and we are glad to offer extra results if needed . - * The synthetic results would be strengthened by including multiple runs with different initializations so they can include confidence intervals . * * * Response * * : We agree with your comments and have added a modified Fig 1 in Section C.4 . We decided not to add confidence intervals in Fig 2 because , in different experiments , neither the topic word embedding norms nor the scores have the same values , which makes the confidence interval not applicable . Instead , for each model , we repeated the experiments five times and plotted the experimental SEN curves of the same topic word ( see Fig 12 in Section C.4 . ) . We will use the modified figures to replace Fig 1 and Fig 2 in the revised paper . - * Does this mutual amplification effect have any ramifications for the debate over whether attention weights can be used as a proxy for saliency ? * * * Response * * : Yes . The mutual amplification effect implies that the word with a faster embedding elongation will be assigned with a higher weight . Also , a fast word embedding extension hints the training of the word accounts for most of the training loss drop in comparison to the others . Thus , if we think a word saliency is related to its capability of causing a faster training loss drop , then the attention assigns the weights based on the word 's importance and can be used as a proxy for saliency . - * In Lemma 1 , there is a reference to the attention block 's capacity which is difficult to decipher . What do you mean here by capacity ? * * * Response * * : When two models contain precisely the same family of hypotheses , we say they have the same capacity . - * The assumption that word embeddings are sampled from a distribution with small variance seems likely to apply early in training , but not later . Have you checked the actual variance that would be associated with word embeddings late in training ? * * * Response * * : Our analysis only requires that the non-topic word maintains the word embeddings of a small variance . In the experiment on the artificial data introduced in Section 5.1 , if the embedding is initialized with variance 0.01 , then the sample variances of all random word embeddings on a randomly selected dimension are : 0.01005027 , 0.01005502 , 0.01007887 , 0.01008423 , 0.01008448 at the 1K-th , 2K-th , ... , 5K-th epochs . We can observe that the variance increases at a very low and decreasing speed . So , the assumption is well held ."}, "2": {"review_id": "1OCTOShAmqB-2", "review_text": "( Summary ) The paper investigates the dynamics of attention mechanism by configurating a controlled experiment on a simple topic classification task and training via gradient descent . Each random sentence in the training data is synthesized to include only one topic word among many . Then the authors try to find an intrinsic mechanism that triggers the attention model to discover the topic word and accelerates training via mutual promotion . They further experiment the evolution of models during optimization when no clear distinction between topic and non-topic words exist like in real data . ( Originality and Contribution ) The paper proposes an artificial topic classification task and shows a positive score-and-embedding-norm relationship for the topic words to which the model must attend to . The authors also show that attention mechanism is highly helpful when the classifier has only limited capacity . They also demonstrate mutual promotion effect that leads a faster dropping of training loss than the fixed score and fixed embeddings . This discovery sounds to be original and relevant contribution to the field . ( Strength and Weakness ) - Strength : Design and run novel controlled experiment . Extensive analysis . - Weakness : Too much notational overloading . Writing quality . ( Concerns , Questions , and Suggestions ) 1 ) It is unclear why $ M > > N $ implies that a topic word appears more frequently in the sentences than a non-topic word . Section 3 describes that each sentence consists of only one topic word , then combining with $ m $ non-topic words drawn uniformly at random . When the total number of topics $ N $ is much smaller than the size of non-topic word dictionary $ M $ , what increases frequency of topic word ? 2 ) Overall simplifying some notations and avoiding notational overloading would greatly increase the readability of the paper . 3 ) To reduce confusion , it would be great to change the iterator of summation for the partition function Z into $ \\sum_ { w \u2019 \\in S_k } $ rather than using the same $ w $ . 4 ) In Lemma 1 , assume $ q \\neq 0 $ - > $ q \\neq \\vec { 0 } $ .", "rating": "7: Good paper, accept", "reply_text": "Thank you for your reviews ! We believe you have precisely captured the main concepts of our paper . We have carefully gone through them , and here are our responses and explanations . - It is unclear why $ M > > N $ implies that a topic word appears more frequently in the sentences than a non-topic word . Section 3 describes that each sentence consists of only one topic word , then combining with $ m $ non-topic words drawn uniformly at random . When the total number of topics $ N $ is much smaller than the size of non-topic word dictionary $ M $ , what increases frequency of topic word ? * * Response * * : We will use an example to clarify this point . For simplicity , assume we have $ N $ topics and each topic only has one topic word ( so we have $ N $ topic words in total ) . Consider the sentence generating process introduced in the paper . For a given topic word , the probability that the topic word appears in a sentence is $ 1/N $ , while the probability of a given non-topic word appearing in a sentence of length $ m $ is $ 1- ( ( M-1 ) /M ) ^m $ . As a result , for a fixed $ m $ and $ N $ , if $ M > > N $ , $ 1- ( ( M-1 ) /M ) ^m $ approaches to zero . Thus a topic word appears more frequently in a sentence than a non-topic word . - Other problems regarding notations and writing quality . * * Response * * : We will simplify our notations and make the manuscript flow better in the revised version ."}, "3": {"review_id": "1OCTOShAmqB-3", "review_text": "This paper studies the dynamics of attention in a task of simplified topic modeling , over the course of training for a specific model , where the context vector is the sum over words in a sentence of their embedding weighted by the exponential of the dot-product their key embedding with a global query vector , normalized . Due to the simplification of the topic modeling problem ( two null-intersect sets of words : topic vs. non-topic ) , they consider the embeddings of the non-topic words to be fixed over the course of training for their theoretical analysis . The applicability of the theoretical result is close to zero , and a somewhat known property ( e.g.in word2vec , Mikolov et al.2013 ) .The experimental results include two parts . One on a tiny synthetic dataset that matches the simplified topic modeling problem and serves as illustration . The other is on SST2 and SST5 ( movie comments and ratings , sentiment analysis ) , where the results are poor ( obviously , as the model is simple ) , e.g.yielding 79.59 % on SST while the SOTA is 97.4 , and BERT base is at 91.2 . The analysis is interesting , but does not lead to new insights . For a simple analysis , the paper is at times hard to follow , and could benefit from more structure ( presenting `` what '' before `` how '' ) and better notation ( e.g.\\nu , v , $ v $ all attached to ( forms of the ) the context vector ) . Overall , the contribution does not seem sufficient enough for inclusion at ICLR . The paper could be a good fit for a workshop on topic modeling or attention-based models .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review ! We have carefully read them and here are our responses and explanations . - * The applicability of the theoretical result is close to zero , and a somewhat known property ( e.g.in word2vec , Mikolov et al.2013 ) .The experimental results include two parts . One on a tiny synthetic dataset that matches the simplified topic modeling problem and serves as illustration . The other is on SST2 and SST5 ( movie comments and ratings , sentiment analysis ) , where the results are poor ( obviously , as the model is simple ) , e.g.yielding 79.59 % on SST while the SOTA is 97.4 , and BERT base is at 91.2 . The analysis is interesting , but does not lead to new insights . * * * Response * * : We wish to note that the purpose of this work is not to produce SOTA results for specific NLP tasks . Rather our sole interest is to understand how attention works . Despite the nearly universal applicability of the attention module in neural net based language models , its working mechanism is poorly understood to date . In this work , our hope is to characterize the dynamics of training attention models , using a simplified task . Although research at such a fundamental level may appear somewhat distant from immediate applications , it contributes to developing concrete insights and deep understanding in neural networks , and potentially has a long term impact . Without such understanding and insights , neural networks or deep learning may remain as black boxes , and practitioners can only rely on heuristics in model design and alchemy-like hyper-parameter tuning in model training . For this reason , we can not agree that the applicability of this research is zero . It is unfortunate that our disagreement on this matter perhaps has merely reflected our distinct research taste from yours . - * For a simple analysis , the paper is at times hard to follow , and could benefit from more structure ( presenting `` what '' before `` how '' ) and better notation . * * * Response * * : We will make an effort to simplify our notations and make the manuscript flow better in the revised version ."}}