{"year": "2018", "forum": "ryazCMbR-", "title": "Communication Algorithms via Deep Learning", "decision": "Accept (Poster)", "meta_review": "This paper studies trainable deep encoders/decoders in the context of coding theory, based on recurrent neural networks. It presents highly promising results showing that one may be able to use learnt encoders and decoders on channels where no predefined codes are known.\n\nBesides these encouraging aspects, there are important concerns that the authors are encouraged to address; in particular, reviewers noted that the main contribution of this paper is mostly on the learnt encoding/decoding scheme rather than in the replacement of Viterbi/BCJR. Also, complexity should be taken into account when comparing different decoding schemes.\n\nOverall, the AC leans towards acceptance, since this paper may trigger further research in this direction. ", "reviews": [{"review_id": "ryazCMbR--0", "review_text": "In this paper the authors propose to use RNNs and LSTMs for channel coding. But I have the impression the authors completely miss the state of the art in channel coding and the results are completely useless for any current communication system. I believe that machine learning, in general, and deep learning, in particular, might be of useful for physical layer communications. I just do not see why it would be useful for channel coding over the AWGN channel. Let me explain. If the decoder knows that the encoder is using a convolutional code, why does it need to learn the decoder instead of using the Viterbi or BCJR algorithms that are known to be optimal for sequences and symbols, respectively. I cannot imagine an scenario in which the decoder does not know the convolutional code that it is being used and the encoder sends 120,000 bits of training sequence (useless bits from information standpoint) for the decoder to learn it. More important question, do the authors envision that this learning is done every time there is a new connection or it is learnt once and for all. If it is learnt every time that would be ideal if we were discovering new channel codes everyday, clearly not the case. If we learnt it one and for all and then we incorporated in the standard that would only make sense if the GRU structure was computationally better than the BCJR or Viterbi. I would be surprise if it is. If instead of using 2 or 3 memories, we used 6-8 does 120,000 bits be good enough or we need to exponentially increase the training sequence? So the first result in the paper shows that a tailored structure for convolutional encoding can learn to decode it. Basically, the authors are solving a problem that does not need solving. For the Turbocodes the same principle as before applies. In this case the comments of the authors really show that they do not know anything about coding. In Page 6, we can read: \u201cUnlike the convolutional codes, the state of the art (message-passing) decoders for turbo codes are not the corresponding MAP decoders, so there is no contradiction in that our neural decoder would beat the message-passing ones\u201d. This is so true, so I expected the DNN structure to be significantly better than turbodecoding. But actually, they do not. These results are in Figure 15 page 6 and the solution for the turbo decoders and the DNN architecture are equivalent. I am sure that the differences in the plots can be explained by the variability in the received sequence and not because the DNN is superior to the turbodecoder. Also in this case the training sequence is measured in the megabits for extremely simple components. If the convolutional encoders were larger 6-8 bits, we would be talking about significantly longer training sequences and more complicated NNs. In the third set the NNs seems to be superior to the standard methods when burst-y noise is used, but the authors seems to indicate that that NN is trained with more information about these bursts that the other methods do not have. My impression is that the authors would be better of focusing on this example and explain it in a way that it is reproducible. This experiment is clearly not well explained and it is hard to know if there is any merit for the proposed NN structure. Finally, the last result would be the more interesting one, because it would show that we can learn a better channel coding and decoding mechanism that the ones humans have been able to come up with. In this sense, if NNs can solve this problem that would be impressive and would turn around how channel coding is done nowadays. If this result were good enough, the authors should only focus in it and forget about the other 3 cases. The issue with this result is that it actually does not make sense. The main problem with the procedure is that the feedback proposal is unrealistic, this is easy to see in Figure 16 in which the neural encoder is proposed. It basically assumes that the received real-valued y_k can be sent (almost) noiselessly to the encoder with minimal delay and almost instantaneously. So the encoder knows the received error and is able to cancel it out. Even if this procedure could be implemented, which it cannot be. The code only uses 50 bits and it needed 10^7 iterations (500Mbs) to converge. The authors do not show how far they are from the Shannon limit, but I can imagine that with 50 bit code, it should be pretty far. We know that with long enough LDPC codes we can (almost) reach the Shannon limit, so new structure are not needed. If we are focusing on shorter codes (e.g. latency?) then it will be good to understand why do we need to learn the channel codes. A comparison to the state of the art would be needed. Because clearly the used codes are not close to state of the art. For me the authors either do not know about coding or are assuming that we do not, which explains part of the tone of this review. ", "rating": "2: Strong rejection", "reply_text": "Q5.NN is trained with more information about burst noise model than others ? My impression is that the authors would be better of focusing on this example and explain it in a way that it is reproducible . A5.The two state-of-the-art heuristics - erasure thresholding and saturation thresholding ( Safavi-Naeini et al.2015 ) that we are comparing the NN decoder against to - fully utilize the knowledge on the bursty channel model . Specifically , the threshold value in those methods is choosen based on the burst noise model . We believe the experiment is fully explained and entirely reproducible ( we are also uploading our code base on Github after the ICLR review process is completed ) . Perhaps the reviewer can be specific on exactly which parts of the experiment could use a better explanation . Q6.The feedback proposal is unrealistic . It basically assumes that the received real-valued y_k can be sent ( almost ) noiselessly to the encoder with minimal delay and almost instantaneously . So the encoder knows the received noise and is able to cancel it out . A6 . ( 1 ) The AWGN channel with output feedback is the most classical of models in communication theory ( studied by Shannon himself in 1956.There has been a huge effort in the literature over the ensuing decades ( the important of which we have amply cited in our manuscript ) and is of very basic importance to multiple professional societies ( including IEEE communication society and IEEE Information theory society ) . Although idealistic , it provides a valuable training ground to understand how to use feedback to more efficiently communicate . ( 2 ) The `` W '' in the phrase AWGN ( which is our channel model refers to `` white '' which means the noise is memoryless across different time symbols.So even a single time step delay ( not to mention `` minimal delay '' ) does not allow the `` the encoder knows the received noise and is able to cancel it out . '' Perhaps the reviewer would like to reconsider his/her ratiocination ?"}, {"review_id": "ryazCMbR--1", "review_text": "Error-correcting codes constitute a well-researched area of study within communication engineering. In communication, messages that are to be transmitted are encoded into binary vector called codewords that contained some redundancy. The codewords are then transmitted over a channel that has some random noise. At the receiving end the noisy codewords are then decoded to recover the messages. Many well known families of codes exist, notably convolutional codes and Turbo codes, two code families that are central to this paper, that achieve the near optimal possible performance with efficient algorithms. For Turbo and convolutional codes the efficient MAP decodings are known as Viterbi decoder and the BCJR decoder. For drawing baselines, it is assumed that the random noise in channel is additive Gaussian (AWGN). This paper makes two contributions. First, recurrent neural networks (RNN) are proposed to replace the Viterbi and BCJR algorithms for decoding of convolutional and Turbo decoders. These decoders are robust to changes in noise model and blocklength - and shows near optimal performance. It is unclear to me what is the advantage of using RNNs instead of Viterbi or BCJR, both of which are optimal, iterative and runs in linear time. Moreover the authors point out that RNNs are shown to emulate BCJR and Viterbi decodings in prior works - in light of that, why their good performance surprising? The second contribution of the paper constitutes the design and decoding of codes based on RNNs for a Gaussian channel with noisy feedback. For this channel the optimal codes are unknown. The authors propose an architecture to design codes for this channel. This is a nice step. However, in the performance plot (figure 8), the RNN based code-decoder does not seem to be outperforming the existing codes except for two points. For both in high and low SNR the performance is suboptimal to Turbo codes and a code by Schalkwijk & Kailath. The section is also super-concise to follow. I think it was necessary to introduce an LSTM encoder - it was hard to understand the overall encoder. This is an issue with the paper - the authors previously mentioned (8,16) polar code without mentioning what the numbers mean. However, I overall liked the idea of using neural nets to design codes for some non-standard channels. While at the decoding end it does not bring in anything new (modern coding theory already relies on iterative decoders, that are super fast), at the designing-end the Gaussian feedback channel part can be a new direction. This paper lacks theoretical aspect, as to no indication is given why RNN based design/decoders can be good. I am mostly satisfied with the experiments, barring Fig 8, which does not show the results that the authors claim. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . 1.Representability , Learnability and Generalization : There are three aspects to showing that a learning problem can be solved through a parametric architecture . ( 1 ) Representability : The ability to represent the needed function through a neural network . For Viterbi/BCJR algorithms , this representability was shown in prior work by handcrafting parameters that represent the Viterbi/BCJR algorithms . We note that neural networks with sufficient number of parameters can indeed represent any function through the universal approximation theorem for feedforward networks and RNNs ( Cybenko , G.1989 , Siegelmann , H.T. & Sontag , E.D.1995 ) and therefore this result is not that surprising . ( 2 ) Learnability : Can the required function be learnt directly through gradient descent on the observed data ? For Viterbi and BCJR , learnability was neither known through prior work nor is it obvious . One of the main contributions of our work is that those algorithms can be learnt from observed data . ( 3 ) Generalization : Does the learnt function/algorithm generalize to unobserved data ? We show this not only at the level of new unobserved codewords , but also show that the learnt algorithm trained on shorter blocks of length 100 can generalize well to longer blocks of length up to 10,000 . Such generalization is rare in many realistic problems . To summarize , out of the three aspects , only representability was known from prior work ( and , we agree with the reviewer , that it is the least surprising given universal representability ) . Learnability and generalization of the learnt Viterbi and BCJR algorithms to much larger block lengths are both unknown from prior art and they are surprising , and interesting in their own right . We note that Viterbi and BCJR algorithms are useful in machine learning beyond communications problem , representing dynamic programming and forward-backward algorithms , respectively . Peter Elias introduced convolutional codes in 1955 but efficient decoding through dynamic programming ( Viterbi decoding ) was only available in 1967 requiring mathematical innovation . We note that ability to learn the Viterbi algorithm from short block length data ( which can be generated by full-search ) and generalizing them to much longer blocks implies an alternative methodology to solve the convolutional code problem . Such an approach could have significant benefits in problems where corresponding mathematically optimal algorithms are not known at the moment . We demonstrate the power of this approach by studying the problem of channel-with-feedback where no good coding schemes are known despite 70 years of research . 2.Advantages of using RNNs instead of Viterbi or BCJR : There are two advantages to RNN decoders , that go beyond mimicing Viterbi/BCJR . ( 1 ) Robustness : Viterbi and BCJR decoders are known to be vulnerable to changes in the channel , as those are highly tailored for the AWGN . We show in Section 3 , via numerical experiments with T-dsitributed noise , that the neural network decoder trained on AWGN is much more robust against the changes in the channel . This makes , among other things , our neural network decoder much more attractive alternative to Viterbi/BCJR decoders in practice , where the channel model is not available . ( 2 ) Adaptivity : It is not easy to extend the idea of Viterbi decoder and iterative Turbo decoding beyond the simple convolutional codes and the standard Gaussian channel ( or any other Discrete Memoryless Channel ) . On the other hand , our neural network decoder provides a new paradigm for decoding that can be applied to any encoder and any channel , as it learns from training examples . To showcase the power of this \u201c adaptivity \u201d , we show improved performance on the bursty channel . A more stark example of the utility presents itself in the feedback channel . There exists no known practical encoding-decoding scheme for a feedback channel . Only because we have a neural network decoder that can adapt to any encoder , we are able to find a novel encoder ( also neural network based ) that uses the feedback information correctly and achieves the performance significantly better than any other competing schemes . This would have not been possible without a neural network decoder and the techniques we learned in training one to mimic the simple Viterbi . 3.Updated curve for new codes on AWGN channel with feedback : We have improved our encoder significantly by borrowing the idea of zero-padding from coding theory . In short , most of the errors occurs in the last bit , whose feedback information was not utilized by our encoder . We resolved this issue by padding a zero in the end of information bits ( Hence , the codeword length is 3 ( K+1 ) for K information bits ) . This significantly improves the performance as shown in the new Figure 8 . A full description of the encoder-decoder architecture is provided in Appendix D. 4 . We replaced `` ( 8,16 ) polar code '' by \u201c rate 1/2 polar code over 8 information bits \u201d ."}, {"review_id": "ryazCMbR--2", "review_text": "This paper shows how RNNs can be used to decode convolutional error correcting codes. While previous recent work has shown neural decoders for block codes results had limited success and for small block lengths. This paper shows that RNNs are very suitable for convolutional codes and achieves state of the art performance for the first time. The second contribution is on adaptivity outside the AWGN noise model. The authors show that their decoder performs well for different noise statistics outside what it was trained on. This is very interesting and encouraging. It was not very clear to me if the baseline decoders (Turbo/BCJR) are fairly compared here since better decoders may be used for the different statistics, or some adaptivity could be used in standard decoders in various natural ways. The last part goes further in designing new error correcting schemes using RNN encoders and decoders for noisy feedback communication. For this case capacity is known to be impossible to improve, but the bit error error can be improved for finite lenghts. It seems quite remarkable that they beat Schalkwijk and Kailath and shows great promise for other communication problems. The paper is very well written with good historical context and great empirical results. I think it opens a new area for information theory and communications with new tools. My only concern is that perhaps the neural decoders can be attacked with adversarial noise (which would not be possible for good-old Viterbi ). It would be interesting to discuss this briefly. A second (related) concern is the lack of theoretical understanding of these new decoders. It would be nice if we could prove something about them, but of course this will probably be challenging. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for your comments . 1.Neural decoders can be attacked with adversarial noise : This is a great point , which is related to the current ongoing advances in other areas of neural networks ( e.g.classification ) . At a high level , there are two types of adversarial noise that can hurt our approach . The first one is poisoning training data . If we are training on data collected from real channels , an adversary who knows that we are training can intervene and add adversarial noise to make our trained decoder useless . This proposes an interesting game between the designer ( us ) and the attacker , in the form of how much noise power does the adversary need in order to make our decoder fail . This we believe is a fascinating research question , and we will add discussions in the final version of our manuscript . The second type of attack is adversarial examples , where at the test time an adversary changes the channel to make our decoder fail . In this scenario , both Viterbi and our decoder are vulnerable . Our numerical experiments on robustness is inspired by such scenarios , where we show that neural network decoders are more robust against such attacks ( or natural dynamic changes ) in Section 3 . 2.Fair comparison to baseline decoders : There are two ways to run fair experiments on other channels , in our opinion . One is to mimic dynamic environment of real world by using encoder-decoders that are tailored for AWGN ( both Turbo/BCJR and Neural Network decoder trained on AWGN ) and see how robust it is against changes in the channel . This is the experiments we run with T-distribution . Another is , as suggested by the reviewer , to design decoders based on the new statistics of the channel that work well outside of AWGN . This is the experiments we run with bursty channels . We agree that these two experiments are addressing two different questions , but we believe we are fair in the comparisons to competing decoders within each setting . 3.Theoretical understanding of these neural decoders/coding schemes is a challenging but very interesting future research direction ."}], "0": {"review_id": "ryazCMbR--0", "review_text": "In this paper the authors propose to use RNNs and LSTMs for channel coding. But I have the impression the authors completely miss the state of the art in channel coding and the results are completely useless for any current communication system. I believe that machine learning, in general, and deep learning, in particular, might be of useful for physical layer communications. I just do not see why it would be useful for channel coding over the AWGN channel. Let me explain. If the decoder knows that the encoder is using a convolutional code, why does it need to learn the decoder instead of using the Viterbi or BCJR algorithms that are known to be optimal for sequences and symbols, respectively. I cannot imagine an scenario in which the decoder does not know the convolutional code that it is being used and the encoder sends 120,000 bits of training sequence (useless bits from information standpoint) for the decoder to learn it. More important question, do the authors envision that this learning is done every time there is a new connection or it is learnt once and for all. If it is learnt every time that would be ideal if we were discovering new channel codes everyday, clearly not the case. If we learnt it one and for all and then we incorporated in the standard that would only make sense if the GRU structure was computationally better than the BCJR or Viterbi. I would be surprise if it is. If instead of using 2 or 3 memories, we used 6-8 does 120,000 bits be good enough or we need to exponentially increase the training sequence? So the first result in the paper shows that a tailored structure for convolutional encoding can learn to decode it. Basically, the authors are solving a problem that does not need solving. For the Turbocodes the same principle as before applies. In this case the comments of the authors really show that they do not know anything about coding. In Page 6, we can read: \u201cUnlike the convolutional codes, the state of the art (message-passing) decoders for turbo codes are not the corresponding MAP decoders, so there is no contradiction in that our neural decoder would beat the message-passing ones\u201d. This is so true, so I expected the DNN structure to be significantly better than turbodecoding. But actually, they do not. These results are in Figure 15 page 6 and the solution for the turbo decoders and the DNN architecture are equivalent. I am sure that the differences in the plots can be explained by the variability in the received sequence and not because the DNN is superior to the turbodecoder. Also in this case the training sequence is measured in the megabits for extremely simple components. If the convolutional encoders were larger 6-8 bits, we would be talking about significantly longer training sequences and more complicated NNs. In the third set the NNs seems to be superior to the standard methods when burst-y noise is used, but the authors seems to indicate that that NN is trained with more information about these bursts that the other methods do not have. My impression is that the authors would be better of focusing on this example and explain it in a way that it is reproducible. This experiment is clearly not well explained and it is hard to know if there is any merit for the proposed NN structure. Finally, the last result would be the more interesting one, because it would show that we can learn a better channel coding and decoding mechanism that the ones humans have been able to come up with. In this sense, if NNs can solve this problem that would be impressive and would turn around how channel coding is done nowadays. If this result were good enough, the authors should only focus in it and forget about the other 3 cases. The issue with this result is that it actually does not make sense. The main problem with the procedure is that the feedback proposal is unrealistic, this is easy to see in Figure 16 in which the neural encoder is proposed. It basically assumes that the received real-valued y_k can be sent (almost) noiselessly to the encoder with minimal delay and almost instantaneously. So the encoder knows the received error and is able to cancel it out. Even if this procedure could be implemented, which it cannot be. The code only uses 50 bits and it needed 10^7 iterations (500Mbs) to converge. The authors do not show how far they are from the Shannon limit, but I can imagine that with 50 bit code, it should be pretty far. We know that with long enough LDPC codes we can (almost) reach the Shannon limit, so new structure are not needed. If we are focusing on shorter codes (e.g. latency?) then it will be good to understand why do we need to learn the channel codes. A comparison to the state of the art would be needed. Because clearly the used codes are not close to state of the art. For me the authors either do not know about coding or are assuming that we do not, which explains part of the tone of this review. ", "rating": "2: Strong rejection", "reply_text": "Q5.NN is trained with more information about burst noise model than others ? My impression is that the authors would be better of focusing on this example and explain it in a way that it is reproducible . A5.The two state-of-the-art heuristics - erasure thresholding and saturation thresholding ( Safavi-Naeini et al.2015 ) that we are comparing the NN decoder against to - fully utilize the knowledge on the bursty channel model . Specifically , the threshold value in those methods is choosen based on the burst noise model . We believe the experiment is fully explained and entirely reproducible ( we are also uploading our code base on Github after the ICLR review process is completed ) . Perhaps the reviewer can be specific on exactly which parts of the experiment could use a better explanation . Q6.The feedback proposal is unrealistic . It basically assumes that the received real-valued y_k can be sent ( almost ) noiselessly to the encoder with minimal delay and almost instantaneously . So the encoder knows the received noise and is able to cancel it out . A6 . ( 1 ) The AWGN channel with output feedback is the most classical of models in communication theory ( studied by Shannon himself in 1956.There has been a huge effort in the literature over the ensuing decades ( the important of which we have amply cited in our manuscript ) and is of very basic importance to multiple professional societies ( including IEEE communication society and IEEE Information theory society ) . Although idealistic , it provides a valuable training ground to understand how to use feedback to more efficiently communicate . ( 2 ) The `` W '' in the phrase AWGN ( which is our channel model refers to `` white '' which means the noise is memoryless across different time symbols.So even a single time step delay ( not to mention `` minimal delay '' ) does not allow the `` the encoder knows the received noise and is able to cancel it out . '' Perhaps the reviewer would like to reconsider his/her ratiocination ?"}, "1": {"review_id": "ryazCMbR--1", "review_text": "Error-correcting codes constitute a well-researched area of study within communication engineering. In communication, messages that are to be transmitted are encoded into binary vector called codewords that contained some redundancy. The codewords are then transmitted over a channel that has some random noise. At the receiving end the noisy codewords are then decoded to recover the messages. Many well known families of codes exist, notably convolutional codes and Turbo codes, two code families that are central to this paper, that achieve the near optimal possible performance with efficient algorithms. For Turbo and convolutional codes the efficient MAP decodings are known as Viterbi decoder and the BCJR decoder. For drawing baselines, it is assumed that the random noise in channel is additive Gaussian (AWGN). This paper makes two contributions. First, recurrent neural networks (RNN) are proposed to replace the Viterbi and BCJR algorithms for decoding of convolutional and Turbo decoders. These decoders are robust to changes in noise model and blocklength - and shows near optimal performance. It is unclear to me what is the advantage of using RNNs instead of Viterbi or BCJR, both of which are optimal, iterative and runs in linear time. Moreover the authors point out that RNNs are shown to emulate BCJR and Viterbi decodings in prior works - in light of that, why their good performance surprising? The second contribution of the paper constitutes the design and decoding of codes based on RNNs for a Gaussian channel with noisy feedback. For this channel the optimal codes are unknown. The authors propose an architecture to design codes for this channel. This is a nice step. However, in the performance plot (figure 8), the RNN based code-decoder does not seem to be outperforming the existing codes except for two points. For both in high and low SNR the performance is suboptimal to Turbo codes and a code by Schalkwijk & Kailath. The section is also super-concise to follow. I think it was necessary to introduce an LSTM encoder - it was hard to understand the overall encoder. This is an issue with the paper - the authors previously mentioned (8,16) polar code without mentioning what the numbers mean. However, I overall liked the idea of using neural nets to design codes for some non-standard channels. While at the decoding end it does not bring in anything new (modern coding theory already relies on iterative decoders, that are super fast), at the designing-end the Gaussian feedback channel part can be a new direction. This paper lacks theoretical aspect, as to no indication is given why RNN based design/decoders can be good. I am mostly satisfied with the experiments, barring Fig 8, which does not show the results that the authors claim. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . 1.Representability , Learnability and Generalization : There are three aspects to showing that a learning problem can be solved through a parametric architecture . ( 1 ) Representability : The ability to represent the needed function through a neural network . For Viterbi/BCJR algorithms , this representability was shown in prior work by handcrafting parameters that represent the Viterbi/BCJR algorithms . We note that neural networks with sufficient number of parameters can indeed represent any function through the universal approximation theorem for feedforward networks and RNNs ( Cybenko , G.1989 , Siegelmann , H.T. & Sontag , E.D.1995 ) and therefore this result is not that surprising . ( 2 ) Learnability : Can the required function be learnt directly through gradient descent on the observed data ? For Viterbi and BCJR , learnability was neither known through prior work nor is it obvious . One of the main contributions of our work is that those algorithms can be learnt from observed data . ( 3 ) Generalization : Does the learnt function/algorithm generalize to unobserved data ? We show this not only at the level of new unobserved codewords , but also show that the learnt algorithm trained on shorter blocks of length 100 can generalize well to longer blocks of length up to 10,000 . Such generalization is rare in many realistic problems . To summarize , out of the three aspects , only representability was known from prior work ( and , we agree with the reviewer , that it is the least surprising given universal representability ) . Learnability and generalization of the learnt Viterbi and BCJR algorithms to much larger block lengths are both unknown from prior art and they are surprising , and interesting in their own right . We note that Viterbi and BCJR algorithms are useful in machine learning beyond communications problem , representing dynamic programming and forward-backward algorithms , respectively . Peter Elias introduced convolutional codes in 1955 but efficient decoding through dynamic programming ( Viterbi decoding ) was only available in 1967 requiring mathematical innovation . We note that ability to learn the Viterbi algorithm from short block length data ( which can be generated by full-search ) and generalizing them to much longer blocks implies an alternative methodology to solve the convolutional code problem . Such an approach could have significant benefits in problems where corresponding mathematically optimal algorithms are not known at the moment . We demonstrate the power of this approach by studying the problem of channel-with-feedback where no good coding schemes are known despite 70 years of research . 2.Advantages of using RNNs instead of Viterbi or BCJR : There are two advantages to RNN decoders , that go beyond mimicing Viterbi/BCJR . ( 1 ) Robustness : Viterbi and BCJR decoders are known to be vulnerable to changes in the channel , as those are highly tailored for the AWGN . We show in Section 3 , via numerical experiments with T-dsitributed noise , that the neural network decoder trained on AWGN is much more robust against the changes in the channel . This makes , among other things , our neural network decoder much more attractive alternative to Viterbi/BCJR decoders in practice , where the channel model is not available . ( 2 ) Adaptivity : It is not easy to extend the idea of Viterbi decoder and iterative Turbo decoding beyond the simple convolutional codes and the standard Gaussian channel ( or any other Discrete Memoryless Channel ) . On the other hand , our neural network decoder provides a new paradigm for decoding that can be applied to any encoder and any channel , as it learns from training examples . To showcase the power of this \u201c adaptivity \u201d , we show improved performance on the bursty channel . A more stark example of the utility presents itself in the feedback channel . There exists no known practical encoding-decoding scheme for a feedback channel . Only because we have a neural network decoder that can adapt to any encoder , we are able to find a novel encoder ( also neural network based ) that uses the feedback information correctly and achieves the performance significantly better than any other competing schemes . This would have not been possible without a neural network decoder and the techniques we learned in training one to mimic the simple Viterbi . 3.Updated curve for new codes on AWGN channel with feedback : We have improved our encoder significantly by borrowing the idea of zero-padding from coding theory . In short , most of the errors occurs in the last bit , whose feedback information was not utilized by our encoder . We resolved this issue by padding a zero in the end of information bits ( Hence , the codeword length is 3 ( K+1 ) for K information bits ) . This significantly improves the performance as shown in the new Figure 8 . A full description of the encoder-decoder architecture is provided in Appendix D. 4 . We replaced `` ( 8,16 ) polar code '' by \u201c rate 1/2 polar code over 8 information bits \u201d ."}, "2": {"review_id": "ryazCMbR--2", "review_text": "This paper shows how RNNs can be used to decode convolutional error correcting codes. While previous recent work has shown neural decoders for block codes results had limited success and for small block lengths. This paper shows that RNNs are very suitable for convolutional codes and achieves state of the art performance for the first time. The second contribution is on adaptivity outside the AWGN noise model. The authors show that their decoder performs well for different noise statistics outside what it was trained on. This is very interesting and encouraging. It was not very clear to me if the baseline decoders (Turbo/BCJR) are fairly compared here since better decoders may be used for the different statistics, or some adaptivity could be used in standard decoders in various natural ways. The last part goes further in designing new error correcting schemes using RNN encoders and decoders for noisy feedback communication. For this case capacity is known to be impossible to improve, but the bit error error can be improved for finite lenghts. It seems quite remarkable that they beat Schalkwijk and Kailath and shows great promise for other communication problems. The paper is very well written with good historical context and great empirical results. I think it opens a new area for information theory and communications with new tools. My only concern is that perhaps the neural decoders can be attacked with adversarial noise (which would not be possible for good-old Viterbi ). It would be interesting to discuss this briefly. A second (related) concern is the lack of theoretical understanding of these new decoders. It would be nice if we could prove something about them, but of course this will probably be challenging. ", "rating": "9: Top 15% of accepted papers, strong accept", "reply_text": "Thank you for your comments . 1.Neural decoders can be attacked with adversarial noise : This is a great point , which is related to the current ongoing advances in other areas of neural networks ( e.g.classification ) . At a high level , there are two types of adversarial noise that can hurt our approach . The first one is poisoning training data . If we are training on data collected from real channels , an adversary who knows that we are training can intervene and add adversarial noise to make our trained decoder useless . This proposes an interesting game between the designer ( us ) and the attacker , in the form of how much noise power does the adversary need in order to make our decoder fail . This we believe is a fascinating research question , and we will add discussions in the final version of our manuscript . The second type of attack is adversarial examples , where at the test time an adversary changes the channel to make our decoder fail . In this scenario , both Viterbi and our decoder are vulnerable . Our numerical experiments on robustness is inspired by such scenarios , where we show that neural network decoders are more robust against such attacks ( or natural dynamic changes ) in Section 3 . 2.Fair comparison to baseline decoders : There are two ways to run fair experiments on other channels , in our opinion . One is to mimic dynamic environment of real world by using encoder-decoders that are tailored for AWGN ( both Turbo/BCJR and Neural Network decoder trained on AWGN ) and see how robust it is against changes in the channel . This is the experiments we run with T-distribution . Another is , as suggested by the reviewer , to design decoders based on the new statistics of the channel that work well outside of AWGN . This is the experiments we run with bursty channels . We agree that these two experiments are addressing two different questions , but we believe we are fair in the comparisons to competing decoders within each setting . 3.Theoretical understanding of these neural decoders/coding schemes is a challenging but very interesting future research direction ."}}