{"year": "2021", "forum": "o2ko2D_uvXJ", "title": "Group-Connected Multilayer Perceptron Networks", "decision": "Reject", "meta_review": "The paper proposes an MLP based approach for data without known structure (such as tabular data). At first, the data are partitioned into K blocks in a differentiable way, then the standard MLP is applied to each block. The results are then aggregated recursively to produce the final output. \n\nPros:\n1. Handling less structured data is surely an important problem in machine learning and is much less explored. \n2. The paper is well written, easily understandable even with a fast browsing. \n3. The experimental results show some improvement. \n\nCons:\n1. The approach is somewhat trivial, and the framework could be improved, see, e.g. Reviewers #3&#4. \n2. By the structure of the approach and the type of target data, a more reasonable comparison is with random forest (echoing Reviewer #1), which the authors added during rebuttal, rather than MLP etc. Maybe should even compare with deep random forest. Although the comparison with MLP etc. is quite favorable, the advantage over random forest is somewhat marginal (except on HAPT, which is a imagery data set and random forest may not be good at; also echoing Reviewers #1&#4's comment on why using imagery data, which do not fit the theme of the paper). Reviewers #3&#4 also had some concerns with the experiments. Reviewer #4 confirmed in the confidential comment that the performance improvement is incremental.\n\nAlthough the rebuttal seemed to be successful, thus both Reviewers #1 and #4 raised their scores, the average score is still at the borderline. Due to the limited acceptance rate, the area chair has to reject the paper.", "reviews": [{"review_id": "o2ko2D_uvXJ-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pro : $ \\bullet $ It is an exciting idea to learn the relations in among feature dimensions , group them , and apply the operations within groups to train an MLP . The experiments on various datasets , and important ablations such as the effect of number and size of groups , types of pooling were done . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : $ \\bullet $ $ \\mathtt { Group-Pool } $ operation applies pooling inside feature group . Looking into Table 6 of the Appendix , mean and max-pooling were used in different datasets . The ablation study is done on CIFAR-10 ( Fig.4 ) .The assumption is always to have pairs of groups and apply pooling to consecutive group members . It contradicts the initial idea . Why did n't you apply a routing layer to learn which layers to pool together ? $ \\bullet $ * Ablation studies on image dataset : * Among many datasets experimented on ; it is difficult to understand why all ablation studies are conducted on CIFAR-10 . The paper 's main motivation is to propose a method that works on domains beyond image , voice , and graphs . However , all ablations are done on an image dataset naturally that has spatial relations . $ \\bullet $ * Incremental performance gain : * Looking into the performance comparison with vanilla MLP and other methods ( SNN , SET , and FGR ) , the only considerable improvement from the MLP baseline is on the CIFAR-10 dataset . The results on all other datasets , accuracy , and area under curve scores are incremental . How many trials did you conduct the same experiments ( the stdev values inside brackets of Table 2 ) ? Did you run any statistical significance test between the trials of GMLP and MLP ? MLP baselines can be reported from an architecture matching with the number of parameters in GMLP . In Table 2 , the number of parameters should be written or standardized in each method . $ \\bullet $ * What are `` MIT-BIH datasets '' ? * They are not mentioned , referred to in the text , but given in Fig.2 ( b ) $ \\bullet $ * Finding optimal numbers of m and k : * Finding the optimal number of groups and group sizes can introduce a large computational overhead . Fig.5 and 6 show the performance margin could be 10-15 % . Did you compare the optimal numbers of $ m $ and $ k $ between the datasets with a similar feature dimension ? In other words , are the optimal values calculated in one dataset generalizable to other datasets to eliminate the necessity to train many models to find a good value . $ \\bullet $ How does the group routing evolve during training ? In a learned model , what is the relation between inter-group and intra-group feature correlations ? ( Are the members of the same group more relevant to each other in vice versa ? ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Minor : $ \\bullet $ * As a term , the use of `` group '' : * Use of group as a term causes confusion in the reader . In the introduction , it may be useful to include that the expression is not related to group ina mathematical sense , and only represents a subset of feature dimensions .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Comment : What are `` MIT-BIH datasets '' ? They are not mentioned , referred to in the text , but given in Fig.2 ( b ) * * Response : Thank you for pointing out the missing information . We added the MIT-BIH dataset to the text and included the details as well as a link to the data in the revised Table 1 . - * * Comment : Finding optimal numbers of m and k : Finding the optimal number of groups and group sizes can introduce a large computational overhead . Fig.5 and 6 show the performance margin could be 10-15 % . Did you compare the optimal numbers of m and k between the datasets with a similar feature dimension ? In other words , are the optimal values calculated in one dataset generalizable to other datasets to eliminate the necessity to train many models to find a good value . * * Response : In our experiments , we searched for the optimal m and k hyperparameters for each dataset ( see Appendix A ) . In general , and as we see from the experimental results , the optimal m and k values are highly dependent on the dataset . There is no exact relationship between the number of features and optimal m and k values . However , as a rule of thumb , we found that the number of features ( d ) usually has the same order as m * k. Regarding your concern about the computational overhead , in our experiments , we were able to find appropriate hyperparameters for our largest datasets in less than 72 hours on a mid-range machine . - * * Comment : How does the group routing evolve during training ? In a learned model , what is the relation between inter-group and intra-group feature correlations ? ( Are the members of the same group more relevant to each other in vice versa ? ) * * Response : To address your comment , as suggested , we conducted experiments to analyze inter-group and intra-group feature correlations . See Appendix J : \u201c Analysis of Intra-Group and Inter-Group Correlations \u201d in the revised paper . Based on the results in Fig.22 and 23 , we did not find any significant difference in the correlation values for features within each group and features between different groups . We believe that this is an expected result as the objective function is based on a classification loss and does not enforce any correlation among the learned feature groups . Note that often a level of inter-group and intra-group redundancy improves the robustness of the trained models . In addition to this analysis , in Appendix G , we provided a method to translate feature groups to a graph representation and then used graph visualization and clustering techniques to visually demonstrate feature selection and coappearance in GMLP networks . Based on the results , feature groups appear as clusters in the graph representation and their connection with other groups is highly dependent on the dataset and the specific group itself . In general , we observed patterns such as features that appear multiple times in many different groups and features that only appear in certain groups ( see Appendix G and Fig.18 ) .- * * Comment : As a term , the use of `` group '' : Use of group as a term causes confusion in the reader . In the introduction , it may be useful to include that the expression is not related to group in a mathematical sense , and only represents a subset of feature dimensions . * * Response : Thank you for suggesting this . We added a footnote in the introduction section to clarify this for our readers : \u201c In this paper , the expression `` group '' is not related to the group in a mathematical sense , and it only represents a subset of features . \u201d"}, {"review_id": "o2ko2D_uvXJ-1", "review_text": "The paper describes an MLP architectures for problems in which the features do not have a known structure ( eg , tabular data ) . A `` differentiable routing matrix '' partitions the data into K blocks . Then , standard MLPs are applied to each block and the results are recursively aggregated by moving forward in the model . On the plus side , the paper is well written , the topic is significant ( as evidenced by any Kaggle competition on tabular data ) , and the model is simple enough to be understandable even on a quick reading . However , I do have a few concerns that put the paper on a borderline situation . [ Addendum after review : most of the concerns have been addressed by the authors with a large set of additional experiments . ] 1.The paper is based on the assumption that a `` split-then-combine '' prior is sufficiently flexible to handle most real-world cases . However , this is kind of strongly limiting , eg , each feature can only contribute to a single `` high-level concept '' in the network . I do n't feel that the paper does a strong job in justifying such an inductive bias . [ Addendum after review : As the authors point out , this is only partially true , as each feature can be selected for multiple groups . ] 2.In the experimental part , I do n't understand the motivation for using images . I ca n't imagine a single case in which one is given raw sensory data with absolutely no structure on it . 3.Concerning the other datasets ( the tabular ones ) , the authors report a decent improvement compared to an MLP , with a significant decrease in complexity . However , I do not understand the rationale for excluding random forests ( or any non-neural baselines ) from this comparison . A random forest would be the default algorithm in these situations ( both in terms of accuracy and speed ) , and making MLPs competitive is one of the motivating factors in the paper itself . [ Addendum after review : the authors have added a random forest to the experiments . However , this is the only method whose parameters are not fine-tuned ( which might be a smaller problem for random forests ) . No time comparison is given , and some accuracy deviations appear well within the standard deviations . Still , this is a very good addition to the paper . ] 4.Is interpretability a concern here ? Inspecting the routing matrix should reveal information on the feature grouping . I feel this can provide a strong boost to the paper . Summarizing , point ( 3 ) is the most important . Showing that the proposed Group MLP is superior to a classical alternative should be the critical aim of the comparisons . [ Addendum after review : This point has been partially addressed . ]", "rating": "7: Good paper, accept", "reply_text": "Thank you for reviewing the manuscript and helpful comments . Please find a point-to-point response to your comments in the following . - * * Comment : The paper is based on the assumption that a `` split-then-combine '' prior is sufficiently flexible to handle most real-world cases . However , this is kind of strongly limiting , eg , each feature can only contribute to a single `` high-level concept '' in the network . I do n't feel that the paper does a strong job in justifying such an inductive bias . * * Response : In the proposed method each feature can contribute to multiple concepts as there are no restrictions on features being selected multiple times in different groups . Note that in Section 3.3 , the routing matrix in the group-select layer has the flexibility to include each feature in different groups as the rows in the \\psi matrix are trained independently . As such , the proposed method is not as restrictive as a \u201c split-then-combine \u201d inductive bias . To address your comment and to clarify this for our readers , we added the following explanation in the revised paper ( see Section 3.3 after eq.1 ) : \u201c Note that this formulation allows each feature to contribute to multiple groups as it allows features to be selected multiple times in different groups. \u201d - * * Comment : In the experimental part , I do n't understand the motivation for using images . I ca n't imagine a single case in which one is given raw sensory data with absolutely no structure on it . * * Response : We agree with you that image datasets are not the best use case for the suggested method . We only included the CIFAR10 results as this commonly well-known benchmark used in the literature enabling further comparisons with other SOTA methods . In our experiments , in addition to one image dataset , we used 7 real-world non-image datasets to demonstrate the effectiveness of the proposed method . To address your comment , in the revised version , we added a new appendix ( Appendix E ) and used the Diabetes dataset to re-evaluate the ablation studies for a non-image dataset . Further , in the revised manuscript , we conducted all additional experiments on non-image datasets to conform with your comment , and provide our readers with a better view of the proposed method and the problem it intends to solve . - * * Comment : Concerning the other datasets ( the tabular ones ) , the authors report a decent improvement compared to an MLP , with a significant decrease in complexity . However , I do not understand the rationale for excluding random forests ( or any non-neural baselines ) from this comparison . A random forest would be the default algorithm in these situations ( both in terms of accuracy and speed ) , and making MLPs competitive is one of the motivating factors in the paper itself . * * Response : To address your comment , as suggested , we included random forest classification ( RFC ) results in the revised Table 2 . In our RFC implementation , we used ensembles of 1000 trees trained with the gini criterion and adjusted the maximum tree depth for the best accuracy . Based on the results , GMLP outperforms the random forest baseline on all of the datasets . - * * Comment : Is interpretability a concern here ? Inspecting the routing matrix should reveal information on the feature grouping . I feel this can provide a strong boost to the paper . * * Response : Thank you for suggesting this . We do not believe that interpretability is a main concern of this paper as the suggested objective function does not focus on interpretability . However , we believe that studying interpretability for GMLP architectures would be a very interesting subject for future studies . In the revised paper , we added a new appendix ( see Appendix G ) to use graph visualizations to demonstrate feature groups and their relationship for GMLP networks trained on three different datasets . Graph representation of feature groups can be a potential avenue for further analysis of group interactions ."}, {"review_id": "o2ko2D_uvXJ-2", "review_text": "The authors proposed a neural network architecture , Group-Connected Multilayer Perceptron ( GMLP ) , which automatically groups the input features and extracts high level representations according to the groups . This paper focuses on classification problems . The architecture can be decomposed to three stages . The first stage is to automatically group the input features by multiplying soft-max of a routing matrix . At the second stage , a locally fully connected layer with the corresponding activation functions is used for each group , and a pooling layer merges two groups to a new group . At the final stage , all the groups would be concatenated and input to a fully connect layer to get the final output . The experiments showed that GMLP outperforms vanilla MLP , SNN , SET , FGR on seven real-world classification datasets in different domains . GMLP ensures higher accuracy with lower complexity compared to vanilla MLPs . The authors claimed that if we consider the groups as leaves , this method then becomes growing a binary tree from the leaf to the root . The extensive experiment results showed the effect of GMLP hyper-parameter choices , e.g.number of groups ( number of nodes ) , width of each group ( size of nodes ) and type of pooling layer ( way to built parent nodes ) . But in terms of a tree , it would be interesting to have some experiments to show the effect of the way combining feature groups . The experiments on the simulated Bayesian network dataset supported the claim that this architecture can utilize the fact that some of the features are not related and do not need to interact with each other . However , the architecture the authors used corresponds to the model that generated data , which is almost impossible in many real life problems . It can be helpful to have some results on simulated data with mismatched architectures from the model to help better understand the performance . One of the most important ideas in this paper is limiting the group-wise interactions . The size and number of groups the experiments chose would lead to many overlapped groups and many features chosen multiple times . It would be nice to have some analyses on the chosen groups and selected features , e.g.the existence of a set of features that always come into one group , and/or comparison of derived groups by GMLP and randomly chosen groups . A more detailed explanation of the dataset would be needed . For example , the authors used MIT-BIH dataset to compare the accuracy of GMLP and MLP with different sizes without introducing the dataset . If the complexity analysis of GMLP , equation ( 7 ) , is only for inference , please also include the training complexity . Another concern is that the results suggest that the number of feature groups may need to be quite large ( compared to the number of original features ) . In addition to figure 2 , please provide the analysis of model size in addition to complexity analysis . GMLP selects features by using soft-max of a $ km \\times d $ matrix . The authors may want to investigate reparametrization tricks to solve similar problems , including concrete relaxation in the following references : C. J. Maddison , Andriy Mnih , and Yee Whye Teh . The concrete distribution : A continuous relaxation of discrete random variables . In ICLR , 2017 . Muhammed Fatih Balin , Abubakar Abid , and James Y. Zou . Concrete autoencoders : Differentiable feature selection and reconstruction . In ICML , 2019 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Comment : A more detailed explanation of the dataset would be needed . For example , the authors used MIT-BIH dataset to compare the accuracy of GMLP and MLP with different sizes without introducing the dataset . * * Response : Thank you for pointing out the missing information . We added the MIT-BIH dataset to the text and included the details as well as a link to the data in the revised Table 1 . - * * Comment : If the complexity analysis of GMLP , equation ( 7 ) , is only for inference , please also include the training complexity . Another concern is that the results suggest that the number of feature groups may need to be quite large ( compared to the number of original features ) . In addition to figure 2 , please provide the analysis of model size in addition to complexity analysis . * * Response : To address your comment , we provided the training time memory and compute complexity as well as a discussion on the model size in Appendix I of the revised version . Regarding your concern about model size or memory use , we were able to conduct all the experiments in this paper on a mid-range GPU with 11GB memory . - * * Comment : GMLP selects features by using soft-max of a km\u00d7d matrix . The authors may want to investigate reparametrization tricks to solve similar problems , including concrete relaxation in the following references : C. J. Maddison , Andriy Mnih , and Yee Whye Teh . The concrete distribution : A continuous relaxation of discrete random variables . In ICLR , 2017 . Muhammed Fatih Balin , Abubakar Abid , and James Y. Zou . Concrete autoencoders : Differentiable feature selection and reconstruction . In ICML , 2019 . * * Response : As suggested , we conducted experiments using Group-Select layers that are implemented using the concrete relaxation method . See Appendix K : \u201c Comparison of the Softmax and Concrete Relaxations \u201d of the revised paper . In summary , we did not observe any improvement using the concrete distribution . We hypothesize that as the major use case of the concrete distribution is in variational methods and it involves random sampling , it might be injecting a level of noise and variation that is not necessarily helpful for learning the feature groups ."}], "0": {"review_id": "o2ko2D_uvXJ-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pro : $ \\bullet $ It is an exciting idea to learn the relations in among feature dimensions , group them , and apply the operations within groups to train an MLP . The experiments on various datasets , and important ablations such as the effect of number and size of groups , types of pooling were done . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : $ \\bullet $ $ \\mathtt { Group-Pool } $ operation applies pooling inside feature group . Looking into Table 6 of the Appendix , mean and max-pooling were used in different datasets . The ablation study is done on CIFAR-10 ( Fig.4 ) .The assumption is always to have pairs of groups and apply pooling to consecutive group members . It contradicts the initial idea . Why did n't you apply a routing layer to learn which layers to pool together ? $ \\bullet $ * Ablation studies on image dataset : * Among many datasets experimented on ; it is difficult to understand why all ablation studies are conducted on CIFAR-10 . The paper 's main motivation is to propose a method that works on domains beyond image , voice , and graphs . However , all ablations are done on an image dataset naturally that has spatial relations . $ \\bullet $ * Incremental performance gain : * Looking into the performance comparison with vanilla MLP and other methods ( SNN , SET , and FGR ) , the only considerable improvement from the MLP baseline is on the CIFAR-10 dataset . The results on all other datasets , accuracy , and area under curve scores are incremental . How many trials did you conduct the same experiments ( the stdev values inside brackets of Table 2 ) ? Did you run any statistical significance test between the trials of GMLP and MLP ? MLP baselines can be reported from an architecture matching with the number of parameters in GMLP . In Table 2 , the number of parameters should be written or standardized in each method . $ \\bullet $ * What are `` MIT-BIH datasets '' ? * They are not mentioned , referred to in the text , but given in Fig.2 ( b ) $ \\bullet $ * Finding optimal numbers of m and k : * Finding the optimal number of groups and group sizes can introduce a large computational overhead . Fig.5 and 6 show the performance margin could be 10-15 % . Did you compare the optimal numbers of $ m $ and $ k $ between the datasets with a similar feature dimension ? In other words , are the optimal values calculated in one dataset generalizable to other datasets to eliminate the necessity to train many models to find a good value . $ \\bullet $ How does the group routing evolve during training ? In a learned model , what is the relation between inter-group and intra-group feature correlations ? ( Are the members of the same group more relevant to each other in vice versa ? ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Minor : $ \\bullet $ * As a term , the use of `` group '' : * Use of group as a term causes confusion in the reader . In the introduction , it may be useful to include that the expression is not related to group ina mathematical sense , and only represents a subset of feature dimensions .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Comment : What are `` MIT-BIH datasets '' ? They are not mentioned , referred to in the text , but given in Fig.2 ( b ) * * Response : Thank you for pointing out the missing information . We added the MIT-BIH dataset to the text and included the details as well as a link to the data in the revised Table 1 . - * * Comment : Finding optimal numbers of m and k : Finding the optimal number of groups and group sizes can introduce a large computational overhead . Fig.5 and 6 show the performance margin could be 10-15 % . Did you compare the optimal numbers of m and k between the datasets with a similar feature dimension ? In other words , are the optimal values calculated in one dataset generalizable to other datasets to eliminate the necessity to train many models to find a good value . * * Response : In our experiments , we searched for the optimal m and k hyperparameters for each dataset ( see Appendix A ) . In general , and as we see from the experimental results , the optimal m and k values are highly dependent on the dataset . There is no exact relationship between the number of features and optimal m and k values . However , as a rule of thumb , we found that the number of features ( d ) usually has the same order as m * k. Regarding your concern about the computational overhead , in our experiments , we were able to find appropriate hyperparameters for our largest datasets in less than 72 hours on a mid-range machine . - * * Comment : How does the group routing evolve during training ? In a learned model , what is the relation between inter-group and intra-group feature correlations ? ( Are the members of the same group more relevant to each other in vice versa ? ) * * Response : To address your comment , as suggested , we conducted experiments to analyze inter-group and intra-group feature correlations . See Appendix J : \u201c Analysis of Intra-Group and Inter-Group Correlations \u201d in the revised paper . Based on the results in Fig.22 and 23 , we did not find any significant difference in the correlation values for features within each group and features between different groups . We believe that this is an expected result as the objective function is based on a classification loss and does not enforce any correlation among the learned feature groups . Note that often a level of inter-group and intra-group redundancy improves the robustness of the trained models . In addition to this analysis , in Appendix G , we provided a method to translate feature groups to a graph representation and then used graph visualization and clustering techniques to visually demonstrate feature selection and coappearance in GMLP networks . Based on the results , feature groups appear as clusters in the graph representation and their connection with other groups is highly dependent on the dataset and the specific group itself . In general , we observed patterns such as features that appear multiple times in many different groups and features that only appear in certain groups ( see Appendix G and Fig.18 ) .- * * Comment : As a term , the use of `` group '' : Use of group as a term causes confusion in the reader . In the introduction , it may be useful to include that the expression is not related to group in a mathematical sense , and only represents a subset of feature dimensions . * * Response : Thank you for suggesting this . We added a footnote in the introduction section to clarify this for our readers : \u201c In this paper , the expression `` group '' is not related to the group in a mathematical sense , and it only represents a subset of features . \u201d"}, "1": {"review_id": "o2ko2D_uvXJ-1", "review_text": "The paper describes an MLP architectures for problems in which the features do not have a known structure ( eg , tabular data ) . A `` differentiable routing matrix '' partitions the data into K blocks . Then , standard MLPs are applied to each block and the results are recursively aggregated by moving forward in the model . On the plus side , the paper is well written , the topic is significant ( as evidenced by any Kaggle competition on tabular data ) , and the model is simple enough to be understandable even on a quick reading . However , I do have a few concerns that put the paper on a borderline situation . [ Addendum after review : most of the concerns have been addressed by the authors with a large set of additional experiments . ] 1.The paper is based on the assumption that a `` split-then-combine '' prior is sufficiently flexible to handle most real-world cases . However , this is kind of strongly limiting , eg , each feature can only contribute to a single `` high-level concept '' in the network . I do n't feel that the paper does a strong job in justifying such an inductive bias . [ Addendum after review : As the authors point out , this is only partially true , as each feature can be selected for multiple groups . ] 2.In the experimental part , I do n't understand the motivation for using images . I ca n't imagine a single case in which one is given raw sensory data with absolutely no structure on it . 3.Concerning the other datasets ( the tabular ones ) , the authors report a decent improvement compared to an MLP , with a significant decrease in complexity . However , I do not understand the rationale for excluding random forests ( or any non-neural baselines ) from this comparison . A random forest would be the default algorithm in these situations ( both in terms of accuracy and speed ) , and making MLPs competitive is one of the motivating factors in the paper itself . [ Addendum after review : the authors have added a random forest to the experiments . However , this is the only method whose parameters are not fine-tuned ( which might be a smaller problem for random forests ) . No time comparison is given , and some accuracy deviations appear well within the standard deviations . Still , this is a very good addition to the paper . ] 4.Is interpretability a concern here ? Inspecting the routing matrix should reveal information on the feature grouping . I feel this can provide a strong boost to the paper . Summarizing , point ( 3 ) is the most important . Showing that the proposed Group MLP is superior to a classical alternative should be the critical aim of the comparisons . [ Addendum after review : This point has been partially addressed . ]", "rating": "7: Good paper, accept", "reply_text": "Thank you for reviewing the manuscript and helpful comments . Please find a point-to-point response to your comments in the following . - * * Comment : The paper is based on the assumption that a `` split-then-combine '' prior is sufficiently flexible to handle most real-world cases . However , this is kind of strongly limiting , eg , each feature can only contribute to a single `` high-level concept '' in the network . I do n't feel that the paper does a strong job in justifying such an inductive bias . * * Response : In the proposed method each feature can contribute to multiple concepts as there are no restrictions on features being selected multiple times in different groups . Note that in Section 3.3 , the routing matrix in the group-select layer has the flexibility to include each feature in different groups as the rows in the \\psi matrix are trained independently . As such , the proposed method is not as restrictive as a \u201c split-then-combine \u201d inductive bias . To address your comment and to clarify this for our readers , we added the following explanation in the revised paper ( see Section 3.3 after eq.1 ) : \u201c Note that this formulation allows each feature to contribute to multiple groups as it allows features to be selected multiple times in different groups. \u201d - * * Comment : In the experimental part , I do n't understand the motivation for using images . I ca n't imagine a single case in which one is given raw sensory data with absolutely no structure on it . * * Response : We agree with you that image datasets are not the best use case for the suggested method . We only included the CIFAR10 results as this commonly well-known benchmark used in the literature enabling further comparisons with other SOTA methods . In our experiments , in addition to one image dataset , we used 7 real-world non-image datasets to demonstrate the effectiveness of the proposed method . To address your comment , in the revised version , we added a new appendix ( Appendix E ) and used the Diabetes dataset to re-evaluate the ablation studies for a non-image dataset . Further , in the revised manuscript , we conducted all additional experiments on non-image datasets to conform with your comment , and provide our readers with a better view of the proposed method and the problem it intends to solve . - * * Comment : Concerning the other datasets ( the tabular ones ) , the authors report a decent improvement compared to an MLP , with a significant decrease in complexity . However , I do not understand the rationale for excluding random forests ( or any non-neural baselines ) from this comparison . A random forest would be the default algorithm in these situations ( both in terms of accuracy and speed ) , and making MLPs competitive is one of the motivating factors in the paper itself . * * Response : To address your comment , as suggested , we included random forest classification ( RFC ) results in the revised Table 2 . In our RFC implementation , we used ensembles of 1000 trees trained with the gini criterion and adjusted the maximum tree depth for the best accuracy . Based on the results , GMLP outperforms the random forest baseline on all of the datasets . - * * Comment : Is interpretability a concern here ? Inspecting the routing matrix should reveal information on the feature grouping . I feel this can provide a strong boost to the paper . * * Response : Thank you for suggesting this . We do not believe that interpretability is a main concern of this paper as the suggested objective function does not focus on interpretability . However , we believe that studying interpretability for GMLP architectures would be a very interesting subject for future studies . In the revised paper , we added a new appendix ( see Appendix G ) to use graph visualizations to demonstrate feature groups and their relationship for GMLP networks trained on three different datasets . Graph representation of feature groups can be a potential avenue for further analysis of group interactions ."}, "2": {"review_id": "o2ko2D_uvXJ-2", "review_text": "The authors proposed a neural network architecture , Group-Connected Multilayer Perceptron ( GMLP ) , which automatically groups the input features and extracts high level representations according to the groups . This paper focuses on classification problems . The architecture can be decomposed to three stages . The first stage is to automatically group the input features by multiplying soft-max of a routing matrix . At the second stage , a locally fully connected layer with the corresponding activation functions is used for each group , and a pooling layer merges two groups to a new group . At the final stage , all the groups would be concatenated and input to a fully connect layer to get the final output . The experiments showed that GMLP outperforms vanilla MLP , SNN , SET , FGR on seven real-world classification datasets in different domains . GMLP ensures higher accuracy with lower complexity compared to vanilla MLPs . The authors claimed that if we consider the groups as leaves , this method then becomes growing a binary tree from the leaf to the root . The extensive experiment results showed the effect of GMLP hyper-parameter choices , e.g.number of groups ( number of nodes ) , width of each group ( size of nodes ) and type of pooling layer ( way to built parent nodes ) . But in terms of a tree , it would be interesting to have some experiments to show the effect of the way combining feature groups . The experiments on the simulated Bayesian network dataset supported the claim that this architecture can utilize the fact that some of the features are not related and do not need to interact with each other . However , the architecture the authors used corresponds to the model that generated data , which is almost impossible in many real life problems . It can be helpful to have some results on simulated data with mismatched architectures from the model to help better understand the performance . One of the most important ideas in this paper is limiting the group-wise interactions . The size and number of groups the experiments chose would lead to many overlapped groups and many features chosen multiple times . It would be nice to have some analyses on the chosen groups and selected features , e.g.the existence of a set of features that always come into one group , and/or comparison of derived groups by GMLP and randomly chosen groups . A more detailed explanation of the dataset would be needed . For example , the authors used MIT-BIH dataset to compare the accuracy of GMLP and MLP with different sizes without introducing the dataset . If the complexity analysis of GMLP , equation ( 7 ) , is only for inference , please also include the training complexity . Another concern is that the results suggest that the number of feature groups may need to be quite large ( compared to the number of original features ) . In addition to figure 2 , please provide the analysis of model size in addition to complexity analysis . GMLP selects features by using soft-max of a $ km \\times d $ matrix . The authors may want to investigate reparametrization tricks to solve similar problems , including concrete relaxation in the following references : C. J. Maddison , Andriy Mnih , and Yee Whye Teh . The concrete distribution : A continuous relaxation of discrete random variables . In ICLR , 2017 . Muhammed Fatih Balin , Abubakar Abid , and James Y. Zou . Concrete autoencoders : Differentiable feature selection and reconstruction . In ICML , 2019 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * Comment : A more detailed explanation of the dataset would be needed . For example , the authors used MIT-BIH dataset to compare the accuracy of GMLP and MLP with different sizes without introducing the dataset . * * Response : Thank you for pointing out the missing information . We added the MIT-BIH dataset to the text and included the details as well as a link to the data in the revised Table 1 . - * * Comment : If the complexity analysis of GMLP , equation ( 7 ) , is only for inference , please also include the training complexity . Another concern is that the results suggest that the number of feature groups may need to be quite large ( compared to the number of original features ) . In addition to figure 2 , please provide the analysis of model size in addition to complexity analysis . * * Response : To address your comment , we provided the training time memory and compute complexity as well as a discussion on the model size in Appendix I of the revised version . Regarding your concern about model size or memory use , we were able to conduct all the experiments in this paper on a mid-range GPU with 11GB memory . - * * Comment : GMLP selects features by using soft-max of a km\u00d7d matrix . The authors may want to investigate reparametrization tricks to solve similar problems , including concrete relaxation in the following references : C. J. Maddison , Andriy Mnih , and Yee Whye Teh . The concrete distribution : A continuous relaxation of discrete random variables . In ICLR , 2017 . Muhammed Fatih Balin , Abubakar Abid , and James Y. Zou . Concrete autoencoders : Differentiable feature selection and reconstruction . In ICML , 2019 . * * Response : As suggested , we conducted experiments using Group-Select layers that are implemented using the concrete relaxation method . See Appendix K : \u201c Comparison of the Softmax and Concrete Relaxations \u201d of the revised paper . In summary , we did not observe any improvement using the concrete distribution . We hypothesize that as the major use case of the concrete distribution is in variational methods and it involves random sampling , it might be injecting a level of noise and variation that is not necessarily helpful for learning the feature groups ."}}