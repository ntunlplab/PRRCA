{"year": "2019", "forum": "SJlt6oA9Fm", "title": "Selective Convolutional Units: Improving CNNs via Channel Selectivity", "decision": "Reject", "meta_review": "This paper proposed Selective Convolutional Unit (SCU) for improving the 1x1 convolutions used in the bottleneck of a ResNet block. The main idea is to remove channels of low \u201cimportance\u201d and replace them by other ones which are in a similar fashion found to be important. To this end the authors propose the so-called expected channel damage score (ECDS) which is used for channel selection. The authors also show the effectiveness of SCU on CIFAR-10, CIFAR-100 and Imagenet.\n\nThe major concerns from various reviewers are that the design seems the over-complicated as well as the experiments are not state-of-the-art. In response, the authors add some explanations on the design idea and new experiments of DenseNet-BC-190 on CIFAR10/100. But the reviewers\u2019 major concerns are still there and did not change their ratings (6,5,5). Based on current results, the paper is proposed for borderline lean reject.  \n", "reviews": [{"review_id": "SJlt6oA9Fm-0", "review_text": "This paper propose Selective Convolutional Unit (SCU), which can replace the bottleneck in Resnet block. The difference between SCU and bottleneck is that SCU adds Channel Distributor (CD) and Noise Controller (NC) to reduce and replace the channels. This paper also propose Expected channel damage score (ECDS) to measure the importance of a channel to decide weather remove or replace it. Then the experiment shows result on cifar10/100 and imagenet data set with different network architectures. The idea is interesting, however, the parameter flops reduced rate seems not very impressive. The SCU seems too complicated,so I want to know that if the SCU could accelerate the forward process on modern GPU or mobile devices? The result of these networks seems not the state-of-the-art, if the result can be improved, the SCU could be more convincing. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Many thanks for your time and effort to review our paper . We respond to your questions and concerns one-by-one in what follows . In addition , please check out the common response we have posted together , that addresses several important concerns raised by multiple reviewers in common . Q1. \u201c The parameter flops reduced rate seems not very impressive. \u201d - As we mentioned in our common response , \u201c improving pruning efficiency \u201d is not our major goal . Even though we place NC for inducing more sparsity on training ( as pruning efficiency depends on training scheme ) , this is far from our key contributions , but closer to adopting Bayesian learning for better efficiency . Rather , we aim to explore a \u201c safer way \u201d of pruning and rewiring , without any post-processing after training . Namely , our goal is to achieve accuracy improvement and model compression simultaneously on a single pass of training . In this regard , our work is more about designing a new training scheme , complementary to any pruning works , for improving the network efficiency that allows some balancing between accuracy and compression on demand . Q2. \u201c Can the SCU accelerate the forward process ? \u201d - Although implementing SCU for maximal acceleration is outside our scope , we expect that our method will do help on acceleration of networks at the inference time . Recall that SCU has two additional layers ( NC and CD ) compared to the standard BN-ReLU-Conv . The complexity from the layers , however , can be eliminated for those who need efficient inference : ( a ) As mentioned in Section 2.4 , noises in NC can be replaced by its expected values for faster inference . In fact , even the entire NC layer can be omitted by multiplying the expected values to the parameters in the former BN layer . ( b ) In the case that SCU is trained using only dealloc ( for compression ) , CD contains no spatial shifting so that the role of CD is nothing but re-indexing channels . Overall , the only expense of using SCU is a channel-selection layer via tensor indexing operation , while the remaining layers can work in a much smaller dimension in return . Comparative evaluations of CPU inference time in the table below show that SCU further improves the efficiency of CondenseNet-182 model through the training process , while outperforming other competitive models . CPU * inference time ( per image ) + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- + | Model | Before | After | Error | | | training | training | rates | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- + | ResNeXt-29 | - | 471.2ms | 3.58 % | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- + | DenseNet-BC-250 ( k=24 ) | - | 399.5ms | 3.64 % | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- + | CondenseNet-SCU-182 | 114.8ms | * 52.5ms * | 3.63 % | | | | ( -54.3 % ) | | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- + * Intel Xeon E5-2630v4"}, {"review_id": "SJlt6oA9Fm-1", "review_text": "This is an architecture design paper. It proposes a general structure called Selective Convolutional Unit that the authors claim to be useful for various CNN models. The SCU structure contains two major parts: CD and NC. CD for compressing/pruning channels and NC for multiplicative noise. The paper gives a measure, called expected channel damage score, on the change of the output for SCU. It also shows the effectiveness of SCU on CIFAR-10, CIFAR-100 and imagenet. Some questions and concerns: 1. The paper spends too much space introducing the bottleneck structures and a whole lot of the details on the optimization of NC and CD are put in the appendix. I would suggest to reduce the section of introductory part and put a shorter version of appendix A and B to the main text so that the readers know more about the architecture and how it is optimized. In particular, the description on NC is confusing since without looking at the appendix it is not clear how the prior p(\\theta) is used. 2. The experiment shows improvement on densenet and resnetXT, but the result is not the state-of-the-art. Wide-Resnet seems to get better accuracy on both CIFAR-10 and CIFAR-100 compared to the best accuracy reported in this paper. Also the number reported by the original densenet paper on imagenet seems to be better (densenet-264 has an error rate of 22.15/20.80) 3. In your CD design, channel assignment \\pi is a discrete variable. How is it optimized in the training process? 4. The proof of proposition 1 does not look correct to me. The optimization procedure makes use of the data X to determine your NC variable \\theta so \\theta depends on X. In this way you cannot factorize the expectation in the equation below (20) in your appendix. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Many thanks for your time and effort to review our paper . We respond to your questions and concerns one-by-one in what follows . In addition , please check out the common response we have posted together , that addresses several important concerns raised by multiple reviewers in common . Q1.For your editorial suggestions - Many thanks for your thoughtful editorial suggestions . We revised the draft following them , where major revisions are marked by \u201c red \u201d . Q2. \u201c How is the discrete \\pi optimized in the training process ? \u201d - As we describe in Section 2.4 , \\pi is not directly trained via SGD , but updated via dealloc and realloc operations during training . Q3. \u201c The proof of proposition 1 does not look correct to me. \u201d - We remark that Proposition 1 does not involve anything related to optimization with X . Proposition 1 can be applied regardless on whether the network is trained or not ( even in the case that the network is randomly initialized ) . Given that the network is fixed , \\theta is completely independent on the distribution of X by design of NC , so that we can factorize the expectation ."}, {"review_id": "SJlt6oA9Fm-2", "review_text": "The main contribution of the paper are a set of new layers for improving the 1x1 convolutions used in the bottleneck of a ResNet block. The main idea is to remove channels of low \u201cimportance\u201d and replace them by other ones which are in a similar fashion found to be important. To this end the authors propose the so-called expected channel damage score (ECDS) which is used for channel selection. The authors have shown in their paper that the new layers improve performance mainly on CIFAR, while there\u2019s also an experiment on ImageNet It looks to me that the proposed method is overly complicated. It is also described in a complicated manner. I don't see clear motivation for re-using the same features. Also I did not understand the usefulness of applying the spatial shifting of the so-called Channel Distributor. It is also not clear whether the proposed technique is applicable to only bottleneck layers. The results show some improvement but not great and over results that as far as I know are not state-of-the-art (to my knowledge the presented results on CIFAR are not state-of-the-art). The results on ImageNet also show decent but not great improvement. Moreover, the gain in reducing the model parameters is not that great as the R parameters are only a small fraction of the total model parameters. Overall, the paper presents some interesting ideas but the proposed approach seems over-complicated", "rating": "5: Marginally below acceptance threshold", "reply_text": "Many thanks for your time and effort to review our paper . We respond to your questions and concerns one-by-one in what follows . In addition , please check out the common response we have posted together , that addresses several important concerns raised by multiple reviewers in common . Q1 . `` I do n't see clear motivation for re-using the same features . '' - The motivation for the re-using is to give important features more parameters . As explained in Section 2.1 , notice that a 1x1 convolution performs nothing but a `` pixel-wise linear transformation '' on feature dimension , so that its parameters can be represented by a N x N \u2019 matrix , provided that N= ( # input channels ) and N \u2019 = ( # output channels ) . This implies that a single input channel is processed by 1 x N \u2019 parameters when an input comes into the layer , and therefore re-using a feature n times implies that the feature is processed by n x N \u2019 parameters . Q2 . `` I did not understand the usefulness of applying the spatial shifting . '' - As explained in Section 2.2 in more details , spatial shifting is a trick to properly utilize the re-used parameters . Considering again that n copies of a feature occupies n x N \u2019 parameters of the matrix ( Q1-1 above ) , one may notice that the naive copy would not help on expressivity of the convolution , since it is basically a linear transformation . Even though SCU contains ReLU inside the structure , this kind of phenomenon does happen during training . By using spatial shifting , we now can utilize the n x N \u2019 parameters for `` enlarging '' the convolution kernel specially for the feature . Ablation study on spatial shifting demonstrated in Figure 3a clearly shows its effectiveness . Q3 . `` It is also not clear whether the proposed technique is applicable to only bottleneck layers . '' - We expect that the proposed method is still valid for other than bottleneck ( it is mentioned in Section 4 ) . Nevertheless , we primarily focus on the bottleneck setting under the presence of identity connection , because we expect this scenario is one of the best applications of channel-selectivity . In Section 2.1 of the revised draft , we provide more detailed intuitions and motivations why we study such bottleneck layers . Q4. \u201c The gain in reducing the model parameters is not that great as the R parameters are only a small fraction of the total model parameters. \u201d - The fraction of bottlenecks for the total parameters is NOT always small , and several state-of-the-art models invest very large portion of parameters on bottlenecks as follows : ( a ) CondenseNet-SCU-182 model presented in Table 3 is a nice example of achieving high efficiency by exploiting its high fraction of bottlenecks . Initially , CondenseNet-SCU-182 has 6.29M parameters with 741M FLOPs in total before training the model . As reported in Table 3 , these values can be reduced to 2.59M and 286M , respectively , and this reduction is mainly due to compression on the bottlenecks . In fact , this model invests 5.89M parameters only for bottlenecks , which is * 93.7 % * of the total parameters . ( b ) DenseNet-BC-190 , newly added in the revised draft as the state-of-art model also invests a lot of parameters for bottleneck , namely 17.5M ( as reported in Table 1 ) out of 25.6M . In general , DenseNet models heavily rely on bottleneck structure for efficiency , and the overhead from the bottleneck itself becomes increasingly large as the model grows . As the examples demonstrate , reducing overhead from bottlenecks has been one of the crucial barriers for designing a large-scale , yet efficient CNN model ."}], "0": {"review_id": "SJlt6oA9Fm-0", "review_text": "This paper propose Selective Convolutional Unit (SCU), which can replace the bottleneck in Resnet block. The difference between SCU and bottleneck is that SCU adds Channel Distributor (CD) and Noise Controller (NC) to reduce and replace the channels. This paper also propose Expected channel damage score (ECDS) to measure the importance of a channel to decide weather remove or replace it. Then the experiment shows result on cifar10/100 and imagenet data set with different network architectures. The idea is interesting, however, the parameter flops reduced rate seems not very impressive. The SCU seems too complicated,so I want to know that if the SCU could accelerate the forward process on modern GPU or mobile devices? The result of these networks seems not the state-of-the-art, if the result can be improved, the SCU could be more convincing. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Many thanks for your time and effort to review our paper . We respond to your questions and concerns one-by-one in what follows . In addition , please check out the common response we have posted together , that addresses several important concerns raised by multiple reviewers in common . Q1. \u201c The parameter flops reduced rate seems not very impressive. \u201d - As we mentioned in our common response , \u201c improving pruning efficiency \u201d is not our major goal . Even though we place NC for inducing more sparsity on training ( as pruning efficiency depends on training scheme ) , this is far from our key contributions , but closer to adopting Bayesian learning for better efficiency . Rather , we aim to explore a \u201c safer way \u201d of pruning and rewiring , without any post-processing after training . Namely , our goal is to achieve accuracy improvement and model compression simultaneously on a single pass of training . In this regard , our work is more about designing a new training scheme , complementary to any pruning works , for improving the network efficiency that allows some balancing between accuracy and compression on demand . Q2. \u201c Can the SCU accelerate the forward process ? \u201d - Although implementing SCU for maximal acceleration is outside our scope , we expect that our method will do help on acceleration of networks at the inference time . Recall that SCU has two additional layers ( NC and CD ) compared to the standard BN-ReLU-Conv . The complexity from the layers , however , can be eliminated for those who need efficient inference : ( a ) As mentioned in Section 2.4 , noises in NC can be replaced by its expected values for faster inference . In fact , even the entire NC layer can be omitted by multiplying the expected values to the parameters in the former BN layer . ( b ) In the case that SCU is trained using only dealloc ( for compression ) , CD contains no spatial shifting so that the role of CD is nothing but re-indexing channels . Overall , the only expense of using SCU is a channel-selection layer via tensor indexing operation , while the remaining layers can work in a much smaller dimension in return . Comparative evaluations of CPU inference time in the table below show that SCU further improves the efficiency of CondenseNet-182 model through the training process , while outperforming other competitive models . CPU * inference time ( per image ) + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- + | Model | Before | After | Error | | | training | training | rates | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- + | ResNeXt-29 | - | 471.2ms | 3.58 % | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- + | DenseNet-BC-250 ( k=24 ) | - | 399.5ms | 3.64 % | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- + | CondenseNet-SCU-182 | 114.8ms | * 52.5ms * | 3.63 % | | | | ( -54.3 % ) | | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + -- -- -- -- -- + * Intel Xeon E5-2630v4"}, "1": {"review_id": "SJlt6oA9Fm-1", "review_text": "This is an architecture design paper. It proposes a general structure called Selective Convolutional Unit that the authors claim to be useful for various CNN models. The SCU structure contains two major parts: CD and NC. CD for compressing/pruning channels and NC for multiplicative noise. The paper gives a measure, called expected channel damage score, on the change of the output for SCU. It also shows the effectiveness of SCU on CIFAR-10, CIFAR-100 and imagenet. Some questions and concerns: 1. The paper spends too much space introducing the bottleneck structures and a whole lot of the details on the optimization of NC and CD are put in the appendix. I would suggest to reduce the section of introductory part and put a shorter version of appendix A and B to the main text so that the readers know more about the architecture and how it is optimized. In particular, the description on NC is confusing since without looking at the appendix it is not clear how the prior p(\\theta) is used. 2. The experiment shows improvement on densenet and resnetXT, but the result is not the state-of-the-art. Wide-Resnet seems to get better accuracy on both CIFAR-10 and CIFAR-100 compared to the best accuracy reported in this paper. Also the number reported by the original densenet paper on imagenet seems to be better (densenet-264 has an error rate of 22.15/20.80) 3. In your CD design, channel assignment \\pi is a discrete variable. How is it optimized in the training process? 4. The proof of proposition 1 does not look correct to me. The optimization procedure makes use of the data X to determine your NC variable \\theta so \\theta depends on X. In this way you cannot factorize the expectation in the equation below (20) in your appendix. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Many thanks for your time and effort to review our paper . We respond to your questions and concerns one-by-one in what follows . In addition , please check out the common response we have posted together , that addresses several important concerns raised by multiple reviewers in common . Q1.For your editorial suggestions - Many thanks for your thoughtful editorial suggestions . We revised the draft following them , where major revisions are marked by \u201c red \u201d . Q2. \u201c How is the discrete \\pi optimized in the training process ? \u201d - As we describe in Section 2.4 , \\pi is not directly trained via SGD , but updated via dealloc and realloc operations during training . Q3. \u201c The proof of proposition 1 does not look correct to me. \u201d - We remark that Proposition 1 does not involve anything related to optimization with X . Proposition 1 can be applied regardless on whether the network is trained or not ( even in the case that the network is randomly initialized ) . Given that the network is fixed , \\theta is completely independent on the distribution of X by design of NC , so that we can factorize the expectation ."}, "2": {"review_id": "SJlt6oA9Fm-2", "review_text": "The main contribution of the paper are a set of new layers for improving the 1x1 convolutions used in the bottleneck of a ResNet block. The main idea is to remove channels of low \u201cimportance\u201d and replace them by other ones which are in a similar fashion found to be important. To this end the authors propose the so-called expected channel damage score (ECDS) which is used for channel selection. The authors have shown in their paper that the new layers improve performance mainly on CIFAR, while there\u2019s also an experiment on ImageNet It looks to me that the proposed method is overly complicated. It is also described in a complicated manner. I don't see clear motivation for re-using the same features. Also I did not understand the usefulness of applying the spatial shifting of the so-called Channel Distributor. It is also not clear whether the proposed technique is applicable to only bottleneck layers. The results show some improvement but not great and over results that as far as I know are not state-of-the-art (to my knowledge the presented results on CIFAR are not state-of-the-art). The results on ImageNet also show decent but not great improvement. Moreover, the gain in reducing the model parameters is not that great as the R parameters are only a small fraction of the total model parameters. Overall, the paper presents some interesting ideas but the proposed approach seems over-complicated", "rating": "5: Marginally below acceptance threshold", "reply_text": "Many thanks for your time and effort to review our paper . We respond to your questions and concerns one-by-one in what follows . In addition , please check out the common response we have posted together , that addresses several important concerns raised by multiple reviewers in common . Q1 . `` I do n't see clear motivation for re-using the same features . '' - The motivation for the re-using is to give important features more parameters . As explained in Section 2.1 , notice that a 1x1 convolution performs nothing but a `` pixel-wise linear transformation '' on feature dimension , so that its parameters can be represented by a N x N \u2019 matrix , provided that N= ( # input channels ) and N \u2019 = ( # output channels ) . This implies that a single input channel is processed by 1 x N \u2019 parameters when an input comes into the layer , and therefore re-using a feature n times implies that the feature is processed by n x N \u2019 parameters . Q2 . `` I did not understand the usefulness of applying the spatial shifting . '' - As explained in Section 2.2 in more details , spatial shifting is a trick to properly utilize the re-used parameters . Considering again that n copies of a feature occupies n x N \u2019 parameters of the matrix ( Q1-1 above ) , one may notice that the naive copy would not help on expressivity of the convolution , since it is basically a linear transformation . Even though SCU contains ReLU inside the structure , this kind of phenomenon does happen during training . By using spatial shifting , we now can utilize the n x N \u2019 parameters for `` enlarging '' the convolution kernel specially for the feature . Ablation study on spatial shifting demonstrated in Figure 3a clearly shows its effectiveness . Q3 . `` It is also not clear whether the proposed technique is applicable to only bottleneck layers . '' - We expect that the proposed method is still valid for other than bottleneck ( it is mentioned in Section 4 ) . Nevertheless , we primarily focus on the bottleneck setting under the presence of identity connection , because we expect this scenario is one of the best applications of channel-selectivity . In Section 2.1 of the revised draft , we provide more detailed intuitions and motivations why we study such bottleneck layers . Q4. \u201c The gain in reducing the model parameters is not that great as the R parameters are only a small fraction of the total model parameters. \u201d - The fraction of bottlenecks for the total parameters is NOT always small , and several state-of-the-art models invest very large portion of parameters on bottlenecks as follows : ( a ) CondenseNet-SCU-182 model presented in Table 3 is a nice example of achieving high efficiency by exploiting its high fraction of bottlenecks . Initially , CondenseNet-SCU-182 has 6.29M parameters with 741M FLOPs in total before training the model . As reported in Table 3 , these values can be reduced to 2.59M and 286M , respectively , and this reduction is mainly due to compression on the bottlenecks . In fact , this model invests 5.89M parameters only for bottlenecks , which is * 93.7 % * of the total parameters . ( b ) DenseNet-BC-190 , newly added in the revised draft as the state-of-art model also invests a lot of parameters for bottleneck , namely 17.5M ( as reported in Table 1 ) out of 25.6M . In general , DenseNet models heavily rely on bottleneck structure for efficiency , and the overhead from the bottleneck itself becomes increasingly large as the model grows . As the examples demonstrate , reducing overhead from bottlenecks has been one of the crucial barriers for designing a large-scale , yet efficient CNN model ."}}