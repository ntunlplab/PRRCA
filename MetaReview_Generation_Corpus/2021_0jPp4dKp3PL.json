{"year": "2021", "forum": "0jPp4dKp3PL", "title": "Integrating linguistic knowledge into DNNs: Application to online grooming detection", "decision": "Reject", "meta_review": "Most reviewers did not feel that this paper was ready for publication. I thank the authors for answering all the concerns of the reviewers, running new experiments and submitting a revised version, however, this was not not enough to alleviate the reviewers' concerns, notably relating to the handling of the ethical consideration in the writing of the manuscript.", "reviews": [{"review_id": "0jPp4dKp3PL-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper proposes two families of methods for integrating external knowledge into neural networks aimed at classifying instances of online grooming . The first family focuses on incorporating knowledge of word semantic similarity into their representations . The second , on different attention mechanisms for incorporating knowledge about theoretical stages of online grooming . The authors perform a solid amount of experiments to assess the differences between their suggested methods and baseline models . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : However , the way the paper is written and structured make it difficult to understand . While looking at the results , it is hard to really assess the contribution of each suggested strategy , and how they compare to each other . Finally , the paper presents some serious conceptual gaps that undermine its overall credibility . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : - Fair amount of experiments for assessing impact of each proposed strategy . - The ideas for modeling different word variants could be built upon . I especially like the idea of Elastic Pulling for combining the semantic information of word representations . This idea would be useful to the research community focusing on combining word representations . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : - Some serious conceptual gaps , and wrong claims . .- The paper is overall unclear and difficult to understand . - The task the paper is addressing is not well specified . - The methods are also not clearly explained . - Results are difficult to interpret and analyses are lacking . - Lack of ethical considerations for a system that could be used in law enforcement . I would have liked to see a more detailed discussion on the societal implications that systems aiding law enforcement could have , and in particular , which measures should be taken for avoiding the prosecution of innocent people by systems like this . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Comments and suggestions for the authors : - The term `` text normalisation '' is spread throughout the paper , but is never concretely defined , and is not immediately inferable . - I was not familiar with the `` word semantics representation '' noun phrase . After reading the paper I am pretty sure that you mean `` vector space model '' , or word embeddings . Why not use these terms that will probably be more familiar to potential readers ? - In Figure 1 ( left ) , you wrote Embedding ; at the right you wrote WSR . Are these equivalent ? - I understand that you are doing classification at the conversation level , but in page 3 , in the `` Base models '' paragraph , you mention that `` with the WSR 's embedding provided as input to the OG classifier in place of a sentence embedding '' . In order to classify a conversation , you need a vector representation of it . How is this obtained ? In other words , how are you aggregating the contextualized word representations ( i.e. , the output of the LSTM or the XLNet encoder ) , into a single vector representation of the conversation ? Are you using the last hidden state of the LSTM or a pooling method ? Are you using the [ CLS ] token of XLNet or something else ? - You mentioned that your dataset contains full conversations with an average of 431 messages per conversation . Are all the conversation turns separated by the [ SEP ] token ? What is the average message length ? What max input length did you use as a hyperparameter ? Did you use the same text input for Model 1 and Model 2 ? - Saying XLNet is the SoTA for NLP is a false statement ( p. 3 second-last paragraph ) . First of all , NLP encompasses several tasks and there is no single model superior to all the others in every task . Second , XLNet has already been beaten in several tasks . See the Glue benchmark ( https : //gluebenchmark.com/leaderboard ) for a few examples . You mention that XLNet is the SoTA of NLP again in p. 6 ; sec.5.1 ; second paragraph . - Saying that XLNet iteratively refines word embeddings from a WSR similar to that of LIU et al . ( 2017 ) is only tangentially true and is misleading , in my opinion . Recurrent models such as those relying on LSTMs , are profoundly different to those based on transformers such as XLNet . - I am not sure that there is a clear correspondence between using recurrent models such as LSTMs and better handling of class imbalance . How is a two-layer LSTM going to help handling class imbalance ? - You mentioned several times that `` text normalisation '' was a strong point of your contribution , but in the last paragraph of section 4 ( p.4 ) you say that you do not apply `` text normalisation '' . I understand that you might be referring to different kinds of normalisation , but I think this terminology is confusing . I suggest using more precise language to clearly differentiate what you are referring to . - What do you mean by `` intersection tests '' ? - I suggest using the word `` representations '' rather than `` coordinates '' for referring to the vector representation of a word ( p. 4 ; elastic pulling paragraph ) . - The correspondence between rows in Table 1 and the strategies discussed in section 4.2 , are not immediately apparent . - When not using GloVe embeddings , how did you initialize your word representations ? - You mention in Table 1 that bold are improved results , but improved with respect to what ? - You report precision and recall in Table 2 , but not in Table 1 . I think it would be valuable to include these in Table 1 , rather than the distance reduction and average resulting distance metrics which are only valid for a few strategies . - You could check the following papers for more context on combining representations of different word variants : * [ Attention-based Conditioning Methods for External Knowledge Integration ] ( https : //www.aclweb.org/anthology/P19-1385/ ) * [ Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics ] ( https : //www.aclweb.org/anthology/P13-1149/ ) * [ Composition in Distributional Models of Semantics ] ( https : //onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01106.x ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Some typos : - p. 4 ; Manifold Learning - `` by building , using manifold learning , a new space '' could be better written as `` by building through manifold learning a new space '' or `` by building a new space through manifold learning '' - p. 4 - `` it is possible reduce dimensionality '' - > `` it is possible to reduce dimensionality '' - p.5 , sec.4.2 , first paragraph - `` collocates and std the span '' . Not sure what std is supposed to mean here .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their insights . We are sorry that the reviewer finds the methods and discussions of experimental results hard to understand . In the revised paper , we have expanded ( as much as page limit allows ) the descriptions and discussions to alleviate this issue . We especially expanded the discussions around the ablation study , which evaluates and compares each suggested strategy . Next , we respond in turn to their raised questions and comments . We hope that this will address the criticism about `` conceptual gaps '' . If not , we would appreciate a more explicit description of these gaps , to allow us to address them . - * The task the paper is addressing is not well specified . * The task is the classification of conversations between instances of online grooming ( positive class ) , and neutral ( negative class ) conversations . In the revised submission , we have expanded the second sentence of the paper ( originally : \u201c The aim of this work is to detect instances of OG through classification of whole conversations. \u201d ) to clarify this . - * Lack of ethical considerations * As discussed in the introduction and in the dedicated discussion in Section 6 \u201c Perspectives for OG prevention \u201d , the system was co-developed with specialised law enforcement , and its \u201c intended usage is to facilitate triage by law enforcement \u201d . It is not intended to replace a human decision . Within this usage scenario , there is , therefore , no risk of innocents to be automatically prosecuted , since this decision is always made by a trained police officer after reviewing the conversation . In fact , not all courts would accept linguistic evidence . Our law enforcement partners only aim at using this system when they already have enough evidence , as part of their toolkit to analyse large quantities of seized digital materials including conversations . The co-development of the system with law enforcement ensures that their own security protocols will be followed throughout the usage of the system , to preserve integrity . In particular , the system would not be provided to any police officer , but only to trained operators , using robust mechanisms that are already being in use by law enforcement in the context of OG investigations . Therefore , while a human review of the flagged conversations might in general suffer from biases and prejudices , security protocols are already in place to mitigate these natural human biases . The aim of our work is * not * to address the possible biases in the human decision . However , our method may indirectly help in achieving a fairer handling of the flagged conversations . Indeed , the visualisation provided by our method eases the analysis of key parts of the conversation . Additionally , the triaging would allow operators to focus on a few flagged conversations while spending less time on the others . This reduced workload and associated lowered time pressure , associated with the visualisation , may allow a more thorough and fairer investigation . The \u201c Perspectives for OG prevention \u201d section has been expanded in the revised manuscript to reflect this discussion . While one may argue that our system may be diverted from its intended use and deployed more largely online without following the law enforcement \u2019 s robustness and security protocols , we wish to point out that this is the case for many AI systems , and therefore that this should be discussed at the higher level of the whole AI field and together with experts of other disciplines such as jurists . Indeed , the question of liability and responsibility arises in many AI applications , for example when an AI system may perturb stock exchange markets and economy or may endanger or even take the life of passengers of automatic cars . The AI-assisted detection of online grooming is indeed concerned with these questions of unintended usage and liability around AI . However , for a fuller and clearer debate , these questions should be ( and are [ 1-3 ] ) discussed more globally . [ 1 ] Liability for Artificial Intelligence and other emerging digital technologies , European Commission Report from the Expert Group on Liability and New Technologies \u2013 New Technologies Formation , 2019 ISBN 978-92-76-12959-2 , doi:10.2838/573689 https : //ec.europa.eu/transparency/regexpert/index.cfm ? do=groupDetail.groupMeetingDoc & docid=36608 [ 2 ] A. Bertolini , Artificial Intelligence and Civil Liability , Report from the European Parliament 's Committee on Legal Affairs , 2020 https : //www.europarl.europa.eu/RegData/etudes/STUD/2020/621926/IPOL_STU ( 2020 ) 621926_EN.pdf [ 3 ] J.K.C . Kingston , Artificial Intelligence and Legal Liability , International Conference on Innovative Techniques and Applications of Artificial Intelligence , 2016 https : //arxiv.org/ftp/arxiv/papers/1802/1802.07782.pdf"}, {"review_id": "0jPp4dKp3PL-1", "review_text": "This paper presents an approach to natural language processing which integrates corpus linguistics knowledge within deep neural networks ( namely , an LSTM-based architecture with attention ) . The approach is tailored and evaluated on a specific application , namely online grooming detection . The approach is based on ( 1 ) the normalization of word embeddings by exploiting word semantics representations and word variants ; ( 2 ) the decomposition of conversation analysis to identify subgoals , by exploiting online grooming processes ( or phases ) ; ( 3 ) the use of attention to modulate the input gate of LSTM cells . Another significant contribution of the paper is a novel corpus , which extends a previous one ( PAN2012 ) . The proposed neural architecture presents several variants in otder to incorporate linguistic knowledge within the model , and the paper reports about such an ablation study . In the experimental evaluation , linguistic knowledge is injected within two base models , namely one based on LSTMs , and the other one on XL-Net . Performance is shown to be improved with respect to the state-of-the-art . Although the considered task is indeed very important , a weak point of the paper is that it considers a single domain , and the method looks tailored to such domain . The paper asserts that the same methodology could be applied to different scenarios in the domain of chat conversations , but this is not confirmed by the experimental evaluation . For example , what about the process of identification of `` variants '' : is such process hand-made ? Would it be possible to have more details on such part ? - Pag.7 , `` This may be due to this capturing of language subtleties helping with distinguishing OG conversations ... '' - > this sentence should probably be rephrased - Pag . 7 , `` have same aim '' - > `` have the same aim ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their insights . We respond next to their raised questions and comments : * Although the considered task is indeed very important , a weak point of the paper is that it considers a single domain , and the method looks tailored to such domain . The paper asserts that the same methodology could be applied to different scenarios in the domain of chat conversations , but this is not confirmed by the experimental evaluation . For example , what about the process of identification of `` variants '' : is such process hand-made ? Would it be possible to have more details on such part ? * As briefly indicated in Section 3 , the CL analysis that produced our annotations involves a heavy use of manual analysis by CL experts . Indeed , expert knowledge doesn \u2019 t come for free . However , in such multidisciplinary studies , it is often the case that expert CL knowledge already exists . The word variants that were ( manually ) identified and used for this study would be largely re-usable in other contexts and applications of analysing chat conversations . Indeed , a significant number of these variants are related to digital language rather than specifically to online grooming . Only a few variants could be more specifically linked to online grooming , where they would aim to minimize the sexual meaning of some terms for example . We reckon that these few samples would not prevent the re-use of the whole set of variants in other applications . The evaluation of the discriminative aspect of the variants for a given classification can be done easily and automatically , following the procedure described at the beginning of Section 4.1 , using empirical occurrences in positive and negative conversations . In conclusion , the first strategy of integrating knowledge on discriminative word variants into DNNs , is easily reusable in other applications of analysing chat conversations . The decomposition of a conversation \u2019 s aim into subgoals has been the focus of many social science studies . For example , for extreme ideology groups , such as radical right hate speech and radicalisation , a large corpus of works have identified strategies for persuasion / manipulation through conversations [ 1-5 ] . This established baseline of knowledge may be used for our second strategy of integrating knowledge into DNNs through decomposing conversations into subgoals . The identification of frequent 3-word collocates is automated , as described in ( Lorenzo-Dus et al. , 2016 ) . The association of their occurrences to the identified subgoals is the only task that may require additional manual work . For these reasons , the prior knowledge integration methods that we present are not only tailored to online grooming detection , but they could also be used for other classification tasks of chat conversation , such as detecting radicalisation for example . The existing CL knowledge of these applications can be exploited to reach this goal in a multidisciplinary context . This discussion has been provided in the new version of the paper . [ 1 ] Saridakis , I. and Mouka , E. ( 2020 ) A corpus study of outgrouping in Greek radical right computer-mediated discourses , Journal of Language Aggression and Conflict , 8 : 188 \u2013 231 ; DOI : https : //doi.org/10.1075/jlac.00038.sar [ 2 ] Baker , P. et al ( forthcoming , February 2021 ) The Language of Violent Jihad , Cambridge : Cambridge University Press . [ 3 ] Brindle , A . ( 2016 ) The Language of Hate : A Corpus Lingusitic Analysis of White Supremacist Language , London : Routlege . [ 4 ] Nouri , L. & Lorenzo-Dus , N. ( 2019 ) . Investigating Reclaim Australia and Britain First \u2019 s use of social media : Developing a new model of imagined political communities online , Journal for Deradicalization , 18 : 1-37 . [ 5 ] Lorenzo-Dus , N. & Nouri , L. ( 2020 ) The discourse of the US alt-right online \u2013 a case study of the Traditionalist Worker Party blog , Critical Discourse Studies , Critical Discourse Studies , DOI : 10.1080/17405904.2019.1708763"}, {"review_id": "0jPp4dKp3PL-2", "review_text": "Summary This work proposes the approach of integrating priors into a DNN in the form of Linguistic sub-models that capture characteristics of OG . The authors use the example of the PAN-12 dataset for sexual predators to use information about linguistics behaviour for the grooming phases . The work then goes to highlight the augmentations that are done on baseline DNN models to include these CL characteristics . The authors then go on to show the impact of these augmenations on performance of classification on the PAN-12 dataset . Questions - When reading the descriptions of the linguistic sub-models in the DNN , one has the question if we could compare subsets of a DNN that is trained without these explicit sub-models . and may have learned these representations for normalisation etc . vs the integrated CL knowledge . Could we work to extract interpretable pieces fo the DNN that will then be comparable to the proposed CL augmentations ? The above is important as work on the PAN-12 dataset has tried to reconcile the NLP approach with also understanding the behaviour of sexual predators , so if we can learn how the DNNs are extracting information , we can better create interpretability models that can be more general for NLP + DNNs . - The work does well to show the gains we get from including these priors . I think we would be better suited if we also understood in the base models , how much of the priors were learnt . - Please also include a note about some of the ethical considerations when dealing with the PAN-12 dataset and how the data was created .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their insights . We respond in turn to their raised questions and comments : - * When reading the descriptions of the linguistic sub-models in the DNN , one has the question if we could compare subsets of a DNN that is trained without these explicit sub-models . and may have learned these representations for normalisation etc . vs the integrated CL knowledge . * In our ablation studies , we did perform comparisons where no or only part of the CL knowledge was used for enhancing the DNN . These experimental results are shown in Table 1 . The improved results when integrating CL knowledge demonstrate that the non-augmented DNNs could not fully discover this knowledge on their own . For the selective text normalisation , in particular , our experimental results demonstrate that the non-augmented DNNs fail to discover the knowledge on discriminative word variants . Indeed , fine-tuning on our dataset did not allow the DNNs to identify on their own the variants that , at the same time , have the same meanings and are not discriminative of groomer language . These variants were kept separate in the word embeddings , as indicated by the reported distances for non-augmented models in Table 1 . In the revised submission , we have added a discussion on the inability of the two non-augmented DNNs to discover on their own the same level of CL knowledge that we use to augment the models . - * Could we work to extract interpretable pieces fo the DNN that will then be comparable to the proposed CL augmentations ? * Using machine learning to discover new knowledge is an interesting research field . However , this would be an entirely different study . Here , we are interested in how we can exploit the knowledge that experts already have , in order to augment DNNs and improve their results . - * The above is important as work on the PAN-12 dataset has tried to reconcile the NLP approach with also understanding the behaviour of sexual predators , so if we can learn how the DNNs are extracting information , we can better create interpretability models that can be more general for NLP + DNNs . * In our study , we leave the understanding of sexual predators to CL experts , who have published the results of their analysis e.g.in the cited paper ( Lorenzo-Dus et al. , 2016 ) . However , we demonstrate that augmenting the DNNs using this expert knowledge , in a multidisciplinary research context , does help in improving the interpretability of the model ( see Section 5.2 \u201c Visualisation \u201d ) . - * The work does well to show the gains we get from including these priors . I think we would be better suited if we also understood in the base models , how much of the priors were learnt . * As discussed previously , the knowledge on discriminative word variants could not be discovered by the non-augmented DNNs , and this is highlighted in the revised paper . Testing whether the non-augmented DNNs could identify the existence of sub-goals and the expression of their related contexts is more difficult without introducing this knowledge in the process . As a simple test , in a new experiment , we could visualise the attention energies of the attention module of base model 1 , and of the last self attention layer of base model 2 , to check whether the model learnt on its own to focus on the contexts of some sub-goals . This new experiment will take a few days ( due to current technical issues with our local computation resources ) , and we will update this response with the results when we get them . - * Please also include a note about some of the ethical considerations when dealing with the PAN-12 dataset and how the data was created . * More discussion has been added in Section 3 . The data itself being freely available online , its use does not raise any peculiar ethical concern . Its initial collection by the PJ website ( we are not involved in this process ) was debated and discussed for example in ( Chiang & Grant , 2019 ; Schneevogt et al. , 2018 ) , which are cited in our paper . The annotation of the data by CL experts was performed following the method developed in another study ( Lorenzo-Dus et al. , 2016 ) and that is cited in our paper . This annotation being mostly manual , due care has been taken to preserve the mental health of the CL experts ."}], "0": {"review_id": "0jPp4dKp3PL-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : This paper proposes two families of methods for integrating external knowledge into neural networks aimed at classifying instances of online grooming . The first family focuses on incorporating knowledge of word semantic similarity into their representations . The second , on different attention mechanisms for incorporating knowledge about theoretical stages of online grooming . The authors perform a solid amount of experiments to assess the differences between their suggested methods and baseline models . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : However , the way the paper is written and structured make it difficult to understand . While looking at the results , it is hard to really assess the contribution of each suggested strategy , and how they compare to each other . Finally , the paper presents some serious conceptual gaps that undermine its overall credibility . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : - Fair amount of experiments for assessing impact of each proposed strategy . - The ideas for modeling different word variants could be built upon . I especially like the idea of Elastic Pulling for combining the semantic information of word representations . This idea would be useful to the research community focusing on combining word representations . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : - Some serious conceptual gaps , and wrong claims . .- The paper is overall unclear and difficult to understand . - The task the paper is addressing is not well specified . - The methods are also not clearly explained . - Results are difficult to interpret and analyses are lacking . - Lack of ethical considerations for a system that could be used in law enforcement . I would have liked to see a more detailed discussion on the societal implications that systems aiding law enforcement could have , and in particular , which measures should be taken for avoiding the prosecution of innocent people by systems like this . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Comments and suggestions for the authors : - The term `` text normalisation '' is spread throughout the paper , but is never concretely defined , and is not immediately inferable . - I was not familiar with the `` word semantics representation '' noun phrase . After reading the paper I am pretty sure that you mean `` vector space model '' , or word embeddings . Why not use these terms that will probably be more familiar to potential readers ? - In Figure 1 ( left ) , you wrote Embedding ; at the right you wrote WSR . Are these equivalent ? - I understand that you are doing classification at the conversation level , but in page 3 , in the `` Base models '' paragraph , you mention that `` with the WSR 's embedding provided as input to the OG classifier in place of a sentence embedding '' . In order to classify a conversation , you need a vector representation of it . How is this obtained ? In other words , how are you aggregating the contextualized word representations ( i.e. , the output of the LSTM or the XLNet encoder ) , into a single vector representation of the conversation ? Are you using the last hidden state of the LSTM or a pooling method ? Are you using the [ CLS ] token of XLNet or something else ? - You mentioned that your dataset contains full conversations with an average of 431 messages per conversation . Are all the conversation turns separated by the [ SEP ] token ? What is the average message length ? What max input length did you use as a hyperparameter ? Did you use the same text input for Model 1 and Model 2 ? - Saying XLNet is the SoTA for NLP is a false statement ( p. 3 second-last paragraph ) . First of all , NLP encompasses several tasks and there is no single model superior to all the others in every task . Second , XLNet has already been beaten in several tasks . See the Glue benchmark ( https : //gluebenchmark.com/leaderboard ) for a few examples . You mention that XLNet is the SoTA of NLP again in p. 6 ; sec.5.1 ; second paragraph . - Saying that XLNet iteratively refines word embeddings from a WSR similar to that of LIU et al . ( 2017 ) is only tangentially true and is misleading , in my opinion . Recurrent models such as those relying on LSTMs , are profoundly different to those based on transformers such as XLNet . - I am not sure that there is a clear correspondence between using recurrent models such as LSTMs and better handling of class imbalance . How is a two-layer LSTM going to help handling class imbalance ? - You mentioned several times that `` text normalisation '' was a strong point of your contribution , but in the last paragraph of section 4 ( p.4 ) you say that you do not apply `` text normalisation '' . I understand that you might be referring to different kinds of normalisation , but I think this terminology is confusing . I suggest using more precise language to clearly differentiate what you are referring to . - What do you mean by `` intersection tests '' ? - I suggest using the word `` representations '' rather than `` coordinates '' for referring to the vector representation of a word ( p. 4 ; elastic pulling paragraph ) . - The correspondence between rows in Table 1 and the strategies discussed in section 4.2 , are not immediately apparent . - When not using GloVe embeddings , how did you initialize your word representations ? - You mention in Table 1 that bold are improved results , but improved with respect to what ? - You report precision and recall in Table 2 , but not in Table 1 . I think it would be valuable to include these in Table 1 , rather than the distance reduction and average resulting distance metrics which are only valid for a few strategies . - You could check the following papers for more context on combining representations of different word variants : * [ Attention-based Conditioning Methods for External Knowledge Integration ] ( https : //www.aclweb.org/anthology/P19-1385/ ) * [ Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics ] ( https : //www.aclweb.org/anthology/P13-1149/ ) * [ Composition in Distributional Models of Semantics ] ( https : //onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01106.x ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Some typos : - p. 4 ; Manifold Learning - `` by building , using manifold learning , a new space '' could be better written as `` by building through manifold learning a new space '' or `` by building a new space through manifold learning '' - p. 4 - `` it is possible reduce dimensionality '' - > `` it is possible to reduce dimensionality '' - p.5 , sec.4.2 , first paragraph - `` collocates and std the span '' . Not sure what std is supposed to mean here .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their insights . We are sorry that the reviewer finds the methods and discussions of experimental results hard to understand . In the revised paper , we have expanded ( as much as page limit allows ) the descriptions and discussions to alleviate this issue . We especially expanded the discussions around the ablation study , which evaluates and compares each suggested strategy . Next , we respond in turn to their raised questions and comments . We hope that this will address the criticism about `` conceptual gaps '' . If not , we would appreciate a more explicit description of these gaps , to allow us to address them . - * The task the paper is addressing is not well specified . * The task is the classification of conversations between instances of online grooming ( positive class ) , and neutral ( negative class ) conversations . In the revised submission , we have expanded the second sentence of the paper ( originally : \u201c The aim of this work is to detect instances of OG through classification of whole conversations. \u201d ) to clarify this . - * Lack of ethical considerations * As discussed in the introduction and in the dedicated discussion in Section 6 \u201c Perspectives for OG prevention \u201d , the system was co-developed with specialised law enforcement , and its \u201c intended usage is to facilitate triage by law enforcement \u201d . It is not intended to replace a human decision . Within this usage scenario , there is , therefore , no risk of innocents to be automatically prosecuted , since this decision is always made by a trained police officer after reviewing the conversation . In fact , not all courts would accept linguistic evidence . Our law enforcement partners only aim at using this system when they already have enough evidence , as part of their toolkit to analyse large quantities of seized digital materials including conversations . The co-development of the system with law enforcement ensures that their own security protocols will be followed throughout the usage of the system , to preserve integrity . In particular , the system would not be provided to any police officer , but only to trained operators , using robust mechanisms that are already being in use by law enforcement in the context of OG investigations . Therefore , while a human review of the flagged conversations might in general suffer from biases and prejudices , security protocols are already in place to mitigate these natural human biases . The aim of our work is * not * to address the possible biases in the human decision . However , our method may indirectly help in achieving a fairer handling of the flagged conversations . Indeed , the visualisation provided by our method eases the analysis of key parts of the conversation . Additionally , the triaging would allow operators to focus on a few flagged conversations while spending less time on the others . This reduced workload and associated lowered time pressure , associated with the visualisation , may allow a more thorough and fairer investigation . The \u201c Perspectives for OG prevention \u201d section has been expanded in the revised manuscript to reflect this discussion . While one may argue that our system may be diverted from its intended use and deployed more largely online without following the law enforcement \u2019 s robustness and security protocols , we wish to point out that this is the case for many AI systems , and therefore that this should be discussed at the higher level of the whole AI field and together with experts of other disciplines such as jurists . Indeed , the question of liability and responsibility arises in many AI applications , for example when an AI system may perturb stock exchange markets and economy or may endanger or even take the life of passengers of automatic cars . The AI-assisted detection of online grooming is indeed concerned with these questions of unintended usage and liability around AI . However , for a fuller and clearer debate , these questions should be ( and are [ 1-3 ] ) discussed more globally . [ 1 ] Liability for Artificial Intelligence and other emerging digital technologies , European Commission Report from the Expert Group on Liability and New Technologies \u2013 New Technologies Formation , 2019 ISBN 978-92-76-12959-2 , doi:10.2838/573689 https : //ec.europa.eu/transparency/regexpert/index.cfm ? do=groupDetail.groupMeetingDoc & docid=36608 [ 2 ] A. Bertolini , Artificial Intelligence and Civil Liability , Report from the European Parliament 's Committee on Legal Affairs , 2020 https : //www.europarl.europa.eu/RegData/etudes/STUD/2020/621926/IPOL_STU ( 2020 ) 621926_EN.pdf [ 3 ] J.K.C . Kingston , Artificial Intelligence and Legal Liability , International Conference on Innovative Techniques and Applications of Artificial Intelligence , 2016 https : //arxiv.org/ftp/arxiv/papers/1802/1802.07782.pdf"}, "1": {"review_id": "0jPp4dKp3PL-1", "review_text": "This paper presents an approach to natural language processing which integrates corpus linguistics knowledge within deep neural networks ( namely , an LSTM-based architecture with attention ) . The approach is tailored and evaluated on a specific application , namely online grooming detection . The approach is based on ( 1 ) the normalization of word embeddings by exploiting word semantics representations and word variants ; ( 2 ) the decomposition of conversation analysis to identify subgoals , by exploiting online grooming processes ( or phases ) ; ( 3 ) the use of attention to modulate the input gate of LSTM cells . Another significant contribution of the paper is a novel corpus , which extends a previous one ( PAN2012 ) . The proposed neural architecture presents several variants in otder to incorporate linguistic knowledge within the model , and the paper reports about such an ablation study . In the experimental evaluation , linguistic knowledge is injected within two base models , namely one based on LSTMs , and the other one on XL-Net . Performance is shown to be improved with respect to the state-of-the-art . Although the considered task is indeed very important , a weak point of the paper is that it considers a single domain , and the method looks tailored to such domain . The paper asserts that the same methodology could be applied to different scenarios in the domain of chat conversations , but this is not confirmed by the experimental evaluation . For example , what about the process of identification of `` variants '' : is such process hand-made ? Would it be possible to have more details on such part ? - Pag.7 , `` This may be due to this capturing of language subtleties helping with distinguishing OG conversations ... '' - > this sentence should probably be rephrased - Pag . 7 , `` have same aim '' - > `` have the same aim ''", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their insights . We respond next to their raised questions and comments : * Although the considered task is indeed very important , a weak point of the paper is that it considers a single domain , and the method looks tailored to such domain . The paper asserts that the same methodology could be applied to different scenarios in the domain of chat conversations , but this is not confirmed by the experimental evaluation . For example , what about the process of identification of `` variants '' : is such process hand-made ? Would it be possible to have more details on such part ? * As briefly indicated in Section 3 , the CL analysis that produced our annotations involves a heavy use of manual analysis by CL experts . Indeed , expert knowledge doesn \u2019 t come for free . However , in such multidisciplinary studies , it is often the case that expert CL knowledge already exists . The word variants that were ( manually ) identified and used for this study would be largely re-usable in other contexts and applications of analysing chat conversations . Indeed , a significant number of these variants are related to digital language rather than specifically to online grooming . Only a few variants could be more specifically linked to online grooming , where they would aim to minimize the sexual meaning of some terms for example . We reckon that these few samples would not prevent the re-use of the whole set of variants in other applications . The evaluation of the discriminative aspect of the variants for a given classification can be done easily and automatically , following the procedure described at the beginning of Section 4.1 , using empirical occurrences in positive and negative conversations . In conclusion , the first strategy of integrating knowledge on discriminative word variants into DNNs , is easily reusable in other applications of analysing chat conversations . The decomposition of a conversation \u2019 s aim into subgoals has been the focus of many social science studies . For example , for extreme ideology groups , such as radical right hate speech and radicalisation , a large corpus of works have identified strategies for persuasion / manipulation through conversations [ 1-5 ] . This established baseline of knowledge may be used for our second strategy of integrating knowledge into DNNs through decomposing conversations into subgoals . The identification of frequent 3-word collocates is automated , as described in ( Lorenzo-Dus et al. , 2016 ) . The association of their occurrences to the identified subgoals is the only task that may require additional manual work . For these reasons , the prior knowledge integration methods that we present are not only tailored to online grooming detection , but they could also be used for other classification tasks of chat conversation , such as detecting radicalisation for example . The existing CL knowledge of these applications can be exploited to reach this goal in a multidisciplinary context . This discussion has been provided in the new version of the paper . [ 1 ] Saridakis , I. and Mouka , E. ( 2020 ) A corpus study of outgrouping in Greek radical right computer-mediated discourses , Journal of Language Aggression and Conflict , 8 : 188 \u2013 231 ; DOI : https : //doi.org/10.1075/jlac.00038.sar [ 2 ] Baker , P. et al ( forthcoming , February 2021 ) The Language of Violent Jihad , Cambridge : Cambridge University Press . [ 3 ] Brindle , A . ( 2016 ) The Language of Hate : A Corpus Lingusitic Analysis of White Supremacist Language , London : Routlege . [ 4 ] Nouri , L. & Lorenzo-Dus , N. ( 2019 ) . Investigating Reclaim Australia and Britain First \u2019 s use of social media : Developing a new model of imagined political communities online , Journal for Deradicalization , 18 : 1-37 . [ 5 ] Lorenzo-Dus , N. & Nouri , L. ( 2020 ) The discourse of the US alt-right online \u2013 a case study of the Traditionalist Worker Party blog , Critical Discourse Studies , Critical Discourse Studies , DOI : 10.1080/17405904.2019.1708763"}, "2": {"review_id": "0jPp4dKp3PL-2", "review_text": "Summary This work proposes the approach of integrating priors into a DNN in the form of Linguistic sub-models that capture characteristics of OG . The authors use the example of the PAN-12 dataset for sexual predators to use information about linguistics behaviour for the grooming phases . The work then goes to highlight the augmentations that are done on baseline DNN models to include these CL characteristics . The authors then go on to show the impact of these augmenations on performance of classification on the PAN-12 dataset . Questions - When reading the descriptions of the linguistic sub-models in the DNN , one has the question if we could compare subsets of a DNN that is trained without these explicit sub-models . and may have learned these representations for normalisation etc . vs the integrated CL knowledge . Could we work to extract interpretable pieces fo the DNN that will then be comparable to the proposed CL augmentations ? The above is important as work on the PAN-12 dataset has tried to reconcile the NLP approach with also understanding the behaviour of sexual predators , so if we can learn how the DNNs are extracting information , we can better create interpretability models that can be more general for NLP + DNNs . - The work does well to show the gains we get from including these priors . I think we would be better suited if we also understood in the base models , how much of the priors were learnt . - Please also include a note about some of the ethical considerations when dealing with the PAN-12 dataset and how the data was created .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for their insights . We respond in turn to their raised questions and comments : - * When reading the descriptions of the linguistic sub-models in the DNN , one has the question if we could compare subsets of a DNN that is trained without these explicit sub-models . and may have learned these representations for normalisation etc . vs the integrated CL knowledge . * In our ablation studies , we did perform comparisons where no or only part of the CL knowledge was used for enhancing the DNN . These experimental results are shown in Table 1 . The improved results when integrating CL knowledge demonstrate that the non-augmented DNNs could not fully discover this knowledge on their own . For the selective text normalisation , in particular , our experimental results demonstrate that the non-augmented DNNs fail to discover the knowledge on discriminative word variants . Indeed , fine-tuning on our dataset did not allow the DNNs to identify on their own the variants that , at the same time , have the same meanings and are not discriminative of groomer language . These variants were kept separate in the word embeddings , as indicated by the reported distances for non-augmented models in Table 1 . In the revised submission , we have added a discussion on the inability of the two non-augmented DNNs to discover on their own the same level of CL knowledge that we use to augment the models . - * Could we work to extract interpretable pieces fo the DNN that will then be comparable to the proposed CL augmentations ? * Using machine learning to discover new knowledge is an interesting research field . However , this would be an entirely different study . Here , we are interested in how we can exploit the knowledge that experts already have , in order to augment DNNs and improve their results . - * The above is important as work on the PAN-12 dataset has tried to reconcile the NLP approach with also understanding the behaviour of sexual predators , so if we can learn how the DNNs are extracting information , we can better create interpretability models that can be more general for NLP + DNNs . * In our study , we leave the understanding of sexual predators to CL experts , who have published the results of their analysis e.g.in the cited paper ( Lorenzo-Dus et al. , 2016 ) . However , we demonstrate that augmenting the DNNs using this expert knowledge , in a multidisciplinary research context , does help in improving the interpretability of the model ( see Section 5.2 \u201c Visualisation \u201d ) . - * The work does well to show the gains we get from including these priors . I think we would be better suited if we also understood in the base models , how much of the priors were learnt . * As discussed previously , the knowledge on discriminative word variants could not be discovered by the non-augmented DNNs , and this is highlighted in the revised paper . Testing whether the non-augmented DNNs could identify the existence of sub-goals and the expression of their related contexts is more difficult without introducing this knowledge in the process . As a simple test , in a new experiment , we could visualise the attention energies of the attention module of base model 1 , and of the last self attention layer of base model 2 , to check whether the model learnt on its own to focus on the contexts of some sub-goals . This new experiment will take a few days ( due to current technical issues with our local computation resources ) , and we will update this response with the results when we get them . - * Please also include a note about some of the ethical considerations when dealing with the PAN-12 dataset and how the data was created . * More discussion has been added in Section 3 . The data itself being freely available online , its use does not raise any peculiar ethical concern . Its initial collection by the PJ website ( we are not involved in this process ) was debated and discussed for example in ( Chiang & Grant , 2019 ; Schneevogt et al. , 2018 ) , which are cited in our paper . The annotation of the data by CL experts was performed following the method developed in another study ( Lorenzo-Dus et al. , 2016 ) and that is cited in our paper . This annotation being mostly manual , due care has been taken to preserve the mental health of the CL experts ."}}