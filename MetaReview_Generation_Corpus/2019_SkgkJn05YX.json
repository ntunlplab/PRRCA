{"year": "2019", "forum": "SkgkJn05YX", "title": "RANDOM MASK: Towards Robust Convolutional Neural Networks", "decision": "Reject", "meta_review": "This paper presents a new technique for modifying neural network structure, and suggest that this structure provides improved robustness to black-box attacks, as compared to standard architectures. The paper is very thorough in its experimentation, and the method is simple and quite easy to understand. It also raises some important questions about adversarial examples. \n\nHowever, there are serious concerns regarding the evaluation methodology. In particular, the authors claim \"black-box robustness\" but do not test against any query-based attacks, which are known to perform better against gradient masking-based adversarial defenses. Furthermore, it is not clear why one would expect adversarial examples to transfer between models representing two completely different functions (i.e. from a standard model to a random mask model). So, the gray-box evaluation is much more informative and, unfortunately, random-mask seems to provide little to no robustness in this setting.\n\nGiven how fundamental sound and convincing evaluation is for proposed defense methods, the submission is not ready for publication yet. In particular, the authors are urged to (a) evaluate on stronger black-box attacks, and (b) compare to a baseline that is known to be non-robust, (e.g. JPEG encoding or SAP), to verify that these results are actually due to black-box robustness and not simply obfuscation.", "reviews": [{"review_id": "SkgkJn05YX-0", "review_text": "I am upgrading my reviews after the rebuttal, which actually has convinced me that there is something interesting going on in this paper. However, I'm not entirely convinced as the approach seems to be ad hoc. the intuitions provided are somewhat satisfactory, but it's not clear why the method works.. for example, the approach is highly sensitive to the hyperparameter \"drop rate\" and there is no way to find a good value for it. I'm inclined towards rejection as, even though results are almost satisfying, I yet don't understand what exactly is happening. Most of the arguments seems to be handwavy. I personally feel like a paper as simple as this one with not enough conceptual justifications, but good results (like this one), should go to a workshop. ====== The authors propose to randomly drop a few parameters at the beginning and fix the resulting architecture for train and test. The claim is that the resulting network is robust to adversarial attacks. Major concerns: An extremely simple approach of pruning neural networks (randomly dropping weights) with no justification whatsoever. There are so many other network pruning papers available. If the point is to use pruned network then the authors must provide analysis over other pruning schemes as well. Another major concern (technical contributions): How is the idea of randomly dropping weights different from Deep Expander Networks (Prabhu et al., ECCV 2018)? Please clarify. Minor suggestion: Another simple approach to test the hypotheses would be to try dropout at test time and see the performance.", "rating": "4: Ok but not good enough - rejection", "reply_text": "It seems there are misunderstandings of our method . For your two major concerns , we first give a brief answer and then provide detailed explanations . 1.Random Mask is NOT a weight-dropping pruning method . 2.We conduct experiments comparing Random Mask with pruning methods ( both you mentioned and those commonly used ) . It turns out a network with Random mask is far more robust than pruning . Detailed Explanations : 1 . Random Mask is a simple but carefully designed method . It removes some nodes in the shallow convolutional layers whose receptive fields are relatively small . This is very different from typical pruning methods which drop weights , remove channels or restrict connections between channels in two adjacent layers . The key idea of Random Mask is that by removing a part of such neurons , the remaining neurons in the shallow layers can not only response to features , but also automatically record the locations of the features . As a result , these remaining neurons in shallow layers together detect the spatial structure of the features , much better than the standard neural networks . The motivation of our design comes from the recent observation of adversarial examples . In many cases , the adversarial examples change a patch of the original image so that the perturbed patch looks like a small part of the incorrectly classified object . This perturbed patch , although contains crucial features of the incorrectly classified object , usually appears at the wrong location and does not have the right spatial structure with other parts of the image . For example , the adversarial example of a panda image is misclassified as a monkey because a patch of the panda skin is perturbed adversarially so that it alone looks like the monkey \u2019 s face . However , this patch does not form a right structure of a monkey with other parts of the images ( see Figure 11 in [ 1 ] ) . In sum , current deep neural networks are strong at detecting features , but relatively weak at telling if the spatial location/structure is right . Random Mask tries to strengthen the ability of neural networks in utilizing the spatial information . 2.The experimental results comparing Random Mask with typical pruning methods are given below . Common pruning methods do not improve the robustness of neural networks significantly , while a network with Random Mask is far more robust . We test the black-box defense ability of a ResNet-18 with an expander graph compressing all connections between channels by a factor of 2 ( following the method proposed in [ 2 ] ) . For comparison , we also list the performance of a ResNet-18 which prunes whole channels in the shallow layers ( i.e.Shallow_ { DC } in our paper ) and a ResNet-18 equipped with Random Mask . The results are listed in the table below . Networks in the first row are the source models to generate adversarial examples by PGD with perturbation scale of 16 , step size of 1 and 20 steps . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- | | DenseNet | SENet | TestAcc | | Normal ResNet | 2.96 % | 1.38 % | 95.33 % | | Expander ResNet | 3.13 % | 1.46 % | 94.99 % | | Pruning Channel | 4.68 % | 2.13 % | 94.97 % | | Random Mask | 26.50 % | 21.42 % | 93.39 % | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- As for your suggestion , we are not sure what hypotheses you hoped to verify by trying dropout at test time . Nonetheless , we think trying dropping at test time is similar to Stochastic Activation Pruning ( [ 3 ] ) . In their work , they tested SAP in terms of black-box defense ( Figure 1 ( c ) SAP-100 vs Dense in [ 3 ] ) , yet the performance is not as good as our results when the perturbation scale is 8 , 16 and 32 . Also , directly dropping at test time and scaling up the remaining will incur a significant drop in test accuracy . We tried to mask out 50 % of the neurons in the shallow blocks of a ResNet-18 at test time only , and scale up the rest by a factor of 2 . It turned out that the test accuracy dropped to around 20 % , which is not acceptable . Further discussion is welcomed if our reply does not address your concerns . [ 1 ] Liu , Mengchen , et al . `` Analyzing the Noise Robustness of Deep Neural Networks . '' arXiv preprint arXiv:1810.03913 ( 2018 ) . [ 2 ] Prabhu , Ameya , Girish Varma , and Anoop Namboodiri . `` Deep Expander Networks : Efficient Deep Networks from Graph Theory . '' arXiv preprint arXiv:1711.08757 ( 2017 ) . [ 3 ] Dhillon , Guneet S. , et al . `` Stochastic activation pruning for robust adversarial defense . '' arXiv preprint arXiv:1803.01442 ( 2018 ) ."}, {"review_id": "SkgkJn05YX-1", "review_text": "This paper proposes a surprisingly simple technique for improving the robustness of neural networks against black-box attacks. The proposed method creates a *fixed* random mask to zero out lower layer activations during training and test. Extensive experiments show that the proposed method without adversarial training is competitive with a state-of-the-art defense method under blackbox attacks. Pros: -- simplicity and effectiveness of the method -- extensive experimental results under different settings Cons: -- it's not clear why the method works besides some not-yet-validated hypotheses. -- graybox results seem to suggest that the effectiveness of the method is due to the baseline CNNs and the proposed CNNs learning very different functions; source models within the same family still produce strong transferable attacks. It would have been much more impressive if different randomness could result in very different functions, leading to strong defense in the graybox setting.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your review . > it 's not clear why the method works besides some not-yet-validated hypotheses . Although our method seems simple , it is carefully designed . The method removes some neurons in the * shallow * layers of the neural network . We emphasize that the neurons in the shallow layers have relatively small receptive fields . Therefore , by removing a part of such neurons , the remaining neurons in the shallow layers not only response to features , but also automatically record the locations of the features . As a result , these remaining neurons in shallow layers together detect the * spatial structure * of the features , much better than the standard neural networks . The motivation of our design comes from the recent observation ( [ 1 ] ) of adversarial examples . In many cases , the adversarial examples change a patch of the original image so that the perturbed patch looks like a small part of the incorrectly classified object . This perturbed patch , although contains crucial features of the incorrectly classified object , usually appears at the wrong location and does not have the right spatial structure with other parts of the image . For example , the adversarial example of a panda image is misclassified as a monkey because a patch of the panda skin is perturbed adversarially so that it alone looks like the monkey \u2019 s face . However , this patch does not form a right structure of a monkey with other parts of the images ( see Figure 11 in [ 1 ] ) . In sum , current deep neural networks are strong at detecting features , but relatively weak at telling if the spatial location/structure is right . Random Mask tries to strengthen the ability of neural networks in utilizing the spatial information . > graybox results seem to suggest that the effectiveness of the method is due to the baseline ... The grey-box attacks are very similar to white-box attacks in our setting . We demonstrate in the paper that the adversarial examples ( eps = 16,32 ) generated by the white-box attacks for Random Mask often fool human as well . We then raise the following questions : 1 ) Should these adversarial examples be classified as their original categories ? 2 ) How to evaluate the robustness of a method ? 3 ) Can we entirely rely on the currently used performance measures ? In sum , one of the major goal of this paper is to move a tiny step towards a better understanding of the problem of adversarial example . [ 1 ] Liu , Mengchen , et al . `` Analyzing the Noise Robustness of Deep Neural Networks . '' arXiv preprint arXiv:1810.03913 ( 2018 ) ."}, {"review_id": "SkgkJn05YX-2", "review_text": "The authors propose a simple method for increasing the robustness of convolutional neural networks against adversarial examples. This method is simple but seems to achieve surprisingly good results. It consist in randomly remove neurons from the network architecture. The deleted neurons are selected before training and remain deleted during the training and test phase. The authors also study the adversarial examples that still fool the network after applying their method and find than those examples also fool human. This finding raises the question of what is an adversarial example if both humans and networks are fooled by the same example. Using Random Masks in neural network is not a new idea since it was already proposed for DropOut or DropConnect (Regularization of Neural Networks using DropConnect, ICML2013) and in the context of adversarial attacks (Dhillon et al. 2018) as reported by the authors. The discussion (Section 2) about the impact of random masks on what convolution layers capture in the spatial organisation of the input is interesting: whereas standard CNNs focus on detecting the presence of a feature in the output, random mask could force the CNN layers to learn how a specific feature distributes on the whole input maps. This limitation of the CNN has already been pointed up and solutions have been proposed for example Capsule Networks (Dynamic Routing Between Capsules, NIPS 2017). This intuition is experimentally supported by a simple random shuffle by block of the input image (Appendix A). In Section 3, the authors present a large number of experiments to demonstrate the robustness of their method. Most of the details are given in the 13 (!) pages of appendix. Experiments against black-box attack, random noise, white-box attack, grey-box are presented. Most of the experiments are on CIFAR10 but one experiment is also presented on MNIST. One could regret that only one architecture of CNN is tested (ResNet18) except for gray-box attack, for which DenseNet121 and VG19 are tested. One could ask why the type of models tested is not consistent across the different experiments. For black-box attack, random masks compare favourably to Madry\u2019s defence. For white box defence, Random Mask is not compared to another defence method, which seems a weakness to me but I am not familiar enough with papers in this area to estimate if this is a common practice. In most of the experiments, the drop ratio is between 0.5 and 0.9, which seems to indicate that the size the initial network could be reduced by more than 50% to increase the robustness to attack. This ratio is larger than what is usually used for dropout (0.5 at most). In section 3.3, different strategies for random masks are explored : where to apply random masks, random mask versus random channels, random masks versus same masks. Results are given in table 2. The caption of Table 2 could be more explicit : what are the presented percent ? Experiments on masking shallow versus deep layers are interesting. Best results for robustness are obtained with masking shallow layers at quite a high ratio (0.9). One could ask if this result could be due to the type or the parameters of adversarial attacks which are not adapted to such a high sparseness on shallow layers or to the specific kind of sparseness induced by the masks. A comparison to a regular network with the same number of free parameters as the masked network could give insight on this aspect. pros : simple to implement, good robustness shown agains a variety of attack types cons : mainly tested on a single architecture (ResNet) and on a single datatbase CIFAR. Maybe not robust against the latest techniques of adversarial attack.", "rating": "7: Good paper, accept", "reply_text": "Thanks for your review . > The caption of Table 2 could be more explicit : what are the presented percent ? Thanks for your suggestion on the caption of Table 2 . We have fixed it . > A comparison to a regular network with the same number of free parameters as the masked network could give insight on this aspect . We already compared the performance of our structure to that of a regular network with the same number of neurons in Section 3.3 . The control group is called Channel Mask , which means randomly dropping out whole channels ( or kernels , equivalently ) according to the same ratio . The results ( 0.5-Shallow and 0.5-Shallow_ { DC } ) show that simply reducing the number of neurons without breaking the symmetry of channels can not significantly enhance defense performance . We hope that the comparisons made in Section 3.3 and the full information on experiments presented in Appendix F.5 can bring about insights on how to improve the robustness of a network via changing its structure . > mainly tested on a single architecture ( ResNet ) and on a single database CIFAR . Thanks for your suggestion that Random Mask should be tested on architectures other than ResNet-18 , and on datasets other than CIFAR-10 . In the new version of our paper , we have added experiments on CIFAR-10 and MNIST with Random Mask applied to ResNet-50 , DenseNet , SENet and VGG . > Maybe not robust against the latest techniques of adversarial attack . We have tested the robustness of CNNs with Random Mask with respect to black-box defense on three popular attack methods ( FGSM , PGD and CW ) , and most of the results are listed in Appendix F.5 . In particular , [ 1 ] suggested that PGD is \u201c a \u2018 universal \u2019 adversary among first-order approaches \u201d . Besides , our model is able to effectively defend against Gaussian random noise and to generate human-fooling adversarial examples . As for your suggestion to test on more advanced black-box attacks , we think they are out of the scope of this work since these methods have few baselines to compare with . Most works concerning the robustness of neural networks focus on the three attack methods mentioned above . In our paper , the black-box defense mainly serves as an approach to evaluating robustness , and we believe the three attack methods we used are sufficient for this purpose . [ 1 ] Madry , Aleksander , et al . `` Towards deep learning models resistant to adversarial attacks . '' arXiv preprint arXiv:1706.06083 ( 2017 ) ."}], "0": {"review_id": "SkgkJn05YX-0", "review_text": "I am upgrading my reviews after the rebuttal, which actually has convinced me that there is something interesting going on in this paper. However, I'm not entirely convinced as the approach seems to be ad hoc. the intuitions provided are somewhat satisfactory, but it's not clear why the method works.. for example, the approach is highly sensitive to the hyperparameter \"drop rate\" and there is no way to find a good value for it. I'm inclined towards rejection as, even though results are almost satisfying, I yet don't understand what exactly is happening. Most of the arguments seems to be handwavy. I personally feel like a paper as simple as this one with not enough conceptual justifications, but good results (like this one), should go to a workshop. ====== The authors propose to randomly drop a few parameters at the beginning and fix the resulting architecture for train and test. The claim is that the resulting network is robust to adversarial attacks. Major concerns: An extremely simple approach of pruning neural networks (randomly dropping weights) with no justification whatsoever. There are so many other network pruning papers available. If the point is to use pruned network then the authors must provide analysis over other pruning schemes as well. Another major concern (technical contributions): How is the idea of randomly dropping weights different from Deep Expander Networks (Prabhu et al., ECCV 2018)? Please clarify. Minor suggestion: Another simple approach to test the hypotheses would be to try dropout at test time and see the performance.", "rating": "4: Ok but not good enough - rejection", "reply_text": "It seems there are misunderstandings of our method . For your two major concerns , we first give a brief answer and then provide detailed explanations . 1.Random Mask is NOT a weight-dropping pruning method . 2.We conduct experiments comparing Random Mask with pruning methods ( both you mentioned and those commonly used ) . It turns out a network with Random mask is far more robust than pruning . Detailed Explanations : 1 . Random Mask is a simple but carefully designed method . It removes some nodes in the shallow convolutional layers whose receptive fields are relatively small . This is very different from typical pruning methods which drop weights , remove channels or restrict connections between channels in two adjacent layers . The key idea of Random Mask is that by removing a part of such neurons , the remaining neurons in the shallow layers can not only response to features , but also automatically record the locations of the features . As a result , these remaining neurons in shallow layers together detect the spatial structure of the features , much better than the standard neural networks . The motivation of our design comes from the recent observation of adversarial examples . In many cases , the adversarial examples change a patch of the original image so that the perturbed patch looks like a small part of the incorrectly classified object . This perturbed patch , although contains crucial features of the incorrectly classified object , usually appears at the wrong location and does not have the right spatial structure with other parts of the image . For example , the adversarial example of a panda image is misclassified as a monkey because a patch of the panda skin is perturbed adversarially so that it alone looks like the monkey \u2019 s face . However , this patch does not form a right structure of a monkey with other parts of the images ( see Figure 11 in [ 1 ] ) . In sum , current deep neural networks are strong at detecting features , but relatively weak at telling if the spatial location/structure is right . Random Mask tries to strengthen the ability of neural networks in utilizing the spatial information . 2.The experimental results comparing Random Mask with typical pruning methods are given below . Common pruning methods do not improve the robustness of neural networks significantly , while a network with Random Mask is far more robust . We test the black-box defense ability of a ResNet-18 with an expander graph compressing all connections between channels by a factor of 2 ( following the method proposed in [ 2 ] ) . For comparison , we also list the performance of a ResNet-18 which prunes whole channels in the shallow layers ( i.e.Shallow_ { DC } in our paper ) and a ResNet-18 equipped with Random Mask . The results are listed in the table below . Networks in the first row are the source models to generate adversarial examples by PGD with perturbation scale of 16 , step size of 1 and 20 steps . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- | | DenseNet | SENet | TestAcc | | Normal ResNet | 2.96 % | 1.38 % | 95.33 % | | Expander ResNet | 3.13 % | 1.46 % | 94.99 % | | Pruning Channel | 4.68 % | 2.13 % | 94.97 % | | Random Mask | 26.50 % | 21.42 % | 93.39 % | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- As for your suggestion , we are not sure what hypotheses you hoped to verify by trying dropout at test time . Nonetheless , we think trying dropping at test time is similar to Stochastic Activation Pruning ( [ 3 ] ) . In their work , they tested SAP in terms of black-box defense ( Figure 1 ( c ) SAP-100 vs Dense in [ 3 ] ) , yet the performance is not as good as our results when the perturbation scale is 8 , 16 and 32 . Also , directly dropping at test time and scaling up the remaining will incur a significant drop in test accuracy . We tried to mask out 50 % of the neurons in the shallow blocks of a ResNet-18 at test time only , and scale up the rest by a factor of 2 . It turned out that the test accuracy dropped to around 20 % , which is not acceptable . Further discussion is welcomed if our reply does not address your concerns . [ 1 ] Liu , Mengchen , et al . `` Analyzing the Noise Robustness of Deep Neural Networks . '' arXiv preprint arXiv:1810.03913 ( 2018 ) . [ 2 ] Prabhu , Ameya , Girish Varma , and Anoop Namboodiri . `` Deep Expander Networks : Efficient Deep Networks from Graph Theory . '' arXiv preprint arXiv:1711.08757 ( 2017 ) . [ 3 ] Dhillon , Guneet S. , et al . `` Stochastic activation pruning for robust adversarial defense . '' arXiv preprint arXiv:1803.01442 ( 2018 ) ."}, "1": {"review_id": "SkgkJn05YX-1", "review_text": "This paper proposes a surprisingly simple technique for improving the robustness of neural networks against black-box attacks. The proposed method creates a *fixed* random mask to zero out lower layer activations during training and test. Extensive experiments show that the proposed method without adversarial training is competitive with a state-of-the-art defense method under blackbox attacks. Pros: -- simplicity and effectiveness of the method -- extensive experimental results under different settings Cons: -- it's not clear why the method works besides some not-yet-validated hypotheses. -- graybox results seem to suggest that the effectiveness of the method is due to the baseline CNNs and the proposed CNNs learning very different functions; source models within the same family still produce strong transferable attacks. It would have been much more impressive if different randomness could result in very different functions, leading to strong defense in the graybox setting.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your review . > it 's not clear why the method works besides some not-yet-validated hypotheses . Although our method seems simple , it is carefully designed . The method removes some neurons in the * shallow * layers of the neural network . We emphasize that the neurons in the shallow layers have relatively small receptive fields . Therefore , by removing a part of such neurons , the remaining neurons in the shallow layers not only response to features , but also automatically record the locations of the features . As a result , these remaining neurons in shallow layers together detect the * spatial structure * of the features , much better than the standard neural networks . The motivation of our design comes from the recent observation ( [ 1 ] ) of adversarial examples . In many cases , the adversarial examples change a patch of the original image so that the perturbed patch looks like a small part of the incorrectly classified object . This perturbed patch , although contains crucial features of the incorrectly classified object , usually appears at the wrong location and does not have the right spatial structure with other parts of the image . For example , the adversarial example of a panda image is misclassified as a monkey because a patch of the panda skin is perturbed adversarially so that it alone looks like the monkey \u2019 s face . However , this patch does not form a right structure of a monkey with other parts of the images ( see Figure 11 in [ 1 ] ) . In sum , current deep neural networks are strong at detecting features , but relatively weak at telling if the spatial location/structure is right . Random Mask tries to strengthen the ability of neural networks in utilizing the spatial information . > graybox results seem to suggest that the effectiveness of the method is due to the baseline ... The grey-box attacks are very similar to white-box attacks in our setting . We demonstrate in the paper that the adversarial examples ( eps = 16,32 ) generated by the white-box attacks for Random Mask often fool human as well . We then raise the following questions : 1 ) Should these adversarial examples be classified as their original categories ? 2 ) How to evaluate the robustness of a method ? 3 ) Can we entirely rely on the currently used performance measures ? In sum , one of the major goal of this paper is to move a tiny step towards a better understanding of the problem of adversarial example . [ 1 ] Liu , Mengchen , et al . `` Analyzing the Noise Robustness of Deep Neural Networks . '' arXiv preprint arXiv:1810.03913 ( 2018 ) ."}, "2": {"review_id": "SkgkJn05YX-2", "review_text": "The authors propose a simple method for increasing the robustness of convolutional neural networks against adversarial examples. This method is simple but seems to achieve surprisingly good results. It consist in randomly remove neurons from the network architecture. The deleted neurons are selected before training and remain deleted during the training and test phase. The authors also study the adversarial examples that still fool the network after applying their method and find than those examples also fool human. This finding raises the question of what is an adversarial example if both humans and networks are fooled by the same example. Using Random Masks in neural network is not a new idea since it was already proposed for DropOut or DropConnect (Regularization of Neural Networks using DropConnect, ICML2013) and in the context of adversarial attacks (Dhillon et al. 2018) as reported by the authors. The discussion (Section 2) about the impact of random masks on what convolution layers capture in the spatial organisation of the input is interesting: whereas standard CNNs focus on detecting the presence of a feature in the output, random mask could force the CNN layers to learn how a specific feature distributes on the whole input maps. This limitation of the CNN has already been pointed up and solutions have been proposed for example Capsule Networks (Dynamic Routing Between Capsules, NIPS 2017). This intuition is experimentally supported by a simple random shuffle by block of the input image (Appendix A). In Section 3, the authors present a large number of experiments to demonstrate the robustness of their method. Most of the details are given in the 13 (!) pages of appendix. Experiments against black-box attack, random noise, white-box attack, grey-box are presented. Most of the experiments are on CIFAR10 but one experiment is also presented on MNIST. One could regret that only one architecture of CNN is tested (ResNet18) except for gray-box attack, for which DenseNet121 and VG19 are tested. One could ask why the type of models tested is not consistent across the different experiments. For black-box attack, random masks compare favourably to Madry\u2019s defence. For white box defence, Random Mask is not compared to another defence method, which seems a weakness to me but I am not familiar enough with papers in this area to estimate if this is a common practice. In most of the experiments, the drop ratio is between 0.5 and 0.9, which seems to indicate that the size the initial network could be reduced by more than 50% to increase the robustness to attack. This ratio is larger than what is usually used for dropout (0.5 at most). In section 3.3, different strategies for random masks are explored : where to apply random masks, random mask versus random channels, random masks versus same masks. Results are given in table 2. The caption of Table 2 could be more explicit : what are the presented percent ? Experiments on masking shallow versus deep layers are interesting. Best results for robustness are obtained with masking shallow layers at quite a high ratio (0.9). One could ask if this result could be due to the type or the parameters of adversarial attacks which are not adapted to such a high sparseness on shallow layers or to the specific kind of sparseness induced by the masks. A comparison to a regular network with the same number of free parameters as the masked network could give insight on this aspect. pros : simple to implement, good robustness shown agains a variety of attack types cons : mainly tested on a single architecture (ResNet) and on a single datatbase CIFAR. Maybe not robust against the latest techniques of adversarial attack.", "rating": "7: Good paper, accept", "reply_text": "Thanks for your review . > The caption of Table 2 could be more explicit : what are the presented percent ? Thanks for your suggestion on the caption of Table 2 . We have fixed it . > A comparison to a regular network with the same number of free parameters as the masked network could give insight on this aspect . We already compared the performance of our structure to that of a regular network with the same number of neurons in Section 3.3 . The control group is called Channel Mask , which means randomly dropping out whole channels ( or kernels , equivalently ) according to the same ratio . The results ( 0.5-Shallow and 0.5-Shallow_ { DC } ) show that simply reducing the number of neurons without breaking the symmetry of channels can not significantly enhance defense performance . We hope that the comparisons made in Section 3.3 and the full information on experiments presented in Appendix F.5 can bring about insights on how to improve the robustness of a network via changing its structure . > mainly tested on a single architecture ( ResNet ) and on a single database CIFAR . Thanks for your suggestion that Random Mask should be tested on architectures other than ResNet-18 , and on datasets other than CIFAR-10 . In the new version of our paper , we have added experiments on CIFAR-10 and MNIST with Random Mask applied to ResNet-50 , DenseNet , SENet and VGG . > Maybe not robust against the latest techniques of adversarial attack . We have tested the robustness of CNNs with Random Mask with respect to black-box defense on three popular attack methods ( FGSM , PGD and CW ) , and most of the results are listed in Appendix F.5 . In particular , [ 1 ] suggested that PGD is \u201c a \u2018 universal \u2019 adversary among first-order approaches \u201d . Besides , our model is able to effectively defend against Gaussian random noise and to generate human-fooling adversarial examples . As for your suggestion to test on more advanced black-box attacks , we think they are out of the scope of this work since these methods have few baselines to compare with . Most works concerning the robustness of neural networks focus on the three attack methods mentioned above . In our paper , the black-box defense mainly serves as an approach to evaluating robustness , and we believe the three attack methods we used are sufficient for this purpose . [ 1 ] Madry , Aleksander , et al . `` Towards deep learning models resistant to adversarial attacks . '' arXiv preprint arXiv:1706.06083 ( 2017 ) ."}}