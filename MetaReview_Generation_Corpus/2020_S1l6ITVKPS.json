{"year": "2020", "forum": "S1l6ITVKPS", "title": "An Explicitly Relational Neural Network Architecture", "decision": "Reject", "meta_review": "This paper proposes a model that can learn predicates (symbolic relations) from pixels and can be trained end to end.  They show that the relations learned generate a representation that generalizes well, and provide some interpretation of the model.\n\nThough it is reasonable to develop a model with synthetic data, the reviewers did wonder if the findings would generalize to new data from real situations.  The authors argue that a new model should be understood (using synthetic data) before it can reasonably be applied to natural data.  I hope the reviews have shown the authors which areas of the paper need further explanation, and that the use of a synthetic dataset needs to strong justification, or perhaps show some evidence that the method will probably work on real data (e.g. how it could be extended to natural images).", "reviews": [{"review_id": "S1l6ITVKPS-0", "review_text": "This paper presents PrediNet: an architecture explicitly designed to extract representations in the form of three-place predicates (relations). They evaluate the architecture on a visual relational task called the \"Relations Game\" which involves comparing Tetris-like shapes according to their appearance, relative positions, etc.. They show that their architecture leads to useful generalizable representations in the sense that they can be used for new tasks without retraining. I think this paper contains a number of unusual and interesting ideas but is let down by its presentation. The writing is good, but provides very little intuition for why we should expect this approach to work (aside from its connection to equation 1) - I discuss this in more depth below. The experimental task is interesting (I'm okay with synthetic tasks of this form for unusual new architectures like this), but I'm not sure what it tests that isn't tested in the CLEVR and sort-of -CLEVR datasets which rely on similar relational reasoning to solve. The advantage of those datasets is they are well-established with strong baselines so we can be more certain that a fair comparison is been made. I've voted to reject this paper because I feel its premature in its current form. Expanding on this - The description of PrediNet covers the basics, but missing detail and intuition for why certain choices are made. For example, - why is L flattened for the queries (I think it\u2019s because the query is independent of pixel location, but flattening seems of when L also includes co-ordinates) but not the key, K? - Why is the key space shared between heads (this seems more intuitive - keys should have consistent meaning\u2026 but if that\u2019s the case, make that intention explicit)? - Also, writing the dimensionality of the matrices, would help (e.g. is W_S in R^{m x j} or R^{m x 1} or something else?). - What is the meaning of the position features in E_1 and E_2? From the softmax product it seems they should be a weighted sum of the pixels that are addressed - implying that it is the weighted average location? - The final representation mostly consists of an h x (j) vector (ignoring positions) containing the output of the comparators. Could you provide some intuition for why we would expect such a representation to be useful for the downstream task? This representation seems to differ substantially from what is used in the baseline methods: i.e. attention-weighted sums of the input features. ", "rating": "6: Weak Accept", "reply_text": "Many thanks for the thoughtful review . We have revised the paper accordingly . We hope you feel your concerns have been addressed , and that you will consider recommending acceptance . ( Note that this is a two-part response . ) Reviewer # 1 : \u201c I 'm not sure what [ the Relations Game ] tests that is n't tested in the CLEVR and sort-of -CLEVR datasets which rely on similar relational reasoning to solve \u2026 \u201d Response : Although it \u2019 s gratifying that the PrediNet out-performs the baselines in various respects , our primary aim in this work is not to demonstrate superior performance , but to present a novel architecture conforming to certain high-level design principles , and to explore some of its basic properties . So we wanted to use simple datasets , and were keen to avoid tasks with confounding features . This ruled out CLEVR on a couple of grounds . First , the 3D nature of the images entails confounding visual complexities such as occlusion and shadows . Second , the CLEVR task involves language input , which is another confounding factor . The sort-of-CLEVR dataset was similarly ruled out , on the second of these grounds ( though not the first ) . A further consideration was that we needed datasets that could be split into multiple tasks in many different ways , so that we could follow the three-step protocol of pre-training , freezing , and re-training ( Fig.3 of the paper ) . Neither CLEVR nor sort-of-CLEVR lend themselves to this kind of task-level split . Finally , at test time we wanted to use fully held-out objects - that is to say objects with colours not seen during training and shapes not seen during training - which neither CLEVR nor sort-of-CLEVR provide for . We weren \u2019 t aware of any existing datasets meeting these criteria , so we devised a new one ( the Relations Game ) . ( Indeed , we consider the Relations Game family of tasks as one of the contributions of the paper , albeit only a minor one . ) We have added the following paraphrased version of the above paragraph to Section 3 the revised paper : \u201c Existing datasets for relational reasoning tasks , such as CLEVR ( Johnson et al ( 2017 ) ) and sort-of-CLEVR ( Santoro et al ( 2017 ) ) , were ruled out because they include confounding complexities , such as occlusion and shadows or language input , and/or because they don \u2019 t lend themselves to the fine-grained task-level splits we required . Consequently , we devised a new configurable family of simple classification tasks that we collectively call the Relations Game. \u201d Reviewer # 1 : \u201c Why is L flattened for the queries ( I think it \u2019 s because the query is independent of pixel location , but flattening seems of when L also includes co-ordinates ) but not the key , K ? \u201d Response : The whole ( flattened ) image is used to generate queries so that what a head attends to can depend on the full ( non-local ) context of what \u2019 s in the image . For example , perhaps a head needs to pick out the red object in the top-left of the image because there is another red object in the bottom right of the image . By contrast , the shared key space can be determined from local information only . We have added the following paraphrased version of the above sentences to Section 2 of the revised paper : \u201c The whole ( flattened ) image is used to generate queries , allowing attention masks to depend on its full ( non-local ) content. \u201d Reviewer # 1 : \u201c Why is the key space shared between heads ( this seems more intuitive - keys should have consistent meaning\u2026 but if that \u2019 s the case , make that intention explicit ) ? \u201d Response : Yes , this ensures that the set of entities that are candidates for attention is consistent across heads . We have added the following clause to the relevant sentence in Section 2 of the revised paper : \u201c \u2026 so that the set of entities that are candidates for attention is consistent across heads . \u201d"}, {"review_id": "S1l6ITVKPS-1", "review_text": "This paper presents a network architecture based on the multi-head self-attention module to learn a new form of relational representations. The proposed method is shown to improve data efficiency and generalization ability on a sequence of curriculum learning tasks. + The major novelty of this paper is a new attention-based network architecture that aims to discover objects and the relations between them. The idea of explicitly appending the patch positions to the representations is interesting, though I am not sure whether it can be generalized to real data. - Since the proposed network takes patches of full images as inputs, my major concern is about its effectiveness on high-dimensional images with more realistic objects as in the CLEVR dataset other than 2d-grid objects. It would be better if the authors could extend their method to natural images. - A closely related work is the NLM model [1], which can perfectly generalized to new tasks. Please compare the proposed model with it. - Minor: Figures are understandable, but some of them are too small, especially for the graphical legend. As space is an issue, I would suggest removing some plots and increasing the size of the ones provided. - In Figure 1, is g equal to n? [1] Neural Logic Machines. Dong et al., ICLR 2019.", "rating": "6: Weak Accept", "reply_text": "Many thanks for the review and suggestions . We \u2019 ll respond to them in reverse order . ( If you feel we \u2019 ve dealt with them well , please consider increasing your score . ) Reviewer # 2 : \u201c In Figure 1 , is g equal to n ? \u201d Response : They aren \u2019 t the same . n is the number of feature vectors output by the CNN . g is the size of the keys and queries . ( If you feel this isn \u2019 t clear from Fig.1 we could try to squeeze in a sentence stating this explicitly , but we are very pressed for space . ) Reviewer # 2 : \u201c I would suggest removing some plots and increasing the size of the ones provided. \u201d Response : Sorry the text is so small in the figures . We hope it didn \u2019 t make the paper too hard to read . But the resolution is high , so everything should be legible if viewed on screen . As you point out , space is an issue ; we \u2019 re now at the very limit . However , we \u2019 re extremely reluctant to relegate any more plots to the Supplementary Material . So we hope you will let us off the hook : - ) Reviewer # 2 : \u201c A closely related work is the NLM model [ 1 ] , which can perfectly generalized to new tasks . Please compare the proposed model with it. \u201d Response : Thank you for drawing our attention to this work ( Dong et al ( 2019 ) ) . We have added it to the bibliography of the revised paper , and cited it in the Related Work section . As we point out there , Dong et al tackle an orthogonal problem to the one we \u2019 ve tackled . Dong et al \u2019 s NLM carries out inductive inference , assuming input is given in terms of propositions , relations , and objects . Our primary contribution is an architecture ( the PrediNet ) that _discovers_ objects and relations in the raw data , and learns to generate representations in propositional form built out of the objects and relations it has discovered . So , like the other papers we cite in this context , their work complements ours . Reviewer # 2 : \u201c my major concern is about its effectiveness on high-dimensional images with more realistic objects as in the CLEVR dataset other than 2d-grid objects . It would be better if the authors could extend their method to natural images. \u201d Response : You are right , of course , that a novel architecture eventually needs to be proven against rich , real-world data . But , as we argue in Section 3 of the paper , we believe it would be \u201c premature to apply the PrediNet architecture to rich , complex data before we have a basic understanding of its properties and its behaviour \u201d . The best way to do this is with simple datasets and tasks . ( We note that reviewer # 1 is sympathetic to this . ) The CLEVR dataset didn \u2019 t meet our requirements for reasons we have set out in Section 3 of the revised paper as follows : \u201c Existing datasets for relational reasoning tasks , such as CLEVR ( Johnson et al ( 2017 ) ) and sort-of-CLEVR ( Santoro et al ( 2017 ) ) , were ruled out because they include confounding complexities , such as occlusion and shadows or language input , and/or because they don \u2019 t lend themselves to the fine-grained task-level splits we required. \u201d See also our response to reviewer # 1 ."}, {"review_id": "S1l6ITVKPS-2", "review_text": "The authors propose a new neural network architecture that learns to form propositional representations with an explicitly relational structure from raw pixel data. The authors testified the proposed algorithm using the Relations Game, whose aim is to label an image containing a number of objects as True or False according to whether a given relationship holds among the objects in the image. This paper is well organized. The applied methods are introduced in detail. The authors showed the improvement using the Relations Game.", "rating": "6: Weak Accept", "reply_text": "Thank you for your review . ( If you feel we have responded well to the other reviewers ' comments , please consider raising your score . )"}], "0": {"review_id": "S1l6ITVKPS-0", "review_text": "This paper presents PrediNet: an architecture explicitly designed to extract representations in the form of three-place predicates (relations). They evaluate the architecture on a visual relational task called the \"Relations Game\" which involves comparing Tetris-like shapes according to their appearance, relative positions, etc.. They show that their architecture leads to useful generalizable representations in the sense that they can be used for new tasks without retraining. I think this paper contains a number of unusual and interesting ideas but is let down by its presentation. The writing is good, but provides very little intuition for why we should expect this approach to work (aside from its connection to equation 1) - I discuss this in more depth below. The experimental task is interesting (I'm okay with synthetic tasks of this form for unusual new architectures like this), but I'm not sure what it tests that isn't tested in the CLEVR and sort-of -CLEVR datasets which rely on similar relational reasoning to solve. The advantage of those datasets is they are well-established with strong baselines so we can be more certain that a fair comparison is been made. I've voted to reject this paper because I feel its premature in its current form. Expanding on this - The description of PrediNet covers the basics, but missing detail and intuition for why certain choices are made. For example, - why is L flattened for the queries (I think it\u2019s because the query is independent of pixel location, but flattening seems of when L also includes co-ordinates) but not the key, K? - Why is the key space shared between heads (this seems more intuitive - keys should have consistent meaning\u2026 but if that\u2019s the case, make that intention explicit)? - Also, writing the dimensionality of the matrices, would help (e.g. is W_S in R^{m x j} or R^{m x 1} or something else?). - What is the meaning of the position features in E_1 and E_2? From the softmax product it seems they should be a weighted sum of the pixels that are addressed - implying that it is the weighted average location? - The final representation mostly consists of an h x (j) vector (ignoring positions) containing the output of the comparators. Could you provide some intuition for why we would expect such a representation to be useful for the downstream task? This representation seems to differ substantially from what is used in the baseline methods: i.e. attention-weighted sums of the input features. ", "rating": "6: Weak Accept", "reply_text": "Many thanks for the thoughtful review . We have revised the paper accordingly . We hope you feel your concerns have been addressed , and that you will consider recommending acceptance . ( Note that this is a two-part response . ) Reviewer # 1 : \u201c I 'm not sure what [ the Relations Game ] tests that is n't tested in the CLEVR and sort-of -CLEVR datasets which rely on similar relational reasoning to solve \u2026 \u201d Response : Although it \u2019 s gratifying that the PrediNet out-performs the baselines in various respects , our primary aim in this work is not to demonstrate superior performance , but to present a novel architecture conforming to certain high-level design principles , and to explore some of its basic properties . So we wanted to use simple datasets , and were keen to avoid tasks with confounding features . This ruled out CLEVR on a couple of grounds . First , the 3D nature of the images entails confounding visual complexities such as occlusion and shadows . Second , the CLEVR task involves language input , which is another confounding factor . The sort-of-CLEVR dataset was similarly ruled out , on the second of these grounds ( though not the first ) . A further consideration was that we needed datasets that could be split into multiple tasks in many different ways , so that we could follow the three-step protocol of pre-training , freezing , and re-training ( Fig.3 of the paper ) . Neither CLEVR nor sort-of-CLEVR lend themselves to this kind of task-level split . Finally , at test time we wanted to use fully held-out objects - that is to say objects with colours not seen during training and shapes not seen during training - which neither CLEVR nor sort-of-CLEVR provide for . We weren \u2019 t aware of any existing datasets meeting these criteria , so we devised a new one ( the Relations Game ) . ( Indeed , we consider the Relations Game family of tasks as one of the contributions of the paper , albeit only a minor one . ) We have added the following paraphrased version of the above paragraph to Section 3 the revised paper : \u201c Existing datasets for relational reasoning tasks , such as CLEVR ( Johnson et al ( 2017 ) ) and sort-of-CLEVR ( Santoro et al ( 2017 ) ) , were ruled out because they include confounding complexities , such as occlusion and shadows or language input , and/or because they don \u2019 t lend themselves to the fine-grained task-level splits we required . Consequently , we devised a new configurable family of simple classification tasks that we collectively call the Relations Game. \u201d Reviewer # 1 : \u201c Why is L flattened for the queries ( I think it \u2019 s because the query is independent of pixel location , but flattening seems of when L also includes co-ordinates ) but not the key , K ? \u201d Response : The whole ( flattened ) image is used to generate queries so that what a head attends to can depend on the full ( non-local ) context of what \u2019 s in the image . For example , perhaps a head needs to pick out the red object in the top-left of the image because there is another red object in the bottom right of the image . By contrast , the shared key space can be determined from local information only . We have added the following paraphrased version of the above sentences to Section 2 of the revised paper : \u201c The whole ( flattened ) image is used to generate queries , allowing attention masks to depend on its full ( non-local ) content. \u201d Reviewer # 1 : \u201c Why is the key space shared between heads ( this seems more intuitive - keys should have consistent meaning\u2026 but if that \u2019 s the case , make that intention explicit ) ? \u201d Response : Yes , this ensures that the set of entities that are candidates for attention is consistent across heads . We have added the following clause to the relevant sentence in Section 2 of the revised paper : \u201c \u2026 so that the set of entities that are candidates for attention is consistent across heads . \u201d"}, "1": {"review_id": "S1l6ITVKPS-1", "review_text": "This paper presents a network architecture based on the multi-head self-attention module to learn a new form of relational representations. The proposed method is shown to improve data efficiency and generalization ability on a sequence of curriculum learning tasks. + The major novelty of this paper is a new attention-based network architecture that aims to discover objects and the relations between them. The idea of explicitly appending the patch positions to the representations is interesting, though I am not sure whether it can be generalized to real data. - Since the proposed network takes patches of full images as inputs, my major concern is about its effectiveness on high-dimensional images with more realistic objects as in the CLEVR dataset other than 2d-grid objects. It would be better if the authors could extend their method to natural images. - A closely related work is the NLM model [1], which can perfectly generalized to new tasks. Please compare the proposed model with it. - Minor: Figures are understandable, but some of them are too small, especially for the graphical legend. As space is an issue, I would suggest removing some plots and increasing the size of the ones provided. - In Figure 1, is g equal to n? [1] Neural Logic Machines. Dong et al., ICLR 2019.", "rating": "6: Weak Accept", "reply_text": "Many thanks for the review and suggestions . We \u2019 ll respond to them in reverse order . ( If you feel we \u2019 ve dealt with them well , please consider increasing your score . ) Reviewer # 2 : \u201c In Figure 1 , is g equal to n ? \u201d Response : They aren \u2019 t the same . n is the number of feature vectors output by the CNN . g is the size of the keys and queries . ( If you feel this isn \u2019 t clear from Fig.1 we could try to squeeze in a sentence stating this explicitly , but we are very pressed for space . ) Reviewer # 2 : \u201c I would suggest removing some plots and increasing the size of the ones provided. \u201d Response : Sorry the text is so small in the figures . We hope it didn \u2019 t make the paper too hard to read . But the resolution is high , so everything should be legible if viewed on screen . As you point out , space is an issue ; we \u2019 re now at the very limit . However , we \u2019 re extremely reluctant to relegate any more plots to the Supplementary Material . So we hope you will let us off the hook : - ) Reviewer # 2 : \u201c A closely related work is the NLM model [ 1 ] , which can perfectly generalized to new tasks . Please compare the proposed model with it. \u201d Response : Thank you for drawing our attention to this work ( Dong et al ( 2019 ) ) . We have added it to the bibliography of the revised paper , and cited it in the Related Work section . As we point out there , Dong et al tackle an orthogonal problem to the one we \u2019 ve tackled . Dong et al \u2019 s NLM carries out inductive inference , assuming input is given in terms of propositions , relations , and objects . Our primary contribution is an architecture ( the PrediNet ) that _discovers_ objects and relations in the raw data , and learns to generate representations in propositional form built out of the objects and relations it has discovered . So , like the other papers we cite in this context , their work complements ours . Reviewer # 2 : \u201c my major concern is about its effectiveness on high-dimensional images with more realistic objects as in the CLEVR dataset other than 2d-grid objects . It would be better if the authors could extend their method to natural images. \u201d Response : You are right , of course , that a novel architecture eventually needs to be proven against rich , real-world data . But , as we argue in Section 3 of the paper , we believe it would be \u201c premature to apply the PrediNet architecture to rich , complex data before we have a basic understanding of its properties and its behaviour \u201d . The best way to do this is with simple datasets and tasks . ( We note that reviewer # 1 is sympathetic to this . ) The CLEVR dataset didn \u2019 t meet our requirements for reasons we have set out in Section 3 of the revised paper as follows : \u201c Existing datasets for relational reasoning tasks , such as CLEVR ( Johnson et al ( 2017 ) ) and sort-of-CLEVR ( Santoro et al ( 2017 ) ) , were ruled out because they include confounding complexities , such as occlusion and shadows or language input , and/or because they don \u2019 t lend themselves to the fine-grained task-level splits we required. \u201d See also our response to reviewer # 1 ."}, "2": {"review_id": "S1l6ITVKPS-2", "review_text": "The authors propose a new neural network architecture that learns to form propositional representations with an explicitly relational structure from raw pixel data. The authors testified the proposed algorithm using the Relations Game, whose aim is to label an image containing a number of objects as True or False according to whether a given relationship holds among the objects in the image. This paper is well organized. The applied methods are introduced in detail. The authors showed the improvement using the Relations Game.", "rating": "6: Weak Accept", "reply_text": "Thank you for your review . ( If you feel we have responded well to the other reviewers ' comments , please consider raising your score . )"}}