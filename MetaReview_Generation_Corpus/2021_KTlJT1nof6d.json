{"year": "2021", "forum": "KTlJT1nof6d", "title": "Initialization and Regularization of Factorized Neural Layers", "decision": "Accept (Poster)", "meta_review": "This paper applies spectral initialization and weight decay to neural nets with factorized layers. Although these ideas have been extensively studied in other areas, formalizing and applying them to deep neural nets is of potential interest to the community. The simulation results are nice, especially the experiments on compression methods (comparison to sparse pruning e.g. lottery tickets) and Transformers. I recommend acceptance. \n", "reviews": [{"review_id": "KTlJT1nof6d-0", "review_text": "This paper studies how to initialize via spectral initialization and regularize DNNs via Frobenius decay . The benefits of spectral initialization and Frobenius decay are demonstrated via many experiments . This paper focuses on the empirical study of both SI and FD . My major concern is on its technical contribution . The spectral initialization is a scheme commonly used for low-rank models . It is well understood that spectral initialization is better than random initialization in low-rank literatures . Moreover , the proposed Frobenius decay is simply a penalization on the squared norm of the whole factorization . That is , it replaces the weight-decay $ \\|W\\|_F^2 $ in deep nets by plugging in the low-rank formulation of $ W $ . The behaviors of these two components in the experiments are expected . These two components have not provided much new insight . ~~~~~ This major concern is relieved after rebuttal .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Please see Point 1 in our general author response for the novelty and detailed contributions in our work . We would be happy to answer any questions about these claims or about the revision . Below we address some of your specific concerns about the two techniques we study . 1._ '' It is well understood that spectral initialization is better than random initialization in low-rank literatures . `` _ We would be interested in any references that compare spectral and random initialization so that we can cite them . The use of spectral initialization in past work is something we note in the second paragraph of Section 1 , but to our knowledge these works do not make such comparisons and generally focus on initializing from pretrained full-rank networks rather than training from scratch . 2._ '' Moreover , the proposed Frobenius decay is simply a penalization on the squared norm of the whole factorization. \u201d _ We believe simplicity is a benefit of the method . That such an effective and simple method is not more widely used makes it more necessary to formalize it and demonstrate its effectiveness . 3._ '' The behaviors of these two components in the experiments are expected . These two components have not provided much new insight . `` _ That the methods perform better is perhaps not surprising . However , ( a ) this behavior has not previously been analyzed mathematically and ( b ) we view many of our empirical findings as unexpected or at least not obvious , including the following : - Weight-decay implicitly penalizes the nuclear norm of the product despite explicitly regularizing only a loose upper bound ( Figure 1 ) . - Frobenius decay is better than weight-decay even after tuning the latter ( Figures 2 and 3 ) . - In some cases spectral initialization and Frobenius decay can decrease accuracy unless used jointly ( Table 1 ) ."}, {"review_id": "KTlJT1nof6d-1", "review_text": "This paper studies initialization and regularization in factorized neural networks ( reparameterize a weight matrix by the product of several weight matrices ) . The authors proposed spectral initialization , that is to initialize the factorized matrices using the SVD of the un-factorized matrix . The authors also proposed Frobenius decay that is to regularize the Frobenius norm of the product of the factorized weight matrices . The motivation is to simulate the routines for non-decomposed counterparts . The authors empirically showed the effectiveness of spectral initialization and Frobenius decay in different applications : compressed model training , knowledge distillation , and multi-head self-training . I think it \u2019 s important to study the initialization and regularization for factorized neural networks . A priori , it needs different initialization and regularization methods due to different architecture compared with its non-decomposed counter-part . This paper gave very simple and natural solutions and was able to show its effectiveness in experiments . I also have some questions as below : 1 . In the experiments in section 5 ( knowledge distillation ) , default initialization is used instead of spectral initialization . I wonder if SI leads to a bad performance here . If that \u2019 s the case , it requires more explanation of why SI fails in this setting . 2.In Figure 1 , it seems FD is a stronger regularizer compared with default weight decay . It seems if the regularization coefficient is carefully tuned for each regularizer , the benefits of FD is actually not very significant . Also , what \u2019 s `` no decay ( normalized ) '' ? 3.In section 2 , the definition of the factorized CNN is not very clear to me . It might be good to give more detailed definitions here . 4.Spectral initialization requires computing SVD of the weight matrix . If the matrix dimension is high , this step can be very time-consuming . I wonder if there is any more efficient way to construct the factorized matrices so that their product is still as i.i.d.Gaussian matrix . Because we do n't need to compute the SVD for an arbitrary matrix , what we need is only to make sure that the product of the factorized matrixes is distributed as i.i.d.Gaussian .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the positive review . We hope to address your questions below . 1._ \u201c In the experiments in section 5 ( knowledge distillation ) , default initialization is used instead of spectral initialization . I wonder if SI leads to a bad performance here . If that \u2019 s the case , it requires more explanation of why SI fails in this setting. \u201d _ We do not use SI in Section 5 because there is no obvious way to apply it to overcomplete factorizations ( when the rank is higher than the original rank ) . For the full-rank case ( the \u201c full \u201d setting ) , the median performance is roughly the same with and without SI . Here are the results for ResNet56 : | | CIFAR-10 | CIFAR-100 | |-|-| -- | | WD | 92.45 | 69.23 | | SI+WD | 92.49 | 70.42 | | FD | 93.68 | 72.18 | | SI+FD | 93.48 | 72.19 | 2 . - _ \u201c In Figure 1 , it seems FD is a stronger regularizer compared with default weight decay . It seems if the regularization coefficient is carefully tuned for each regularizer , the benefits of FD is actually not very significant. \u201d _ In Figure 1 [ note : in the revision the relevant plot is now in Figure 2 ] , tuning weight-decay from the default value ( 1E-4 ) shows little to no improvement for both ResNet20 and ResNet44 , while tuning FD does yield a noticeable improvement on ResNet20 . We thus view this plot as showing that we can not achieve FD performance by tuning weight-decay , and that tuning weight-decay does not yield significant improvements . The gap between their performances is 0.5-1 % , which is not too small at this accuracy on CIFAR-10 , and the gaps in the other experiments are more significant ( usually 1-2 % in Table 1 , 0.5-1.5 % in Table 2 at very high accuracies , and usually 1-2 % in Table 3 ) . While we did not extensively tune weight-decay in those settings , we believe that ( 1 ) the lack of need of tuning FD is a significant advantage in a setting where we aim for efficient training and ( 2 ) the figure suggests tuning weight-decay does not help much anyway . - _ \u201c [ W ] hat \u2019 s `` no decay ( normalized ) '' ? \u201d _ \u201c no decay ( normalized ) \u201d refers to the setting where we first train a network using FD and save the Frobenius norm of the ( recomposed ) weight matrix UV^T at each layer after each SGD step ; we then re-initialize and train the network with zero weight-decay , but after each SGD step we normalize U and V at each so that their product UV^T has the same Frobenius norm as that of the same layer at the same time step of the previous optimization . This experiment is described in the last paragraph of Section 3.3 ; in the revision we have added a note about this in the caption . 3._ \u201c In section 2 , the definition of the factorized CNN is not very clear to me . It might be good to give more detailed definitions here. \u201d _ We have added details about this in the revision ( Section 2.2 ) . 4._ \u201c Spectral initialization requires computing SVD of the weight matrix . If the matrix dimension is high , this step can be very time-consuming . I wonder if there is any more efficient way to construct the factorized matrices so that their product is still as i.i.d.Gaussian matrix . Because we do n't need to compute the SVD for an arbitrary matrix , what we need is only to make sure that the product of the factorized matrixes is distributed as i.i.d.Gaussian. \u201d _ Please see Point 2 of our general author response for a discussion of alternative initializations and the complexity of spectral initialization ."}, {"review_id": "KTlJT1nof6d-2", "review_text": "This paper discusses about applying low-rank matrix and tensor factorization of weight and applying weight decay on them . This type of low-rank regularization is an important mechanism in deep learning models and already many researchers have shown interest in this topic , hence this paper would interest many researchers in the community . The technical aspects of the paper seem to be correct . The experimental results are very encouraging since good improvements are shown with popular datasets . Also , the paper covers compression using ideas related to the state of the art tensor factorization methods . The most important component in learning with factorization methods is specifying the appropriate rank of matrices and tensors . When using factorization methods problems such as matrix/tensor completion , the performance will strongly depend on the rank . It is not clear how the rank of weight matrices would affect the performance of the deep learning model . This paper does not address this issue . Furthermore , there are no theoretical results shown that take the rank into consideration . Can the authors add an experiment to show how different factorization with respect to ranks affects the performance ? And/or can they give a theoretical result explaining how the rank relates to the improvement of the learning model ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the positive review . We hope to address your questions below . - _ \u201c It is not clear how the rank of weight matrices would affect the performance of the deep learning model . This paper does not address this issue . [ \u2026 ] Can the authors add an experiment to show how different factorization with respect to ranks affects the performance ? \u201d _ Note that the experiments where we report performance as a function of compression rate or parameter count ( e.g.Tables 1 and 2 ) effectively demonstrate the effect of rank on performance since the compression/parameter count is directly determined by varying the rank of the weight-matrices . However , in the revision we have added a plot ( Figure 2 , center and right ) demonstrating the performance of ResNet as a function of the rank , showing that FD improves upon regular weight-decay consistently in both the low-rank and over-complete case ( note that SI does not apply in the latter case ) . - _ \u201c [ T ] here are no theoretical results shown that take the rank into consideration . [ \u2026 ] [ Can the authors ] give a theoretical result explaining how the rank relates to the improvement of the learning model ? \u201d _ In the low-rank case , standard learning theory suggests that reducing the size of the model class ( e.g.by lowering the rank ) can improve generalization error . There also exist modern generalization error bounds that improve with lower stable rank ( and thus also rank since stable rank < = rank ) , c.f . Neyshabur et al . ( 2018 ) .Thus , since test error = training error + generalization error , the focus of our analysis is on improving the optimization of low-rank models in order the make the training error small , since these existing results already suggest that the generalization error will be small . We have slightly expanded upon this discussion in the revision ( Section 2.1 ) . For the high-rank ( self-taught distillation ) case , in the submission we pointed to recent work ( Du & Hu , 2019 ) that shows that width can improve optimization of deep nets ."}, {"review_id": "KTlJT1nof6d-3", "review_text": "The paper studies how the initialization and regularization of the factorized layers $ W=U\\Pi_i M_i V^T $ . It focused on the two known techniques , spectral initialization ( SI ) and Frobenius decay ( FD ) ( i.e. , regularize the weight $ W $ rather than the factorized terms $ U $ , $ V $ , and $ M_i $ 's ) , to initialize and regularize such layers for training . From the technical point of view , the paper is not novel , however , its strength is motivating the need for SI & FD , and providing various empirical studies in different applications such as model compression and self-taught distillation . Strengths : 1 . The paper is generally well-written and clear . The majority of claims are either cited appropriately or shown via extensive simulations . 2.Good motivations , supported by simulations ( Figures 1 , 2 ) and claim 3.1 3 . Extensive simulations for different tasks : compressed model training , knowledge distillation , and multi-head attention , in different areas computer vision , question-answering , transformer training , and BERT . Weaknesses : The paper can be seen as merely applying known methods to different areas and observing its performance . It lefts many questions unanswered and raises some other concerns ; 1 . SI initialization is reasonable when training a compressed model from a `` pre-trained '' model since the decomposition error is minimized $ W\\approx UV^T $ . However , when training from scratch , to my understanding , the method first generates a normal $ W\\sim N ( 0 , \\sigma^2 I ) $ , and uses SVD to find $ U $ and $ V $ . SI + FD can be interpreted as assuming a Gaussian prior on $ W $ and enforcing that prior on $ W $ during training . One immediate question would be why such a prior is a good assumption in general ? From VGG experiment ( Table 1 ) , one can conclude that specific initialization is not an important factor in training . Probably , for other simpler initializations for $ U $ and $ V $ ( e.g. , $ U $ and random Rademacher and $ V $ random normal , or even one of them being some truncated identity matrix ) the FD will work too . The claims can be interpreted as 'If Gaussian prior is necessary for the parameters , SI+FD is the way for training . '' 2.Figure 1 shows the bound in equation ( 2 ) is tight for the compression . Is there any similar result for the distillation ( i.e. , $ r > max ( m , n ) $ ) ? 3.How to extend SI to have factorization $ W=U ( \\Pi_ { i=1 } ^d M_i ) V^T $ for $ d > 0 $ ? There are a few minor points/typos that the paper can be improved on : - The RHS equality in equation ( 2 ) is not trivial . It needs a proper reference or proof . - In Table 1 , Dynamic Sparsity for CIFAR100 at compression 0.02 performs better than SI & FD . The bold text in the table incorrectly implies that SI & FD is better .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your positive review . We hope to address your questions below . - _ \u201c The paper can be seen as merely applying known methods to different areas and observing its performance. \u201d _ Please see Point 1 of our general author response for a discussion of novelty and contributions of the work . 1.- _ \u201c SI + FD can be interpreted as assuming a Gaussian prior on W and enforcing that prior on W during training . One immediate question would be why such a prior is a good assumption in general ? [ \u2026 ] 'If Gaussian prior is necessary for the parameters , SI+FD is the way for training . `` \u201d _ Thank you for pointing this out . We agree that in general the Gaussian prior might not be a good assumption , and that its success in our experiments may be because it has been shown to be effective for training unfactorized models , thus making it reasonable to enforce it in the factorized case . However , if a different priorimplemented via some initialization distribution and some regularization termis used in the unfactorized case , we believe our results suggest to ( 1 ) initialize using SVD on the full-rank matrix initialized with the same distribution and ( 2 ) applying the same regularization term on the recomposed matrix rather than the individual factors . So this would still be SI but the FD would be replaced by a different type of regularization . We have added a note about this in the revision ( Section 7 ) . - _ \u201c From VGG experiment ( Table 1 ) , one can conclude that specific initialization is not an important factor in training. \u201d _ Even in the case of VGG , SI+FD improves upon FD alone on two of the three datasets . However , perhaps its more limited effect here does suggest that a different initialization could lead to further improvements . - _ \u201c Probably , for other simpler initializations for U and V ( e.g. , U and random Rademacher and V random normal , or even one of them being some truncated identity matrix ) the FD will work too. \u201d _ Yes , we do not have strong evidence that FD always requires SI to work well ; in fact FD works well without it for the self-taught distillation experiments . Please also see Point 2 of our general author response for a discussion of alternative initializations . 2._ \u201c Figure 1 shows the bound in equation ( 2 ) is tight for the compression . Is there any similar result for the distillation ( i.e. , r > max ( m , n ) ) ? \u201d _ Thank you asking about this \u2013 we checked and found that the bound is indeed tight for the distillation case as well , suggesting that weight-decay is penalizing the nuclear norm across all ranks . We have included this result in the revision ( Figure 1 , right ) . 3._ \u201c How to extend SI [ \u2026 ] for d > 0 ? . \u201d _ For deeper factorizations one can initialize the inner matrices M to be the identity ( or a small perturbation ) . This keeps the distribution of the recomposed matrix the same . However , note that we only used d > 0 in our self-taught distillation experiments , where we did not use SI since it is not clear how to apply it when the factorization is over-complete . - _ \u201c There are a few minor points/typos that the paper can be improved on. \u201d _ Thank you , both are fixed in the revision ."}], "0": {"review_id": "KTlJT1nof6d-0", "review_text": "This paper studies how to initialize via spectral initialization and regularize DNNs via Frobenius decay . The benefits of spectral initialization and Frobenius decay are demonstrated via many experiments . This paper focuses on the empirical study of both SI and FD . My major concern is on its technical contribution . The spectral initialization is a scheme commonly used for low-rank models . It is well understood that spectral initialization is better than random initialization in low-rank literatures . Moreover , the proposed Frobenius decay is simply a penalization on the squared norm of the whole factorization . That is , it replaces the weight-decay $ \\|W\\|_F^2 $ in deep nets by plugging in the low-rank formulation of $ W $ . The behaviors of these two components in the experiments are expected . These two components have not provided much new insight . ~~~~~ This major concern is relieved after rebuttal .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Please see Point 1 in our general author response for the novelty and detailed contributions in our work . We would be happy to answer any questions about these claims or about the revision . Below we address some of your specific concerns about the two techniques we study . 1._ '' It is well understood that spectral initialization is better than random initialization in low-rank literatures . `` _ We would be interested in any references that compare spectral and random initialization so that we can cite them . The use of spectral initialization in past work is something we note in the second paragraph of Section 1 , but to our knowledge these works do not make such comparisons and generally focus on initializing from pretrained full-rank networks rather than training from scratch . 2._ '' Moreover , the proposed Frobenius decay is simply a penalization on the squared norm of the whole factorization. \u201d _ We believe simplicity is a benefit of the method . That such an effective and simple method is not more widely used makes it more necessary to formalize it and demonstrate its effectiveness . 3._ '' The behaviors of these two components in the experiments are expected . These two components have not provided much new insight . `` _ That the methods perform better is perhaps not surprising . However , ( a ) this behavior has not previously been analyzed mathematically and ( b ) we view many of our empirical findings as unexpected or at least not obvious , including the following : - Weight-decay implicitly penalizes the nuclear norm of the product despite explicitly regularizing only a loose upper bound ( Figure 1 ) . - Frobenius decay is better than weight-decay even after tuning the latter ( Figures 2 and 3 ) . - In some cases spectral initialization and Frobenius decay can decrease accuracy unless used jointly ( Table 1 ) ."}, "1": {"review_id": "KTlJT1nof6d-1", "review_text": "This paper studies initialization and regularization in factorized neural networks ( reparameterize a weight matrix by the product of several weight matrices ) . The authors proposed spectral initialization , that is to initialize the factorized matrices using the SVD of the un-factorized matrix . The authors also proposed Frobenius decay that is to regularize the Frobenius norm of the product of the factorized weight matrices . The motivation is to simulate the routines for non-decomposed counterparts . The authors empirically showed the effectiveness of spectral initialization and Frobenius decay in different applications : compressed model training , knowledge distillation , and multi-head self-training . I think it \u2019 s important to study the initialization and regularization for factorized neural networks . A priori , it needs different initialization and regularization methods due to different architecture compared with its non-decomposed counter-part . This paper gave very simple and natural solutions and was able to show its effectiveness in experiments . I also have some questions as below : 1 . In the experiments in section 5 ( knowledge distillation ) , default initialization is used instead of spectral initialization . I wonder if SI leads to a bad performance here . If that \u2019 s the case , it requires more explanation of why SI fails in this setting . 2.In Figure 1 , it seems FD is a stronger regularizer compared with default weight decay . It seems if the regularization coefficient is carefully tuned for each regularizer , the benefits of FD is actually not very significant . Also , what \u2019 s `` no decay ( normalized ) '' ? 3.In section 2 , the definition of the factorized CNN is not very clear to me . It might be good to give more detailed definitions here . 4.Spectral initialization requires computing SVD of the weight matrix . If the matrix dimension is high , this step can be very time-consuming . I wonder if there is any more efficient way to construct the factorized matrices so that their product is still as i.i.d.Gaussian matrix . Because we do n't need to compute the SVD for an arbitrary matrix , what we need is only to make sure that the product of the factorized matrixes is distributed as i.i.d.Gaussian .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the positive review . We hope to address your questions below . 1._ \u201c In the experiments in section 5 ( knowledge distillation ) , default initialization is used instead of spectral initialization . I wonder if SI leads to a bad performance here . If that \u2019 s the case , it requires more explanation of why SI fails in this setting. \u201d _ We do not use SI in Section 5 because there is no obvious way to apply it to overcomplete factorizations ( when the rank is higher than the original rank ) . For the full-rank case ( the \u201c full \u201d setting ) , the median performance is roughly the same with and without SI . Here are the results for ResNet56 : | | CIFAR-10 | CIFAR-100 | |-|-| -- | | WD | 92.45 | 69.23 | | SI+WD | 92.49 | 70.42 | | FD | 93.68 | 72.18 | | SI+FD | 93.48 | 72.19 | 2 . - _ \u201c In Figure 1 , it seems FD is a stronger regularizer compared with default weight decay . It seems if the regularization coefficient is carefully tuned for each regularizer , the benefits of FD is actually not very significant. \u201d _ In Figure 1 [ note : in the revision the relevant plot is now in Figure 2 ] , tuning weight-decay from the default value ( 1E-4 ) shows little to no improvement for both ResNet20 and ResNet44 , while tuning FD does yield a noticeable improvement on ResNet20 . We thus view this plot as showing that we can not achieve FD performance by tuning weight-decay , and that tuning weight-decay does not yield significant improvements . The gap between their performances is 0.5-1 % , which is not too small at this accuracy on CIFAR-10 , and the gaps in the other experiments are more significant ( usually 1-2 % in Table 1 , 0.5-1.5 % in Table 2 at very high accuracies , and usually 1-2 % in Table 3 ) . While we did not extensively tune weight-decay in those settings , we believe that ( 1 ) the lack of need of tuning FD is a significant advantage in a setting where we aim for efficient training and ( 2 ) the figure suggests tuning weight-decay does not help much anyway . - _ \u201c [ W ] hat \u2019 s `` no decay ( normalized ) '' ? \u201d _ \u201c no decay ( normalized ) \u201d refers to the setting where we first train a network using FD and save the Frobenius norm of the ( recomposed ) weight matrix UV^T at each layer after each SGD step ; we then re-initialize and train the network with zero weight-decay , but after each SGD step we normalize U and V at each so that their product UV^T has the same Frobenius norm as that of the same layer at the same time step of the previous optimization . This experiment is described in the last paragraph of Section 3.3 ; in the revision we have added a note about this in the caption . 3._ \u201c In section 2 , the definition of the factorized CNN is not very clear to me . It might be good to give more detailed definitions here. \u201d _ We have added details about this in the revision ( Section 2.2 ) . 4._ \u201c Spectral initialization requires computing SVD of the weight matrix . If the matrix dimension is high , this step can be very time-consuming . I wonder if there is any more efficient way to construct the factorized matrices so that their product is still as i.i.d.Gaussian matrix . Because we do n't need to compute the SVD for an arbitrary matrix , what we need is only to make sure that the product of the factorized matrixes is distributed as i.i.d.Gaussian. \u201d _ Please see Point 2 of our general author response for a discussion of alternative initializations and the complexity of spectral initialization ."}, "2": {"review_id": "KTlJT1nof6d-2", "review_text": "This paper discusses about applying low-rank matrix and tensor factorization of weight and applying weight decay on them . This type of low-rank regularization is an important mechanism in deep learning models and already many researchers have shown interest in this topic , hence this paper would interest many researchers in the community . The technical aspects of the paper seem to be correct . The experimental results are very encouraging since good improvements are shown with popular datasets . Also , the paper covers compression using ideas related to the state of the art tensor factorization methods . The most important component in learning with factorization methods is specifying the appropriate rank of matrices and tensors . When using factorization methods problems such as matrix/tensor completion , the performance will strongly depend on the rank . It is not clear how the rank of weight matrices would affect the performance of the deep learning model . This paper does not address this issue . Furthermore , there are no theoretical results shown that take the rank into consideration . Can the authors add an experiment to show how different factorization with respect to ranks affects the performance ? And/or can they give a theoretical result explaining how the rank relates to the improvement of the learning model ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the positive review . We hope to address your questions below . - _ \u201c It is not clear how the rank of weight matrices would affect the performance of the deep learning model . This paper does not address this issue . [ \u2026 ] Can the authors add an experiment to show how different factorization with respect to ranks affects the performance ? \u201d _ Note that the experiments where we report performance as a function of compression rate or parameter count ( e.g.Tables 1 and 2 ) effectively demonstrate the effect of rank on performance since the compression/parameter count is directly determined by varying the rank of the weight-matrices . However , in the revision we have added a plot ( Figure 2 , center and right ) demonstrating the performance of ResNet as a function of the rank , showing that FD improves upon regular weight-decay consistently in both the low-rank and over-complete case ( note that SI does not apply in the latter case ) . - _ \u201c [ T ] here are no theoretical results shown that take the rank into consideration . [ \u2026 ] [ Can the authors ] give a theoretical result explaining how the rank relates to the improvement of the learning model ? \u201d _ In the low-rank case , standard learning theory suggests that reducing the size of the model class ( e.g.by lowering the rank ) can improve generalization error . There also exist modern generalization error bounds that improve with lower stable rank ( and thus also rank since stable rank < = rank ) , c.f . Neyshabur et al . ( 2018 ) .Thus , since test error = training error + generalization error , the focus of our analysis is on improving the optimization of low-rank models in order the make the training error small , since these existing results already suggest that the generalization error will be small . We have slightly expanded upon this discussion in the revision ( Section 2.1 ) . For the high-rank ( self-taught distillation ) case , in the submission we pointed to recent work ( Du & Hu , 2019 ) that shows that width can improve optimization of deep nets ."}, "3": {"review_id": "KTlJT1nof6d-3", "review_text": "The paper studies how the initialization and regularization of the factorized layers $ W=U\\Pi_i M_i V^T $ . It focused on the two known techniques , spectral initialization ( SI ) and Frobenius decay ( FD ) ( i.e. , regularize the weight $ W $ rather than the factorized terms $ U $ , $ V $ , and $ M_i $ 's ) , to initialize and regularize such layers for training . From the technical point of view , the paper is not novel , however , its strength is motivating the need for SI & FD , and providing various empirical studies in different applications such as model compression and self-taught distillation . Strengths : 1 . The paper is generally well-written and clear . The majority of claims are either cited appropriately or shown via extensive simulations . 2.Good motivations , supported by simulations ( Figures 1 , 2 ) and claim 3.1 3 . Extensive simulations for different tasks : compressed model training , knowledge distillation , and multi-head attention , in different areas computer vision , question-answering , transformer training , and BERT . Weaknesses : The paper can be seen as merely applying known methods to different areas and observing its performance . It lefts many questions unanswered and raises some other concerns ; 1 . SI initialization is reasonable when training a compressed model from a `` pre-trained '' model since the decomposition error is minimized $ W\\approx UV^T $ . However , when training from scratch , to my understanding , the method first generates a normal $ W\\sim N ( 0 , \\sigma^2 I ) $ , and uses SVD to find $ U $ and $ V $ . SI + FD can be interpreted as assuming a Gaussian prior on $ W $ and enforcing that prior on $ W $ during training . One immediate question would be why such a prior is a good assumption in general ? From VGG experiment ( Table 1 ) , one can conclude that specific initialization is not an important factor in training . Probably , for other simpler initializations for $ U $ and $ V $ ( e.g. , $ U $ and random Rademacher and $ V $ random normal , or even one of them being some truncated identity matrix ) the FD will work too . The claims can be interpreted as 'If Gaussian prior is necessary for the parameters , SI+FD is the way for training . '' 2.Figure 1 shows the bound in equation ( 2 ) is tight for the compression . Is there any similar result for the distillation ( i.e. , $ r > max ( m , n ) $ ) ? 3.How to extend SI to have factorization $ W=U ( \\Pi_ { i=1 } ^d M_i ) V^T $ for $ d > 0 $ ? There are a few minor points/typos that the paper can be improved on : - The RHS equality in equation ( 2 ) is not trivial . It needs a proper reference or proof . - In Table 1 , Dynamic Sparsity for CIFAR100 at compression 0.02 performs better than SI & FD . The bold text in the table incorrectly implies that SI & FD is better .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your positive review . We hope to address your questions below . - _ \u201c The paper can be seen as merely applying known methods to different areas and observing its performance. \u201d _ Please see Point 1 of our general author response for a discussion of novelty and contributions of the work . 1.- _ \u201c SI + FD can be interpreted as assuming a Gaussian prior on W and enforcing that prior on W during training . One immediate question would be why such a prior is a good assumption in general ? [ \u2026 ] 'If Gaussian prior is necessary for the parameters , SI+FD is the way for training . `` \u201d _ Thank you for pointing this out . We agree that in general the Gaussian prior might not be a good assumption , and that its success in our experiments may be because it has been shown to be effective for training unfactorized models , thus making it reasonable to enforce it in the factorized case . However , if a different priorimplemented via some initialization distribution and some regularization termis used in the unfactorized case , we believe our results suggest to ( 1 ) initialize using SVD on the full-rank matrix initialized with the same distribution and ( 2 ) applying the same regularization term on the recomposed matrix rather than the individual factors . So this would still be SI but the FD would be replaced by a different type of regularization . We have added a note about this in the revision ( Section 7 ) . - _ \u201c From VGG experiment ( Table 1 ) , one can conclude that specific initialization is not an important factor in training. \u201d _ Even in the case of VGG , SI+FD improves upon FD alone on two of the three datasets . However , perhaps its more limited effect here does suggest that a different initialization could lead to further improvements . - _ \u201c Probably , for other simpler initializations for U and V ( e.g. , U and random Rademacher and V random normal , or even one of them being some truncated identity matrix ) the FD will work too. \u201d _ Yes , we do not have strong evidence that FD always requires SI to work well ; in fact FD works well without it for the self-taught distillation experiments . Please also see Point 2 of our general author response for a discussion of alternative initializations . 2._ \u201c Figure 1 shows the bound in equation ( 2 ) is tight for the compression . Is there any similar result for the distillation ( i.e. , r > max ( m , n ) ) ? \u201d _ Thank you asking about this \u2013 we checked and found that the bound is indeed tight for the distillation case as well , suggesting that weight-decay is penalizing the nuclear norm across all ranks . We have included this result in the revision ( Figure 1 , right ) . 3._ \u201c How to extend SI [ \u2026 ] for d > 0 ? . \u201d _ For deeper factorizations one can initialize the inner matrices M to be the identity ( or a small perturbation ) . This keeps the distribution of the recomposed matrix the same . However , note that we only used d > 0 in our self-taught distillation experiments , where we did not use SI since it is not clear how to apply it when the factorization is over-complete . - _ \u201c There are a few minor points/typos that the paper can be improved on. \u201d _ Thank you , both are fixed in the revision ."}}