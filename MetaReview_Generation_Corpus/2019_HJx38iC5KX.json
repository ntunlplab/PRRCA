{"year": "2019", "forum": "HJx38iC5KX", "title": "Domain Generalization via Invariant Representation under Domain-Class Dependency", "decision": "Reject", "meta_review": "This paper proposes a new solution to the problem of domain generalization where the label distribution may differ across domains. The authors argue that prior work which ignores this observation suffers from an accuracy-vs-invariance trade-off while their work does not. \n\nThe main contribution of the work is to 1) consider the case of different label distributions across domains and 2) to propose a regularizer extension to Xie 2017 to handle this. \n\nThere was disagreement between the reviewers on whether or not this contribution is significant enough to warrant publication. Two reviewers expressed concern of whether 1) naturally occurring data sources suffer substantially from this label distribution mismatch and 2) whether label distribution mismatch in practice results in significant performance loss for existing domain generalization techniques. Based on the experiments and discussions available now the answer to the above two points remains unclear. These key questions should be clarified and further justified before publication.", "reviews": [{"review_id": "HJx38iC5KX-0", "review_text": "In this paper, the author(s) propose a method, invariant feature learning under optimal classifier constrains (IFLOC), which maintains accuracy while improving domain-invariance. Here is a list of suggestions that will help the author(s) to improve this paper. 1.The paper explains the necessity and effectiveness of the method from the theoretical and experimental aspects, but the paper does not support the innovation point enough, and the explanation is too simple. 2.In this paper, Figure3-(b) shows that the classification accuracy of IFLOC-abl method decreases a lot when \u03b3 is taken to 0. Figure3-(c) shows that the domain invariance of IFLOC-abl method becomes significantly worse when \u03b3 is 10. The author(s) should explain the reasons in detail. 3. The lack of analysis on domain-class dependency of each dataset makes the analysis of experimental results weak. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your critical feedback . We hope to clarify and address your concerns and questions . We respond in detail to each comment below . # # # Reply to comment 1 Regarding this comment , we would very much appreciate if you could give us more details so that we could exactly address your concerns and improve the paper . For example , we would like to know ( 1 ) why you think `` the paper does not support the innovation point enough '' and ( 2 ) which parts of `` the explanation is too simple '' and why simple is a weak point . Here , we would like to clarify three innovation points of the paper we think . ( 1 ) We elaborate on the trade-off problem under domain-class dependency , both theoretically ( Sec.4.1.1 ) and experimentally ( 1st-paragraph of Sec.5.4 ) , for the first time in domain generalization context . Here , note that domain generalization is different from domain adaptation in that we can not obtain input and label data from target domain ( s ) , but has been attracting considerable attention in recent years . ( 2 ) We propose to maximize domain-invariance within a range that does not interfere with classification accuracy and provide the theoretical analysis which derives the novel approach ( i.e. , to regularize latent representation so that H ( d|y ) =H ( d|h ) holds ) to address the aforementioned problem in Sec.4.1.2 . ( 3 ) We confirm the efficacy of the proposed approch with the novel algorighm ( IFLOC ) in Sec.5.4 . Moreover , we have added the below contents to the updated paper in order to clarify the inovation points . 1.To make it clear that domain generalization differs from domain adaptation , we have added the following sentence : `` ` ( Sec.2 , para.2 ) Domain generalization has been attracting considerable attention in recent years ( Blanchard et al . ( 2011 ) ; Muandet et al . ( 2013 ) ; Shankar et al . ( 2018 ) ) .Note that it is different from domain adaptation in that we can not obtain input and label data from target domain ( s ) . `` ` 2.To make it clear that domain-class dependency ( p ( y|d ) \\neq p ( y ) ) is a novel and important problem setting in domain generalization , we compare it with conditional probability shift ( p ( y|x ) and p ( x ) change across domains ) and showed that domain-class dependency is a root cause of the trade-off problem : `` ` ( Sec.1 , para.3 ) Domain-class dependency might be similar to the situation where p ( y|x ) and p ( x ) change across domains due to the causal structure y \u2192 x ( Zhang et al . ( 2013 ) ; Gong et al . ( 2016 ) in domain adaptation and Li et al . ( 2018c ) in domain generalization ) , which we call conditional probability shift . However , the shift does not cause the trade-off as long as y and d are independent ( Figure 1-left ) , so it is necessary to focus on the relationship between y and d. ( Sec.2 , para.5 ) There are several kinds of distributional shifts other than domain-class dependency , such as conditional probability shift . Although the distinction between that shift and domain-class dependency is important , it has been received less attention . For example , Li et al . ( 2018c ) claimed that conditional probability shift might harm the performance of domain-invariance-based methods , but our analysis in Sec.4.1.1 suggests that the root cause of the performance degradation is not it but domain-class dependency . `` `"}, {"review_id": "HJx38iC5KX-1", "review_text": "This paper proposed to address domain generalization under inter-dependence of domains and classes. It motivates a new regularization term by analyzing an existing work, DAN. It shows that this term can improve the generalization performance when the classes and domains are not independent. Experiments are extensive and supportive. I do not have many comments about this paper. It was a joy to read. The proposed idea is well motivated. It is simple and seems like effective. Experiments are extensive. While the regularization term is motivated by analyzing DAN, it would be nice to discuss its application to other domain adaptation/generalization methods. What is even better is to show its effectiveness on another method in the experiments.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your positive review . We really appreciate the remarks that our \u201c idea is well motivated \u201d and \u201c experiments are extensive and supportive \u201d . # # # Reply to \u201c to discuss its application to other domain adaptation/generalization methods and to show its effectiveness on another method in the experiments \u201d would improve the paper . Thank you for the suggestion . As you pointed out , the regularization term is motivated by analyzing DAN . So we acknowledge that its application to other domain adaptation/generalization methods is not trivial and discussing it would improve the paper . One possible direction is to modify conditional VAE ( Louizos+2015 ) or CrossGrad ( Shankar+2018 ) to make H ( d|h ) = H ( d|y ) holds and to use it in domain generalization . These methods have clear and tractable data generating process but assume the independence of y and d , so we hope we can somehow modify it to H ( d|h ) = H ( d|y ) holds . However , we unfortunately might not have time to develop concrete methods and conduct experiments , so we have added the above discussion to the paper . # # # References Christos Louizos , Kevin Swersky , Yujia Li , Max Welling , and Richard S. Zemel . The variational fair autoencoder . ICLR2016 Shiv Shankar , Vihari Piratla , Soumen Chakrabarti , Siddhartha Chaudhuri , Preethi Jyothi , and Sunita Sarawagi . Generalizing across domains via cross-gradient training . ICLR2018"}, {"review_id": "HJx38iC5KX-2", "review_text": "The paper proposed a problem that most prior methods overlooked the underlying dependency of classes on domains, namely p (y|d) \\= p(y). Figure 1 is used to illustrate this issue. If the conditional probability of source domain and target domain is not equal (i.e., p(y|x_S) \\= p(y|x_T) ), the optimal invariance can lead the same generalization problem. Unfortunately, a lot of works has been done [1,2] in matching domain classifier or conditional probability. It is desirable to discuss the difference between these two problems and compared with the missing references in experiments. It is also suggested to conduct the analysis of why the datasets satisfy the assumption of the dependence of class and domains. Reference: [1] Flexible Transfer Learning under Support and Model Shift, NIPS 2014. [2]Conditional Adversarial Domain Adaptation, NIPS 2018", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your critical feedback . We hope to clarify and address your concerns and questions . We respond in detail to each comment below . # # # Reply to \u201c it is desirable to discuss the difference between these two problems \u201d In our understanding , your main concern is the novelty of our problem setting , i.e. , \u201c is domain generalization under domain-class dependency ( p ( y|d ) \\neq p ( y ) ) is different from domain adaptation under p ( y|x_S ) \\neq p ( y|x_T ) ? \u201d We acknowledge that we lack the discussion about the difference between these two ( though they are indeed considerably different problems ) , so we have added the below discussion to the paper and emphasized the novelty of our problem setting . Firstly , the paper addresses { \\em domain generalization } , not domain adaptation , as noted in abstract , Sec.1 , etc . These two have different assumptions and purposes . Concretely , domain adaptation methods require either labeled or unlabeled data from the target domain at training time . In contrast , domain generalization methods do not require any data from target domains during training but instead , require labeled data from several source domains . Then the methods collectively exploit them so that the trained system can handle new domains without any adaptation step . Due to the difference , domain adaptation methods are not always applicable to domain generalization . For example , Wang+2014 , which you suggested for us , transform unlabeled target data so that they can correct distributional shift , but in domain generalization , target data are unavailable . Also , please note that we care about the shifts within source domains , because domain generalization methods are agnostic on the target domain . So we think p ( y|x_S ) \\neq p ( y|x_T ) should be rewritten as p ( y|x , d ) \\neq p ( y|x ) ( we call it conditional probability shift ) in domain generalization , so that clarify we focus on the shift within source domains ( not between S and T ) . Secondly , conditional probability shift ( p ( y|x , d ) \\neq p ( y|x ) ) , which is often caused by the causal structure y - > x , is not a sufficient condition for p ( y|d ) \\neq p ( y ) . While conditional probability shift was previously addressed by Li+2018 in domain generalization context , domain-class dependency has been overlooked . The relation between these two problems is illustrated in Figure 1 in our updated paper . Thirdly and most importantly , in domain generalization , conditional probability shift does not cause the trade-off problem as long as the domain-class dependency does not exist . In other words , p ( y|x , d ) \\neq p ( y|x ) is not a root cause of the trade-off problem , but domain-class dependency is , so it is essential to consider and address domain-class dependency problem . Again the relation between these two problems is illustrated in Figure 1 in our updated paper ."}], "0": {"review_id": "HJx38iC5KX-0", "review_text": "In this paper, the author(s) propose a method, invariant feature learning under optimal classifier constrains (IFLOC), which maintains accuracy while improving domain-invariance. Here is a list of suggestions that will help the author(s) to improve this paper. 1.The paper explains the necessity and effectiveness of the method from the theoretical and experimental aspects, but the paper does not support the innovation point enough, and the explanation is too simple. 2.In this paper, Figure3-(b) shows that the classification accuracy of IFLOC-abl method decreases a lot when \u03b3 is taken to 0. Figure3-(c) shows that the domain invariance of IFLOC-abl method becomes significantly worse when \u03b3 is 10. The author(s) should explain the reasons in detail. 3. The lack of analysis on domain-class dependency of each dataset makes the analysis of experimental results weak. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your critical feedback . We hope to clarify and address your concerns and questions . We respond in detail to each comment below . # # # Reply to comment 1 Regarding this comment , we would very much appreciate if you could give us more details so that we could exactly address your concerns and improve the paper . For example , we would like to know ( 1 ) why you think `` the paper does not support the innovation point enough '' and ( 2 ) which parts of `` the explanation is too simple '' and why simple is a weak point . Here , we would like to clarify three innovation points of the paper we think . ( 1 ) We elaborate on the trade-off problem under domain-class dependency , both theoretically ( Sec.4.1.1 ) and experimentally ( 1st-paragraph of Sec.5.4 ) , for the first time in domain generalization context . Here , note that domain generalization is different from domain adaptation in that we can not obtain input and label data from target domain ( s ) , but has been attracting considerable attention in recent years . ( 2 ) We propose to maximize domain-invariance within a range that does not interfere with classification accuracy and provide the theoretical analysis which derives the novel approach ( i.e. , to regularize latent representation so that H ( d|y ) =H ( d|h ) holds ) to address the aforementioned problem in Sec.4.1.2 . ( 3 ) We confirm the efficacy of the proposed approch with the novel algorighm ( IFLOC ) in Sec.5.4 . Moreover , we have added the below contents to the updated paper in order to clarify the inovation points . 1.To make it clear that domain generalization differs from domain adaptation , we have added the following sentence : `` ` ( Sec.2 , para.2 ) Domain generalization has been attracting considerable attention in recent years ( Blanchard et al . ( 2011 ) ; Muandet et al . ( 2013 ) ; Shankar et al . ( 2018 ) ) .Note that it is different from domain adaptation in that we can not obtain input and label data from target domain ( s ) . `` ` 2.To make it clear that domain-class dependency ( p ( y|d ) \\neq p ( y ) ) is a novel and important problem setting in domain generalization , we compare it with conditional probability shift ( p ( y|x ) and p ( x ) change across domains ) and showed that domain-class dependency is a root cause of the trade-off problem : `` ` ( Sec.1 , para.3 ) Domain-class dependency might be similar to the situation where p ( y|x ) and p ( x ) change across domains due to the causal structure y \u2192 x ( Zhang et al . ( 2013 ) ; Gong et al . ( 2016 ) in domain adaptation and Li et al . ( 2018c ) in domain generalization ) , which we call conditional probability shift . However , the shift does not cause the trade-off as long as y and d are independent ( Figure 1-left ) , so it is necessary to focus on the relationship between y and d. ( Sec.2 , para.5 ) There are several kinds of distributional shifts other than domain-class dependency , such as conditional probability shift . Although the distinction between that shift and domain-class dependency is important , it has been received less attention . For example , Li et al . ( 2018c ) claimed that conditional probability shift might harm the performance of domain-invariance-based methods , but our analysis in Sec.4.1.1 suggests that the root cause of the performance degradation is not it but domain-class dependency . `` `"}, "1": {"review_id": "HJx38iC5KX-1", "review_text": "This paper proposed to address domain generalization under inter-dependence of domains and classes. It motivates a new regularization term by analyzing an existing work, DAN. It shows that this term can improve the generalization performance when the classes and domains are not independent. Experiments are extensive and supportive. I do not have many comments about this paper. It was a joy to read. The proposed idea is well motivated. It is simple and seems like effective. Experiments are extensive. While the regularization term is motivated by analyzing DAN, it would be nice to discuss its application to other domain adaptation/generalization methods. What is even better is to show its effectiveness on another method in the experiments.", "rating": "7: Good paper, accept", "reply_text": "Thank you for your positive review . We really appreciate the remarks that our \u201c idea is well motivated \u201d and \u201c experiments are extensive and supportive \u201d . # # # Reply to \u201c to discuss its application to other domain adaptation/generalization methods and to show its effectiveness on another method in the experiments \u201d would improve the paper . Thank you for the suggestion . As you pointed out , the regularization term is motivated by analyzing DAN . So we acknowledge that its application to other domain adaptation/generalization methods is not trivial and discussing it would improve the paper . One possible direction is to modify conditional VAE ( Louizos+2015 ) or CrossGrad ( Shankar+2018 ) to make H ( d|h ) = H ( d|y ) holds and to use it in domain generalization . These methods have clear and tractable data generating process but assume the independence of y and d , so we hope we can somehow modify it to H ( d|h ) = H ( d|y ) holds . However , we unfortunately might not have time to develop concrete methods and conduct experiments , so we have added the above discussion to the paper . # # # References Christos Louizos , Kevin Swersky , Yujia Li , Max Welling , and Richard S. Zemel . The variational fair autoencoder . ICLR2016 Shiv Shankar , Vihari Piratla , Soumen Chakrabarti , Siddhartha Chaudhuri , Preethi Jyothi , and Sunita Sarawagi . Generalizing across domains via cross-gradient training . ICLR2018"}, "2": {"review_id": "HJx38iC5KX-2", "review_text": "The paper proposed a problem that most prior methods overlooked the underlying dependency of classes on domains, namely p (y|d) \\= p(y). Figure 1 is used to illustrate this issue. If the conditional probability of source domain and target domain is not equal (i.e., p(y|x_S) \\= p(y|x_T) ), the optimal invariance can lead the same generalization problem. Unfortunately, a lot of works has been done [1,2] in matching domain classifier or conditional probability. It is desirable to discuss the difference between these two problems and compared with the missing references in experiments. It is also suggested to conduct the analysis of why the datasets satisfy the assumption of the dependence of class and domains. Reference: [1] Flexible Transfer Learning under Support and Model Shift, NIPS 2014. [2]Conditional Adversarial Domain Adaptation, NIPS 2018", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your critical feedback . We hope to clarify and address your concerns and questions . We respond in detail to each comment below . # # # Reply to \u201c it is desirable to discuss the difference between these two problems \u201d In our understanding , your main concern is the novelty of our problem setting , i.e. , \u201c is domain generalization under domain-class dependency ( p ( y|d ) \\neq p ( y ) ) is different from domain adaptation under p ( y|x_S ) \\neq p ( y|x_T ) ? \u201d We acknowledge that we lack the discussion about the difference between these two ( though they are indeed considerably different problems ) , so we have added the below discussion to the paper and emphasized the novelty of our problem setting . Firstly , the paper addresses { \\em domain generalization } , not domain adaptation , as noted in abstract , Sec.1 , etc . These two have different assumptions and purposes . Concretely , domain adaptation methods require either labeled or unlabeled data from the target domain at training time . In contrast , domain generalization methods do not require any data from target domains during training but instead , require labeled data from several source domains . Then the methods collectively exploit them so that the trained system can handle new domains without any adaptation step . Due to the difference , domain adaptation methods are not always applicable to domain generalization . For example , Wang+2014 , which you suggested for us , transform unlabeled target data so that they can correct distributional shift , but in domain generalization , target data are unavailable . Also , please note that we care about the shifts within source domains , because domain generalization methods are agnostic on the target domain . So we think p ( y|x_S ) \\neq p ( y|x_T ) should be rewritten as p ( y|x , d ) \\neq p ( y|x ) ( we call it conditional probability shift ) in domain generalization , so that clarify we focus on the shift within source domains ( not between S and T ) . Secondly , conditional probability shift ( p ( y|x , d ) \\neq p ( y|x ) ) , which is often caused by the causal structure y - > x , is not a sufficient condition for p ( y|d ) \\neq p ( y ) . While conditional probability shift was previously addressed by Li+2018 in domain generalization context , domain-class dependency has been overlooked . The relation between these two problems is illustrated in Figure 1 in our updated paper . Thirdly and most importantly , in domain generalization , conditional probability shift does not cause the trade-off problem as long as the domain-class dependency does not exist . In other words , p ( y|x , d ) \\neq p ( y|x ) is not a root cause of the trade-off problem , but domain-class dependency is , so it is essential to consider and address domain-class dependency problem . Again the relation between these two problems is illustrated in Figure 1 in our updated paper ."}}