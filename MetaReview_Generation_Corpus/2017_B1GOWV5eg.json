{"year": "2017", "forum": "B1GOWV5eg", "title": "Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "The basic idea of this paper is simple: run RL over an action space that models both the actions and the number of times they are repeated. It's a simple idea, but seems to work really well on a pretty substantial variety of domains, and it can be easily adapted to many different settings. In several settings, the improvement using this approach are dramatic. I think this is an obvious accept: a simple addition to existing RL algorithms that can often perform much better.\n \n Pros:\n + Simple and intuitive approach, easy to implement\n + Extensive evaluation, showing very good performance\n \n Cons:\n - Sometimes unclear _why_ certain domains benefit so much from this", "reviews": [{"review_id": "B1GOWV5eg-0", "review_text": "This paper shows that extending deep RL algorithms to decide which action to take as well as how many times to repeat it leads to improved performance on a number of domains. The evaluation is very thorough and shows that this simple idea works well in both discrete and continuous actions spaces. A few comments/questions: - Table 1 could be easier to interpret as a figure of histograms. - Figure 3 could be easier to interpret as a table. - How was the subset of Atari games selected? - The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration (e.g. Freeway and Seaquest), but it would be nice to see a full evaluation on 57 games. This has become quite standard and would make it possible to compare overall performance using mean and median scores. - It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al., which aims to solve some of the same problems as FiGAR. - FiGAR currently discards frames between action decisions. There might be a tradeoff between repeating an action more times and throwing away more information. Have you thought about separating these effects? You could train a model that does process intermediate frames. Just a thought. Overall, this is a nice simple addition to deep RL algorithms that many people will probably start using. -------------------- I'm increasing my score to 8 based on the rebuttal and the revised paper.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for reviewing the paper , the comments and questions ! We believe addressing these questions will increase the quality of the work , and we will definitely do that . - Table 1 could be easier to interpret as a figure of histograms . Thanks for pointing this out . We will definitely add a histogram to the final version of the paper corresponding to Table 1 . - Figure 3 could be easier to interpret as a table . We added a histogram version of this data ( Figure3 ) because we wanted to illustrate that regardless of the action repetition set chosen , the rough `` magnitude '' of improvement is still the same . That all FiGAR variants continue to significantly outperform the baseline in the chosen games . We will definitely add a corresponding table of raw data in the appendix so that it can be looked up . - How was the subset of Atari games selected ? The subset was chosen arbitrarily . - The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration ( e.g.Freeway and Seaquest ) , but it would be nice to see a full evaluation on 57 games . This has become quite standard and would make it possible to compare overall performance using mean and median scores . The results we have reported on 31 games was possible only after 3 months of computing . It might be difficult to report numbers on all 57 games , however we will definitely try to make the number of games on which we report results as large as possible ( We already have results on 2 more games that we will add to the final version of the paper ) . - It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al. , which aims to solve some of the same problems as FiGAR . STRAW was run on a very small subset of games , namely 8 Atari games . The intersection of games on which both our work and STRAW report results is even smaller at 5 games . Such a comparison is likely to be skewed . Having said that , we could definitely add scores reported by STRAW on the 5 games that we have evaluated on to Table 4 . - FiGAR currently discards frames between action decisions . There might be a tradeoff between repeating an action more times and throwing away more information . Have you thought about separating these effects ? You could train a model that does process intermediate frames . Just a thought . As pointed out in one of the comments : `` After learning is complete , did you try forward propagating through the network to find actions for every time-step as opposed to repeating actions ? Concretely , if at t=5 , action suggested by the network is a_3 with a repetition of 4 , instead of sticking with a_3 for times t= { 5,6,7,8 } perform action a_3 for just t=5 , and forward prop through the policy again at t=6 . '' This is definitely an experiment worth trying out and we intend to do that and include results if they turn out to be significant . Having said that , this only makes use of the discarded frames in the testing phase , not in the training phase . A possible way to trade-off between discarding frames and action repeats is to construct a separate , second network ( consisting of a convnet followed by an LSTM ) which processes every kth frame , much like A3C , and concatenate representations learnt by this network to those learnt by the usual A3C network , while making decisions on action selection . The reason one would like to do this is because as you rightly pointed out , skipped frames might also contain crucial information needed for finding out the optimal action as well as action repetition in the next action decision step . We will definitely explore this direction of work as future research . Thanks for the idea !"}, {"review_id": "B1GOWV5eg-1", "review_text": "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it. Comments: - The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running their replication to the same settings as Mnih et al provide similar results? - It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C. Nevertheless, the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often. - Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5) - In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps? - The section on DDPG is confusingly written. \"Concatenating\" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? - Is the 'name_this_game' name in the tables intentional? - A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR).", "rating": "7: Good paper, accept", "reply_text": "Thanks for reviewing the paper , the comments and questions ! We believe addressing these questions will increase the quality of the work , and we will definitely do that . - The scores reported on A3C in this paper and in the Mnih et al.publication ( table S3 ) differ significantly . Where does this discrepancy come from ? If it 's from a different training regime ( fewer iterations , for instance ) , did the authors confirm that running their replication to the same settings as Mnih et al provide similar results ? The reason why the scores differ significantly is because of 3 reasons : 1 . Mnih et al.publication [ 1 ] reports average scores on best 5 replicas out of 50 replicas that they started with . We did not mimic this setup because we do not possess the compute resources to run 50 different replicas for each game . 2.The evaluation method used was very different . They used human starts evaluation metric . However , in the absence of the same human trajectories it would be very difficult to ensure a fair or repeatable evaluation setup . 3.In fact we have found that the scores in general and evaluation setup in specific reported by Mnih et al . [ 1 ] are difficult to reproduce not only for us , but also researchers at Deepmind . In Unifying Count-Based Exploration and Intrinsic Motivation [ 2 ] the scores reported for A3C differ very drastically from those reported by the original A3C publication [ 1 ] , even though the same evaluation metric ( human starts ) was followed , and hopefully the same set of human start trajectories was used ( we do not know this for sure ) . The scores for many games are orders of magnitude lower for A3C in [ 2 ] . In conclusion we \u2019 d like to say that the training as well as testing setup of [ 1 ] are difficult to reproduce , which in turn makes it difficult to replicate the scores . - It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate . This seems to imply that for those , the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 ( and therefore has 4 times fewer gradient updates ) . A3C could be run for a comparable computation cost with a lower action repeat , which would probably result in increased performance of A3C . Nevertheless , the automatic determination of the appropriate action repeat is interesting , even if the overall message seems to be to not repeat actions too often . It is true that for many games the lower action repetitions dominate in the sense that they are chosen for a large fraction of time . However , the average action repetition ( ARR ) is a fairer metric to compare the computation cost since FiGAR still makes up by choosing large action repetition at other points in time . Table 5,6,7 in Appendix B seek to demonstrate the action repetition distribution and the ARR for all the games . It can be seen that for 28 out of 31 games , the average action repetition for FiGAR is greater than 4 ( which is the ARR for A3C ) . Concretely for the best 4 games by gameplay performance , the average action repetitions are ( numbers taken from Table 7 , page 18 , Appendix B ) : Atlantis : 7.2 Seaquest : 5.33 Asterix : 4.22 Wizard of wor : 9.87 - Slightly problematic notation , where r sometimes denotes rewards , sometimes denotes elements of the repetition set R ( top of page 5 ) Thanks for pointing this out . We will change this in the next revision . - In the equation at the bottom of page 5 - since the sum is indexed over decision steps , not time steps , should n't the rewards r_k be modified to be the sum of rewards ( appropriately discounted ) between those time steps ? Thanks for pointing this out . The question is how should the reward for a macro action m = ( a , x ) be constructed . Should it be the discounted sum of intermediate rewards encountered during the execution of m or should it be the cumulative undiscounted sum of rewards ? We went with the second formalism since we did not want to penalize the agent for choosing larger action repetitions . This we believe would encourage the agent to pick larger action repetitions . - The section on DDPG is confusingly written . `` Concatenating '' loss is a strange operation ; does n't FiGAR correspond to a loss to roughly looks like Q ( x , mu ( x ) ) + R log p ( x ) ( with separate loss for learning the critic ) ? It feels that REINFORCE should be applied for the repetition variable x ( second term of the sum ) and reparametrization for the action a ( first term ) ? Sorry for this . In DDPG , there is only a single loss function , the critic loss function . There is no loss function for the actor . The actor simply receives gradients from the critic . This is because the actor \u2019 s proposed policy is directly fed to the critic and the critic provides the actor with gradients which the proposed policy follows for improvement . Hence , the actor does not really have a loss function per se , but only gradients provided by the critic . In FiGAR the total policy \\pi is a concatenation of vectors \\pi_ { a } and \\pi_ { x } . Hence the gradients for the total policy are also simply the concatenation of the gradients for the policies \\pi_ { a } and \\pi_ { x } . This is what we meant by the concatenation operator . We will make the section clearer in the next revision . - Is the 'name_this_game ' name in the tables intentional ? It is.This is the name of a game in the Atari 2600 domain . Here is a video which shows gameplay in this game : https : //www.youtube.com/watch ? v=7obD1q85_kw . Note that this video is in no way related to FiGAR and only demonstrates general gameplay in this game . - A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps , independently of what happens next . Have the authors considered a scheme in which , at each time step , the agent decides to stick with the current decision or not ? ( It feels like it might be a relatively simple modification of FiGAR ) . We agree with the reviewer and explain the need for and a possible solution for stopping macro-actions . Atari , TORCS and MuJoCo represent environments which are largely deterministic with a minimal degree of stochasticity in environment dynamics . In such highly deterministic environments we would expect FiGAR agents to build a latent model of the environment dynamics and hence be able to execute large action repetitions without dying . This is exactly what we see in a highly deterministic environment like the game \u201c Freeway \u201d . Figure 1 ( a ) demonstrates that the chicken is able to judge the speed of the approaching cars appropriately and cross the road in a manner which takes it to the goal without colliding with the cars and at the same time avoiding them narrowly . Having said that , certainly the ability to stop an action repetition ( or a macro-action ) in general would be very important , especially in stochastic environments . In our setup , we do not consider the ability to stop executing a macro-action that the agent has committed to . However , this is a necessary skill in the event of unexpected changes in the environment while executing a chosen macro-action . Thus , stop and start actions for stopping and committing to macro-actions can be added to the basic dynamic time scale setup for more robust policies . We believe the modification could work for more general stochastic worlds like Minecraft and leave it for future work . We will also add this discussion to the conclusion section to reflect possible shortcomings of FiGAR . [ 1 ] - Asynchronous Method for Deep Reinforcement Learning , Mnih et al , ICML 2016 [ 2 ] - Unifying Count-Based Exploration and Intrinsic Motivation , Bellemare et al , NIPS 2016 -- Sahil & Aravind"}, {"review_id": "B1GOWV5eg-2", "review_text": "This paper proposes a simple but effective extension to reinforcement learning algorithms, by adding a temporal repetition component as part of the action space, enabling the policy to select how long to repeat the chosen action for. The extension applies to all reinforcement learning algorithms, including both discrete and continuous domains, as it is primarily changing the action parametrization. The paper is well-written, and the experiments extensively evaluate the approach with 3 different RL algorithms in 3 different domains (Atari, MuJoCo, and TORCS). Here are some comments and questions, for improving the paper: The introduction states that \"all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k\". This statement is too strong, and is actually disproved in the experiments \u2014 repeating an action is helpful in many tasks, but not in all tasks. The sentence should be rephrased to be more precise. In the related work, a discussion of the relation to semi-MDPs would be useful to help the reader better understand the approach and how it compares and differs (e.g. the response from the pre-review questions) Experiments: Can you provide error bars on the experimental results? (from running multiple random seeds) It would be useful to see experiments with parameter sharing in the TRPO experiments, to be more consistent with the other domains, especially since it seems that the improvement in the TRPO experiments is smaller than that of the other two domains. Right now, it is hard to tell if the smaller improvement is because of the nature of the task, because of the lack of parameter sharing, or something else. The TRPO evaluation is different from the results reported in Duan et al. ICML \u201916. Why not use the same benchmark? Videos only show the policies learned with FiGAR, which are uninformative without also seeing the policies learned without FiGAR. Can you also include videos of the policies learned without FiGAR, as a comparison point? How many laps does DDPG complete without FiGAR? The difference in reward achieved seems quite substantial (557K vs. 59K). Can the tables be visualized as histograms? This seems like it would more effectively and efficiently communicate the results. Minor comments: -- On the plot in Figure 2, the label for the first bar should be changed from 1000 to 3500. -- \u201cidea of deciding when necessary\u201d - seems like it would be better to say \u201cidea of only deciding when necessary\" -- \"spaces.Durugkar et al.\u201d \u2014 missing a space. -- \u201cR={4}\u201d \u2014 why 4? Could you use a letter to indicate a constant instead? (or a different notation) ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for reviewing the paper , the comments and questions ! We believe addressing these questions will increase the quality of the work , and we will certainly do that . -The introduction states that `` all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k '' . This statement is too strong , and is actually disproved in the experiments \u2014 repeating an action is helpful in many tasks , but not in all tasks . The sentence should be rephrased to be more precise . We agree , the statement in its current form is incorrect . We will change it to \u201c many DRL algorithms execute a chosen action for fixed number of time steps k \u201d in the next revision . -In the related work , a discussion of the relation to semi-MDPs would be useful to help the reader better understand the approach and how it compares and differs ( e.g.the response from the pre-review questions ) Definitely . We will add this to the next revision . -Experiments : -Can you provide error bars on the experimental results ? ( from running multiple random seeds ) The current set of experiments took us nearly 3 months to run . Running them for a significantly large number of random seeds ( say 3 or 5 ) would be very difficult due to the limited nature of compute resources available to us . -It would be useful to see experiments with parameter sharing in the TRPO experiments , to be more consistent with the other domains , especially since it seems that the improvement in the TRPO experiments is smaller than that of the other two domains . Right now , it is hard to tell if the smaller improvement is because of the nature of the task , because of the lack of parameter sharing , or something else . We agree.It might take us some time to add those results since we do not have access to the compute resources right now . We will definitely try to add these to the final version . -The TRPO evaluation is different from the results reported in Duan et al.ICML \u2019 16.Why not use the same benchmark ? The evaluation procedure we have used is very similar to that used by Duan et al.ICML \u2018 16.The only difference is that instead of reporting average performance on training trajectories ( the number of these trajectories used varies across training epochs ) , we report performance on a testing epoch consisting of a fixed number of trajectories , which has been inserted between every two consecutive training epochs . Note that this is to be consistent with the notion of \u201c solving a task \u201d as introduced by openai.com . We test for 100 episodes between every 2 training epochs . -Videos only show the policies learned with FiGAR , which are uninformative without also seeing the policies learned without FiGAR . Can you also include videos of the policies learned without FiGAR , as a comparison point ? We have already included the videos for baseline as well . Probably youtube \u2019 s default player did not suggest the correct order for the videos . We will make sure that the next revision has a link to a playlist which contains all the videos . We will additionally also add videos of Atari gameplay in the final version of the paper . -How many laps does DDPG complete without FiGAR ? The difference in reward achieved seems quite substantial ( 557K vs. 59K ) . DDPG completes 2 laps without FiGAR . The complete task consists of 20 laps . -Can the tables be visualized as histograms ? This seems like it would more effectively and efficiently communicate the results . Definitely . We will add the histograms in the main paper and shift the tables to the appendix in the final version . -- Sahil & Aravind"}], "0": {"review_id": "B1GOWV5eg-0", "review_text": "This paper shows that extending deep RL algorithms to decide which action to take as well as how many times to repeat it leads to improved performance on a number of domains. The evaluation is very thorough and shows that this simple idea works well in both discrete and continuous actions spaces. A few comments/questions: - Table 1 could be easier to interpret as a figure of histograms. - Figure 3 could be easier to interpret as a table. - How was the subset of Atari games selected? - The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration (e.g. Freeway and Seaquest), but it would be nice to see a full evaluation on 57 games. This has become quite standard and would make it possible to compare overall performance using mean and median scores. - It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al., which aims to solve some of the same problems as FiGAR. - FiGAR currently discards frames between action decisions. There might be a tradeoff between repeating an action more times and throwing away more information. Have you thought about separating these effects? You could train a model that does process intermediate frames. Just a thought. Overall, this is a nice simple addition to deep RL algorithms that many people will probably start using. -------------------- I'm increasing my score to 8 based on the rebuttal and the revised paper.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for reviewing the paper , the comments and questions ! We believe addressing these questions will increase the quality of the work , and we will definitely do that . - Table 1 could be easier to interpret as a figure of histograms . Thanks for pointing this out . We will definitely add a histogram to the final version of the paper corresponding to Table 1 . - Figure 3 could be easier to interpret as a table . We added a histogram version of this data ( Figure3 ) because we wanted to illustrate that regardless of the action repetition set chosen , the rough `` magnitude '' of improvement is still the same . That all FiGAR variants continue to significantly outperform the baseline in the chosen games . We will definitely add a corresponding table of raw data in the appendix so that it can be looked up . - How was the subset of Atari games selected ? The subset was chosen arbitrarily . - The Atari evaluation does show convincing improvements over A3C on games requiring extended exploration ( e.g.Freeway and Seaquest ) , but it would be nice to see a full evaluation on 57 games . This has become quite standard and would make it possible to compare overall performance using mean and median scores . The results we have reported on 31 games was possible only after 3 months of computing . It might be difficult to report numbers on all 57 games , however we will definitely try to make the number of games on which we report results as large as possible ( We already have results on 2 more games that we will add to the final version of the paper ) . - It would also be nice to see a more direct comparison to the STRAW model of Vezhnevets et al. , which aims to solve some of the same problems as FiGAR . STRAW was run on a very small subset of games , namely 8 Atari games . The intersection of games on which both our work and STRAW report results is even smaller at 5 games . Such a comparison is likely to be skewed . Having said that , we could definitely add scores reported by STRAW on the 5 games that we have evaluated on to Table 4 . - FiGAR currently discards frames between action decisions . There might be a tradeoff between repeating an action more times and throwing away more information . Have you thought about separating these effects ? You could train a model that does process intermediate frames . Just a thought . As pointed out in one of the comments : `` After learning is complete , did you try forward propagating through the network to find actions for every time-step as opposed to repeating actions ? Concretely , if at t=5 , action suggested by the network is a_3 with a repetition of 4 , instead of sticking with a_3 for times t= { 5,6,7,8 } perform action a_3 for just t=5 , and forward prop through the policy again at t=6 . '' This is definitely an experiment worth trying out and we intend to do that and include results if they turn out to be significant . Having said that , this only makes use of the discarded frames in the testing phase , not in the training phase . A possible way to trade-off between discarding frames and action repeats is to construct a separate , second network ( consisting of a convnet followed by an LSTM ) which processes every kth frame , much like A3C , and concatenate representations learnt by this network to those learnt by the usual A3C network , while making decisions on action selection . The reason one would like to do this is because as you rightly pointed out , skipped frames might also contain crucial information needed for finding out the optimal action as well as action repetition in the next action decision step . We will definitely explore this direction of work as future research . Thanks for the idea !"}, "1": {"review_id": "B1GOWV5eg-1", "review_text": "This paper provides a simple method to handle action repetitions. They make the action a tuple (a,x), where a is the action chosen, and x the number of repetitions. Overall they report some improvements over A3C/DDPG, dramatic in some games, moderate in other. The idea seems natural and there is a wealth of experiment to support it. Comments: - The scores reported on A3C in this paper and in the Mnih et al. publication (table S3) differ significantly. Where does this discrepancy come from? If it's from a different training regime (fewer iterations, for instance), did the authors confirm that running their replication to the same settings as Mnih et al provide similar results? - It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate. This seems to imply that for those, the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 (and therefore has 4 times fewer gradient updates). A3C could be run for a comparable computation cost with a lower action repeat, which would probably result in increased performance of A3C. Nevertheless, the automatic determination of the appropriate action repeat is interesting, even if the overall message seems to be to not repeat actions too often. - Slightly problematic notation, where r sometimes denotes rewards, sometimes denotes elements of the repetition set R (top of page 5) - In the equation at the bottom of page 5 - since the sum is not indexed over decision steps, not time steps, shouldn't the rewards r_k be modified to be the sum of rewards (appropriately discounted) between those time steps? - The section on DDPG is confusingly written. \"Concatenating\" loss is a strange operation; doesn't FiGAR correspond to a loss to roughly looks like Q(x,mu(x)) + R log p(x) (with separate loss for learning the critic)? It feels that REINFORCE should be applied for the repetition variable x (second term of the sum) and reparametrization for the action a (first term)? - Is the 'name_this_game' name in the tables intentional? - A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps, independently of what happens next. Have the authors considered a scheme in which, at each time step, the agent decides to stick with the current decision or not? (It feels like it might be a relatively simple modification of FiGAR).", "rating": "7: Good paper, accept", "reply_text": "Thanks for reviewing the paper , the comments and questions ! We believe addressing these questions will increase the quality of the work , and we will definitely do that . - The scores reported on A3C in this paper and in the Mnih et al.publication ( table S3 ) differ significantly . Where does this discrepancy come from ? If it 's from a different training regime ( fewer iterations , for instance ) , did the authors confirm that running their replication to the same settings as Mnih et al provide similar results ? The reason why the scores differ significantly is because of 3 reasons : 1 . Mnih et al.publication [ 1 ] reports average scores on best 5 replicas out of 50 replicas that they started with . We did not mimic this setup because we do not possess the compute resources to run 50 different replicas for each game . 2.The evaluation method used was very different . They used human starts evaluation metric . However , in the absence of the same human trajectories it would be very difficult to ensure a fair or repeatable evaluation setup . 3.In fact we have found that the scores in general and evaluation setup in specific reported by Mnih et al . [ 1 ] are difficult to reproduce not only for us , but also researchers at Deepmind . In Unifying Count-Based Exploration and Intrinsic Motivation [ 2 ] the scores reported for A3C differ very drastically from those reported by the original A3C publication [ 1 ] , even though the same evaluation metric ( human starts ) was followed , and hopefully the same set of human start trajectories was used ( we do not know this for sure ) . The scores for many games are orders of magnitude lower for A3C in [ 2 ] . In conclusion we \u2019 d like to say that the training as well as testing setup of [ 1 ] are difficult to reproduce , which in turn makes it difficult to replicate the scores . - It is intriguing that the best results of FiGAR are reported on games where few actions repeat dominate . This seems to imply that for those , the performance overhead of FiGAR over A3C is high since A3C uses an action repeat of 4 ( and therefore has 4 times fewer gradient updates ) . A3C could be run for a comparable computation cost with a lower action repeat , which would probably result in increased performance of A3C . Nevertheless , the automatic determination of the appropriate action repeat is interesting , even if the overall message seems to be to not repeat actions too often . It is true that for many games the lower action repetitions dominate in the sense that they are chosen for a large fraction of time . However , the average action repetition ( ARR ) is a fairer metric to compare the computation cost since FiGAR still makes up by choosing large action repetition at other points in time . Table 5,6,7 in Appendix B seek to demonstrate the action repetition distribution and the ARR for all the games . It can be seen that for 28 out of 31 games , the average action repetition for FiGAR is greater than 4 ( which is the ARR for A3C ) . Concretely for the best 4 games by gameplay performance , the average action repetitions are ( numbers taken from Table 7 , page 18 , Appendix B ) : Atlantis : 7.2 Seaquest : 5.33 Asterix : 4.22 Wizard of wor : 9.87 - Slightly problematic notation , where r sometimes denotes rewards , sometimes denotes elements of the repetition set R ( top of page 5 ) Thanks for pointing this out . We will change this in the next revision . - In the equation at the bottom of page 5 - since the sum is indexed over decision steps , not time steps , should n't the rewards r_k be modified to be the sum of rewards ( appropriately discounted ) between those time steps ? Thanks for pointing this out . The question is how should the reward for a macro action m = ( a , x ) be constructed . Should it be the discounted sum of intermediate rewards encountered during the execution of m or should it be the cumulative undiscounted sum of rewards ? We went with the second formalism since we did not want to penalize the agent for choosing larger action repetitions . This we believe would encourage the agent to pick larger action repetitions . - The section on DDPG is confusingly written . `` Concatenating '' loss is a strange operation ; does n't FiGAR correspond to a loss to roughly looks like Q ( x , mu ( x ) ) + R log p ( x ) ( with separate loss for learning the critic ) ? It feels that REINFORCE should be applied for the repetition variable x ( second term of the sum ) and reparametrization for the action a ( first term ) ? Sorry for this . In DDPG , there is only a single loss function , the critic loss function . There is no loss function for the actor . The actor simply receives gradients from the critic . This is because the actor \u2019 s proposed policy is directly fed to the critic and the critic provides the actor with gradients which the proposed policy follows for improvement . Hence , the actor does not really have a loss function per se , but only gradients provided by the critic . In FiGAR the total policy \\pi is a concatenation of vectors \\pi_ { a } and \\pi_ { x } . Hence the gradients for the total policy are also simply the concatenation of the gradients for the policies \\pi_ { a } and \\pi_ { x } . This is what we meant by the concatenation operator . We will make the section clearer in the next revision . - Is the 'name_this_game ' name in the tables intentional ? It is.This is the name of a game in the Atari 2600 domain . Here is a video which shows gameplay in this game : https : //www.youtube.com/watch ? v=7obD1q85_kw . Note that this video is in no way related to FiGAR and only demonstrates general gameplay in this game . - A potential weakness of the method is that the agent must decide to commit to an action for a fixed number of steps , independently of what happens next . Have the authors considered a scheme in which , at each time step , the agent decides to stick with the current decision or not ? ( It feels like it might be a relatively simple modification of FiGAR ) . We agree with the reviewer and explain the need for and a possible solution for stopping macro-actions . Atari , TORCS and MuJoCo represent environments which are largely deterministic with a minimal degree of stochasticity in environment dynamics . In such highly deterministic environments we would expect FiGAR agents to build a latent model of the environment dynamics and hence be able to execute large action repetitions without dying . This is exactly what we see in a highly deterministic environment like the game \u201c Freeway \u201d . Figure 1 ( a ) demonstrates that the chicken is able to judge the speed of the approaching cars appropriately and cross the road in a manner which takes it to the goal without colliding with the cars and at the same time avoiding them narrowly . Having said that , certainly the ability to stop an action repetition ( or a macro-action ) in general would be very important , especially in stochastic environments . In our setup , we do not consider the ability to stop executing a macro-action that the agent has committed to . However , this is a necessary skill in the event of unexpected changes in the environment while executing a chosen macro-action . Thus , stop and start actions for stopping and committing to macro-actions can be added to the basic dynamic time scale setup for more robust policies . We believe the modification could work for more general stochastic worlds like Minecraft and leave it for future work . We will also add this discussion to the conclusion section to reflect possible shortcomings of FiGAR . [ 1 ] - Asynchronous Method for Deep Reinforcement Learning , Mnih et al , ICML 2016 [ 2 ] - Unifying Count-Based Exploration and Intrinsic Motivation , Bellemare et al , NIPS 2016 -- Sahil & Aravind"}, "2": {"review_id": "B1GOWV5eg-2", "review_text": "This paper proposes a simple but effective extension to reinforcement learning algorithms, by adding a temporal repetition component as part of the action space, enabling the policy to select how long to repeat the chosen action for. The extension applies to all reinforcement learning algorithms, including both discrete and continuous domains, as it is primarily changing the action parametrization. The paper is well-written, and the experiments extensively evaluate the approach with 3 different RL algorithms in 3 different domains (Atari, MuJoCo, and TORCS). Here are some comments and questions, for improving the paper: The introduction states that \"all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k\". This statement is too strong, and is actually disproved in the experiments \u2014 repeating an action is helpful in many tasks, but not in all tasks. The sentence should be rephrased to be more precise. In the related work, a discussion of the relation to semi-MDPs would be useful to help the reader better understand the approach and how it compares and differs (e.g. the response from the pre-review questions) Experiments: Can you provide error bars on the experimental results? (from running multiple random seeds) It would be useful to see experiments with parameter sharing in the TRPO experiments, to be more consistent with the other domains, especially since it seems that the improvement in the TRPO experiments is smaller than that of the other two domains. Right now, it is hard to tell if the smaller improvement is because of the nature of the task, because of the lack of parameter sharing, or something else. The TRPO evaluation is different from the results reported in Duan et al. ICML \u201916. Why not use the same benchmark? Videos only show the policies learned with FiGAR, which are uninformative without also seeing the policies learned without FiGAR. Can you also include videos of the policies learned without FiGAR, as a comparison point? How many laps does DDPG complete without FiGAR? The difference in reward achieved seems quite substantial (557K vs. 59K). Can the tables be visualized as histograms? This seems like it would more effectively and efficiently communicate the results. Minor comments: -- On the plot in Figure 2, the label for the first bar should be changed from 1000 to 3500. -- \u201cidea of deciding when necessary\u201d - seems like it would be better to say \u201cidea of only deciding when necessary\" -- \"spaces.Durugkar et al.\u201d \u2014 missing a space. -- \u201cR={4}\u201d \u2014 why 4? Could you use a letter to indicate a constant instead? (or a different notation) ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for reviewing the paper , the comments and questions ! We believe addressing these questions will increase the quality of the work , and we will certainly do that . -The introduction states that `` all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k '' . This statement is too strong , and is actually disproved in the experiments \u2014 repeating an action is helpful in many tasks , but not in all tasks . The sentence should be rephrased to be more precise . We agree , the statement in its current form is incorrect . We will change it to \u201c many DRL algorithms execute a chosen action for fixed number of time steps k \u201d in the next revision . -In the related work , a discussion of the relation to semi-MDPs would be useful to help the reader better understand the approach and how it compares and differs ( e.g.the response from the pre-review questions ) Definitely . We will add this to the next revision . -Experiments : -Can you provide error bars on the experimental results ? ( from running multiple random seeds ) The current set of experiments took us nearly 3 months to run . Running them for a significantly large number of random seeds ( say 3 or 5 ) would be very difficult due to the limited nature of compute resources available to us . -It would be useful to see experiments with parameter sharing in the TRPO experiments , to be more consistent with the other domains , especially since it seems that the improvement in the TRPO experiments is smaller than that of the other two domains . Right now , it is hard to tell if the smaller improvement is because of the nature of the task , because of the lack of parameter sharing , or something else . We agree.It might take us some time to add those results since we do not have access to the compute resources right now . We will definitely try to add these to the final version . -The TRPO evaluation is different from the results reported in Duan et al.ICML \u2019 16.Why not use the same benchmark ? The evaluation procedure we have used is very similar to that used by Duan et al.ICML \u2018 16.The only difference is that instead of reporting average performance on training trajectories ( the number of these trajectories used varies across training epochs ) , we report performance on a testing epoch consisting of a fixed number of trajectories , which has been inserted between every two consecutive training epochs . Note that this is to be consistent with the notion of \u201c solving a task \u201d as introduced by openai.com . We test for 100 episodes between every 2 training epochs . -Videos only show the policies learned with FiGAR , which are uninformative without also seeing the policies learned without FiGAR . Can you also include videos of the policies learned without FiGAR , as a comparison point ? We have already included the videos for baseline as well . Probably youtube \u2019 s default player did not suggest the correct order for the videos . We will make sure that the next revision has a link to a playlist which contains all the videos . We will additionally also add videos of Atari gameplay in the final version of the paper . -How many laps does DDPG complete without FiGAR ? The difference in reward achieved seems quite substantial ( 557K vs. 59K ) . DDPG completes 2 laps without FiGAR . The complete task consists of 20 laps . -Can the tables be visualized as histograms ? This seems like it would more effectively and efficiently communicate the results . Definitely . We will add the histograms in the main paper and shift the tables to the appendix in the final version . -- Sahil & Aravind"}}