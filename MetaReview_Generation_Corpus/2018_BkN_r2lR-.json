{"year": "2018", "forum": "BkN_r2lR-", "title": "Identifying Analogies Across Domains", "decision": "Accept (Poster)", "meta_review": "This paper builds on top of Cycle GAN ideas where the main idea is to jointly optimize the domain-level translation function with an instance-level matching objective. Initially the paper received two negative reviews (4,5) and a positive (7). After the rebuttal and several back and forth between the first reviewer and the authors, the reviewer was finally swayed by the new experiments. While not officially changing the score, the reviewer recommended acceptance. The AC agrees that the paper is interesting and of value to the ICLR audience.", "reviews": [{"review_id": "BkN_r2lR--0", "review_text": "This paper presents an image-to-image cross domain translation framework based on generative adversarial networks. The contribution is the addition of an explicit exemplar constraint into the formulation which allows best matches from the other domain to be retrieved. The results show that the proposed method is superior for the task of exact correspondence identification and that AN-GAN rivals the performance of pix2pix with strong supervision. Negatives: 1.) The task of exact correspondence identification seems contrived. It is not clear which real-world problems have this property of having both all inputs and all outputs in the dataset, with just the correspondence information between inputs and outputs missing. 2.) The supervised vs unsupervised experiment on Facades->Labels (Table 3) is only one scenario where applying a supervised method on top of AN-GAN\u2019s matches is better than an unsupervised method. More transfer experiments of this kind would greatly benefit the paper and support the conclusion that \u201cour self-supervised method performs similarly to the fully supervised method.\u201d Positives: 1.) The paper does a good job motivating the need for an explicit image matching term inside a GAN framework 2.) The paper shows promising results on applying a supervised method on top of AN-GAN\u2019s matches. Minor comments: 1. The paper sometimes uses L1 and sometimes L_1, it should be L_1 in all cases. 2. DiscoGAN should have the Kim et al citation, right after the first time it is used. I had to look up DiscoGAN to realize it is just Kim et al.", "rating": "7: Good paper, accept", "reply_text": "We thank you for highlighting the novelty and successful motivation of the exemplar-based matching loss . We think that the exact-analogy problem is very important . Please refer to our comment to AnonReviewer2 for an extensive discussion . Following your request , we have added AN-GAN supervised experiments for the edges2shoes and edges2handbags datasets . The results as for the Facades case are very good . Thank you for highlighting the inconsistency in L_1 notation and the confusing reference . This has been fixed in the revised version ."}, {"review_id": "BkN_r2lR--1", "review_text": "The paper presents a method for finding related images (analogies) from different domains based on matching-by-synthesis. The general idea is interesting and the results show improvements over previous approaches, such as CycleGAN (with different initializations, pre-learned or not). The algorithm is tested on three datasets. While the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts), the paper lacks clarity and sufficient details. Instead of the longer intro and related work discussion, I would prefer to see a Figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments. Also, the matching part, which is discussed at the theoretical level, could be better explained and presented at a more visual level. It is hard to understand sufficiently well what the formalism means without more insight. Also, the experiments need more details. For example, it is not clear what the numbers in Table 2 mean. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your positive feedback on the theoretical and experimental merits of this paper . Following your feedback on the clarity of presentation of the method . we included a diagram ( including example images ) illustrating the algorithm . To help keep the length under control , we shortened the introduction and related work section as you suggested . We further clarified the text of the experiments . Specifically the numbers in Tab 2 are the top-1 accuracy for both directions ( A to B and B to A ) when 0 % , 10 % and 25 % of examples do not have matches in the other domain . If some details remain unclear , we would be glad to clarify them . We hope that your positive opinion of the content of the paper with the improvement in clarity of presentation will merit an acceptance ."}, {"review_id": "BkN_r2lR--2", "review_text": "This paper adds an interesting twist on top of recent unpaired image translation work. A domain-level translation function is jointly optimized with an instance-level matching objective. This yields the ability to extract corresponding image pairs out of two unpaired datasets, and also to potentially refine unpaired translation by subsequently training a paired translation function on the discovered matches. I think this is a promising direction, but the current paper has unconvincing results, and it\u2019s not clear if the method is really solving an important problem yet. My main criticism is with the experiments and results. The experiments focus almost entirely on the setting where there actually exist exact matches between the two image sets. Even the partial matching experiments in Section 4.1.2 only quantify performance on the images that have exact matches. This is a major limitation since the compelling use cases of the method are in scenarios where we do not have exact matches. It feels rather contrived to focus so much on the datasets with exact matches since, 1) these datasets actually come as paired data and, in actual practice, supervised translation can be run directly, 2) it\u2019s hard to imagine datasets that have exact but unknown matches (I welcome the authors to put forward some such scenarios), 3) when exact matches exist, simpler methods may be sufficient, such as matching edges. There is no comparison to any such simple baselines. I think finding analogies that are not exact matches is much more compelling. Quantifying performance in this case may be hard, and the current paper only offers a few qualitative results. I\u2019d like to see far more results, and some attempt at a metric. One option would be to run user studies where humans judge the quality of the matches. The results shown in Figure 2 don\u2019t convince me, not just because they are qualitative and few, but also because I\u2019m not sure I even agree that the proposed method is producing better results: for example, the DiscoGAN results have some artifacts but capture the texture better in row 3. I was also not convinced by the supervised second step in Section 4.3. Given that the first step achieves 97% alignment accuracy, it\u2019s no surprised that running an off-the-shelf supervised method on top of this will match the performance of running on 100% correct data. In other words, this section does not really add much new information beyond what we could already infer given that the first stage alignment was so successful. What I think would be really interesting is if the method can improve performance on datasets that actually do not have ground truth exact matches. For example, the shoes and handbags dataset or even better, domain adaptation datasets like sim to real. I\u2019d like to see more discussion of why the second stage supervised problem is beneficial. Would it not be sufficient to iterate alpha and T iterations enough times until alpha is one-hot and T is simply training against a supervised objective (Equation 7)? Minor comments: 1. In the intro, it would be useful to have a clear definition of \u201canalogy\u201d for the present context. 2. Page 2: a link should be provided for the Putin example, as it is not actually in Zhu et al. 2017. 3. Page 3: \u201cWeakly Supervised Mapping\u201d \u2014 I wouldn\u2019t call this weakly supervised. Rather, I\u2019d say it\u2019s just another constraint / prior, similar to cycle-consistency, which was referred to under the \u201cUnsupervised\u201d section. 4. Page 4 and throughout: It\u2019s hard to follow which variables are being optimized over when. For example, in Eqn. 7, it would be clearer to write out the min over optimization variables. 5. Page 6: The Maps dataset was introduced in Isola et al. 2017, not Zhu et al. 2017. 6. Page 7: The following sentence is confusing and should be clarified: \u201cThis shows that the distribution matching is able to map source images that are semantically similar in the target domain.\u201d 7. Page 7: \u201cThis shows that a good initialization is important for this task.\u201d \u2014 Isn\u2019t this more than initialization? Rather, removing the distributional and cycle constraints changes the overall objective being optimized. 8. In Figure 2, are the outputs the matched training images, or are they outputs of the translation function? 9. Throughout the paper, some citations are missing enclosing parentheses.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the extensive style and reference comments . They have been fixed in the revised version : 1 . A definition of \u201c analogy \u201d for the present context added to intro . 2.Putin example removed for need of space . 3. \u201c Weakly Supervised Mapping \u201d previous work section removed and references merged for need of space . 4.Optimization variables have been explicitly added to equations . 5.Maps dataset citation was changed to Isola et al.2017 6.Removed confusing comment : \u201c This shows that the distribution matching is able to map source images that are semantically similar in the target domain. \u201d 7 . \u201c This shows that a good initialization is important for this task. \u201d : one way of looking at it , is that the exemplar loss optimizes the matching problem that we care about but is a hard optimization task . The two other losses are auxiliary losses that help optimization converge . Clarification added in text . 8.The results shown for inexact matching are as follows : For alpha iterations and ANGAN we show the matches recovered by our methods , The DiscoGAN results are the outputs of the translation function . 9.Parentheses added to all citations . We hope that this has convinced the reviewer of the importance of this work and are keen to answer any further questions ."}], "0": {"review_id": "BkN_r2lR--0", "review_text": "This paper presents an image-to-image cross domain translation framework based on generative adversarial networks. The contribution is the addition of an explicit exemplar constraint into the formulation which allows best matches from the other domain to be retrieved. The results show that the proposed method is superior for the task of exact correspondence identification and that AN-GAN rivals the performance of pix2pix with strong supervision. Negatives: 1.) The task of exact correspondence identification seems contrived. It is not clear which real-world problems have this property of having both all inputs and all outputs in the dataset, with just the correspondence information between inputs and outputs missing. 2.) The supervised vs unsupervised experiment on Facades->Labels (Table 3) is only one scenario where applying a supervised method on top of AN-GAN\u2019s matches is better than an unsupervised method. More transfer experiments of this kind would greatly benefit the paper and support the conclusion that \u201cour self-supervised method performs similarly to the fully supervised method.\u201d Positives: 1.) The paper does a good job motivating the need for an explicit image matching term inside a GAN framework 2.) The paper shows promising results on applying a supervised method on top of AN-GAN\u2019s matches. Minor comments: 1. The paper sometimes uses L1 and sometimes L_1, it should be L_1 in all cases. 2. DiscoGAN should have the Kim et al citation, right after the first time it is used. I had to look up DiscoGAN to realize it is just Kim et al.", "rating": "7: Good paper, accept", "reply_text": "We thank you for highlighting the novelty and successful motivation of the exemplar-based matching loss . We think that the exact-analogy problem is very important . Please refer to our comment to AnonReviewer2 for an extensive discussion . Following your request , we have added AN-GAN supervised experiments for the edges2shoes and edges2handbags datasets . The results as for the Facades case are very good . Thank you for highlighting the inconsistency in L_1 notation and the confusing reference . This has been fixed in the revised version ."}, "1": {"review_id": "BkN_r2lR--1", "review_text": "The paper presents a method for finding related images (analogies) from different domains based on matching-by-synthesis. The general idea is interesting and the results show improvements over previous approaches, such as CycleGAN (with different initializations, pre-learned or not). The algorithm is tested on three datasets. While the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts), the paper lacks clarity and sufficient details. Instead of the longer intro and related work discussion, I would prefer to see a Figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments. Also, the matching part, which is discussed at the theoretical level, could be better explained and presented at a more visual level. It is hard to understand sufficiently well what the formalism means without more insight. Also, the experiments need more details. For example, it is not clear what the numbers in Table 2 mean. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your positive feedback on the theoretical and experimental merits of this paper . Following your feedback on the clarity of presentation of the method . we included a diagram ( including example images ) illustrating the algorithm . To help keep the length under control , we shortened the introduction and related work section as you suggested . We further clarified the text of the experiments . Specifically the numbers in Tab 2 are the top-1 accuracy for both directions ( A to B and B to A ) when 0 % , 10 % and 25 % of examples do not have matches in the other domain . If some details remain unclear , we would be glad to clarify them . We hope that your positive opinion of the content of the paper with the improvement in clarity of presentation will merit an acceptance ."}, "2": {"review_id": "BkN_r2lR--2", "review_text": "This paper adds an interesting twist on top of recent unpaired image translation work. A domain-level translation function is jointly optimized with an instance-level matching objective. This yields the ability to extract corresponding image pairs out of two unpaired datasets, and also to potentially refine unpaired translation by subsequently training a paired translation function on the discovered matches. I think this is a promising direction, but the current paper has unconvincing results, and it\u2019s not clear if the method is really solving an important problem yet. My main criticism is with the experiments and results. The experiments focus almost entirely on the setting where there actually exist exact matches between the two image sets. Even the partial matching experiments in Section 4.1.2 only quantify performance on the images that have exact matches. This is a major limitation since the compelling use cases of the method are in scenarios where we do not have exact matches. It feels rather contrived to focus so much on the datasets with exact matches since, 1) these datasets actually come as paired data and, in actual practice, supervised translation can be run directly, 2) it\u2019s hard to imagine datasets that have exact but unknown matches (I welcome the authors to put forward some such scenarios), 3) when exact matches exist, simpler methods may be sufficient, such as matching edges. There is no comparison to any such simple baselines. I think finding analogies that are not exact matches is much more compelling. Quantifying performance in this case may be hard, and the current paper only offers a few qualitative results. I\u2019d like to see far more results, and some attempt at a metric. One option would be to run user studies where humans judge the quality of the matches. The results shown in Figure 2 don\u2019t convince me, not just because they are qualitative and few, but also because I\u2019m not sure I even agree that the proposed method is producing better results: for example, the DiscoGAN results have some artifacts but capture the texture better in row 3. I was also not convinced by the supervised second step in Section 4.3. Given that the first step achieves 97% alignment accuracy, it\u2019s no surprised that running an off-the-shelf supervised method on top of this will match the performance of running on 100% correct data. In other words, this section does not really add much new information beyond what we could already infer given that the first stage alignment was so successful. What I think would be really interesting is if the method can improve performance on datasets that actually do not have ground truth exact matches. For example, the shoes and handbags dataset or even better, domain adaptation datasets like sim to real. I\u2019d like to see more discussion of why the second stage supervised problem is beneficial. Would it not be sufficient to iterate alpha and T iterations enough times until alpha is one-hot and T is simply training against a supervised objective (Equation 7)? Minor comments: 1. In the intro, it would be useful to have a clear definition of \u201canalogy\u201d for the present context. 2. Page 2: a link should be provided for the Putin example, as it is not actually in Zhu et al. 2017. 3. Page 3: \u201cWeakly Supervised Mapping\u201d \u2014 I wouldn\u2019t call this weakly supervised. Rather, I\u2019d say it\u2019s just another constraint / prior, similar to cycle-consistency, which was referred to under the \u201cUnsupervised\u201d section. 4. Page 4 and throughout: It\u2019s hard to follow which variables are being optimized over when. For example, in Eqn. 7, it would be clearer to write out the min over optimization variables. 5. Page 6: The Maps dataset was introduced in Isola et al. 2017, not Zhu et al. 2017. 6. Page 7: The following sentence is confusing and should be clarified: \u201cThis shows that the distribution matching is able to map source images that are semantically similar in the target domain.\u201d 7. Page 7: \u201cThis shows that a good initialization is important for this task.\u201d \u2014 Isn\u2019t this more than initialization? Rather, removing the distributional and cycle constraints changes the overall objective being optimized. 8. In Figure 2, are the outputs the matched training images, or are they outputs of the translation function? 9. Throughout the paper, some citations are missing enclosing parentheses.", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for the extensive style and reference comments . They have been fixed in the revised version : 1 . A definition of \u201c analogy \u201d for the present context added to intro . 2.Putin example removed for need of space . 3. \u201c Weakly Supervised Mapping \u201d previous work section removed and references merged for need of space . 4.Optimization variables have been explicitly added to equations . 5.Maps dataset citation was changed to Isola et al.2017 6.Removed confusing comment : \u201c This shows that the distribution matching is able to map source images that are semantically similar in the target domain. \u201d 7 . \u201c This shows that a good initialization is important for this task. \u201d : one way of looking at it , is that the exemplar loss optimizes the matching problem that we care about but is a hard optimization task . The two other losses are auxiliary losses that help optimization converge . Clarification added in text . 8.The results shown for inexact matching are as follows : For alpha iterations and ANGAN we show the matches recovered by our methods , The DiscoGAN results are the outputs of the translation function . 9.Parentheses added to all citations . We hope that this has convinced the reviewer of the importance of this work and are keen to answer any further questions ."}}