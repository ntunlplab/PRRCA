{"year": "2019", "forum": "Hy4R2oRqKQ", "title": "Canonical Correlation Analysis with Implicit Distributions", "decision": "Reject", "meta_review": "This manuscript proposes an implicit generative modeling approach for the non-linear CCA problem. One contribution is the proposal of Conditional Mutual Information (CMI) as a criterion to capture nonlinear dependency, resulting in an objective that can be solved using implicit distributions. The work seems to be well motivated and of interest to the community.\n\nThe reviewers and AC opinions were mixed, and the rebuttal did not completely address the concerns. In particular, a reviewer pointed out an issue with a derivation in the paper, and the issue was not satisfactorily resolved by the authors. Some additional reading suggests that the misunderstanding may be partially due to incomplete notation and other issues with clarity of writing.", "reviews": [{"review_id": "Hy4R2oRqKQ-0", "review_text": "In this paper, the authors attempt to provide a perspective on CCA that is based on implicit distributions. The authors compare and discuss several variants on CCA that have been proposed over the years, ranging from Linear CCA to Deep CCA and autoencoder variants. In order to overcome the prior/likelihood distribution assumptions, the authors propose a CCA view that is based on learning implicit distributions, e.g, by using generative adversarial networks. The authors further motivate their work by comparing with (Bi-)VCCA, claiming that the underlying assumptions lead to inconsistent constraints (or idealistic). I think the work has merit, and I like the motivation. Nevetheless, I think stronger experiments are required, as well as improvements in terms of clarity in the writing of the paper, and stronger support for the motivation. Figure 2 should be better explained in text. The MNIST experiment is useful, but using GANs usually results in sharper images than say VAE. Also, comparisons with (i) other models besides Bi-VCCA, and (ii) on other multi-view real-world data (besides the MNIST_LR) would be very useful in terms of communicating the true benefits of this model.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your constructive comments and suggestions for improvement of our work . We have made revisions in the adapted version for a better understanding of our work . 1 ) .Section 1 and section 3 are revised for clarity of our work ; 2 ) . Figure1 is further explained in the caption ; Furthermore , our primary objective for MNIST cross-view generation task is to show the multi-view consistency achieved by the proposed ACCA . This VAE generation is consistent with our formulation ( section 4.3 ) and structure design ( Figure2 ) of ACCA . Consequently , the quality of the generated images is not the major consideration in this part . However , we can easily adopt GANs , e. g. VAE- GANs , to improve the quality of the generated images of our model . We will address your suggestion about the experiments in the future version ."}, {"review_id": "Hy4R2oRqKQ-1", "review_text": "This paper proposes to improve deep variational canonical correlation analysis (VCCA, Bi-VCCA) by 1) applying adversarial autoencoders (Makhzani et al. ICLR 2016) to model the encoding from multiple data views (X, Y, XY) to the latent representation (Z); and 2) introducing q(z|x,y) to explicitly encode the joint distribution of two views X,Y. The proposed approach, called adversarial canonical correlation analysis (ACCA), is essentially the application of adversarial autoencoder to multiple data views. Experiments on benchmark datasets, including the MNIST left right halved dataset, MNIST noisy dataset, and Wisconsin X-ray microbeam database, show the proposed ACCA result in higher dependence (measured by the normalized HSIC) between two data views compared to Bi-VCCA. This paper is well motivated. Since adversarial autoencoder aims to improve based on VAE, it's natural to make use of adversarial autoencoder to improve the original VCCA. The advantage of ACCA is well supported by the experimental result. In ACCA_NoCV, does the author use a Gaussian prior? If so, could the author provide more intuition to explain why ACCA_NoCV would outperform Bi-VCCA, which 1) also use a Gaussian prior; and 2) also does not use the complementary view XY? Why would adversarial training improve the result? In ACCA, does the form of the prior distribution have to be specified in advance, such as Gaussian or the Gaussian mixture? Are the parameters of the prior learned during the training? When comparing the performance of different models, besides normalized HSIC, which is a quite recent approach, does the author compute the log-likelihood on the test set for Bi-VCCA and different variants of ACCA? Which model can achieve the highest test log-likelihood? According to equation (6), in principle, only q(z|x,y) is needed to approximate the true posterior distribution p(z|x,y). Did the author try to remove the first two terms in the right hand side of Equation (11), i.e., the expectation w.r.t. q_x(z) and q_y(z), and see how the model performance was affected? Does adversarial training introduce longer training time compared to the Bi-VCCA?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your constructive comments and suggestions for improvement of our work . Below , we answer your questions detailedly . A -- 1 ) .In ACCA_NoCV , we use Gaussian prior- the same prior as Bi-VCCA . Although it does not use the complementary view , it can outperform Bi-VCCA because the adversarial learning scheme can still provide a consistent marginalization for the two inference models , which alleviates the misaligned encoding problem in Bi-VCCA . Specifically , different from the KL divergence which matches the conditional distribution of z with individual points in each view , adversarial learning drives the approximation of the marginalized distributions of z for the two views ( as illustrated with Eq.10 and Eq.11 ) .As the variable in each view is marginalized out , this approximation is more robust and can achieve consistent encoding for the two views . As is presented with section 7.2 in the Appendix , Figure 5 obviously shows that $ z_ { y } $ of Bi-VCCA fails to show good clustering results . The comparison between that of $ z_ { x } $ indicates that Bi-VCCA suffers from a misalignment encoding for the incorporated two views . While for ACCA_NoCV in Figure 6 , the clustering result presents better alignment for the two views compared with that of Bi-VCCA , which indicates that the adopted adversarial learning scheme benefits the consistent encoding for the two views . A -- 2 ) .In the reported experiments , the prior distributions are all set to be Gaussian Mixture and the parameters of the prior are specified in advance . The intention of this setting is to initially verify that better performance would be obtained in generative CCA models when given the suitable prior distribution ( In table 4 , higher correlation is captured with non\u2010Gaussian prior ) . This verifies that ACCA achieves superiority for handling implicit distributions compared with VCCA . A -- 3 ) .For the same setting on the nHSIC computation , the average negative log-likelihood achieved with each encoding on the test set of Bi-VCCA , ACCA_NoCV , ACCA ( G ) , ACCA ( GM ) are 112.75 , 107.26 , 94.41 and 103.10 respectively . Consequently , regarding the log-likelihood , ACCA ( G ) achieves the best result , while Bi-VCCA achieves the worst . A -- 4 ) . $ q ( z|x ) $ and $ q ( z|y ) $ are the two principle encodings of CCA . Without them , analysis on the multi-views of CCA , such as correlation analysis and cross-view generation , can not be conducted . Actually , the model with the single encoding of $ q ( z|x , y ) $ can be adopted to learn a common embedding for multi-view data , the variant model is worth to be further studied for multi-view embedding task . A -- 5 ) .The introduction of adversarial training indeed increases the training time of ACCA ( 1503s vs 2806s ) , but the increase is tolerable considering its superiority on the result ."}, {"review_id": "Hy4R2oRqKQ-2", "review_text": "I don't quite see how the proposed approach addresses non-linear canonical correlation analysis. In particular: 1) The main motivation is the minimization of the conditional mutual information I(X;Y|Z), where X and Y correspond to the two views and Z is latent. First of all, what uncertainty does this expression has when X and Y are observations and Z is given? My understanding is that the main objective of any CCA problem should be to find some transformations, say f(X) and g(Y), with some (to be defined) desirable properties. For example, these would correspond to linear transformations, say Ax and By, for classical CCA. Therefore, should not one be interested in minimizing something like I(f(X);g(Y)|Z)? 2) Assuming that the minimization of the conditional mutual information I(X;Y|Z) would be the goal, I don't quite see why the formulation in equation (6) would actually be equivalent (or be some reasonable approximation)? 3) It is well known that differential entropy can be negative (e.g., Cover and Thomas, 2006). Why would the conditional mutual information in equation (4) be non-negative? Alternatively, what would negative values of I(X;Y|Z) mean in the CCA context? My understanding is that one should be interested in minimizing I(X;Y|Z), or its variants with transformations, in *absolute value* to ensure some closeness to conditional independence. 4) Expressions in equation (5)-(6) are general and hold with no assumptions whatsoever for any random variables X, Y, Z (given the expectations/integrals exist). It is therefore not clear what are the variables of this minimization problem? (parameters? but what is the parametric model?) 5) Assuming solving (6) is the goal, this formulation as mentioned by the authors is actually is quite a challenging problem involving latent variables. Some form of this approach explanation would I can not quite see how the proposed adversarial version would correct or supplement any of these questions. Other comments: 1) It would be appropriate to cite the probabilistic CCA paper by Bach and Jordan (2005); a better citation for classical CCA would be Hotelling (1936). 2) I find the multiple mentioning of the *non-linear* (in-)dependence confusing. Is this in statistical sense? And how exactly is this related to CCA? Does it have anything to do with the fact that the third and higher order cumulants are zero only for independent variables unless they are Gaussian? Moreover, does this linear independence have any connection with the non-linearity of the proposed CCA approach? 3) What exactly is the *linear correlation criterion* and how does it enter the classical CCA or PCCA formulation (Introduction; bullet point 2)? 4) It would be helpful to introduce the original CCA problem emphasizing that each view, X and Y, are *different* linear transformation of *the same* latent codes z. Moreover, the full description of the models (classical CCA/ PCCA) wouldn't take more than one-two paragraphs and would help the readers to avoid any misunderstanding. 5) Are any assumptions necessary to ensure existence? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the constructive comments and suggestions on our work . We answer your concerns on the proposed ACCA in the following aspects : 1 ) . Our proposed ICCA studies multi-view learning from the generative perspective . ( A1 ) 2 ) .The CMI is a theoretically sound criteria to satisfy the requirement for CCA , based on our intention to capture nonlinear dependency with implicit distributions . ( A1 , A3 ) 3 ) . The feasibility of ICCA and ACCA can be supported by representative works on Deep Generative Models , e.g.VAEs and AAEs . ( A2 , A3 , A4 , A5 ) Below , we answer each of your questions detailedly . A -- 1 ) .Your concerns on our metric can be answered with two key points of our motivation : ( 1 ) . We aim to interpret nonlinear CCA models from the generative perspective ; ( 2 ) . We aim to relax the explicit distribution assumptions on the data for these CCA modeling . From what we understand here , you interpret the main objective of CCA from the discriminative perspective , which is deviating from our motivations . From the generative perspective , the objective of CCA can be interpreted as learning a compact set of the shared latent variables z that represent a distribution over the observed two-view data x and y ( as depicted in Figure1 ) . For this generative model , the latent variable z is to be inferred , instead of \u201c given \u201d as in your understanding . Actually , $ I ( f ( X ) , g ( Y ) ) $ has been adopted as a metric to capture non-linear dependency ( CIA in Table 1 ) . However , explicit distributions are required for f ( X ) and g ( Y ) , which are intractable to be estimated in these complex expressive nonlinear CCA models . This restricts the model to capture nonlinear dependency ( Page 3 , Line 1-4 ) . Consequently , we present $ I ( X ; Y|Z ) $ as the metric , which achieves the following benefits simultaneously : 1 ) . It suits the generative interpretation for multi-view learning problem ; 2 ) . It can capture nonlinear dependency implicitly based on the proposed formulation . ( Notes : To be detailedly explained with Q2 and Q3 ) . Note that , although $ I ( X ; Y|Z ) $ is the criteria of the proposed ACCA model , the transformations , $ f ( X ) $ and $ G ( Y ) $ , are already implicitly implemented in our network ( Figure2 ) . Therefore , the proposed $ I ( X ; Y|Z ) $ can be a sound criteria for generative interpretation of nonlinear CCA models . A -- 2 ) .As we aim to conduct CCA with implicit distributions , following the derivation of ELBO , we derive a surrogate for the proposed $ I ( X ; Y|Z ) $ criteria , to eliminate the explicit distribution requirement ( Page 3 , Line 1-4 ) for its estimation . As described in Section3.3 , we prove that the optimization of Eq . ( 7 ) ( refer to Eq . ( 6 ) in the original version ) implicitly leads to $ min I ( X ; Y|Z ) $ , through the derivation shown in the appendix . The presented derivation can be supported by the ELBO derivation in the variational inference [ 1 ] . A -- 3 ) .Although differential entropy can be negative , the conditional mutual information is always nonnegative based on the Jensen 's inequality [ 2 ] . Consequently , $ I ( X ; Y|Z ) = 0 $ will be the optimal value of the generative CCA problem . Therefore , we do not need to consider the * absolute value * here . A -- 4 ) .As explained in Q2 , in our paper , Eq . ( 6 ) and Eq . ( 7 ) ( refer to Eq . ( 5 ) and Eq . ( 6 ) in the original version ) are presented to show that connection between the presented $ I ( X ; Y|Z ) $ criteria and deduced surrogate objective of the proposed ICCA . ( The proof is presented in the appendix ) Practical models are instantiated with different approximate inference methods within the ICCA framework . In these instantiations , the model parameters are what we learn . ( Eq.4 , Eq.8 and Eq.9 ) . A -- 5 ) .As stated in section 4.1 , our proposed problem is challenging in two aspects : 1 ) . we study CCA with implicit distributions ; 2 ) we intend to handle task which requires high precision of alignment . Existing methods fail in these two cases . As illustrated in [ 3 ] , adversarial training criterion can regularize the aggregated posterior distribution of the latent representation of the autoencoder to arbitrary prior distributions . This kind of approximate inference technique achieves two properties . ( 1 ) .It allows $ q ( z|x ) $ to act as a deterministic function of x , without explicit assumptions on the posterior distributions . ( 2 ) .As the technique drives the approximation of the aggregated posterior to the prior , it achieves a compact latent space in which samples generated from any part of the latent space would be meaningful . Consequently , in the proposed ACCA , we adopt the adversarial training criterion on the multi-view encodings and adopt a shared discriminator to drive the approximation of these encodings simultaneously . This design enables ACCA to be superior to VCCA in two aspects . ( 1 ) .ACCA can handle CCA problem with implicit distributions . ( 2 ) .As ACCA drives the approximation of the three aggregated posteriors to the prior distribution ( Eq.11 ) , it overcomes the misaligned encoding problem in Bi-VCCA ( Section 4.2 ) ."}], "0": {"review_id": "Hy4R2oRqKQ-0", "review_text": "In this paper, the authors attempt to provide a perspective on CCA that is based on implicit distributions. The authors compare and discuss several variants on CCA that have been proposed over the years, ranging from Linear CCA to Deep CCA and autoencoder variants. In order to overcome the prior/likelihood distribution assumptions, the authors propose a CCA view that is based on learning implicit distributions, e.g, by using generative adversarial networks. The authors further motivate their work by comparing with (Bi-)VCCA, claiming that the underlying assumptions lead to inconsistent constraints (or idealistic). I think the work has merit, and I like the motivation. Nevetheless, I think stronger experiments are required, as well as improvements in terms of clarity in the writing of the paper, and stronger support for the motivation. Figure 2 should be better explained in text. The MNIST experiment is useful, but using GANs usually results in sharper images than say VAE. Also, comparisons with (i) other models besides Bi-VCCA, and (ii) on other multi-view real-world data (besides the MNIST_LR) would be very useful in terms of communicating the true benefits of this model.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thanks for your constructive comments and suggestions for improvement of our work . We have made revisions in the adapted version for a better understanding of our work . 1 ) .Section 1 and section 3 are revised for clarity of our work ; 2 ) . Figure1 is further explained in the caption ; Furthermore , our primary objective for MNIST cross-view generation task is to show the multi-view consistency achieved by the proposed ACCA . This VAE generation is consistent with our formulation ( section 4.3 ) and structure design ( Figure2 ) of ACCA . Consequently , the quality of the generated images is not the major consideration in this part . However , we can easily adopt GANs , e. g. VAE- GANs , to improve the quality of the generated images of our model . We will address your suggestion about the experiments in the future version ."}, "1": {"review_id": "Hy4R2oRqKQ-1", "review_text": "This paper proposes to improve deep variational canonical correlation analysis (VCCA, Bi-VCCA) by 1) applying adversarial autoencoders (Makhzani et al. ICLR 2016) to model the encoding from multiple data views (X, Y, XY) to the latent representation (Z); and 2) introducing q(z|x,y) to explicitly encode the joint distribution of two views X,Y. The proposed approach, called adversarial canonical correlation analysis (ACCA), is essentially the application of adversarial autoencoder to multiple data views. Experiments on benchmark datasets, including the MNIST left right halved dataset, MNIST noisy dataset, and Wisconsin X-ray microbeam database, show the proposed ACCA result in higher dependence (measured by the normalized HSIC) between two data views compared to Bi-VCCA. This paper is well motivated. Since adversarial autoencoder aims to improve based on VAE, it's natural to make use of adversarial autoencoder to improve the original VCCA. The advantage of ACCA is well supported by the experimental result. In ACCA_NoCV, does the author use a Gaussian prior? If so, could the author provide more intuition to explain why ACCA_NoCV would outperform Bi-VCCA, which 1) also use a Gaussian prior; and 2) also does not use the complementary view XY? Why would adversarial training improve the result? In ACCA, does the form of the prior distribution have to be specified in advance, such as Gaussian or the Gaussian mixture? Are the parameters of the prior learned during the training? When comparing the performance of different models, besides normalized HSIC, which is a quite recent approach, does the author compute the log-likelihood on the test set for Bi-VCCA and different variants of ACCA? Which model can achieve the highest test log-likelihood? According to equation (6), in principle, only q(z|x,y) is needed to approximate the true posterior distribution p(z|x,y). Did the author try to remove the first two terms in the right hand side of Equation (11), i.e., the expectation w.r.t. q_x(z) and q_y(z), and see how the model performance was affected? Does adversarial training introduce longer training time compared to the Bi-VCCA?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your constructive comments and suggestions for improvement of our work . Below , we answer your questions detailedly . A -- 1 ) .In ACCA_NoCV , we use Gaussian prior- the same prior as Bi-VCCA . Although it does not use the complementary view , it can outperform Bi-VCCA because the adversarial learning scheme can still provide a consistent marginalization for the two inference models , which alleviates the misaligned encoding problem in Bi-VCCA . Specifically , different from the KL divergence which matches the conditional distribution of z with individual points in each view , adversarial learning drives the approximation of the marginalized distributions of z for the two views ( as illustrated with Eq.10 and Eq.11 ) .As the variable in each view is marginalized out , this approximation is more robust and can achieve consistent encoding for the two views . As is presented with section 7.2 in the Appendix , Figure 5 obviously shows that $ z_ { y } $ of Bi-VCCA fails to show good clustering results . The comparison between that of $ z_ { x } $ indicates that Bi-VCCA suffers from a misalignment encoding for the incorporated two views . While for ACCA_NoCV in Figure 6 , the clustering result presents better alignment for the two views compared with that of Bi-VCCA , which indicates that the adopted adversarial learning scheme benefits the consistent encoding for the two views . A -- 2 ) .In the reported experiments , the prior distributions are all set to be Gaussian Mixture and the parameters of the prior are specified in advance . The intention of this setting is to initially verify that better performance would be obtained in generative CCA models when given the suitable prior distribution ( In table 4 , higher correlation is captured with non\u2010Gaussian prior ) . This verifies that ACCA achieves superiority for handling implicit distributions compared with VCCA . A -- 3 ) .For the same setting on the nHSIC computation , the average negative log-likelihood achieved with each encoding on the test set of Bi-VCCA , ACCA_NoCV , ACCA ( G ) , ACCA ( GM ) are 112.75 , 107.26 , 94.41 and 103.10 respectively . Consequently , regarding the log-likelihood , ACCA ( G ) achieves the best result , while Bi-VCCA achieves the worst . A -- 4 ) . $ q ( z|x ) $ and $ q ( z|y ) $ are the two principle encodings of CCA . Without them , analysis on the multi-views of CCA , such as correlation analysis and cross-view generation , can not be conducted . Actually , the model with the single encoding of $ q ( z|x , y ) $ can be adopted to learn a common embedding for multi-view data , the variant model is worth to be further studied for multi-view embedding task . A -- 5 ) .The introduction of adversarial training indeed increases the training time of ACCA ( 1503s vs 2806s ) , but the increase is tolerable considering its superiority on the result ."}, "2": {"review_id": "Hy4R2oRqKQ-2", "review_text": "I don't quite see how the proposed approach addresses non-linear canonical correlation analysis. In particular: 1) The main motivation is the minimization of the conditional mutual information I(X;Y|Z), where X and Y correspond to the two views and Z is latent. First of all, what uncertainty does this expression has when X and Y are observations and Z is given? My understanding is that the main objective of any CCA problem should be to find some transformations, say f(X) and g(Y), with some (to be defined) desirable properties. For example, these would correspond to linear transformations, say Ax and By, for classical CCA. Therefore, should not one be interested in minimizing something like I(f(X);g(Y)|Z)? 2) Assuming that the minimization of the conditional mutual information I(X;Y|Z) would be the goal, I don't quite see why the formulation in equation (6) would actually be equivalent (or be some reasonable approximation)? 3) It is well known that differential entropy can be negative (e.g., Cover and Thomas, 2006). Why would the conditional mutual information in equation (4) be non-negative? Alternatively, what would negative values of I(X;Y|Z) mean in the CCA context? My understanding is that one should be interested in minimizing I(X;Y|Z), or its variants with transformations, in *absolute value* to ensure some closeness to conditional independence. 4) Expressions in equation (5)-(6) are general and hold with no assumptions whatsoever for any random variables X, Y, Z (given the expectations/integrals exist). It is therefore not clear what are the variables of this minimization problem? (parameters? but what is the parametric model?) 5) Assuming solving (6) is the goal, this formulation as mentioned by the authors is actually is quite a challenging problem involving latent variables. Some form of this approach explanation would I can not quite see how the proposed adversarial version would correct or supplement any of these questions. Other comments: 1) It would be appropriate to cite the probabilistic CCA paper by Bach and Jordan (2005); a better citation for classical CCA would be Hotelling (1936). 2) I find the multiple mentioning of the *non-linear* (in-)dependence confusing. Is this in statistical sense? And how exactly is this related to CCA? Does it have anything to do with the fact that the third and higher order cumulants are zero only for independent variables unless they are Gaussian? Moreover, does this linear independence have any connection with the non-linearity of the proposed CCA approach? 3) What exactly is the *linear correlation criterion* and how does it enter the classical CCA or PCCA formulation (Introduction; bullet point 2)? 4) It would be helpful to introduce the original CCA problem emphasizing that each view, X and Y, are *different* linear transformation of *the same* latent codes z. Moreover, the full description of the models (classical CCA/ PCCA) wouldn't take more than one-two paragraphs and would help the readers to avoid any misunderstanding. 5) Are any assumptions necessary to ensure existence? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for the constructive comments and suggestions on our work . We answer your concerns on the proposed ACCA in the following aspects : 1 ) . Our proposed ICCA studies multi-view learning from the generative perspective . ( A1 ) 2 ) .The CMI is a theoretically sound criteria to satisfy the requirement for CCA , based on our intention to capture nonlinear dependency with implicit distributions . ( A1 , A3 ) 3 ) . The feasibility of ICCA and ACCA can be supported by representative works on Deep Generative Models , e.g.VAEs and AAEs . ( A2 , A3 , A4 , A5 ) Below , we answer each of your questions detailedly . A -- 1 ) .Your concerns on our metric can be answered with two key points of our motivation : ( 1 ) . We aim to interpret nonlinear CCA models from the generative perspective ; ( 2 ) . We aim to relax the explicit distribution assumptions on the data for these CCA modeling . From what we understand here , you interpret the main objective of CCA from the discriminative perspective , which is deviating from our motivations . From the generative perspective , the objective of CCA can be interpreted as learning a compact set of the shared latent variables z that represent a distribution over the observed two-view data x and y ( as depicted in Figure1 ) . For this generative model , the latent variable z is to be inferred , instead of \u201c given \u201d as in your understanding . Actually , $ I ( f ( X ) , g ( Y ) ) $ has been adopted as a metric to capture non-linear dependency ( CIA in Table 1 ) . However , explicit distributions are required for f ( X ) and g ( Y ) , which are intractable to be estimated in these complex expressive nonlinear CCA models . This restricts the model to capture nonlinear dependency ( Page 3 , Line 1-4 ) . Consequently , we present $ I ( X ; Y|Z ) $ as the metric , which achieves the following benefits simultaneously : 1 ) . It suits the generative interpretation for multi-view learning problem ; 2 ) . It can capture nonlinear dependency implicitly based on the proposed formulation . ( Notes : To be detailedly explained with Q2 and Q3 ) . Note that , although $ I ( X ; Y|Z ) $ is the criteria of the proposed ACCA model , the transformations , $ f ( X ) $ and $ G ( Y ) $ , are already implicitly implemented in our network ( Figure2 ) . Therefore , the proposed $ I ( X ; Y|Z ) $ can be a sound criteria for generative interpretation of nonlinear CCA models . A -- 2 ) .As we aim to conduct CCA with implicit distributions , following the derivation of ELBO , we derive a surrogate for the proposed $ I ( X ; Y|Z ) $ criteria , to eliminate the explicit distribution requirement ( Page 3 , Line 1-4 ) for its estimation . As described in Section3.3 , we prove that the optimization of Eq . ( 7 ) ( refer to Eq . ( 6 ) in the original version ) implicitly leads to $ min I ( X ; Y|Z ) $ , through the derivation shown in the appendix . The presented derivation can be supported by the ELBO derivation in the variational inference [ 1 ] . A -- 3 ) .Although differential entropy can be negative , the conditional mutual information is always nonnegative based on the Jensen 's inequality [ 2 ] . Consequently , $ I ( X ; Y|Z ) = 0 $ will be the optimal value of the generative CCA problem . Therefore , we do not need to consider the * absolute value * here . A -- 4 ) .As explained in Q2 , in our paper , Eq . ( 6 ) and Eq . ( 7 ) ( refer to Eq . ( 5 ) and Eq . ( 6 ) in the original version ) are presented to show that connection between the presented $ I ( X ; Y|Z ) $ criteria and deduced surrogate objective of the proposed ICCA . ( The proof is presented in the appendix ) Practical models are instantiated with different approximate inference methods within the ICCA framework . In these instantiations , the model parameters are what we learn . ( Eq.4 , Eq.8 and Eq.9 ) . A -- 5 ) .As stated in section 4.1 , our proposed problem is challenging in two aspects : 1 ) . we study CCA with implicit distributions ; 2 ) we intend to handle task which requires high precision of alignment . Existing methods fail in these two cases . As illustrated in [ 3 ] , adversarial training criterion can regularize the aggregated posterior distribution of the latent representation of the autoencoder to arbitrary prior distributions . This kind of approximate inference technique achieves two properties . ( 1 ) .It allows $ q ( z|x ) $ to act as a deterministic function of x , without explicit assumptions on the posterior distributions . ( 2 ) .As the technique drives the approximation of the aggregated posterior to the prior , it achieves a compact latent space in which samples generated from any part of the latent space would be meaningful . Consequently , in the proposed ACCA , we adopt the adversarial training criterion on the multi-view encodings and adopt a shared discriminator to drive the approximation of these encodings simultaneously . This design enables ACCA to be superior to VCCA in two aspects . ( 1 ) .ACCA can handle CCA problem with implicit distributions . ( 2 ) .As ACCA drives the approximation of the three aggregated posteriors to the prior distribution ( Eq.11 ) , it overcomes the misaligned encoding problem in Bi-VCCA ( Section 4.2 ) ."}}