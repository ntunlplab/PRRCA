{"year": "2021", "forum": "LT0KSFnQDWF", "title": "Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting", "decision": "Reject", "meta_review": "We thank the authors for their detailed responses and the revised version, which addresses several of the questions raised by the reviewers.\n\nThe paper is correct and clearly written. All reviewers agree that the idea to add structural features in the message passing of graph neural networks is sensible. While different from previous work, the novelty is a bit incremental though, particularly given the previous work on colored graph neural network. The significance of the work is weak, given 1) the need to select \"by hand\" structural features that are passed as information, 2) the increased time complexity to compute the structural features compared to other GCNN, and 3) the experimental results that suggest that the benefit of the new approach is limited, particularly on challenging task.\n\nTo summarize, this is not a bad paper, but we consider it below the standard of ICLR in terms of originality and significance.", "reviews": [{"review_id": "LT0KSFnQDWF-0", "review_text": "This paper presents a natural extension of Message Passing Neural Net ( MPNN ) by incorporating structural features . These structural features are computed as the counts from different substructures ( like small lines , stars or complete graphs ) induced in the original graph . These counts are combined to obtain a new feature per node or per edge . Then these features are used in a standard MPNN . The authors then show that the resulting GNN is more expressive and they validate this claim experimentally . This idea is interesting and clearly explained in the paper but I think the paper could be greatly improved after addressing the following issues : 1- the authors should clarify their position with regards to invariance . Indeed , as explained shortly on page 3 when commenting Loukas ( 2020 ) , it is easy to make a GNN powerful if we remove the constraint to be invariant ( or equivariant ) . Hence , as I understand it , the authors are proposing an algorithm that is equivariant . If this is the case , it would be great to have a clear formal statement that GSN are equivariant and to give a mathematical proof . 2- the theoretical content of the paper should be improved : a- the first part of Proposition 3.1 is straightforward and I find the wording of the second statement unclear : what does ' ... or any not necessarily induced subgraph ... ' mean ? b- from a theoretical perspective , it seems that both GSN-v abd GSN-e have the same expressive power . Is it true ? c- a similar idea as the one presented in this paper was presented in : Coloring graph neural networks for node disambiguation by George Dasoulas , Ludovic Dos Santos , Kevin Scaman , Aladin Virmaux https : //arxiv.org/abs/1912.06058 [ arxiv-col ] The main advantage of the current paper as opposed to [ arxiv-col ] is to propose an explicit coloring thanks to the structural features . But the theoretical analysis made in [ arxiv-col ] goes much deeper than this paper and probably could be adapted by the authors . For example , Corollary 3.1 could probably be replaced by a universality property , i.e.GSN with k=n-1 is universal . 3- the experimental evaluation is not convincing . To make it more convincing , the authors should include an ablation study for all their experiments by comparing their performances with the performances obtained with the structural features only . Such an ablation study would show the benefit of adding the MPNN on top of these features . [ After rebuttal ] I think the authors improved their paper by taking into account the remarks . Given the last results obtained in Table 4 , it looks like the structural features are indeed very good features in practice as they allow to boost the performances of a very simple invariant architecture like Deepset . I think the authors should explore how they can combine this approach with the coloring approach to get better GNN .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * 2.c.Relation to Dasoulas et al. , IJCAI 2020 * * We thank the reviewer for pointing out this relevant paper . Indeed , enhancing the node features with different colourings leads to a unique identification scheme similar to Loukas , ICLR 2020 , which was an important inspiration for our work . In fact , an interesting observation than can be made is that substructures and colourings can be used in combination , in order to benefit from the best of both worlds . In particular , GSN is permutation equivariant , but universality is so far guaranteed when the size of the substructures is in the order of the size of the graph $ k=n-1 $ ( subject to the validity of the reconstruction conjecture ) . On the other hand , colourings and in general arbitrarily chosen unique identifiers are universal , but compromise the permutation equivariance property of GNNs , while in order for a colouring graph network to be equivariant , all possible colour combinations should be used ( equation ( 6 ) of the mentioned paper : there exist $ \\prod_ { k=1 } ^K |V_k| ! $ different colourings , where each $ V_k $ contains nodes with the identical node attributes ) . However , by counting substructures , we can reduce the size of the sets $ V_k $ , since structural identifiers usually manage to break a significant amount of symmetries in the graph ( e.g.see Table 5 ) . Thus , by combining the two approaches , one can obtain universality and permutation equivariance while using fewer substructures and less colourings at the same time . This can be an interesting direction for future work . Regarding Corollary 3.1 , indeed being able to solve graph isomorphism implies universality as shown in Dasoulas et al. , IJCAI 2020 and Chen et al. , NeurIPS 2019 . The corollary has been rephrased in the updated version of the paper . * * 3.Comparison with structural features only : * * We thank the reviewer for suggesting this interesting ablation study . Agreeing on the relevance of studying the benefit of message passing of structural identifiers , we comprehensively tested a connectivity-agnostic baseline ( similar baselines have been used in Dwivedi et al. , arXiv 2020 ) treating input node and edge features , along with the structural identifiers , as a * set * . In particular , we consider each graph as a set of independent edges $ ( v , u ) $ endowed with the features of the endpoint nodes , their structural identifiers and edge features and we implement a DeepSets universal set function approximator ( Zaheer et al. , NIPS 2017 ) to learn a prediction function $ f\\bigg ( \\\\ { \\big ( h ( v ) , h ( u ) , x_V ( v ) , x_V ( u ) , e ( v , u ) \\big ) : \\\\ { v , u\\\\ } \\in E_G\\\\ } \\bigg ) = \\rho\\bigg ( \\sum_ { ( v , u ) \\in E_G } \\phi\\big ( h ( v ) , h ( u ) , x_V ( v ) , x_V ( u ) , e ( v , u ) \\big ) \\bigg ) $ , with $ E_G $ the edge set of the graph and $ \\rho , \\phi $ , modeled as MLPs . This baseline is naturally extended to the case where we consider edge structural identifiers by replacing $ x_V ( v ) , x_V ( u ) $ with $ x_E ( v , u ) $ . For fairness of evaluation , we follow the exact same hyperparameter tuning procedure as the one we followed for our GSN models for each benchmark . We refer the reviewer and interested reader to the Appendix C.5 in the updated version of the paper for additional experimental details . The results are reported in Table 4 of the main paper . As can be clearly seen , our baseline attains particularly strong performance across a variety of datasets and often outperforms other traditional message passing baselines which do not make use of structural identifiers . This demonstrates once more the benefits of these additional features and motivates their introduction in GNNs , which are unable to compute them . As expected , we observe the additional message passing layers applied on top of these features by GSN to generally bring performance improvements , sometimes considerably , as in the ZINC dataset . Interestingly , our DeepSets baseline performs on par with GSN on the Proteins benchmark , and better than any other model -- including GSN -- on MUTAG , where it attains state-of-the-art performance . This last is a relatively small dataset ( $ 188 $ graphs ) and we hypothesize our baseline is able to effectively balance model capacity and inductive priors into a strong sample-efficient architecture ."}, {"review_id": "LT0KSFnQDWF-1", "review_text": "This work proposes the Graph Substructure Network ( GSN ) to encode structural roles for different nodes so that the expressivity of Graph Neural Networks is improved . The core idea is to count the number of certain substructures , such as cycles , cliques , and triangles . Then the proposed MPNN encodes such substructure counting information into the message passing . Experimental results show that the proposed method can obtain better performance than the comparing methods . Strengths : + The proposed method can encode important substructure information . It is important for graphs since , in many applications , the substructures can determine the functionality of graphs . + The proposed method leads to better performance on different graph classification datasets . The experimental results can show the effectiveness of the proposed method . Weaknesses : - The main contribution of this work is counting the substructure information , which can be regarded as the preprocessing of graphs . However , how to properly select the graph set { H_1 , \u2026 , H_K } ? With different datasets at hand , the best choice can be quite different . Then how should we apply the proposed method ? - The second concern is the complexity of the proposed methods . Counting different types of substructures can be very time-consuming . As mentioned in this work , the worst case can have O ( n^k ) complexity . When the graph size is large , we may need to count too many types of substructures . - The datasets are relatively small ; most of them have less than 100 nodes per graph . Then larger datasets , such as RDT-B , RDT-M5K , and RDT-M12K , should be considered . I am wondering how many types of substructures need to be considered for these larger datasets to get better performance . - This work explicitly encodes substructure information into GNNs . Other existing methods , such as graph pooling methods , which can be considered to implicitly encode structural information . Then advanced pooling methods , such as Diffpool , Structpool , Min-cut Pool , etc. , should be discussed and compared . I am willing to adjust my score if my concerns are properly addressed . ==Update after rebuttal== I have read the authors ' rebuttal . Considering the limitations and non-superior performance for larger datasets , I am keeping my score unchanged .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their constructive feedback and helpful comments . * * 1.Substructure selection : * * We refer the reviewer to our general comment and the discussion in the paper , section 3 , paragraph `` How to choose the substructures ? `` . * * 2.Computational complexity : * * Regarding computational complexity , we refer the reviewer to the general comment . We believe it is important to also stress one additional point : in practice , the number of graphs in the substructure collection does not depend on the graph size , since what we are aiming at is generalisation and not expressivity alone . Even one single substructure family ( e.g.cliques or cycles ) -- or a small subset thereof -- can potentially yield small test error performance if conveying relevant domain knowledge and if it appears frequently within the graph distribution characterising the task at hand . As we highlight in the paper , a small number of small substructures is generally to be preferred aiming at best generalisation . * * 3.Larger datasets : * * As suggested by the reviewer , we performed additional experiments on the REDDIT-B and REDDIT-M5K datasets , where the average graph size is larger , comparing a baseline GIN architecture with GSN models equipped either with $ 3 $ -cliques or $ 3 $ -paths . As mentioned above , the size of substructures is chosen independently of the size of graphs they are matched onto . We tuned the hyperparameters of these models according to the procedure followed for the TU datasets . Results are reported here below as mean and standard deviation over $ 10 $ random dataset splits : | | REDDIT-B | REDDIT-M5K| |||| |GIN | 0.9005 \u00b1 0.00961 | 0.5695 \u00b1 0.02024|| |GSN ( 3-paths ) | 0.9045 \u00b1 0.01507 | 0.5713 \u00b1 0.02398| |GSN ( 3-cliques ) | 0.9070 \u00b1 0.01249 | 0.5707 \u00b1 0.02088 | We observe that GSN improves the performance but only marginally . These results were expected , as we observe that the Reddit datasets mostly contain tree-shaped graphs characterised by an abundance of star graphs and the presence of hub-nodes . Standard message passing may , therefore , be sufficiently expressive . As mentioned in the general comment , experiments on graphs with larger sizes are also performed for the Proteins dataset , where once again substructures of small size are sufficient . * * 4.Comparison with graph pooling : * * Graph pooling is a component frequently used in modern deep learning architectures for graphs . Although these methods modify the adjacency matrix of the graph , hence its isomorphism class , to date we are not aware of any theoretical evidence that graph pooling improves the expressive power of the GNN , while recently empirical results have also been challenged ( Mesquita et al. , \u201c Rethinking pooling in graph neural networks \u201d , NeurIPS 2020 ) . In fact , since most pooling methods are formulated as cluster assignment problems , where the clustering matrix is inferred by a traditional MPNN , it is likely that pooling will inherit the limitations of the MPNN . In particular , since the MPNN can not detect the nodes that belong to a given substructure , then there is no guarantee that these nodes will be clustered together , i.e.there is no guarantee that graph pooling will manage to infer the structural information that we explicitly encode with GSN . Below we show an example of two non-isomorphic graphs that a cluster assignment-based pooling will fail to distinguish , just as the MPNN that is built on top of : - G1 is a disconnected graph with two triangles , G2 is a 6-cycle ( Figure 1 , Maron et al. , `` Provably Powerful Graph Networks '' , NeurIPS 2019 ) . - Let the cluster assignment matrix $ S = \\mathrm { MPNN ( G ) } \\in R^ { c \\times n } $ , where $ n $ the number of nodes , $ c $ the number of clusters . The coarsened adjacency matrix will be given by $ A^\\prime = SAS^T $ . - In this case , the MPNN will assign the same feature $ s_u = x \\in R^ { c \\times 1 } $ to each node $ u $ for both graphs , thus the probability of a node being assigned to a cluster of the coarsened graph will be the same for all the nodes . Then the coarsened adjacency matrix $ A^\\prime_ { ij } = x_i x_j \\sum_ { u , v \\in V_G } A_ { uv } = 2|E_G| x_i x_j $ . Thus , since both graphs have the same number of edges , then the coarsened graphs will be isomorphic and the neural network will fail to distinguish them . Overall , although pooling might be implementing mechanisms orthogonal to message passing , there is neither any theoretical evidence that these improve expressivity nor that they can infer the structural information we take into account ."}, {"review_id": "LT0KSFnQDWF-2", "review_text": "This paper studies the expressivity of graph neural networks , and proposes a new approach to improve GNN \u2019 s expressivity by encoding nodes and edges with features via subgraph isomorphism counting . The proposed solution contains some merit , and the experimental results on graph classification task demonstrates the superiority of the proposed approach . Pros : 1.This paper addresses an important problem in GNNs , which is to improve the expressivity of GNNs . 2.The proposed solution is interesting , which is to include how many isomorphic subgraphs from a given list a node or an edge is contained in as additional features . 3.The experimental results on multiple datasets and different graph-level tasks are better than baselines . 4.Nice theoretical analysis . Cons : 1.My biggest concern lies in the time complexity of the proposed approach . Although the paper claims that in practice it is not that bad , the worst time complexity is still high . Also , the substructure selection brings us back to feature engineering , or we will face too many possible substructures . 2.More challenging graph tasks are expected to demonstrate the necessity of the proposed approach . Detailed comments : 1 . In the abstract and introduction , it is mentioned that existing GNNs are bounded by WL-test , and are not able to detect and count graph structures . It is expected to see experiments are on these more challenging tasks , in addition to graph classification and regression tasks . Graph isomorphism test is an interesting task , and the design is smart . 2.Discussions on how to select substructures on bigger size of graphs are expected .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their constructive feedback and helpful comments . * * 1a.Computational complexity : * * Regarding computational complexity , we refer the reviewer to the general comment . Here we would like to briefly remark a few points . First , the worst case complexity is rarely encountered in real-world graph distributions , and we empirically show this in Figure 3 of the updated version of the paper . Second , there exist several exact and approximate algorithms that improve on the theoretical worst case complexity . * * 1b.Substructure Selection : * * Regarding substructure selection and feature engineering , we refer the reviewer to the general comment . We acknowledge that employing all possible substructures is not feasible and that a selection procedure is necessary . In practice , domain knowledge allowed us to seamlessly narrow down the substructure space and opt for a single substructure family , leaving us with having to choose only the size of the larger substructure $ k $ . Despite the recent trend of abandoning feature engineering towards end-to-end learning pipelines , we believe that in certain domains this should be embraced with caution , since , so far , the most popular architectures for learning on graphs , i.e.MPNNs , have been shown not to be universal , contrary to CNNs , RNNs , Transformers etc. , and in fact they are unable to compute these structural features . Regarding larger graphs , we stress here that substructure selection should be done independently of the size of the graph in order to obtain better generalisation properties . Please refer to the general comment and our paper for further discussion . * * 2.More challenging graph tasks : * * In this paper , we study expressivity from the perspective of graph isomorphism and thus we performed experiments on this task in order to validate our theoretical claims . However , we certainly agree with the reviewer that dealing with more challenging ( computationally hard in the worst case ) graph tasks ( e.g.subgraph counting , graph edit distance , combinatorial optimisation ) is an interesting topic not only experimentally , but also and foremost from a theoretical standpoint ( e.g.can we combine message passing with small substructures to compose larger ones ? , can we improve size generalisation using graph substructures ? - see `` On Size Generalisation in Graph Neural Networks '' , Anonymous , submitted to ICLR 2021 ) Thus , experimentation on such tasks should be accompanied by the appropriate theoretical analysis . These questions were beyond the scope of this paper and we are planning to address them in future work ."}, {"review_id": "LT0KSFnQDWF-3", "review_text": "Reasons for score : The idea of using small graphs to characterize local topologies and guide message passing in interesting . However , the graph isomorphism computation part has problem . Experimental results are not conclusive . The written need improvement in some part of the manuscript . Pros : A new method ( GSN : Graph Substructure Network ) is proposed to a topologically-aware message passing method that better utilize graph substructure information . The method tries to tackles the limitation of traditional GNN in exploring graph structure . It is a good idea to pass messages differently depending on their local topologies . This is done through using a set of predefine small graphs to characterize local topologies . The authors showed that GSN was more expressive than traditional GNNs . A good number of experimental evaluations were performed . Cons : It is not clear priorly how to define a good set of small graphs , especially when considering beyond immediate neighbors . In addition , node features are not considered in graph isomorphism , which can lead to incorrect subgraph matching . The experimental results on some of the datasets ( such as , MUTAG , PTC , Proteins and NCI1 in table 1 ) do not appear to be significantly better than those of the previous approaches when considering the variances of different runs . In addition , much better results were reported on the ogb-molhiv leaderboard ( https : //ogb.stanford.edu/docs/leader_graphprop/ # ogbg-molhiv ) . Figure 1 is confusing . Should the number in the yellow square on the left be 5 ? A more self-contained explanation of the figure is appreciated . It will help readers if the authors can visualize a few examples ( e.g. , contributions of small graphs ) to explain why their approach works better .", "rating": "3: Clear rejection", "reply_text": "* * 3.Experimental results ( 2/2 ) * * : 2 . * * Regarding the ogbg-molhiv leaderboard : * * We respectfully disagree for the following reasons : - Our intention was to show the our method can easily be adapted in a `` plug-and-play '' fashion to Graph Neural Network architectures to improve their performance : on this dataset the simple adoption of structural identifiers was sufficient to get ~ 1\\ % improvement ( note that we did not change any of the optimisation or model hyperparameters ) ; - Some of the methods that achieve better results than ours in the public OGB leaderboard , e.g.GraphNorm , DeeperGCNs , FLAG , deal with aspects that are orthogonal to structural identifiers and GSN and they can seamlessly be combined so as to jointly address different limitations of message passing baselines ; - It is important to stress here that the aforementioned methods are concurrent works and currently undergoing the peer-review process at ICLR , thus we deem the demand for such a comparison unfair . - The scope of our work is that of studying how the expressivity of Graph Neural Networks can be improved rather than conceiving the most appropriate architecture to model molecular graph distributions . Other methods reported in the leaderboard ( HIMP , Morgan fingerprints ) are specifically designed to address the latter . We firmly believe that it is more natural and probably more insightful to compare GSN with other architectures focusing on the former ( GIN , PPGNN , Natural Graph Networks ) . We would also like to point out here an interesting fact regarding the ogbg-molhiv dataset : In order to obtain training , validation and test sets , the authors of the OGB benchmark ( Hu et al , NeuIPS 2020 ) , apply a `` scaffold splitting '' procedure ( molecules with different 2D-structure are assigned to a different partition of the dataset ) . This is a challenging setting for machine learning models since , train , test and validation distributions might differ substantially . We investigated the impact of this aspect on GSN performance and observed that substructures that yielded the best test score did not always attain best validation performance as well , this confirming the characteristic discrepancy between the test and validation distributions . Naturally , coherently with standard model selection procedures , in the main paper we reported the results corresponding to the substructure size with best validation performance , but it is interesting to notice that larger substructures attain better test results with * validation scores which are still the highest reported on this benchmark * ( based on the public leaderboard ) . These results are shown below , and in Section C.4 of the paper Appendix . |k |Train | Validation | Test | ||||| |6 |94.29 \u00b1 3.38 | 86.58 \u00b1 0.84 | 77.99 \u00b1 1.00| |8 |94.33 \u00b1 2.38 | 85.59 \u00b1 0.82 | 78.20 \u00b1 1.69| One last interesting observation is that these results were obtained with a GSN model using the structural features only at its input . In fact , it is possible to `` inject '' the structural features at each message passing layer of the architecture , effectively accounting for a skip connection acting on structural identifiers . Further performance improvements -- although minor -- were observed , and we report the results here below . |k |Train | Validation | Test | ||||| |6 |93.77 \u00b1 3.28 | 86.44 \u00b1 1.09 | 78.07 \u00b1 0.83| |8 |93.97 \u00b1 2.70 | 85.30 \u00b1 1.01 | 78.55 \u00b1 1.25| * * 4 . Figure 1 : * * We have checked the computation of the subgraph isomorphism example in Figure 1 and we can confirm its correctness . As already specified in the caption , we consider * * induced * * paths . Note that the path that is formed by the nodes of the triangle is not induced since its endpoints are connected and , therefore , must not be included in the computation . * * 5.Contribution of certain substructures : * * An ablation study on different substructures has been performed on the ZINC dataset , where we show that cycles of certain size achieve significantly better generalisation than others . We contrasted them with paths and trees which , although characterised by much larger disambiguation scores $ \\delta $ ( see also Table 5 ) , do not perform comparably . We refer the reviewer to Figure 4 of the main paper ."}], "0": {"review_id": "LT0KSFnQDWF-0", "review_text": "This paper presents a natural extension of Message Passing Neural Net ( MPNN ) by incorporating structural features . These structural features are computed as the counts from different substructures ( like small lines , stars or complete graphs ) induced in the original graph . These counts are combined to obtain a new feature per node or per edge . Then these features are used in a standard MPNN . The authors then show that the resulting GNN is more expressive and they validate this claim experimentally . This idea is interesting and clearly explained in the paper but I think the paper could be greatly improved after addressing the following issues : 1- the authors should clarify their position with regards to invariance . Indeed , as explained shortly on page 3 when commenting Loukas ( 2020 ) , it is easy to make a GNN powerful if we remove the constraint to be invariant ( or equivariant ) . Hence , as I understand it , the authors are proposing an algorithm that is equivariant . If this is the case , it would be great to have a clear formal statement that GSN are equivariant and to give a mathematical proof . 2- the theoretical content of the paper should be improved : a- the first part of Proposition 3.1 is straightforward and I find the wording of the second statement unclear : what does ' ... or any not necessarily induced subgraph ... ' mean ? b- from a theoretical perspective , it seems that both GSN-v abd GSN-e have the same expressive power . Is it true ? c- a similar idea as the one presented in this paper was presented in : Coloring graph neural networks for node disambiguation by George Dasoulas , Ludovic Dos Santos , Kevin Scaman , Aladin Virmaux https : //arxiv.org/abs/1912.06058 [ arxiv-col ] The main advantage of the current paper as opposed to [ arxiv-col ] is to propose an explicit coloring thanks to the structural features . But the theoretical analysis made in [ arxiv-col ] goes much deeper than this paper and probably could be adapted by the authors . For example , Corollary 3.1 could probably be replaced by a universality property , i.e.GSN with k=n-1 is universal . 3- the experimental evaluation is not convincing . To make it more convincing , the authors should include an ablation study for all their experiments by comparing their performances with the performances obtained with the structural features only . Such an ablation study would show the benefit of adding the MPNN on top of these features . [ After rebuttal ] I think the authors improved their paper by taking into account the remarks . Given the last results obtained in Table 4 , it looks like the structural features are indeed very good features in practice as they allow to boost the performances of a very simple invariant architecture like Deepset . I think the authors should explore how they can combine this approach with the coloring approach to get better GNN .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * 2.c.Relation to Dasoulas et al. , IJCAI 2020 * * We thank the reviewer for pointing out this relevant paper . Indeed , enhancing the node features with different colourings leads to a unique identification scheme similar to Loukas , ICLR 2020 , which was an important inspiration for our work . In fact , an interesting observation than can be made is that substructures and colourings can be used in combination , in order to benefit from the best of both worlds . In particular , GSN is permutation equivariant , but universality is so far guaranteed when the size of the substructures is in the order of the size of the graph $ k=n-1 $ ( subject to the validity of the reconstruction conjecture ) . On the other hand , colourings and in general arbitrarily chosen unique identifiers are universal , but compromise the permutation equivariance property of GNNs , while in order for a colouring graph network to be equivariant , all possible colour combinations should be used ( equation ( 6 ) of the mentioned paper : there exist $ \\prod_ { k=1 } ^K |V_k| ! $ different colourings , where each $ V_k $ contains nodes with the identical node attributes ) . However , by counting substructures , we can reduce the size of the sets $ V_k $ , since structural identifiers usually manage to break a significant amount of symmetries in the graph ( e.g.see Table 5 ) . Thus , by combining the two approaches , one can obtain universality and permutation equivariance while using fewer substructures and less colourings at the same time . This can be an interesting direction for future work . Regarding Corollary 3.1 , indeed being able to solve graph isomorphism implies universality as shown in Dasoulas et al. , IJCAI 2020 and Chen et al. , NeurIPS 2019 . The corollary has been rephrased in the updated version of the paper . * * 3.Comparison with structural features only : * * We thank the reviewer for suggesting this interesting ablation study . Agreeing on the relevance of studying the benefit of message passing of structural identifiers , we comprehensively tested a connectivity-agnostic baseline ( similar baselines have been used in Dwivedi et al. , arXiv 2020 ) treating input node and edge features , along with the structural identifiers , as a * set * . In particular , we consider each graph as a set of independent edges $ ( v , u ) $ endowed with the features of the endpoint nodes , their structural identifiers and edge features and we implement a DeepSets universal set function approximator ( Zaheer et al. , NIPS 2017 ) to learn a prediction function $ f\\bigg ( \\\\ { \\big ( h ( v ) , h ( u ) , x_V ( v ) , x_V ( u ) , e ( v , u ) \\big ) : \\\\ { v , u\\\\ } \\in E_G\\\\ } \\bigg ) = \\rho\\bigg ( \\sum_ { ( v , u ) \\in E_G } \\phi\\big ( h ( v ) , h ( u ) , x_V ( v ) , x_V ( u ) , e ( v , u ) \\big ) \\bigg ) $ , with $ E_G $ the edge set of the graph and $ \\rho , \\phi $ , modeled as MLPs . This baseline is naturally extended to the case where we consider edge structural identifiers by replacing $ x_V ( v ) , x_V ( u ) $ with $ x_E ( v , u ) $ . For fairness of evaluation , we follow the exact same hyperparameter tuning procedure as the one we followed for our GSN models for each benchmark . We refer the reviewer and interested reader to the Appendix C.5 in the updated version of the paper for additional experimental details . The results are reported in Table 4 of the main paper . As can be clearly seen , our baseline attains particularly strong performance across a variety of datasets and often outperforms other traditional message passing baselines which do not make use of structural identifiers . This demonstrates once more the benefits of these additional features and motivates their introduction in GNNs , which are unable to compute them . As expected , we observe the additional message passing layers applied on top of these features by GSN to generally bring performance improvements , sometimes considerably , as in the ZINC dataset . Interestingly , our DeepSets baseline performs on par with GSN on the Proteins benchmark , and better than any other model -- including GSN -- on MUTAG , where it attains state-of-the-art performance . This last is a relatively small dataset ( $ 188 $ graphs ) and we hypothesize our baseline is able to effectively balance model capacity and inductive priors into a strong sample-efficient architecture ."}, "1": {"review_id": "LT0KSFnQDWF-1", "review_text": "This work proposes the Graph Substructure Network ( GSN ) to encode structural roles for different nodes so that the expressivity of Graph Neural Networks is improved . The core idea is to count the number of certain substructures , such as cycles , cliques , and triangles . Then the proposed MPNN encodes such substructure counting information into the message passing . Experimental results show that the proposed method can obtain better performance than the comparing methods . Strengths : + The proposed method can encode important substructure information . It is important for graphs since , in many applications , the substructures can determine the functionality of graphs . + The proposed method leads to better performance on different graph classification datasets . The experimental results can show the effectiveness of the proposed method . Weaknesses : - The main contribution of this work is counting the substructure information , which can be regarded as the preprocessing of graphs . However , how to properly select the graph set { H_1 , \u2026 , H_K } ? With different datasets at hand , the best choice can be quite different . Then how should we apply the proposed method ? - The second concern is the complexity of the proposed methods . Counting different types of substructures can be very time-consuming . As mentioned in this work , the worst case can have O ( n^k ) complexity . When the graph size is large , we may need to count too many types of substructures . - The datasets are relatively small ; most of them have less than 100 nodes per graph . Then larger datasets , such as RDT-B , RDT-M5K , and RDT-M12K , should be considered . I am wondering how many types of substructures need to be considered for these larger datasets to get better performance . - This work explicitly encodes substructure information into GNNs . Other existing methods , such as graph pooling methods , which can be considered to implicitly encode structural information . Then advanced pooling methods , such as Diffpool , Structpool , Min-cut Pool , etc. , should be discussed and compared . I am willing to adjust my score if my concerns are properly addressed . ==Update after rebuttal== I have read the authors ' rebuttal . Considering the limitations and non-superior performance for larger datasets , I am keeping my score unchanged .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their constructive feedback and helpful comments . * * 1.Substructure selection : * * We refer the reviewer to our general comment and the discussion in the paper , section 3 , paragraph `` How to choose the substructures ? `` . * * 2.Computational complexity : * * Regarding computational complexity , we refer the reviewer to the general comment . We believe it is important to also stress one additional point : in practice , the number of graphs in the substructure collection does not depend on the graph size , since what we are aiming at is generalisation and not expressivity alone . Even one single substructure family ( e.g.cliques or cycles ) -- or a small subset thereof -- can potentially yield small test error performance if conveying relevant domain knowledge and if it appears frequently within the graph distribution characterising the task at hand . As we highlight in the paper , a small number of small substructures is generally to be preferred aiming at best generalisation . * * 3.Larger datasets : * * As suggested by the reviewer , we performed additional experiments on the REDDIT-B and REDDIT-M5K datasets , where the average graph size is larger , comparing a baseline GIN architecture with GSN models equipped either with $ 3 $ -cliques or $ 3 $ -paths . As mentioned above , the size of substructures is chosen independently of the size of graphs they are matched onto . We tuned the hyperparameters of these models according to the procedure followed for the TU datasets . Results are reported here below as mean and standard deviation over $ 10 $ random dataset splits : | | REDDIT-B | REDDIT-M5K| |||| |GIN | 0.9005 \u00b1 0.00961 | 0.5695 \u00b1 0.02024|| |GSN ( 3-paths ) | 0.9045 \u00b1 0.01507 | 0.5713 \u00b1 0.02398| |GSN ( 3-cliques ) | 0.9070 \u00b1 0.01249 | 0.5707 \u00b1 0.02088 | We observe that GSN improves the performance but only marginally . These results were expected , as we observe that the Reddit datasets mostly contain tree-shaped graphs characterised by an abundance of star graphs and the presence of hub-nodes . Standard message passing may , therefore , be sufficiently expressive . As mentioned in the general comment , experiments on graphs with larger sizes are also performed for the Proteins dataset , where once again substructures of small size are sufficient . * * 4.Comparison with graph pooling : * * Graph pooling is a component frequently used in modern deep learning architectures for graphs . Although these methods modify the adjacency matrix of the graph , hence its isomorphism class , to date we are not aware of any theoretical evidence that graph pooling improves the expressive power of the GNN , while recently empirical results have also been challenged ( Mesquita et al. , \u201c Rethinking pooling in graph neural networks \u201d , NeurIPS 2020 ) . In fact , since most pooling methods are formulated as cluster assignment problems , where the clustering matrix is inferred by a traditional MPNN , it is likely that pooling will inherit the limitations of the MPNN . In particular , since the MPNN can not detect the nodes that belong to a given substructure , then there is no guarantee that these nodes will be clustered together , i.e.there is no guarantee that graph pooling will manage to infer the structural information that we explicitly encode with GSN . Below we show an example of two non-isomorphic graphs that a cluster assignment-based pooling will fail to distinguish , just as the MPNN that is built on top of : - G1 is a disconnected graph with two triangles , G2 is a 6-cycle ( Figure 1 , Maron et al. , `` Provably Powerful Graph Networks '' , NeurIPS 2019 ) . - Let the cluster assignment matrix $ S = \\mathrm { MPNN ( G ) } \\in R^ { c \\times n } $ , where $ n $ the number of nodes , $ c $ the number of clusters . The coarsened adjacency matrix will be given by $ A^\\prime = SAS^T $ . - In this case , the MPNN will assign the same feature $ s_u = x \\in R^ { c \\times 1 } $ to each node $ u $ for both graphs , thus the probability of a node being assigned to a cluster of the coarsened graph will be the same for all the nodes . Then the coarsened adjacency matrix $ A^\\prime_ { ij } = x_i x_j \\sum_ { u , v \\in V_G } A_ { uv } = 2|E_G| x_i x_j $ . Thus , since both graphs have the same number of edges , then the coarsened graphs will be isomorphic and the neural network will fail to distinguish them . Overall , although pooling might be implementing mechanisms orthogonal to message passing , there is neither any theoretical evidence that these improve expressivity nor that they can infer the structural information we take into account ."}, "2": {"review_id": "LT0KSFnQDWF-2", "review_text": "This paper studies the expressivity of graph neural networks , and proposes a new approach to improve GNN \u2019 s expressivity by encoding nodes and edges with features via subgraph isomorphism counting . The proposed solution contains some merit , and the experimental results on graph classification task demonstrates the superiority of the proposed approach . Pros : 1.This paper addresses an important problem in GNNs , which is to improve the expressivity of GNNs . 2.The proposed solution is interesting , which is to include how many isomorphic subgraphs from a given list a node or an edge is contained in as additional features . 3.The experimental results on multiple datasets and different graph-level tasks are better than baselines . 4.Nice theoretical analysis . Cons : 1.My biggest concern lies in the time complexity of the proposed approach . Although the paper claims that in practice it is not that bad , the worst time complexity is still high . Also , the substructure selection brings us back to feature engineering , or we will face too many possible substructures . 2.More challenging graph tasks are expected to demonstrate the necessity of the proposed approach . Detailed comments : 1 . In the abstract and introduction , it is mentioned that existing GNNs are bounded by WL-test , and are not able to detect and count graph structures . It is expected to see experiments are on these more challenging tasks , in addition to graph classification and regression tasks . Graph isomorphism test is an interesting task , and the design is smart . 2.Discussions on how to select substructures on bigger size of graphs are expected .", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their constructive feedback and helpful comments . * * 1a.Computational complexity : * * Regarding computational complexity , we refer the reviewer to the general comment . Here we would like to briefly remark a few points . First , the worst case complexity is rarely encountered in real-world graph distributions , and we empirically show this in Figure 3 of the updated version of the paper . Second , there exist several exact and approximate algorithms that improve on the theoretical worst case complexity . * * 1b.Substructure Selection : * * Regarding substructure selection and feature engineering , we refer the reviewer to the general comment . We acknowledge that employing all possible substructures is not feasible and that a selection procedure is necessary . In practice , domain knowledge allowed us to seamlessly narrow down the substructure space and opt for a single substructure family , leaving us with having to choose only the size of the larger substructure $ k $ . Despite the recent trend of abandoning feature engineering towards end-to-end learning pipelines , we believe that in certain domains this should be embraced with caution , since , so far , the most popular architectures for learning on graphs , i.e.MPNNs , have been shown not to be universal , contrary to CNNs , RNNs , Transformers etc. , and in fact they are unable to compute these structural features . Regarding larger graphs , we stress here that substructure selection should be done independently of the size of the graph in order to obtain better generalisation properties . Please refer to the general comment and our paper for further discussion . * * 2.More challenging graph tasks : * * In this paper , we study expressivity from the perspective of graph isomorphism and thus we performed experiments on this task in order to validate our theoretical claims . However , we certainly agree with the reviewer that dealing with more challenging ( computationally hard in the worst case ) graph tasks ( e.g.subgraph counting , graph edit distance , combinatorial optimisation ) is an interesting topic not only experimentally , but also and foremost from a theoretical standpoint ( e.g.can we combine message passing with small substructures to compose larger ones ? , can we improve size generalisation using graph substructures ? - see `` On Size Generalisation in Graph Neural Networks '' , Anonymous , submitted to ICLR 2021 ) Thus , experimentation on such tasks should be accompanied by the appropriate theoretical analysis . These questions were beyond the scope of this paper and we are planning to address them in future work ."}, "3": {"review_id": "LT0KSFnQDWF-3", "review_text": "Reasons for score : The idea of using small graphs to characterize local topologies and guide message passing in interesting . However , the graph isomorphism computation part has problem . Experimental results are not conclusive . The written need improvement in some part of the manuscript . Pros : A new method ( GSN : Graph Substructure Network ) is proposed to a topologically-aware message passing method that better utilize graph substructure information . The method tries to tackles the limitation of traditional GNN in exploring graph structure . It is a good idea to pass messages differently depending on their local topologies . This is done through using a set of predefine small graphs to characterize local topologies . The authors showed that GSN was more expressive than traditional GNNs . A good number of experimental evaluations were performed . Cons : It is not clear priorly how to define a good set of small graphs , especially when considering beyond immediate neighbors . In addition , node features are not considered in graph isomorphism , which can lead to incorrect subgraph matching . The experimental results on some of the datasets ( such as , MUTAG , PTC , Proteins and NCI1 in table 1 ) do not appear to be significantly better than those of the previous approaches when considering the variances of different runs . In addition , much better results were reported on the ogb-molhiv leaderboard ( https : //ogb.stanford.edu/docs/leader_graphprop/ # ogbg-molhiv ) . Figure 1 is confusing . Should the number in the yellow square on the left be 5 ? A more self-contained explanation of the figure is appreciated . It will help readers if the authors can visualize a few examples ( e.g. , contributions of small graphs ) to explain why their approach works better .", "rating": "3: Clear rejection", "reply_text": "* * 3.Experimental results ( 2/2 ) * * : 2 . * * Regarding the ogbg-molhiv leaderboard : * * We respectfully disagree for the following reasons : - Our intention was to show the our method can easily be adapted in a `` plug-and-play '' fashion to Graph Neural Network architectures to improve their performance : on this dataset the simple adoption of structural identifiers was sufficient to get ~ 1\\ % improvement ( note that we did not change any of the optimisation or model hyperparameters ) ; - Some of the methods that achieve better results than ours in the public OGB leaderboard , e.g.GraphNorm , DeeperGCNs , FLAG , deal with aspects that are orthogonal to structural identifiers and GSN and they can seamlessly be combined so as to jointly address different limitations of message passing baselines ; - It is important to stress here that the aforementioned methods are concurrent works and currently undergoing the peer-review process at ICLR , thus we deem the demand for such a comparison unfair . - The scope of our work is that of studying how the expressivity of Graph Neural Networks can be improved rather than conceiving the most appropriate architecture to model molecular graph distributions . Other methods reported in the leaderboard ( HIMP , Morgan fingerprints ) are specifically designed to address the latter . We firmly believe that it is more natural and probably more insightful to compare GSN with other architectures focusing on the former ( GIN , PPGNN , Natural Graph Networks ) . We would also like to point out here an interesting fact regarding the ogbg-molhiv dataset : In order to obtain training , validation and test sets , the authors of the OGB benchmark ( Hu et al , NeuIPS 2020 ) , apply a `` scaffold splitting '' procedure ( molecules with different 2D-structure are assigned to a different partition of the dataset ) . This is a challenging setting for machine learning models since , train , test and validation distributions might differ substantially . We investigated the impact of this aspect on GSN performance and observed that substructures that yielded the best test score did not always attain best validation performance as well , this confirming the characteristic discrepancy between the test and validation distributions . Naturally , coherently with standard model selection procedures , in the main paper we reported the results corresponding to the substructure size with best validation performance , but it is interesting to notice that larger substructures attain better test results with * validation scores which are still the highest reported on this benchmark * ( based on the public leaderboard ) . These results are shown below , and in Section C.4 of the paper Appendix . |k |Train | Validation | Test | ||||| |6 |94.29 \u00b1 3.38 | 86.58 \u00b1 0.84 | 77.99 \u00b1 1.00| |8 |94.33 \u00b1 2.38 | 85.59 \u00b1 0.82 | 78.20 \u00b1 1.69| One last interesting observation is that these results were obtained with a GSN model using the structural features only at its input . In fact , it is possible to `` inject '' the structural features at each message passing layer of the architecture , effectively accounting for a skip connection acting on structural identifiers . Further performance improvements -- although minor -- were observed , and we report the results here below . |k |Train | Validation | Test | ||||| |6 |93.77 \u00b1 3.28 | 86.44 \u00b1 1.09 | 78.07 \u00b1 0.83| |8 |93.97 \u00b1 2.70 | 85.30 \u00b1 1.01 | 78.55 \u00b1 1.25| * * 4 . Figure 1 : * * We have checked the computation of the subgraph isomorphism example in Figure 1 and we can confirm its correctness . As already specified in the caption , we consider * * induced * * paths . Note that the path that is formed by the nodes of the triangle is not induced since its endpoints are connected and , therefore , must not be included in the computation . * * 5.Contribution of certain substructures : * * An ablation study on different substructures has been performed on the ZINC dataset , where we show that cycles of certain size achieve significantly better generalisation than others . We contrasted them with paths and trees which , although characterised by much larger disambiguation scores $ \\delta $ ( see also Table 5 ) , do not perform comparably . We refer the reviewer to Figure 4 of the main paper ."}}