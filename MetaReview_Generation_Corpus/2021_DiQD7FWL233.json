{"year": "2021", "forum": "DiQD7FWL233", "title": "Improving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein", "decision": "Accept (Poster)", "meta_review": "This is a solid paper that proposes a new slicing approach to the fused Gromov Wasserstein distance using projection on directions sampled from  von-mises fisher direction, the location parameter of the von mises fisher is choosen to be maximally discriminating between the distribution \n$(\\max_{\\epsilon}\\mathbb{E}_{\\theta|vMF(\\theta|\\epsilon,\\kappa)}\\beta W(\\theta\\mu,\\theta\\nu) +(1-\\beta)GW(\\theta\\mu,\\theta\\nu))$, the new sliced distance is analyzed and extended to mixture of von mises distributions with $k$ locations or directions.  This contribution of the paper is of general interest beyond the application of the paper as mentioned by the reviewers.  Authors applies the new sliced Fused Gromov Wasserstein distance to relational auto encoders and show improvement.\n\nThe spherical slicing is original and new and of independent interest and the application is good as it pushes the boundary of relational auto encoders .Reviewers  and AC did not have any concerns with the paper and the rebuttal and revisions addressed all questions raised. Accept\n\n", "reviews": [{"review_id": "DiQD7FWL233-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The paper presents a novel method to improve the relational regularized autoencoders . The proposed method is based on the new relational discrepancy which is called the spherical sliced fused Gromov Wasserstein ( SSFG ) . It is seen that the SSFG is an extension of the sliced fused Gromov Wasserstein ( SFG ) and its max version . Two variants of the SSFG are also presented . Experiments suggest that the proposed autoencoders outperform some existing autoencoders in terms of generative performance in comparable computational time . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I have a positive impression about the paper . I think that the proposed relational discrepancy using the von Mises -- Fisher distribution is a reasonable extension of the SFG using the uniform distribution . My concern is whether the estimation of the tuning parameters of the proposed relational discrepancies could be computationally expensive in practice ( see cons below ) . Hopefully the authors can address my concern in the rebuttal period . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : ( 1 ) Applying the fact the von Mises -- Fisher distribution is an extension of the uniform distribution and the Dirac distribution , the authors successfully presented an extended relational discrepancy of the SFG and its max version . ( 2 ) The mixture spherical sliced fused Gromov Wasserstein ( MSSFG ) achieves an even better flexibility than the SSFG . I reckon that the MSSFG , which adopts a mixture of the von Mises -- Fisher distributions , is a reasonable extension of the SSFG . ( 3 ) The extensive experiments suggest that the proposed autoencoders show satisfactory performance in terms of FID scores and reconstruction losses and are not particularly expensive in terms of computational time . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : ( 1 ) I wonder whether the estimation of the tuning parameters of the proposed relational discrepancies could be computationally expensive in practice . For example , the MSSFG requires the values of $ k $ , $ \\kappa_1 , \\ldots , \\kappa_k , \\alpha_1 , \\ldots , \\alpha_k $ . I fear that the estimation of these tuning parameters requires expensive computational cost . In the experiments of the paper , only limited combinations of these tuning parameters are considered . I wonder whether these combinations really cover a sufficient area of the parameter space . ( 2 ) The von Mises -- Fisher distribution has the property that as the concentration parameter $ \\kappa $ increases , the concentration of the distribution monotonically increases . In particular , the von Mises -- Fisher distribution tends to the Dirac distribution as $ \\kappa $ goes to infinity . However I am not sure this nice property also holds for the power distribution . If not , I wonder whether the power SSFG and power spherical DRAE have sufficient flexibility compared with the SSFG and spherical DRAE . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : Please address and clarify the cons above . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Typo : ( 1 ) p.1 , abstract , last line : generation , and reconstruction - > generation , and reconstruction # # Updates : The authors have carefully responded to my comments . Their response addresses most of my concerns . I will keep my score high . I understand that the choice of the hyperparameters can be computationally heavy , but the authors have given an idea to solve this problem . It is good to find that the Power Spherical distribution also includes the Dirac distribution as a limiting case .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your time . We have revised our papers based on your comments . The changes are marked in blue color . Question 1 : \u201c I wonder whether the estimation of the tuning parameters of the proposed relational discrepancies could be computationally expensive in practice . For example , the MSSFG requires the values of k , \u03ba1 , \u2026 , \u03bak , \u03b11 , \u2026 , \u03b1k . I fear that the estimation of these tuning parameters requires expensive computational cost . In the experiments of the paper , only limited combinations of these tuning parameters are considered . I wonder whether these combinations really cover a sufficient area of the parameter space. \u201d Answer : In practice , we use uniform weights for { $ \\alpha_ { 1 : k } $ } and the same value $ \\kappa $ for $ \\kappa_ { 1 } , \u2026 , \\kappa_ { k } $ because we would like every component of the mixtures of vMF distributions to be treated similarly . Therefore , we eventually only need to tune for two hyperparameters $ \\kappa $ and $ k $ . In practice , we have searched only for $ \\kappa \\in $ { 1,5,10,50,100 } and $ k \\in $ { 2 , 5 , 10 , 50 } as we observed that the performance of MSSFG when $ \\kappa > 100 $ is rather similar to when $ \\kappa = 100 $ ( similarly , the performance of MSSFG when $ \\kappa < 1 $ is quite similar to when $ \\kappa = 1 $ ) . That 's why we only consider $ \\kappa \\in $ { 1 , 5 , 10 , 50 , 100 } . For the choice of the number of components $ k $ , we observe that $ k \\leq 50 $ is well-suited for the current applications considered in the paper . However , we anticipate that for much larger-scale applications , a bigger $ k $ should be chosen . One possible method for automatically choosing $ k $ without tuning it is by using the idea from Bayesian nonparametrics , namely , we model the projections as Dirichlet process mixture models ( cf . [ 1 ] and [ 2 ] ) where the number of components $ k $ grows automatically at certain rates ( controlled by the hyperparameters in the Dirichlet process ) when the number of projections is increasing . We believe that it is an important direction to explore and we leave this direction for the future work . [ 1 ] T. S. Ferguson . A Bayesian Analysis of Some Nonparametric Problems . Annals of Statistics , 1973 . [ 2 ] C. E. Antoniak . Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems . Annals of Statistics , 1974 . Question 2 : \u201c The von Mises -- Fisher distribution has the property that as the concentration parameter \u03ba increases , the concentration of the distribution monotonically increases . In particular , the von Mises -- Fisher distribution tends to the Dirac distribution as \u03ba goes to infinity . However I am not sure this nice property also holds for the power distribution . If not , I wonder whether the power SSFG and power spherical DRAE have sufficient flexibility compared with the SSFG and spherical DRAE. \u201d Answer : Thank you for your comment . In the revised version , we have proved that the Power Spherical distribution is also the interpolation between the uniform distribution and the Dirac distribution . This result is proved in Lemma 1 in Appendix D. Therefore , the power SSFG and power spherical DRAE are also as flexible as the SSFG and spherical DRAE . Question 3 : \u201c ( 1 ) p.1 , abstract , last line : generation , and reconstruction - > generation , and reconstruction. \u201d Answer : We have fixed these typos in the revised version in blue color ."}, {"review_id": "DiQD7FWL233-1", "review_text": "The paper proposes a new pseudo-distance called the spherical slices fused Gromov Wasserstein distance ( SSFGW ) . It builds on top of the slices fused Gromov Wasserstein distance ( SFGW ) that takes a weighted combination of the Wasserstein distance and the Gromov Wasserstein distance on sliced spaces . The paper tackles the problem of solving for the best sampling directions of slicing . Existing approaches either assume uniform sampling strategies or a single direction that maximizes the discrepancy of two measures . This paper uses von Mises-Fisher measures ( vMF ) as the bridge to combine the advantages of the two . Changing the parameters of vMF is equivalent to ( non-linearly ) interpolating between a Dirac impulse and a uniform distribution , which converges to uniform-sliced Wasserstein and max-sliced Wasserstein , respectively . The authors provided the proof of its pseudo metric properties and upper bound and extended their SSFGW to mixtures of vFW , creating MSSFGW , and applied these variants to deterministic relational regularized autoencoder ( DRAE ) . The results of comparing the DRAE equipped with proposed distance with other DRAEs demonstrate its superiority in stability and generative capacity . Some comments and questions : -- Where does von Mises-Fisher distribution come from ? If I understand it correctly , von Mises-Fisher is a variant of Gaussian on hyperspheres . The authors can consider citing existing works or proving some arguments of choosing vMF as an interpolation of a uniform distribution and a Dirac . -- Please refine 2 Background . Definition 2 : is $ I_v $ actually $ I_d $ in $ C_d ( k ) $ ? `` The vMF distribution provides a way to have concentrated weight on the most important directions and assigns less weight to further directions . Therefore , we gain a better representation of the discrepancy between probability measures . '' Swap SFG and DRAE in the title of 2.1 to consist with the content of 2.1 . -- The introduction of power SSFG at the end of Section 3 seems abrupt . If power SSFG performs better than SSFG , then what is the point of introducing vMF ? Can we build mixtures of power SSFG ? The paper does not mention an mps-DRAE . On page 20 of the Appendix , at the top , it seems power SSFG is superior to SSFG in all desired aspects , but ps-DRAE underperforms s-DRAE , as shown in Table 1 . Any explanation ? -- Empirical evidence of interpolating between a uniform and a Dirac In Figure 12 , abd , the graphs of s-DRAE cross the red baseline with a large momentum when $ k $ decreases to 1 , which suggests that it could go far away from the baseline when $ k \\rightarrow 0 $ because when $ k=1 $ , vFM is still very far from a uniform distribution . Please explain that . -- A Wasserstein paper or a generative modeling paper ? The main contribution of the paper , which tackles the problem of generalizing sliced Wasserstein distance and max-sliced Wasserstein distance , is on the new pseudo metric but the paper only argues the contribution from the perspective of its power in generative modeling . I wonder if the authors ever considered evaluating the metric without add-ons like a neural network . Some typical areas of applications of the Wasserstein distances like color transfers , shape interpolation , rigid transformation since it is Gromov Wasserstein , etc ? Or is the evaluation without an AE setup not necessary ? -- AFTER REBUTTAL- I appreciate the authors response . Most of my trivial comments and questions have been resolved . I stand by my initial rating . This is a solid paper . The authors clearly introduce the problem and develop a clear story with a straightforward solution . The paper ends with extensive experiments . My main concern remains : the contributions of the paper to the ML community is moderate because the story is very narrow . I think the idea can be substantially extended to solving the fundamental problems in sliced Wasserstein distances but I do n't object acceptance of the paper in the current form . I recommend the authors incorporate suggestions from all the reviewers and polish the language especially Section 2 to make the paper more accessible for readers outside the sliced Wasserstein community . Thank you .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your time . We have revised our paper based on your comments . The changes are marked in blue color . Question 1 : \u201c Where does von Mises-Fisher distribution come from ? If I understand it correctly , von Mises-Fisher is a variant of Gaussian on hyperspheres. \u201d Answer : Thank you for your comment . There are two main motivations for why we use von Mises-Fisher distribution : First , by adjusting the concentration parameter of vMF distribution , we can interpolate between the uniform distribution on the unit sphere and the Dirac distribution ( cf.the reference [ 0 ] ) . Therefore , it allows us to control distributing weight to the most important direction and other directions based on the geodesic distance on the sphere . Optimizing over the family of vMF distributions helps us to identify where the best direction is as well as how much weight we need to put there in comparison with other less important directions . Second , von Mises Fisher can be viewed as a good alternative to the Gaussian distribution on the sphere . It is well-known that mixtures of Gaussian distributions can approximate any distribution well ( cf.the reference [ 1 ] ) . Therefore , mixtures of von Mises Fisher distributions can be viewed as a good alternative to the uniform distribution on the sphere . Furthermore , by using the finite mixture of von Mises Fisher distribution as the prior distribution on the projections , we can guarantee that the samples drawn from the mixture will concentrate around the centers/ locations of the mixture with high probability . Therefore , optimizing the centers/ locations of the mixture allows us to not only reduce the unimportant samples but also encourage these centers to be sufficiently separate from each other such that the finite mixture can cover the uniform distribution better . [ 0 ] S. Sra . Directional statistics in machine learning : a brief review . arXiv preprint arXiv:1605.00316,2016 . [ 1 ] S. Ghosal and A. van der Vaart . Entropies and rates of convergence for maximum likelihood and Bayes estimation for mixtures of normal densities . Annals of Statistics , 1233\u20131263 , 2001 . Question 2 : \u201c The authors can consider citing existing works or proving some arguments of choosing vMF as an interpolation of a uniform distribution and a Dirac. \u201d Answer : Thank you for your suggestion . We already cited references in the proof of Theorem 2 ( namely , the reference [ 0 ] that we mentioned in our response to your first question ) in the Appendix ( page 12 ) to show the vMF is the interpolation between the uniform distribution and the Dirac distribution . Moreover , in the revised version , we also included Lemma 1 in Appendix D to show that Power Spherical distribution is also an interpolation between the uniform distribution and the Dirac distribution . Question 3 : \u201d Please refine 2 Background . Definition 2 : is Iv actually Id in Cd ( k ) ? Swap SFG and DRAE in the title of 2.1 to consist with the content of 2.1. \u201d Answer : We have fixed these issues in the revised version in blue color . Thank you for your comment . Question 4 : \u201c The introduction of power SSFG at the end of Section 3 seems abrupt . If power SSFG performs better than SSFG , then what is the point of introducing vMF ? \u201d Answer : Thank you for your question . Power spherical distribution is a recently introduced distribution ( cf.the reference ( De Cao & Aziz , 2020 ) . The Power Spherical distribution ) , so its statistical properties are not as well-studied as those of the vMF distribution , which is arguably the most well-known distribution on the unit sphere . Our main motivation for using power spherical distribution is from the computational side as the power spherical distribution has a faster sampling time than vMF distribution in high dimension settings . Furthermore , mixtures of vMF distributions can yield a good coverage of the uniform distribution over the unit sphere while it is unclear whether this property holds for power spherical distribution . Therefore , we think that it is necessary to include both the vMF and power spherical distributions in the paper since each distribution has its own merits in either statistical or computational front ."}, {"review_id": "DiQD7FWL233-2", "review_text": "This paper builds on the work of Xu and colleagues ( 2020 ) on auto encoders ( AE ) with relational regularization . The core idea is to enforce a notion of structure in the latent space of an AE , by measuring the ( composed , structrural ) divergence with a target distribution . In order to do so , a quadratic optimal transport problem is used ( the fused Gromov-Wasserstein -FGW- ) , which has a super-cubical complexity . In order to alleviate this cost , Xu et al.defined a sliced version of it ( akin to sliced Wasserstein -SW- ) , which consists in solving several simple 1D versions of the problem , that admit a close form solution , after projecting onto random directions drawn uniformly on the unit hypersphere . This work proposes to replace this uniform distribution over the sphere by a von Mises-Fisher distribution on the sphere , that is alike a \u2018 Gaussian distribution \u2019 on the sphere , and also a mixture of those distributions . Their parameters are optimized during the training so that it maximises the FGW divergence . This strategy is similar to recent trends in computing SW , that also replace the uniform distribution by either the replacing the expectation by a max or looking for subspaces that maximizes this the expected SW . The paper is clearly written and interesting . It can be seen as incremental with respect to the work of Xu et al. , but the formulation of there sliced optimal transport problem with a parametrized von Mises-Fisher distribution is novel and could also been applied to the original sliced Wasserstein , but maybe also to compute the sliced Fused Gromov Wasserstein for other task such as graph classification , as in the original FGW paper ( see minor comment ) . The experimental results are very good and clearly show the benefits of the method . Yet I have some questions , which answers might be critical for the final evaluation of the paper : - in the mixture part , how do you train for the parameters of the mixture ? Do you have a kind of EM algorithm , or do you perform a gradient ascent ? - in general , directions are drawn randomly for every batches of samples . What is the meaning of fixing in advance the number of projections , as done in Figure 3 and the related experiment ? - it seems that the sliced FGW is computed on mini-batches of samples . While I acknowledge there is a common practice to do so , a 1D FGW on a mini batch is not the same as computing the 1D FGW on the full dataset . As such , this mini batch version of FGW is not the same as computing the true FGW . In the end , i ) the size of the mini-batch might have an impact on the estimation quality , that should be discussed ii ) if computing 1D FGW on mini batches , why not computing and comparing with the mini batch version of the original version of FGW ? From the paper , the batch size is 100 , this would be solved very quickly by current FGW solvers . In the end , I think that using the von Mises Fisher distribution is an interesting and original idea , which might have a broader impact than the sliced FGW . The paper has also the merit to push a little forward the structural regularized AE which is , in the reviewer \u2019 s opinion , a good point . On the negative side , this work can be considered a little bit as incremental , and some questions remain on the experimental part . I am willing to change my rating depending on the answers to my comments . Minor remarks It is interesting to note that another paper in consideration for ICLR is developing a similar strategy in the case of the simple sliced Wasserstein distance : https : //openreview.net/forum ? id=QYjO70ACDK It is also a little bit strange that the original fused Gromov-Wasserstein paper is not credited in the paper : [ 1 ] Titouan , Vayer , et al . `` Optimal Transport for structured data with application on graphs . '' International Conference on Machine Learning . 2019. # # # After author response I thank the authors for their detailed response to my comments . I do not agree with the complexity of FGW being solved in $ n^4 $ which should be more related to $ n^3 $ for the type of distance considered in the paper ( see analysis in [ 1 ] ) , but yet the point is still sensible for considering the minibatch version . My other comments have been adressed , and I am changing my note to the score of 7 .", "rating": "7: Good paper, accept", "reply_text": "We would like to thank you for your comments and suggestions . Below , you can find our responses to your reviews : Question 1 : \u201c In the mixture part , how do you train for the parameters of the mixture ? Do you have a kind of EM algorithm , or do you perform a gradient ascent ? \u201d Answer : In practice , the mixture weights are chosen to be uniform and mixture concentration parameters , i.e. , $ \\kappa_ { 1 } , \u2026 , \\kappa_ { k } $ , are set to be equal to $ \\kappa $ . It is because we want every component of the mixtures of vMF distributions to be treated similarly . In our model , the values of $ k $ , the number of components , and $ \\kappa $ will be tuned while the locations of vMF components will be optimized via the stochastic gradient ascent with the usage of the reparameterization trick . Question 2 : \u201c In general , directions are drawn randomly for every batch of samples . What is the meaning of fixing in advance the number of projections , as done in Figure 3 and the related experiment ? \u201d Answer : Thank you for your comment . For each batch of samples , the directions are drawn randomly . By fixing in advance the number of projections , we mean that the number of projections is the same for each batch of samples . Question 3 : \u201c The size of the mini-batch might have an impact on the estimation quality , that should be discussed. \u201d Answer : In the revised version , we have conducted extra experiments to investigate the effect of minibatch \u2019 s size in Table 2 in Appendix E. We set the size of minibatch $ \\in $ { 10 , 50,100 , 150 } in training DRAE , s-DRAE , ps-DRAE and we also adjust the number of epochs in each case to get the same number of iterations . From these experiments , we observe that the size of minibatch affects strongly the quality of DRAE , s-DRAE , and ps-DRAE . When the size is very small ( = 10 ) , all models perform poorly in both generation and reconstruction . On the other hand , a bigger minibatch size leads to better performance , namely , the FID score and reconstruction score decrease ."}], "0": {"review_id": "DiQD7FWL233-0", "review_text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Summary : The paper presents a novel method to improve the relational regularized autoencoders . The proposed method is based on the new relational discrepancy which is called the spherical sliced fused Gromov Wasserstein ( SSFG ) . It is seen that the SSFG is an extension of the sliced fused Gromov Wasserstein ( SFG ) and its max version . Two variants of the SSFG are also presented . Experiments suggest that the proposed autoencoders outperform some existing autoencoders in terms of generative performance in comparable computational time . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Reasons for score : Overall , I have a positive impression about the paper . I think that the proposed relational discrepancy using the von Mises -- Fisher distribution is a reasonable extension of the SFG using the uniform distribution . My concern is whether the estimation of the tuning parameters of the proposed relational discrepancies could be computationally expensive in practice ( see cons below ) . Hopefully the authors can address my concern in the rebuttal period . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Pros : ( 1 ) Applying the fact the von Mises -- Fisher distribution is an extension of the uniform distribution and the Dirac distribution , the authors successfully presented an extended relational discrepancy of the SFG and its max version . ( 2 ) The mixture spherical sliced fused Gromov Wasserstein ( MSSFG ) achieves an even better flexibility than the SSFG . I reckon that the MSSFG , which adopts a mixture of the von Mises -- Fisher distributions , is a reasonable extension of the SSFG . ( 3 ) The extensive experiments suggest that the proposed autoencoders show satisfactory performance in terms of FID scores and reconstruction losses and are not particularly expensive in terms of computational time . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Cons : ( 1 ) I wonder whether the estimation of the tuning parameters of the proposed relational discrepancies could be computationally expensive in practice . For example , the MSSFG requires the values of $ k $ , $ \\kappa_1 , \\ldots , \\kappa_k , \\alpha_1 , \\ldots , \\alpha_k $ . I fear that the estimation of these tuning parameters requires expensive computational cost . In the experiments of the paper , only limited combinations of these tuning parameters are considered . I wonder whether these combinations really cover a sufficient area of the parameter space . ( 2 ) The von Mises -- Fisher distribution has the property that as the concentration parameter $ \\kappa $ increases , the concentration of the distribution monotonically increases . In particular , the von Mises -- Fisher distribution tends to the Dirac distribution as $ \\kappa $ goes to infinity . However I am not sure this nice property also holds for the power distribution . If not , I wonder whether the power SSFG and power spherical DRAE have sufficient flexibility compared with the SSFG and spherical DRAE . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Questions during rebuttal period : Please address and clarify the cons above . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Typo : ( 1 ) p.1 , abstract , last line : generation , and reconstruction - > generation , and reconstruction # # Updates : The authors have carefully responded to my comments . Their response addresses most of my concerns . I will keep my score high . I understand that the choice of the hyperparameters can be computationally heavy , but the authors have given an idea to solve this problem . It is good to find that the Power Spherical distribution also includes the Dirac distribution as a limiting case .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your time . We have revised our papers based on your comments . The changes are marked in blue color . Question 1 : \u201c I wonder whether the estimation of the tuning parameters of the proposed relational discrepancies could be computationally expensive in practice . For example , the MSSFG requires the values of k , \u03ba1 , \u2026 , \u03bak , \u03b11 , \u2026 , \u03b1k . I fear that the estimation of these tuning parameters requires expensive computational cost . In the experiments of the paper , only limited combinations of these tuning parameters are considered . I wonder whether these combinations really cover a sufficient area of the parameter space. \u201d Answer : In practice , we use uniform weights for { $ \\alpha_ { 1 : k } $ } and the same value $ \\kappa $ for $ \\kappa_ { 1 } , \u2026 , \\kappa_ { k } $ because we would like every component of the mixtures of vMF distributions to be treated similarly . Therefore , we eventually only need to tune for two hyperparameters $ \\kappa $ and $ k $ . In practice , we have searched only for $ \\kappa \\in $ { 1,5,10,50,100 } and $ k \\in $ { 2 , 5 , 10 , 50 } as we observed that the performance of MSSFG when $ \\kappa > 100 $ is rather similar to when $ \\kappa = 100 $ ( similarly , the performance of MSSFG when $ \\kappa < 1 $ is quite similar to when $ \\kappa = 1 $ ) . That 's why we only consider $ \\kappa \\in $ { 1 , 5 , 10 , 50 , 100 } . For the choice of the number of components $ k $ , we observe that $ k \\leq 50 $ is well-suited for the current applications considered in the paper . However , we anticipate that for much larger-scale applications , a bigger $ k $ should be chosen . One possible method for automatically choosing $ k $ without tuning it is by using the idea from Bayesian nonparametrics , namely , we model the projections as Dirichlet process mixture models ( cf . [ 1 ] and [ 2 ] ) where the number of components $ k $ grows automatically at certain rates ( controlled by the hyperparameters in the Dirichlet process ) when the number of projections is increasing . We believe that it is an important direction to explore and we leave this direction for the future work . [ 1 ] T. S. Ferguson . A Bayesian Analysis of Some Nonparametric Problems . Annals of Statistics , 1973 . [ 2 ] C. E. Antoniak . Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems . Annals of Statistics , 1974 . Question 2 : \u201c The von Mises -- Fisher distribution has the property that as the concentration parameter \u03ba increases , the concentration of the distribution monotonically increases . In particular , the von Mises -- Fisher distribution tends to the Dirac distribution as \u03ba goes to infinity . However I am not sure this nice property also holds for the power distribution . If not , I wonder whether the power SSFG and power spherical DRAE have sufficient flexibility compared with the SSFG and spherical DRAE. \u201d Answer : Thank you for your comment . In the revised version , we have proved that the Power Spherical distribution is also the interpolation between the uniform distribution and the Dirac distribution . This result is proved in Lemma 1 in Appendix D. Therefore , the power SSFG and power spherical DRAE are also as flexible as the SSFG and spherical DRAE . Question 3 : \u201c ( 1 ) p.1 , abstract , last line : generation , and reconstruction - > generation , and reconstruction. \u201d Answer : We have fixed these typos in the revised version in blue color ."}, "1": {"review_id": "DiQD7FWL233-1", "review_text": "The paper proposes a new pseudo-distance called the spherical slices fused Gromov Wasserstein distance ( SSFGW ) . It builds on top of the slices fused Gromov Wasserstein distance ( SFGW ) that takes a weighted combination of the Wasserstein distance and the Gromov Wasserstein distance on sliced spaces . The paper tackles the problem of solving for the best sampling directions of slicing . Existing approaches either assume uniform sampling strategies or a single direction that maximizes the discrepancy of two measures . This paper uses von Mises-Fisher measures ( vMF ) as the bridge to combine the advantages of the two . Changing the parameters of vMF is equivalent to ( non-linearly ) interpolating between a Dirac impulse and a uniform distribution , which converges to uniform-sliced Wasserstein and max-sliced Wasserstein , respectively . The authors provided the proof of its pseudo metric properties and upper bound and extended their SSFGW to mixtures of vFW , creating MSSFGW , and applied these variants to deterministic relational regularized autoencoder ( DRAE ) . The results of comparing the DRAE equipped with proposed distance with other DRAEs demonstrate its superiority in stability and generative capacity . Some comments and questions : -- Where does von Mises-Fisher distribution come from ? If I understand it correctly , von Mises-Fisher is a variant of Gaussian on hyperspheres . The authors can consider citing existing works or proving some arguments of choosing vMF as an interpolation of a uniform distribution and a Dirac . -- Please refine 2 Background . Definition 2 : is $ I_v $ actually $ I_d $ in $ C_d ( k ) $ ? `` The vMF distribution provides a way to have concentrated weight on the most important directions and assigns less weight to further directions . Therefore , we gain a better representation of the discrepancy between probability measures . '' Swap SFG and DRAE in the title of 2.1 to consist with the content of 2.1 . -- The introduction of power SSFG at the end of Section 3 seems abrupt . If power SSFG performs better than SSFG , then what is the point of introducing vMF ? Can we build mixtures of power SSFG ? The paper does not mention an mps-DRAE . On page 20 of the Appendix , at the top , it seems power SSFG is superior to SSFG in all desired aspects , but ps-DRAE underperforms s-DRAE , as shown in Table 1 . Any explanation ? -- Empirical evidence of interpolating between a uniform and a Dirac In Figure 12 , abd , the graphs of s-DRAE cross the red baseline with a large momentum when $ k $ decreases to 1 , which suggests that it could go far away from the baseline when $ k \\rightarrow 0 $ because when $ k=1 $ , vFM is still very far from a uniform distribution . Please explain that . -- A Wasserstein paper or a generative modeling paper ? The main contribution of the paper , which tackles the problem of generalizing sliced Wasserstein distance and max-sliced Wasserstein distance , is on the new pseudo metric but the paper only argues the contribution from the perspective of its power in generative modeling . I wonder if the authors ever considered evaluating the metric without add-ons like a neural network . Some typical areas of applications of the Wasserstein distances like color transfers , shape interpolation , rigid transformation since it is Gromov Wasserstein , etc ? Or is the evaluation without an AE setup not necessary ? -- AFTER REBUTTAL- I appreciate the authors response . Most of my trivial comments and questions have been resolved . I stand by my initial rating . This is a solid paper . The authors clearly introduce the problem and develop a clear story with a straightforward solution . The paper ends with extensive experiments . My main concern remains : the contributions of the paper to the ML community is moderate because the story is very narrow . I think the idea can be substantially extended to solving the fundamental problems in sliced Wasserstein distances but I do n't object acceptance of the paper in the current form . I recommend the authors incorporate suggestions from all the reviewers and polish the language especially Section 2 to make the paper more accessible for readers outside the sliced Wasserstein community . Thank you .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your time . We have revised our paper based on your comments . The changes are marked in blue color . Question 1 : \u201c Where does von Mises-Fisher distribution come from ? If I understand it correctly , von Mises-Fisher is a variant of Gaussian on hyperspheres. \u201d Answer : Thank you for your comment . There are two main motivations for why we use von Mises-Fisher distribution : First , by adjusting the concentration parameter of vMF distribution , we can interpolate between the uniform distribution on the unit sphere and the Dirac distribution ( cf.the reference [ 0 ] ) . Therefore , it allows us to control distributing weight to the most important direction and other directions based on the geodesic distance on the sphere . Optimizing over the family of vMF distributions helps us to identify where the best direction is as well as how much weight we need to put there in comparison with other less important directions . Second , von Mises Fisher can be viewed as a good alternative to the Gaussian distribution on the sphere . It is well-known that mixtures of Gaussian distributions can approximate any distribution well ( cf.the reference [ 1 ] ) . Therefore , mixtures of von Mises Fisher distributions can be viewed as a good alternative to the uniform distribution on the sphere . Furthermore , by using the finite mixture of von Mises Fisher distribution as the prior distribution on the projections , we can guarantee that the samples drawn from the mixture will concentrate around the centers/ locations of the mixture with high probability . Therefore , optimizing the centers/ locations of the mixture allows us to not only reduce the unimportant samples but also encourage these centers to be sufficiently separate from each other such that the finite mixture can cover the uniform distribution better . [ 0 ] S. Sra . Directional statistics in machine learning : a brief review . arXiv preprint arXiv:1605.00316,2016 . [ 1 ] S. Ghosal and A. van der Vaart . Entropies and rates of convergence for maximum likelihood and Bayes estimation for mixtures of normal densities . Annals of Statistics , 1233\u20131263 , 2001 . Question 2 : \u201c The authors can consider citing existing works or proving some arguments of choosing vMF as an interpolation of a uniform distribution and a Dirac. \u201d Answer : Thank you for your suggestion . We already cited references in the proof of Theorem 2 ( namely , the reference [ 0 ] that we mentioned in our response to your first question ) in the Appendix ( page 12 ) to show the vMF is the interpolation between the uniform distribution and the Dirac distribution . Moreover , in the revised version , we also included Lemma 1 in Appendix D to show that Power Spherical distribution is also an interpolation between the uniform distribution and the Dirac distribution . Question 3 : \u201d Please refine 2 Background . Definition 2 : is Iv actually Id in Cd ( k ) ? Swap SFG and DRAE in the title of 2.1 to consist with the content of 2.1. \u201d Answer : We have fixed these issues in the revised version in blue color . Thank you for your comment . Question 4 : \u201c The introduction of power SSFG at the end of Section 3 seems abrupt . If power SSFG performs better than SSFG , then what is the point of introducing vMF ? \u201d Answer : Thank you for your question . Power spherical distribution is a recently introduced distribution ( cf.the reference ( De Cao & Aziz , 2020 ) . The Power Spherical distribution ) , so its statistical properties are not as well-studied as those of the vMF distribution , which is arguably the most well-known distribution on the unit sphere . Our main motivation for using power spherical distribution is from the computational side as the power spherical distribution has a faster sampling time than vMF distribution in high dimension settings . Furthermore , mixtures of vMF distributions can yield a good coverage of the uniform distribution over the unit sphere while it is unclear whether this property holds for power spherical distribution . Therefore , we think that it is necessary to include both the vMF and power spherical distributions in the paper since each distribution has its own merits in either statistical or computational front ."}, "2": {"review_id": "DiQD7FWL233-2", "review_text": "This paper builds on the work of Xu and colleagues ( 2020 ) on auto encoders ( AE ) with relational regularization . The core idea is to enforce a notion of structure in the latent space of an AE , by measuring the ( composed , structrural ) divergence with a target distribution . In order to do so , a quadratic optimal transport problem is used ( the fused Gromov-Wasserstein -FGW- ) , which has a super-cubical complexity . In order to alleviate this cost , Xu et al.defined a sliced version of it ( akin to sliced Wasserstein -SW- ) , which consists in solving several simple 1D versions of the problem , that admit a close form solution , after projecting onto random directions drawn uniformly on the unit hypersphere . This work proposes to replace this uniform distribution over the sphere by a von Mises-Fisher distribution on the sphere , that is alike a \u2018 Gaussian distribution \u2019 on the sphere , and also a mixture of those distributions . Their parameters are optimized during the training so that it maximises the FGW divergence . This strategy is similar to recent trends in computing SW , that also replace the uniform distribution by either the replacing the expectation by a max or looking for subspaces that maximizes this the expected SW . The paper is clearly written and interesting . It can be seen as incremental with respect to the work of Xu et al. , but the formulation of there sliced optimal transport problem with a parametrized von Mises-Fisher distribution is novel and could also been applied to the original sliced Wasserstein , but maybe also to compute the sliced Fused Gromov Wasserstein for other task such as graph classification , as in the original FGW paper ( see minor comment ) . The experimental results are very good and clearly show the benefits of the method . Yet I have some questions , which answers might be critical for the final evaluation of the paper : - in the mixture part , how do you train for the parameters of the mixture ? Do you have a kind of EM algorithm , or do you perform a gradient ascent ? - in general , directions are drawn randomly for every batches of samples . What is the meaning of fixing in advance the number of projections , as done in Figure 3 and the related experiment ? - it seems that the sliced FGW is computed on mini-batches of samples . While I acknowledge there is a common practice to do so , a 1D FGW on a mini batch is not the same as computing the 1D FGW on the full dataset . As such , this mini batch version of FGW is not the same as computing the true FGW . In the end , i ) the size of the mini-batch might have an impact on the estimation quality , that should be discussed ii ) if computing 1D FGW on mini batches , why not computing and comparing with the mini batch version of the original version of FGW ? From the paper , the batch size is 100 , this would be solved very quickly by current FGW solvers . In the end , I think that using the von Mises Fisher distribution is an interesting and original idea , which might have a broader impact than the sliced FGW . The paper has also the merit to push a little forward the structural regularized AE which is , in the reviewer \u2019 s opinion , a good point . On the negative side , this work can be considered a little bit as incremental , and some questions remain on the experimental part . I am willing to change my rating depending on the answers to my comments . Minor remarks It is interesting to note that another paper in consideration for ICLR is developing a similar strategy in the case of the simple sliced Wasserstein distance : https : //openreview.net/forum ? id=QYjO70ACDK It is also a little bit strange that the original fused Gromov-Wasserstein paper is not credited in the paper : [ 1 ] Titouan , Vayer , et al . `` Optimal Transport for structured data with application on graphs . '' International Conference on Machine Learning . 2019. # # # After author response I thank the authors for their detailed response to my comments . I do not agree with the complexity of FGW being solved in $ n^4 $ which should be more related to $ n^3 $ for the type of distance considered in the paper ( see analysis in [ 1 ] ) , but yet the point is still sensible for considering the minibatch version . My other comments have been adressed , and I am changing my note to the score of 7 .", "rating": "7: Good paper, accept", "reply_text": "We would like to thank you for your comments and suggestions . Below , you can find our responses to your reviews : Question 1 : \u201c In the mixture part , how do you train for the parameters of the mixture ? Do you have a kind of EM algorithm , or do you perform a gradient ascent ? \u201d Answer : In practice , the mixture weights are chosen to be uniform and mixture concentration parameters , i.e. , $ \\kappa_ { 1 } , \u2026 , \\kappa_ { k } $ , are set to be equal to $ \\kappa $ . It is because we want every component of the mixtures of vMF distributions to be treated similarly . In our model , the values of $ k $ , the number of components , and $ \\kappa $ will be tuned while the locations of vMF components will be optimized via the stochastic gradient ascent with the usage of the reparameterization trick . Question 2 : \u201c In general , directions are drawn randomly for every batch of samples . What is the meaning of fixing in advance the number of projections , as done in Figure 3 and the related experiment ? \u201d Answer : Thank you for your comment . For each batch of samples , the directions are drawn randomly . By fixing in advance the number of projections , we mean that the number of projections is the same for each batch of samples . Question 3 : \u201c The size of the mini-batch might have an impact on the estimation quality , that should be discussed. \u201d Answer : In the revised version , we have conducted extra experiments to investigate the effect of minibatch \u2019 s size in Table 2 in Appendix E. We set the size of minibatch $ \\in $ { 10 , 50,100 , 150 } in training DRAE , s-DRAE , ps-DRAE and we also adjust the number of epochs in each case to get the same number of iterations . From these experiments , we observe that the size of minibatch affects strongly the quality of DRAE , s-DRAE , and ps-DRAE . When the size is very small ( = 10 ) , all models perform poorly in both generation and reconstruction . On the other hand , a bigger minibatch size leads to better performance , namely , the FID score and reconstruction score decrease ."}}