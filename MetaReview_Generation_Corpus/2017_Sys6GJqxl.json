{"year": "2017", "forum": "Sys6GJqxl", "title": "Delving into Transferable Adversarial Examples and Black-box Attacks", "decision": "Accept (Poster)", "meta_review": "The paper is the first to demonstrate that it is possible for an adversary to change the label that a convolutional network predicts for an image to a specific value. Like Papernot et al., it presents a successful attack on Clarifai's image-recognition system. I encourage the authors to condense the paper to its key results (13 pages without / 24 pages with supplemental material is too long for a conference paper).", "reviews": [{"review_id": "Sys6GJqxl-0", "review_text": "The paper presents an interesting and very detailed study of targeted and non-targeted adversarial examples in CNNs. I\u2019m on the fence about this paper but am leaning towards acceptance. Such detailed empirical explorations are difficult and time-consuming to construct yet can serve as important stepping stones for future work. I see the length of the paper as a strength since it allows for a very in-depth look into the effectiveness and transferability of different kinds of adversarial examples. There are, however, some concerns: 1) While the length of the paper is a strength in my mind, the key contributions should be made much more clear. As evidenced by my comment earlier, I got confused at some point between the ensemble/non-ensemble method, and about the contribution of the Clarifai evaluation and what I should be focusing on where. I\u2019d strongly suggest a radical revision which more clearly focuses the story: - First, we demonstrate that non-targeted attacks are easy while targeted attacks are hard (evidenced by a key experiment comparing the two; we refer to appendix or later sections for the extensive exploration of e.g., current Section 3) - Thus, we propose an ensemble method that is able to handle targeted attacks much better (evidenced by experiments focusing on the comparison between ensemble and non-ensemble method, both in a controlled setting and on Clarifai) - Also, here are all the other details and explorations. 2) Instead of using ResNet-152, Res-Net-101 and ResNet-50 as three of the five models, it would've been better to use one ResNet architecture and the other two, say, AlexNet and Network-in-Network. This would make the ensemble results a lot more compelling.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the suggestions . In fact , we have added a contribution section to highlight the most important take-aways . We will rework the story part of the paper as suggested to make the messages come across . We choose to report the results for three ResNet models , because these results show the transferability between models using the same architecture but with different hyper-parameters . As an in-depth study , we believe both same-architecture transferability and cross-architecture transferability are meaningful to investigate . In fact , we indeed make some interesting unexpected and expected observations : 1 ) using single-model approaches , transferability between ResNet models is not significantly better than other models . For example , VGG-16 \u2019 s transferability to ResNet-50 is better than both ResNet-101 \u2019 s and ResNet-152 \u2019 s ( Table 2 ) ; 2 ) at the same time , we found that ResNet models are more frequently making the same mistakes than others ( Table 3 ) . There are more findings , and we will highlight them in the next revision . For AlexNet and NIN , we had technical issues , i.e. , we can not choose 100 images from the testset that all models make correct predictions for them . We can mitigate this issue by omitting the model with the lowest test accuracy , i.e. , AlexNet , choosing a smaller test set , or choosing test images not only from the ILSVRC validation dataset but also from its training dataset . However , any of these choices will require a re-run of all experiments . We will work on this , but given the expected time to be consumed , we are afraid that we may not have time to update the paper to reflect these results before Jan 19 , 2017 ."}, {"review_id": "Sys6GJqxl-1", "review_text": "This paper present an experimental study of the robustness of state-of-the-art CNNs to different types of \"attacks\" in the context of image classication. Specifically, an attack aims to fool the classification system with a specially corrupted image, i.e. making it misclassify the image as (1) any wrong class (non-targeted attack) or (2) a target class, chosen in advance by the attacker (targeted attack). For instance, the attacker could corrupt an image of an ostrich in such a way that it would be classified as a megalith. Even though the attacker's agenda is not so clear in this example, it is still interesting to study the weaknesses of current systems in view of (1) improving them in general and (2) actual risks with e.g. autonomous vehicles. The paper is mostly experimental. In short, it compares different strategies (already published in previous papers) for all popular networks (VGG, GoogLeNet, ResNet-50/101/152) and the two aforementionned types of attacks. The experiments are well conducted and clearly exposed. A convincing point is that attacks are also conducted on \"clarifai.com\" which is a black-box classification system. Some analysis and insightful explanations are also provided to help understanding why CNNs are prone to such attacks (Section 6). To sum up, the main findings are that non-targeted attacks are easy to perform, even on a black-box system. Non-targeted attacks are more difficult to realize with existing schemes, but the authors propose a new approach for that that vastly improves over existing attacks (even though it's still far from perfect: ~20% success rate on clarifai.com versus 2% with previous schemes). Arguably, The paper still has some weaknesses: - The authors are treating the 3 ResNet-based networks as different, yet they are obviously clearly correlated. See Table 7 for instance. This is naturally expected because their architecture is similar (only their depth varies). Hence, it does not sound very fair to state that \"One interesting finding is that [...] the first misclassified label (non-targeted) is the same for all models except VGG-16 and GoogLeNet.\", i.e., the three ResNet-based networks. - A subjective measure is employed to evaluate the effectiveness of the attacks on the black box system. While this is for a good reason (clarifai.com returns image labels that are different from ImageNet), it is not certain that the reported numbers are fair (even though the qualitative results look convincing). - The novelty of the proposed approach (optimizing an ensemble of network instead of a single network) is limited. However, this was not really the point of the paper, and it is effective, so it seems ok overall. - The paper is quite long. This is expected because it is an extensive evaluation study, but still. I suggest the authors prune some near-duplicate content (e.g. Section 2.3 has a high overlap with Section 1, etc.). - The paper would benefit from additional discussions with the recent and related work of Fawzi et al (NIPS'16) in Section 6. Indeed the work of Fawzi et al. is mostly theoretical and well aligned with the experimental findings and observations (in particular in Section 6). To conclude, I think that this paper is somewhat useful for the community and could help to further improve existing architectures, as well as better assess their flaws and weaknesses.", "rating": "7: Good paper, accept", "reply_text": "Specifically , we will revise the observations for the decision boundary in Section 6 ; do our best to remove redundant information ; and cite Fawzi et al ( 2016 ) properly in Section 6 ."}, {"review_id": "Sys6GJqxl-2", "review_text": "I reviewed the manuscript as of December 7th. Summary: The authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label. The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples. Major Comments: 1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed. 2. The section of geometric understanding is similar to results of 'Adversarial Perturbations of Deep Neural Networks' in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them. 3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult. 4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016). As far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label. The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work. Areas to Trim the Paper: - Table 1 is not necessary. Just cite other results or write the Top-1 numbers in the text. - Condense Section 2.2.1 and cite heavily. - Figure 2 panels may be overlaid to highlight a comparison. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the suggestions to shrink the paper . Before we give a full response , we would like to ask a quick clarification question . The reviewer mentioned the paper , 'Adversarial Perturbations of Deep Neural Networks ' in Warde-Farley and Goodfellow ( 2015 ) , and commented that our geometric understanding section is similar to this paper . However , we tried our best , but could not find this paper . Can we ask the reviewer to provide the full citation of this referred paper , so that we can adjust our paper accordingly ?"}], "0": {"review_id": "Sys6GJqxl-0", "review_text": "The paper presents an interesting and very detailed study of targeted and non-targeted adversarial examples in CNNs. I\u2019m on the fence about this paper but am leaning towards acceptance. Such detailed empirical explorations are difficult and time-consuming to construct yet can serve as important stepping stones for future work. I see the length of the paper as a strength since it allows for a very in-depth look into the effectiveness and transferability of different kinds of adversarial examples. There are, however, some concerns: 1) While the length of the paper is a strength in my mind, the key contributions should be made much more clear. As evidenced by my comment earlier, I got confused at some point between the ensemble/non-ensemble method, and about the contribution of the Clarifai evaluation and what I should be focusing on where. I\u2019d strongly suggest a radical revision which more clearly focuses the story: - First, we demonstrate that non-targeted attacks are easy while targeted attacks are hard (evidenced by a key experiment comparing the two; we refer to appendix or later sections for the extensive exploration of e.g., current Section 3) - Thus, we propose an ensemble method that is able to handle targeted attacks much better (evidenced by experiments focusing on the comparison between ensemble and non-ensemble method, both in a controlled setting and on Clarifai) - Also, here are all the other details and explorations. 2) Instead of using ResNet-152, Res-Net-101 and ResNet-50 as three of the five models, it would've been better to use one ResNet architecture and the other two, say, AlexNet and Network-in-Network. This would make the ensemble results a lot more compelling.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the suggestions . In fact , we have added a contribution section to highlight the most important take-aways . We will rework the story part of the paper as suggested to make the messages come across . We choose to report the results for three ResNet models , because these results show the transferability between models using the same architecture but with different hyper-parameters . As an in-depth study , we believe both same-architecture transferability and cross-architecture transferability are meaningful to investigate . In fact , we indeed make some interesting unexpected and expected observations : 1 ) using single-model approaches , transferability between ResNet models is not significantly better than other models . For example , VGG-16 \u2019 s transferability to ResNet-50 is better than both ResNet-101 \u2019 s and ResNet-152 \u2019 s ( Table 2 ) ; 2 ) at the same time , we found that ResNet models are more frequently making the same mistakes than others ( Table 3 ) . There are more findings , and we will highlight them in the next revision . For AlexNet and NIN , we had technical issues , i.e. , we can not choose 100 images from the testset that all models make correct predictions for them . We can mitigate this issue by omitting the model with the lowest test accuracy , i.e. , AlexNet , choosing a smaller test set , or choosing test images not only from the ILSVRC validation dataset but also from its training dataset . However , any of these choices will require a re-run of all experiments . We will work on this , but given the expected time to be consumed , we are afraid that we may not have time to update the paper to reflect these results before Jan 19 , 2017 ."}, "1": {"review_id": "Sys6GJqxl-1", "review_text": "This paper present an experimental study of the robustness of state-of-the-art CNNs to different types of \"attacks\" in the context of image classication. Specifically, an attack aims to fool the classification system with a specially corrupted image, i.e. making it misclassify the image as (1) any wrong class (non-targeted attack) or (2) a target class, chosen in advance by the attacker (targeted attack). For instance, the attacker could corrupt an image of an ostrich in such a way that it would be classified as a megalith. Even though the attacker's agenda is not so clear in this example, it is still interesting to study the weaknesses of current systems in view of (1) improving them in general and (2) actual risks with e.g. autonomous vehicles. The paper is mostly experimental. In short, it compares different strategies (already published in previous papers) for all popular networks (VGG, GoogLeNet, ResNet-50/101/152) and the two aforementionned types of attacks. The experiments are well conducted and clearly exposed. A convincing point is that attacks are also conducted on \"clarifai.com\" which is a black-box classification system. Some analysis and insightful explanations are also provided to help understanding why CNNs are prone to such attacks (Section 6). To sum up, the main findings are that non-targeted attacks are easy to perform, even on a black-box system. Non-targeted attacks are more difficult to realize with existing schemes, but the authors propose a new approach for that that vastly improves over existing attacks (even though it's still far from perfect: ~20% success rate on clarifai.com versus 2% with previous schemes). Arguably, The paper still has some weaknesses: - The authors are treating the 3 ResNet-based networks as different, yet they are obviously clearly correlated. See Table 7 for instance. This is naturally expected because their architecture is similar (only their depth varies). Hence, it does not sound very fair to state that \"One interesting finding is that [...] the first misclassified label (non-targeted) is the same for all models except VGG-16 and GoogLeNet.\", i.e., the three ResNet-based networks. - A subjective measure is employed to evaluate the effectiveness of the attacks on the black box system. While this is for a good reason (clarifai.com returns image labels that are different from ImageNet), it is not certain that the reported numbers are fair (even though the qualitative results look convincing). - The novelty of the proposed approach (optimizing an ensemble of network instead of a single network) is limited. However, this was not really the point of the paper, and it is effective, so it seems ok overall. - The paper is quite long. This is expected because it is an extensive evaluation study, but still. I suggest the authors prune some near-duplicate content (e.g. Section 2.3 has a high overlap with Section 1, etc.). - The paper would benefit from additional discussions with the recent and related work of Fawzi et al (NIPS'16) in Section 6. Indeed the work of Fawzi et al. is mostly theoretical and well aligned with the experimental findings and observations (in particular in Section 6). To conclude, I think that this paper is somewhat useful for the community and could help to further improve existing architectures, as well as better assess their flaws and weaknesses.", "rating": "7: Good paper, accept", "reply_text": "Specifically , we will revise the observations for the decision boundary in Section 6 ; do our best to remove redundant information ; and cite Fawzi et al ( 2016 ) properly in Section 6 ."}, "2": {"review_id": "Sys6GJqxl-2", "review_text": "I reviewed the manuscript as of December 7th. Summary: The authors investigate the transferability of adversarial examples in deep networks. The authors confirm that transferability exists even in large models but demonstrate that it is difficult to manipulate the network to adversarially perturb an image into a specifically desired label. The authors additionally demonstrate real world attacks on a vision web service and explore the geometric properties of adversarial examples. Major Comments: 1. The paper contains a list of many results and it is not clear what single message this paper provides. As mentioned in the comments, this paper is effectively 15 pages and 9 page of results in the Appendix heavily discussed throughout the main body of the paper. Although there is no strict page limit for this conference, I do feel this pushes the spirit of a conference publication. I do not rule out this paper for acceptance based on the length but I do hold it as a negative because clarity of presentation is an important quality. If this paper is ultimately accepted, I would suggest that the authors make some effort to cut down the length even further beyond the 13 pages posted elsewhere. I have marked some sections to highlight areas that may be trimmed. 2. The section of geometric understanding is similar to results of 'Adversarial Perturbations of Deep Neural Networks' in Warde-Farley and Goodfellow (2015). See Figure 1.2. I am not clear what the authors show above-and-beyond these results. If there are additional findings, the authors should emphasize them. 3. The authors expand on observations by Goodfellow et al (2014) and Szegedy et al (2013) demonstrating that large-scale models are susceptible to adversarial perturbations (see also Kurakin et al (2016)). The authors additionally demonstrate that attempting to perform adversarial manipulation to convert an image to a particular, desired label is more difficult. 4. The authors demonstrate that they can target a real-world vision API. These results are compelling but it is not clear what these results demonstrate above-and-beyond Papernot et al (2016). As far I can understand, I think that the most interesting result from this paper not previously described in the literature is to note about the unique difficulty about performing adversarial manipulation to convert an image to a particular, desired label. The rest of the results appear to expand on other results that have already appeared in the literature and the authors need to better explain what these makes these results unique above-and-beyond previous work. Areas to Trim the Paper: - Table 1 is not necessary. Just cite other results or write the Top-1 numbers in the text. - Condense Section 2.2.1 and cite heavily. - Figure 2 panels may be overlaid to highlight a comparison. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for the suggestions to shrink the paper . Before we give a full response , we would like to ask a quick clarification question . The reviewer mentioned the paper , 'Adversarial Perturbations of Deep Neural Networks ' in Warde-Farley and Goodfellow ( 2015 ) , and commented that our geometric understanding section is similar to this paper . However , we tried our best , but could not find this paper . Can we ask the reviewer to provide the full citation of this referred paper , so that we can adjust our paper accordingly ?"}}