{"year": "2017", "forum": "ByToKu9ll", "title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "decision": "Reject", "meta_review": "The paper investigates several retraining approached based upon adversarial data. While the experimental evaluation looks reasonable, the actual contribution of this paper is quite small. The approaches being evaluated, for the most part, are already proposed in the literature, with the one exception being the \"improved autoencoder stacked with classifier\" (IAEC), which is really just a minor modification to the existing AEC approach with an additional regularization term. The results are fairly thorough, and seem to suggest that the IAEC method performs best in some cases, but this is definitely not a novel enough contribution to warrant publication at ICLR.\n \n Pros:\n + Nice empirical evaluation of several adversarial retraining methods\n \n Cons:\n - Extremely minor algorithmic advances\n - Not clear what is the significant contribution of the paper", "reviews": [{"review_id": "ByToKu9ll-0", "review_text": "This paper performs a series of experiments to systematically evaluate the robustness of several defense methods, including RAD, AEC and its improved version etc.. It provides interesting observations. Overall, RAD and distillation have the best performances, but none of the methods can really resist the 'additional' attack from cg or adam. Since it is an experimental paper, my main concern is about its clarity. See the comments below for details. Pros: 1. This paper provides a good comparison of the performances for the selected methods. 2. Section 3.3 (the 'additional' attack) is a interesting investigation. Although the final result about the defense methods is negative, its results are still inspiring. 3. Overall, this paper provides interesting and inspiring experimental results about the selected methods. Cons: 1. There are several other methods in the literature that are missing from the paper. For example the defense methods and the attack methods in the papers [1,2]. 2. Although a long list of experimental results are provided in the paper, many details are skipped. For example, details of the experiments that generate the results in Table 5. 3. Without further explanations and analyses about the experimental results, the contribution of the paper seems limited. 4. This paper proposed an improved version of the AEC algorithm. But its experimental results seems not promising. Minor comments: Page 3: Equation (3) is also non-convex. So the non-convexity of Equation (2) should not be the motivation of Equation (3). [1] https://arxiv.org/abs/1507.00677 [2] https://arxiv.org/abs/1511.03034", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review and suggestions ! As for attack and defense methods evaluated in our paper , we are not intended to cover all proposed methods in our experiments ; instead , we select several representative methods . We will add more discussion about different proposed methods and provide more comparisons in our revision . In particular , as for the references you mentioned , Miyato et al . ( [ 1 ] ) is already mentioned in paragraph 2 of the Introduction in our paper , and we will add the discussion of Huang et al . ( [ 2 ] ) .We will provide more experimental details for the results . Meanwhile , we will polish the paper to provide further analysis of experimental results and highlight our contributions . The improved version of AEC actually performs much better than AEC ( see Table 1 ) . Our goal of proposing IAEC is to provide the strongest version of this strategy for later comparisons with other defense methods . We will address the minor comment in our revision ."}, {"review_id": "ByToKu9ll-1", "review_text": "The paper compares several defense mechanisms against adversarial attacks: retraining, two kinds of autoencoders and distillation with the conclusion that the retraining methodology proposed by Li et al. works best of those approaches. The paper documents a series of experiments on making models robust against adversarial examples. The methods proposed here are not all too original, RAD was proposed by Li et al, distillation was proposed in Goodfellow et al's \"Explaining and harnessing adversarial examples\", stacked autoencoders were proposed by Szegedy et al's \"Intriguing Properties of Neural Networks\". The most original part of the paper is the improved version of autoencoders proposed in this paper. The paper establishes experimental evidence that the RAD framework provides the best defense mechanism against adversarial attacks which makes the introduction of the improved autoencoder mechanism less appealing. Although the paper establishes interesting measurement points and therefore it has the potential for being cited as a reference, its relative lack of originality decreases its significance.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and suggestions ! The \u201c Adversarial retraining framework \u201d has been proposed but no work has been done to evaluate its efficiency in general or provide insights about its robustness and weaknesses . Therefore , to fill up this gap , we propose this comparison work to offer a general reference for how and when to use adversarial retraining framework and what to expect . Our contributions here are trying to evaluate the retraining framework on adversarial examples generated using different attack methods . The evaluation of cross-model efficiency ( Section 3.2 ) and additional attacks ( Section 3.3 ) are also our contributions . We will revise our paper to make our points clearer ."}, {"review_id": "ByToKu9ll-2", "review_text": "I reviewed the manuscript as of December 6th. The authors perform a systematic investigation of various retraining methods for making a classification network robust to adversarial examples. The authors achieve lower error rates using their RAD and IAEC methods perform better then previously introduced distillation methods for retraining networks to be robust to adversarial examples. This method suggests a promising direction for building a defense for adversarial examples. Major Comments: I find the paper to not be lacking in exposition and clarity. The paper has a laundry list of related results (page 2) but no clear message. I *think* one larger point is the superior performance of their retraining techniques but it is not clear how well these techniques perform compared to other retraining techniques, nor are the details of the retraining techniques clear. The paper requires more discussion and a clear exposition about the methods the authors introduced (i.e. RAD, IAEC). What follow are some more detailed comments along this theme of improving the exposition and clarity: - The authors should provide more details about how they constructed the auto-encoder in the IAEC method (diagram?). The same needs to be said for the RAD method. The authors point to a previous workshop submission (https://arxiv.org/abs/1604.02606) but the authors need more discussion about what this method entails and how it compares to other retraining methods (e.g. [1,2]) since this is the first peer-review of this work. - Section 3.2 indicates that the authors are concerned with 'cross-model' efficiency but it is not clear from the text what this means. Are the author exploring the phenomenon of retraining off one algorithm and then evaluating adversarial images derived on another? Or, are the authors examining how examples derived from one instance of a trained model may fool or trick a second instance of a model? The latter point is quite important because this points towards examples and retraining procedures that can generalize across the class of all models. - How do RAD compare with basic retraining methods described in [1, 2]? Since the main contribution of this paper seems to be evaluating the efficacy of RAD, AEC and IAEC, I would suggest that the authors provide more discussion and exposition. - Why are the authors measuring 'recall' (https://en.wikipedia.org/wiki/Precision_and_recall) in Section 3.1? What does recall mean in this context? I would expect the authors to measure something more like the error rate of the classifier after employing the retraining procedure. This needs to be clarified in the manuscript. [1] https://arxiv.org/abs/1412.6572 [2] https://arxiv.org/abs/1611.01236", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review and suggestions ! We will add more details about the experimental settings . The \u201c cross-model \u201d here means the first point as you mentioned , i.e. , add instances crafted using one adversarial example generation algorithm and then evaluate adversarial images derived by another . Actually , we aim to evaluate the ability of defender here to check if the defender can defend against various adversarial models if he applies a different algorithm to generate the retraining instances and improve the classifier . This provides information about how retraining framework can generalize across different adversarial models . However , the second point mentioned by the reviewer is from the attacker \u2019 s perspective and hope to check if the attacker can generalize their attacks against different defensive models . This is one interesting point but not the scope of this paper here , and in fact such work on transferability of attacks have been discussed in Introduction as reference [ Papernot et al. , 2016a ; b ] . We will also add the discussion of work [ 1 ] . We didn \u2019 t discuss work [ 2 ] when we wrote the paper , since it is another submission for this conference at the same time . Also , work [ 2 ] aims to solve the scalability issue of adversarial training , which is not the main topic of our paper . We aim to test the efficacy of different adversarial example generation algorithms here , but we are happy to discuss this work in our revision . The \u201c recall \u201d here means the true positive rate . Since we focus on the binary classification problem and only allow the malicious ( positive ) instances to evade the classifier as benign ( negative ) ones , we want to check how many of these modified instances are correctly classified after retraining . Meanwhile , we have reported the classification error for all the experimental comparisons as suggested , the classification errors corresponding to experimental settings of Figure 1 and Figure 2 are reported in Table 1 and Table 2 respectively ."}], "0": {"review_id": "ByToKu9ll-0", "review_text": "This paper performs a series of experiments to systematically evaluate the robustness of several defense methods, including RAD, AEC and its improved version etc.. It provides interesting observations. Overall, RAD and distillation have the best performances, but none of the methods can really resist the 'additional' attack from cg or adam. Since it is an experimental paper, my main concern is about its clarity. See the comments below for details. Pros: 1. This paper provides a good comparison of the performances for the selected methods. 2. Section 3.3 (the 'additional' attack) is a interesting investigation. Although the final result about the defense methods is negative, its results are still inspiring. 3. Overall, this paper provides interesting and inspiring experimental results about the selected methods. Cons: 1. There are several other methods in the literature that are missing from the paper. For example the defense methods and the attack methods in the papers [1,2]. 2. Although a long list of experimental results are provided in the paper, many details are skipped. For example, details of the experiments that generate the results in Table 5. 3. Without further explanations and analyses about the experimental results, the contribution of the paper seems limited. 4. This paper proposed an improved version of the AEC algorithm. But its experimental results seems not promising. Minor comments: Page 3: Equation (3) is also non-convex. So the non-convexity of Equation (2) should not be the motivation of Equation (3). [1] https://arxiv.org/abs/1507.00677 [2] https://arxiv.org/abs/1511.03034", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review and suggestions ! As for attack and defense methods evaluated in our paper , we are not intended to cover all proposed methods in our experiments ; instead , we select several representative methods . We will add more discussion about different proposed methods and provide more comparisons in our revision . In particular , as for the references you mentioned , Miyato et al . ( [ 1 ] ) is already mentioned in paragraph 2 of the Introduction in our paper , and we will add the discussion of Huang et al . ( [ 2 ] ) .We will provide more experimental details for the results . Meanwhile , we will polish the paper to provide further analysis of experimental results and highlight our contributions . The improved version of AEC actually performs much better than AEC ( see Table 1 ) . Our goal of proposing IAEC is to provide the strongest version of this strategy for later comparisons with other defense methods . We will address the minor comment in our revision ."}, "1": {"review_id": "ByToKu9ll-1", "review_text": "The paper compares several defense mechanisms against adversarial attacks: retraining, two kinds of autoencoders and distillation with the conclusion that the retraining methodology proposed by Li et al. works best of those approaches. The paper documents a series of experiments on making models robust against adversarial examples. The methods proposed here are not all too original, RAD was proposed by Li et al, distillation was proposed in Goodfellow et al's \"Explaining and harnessing adversarial examples\", stacked autoencoders were proposed by Szegedy et al's \"Intriguing Properties of Neural Networks\". The most original part of the paper is the improved version of autoencoders proposed in this paper. The paper establishes experimental evidence that the RAD framework provides the best defense mechanism against adversarial attacks which makes the introduction of the improved autoencoder mechanism less appealing. Although the paper establishes interesting measurement points and therefore it has the potential for being cited as a reference, its relative lack of originality decreases its significance.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review and suggestions ! The \u201c Adversarial retraining framework \u201d has been proposed but no work has been done to evaluate its efficiency in general or provide insights about its robustness and weaknesses . Therefore , to fill up this gap , we propose this comparison work to offer a general reference for how and when to use adversarial retraining framework and what to expect . Our contributions here are trying to evaluate the retraining framework on adversarial examples generated using different attack methods . The evaluation of cross-model efficiency ( Section 3.2 ) and additional attacks ( Section 3.3 ) are also our contributions . We will revise our paper to make our points clearer ."}, "2": {"review_id": "ByToKu9ll-2", "review_text": "I reviewed the manuscript as of December 6th. The authors perform a systematic investigation of various retraining methods for making a classification network robust to adversarial examples. The authors achieve lower error rates using their RAD and IAEC methods perform better then previously introduced distillation methods for retraining networks to be robust to adversarial examples. This method suggests a promising direction for building a defense for adversarial examples. Major Comments: I find the paper to not be lacking in exposition and clarity. The paper has a laundry list of related results (page 2) but no clear message. I *think* one larger point is the superior performance of their retraining techniques but it is not clear how well these techniques perform compared to other retraining techniques, nor are the details of the retraining techniques clear. The paper requires more discussion and a clear exposition about the methods the authors introduced (i.e. RAD, IAEC). What follow are some more detailed comments along this theme of improving the exposition and clarity: - The authors should provide more details about how they constructed the auto-encoder in the IAEC method (diagram?). The same needs to be said for the RAD method. The authors point to a previous workshop submission (https://arxiv.org/abs/1604.02606) but the authors need more discussion about what this method entails and how it compares to other retraining methods (e.g. [1,2]) since this is the first peer-review of this work. - Section 3.2 indicates that the authors are concerned with 'cross-model' efficiency but it is not clear from the text what this means. Are the author exploring the phenomenon of retraining off one algorithm and then evaluating adversarial images derived on another? Or, are the authors examining how examples derived from one instance of a trained model may fool or trick a second instance of a model? The latter point is quite important because this points towards examples and retraining procedures that can generalize across the class of all models. - How do RAD compare with basic retraining methods described in [1, 2]? Since the main contribution of this paper seems to be evaluating the efficacy of RAD, AEC and IAEC, I would suggest that the authors provide more discussion and exposition. - Why are the authors measuring 'recall' (https://en.wikipedia.org/wiki/Precision_and_recall) in Section 3.1? What does recall mean in this context? I would expect the authors to measure something more like the error rate of the classifier after employing the retraining procedure. This needs to be clarified in the manuscript. [1] https://arxiv.org/abs/1412.6572 [2] https://arxiv.org/abs/1611.01236", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review and suggestions ! We will add more details about the experimental settings . The \u201c cross-model \u201d here means the first point as you mentioned , i.e. , add instances crafted using one adversarial example generation algorithm and then evaluate adversarial images derived by another . Actually , we aim to evaluate the ability of defender here to check if the defender can defend against various adversarial models if he applies a different algorithm to generate the retraining instances and improve the classifier . This provides information about how retraining framework can generalize across different adversarial models . However , the second point mentioned by the reviewer is from the attacker \u2019 s perspective and hope to check if the attacker can generalize their attacks against different defensive models . This is one interesting point but not the scope of this paper here , and in fact such work on transferability of attacks have been discussed in Introduction as reference [ Papernot et al. , 2016a ; b ] . We will also add the discussion of work [ 1 ] . We didn \u2019 t discuss work [ 2 ] when we wrote the paper , since it is another submission for this conference at the same time . Also , work [ 2 ] aims to solve the scalability issue of adversarial training , which is not the main topic of our paper . We aim to test the efficacy of different adversarial example generation algorithms here , but we are happy to discuss this work in our revision . The \u201c recall \u201d here means the true positive rate . Since we focus on the binary classification problem and only allow the malicious ( positive ) instances to evade the classifier as benign ( negative ) ones , we want to check how many of these modified instances are correctly classified after retraining . Meanwhile , we have reported the classification error for all the experimental comparisons as suggested , the classification errors corresponding to experimental settings of Figure 1 and Figure 2 are reported in Table 1 and Table 2 respectively ."}}