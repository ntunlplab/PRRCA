{"year": "2021", "forum": "ygWoT6hOc28", "title": "Regression Prior Networks", "decision": "Reject", "meta_review": "This paper presents a useful contribution to the growing literature on uncertainty estimation with deep learning. The review process has significantly helped with strengthening this paper, specifically with the concerns about novelty and sufficient comparisons to existing work. I hope you will continue to improve this work for submission to a future venue.", "reviews": [{"review_id": "ygWoT6hOc28-0", "review_text": "This paper extends Prior networks models , previously introduced for classification , to regression problems . Prior networks are neural networks whose main target is to `` modelling uncertainty in classification tasks by emulating an ensemble using a single model '' . Standard Prior networks models output the parameters of a Dirichlet probability distribution . This Dirichlet probability distribution then defines a distribution over categorical probability distributions over the different classes . This hierarchical approach allows to better capture uncertainty . The presented approach extends this framework to regression tasks . So , instead of returning the parameters of a Dirichlet distribution , it returns the parameters of a Normal-Wishart distribution , which then defines a probability distribution over Normal distributions , and , in turn , each Normal distribution defines a probability distribution over the value of the target variable . Pros : * The presented approach is sound and addresses a relevant problem , which is modelling uncertainty for regression problems . * A method for distilling an ensemble model into a single model while maintaining accuracy is also proposed . * The proposed approach does not incur in computational and memory overheads like standard deep ensembles . * This work properly approaches technical difficulties ( such as employing numerical stable precision parametrizations of the Normal-Wishart distribution ) that arise in this kind of problems . Cons : * The presented approach does not introduce any novel idea or insight . It 's a relatively simple extension of a previously published method . * The empirical results do not show a clear advantage of the presented approach wrt previously published proposals . * The advantage of having a small computational and memory overhead is not properly evaluated with other proposals which also have a small computational and memory overhead [ 1 ] ( although this proposal has not been defined for regression problems , the adaptation to regression is as simple as the adaptation of the DeepEnsembles models employed in this work ) . I can not recommend the acceptation of this paper because I find the originality of the work quite limited . Although the extension of prior networks to regression task is mot really straightforward because of technical issues related to the problem of learning the parameters of a Normal-Wishart distribution . The general strategy to do that exactly matches the previous steps employed when introducing prior networks . In consequence , this work does not provide any new relevant insight into the problem of modelling uncertainty and learning models with well-calibrated predictions . Minor comments : - Eq ( 14 ) : T parameter is not defined . Temperature ? - Typo at the end of Page 5 : [ -25,20 ] -- > [ -25 , -20 ] - ENSM is defined after Table 1 . - Fix the following reference : Andrey Malinin and Mark JF Gales . Reverse kl-divergence training of prior networks : Improved uncertainty and adversarial robustness . 2019.Post-rebuttal : I thank the authors ' effort for the improvement of the manuscript following the comments of the different reviewers . I think the overall quality of the paper has really improved . But , after many thoughts , I still think there is a limited novelty in this paper . I have increased my score to 5 . But I can not recommend this paper for publication . adding baseline models to the paper and missing citations . I do think this improves the overall paper by a lot . As mentioned already in my paper , I do believe this is a nice idea and executed well , even though novelty might be limited . I am keeping my score and recommending an accept . [ 1 ] Wen , Y. , Tran , D. , & Ba , J . ( 2020 ) .BatchEnsemble : an Alternative Approach to Efficient Ensemble and Lifelong Learning . arXiv preprint arXiv:2002.06715 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review ! Please allow us to address your concerns : 1 . Regarding empirical results : Could you please elaborate what you would see as a clear advantage ? In terms of inference-time compute and memory the M-fold ( where M is the ensemble size ) advantage over Ensembles is clear . In terms of predictive performance - we outperform single models , and get close to the ensemble . Replicating the ensemble \u2019 s predictive performance completely is an upper bound . In terms of OOD Ensemble-Distribution Distilled RPNs outperform the ensemble . If there some specific comparison you would like us to provide which would convince you ? 2.Regarding BatchEnsemble : BatchEnsembles are interesting , however an efficient implementation of BatchEnsembles is non-trivial and there is no available code in pytorch ( The original work was done in Edward ) . A naive implementation would be as expensive during inference as DeepEnsembles , if not more so , as it may require a larger ensemble to reach the same performance . If you insist , we will explore this approach , but this will likely be infeasible within the time-frame of the rebuttal period . P.S.Thank you for finding the minor errors . We will fix them ."}, {"review_id": "ygWoT6hOc28-1", "review_text": "Prior Networks ( Malinin & Gales , 2018 ) use Dirichlet prior over categorical predictive distributions to distill ensembles for classification tasks . This paper extends Prior Networks to the regression setting by using a Normal-Wishart prior in order to attempt to match the predictive diversity . The authors define the model and loss terms including analytical derivation and evaluate their proposed approach with synthetic data , UCI datasets and monocular depth estimation . _Strengths_ : - The paper is well-written and clearly structured . - Most design choices are justified . - Simple idea ( in a good way ! ) which seemed to work well , shown by the evaluation . _Weaknesses_ : - Most of the work seems to be heavily based on Prior Networks ( Malinin & Gales , 2018 ) . Even Section 2.1 seems to be exactly like the Subsection in the paper about Prior Networks . This paper mainly focuses on an extension to the regression task . Therefore , the contribution / novelty of this paper is incremental . However , I still think the authors did a good job to present a general distillation method for regression task . Therefore , I would consider the novelty a minor weakness . - I am on the fence about specifying the OOD dataset for learning with the loss in Eq.8.I believe it is difficult to decide what kind of model to use for generating the OOD dataset , thus , the model choice can lead to large differences in performance . This is not really discussed . Further , the models trained have more data available for training , I believe it is not quite fair to compare against models which only have been trained on in-domain-data . - There are no comparisons to other approaches for distillation of regression tasks . I understand , that this paper wants to show a viable general approach for regression distillation , however , this work is not the first one to do so and therefore should consider existing work . _Overall assessment_ : For me , this paper is borderline . The weaknesses , especially the OOD dataset used for training and the lack of comparisons in the evaluation are concerns . However , I like the idea and the execution so therefore , I would recommend a weak accept ( 6 ) . _Detailed comments and questions_ : - OOD data : I have seen that you have an ablation for the degree of regularization on the OOD dataset . However , what about different OOD data ? Why choose KITTY and not a different dataset ? Were there any large difference in performance ? - Table 3 : I notice that NLL performance of distilled models are better than the actual ensemble , how can this be ? - OOD detection for monocular depth estimation : Did you also trained the comparing models with the OOD data , e.g.DD ? - Comparing models : Have you consider comparing your model to other ones , e.g . [ 1 , 2 ] ? This could improve your paper and approach to show that it also consider existing work on regression distillation . _Post-rebuttal_ : I really appreciate the authors adding baseline models to the paper and missing citations . I do think this improves the overall paper by a lot . As mentioned already in my paper , I do believe this is a nice idea and executed well , even though novelty might be limited . I am keeping my score and recommending an accept . [ 1 ] Chen , G. , Choi , W. , Yu , X. , Han , T. and Chandraker , M. , 2017 . Learning efficient object detection models with knowledge distillation . In Advances in Neural Information Processing Systems ( pp.742-751 ) . [ 2 ] Saputra , M.R.U. , de Gusmao , P.P. , Almalioglu , Y. , Markham , A. and Trigoni , N. , 2019 . Distilling knowledge from a deep pose regressor network . In Proceedings of the IEEE International Conference on Computer Vision ( pp.263-272 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review ! Allow us to address your concerns on a point-by-point basis . REGARDING WEAKNESSES 1 . We agree with your concerns regarding the choice of OOD dataset . Defining an appropriate one for classification tasks is already non-trivial - doing so is even more challenging for regression . This is why we place greater emphasis on Ensemble Distribution Distillation - it does not require an OOD dataset and yields superior predictive performance relative to RKL-trained Regression Prior Networks . We will use the extra page to present a discussion regarding difficulties of using an OOD dataset , and will shortly upload an updated manuscript . 2.With regards to regression distillation , we would like to point out that previous work has examined the distillation of a * single model into a single model * . In our work we consider distillation of * an ensemble of probabilistic models into a single probabilistic model * . Limited prior work has examined this scenario , and it is difficult to provide a sensible baseline . We have attempted to do so through Ensemble Distillation ( EnD ) , though it seems this approach also has its limitations . It is , in general , not entirely clear whether combining an ensemble of probabilistic models is better done as an arithmetic or geometric mixture . A full analysis of ensembles of probabilistic regression models deserves an investigation of its own . Furthermore , to our knowledge , probabilistic ensemble distillation for regression has been a generally under-explored area . If you could point us to a more appropriate baseline , we would be happy to consider it ! We will add a detailed discussion of this issue into section 2.3 and upload an updated manuscript shortly . REGARDING DETAILED COMMENTS 1 We were limited in the compute we had available for this project and decided to focus on the ablation study we did , rather than swapping out OOD datasets . In general , for Depth Estimation , we would like to place greater emphasis on RPNs trained through EnD $ ^2 $ , rather than RPNs trained via RKL on OOD datasets . Indeed , one of the conceptual reasons for not further exploring choice of OOD datasets for RPN+RKL is that we believe ( and show ) that RPNs+EnD $ ^2 $ to be the superior approach . We will clarify this point in an updated manuscript we will shortly upload . 2.We believe this is a result of the fact that the EnD $ ^2 $ will overestimate the support of the ensemble ( as a natural consequence of ML training ) . As a result , it will be less over-confident . 3.We didn \u2019 t . To be clear - we intended our main comparison for Depth Estimation to be Ensembles vs EnD $ ^2 $ . Note that RPNs trained via RKL on OOD data in section 5 suffer degraded predictive performance . On the other hand , RPNs trained via EnD $ ^2 $ show better predictive performance ( relative to EnD , Single models and RPN+RKL ) . 4. : Thank you for pointing out this work . However , as previously stated , these papers consider the distillation of single model into single model , and thus can not be used as a meaningful baselines . However we will cite them when discussing the nature of regression distillation and highlighting how our work is different ."}, {"review_id": "ygWoT6hOc28-2", "review_text": "Summary of the Paper : This paper introduces regression prior networks . These are models that aim at capture predictive uncertainty , both epistemic and aleatoric , in the context of regression problems . Regression prior networks can also be used to compress an ensemble of predictors into a single model while keeping the benefits of the ensemble . That is , better predictive performance and uncertainty estimates . The method is validated on several problems from the UCI repository and compared with ensemble methods . Specific details : I believe that this is a nice paper that illustrates an appealing method for uncertainty estimation in the context of neural networks . My main concern , however , is that it builds heavily on previous work . In particular , prior networks have already been proposed for classification and they have also been used to distill ( compress ) an ensemble . There is hence not much novelty here , only the extension to regression problems since , previously , only classification problems have been addressed . The use of prior networks for ensemble distillation is also not new . All this questions the novelty of the proposed approach . The extension to regression seems to follow very closely the work already carried out for classification . The only difference is that a Normal Wishart distribution is used instead of a Dirichlet distribution . The experiments carried out are extensive and consider different tasks involving prediction accuracy and out of distribution data detection . My main concern , however , is that no comparison is carried out with alternative methods to estimate prediction uncertainty such as those of Bayesian neural networks using variational inference or dropout . The authors should comment on the advantages of their method with respect to these techniques . The method proposed is also complicated and has several training parameters . The authors give specific values for them , but it is not clear the motivation for them or the sensitivity to their values . The paper is clearly written but heavily relies on previous work , making the reading difficult for someone who is not familiar with it . The paper is not self-contained . Summing up I believe that this could be an interesting contribution for the conference , suffering from a reduced amount of novelty .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments ! We will now address your comments point-by-point : 1 . Regarding alternative Bayesian baselines : The approach we use to generate ensembles - Deep Ensembles [ 4 ] , are already the current go-to SOTA Bayesian approach to uncertainty estimation [ 1,2,3 ] . Approaches like Dropout , while also capable of generating ensemble , are shown to be consistently inferior . Variational Inference is typically even worse and has never been successfully scaled to complex tasks such as Depth Estimation , to our knowledge . Our favoured proposed approach - Ensemble Distribution Distillation for regression , allows us to take a SOTA DeepEnsemble ( which is the baseline relative to which we compare ) and distill it into a single model , generally preserving most of the ensemble \u2019 s gains . This allows us to replicate both the ensemble \u2019 s predictive performance as well as uncertainty measures at the computational and memory cost of a single model . Thus , suffer a minor reduction in predictive quality ( and no loss in the quality of uncertainty estimates ) for an M-fold ( where M is the ensemble size ) reduction in computational and memory cost relative to the ensemble baseline . [ 1 ] Can you trust your model \u2019 s uncertainty ? Evaluating predictive uncertainty under dataset shift . [ 2 ] Pitfalls of in-domain uncertainty estimation and ensembling in deep learning . [ 3 ] Deep ensembles : A loss landscape perspective . [ 4 ] Simple and Scalable Predictive UncertaintyEstimation using Deep Ensembles 2 . Regarding additional training parameters - Could you please be more specific , so that we could address your concerns in detail ? 3.Regarding the difficulty of understanding the paper - Are there particular changes you would like us to implement which you think would make this paper more accessible ?"}, {"review_id": "ygWoT6hOc28-3", "review_text": "This paper addresses interpretable uncertainty quantification for data driven models . In particular , the authors focus on a sub-class of methods known as Prior Networks and attempt to extend these methods to regression tasks as existing approaches address classification only . The author contribution is thus clearly stated and positioned w.r.t.prior arts and tackle a non-trivial issue . In the classification setting , the Dirichlet distribution is pretty much the universal model for the parameters of multinomial distributions . For regression , i.e.continuous r.v. , there is no such universal solution and the authors chose to focus on outputs that have a normal distribution . The parameters of this latter are assumed to be normal-Wishart . Although , the proposed method is de facto non-applicable to other types of distributions , it can be argued that this already covers a majority of situations . The paper is rather well organized and seems technically sound . This said , a few mathematical details are missing and , most importantly , the experiments are not very convincing . These concerns , also with other minor remarks are detailed below , section by section . sec 2.2 Maybe give the explicit definition of Z to clarify that is does not depend on network parameters . The presence of the OOD loss term in ( 8 ) is a bit artificial as it boils down to regularizing because of the choice of beta . Is this choice systematic ? In ( 9 ) , how is p ( y | mu , Lambda ) computed ? Is it a T distribution ? 2.3 ( 12 ) lacks clarity : dataset is equal to an empirical distribution ... Do you mean p hat is a sum of Dirac ? What does phi represent ? 3 The acronym ENSM is not explained . I believe this corresponds to the deep ensemble . The Prior Networks achieve a form of disambiguation but the quality of it is a bit disappointing compared to ENSM . In particular , data uncertainty raises quickly for out-of-domain inputs . 4 The presentation of the experimental protocol in 4 lacks clarity thereby impairing the interpretation of the results . The definition of the unconventional performance criteria [ ( Malinin et al.2020 ] must be recalled ( at least in an appendix ) . In addition , as honestly mentioned by the authors , these datasets may not offer sufficiently rich problems to provide interesting comparisons . Besides , the way that OOD data is generated does not seem to necessarily produce inputs that are not covered by the in-domain distribution . Perhaps , the authors could use a `` bad GAN '' to obtain such data points , I mean a GAN where the generator and the discriminator would co-operate instead of being adversaries . If it converges , the generator would produce synthetic inputs that are easy to discriminate , thus far from true inputs . 5 While the dataset used in this section is more challenging , the experiment description is confusing . Again , performance criteria are not sufficiently explained and the general message becomes cryptic . Table 3 is overly complicated , I think RMSE is fairly enough to depict regression performances . Moreover , the definition of some columns are missing . In Table 4 , the performances of the methods seem quite unstable . For example , NWPN works fairly well for a given dataset configuration for one knowledge uncertainty criterion but fails miserably using another criterion on the same data . On Fig 3 , from what dataset are these image coming from ? Why are these or that object presumably `` unknown '' to the model ? I think the whole section deserves some re-writing . Final remark : there are a few English mistakes that should be wiped out .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your detailed comments ! Allow us to address them : SEC 2.2 - A . Z is indeed a constant that does n't depend on the parameters of the model . We will make this clear . B. I 'm afraid that this is n't the case . The OOD loss does n't regularise the choice of beta . Rather the OOD loss is supposed to inform the model of regions beyond which it has no understanding of the data . Clearly , this requires one to decide on and choose an OOD dataset , which is non-trivial . C. p ( y | mu , Lambda ) represents a Normal distribution sampled from the Normal-Wishart . SEC 2.3 - Yes , this is what I mean - the dataset can be seen an empirical distribution to which we minimise KL , or equivalently , maximise likelihood . Phi represents the parameters of the model into which we are distribution-distilling the ensemble . SEC 3.ENSM is the Deep Ensemble . Respectfully , the behaviour of the estimates of data uncertainty out of domain is not relevant - data uncertainty is only important in-domain . Indeed , we can not give any guarantees on the behaviour of data uncertainty in the OOD region . What we actually care about is that estimates of * knowledge uncertainty * increase as we move further out of domain , which is the case ( though perhaps not so easy to see from the picture ) . We will update the picture to make this clearer . SEC 4 A.We will make the experimental protocol clearer in this section . We will add the description of the Prediction Rejection Ratio in the appendix . It shows what part of the best possible error-detection performance our algorithm covers . B. UCI datasets are very common datasets for evaluation in related works , that \u2019 s why we decided to add them despite their simplicity . C. To obtain train-OOD-data for RKL , we used factor analysis with increased noise and latent variance . This is a simple generative model . We trained it on in-domain data and added noise to the latent variables to generate out-of-domain examples for RKL . This generative model is simple and appropriate for table data , while GANs are not usual for table data . Also , UCI datasets have few examples and small feature spaces , therefore it could be hard to train GANs on them . D. For the evaluation of OOD-detection performance , we took parts of other UCI datasets as OOD data . We made sure that the OOD-data comes from different domains and feature distributions are different . We felt that this was the best we could , as , to the best of our knowledge , there has been no established research on OOD detection for tabular datasets . SEC 5 Performance metrics in table 3 are usual for Monocular Depth Estimation . They describe model performance from different sides and are usually shown in all papers on this topic . A good description of these metrics can be found in the original Monocular Depth Estimation paper \u201c Depth Map Prediction from a Single Image using a Multi-Scale Deep Network \u201d by Eigen et al. , in section 4.3 . Delta 1,2,3 shows a percent of predictions such that the maximum of two fractions : ( a ) between predictions and targets , ( b ) between targets and predictions is less than corresponding thresholds : 1.25 , 1.25^2 , and 1.25^3 . Rel stands for absolute relative error and log10 for RMSE between logarithms of predictions and targets . These losses show different properties of the model : deltas help to understand confidence intervals of the model , Rel shows the ratio between prediction error and target , and log10 shows error in the log-space . We will add the definition of these metrics to the text and attempt to simplify table 3 as much as possible . We fully understand where you are coming from regarding table 4 and figure 3 . We will rewrite this section and make it more understandable , it was hard to fit everything into a given space . Regarding Table 4 and the behaviour of the NWPN ( RPN+RKL ) model - we hypothesise this is the result of the interaction between the in-domain and OOD training data . It was very hard to get the models to appropriately train . Likely because discrimination between ID/OOD is a very global task ( global scene understanding ) , while depth estimation requires more local data . The tasks are therefore anti-correlated in training . In contrast , EnD $ ^2 $ does n't suffer from the same problems and only relies on ID training data . Thus , what we aim to show is that : 1 ) EnD $ ^2 $ can appropriately replicate and surpass the ensemble 's OOD performance . 2 ) NWPN ( RPN+RKL ) can sometimes do near-perfect OOD detection , but is n't as reliable in this particular task with this choice of OOD data . In Figure 3 the left image is from KITTI and the right image is from NYU datasets . Using these images we aim to show that error of a prediction correlates with increased uncertainty of the model . Additionally , we wanted to show how the uncertainties of the ensemble and EnD $ ^2 $ model compare , and we can see that the EnD $ ^2 $ model consistently yields higher uncertainties , as it over-estimates the support of the ensemble ."}], "0": {"review_id": "ygWoT6hOc28-0", "review_text": "This paper extends Prior networks models , previously introduced for classification , to regression problems . Prior networks are neural networks whose main target is to `` modelling uncertainty in classification tasks by emulating an ensemble using a single model '' . Standard Prior networks models output the parameters of a Dirichlet probability distribution . This Dirichlet probability distribution then defines a distribution over categorical probability distributions over the different classes . This hierarchical approach allows to better capture uncertainty . The presented approach extends this framework to regression tasks . So , instead of returning the parameters of a Dirichlet distribution , it returns the parameters of a Normal-Wishart distribution , which then defines a probability distribution over Normal distributions , and , in turn , each Normal distribution defines a probability distribution over the value of the target variable . Pros : * The presented approach is sound and addresses a relevant problem , which is modelling uncertainty for regression problems . * A method for distilling an ensemble model into a single model while maintaining accuracy is also proposed . * The proposed approach does not incur in computational and memory overheads like standard deep ensembles . * This work properly approaches technical difficulties ( such as employing numerical stable precision parametrizations of the Normal-Wishart distribution ) that arise in this kind of problems . Cons : * The presented approach does not introduce any novel idea or insight . It 's a relatively simple extension of a previously published method . * The empirical results do not show a clear advantage of the presented approach wrt previously published proposals . * The advantage of having a small computational and memory overhead is not properly evaluated with other proposals which also have a small computational and memory overhead [ 1 ] ( although this proposal has not been defined for regression problems , the adaptation to regression is as simple as the adaptation of the DeepEnsembles models employed in this work ) . I can not recommend the acceptation of this paper because I find the originality of the work quite limited . Although the extension of prior networks to regression task is mot really straightforward because of technical issues related to the problem of learning the parameters of a Normal-Wishart distribution . The general strategy to do that exactly matches the previous steps employed when introducing prior networks . In consequence , this work does not provide any new relevant insight into the problem of modelling uncertainty and learning models with well-calibrated predictions . Minor comments : - Eq ( 14 ) : T parameter is not defined . Temperature ? - Typo at the end of Page 5 : [ -25,20 ] -- > [ -25 , -20 ] - ENSM is defined after Table 1 . - Fix the following reference : Andrey Malinin and Mark JF Gales . Reverse kl-divergence training of prior networks : Improved uncertainty and adversarial robustness . 2019.Post-rebuttal : I thank the authors ' effort for the improvement of the manuscript following the comments of the different reviewers . I think the overall quality of the paper has really improved . But , after many thoughts , I still think there is a limited novelty in this paper . I have increased my score to 5 . But I can not recommend this paper for publication . adding baseline models to the paper and missing citations . I do think this improves the overall paper by a lot . As mentioned already in my paper , I do believe this is a nice idea and executed well , even though novelty might be limited . I am keeping my score and recommending an accept . [ 1 ] Wen , Y. , Tran , D. , & Ba , J . ( 2020 ) .BatchEnsemble : an Alternative Approach to Efficient Ensemble and Lifelong Learning . arXiv preprint arXiv:2002.06715 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your review ! Please allow us to address your concerns : 1 . Regarding empirical results : Could you please elaborate what you would see as a clear advantage ? In terms of inference-time compute and memory the M-fold ( where M is the ensemble size ) advantage over Ensembles is clear . In terms of predictive performance - we outperform single models , and get close to the ensemble . Replicating the ensemble \u2019 s predictive performance completely is an upper bound . In terms of OOD Ensemble-Distribution Distilled RPNs outperform the ensemble . If there some specific comparison you would like us to provide which would convince you ? 2.Regarding BatchEnsemble : BatchEnsembles are interesting , however an efficient implementation of BatchEnsembles is non-trivial and there is no available code in pytorch ( The original work was done in Edward ) . A naive implementation would be as expensive during inference as DeepEnsembles , if not more so , as it may require a larger ensemble to reach the same performance . If you insist , we will explore this approach , but this will likely be infeasible within the time-frame of the rebuttal period . P.S.Thank you for finding the minor errors . We will fix them ."}, "1": {"review_id": "ygWoT6hOc28-1", "review_text": "Prior Networks ( Malinin & Gales , 2018 ) use Dirichlet prior over categorical predictive distributions to distill ensembles for classification tasks . This paper extends Prior Networks to the regression setting by using a Normal-Wishart prior in order to attempt to match the predictive diversity . The authors define the model and loss terms including analytical derivation and evaluate their proposed approach with synthetic data , UCI datasets and monocular depth estimation . _Strengths_ : - The paper is well-written and clearly structured . - Most design choices are justified . - Simple idea ( in a good way ! ) which seemed to work well , shown by the evaluation . _Weaknesses_ : - Most of the work seems to be heavily based on Prior Networks ( Malinin & Gales , 2018 ) . Even Section 2.1 seems to be exactly like the Subsection in the paper about Prior Networks . This paper mainly focuses on an extension to the regression task . Therefore , the contribution / novelty of this paper is incremental . However , I still think the authors did a good job to present a general distillation method for regression task . Therefore , I would consider the novelty a minor weakness . - I am on the fence about specifying the OOD dataset for learning with the loss in Eq.8.I believe it is difficult to decide what kind of model to use for generating the OOD dataset , thus , the model choice can lead to large differences in performance . This is not really discussed . Further , the models trained have more data available for training , I believe it is not quite fair to compare against models which only have been trained on in-domain-data . - There are no comparisons to other approaches for distillation of regression tasks . I understand , that this paper wants to show a viable general approach for regression distillation , however , this work is not the first one to do so and therefore should consider existing work . _Overall assessment_ : For me , this paper is borderline . The weaknesses , especially the OOD dataset used for training and the lack of comparisons in the evaluation are concerns . However , I like the idea and the execution so therefore , I would recommend a weak accept ( 6 ) . _Detailed comments and questions_ : - OOD data : I have seen that you have an ablation for the degree of regularization on the OOD dataset . However , what about different OOD data ? Why choose KITTY and not a different dataset ? Were there any large difference in performance ? - Table 3 : I notice that NLL performance of distilled models are better than the actual ensemble , how can this be ? - OOD detection for monocular depth estimation : Did you also trained the comparing models with the OOD data , e.g.DD ? - Comparing models : Have you consider comparing your model to other ones , e.g . [ 1 , 2 ] ? This could improve your paper and approach to show that it also consider existing work on regression distillation . _Post-rebuttal_ : I really appreciate the authors adding baseline models to the paper and missing citations . I do think this improves the overall paper by a lot . As mentioned already in my paper , I do believe this is a nice idea and executed well , even though novelty might be limited . I am keeping my score and recommending an accept . [ 1 ] Chen , G. , Choi , W. , Yu , X. , Han , T. and Chandraker , M. , 2017 . Learning efficient object detection models with knowledge distillation . In Advances in Neural Information Processing Systems ( pp.742-751 ) . [ 2 ] Saputra , M.R.U. , de Gusmao , P.P. , Almalioglu , Y. , Markham , A. and Trigoni , N. , 2019 . Distilling knowledge from a deep pose regressor network . In Proceedings of the IEEE International Conference on Computer Vision ( pp.263-272 ) .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review ! Allow us to address your concerns on a point-by-point basis . REGARDING WEAKNESSES 1 . We agree with your concerns regarding the choice of OOD dataset . Defining an appropriate one for classification tasks is already non-trivial - doing so is even more challenging for regression . This is why we place greater emphasis on Ensemble Distribution Distillation - it does not require an OOD dataset and yields superior predictive performance relative to RKL-trained Regression Prior Networks . We will use the extra page to present a discussion regarding difficulties of using an OOD dataset , and will shortly upload an updated manuscript . 2.With regards to regression distillation , we would like to point out that previous work has examined the distillation of a * single model into a single model * . In our work we consider distillation of * an ensemble of probabilistic models into a single probabilistic model * . Limited prior work has examined this scenario , and it is difficult to provide a sensible baseline . We have attempted to do so through Ensemble Distillation ( EnD ) , though it seems this approach also has its limitations . It is , in general , not entirely clear whether combining an ensemble of probabilistic models is better done as an arithmetic or geometric mixture . A full analysis of ensembles of probabilistic regression models deserves an investigation of its own . Furthermore , to our knowledge , probabilistic ensemble distillation for regression has been a generally under-explored area . If you could point us to a more appropriate baseline , we would be happy to consider it ! We will add a detailed discussion of this issue into section 2.3 and upload an updated manuscript shortly . REGARDING DETAILED COMMENTS 1 We were limited in the compute we had available for this project and decided to focus on the ablation study we did , rather than swapping out OOD datasets . In general , for Depth Estimation , we would like to place greater emphasis on RPNs trained through EnD $ ^2 $ , rather than RPNs trained via RKL on OOD datasets . Indeed , one of the conceptual reasons for not further exploring choice of OOD datasets for RPN+RKL is that we believe ( and show ) that RPNs+EnD $ ^2 $ to be the superior approach . We will clarify this point in an updated manuscript we will shortly upload . 2.We believe this is a result of the fact that the EnD $ ^2 $ will overestimate the support of the ensemble ( as a natural consequence of ML training ) . As a result , it will be less over-confident . 3.We didn \u2019 t . To be clear - we intended our main comparison for Depth Estimation to be Ensembles vs EnD $ ^2 $ . Note that RPNs trained via RKL on OOD data in section 5 suffer degraded predictive performance . On the other hand , RPNs trained via EnD $ ^2 $ show better predictive performance ( relative to EnD , Single models and RPN+RKL ) . 4. : Thank you for pointing out this work . However , as previously stated , these papers consider the distillation of single model into single model , and thus can not be used as a meaningful baselines . However we will cite them when discussing the nature of regression distillation and highlighting how our work is different ."}, "2": {"review_id": "ygWoT6hOc28-2", "review_text": "Summary of the Paper : This paper introduces regression prior networks . These are models that aim at capture predictive uncertainty , both epistemic and aleatoric , in the context of regression problems . Regression prior networks can also be used to compress an ensemble of predictors into a single model while keeping the benefits of the ensemble . That is , better predictive performance and uncertainty estimates . The method is validated on several problems from the UCI repository and compared with ensemble methods . Specific details : I believe that this is a nice paper that illustrates an appealing method for uncertainty estimation in the context of neural networks . My main concern , however , is that it builds heavily on previous work . In particular , prior networks have already been proposed for classification and they have also been used to distill ( compress ) an ensemble . There is hence not much novelty here , only the extension to regression problems since , previously , only classification problems have been addressed . The use of prior networks for ensemble distillation is also not new . All this questions the novelty of the proposed approach . The extension to regression seems to follow very closely the work already carried out for classification . The only difference is that a Normal Wishart distribution is used instead of a Dirichlet distribution . The experiments carried out are extensive and consider different tasks involving prediction accuracy and out of distribution data detection . My main concern , however , is that no comparison is carried out with alternative methods to estimate prediction uncertainty such as those of Bayesian neural networks using variational inference or dropout . The authors should comment on the advantages of their method with respect to these techniques . The method proposed is also complicated and has several training parameters . The authors give specific values for them , but it is not clear the motivation for them or the sensitivity to their values . The paper is clearly written but heavily relies on previous work , making the reading difficult for someone who is not familiar with it . The paper is not self-contained . Summing up I believe that this could be an interesting contribution for the conference , suffering from a reduced amount of novelty .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments ! We will now address your comments point-by-point : 1 . Regarding alternative Bayesian baselines : The approach we use to generate ensembles - Deep Ensembles [ 4 ] , are already the current go-to SOTA Bayesian approach to uncertainty estimation [ 1,2,3 ] . Approaches like Dropout , while also capable of generating ensemble , are shown to be consistently inferior . Variational Inference is typically even worse and has never been successfully scaled to complex tasks such as Depth Estimation , to our knowledge . Our favoured proposed approach - Ensemble Distribution Distillation for regression , allows us to take a SOTA DeepEnsemble ( which is the baseline relative to which we compare ) and distill it into a single model , generally preserving most of the ensemble \u2019 s gains . This allows us to replicate both the ensemble \u2019 s predictive performance as well as uncertainty measures at the computational and memory cost of a single model . Thus , suffer a minor reduction in predictive quality ( and no loss in the quality of uncertainty estimates ) for an M-fold ( where M is the ensemble size ) reduction in computational and memory cost relative to the ensemble baseline . [ 1 ] Can you trust your model \u2019 s uncertainty ? Evaluating predictive uncertainty under dataset shift . [ 2 ] Pitfalls of in-domain uncertainty estimation and ensembling in deep learning . [ 3 ] Deep ensembles : A loss landscape perspective . [ 4 ] Simple and Scalable Predictive UncertaintyEstimation using Deep Ensembles 2 . Regarding additional training parameters - Could you please be more specific , so that we could address your concerns in detail ? 3.Regarding the difficulty of understanding the paper - Are there particular changes you would like us to implement which you think would make this paper more accessible ?"}, "3": {"review_id": "ygWoT6hOc28-3", "review_text": "This paper addresses interpretable uncertainty quantification for data driven models . In particular , the authors focus on a sub-class of methods known as Prior Networks and attempt to extend these methods to regression tasks as existing approaches address classification only . The author contribution is thus clearly stated and positioned w.r.t.prior arts and tackle a non-trivial issue . In the classification setting , the Dirichlet distribution is pretty much the universal model for the parameters of multinomial distributions . For regression , i.e.continuous r.v. , there is no such universal solution and the authors chose to focus on outputs that have a normal distribution . The parameters of this latter are assumed to be normal-Wishart . Although , the proposed method is de facto non-applicable to other types of distributions , it can be argued that this already covers a majority of situations . The paper is rather well organized and seems technically sound . This said , a few mathematical details are missing and , most importantly , the experiments are not very convincing . These concerns , also with other minor remarks are detailed below , section by section . sec 2.2 Maybe give the explicit definition of Z to clarify that is does not depend on network parameters . The presence of the OOD loss term in ( 8 ) is a bit artificial as it boils down to regularizing because of the choice of beta . Is this choice systematic ? In ( 9 ) , how is p ( y | mu , Lambda ) computed ? Is it a T distribution ? 2.3 ( 12 ) lacks clarity : dataset is equal to an empirical distribution ... Do you mean p hat is a sum of Dirac ? What does phi represent ? 3 The acronym ENSM is not explained . I believe this corresponds to the deep ensemble . The Prior Networks achieve a form of disambiguation but the quality of it is a bit disappointing compared to ENSM . In particular , data uncertainty raises quickly for out-of-domain inputs . 4 The presentation of the experimental protocol in 4 lacks clarity thereby impairing the interpretation of the results . The definition of the unconventional performance criteria [ ( Malinin et al.2020 ] must be recalled ( at least in an appendix ) . In addition , as honestly mentioned by the authors , these datasets may not offer sufficiently rich problems to provide interesting comparisons . Besides , the way that OOD data is generated does not seem to necessarily produce inputs that are not covered by the in-domain distribution . Perhaps , the authors could use a `` bad GAN '' to obtain such data points , I mean a GAN where the generator and the discriminator would co-operate instead of being adversaries . If it converges , the generator would produce synthetic inputs that are easy to discriminate , thus far from true inputs . 5 While the dataset used in this section is more challenging , the experiment description is confusing . Again , performance criteria are not sufficiently explained and the general message becomes cryptic . Table 3 is overly complicated , I think RMSE is fairly enough to depict regression performances . Moreover , the definition of some columns are missing . In Table 4 , the performances of the methods seem quite unstable . For example , NWPN works fairly well for a given dataset configuration for one knowledge uncertainty criterion but fails miserably using another criterion on the same data . On Fig 3 , from what dataset are these image coming from ? Why are these or that object presumably `` unknown '' to the model ? I think the whole section deserves some re-writing . Final remark : there are a few English mistakes that should be wiped out .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your detailed comments ! Allow us to address them : SEC 2.2 - A . Z is indeed a constant that does n't depend on the parameters of the model . We will make this clear . B. I 'm afraid that this is n't the case . The OOD loss does n't regularise the choice of beta . Rather the OOD loss is supposed to inform the model of regions beyond which it has no understanding of the data . Clearly , this requires one to decide on and choose an OOD dataset , which is non-trivial . C. p ( y | mu , Lambda ) represents a Normal distribution sampled from the Normal-Wishart . SEC 2.3 - Yes , this is what I mean - the dataset can be seen an empirical distribution to which we minimise KL , or equivalently , maximise likelihood . Phi represents the parameters of the model into which we are distribution-distilling the ensemble . SEC 3.ENSM is the Deep Ensemble . Respectfully , the behaviour of the estimates of data uncertainty out of domain is not relevant - data uncertainty is only important in-domain . Indeed , we can not give any guarantees on the behaviour of data uncertainty in the OOD region . What we actually care about is that estimates of * knowledge uncertainty * increase as we move further out of domain , which is the case ( though perhaps not so easy to see from the picture ) . We will update the picture to make this clearer . SEC 4 A.We will make the experimental protocol clearer in this section . We will add the description of the Prediction Rejection Ratio in the appendix . It shows what part of the best possible error-detection performance our algorithm covers . B. UCI datasets are very common datasets for evaluation in related works , that \u2019 s why we decided to add them despite their simplicity . C. To obtain train-OOD-data for RKL , we used factor analysis with increased noise and latent variance . This is a simple generative model . We trained it on in-domain data and added noise to the latent variables to generate out-of-domain examples for RKL . This generative model is simple and appropriate for table data , while GANs are not usual for table data . Also , UCI datasets have few examples and small feature spaces , therefore it could be hard to train GANs on them . D. For the evaluation of OOD-detection performance , we took parts of other UCI datasets as OOD data . We made sure that the OOD-data comes from different domains and feature distributions are different . We felt that this was the best we could , as , to the best of our knowledge , there has been no established research on OOD detection for tabular datasets . SEC 5 Performance metrics in table 3 are usual for Monocular Depth Estimation . They describe model performance from different sides and are usually shown in all papers on this topic . A good description of these metrics can be found in the original Monocular Depth Estimation paper \u201c Depth Map Prediction from a Single Image using a Multi-Scale Deep Network \u201d by Eigen et al. , in section 4.3 . Delta 1,2,3 shows a percent of predictions such that the maximum of two fractions : ( a ) between predictions and targets , ( b ) between targets and predictions is less than corresponding thresholds : 1.25 , 1.25^2 , and 1.25^3 . Rel stands for absolute relative error and log10 for RMSE between logarithms of predictions and targets . These losses show different properties of the model : deltas help to understand confidence intervals of the model , Rel shows the ratio between prediction error and target , and log10 shows error in the log-space . We will add the definition of these metrics to the text and attempt to simplify table 3 as much as possible . We fully understand where you are coming from regarding table 4 and figure 3 . We will rewrite this section and make it more understandable , it was hard to fit everything into a given space . Regarding Table 4 and the behaviour of the NWPN ( RPN+RKL ) model - we hypothesise this is the result of the interaction between the in-domain and OOD training data . It was very hard to get the models to appropriately train . Likely because discrimination between ID/OOD is a very global task ( global scene understanding ) , while depth estimation requires more local data . The tasks are therefore anti-correlated in training . In contrast , EnD $ ^2 $ does n't suffer from the same problems and only relies on ID training data . Thus , what we aim to show is that : 1 ) EnD $ ^2 $ can appropriately replicate and surpass the ensemble 's OOD performance . 2 ) NWPN ( RPN+RKL ) can sometimes do near-perfect OOD detection , but is n't as reliable in this particular task with this choice of OOD data . In Figure 3 the left image is from KITTI and the right image is from NYU datasets . Using these images we aim to show that error of a prediction correlates with increased uncertainty of the model . Additionally , we wanted to show how the uncertainties of the ensemble and EnD $ ^2 $ model compare , and we can see that the EnD $ ^2 $ model consistently yields higher uncertainties , as it over-estimates the support of the ensemble ."}}