{"year": "2020", "forum": "SJxTZeHFPH", "title": "The Intriguing Effects of Focal Loss on the Calibration of Deep Neural Networks", "decision": "Reject", "meta_review": "The paper investigates the effect of focal loss on calibration of neural nets.\n\nOn one hand, the reviewers agree that this paper is well-written and the empirical results are interesting. On the other hand, the reviewers felt that there could be better evaluation of the effect of calibration on downstream tasks, and better justification for the choice of optimal gamma (e.g. on a simpler problem setup).\n\nI encourage the others to revise the draft and resubmit to a different venue.  \n", "reviews": [{"review_id": "SJxTZeHFPH-0", "review_text": "Summary: This paper studies the effect of the focal loss, proposed by Lin et al. in 2017 on network miscalibration, which appears when the network's confidence in its prediction does not match its correctness. The authors provide a theoretical explanation to the superior results of the focal loss for calibration. The temperature scaling technique of Guo et al. 2017 is applied (dividing the network's logits by a scalar learnt on a val set prior to softmax) to networks trained using the focal loss, with different options for the focal parameter, as well as the standard multi-class cross entropy and a few others. The experiments on CIFAR10/100 as well as two text dataset (20 Newsgroups, Stanford Sentiment Treebank) reach lower expected calibration error compared to the cross entropy (75% of relative improvement on cifar100 for instance). The importance of the contribution will probably be discussed here. At first glance, it seems that the works build mainly on advances from Lin et al & Guo et al, but the authors do a promising job in combining the two. Positive aspects: - The paper is well written. - Experiments on both image and text dataset demonstrate the superiority of the focal loss on several calibration metrics. - The theoretical explanation is convincing. Negative points: - The importance of the problem is motivated by future assessments by downstream tasks but do not address this aspect in the experiments. In particular, as the images experiments are conducted on tiny images, an experiment on a real size image dataset would strengthen the paper. - The policy that works best for defining the sample wise tuning of the focal parameter was hand-made but ultimately uses only 3 parameters so finally it is not so bad. Minor: - It'd be nice to illustrate the confidence improvements on a few qualitative examples, maybe in appendix. - 10 pages is too much (given that were were given instructions to be more severe with long paper) table 6 and 3 could be merged for instance. - The focal loss column results of table 1 should be the same as Table 5 (sample wise)? - could specify what MMCE means - clean the bibliography I've read the other reviews and authors' responses. Experiences on Tiny ImageNet are better than CIFAR but still a little far from what I'd call real images but I understand it can be difficult to run experiments on ImageNet. Since the choice of gamma seems to be leading consistent results also on tinyIN, I find it less concerning. ", "rating": "6: Weak Accept", "reply_text": "We sincerely thank the reviewer for this comment which we address as follows : To compare the performance on a bigger image dataset , we trained the ResNet-50 network using cross entropy , focal loss with a fixed gamma value of 3 and focal loss with the sample-wise gamma policy of 5,3 on Tiny ImageNet . The Tiny ImageNet dataset is a subset of ImageNet with 64 x 64 dimensional images , 200 classes and 500 images per class in the training set and 50 images per class in the validation set . The image dimensions of Tiny ImageNet are twice the size of the CIFAR-10/100 dataset images . We use SGD with a momentum of 0.9 as our optimiser and train the networks for 100 epochs with a learning rate of 0.1 for the first 40 epochs , 0.01 for the next 20 epochs and 0.001 for the last 40 epochs . We use a training batch size of 64 . We also augment the training images with random crops and random horizontal flips . It should be noted that we saved 50 samples per class ( i.e. , a total of 10000 samples ) from the training set as our own validation set to fine-tune the temperature parameter on ( hence , we trained on 90000 images ) and we use the Tiny ImageNet validation set as our test set . We report the Tiny ImageNet validation set error % , ECE % both before and after temperature scaling and Ada-ECE % both before and after temperature scaling in the table below . + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -+ | Loss Function | Error ( % ) | ECE ( % ) ( Pre T ) | ECE ( % ) ( Post T ) | Ada-ECE ( % ) ( Pre T ) | Ada-ECE ( % ) ( Post T ) | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -+ | Cross Entropy | 49.88 | 14.98 | 5.05 ( 1.4 ) | 14.98 | 5.05 ( 1.4 ) | | Focal Loss ( gamma = 3 ) | 48.37 | 2.08 | 2.08 ( 1.0 ) | 1.71 | 1.71 ( 1.0 ) | | Focal Loss ( Sample-wise gamma-5,3 ) | 48.43 | 1.80 | 1.80 ( 1.0 ) | 2.06 | 2.06 ( 1.0 ) | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- Secondly , we also observe a significant improvement in ECE and Ada-ECE values both before and after temperature scaling for models trained on focal loss indicating that these models are not only more accurate but also much more calibrated . Finally , we note that the optimal temperature for both models trained using focal loss is 1 which indicates that temperature scaling could not make these models any more calibrated . We will add all these baselines to the paper . We report these preliminary results here ( and not in the paper ) because we have to train quite a few other networks ( like ResNet-110 , DenseNet , Wide ResNet etc . ) and also we have to train other baselines ( like MMCE , Brier Score and other versions of focal loss ) to obtain the complete set of results which we can then add to the paper ."}, {"review_id": "SJxTZeHFPH-1", "review_text": "The paper explores how focal loss can be used to improve calibration for classifiers. Focal loss extends the cross-entropy loss, which is -log(p_label), with a multiplicative factor equal to (1 - p_label)^gamma. Intuitively, this downweights the loss for elements where the probability of the correct label p_label is close to 1, relatively increasing the weight of the misclassified examples. Somewhat surprisingly, this tends to improve the calibration of the model. I say surprisingly because the focal loss is not a bregman divergence for all values of alpha so in general the expected minimizer of the focal loss for a fractional label is not the fractional label (i.e. the minimizer wrt x of - p (1-x)^gamma log(x) - (1-p) x^gamma log (1 -x) is not in general p). The paper shows somewhat thorough experiments on many datasets justifying this observation, but the theoretical part is rather weak since it doesn't seem to address this issue with the focal loss. It's also not very clear from reading the paper what the p0 should be when using the rule to automatically select the gamma of the focal loss. I'd support accepting the paper if the calibration properties of the focal loss itself was better analyzed on a simpler setup (linear models, or single parameter models) so it's easier to understand how it's helping calibration in the deep network setup and if the algorithm for choosing per-example gammas was more clearly stated out.", "rating": "3: Weak Reject", "reply_text": "We sincerely thank the reviewer for this comment which we address as follows . We deal with a classification problem where $ p=0 $ or $ p=1 $ ( with one-hot encodings ) and expected minimizer of focal loss for it comes out to be $ x=p $ . However , we believe that the following question needs to be addressed : Q. Focal loss is not a bregman divergence , thus the minimizer of focal loss is not the original label when the original label is fractional . So , what exactly is it minimizing ? Ans : Yes focal loss is not a bregman divergence . However , in Appendix H we show that it is a regularized bregman divergence in the sense that while $ \\mathrm { CrossEntropy } ( p , q ) = \\mathrm { KL } ( p||q ) + \\mathrm { Entropy } ( p ) $ , we have $ \\mathrm { FocalLoss } ( p , q ) > \\mathrm { KL } ( p||q ) + \\mathrm { Entropy } ( p ) - \\gamma * \\mathrm { Entropy } ( q ) $ . Thus it is minimizing the KL-divergence between the target and predicted label distribution while ensuring that the entropy of the predicted distribution is large and $ \\gamma $ is essentially the regularization coefficient . Having higher entropy on the predicted distribution can help avoid overconfident predictions observed in modern neural networks , thus leading to better calibration . We provide the related proof in Appendix H. Please let us know if this answers your question or if not , it would be very helpful if you could give us some more clarity about what you are looking for in the theoretical part ."}, {"review_id": "SJxTZeHFPH-2", "review_text": "The paper describes how the use of the now-standard focal loss can lead to improved calibration results when used to fit deep-models. When fitting a large capacity model with NLL, the model can often try to drive its predictions close to 1 (i.e. infinity pre-softmax) on the training set, ultimately leading to poorly calibrated models and overfitting behaviour. The focal loss appears to mitigate this issue. The approach is extremely simple to implement, the theoretical justifications are believable, and the calibration/accuracy performances seem to be good -- for this reasons, I think that the paper should be accepted. (1) it would be interesting to compare the approach to using the standard cross-entropy applied to smoothed labels (i.e. (1-eps,eps) instead of (1,0) in binary classification and obvious generalisation in multi-class setting). (2) data-augmentation often greatly helps with calibration -- the paper did not describe in details what has been done on that front for the numerical investigations.", "rating": "6: Weak Accept", "reply_text": "We sincerely thank the reviewer for this comment which we address as follows . To empirically observe the effects of training networks using cross entropy loss with smoothed labels , we trained ResNet-50 and ResNet-110 on both CIFAR-10 and CIFAR-100 using cross entropy loss with smoothing factors of 0.05 and 0.1 . In simple terms , if the smoothing factor is $ \\alpha $ and if for a sample we have a one-hot label vector $ Y $ , then its smoothed label vector $ S $ will be such that $ S_i = ( 1 - \\alpha ) * Y_i + \\alpha * ( 1 - Y_i ) / ( K-1 ) $ where $ K $ is the number of classes . In Table 1 , we present the test set error % , ECE ( % ) both pre and post temperature scaling and Ada-ECE ( % ) both pre and post temperature scaling for each of these configurations . We also provide the same metrics for ResNet-50 and ResNet-110 trained using cross entropy and focal loss with one-hot labels ( these numbers were taken from Tables 1,2 and 3 in the paper ) for ease of comparison in Table 2 . We provide Table 2 in a separate comment due to lack of space . The focal loss numbers in Table 2 are for focal loss with the sample-wise gamma approach reported in Tables 1 and 2 of the paper . Table 1 + -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -+ | Loss | Smoothing | Dataset | Model | Error ( % ) | ECE ( % ) ( Pre T ) | ECE ( % ) ( Post T ) | Ada-ECE ( % ) ( Pre T ) | Ada-ECE ( % ) ( Post T ) | + -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -+ | CE | 0.05 | CIFAR-10 | ResNet-50 | 4.99 | 3.08 | 1.33 ( 0.9 ) | 3.74 | 2.89 ( 0.8 ) | | CE | 0.05 | CIFAR-10 | ResNet-110 | 5.11 | 1.56 | 1.82 ( 0.9 ) | 3.23 | 2.52 ( 0.9 ) | | CE | 0.05 | CIFAR-100 | ResNet-50 | 22.10 | 7.61 | 4.19 ( 1.1 ) | 7.80 | 6.32 ( 1.1 ) | | CE | 0.05 | CIFAR-100 | ResNet-110 | 23.45 | 10.89 | * All the networks trained on smoothed labels are able to achieve test set accuracies which are in the state-of-the-art ballpark . Moreover , we observe a significant improvement in both ECE and Ada-ECE values before temperature scaling for models trained using cross entropy loss with smoothed labels as compared to models trained using cross entropy loss with one-hot labels . These improvements however , are not reflected in the ECE and Ada-ECE numbers obtained after temperature scaling . * It is quite interesting to note that training on smoothed labels causes the models to become less confident on their predictions in general as we often obtain optimal temperatures which are lower than 1 . This means that temperature scaling for these models is increasing their confidence . On the other hand , optimal temperatures for models trained using cross entropy with one-hot labels are much greater than 1 and hence , temperature scaling is lowering the confidence of these models . We will add the numbers obtained from models trained using cross entropy with smoothed labels ( both with smoothing factors 0.1 and 0.05 ) to the paper . We present the preliminary set of results here ( and not in the paper ) because we need to train other networks ( Wide ResNet , DenseNet , etc . ) as well to have a complete set of results which can then be included in the paper ."}], "0": {"review_id": "SJxTZeHFPH-0", "review_text": "Summary: This paper studies the effect of the focal loss, proposed by Lin et al. in 2017 on network miscalibration, which appears when the network's confidence in its prediction does not match its correctness. The authors provide a theoretical explanation to the superior results of the focal loss for calibration. The temperature scaling technique of Guo et al. 2017 is applied (dividing the network's logits by a scalar learnt on a val set prior to softmax) to networks trained using the focal loss, with different options for the focal parameter, as well as the standard multi-class cross entropy and a few others. The experiments on CIFAR10/100 as well as two text dataset (20 Newsgroups, Stanford Sentiment Treebank) reach lower expected calibration error compared to the cross entropy (75% of relative improvement on cifar100 for instance). The importance of the contribution will probably be discussed here. At first glance, it seems that the works build mainly on advances from Lin et al & Guo et al, but the authors do a promising job in combining the two. Positive aspects: - The paper is well written. - Experiments on both image and text dataset demonstrate the superiority of the focal loss on several calibration metrics. - The theoretical explanation is convincing. Negative points: - The importance of the problem is motivated by future assessments by downstream tasks but do not address this aspect in the experiments. In particular, as the images experiments are conducted on tiny images, an experiment on a real size image dataset would strengthen the paper. - The policy that works best for defining the sample wise tuning of the focal parameter was hand-made but ultimately uses only 3 parameters so finally it is not so bad. Minor: - It'd be nice to illustrate the confidence improvements on a few qualitative examples, maybe in appendix. - 10 pages is too much (given that were were given instructions to be more severe with long paper) table 6 and 3 could be merged for instance. - The focal loss column results of table 1 should be the same as Table 5 (sample wise)? - could specify what MMCE means - clean the bibliography I've read the other reviews and authors' responses. Experiences on Tiny ImageNet are better than CIFAR but still a little far from what I'd call real images but I understand it can be difficult to run experiments on ImageNet. Since the choice of gamma seems to be leading consistent results also on tinyIN, I find it less concerning. ", "rating": "6: Weak Accept", "reply_text": "We sincerely thank the reviewer for this comment which we address as follows : To compare the performance on a bigger image dataset , we trained the ResNet-50 network using cross entropy , focal loss with a fixed gamma value of 3 and focal loss with the sample-wise gamma policy of 5,3 on Tiny ImageNet . The Tiny ImageNet dataset is a subset of ImageNet with 64 x 64 dimensional images , 200 classes and 500 images per class in the training set and 50 images per class in the validation set . The image dimensions of Tiny ImageNet are twice the size of the CIFAR-10/100 dataset images . We use SGD with a momentum of 0.9 as our optimiser and train the networks for 100 epochs with a learning rate of 0.1 for the first 40 epochs , 0.01 for the next 20 epochs and 0.001 for the last 40 epochs . We use a training batch size of 64 . We also augment the training images with random crops and random horizontal flips . It should be noted that we saved 50 samples per class ( i.e. , a total of 10000 samples ) from the training set as our own validation set to fine-tune the temperature parameter on ( hence , we trained on 90000 images ) and we use the Tiny ImageNet validation set as our test set . We report the Tiny ImageNet validation set error % , ECE % both before and after temperature scaling and Ada-ECE % both before and after temperature scaling in the table below . + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -+ | Loss Function | Error ( % ) | ECE ( % ) ( Pre T ) | ECE ( % ) ( Post T ) | Ada-ECE ( % ) ( Pre T ) | Ada-ECE ( % ) ( Post T ) | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -+ | Cross Entropy | 49.88 | 14.98 | 5.05 ( 1.4 ) | 14.98 | 5.05 ( 1.4 ) | | Focal Loss ( gamma = 3 ) | 48.37 | 2.08 | 2.08 ( 1.0 ) | 1.71 | 1.71 ( 1.0 ) | | Focal Loss ( Sample-wise gamma-5,3 ) | 48.43 | 1.80 | 1.80 ( 1.0 ) | 2.06 | 2.06 ( 1.0 ) | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- Secondly , we also observe a significant improvement in ECE and Ada-ECE values both before and after temperature scaling for models trained on focal loss indicating that these models are not only more accurate but also much more calibrated . Finally , we note that the optimal temperature for both models trained using focal loss is 1 which indicates that temperature scaling could not make these models any more calibrated . We will add all these baselines to the paper . We report these preliminary results here ( and not in the paper ) because we have to train quite a few other networks ( like ResNet-110 , DenseNet , Wide ResNet etc . ) and also we have to train other baselines ( like MMCE , Brier Score and other versions of focal loss ) to obtain the complete set of results which we can then add to the paper ."}, "1": {"review_id": "SJxTZeHFPH-1", "review_text": "The paper explores how focal loss can be used to improve calibration for classifiers. Focal loss extends the cross-entropy loss, which is -log(p_label), with a multiplicative factor equal to (1 - p_label)^gamma. Intuitively, this downweights the loss for elements where the probability of the correct label p_label is close to 1, relatively increasing the weight of the misclassified examples. Somewhat surprisingly, this tends to improve the calibration of the model. I say surprisingly because the focal loss is not a bregman divergence for all values of alpha so in general the expected minimizer of the focal loss for a fractional label is not the fractional label (i.e. the minimizer wrt x of - p (1-x)^gamma log(x) - (1-p) x^gamma log (1 -x) is not in general p). The paper shows somewhat thorough experiments on many datasets justifying this observation, but the theoretical part is rather weak since it doesn't seem to address this issue with the focal loss. It's also not very clear from reading the paper what the p0 should be when using the rule to automatically select the gamma of the focal loss. I'd support accepting the paper if the calibration properties of the focal loss itself was better analyzed on a simpler setup (linear models, or single parameter models) so it's easier to understand how it's helping calibration in the deep network setup and if the algorithm for choosing per-example gammas was more clearly stated out.", "rating": "3: Weak Reject", "reply_text": "We sincerely thank the reviewer for this comment which we address as follows . We deal with a classification problem where $ p=0 $ or $ p=1 $ ( with one-hot encodings ) and expected minimizer of focal loss for it comes out to be $ x=p $ . However , we believe that the following question needs to be addressed : Q. Focal loss is not a bregman divergence , thus the minimizer of focal loss is not the original label when the original label is fractional . So , what exactly is it minimizing ? Ans : Yes focal loss is not a bregman divergence . However , in Appendix H we show that it is a regularized bregman divergence in the sense that while $ \\mathrm { CrossEntropy } ( p , q ) = \\mathrm { KL } ( p||q ) + \\mathrm { Entropy } ( p ) $ , we have $ \\mathrm { FocalLoss } ( p , q ) > \\mathrm { KL } ( p||q ) + \\mathrm { Entropy } ( p ) - \\gamma * \\mathrm { Entropy } ( q ) $ . Thus it is minimizing the KL-divergence between the target and predicted label distribution while ensuring that the entropy of the predicted distribution is large and $ \\gamma $ is essentially the regularization coefficient . Having higher entropy on the predicted distribution can help avoid overconfident predictions observed in modern neural networks , thus leading to better calibration . We provide the related proof in Appendix H. Please let us know if this answers your question or if not , it would be very helpful if you could give us some more clarity about what you are looking for in the theoretical part ."}, "2": {"review_id": "SJxTZeHFPH-2", "review_text": "The paper describes how the use of the now-standard focal loss can lead to improved calibration results when used to fit deep-models. When fitting a large capacity model with NLL, the model can often try to drive its predictions close to 1 (i.e. infinity pre-softmax) on the training set, ultimately leading to poorly calibrated models and overfitting behaviour. The focal loss appears to mitigate this issue. The approach is extremely simple to implement, the theoretical justifications are believable, and the calibration/accuracy performances seem to be good -- for this reasons, I think that the paper should be accepted. (1) it would be interesting to compare the approach to using the standard cross-entropy applied to smoothed labels (i.e. (1-eps,eps) instead of (1,0) in binary classification and obvious generalisation in multi-class setting). (2) data-augmentation often greatly helps with calibration -- the paper did not describe in details what has been done on that front for the numerical investigations.", "rating": "6: Weak Accept", "reply_text": "We sincerely thank the reviewer for this comment which we address as follows . To empirically observe the effects of training networks using cross entropy loss with smoothed labels , we trained ResNet-50 and ResNet-110 on both CIFAR-10 and CIFAR-100 using cross entropy loss with smoothing factors of 0.05 and 0.1 . In simple terms , if the smoothing factor is $ \\alpha $ and if for a sample we have a one-hot label vector $ Y $ , then its smoothed label vector $ S $ will be such that $ S_i = ( 1 - \\alpha ) * Y_i + \\alpha * ( 1 - Y_i ) / ( K-1 ) $ where $ K $ is the number of classes . In Table 1 , we present the test set error % , ECE ( % ) both pre and post temperature scaling and Ada-ECE ( % ) both pre and post temperature scaling for each of these configurations . We also provide the same metrics for ResNet-50 and ResNet-110 trained using cross entropy and focal loss with one-hot labels ( these numbers were taken from Tables 1,2 and 3 in the paper ) for ease of comparison in Table 2 . We provide Table 2 in a separate comment due to lack of space . The focal loss numbers in Table 2 are for focal loss with the sample-wise gamma approach reported in Tables 1 and 2 of the paper . Table 1 + -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -+ | Loss | Smoothing | Dataset | Model | Error ( % ) | ECE ( % ) ( Pre T ) | ECE ( % ) ( Post T ) | Ada-ECE ( % ) ( Pre T ) | Ada-ECE ( % ) ( Post T ) | + -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -+ -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -+ | CE | 0.05 | CIFAR-10 | ResNet-50 | 4.99 | 3.08 | 1.33 ( 0.9 ) | 3.74 | 2.89 ( 0.8 ) | | CE | 0.05 | CIFAR-10 | ResNet-110 | 5.11 | 1.56 | 1.82 ( 0.9 ) | 3.23 | 2.52 ( 0.9 ) | | CE | 0.05 | CIFAR-100 | ResNet-50 | 22.10 | 7.61 | 4.19 ( 1.1 ) | 7.80 | 6.32 ( 1.1 ) | | CE | 0.05 | CIFAR-100 | ResNet-110 | 23.45 | 10.89 | * All the networks trained on smoothed labels are able to achieve test set accuracies which are in the state-of-the-art ballpark . Moreover , we observe a significant improvement in both ECE and Ada-ECE values before temperature scaling for models trained using cross entropy loss with smoothed labels as compared to models trained using cross entropy loss with one-hot labels . These improvements however , are not reflected in the ECE and Ada-ECE numbers obtained after temperature scaling . * It is quite interesting to note that training on smoothed labels causes the models to become less confident on their predictions in general as we often obtain optimal temperatures which are lower than 1 . This means that temperature scaling for these models is increasing their confidence . On the other hand , optimal temperatures for models trained using cross entropy with one-hot labels are much greater than 1 and hence , temperature scaling is lowering the confidence of these models . We will add the numbers obtained from models trained using cross entropy with smoothed labels ( both with smoothing factors 0.1 and 0.05 ) to the paper . We present the preliminary set of results here ( and not in the paper ) because we need to train other networks ( Wide ResNet , DenseNet , etc . ) as well to have a complete set of results which can then be included in the paper ."}}