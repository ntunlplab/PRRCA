{"year": "2019", "forum": "rkgoyn09KQ", "title": "textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE TOPIC MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR", "decision": "Accept (Poster)", "meta_review": "This paper presents an extension of an existing topic model, DocNADE. Compared to DocNADE and other existing bag-of-word topic models, the primary contribution of this work is to integrate neural language models into the topic model in order to address two limitations of the bag-of-word topic models: expressiveness and interpretability. In addtion, the paper presents an approach to integrate external knowledge into the neural topic models to address the empirical challenges of the application scenarios where there might be only a small training corpus or limited context available. \n\nPros: \nThe paper presents strong and extensive empirical results. The authors went above and beyond to strengthen their paper during the rebuttal and address all the reviewers' questions and suggestions (e.g., the submitted version had 7 baselines, and the revised version has 6 additional baselines per reviewers' requests).\n\nCons:\nThe paper builds on an earlier paper that introduced the DocNADE model. Thus, the modeling contribution is relatively marginal. On the other hand, the extended model, albeit based on a relatively simple idea, is still new and demonstrates strong empirical results.\n\nVerdict:\nProbably accept. While not groundbreaking, the proposed model is new and the empirical results are strong. ", "reviews": [{"review_id": "rkgoyn09KQ-0", "review_text": "DocNADE has great performance so this is a welcome bit of research extending it. There has been a huge amount of activity in combining topic models with (1) embeddings and (2) neural networks such as LSTMs and RNNs. I will say I have great sympathy for the poor author trying to do fair comparisons against start-of-the-art because the standards are moving quickly. In this case, some neural network papers I have seen are TopicRNNs by Dieng, Wang Gao and Paisley, and LLA by Zaheer, Ahmed and Smola. The latter is still a bag-of-words model and but places the LSTM over the sequence of topic proportions. The Gauss-LDA and glove-DMM work is fairly dated (in our fast-paced ML world) and their performance is known to be poor, as some papers in 2017 show. Now I know historically LDA has been fairly poor with IR tasks, but I would expect the recent supervised LDA methods, some also have word embeddings, to do better as well. So the discussion of related work and comparative experiments are poor. If you want to illustrated good improvememts gained using embeddings, it helps to try different proportions, say 20/40/60/80% of a data set and plot. Usually, you should see embeddings aid performance dramatically for smaller fractions of data sets. Hence, your results seem strange. Note the data sets are all fairly small, which makes me wonder about the computation time. Could you give some computational performance stats for a data set? In section 2.2 top of page 5, why is it \"pseudo\" log likelihood? Isn't that formula exact? The paper has a relatively small part devoted to the model, and virtually nothing on the algorithm, although this is probably covered in earlier DocNADE papers. I'm assuming the model is trained by SGD on the log likelihood with all the parameters shoved in there in one go. Is that right? Would be nice to mention whatever it is. The use of four different kinds of evaluations (classification, IR, perplexity, etc.) is good. Note that the improvement over the earlier DocNADE is quite small but clearly significant, and improvement of adding embeddings seems even smaller, though seems better for short texts. I wonder if the method for including embeddings is much good! Not fully convinced. AFTER RESPONSE: Wow guys, what a great revision. Thanks so much.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your positive comments , especially about different kinds of evaluations and improvements . We agree that the fast-paced ML research in this area makes exhaustive comparisons challenging . However , we do attempt to cover the most recent , strong and comparable/fair baselines ( within the limits on submission size ) and have now included several additional baselines in the revised version . More specifically we have followed your suggestions and now include TDLM , Topic-RNN and TCNLM ( `` additional baselines '' , section 3.2 ) in the revised version ( see also Table 3 , Table 7 , Figure 3 and Figure 4 in the revised version ) . However , most recent related studies focus on improving language models ( LMs ) using TMs : in contrast , the focus of our work is on improving TM for textual representations ( short-text or long-text documents ) by incorporating language concepts ( e.g. , word ordering , syntax , semantics , etc . ) via neural LMs . In addition , we also address the challenges of topic learning that arise from limited context in collections of `` short-text '' and/or `` few '' documents . Therefore , as part of our second contribution , we incorporate external knowledge , i.e. , word embeddings into neural autoregressive TM to better model such document collections . It is worth noting that in contrast to this sparse data setting , the related approaches you mention were designed for collections of long-text as well as a sufficient number of documents . `` Additional Baselines '' : We now compare our models to other approaches ( TDLM , Topic-RNN and TCNLM ) combining TMs with LM models . To this end , we follow the most recent approach presented in `` Topic Compositional Neural Language Model '' ( Wang et al. , 2018 ) and quantitatively compare TM performance in terms of topic coherence ( NPMI ) on the BNC dataset . * Additional experimental results on topic coherence * ( comparing LDA , NTM , TDLM , Topic-RNN , TCNLM , DocNADE , ctx-DocNADE and ctx-DocNADEe on the BNC dataset ) are included in the revised version ( Table 7 , Left and Right ) , showing competitive performance of our approach , although the related studies focus on improving topic models . * * Additional experimental results on IR and classification tasks * * : We run additional experiments and execute TDLM on all the short-text datasets to show its performance for the IR and classification tasks . Please see Table 3 in the revised version . In Figure 3 , we have additionally included IR-precision by TDLM model at different fractions and show that our contributions ( i.e. , ctx-DocNADE and ctx-DocNADEe ) outperform TDLM . Remark : Code for the other recent studies is not available . `` embedding gain with different data fraction '' : As suggested , we have now included additional analysis to demonstrate embedding gains with different data fractions . Please , see section 3.4 and Figure 4 . `` pseudo log likelihood '' : p ( v ) is exact for DocNADE ( see details in Larochelle and Lauly ( 2012 ) ) , however not in its extensions ctx-DocNADE and ctx-DocNADEe . While in the proposed models ( as discussed in section 2.2 ) , each autoregressive conditional p ( v_i | v < i ) is a function of h_i and h_i ( Table 1 ) is a function of v < i and context c_i , the likelihood is not exact due to difference in contexts : v < i and c_i . In further detail , v < i is based on the orderless BoW ( and therefore an arbitary order ) where each v_i is the index in the vocabulary . In contrast , c_i is the context of ith word in the document ( and therefore the original word sequence of the context preceeding the ith word ) . Importantly , to compute PPL scores during testing , we set lambda to 0 , allowing a fair comparison between DocNADE and our proposed models ( that is , the test liklihood is exact again ) . `` trained by SGD on the log likelihood '' : Yes . We have now included such details in the revised version . `` model description '' : We have devoted section 2.1 , section 2.2 , algorithm 1 and Table 1 to explain our proposed models as well as DocNADE in more detail . `` performance on including embeddings '' : We have extended DocNADE models specifically to better model short-text or a corpus of few documents ( see also `` motivation2 '' ) ; we have introduced word embeddings to better deal with this data sparsity and hence expect improvements resulting from embeddings mainly for such sparse data settings . For such short-text datasets , ctx-DocNADEe results in substantially improved IR , F1 , PPL and topic coherence : ( 1 ) improved IR ( DocNADE vs ctx-DocNADEe ) : 0.600 vs 0.630 in Table 3 , ( 2 ) improved F1 ( DocNADE vs ctx-DocNADEe ) : 0.683 vs 0.705 in Table 3 , ( 3 ) improved PPL in Table 5 and ( 4 ) improved topic coherence ( DocNADE vs ctx-DocNADEe ) : 0.755 vs 0.790 . As suggested , we have now included additional analysis to demonstrate embedding gains with different data fractions . Please , see section 3.4 and Figure 4 . Given these results on several datasets for the four tasks , we argue that the introduction of embeddings helps in improving topic models , especially in sparse data settings ."}, {"review_id": "rkgoyn09KQ-1", "review_text": " Cons: The proposed method is not novel. For example, Lauly et al., 2017 have proposed a similar way of combining LM and DocNADE. This paper does not provide some motivations or theories behind such artificial combination (i.e., just linearly combine their hidden state) to explain why it works better than other alternatives (e.g., what about adding some linear layers before combining h_i^{DN} and h_i^{LM}). Pros: However, the results seem to be solid and significantly better than the previous state-of-the-art methods. I think some recent neural topic models such as [1,2,3] are still missing even though there are already many tables in the paper (I am not an expert on neural topic modeling or embedding for IR tasks, so there might be others missing state of the arts which I am not aware of). In addition, why does Table 5 only compares perplexity between 3 methods and Table 6 only compares coherence between 4 or 5 methods, while there are 9 or 12 methods are compared in IR task (Table 3 and 4). What's the difficulty of comparing the coherence and perplexity of all different topic models (including [1,2,3])? I will vote for acceptance if the mentioned baselines are also compared or there are good reasons why they cannot be compared. Writing and presentation: The quality of writing should be improved. Here are several examples. 1. In the abstract, the following sentence needs to be rewritten and the rule of capitalization should be consistent. \"(2) Limited Context and/or Smaller training corpus of documents: In settings with a small number of word occurrences (i.e., lack of context) in short text or data sparsity in a corpus of few documents, the application of TMs is challenging.\" 2. I do not understand what's the purpose of the right figure in Figure 1. I think the paper does not do any matching like that. 3. In the 3rd paragraph of the introduction, \"topmost\" -> top most 4. The paper should have a related work section. In addition to the related work discussion scattered in the introduction, authors should discuss the difference between this work and Lauly et al., 2017. Authors should also include some related work such as [1,2,3]. 5. Just below (1), \"where,\" -> , where 6. In the last sentence of the paragraph after (1), you mentioned \"v_{<i} are orderless\", so what's the ordering used in experiments? Random ordering? 7. I guess \"a\" in algorithm 1 means sum_{k<i}(W_{:,v_k}), but I cannot find the explicit explanation about the purpose of \"a\". 8. For ctx-DocNADEe, is W+E the embedding of words at input layer in LM? 9. In the 3rd paragraph of section 2.2, you said: \"each row vector W_{j,:} is a distribution over vocabulary of size K\". Could W has negative values during optimization? If yes, why a distribution representing a topic could have negative value. If no, you should explicitly mention this non-negativity constraint. 10. Why are some values in Table 12 and 13 missing? [1] Cao, Z., Li, S., Liu, Y., Li, W., & Ji, H. (2015, January). A Novel Neural Topic Model and Its Supervised Extension. In AAAI (pp. 2210-2216). [2] Srivastava, A., & Sutton, C. (2017). Autoencoding variational inference for topic models. ICLR [3] Card, D., Tan, C., & Smith, N. A. (2017). A Neural Framework for Generalized Topic Models. arXiv preprint arXiv:1705.09296.", "rating": "7: Good paper, accept", "reply_text": "`` the purpose of the right figure in Figure 1 '' : We illustrate the motivation for incorporating word embeddings into topic models . `` a section about related works '' : We have now included the suggested related works in the revised version where we have motivated our task and contributions , for instance , `` contribution1 '' and `` contribution2 '' in the introduction section . DocNADE-LM ( Lauly et al. , 2017 ) is briefly mentioned in paragraph before the `` contribution1 '' . We have included [ 1 ] ; however , as discussed above [ 2 ] and [ 3 ] have different motivation to our work , although a comparison is provided in the experimental section . Additionally , we have included more related works , such as TDLM , Topic-RNN and TCNLM . `` linearly combine their hidden '' : ELMo ( Peters et al. , 2018 ) , a recent study , demonstrated that the hidden/internal state of each word in LSTM-LM encodes language concepts such as word ordering , syntactic and semantic information . We employ these internal states to improve latent topic vectors , h^DN ( equation 2 ) . In our proposed modeling , we motivate the linear combination of h_i^ { DN } and h_i^ { LM } to maintain architectural simplicity , following Lauly et al . ( 2017 ) .The motivation behind the combination is : For each word v_i of a document v , the hidden state h_ { i } ^ { DN } at the ith autoregressive step encodes topic semantics via DocNADE , while h_ { i } ^ { LM } encodes language concepts via LSTM-LM . However , further investigations about applying linear layers would be an interesting future activity . `` ordering '' : Yes . Random ordering , following DocNADE . `` purpose of `` a '' '' : It is a linear activation , mentioned on page 5 in paragraph before `` ctx-DeepDNEe '' . `` W+E the embedding of words at input layer in LM ? `` : Yes , where W is trainable . `` distribution representing a topic '' : Thanks for your insightful observation . Yes , we have rephrased it though it is the property of DocNADE . As mentioned in section 6.3 ( of Larochelle and Lauly , 2012 ) , the matrix W is being used to compute topics as well as word representations . `` values in Table 12 and 13 of appendices '' : Given the extensive evaluation on 15 datasets in our work , we had to a run a large number of experiments ( > 400 ) and therefore tried to minimize the grid-search . We have included the missing numbers in the appendices of the revised version ."}, {"review_id": "rkgoyn09KQ-2", "review_text": "The paper extends an existing topic model - DocNADE - by replacing the feedforward part of the network which combines the textual context with an LSTM sequence model. Hence this paper fits in a long tradition of work which aims to extend the bad-of-words model from the original LDA paper with some sequence information. The authors do a commendable job in thoroughly evaluating the proposed extension, using a number of evaluations based on perplexity, topic coherence, and text retrieval and categorization. My main problem with the paper as it stands is that it a) arguably oversells the contribution, and b) is unclear when explaining certain crucial aspects of the model. It would also help to have a clearer statement of whether the contribution here is on the document modeling side, or the language modeling side. Motivation is provided from both angles, but the evaluation focuses largely on the topic modeling (which is fine, just need to say it). More specific comments: -- The abstract should mention that the DocNADE model already exists, and that the contribution of the current work is to extend that existing model in a particular way. For those readers unfamiliar with DocNADE, this will help situate the work with regard to the existing literature. Using existing word embeddings as a 'prior' for the LSTM word embeddings is a completely standard alternative now to learning those embeddings from scratch. I'm not sure that can count as a second, major contribution of the paper. (I'm also not sure that either extension to DocNADE warrants a new name, but I'll leave that to the authors' judgement.) I'm confused by one aspect of the DocNADE model: \"the topic assigned ...equally depends all the other words appearing in the same document\". But the model is generative, no? And eqn 1 suggests that each word is generated conditioned on the *previous* words in the document, or did I miss something? Related to this point, DocNADE transforms its BoWs into a sequence. But what's the order? Is it just the order of the words in the document? In which case it's very similar to the LSTM extension, except the LSTM keeps the order in the history, whereas the bag-of-words model doesn't. Relation to generative models: LDA is a generative model with a generative story. It's not completely obvious to me what the generative story is in the new model. Talking about \"distributed compositional priors\" doesn't help, since I'm assuming these aren't priors in a Bayesian modeling sense? (It's also not clear in what sense these \"priors\" are compositional, but that's a separate question.) Equation 2: what's the motivation for mixing the LSTM history with the bag-of-words (esp. if the history is from the same bag of words in each case). Why not just use the LSTM? It would be useful to state in the main body of the text what the value of lambda ends up being. In 3.1 there's a suggestion this might be 0.01, but that effectively ignores the LSTM? Similar question: how can the DocNADE model provide a *global* context if the model is generative? Perplexity is a reasonable thing to measure, but presumably the auto-regressive nature of the LSTM means that it's more-or-less guaranteed to do better than a bag-of-words model? I wonder if it's worth acknowledging this fact? I don't understand why lambda has to be zero \"to compute the exact log-likelihood\". The first line of the conclusion doesn't say much: it's pretty obvious that the ordering of the words is going to help better estimate the probability of a word in a given context; 50 years of language modeling research has already taught us that. Minor presentational comments: -- Some of the hyphenation looks odd, eg in the title. Are you using the standard LaTeX hyphenation settings? Strictly speaking, I'm not sure that 'bear' in the example is a proper noun. \"orderless sets of words\": bags, not sets, since the counts matter, no? The tables are too small, with a lot of numbers in them. One option is to move some of the details to the Appendix. Either way there needs to be more summary in the main body explaining what the numbers tell us. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your review comments and positive feedback about evaluation : `` commendable job in thoroughly evaluating '' . Please , see Table 7 , Table 3 , Figure 3 and Figure 4 in the revised version for additional experimental results . `` explaining certain crucial aspects '' : We have tried to better motivate our tasks/contributions in the revised version , along with recent studies ( including the suggested baselines ) in the introduction and baseline sections . We would appreciate if could specifically point out the crucial aspects , in case we missed any . `` clearer statement ... which is fine , just need to say it '' : We state several times throughout the paper ( e.g. , in the abstract , contribution1 , contribution2 , etc . ) that we focus on improving topic models using language concepts learned from language models . Based on your feedback , we now also emphasize this in the evaluation section ( 1st paragraph ) and also updated the title . `` embeddings in LSTM '' : We clarify that our contribiution2 is about incorporating word embeddings into topic models to handle data sparsity challenges in topic models . To this end , we incorporate word embeddings into topic models via an LSTM and compare our contribution , i.e. , ctx-DocNADEe with related works , such as glove-LDA , glove-DMM and gaussian-LDA that also extend topic models with word embedding features . Our modeling approach offers an easy integration of word embeddings into topic models , that substantially improves topic coherence . `` DocNADE : generative and global semantics '' : DocNADE is a generative model with orderless BoW input . As detailed in equation 12 ( Larochelle and Lauly , 2012 ) , a random permutation of its words is used to produce the observed document - this random order is re-shuffled during learning so that all information related to the original word order is lost . The order of the input in DocNADE is arbitary , while the original order of words in the document is presented via LSTM . Our contribution of combining the two networks is further motivated by recent studies , such as ELMo ( Peters et al. , 2018 ) that have shown that internal states of LSTM-LM capture language concepts ( such as word order , syntactic and semantic information ) . Therefore , we have extended DocNADE by introducing these language concepts and shown improvements on different tasks using 15 datasets . `` Prior and compositional '' : The term 'prior ' is used for the external information i.e. , word embeddings encoding word relatedness in distributional semantic space . The term 'compositional ' is used for the compositional property of RNN-LSTM , where LSTM-LM generates latent vectors ( i.e. , internal states of LSTM ) for each input context via sequential compositionality . Each of the internal states for the corresponding word sequence captures language concepts that are presented to DocNADE , correspondingly at each of the autoregressive steps . `` history '' : The two history vectors : v < i and c_i are two different representations , in the sense that v < i is sampled from orderless BoW . This arbitary sequence is different to c_i in the sense that c_i is the original sequence of preceding words for the ith word in the document . The motivation is to combine the benefits of the two networks ( i.e. , DocNADE and LSTM-LM ) to improve topic models , i.e. , introduce local dynamics into global semantics of topic models . We control the mixture via a mixture weight $ \\lambda $ . `` lambda values '' : It is a hyperparamter , determined using the validation set ; optimal values vary between datasets . Please see appendices ( Table 13 ) for the ablation study . `` perplexity measure '' : As discussed , we set lambda to 0 during evaluation , i.e. , no LSTM component is used during testing . We only exploit LSTM-LM during training to learn language concepts , encoded in the W matrix . Beyond perplexity , we also perform topic coherence , text retrieval and classification tasks . `` pseudo log likelihood '' : p ( v ) is exact for DocNADE ( see details in Larochelle and Lauly ( 2012 ) ) , however not in its extensions ctx-DocNADE and ctx-DocNADEe . While in the proposed models ( as discussed in section 2.2 ) , each autoregressive conditional p ( v_i | v < i ) is a function of h_i and h_i ( Table 1 ) is a function of v < i and context c_i , the likelihood is not exact due to difference in contexts : v < i and c_i . In further detail , v < i is based on the orderless BoW ( and therefore an arbitary order ) where each v_i is the index in the vocabulary . In contrast , c_i is the context of ith word in the document ( and therefore the original word sequence of the context preceding the ith word ) . Importantly , to compute PPL scores during testing , we set lambda to 0 , allowing a fair comparison between DocNADE and our proposed models ( that is , the test liklihood is exact again ) . `` first line of conclusion '' : We have updated the conclusion section . `` using the standard LaTeX hyphenation settings ? `` : Yes . `` bear as proper noun '' : We use stanford parser ( http : //nlp.stanford.edu:8080/parser/index.jsp ) to perform tagging ."}], "0": {"review_id": "rkgoyn09KQ-0", "review_text": "DocNADE has great performance so this is a welcome bit of research extending it. There has been a huge amount of activity in combining topic models with (1) embeddings and (2) neural networks such as LSTMs and RNNs. I will say I have great sympathy for the poor author trying to do fair comparisons against start-of-the-art because the standards are moving quickly. In this case, some neural network papers I have seen are TopicRNNs by Dieng, Wang Gao and Paisley, and LLA by Zaheer, Ahmed and Smola. The latter is still a bag-of-words model and but places the LSTM over the sequence of topic proportions. The Gauss-LDA and glove-DMM work is fairly dated (in our fast-paced ML world) and their performance is known to be poor, as some papers in 2017 show. Now I know historically LDA has been fairly poor with IR tasks, but I would expect the recent supervised LDA methods, some also have word embeddings, to do better as well. So the discussion of related work and comparative experiments are poor. If you want to illustrated good improvememts gained using embeddings, it helps to try different proportions, say 20/40/60/80% of a data set and plot. Usually, you should see embeddings aid performance dramatically for smaller fractions of data sets. Hence, your results seem strange. Note the data sets are all fairly small, which makes me wonder about the computation time. Could you give some computational performance stats for a data set? In section 2.2 top of page 5, why is it \"pseudo\" log likelihood? Isn't that formula exact? The paper has a relatively small part devoted to the model, and virtually nothing on the algorithm, although this is probably covered in earlier DocNADE papers. I'm assuming the model is trained by SGD on the log likelihood with all the parameters shoved in there in one go. Is that right? Would be nice to mention whatever it is. The use of four different kinds of evaluations (classification, IR, perplexity, etc.) is good. Note that the improvement over the earlier DocNADE is quite small but clearly significant, and improvement of adding embeddings seems even smaller, though seems better for short texts. I wonder if the method for including embeddings is much good! Not fully convinced. AFTER RESPONSE: Wow guys, what a great revision. Thanks so much.", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for your positive comments , especially about different kinds of evaluations and improvements . We agree that the fast-paced ML research in this area makes exhaustive comparisons challenging . However , we do attempt to cover the most recent , strong and comparable/fair baselines ( within the limits on submission size ) and have now included several additional baselines in the revised version . More specifically we have followed your suggestions and now include TDLM , Topic-RNN and TCNLM ( `` additional baselines '' , section 3.2 ) in the revised version ( see also Table 3 , Table 7 , Figure 3 and Figure 4 in the revised version ) . However , most recent related studies focus on improving language models ( LMs ) using TMs : in contrast , the focus of our work is on improving TM for textual representations ( short-text or long-text documents ) by incorporating language concepts ( e.g. , word ordering , syntax , semantics , etc . ) via neural LMs . In addition , we also address the challenges of topic learning that arise from limited context in collections of `` short-text '' and/or `` few '' documents . Therefore , as part of our second contribution , we incorporate external knowledge , i.e. , word embeddings into neural autoregressive TM to better model such document collections . It is worth noting that in contrast to this sparse data setting , the related approaches you mention were designed for collections of long-text as well as a sufficient number of documents . `` Additional Baselines '' : We now compare our models to other approaches ( TDLM , Topic-RNN and TCNLM ) combining TMs with LM models . To this end , we follow the most recent approach presented in `` Topic Compositional Neural Language Model '' ( Wang et al. , 2018 ) and quantitatively compare TM performance in terms of topic coherence ( NPMI ) on the BNC dataset . * Additional experimental results on topic coherence * ( comparing LDA , NTM , TDLM , Topic-RNN , TCNLM , DocNADE , ctx-DocNADE and ctx-DocNADEe on the BNC dataset ) are included in the revised version ( Table 7 , Left and Right ) , showing competitive performance of our approach , although the related studies focus on improving topic models . * * Additional experimental results on IR and classification tasks * * : We run additional experiments and execute TDLM on all the short-text datasets to show its performance for the IR and classification tasks . Please see Table 3 in the revised version . In Figure 3 , we have additionally included IR-precision by TDLM model at different fractions and show that our contributions ( i.e. , ctx-DocNADE and ctx-DocNADEe ) outperform TDLM . Remark : Code for the other recent studies is not available . `` embedding gain with different data fraction '' : As suggested , we have now included additional analysis to demonstrate embedding gains with different data fractions . Please , see section 3.4 and Figure 4 . `` pseudo log likelihood '' : p ( v ) is exact for DocNADE ( see details in Larochelle and Lauly ( 2012 ) ) , however not in its extensions ctx-DocNADE and ctx-DocNADEe . While in the proposed models ( as discussed in section 2.2 ) , each autoregressive conditional p ( v_i | v < i ) is a function of h_i and h_i ( Table 1 ) is a function of v < i and context c_i , the likelihood is not exact due to difference in contexts : v < i and c_i . In further detail , v < i is based on the orderless BoW ( and therefore an arbitary order ) where each v_i is the index in the vocabulary . In contrast , c_i is the context of ith word in the document ( and therefore the original word sequence of the context preceeding the ith word ) . Importantly , to compute PPL scores during testing , we set lambda to 0 , allowing a fair comparison between DocNADE and our proposed models ( that is , the test liklihood is exact again ) . `` trained by SGD on the log likelihood '' : Yes . We have now included such details in the revised version . `` model description '' : We have devoted section 2.1 , section 2.2 , algorithm 1 and Table 1 to explain our proposed models as well as DocNADE in more detail . `` performance on including embeddings '' : We have extended DocNADE models specifically to better model short-text or a corpus of few documents ( see also `` motivation2 '' ) ; we have introduced word embeddings to better deal with this data sparsity and hence expect improvements resulting from embeddings mainly for such sparse data settings . For such short-text datasets , ctx-DocNADEe results in substantially improved IR , F1 , PPL and topic coherence : ( 1 ) improved IR ( DocNADE vs ctx-DocNADEe ) : 0.600 vs 0.630 in Table 3 , ( 2 ) improved F1 ( DocNADE vs ctx-DocNADEe ) : 0.683 vs 0.705 in Table 3 , ( 3 ) improved PPL in Table 5 and ( 4 ) improved topic coherence ( DocNADE vs ctx-DocNADEe ) : 0.755 vs 0.790 . As suggested , we have now included additional analysis to demonstrate embedding gains with different data fractions . Please , see section 3.4 and Figure 4 . Given these results on several datasets for the four tasks , we argue that the introduction of embeddings helps in improving topic models , especially in sparse data settings ."}, "1": {"review_id": "rkgoyn09KQ-1", "review_text": " Cons: The proposed method is not novel. For example, Lauly et al., 2017 have proposed a similar way of combining LM and DocNADE. This paper does not provide some motivations or theories behind such artificial combination (i.e., just linearly combine their hidden state) to explain why it works better than other alternatives (e.g., what about adding some linear layers before combining h_i^{DN} and h_i^{LM}). Pros: However, the results seem to be solid and significantly better than the previous state-of-the-art methods. I think some recent neural topic models such as [1,2,3] are still missing even though there are already many tables in the paper (I am not an expert on neural topic modeling or embedding for IR tasks, so there might be others missing state of the arts which I am not aware of). In addition, why does Table 5 only compares perplexity between 3 methods and Table 6 only compares coherence between 4 or 5 methods, while there are 9 or 12 methods are compared in IR task (Table 3 and 4). What's the difficulty of comparing the coherence and perplexity of all different topic models (including [1,2,3])? I will vote for acceptance if the mentioned baselines are also compared or there are good reasons why they cannot be compared. Writing and presentation: The quality of writing should be improved. Here are several examples. 1. In the abstract, the following sentence needs to be rewritten and the rule of capitalization should be consistent. \"(2) Limited Context and/or Smaller training corpus of documents: In settings with a small number of word occurrences (i.e., lack of context) in short text or data sparsity in a corpus of few documents, the application of TMs is challenging.\" 2. I do not understand what's the purpose of the right figure in Figure 1. I think the paper does not do any matching like that. 3. In the 3rd paragraph of the introduction, \"topmost\" -> top most 4. The paper should have a related work section. In addition to the related work discussion scattered in the introduction, authors should discuss the difference between this work and Lauly et al., 2017. Authors should also include some related work such as [1,2,3]. 5. Just below (1), \"where,\" -> , where 6. In the last sentence of the paragraph after (1), you mentioned \"v_{<i} are orderless\", so what's the ordering used in experiments? Random ordering? 7. I guess \"a\" in algorithm 1 means sum_{k<i}(W_{:,v_k}), but I cannot find the explicit explanation about the purpose of \"a\". 8. For ctx-DocNADEe, is W+E the embedding of words at input layer in LM? 9. In the 3rd paragraph of section 2.2, you said: \"each row vector W_{j,:} is a distribution over vocabulary of size K\". Could W has negative values during optimization? If yes, why a distribution representing a topic could have negative value. If no, you should explicitly mention this non-negativity constraint. 10. Why are some values in Table 12 and 13 missing? [1] Cao, Z., Li, S., Liu, Y., Li, W., & Ji, H. (2015, January). A Novel Neural Topic Model and Its Supervised Extension. In AAAI (pp. 2210-2216). [2] Srivastava, A., & Sutton, C. (2017). Autoencoding variational inference for topic models. ICLR [3] Card, D., Tan, C., & Smith, N. A. (2017). A Neural Framework for Generalized Topic Models. arXiv preprint arXiv:1705.09296.", "rating": "7: Good paper, accept", "reply_text": "`` the purpose of the right figure in Figure 1 '' : We illustrate the motivation for incorporating word embeddings into topic models . `` a section about related works '' : We have now included the suggested related works in the revised version where we have motivated our task and contributions , for instance , `` contribution1 '' and `` contribution2 '' in the introduction section . DocNADE-LM ( Lauly et al. , 2017 ) is briefly mentioned in paragraph before the `` contribution1 '' . We have included [ 1 ] ; however , as discussed above [ 2 ] and [ 3 ] have different motivation to our work , although a comparison is provided in the experimental section . Additionally , we have included more related works , such as TDLM , Topic-RNN and TCNLM . `` linearly combine their hidden '' : ELMo ( Peters et al. , 2018 ) , a recent study , demonstrated that the hidden/internal state of each word in LSTM-LM encodes language concepts such as word ordering , syntactic and semantic information . We employ these internal states to improve latent topic vectors , h^DN ( equation 2 ) . In our proposed modeling , we motivate the linear combination of h_i^ { DN } and h_i^ { LM } to maintain architectural simplicity , following Lauly et al . ( 2017 ) .The motivation behind the combination is : For each word v_i of a document v , the hidden state h_ { i } ^ { DN } at the ith autoregressive step encodes topic semantics via DocNADE , while h_ { i } ^ { LM } encodes language concepts via LSTM-LM . However , further investigations about applying linear layers would be an interesting future activity . `` ordering '' : Yes . Random ordering , following DocNADE . `` purpose of `` a '' '' : It is a linear activation , mentioned on page 5 in paragraph before `` ctx-DeepDNEe '' . `` W+E the embedding of words at input layer in LM ? `` : Yes , where W is trainable . `` distribution representing a topic '' : Thanks for your insightful observation . Yes , we have rephrased it though it is the property of DocNADE . As mentioned in section 6.3 ( of Larochelle and Lauly , 2012 ) , the matrix W is being used to compute topics as well as word representations . `` values in Table 12 and 13 of appendices '' : Given the extensive evaluation on 15 datasets in our work , we had to a run a large number of experiments ( > 400 ) and therefore tried to minimize the grid-search . We have included the missing numbers in the appendices of the revised version ."}, "2": {"review_id": "rkgoyn09KQ-2", "review_text": "The paper extends an existing topic model - DocNADE - by replacing the feedforward part of the network which combines the textual context with an LSTM sequence model. Hence this paper fits in a long tradition of work which aims to extend the bad-of-words model from the original LDA paper with some sequence information. The authors do a commendable job in thoroughly evaluating the proposed extension, using a number of evaluations based on perplexity, topic coherence, and text retrieval and categorization. My main problem with the paper as it stands is that it a) arguably oversells the contribution, and b) is unclear when explaining certain crucial aspects of the model. It would also help to have a clearer statement of whether the contribution here is on the document modeling side, or the language modeling side. Motivation is provided from both angles, but the evaluation focuses largely on the topic modeling (which is fine, just need to say it). More specific comments: -- The abstract should mention that the DocNADE model already exists, and that the contribution of the current work is to extend that existing model in a particular way. For those readers unfamiliar with DocNADE, this will help situate the work with regard to the existing literature. Using existing word embeddings as a 'prior' for the LSTM word embeddings is a completely standard alternative now to learning those embeddings from scratch. I'm not sure that can count as a second, major contribution of the paper. (I'm also not sure that either extension to DocNADE warrants a new name, but I'll leave that to the authors' judgement.) I'm confused by one aspect of the DocNADE model: \"the topic assigned ...equally depends all the other words appearing in the same document\". But the model is generative, no? And eqn 1 suggests that each word is generated conditioned on the *previous* words in the document, or did I miss something? Related to this point, DocNADE transforms its BoWs into a sequence. But what's the order? Is it just the order of the words in the document? In which case it's very similar to the LSTM extension, except the LSTM keeps the order in the history, whereas the bag-of-words model doesn't. Relation to generative models: LDA is a generative model with a generative story. It's not completely obvious to me what the generative story is in the new model. Talking about \"distributed compositional priors\" doesn't help, since I'm assuming these aren't priors in a Bayesian modeling sense? (It's also not clear in what sense these \"priors\" are compositional, but that's a separate question.) Equation 2: what's the motivation for mixing the LSTM history with the bag-of-words (esp. if the history is from the same bag of words in each case). Why not just use the LSTM? It would be useful to state in the main body of the text what the value of lambda ends up being. In 3.1 there's a suggestion this might be 0.01, but that effectively ignores the LSTM? Similar question: how can the DocNADE model provide a *global* context if the model is generative? Perplexity is a reasonable thing to measure, but presumably the auto-regressive nature of the LSTM means that it's more-or-less guaranteed to do better than a bag-of-words model? I wonder if it's worth acknowledging this fact? I don't understand why lambda has to be zero \"to compute the exact log-likelihood\". The first line of the conclusion doesn't say much: it's pretty obvious that the ordering of the words is going to help better estimate the probability of a word in a given context; 50 years of language modeling research has already taught us that. Minor presentational comments: -- Some of the hyphenation looks odd, eg in the title. Are you using the standard LaTeX hyphenation settings? Strictly speaking, I'm not sure that 'bear' in the example is a proper noun. \"orderless sets of words\": bags, not sets, since the counts matter, no? The tables are too small, with a lot of numbers in them. One option is to move some of the details to the Appendix. Either way there needs to be more summary in the main body explaining what the numbers tell us. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for your review comments and positive feedback about evaluation : `` commendable job in thoroughly evaluating '' . Please , see Table 7 , Table 3 , Figure 3 and Figure 4 in the revised version for additional experimental results . `` explaining certain crucial aspects '' : We have tried to better motivate our tasks/contributions in the revised version , along with recent studies ( including the suggested baselines ) in the introduction and baseline sections . We would appreciate if could specifically point out the crucial aspects , in case we missed any . `` clearer statement ... which is fine , just need to say it '' : We state several times throughout the paper ( e.g. , in the abstract , contribution1 , contribution2 , etc . ) that we focus on improving topic models using language concepts learned from language models . Based on your feedback , we now also emphasize this in the evaluation section ( 1st paragraph ) and also updated the title . `` embeddings in LSTM '' : We clarify that our contribiution2 is about incorporating word embeddings into topic models to handle data sparsity challenges in topic models . To this end , we incorporate word embeddings into topic models via an LSTM and compare our contribution , i.e. , ctx-DocNADEe with related works , such as glove-LDA , glove-DMM and gaussian-LDA that also extend topic models with word embedding features . Our modeling approach offers an easy integration of word embeddings into topic models , that substantially improves topic coherence . `` DocNADE : generative and global semantics '' : DocNADE is a generative model with orderless BoW input . As detailed in equation 12 ( Larochelle and Lauly , 2012 ) , a random permutation of its words is used to produce the observed document - this random order is re-shuffled during learning so that all information related to the original word order is lost . The order of the input in DocNADE is arbitary , while the original order of words in the document is presented via LSTM . Our contribution of combining the two networks is further motivated by recent studies , such as ELMo ( Peters et al. , 2018 ) that have shown that internal states of LSTM-LM capture language concepts ( such as word order , syntactic and semantic information ) . Therefore , we have extended DocNADE by introducing these language concepts and shown improvements on different tasks using 15 datasets . `` Prior and compositional '' : The term 'prior ' is used for the external information i.e. , word embeddings encoding word relatedness in distributional semantic space . The term 'compositional ' is used for the compositional property of RNN-LSTM , where LSTM-LM generates latent vectors ( i.e. , internal states of LSTM ) for each input context via sequential compositionality . Each of the internal states for the corresponding word sequence captures language concepts that are presented to DocNADE , correspondingly at each of the autoregressive steps . `` history '' : The two history vectors : v < i and c_i are two different representations , in the sense that v < i is sampled from orderless BoW . This arbitary sequence is different to c_i in the sense that c_i is the original sequence of preceding words for the ith word in the document . The motivation is to combine the benefits of the two networks ( i.e. , DocNADE and LSTM-LM ) to improve topic models , i.e. , introduce local dynamics into global semantics of topic models . We control the mixture via a mixture weight $ \\lambda $ . `` lambda values '' : It is a hyperparamter , determined using the validation set ; optimal values vary between datasets . Please see appendices ( Table 13 ) for the ablation study . `` perplexity measure '' : As discussed , we set lambda to 0 during evaluation , i.e. , no LSTM component is used during testing . We only exploit LSTM-LM during training to learn language concepts , encoded in the W matrix . Beyond perplexity , we also perform topic coherence , text retrieval and classification tasks . `` pseudo log likelihood '' : p ( v ) is exact for DocNADE ( see details in Larochelle and Lauly ( 2012 ) ) , however not in its extensions ctx-DocNADE and ctx-DocNADEe . While in the proposed models ( as discussed in section 2.2 ) , each autoregressive conditional p ( v_i | v < i ) is a function of h_i and h_i ( Table 1 ) is a function of v < i and context c_i , the likelihood is not exact due to difference in contexts : v < i and c_i . In further detail , v < i is based on the orderless BoW ( and therefore an arbitary order ) where each v_i is the index in the vocabulary . In contrast , c_i is the context of ith word in the document ( and therefore the original word sequence of the context preceding the ith word ) . Importantly , to compute PPL scores during testing , we set lambda to 0 , allowing a fair comparison between DocNADE and our proposed models ( that is , the test liklihood is exact again ) . `` first line of conclusion '' : We have updated the conclusion section . `` using the standard LaTeX hyphenation settings ? `` : Yes . `` bear as proper noun '' : We use stanford parser ( http : //nlp.stanford.edu:8080/parser/index.jsp ) to perform tagging ."}}