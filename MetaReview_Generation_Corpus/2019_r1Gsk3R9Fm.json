{"year": "2019", "forum": "r1Gsk3R9Fm", "title": "Shallow Learning For Deep Networks", "decision": "Reject", "meta_review": "The paper discusses layer-wise training of deep networks. The authors show that it's possible to achieve reasonable performance by training deep nets layer by layer, as opposed to now widely adopted end-to-end training. While such a training procedure is not novel, the authors argue that this is an interesting result, considering that such a training procedure is often dismissed as sub-optimal and leading to inferior results. However, the results show exactly that, as the performance of the models is significantly worse than the state of the art, and it is unclear what other advantages such a training scheme can offer. The authors mention that layer-wise training could be useful for theoretical understanding of deep nets, but they don\u2019t really perform such analysis in this submission, and it\u2019s also unclear whether conclusions of such analysis would extend to deep nets trained end-to-end.\n\nIn its current form, the paper is not ready for acceptance. I encourage the authors to make a more clear case for the method: either by improving results to match end-to-end training, or by actually demonstrating that layer-wise training has certain advantages over end-to-end learning.\n", "reviews": [{"review_id": "r1Gsk3R9Fm-0", "review_text": " The authors propose to train deep convolutional neural networks in a layerwise fashion. This is contrary to the traditional joint end-to-end training of deep CNNs. As their motivation, the authors quote computational and memory benefits at the time of training in addition to being able to extend shallow-network analytical frameworks to the individual network layers thus allowing for a theoretical interpretation of their optima. Their method is simple and clearly explained. (Note: In the 10th line on Pg. 4, is there a 'j' missing in the subscript of x^n?) The experimental results are interesting. The authors are able to demonstrate 'some' architecture that, in solely a layerwise training, is able to show competitive results with respect to AlexNet when trained in an end-to-end manner on ImageNet. These results can seem questionable as both the architectures and training routines are being varied and hence the precise contribution of the layerwise training is unclear. However, as per my understanding, the aim of the paper wasn't to show that a layerwise training can work better that end-to-end training. The aim was, on the contrary to show, that 'even' a layerwise training can offer competitive performance for 'some' network and hence may come handy when memory is limited. Their underlying claim, which could be more clearly stated is that the memory benefits during training can be enjoyed when the individual layers of a network (net1) are smaller in parameter count as compared to another net (net2) although the net1 in totality maybe larger than net2. This is because net1 will be trained in a layerwise fashion, while net2 would be trained in an end-to-end manner. I would like to see the authors confirm or reject this understanding and rationalise their experimental regimen. Further, I would like to know how their work compares to the following: https://arxiv.org/abs/1703.07115 https://arxiv.org/abs/1611.02185 Finally, while the authors state that the layerwise training makes the individual layers amenable to theoretical analysis/interpretation, no such discussion is presented/initiated in the paper. The only analysis presented is on the ability of the individually trained layers to linear separate the data. To round the analysis, it should also be extended to the representations learned by end-to-end trained networks. All in all, while the paper raises some interesting ideas, its execution in terms of a method that learns a classifier on each individual layer is rather simplistic. Don't get me wrong, simple can indeed be elegant, but at the minute the comparisons are not very convincing. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for the review that will help us to improve the general quality of the paper . We answer below to each of your points . < < As their motivation , the authors quote computational and memory benefits at the time of training in addition to being able to extend shallow-network analytical frameworks to the individual network layers thus allowing for a theoretical interpretation of their optima. > > The computation and memory are indeed a potential benefit of greedy layerwise learning but it is not the only and not the primary motivation we have for greedy layerwise learning in the paper . Our paper emphasizes that it is easier to apply existing theoretical results to layerwise learning methods and that layerwise learning methods induce properties in the network that allow to more clearly study the behavior of each layer . The motivation of our work is to empirically determine if such greedy layerwise methods are viable ( something not demonstrated in any related literature ) . In terms of the potential advantages of greedy layerwise methods we discuss extensively the ramifications for theoretical study of deep networks by giving a concrete example for optimization in Prop 3.2 and an empirical study that can only be done in this context in Sec 4.2 . See also general comment and response to R1 . < < Their method is simple and clearly explained . ( Note : In the 10th line on Pg.4 , is there a ' j ' missing in the subscript of x^n ? ) > > Thanks , we have fixed the typo < < These results can seem questionable as both the architectures and training routines are being varied and hence the precise contribution of the layerwise training is unclear. > > We would like to emphasize that in Sec.4.3 , we obtain VGG-performance using an extension of the basic k=1 approach . We believe the reviewer is referring to the indirect comparison in Sec 4.1 to AlexNet without an e2e comparison . Please correct us if we have misinterpreted . We apologize for confusion for this and have addressed this point regarding the goal of 4.1 in the response we give to R1 and the general comment . To summarize briefly : -Sec 4.1 was aimed to show the most basic greedy layerwise method k=1 can already reach a performance barrier only obtained by deep learning methods on imagenet . For completeness we now report e2e comparison on CIFAR for same architecture . -Sec 4.3 already reports a very direct comparison , with the imagenet dataset , using end-to-end training and greedy layerwise training on the same architecture using the same number of gradient updates for each layer , and an architecture which performs as well as VGG in the e2e setting while being more compact . < < Their underlying claim , which could be more clearly stated is that the memory benefits during training can be enjoyed when the individual layers of a network ( net1 ) are smaller in parameter count as compared to another net ( net2 ) although the net1 in totality maybe larger than net2 .... > > This is indeed a stated potential practical advantage ( and one we have benefited from in our own experiments ) . However , as emphasized in the paper in the introduction and Sec 3.3 and 4.3 there are many other reasons greedy layerwise learning is relevant . Our work is the only one that has shown this class of methods is potentially viable for real problems . < < Further , I would like to know how their work compares to the following : https : //arxiv.org/abs/1703.07115 https : //arxiv.org/abs/1611.02185 > > Thanks for the references . We believe the contrast to these highlights well the contribution of our paper and have added them to the manuscript . The first one considers a layerwise supervised procedure and is indeed related in spirit . However , it shows numerical results only on the MNIST dataset of < 96 % and less than 50 % on CIFAR . These are trivial accuracies even for those simple datasets , achievable without need for deep learning methods and do not in any way demonstrate that greedy layerwise construction can be used to build deep networks that show the kind of results that have made deep learning successful . The second reference is a layerwise scheme , but not greedy : although one layer is updated at a time the problem is solved jointly . This method is however an alternative to standard end-to-end back-propagation but we wish to bring attention that it is only shown to work as a strategy applied after typical end-to-end back-propagation . Our work is the first that shows greedy layerwise methods can achieve accuracies only obtained by e2e deep learning , and to the best of our knowledge it is the first alternative to end-to-end back-propagation that scales to imagenet and does not fall back on end to end learning ( contrary to e.g.the 2nd reference which does ) ."}, {"review_id": "r1Gsk3R9Fm-1", "review_text": "This paper is of reasonable quality and clarity, rather modest originality, perhaps considerable significance in some applications. Strengths: - I think this kind of method could be useful for data of very high dimensionality, when it is not possible to train everything end to end. - The experiments seem to be conducted correctly. - The paper is well written. Weaknesses: - (minor) Abstract: it's kind of funny to say that CIFAR-10 is a large scale image recognition problem. - What the authors are proposing is quite similar to Lee et al. [1], which was not mentioned in the paper as well as a wide range of papers, which were mentioned. I think it is kind interesting for people to revise these techniques from 10 years ago, but this method is just not that novel. - The authors highlight that their goal is not using this method as a pre-training strategy, but it would be interesting to see whether it would indeed work better if after the layer-wise training, the whole network would be trained end-to-end. - Maths in this paper is mostly decorative. - When comparing different models or training methods (e.g. layer-wise trained AlexNet and end-to-end trained AlexNet), it would make sense to do some hyperparameter search. It is very risky to conclude anything otherwise. - I would like to see a wall clock time comparison between this and end-to-end training. [1] Lee et al. Deeply-Supervised Nets. 2015.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer 1 , We would like to thank you very much for your review and time . We answer to your points below , < < ( minor ) Abstract : it 's kind of funny to say that CIFAR-10 is a large scale image recognition problem > > We agree with you . This was a typo aimed to simplify the sentence flow and we have corrected this statement in the uploaded revision . < < What the authors are proposing is quite similar to Lee et al . [ 1 ] , which was not mentioned in the paper as well as a wide range of papers , which were mentioned. > > Thank you for the reference - we added it in the revision . While there is some similarity in the final overall architecture using an auxiliary objective , Note that they use a completely different training technique ( end to end ) and their contribution bears little resemblance overall to our work . Indeed , the model in Lee et al.is trained end-to-end and the contribution is aimed at stabilizing/guiding end-to-end training . Our work focuses on the viability of a training method that utilizes individually solved auxiliary problems only . Please also see the extensive literature review which highlights major differences to our work from many other potentially related ones . < < I think it is kind interesting for people to revise these techniques from 10 years ago , but this method is just not that novel. > > We agree that the high-level technique has similarities to a few specific existing works , mainly ( Huang et al . ( 2017 ) , Bengio et al . ( 2006 ) ) .Because our goal here was not to propose an extremely specific technique but to test the assumption that end-to-end learning of layers is essential , we consider it useful and interesting that we were able to achieve our result with a relatively common approach . We believe that our - counterintuitive - numerical results alone represent a major contribution to the literature and are a significant novelty . As a more minor but still substantial contribution , there has not been any scalable ( even just computationally speaking ) greedy layerwise method in the literature for CNNs . Thus even the specific proposed method has some novel aspects . Please also see our general comment discussion on this point ( above ) . < < The authors highlight that their goal is not using this method as a pre-training strategy , but it would be interesting to see whether it would indeed work better if after the layer-wise training , the whole network would be trained end-to-end. > > We think this is an interesting idea and could potentially work with careful investigation , but it is rather outside of the scope of this paper . Many works considering layerwise methods ( e.g.Huang et al . ( 2017 ) ) resort to a final end to end learning phases to make the results more competitive , but this portrays the message that end-to-end learning is an absolutely necessary condition to get competitive performance . This is contrary to the message we are trying to convey : that greedy layerwise learning without end-to-end works much better than a large portion of the community would expect ."}, {"review_id": "r1Gsk3R9Fm-2", "review_text": "Summary: This paper proposes layer wise training of neural networks using classification auxiliary tasks for training each layer. Experiments are presented on CIFAR10 and Imagenet. Accuracies close to end to end training are obtained. The layer wise training is repeated for J steps, the auxiliary tasks are trained on top of the shallow one layer (of width M ) with a network of depth k and width tilde{M}. Layerwise training is done using sgd with momentum, and the learning rate is decayed through epochs. Note that the layer wise training is done with large width M than typical end to end networks in use. The authors argue and test the hypothesis that auxiliary tasks encourage the linear separability of CNN features. To reduce the size of the learned network the author propose a layer wise compression using the filter removal technique of Molchanov et al . Reproducibility: This empirical work has been investigated for a while with mild success, the authors should make their code available to the community to confirm and reproduce their findings. I encourage the authors to make their code available during the review/discussion period. Significance of the work: From reading the paper it is not clear what is the main ingredient that makes this layer wise training successful, negative results would help in understanding what is important for the accuracy. Some more ablation studies and negative results will be insightful and here are few suggestions in that direction: - Authors claim that they used invertible downsampling as max or average pooling lead to information loss. Does the layer wise training give worst results with average or max pooling? If so please report those numbers to know what is the implications of this choice of pooling. - On the width of the networks, seems it is key for the success of the approach. What if you train wider networks with J that is small? ( J=3 for instance but much wider networks, instead of J=8 now for imagenet.) - To answer the same question above one needs also to see what are the accuracies for J=8 with thiner networks (smaller M )? - Would the accuracy with the layer wise training reach a plateau if one uses an architecture with J higher than 8? - Transferability of the learned features: end to end features are know to be transferable. It would be good to see if this still holds using the network layer wise trained on imagenet for CIFAR10 or other datasets. Other Questions: - Section 3.2 is vague. In Proposition 3.1 and Proposition 3.2 can you add some text to explain what are the implicitions of the claims? \u201cThus our training permits to extend some results on shallow CNN to deeper CNNs \u2026\u201d which shallow results ? - \u201cFor k>1 batch normalization was useful \u201c is this only on the auxiliary problems networks or you used also batch norm for the layer ? - The ensemble used is Z=\\sum_{j=1}^J 2^j z_j , this uses the network of J layers , also the O(J) auxiliary networks of depth k. Please report the number of parameters for all models (single and ensembles) in Table 1 and Table 2. - In the conclusion: \u201cThe framework can be extendable to the parallel training \u2026\u201d how would this possible since one needs the output of the first training to do the training of the next layer. can you elaborate on what is meant here? Minor: page 2 bottom have competitorsand -> have competitors and the non linearity rho in equation 1 and throughout the paper put a bracket for its argument \\rho(x) not \\rho x Page 6 , Imagenet paragraph : W \u2014> We section 4.2 we define linear separability etc\u2026 a space is missing before Further section 4.3 we report our results .. (Imagenet) a space is missing after ImageNet) Overall Assessment: This is a good paper, making the code available and adding more ablation studies and explanations of width versus depth and the choice of pooling will make the contributions easier to understand. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your positive review . We agree that our work raises a lot of interesting questions , and we have added several appendices to address your concerns . We address the points sequentially below : < < I encourage the authors to make their code available during the review/discussion period . > > As requested we have released an anonymized code repository : https : //anonymous.4open.science/repository/75115ffe-d110-4814-ade0-060dd12b9f7e/ Please note this is a very preliminary release created as per request and we plan to have a more clean code at a later date . < < From reading the paper it is not clear what is the main ingredient that makes this layer wise training successful > > We believe the most critical , yet simple , engineering ingredient to the scalability is the spatial averaging operation applied as the first part of the fc in auxiliary classifier . For example even in k=1 without this operation the sizes of the auxillary layers would explode for imagenet-size images ( 224x224 ) . We have included some additional ablations in the appendix based on your questions . < < Does the layer wise training give worst results with average or max pooling ? > > Thanks for pointing this out , in the text we did not say the max or average pooling is worse , but can see how this might sound implied . The choice of invertible downsampling is driven by the idea of eliminating as many factors as possible that might conceivably be incompatible with greedy learning . One of them that we identified early on was potential loss of target information ( note t that this is an issue hypothesized as a problem with such an approach in ( Bengio et al 2006 ) ) . Indeed we never established a clear difference in preliminary testing , but still want the inherent lossiness of average pooling and the slightly more complex analytical structure of max pooling to not be a potential factor in our experiments . We have now completed a more extensive set of ablations included in the revision in Appendix C.1 which shows that , at least for the case of the CIFAR dataset , strided convolutions , max-pooling , and average pooling as downsampling operations yield similar results to invertible downsampling . Invertible downsampling performs marginally better . This shows that the method is rather generic and also contradicts some of the concerns in difficulty to maintain target information in ( Bengio et al 2006 ) . < < On the width of the networks , seems it is key for the success of the approach . What if you train wider networks with J that is small ? ( J=3 for instance but much wider networks , instead of J=8 now for imagenet . ) To answer the same question above one needs also to see what are the accuracies for J=8 with thiner networks ( smaller M ) ? > > As in typical CNNs the width increases can improve performance to some ( relatively small with diminishing returns ) degree , see e.g . ( Zagoruyko et al , 2016 ) where increases of 2 % top 5 on imagenet are observed for high end models when doubling width . However , we emphasize that the width alone is not sufficient to explain the observed results . Take the example of the extreme cases of the k=1 model where the 1st layer obtains only 23 % top 5 accuracy : it seems unlikely that simply increasing width for these shallow cases can achieve the desired accuracies . We point out several results from the existing appendix and a new ablation added in C.2 to address this point : - We have results that demonstrate the change in width for CIFAR in Figure 4 of the appendix showing that larger M can improve results , but not in an extreme fashion . - We also refer the reviewer to examine the k=1 accuracy progression in Figure 3 which shows how the accuracy increases nearly linearly at the early layers . - We include an ablation that shows the k=3 network described in Sec 4.3 with the widths halved : as predicted from ( Zagoruyko et al , 2016 ) the accuracy changes from 88.7 to 86.8 -- a rather modest change compared to the effect of the cascade we can derive from Figure 3 . This effect would be most pronounced if comparing the per-layer widths of k=1 , we will try to do this in a future revision but currently are limited in resources to run ablations on imagenet . - Finally we bring the reviewers \u2019 attention to the fact the receptive field for very low depths might be too small with respect to the full image . Typical cnn designs attempt to cover a substantial part or all of the input image in the receptive field . < < Would the accuracy with the layer wise training reach a plateau if one uses an architecture with J higher than 8 ? > > There are certainly diminishing returns . As shown in Figure 3 in Appendix the accuracies for k=2,3 plateau around J=8 . For completeness we have computed an additional layer for the k=2 imagenet model to further extend this chart ( J=9 ) . The result is 86.5 compared to the previous layers 86.3 . Note that end-to-end networks also certainly have significantly diminishing returns for more stacked layers ."}], "0": {"review_id": "r1Gsk3R9Fm-0", "review_text": " The authors propose to train deep convolutional neural networks in a layerwise fashion. This is contrary to the traditional joint end-to-end training of deep CNNs. As their motivation, the authors quote computational and memory benefits at the time of training in addition to being able to extend shallow-network analytical frameworks to the individual network layers thus allowing for a theoretical interpretation of their optima. Their method is simple and clearly explained. (Note: In the 10th line on Pg. 4, is there a 'j' missing in the subscript of x^n?) The experimental results are interesting. The authors are able to demonstrate 'some' architecture that, in solely a layerwise training, is able to show competitive results with respect to AlexNet when trained in an end-to-end manner on ImageNet. These results can seem questionable as both the architectures and training routines are being varied and hence the precise contribution of the layerwise training is unclear. However, as per my understanding, the aim of the paper wasn't to show that a layerwise training can work better that end-to-end training. The aim was, on the contrary to show, that 'even' a layerwise training can offer competitive performance for 'some' network and hence may come handy when memory is limited. Their underlying claim, which could be more clearly stated is that the memory benefits during training can be enjoyed when the individual layers of a network (net1) are smaller in parameter count as compared to another net (net2) although the net1 in totality maybe larger than net2. This is because net1 will be trained in a layerwise fashion, while net2 would be trained in an end-to-end manner. I would like to see the authors confirm or reject this understanding and rationalise their experimental regimen. Further, I would like to know how their work compares to the following: https://arxiv.org/abs/1703.07115 https://arxiv.org/abs/1611.02185 Finally, while the authors state that the layerwise training makes the individual layers amenable to theoretical analysis/interpretation, no such discussion is presented/initiated in the paper. The only analysis presented is on the ability of the individually trained layers to linear separate the data. To round the analysis, it should also be extended to the representations learned by end-to-end trained networks. All in all, while the paper raises some interesting ideas, its execution in terms of a method that learns a classifier on each individual layer is rather simplistic. Don't get me wrong, simple can indeed be elegant, but at the minute the comparisons are not very convincing. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank you for the review that will help us to improve the general quality of the paper . We answer below to each of your points . < < As their motivation , the authors quote computational and memory benefits at the time of training in addition to being able to extend shallow-network analytical frameworks to the individual network layers thus allowing for a theoretical interpretation of their optima. > > The computation and memory are indeed a potential benefit of greedy layerwise learning but it is not the only and not the primary motivation we have for greedy layerwise learning in the paper . Our paper emphasizes that it is easier to apply existing theoretical results to layerwise learning methods and that layerwise learning methods induce properties in the network that allow to more clearly study the behavior of each layer . The motivation of our work is to empirically determine if such greedy layerwise methods are viable ( something not demonstrated in any related literature ) . In terms of the potential advantages of greedy layerwise methods we discuss extensively the ramifications for theoretical study of deep networks by giving a concrete example for optimization in Prop 3.2 and an empirical study that can only be done in this context in Sec 4.2 . See also general comment and response to R1 . < < Their method is simple and clearly explained . ( Note : In the 10th line on Pg.4 , is there a ' j ' missing in the subscript of x^n ? ) > > Thanks , we have fixed the typo < < These results can seem questionable as both the architectures and training routines are being varied and hence the precise contribution of the layerwise training is unclear. > > We would like to emphasize that in Sec.4.3 , we obtain VGG-performance using an extension of the basic k=1 approach . We believe the reviewer is referring to the indirect comparison in Sec 4.1 to AlexNet without an e2e comparison . Please correct us if we have misinterpreted . We apologize for confusion for this and have addressed this point regarding the goal of 4.1 in the response we give to R1 and the general comment . To summarize briefly : -Sec 4.1 was aimed to show the most basic greedy layerwise method k=1 can already reach a performance barrier only obtained by deep learning methods on imagenet . For completeness we now report e2e comparison on CIFAR for same architecture . -Sec 4.3 already reports a very direct comparison , with the imagenet dataset , using end-to-end training and greedy layerwise training on the same architecture using the same number of gradient updates for each layer , and an architecture which performs as well as VGG in the e2e setting while being more compact . < < Their underlying claim , which could be more clearly stated is that the memory benefits during training can be enjoyed when the individual layers of a network ( net1 ) are smaller in parameter count as compared to another net ( net2 ) although the net1 in totality maybe larger than net2 .... > > This is indeed a stated potential practical advantage ( and one we have benefited from in our own experiments ) . However , as emphasized in the paper in the introduction and Sec 3.3 and 4.3 there are many other reasons greedy layerwise learning is relevant . Our work is the only one that has shown this class of methods is potentially viable for real problems . < < Further , I would like to know how their work compares to the following : https : //arxiv.org/abs/1703.07115 https : //arxiv.org/abs/1611.02185 > > Thanks for the references . We believe the contrast to these highlights well the contribution of our paper and have added them to the manuscript . The first one considers a layerwise supervised procedure and is indeed related in spirit . However , it shows numerical results only on the MNIST dataset of < 96 % and less than 50 % on CIFAR . These are trivial accuracies even for those simple datasets , achievable without need for deep learning methods and do not in any way demonstrate that greedy layerwise construction can be used to build deep networks that show the kind of results that have made deep learning successful . The second reference is a layerwise scheme , but not greedy : although one layer is updated at a time the problem is solved jointly . This method is however an alternative to standard end-to-end back-propagation but we wish to bring attention that it is only shown to work as a strategy applied after typical end-to-end back-propagation . Our work is the first that shows greedy layerwise methods can achieve accuracies only obtained by e2e deep learning , and to the best of our knowledge it is the first alternative to end-to-end back-propagation that scales to imagenet and does not fall back on end to end learning ( contrary to e.g.the 2nd reference which does ) ."}, "1": {"review_id": "r1Gsk3R9Fm-1", "review_text": "This paper is of reasonable quality and clarity, rather modest originality, perhaps considerable significance in some applications. Strengths: - I think this kind of method could be useful for data of very high dimensionality, when it is not possible to train everything end to end. - The experiments seem to be conducted correctly. - The paper is well written. Weaknesses: - (minor) Abstract: it's kind of funny to say that CIFAR-10 is a large scale image recognition problem. - What the authors are proposing is quite similar to Lee et al. [1], which was not mentioned in the paper as well as a wide range of papers, which were mentioned. I think it is kind interesting for people to revise these techniques from 10 years ago, but this method is just not that novel. - The authors highlight that their goal is not using this method as a pre-training strategy, but it would be interesting to see whether it would indeed work better if after the layer-wise training, the whole network would be trained end-to-end. - Maths in this paper is mostly decorative. - When comparing different models or training methods (e.g. layer-wise trained AlexNet and end-to-end trained AlexNet), it would make sense to do some hyperparameter search. It is very risky to conclude anything otherwise. - I would like to see a wall clock time comparison between this and end-to-end training. [1] Lee et al. Deeply-Supervised Nets. 2015.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer 1 , We would like to thank you very much for your review and time . We answer to your points below , < < ( minor ) Abstract : it 's kind of funny to say that CIFAR-10 is a large scale image recognition problem > > We agree with you . This was a typo aimed to simplify the sentence flow and we have corrected this statement in the uploaded revision . < < What the authors are proposing is quite similar to Lee et al . [ 1 ] , which was not mentioned in the paper as well as a wide range of papers , which were mentioned. > > Thank you for the reference - we added it in the revision . While there is some similarity in the final overall architecture using an auxiliary objective , Note that they use a completely different training technique ( end to end ) and their contribution bears little resemblance overall to our work . Indeed , the model in Lee et al.is trained end-to-end and the contribution is aimed at stabilizing/guiding end-to-end training . Our work focuses on the viability of a training method that utilizes individually solved auxiliary problems only . Please also see the extensive literature review which highlights major differences to our work from many other potentially related ones . < < I think it is kind interesting for people to revise these techniques from 10 years ago , but this method is just not that novel. > > We agree that the high-level technique has similarities to a few specific existing works , mainly ( Huang et al . ( 2017 ) , Bengio et al . ( 2006 ) ) .Because our goal here was not to propose an extremely specific technique but to test the assumption that end-to-end learning of layers is essential , we consider it useful and interesting that we were able to achieve our result with a relatively common approach . We believe that our - counterintuitive - numerical results alone represent a major contribution to the literature and are a significant novelty . As a more minor but still substantial contribution , there has not been any scalable ( even just computationally speaking ) greedy layerwise method in the literature for CNNs . Thus even the specific proposed method has some novel aspects . Please also see our general comment discussion on this point ( above ) . < < The authors highlight that their goal is not using this method as a pre-training strategy , but it would be interesting to see whether it would indeed work better if after the layer-wise training , the whole network would be trained end-to-end. > > We think this is an interesting idea and could potentially work with careful investigation , but it is rather outside of the scope of this paper . Many works considering layerwise methods ( e.g.Huang et al . ( 2017 ) ) resort to a final end to end learning phases to make the results more competitive , but this portrays the message that end-to-end learning is an absolutely necessary condition to get competitive performance . This is contrary to the message we are trying to convey : that greedy layerwise learning without end-to-end works much better than a large portion of the community would expect ."}, "2": {"review_id": "r1Gsk3R9Fm-2", "review_text": "Summary: This paper proposes layer wise training of neural networks using classification auxiliary tasks for training each layer. Experiments are presented on CIFAR10 and Imagenet. Accuracies close to end to end training are obtained. The layer wise training is repeated for J steps, the auxiliary tasks are trained on top of the shallow one layer (of width M ) with a network of depth k and width tilde{M}. Layerwise training is done using sgd with momentum, and the learning rate is decayed through epochs. Note that the layer wise training is done with large width M than typical end to end networks in use. The authors argue and test the hypothesis that auxiliary tasks encourage the linear separability of CNN features. To reduce the size of the learned network the author propose a layer wise compression using the filter removal technique of Molchanov et al . Reproducibility: This empirical work has been investigated for a while with mild success, the authors should make their code available to the community to confirm and reproduce their findings. I encourage the authors to make their code available during the review/discussion period. Significance of the work: From reading the paper it is not clear what is the main ingredient that makes this layer wise training successful, negative results would help in understanding what is important for the accuracy. Some more ablation studies and negative results will be insightful and here are few suggestions in that direction: - Authors claim that they used invertible downsampling as max or average pooling lead to information loss. Does the layer wise training give worst results with average or max pooling? If so please report those numbers to know what is the implications of this choice of pooling. - On the width of the networks, seems it is key for the success of the approach. What if you train wider networks with J that is small? ( J=3 for instance but much wider networks, instead of J=8 now for imagenet.) - To answer the same question above one needs also to see what are the accuracies for J=8 with thiner networks (smaller M )? - Would the accuracy with the layer wise training reach a plateau if one uses an architecture with J higher than 8? - Transferability of the learned features: end to end features are know to be transferable. It would be good to see if this still holds using the network layer wise trained on imagenet for CIFAR10 or other datasets. Other Questions: - Section 3.2 is vague. In Proposition 3.1 and Proposition 3.2 can you add some text to explain what are the implicitions of the claims? \u201cThus our training permits to extend some results on shallow CNN to deeper CNNs \u2026\u201d which shallow results ? - \u201cFor k>1 batch normalization was useful \u201c is this only on the auxiliary problems networks or you used also batch norm for the layer ? - The ensemble used is Z=\\sum_{j=1}^J 2^j z_j , this uses the network of J layers , also the O(J) auxiliary networks of depth k. Please report the number of parameters for all models (single and ensembles) in Table 1 and Table 2. - In the conclusion: \u201cThe framework can be extendable to the parallel training \u2026\u201d how would this possible since one needs the output of the first training to do the training of the next layer. can you elaborate on what is meant here? Minor: page 2 bottom have competitorsand -> have competitors and the non linearity rho in equation 1 and throughout the paper put a bracket for its argument \\rho(x) not \\rho x Page 6 , Imagenet paragraph : W \u2014> We section 4.2 we define linear separability etc\u2026 a space is missing before Further section 4.3 we report our results .. (Imagenet) a space is missing after ImageNet) Overall Assessment: This is a good paper, making the code available and adding more ablation studies and explanations of width versus depth and the choice of pooling will make the contributions easier to understand. ", "rating": "7: Good paper, accept", "reply_text": "Thank you for your positive review . We agree that our work raises a lot of interesting questions , and we have added several appendices to address your concerns . We address the points sequentially below : < < I encourage the authors to make their code available during the review/discussion period . > > As requested we have released an anonymized code repository : https : //anonymous.4open.science/repository/75115ffe-d110-4814-ade0-060dd12b9f7e/ Please note this is a very preliminary release created as per request and we plan to have a more clean code at a later date . < < From reading the paper it is not clear what is the main ingredient that makes this layer wise training successful > > We believe the most critical , yet simple , engineering ingredient to the scalability is the spatial averaging operation applied as the first part of the fc in auxiliary classifier . For example even in k=1 without this operation the sizes of the auxillary layers would explode for imagenet-size images ( 224x224 ) . We have included some additional ablations in the appendix based on your questions . < < Does the layer wise training give worst results with average or max pooling ? > > Thanks for pointing this out , in the text we did not say the max or average pooling is worse , but can see how this might sound implied . The choice of invertible downsampling is driven by the idea of eliminating as many factors as possible that might conceivably be incompatible with greedy learning . One of them that we identified early on was potential loss of target information ( note t that this is an issue hypothesized as a problem with such an approach in ( Bengio et al 2006 ) ) . Indeed we never established a clear difference in preliminary testing , but still want the inherent lossiness of average pooling and the slightly more complex analytical structure of max pooling to not be a potential factor in our experiments . We have now completed a more extensive set of ablations included in the revision in Appendix C.1 which shows that , at least for the case of the CIFAR dataset , strided convolutions , max-pooling , and average pooling as downsampling operations yield similar results to invertible downsampling . Invertible downsampling performs marginally better . This shows that the method is rather generic and also contradicts some of the concerns in difficulty to maintain target information in ( Bengio et al 2006 ) . < < On the width of the networks , seems it is key for the success of the approach . What if you train wider networks with J that is small ? ( J=3 for instance but much wider networks , instead of J=8 now for imagenet . ) To answer the same question above one needs also to see what are the accuracies for J=8 with thiner networks ( smaller M ) ? > > As in typical CNNs the width increases can improve performance to some ( relatively small with diminishing returns ) degree , see e.g . ( Zagoruyko et al , 2016 ) where increases of 2 % top 5 on imagenet are observed for high end models when doubling width . However , we emphasize that the width alone is not sufficient to explain the observed results . Take the example of the extreme cases of the k=1 model where the 1st layer obtains only 23 % top 5 accuracy : it seems unlikely that simply increasing width for these shallow cases can achieve the desired accuracies . We point out several results from the existing appendix and a new ablation added in C.2 to address this point : - We have results that demonstrate the change in width for CIFAR in Figure 4 of the appendix showing that larger M can improve results , but not in an extreme fashion . - We also refer the reviewer to examine the k=1 accuracy progression in Figure 3 which shows how the accuracy increases nearly linearly at the early layers . - We include an ablation that shows the k=3 network described in Sec 4.3 with the widths halved : as predicted from ( Zagoruyko et al , 2016 ) the accuracy changes from 88.7 to 86.8 -- a rather modest change compared to the effect of the cascade we can derive from Figure 3 . This effect would be most pronounced if comparing the per-layer widths of k=1 , we will try to do this in a future revision but currently are limited in resources to run ablations on imagenet . - Finally we bring the reviewers \u2019 attention to the fact the receptive field for very low depths might be too small with respect to the full image . Typical cnn designs attempt to cover a substantial part or all of the input image in the receptive field . < < Would the accuracy with the layer wise training reach a plateau if one uses an architecture with J higher than 8 ? > > There are certainly diminishing returns . As shown in Figure 3 in Appendix the accuracies for k=2,3 plateau around J=8 . For completeness we have computed an additional layer for the k=2 imagenet model to further extend this chart ( J=9 ) . The result is 86.5 compared to the previous layers 86.3 . Note that end-to-end networks also certainly have significantly diminishing returns for more stacked layers ."}}