{"year": "2019", "forum": "S1eYHoC5FX", "title": "DARTS: Differentiable Architecture Search", "decision": "Accept (Poster)", "meta_review": "This paper introduces a very simple but effective method for the neural architecture search problem. The key idea of the method is a particular continuous relaxation of the architecture representation to enable gradient descent-like differentiable optimization. Results are quite good. Source code is also available. A concern of the approach is the (possibly large) integrality gap between the continuous solution and the discretized architecture. The solution provided in the paper is a heuristic without guarantees.  Overall, this is a good paper. I recommend acceptance.", "reviews": [{"review_id": "S1eYHoC5FX-0", "review_text": "(Disclaimers: I am not not active in the sub-field, just generally interested in the topic, it is easy however to find this paper in the wild and references to it, so I accidentally found out the name of the authors, but had not heard about them before reviewing this, so I do not think this biased my review). DARTS, the algorithm described in this paper, is part of the one-shot family of architecture search algorithms. In practice this means training an over-parameterized architecture is, of which the architectures being searched for are sub-graphs. Once this bigger network is trained it is pruned into the desired sub-graph. DARTS has \"indicator\" weights that indicate how active components are during training, and then alternatively trains these weights (using the validation sets), and all other weights (using the training set). Those indicators are then chosen to select the final sub-graph. More detailed comments: It seems that the justification of equations (3) and (4) is not immediately obvious, in particular, from an abstract point of view, splitting the weights into w, and \\eta to perform the bi-level optimizations appears somewhat arbitrary. It almost looks like optimizing the second over the validation could be interpreted as some form of regularization. Is there a stronger motivation than that is similar to more classical model/architecture selection? There are some papers that seem to be pretty relevant and are worth looking at and that are not in the references: http://proceedings.mlr.press/v80/bender18a.html https://openreview.net/forum?id=HylVB3AqYm (under parallel review at ICLR, WARNIGN TO REVIEWERS: contains references to a non anonymized version of this paper ) I think architecture pruning literature is relevant too, it would be nice to discuss the connection between NAS and this sub-field, as I think there are very strong similarity between the two. Pros: * available source code * good experimental results * easy to read * interesting idea of encoding how active the various possible operations are with special weights Cons * tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture, in particular it was tested on two data set on which they train DARTS models, which they then show to transfer to two other data sets, respectively * shared with most NAS papers: does not really find novel architectures in a broad sense, instead only looks for variations of a fairly limited class of architectures * theoretically not very strong, the derivation of the bi-level optimization is interesting, but I believe it is not that clear why iterating between test and validation set is the right thing to do, although admittedly it leads to good results in the settings tested ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the feedback . > \u201c It seems that the justification of equations ( 3 ) and ( 4 ) is not immediately obvious \u201d In this work we treat \\alpha as a high-dimensional hyperparameter . The bilevel formulation offers a mathematical characterization of the standard hyperparameter tuning procedure , namely to find hyperparameter \\alpha that leads to the best validation performance ( eq . ( 4 ) ) after regular parameters w are trained until convergence on the training set ( eq . ( 3 ) ) given \\alpha . > `` it is not that clear why iterating between test and validation set is the right thing to do '' Using two separate data splits for \\alpha and w as in the bilevel formulation should effectively prevent hyperparameter/architecture from overfitting the training data . Advantage of doing so has also been empirically verified by our experiments . Please refer to \u201c Alternative Optimization Strategies \u201d in sect . 3.3 of the revised draft . From the algorithmic point of view , each architecture gradient step consists of two subroutines : ( i ) Obtaining w^ * ( \\alpha ) , namely weights trained until convergence for the given architecture , by solving the inner optimization eq ( 4 ) . This can normally be achieved by taking a large number of gradient descent steps of w wrt the training loss . ( ii ) Descending \\alpha wrt the validation loss defined based on w^ * ( \\alpha ) . Our iterative algorithm is a truncated version of the above by approximating the optimization procedure in ( i ) using only a single gradient step . > \u201c I think architecture pruning literature is relevant too \u201d Yes , network pruning and ( differentiable ) architecture search are related despite somewhat different goals . The former aims to learn fine-grained sparsity patterns ( e.g.which neurons or channels should be kept ) that best approximate a given unpruned network . The latter aims to learn macro-level sparsity patterns that represent an architecture ."}, {"review_id": "S1eYHoC5FX-1", "review_text": "This paper proposes a novel way to formulate neural architecture search as a differentiable problem. It uses the idea of weight sharing introduced in previous papers (convolutional neural fabrics, ENAS, and Bender et al's one shot model) and combines this with a relaxation of discrete choices between k operators into k continuous weights. Then, it uses methods based on hyperparameter gradient search methods to optimize in this space and in the end removes the relaxation by dropping weak connections and selecting the single choice of the k options with the highest weight. This leads to an efficient solution for architecture search. Overall, this is a very interesting paper that has already created quite a buzz due to the simplicity of the methods and the strong results. It is a huge plus that there is code with the paper! This will dramatically increase the paper's impact. In my first read through, I thought this might be a candidate for an award paper, but the more time I spent with it the more issues I found. I still think the paper should be accepted, but I do have several points of criticism / questions I detail below, and to which I would appreciate a response. Some criticisms / questions: 1. The last step of how to move from the one-shot model to a single model is in a sense the most interesting aspect of this work, but also the one that leaves the most questions open: Why does this work? Are there cases where we lose arbitrarily badly by rounding the solution to the closest discrete value or is the performance loss bounded? How would other ways of moving from the relaxation to a discrete choice work? I don't expect the paper to answer all of these questions, but it would be useful if the authors acknowledge that this is a critical part of the work that deserves further study. Any insights from other approaches the authors may have tried before the mechanism in Section 2.4 would also be useful. 2. The related work is missing several papers, namely the entire category of work on using network morphisms to speed up the optimization process, Bender et al's one shot model, and several early papers on neural architecture search (work on NAS did not only start in 2017 but goes back to work in the 1990s on neuroevolution that is very similar to the evolution approach by Real). This is a useful survey useful for further references: https://arxiv.org/abs/1808.05377 3. I find a few of the claims to be a bit too strong. In the introduction, the paper claims to outperform ENAS, but really the paper doesn't give a head-to-head comparison. In the experiments, ENAS is faster and gives slightly worse results. The authors state explicitly that their method is slower because they run it 4 times and pick the best result. One could obviously also do that with ENAS, and since ENAS is 8 times faster one could even run it 8 times! This is unfair and should be fixed. I don't really care even if it turns out that ENAS performs a bit better with the same budget, but comparisons should be fair and on even ground in order to help our science advance -- something that is far too often ignored in the ML literature in order to obtain a table with bold numbers in one's own row. Likewise, why is ENAS missing in the Figure 3 plots for CIFAR, and why is its performance not plotted over time like that of DARTS? 4. The paper is not really forthcoming about clearly stating the time required to obtain the results: - On CIFAR, there are 4 DARTS run of 1 day each - Then, the result of each of these is evaluated for 100 epochs (which is only stated in the caption of Figure 3) to pick the best. Each of these validation runs takes 4 hours (which, again, one has to be inferred from the fact that random search can do 24 such evaluations in 4 GPU days), so this step takes another 16 GPU hours. - Then, one needs to train the final network for 600 epochs; this is a larger network, so this should take another 2-3 GPU days. So, overall, to obtain the result on CIFAR-10 requires about one GPU week. That's still cheap, but it's a different story than 1 day. Likewise, DARTS is *not* able to obtain 55.7 perplexity on PTB in 6 hours with 4 GPUs; again, there is the selection step (probably another 4*6 hours?) and I think training the final model takes about 2 GPU days. These numbers should be stated prominently next to the stated \"search times\" to not mislead the reader. 5. One big question I have is where the hyperparameters come from, for both the training pipeline and the final evaluation pipeline (which actually differ a lot!). For example, here are the hyperparameters for CIFAR, in this format: training pipeline value -> final evaluation pipeline value: #cells: 8 -> 20 batch size: 64 -> 96 initial channels: 16 -> 36 #epochs: 50 -> 600 droppath: no -> yes (with probability 0.2) auxiliary head: no -> yes (with weight 0.4) BatchNorm: enabled (no learnable parameters) -> enabled The situation is similar for PTB: embedding size: 300 -> 850 hidden units per RNN layer: 300 -> 850 #epochs: 500 -> 8000 batch size: 256 (SGD) -> 64 (ASGD), sped up by starting with SGD weight decay: 5e-7 -> 8e-7 BatchNorm: enabled (no learnable parameters) -> disabled The fact that there are so many differences in the pipelines is disconcerting, since it looks like a lot of manual work is required to get these right. Now you need to tune hyperparameters for both the training and the final evaluation pipeline? If you have to tune them for the final evaluation pipeline, then you can't capitalize at all on the fact that DARTS is fast, since hyperparameter optimization on the full final evaluation pipeline will be order of magnitudes more expensive than running DARTS. 6. How was the final evaluation pipeline chosen? Before running DARTS the first time, or was it chosen to be tuned for architectures found by DARTS? 7. A question about how the best of 4 DARTS runs is selected, and how the best of the 24 random samples in random search is evaluated: is this based on 100 epochs using the *training* procedure or the *final evaluation* procedure? Seeing how different the hyperparameters are above, this should be stated. 8. A few questions to the authors related to the above: how did you choose the hyperparameters of DARTS? The DARTS learning rate for PTB is 10 times higher than for CIFAR-10, and the momentum also differs a lot (0.9 vs. 0.5). Did you ever consider different hyperparameters for DARTS? If so, how did you decide on the ones used? Is it sensitive to the choice of hyperparameters? In the author response period, could you please report the (1) result of running DARTS on PTB using the same DARTS hyperparameters as used for CIFAR-10 (learning rate 3*e-4 and momentum (0.5,0.999)) and (2) result of running DARTS on CIFAR-10 using the same DARTS hyperparameters as used for PTB (learning rate 3*e-3 and momentum (0.9,0.999))? 9. DARTS is being critizized in https://openreview.net/pdf?id=rylqooRqK7#page=10&zoom=180,-16,84 I am wondering whether the authors have a reply to this. The algorithm for solving the relaxed problem is also not mathematically derived from the optimization problem to be solved (equations 3,4), but it is more a heuristic. A derivation, or at least a clearer motivation for the algorithm would be useful. 10. Further comments: - Equation 1: This looks like a typo, shouldn't this be x(j) = \\sum_{i<j} o(i,j) x(i) ? Even if the authors wanted to use the non-intuitive way of edges going from j to i, then o(i,j) should still be o(j,i). - Just above Equation 5: \"the the\" - Equation 5: I would have found it more intuitive had \\alpha_{k-1} already just been a generic \\alpha here. - It would be nice if the authors gave the explicit equations for the extension with momentum in the appendix for completeness. - The authors should include citations for techniques such as batch normalization, Adam, and cosine annealing. Despite these issues (which I hope the authors will address in the author response and the final version), as stated above, I'm arguing for accepting the paper, due to the simplicity of the method combined with its very promising results and the direct availability of code.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the detailed comments and questions . We have fixed the missing references ( Q2 ) and presentational issues ( Q4 , Q10 ) in the revision . Below we focus on the major points : > Regarding discretization schemes ( Q1 ) The current discretization scheme can be viewed as a heuristic to minimize the per-node rounding error , as described in the revised sect . 2.4.While refining this part was not our primary focus , it indeed deserves further study . We have also added a remark in the draft to make readers aware of this potential limitation . To reduce the rounding error , in our preliminary experiments we tried annealing the softmax temperature to enforce one-hot selection , but did not observe clear differences in terms of the quality of the derived cells . Note that a large rounding error does not necessarily imply poor performance , since the current discretization mechanism only depends on the ranking among the strengths of the incoming edges . > \u201c since ENAS is 8 times faster one could even run it 8 times \u201d ( Q3 ) We agree it would be informative to compare DARTS and ENAS given the same search cost ( e.g. , 4 GPU days ) . Following your suggestion , we repeated the search process of ENAS for 8 times on CIFAR-10 using the authors ' implementation and their best setup . We then used the same selection protocol as for DARTS by training the candidate cells for 100 epochs using half of the CIFAR-10 training data to get the validation performance on the other half . The best ENAS cell out of 8 runs achieves 2.91 % test error using 4.2M params in the final evaluation , which is slightly worse than 4 runs of DARTS ( 2.76 % error using 3.3M params ) . These new results have been included in Table 1 of the revised draft . > \u201c One big question I have is where the hyperparameters come from \u201d ( Q5 , Q6 ) . Let us explain our reasoning for each of these hyperparameters in detail : For convolutional cells : Our setup of # cells ( 8- > 20 ) , # epochs ( 600 ) and weight for the auxiliary head ( 0.4 ) in the final evaluation exactly follows Zoph et al. , 2018 . The # init_channels is enlarged from 16 to 36 to ensure a comparable model size ( ~3M ) with other baselines . Given those settings , we then use the largest possible batch size ( 96 ) for a single GPU . The drop path probability was tuned wrt the validation set among the choices of ( 0.1 , 0.2 , 0.3 ) given the best cell learned by DARTS . We treat droppath , auxiliary towers and cutout as additional augmentations only for the final evaluation . Learnable affine parameters in the batch normalisation are disabled during the search phase to avoid arbitrary rescaling of the nodes , as explained in sect A.1.1 . They are enabled in the evaluation phase to ensure fair comparison with other baseline networks . For recurrent cells : We always use the same # units for both embedding and hidden layers , which is enlarged from 300 to 850 in the final evaluation to make our # params ( ~23M ) comparable with other models in the literature . We then use the largest possible batch size ( 64 ) to fit our model in a single GPU . The l2 weight decay was tuned on the validation set given the best recurrent cell . We do not trigger ASGD during the search phase for simplicity and also to accommodate our current approximation scheme which does not take into account model averaging ( though it can be modified to support it ) . Batch normalisation is useful during architecture search to prevent gradient explosion ( Sect 3.1.2 ) . Similar to the case of convnets , learnable affine params are disabled to avoid node rescaling , as explained in A.1.1 and A.1.2 . Once the cell is learned , batch normalisation layers are omitted in the final evaluation for fair comparison with existing language models which usually do not involve normalisation . Our usage of batch normalisation for RNN architecture search follows ENAS . > \u201c how the best of the 24 random samples in random search is evaluated \u201d ( Q7 ) : The same script is used for cell selection of DARTS and random search . All the hyperparameters , except # epochs , are identical to those in our final evaluation pipeline ."}, {"review_id": "S1eYHoC5FX-2", "review_text": "The authors introduce a continuous relaxation for categorical variables so as to utilize the gradient descent to optimize the connection weights and the network architecture. It is a cool idea and I enjoyed the paper. One question, which I think is relevant in practice, is the initialization of the architecture parameters. I might be just missing, but I couldn't find description of the initial parameter values. As it is gradient based, it might be sensitive to the initial value of alpha. In (5), the subscript for alpha should be removed as it defines a function of alpha. I think (5) is misleading as it is because of k-1. (and remove one \"the\" in \"minimize the the validation\" in the sentence above (5))", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for the feedback . > Regarding the initialization of \\alpha We use zero initialization which implies equal amount of attention ( after taking the softmax ) over all possible ops . At the early stage this ensures weights in every candidate op to receive sufficient learning signal ( more exploration ) . This detail has been added to the revised draft . > \u201c I think ( 5 ) is misleading as it is because of k-1. \u201d Thank you for the suggestion . This has been fixed in the revised sect . 2.3 ."}], "0": {"review_id": "S1eYHoC5FX-0", "review_text": "(Disclaimers: I am not not active in the sub-field, just generally interested in the topic, it is easy however to find this paper in the wild and references to it, so I accidentally found out the name of the authors, but had not heard about them before reviewing this, so I do not think this biased my review). DARTS, the algorithm described in this paper, is part of the one-shot family of architecture search algorithms. In practice this means training an over-parameterized architecture is, of which the architectures being searched for are sub-graphs. Once this bigger network is trained it is pruned into the desired sub-graph. DARTS has \"indicator\" weights that indicate how active components are during training, and then alternatively trains these weights (using the validation sets), and all other weights (using the training set). Those indicators are then chosen to select the final sub-graph. More detailed comments: It seems that the justification of equations (3) and (4) is not immediately obvious, in particular, from an abstract point of view, splitting the weights into w, and \\eta to perform the bi-level optimizations appears somewhat arbitrary. It almost looks like optimizing the second over the validation could be interpreted as some form of regularization. Is there a stronger motivation than that is similar to more classical model/architecture selection? There are some papers that seem to be pretty relevant and are worth looking at and that are not in the references: http://proceedings.mlr.press/v80/bender18a.html https://openreview.net/forum?id=HylVB3AqYm (under parallel review at ICLR, WARNIGN TO REVIEWERS: contains references to a non anonymized version of this paper ) I think architecture pruning literature is relevant too, it would be nice to discuss the connection between NAS and this sub-field, as I think there are very strong similarity between the two. Pros: * available source code * good experimental results * easy to read * interesting idea of encoding how active the various possible operations are with special weights Cons * tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture, in particular it was tested on two data set on which they train DARTS models, which they then show to transfer to two other data sets, respectively * shared with most NAS papers: does not really find novel architectures in a broad sense, instead only looks for variations of a fairly limited class of architectures * theoretically not very strong, the derivation of the bi-level optimization is interesting, but I believe it is not that clear why iterating between test and validation set is the right thing to do, although admittedly it leads to good results in the settings tested ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for the feedback . > \u201c It seems that the justification of equations ( 3 ) and ( 4 ) is not immediately obvious \u201d In this work we treat \\alpha as a high-dimensional hyperparameter . The bilevel formulation offers a mathematical characterization of the standard hyperparameter tuning procedure , namely to find hyperparameter \\alpha that leads to the best validation performance ( eq . ( 4 ) ) after regular parameters w are trained until convergence on the training set ( eq . ( 3 ) ) given \\alpha . > `` it is not that clear why iterating between test and validation set is the right thing to do '' Using two separate data splits for \\alpha and w as in the bilevel formulation should effectively prevent hyperparameter/architecture from overfitting the training data . Advantage of doing so has also been empirically verified by our experiments . Please refer to \u201c Alternative Optimization Strategies \u201d in sect . 3.3 of the revised draft . From the algorithmic point of view , each architecture gradient step consists of two subroutines : ( i ) Obtaining w^ * ( \\alpha ) , namely weights trained until convergence for the given architecture , by solving the inner optimization eq ( 4 ) . This can normally be achieved by taking a large number of gradient descent steps of w wrt the training loss . ( ii ) Descending \\alpha wrt the validation loss defined based on w^ * ( \\alpha ) . Our iterative algorithm is a truncated version of the above by approximating the optimization procedure in ( i ) using only a single gradient step . > \u201c I think architecture pruning literature is relevant too \u201d Yes , network pruning and ( differentiable ) architecture search are related despite somewhat different goals . The former aims to learn fine-grained sparsity patterns ( e.g.which neurons or channels should be kept ) that best approximate a given unpruned network . The latter aims to learn macro-level sparsity patterns that represent an architecture ."}, "1": {"review_id": "S1eYHoC5FX-1", "review_text": "This paper proposes a novel way to formulate neural architecture search as a differentiable problem. It uses the idea of weight sharing introduced in previous papers (convolutional neural fabrics, ENAS, and Bender et al's one shot model) and combines this with a relaxation of discrete choices between k operators into k continuous weights. Then, it uses methods based on hyperparameter gradient search methods to optimize in this space and in the end removes the relaxation by dropping weak connections and selecting the single choice of the k options with the highest weight. This leads to an efficient solution for architecture search. Overall, this is a very interesting paper that has already created quite a buzz due to the simplicity of the methods and the strong results. It is a huge plus that there is code with the paper! This will dramatically increase the paper's impact. In my first read through, I thought this might be a candidate for an award paper, but the more time I spent with it the more issues I found. I still think the paper should be accepted, but I do have several points of criticism / questions I detail below, and to which I would appreciate a response. Some criticisms / questions: 1. The last step of how to move from the one-shot model to a single model is in a sense the most interesting aspect of this work, but also the one that leaves the most questions open: Why does this work? Are there cases where we lose arbitrarily badly by rounding the solution to the closest discrete value or is the performance loss bounded? How would other ways of moving from the relaxation to a discrete choice work? I don't expect the paper to answer all of these questions, but it would be useful if the authors acknowledge that this is a critical part of the work that deserves further study. Any insights from other approaches the authors may have tried before the mechanism in Section 2.4 would also be useful. 2. The related work is missing several papers, namely the entire category of work on using network morphisms to speed up the optimization process, Bender et al's one shot model, and several early papers on neural architecture search (work on NAS did not only start in 2017 but goes back to work in the 1990s on neuroevolution that is very similar to the evolution approach by Real). This is a useful survey useful for further references: https://arxiv.org/abs/1808.05377 3. I find a few of the claims to be a bit too strong. In the introduction, the paper claims to outperform ENAS, but really the paper doesn't give a head-to-head comparison. In the experiments, ENAS is faster and gives slightly worse results. The authors state explicitly that their method is slower because they run it 4 times and pick the best result. One could obviously also do that with ENAS, and since ENAS is 8 times faster one could even run it 8 times! This is unfair and should be fixed. I don't really care even if it turns out that ENAS performs a bit better with the same budget, but comparisons should be fair and on even ground in order to help our science advance -- something that is far too often ignored in the ML literature in order to obtain a table with bold numbers in one's own row. Likewise, why is ENAS missing in the Figure 3 plots for CIFAR, and why is its performance not plotted over time like that of DARTS? 4. The paper is not really forthcoming about clearly stating the time required to obtain the results: - On CIFAR, there are 4 DARTS run of 1 day each - Then, the result of each of these is evaluated for 100 epochs (which is only stated in the caption of Figure 3) to pick the best. Each of these validation runs takes 4 hours (which, again, one has to be inferred from the fact that random search can do 24 such evaluations in 4 GPU days), so this step takes another 16 GPU hours. - Then, one needs to train the final network for 600 epochs; this is a larger network, so this should take another 2-3 GPU days. So, overall, to obtain the result on CIFAR-10 requires about one GPU week. That's still cheap, but it's a different story than 1 day. Likewise, DARTS is *not* able to obtain 55.7 perplexity on PTB in 6 hours with 4 GPUs; again, there is the selection step (probably another 4*6 hours?) and I think training the final model takes about 2 GPU days. These numbers should be stated prominently next to the stated \"search times\" to not mislead the reader. 5. One big question I have is where the hyperparameters come from, for both the training pipeline and the final evaluation pipeline (which actually differ a lot!). For example, here are the hyperparameters for CIFAR, in this format: training pipeline value -> final evaluation pipeline value: #cells: 8 -> 20 batch size: 64 -> 96 initial channels: 16 -> 36 #epochs: 50 -> 600 droppath: no -> yes (with probability 0.2) auxiliary head: no -> yes (with weight 0.4) BatchNorm: enabled (no learnable parameters) -> enabled The situation is similar for PTB: embedding size: 300 -> 850 hidden units per RNN layer: 300 -> 850 #epochs: 500 -> 8000 batch size: 256 (SGD) -> 64 (ASGD), sped up by starting with SGD weight decay: 5e-7 -> 8e-7 BatchNorm: enabled (no learnable parameters) -> disabled The fact that there are so many differences in the pipelines is disconcerting, since it looks like a lot of manual work is required to get these right. Now you need to tune hyperparameters for both the training and the final evaluation pipeline? If you have to tune them for the final evaluation pipeline, then you can't capitalize at all on the fact that DARTS is fast, since hyperparameter optimization on the full final evaluation pipeline will be order of magnitudes more expensive than running DARTS. 6. How was the final evaluation pipeline chosen? Before running DARTS the first time, or was it chosen to be tuned for architectures found by DARTS? 7. A question about how the best of 4 DARTS runs is selected, and how the best of the 24 random samples in random search is evaluated: is this based on 100 epochs using the *training* procedure or the *final evaluation* procedure? Seeing how different the hyperparameters are above, this should be stated. 8. A few questions to the authors related to the above: how did you choose the hyperparameters of DARTS? The DARTS learning rate for PTB is 10 times higher than for CIFAR-10, and the momentum also differs a lot (0.9 vs. 0.5). Did you ever consider different hyperparameters for DARTS? If so, how did you decide on the ones used? Is it sensitive to the choice of hyperparameters? In the author response period, could you please report the (1) result of running DARTS on PTB using the same DARTS hyperparameters as used for CIFAR-10 (learning rate 3*e-4 and momentum (0.5,0.999)) and (2) result of running DARTS on CIFAR-10 using the same DARTS hyperparameters as used for PTB (learning rate 3*e-3 and momentum (0.9,0.999))? 9. DARTS is being critizized in https://openreview.net/pdf?id=rylqooRqK7#page=10&zoom=180,-16,84 I am wondering whether the authors have a reply to this. The algorithm for solving the relaxed problem is also not mathematically derived from the optimization problem to be solved (equations 3,4), but it is more a heuristic. A derivation, or at least a clearer motivation for the algorithm would be useful. 10. Further comments: - Equation 1: This looks like a typo, shouldn't this be x(j) = \\sum_{i<j} o(i,j) x(i) ? Even if the authors wanted to use the non-intuitive way of edges going from j to i, then o(i,j) should still be o(j,i). - Just above Equation 5: \"the the\" - Equation 5: I would have found it more intuitive had \\alpha_{k-1} already just been a generic \\alpha here. - It would be nice if the authors gave the explicit equations for the extension with momentum in the appendix for completeness. - The authors should include citations for techniques such as batch normalization, Adam, and cosine annealing. Despite these issues (which I hope the authors will address in the author response and the final version), as stated above, I'm arguing for accepting the paper, due to the simplicity of the method combined with its very promising results and the direct availability of code.", "rating": "7: Good paper, accept", "reply_text": "Thank you for the detailed comments and questions . We have fixed the missing references ( Q2 ) and presentational issues ( Q4 , Q10 ) in the revision . Below we focus on the major points : > Regarding discretization schemes ( Q1 ) The current discretization scheme can be viewed as a heuristic to minimize the per-node rounding error , as described in the revised sect . 2.4.While refining this part was not our primary focus , it indeed deserves further study . We have also added a remark in the draft to make readers aware of this potential limitation . To reduce the rounding error , in our preliminary experiments we tried annealing the softmax temperature to enforce one-hot selection , but did not observe clear differences in terms of the quality of the derived cells . Note that a large rounding error does not necessarily imply poor performance , since the current discretization mechanism only depends on the ranking among the strengths of the incoming edges . > \u201c since ENAS is 8 times faster one could even run it 8 times \u201d ( Q3 ) We agree it would be informative to compare DARTS and ENAS given the same search cost ( e.g. , 4 GPU days ) . Following your suggestion , we repeated the search process of ENAS for 8 times on CIFAR-10 using the authors ' implementation and their best setup . We then used the same selection protocol as for DARTS by training the candidate cells for 100 epochs using half of the CIFAR-10 training data to get the validation performance on the other half . The best ENAS cell out of 8 runs achieves 2.91 % test error using 4.2M params in the final evaluation , which is slightly worse than 4 runs of DARTS ( 2.76 % error using 3.3M params ) . These new results have been included in Table 1 of the revised draft . > \u201c One big question I have is where the hyperparameters come from \u201d ( Q5 , Q6 ) . Let us explain our reasoning for each of these hyperparameters in detail : For convolutional cells : Our setup of # cells ( 8- > 20 ) , # epochs ( 600 ) and weight for the auxiliary head ( 0.4 ) in the final evaluation exactly follows Zoph et al. , 2018 . The # init_channels is enlarged from 16 to 36 to ensure a comparable model size ( ~3M ) with other baselines . Given those settings , we then use the largest possible batch size ( 96 ) for a single GPU . The drop path probability was tuned wrt the validation set among the choices of ( 0.1 , 0.2 , 0.3 ) given the best cell learned by DARTS . We treat droppath , auxiliary towers and cutout as additional augmentations only for the final evaluation . Learnable affine parameters in the batch normalisation are disabled during the search phase to avoid arbitrary rescaling of the nodes , as explained in sect A.1.1 . They are enabled in the evaluation phase to ensure fair comparison with other baseline networks . For recurrent cells : We always use the same # units for both embedding and hidden layers , which is enlarged from 300 to 850 in the final evaluation to make our # params ( ~23M ) comparable with other models in the literature . We then use the largest possible batch size ( 64 ) to fit our model in a single GPU . The l2 weight decay was tuned on the validation set given the best recurrent cell . We do not trigger ASGD during the search phase for simplicity and also to accommodate our current approximation scheme which does not take into account model averaging ( though it can be modified to support it ) . Batch normalisation is useful during architecture search to prevent gradient explosion ( Sect 3.1.2 ) . Similar to the case of convnets , learnable affine params are disabled to avoid node rescaling , as explained in A.1.1 and A.1.2 . Once the cell is learned , batch normalisation layers are omitted in the final evaluation for fair comparison with existing language models which usually do not involve normalisation . Our usage of batch normalisation for RNN architecture search follows ENAS . > \u201c how the best of the 24 random samples in random search is evaluated \u201d ( Q7 ) : The same script is used for cell selection of DARTS and random search . All the hyperparameters , except # epochs , are identical to those in our final evaluation pipeline ."}, "2": {"review_id": "S1eYHoC5FX-2", "review_text": "The authors introduce a continuous relaxation for categorical variables so as to utilize the gradient descent to optimize the connection weights and the network architecture. It is a cool idea and I enjoyed the paper. One question, which I think is relevant in practice, is the initialization of the architecture parameters. I might be just missing, but I couldn't find description of the initial parameter values. As it is gradient based, it might be sensitive to the initial value of alpha. In (5), the subscript for alpha should be removed as it defines a function of alpha. I think (5) is misleading as it is because of k-1. (and remove one \"the\" in \"minimize the the validation\" in the sentence above (5))", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you for the feedback . > Regarding the initialization of \\alpha We use zero initialization which implies equal amount of attention ( after taking the softmax ) over all possible ops . At the early stage this ensures weights in every candidate op to receive sufficient learning signal ( more exploration ) . This detail has been added to the revised draft . > \u201c I think ( 5 ) is misleading as it is because of k-1. \u201d Thank you for the suggestion . This has been fixed in the revised sect . 2.3 ."}}