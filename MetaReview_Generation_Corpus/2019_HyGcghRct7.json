{"year": "2019", "forum": "HyGcghRct7", "title": "Random mesh projectors for inverse problems", "decision": "Accept (Poster)", "meta_review": "This paper proposes a novel method of solving inverse problems that avoids direct inversion by  first reconstructing various piecewise-constant projections of the unknown image (using a different CNN to learn each)  and then combining them via optimization to solve the final inversion. \nTwo of the reviewers requested more intuitions into why this two stage process would  fight the inherent ambiguity. \nAt the end of the discussion, two of the three reviewers are convinced by the derivations and empirical justification of the paper.\nThe authors also have significantly improved the clarity of the manuscript throughout the discussion period.\nIt would be interesting to see if there are any connections between such inversion via optimization with deep component analysis methods, e.g. \u201cDeep Component Analysis via Alternating Direction Neural Networks\n\u201d of Murdock et al. , that train neural architectures to effectively carry out the second step of optimization, as opposed to learning  a feedforward mapping. \n", "reviews": [{"review_id": "HyGcghRct7-0", "review_text": "Summary: Given an inverse problem, we want to infer (x) s.t. Ax = y, but in situations where the number of observations are very sparse, and do not enable direct inversion. The paper tackles scenarios where 'x' is of the form of an image. The proposed approach is a learning based one which trains CNNs to infer x given y (actually an initial least square solution x_init is used instead of y). The key insight is that instead of training to directly predict x, the paper proposes to predict different piecewise constant projections of x from x_init , with one CNN trained for each projection, each projection space defined from a random delaunay triangulation, with the hope that learning prediction for each projection is more sample efficient. The desired x is then optimized for given the predicted predicted projections. Pros: - The proposed approach is interesting and novel - I've not previously seen the idea of predicting different picewise constant projections instead of directly predicting the desired output (although using random projections has been explored) - The presented results are quantitatively and qualitatively better compared to a direct prediction baseline - The paper is generally well written, and interesting to read Cons: While the method is interesting, it is apriori unclear why this works, and why this has been only explored in context of linear inverse problems if it really does work. - Regarding limited demonstration: The central idea presented here is is generally applicable to any per-pixel regression task. Given this, I am not sure why this paper only explores it in the particular case of linear inversion and not other general tasks (e.g. depth prediction from a single image). Is there some limitation which would prevent such applications? If yes, a discussion would help. If not, it would be convincing to see such applications. - Regarding why it works: While learning a single projection maybe more sample efficient, learning all of them s.t. the obtained x is accurate may not be. Given this, I'm not entirely sure why the proposed approach is supposed to work. One hypothesis is that the different learned CNNs that each predict a piecewise projection are implicitly yielding an ensembling effect, and therefore a more fair baseline to compare would be a 'direct-ensemble' where many different (number = number of projections) direct CNNs (with different seeds etc.) are trained, and their predictions ensembled. Overall, while the paper is interesting to read and shows some nice results in a particular domain, it is unclear why the proposed approach should work in general and whether it is simply implicitly similar to an ensemble of predictors.", "rating": "6: Marginally above acceptance threshold", "reply_text": "> > \u201c Pros : - The proposed approach is interesting and novel - I 've not previously seen the idea of predicting different picewise constant projections instead of directly predicting the desired output ( although using random projections has been explored ) - The presented results are quantitatively and qualitatively better compared to a direct prediction baseline - The paper is generally well written , and interesting to read \u201d Response : We are glad that the reviewer found the paper interesting . > > \u201c While the method is interesting , it is apriori unclear why this works , and why this has been only explored in context of linear inverse problems if it really does work . '' > > `` Regarding limited demonstration : The central idea presented here is is generally applicable to any per-pixel regression task . Given this , I am not sure why this paper only explores it in the particular case of linear inversion and not other general tasks ( e.g.depth prediction from a single image ) . Is there some limitation which would prevent such applications ? If yes , a discussion would help . If not , it would be convincing to see such applications. \u201d Response : While we agree with the reviewer that the central idea is more widely applicable , we wish to emphasize that what the reviewer calls a \u201c particular case of linear inversion \u201d covers a very large variety of practically relevant problems . The list includes super-resolution , deconvolution , computed tomography , inverse scattering , synthetic aperture radar , seismic tomography , radio-interferometric astronomy , and many other problems . Importantly , the fact that the forward problem is linear ( which is why the corresponding inverse problems are unfortunately called linear ) does not at all imply that the sought inverse map which we are trying to learn ( the solution operator ) is linear . The inverse map of interest will not be linear for anything but the simplest Tikhonov regularized solution ( and variations thereof ) . For instance , if x is modeled as sparse in a dictionary , the inverse map is nonlinear even though the vast majority of inverse problems regularized by sparsity are \u201c linear '' . The entire field of compressive sensing is concerned with linear inverse problems . With general manifold models for x , such as the one assumed in the paper , we depart further from linear inverse maps . We now state this more explicitly in Section 3.1 and a new Appendix A . The ability to adapt to such nonlinear prior models is part of the reason why CNNs perform well on related problems . Additionally , these nonlinear inverses may be arbitrarily ill-posed , which calls for ever more sophisticated regularizers . In this sense , we are looking at a very large class of hard , practically relevant problems , whose solution operators are nonlinear . While nothing prevents practical application of our proposed method to problems such as single-image depth estimation , one benefit of studying linear inverse problems is that as soon as we are in finite dimensions ( e.g. , a low-dimensional manifold in R^N and a finite number of measurements ) , and the forward operator is injective , Lipschitz stability is guaranteed ( refer added citation : [ 1 ] ) . Injectivity can be generically achieved with a sufficient number of measurements that depends only on the manifold dimension . In applications such as depth estimation from a single image it is less straightforward to obtain similar guarantees . Namely , injectivity fails as one can easily construct cases where the same 2D depth map corresponds to multiple 2D images . So , while in practice our method might give good results , the justification would require additional work ."}, {"review_id": "HyGcghRct7-1", "review_text": "This paper proposes a novel method of solving ill-posed inverse problems and specifically focuses on geophysical imaging and remote sensing where high-res samples are rare and expensive. The motivation is that previous inversion methods are often not stable since the problem is highly under-determined. To alleviate these problems, this paper proposes a novel idea: instead of fully reconstructing in the original space, the authors create reconstructions in projected spaces. The projected spaces they use have very low dimensions so the corresponding Lipschitz constant is small. The specific low-dimensional reconstructions they obtain are piecewise constant images on random Delaunay trinagulations. This is theoretically motivated by classical work (Omohundro'89) and has the further advantage that the low-res reconstructions are interpretable. One can visually see how closely they capture the large shapes of the unknown image. These low-dimensional reconstructions are subsequently combined in the second stage of the proposed algorithm, to get a high-resolution reconstruction. The important aspect is that the piecewise linear reconstructions are now treated as measurments which however are local in the pixel-space and hence lead to more stable reconstructions. The problem of reconstruction from these piecewise constant projections is of independent interest. Improving this second stage of their algorithm, the authors would get a better result overall. For example I would recommend using Deep Image prior as an alternative technique of reconstructing a high-res image from multiple piecewise constant images, but this can be future work. Overall I like this paper. It contains a truly novel idea for an architecture in solving inverse problems. The two steps can be individually improved but the idea of separation is quite interesting and novel. ", "rating": "7: Good paper, accept", "reply_text": "We are glad that the reviewer enjoyed the paper . Indeed one of the main ideas put forward is the separation into information that can be stably ( but nonlinearly ) extracted from the measurements in this very ill-posed , no ground truth regime , and information that requires a stronger regularizing idea which kicks in at stage 2 . We find it encouraging that the reviewer \u2019 s comments on improving stage 2 are quite similar to our ideas on extending this work ( we now mention this in the concluding remarks ) . Further , we now provide an additional discussion of why the method can work and why nonlinear regressors are necessary in Appendix A and an updated Section 3.1 , as an effort to address the comments of other reviewers ."}, {"review_id": "HyGcghRct7-2", "review_text": "This paper describes a novel method for solving inverse problems in imaging. The basic idea of this approach is use the following steps: 1. initialize with nonnegative least squares solution to inverse problem (x0) 2. compute m different projections of x0 3. estimate x from the m different projections by solving \"reformuated\" inverse problem using TV regularization. The learning part of this algorithm is in step 2, where m different convolutional neural networks are used to learn m good projections. The projections correspond to computing a random Delaunay triangulation over the image domain and then computing pixel averages within each triangle. It's not clear exactly what the learning part is doing, i.e. what makes a \"good\" triangulation, why a CNN might accurately represent one, and what the shortcomings of truly random triangulations might be. More specifically, for each projection the authors start with a random set of points in the image domain and compute a Delaunay triangulation. They average x0 in each of the Delaunay triangles. Then since the projection is constant on each triangle, the projection into the lower-dimensional space is given by the magnitude of the function over each of the triangular regions. Next they train a convolutional neural network to approximate the above projection. The do this m times. It's not clear why the neural network approximation is necessary or helpful. Empirically, this method outperforms a straightforward use of a convolutional U-Net to invert the problem. The core novelty of this paper is the portion that uses a neural network to calculate a projection onto a random Delaunay triangulation. The idea of reconstructing images using random projections is not especially new, and much of the \"inverse-ness\" of the problem here is removed by first taking the pseudoinverse of the forward operator and applying it to the observations. Then the core idea at the heart of the paper is to speed up this reconstruction using a neural network by viewing the projection onto the mesh space as a set of special filter banks which can be learned. At the heart of this paper is the idea that for an L-Lipschitz function f : R^k \u2192 R the sample complexity is O(L^k), so the authors want to use the random projections to essentially reduce L. However, the Cooper sample complexity bound scales with k like k^{1+k/2}, so the focus on the Lipschitz constant seems misguided. This isn't damning, but it seems like the piecewise-constant estimators are a sort of regularizer, and that's where we really get the benefits. The authors only compare to another U-Net, and it's not entirely clear how they even trained that U-Net. It'd be nice to see if you get any benefit here from their method relative to other approaches in the literature, or if this is just better than inversion using a U-Net. Even how well a pseudoinverse does would be nice to see or TV-regularized least squares. Practically I'm quite concerned about their method requiring training 130 separate convolutional neural nets. The fact that all the different datasets give equal quality triangulations seems a bit odd, too. Is it possible that any network at all would be okay? Can we just reconstruct the image from regression on 130 randomly-initialized convolutional networks? The proposed method isn't bad, and the idea is interesting. But I can't help but wonder whether it works just because what we're doing is denoising the least squares reconstruction, and regression on many random projections might be pretty good for that. Unfortunately, the experiments don't help with developing a deeper understanding. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "The reviewer summarizes our method as > > \u201c This paper describes a novel method for solving inverse problems in imaging . The basic idea of this approach is use the following steps : 1. initialize with nonnegative least squares solution to inverse problem ( x0 ) 2. compute m different projections of x0 3. estimate x from the m different projections by solving `` reformuated '' inverse problem using TV regularization. \u201d Response : We have to respectfully disagree with this summary , especially because it informs the remainder of the reviewer \u2019 s comments . There seems to be a misunderstanding about Step 2 and many later comments appear to stem from it . Since this step is the crux of our proposed method , we begin by summarizing it here , with references to the relevant parts of the manuscript . Instead of computing m different projections of x0 as the reviewer suggests , we regress subspace projections of x , the true image ( see Section 3.1.1 , Paragraphs 3 and 4 ) . To do so , we must train a nonlinear regressor , in our case a convolutional neural network . ( The need for nonlinearity is explained below . ) To make this point clearer in the manuscript , we updated Figure 2 to explicitly show that x0 is not fed into linear subspace projectors of itself , but rather used as data from which we estimate projections of x . Indeed , projecting x0 would not be very interesting since it would simply imply various linear ways of looking at x0 and the networks would not be doing any actual inversion or data modeling . Again , what we actually do is that we compute * orthogonal * projections P_S x from y = Ax ( or x0 = pinv ( A ) y or something similar ) into a collection of subspaces { S_\\lambda } _ { \\lambda=1 } ^ { \\Lambda } ( see Section 3.1.1 , Paragraph 3 ) . While projecting x0 is a simple linear operation , regressing projections of an unknown x from the measurement data y is not . To explain why we need nonlinear regressors , we added a new figure and a short discussion to the manuscript ( please see the new Appendix A ) . For the reviewer \u2019 s convenience , we summarize the discussion here ( although it might be easier to read in the typeset pdf version ) : Suppose that there exists a linear operator F \\in R^ { N \\times M } which maps y ( or pinv ( A ) y ) to P_S x . The simplest requirement on such an F is consistency : if x already lives in the subspace S , then we would like to have F A x = x . Another way to write this is that for any x , not necessarily in S , we require FA FA x = FA x , which implies that FA = ( FA ) ^2 is an idempotent operator . However , because range ( F ) = S \\neq range ( A^ * ) , it will in general not hold that ( FA ) ^ * = FA . This implies that FA is not an orthogonal projection , but rather an oblique one . As we show in the new Figure 8 ( Appendix A ) , this oblique projection can be an arbitrarily poor approximation of the actual orthogonal projection that we seek . The nullspace of this projection is precisely N ( A ) = range^\\perp ( A^ * ) . Similar conclusions can be drawn for any other ( ad hoc ) linear operator , which would not even be a projection . There are various assumptions one can make to guarantee that the map from Ax to P_S x exists . We assume that the models live on a low-dimensional manifold ( please see updated Section 3.1 ; this low-dimensional structure assumption has previously been a footnote ) , and that the measurements are in general position with respect to this manifold . Our future work involves making quantitative statements about this aspect of the method ."}], "0": {"review_id": "HyGcghRct7-0", "review_text": "Summary: Given an inverse problem, we want to infer (x) s.t. Ax = y, but in situations where the number of observations are very sparse, and do not enable direct inversion. The paper tackles scenarios where 'x' is of the form of an image. The proposed approach is a learning based one which trains CNNs to infer x given y (actually an initial least square solution x_init is used instead of y). The key insight is that instead of training to directly predict x, the paper proposes to predict different piecewise constant projections of x from x_init , with one CNN trained for each projection, each projection space defined from a random delaunay triangulation, with the hope that learning prediction for each projection is more sample efficient. The desired x is then optimized for given the predicted predicted projections. Pros: - The proposed approach is interesting and novel - I've not previously seen the idea of predicting different picewise constant projections instead of directly predicting the desired output (although using random projections has been explored) - The presented results are quantitatively and qualitatively better compared to a direct prediction baseline - The paper is generally well written, and interesting to read Cons: While the method is interesting, it is apriori unclear why this works, and why this has been only explored in context of linear inverse problems if it really does work. - Regarding limited demonstration: The central idea presented here is is generally applicable to any per-pixel regression task. Given this, I am not sure why this paper only explores it in the particular case of linear inversion and not other general tasks (e.g. depth prediction from a single image). Is there some limitation which would prevent such applications? If yes, a discussion would help. If not, it would be convincing to see such applications. - Regarding why it works: While learning a single projection maybe more sample efficient, learning all of them s.t. the obtained x is accurate may not be. Given this, I'm not entirely sure why the proposed approach is supposed to work. One hypothesis is that the different learned CNNs that each predict a piecewise projection are implicitly yielding an ensembling effect, and therefore a more fair baseline to compare would be a 'direct-ensemble' where many different (number = number of projections) direct CNNs (with different seeds etc.) are trained, and their predictions ensembled. Overall, while the paper is interesting to read and shows some nice results in a particular domain, it is unclear why the proposed approach should work in general and whether it is simply implicitly similar to an ensemble of predictors.", "rating": "6: Marginally above acceptance threshold", "reply_text": "> > \u201c Pros : - The proposed approach is interesting and novel - I 've not previously seen the idea of predicting different picewise constant projections instead of directly predicting the desired output ( although using random projections has been explored ) - The presented results are quantitatively and qualitatively better compared to a direct prediction baseline - The paper is generally well written , and interesting to read \u201d Response : We are glad that the reviewer found the paper interesting . > > \u201c While the method is interesting , it is apriori unclear why this works , and why this has been only explored in context of linear inverse problems if it really does work . '' > > `` Regarding limited demonstration : The central idea presented here is is generally applicable to any per-pixel regression task . Given this , I am not sure why this paper only explores it in the particular case of linear inversion and not other general tasks ( e.g.depth prediction from a single image ) . Is there some limitation which would prevent such applications ? If yes , a discussion would help . If not , it would be convincing to see such applications. \u201d Response : While we agree with the reviewer that the central idea is more widely applicable , we wish to emphasize that what the reviewer calls a \u201c particular case of linear inversion \u201d covers a very large variety of practically relevant problems . The list includes super-resolution , deconvolution , computed tomography , inverse scattering , synthetic aperture radar , seismic tomography , radio-interferometric astronomy , and many other problems . Importantly , the fact that the forward problem is linear ( which is why the corresponding inverse problems are unfortunately called linear ) does not at all imply that the sought inverse map which we are trying to learn ( the solution operator ) is linear . The inverse map of interest will not be linear for anything but the simplest Tikhonov regularized solution ( and variations thereof ) . For instance , if x is modeled as sparse in a dictionary , the inverse map is nonlinear even though the vast majority of inverse problems regularized by sparsity are \u201c linear '' . The entire field of compressive sensing is concerned with linear inverse problems . With general manifold models for x , such as the one assumed in the paper , we depart further from linear inverse maps . We now state this more explicitly in Section 3.1 and a new Appendix A . The ability to adapt to such nonlinear prior models is part of the reason why CNNs perform well on related problems . Additionally , these nonlinear inverses may be arbitrarily ill-posed , which calls for ever more sophisticated regularizers . In this sense , we are looking at a very large class of hard , practically relevant problems , whose solution operators are nonlinear . While nothing prevents practical application of our proposed method to problems such as single-image depth estimation , one benefit of studying linear inverse problems is that as soon as we are in finite dimensions ( e.g. , a low-dimensional manifold in R^N and a finite number of measurements ) , and the forward operator is injective , Lipschitz stability is guaranteed ( refer added citation : [ 1 ] ) . Injectivity can be generically achieved with a sufficient number of measurements that depends only on the manifold dimension . In applications such as depth estimation from a single image it is less straightforward to obtain similar guarantees . Namely , injectivity fails as one can easily construct cases where the same 2D depth map corresponds to multiple 2D images . So , while in practice our method might give good results , the justification would require additional work ."}, "1": {"review_id": "HyGcghRct7-1", "review_text": "This paper proposes a novel method of solving ill-posed inverse problems and specifically focuses on geophysical imaging and remote sensing where high-res samples are rare and expensive. The motivation is that previous inversion methods are often not stable since the problem is highly under-determined. To alleviate these problems, this paper proposes a novel idea: instead of fully reconstructing in the original space, the authors create reconstructions in projected spaces. The projected spaces they use have very low dimensions so the corresponding Lipschitz constant is small. The specific low-dimensional reconstructions they obtain are piecewise constant images on random Delaunay trinagulations. This is theoretically motivated by classical work (Omohundro'89) and has the further advantage that the low-res reconstructions are interpretable. One can visually see how closely they capture the large shapes of the unknown image. These low-dimensional reconstructions are subsequently combined in the second stage of the proposed algorithm, to get a high-resolution reconstruction. The important aspect is that the piecewise linear reconstructions are now treated as measurments which however are local in the pixel-space and hence lead to more stable reconstructions. The problem of reconstruction from these piecewise constant projections is of independent interest. Improving this second stage of their algorithm, the authors would get a better result overall. For example I would recommend using Deep Image prior as an alternative technique of reconstructing a high-res image from multiple piecewise constant images, but this can be future work. Overall I like this paper. It contains a truly novel idea for an architecture in solving inverse problems. The two steps can be individually improved but the idea of separation is quite interesting and novel. ", "rating": "7: Good paper, accept", "reply_text": "We are glad that the reviewer enjoyed the paper . Indeed one of the main ideas put forward is the separation into information that can be stably ( but nonlinearly ) extracted from the measurements in this very ill-posed , no ground truth regime , and information that requires a stronger regularizing idea which kicks in at stage 2 . We find it encouraging that the reviewer \u2019 s comments on improving stage 2 are quite similar to our ideas on extending this work ( we now mention this in the concluding remarks ) . Further , we now provide an additional discussion of why the method can work and why nonlinear regressors are necessary in Appendix A and an updated Section 3.1 , as an effort to address the comments of other reviewers ."}, "2": {"review_id": "HyGcghRct7-2", "review_text": "This paper describes a novel method for solving inverse problems in imaging. The basic idea of this approach is use the following steps: 1. initialize with nonnegative least squares solution to inverse problem (x0) 2. compute m different projections of x0 3. estimate x from the m different projections by solving \"reformuated\" inverse problem using TV regularization. The learning part of this algorithm is in step 2, where m different convolutional neural networks are used to learn m good projections. The projections correspond to computing a random Delaunay triangulation over the image domain and then computing pixel averages within each triangle. It's not clear exactly what the learning part is doing, i.e. what makes a \"good\" triangulation, why a CNN might accurately represent one, and what the shortcomings of truly random triangulations might be. More specifically, for each projection the authors start with a random set of points in the image domain and compute a Delaunay triangulation. They average x0 in each of the Delaunay triangles. Then since the projection is constant on each triangle, the projection into the lower-dimensional space is given by the magnitude of the function over each of the triangular regions. Next they train a convolutional neural network to approximate the above projection. The do this m times. It's not clear why the neural network approximation is necessary or helpful. Empirically, this method outperforms a straightforward use of a convolutional U-Net to invert the problem. The core novelty of this paper is the portion that uses a neural network to calculate a projection onto a random Delaunay triangulation. The idea of reconstructing images using random projections is not especially new, and much of the \"inverse-ness\" of the problem here is removed by first taking the pseudoinverse of the forward operator and applying it to the observations. Then the core idea at the heart of the paper is to speed up this reconstruction using a neural network by viewing the projection onto the mesh space as a set of special filter banks which can be learned. At the heart of this paper is the idea that for an L-Lipschitz function f : R^k \u2192 R the sample complexity is O(L^k), so the authors want to use the random projections to essentially reduce L. However, the Cooper sample complexity bound scales with k like k^{1+k/2}, so the focus on the Lipschitz constant seems misguided. This isn't damning, but it seems like the piecewise-constant estimators are a sort of regularizer, and that's where we really get the benefits. The authors only compare to another U-Net, and it's not entirely clear how they even trained that U-Net. It'd be nice to see if you get any benefit here from their method relative to other approaches in the literature, or if this is just better than inversion using a U-Net. Even how well a pseudoinverse does would be nice to see or TV-regularized least squares. Practically I'm quite concerned about their method requiring training 130 separate convolutional neural nets. The fact that all the different datasets give equal quality triangulations seems a bit odd, too. Is it possible that any network at all would be okay? Can we just reconstruct the image from regression on 130 randomly-initialized convolutional networks? The proposed method isn't bad, and the idea is interesting. But I can't help but wonder whether it works just because what we're doing is denoising the least squares reconstruction, and regression on many random projections might be pretty good for that. Unfortunately, the experiments don't help with developing a deeper understanding. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "The reviewer summarizes our method as > > \u201c This paper describes a novel method for solving inverse problems in imaging . The basic idea of this approach is use the following steps : 1. initialize with nonnegative least squares solution to inverse problem ( x0 ) 2. compute m different projections of x0 3. estimate x from the m different projections by solving `` reformuated '' inverse problem using TV regularization. \u201d Response : We have to respectfully disagree with this summary , especially because it informs the remainder of the reviewer \u2019 s comments . There seems to be a misunderstanding about Step 2 and many later comments appear to stem from it . Since this step is the crux of our proposed method , we begin by summarizing it here , with references to the relevant parts of the manuscript . Instead of computing m different projections of x0 as the reviewer suggests , we regress subspace projections of x , the true image ( see Section 3.1.1 , Paragraphs 3 and 4 ) . To do so , we must train a nonlinear regressor , in our case a convolutional neural network . ( The need for nonlinearity is explained below . ) To make this point clearer in the manuscript , we updated Figure 2 to explicitly show that x0 is not fed into linear subspace projectors of itself , but rather used as data from which we estimate projections of x . Indeed , projecting x0 would not be very interesting since it would simply imply various linear ways of looking at x0 and the networks would not be doing any actual inversion or data modeling . Again , what we actually do is that we compute * orthogonal * projections P_S x from y = Ax ( or x0 = pinv ( A ) y or something similar ) into a collection of subspaces { S_\\lambda } _ { \\lambda=1 } ^ { \\Lambda } ( see Section 3.1.1 , Paragraph 3 ) . While projecting x0 is a simple linear operation , regressing projections of an unknown x from the measurement data y is not . To explain why we need nonlinear regressors , we added a new figure and a short discussion to the manuscript ( please see the new Appendix A ) . For the reviewer \u2019 s convenience , we summarize the discussion here ( although it might be easier to read in the typeset pdf version ) : Suppose that there exists a linear operator F \\in R^ { N \\times M } which maps y ( or pinv ( A ) y ) to P_S x . The simplest requirement on such an F is consistency : if x already lives in the subspace S , then we would like to have F A x = x . Another way to write this is that for any x , not necessarily in S , we require FA FA x = FA x , which implies that FA = ( FA ) ^2 is an idempotent operator . However , because range ( F ) = S \\neq range ( A^ * ) , it will in general not hold that ( FA ) ^ * = FA . This implies that FA is not an orthogonal projection , but rather an oblique one . As we show in the new Figure 8 ( Appendix A ) , this oblique projection can be an arbitrarily poor approximation of the actual orthogonal projection that we seek . The nullspace of this projection is precisely N ( A ) = range^\\perp ( A^ * ) . Similar conclusions can be drawn for any other ( ad hoc ) linear operator , which would not even be a projection . There are various assumptions one can make to guarantee that the map from Ax to P_S x exists . We assume that the models live on a low-dimensional manifold ( please see updated Section 3.1 ; this low-dimensional structure assumption has previously been a footnote ) , and that the measurements are in general position with respect to this manifold . Our future work involves making quantitative statements about this aspect of the method ."}}