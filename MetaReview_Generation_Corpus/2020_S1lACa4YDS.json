{"year": "2020", "forum": "S1lACa4YDS", "title": "Meta-Learning for Variational Inference", "decision": "Reject", "meta_review": "The paper proposes a meta-learning algorithm to learn the divergence measure of variational inference as well as the initialization of the variational parameters (which reduces optimization steps of VI). Improved performance by the learned divergence against hand-designed ones are empirically shown on: Gaussian mixture approximation, Bayesian neural regression, and p-VAE based recommender.\nReviewers initally raised some concerns on hyperparameters selection, weakness of experiments, and motivation for the proposed scheme. The authors responded by adding additional experiments (MNIST) as well as some new sections in their appendix about details of their method or the baselines.\nThe reviewers greatly appreciated the response and commonly believed that the revised version is significantly improved over the initial draft  and the improvements of the draft. As a result of that, some reviewers increased their scores. However, some of their concerns did not resolve. In particular, R1 questions the impact of the work and importance of learning divergence measure (referring to GAN or VQ-VAE for obtaining realistic samples). Also R1 finds evaluation based on MNIST unsatisfactory, as it is commonly considered as a toy dataset. To motivate the method, it is suggested that the authors think about real applications which can highlight the benefits of their method in practice. Similar concerns are shared by R2 after authors' response. In particular, R2 is not convinced about motivation and the necessity of using meta-learning for learning the divergence. I suggest authors improve on issues around motivation and support the impact of their scheme in a more practical setting.", "reviews": [{"review_id": "S1lACa4YDS-0", "review_text": "Summary of the paper: This paper proposes to meta-learn a parametric divergence measure for variational inference (VI). Specifically, the paper focuses on alpha-divergence and f-divergence, aims at choosing a good alpha or f for a particular task leveraging the experience learned from previous tasks. The meta-learned divergence is applied to approximate a mixture of two Gaussians, regress sinusoid function with Bayesian neural networks, and learn recommender systems with partial VAE. Summary of my opinion: I am leaning towards reject this paper because 1) I'm not convinced by their motivation -- \"this line of work has also shown that the optimal divergence can vary depending on tasks\". There are many key factors to achieve good performance in VI, such as a good likelihood model, a good variational posterior, a good optimizer and so on. The divergence itself, in my opinion, is less significant. If you really care about the flexibility, the Wasserstein distance may be a better choice. 2) Choosing the alpha or f is a hyper-parameter search problem to me. What you need to do is to prepare a validation set and select the best hyper-parameters based on the performance there. It is impractical to collect M tasks and search for hyper-parameters according to some meta-losses. 3) The experimental part is weak -- two toy problems plus an unusual recommendation system problem is not the typical way that people choose to evaluate VI methods. I suggest to evaluate on standard VAE/BNN benchmarks. Major comments: 1. From eq.(4) and eq.(8), it seems one has to compute the value of the density function p(theta, D) in order to compute the gradients, which is obviously not possible for BNNs and VAEs. More details on how to perform VI with alpha-divergence and f-divergence should be covered. 2. Can you let the meta-loss to be equal to D_eta? Meta-learning algorithms often optimize the same loss function in the inner and outer level. I couldn't see the point why you want to learn a f-divergence while setting the meta-loss to be D_0.5. 3. Do you have a validation set for the Bayesian optimization (BO) baseline or do you use cross-validation? The details of this baseline should be elaborated. In fact, it is possible to choose the best alpha for each task using BO. Should this be compared? 4. What does VB mean in Section 4.2? If VB uses KL rather than alpha-divergence or f-divergence, the outperformance may only suggest that KL is insufficient there. 5. Have you considered input convex neural networks (Amos et al. 2016) for implementing f? Minor comments: 1. The proofs of Prop 1 & 2 are missing. 2. \"However, using this KL divergence for VI has been criticized for under-estimating the uncertainty...\" Any reference to this? 3. Figure 1: alpha is used before it is defined. 4. \"Other existing definitions of alpha-divergences have dis-continuous gradients at the alpha values corresponding to KL divergences\" -- missing reference. 5. \"When D_0.5 is in use as the meta-loss, the corresponding log f*\" is analytical\" -- could you elaborate on this? ------------------------------ After rebuttal: I would like to thank the authors for their extremely detailed responses! The paper is now greatly improved. I would like to increase my score from 3 to 5, however, there is no such option this year. The reason why I am still hesitated to accept this paper is mainly because I am not sure how useful learning the divergence to VI is. For unsupervised learning, people use GAN or VQ-VAE to obtain realistic samples, but which is not attributed to better divergence measures. Plus, what we refer to \"better\" depends on the supplied meta-loss, which may not be better at all. Another unsatisfactory point of this paper in my opinion is that it only demonstrated results on toy datasets and MNIST. Although the proposed method is valid for alpha-divergence and f-divergence, I suggest the authors to find a real application which demonstrates the merit of the idea in practice. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your thoughtful and constructive review . We answer your questions below . Q1 : I 'm not convinced by their motivation -- the divergence itself is less significant . If you really care about the flexibility , the Wasserstein distance may be a better choice . A1 : We want to emphasize that the divergence metric essentially defines a variational inference ( VI ) algorithm , therefore divergence metric is a crucial factor of making VI successful . Studying the divergence in VI is an important problem in the literature such as [ 1,2,3,4,5 ] , and they all have shown that the divergence plays an important role in improving VI \u2019 s performance . Our work is situated with this line of work that improves the variational inference by improving the divergence measure . This paper addresses divergence learning via meta-learning . Our divergence measure is learnable and can be selected in an automatic fashion for a certain type of tasks , which enables the advances of using a flexible family . For the choice of divergence family , we agree that the Wasserstein distance with learnable transportation cost is also very flexible . However , parameterizing transportation cost in $ \\theta $ space is much more involved ( e.g.in [ 6 ] the cost function is parameterized by an auto-encoder . ) , especially when $ \\theta $ is of high dimensions ( e.g.in BNNs ) .Instead for $ f $ -divergences $ f ( t ) $ is an $ \\mathbb { R } \\rightarrow \\mathbb { R } $ function regardless of $ \\theta $ dimension , therefore it is much easier to learn with a small neural network . As shown in the paper , we are able to efficiently learn the convex $ f $ function in $ f $ -divergence . We agree with the reviewer that the model and variational distribution choices are also important . However , we argue that different likelihood design is a modeling choice , not an inference choice . Also , the selection of variational distribution is orthogonal to the choice of the inference algorithm , and for VI the algorithm is determined by the divergence design . Our work focuses on learning the inference algorithm , and the proposed method applies to any model design and variational distribution choice . Q2 : Choosing the alpha or f is a hyper-parameter search problem to me . It is impractical to collect M tasks and search for hyper-parameters according to some meta-losses . A2 : The setting in the paper follows a typical meta-learning setting [ 7 , 8 ] : we have several training tasks and we train a learner on them to gain common knowledge which will generalize well on the future tasks . The learner in our method is either the divergence ( meta-D ) or the divergence and the model initialization ( meta-D & phi ) . This meta-learning setting is practical as demonstrated in many previous work [ 7 , 15 , 16 ] , including meta-learning for Bayesian inference [ 8 ] . Also , this is needed in many real-world applications such as a recommender platform that needs to provide recommender service to many different companies . Besides , learning f-divergence can not be solved by hyperparameter search because it is parameterized by a deep neural network . It will be very inefficient to optimize this neural network by hyperparameter search . Our methods instead optimize it using an optimizer such as SGD or ADAM which is clearly more efficient . Q3 : The experimental part is weak . It is not the typical way that people choose to evaluate VI methods . I suggest to evaluate on standard VAE/BNN benchmarks . A3 : The experiments in this paper follow the meta-learning setup as stated in Section 3 , which is consistent with the experimental setup in meta-learning research [ 7,8,15,16 ] with the aim to reflect the real-world application needs . With such standard meta-learning tasks construction such as the BNN one , we can compare our results with related work where only meta-learning of the model parameters are used ( e.g.MAML [ 7 ] ) . Apart from the recommender system with partial VAE in the submitted version of the paper , we additionally provide the experimental results on another real-world dataset , MNIST . Please see the response \u201c Changes in the Revised Paper \u201d for details . These experimental settings highly align with real-world applications where many datasets/tasks from different companies are available for a shared machine learning platform ."}, {"review_id": "S1lACa4YDS-1", "review_text": "This paper proposes to use a meta-learning approach to learn the divergence used in variational inference and initial variational parameters. It considers two families of learnable divergences, including the alpha-divergence and f-divergence, and proposes two double loop algorithms. In both algorithms, the inner loop adjusts the variational parameters for a specific variational inference task. The outer loop adjusts the parameters in the divergence (and perhaps an initial variational parameter for all tasks) in terms of a human-designed meta-loss. Proposition 1 (following Wang et al. 2018a) shows that the gradients with respect to the meta-parameters in the f-divergence can be obtained using f\". This makes it convenient to parameterize f\" by neural networks to implicitly model f. The experiments show that the proposed method can outperform a Bayesian optimization (BO) baseline if the meta-loss is the metric of interest in the MoG example. For the regression and recommendation system examples, it compares with a VB baseline and a p-VAE baseline and shows better results. As for the significance of the paper, I have the following issues: 1. To apply the proposed method, we need to have a family of tasks that shares similarity at hand, which seems to be restricted. For instance, the first two examples are synthesized by hand. The recommendation system example is from real life but also crafted by splitting groups by age. If we just use all of the data by a p-VAE and fine-tune the hyperparameters a little bit (meta-learning needs more time than a p-VAE in a single configuration), can we obtain better results? If so, the last example is not appealing. 2. To apply the proposed method, we need to have some knowledge about our preference in the evaluation and encode that knowledge into the meta-loss. In the first example, the comparison with BO in the alpha=.5 case is unfair because Meta-D uses this knowledge as the meta-loss while BO does not leverage such information. Besides, if we already know the preferred divergence, it is not necessary to learn that divergence. For the other settings where Meta-D does not leverage any knowledge about the evaluation preference, it lacks stronger baselines such as BO. 3. The idea is straightforward and the most challenging part of how to model a convex function by a neural network is solved by existing work. Based on these issues, I think the contribution of the paper is not sufficient and I tend to reject the paper. Also, note that the paper length extends 8 pages. The motivation should be strengthened and the experiments should be more precise to improve the paper. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your thoughtful review . Our responses are below . Q1 : To apply the proposed method , we need to have a family of tasks that shares similarity at hand , which seems to be restricted . A1 : The setting in the paper follows a typical meta-learning setting [ 1,5 ] : we have several training tasks and we train a learner on them to gain common knowledge which will generalize well on the future tasks . The meta-learning setting is in need in many real-world applications where one type of machine learning solution is provided for different tasks , for example , one recommender solution provider may have many customers from different companies or different departments . In the same setting as a generic meta-learning task , we propose to meta-learn the inference algorithm . The learner in our method is either the divergence ( meta-D ) or the divergence and the model initialization ( meta-D & phi ) . This meta-learning setting is practical as demonstrated in many previous work [ 5 , 6 , 7 ] , including meta-learning for Bayesian inference [ 1 ] . Therefore the situation of applying our methods is essentially common in practice . Q2 : For instance , the first two examples are synthesized by hand . The recommendation system example is from real life but also crafted by splitting groups by age . If we just use all of the data by a p-VAE and fine-tune the hyperparameters a little bit ( meta-learning needs more time than a p-VAE in a single configuration ) , can we obtain better results ? If so , the last example is not appealing . A2 : The experiments in this paper follow the meta-learning setup as stated in Section 3 , which is consistent with the experimental setup in meta-learning research [ 1,5,6,7 ] with the aim to reflect the real-world application needs . The mixture of Gaussian experiment is to illustrate the basic principles of our method and demonstrate that our methods are able to learn a good divergence . The sinusoid regression is a benchmark task in meta-learning [ 5,6,7 ] and is an application of Bayesian neural networks . The recommender system task aligns with real-world applications where many datasets/tasks from different companies are available for a shared machine learning platform . Training p-VAE on all of the data is not practical . It means that whenever we get a new task , we have to train the model on all the data including the previous tasks and the new task . What we do in the paper is to follow the meta-learning setting , i.e.we learn the common knowledge based on the tasks we already have . When a new task comes , we can use this knowledge to better deal with this task without training on old tasks . This is clearly more efficient and practical than the former way . Splitting users by age is one set up to simulate this realistic situation . What we care about is to acquire common knowledge of this type of tasks that can generalize well for future tasks , rather than getting good results on the tasks we have . We additionally provide the experimental results on another real-world dataset , MNIST . Please see the response \u201c Changes in the Revised Paper \u201d for details . Q3 : In the first example , the comparison with BO in the alpha=.5 case is unfair because Meta-D uses this knowledge as the meta-loss while BO does not leverage such information . A3 : BO uses the same information as meta-VI and the comparisons in the paper are fair . The meta-loss is the objective function that BO aims to minimize thus BO also utilizes the knowledge about our preference . Every time BO selects an $ \\alpha $ , we train 10 models with that $ alpha $ -divergence on the support sets of 10 training tasks respectively and get the mean of $ D_ { 0.5 } $ ( the meta-loss ) on the query sets of the 10 training tasks . Based on the mean of the meta-loss , BO updates the Gaussian process and selects the next $ \\alpha $ . Q4 : If we already know the preferred divergence , it is not necessary to learn that divergence . A4 : We do not know the preferred divergence in practice . Setting the meta-loss to be $ D_ { 0.5 } $ is a synthetic setting serving for evaluation . In this setting it allows us to directly evaluate the learned divergence by comparing it with the known preferred divergence and thus can directly verify whether our method is able to learn a good divergence or not . We have shown in the paper that the learned divergence from our methods with $ \\alpha $ -divergence or f-divergence , is indeed very close to the known preferred divergence which demonstrates the effectiveness of our methods . We have also shown the results for other meta-losses ( e.g.TV and test log-likelihood ) in the paper . In real-life applications , the choice of the meta loss is determined by the application needs ."}, {"review_id": "S1lACa4YDS-2", "review_text": "SUMMARY OF THE PAPER: This paper proposes a way to automatically select a divergence to use in variational inference (VI) given a set of datasets (tasks). They consider searching within the alpha- divergence and f-divergence families. The proposed algorithm works as follows: do a few gradient descent steps on the variational parameters given a fixed divergence parameter, then update the divergence parameter by taking the gradient with respect to a meta loss which is a task-specific measure of goodness of the variational distribution (like test log marginal likelihood). The latter gradient is computed through the gradient descent computation in the inner loop. The second proposed algorithm does the same, but also learns a good initialization for the variational parameters. This initialization is good in the sense that taking one (or a few) gradient descent step on the variational parameters should give us good variational parameters (MAML style). This is done by taking a gradient with respect to this good initialization parameter through the inner loop gradient descent. STRUCTURE: The paper is well-written and easy to understand. NOVELTY: As far as I know, learning a divergence for VI using meta-learning is new. The related work is discussed well in section 5. EXPERIMENTS: There are experiments on three tasks of increasing complexity. In the simpler experiments, careful ablation studies are done. The results generally show that the proposed method is preferable to alternatives. CONCLUSION: I recommend acceptance.", "rating": "8: Accept", "reply_text": "Thank you for your supportive and valuable review . Your understanding of the paper is correct and we appreciate your recognition of the novelty of our work ."}], "0": {"review_id": "S1lACa4YDS-0", "review_text": "Summary of the paper: This paper proposes to meta-learn a parametric divergence measure for variational inference (VI). Specifically, the paper focuses on alpha-divergence and f-divergence, aims at choosing a good alpha or f for a particular task leveraging the experience learned from previous tasks. The meta-learned divergence is applied to approximate a mixture of two Gaussians, regress sinusoid function with Bayesian neural networks, and learn recommender systems with partial VAE. Summary of my opinion: I am leaning towards reject this paper because 1) I'm not convinced by their motivation -- \"this line of work has also shown that the optimal divergence can vary depending on tasks\". There are many key factors to achieve good performance in VI, such as a good likelihood model, a good variational posterior, a good optimizer and so on. The divergence itself, in my opinion, is less significant. If you really care about the flexibility, the Wasserstein distance may be a better choice. 2) Choosing the alpha or f is a hyper-parameter search problem to me. What you need to do is to prepare a validation set and select the best hyper-parameters based on the performance there. It is impractical to collect M tasks and search for hyper-parameters according to some meta-losses. 3) The experimental part is weak -- two toy problems plus an unusual recommendation system problem is not the typical way that people choose to evaluate VI methods. I suggest to evaluate on standard VAE/BNN benchmarks. Major comments: 1. From eq.(4) and eq.(8), it seems one has to compute the value of the density function p(theta, D) in order to compute the gradients, which is obviously not possible for BNNs and VAEs. More details on how to perform VI with alpha-divergence and f-divergence should be covered. 2. Can you let the meta-loss to be equal to D_eta? Meta-learning algorithms often optimize the same loss function in the inner and outer level. I couldn't see the point why you want to learn a f-divergence while setting the meta-loss to be D_0.5. 3. Do you have a validation set for the Bayesian optimization (BO) baseline or do you use cross-validation? The details of this baseline should be elaborated. In fact, it is possible to choose the best alpha for each task using BO. Should this be compared? 4. What does VB mean in Section 4.2? If VB uses KL rather than alpha-divergence or f-divergence, the outperformance may only suggest that KL is insufficient there. 5. Have you considered input convex neural networks (Amos et al. 2016) for implementing f? Minor comments: 1. The proofs of Prop 1 & 2 are missing. 2. \"However, using this KL divergence for VI has been criticized for under-estimating the uncertainty...\" Any reference to this? 3. Figure 1: alpha is used before it is defined. 4. \"Other existing definitions of alpha-divergences have dis-continuous gradients at the alpha values corresponding to KL divergences\" -- missing reference. 5. \"When D_0.5 is in use as the meta-loss, the corresponding log f*\" is analytical\" -- could you elaborate on this? ------------------------------ After rebuttal: I would like to thank the authors for their extremely detailed responses! The paper is now greatly improved. I would like to increase my score from 3 to 5, however, there is no such option this year. The reason why I am still hesitated to accept this paper is mainly because I am not sure how useful learning the divergence to VI is. For unsupervised learning, people use GAN or VQ-VAE to obtain realistic samples, but which is not attributed to better divergence measures. Plus, what we refer to \"better\" depends on the supplied meta-loss, which may not be better at all. Another unsatisfactory point of this paper in my opinion is that it only demonstrated results on toy datasets and MNIST. Although the proposed method is valid for alpha-divergence and f-divergence, I suggest the authors to find a real application which demonstrates the merit of the idea in practice. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your thoughtful and constructive review . We answer your questions below . Q1 : I 'm not convinced by their motivation -- the divergence itself is less significant . If you really care about the flexibility , the Wasserstein distance may be a better choice . A1 : We want to emphasize that the divergence metric essentially defines a variational inference ( VI ) algorithm , therefore divergence metric is a crucial factor of making VI successful . Studying the divergence in VI is an important problem in the literature such as [ 1,2,3,4,5 ] , and they all have shown that the divergence plays an important role in improving VI \u2019 s performance . Our work is situated with this line of work that improves the variational inference by improving the divergence measure . This paper addresses divergence learning via meta-learning . Our divergence measure is learnable and can be selected in an automatic fashion for a certain type of tasks , which enables the advances of using a flexible family . For the choice of divergence family , we agree that the Wasserstein distance with learnable transportation cost is also very flexible . However , parameterizing transportation cost in $ \\theta $ space is much more involved ( e.g.in [ 6 ] the cost function is parameterized by an auto-encoder . ) , especially when $ \\theta $ is of high dimensions ( e.g.in BNNs ) .Instead for $ f $ -divergences $ f ( t ) $ is an $ \\mathbb { R } \\rightarrow \\mathbb { R } $ function regardless of $ \\theta $ dimension , therefore it is much easier to learn with a small neural network . As shown in the paper , we are able to efficiently learn the convex $ f $ function in $ f $ -divergence . We agree with the reviewer that the model and variational distribution choices are also important . However , we argue that different likelihood design is a modeling choice , not an inference choice . Also , the selection of variational distribution is orthogonal to the choice of the inference algorithm , and for VI the algorithm is determined by the divergence design . Our work focuses on learning the inference algorithm , and the proposed method applies to any model design and variational distribution choice . Q2 : Choosing the alpha or f is a hyper-parameter search problem to me . It is impractical to collect M tasks and search for hyper-parameters according to some meta-losses . A2 : The setting in the paper follows a typical meta-learning setting [ 7 , 8 ] : we have several training tasks and we train a learner on them to gain common knowledge which will generalize well on the future tasks . The learner in our method is either the divergence ( meta-D ) or the divergence and the model initialization ( meta-D & phi ) . This meta-learning setting is practical as demonstrated in many previous work [ 7 , 15 , 16 ] , including meta-learning for Bayesian inference [ 8 ] . Also , this is needed in many real-world applications such as a recommender platform that needs to provide recommender service to many different companies . Besides , learning f-divergence can not be solved by hyperparameter search because it is parameterized by a deep neural network . It will be very inefficient to optimize this neural network by hyperparameter search . Our methods instead optimize it using an optimizer such as SGD or ADAM which is clearly more efficient . Q3 : The experimental part is weak . It is not the typical way that people choose to evaluate VI methods . I suggest to evaluate on standard VAE/BNN benchmarks . A3 : The experiments in this paper follow the meta-learning setup as stated in Section 3 , which is consistent with the experimental setup in meta-learning research [ 7,8,15,16 ] with the aim to reflect the real-world application needs . With such standard meta-learning tasks construction such as the BNN one , we can compare our results with related work where only meta-learning of the model parameters are used ( e.g.MAML [ 7 ] ) . Apart from the recommender system with partial VAE in the submitted version of the paper , we additionally provide the experimental results on another real-world dataset , MNIST . Please see the response \u201c Changes in the Revised Paper \u201d for details . These experimental settings highly align with real-world applications where many datasets/tasks from different companies are available for a shared machine learning platform ."}, "1": {"review_id": "S1lACa4YDS-1", "review_text": "This paper proposes to use a meta-learning approach to learn the divergence used in variational inference and initial variational parameters. It considers two families of learnable divergences, including the alpha-divergence and f-divergence, and proposes two double loop algorithms. In both algorithms, the inner loop adjusts the variational parameters for a specific variational inference task. The outer loop adjusts the parameters in the divergence (and perhaps an initial variational parameter for all tasks) in terms of a human-designed meta-loss. Proposition 1 (following Wang et al. 2018a) shows that the gradients with respect to the meta-parameters in the f-divergence can be obtained using f\". This makes it convenient to parameterize f\" by neural networks to implicitly model f. The experiments show that the proposed method can outperform a Bayesian optimization (BO) baseline if the meta-loss is the metric of interest in the MoG example. For the regression and recommendation system examples, it compares with a VB baseline and a p-VAE baseline and shows better results. As for the significance of the paper, I have the following issues: 1. To apply the proposed method, we need to have a family of tasks that shares similarity at hand, which seems to be restricted. For instance, the first two examples are synthesized by hand. The recommendation system example is from real life but also crafted by splitting groups by age. If we just use all of the data by a p-VAE and fine-tune the hyperparameters a little bit (meta-learning needs more time than a p-VAE in a single configuration), can we obtain better results? If so, the last example is not appealing. 2. To apply the proposed method, we need to have some knowledge about our preference in the evaluation and encode that knowledge into the meta-loss. In the first example, the comparison with BO in the alpha=.5 case is unfair because Meta-D uses this knowledge as the meta-loss while BO does not leverage such information. Besides, if we already know the preferred divergence, it is not necessary to learn that divergence. For the other settings where Meta-D does not leverage any knowledge about the evaluation preference, it lacks stronger baselines such as BO. 3. The idea is straightforward and the most challenging part of how to model a convex function by a neural network is solved by existing work. Based on these issues, I think the contribution of the paper is not sufficient and I tend to reject the paper. Also, note that the paper length extends 8 pages. The motivation should be strengthened and the experiments should be more precise to improve the paper. ", "rating": "3: Weak Reject", "reply_text": "Thank you for your thoughtful review . Our responses are below . Q1 : To apply the proposed method , we need to have a family of tasks that shares similarity at hand , which seems to be restricted . A1 : The setting in the paper follows a typical meta-learning setting [ 1,5 ] : we have several training tasks and we train a learner on them to gain common knowledge which will generalize well on the future tasks . The meta-learning setting is in need in many real-world applications where one type of machine learning solution is provided for different tasks , for example , one recommender solution provider may have many customers from different companies or different departments . In the same setting as a generic meta-learning task , we propose to meta-learn the inference algorithm . The learner in our method is either the divergence ( meta-D ) or the divergence and the model initialization ( meta-D & phi ) . This meta-learning setting is practical as demonstrated in many previous work [ 5 , 6 , 7 ] , including meta-learning for Bayesian inference [ 1 ] . Therefore the situation of applying our methods is essentially common in practice . Q2 : For instance , the first two examples are synthesized by hand . The recommendation system example is from real life but also crafted by splitting groups by age . If we just use all of the data by a p-VAE and fine-tune the hyperparameters a little bit ( meta-learning needs more time than a p-VAE in a single configuration ) , can we obtain better results ? If so , the last example is not appealing . A2 : The experiments in this paper follow the meta-learning setup as stated in Section 3 , which is consistent with the experimental setup in meta-learning research [ 1,5,6,7 ] with the aim to reflect the real-world application needs . The mixture of Gaussian experiment is to illustrate the basic principles of our method and demonstrate that our methods are able to learn a good divergence . The sinusoid regression is a benchmark task in meta-learning [ 5,6,7 ] and is an application of Bayesian neural networks . The recommender system task aligns with real-world applications where many datasets/tasks from different companies are available for a shared machine learning platform . Training p-VAE on all of the data is not practical . It means that whenever we get a new task , we have to train the model on all the data including the previous tasks and the new task . What we do in the paper is to follow the meta-learning setting , i.e.we learn the common knowledge based on the tasks we already have . When a new task comes , we can use this knowledge to better deal with this task without training on old tasks . This is clearly more efficient and practical than the former way . Splitting users by age is one set up to simulate this realistic situation . What we care about is to acquire common knowledge of this type of tasks that can generalize well for future tasks , rather than getting good results on the tasks we have . We additionally provide the experimental results on another real-world dataset , MNIST . Please see the response \u201c Changes in the Revised Paper \u201d for details . Q3 : In the first example , the comparison with BO in the alpha=.5 case is unfair because Meta-D uses this knowledge as the meta-loss while BO does not leverage such information . A3 : BO uses the same information as meta-VI and the comparisons in the paper are fair . The meta-loss is the objective function that BO aims to minimize thus BO also utilizes the knowledge about our preference . Every time BO selects an $ \\alpha $ , we train 10 models with that $ alpha $ -divergence on the support sets of 10 training tasks respectively and get the mean of $ D_ { 0.5 } $ ( the meta-loss ) on the query sets of the 10 training tasks . Based on the mean of the meta-loss , BO updates the Gaussian process and selects the next $ \\alpha $ . Q4 : If we already know the preferred divergence , it is not necessary to learn that divergence . A4 : We do not know the preferred divergence in practice . Setting the meta-loss to be $ D_ { 0.5 } $ is a synthetic setting serving for evaluation . In this setting it allows us to directly evaluate the learned divergence by comparing it with the known preferred divergence and thus can directly verify whether our method is able to learn a good divergence or not . We have shown in the paper that the learned divergence from our methods with $ \\alpha $ -divergence or f-divergence , is indeed very close to the known preferred divergence which demonstrates the effectiveness of our methods . We have also shown the results for other meta-losses ( e.g.TV and test log-likelihood ) in the paper . In real-life applications , the choice of the meta loss is determined by the application needs ."}, "2": {"review_id": "S1lACa4YDS-2", "review_text": "SUMMARY OF THE PAPER: This paper proposes a way to automatically select a divergence to use in variational inference (VI) given a set of datasets (tasks). They consider searching within the alpha- divergence and f-divergence families. The proposed algorithm works as follows: do a few gradient descent steps on the variational parameters given a fixed divergence parameter, then update the divergence parameter by taking the gradient with respect to a meta loss which is a task-specific measure of goodness of the variational distribution (like test log marginal likelihood). The latter gradient is computed through the gradient descent computation in the inner loop. The second proposed algorithm does the same, but also learns a good initialization for the variational parameters. This initialization is good in the sense that taking one (or a few) gradient descent step on the variational parameters should give us good variational parameters (MAML style). This is done by taking a gradient with respect to this good initialization parameter through the inner loop gradient descent. STRUCTURE: The paper is well-written and easy to understand. NOVELTY: As far as I know, learning a divergence for VI using meta-learning is new. The related work is discussed well in section 5. EXPERIMENTS: There are experiments on three tasks of increasing complexity. In the simpler experiments, careful ablation studies are done. The results generally show that the proposed method is preferable to alternatives. CONCLUSION: I recommend acceptance.", "rating": "8: Accept", "reply_text": "Thank you for your supportive and valuable review . Your understanding of the paper is correct and we appreciate your recognition of the novelty of our work ."}}