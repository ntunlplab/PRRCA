{"year": "2021", "forum": "O9bnihsFfXU", "title": "Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning", "decision": "Accept (Poster)", "meta_review": "Knowledgeable R3 found the paper very good (8). He/she found the authors' responses very informative and that edits made the paper much stronger. R2 expressed reservations about rank collapse being the cause of degradation of performance, but also indicated his/her willingness to increase the score if the authors can convincingly respond to his/her concerns. This concerned was shared by other reviewers, and there was an extensive discussion during the discussion period. R3 and R1 found the authors' responses very convincing. Fairly confident R1 found the paper good, appreciated the discussion, and recommends the paper to be accepted. R4 found the paper marginally above the acceptance threshold, however expressing a lower confidence in his/her assessment. In summary, the article contains extensive experiments, theory, and a well motivated idea, elucidating an intriguing phenomenon and useful for designing better bootstrapping-based deep RL methods. Although the reviewers expressed some reservations in their initial reviews, there was a lively discussion with quite positive final feedback. Weighing the ratings by confidence and participation in the discussion, I am recommending the paper for acceptance. I would like to encourage the authors to make efforts in making the presentation as clear as possible, having in mind the discussion and comments from the reviewers. ", "reviews": [{"review_id": "O9bnihsFfXU-0", "review_text": "# # # Summary : This paper identifies a type of implicit under-parameterization phenomenon in deep RL methods that use bootstrapping . It is found that after an initial learning period , the effective rank of the feature matrix keeps decreasing . This implies that the representational power of the network is not fully utilized . The authors call it a type of implicit under-parameterization . Moreover , the emergence of this under-parameterization strongly correlates with the poor performance . Some preliminary theoretical analyses are provided to explain this phenomenon . # # # Pros : The paper is well written , and I can follow the idea very smoothly . The implicit under-parameterization phenomenon seems very intriguing and useful for designing better bootstrapping-based deep RL methods . The theoretical analyses are very illustrative though still preliminary . # # # Cons : 1 . The analysis in Section 4.1 seems not correct to me . For kernel regression , the implicit bias of gradient descent with an infinite number of iterations is to pick up the solutions with the smallest RKHS norm . This implies $ c=0 $ in Eq . ( 1 ) , which would make the subsequent analysis problematic . However , if early stopping is applied , the GD solutions will be equal to the one given in Eq . ( 1 ) with $ c > 0 $ . The value of $ c $ depends on how the early stopping is applied . Please refer to [ 1 ] for more details . # # # Other comments : 1 . The analysis in Section 4.1 is very illustrative due to the use of the kernel model . But all the experiments are done for neural networks . It would be much helpful if some extra experiments with kernel models can be added , for which we can directly compare the experimental results and the theoretical analysis . [ 1 ] Suggala , Arun , Adarsh Prasad , and Pradeep K. Ravikumar . `` Connecting optimization and regularization paths . '' Advances in Neural Information Processing Systems . 2018. # # # Post-rebuttal Comments I thank the authors for the response and the efforts to update the drafts . I think this submission made some original contributions .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Summary * * : We thank the reviewer for constructive comments on the paper . We have added a discussion of the reference pointed out by the reviewer in Section 4.1 and clarified the concern about the correctness of the theorem . * * \u201c The analysis in Section 4.1 seems not correct to me ... The value of $ c $ depends on how the early stopping is applied . Please refer to [ 1 ] for more details. \u201d * * Thank you for suggesting the reference . The choice of $ c $ , which controls the strength of the regularizer , is directly based on the work from Mobahi et al.2020 , independent of RL and bootstrapping . As the reviewer pointed out the connection between optimization and regularization paths , the theory of self-distillation in Mobahi et al.2020 or kernel regression in Section 4.1 with $ c > 0 $ explains the learning dynamics of gradient descent with early stopping in the presence of strongly convex or separable losses . With this choice of $ c > 0 $ , our theorem in Section 4.1basically explains the presence of the rank collapse phenomenon when gradient descent with early stopping is used to optimize the TD error in Fitted Q-iteration . We have added this connection to Section 4.1 , and made it clear that the choice of $ c $ is directly taken from self-distillation and a value of $ c > 0 $ is optimal which matches the practical setting where early stopping or only a few gradient steps are used for training ."}, {"review_id": "O9bnihsFfXU-1", "review_text": "Post discussion review # Summary The authors present evidence that the approximate rank of the features is correlated with the learned policy 's performance and that this rank shrinks when using bootstrapping . They provide empirical evidence in several RL settings and domains and present some theoretical arguments which explain this behavior in the context of kernel regression and deep linear networks . Finally , they propose a simple approach for mitigating the rank collapse and show that this improves the performance of the learned policy in some cases . # Reason for score The authors isolate an interesting phenomenon and present some compelling empirical evidence . This is interesting work and I have have no doubt that it is of sufficient quality for publications . # Pros * The main contributions of this work might help us better understand the effects of using bootstrapping with function approximation and gradient descent , a critical aspect of many RL methods . Using neural nets to learn ( Q- ) value functions on novel domains is still to this day a frustrating experience due to how unstable and unpredictable gradient descent + bootstrapping is . As a result , this subject of this work is quite important and likely of great interest to the field . * The experiments are well designed and relevant to the main thesis . The empirical results are well presented and easy to understand . # Cons * After a very productive and enlightening discussion with the authors , the only noteworthy issue is that this paper contains too many contributions for the format making some of them hard to appreciate . A more focused in depth dive into a subset of the theoretical contributions might have been preferable and possibly provide more insight . # Conclusion I strongly support the acceptance of this submission . After discussion with the authors and resulting updates to the paper , I do n't see any reason for rejecting this paper . All of the major concerns from my initial review have been addressed . Initial review # Summary The authors present evidence that the approximate rank of the features is correlated with the learned policy 's performance and that this rank shrinks when using bootstrapping . They provide empirical evidence in several RL settings and domains and present some theoretical arguments which explain this behavior in the context of kernel regression and deep linear networks . Finally , they propose a simple approach for mitigating the rank collapse and show that this improves the performance of the learned policy in some cases . # Reason for score Although the authors isolate an interesting phenomenon and present some compelling empirical evidence , I have a few concerns about the theoretical contributions which , hopefully , the authors can address or clarify any misunderstanding . This is interesting work and I am more than willing to adjust my review if the authors can assuage my concerns . # Pros * The main contributions of this work might help us better understand the effects of using bootstrapping with function approximation and gradient descent , a critical aspect of many RL methods . Using neural nets to learn ( Q- ) value functions on novel domains is still to this day a frustrating experience due to how unstable and unpredictable gradient descent + bootstrapping is . As a result , this subject of this work is quite important and likely of great interest to the field . * The experiments are well designed and relevant to the main thesis . The empirical results are well presented and easy to understand . # Cons * This did not feel like an 8 page paper . This paper took a long time to review . With 18 pages of appendix , 9 of which are clarifications and proofs , what is left of the theoretical contributions in the main body of the paper does n't provide much insight into the role/importance of the assumptions or into what makes each claim true . * The proof for theorem 4.2 appears to make use of the assumption that $ \\varepsilon ( s , a ) = W_N \\cdot \\zeta [ s ; a ] $ and $ y_k = Q_ { k-1 } + \\varepsilon $ . This is not conveyed in the main body of the paper but seems to be a fairly strong assumption on the form of the bootstrapped targets . * Similarly , I would argue that the premise that the bootstrapped targets will eventually be close to the previous , i.e. , $ y_k \\approx Q_ { k-1 } $ , is flawed . There is no guarantee that applying the Bellman operator will return a function that is inside your function class , even in the linear case . Furthermore , we know this phenomenon to be significant and motivated work on the projected Bellman error , a concept heavily used by the various variants of gradient temporal difference learning . * In theorem 4.1 , the assumption that $ S $ is a normal matrix seems impractical and likely makes this result only applicable to very rare cases . * In proposition 4.1. it is n't immediately apparent where in the proof the assumption that the loss $ L $ is the TD loss is leveraged . If it is n't used , this would suggest that this is a general property of deep linear networks and would n't support the authors observations that the rank issues are specific to bootstrapping . # Questions for the authors 1 . Was anything done to `` normalize '' the results in figure 2 to account for the differing number of total updates as a result of different n ? Can these observations be explained by the fact that more updates results in the parameters traveling further from their initial values ? What happens when plotting the srank vs. # of updates in the setting ? ( These likely do n't need 3 distinct answers ) 2 . Could the authors elaborate on why the normal matrix assumption might be reasonable , or , otherwise , explain why this does n't make it a vacuous result ? 3.What is the purpose of the `` explaining implicit under-parameterization across fitting iterations '' section ? I think I am missing the insight this is trying to provide . Why would the parameters change at all if I reuse the results of the previous minimization as targets ? What does the Bellman error refer to here and what does it mean to attain zero ( or any value ) TD error when the targets are just the Q-values ? 4.The balanced assumption used with the deep linear networks seems critical for the proof . Is my assessment correct or could these results possibly hold without it ? How does this assumption limit the applicability of the insight gained here to more practical neural networks ? # Misc comments and typos * page 2 , Yang et al.do n't seem to use the term `` effective rank '' , but do use the term `` approximate rank '' . * page 4 , `` we first remove the confounding * * with * * issues [ ... ] '' * page 30 , proof , it would help to explicit state the dimensions of $ \\zeta $ . Is the $ \\top $ on $ \\zeta^\\top $ a typo ? Otherwise , why is it not used further down ? ( no need to answer either way , just reporting on something that tripped me up )", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "* * Summary * * : We thank the reviewer for constructive comments . We have substantially updated the writing in Section 4.2 , added high-level summaries for the proofs , and justify assumptions ( normality of $ \\mathbf { S } $ , analysis close to a fixed point ) in more detail . We have re-written the paragraph \u201c Explaining implicit under-parameterization across fitting iterations \u201d to improve its exposition and we have added experiments to empirically justify normality and also plot normalized results for Figure 2 . We will be happy to make more changes if the reviewer has any further suggestions . We respond to specific queries next : * * Assumptions for Theorem 4.2 , \u201c Bellman targets will be eventually close to the previous Q-values .. \u201d * * We have highlighted the assumptions in Section 4.2 and improved the theorem statement . The goal of Theorem 4.2 is to show that even when the Q-function is relatively closer to a fixed point of the Bellman equation , a drop in rank may happen and as we now show on a linear network in Figure 4 , this can prevent convergence to this fixed point by virtue of pushing the Q-function faster from $ Q^\\ * $ . * * \u201c The proof for Theorem 4.2 makes a strong assumption on the form of the bootstrapped targets \u201d * * Typically , an initialization close to the fixed point is a favorable condition for convergence to the optimal Q-function . If a method fails to converge to the optimum starting from this initialization , it is unlikely that the method will perform well more generally . We also emphasize that analyzing dynamics near a fixed point is a standard technique in non-convex analysis ( e.g. , linearizing the function near the optimum ) , analysis of ODEs ( Borkar and Meyn , 2000 ) , and optimal control ( e.g. , local analysis of the Jacobian eigenspectrum ) . * * Why is normality of S in Theorem 4.1 reasonable ? * * Without normality , a result analogous to Theorem 4.1 can be derived using an alternate notion of effective rank that utilizes eigenvalues instead of singular values : $ $ srank_ { \\delta , \\lambda } ( M_k ) = \\min~ \\Big [ k : \\frac { \\sum_ { i=1 } ^k |\\lambda_ { i } ( M_k ) | } { \\sum_ { i=1 } ^ { d } |\\lambda_i ( M_k ) | } \\geq 1 - \\delta \\Big ] . $ $ The normality assumption allows us to associate this eigenvalue-based rank to the practical singular-value based effective rank . When the matrix $ \\mathbf { S } $ is not perfectly normal , but only approximately so , a weakened version of Theorem 4.1 holds ( end of Appendix C ) . We have updated Section 4.1 and Appendix C to reflect this discussion about the normality assumption . * * \u201c S is a normal matrix seems impractical '' * * . Since S can not be computed in practice as it depends on the Green 's kernel , we instead estimate a surrogate based on the feature matrix $ \\Phi $ . We measure the eigenvalues and singular values of this surrogate matrix on Atari games and the gridworld domain and present these results in Figure A.20 and Figure A.21 respectively . We observe that the effective rank computed using norms of eigenvalues is roughly equal to the effective rank computed using singular values in all 5 games tested . Hence , practically , a drop in the effective rank computed using eigenvalue norms translates to a drop in effective rank defined using singular values . This observation , in commonly used Atari domains and the gridworld , justifies that assuming normality of $ \\mathbf { S } $ is not a restrictive assumption since eigenvalue effective rank always satisfies Theorem 4.1 and practically , this other notion of effective rank is similar to $ \\text { srank } _\\delta $ . * * Proposition 4.1 does not use any property specific to TD-loss ( \u201c This would n't support the observations that the rank issues are specific to bootstrapping \u201d ) * * : Although Proposition 4.1 is generally applicable to deep linear nets trained with gradient descent , our main result is to show how the rank drop effect in this proposition is m due to bootstrapping in RL ( Equation 5 , Section 4.2 ) . Supervised learning with neural nets also exhibits a rank drop , however , this effect is more pronounced in RL , resulting in rank collapse . The next paragraph after Proposition 4.1 , which we now clearly demarcate in the revised paper , shows that rank decrease due to supervised learning within one fitting iteration gets compounded when we utilize the labels generated from the previous instance of the network for training in future iterations . * * Balancedness and neural networks ( \u201c limit the applicability of the insight gained here to more practical neural networks ? \u201d ) * * While balancedness is an assumption that only applies to deep linear networks , and is thus limited , it is one of the common models for studying the learning dynamics of deep supervised learning ( Arora et al.2018 , 2019 , Du et al 2018 , Gunasekar et al.2017 , 2018 , 2019 ) , and typically provide insights also applicable to practical neural networks ."}, {"review_id": "O9bnihsFfXU-2", "review_text": "The main contributions of the paper are the following ones : 1 . Identifying feature rank collapse problem in RL algorithms using bootstrapping and gradient descent optimization for value function estimation and pinning down this problem to these two factors . 2.Theoretical analysis of rank collapse based on Neural Tangent Kernel framework and ideas from analysis of continuous-time differential equations . In particular , the authors showed that rank collapses near optimal point when fitting resembles self-distillation . 3.The regularization term heuristic to prevent rank collapse . Overall , the paper contains a very extensive experimental part , theoretical part and very-well motivated idea . However , the authors tried to put too much information into one paper , therefore sometimes it is difficult to follow . For example , Preposition 4.1 is difficult to follow , since a lot of interesting and important details are hidden in Appendix . Some questions and issues . 1.There is an assumption that S is a normal matrix in theorem 4.1 . How restrictive is this assumption ? 2.Is it possible to extend theoretical analysis from policy evaluation settings to policy training settings . i.e.when Bellman optimality operator is used instead of Bellman operator ? 3.I would recommend adding a title to Figure 3 ( a ) . 4.Figure 3 ( d ) contains one red trajectory , for which srank does not collapse . Could you please comment what special properties this trajectory has that srank stays almost the same ? 5 . `` Similar to Arora et al . ( 2018 ; 2019 ) , we assume that all except the last-layer weights share singular values ( a.k.a. \u201c balancedness \u201d ) . '' According to Appendix the stronger assumption W_j * W_j^T = W_ { j+1 } ^T * W_ { j+1 } is required . 6.I assume lambda is missing in equation ( C.12 ) . 7.The question regarding the equation between D.4 and D.5 . I understand how the derivative was computed , but I am not sure that I understand what 0 means in dL_0 ( W_ { N:1 } ) /dW_ { N:1 } . 8.I am a bit puzzled by the fact that Rainbow performance increased , while DQN performance decreased in online settings . What is the key underlying component that leads to different results ?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their constructive and positive feedback on this paper . We have updated the paper to add a discussion on normality of $ \\mathbf { S } $ in Theorem 4.1 and Appendix C , clarified the assumptions in Section 4.2 , and corrected typos in the paper . We answer the specific questions below . * * How restrictive is the assumption that S is a normal matrix in theorem 4.1 ? * * Normality is not a very restrictive assumption : In practice , we found that on gridworlds ( Figure A.20 ) and five Atari games ( Figure A.21 ) , a different notion of effective rank computed using norms of eigenvalues of S is roughly equal to the effective rank , $ srank_\\delta ( \\mathbf { S } ) $ computed using singular values . This new notion of effective rank , denoted as $ srank_ { \\delta , \\lambda } ( \\mathbf { S } ) $ , still enjoys an analogous version of Theorem 4.1 * for any * matrix $ \\mathbf { S } $ , even if it is non-normal , as now discussed in Section 4.1 . Thus , Theorem 4.1 does explain the phenomenon of rank collapse under not so restrictive assumptions . Furthermore , when $ \\mathbf { S } $ is approximately normal , a weakened version of Theorem 4.1 still holds , which we now elaborate on at the end of Appendix C. * * Is it possible to extend theoretical analysis to when Bellman optimality operator is used instead of Bellman operator ? * * For the analysis in Section 4.1 , we suspect that we will need additional assumptions on the alignment of the eigenspaces of the $ P^ { \\pi_i } $ matrices generated by each of the greedy policy iterates $ \\pi_i $ for the Q-function , which are used for the optimality backups . Our analysis in Section 4.2 which analyzes the rank drop near a fixed point still applies to the Bellman optimality backup . We have added a discussion stating that the extension to the Bellman optimality operator is certainly interesting for future work . * * Figure 3 ( d ) contains one red trajectory , for which srank does not collapse . Could you please comment what special properties this trajectory has that srank stays almost the same ? * * Inspecting individual runs for this figure , we observe that the TD error is the smallest of all 5 runs for the run that does not suffer from rank collapse . Quantitatively , TD error for this run is * one-third * of that of the run that attains the next largest rank . For the other runs where rank collapses , TD error takes values in the range of $ 10\\text { x } - 100\\text { x } $ times the TD-error for the highest rank run . This is consistent with our results that there can be a tradeoff between training the Q-function and rank collapse . * * All except the last-layer weights share singular values ( a.k.a. \u201c balancedness \u201d ) . '' According to Appendix the stronger assumption $ W_jW_j^T = W_ { j+1 } ^TW_ { j+1 } $ is required * * The assumption in Equation D.4 is the balancedness assumption , and we now indicate in the revised main paper that we require the assumption on the weight matrices . This condition allows us to relate singular values of consecutive weights matrices , as was discussed previously for simplicity . We also note that this assumption is weaker than the balancedness assumption used in Arora et al . ( 2018 ; 2019 ) , which will trivially give rank 1 features in our setting . * * Typos : Equation ( C.12 ) , Legend in Figure 3 ( a ) , $ L_0 $ in Equation D.4 - D.5 * * We have revised the paper to fix both these typos and the change to Equation C.12 is highlighted in red . This was a typo , thanks for pointing this out . We have fixed this typo by replacing $ L_0 $ with $ L_k $ which stands for the TD error at fitting iteration $ k $ . * * Rainbow performance increased , while DQN performance decreased in online settings . What is the key underlying component that leads to different results ? * * To provide a concrete answer , we have launched a set of experiments that ablate the key differences between Rainbow and DQN with the penalty : a distributional DQN loss , prioritized experience replay , n-step returns . This experiment will take around 2 weeks to run , so we will update the final manuscript with the new analysis . Our current hypothesis is that this is a consequence of the feedback loop between learning and data generation in online RL ( Schaul et al , 2019 [ 1 ] ) . Namely , at the onset of training , DQN learns slower because of the added penalty , causing it to collect poorer data , which in turn makes the agent continue to learn slower ( as compared to without the penalty ) . Rainbow typically learns quickly and collects more diverse data compared to DQN . Improving rank via the penalty can help fit this quickly changing data more effectively for Rainbow , thereby improving performance . Note that in offline RL , where the quality of data trained on is not tied to the intermediary agent performance , DQN does improve with the penalty ( Figure 6 ) . [ 1 ] Schaul , Tom , et al . `` Ray interference : a source of plateaus in deep reinforcement learning . '' arXiv preprint arXiv:1904.11455 ( 2019 ) ."}, {"review_id": "O9bnihsFfXU-3", "review_text": "This paper discusses a phenomenon wherein the feature vectors of the learned value function in reinforcement learning ( RL ) lose their diversity as training progresses . The paper analyzes the rank of the final hidden layer in the model parameterizing the value function and shows experimentally that for offline-RL and online-RL setups on Atari and Gym benchmarks , this rank collapse occurs with a drop in the average return . The paper further develops two models for understanding this phenomenon , ( i ) where the value function is modeled using the neural tangent kernel , and ( ii ) where the value function is modeled using a deep linear network . The paper argues that bootstrapping results in reduction of the rank of the feature matrix as training progresses for these models . A regularization term that equalizes the singular values of the feature matrix is used to mitigate this rank collapse and experimental results on Atari benchmarks are shown with this regularizer . The main claim of this paper is to identify the phenomenon of rank collapse of the feature matrix . I have concerns about the experimental findings of this paper and correctness of its theoretical claims , which are discussed below . I am willing to increase my score if the authors can convincingly argue otherwise . Broadly , I agree this is an interesting direction but current manuscript does not convince the reader that rank collapse is indeed the cause of degradation of performance . Comments.1.Figure 1 does not completely validate the claims on page 3 . In Asterix , increasing the amount of data does not lead to rank collapse but the returns degrade significantly during training , why ? In Seaquest , the returns ( blue ) have degraded essentially to zero even when the rank ( blue ) is at its maximum . This suggests that there are other factors which are causing the drop in performance instead of/in addition to the rank . The trends in Appendix A1 are similarly inconsistent , as is Figure 2 ( Ant-v2 ) . The implication \u201c if low rank , then low returns \u201d is reasonable to expect due to reduced capacity of the value function approximation . But how do the authors deduce from these experiments that \u201c rank collapses in data-efficient RL \u201d ( first sentence of Section 3.1 ) . 2.I have a similar concern about Fig.3b ( Seaquest ) . The rank for n=4 gradient steps/transition clearly collapses , yet the TD error remains small , and yet the returns are quite bad . If rank collapse entails that the TD error is not minimized well-enough , and that is the cause of the drop in returns , then how can one explain this figure ? I suspect the discrepancy is because the TD error is used in Fig.3b.Can you perhaps compute a pseudo-optimal policy using a good RL method ( say Rainbow ) for Seaquest and use its value function as the surrogate for Q * ? 3.The narrative will benefit from being more precise . There is an egregiously large number of sentences where the word \u201c implicit \u201d ( the paper uses this word 37 times in the first 8 pages ) is used in a vague manner ( see for instance Definition 1 ) . Further , \u201c implicit under-parametrization \u201d a bad monicker , should the lottery ticket hypothesis be also called implicit under-parametrization ? 4.Why is Theorem 4.1 here not a direct application of Theorem 5 of Mobahi et al. , 2020 ? Further , the big intellectual gap in the argument is that while we are trying to find the fixed point of the Bellman equation in RL , there is no such fixed point in kernel regression . So while the argument that self-distillation during iterative TD^2-minimization may cause a loss of diversity of the feature space , it does not seem to the only reason , after all some examples in Fig 3 do not show rank collapse . 5.Perhaps the underlying problem is really that minimizing TD^2 is not an appropriate way to find the fixed point of the Bellman iteration when using function approximation . Indeed , if the TD error is small ( Fig.3b , n=4 ) , there is nothing the network can do to improve the returns . TD error is small in this case in spite of the feature matrix having low rank ; it indeed depends on the complexity of the value function . 6.The development in Sec 4.2 using the work of Arora et a. , 2019 around eq . ( 5 ) argues that when Q_k ( s , a ) = Q_ { k+1 } ( s , a ) for all pairs ( s , a ) you get rank collapse ; this is a very special situation where the value function at each ( s , a ) is essentially proportional to the rewards at that state-action pair . I tried to follow the proof of the argument for the botostrapped updates in Theorem 4.2 but to my understanding it hides this same issue , e.g. , in eq . ( D.15 ) it is assumed that zeta is small enough which is not true . By this argument simply rescaling all the rewards to have small magnitude should result in rank collapse .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * How to deduce rank collapses in data-efficient RL + \u201c .. pseudo-optimal policy ... surrogate for Q\\ * ? \u201d * * We have added a new comparison with surrogate Q * for Atari in Figure 2 and we already had an experiment for gridworlds in the submission where we compared the rank of the learned Q-function to the rank of the optimal Q\\ * . As shown by the dashed line in Figure 2 , the rank of the value network with bootstrapping is far lower relative to this pseudo-optimal Q\\ * , which indicates that rank has indeed collapsed . * * Other factors affecting performance , Seaquest in Figure 1 , Asterix in Figure 1 rank drop not aligned with performance drop * * As observed by the reviewer , when the rank does not collapse , performance may still drop due to other factors , which we acknowledge and highlight in red in Section 3 in the revision . Our sole claim is that in many data-efficient RL problems , deep Q-learning can induce very low-rank solutions that heavily alias value predictions at different states and exhibit poor performance . We have also added zoomed-in plots for Atari games in Appendix A.1 ( Figure A.1 ) showing that mostly the optimum for rank appears close enough to the optimum for performance beyond which both degrade validating our claim above . While not perfectly aligned in Seaquest with 4x data , it holds as a general pattern across most environments when more gradient steps are performed ( Figure A.1 ) . * * \u201c Perhaps the underlying problem is really that minimizing TD^2 is not an appropriate way .. \u201d * * We analyze TD^2 since it is the commonly used method for deep Q-learning and actor-critic algorithms , and our goal is to understand why these commonly used methods might fail . Our results on Rainbow in Figure A.17 also study the C51-style cross-entropy loss for deep Q-learning and we observe similar findings . While the problem at a high-level can be attributed to TD^2 not being the optimal way to find the fixed point , the goal of our work is to analyze and understand precisely why this is the case , specifically for neural networks in practice . Our results highlight one of the issues specific to minimizing TD^2 with neural nets and can aid better algorithm design . * * TD error remains small ( Figure 3 ) , returns are quite bad , how can one explain this + `` If rank collapse entails that the TD error is not minimized well-enough .... how can one explain this figure ? \u201d * * The notion of \u201c smallness \u201d of TD-error depends on the problem and value function , which makes it possible to only relatively compare the different curves in Figure 3 ( that is , comparing curves n=1 with n=4 ) . It is unclear if TD error on figure 3 can be considered small , as a desired \u201c small \u201d value of TD error is unknown in Atari . We plot TD error on * * log * * scale and show that larger ranks ( n=1 ) typically correspond to smaller TD error ( n=1 ) . * * \u201c While we are trying to find the fixed point of the Bellman equation in RL , there is no such fixed point in kernel regression. \u201d * * It would be great if the reviewer can clarify this statement . Based on our understanding , both kernel regression and RL with neural network function approximation may not converge to a fixed point in a general scenario , however prior works like kernel LSTD ( Xu et al.2005 , 2007 ) do show that kernel regression converges to a fixed point under certain conditions . In this sense , the convergence of kernel regression in an RL setting does not seem to be any worse than convergence for any other non-linear function approximator . * * Name \u201c implicit under-parameterization \u201d ( .. large number of sentences where the word \u201c implicit \u201d is used in a vague sense .. ) * * We used the word \u201c implicit \u201d heavily to signify that the under-parameterization effect is implicitly caused ( no explicit loss encourages this ) similar to the implicit regularization effect of gradient descent . Specifically , \u201c Implicit under-parameterization \u201d broadly identifies the issue that a deep Q-network behaves as a low-capacity model due to the implicit regularization effect of training with bootstrapped objectives . We are more than happy to consider other names for this phenomenon if the reviewer has any suggestions ."}], "0": {"review_id": "O9bnihsFfXU-0", "review_text": "# # # Summary : This paper identifies a type of implicit under-parameterization phenomenon in deep RL methods that use bootstrapping . It is found that after an initial learning period , the effective rank of the feature matrix keeps decreasing . This implies that the representational power of the network is not fully utilized . The authors call it a type of implicit under-parameterization . Moreover , the emergence of this under-parameterization strongly correlates with the poor performance . Some preliminary theoretical analyses are provided to explain this phenomenon . # # # Pros : The paper is well written , and I can follow the idea very smoothly . The implicit under-parameterization phenomenon seems very intriguing and useful for designing better bootstrapping-based deep RL methods . The theoretical analyses are very illustrative though still preliminary . # # # Cons : 1 . The analysis in Section 4.1 seems not correct to me . For kernel regression , the implicit bias of gradient descent with an infinite number of iterations is to pick up the solutions with the smallest RKHS norm . This implies $ c=0 $ in Eq . ( 1 ) , which would make the subsequent analysis problematic . However , if early stopping is applied , the GD solutions will be equal to the one given in Eq . ( 1 ) with $ c > 0 $ . The value of $ c $ depends on how the early stopping is applied . Please refer to [ 1 ] for more details . # # # Other comments : 1 . The analysis in Section 4.1 is very illustrative due to the use of the kernel model . But all the experiments are done for neural networks . It would be much helpful if some extra experiments with kernel models can be added , for which we can directly compare the experimental results and the theoretical analysis . [ 1 ] Suggala , Arun , Adarsh Prasad , and Pradeep K. Ravikumar . `` Connecting optimization and regularization paths . '' Advances in Neural Information Processing Systems . 2018. # # # Post-rebuttal Comments I thank the authors for the response and the efforts to update the drafts . I think this submission made some original contributions .", "rating": "6: Marginally above acceptance threshold", "reply_text": "* * Summary * * : We thank the reviewer for constructive comments on the paper . We have added a discussion of the reference pointed out by the reviewer in Section 4.1 and clarified the concern about the correctness of the theorem . * * \u201c The analysis in Section 4.1 seems not correct to me ... The value of $ c $ depends on how the early stopping is applied . Please refer to [ 1 ] for more details. \u201d * * Thank you for suggesting the reference . The choice of $ c $ , which controls the strength of the regularizer , is directly based on the work from Mobahi et al.2020 , independent of RL and bootstrapping . As the reviewer pointed out the connection between optimization and regularization paths , the theory of self-distillation in Mobahi et al.2020 or kernel regression in Section 4.1 with $ c > 0 $ explains the learning dynamics of gradient descent with early stopping in the presence of strongly convex or separable losses . With this choice of $ c > 0 $ , our theorem in Section 4.1basically explains the presence of the rank collapse phenomenon when gradient descent with early stopping is used to optimize the TD error in Fitted Q-iteration . We have added this connection to Section 4.1 , and made it clear that the choice of $ c $ is directly taken from self-distillation and a value of $ c > 0 $ is optimal which matches the practical setting where early stopping or only a few gradient steps are used for training ."}, "1": {"review_id": "O9bnihsFfXU-1", "review_text": "Post discussion review # Summary The authors present evidence that the approximate rank of the features is correlated with the learned policy 's performance and that this rank shrinks when using bootstrapping . They provide empirical evidence in several RL settings and domains and present some theoretical arguments which explain this behavior in the context of kernel regression and deep linear networks . Finally , they propose a simple approach for mitigating the rank collapse and show that this improves the performance of the learned policy in some cases . # Reason for score The authors isolate an interesting phenomenon and present some compelling empirical evidence . This is interesting work and I have have no doubt that it is of sufficient quality for publications . # Pros * The main contributions of this work might help us better understand the effects of using bootstrapping with function approximation and gradient descent , a critical aspect of many RL methods . Using neural nets to learn ( Q- ) value functions on novel domains is still to this day a frustrating experience due to how unstable and unpredictable gradient descent + bootstrapping is . As a result , this subject of this work is quite important and likely of great interest to the field . * The experiments are well designed and relevant to the main thesis . The empirical results are well presented and easy to understand . # Cons * After a very productive and enlightening discussion with the authors , the only noteworthy issue is that this paper contains too many contributions for the format making some of them hard to appreciate . A more focused in depth dive into a subset of the theoretical contributions might have been preferable and possibly provide more insight . # Conclusion I strongly support the acceptance of this submission . After discussion with the authors and resulting updates to the paper , I do n't see any reason for rejecting this paper . All of the major concerns from my initial review have been addressed . Initial review # Summary The authors present evidence that the approximate rank of the features is correlated with the learned policy 's performance and that this rank shrinks when using bootstrapping . They provide empirical evidence in several RL settings and domains and present some theoretical arguments which explain this behavior in the context of kernel regression and deep linear networks . Finally , they propose a simple approach for mitigating the rank collapse and show that this improves the performance of the learned policy in some cases . # Reason for score Although the authors isolate an interesting phenomenon and present some compelling empirical evidence , I have a few concerns about the theoretical contributions which , hopefully , the authors can address or clarify any misunderstanding . This is interesting work and I am more than willing to adjust my review if the authors can assuage my concerns . # Pros * The main contributions of this work might help us better understand the effects of using bootstrapping with function approximation and gradient descent , a critical aspect of many RL methods . Using neural nets to learn ( Q- ) value functions on novel domains is still to this day a frustrating experience due to how unstable and unpredictable gradient descent + bootstrapping is . As a result , this subject of this work is quite important and likely of great interest to the field . * The experiments are well designed and relevant to the main thesis . The empirical results are well presented and easy to understand . # Cons * This did not feel like an 8 page paper . This paper took a long time to review . With 18 pages of appendix , 9 of which are clarifications and proofs , what is left of the theoretical contributions in the main body of the paper does n't provide much insight into the role/importance of the assumptions or into what makes each claim true . * The proof for theorem 4.2 appears to make use of the assumption that $ \\varepsilon ( s , a ) = W_N \\cdot \\zeta [ s ; a ] $ and $ y_k = Q_ { k-1 } + \\varepsilon $ . This is not conveyed in the main body of the paper but seems to be a fairly strong assumption on the form of the bootstrapped targets . * Similarly , I would argue that the premise that the bootstrapped targets will eventually be close to the previous , i.e. , $ y_k \\approx Q_ { k-1 } $ , is flawed . There is no guarantee that applying the Bellman operator will return a function that is inside your function class , even in the linear case . Furthermore , we know this phenomenon to be significant and motivated work on the projected Bellman error , a concept heavily used by the various variants of gradient temporal difference learning . * In theorem 4.1 , the assumption that $ S $ is a normal matrix seems impractical and likely makes this result only applicable to very rare cases . * In proposition 4.1. it is n't immediately apparent where in the proof the assumption that the loss $ L $ is the TD loss is leveraged . If it is n't used , this would suggest that this is a general property of deep linear networks and would n't support the authors observations that the rank issues are specific to bootstrapping . # Questions for the authors 1 . Was anything done to `` normalize '' the results in figure 2 to account for the differing number of total updates as a result of different n ? Can these observations be explained by the fact that more updates results in the parameters traveling further from their initial values ? What happens when plotting the srank vs. # of updates in the setting ? ( These likely do n't need 3 distinct answers ) 2 . Could the authors elaborate on why the normal matrix assumption might be reasonable , or , otherwise , explain why this does n't make it a vacuous result ? 3.What is the purpose of the `` explaining implicit under-parameterization across fitting iterations '' section ? I think I am missing the insight this is trying to provide . Why would the parameters change at all if I reuse the results of the previous minimization as targets ? What does the Bellman error refer to here and what does it mean to attain zero ( or any value ) TD error when the targets are just the Q-values ? 4.The balanced assumption used with the deep linear networks seems critical for the proof . Is my assessment correct or could these results possibly hold without it ? How does this assumption limit the applicability of the insight gained here to more practical neural networks ? # Misc comments and typos * page 2 , Yang et al.do n't seem to use the term `` effective rank '' , but do use the term `` approximate rank '' . * page 4 , `` we first remove the confounding * * with * * issues [ ... ] '' * page 30 , proof , it would help to explicit state the dimensions of $ \\zeta $ . Is the $ \\top $ on $ \\zeta^\\top $ a typo ? Otherwise , why is it not used further down ? ( no need to answer either way , just reporting on something that tripped me up )", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "* * Summary * * : We thank the reviewer for constructive comments . We have substantially updated the writing in Section 4.2 , added high-level summaries for the proofs , and justify assumptions ( normality of $ \\mathbf { S } $ , analysis close to a fixed point ) in more detail . We have re-written the paragraph \u201c Explaining implicit under-parameterization across fitting iterations \u201d to improve its exposition and we have added experiments to empirically justify normality and also plot normalized results for Figure 2 . We will be happy to make more changes if the reviewer has any further suggestions . We respond to specific queries next : * * Assumptions for Theorem 4.2 , \u201c Bellman targets will be eventually close to the previous Q-values .. \u201d * * We have highlighted the assumptions in Section 4.2 and improved the theorem statement . The goal of Theorem 4.2 is to show that even when the Q-function is relatively closer to a fixed point of the Bellman equation , a drop in rank may happen and as we now show on a linear network in Figure 4 , this can prevent convergence to this fixed point by virtue of pushing the Q-function faster from $ Q^\\ * $ . * * \u201c The proof for Theorem 4.2 makes a strong assumption on the form of the bootstrapped targets \u201d * * Typically , an initialization close to the fixed point is a favorable condition for convergence to the optimal Q-function . If a method fails to converge to the optimum starting from this initialization , it is unlikely that the method will perform well more generally . We also emphasize that analyzing dynamics near a fixed point is a standard technique in non-convex analysis ( e.g. , linearizing the function near the optimum ) , analysis of ODEs ( Borkar and Meyn , 2000 ) , and optimal control ( e.g. , local analysis of the Jacobian eigenspectrum ) . * * Why is normality of S in Theorem 4.1 reasonable ? * * Without normality , a result analogous to Theorem 4.1 can be derived using an alternate notion of effective rank that utilizes eigenvalues instead of singular values : $ $ srank_ { \\delta , \\lambda } ( M_k ) = \\min~ \\Big [ k : \\frac { \\sum_ { i=1 } ^k |\\lambda_ { i } ( M_k ) | } { \\sum_ { i=1 } ^ { d } |\\lambda_i ( M_k ) | } \\geq 1 - \\delta \\Big ] . $ $ The normality assumption allows us to associate this eigenvalue-based rank to the practical singular-value based effective rank . When the matrix $ \\mathbf { S } $ is not perfectly normal , but only approximately so , a weakened version of Theorem 4.1 holds ( end of Appendix C ) . We have updated Section 4.1 and Appendix C to reflect this discussion about the normality assumption . * * \u201c S is a normal matrix seems impractical '' * * . Since S can not be computed in practice as it depends on the Green 's kernel , we instead estimate a surrogate based on the feature matrix $ \\Phi $ . We measure the eigenvalues and singular values of this surrogate matrix on Atari games and the gridworld domain and present these results in Figure A.20 and Figure A.21 respectively . We observe that the effective rank computed using norms of eigenvalues is roughly equal to the effective rank computed using singular values in all 5 games tested . Hence , practically , a drop in the effective rank computed using eigenvalue norms translates to a drop in effective rank defined using singular values . This observation , in commonly used Atari domains and the gridworld , justifies that assuming normality of $ \\mathbf { S } $ is not a restrictive assumption since eigenvalue effective rank always satisfies Theorem 4.1 and practically , this other notion of effective rank is similar to $ \\text { srank } _\\delta $ . * * Proposition 4.1 does not use any property specific to TD-loss ( \u201c This would n't support the observations that the rank issues are specific to bootstrapping \u201d ) * * : Although Proposition 4.1 is generally applicable to deep linear nets trained with gradient descent , our main result is to show how the rank drop effect in this proposition is m due to bootstrapping in RL ( Equation 5 , Section 4.2 ) . Supervised learning with neural nets also exhibits a rank drop , however , this effect is more pronounced in RL , resulting in rank collapse . The next paragraph after Proposition 4.1 , which we now clearly demarcate in the revised paper , shows that rank decrease due to supervised learning within one fitting iteration gets compounded when we utilize the labels generated from the previous instance of the network for training in future iterations . * * Balancedness and neural networks ( \u201c limit the applicability of the insight gained here to more practical neural networks ? \u201d ) * * While balancedness is an assumption that only applies to deep linear networks , and is thus limited , it is one of the common models for studying the learning dynamics of deep supervised learning ( Arora et al.2018 , 2019 , Du et al 2018 , Gunasekar et al.2017 , 2018 , 2019 ) , and typically provide insights also applicable to practical neural networks ."}, "2": {"review_id": "O9bnihsFfXU-2", "review_text": "The main contributions of the paper are the following ones : 1 . Identifying feature rank collapse problem in RL algorithms using bootstrapping and gradient descent optimization for value function estimation and pinning down this problem to these two factors . 2.Theoretical analysis of rank collapse based on Neural Tangent Kernel framework and ideas from analysis of continuous-time differential equations . In particular , the authors showed that rank collapses near optimal point when fitting resembles self-distillation . 3.The regularization term heuristic to prevent rank collapse . Overall , the paper contains a very extensive experimental part , theoretical part and very-well motivated idea . However , the authors tried to put too much information into one paper , therefore sometimes it is difficult to follow . For example , Preposition 4.1 is difficult to follow , since a lot of interesting and important details are hidden in Appendix . Some questions and issues . 1.There is an assumption that S is a normal matrix in theorem 4.1 . How restrictive is this assumption ? 2.Is it possible to extend theoretical analysis from policy evaluation settings to policy training settings . i.e.when Bellman optimality operator is used instead of Bellman operator ? 3.I would recommend adding a title to Figure 3 ( a ) . 4.Figure 3 ( d ) contains one red trajectory , for which srank does not collapse . Could you please comment what special properties this trajectory has that srank stays almost the same ? 5 . `` Similar to Arora et al . ( 2018 ; 2019 ) , we assume that all except the last-layer weights share singular values ( a.k.a. \u201c balancedness \u201d ) . '' According to Appendix the stronger assumption W_j * W_j^T = W_ { j+1 } ^T * W_ { j+1 } is required . 6.I assume lambda is missing in equation ( C.12 ) . 7.The question regarding the equation between D.4 and D.5 . I understand how the derivative was computed , but I am not sure that I understand what 0 means in dL_0 ( W_ { N:1 } ) /dW_ { N:1 } . 8.I am a bit puzzled by the fact that Rainbow performance increased , while DQN performance decreased in online settings . What is the key underlying component that leads to different results ?", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their constructive and positive feedback on this paper . We have updated the paper to add a discussion on normality of $ \\mathbf { S } $ in Theorem 4.1 and Appendix C , clarified the assumptions in Section 4.2 , and corrected typos in the paper . We answer the specific questions below . * * How restrictive is the assumption that S is a normal matrix in theorem 4.1 ? * * Normality is not a very restrictive assumption : In practice , we found that on gridworlds ( Figure A.20 ) and five Atari games ( Figure A.21 ) , a different notion of effective rank computed using norms of eigenvalues of S is roughly equal to the effective rank , $ srank_\\delta ( \\mathbf { S } ) $ computed using singular values . This new notion of effective rank , denoted as $ srank_ { \\delta , \\lambda } ( \\mathbf { S } ) $ , still enjoys an analogous version of Theorem 4.1 * for any * matrix $ \\mathbf { S } $ , even if it is non-normal , as now discussed in Section 4.1 . Thus , Theorem 4.1 does explain the phenomenon of rank collapse under not so restrictive assumptions . Furthermore , when $ \\mathbf { S } $ is approximately normal , a weakened version of Theorem 4.1 still holds , which we now elaborate on at the end of Appendix C. * * Is it possible to extend theoretical analysis to when Bellman optimality operator is used instead of Bellman operator ? * * For the analysis in Section 4.1 , we suspect that we will need additional assumptions on the alignment of the eigenspaces of the $ P^ { \\pi_i } $ matrices generated by each of the greedy policy iterates $ \\pi_i $ for the Q-function , which are used for the optimality backups . Our analysis in Section 4.2 which analyzes the rank drop near a fixed point still applies to the Bellman optimality backup . We have added a discussion stating that the extension to the Bellman optimality operator is certainly interesting for future work . * * Figure 3 ( d ) contains one red trajectory , for which srank does not collapse . Could you please comment what special properties this trajectory has that srank stays almost the same ? * * Inspecting individual runs for this figure , we observe that the TD error is the smallest of all 5 runs for the run that does not suffer from rank collapse . Quantitatively , TD error for this run is * one-third * of that of the run that attains the next largest rank . For the other runs where rank collapses , TD error takes values in the range of $ 10\\text { x } - 100\\text { x } $ times the TD-error for the highest rank run . This is consistent with our results that there can be a tradeoff between training the Q-function and rank collapse . * * All except the last-layer weights share singular values ( a.k.a. \u201c balancedness \u201d ) . '' According to Appendix the stronger assumption $ W_jW_j^T = W_ { j+1 } ^TW_ { j+1 } $ is required * * The assumption in Equation D.4 is the balancedness assumption , and we now indicate in the revised main paper that we require the assumption on the weight matrices . This condition allows us to relate singular values of consecutive weights matrices , as was discussed previously for simplicity . We also note that this assumption is weaker than the balancedness assumption used in Arora et al . ( 2018 ; 2019 ) , which will trivially give rank 1 features in our setting . * * Typos : Equation ( C.12 ) , Legend in Figure 3 ( a ) , $ L_0 $ in Equation D.4 - D.5 * * We have revised the paper to fix both these typos and the change to Equation C.12 is highlighted in red . This was a typo , thanks for pointing this out . We have fixed this typo by replacing $ L_0 $ with $ L_k $ which stands for the TD error at fitting iteration $ k $ . * * Rainbow performance increased , while DQN performance decreased in online settings . What is the key underlying component that leads to different results ? * * To provide a concrete answer , we have launched a set of experiments that ablate the key differences between Rainbow and DQN with the penalty : a distributional DQN loss , prioritized experience replay , n-step returns . This experiment will take around 2 weeks to run , so we will update the final manuscript with the new analysis . Our current hypothesis is that this is a consequence of the feedback loop between learning and data generation in online RL ( Schaul et al , 2019 [ 1 ] ) . Namely , at the onset of training , DQN learns slower because of the added penalty , causing it to collect poorer data , which in turn makes the agent continue to learn slower ( as compared to without the penalty ) . Rainbow typically learns quickly and collects more diverse data compared to DQN . Improving rank via the penalty can help fit this quickly changing data more effectively for Rainbow , thereby improving performance . Note that in offline RL , where the quality of data trained on is not tied to the intermediary agent performance , DQN does improve with the penalty ( Figure 6 ) . [ 1 ] Schaul , Tom , et al . `` Ray interference : a source of plateaus in deep reinforcement learning . '' arXiv preprint arXiv:1904.11455 ( 2019 ) ."}, "3": {"review_id": "O9bnihsFfXU-3", "review_text": "This paper discusses a phenomenon wherein the feature vectors of the learned value function in reinforcement learning ( RL ) lose their diversity as training progresses . The paper analyzes the rank of the final hidden layer in the model parameterizing the value function and shows experimentally that for offline-RL and online-RL setups on Atari and Gym benchmarks , this rank collapse occurs with a drop in the average return . The paper further develops two models for understanding this phenomenon , ( i ) where the value function is modeled using the neural tangent kernel , and ( ii ) where the value function is modeled using a deep linear network . The paper argues that bootstrapping results in reduction of the rank of the feature matrix as training progresses for these models . A regularization term that equalizes the singular values of the feature matrix is used to mitigate this rank collapse and experimental results on Atari benchmarks are shown with this regularizer . The main claim of this paper is to identify the phenomenon of rank collapse of the feature matrix . I have concerns about the experimental findings of this paper and correctness of its theoretical claims , which are discussed below . I am willing to increase my score if the authors can convincingly argue otherwise . Broadly , I agree this is an interesting direction but current manuscript does not convince the reader that rank collapse is indeed the cause of degradation of performance . Comments.1.Figure 1 does not completely validate the claims on page 3 . In Asterix , increasing the amount of data does not lead to rank collapse but the returns degrade significantly during training , why ? In Seaquest , the returns ( blue ) have degraded essentially to zero even when the rank ( blue ) is at its maximum . This suggests that there are other factors which are causing the drop in performance instead of/in addition to the rank . The trends in Appendix A1 are similarly inconsistent , as is Figure 2 ( Ant-v2 ) . The implication \u201c if low rank , then low returns \u201d is reasonable to expect due to reduced capacity of the value function approximation . But how do the authors deduce from these experiments that \u201c rank collapses in data-efficient RL \u201d ( first sentence of Section 3.1 ) . 2.I have a similar concern about Fig.3b ( Seaquest ) . The rank for n=4 gradient steps/transition clearly collapses , yet the TD error remains small , and yet the returns are quite bad . If rank collapse entails that the TD error is not minimized well-enough , and that is the cause of the drop in returns , then how can one explain this figure ? I suspect the discrepancy is because the TD error is used in Fig.3b.Can you perhaps compute a pseudo-optimal policy using a good RL method ( say Rainbow ) for Seaquest and use its value function as the surrogate for Q * ? 3.The narrative will benefit from being more precise . There is an egregiously large number of sentences where the word \u201c implicit \u201d ( the paper uses this word 37 times in the first 8 pages ) is used in a vague manner ( see for instance Definition 1 ) . Further , \u201c implicit under-parametrization \u201d a bad monicker , should the lottery ticket hypothesis be also called implicit under-parametrization ? 4.Why is Theorem 4.1 here not a direct application of Theorem 5 of Mobahi et al. , 2020 ? Further , the big intellectual gap in the argument is that while we are trying to find the fixed point of the Bellman equation in RL , there is no such fixed point in kernel regression . So while the argument that self-distillation during iterative TD^2-minimization may cause a loss of diversity of the feature space , it does not seem to the only reason , after all some examples in Fig 3 do not show rank collapse . 5.Perhaps the underlying problem is really that minimizing TD^2 is not an appropriate way to find the fixed point of the Bellman iteration when using function approximation . Indeed , if the TD error is small ( Fig.3b , n=4 ) , there is nothing the network can do to improve the returns . TD error is small in this case in spite of the feature matrix having low rank ; it indeed depends on the complexity of the value function . 6.The development in Sec 4.2 using the work of Arora et a. , 2019 around eq . ( 5 ) argues that when Q_k ( s , a ) = Q_ { k+1 } ( s , a ) for all pairs ( s , a ) you get rank collapse ; this is a very special situation where the value function at each ( s , a ) is essentially proportional to the rewards at that state-action pair . I tried to follow the proof of the argument for the botostrapped updates in Theorem 4.2 but to my understanding it hides this same issue , e.g. , in eq . ( D.15 ) it is assumed that zeta is small enough which is not true . By this argument simply rescaling all the rewards to have small magnitude should result in rank collapse .", "rating": "5: Marginally below acceptance threshold", "reply_text": "* * How to deduce rank collapses in data-efficient RL + \u201c .. pseudo-optimal policy ... surrogate for Q\\ * ? \u201d * * We have added a new comparison with surrogate Q * for Atari in Figure 2 and we already had an experiment for gridworlds in the submission where we compared the rank of the learned Q-function to the rank of the optimal Q\\ * . As shown by the dashed line in Figure 2 , the rank of the value network with bootstrapping is far lower relative to this pseudo-optimal Q\\ * , which indicates that rank has indeed collapsed . * * Other factors affecting performance , Seaquest in Figure 1 , Asterix in Figure 1 rank drop not aligned with performance drop * * As observed by the reviewer , when the rank does not collapse , performance may still drop due to other factors , which we acknowledge and highlight in red in Section 3 in the revision . Our sole claim is that in many data-efficient RL problems , deep Q-learning can induce very low-rank solutions that heavily alias value predictions at different states and exhibit poor performance . We have also added zoomed-in plots for Atari games in Appendix A.1 ( Figure A.1 ) showing that mostly the optimum for rank appears close enough to the optimum for performance beyond which both degrade validating our claim above . While not perfectly aligned in Seaquest with 4x data , it holds as a general pattern across most environments when more gradient steps are performed ( Figure A.1 ) . * * \u201c Perhaps the underlying problem is really that minimizing TD^2 is not an appropriate way .. \u201d * * We analyze TD^2 since it is the commonly used method for deep Q-learning and actor-critic algorithms , and our goal is to understand why these commonly used methods might fail . Our results on Rainbow in Figure A.17 also study the C51-style cross-entropy loss for deep Q-learning and we observe similar findings . While the problem at a high-level can be attributed to TD^2 not being the optimal way to find the fixed point , the goal of our work is to analyze and understand precisely why this is the case , specifically for neural networks in practice . Our results highlight one of the issues specific to minimizing TD^2 with neural nets and can aid better algorithm design . * * TD error remains small ( Figure 3 ) , returns are quite bad , how can one explain this + `` If rank collapse entails that the TD error is not minimized well-enough .... how can one explain this figure ? \u201d * * The notion of \u201c smallness \u201d of TD-error depends on the problem and value function , which makes it possible to only relatively compare the different curves in Figure 3 ( that is , comparing curves n=1 with n=4 ) . It is unclear if TD error on figure 3 can be considered small , as a desired \u201c small \u201d value of TD error is unknown in Atari . We plot TD error on * * log * * scale and show that larger ranks ( n=1 ) typically correspond to smaller TD error ( n=1 ) . * * \u201c While we are trying to find the fixed point of the Bellman equation in RL , there is no such fixed point in kernel regression. \u201d * * It would be great if the reviewer can clarify this statement . Based on our understanding , both kernel regression and RL with neural network function approximation may not converge to a fixed point in a general scenario , however prior works like kernel LSTD ( Xu et al.2005 , 2007 ) do show that kernel regression converges to a fixed point under certain conditions . In this sense , the convergence of kernel regression in an RL setting does not seem to be any worse than convergence for any other non-linear function approximator . * * Name \u201c implicit under-parameterization \u201d ( .. large number of sentences where the word \u201c implicit \u201d is used in a vague sense .. ) * * We used the word \u201c implicit \u201d heavily to signify that the under-parameterization effect is implicitly caused ( no explicit loss encourages this ) similar to the implicit regularization effect of gradient descent . Specifically , \u201c Implicit under-parameterization \u201d broadly identifies the issue that a deep Q-network behaves as a low-capacity model due to the implicit regularization effect of training with bootstrapped objectives . We are more than happy to consider other names for this phenomenon if the reviewer has any suggestions ."}}