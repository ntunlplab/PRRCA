{"year": "2017", "forum": "HkEI22jeg", "title": "Multilayer Recurrent Network Models of Primate Retinal Ganglion Cell Responses", "decision": "Accept (Poster)", "meta_review": "This work is an important step in developing the tools for understanding the nonlinear response properties of visual neurons. The methods are sound and the results are meaningful. Reviewer 3 gave a much lower score than the other two reviewers because Rev 3 does not appreciate the improvement of prediction performance as an advance in itself. For understanding of the visual algorithms in the brain, however, prediction performance is the most critical success criterion. The paper provides convincing evidence that the approach is promising and likely to facilitate further advances towards achieving this long-term goal.\n \n I am confident enough to defend acceptance of this paper for a poster.", "reviews": [{"review_id": "HkEI22jeg-0", "review_text": "This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models. A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images. The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models. This work is an important step in understanding the nonlinear response properties of visual neurons. Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models. So this presents an important challenge to the field. If we even had *a* model that works, it would be a starting point. So this work should be seen in that light. The challenge now of course is to tease apart what the rnn is doing. Perhaps it could now be pruned and simplified to see what parts are critical to performance. It would have been nice to see such an analysis. Nevertheless this result is a good first start and I think important for people to know about. I am a bit confused about what is being called a \"movie.\" My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each. But then it is stated that the \"frame rate\" is 1/8.33 ms. I think this must refer to the refresh rate of the monitor, right? I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies. Here I would expect the rnn to have an even more profound effect, and potentially be much more informative. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the review ! The movie shown is a sequence of unrelated images , but within each 1 second segment , the image is jittered on the screen to simulate the small eye movements that occur in a fixating animal . Therefore , there is constant movement . The frame rate of this movie is 120Hz , which is also the refresh rate of the monitor . This represents a common realistic setting where macaques gaze at part of a relatively static scene with fixational eye movements , and then saccade to another part of the scene ( represented by the new image ) . The main difference between these images and natural movies is that there are no objects moving in the environment , only small translations of a sequence of static scenes . An example stimulus can be found at https : //youtu.be/sG_18Uz_6OE ( this link will be added to the manuscript ) ."}, {"review_id": "HkEI22jeg-1", "review_text": "This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina. On the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology. On the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective. I think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale. I suspect followup work building on this proof of concept will be increasingly exciting. Minor comments: Sec 3.2: I didn't understand the role of the 0.833 ms bins. Use \"epoch\" throughout, rather than alternating between \"epoch\" and \"pass through data\". Fig. 4 would be better with the x-axis on a log scale.", "rating": "7: Good paper, accept", "reply_text": "Thanks for the review ! While it is possible that there is non-spiking , low-frequency information similar to the cortical LFP , this is not a measurement typically done in the retina , and due to technical reasons these signals are not recorded on our multielectrode array . As mentioned previously , we plan to incorporate correlations in future work , although doing so using coupling filters from spike trains of neighbouring neurons has not previously had a significant impact on predicting trial-averaged firing rates ( Pillow2008 , Heitman2016 ) .The 0.833 ms bins were used because a post spike history filter operates on a small time scale so we divided the 8.33 ms bins corresponding to the frame rate by 10 . We need small bins for it to capture effects like the refractory period ."}, {"review_id": "HkEI22jeg-2", "review_text": "This paper fits models to spike trains of retinal ganglion cells that are driven by natural images. I think the title should thus include the word \u201cactivity\u201d at the end for otherwise it is actually formally incorrect. Anyhow, this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model. Overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it. In general the paper sounds plausible, though I am not convinced that I learned a lot. The results in figure 2 show that the RNN model can predict the spikes a bit better. So this is nice. But now what? You have shown that a more complicated model can produce better fits to the data, though there are of course still some variations to the real data. Your initial outline was that a better predictive model helps you to better understand the neural processing in the retina. So tell us what you learned. I am not a specialist of the retina, but I know that there are several layers and recurrencies in the retina, so I am not so surprised that the new model is better than the GLM. It seems that more complicated recurrent models such as LSTM do not improve the performance according to a statement in the paper. However, comparisons on this level are also difficult as a more complex models needs more data. Hence, I would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further. I was also a bit puzzled that all the neurons in the network share all the same parameters (weights). While the results show that these simplified models can capture a lot of the spike train characteristics, couldn\u2019t a model with free parameters eventually outperform this one (with correspondingly more training data)? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for taking the time to review our manuscript . We will change the title as suggested . We think , as other reviewers have mentioned , that this work is exciting because it introduces the idea that deep neural networks can be used in a neural modeling context with limited experimental data . In particular , the framework we introduced for sharing information across neurons allows us to fit richer models given less data , and we believe this will be a powerful approach for modeling data from a number of other brain areas - most of which are much less studied / understood than the retina . Yes , a model with free parameters or more complicated structure could eventually outperform the shared model with enough training data but there are real biological and experimental limitations to the amount of data one can collect in these experiments and more broadly in most neuroscience experiments . We also believe we have taken important first steps to understanding the model improvement through model comparisons . Whether the gap between linear-nonlinear model performance and optimal performance could be explained by a combination of long-range spatial interactions and spatial and temporal nonlinearities might seem simple , but it is an important question in the field that has been difficult to address . We showed that the former is not true ( at least up to the level of RNN performance ) and that both spatial nonlinearities and nonlinear temporal processing are important . These results can guide future research and experiments and further demonstrate the utility of our approach . This is a first step : in future work , we plan to further interrogate the RNN models to understand more specifically what information they are capturing . As noted above , we also plan to apply this framework to non-retinal neurons where there are larger gaps between what current models capture and the optimal predictions . In these systems , there is more room for improvement and more basic questions can be answered using our approach ."}], "0": {"review_id": "HkEI22jeg-0", "review_text": "This paper explores the ability of nonlinear recurrent neural networks to account for neural response properties that have otherwise eluded the ability of other models. A multilayer rnn is trained to imitate the stimulus-response mapping measured from actual retinal ganglion cells in response to a sequence of natural images. The rnn performs significantly better, especially in accounting for transient responses, than conventional LN/GLM models. This work is an important step in understanding the nonlinear response properties of visual neurons. Recent results have shown that the responses of even retinal ganglion cells in response to natural movies are difficult to explain in terms of standard receptive field models. So this presents an important challenge to the field. If we even had *a* model that works, it would be a starting point. So this work should be seen in that light. The challenge now of course is to tease apart what the rnn is doing. Perhaps it could now be pruned and simplified to see what parts are critical to performance. It would have been nice to see such an analysis. Nevertheless this result is a good first start and I think important for people to know about. I am a bit confused about what is being called a \"movie.\" My understanding is that it is essentially a sequence of unrelated images shown for 1 sec. each. But then it is stated that the \"frame rate\" is 1/8.33 ms. I think this must refer to the refresh rate of the monitor, right? I would guess that the deviations from the LN model are even stronger when you show actual dynamic natural scenes - i.e., real movies. Here I would expect the rnn to have an even more profound effect, and potentially be much more informative. ", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks for the review ! The movie shown is a sequence of unrelated images , but within each 1 second segment , the image is jittered on the screen to simulate the small eye movements that occur in a fixating animal . Therefore , there is constant movement . The frame rate of this movie is 120Hz , which is also the refresh rate of the monitor . This represents a common realistic setting where macaques gaze at part of a relatively static scene with fixational eye movements , and then saccade to another part of the scene ( represented by the new image ) . The main difference between these images and natural movies is that there are no objects moving in the environment , only small translations of a sequence of static scenes . An example stimulus can be found at https : //youtu.be/sG_18Uz_6OE ( this link will be added to the manuscript ) ."}, "1": {"review_id": "HkEI22jeg-1", "review_text": "This is a clearly written paper with a nice, if straightforward, result: RNNs can be good predictive models of neuron firing rates in the retina. On the one hand, the primary scientific contribution seems to just be to confirm that this approach works. On this particular stimulus locked task the gains from using the RNN seemed relatively modest, and it hasn't yet taught us anything new about the biology. On the other hand, this (along with the concurrent work of McIntosh et al.) is introducing neural network modeling to a field that isn't currently using it, and where it should prove very effective. I think it would be very interesting to see the results of applying a framework like this one with LFP and other neurons as input and on a shorter discretization time scale. I suspect followup work building on this proof of concept will be increasingly exciting. Minor comments: Sec 3.2: I didn't understand the role of the 0.833 ms bins. Use \"epoch\" throughout, rather than alternating between \"epoch\" and \"pass through data\". Fig. 4 would be better with the x-axis on a log scale.", "rating": "7: Good paper, accept", "reply_text": "Thanks for the review ! While it is possible that there is non-spiking , low-frequency information similar to the cortical LFP , this is not a measurement typically done in the retina , and due to technical reasons these signals are not recorded on our multielectrode array . As mentioned previously , we plan to incorporate correlations in future work , although doing so using coupling filters from spike trains of neighbouring neurons has not previously had a significant impact on predicting trial-averaged firing rates ( Pillow2008 , Heitman2016 ) .The 0.833 ms bins were used because a post spike history filter operates on a small time scale so we divided the 8.33 ms bins corresponding to the frame rate by 10 . We need small bins for it to capture effects like the refractory period ."}, "2": {"review_id": "HkEI22jeg-2", "review_text": "This paper fits models to spike trains of retinal ganglion cells that are driven by natural images. I think the title should thus include the word \u201cactivity\u201d at the end for otherwise it is actually formally incorrect. Anyhow, this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model. Overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it. In general the paper sounds plausible, though I am not convinced that I learned a lot. The results in figure 2 show that the RNN model can predict the spikes a bit better. So this is nice. But now what? You have shown that a more complicated model can produce better fits to the data, though there are of course still some variations to the real data. Your initial outline was that a better predictive model helps you to better understand the neural processing in the retina. So tell us what you learned. I am not a specialist of the retina, but I know that there are several layers and recurrencies in the retina, so I am not so surprised that the new model is better than the GLM. It seems that more complicated recurrent models such as LSTM do not improve the performance according to a statement in the paper. However, comparisons on this level are also difficult as a more complex models needs more data. Hence, I would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further. I was also a bit puzzled that all the neurons in the network share all the same parameters (weights). While the results show that these simplified models can capture a lot of the spike train characteristics, couldn\u2019t a model with free parameters eventually outperform this one (with correspondingly more training data)? ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for taking the time to review our manuscript . We will change the title as suggested . We think , as other reviewers have mentioned , that this work is exciting because it introduces the idea that deep neural networks can be used in a neural modeling context with limited experimental data . In particular , the framework we introduced for sharing information across neurons allows us to fit richer models given less data , and we believe this will be a powerful approach for modeling data from a number of other brain areas - most of which are much less studied / understood than the retina . Yes , a model with free parameters or more complicated structure could eventually outperform the shared model with enough training data but there are real biological and experimental limitations to the amount of data one can collect in these experiments and more broadly in most neuroscience experiments . We also believe we have taken important first steps to understanding the model improvement through model comparisons . Whether the gap between linear-nonlinear model performance and optimal performance could be explained by a combination of long-range spatial interactions and spatial and temporal nonlinearities might seem simple , but it is an important question in the field that has been difficult to address . We showed that the former is not true ( at least up to the level of RNN performance ) and that both spatial nonlinearities and nonlinear temporal processing are important . These results can guide future research and experiments and further demonstrate the utility of our approach . This is a first step : in future work , we plan to further interrogate the RNN models to understand more specifically what information they are capturing . As noted above , we also plan to apply this framework to non-retinal neurons where there are larger gaps between what current models capture and the optimal predictions . In these systems , there is more room for improvement and more basic questions can be answered using our approach ."}}