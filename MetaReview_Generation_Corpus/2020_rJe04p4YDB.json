{"year": "2020", "forum": "rJe04p4YDB", "title": "Semi-supervised Learning by Coaching", "decision": "Reject", "meta_review": "Authors propose a new method of semi-supervised learning and provide empirical results. Reviewers found the presentation of the method confusing and poorly motivated. Despite the rebuttal, reviewers still did not find clarity on how or why the method works as well as it does.", "reviews": [{"review_id": "rJe04p4YDB-0", "review_text": "This paper studies the teacher-student models in semi-supervised learning. Unlike previous methods in which only the student will learn from the teacher, this paper proposes a method to let the teacher learn from the student by reinforcement learning. Experimental results demonstrate the proposal\u2019s performance. The paper achieves some good empirical results compared to other baselines. However, the proposed method is implemented with many tricks listed on Page 4, and with data augmentation techniques, which may not be used in previous methods. Additionally, the paper is weak in technology. There is no clear explanation of why the proposed method works except a metaphor for sports coaches. I vote for a clear rejection of the paper. First, the paper is weak in experiments. It works hard to achieve a good experimental result, through many tricks listed in the \u201cAdditional Implementation Details\u201d in Page 4, and through the data augmentation used in Page 5. However, these tricks to improve the performance may not be used in previous methods, as the paper does not run experiments on baselines under the same setting, but use the results reported in Oliver et al. (2018). Additionally, the paper only uses one number of labeled data for each data set, it makes readers doubt that the proposed method only works under this number of labeled data. The paper fails to clearly state why we need to let the teacher learn from the student. Actually, I doubt if this is necessary. Given the strong learning capacity of neural networks, the proposed method will easily be overfitting. Assume we have a very weak student network at the beginning, then by training in the way proposed in the paper, the teacher network will have all labeled data classified correct, and all unlabeled data classified into the same labels as the student network. I cannot see from the simple proposal why such overfitting can be avoided. The paper is weak in both technology and experiments. It is also poorly written without clearly stating the motivation for this problem. I would vote for a reject for the paper. ------------------------------------------- The rebuttal has cleared some of my concerns. However, it is still not clear why the proposed method work and how does it prevents overfitting. The paper also needs more experimental results to confirm its effectiveness. I will increase my score a little bit, but would not vote for an accept this time. But I believe with further revision, the paper may be worth publishing in the future, if the questions in all reviews can be addressed. ", "rating": "3: Weak Reject", "reply_text": "Thank you for time and your comments . By reading your review , we believe there are some points in our paper that you might be misunderstanding . Below , we try to clarify and address these points . [ On the Lack of Explanation for Coaching \u2019 s Strong Performance ] We respectfully disagree with you that we have \u201c no clear explanation of why the proposed method works except a metaphor for sports coaches \u201d . Specifically : We have mentioned in Section 3.3 on Page 7 of our paper that * one advantage * of Coaching is that it avoids overfitting . In fact , in many of our experiments , Coaching models are trained for 1 million steps ( Tables 4 , 5 , 6 ; Appendix C ) . In the low-data regime , * no * previous method can train for that long without severely overfitting . For example , this screenshot ( https : //pasteboard.co/IFkOBbz.png ) shows such a case for ImageNet-10 % after just 50K steps . One can argue that a large dropout rate , a large weight-decay rate , as well as other regularization methods , will also avoid overfitting . However , they are all orthogonal to coaching , and * none * of those methods has ever achieved the strong performance as Coaching . Thus , at the very least , Coaching provides a form of very effective regularization . There could be other reasons for the strong performance of Coaching , and they are not just our Additional Implementation Details , as you incorrectly assumed . [ On the Performance of Coaching without Additional Techniques ] We also disagree with you that the strong performance of Coaching is associated to * just * the Additional Implementation Details . First , we have provided an evidence in Figure 3 on Page 8 , which shows that Coaching+UDA improves over UDA by 4.32 % top-1 accuracy on ImageNet , which is a very significant boost for this dataset . As UDA uses * all * techniques ( consistency loss , data augmentation , etc . ) , the comparison between Coaching+UDA and UDA is a * controlled * experiment that demonstrates the strength of Coaching . Second , our use of cosine distance and a moving average baseline are all the features of the Coaching method . This is not a weakness of Coaching , just like the moving average baseline is not a weakness of * all * Reinforcement Learning algorithms that use policy gradient . To our knowledge , we do not reject policy gradient methods just because they use a baseline to improve their results . Instead , we simply follow such recipe . Third , to further address your concern about our Additional Implementation Techniques , we have also run Coaching on ImageNet-10 % with neither Consistency Regularization nor Data Augmentation ( please see our common response to all reviewers at https : //openreview.net/forum ? id=rJe04p4YDB & noteId=S1lX9YcJsB ) . In this setting , Coaching achieves 62.02 % top-1 accuracy and outperforms RandomAugment \u2019 s 61.88 % ( see Figure 3 , Page 8 ) . [ Experiments with Different Numbers of Labeled Examples ] Per your concern , we have performed experiments with ImageNet using 20 % , 40 % , and 80 % of labeled data . All models are ResNet-50 . The top-1 accuracy for them are as follows : 20 % : 75.41 40 % : 76.21 80 % : 76.91 Thus , with 40 % of the labeled data , we are around the same ballpark with ResNet-50 trained in a fully-supervised manner using 100 % of data . Meanwhile , with 80 % of the labeled data in ImageNet , Coaching achieves the same performance with supervised training using all labeled data ( whose top-1 accuracy is 76.89 % ) . [ TO BE CONTINUED IN THE COMMENT BELOW ]"}, {"review_id": "rJe04p4YDB-1", "review_text": "This paper provides a simple but novel coaching method for teacher-student based semi-supervised learning framework. The coaching method consists of a two-stage update: first update Student network according to the pseudo label produced by Teacher network on the unlabeled dataset; second update Teacher network according to the Student network's performance on the labeled dataset. The authors propose a novel policy gradient update for the Teacher network. The authors evaluate the coaching method on several different semi-supervised learning dataset CIFAR-10, SVHN and ImageNet. The comparison with baselines are thorough. I appreciate that the authors explain the design of the experiments and tuning in details. I vote for acceptance but I still have some questions. I would be willing to increase my score if the authors address my questions in the rebuttal. 1. The motivation of coaching is not accurate. In the second sentence of Introduction, the authors mention \"Although coaches do not play as well as the players\". However, we are training neural networks. There is no such thing as \"coaches do not play as well as the players\". They are the same parametric neural networks. (The authors also use the same architectures for both teacher and student neural networks.) I would remove this analogy sentence. 2. The authors mention that their coaching method beats the fully supervised learning on the CIFAR-10. I am not convinced by this result. The author explained that this is due to the less overfitting in the student network. However, besides coaching, there are many other regularization method we can use to avoid overfitting. Using far less labels in training strictly reduces the amount of information we have. There is a simple test the authors can do. We can use the full labelled data set and use sampled results from Teacher network to train a Student network on it. We can perform coaching on this regime. This coaching on full dataset should do better than coaching on partially-labeled dataset (4000 labels). 3. The state of art for CIFAR-10 is 99% now [1]. It would be nice to see the performance of author's approach applies to the state-of-art network/structure. References: [1] Huang, Yanping, et al. \"Gpipe: Efficient training of giant neural networks using pipeline parallelism.\" arXiv preprint arXiv:1811.06965 (2018). ----- The authors' response does not answer the questions about motivation and why the method works, which is also questioned by the two other reviewers. For example, why the authors need to have two networks is unclear; I guess that true labels should be more helpful to guide a student network to learn than a teacher network. Even though the authors seem to have state-of-art semi-supervised learning results, some extra explanations are needed. ", "rating": "3: Weak Reject", "reply_text": "Thank you for time and your comments . [ On Coaching \u2019 s Motivations ] We agree with your point here . We will consider rephrasing the analogy with coaching in sports in order to avoid causing misunderstandings to our readers . You are correct that our student network shares the same architecture with our teacher network . However , this only entails that these networks have the same * learning capacity * . In fact , when put into our framework , the teacher \u2019 s focus is * not * to learn anything for itself , but instead , to improve the student \u2019 s performance . In our experiments , it is indeed the case that the student typically has a higher accuracy than the teacher . For example , on ImageNet-10 % , our teacher network ( trained with UDA [ 1 ] ) has a top-1 accuracy of 69.11 % , which is around the same performance that [ 1 ] reported . Meanwhile , in that same experiment , the student achieves a much stronger top-1 accuracy of 73.32 % . [ On Coaching with Full CIFAR-10 ] We are not sure what you meant by this request . In order to performance Coaching , we need have both labeled data and unlabeled data . The setting of using 4,000 labeled examples from CIFAR-10 is designed to limit the amount of * labeled data * that a training algorithm can use . If we allow the teacher to see all the labels , as seemingly suggested by your \u201c simple test \u201d , then we would obviously have a stronger student . However , this is not the point of the experiments , which is to demonstrate the Coaching works * in the low-data regime * . [ On CIFAR-10 \u2019 s State-of-the-Art ] To compare training algorithms , accuracies should be compared among the same model architectures , trained using the same amount of labeled data . To achieve 99 % accuracy on CIFAR-10 , the GPipe paper ( Huang et al , 2018 ) that you mentioned uses a large version of AmoebaNet-C which has 600M parameters and hence , is much larger than our WideResNet-28-2 ( 1.45M parameters ) . Furthermore , the 99 % accuracy is achieved by finetuning a model * pretrained on ImageNet * . In fact , with 4000 labeled examples , we have achieved the accuracy of 98.21 % by applying Coaching to EfficientNet-B0 [ 2 ] ( of course , without pretraining on ImageNet ) . This accuracy slightly outperforms the accuracy of 98.1 % that [ 2 ] achieved by pretraining on ImageNet and then finetuning on full CIFAR-10 . Note that EfficientNet-B0 only has 4M parameters , which is much smaller than 600M of AmoebaNet-C. [ 1 ] Unsupervised Data Augmentation for Consistency Training . Qizhe Xie , Zihang Dai , Eduard Hovy , Minh-Thang Luong , Quoc V. Le . https : //arxiv.org/abs/1904.12848 [ 2 ] EfficientNet : Rethinking Model Scaling for Convolutional Neural Networks . Mingxing Tan , Quoc V. Le . https : //arxiv.org/abs/1905.11946"}, {"review_id": "rJe04p4YDB-2", "review_text": "The paper proposes Coaching for semi-supervised learning. The teacher generates pseudo labels for unlabeled data and the performance of the student on labeled data is used as a reward to train the teacher. The empirical results are very impressive. Overall, the paper is clear and easy to follow. The idea looks interesting to me. However, I find that there are some weaknesses. First, the method is not well-motivated in the Introduction section. Second, the derivation of the teacher's update rule is incorrect. In Eq. (11), $g_S^{(t)}(\\hat y_{unl})$ is a vector, $\\frac{\\partial \\ell(x_{unl}, \\hat y_{unl}; \\theta_T)}{\\partial \\theta_T}$ is also a vector. What do you mean by multiplying two vectors? The left side of Eq. (11) is a matrix while the shape does not match on the right side. It is incorrect to get Eq. (3) from Eq. (11), especially the transposed $g_S^{(t)}$. Where does the transpose come from? And $g_T^{(t)}$ in Eq.(3) is inconsistent with line 6 in Algorithm 1, where $\\eta h^{(t)}$ is not included in $g_T^{(t)}$. For experiments, I wonder what is the performance of pure Coaching without RandomAugment and the consistency loss/UDA in Table 1. I think this is a fair comparison with the baselines like Mean Teacher and VAT. I suggest adding this to the ablation study as well. And I expect more explanations on the \"additional implementation details\". For example, why is it correct to use cosine distance instead of the dot product? In this case, is it a valid gradient? Same for other tricks. We need to understand why it works apart from adding a bunch of tricks together. And I doubt whether the improvement is due to these tricks or the method itself. I would be willing to increase the score if all the concerns are addressed in the authors' response. ", "rating": "3: Weak Reject", "reply_text": "Thank you for time and your comments . Below , we try to address your concerns about our paper . [ On the Teacher \u2019 s Update Rules ] We strongly appreciate the fact that you went through the paper , even to the Appendix to figure out the derivation of the Teacher \u2019 s update rule . The update rule is actually correct , but the presentation might be confusing . We apologize for causing you the confusions . We believe such confusions are due to the interchangeable usage of the gradient/Jacobian notations . To avoid these confusions , we have uploaded a revision of our paper , where we clearly stated which mathematical notations are used . We also annotated the dimensions of the quantities in the equations throughout the derivation . We hope the new version is easier to follow and verify . [ On the Use of Cosine Distance ] We explain why the use of cosine distance * makes sense * . Now that we have established that Equation 3 is correct , we see that the dot product between two gradients is simply a scalar , which is subsequently multiplied with the teacher \u2019 s policy gradient . The sign of this scalar governs whether the teacher should increase or decrease its confidence in the pseudo labels that it samples . Replacing dot product by cosine distance does not change this sign . The magnitude of this scalar governs how far the teacher should follow a particular direction . Replacing dot product by cosine distance ensures that the magnitude is < = 1 . This makes the updates more stable . If we wish not to use the cosine distance , we might as well use a learning rate that is a few orders of magnitude smaller . ( this is our rough estimation from the fact that many ResNet-50 models in our experiments have the gradient norm of between 300 and 400 ) . [ On the Performance of Coaching without RandomAugment and Consistency Loss/UDA ] Please see our common response to all reviewers ( https : //openreview.net/forum ? id=rJe04p4YDB & noteId=S1lX9YcJsB ) ."}], "0": {"review_id": "rJe04p4YDB-0", "review_text": "This paper studies the teacher-student models in semi-supervised learning. Unlike previous methods in which only the student will learn from the teacher, this paper proposes a method to let the teacher learn from the student by reinforcement learning. Experimental results demonstrate the proposal\u2019s performance. The paper achieves some good empirical results compared to other baselines. However, the proposed method is implemented with many tricks listed on Page 4, and with data augmentation techniques, which may not be used in previous methods. Additionally, the paper is weak in technology. There is no clear explanation of why the proposed method works except a metaphor for sports coaches. I vote for a clear rejection of the paper. First, the paper is weak in experiments. It works hard to achieve a good experimental result, through many tricks listed in the \u201cAdditional Implementation Details\u201d in Page 4, and through the data augmentation used in Page 5. However, these tricks to improve the performance may not be used in previous methods, as the paper does not run experiments on baselines under the same setting, but use the results reported in Oliver et al. (2018). Additionally, the paper only uses one number of labeled data for each data set, it makes readers doubt that the proposed method only works under this number of labeled data. The paper fails to clearly state why we need to let the teacher learn from the student. Actually, I doubt if this is necessary. Given the strong learning capacity of neural networks, the proposed method will easily be overfitting. Assume we have a very weak student network at the beginning, then by training in the way proposed in the paper, the teacher network will have all labeled data classified correct, and all unlabeled data classified into the same labels as the student network. I cannot see from the simple proposal why such overfitting can be avoided. The paper is weak in both technology and experiments. It is also poorly written without clearly stating the motivation for this problem. I would vote for a reject for the paper. ------------------------------------------- The rebuttal has cleared some of my concerns. However, it is still not clear why the proposed method work and how does it prevents overfitting. The paper also needs more experimental results to confirm its effectiveness. I will increase my score a little bit, but would not vote for an accept this time. But I believe with further revision, the paper may be worth publishing in the future, if the questions in all reviews can be addressed. ", "rating": "3: Weak Reject", "reply_text": "Thank you for time and your comments . By reading your review , we believe there are some points in our paper that you might be misunderstanding . Below , we try to clarify and address these points . [ On the Lack of Explanation for Coaching \u2019 s Strong Performance ] We respectfully disagree with you that we have \u201c no clear explanation of why the proposed method works except a metaphor for sports coaches \u201d . Specifically : We have mentioned in Section 3.3 on Page 7 of our paper that * one advantage * of Coaching is that it avoids overfitting . In fact , in many of our experiments , Coaching models are trained for 1 million steps ( Tables 4 , 5 , 6 ; Appendix C ) . In the low-data regime , * no * previous method can train for that long without severely overfitting . For example , this screenshot ( https : //pasteboard.co/IFkOBbz.png ) shows such a case for ImageNet-10 % after just 50K steps . One can argue that a large dropout rate , a large weight-decay rate , as well as other regularization methods , will also avoid overfitting . However , they are all orthogonal to coaching , and * none * of those methods has ever achieved the strong performance as Coaching . Thus , at the very least , Coaching provides a form of very effective regularization . There could be other reasons for the strong performance of Coaching , and they are not just our Additional Implementation Details , as you incorrectly assumed . [ On the Performance of Coaching without Additional Techniques ] We also disagree with you that the strong performance of Coaching is associated to * just * the Additional Implementation Details . First , we have provided an evidence in Figure 3 on Page 8 , which shows that Coaching+UDA improves over UDA by 4.32 % top-1 accuracy on ImageNet , which is a very significant boost for this dataset . As UDA uses * all * techniques ( consistency loss , data augmentation , etc . ) , the comparison between Coaching+UDA and UDA is a * controlled * experiment that demonstrates the strength of Coaching . Second , our use of cosine distance and a moving average baseline are all the features of the Coaching method . This is not a weakness of Coaching , just like the moving average baseline is not a weakness of * all * Reinforcement Learning algorithms that use policy gradient . To our knowledge , we do not reject policy gradient methods just because they use a baseline to improve their results . Instead , we simply follow such recipe . Third , to further address your concern about our Additional Implementation Techniques , we have also run Coaching on ImageNet-10 % with neither Consistency Regularization nor Data Augmentation ( please see our common response to all reviewers at https : //openreview.net/forum ? id=rJe04p4YDB & noteId=S1lX9YcJsB ) . In this setting , Coaching achieves 62.02 % top-1 accuracy and outperforms RandomAugment \u2019 s 61.88 % ( see Figure 3 , Page 8 ) . [ Experiments with Different Numbers of Labeled Examples ] Per your concern , we have performed experiments with ImageNet using 20 % , 40 % , and 80 % of labeled data . All models are ResNet-50 . The top-1 accuracy for them are as follows : 20 % : 75.41 40 % : 76.21 80 % : 76.91 Thus , with 40 % of the labeled data , we are around the same ballpark with ResNet-50 trained in a fully-supervised manner using 100 % of data . Meanwhile , with 80 % of the labeled data in ImageNet , Coaching achieves the same performance with supervised training using all labeled data ( whose top-1 accuracy is 76.89 % ) . [ TO BE CONTINUED IN THE COMMENT BELOW ]"}, "1": {"review_id": "rJe04p4YDB-1", "review_text": "This paper provides a simple but novel coaching method for teacher-student based semi-supervised learning framework. The coaching method consists of a two-stage update: first update Student network according to the pseudo label produced by Teacher network on the unlabeled dataset; second update Teacher network according to the Student network's performance on the labeled dataset. The authors propose a novel policy gradient update for the Teacher network. The authors evaluate the coaching method on several different semi-supervised learning dataset CIFAR-10, SVHN and ImageNet. The comparison with baselines are thorough. I appreciate that the authors explain the design of the experiments and tuning in details. I vote for acceptance but I still have some questions. I would be willing to increase my score if the authors address my questions in the rebuttal. 1. The motivation of coaching is not accurate. In the second sentence of Introduction, the authors mention \"Although coaches do not play as well as the players\". However, we are training neural networks. There is no such thing as \"coaches do not play as well as the players\". They are the same parametric neural networks. (The authors also use the same architectures for both teacher and student neural networks.) I would remove this analogy sentence. 2. The authors mention that their coaching method beats the fully supervised learning on the CIFAR-10. I am not convinced by this result. The author explained that this is due to the less overfitting in the student network. However, besides coaching, there are many other regularization method we can use to avoid overfitting. Using far less labels in training strictly reduces the amount of information we have. There is a simple test the authors can do. We can use the full labelled data set and use sampled results from Teacher network to train a Student network on it. We can perform coaching on this regime. This coaching on full dataset should do better than coaching on partially-labeled dataset (4000 labels). 3. The state of art for CIFAR-10 is 99% now [1]. It would be nice to see the performance of author's approach applies to the state-of-art network/structure. References: [1] Huang, Yanping, et al. \"Gpipe: Efficient training of giant neural networks using pipeline parallelism.\" arXiv preprint arXiv:1811.06965 (2018). ----- The authors' response does not answer the questions about motivation and why the method works, which is also questioned by the two other reviewers. For example, why the authors need to have two networks is unclear; I guess that true labels should be more helpful to guide a student network to learn than a teacher network. Even though the authors seem to have state-of-art semi-supervised learning results, some extra explanations are needed. ", "rating": "3: Weak Reject", "reply_text": "Thank you for time and your comments . [ On Coaching \u2019 s Motivations ] We agree with your point here . We will consider rephrasing the analogy with coaching in sports in order to avoid causing misunderstandings to our readers . You are correct that our student network shares the same architecture with our teacher network . However , this only entails that these networks have the same * learning capacity * . In fact , when put into our framework , the teacher \u2019 s focus is * not * to learn anything for itself , but instead , to improve the student \u2019 s performance . In our experiments , it is indeed the case that the student typically has a higher accuracy than the teacher . For example , on ImageNet-10 % , our teacher network ( trained with UDA [ 1 ] ) has a top-1 accuracy of 69.11 % , which is around the same performance that [ 1 ] reported . Meanwhile , in that same experiment , the student achieves a much stronger top-1 accuracy of 73.32 % . [ On Coaching with Full CIFAR-10 ] We are not sure what you meant by this request . In order to performance Coaching , we need have both labeled data and unlabeled data . The setting of using 4,000 labeled examples from CIFAR-10 is designed to limit the amount of * labeled data * that a training algorithm can use . If we allow the teacher to see all the labels , as seemingly suggested by your \u201c simple test \u201d , then we would obviously have a stronger student . However , this is not the point of the experiments , which is to demonstrate the Coaching works * in the low-data regime * . [ On CIFAR-10 \u2019 s State-of-the-Art ] To compare training algorithms , accuracies should be compared among the same model architectures , trained using the same amount of labeled data . To achieve 99 % accuracy on CIFAR-10 , the GPipe paper ( Huang et al , 2018 ) that you mentioned uses a large version of AmoebaNet-C which has 600M parameters and hence , is much larger than our WideResNet-28-2 ( 1.45M parameters ) . Furthermore , the 99 % accuracy is achieved by finetuning a model * pretrained on ImageNet * . In fact , with 4000 labeled examples , we have achieved the accuracy of 98.21 % by applying Coaching to EfficientNet-B0 [ 2 ] ( of course , without pretraining on ImageNet ) . This accuracy slightly outperforms the accuracy of 98.1 % that [ 2 ] achieved by pretraining on ImageNet and then finetuning on full CIFAR-10 . Note that EfficientNet-B0 only has 4M parameters , which is much smaller than 600M of AmoebaNet-C. [ 1 ] Unsupervised Data Augmentation for Consistency Training . Qizhe Xie , Zihang Dai , Eduard Hovy , Minh-Thang Luong , Quoc V. Le . https : //arxiv.org/abs/1904.12848 [ 2 ] EfficientNet : Rethinking Model Scaling for Convolutional Neural Networks . Mingxing Tan , Quoc V. Le . https : //arxiv.org/abs/1905.11946"}, "2": {"review_id": "rJe04p4YDB-2", "review_text": "The paper proposes Coaching for semi-supervised learning. The teacher generates pseudo labels for unlabeled data and the performance of the student on labeled data is used as a reward to train the teacher. The empirical results are very impressive. Overall, the paper is clear and easy to follow. The idea looks interesting to me. However, I find that there are some weaknesses. First, the method is not well-motivated in the Introduction section. Second, the derivation of the teacher's update rule is incorrect. In Eq. (11), $g_S^{(t)}(\\hat y_{unl})$ is a vector, $\\frac{\\partial \\ell(x_{unl}, \\hat y_{unl}; \\theta_T)}{\\partial \\theta_T}$ is also a vector. What do you mean by multiplying two vectors? The left side of Eq. (11) is a matrix while the shape does not match on the right side. It is incorrect to get Eq. (3) from Eq. (11), especially the transposed $g_S^{(t)}$. Where does the transpose come from? And $g_T^{(t)}$ in Eq.(3) is inconsistent with line 6 in Algorithm 1, where $\\eta h^{(t)}$ is not included in $g_T^{(t)}$. For experiments, I wonder what is the performance of pure Coaching without RandomAugment and the consistency loss/UDA in Table 1. I think this is a fair comparison with the baselines like Mean Teacher and VAT. I suggest adding this to the ablation study as well. And I expect more explanations on the \"additional implementation details\". For example, why is it correct to use cosine distance instead of the dot product? In this case, is it a valid gradient? Same for other tricks. We need to understand why it works apart from adding a bunch of tricks together. And I doubt whether the improvement is due to these tricks or the method itself. I would be willing to increase the score if all the concerns are addressed in the authors' response. ", "rating": "3: Weak Reject", "reply_text": "Thank you for time and your comments . Below , we try to address your concerns about our paper . [ On the Teacher \u2019 s Update Rules ] We strongly appreciate the fact that you went through the paper , even to the Appendix to figure out the derivation of the Teacher \u2019 s update rule . The update rule is actually correct , but the presentation might be confusing . We apologize for causing you the confusions . We believe such confusions are due to the interchangeable usage of the gradient/Jacobian notations . To avoid these confusions , we have uploaded a revision of our paper , where we clearly stated which mathematical notations are used . We also annotated the dimensions of the quantities in the equations throughout the derivation . We hope the new version is easier to follow and verify . [ On the Use of Cosine Distance ] We explain why the use of cosine distance * makes sense * . Now that we have established that Equation 3 is correct , we see that the dot product between two gradients is simply a scalar , which is subsequently multiplied with the teacher \u2019 s policy gradient . The sign of this scalar governs whether the teacher should increase or decrease its confidence in the pseudo labels that it samples . Replacing dot product by cosine distance does not change this sign . The magnitude of this scalar governs how far the teacher should follow a particular direction . Replacing dot product by cosine distance ensures that the magnitude is < = 1 . This makes the updates more stable . If we wish not to use the cosine distance , we might as well use a learning rate that is a few orders of magnitude smaller . ( this is our rough estimation from the fact that many ResNet-50 models in our experiments have the gradient norm of between 300 and 400 ) . [ On the Performance of Coaching without RandomAugment and Consistency Loss/UDA ] Please see our common response to all reviewers ( https : //openreview.net/forum ? id=rJe04p4YDB & noteId=S1lX9YcJsB ) ."}}