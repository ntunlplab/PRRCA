{"year": "2021", "forum": "RkqYJw5TMD7", "title": "Test-Time Adaptation and Adversarial Robustness", "decision": "Reject", "meta_review": "This paper studies test-time adaptation in the context of adversarial robustness. The key idea is to use a maximin framework, which illustrates non-trivial robustness (under transfer attack) using domain adversarial neural network (DANN) to Linf-norm and unseen adversarial attacks. The approach is sound, well grounded, and quite logical. Results demonstrate the effectiveness.\n\nHowever, there exists some limitations, for example, 1) The adaptive attack results are concerning. Comparing Table 1 and Table 3, with the adaptive attack (J-FPAM), the accuracy in the homogeneous setting is below that of adversarial training (Adv S) in Table 1, which somehow echoes my concern on not testing the transductive setting using strong attacks. It seems that adversarially trained models can better defend the adaptive attacks. 2) The paper says \"This threat model enables us to study whether a large test set can benefit a defender for adversarial robustness\", yet there is no any experiments in the main paper that correspond to this discussion. The appendix seems lacking this discussion either. 3) Due to the page limit, a lot of details have been moved into the appendix, making the paper difficult to read.\n\nIn the end, I think that this paper may not be ready for publication at ICLR, but the next version must be a strong paper if above limitations can be well addressed.", "reviews": [{"review_id": "RkqYJw5TMD7-0", "review_text": "The paper explores adversarial robustness in a new setting of test-time adaptation . It shows this new problem of \u201c test-time-adapted adversarial robustness \u201d is strictly weaker than the \u201c traditional adversarial robustness \u201d when assuming the training data is available for the \u201c test-time-adapted adversarial robustness \u201d . The gap between the two problems is demonstrated by the simple DANN solution which has good \u201c test-time-adapted adversarial robustness \u201d but bad \u201c traditional adversarial robustness \u201d . The paper also explores the subcase of \u201c test-time-adapted adversarial robustness \u201d when assuming the training data is not available and provide some initial result . The paper has clear strong points . It aims to tackle an important problem , the \u201c test time adaptation allowed \u201d extension for the \u201c adversarial robustness \u201d . The paper has a nice global picture and a clear position for this piece of work . I particularly like the way the author approaches the problem . They start from the most abstract and fundamental question , \u201c test-time-adapted adversarial robustness \u201d v.s . \u201c Classic adversarial robustness \u201d , or transductive learning v.s . Inductive learning in the setting of adversarial robustness . To proceed the thinking , they develop a good theoretical framework ( the two threat models from definition 1 and definition 2 ) to formulate the two problems . And then consider a middle setting ( definition 3 ) between the classic minimax and new maximin threat model . Such frameworks help to develop theoretical understanding like one setting the strictly weaker than another ( proposition 1 ) . The weak points of the paper are mainly about the presentation . The paper currently is very dense . Sometimes I feel the author may assume the reader has certain domain knowledge without explanations . For example , dataset \u201c CIFAR10c-fog \u201d appears very early in the introduction , but it is never clearly explained what it is , what is the main difference between it and CIFAR 10 . After reading the paper the only impression I get is they not homogeneous . There also some places in the method , I can not understand , For the maximin threat model , why the game is maximation over U instead of A ( the left side of the equation from proposition 1 ) . Why there are A0 and A1 in the Adversarial semi-supervised minimax threat model ? And why in this game , A0 and A1 are jointly maximizing L ( \\tile { F } , \\tilde { V } \u2019 ) ? More question about experiments : For FPA attacks , is there any baseline method we can compare the DANN with ? Currently , I am not sure how to evaluate the performance of DANN . In experiment ( D ) , it says , \u201c we also evaluate the accuracy of the adapted DANN models in the minimax threat mode \u201d . But where are the results ? I understand it is pretty hard to squeeze so many contents into limited 8 pages . Personally , I think it is helpful to cut off some content and make the main paper more clear , well organized , and strong . Unfortunately , I am not an expert in adversarial robustness . I did not check the technique and experiments deeply . My current score assumes no fatal flaws exist in the theory and experiment . My rating will be changed according to other reviewers \u2019 comments and the author \u2019 s updates .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the positive feedback , and recognizing our conceptual novelty . Below we want to address some questions that we think are important to clarify first : 1 . CIFAR10-c data set ( https : //github.com/hendrycks/robustness ) . The CIFAR10-c dataset is an important recent benchmark for evaluating the robustness of learning in the setting of adaptations , under common corruptions and perturbations . Since its introduction , this dataset has been very commonly used in domain adaptation setting , and in particular , it is used in TTT ( Test Time Training , a oblivious test-time adaptation algorithm ) to demonstrate that test-time adaptation can achieve robustness . 2.In our work , we naturally generalize the consideration , and consider whether test-time adaptation can achieve , not only robustness on common corruptions and perturbations , but also on * * adversarial perturbations * * . We have indeed assumed that readers are familiar with these points . We will clarify the writing by adding relevant details . 3.Why the adversarial game is over $ U $ ? This is because that the adversary can only perturb features , but not labels . Therefore , to make this point precise , we write the maximum to be over $ U $ ( we thank the reviewers to notice these subtleties ) . 4.Why there are two different attack algorithms in the semi-supervised adversarial model ? This is explained in Section B.1 , paragraph * * '' Why do we need to have two algorithms A0 , A1 in Definition 3 ? `` * * Very roughly speaking , the first attack is about attacking the semi-supervised learning algorithm , and the second attack is about attacking the final model we learned . 5.DANN results in the minimax model . We have a sentence * '' Not surprisingly , the accuracy becomes very low '' * ( basically , the accuracy drops to random guessing ~10 % ) . We choose to present in this way because this experiment is simple to perform , and the results are very much as expected . 6.We have clarified the point regarding the baselines for FPA in the thread `` Revision # 1 : Clarifying adaptive fixed point attacks '' ."}, {"review_id": "RkqYJw5TMD7-1", "review_text": "# # # Contributions # # # * The authors formalize test-time adaptation as a maximin threat model and contrast it with the `` threat model for classic adversarial robustness '' and the `` adversarial semi-supervised minimax threat model '' . * The paper studies test-time adaptation via DANN and its robustness in the maximin and minimax threat model . DANN has larger robustness against the proposed attacks in the maximin threat model than in minimax , providing evidence that maximin is a setting that is beneficial for the defender . However , it remains unclear if this would also be the case against stronger attackers . # # # Significance # # # Increasing robustness against adversarial robustness is a topic both relevant to basic research and also more applied research . Test-time adaptation is currently one of the most promising directions and this paper is thus very timely . Its formalization of a threat model for this domain can guide future research in this direction . That being said , the main method investigated ( DANN for test-time adaptation ) seems less practical and significant since it requires large test batch sizes and also huge computational resources at inference time . # # # Originality # # # The paper has only limited novelty : while the formalization of test-time adaptation as maximin threat model is very thorough , it is also mostly straightforward . Also studying DANN as a test-time adaptation is an incremental idea once one assumes that inference is conducted on large batches . Nevertheless , I think papers like this that consolidate recent research directions into a coherent framework are valuable . # # # Clarity # # # Generally , the introduction and the part outlining the threat model are written very clearly and I enjoyed the thorough formalization . A few details could be improved ( even though this is a bit nitpicking ) : * the introduction is very technical ; it could be more high level and shorter and leaving the more technical parts to Section 2 . * Threat models could mention that white-box knowledge of the attacker is assumed * it is not really clear why test inputs are assumed to be labeled in the threat models The experimental part is less clear unfortunately : * since DANN is a central method in the experiments , it should be briefly summarized to make the paper self-contained * it is not really clear to me how specifically DANN is trained here ( what is the objective ? ) . Specifically , is the loss on the labeled data based on standard training or adversarial training ? * generally , all training details ( optimizer , learning rate , batch size , number of epochs etc . ) are missing . the experiments can not be reproduced in the current form . # # # Quality # # # I have some serious doubts about the quality of the empirical evaluation . As the authors state `` While we are not able to prove the existence of a defender solution that separates maximin and minimax threat models , we design and implement experiments to provide evidence that such a strategy may actually exist . '' Thus , one of the main hypothesis of the paper ( maximin being a strictly weaker threat model than minimax ) depends on the design of experiments . My main concern here is that the attackers used are not actually strong : * A transfer attack should clearly only be a baseline that should get surpassed by stronger adaptive attackers * However , the fixed point attack seems to be weaker than the transfer attack . In particular its effectiveness in the homogeneous setting for CIFAR-10 reduces with the number of iterations k. This does not seem to be a reliable way of evaluating DANN 's maximin robustness * I would imagine a strong adaptive attack could be built by generating adversarial inputs with the objective of minimizing the loss of DANN 's domain label classifier . This would minimize the impact of test-time adaptation via DANN ( because both domains would have already very similar representations ) . In any case , more effort should be spend on designing a strong attack against a DANN-based test-time adaptation . I think it is also confusing to add evaluations in the inhomogeneous setting . Clearly , DANN gives strong results here because dealing with domain shift is what it was designed for . However , the paper is on adversarial robustness and confounding this with domain shift is misleading ( my suspicion is it was mainly added because DANN clearly outperforms adversarial training on the source data in this setting ; to proof me wrong , the authors should add a better motivation for this setting or remove it ) . # # # Recommendation # # # To summarize , I think the paper provides a strong formalization of a threat model for test-time adaptation . However , it does not introduce novel approaches , lacks clarity in experimental details , and its empirical results are less trustworthy given that the proposed attacks are rather weak . I thus recommend rejecting the paper in its current form . I encourage the authors to address the current shortcomings ; the paper clearly has potential in general . # # # Final Recommendation after Author Response # # # I would like the authors for the very active and productive rebuttal period . Actually , the current version has significantly deviated from the initially submitted version . I have read most of the paper a second time to take all changes into account . The authors considerably improved the paper and have addressed some of my concerns . I increase my score to 5 . The reason for not increasing the score to 6 or 7 is mainly that clarity of the paper is lacking . To give two examples : * Presumably because of the many changes during the rebuttal , the organization , structure and the quality of the manuscript is not always sufficient : to name one of several examples , the core Theorem 1 comes without any reference to its proof or any proof sketch in the main paper ( there exists a proof in the appendix , finding of which required scrolling through the entire appendix ) . * As stated in my initial review , since DANN is a central method in the experiments , it should be briefly summarized to make the paper self-contained and also the specific training I think the core issue with the submission is that there is simply too much content for one paper : * formalizing 3 threat models * a proof of separation of maximin and minimax * empirical evaluation on two datasets ( MNIST , CIFAR10 ) , three attacks ( transfer , two adaptive attacks ) , three defenses ( DANN , AdvS , TTT ) , and two settings ( homogenous , inhomogeneous ) Because of the page limit , a lot of details have been moved into the appendix , making the paper difficult to read . Moreover , even taking the appendix into account , details remain unclear such as how DANN was trained . Moreover , I do not find it convincing to move related work to the appendix ; relating the current work to other work should be an integral part of the main paper . My recommendation for the authors would be to strengthen the focus of the paper : I think the inhomogeneous setting does not contribute much and could be removed . Also I do n't see much value in MNIST and the weak transfer attacks . Moreover , the parts on preliminaries and threat models is too long for a conference paper and could be shortened . I think if focus were improved and the main document became more self-contained , the quality of the work would be considerably improved . For the time being , I see the submission still marginally below the acceptance threshold .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We sincerely appreciate the reviewer for correctly recognizing the novelty for formulating a threat model for test-time adaptation . We want to , however , rebut first on some criticisms that are critical to understanding our contributions : 1 . First , to set up the context correctly , we want to point out that we are using the original DANN objective ( designed for unsupervised domain adaptation ) as defined in : https : //arxiv.org/abs/1505.07818 ( see equation ( 9 ) of the paper ) . For an intuitive understanding , one can also refer to this link https : //ameroyer.github.io/reading-notes/domain % 20adaptation/2019/05/23/domain_adversarial_training_of_neural_networks.html 2 . We want to highlight that this objective is * * attack agnostic * * , namely in the objective it never explicitly leverages the information of how the adversarially perturbed data are generated ( i.e. , it does not leverage the attack type in the objective ) . Specifically , this answers your question of * '' it is not really clear to me how specifically DANN is trained here ( what is the objective ? ) . Specifically , is the loss on the labeled data based on standard training or adversarial training ? `` * . No , the training on the source data is not adv training at all : If it was the case , this paper would lose all its meaning . And the fact that DANN can provide adversarial robustness is exactly why transductive adversarial learning can be surprising and interesting . 3.We believe that the use of DANN is novel . Before we introduced our threat model , it is not clear at all why one should even consider DANN ( an algorithm for unsupervised domain adaptation ) as an algorithm for * * adversarial * * robustness . Our research builds a bridge to connect two seemingly entirely different research directions , which to us is a clear novelty . 4.Finally , we also want to rebut in this thread about * * '' threat model being straightforward '' * * . We understand where this comes from , but this is an issue of the nature of `` searching for a solution vs. verifying a solution '' . As a conceptual paper , being able to articulate a threat model that is natural and clean is an advantage , rather than a disadvantage . To this end , we want to emphasize that the recent beautiful theory work by Goldwasser et al . ( spotlight in neurips 2020 , https : //arxiv.org/abs/2007.05145 ; Beyond Perturbations : Learning Guarantees with Arbitrary Adversarial Test Examples ) have * * studied a threat model similar in spirit to ours ( see their Definition 1.1 and Section 4.2 ) * * . However , while their threat model is more amenable to theoretical analysis , we view ours as much more practical and relevant to deep learning ( specifically , their formulation is designed to be analyzable for bounded VC dimension concept class , which may not be practical in the deep learning setting ) . Our choices of formalization have made it * * possible * * to connect to deep learning , and demonstrate that unsupervised domain adaptation can benefit adversarial robustness , which to us is very nontrivial . For your other constructive comments , we will post a separate thread to address those concerns ."}, {"review_id": "RkqYJw5TMD7-2", "review_text": "This paper explores a new paradigm referred to as `` adversarial robustness of test-time adaptation '' . The authors propose a new threat model called a `` maximin game '' instead of the widely-accepted `` minimax game '' . In this new setup , the threat model becomes weaker as it first presents the unlabelled adversarial examples to the defender to get prepared . The authors also run some experiments showing that DANN seems to be a preferred method in this new setup . The paper is in general interesting with many novel definitions and discussions , but I do not see how this paper can benefit the community other than a set of new research directions with preliminary results . 1.First of all , I consider the writings of the introduction not professional : the introduction is full of notations , definitions , and even datasets and results that are only elucidated later in the paper . This writing style posted a great challenge to understand and appreciate this paper . 2.The paper interestingly introduces a set of new ideas/definitions . However , I wonder how these new research directions can benefit the community or industry . The authors explained that `` this question is also of practical importance since these 'new ' solutions may possess desirable properties that good solutions in the minimax threat model may lack . '' and then the authors mentioned that `` one such property is that the defender solution is attack agnostic '' . However , it seems the rest of this paper does not correspond to these remarks , e.g. , * The experiments ( in the main paper body ) are within $ \\ell_\\infty $ norm , so it seems there is no evidence about attack agnostic . * I did not find related discussions of other `` desirable properties '' later in this paper . * The authors also mentioned theoretical interest , yet I do n't think , with the current contents , this paper can be appreciated as a theoretical paper . Therefore , I suggest the authors further clarify their contributions , with emphasis on how their contributions can benefit the community and the industry . The current presentation leaves the impression that the paper is a set of new definitions without real-world implications . 3.The paper also says `` This threat model enables us to study whether a large test set can benefit a defender for adversarial robustness '' , yet I do not see any experiments ( in the main paper ) that correspond to this discussion . The appendix seems lacking this discussion either . Similarly , this kind of argument or claims without validations makes the paper read like an arbitrary set of definitions . 4.Following the `` attack agnostic '' argument above , Definition 2 suggests that the solution is also attack specific , as the defender needs to update the model according to the attacked samples . Although this update happens at test time , it happens before the evaluation . 5.The experiments are only about DANN , and seemingly leaving the details of AdvS unexplained . Is the AdvS the same architecture but without the domain component ? Then why not also report performances on S ? Without these details , it 's impossible to evaluate and appreciate the experimental results . 6.While it seems the authors ' focus is data-oblivious test-time adaptation , which , as I understand , does not allow the usage of labeling training data . Yet a major focus of the paper is DANN , which allows such usage . I 'm aware that the authors have mentioned the differences in multiple places , but if the main focus is DANN , I suggest the authors rephrase the paper to be about the setup DANN builds upon , instead of targeting a broader domain but only supporting the claims with experiments limited to a specific setup .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the review . We found that the comments have some significant misread , which we want to point out right away : 1 . Quote : * '' and then the authors mentioned that `` one such property is that the defender solution is attack agnostic '' . However , it seems the rest of this paper does not correspond to these remarks , e.g. , The experiments ( in the main paper body ) are within norm , so it seems there is no evidence about attack agnostic . `` * This is a misread . We have shown that DANN works as a test-time defense mechanism , and DANN is agnostic to the $ \\ell_\\infty $ attacks : This is because that we are using the original DANN algorithm , which is not specifically designed for the $ \\ell_\\infty $ attacks . In the introduction we have specifically mentioned : * * '' This is somewhat surprising as DANN is also agnostic to $ \\ell_\\infty $ attacks '' * * . On pp.6 in experiments we have also specifically mentioned , quote : * * '' ( B ) DANN is however not designed for adversarial robustness . Thus is it a very interesting question whether DANN can provide test-time adversarial robustness against attacks ( e.g.norm-based attacks ) that it is not specifically designed for '' * * Please refer to point 3 below for more discussion about `` attack agnostic '' . 2.Quote : * '' whether a large test set can benefit a defender for adversarial robustness '' , yet I do not see any experiments ( in the main paper ) that correspond to this discussion '' * Please refer to Figure 1 in the experiments : We have explicitly experimented with decreasing test size , and the robustness decreases . 3.Quote : * '' Following the `` attack agnostic '' argument above , Definition 2 suggests that the solution is also attack specific , as the defender needs to update the model according to the attacked samples . `` * We are afraid that there is a misunderstanding of what do we mean by `` attack agnostic '' . It means that the defense mechanism ( the algorithm ) does not leverage explicitly the attack type . For example , while DANN algorithm leverages $ U $ , the * * DANN algorithm itself never explicitly uses the fact that the adversarial data is from $ \\ell_\\infty $ attacks * * . Also , in Section D.3 we have also shown that DANN algorithm can provide robustness for $ \\ell_2 $ attacks as well , which further corroborates the `` attack agnostic point '' . We feel that this is critical to the understanding , so we want to point out as early as possible . 4.Quote : * '' While it seems the authors ' focus is data-oblivious test-time adaptation , which , as I understand , ... '' * We understand where this came from , but an important component of this paper is that we show that the current data-oblivious test-time adaptations ( e.g.Test-Time Training ) fails to have any robustness . Because the experiments for that purpose are so easy to perform , we condense them into Section 5 ( but they are important ) ."}, {"review_id": "RkqYJw5TMD7-3", "review_text": "This paper studies test-time adversarial robustness through a maximin framework and illustrate non-trivial robustness ( under transfer attack ) using domain adversarial neural network ( DANN ) to Linf-norm and unseen adversarial attacks . While I agree that test-time adaptation is an important and practical approach for adversarial robustness , the current version , in my opinion , does not deliver significantly novel insights , nor considering a reasonably practical threat model . My main concerns are detailed as follows . 1.Impractical threat model : At test time , the maximin threat model ( Def.2 ) assumes the attacker to make move prior to the defender 's action , which limits the attacker 's ability and weakens the robustness evaluation . More importantly , it may create a false of security/robustness , as pointed out by ( Athalye et al.2018 ) , that the robustness gain may actually come from information obfuscation and thus the results may fail to provide meaningful robustness evaluation . Although the authors mention the maximin threat model is a weaker ( attack ) model and part of the goal is to find an adaptation method that is `` good '' in this attack-move-first scenario , I could n't see the practical utility and contributions form these results . Even in the test-time adaptation setting , the `` defender-move-first '' setting should be more practical . Better motivation and use cases are needed to justify why the considered setting is important . 2.Due to the assumption of the considered maximin threat model , the experiments are limited to comparing accuracy on transfer attacks , which provide limited understanding of the true robustness of the victim model . Moreover , the baseline models in comparison are too weak and unfair . To have a fair comparison , the authors are suggested to compare robustness on robust models such as TRADES [ R1 ] and adversarially trained models with unlabeled data [ R2 , R3 ] , so that the baseline models also use unlabeled data . If DANN shows limited robustness against white-box attacks but stronger robustness against transfer attacks , one can only conclude that transfer attack is a weaker threat model , which is a known result . I do not see new insights from the reported results . [ R1 ] https : //arxiv.org/abs/1901.08573 [ R2 ] https : //papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness.pdf [ R3 ] https : //deepmind.com/research/publications/Are-Labels-Required-for-Improving-Adversarial-Robustness 3 . In addition to the issue of impractical threat model and lacking motivation , this paper contains too many high-level discussions accompanied with limited or even no empirical evidence . The theorem presented in the paper is a natural use of maximin inequality . In my opinion , the current version requires significant revision . I suggest the authors carefully motivate the research goals ( especially answering why the studied problems and settings are important ) , consolidate the claims on test-time robustness with convincing evidence , and make a broader connection to other test-time defenses other than DANN .", "rating": "3: Clear rejection", "reply_text": "Thanks for the review . The reviewer challenges our motivation for studying transductive adversarial robustness , which we want to rebut right away because this is critical to the understanding . 1.The threat model we studied in the paper is a particular instantiation of `` transductive adversarial robustness '' , which has recently received attention in theory . For example , a novel recent neurips spotlight paper by Goldwasser , Kalai , Kalai and Montasser ( https : //arxiv.org/abs/2007.05145 ) has studied very similar setting to ours . In that paper , it has explicitly proved for bounded-VC dimension concept class , transductive learning offers important robustness guarantees . Roughly speaking , their formulation is more amenable to theoretical study , while ours are more practically relevant for deep learning . 2.Regarding the `` gradient obfuscation '' comment , we have explicitly mentioned the differences with `` online defenses '' and `` gradient obfuscation '' . See pp.3 , paragraph `` Test-time defenses and BPDA . '' Basically , the previous `` online defenses '' apply to a setting where we want to `` sanitize '' $ x $ to $ x ' $ and then send to a pretrained model $ F $ . We investigate a much broader setting where we can modify model ( sometimes completely ) to predict $ x $ . 3.The reviewer mentioned that `` the threat model is impractical '' . We disagree : Many practical machine learning applications run in a batch mode , where one has received a batch $ U $ of test examples ( produced by the adversary potentially ) , and we want to be correct on those $ U $ . In those cases , transductive adversarial robustness is actually very appealing ( we want to refer the reviewer to Reviewer 3 and 4 , who have explicitly commented on this novelty ) . We have explicitly mentioned this in the introduction , quote : * '' First , this question is of practical interest : Many practical ML pipelines run in a batch mode , where they first collect a set of unlabelled data points , and then send them to a model ( e.g.Nado et al . ( 2020 ) ) .In such cases , data in the batch may have been adversarially perturbed , and it is a natural question whether we can leverage the large batch size and test-time adaptation to enhance adversarial robustness '' * 4 . We are very confused about one comment , quote * '' Even in the test-time adaptation setting , the `` defender-move-first '' setting should be more practical . `` * .What does that even mean ? The practical scenario we considered is that in many actual ML pipelines , one received an unlabeled data set $ U $ to classify , which could be corrupted ( but must happen before we receive it ) , and we want to use transductive learning to leverage $ U $ to get correct prediction . In such cases , defender moves * * after * * the attacker moves when test-time adaptation happens ."}], "0": {"review_id": "RkqYJw5TMD7-0", "review_text": "The paper explores adversarial robustness in a new setting of test-time adaptation . It shows this new problem of \u201c test-time-adapted adversarial robustness \u201d is strictly weaker than the \u201c traditional adversarial robustness \u201d when assuming the training data is available for the \u201c test-time-adapted adversarial robustness \u201d . The gap between the two problems is demonstrated by the simple DANN solution which has good \u201c test-time-adapted adversarial robustness \u201d but bad \u201c traditional adversarial robustness \u201d . The paper also explores the subcase of \u201c test-time-adapted adversarial robustness \u201d when assuming the training data is not available and provide some initial result . The paper has clear strong points . It aims to tackle an important problem , the \u201c test time adaptation allowed \u201d extension for the \u201c adversarial robustness \u201d . The paper has a nice global picture and a clear position for this piece of work . I particularly like the way the author approaches the problem . They start from the most abstract and fundamental question , \u201c test-time-adapted adversarial robustness \u201d v.s . \u201c Classic adversarial robustness \u201d , or transductive learning v.s . Inductive learning in the setting of adversarial robustness . To proceed the thinking , they develop a good theoretical framework ( the two threat models from definition 1 and definition 2 ) to formulate the two problems . And then consider a middle setting ( definition 3 ) between the classic minimax and new maximin threat model . Such frameworks help to develop theoretical understanding like one setting the strictly weaker than another ( proposition 1 ) . The weak points of the paper are mainly about the presentation . The paper currently is very dense . Sometimes I feel the author may assume the reader has certain domain knowledge without explanations . For example , dataset \u201c CIFAR10c-fog \u201d appears very early in the introduction , but it is never clearly explained what it is , what is the main difference between it and CIFAR 10 . After reading the paper the only impression I get is they not homogeneous . There also some places in the method , I can not understand , For the maximin threat model , why the game is maximation over U instead of A ( the left side of the equation from proposition 1 ) . Why there are A0 and A1 in the Adversarial semi-supervised minimax threat model ? And why in this game , A0 and A1 are jointly maximizing L ( \\tile { F } , \\tilde { V } \u2019 ) ? More question about experiments : For FPA attacks , is there any baseline method we can compare the DANN with ? Currently , I am not sure how to evaluate the performance of DANN . In experiment ( D ) , it says , \u201c we also evaluate the accuracy of the adapted DANN models in the minimax threat mode \u201d . But where are the results ? I understand it is pretty hard to squeeze so many contents into limited 8 pages . Personally , I think it is helpful to cut off some content and make the main paper more clear , well organized , and strong . Unfortunately , I am not an expert in adversarial robustness . I did not check the technique and experiments deeply . My current score assumes no fatal flaws exist in the theory and experiment . My rating will be changed according to other reviewers \u2019 comments and the author \u2019 s updates .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the positive feedback , and recognizing our conceptual novelty . Below we want to address some questions that we think are important to clarify first : 1 . CIFAR10-c data set ( https : //github.com/hendrycks/robustness ) . The CIFAR10-c dataset is an important recent benchmark for evaluating the robustness of learning in the setting of adaptations , under common corruptions and perturbations . Since its introduction , this dataset has been very commonly used in domain adaptation setting , and in particular , it is used in TTT ( Test Time Training , a oblivious test-time adaptation algorithm ) to demonstrate that test-time adaptation can achieve robustness . 2.In our work , we naturally generalize the consideration , and consider whether test-time adaptation can achieve , not only robustness on common corruptions and perturbations , but also on * * adversarial perturbations * * . We have indeed assumed that readers are familiar with these points . We will clarify the writing by adding relevant details . 3.Why the adversarial game is over $ U $ ? This is because that the adversary can only perturb features , but not labels . Therefore , to make this point precise , we write the maximum to be over $ U $ ( we thank the reviewers to notice these subtleties ) . 4.Why there are two different attack algorithms in the semi-supervised adversarial model ? This is explained in Section B.1 , paragraph * * '' Why do we need to have two algorithms A0 , A1 in Definition 3 ? `` * * Very roughly speaking , the first attack is about attacking the semi-supervised learning algorithm , and the second attack is about attacking the final model we learned . 5.DANN results in the minimax model . We have a sentence * '' Not surprisingly , the accuracy becomes very low '' * ( basically , the accuracy drops to random guessing ~10 % ) . We choose to present in this way because this experiment is simple to perform , and the results are very much as expected . 6.We have clarified the point regarding the baselines for FPA in the thread `` Revision # 1 : Clarifying adaptive fixed point attacks '' ."}, "1": {"review_id": "RkqYJw5TMD7-1", "review_text": "# # # Contributions # # # * The authors formalize test-time adaptation as a maximin threat model and contrast it with the `` threat model for classic adversarial robustness '' and the `` adversarial semi-supervised minimax threat model '' . * The paper studies test-time adaptation via DANN and its robustness in the maximin and minimax threat model . DANN has larger robustness against the proposed attacks in the maximin threat model than in minimax , providing evidence that maximin is a setting that is beneficial for the defender . However , it remains unclear if this would also be the case against stronger attackers . # # # Significance # # # Increasing robustness against adversarial robustness is a topic both relevant to basic research and also more applied research . Test-time adaptation is currently one of the most promising directions and this paper is thus very timely . Its formalization of a threat model for this domain can guide future research in this direction . That being said , the main method investigated ( DANN for test-time adaptation ) seems less practical and significant since it requires large test batch sizes and also huge computational resources at inference time . # # # Originality # # # The paper has only limited novelty : while the formalization of test-time adaptation as maximin threat model is very thorough , it is also mostly straightforward . Also studying DANN as a test-time adaptation is an incremental idea once one assumes that inference is conducted on large batches . Nevertheless , I think papers like this that consolidate recent research directions into a coherent framework are valuable . # # # Clarity # # # Generally , the introduction and the part outlining the threat model are written very clearly and I enjoyed the thorough formalization . A few details could be improved ( even though this is a bit nitpicking ) : * the introduction is very technical ; it could be more high level and shorter and leaving the more technical parts to Section 2 . * Threat models could mention that white-box knowledge of the attacker is assumed * it is not really clear why test inputs are assumed to be labeled in the threat models The experimental part is less clear unfortunately : * since DANN is a central method in the experiments , it should be briefly summarized to make the paper self-contained * it is not really clear to me how specifically DANN is trained here ( what is the objective ? ) . Specifically , is the loss on the labeled data based on standard training or adversarial training ? * generally , all training details ( optimizer , learning rate , batch size , number of epochs etc . ) are missing . the experiments can not be reproduced in the current form . # # # Quality # # # I have some serious doubts about the quality of the empirical evaluation . As the authors state `` While we are not able to prove the existence of a defender solution that separates maximin and minimax threat models , we design and implement experiments to provide evidence that such a strategy may actually exist . '' Thus , one of the main hypothesis of the paper ( maximin being a strictly weaker threat model than minimax ) depends on the design of experiments . My main concern here is that the attackers used are not actually strong : * A transfer attack should clearly only be a baseline that should get surpassed by stronger adaptive attackers * However , the fixed point attack seems to be weaker than the transfer attack . In particular its effectiveness in the homogeneous setting for CIFAR-10 reduces with the number of iterations k. This does not seem to be a reliable way of evaluating DANN 's maximin robustness * I would imagine a strong adaptive attack could be built by generating adversarial inputs with the objective of minimizing the loss of DANN 's domain label classifier . This would minimize the impact of test-time adaptation via DANN ( because both domains would have already very similar representations ) . In any case , more effort should be spend on designing a strong attack against a DANN-based test-time adaptation . I think it is also confusing to add evaluations in the inhomogeneous setting . Clearly , DANN gives strong results here because dealing with domain shift is what it was designed for . However , the paper is on adversarial robustness and confounding this with domain shift is misleading ( my suspicion is it was mainly added because DANN clearly outperforms adversarial training on the source data in this setting ; to proof me wrong , the authors should add a better motivation for this setting or remove it ) . # # # Recommendation # # # To summarize , I think the paper provides a strong formalization of a threat model for test-time adaptation . However , it does not introduce novel approaches , lacks clarity in experimental details , and its empirical results are less trustworthy given that the proposed attacks are rather weak . I thus recommend rejecting the paper in its current form . I encourage the authors to address the current shortcomings ; the paper clearly has potential in general . # # # Final Recommendation after Author Response # # # I would like the authors for the very active and productive rebuttal period . Actually , the current version has significantly deviated from the initially submitted version . I have read most of the paper a second time to take all changes into account . The authors considerably improved the paper and have addressed some of my concerns . I increase my score to 5 . The reason for not increasing the score to 6 or 7 is mainly that clarity of the paper is lacking . To give two examples : * Presumably because of the many changes during the rebuttal , the organization , structure and the quality of the manuscript is not always sufficient : to name one of several examples , the core Theorem 1 comes without any reference to its proof or any proof sketch in the main paper ( there exists a proof in the appendix , finding of which required scrolling through the entire appendix ) . * As stated in my initial review , since DANN is a central method in the experiments , it should be briefly summarized to make the paper self-contained and also the specific training I think the core issue with the submission is that there is simply too much content for one paper : * formalizing 3 threat models * a proof of separation of maximin and minimax * empirical evaluation on two datasets ( MNIST , CIFAR10 ) , three attacks ( transfer , two adaptive attacks ) , three defenses ( DANN , AdvS , TTT ) , and two settings ( homogenous , inhomogeneous ) Because of the page limit , a lot of details have been moved into the appendix , making the paper difficult to read . Moreover , even taking the appendix into account , details remain unclear such as how DANN was trained . Moreover , I do not find it convincing to move related work to the appendix ; relating the current work to other work should be an integral part of the main paper . My recommendation for the authors would be to strengthen the focus of the paper : I think the inhomogeneous setting does not contribute much and could be removed . Also I do n't see much value in MNIST and the weak transfer attacks . Moreover , the parts on preliminaries and threat models is too long for a conference paper and could be shortened . I think if focus were improved and the main document became more self-contained , the quality of the work would be considerably improved . For the time being , I see the submission still marginally below the acceptance threshold .", "rating": "5: Marginally below acceptance threshold", "reply_text": "We sincerely appreciate the reviewer for correctly recognizing the novelty for formulating a threat model for test-time adaptation . We want to , however , rebut first on some criticisms that are critical to understanding our contributions : 1 . First , to set up the context correctly , we want to point out that we are using the original DANN objective ( designed for unsupervised domain adaptation ) as defined in : https : //arxiv.org/abs/1505.07818 ( see equation ( 9 ) of the paper ) . For an intuitive understanding , one can also refer to this link https : //ameroyer.github.io/reading-notes/domain % 20adaptation/2019/05/23/domain_adversarial_training_of_neural_networks.html 2 . We want to highlight that this objective is * * attack agnostic * * , namely in the objective it never explicitly leverages the information of how the adversarially perturbed data are generated ( i.e. , it does not leverage the attack type in the objective ) . Specifically , this answers your question of * '' it is not really clear to me how specifically DANN is trained here ( what is the objective ? ) . Specifically , is the loss on the labeled data based on standard training or adversarial training ? `` * . No , the training on the source data is not adv training at all : If it was the case , this paper would lose all its meaning . And the fact that DANN can provide adversarial robustness is exactly why transductive adversarial learning can be surprising and interesting . 3.We believe that the use of DANN is novel . Before we introduced our threat model , it is not clear at all why one should even consider DANN ( an algorithm for unsupervised domain adaptation ) as an algorithm for * * adversarial * * robustness . Our research builds a bridge to connect two seemingly entirely different research directions , which to us is a clear novelty . 4.Finally , we also want to rebut in this thread about * * '' threat model being straightforward '' * * . We understand where this comes from , but this is an issue of the nature of `` searching for a solution vs. verifying a solution '' . As a conceptual paper , being able to articulate a threat model that is natural and clean is an advantage , rather than a disadvantage . To this end , we want to emphasize that the recent beautiful theory work by Goldwasser et al . ( spotlight in neurips 2020 , https : //arxiv.org/abs/2007.05145 ; Beyond Perturbations : Learning Guarantees with Arbitrary Adversarial Test Examples ) have * * studied a threat model similar in spirit to ours ( see their Definition 1.1 and Section 4.2 ) * * . However , while their threat model is more amenable to theoretical analysis , we view ours as much more practical and relevant to deep learning ( specifically , their formulation is designed to be analyzable for bounded VC dimension concept class , which may not be practical in the deep learning setting ) . Our choices of formalization have made it * * possible * * to connect to deep learning , and demonstrate that unsupervised domain adaptation can benefit adversarial robustness , which to us is very nontrivial . For your other constructive comments , we will post a separate thread to address those concerns ."}, "2": {"review_id": "RkqYJw5TMD7-2", "review_text": "This paper explores a new paradigm referred to as `` adversarial robustness of test-time adaptation '' . The authors propose a new threat model called a `` maximin game '' instead of the widely-accepted `` minimax game '' . In this new setup , the threat model becomes weaker as it first presents the unlabelled adversarial examples to the defender to get prepared . The authors also run some experiments showing that DANN seems to be a preferred method in this new setup . The paper is in general interesting with many novel definitions and discussions , but I do not see how this paper can benefit the community other than a set of new research directions with preliminary results . 1.First of all , I consider the writings of the introduction not professional : the introduction is full of notations , definitions , and even datasets and results that are only elucidated later in the paper . This writing style posted a great challenge to understand and appreciate this paper . 2.The paper interestingly introduces a set of new ideas/definitions . However , I wonder how these new research directions can benefit the community or industry . The authors explained that `` this question is also of practical importance since these 'new ' solutions may possess desirable properties that good solutions in the minimax threat model may lack . '' and then the authors mentioned that `` one such property is that the defender solution is attack agnostic '' . However , it seems the rest of this paper does not correspond to these remarks , e.g. , * The experiments ( in the main paper body ) are within $ \\ell_\\infty $ norm , so it seems there is no evidence about attack agnostic . * I did not find related discussions of other `` desirable properties '' later in this paper . * The authors also mentioned theoretical interest , yet I do n't think , with the current contents , this paper can be appreciated as a theoretical paper . Therefore , I suggest the authors further clarify their contributions , with emphasis on how their contributions can benefit the community and the industry . The current presentation leaves the impression that the paper is a set of new definitions without real-world implications . 3.The paper also says `` This threat model enables us to study whether a large test set can benefit a defender for adversarial robustness '' , yet I do not see any experiments ( in the main paper ) that correspond to this discussion . The appendix seems lacking this discussion either . Similarly , this kind of argument or claims without validations makes the paper read like an arbitrary set of definitions . 4.Following the `` attack agnostic '' argument above , Definition 2 suggests that the solution is also attack specific , as the defender needs to update the model according to the attacked samples . Although this update happens at test time , it happens before the evaluation . 5.The experiments are only about DANN , and seemingly leaving the details of AdvS unexplained . Is the AdvS the same architecture but without the domain component ? Then why not also report performances on S ? Without these details , it 's impossible to evaluate and appreciate the experimental results . 6.While it seems the authors ' focus is data-oblivious test-time adaptation , which , as I understand , does not allow the usage of labeling training data . Yet a major focus of the paper is DANN , which allows such usage . I 'm aware that the authors have mentioned the differences in multiple places , but if the main focus is DANN , I suggest the authors rephrase the paper to be about the setup DANN builds upon , instead of targeting a broader domain but only supporting the claims with experiments limited to a specific setup .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thanks for the review . We found that the comments have some significant misread , which we want to point out right away : 1 . Quote : * '' and then the authors mentioned that `` one such property is that the defender solution is attack agnostic '' . However , it seems the rest of this paper does not correspond to these remarks , e.g. , The experiments ( in the main paper body ) are within norm , so it seems there is no evidence about attack agnostic . `` * This is a misread . We have shown that DANN works as a test-time defense mechanism , and DANN is agnostic to the $ \\ell_\\infty $ attacks : This is because that we are using the original DANN algorithm , which is not specifically designed for the $ \\ell_\\infty $ attacks . In the introduction we have specifically mentioned : * * '' This is somewhat surprising as DANN is also agnostic to $ \\ell_\\infty $ attacks '' * * . On pp.6 in experiments we have also specifically mentioned , quote : * * '' ( B ) DANN is however not designed for adversarial robustness . Thus is it a very interesting question whether DANN can provide test-time adversarial robustness against attacks ( e.g.norm-based attacks ) that it is not specifically designed for '' * * Please refer to point 3 below for more discussion about `` attack agnostic '' . 2.Quote : * '' whether a large test set can benefit a defender for adversarial robustness '' , yet I do not see any experiments ( in the main paper ) that correspond to this discussion '' * Please refer to Figure 1 in the experiments : We have explicitly experimented with decreasing test size , and the robustness decreases . 3.Quote : * '' Following the `` attack agnostic '' argument above , Definition 2 suggests that the solution is also attack specific , as the defender needs to update the model according to the attacked samples . `` * We are afraid that there is a misunderstanding of what do we mean by `` attack agnostic '' . It means that the defense mechanism ( the algorithm ) does not leverage explicitly the attack type . For example , while DANN algorithm leverages $ U $ , the * * DANN algorithm itself never explicitly uses the fact that the adversarial data is from $ \\ell_\\infty $ attacks * * . Also , in Section D.3 we have also shown that DANN algorithm can provide robustness for $ \\ell_2 $ attacks as well , which further corroborates the `` attack agnostic point '' . We feel that this is critical to the understanding , so we want to point out as early as possible . 4.Quote : * '' While it seems the authors ' focus is data-oblivious test-time adaptation , which , as I understand , ... '' * We understand where this came from , but an important component of this paper is that we show that the current data-oblivious test-time adaptations ( e.g.Test-Time Training ) fails to have any robustness . Because the experiments for that purpose are so easy to perform , we condense them into Section 5 ( but they are important ) ."}, "3": {"review_id": "RkqYJw5TMD7-3", "review_text": "This paper studies test-time adversarial robustness through a maximin framework and illustrate non-trivial robustness ( under transfer attack ) using domain adversarial neural network ( DANN ) to Linf-norm and unseen adversarial attacks . While I agree that test-time adaptation is an important and practical approach for adversarial robustness , the current version , in my opinion , does not deliver significantly novel insights , nor considering a reasonably practical threat model . My main concerns are detailed as follows . 1.Impractical threat model : At test time , the maximin threat model ( Def.2 ) assumes the attacker to make move prior to the defender 's action , which limits the attacker 's ability and weakens the robustness evaluation . More importantly , it may create a false of security/robustness , as pointed out by ( Athalye et al.2018 ) , that the robustness gain may actually come from information obfuscation and thus the results may fail to provide meaningful robustness evaluation . Although the authors mention the maximin threat model is a weaker ( attack ) model and part of the goal is to find an adaptation method that is `` good '' in this attack-move-first scenario , I could n't see the practical utility and contributions form these results . Even in the test-time adaptation setting , the `` defender-move-first '' setting should be more practical . Better motivation and use cases are needed to justify why the considered setting is important . 2.Due to the assumption of the considered maximin threat model , the experiments are limited to comparing accuracy on transfer attacks , which provide limited understanding of the true robustness of the victim model . Moreover , the baseline models in comparison are too weak and unfair . To have a fair comparison , the authors are suggested to compare robustness on robust models such as TRADES [ R1 ] and adversarially trained models with unlabeled data [ R2 , R3 ] , so that the baseline models also use unlabeled data . If DANN shows limited robustness against white-box attacks but stronger robustness against transfer attacks , one can only conclude that transfer attack is a weaker threat model , which is a known result . I do not see new insights from the reported results . [ R1 ] https : //arxiv.org/abs/1901.08573 [ R2 ] https : //papers.nips.cc/paper/9298-unlabeled-data-improves-adversarial-robustness.pdf [ R3 ] https : //deepmind.com/research/publications/Are-Labels-Required-for-Improving-Adversarial-Robustness 3 . In addition to the issue of impractical threat model and lacking motivation , this paper contains too many high-level discussions accompanied with limited or even no empirical evidence . The theorem presented in the paper is a natural use of maximin inequality . In my opinion , the current version requires significant revision . I suggest the authors carefully motivate the research goals ( especially answering why the studied problems and settings are important ) , consolidate the claims on test-time robustness with convincing evidence , and make a broader connection to other test-time defenses other than DANN .", "rating": "3: Clear rejection", "reply_text": "Thanks for the review . The reviewer challenges our motivation for studying transductive adversarial robustness , which we want to rebut right away because this is critical to the understanding . 1.The threat model we studied in the paper is a particular instantiation of `` transductive adversarial robustness '' , which has recently received attention in theory . For example , a novel recent neurips spotlight paper by Goldwasser , Kalai , Kalai and Montasser ( https : //arxiv.org/abs/2007.05145 ) has studied very similar setting to ours . In that paper , it has explicitly proved for bounded-VC dimension concept class , transductive learning offers important robustness guarantees . Roughly speaking , their formulation is more amenable to theoretical study , while ours are more practically relevant for deep learning . 2.Regarding the `` gradient obfuscation '' comment , we have explicitly mentioned the differences with `` online defenses '' and `` gradient obfuscation '' . See pp.3 , paragraph `` Test-time defenses and BPDA . '' Basically , the previous `` online defenses '' apply to a setting where we want to `` sanitize '' $ x $ to $ x ' $ and then send to a pretrained model $ F $ . We investigate a much broader setting where we can modify model ( sometimes completely ) to predict $ x $ . 3.The reviewer mentioned that `` the threat model is impractical '' . We disagree : Many practical machine learning applications run in a batch mode , where one has received a batch $ U $ of test examples ( produced by the adversary potentially ) , and we want to be correct on those $ U $ . In those cases , transductive adversarial robustness is actually very appealing ( we want to refer the reviewer to Reviewer 3 and 4 , who have explicitly commented on this novelty ) . We have explicitly mentioned this in the introduction , quote : * '' First , this question is of practical interest : Many practical ML pipelines run in a batch mode , where they first collect a set of unlabelled data points , and then send them to a model ( e.g.Nado et al . ( 2020 ) ) .In such cases , data in the batch may have been adversarially perturbed , and it is a natural question whether we can leverage the large batch size and test-time adaptation to enhance adversarial robustness '' * 4 . We are very confused about one comment , quote * '' Even in the test-time adaptation setting , the `` defender-move-first '' setting should be more practical . `` * .What does that even mean ? The practical scenario we considered is that in many actual ML pipelines , one received an unlabeled data set $ U $ to classify , which could be corrupted ( but must happen before we receive it ) , and we want to use transductive learning to leverage $ U $ to get correct prediction . In such cases , defender moves * * after * * the attacker moves when test-time adaptation happens ."}}