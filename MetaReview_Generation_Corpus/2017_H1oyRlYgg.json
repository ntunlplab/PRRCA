{"year": "2017", "forum": "H1oyRlYgg", "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima", "decision": "Accept (Oral)", "meta_review": "All reviews (including the public one) were extremely positive, and this sheds light on a universal engineering issue that arises in fitting non-convex models. I think the community will benefit a lot from the insights here.", "reviews": [{"review_id": "H1oyRlYgg-0", "review_text": "Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank you for your review and your kind assessment ."}, {"review_id": "H1oyRlYgg-1", "review_text": "The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. Pros and Cons: Although there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks. Significance: I think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes. Comments: Earlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version. ", "rating": "10: Top 5% of accepted papers, seminal paper", "reply_text": "Thanks for your review and your suggestions on how our work could be used in further investigations ."}, {"review_id": "H1oyRlYgg-2", "review_text": "I think that the paper is quite interesting and useful. It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We experimented with additive random Gaussian noise ( both in gradients and in iterates ) , noisy labels and noisy input-data . However , despite significant tuning of the hyperparameters of the random noise , we did not observe any consistent improvements in testing error . Overall , our feeling is that this needs deeper investigation and that LB methods may need to be modified in a more fundamental way to achieve good generalization ."}], "0": {"review_id": "H1oyRlYgg-0", "review_text": "Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "We thank you for your review and your kind assessment ."}, "1": {"review_id": "H1oyRlYgg-1", "review_text": "The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. Pros and Cons: Although there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks. Significance: I think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes. Comments: Earlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version. ", "rating": "10: Top 5% of accepted papers, seminal paper", "reply_text": "Thanks for your review and your suggestions on how our work could be used in further investigations ."}, "2": {"review_id": "H1oyRlYgg-2", "review_text": "I think that the paper is quite interesting and useful. It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your review . We experimented with additive random Gaussian noise ( both in gradients and in iterates ) , noisy labels and noisy input-data . However , despite significant tuning of the hyperparameters of the random noise , we did not observe any consistent improvements in testing error . Overall , our feeling is that this needs deeper investigation and that LB methods may need to be modified in a more fundamental way to achieve good generalization ."}}