{"year": "2021", "forum": "jh-rTtvkGeM", "title": "Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability", "decision": "Accept (Poster)", "meta_review": "The paper demonstrates that Gradient Descents generally operates in a regime where the spectral norm of the Hessian is as large as possible given the learning rate. \n\nThe paper presents a very thorough empirical demonstration of the central claim, which was appreciated by the reviewers.\n\nA central issue to me in accepting the work was its novelty. Prior work has shown very closely related effects for SGD. The reviewers appreciated in discussions the novelty of the precise claim about the spectral norm hovering at around $\\frac{2}{\\eta}$. R4 and R2 also raised the issue that the related work discussion is not sufficient. Please make sure that you discuss very carefully related work in the paper, including a more detailed discussion in the Introduction.\n\nThe two key issues raised by R3, who voted for rejection, were that (1) the work studies Gradient Descent (rather than SGD), and (2) lack of theory. I agree with these concerns. Perhaps the Authors should address (1) by citing more carefully prior work that shows that a similar phenomenon does seem to happen in training with SGD. As for (2), I agree here with R1,R2 and R4 that empirical evaluation is a key strength of the paper. \n\nBased on the above, it is my pleasure to recommend the acceptance of the paper. Thank you for submitting your work to ICLR, and please make sure you address all remarks of the reviewers in the camera-ready version.", "reviews": [{"review_id": "jh-rTtvkGeM-0", "review_text": "This work identifies a new empirical phenomenon in the training dynamics of deep nets : when trained with full-batch GD , the curvature of the train loss increases up to a critical value of 2/ ( step size ) , at which point it plateaus for the remainder of training . This phenomenon is demonstrated robustly for networks trained with MSE loss , across various architectures and datasets , and a slightly weaker version of this holds for cross-entropy loss as well . This work contributes to our understanding of deep network dynamics -- it is a precise and apparently robust phenomenon that was surprisingly not noticed before ( perhaps because of the requirement of GD vs SGD ) . In terms of impact : This work will be instructive for DL optimization theory , since it points out that certain assumptions which are usually made in theoretical works ( e.g.step size < < curvature ) are far from true in practice -- moreover , it guides theory towards more realistic assumptions . It may also have later impact in practice , by leading to a better understanding of the interaction between optimization algorithm , step size , and architecture . Thus I recommend acceptance . Weaknesses and desired clarifications : - It should be mentioned more prominently that these results are primarily for networks trained with MSE loss -- and that a similar but weaker phenomena holds for cross-entropy loss . - Why is the main example in Section 3 given for a non-standard network for CIFAR-10 ? A 2-layer MLP with ELU activation . Why not a standard network with standard activation ? ( VGG-11 or ResNet-18 , etc ) . - The distinction between SGD and GD seems crucial for this phenomenon , so more discussion would be good . In particular , as noted in the related works , some papers using SGD claim an opposite effect . This is especially important to clarify since SGD is most often used in practice . If time allows , experiments with increasing batch size could shed light on the importance of GD vs SGD . - The Related Works is currently written as an account of what previous works do * not * do , as opposed to what they do . It would help contextualize this work to relate it to prior works which are consistent ( or inconsistent ) with this phenomena -- especially works studying the Hessian of deep nets . Some of the mechanisms proposed in prior works ( eg Lewkowycz et al 2020 and works on deep linear networks ) may also be helpful to understand the phenomena in this work . Comments which do not affect the score : - I wonder if you have measured the 2nd eigenvalue during training as well ? In particular , after the 1st eigenvalue has saturated at 2/eta , does the 2nd eigenvalue also `` progressively sharpen '' up to 2/eta ? ( And so on for later eigenvals ) . - I am glad to see the experiments on deep linear networks , it suggests that it may be possible to theoretically understand this phenomenon in such simple settings . This would be a nice topic for future work .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for all of your insightful comments ! Here we respond to : > It should be mentioned more prominently that these results are primarily for networks trained with MSE loss -- and that a similar but weaker phenomena holds for cross-entropy loss . We \u2019 d first like to clarify that your comment applies to just one of the results in our paper . Our paper has essentially three main results : ( 1 ) negative results for optimization theory , ( 2 ) a new qualitative characterization of the behavior of gradient descent ( that GD is constantly `` trying to '' increase the sharpness , but is constantly being blocked from doing so ) , and , finally , ( 3 ) `` a simple rule that can numerically predict , to respectable accuracy , the value of '' the sharpness during gradient descent training . This simple rule is that `` the sharpness hovers right at , or just above , the value $ 2 / \\eta $ . '' Your comment applies to Result ( 3 ) , but not ( 1 ) or ( 2 ) . Regarding Result ( 3 ) , you correctly observe that for some networks/losses , the gap between the sharpness and the value $ 2 / \\eta $ is minuscule ( e.g.Figure 3 , or the square loss networks in Figure 6 ) , while for other networks/losses , the gap between the sharpness and the value 2 / eta is small yet non-miniscule ( e.g.Figure 1 , or the cross-entropy networks in Figure 6 ) . We tried to choose wording -- \u201d just above \u201d -- that applies to both cases , the only difference being one of degree , i.e.of how big is \u201c just above. \u201d If you think that different wording would be better than \u201c the sharpness hovers right at , or just above , the value $ 2/\\eta $ \u201d , then we would be happy to make a change . Additionally , in section 3 we will insert a sentence noting that the gap between the sharpness and the value $ 2/\\eta $ is not always as miniscule as in Figure 3 . As you point out , for the networks in Figure 6 , the loss function makes a difference : for MSE loss , the gap is minuscule , while for cross-entropy loss , the gap is non-minuscule . In our experience , this cross entropy-vs.-MSE pattern is true for smallish networks like the ones in Figure 6 , but not always for big complicated architectures like VGG . When training big complicated architectures like VGG , we sometimes observe that the gap is non-minuscule even when training with the MSE loss . ( In other words , the MSE version of Figure 1 looks much like Figure 1 . ) In the case of big vs. small , we think this is because for big , complicated networks like VGG , the training objective is just generally less well-behaved , so the quadratic Taylor approximation is not as accurate over long distances . Ultimately , while it would definitely be preferable if the gap were always minuscule ( as in Figure 3 , or the top row of Figure 6 ) , we think that even the level of agreement exhibited in Figure 1 is noteworthy , considering that little is known quantitatively about the neural network training process ."}, {"review_id": "jh-rTtvkGeM-1", "review_text": "Summary : This submission numerically shows that during exploring the neural network landscape , GD flow keeps increasing the sharpness . As a result , GD with a fixed learning rate will exhibit two phases during the dynamics . Denote by $ \\eta $ the fixed learning rate . In the first phase , GD follows closely to the GD flow , and it finally converges to a region where the sharpness is roughly $ 2/\\eta $ . Then , it transits into the second phase during which the sharpness hovers right at or above $ 2/\\eta $ . In the second phase , GD can not increase the sharpness anymore due to the dynamical stability constraint . Thus , the authors name it the Edge of Stability phase . What is interesting is that in the edge of stability phase , the loss is still decreasing steadily although not monotonically . Pros : I enjoy reading this submission . It is clearly written and the numerical evaluation is also sufficient . To my best of knowledge , the observation that the edge of stability happens during the whole late phase of GD dynamics is new . It reveals a very complicated dynamical behavior of GD for training neural networks , which has not been systematically investigated before . Thus , I think this submission made a very important and original contribution to the understanding of GD dynamics in deep learning . Cons : The relationship with the previous study on the dynamical stability of ( S ) GD is not sufficient discussed . In my opinion , just saying `` previous works have argued that the stability properties of optimization algorithms could potentially serve as a form of implicit bias in deep learning '' is obviously not precise and enough . A large number of numerical results in [ 1,2 ] already showed that the edge of stability happens for the convergent solutions , which implies that the edge of stability must happen at least in the very late phase of GD dynamics . The new finds of this submission are that the edge of stability actually holds for a large portion of GD dynamics , which is very unexpected . The authors should explicitly mention that the edge of stability was already observed in these previous works . Giving the right credit to the right references does not harm the contribution of this submission . Especially , the jargon `` Edge of stability '' was first used in [ 1 ] , and the authors even did not mention it . [ 1 ] Giladi , Niv , et al . `` At Stability 's Edge : How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks ? . '' arXiv preprint arXiv:1909.12340 ( 2019 ) . [ 2 ] Wu , Lei , Chao Ma , and E. Weinan . `` How sgd selects the global minima in over-parameterized learning : A dynamical stability perspective . '' Advances in Neural Information Processing Systems . 2018 .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks very much for your review ! We agree that our Related Work section does not currently provide sufficient discussion on prior works pertaining to dynamical stability . In the revision , when we have an extra page , we will be sure to discuss prior work on this subject in a much greater level of detail . In response to your specific comments : > A large number of numerical results in [ Wu et al ( 2018 ) , Giladi et al ( 2019 ) ] already showed that the edge of stability happens for the convergent solutions * * Wu et al ( 2018 ) * * : Yes , as you point out , Wu et al ( 2018 ) previously observed that the sharpness of the convergent solution learned by gradient descent was not just * bounded by * $ 2 /\\eta $ ( as expected ) , but was , mysteriously , * approximately equal * to $ 2 / \\eta $ . In the early stages of our research , this finding was very helpful to us : we 'd observed `` Edge of Stability '' ourselves on a few architectures , but we were n't sure how general the phenomenon was . The fact that it had also appeared in the experiments in Wu et al ( 2018 ) gave us confidence that the phenomenon might be quite general , which motivated us to investigate further . We 'd noted in earlier drafts of our paper that Wu et al ( 2018 ) made this observation , but lack of space forced us to condense the section on dynamical stability . In the revision , we will make sure to note that Wu et al ( 2018 ) previously made this observation . * * Giladi et al ( 2019 ) * * : Giladi et al ( 2019 ) did not observe the effect we call `` Edge of Stability '' , neither during training nor at convergence . That paper did argue that the convergent solution reached by gradient descent would have sharpness _less than or equal to_ $ 2 / \\eta $ , but did not suggest or demonstrate that the sharpness would be at the very top of that allowable range . Perhaps you were referring to their Figure 4 ( though if you had something else in mind , we \u2019 d greatly appreciate any pointers ) . In their Figure 4 , they empirically validated that the maximum stable learning rate derived using a quadratic Taylor approximation also can be used to predict local divergence on the real neural network training objective . Namely , they first trained a network to a low-loss iterate ( they call it a `` minimum '' ) $ \\theta_0 $ using some original learning rate $ \\eta_0 $ . Then they computed the sharpness at $ \\theta_0 $ \u2014 call this sharpness $ a $ . However , they did * not * empirically examine the relationship between $ a $ and $ \\eta_0 $ , as we do in our paper . Instead , they demonstrated that they could use $ a $ to accurately predict the learning rates at which gradient descent initialized at $ \\theta_0 $ would escape that immediate neighborhood . That is , they analytically computed the maximum stable learning rate for ( asynchronous ) gradient descent on a quadratic function with sharpness $ a $ , and they plotted this on the left side of Figure 4 . Then they empirically found the maximum learning rate at which gradient descent initialized at $ \\theta_0 $ would remain in the neighborhood of $ \\theta_0 $ ( rather than immediately escaping ) , and they plotted this on the right side of Figure 4 . The point of this experiment was to demonstrate , both theoretically ( left ) and empirically ( right ) , the dependence of gradient descent 's stability properties on both the momentum parameter $ m $ and the time lag $ \\tau $ . > Especially , the jargon `` Edge of stability '' was first used in [ 1 ] , and the authors even did not mention it . As we will clarify in the revision , our jargon `` Edge of Stability '' is indeed inspired by Giladi et al ( 2019 ) . That paper used this term to refer to the well-known fact that for gradient descent , there is a step size threshold for which step sizes over this threshold will trigger ( local ) divergence . However , just to reiterate , their paper did not suggest or demonstrate our main contribution : that , no matter the step size , gradient descent always eventually navigates to a region where _that step size_ is on the edge of stability ."}, {"review_id": "jh-rTtvkGeM-2", "review_text": "This paper presents an interesting observation for GD . That is , the sharpness of the learnt model in the final phase of the training ( measured by the largest eigenvalue of the training loss Hessian ) hovers right at the value 2/\\eta while the training loss . At the same time , the loss goes to unstable and non-monotonically decreasing . This pattern is consistent across architecture , activation functions , tasks , loss functions and BN . Comprehensive experiments are conducted to show this common observation . The paper is easy to follow . Besides the empirical results in the main body , authors give insightful discussions in Intro and related work section . Specifically , authors propose a novel guess , that GD eventually transitions to \u201c Edge of stability \u201d , where GD can finally succeed with non-small enough step size . Although I am not sure how GD can do this , the concept of \u201c Edge of stability \u201d is still attractive . I have two concerns for this work . 1 ) Authors did not investigate why sharpness finally hover over 2/\\eta . Is it a trivial consequence followed by some relationship between the update rule of GD and the definition of sharpness , without any condition ? Even if yes , we may further think about how to leverage it along the existing discussions in this paper . Hope to have authors ' feedbacks on this later . 2 ) Given people use SGD to train neural networks , discussions about the insight from the observation of GD to SGD will enhance the impact of this paper .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your helpful feedback ! We don \u2019 t have a rigorous mathematical proof for why the sharpness hovers just above $ 2/\\eta $ , but we do have a partial explanation which we believe makes intuitive sense : 1 . On the one hand , we empirically observe that in neural network training , the sharpness always \u201c wants to \u201d increase ( progressive sharpening ) . 2.On the other hand , gradient descent with step size $ \\eta $ on a quadratic function will * provably * diverge if the sharpness is greater than $ 2 / \\eta $ . In part # 2 of this response ( a separate comment ) , we show why this is true for _one-dimensional_ quadratic functions ; Appendix B of our paper covers the more general case of a multidimensional quadratic function . Now , neural network training objectives are not globally quadratic , but around any iterate one can make a second-order Taylor approximation to get a * local * quadratic approximation . If the sharpness at some iterate is greater than $ 2/\\eta $ , then gradient descent would provably diverge if run on the quadratic Taylor approximation around that iterate . Therefore , to the extent that gradient descent on the real training objective resembles gradient descent on the quadratic Taylor approximation , gradient descent will be unstable in neighborhoods where the sharpness exceeds $ 2/\\eta $ . In summary , on the one hand , gradient descent always \u201c wants to \u201d increase the sharpness ; but on the other hand , gradient descent can not linger for long in any neighborhood where the sharpness exceeds $ 2/\\eta $ . Thus , it makes sense that gradient descent spends its time in regions where the sharpness is approximately equal to the maximum stable sharpness , $ 2/\\eta $ . Indeed , one can non-rigorously argue that by process of elimination , this is the only possible outcome : the sharpness couldn \u2019 t be consistently much greater than $ 2/\\eta $ , because gradient descent would leave the neighborhood , yet the sharpness couldn \u2019 t be consistently less than $ 2/\\eta $ , because the sharpness would increase . A question you might have is : if gradient descent on quadratic functions diverges whenever the sharpness is greater than $ 2/\\eta $ , how can it be that in Figure 1 ( and many other figures ) the sharpness is consistently a little bit bigger than $ 2/\\eta $ ? The answer likely lies in the fact that on quadratic functions , the * speed * of divergence depends on the margin between the sharpness and the value $ 2/\\eta $ . If the sharpness exceeds $ 2/\\eta $ by a lot , then this divergence will be fast , but if the sharpness exceeds $ 2/\\eta $ by a little , then this divergence will be slow . So , if you run gradient descent in some region of the loss landscape in where the sharpness is just a little bit greater than $ 2/\\eta $ , then yes it \u2019 s true that gradient descent would * eventually * diverge if run on the quadratic Taylor approximation around that point , but this divergence would occur slowly \u2014 slower than the speed at which the quadratic Taylor approximation is itself changing . You can view the momentum experiments in Figure 5 as further confirmation of our hypothesis that the reason why the sharpness hovers at $ 2/\\eta $ during vanilla gradient descent is because $ 2/\\eta $ is the maximum stable sharpness of gradient descent on quadratic functions . For momentum gradient descent ( with either Polyak or Nesterov momentum ) , the maximum stable sharpness is given not by $ 2/\\eta $ but by a more complicated expression that is printed analytically in Equation ( 1 ) and plotted as the dashed horizontal lines in Figure 5 . Sure enough , during momentum gradient descent , the sharpness hovers just above these values . It is true that we do not provide a rigorous mathematical proof for why the sharpness hovers just above $ 2/\\eta $ , nor do we have an explanation for why progressive sharpening occurs . Rather , one major goal of our paper is to bring these phenomena to the attention of the ML theory / optimization theory communities so that future work can make our observations mathematically ironclad . As we write in the conclusion : \u201c We think that understanding _precisely_ [ emphasis new ] how gradient descent is able to succeed at the Edge of Stability should be viewed as an important open problem in optimization theory. \u201d We are eager to hear any more questions or comments that you have ."}, {"review_id": "jh-rTtvkGeM-3", "review_text": "Summary of results : This empirical paper finds that deep neural network `` sharpness '' ( as measured by the top eigenvalue of the Hessian ) tends to saturate at or hover just above the value 2/\\eta , where \\eta is the step size in gradient descent ( GD ) , during the course of optimization . This is accompanied by non-monotonicity in the loss . ( Results also hold for gradient descent with momentum . ) This phenomena occurs for full-batch GD , a variety of tasks , and two different loss functions ( square loss and cross-entropy , although in the latter case late-time dynamics of the sharpness is different , with the sharpness decreasing ) . The result also holds across a variety of architectures ( VGG-11 , with and without batch norm , convolutional networks and fully-connected networks with different nonlinearities , a deep linear network , a Transformer model -- although I comment on the architecture dependence below . ) The authors refer to this phenomenon of sharpness hovering at or above the 2/\\eta bound as optimization on the `` edge of stability . '' The authors posit that this observation goes against our current understanding of optimization in deep learning : ( i ) theoretical work may use assumptions such as monotonic descent or L-smoothness , which is in violation of the `` edge of stability '' regime where the loss behavior is not monotonic ; ( ii ) relatedly , it corresponds to a regime of instability in a quadratic Taylor approximation , so this is not a good assumption either . The empirical observations do not appear to straightway carry over to SGD ( although there is some discussion of similarities in the appendix ) -- the authors leave this is as an open problem . Quality and clarity : The work is of good quality . The experiments that are presented clearly demonstrate the described phenomenon , attempt to sample from a variety of problems ( ranging from a simple 1D regression task to training VGG-11 on CIFAR-10 in Fig.1 ) , and are well-explained . Originality : The observed phenomena ( of sharpness progressively increasing and then hovering at or near 2/\\eta through much of GD dynamics ) appears new to the best of my knowledge . ( However , prior papers have pointed out that GD optimization can stably proceed for learning rates above 2/\\eta and are accompanied with nonmonotonic loss : one example is Lewkowycz , et al. ( 2020 ) . ) Significance : I think the observations are somewhat important , although perhaps not as much as the authors seem to stress in writing ( e.g.in the abstract , `` Our results ... shed significant light on the dynamics of gradient descent with a fixed step size ... '' -- this paper focuses only on empirical observations rather than explanation of the mechanism behind the stability ) . The results are for full-batch GD which somewhat limits the applicability of the results to practice where SGD is more common . Nonetheless , I do think understanding how optimization is stabilized in this regime is an interesting problem . Other comments : I think that a key point not fully understood ( at least , as expressed in the writing ) or investigated in the experiments is the role of network width ( i.e.I 'm skeptical that the observations will be `` consistent across architectures '' as expressed in the opening paragraph in the Introduction if wider networks are also investigated empirically ) . This is a main reason for not giving the paper a higher score . In Appendix D , the authors try to reconcile the results with existing theory on infinite-width limits . Fig.11 ( c ) shows that across networks of varying width but trained at the same learning rate , wider networks end up with smaller values of sharpness at the end of training ( here , say that all experiments are stopped at the same value of the training loss ) . Hence if a narrower network saturates the 2/\\eta bound at late times in gradient descent , the wider networks will fall below this value . ( This is , of course , consistent with no evolution in the curvature in the infinite width limit . ) That is to say , whether or not the `` edge of stability '' regime is reached depends quite strongly on how wide the networks are . While some realistic networks are explored ( e.g.VGG-11 , * narrow * Transformers ) , which is a positive point , all of the remaining networks used in the experiments ( as far as I can gather ) are on the narrow side ( e.g.layer widths of order 100 or 200 ) . I think a shortcoming of the paper is that the strong width ( i.e.architecture ) dependence of the phenomenon is not fully appreciated or discussed by the authors ( e.g.for instance , by discussing in the main text that it only sets in for narrow networks ) or investigated empirically . ( Alternatively , noting for readers that all the networks chosen are rather narrow , out of transparency . ) I note that the authors do mention in Appendix D : `` ... one might hypothesize that progressive sharpening might attenuate as networks ( with NTK parameterization ) become increasingly wide . '' However , I do n't think that NTK parameterization is necessary for this to be true . In short , I think the width dependence of the phenomenon is an important factor that affects the significance and applicability of the observations and could have been treated with greater transparency ( with additional experiments and additions to the main text and abstract ) . A comment on relation to prior work : the authors write that Lewkowycz , et al . ( 2020 ) imply that `` actual progress would occur in regions where the sharpness remains strictly less than 2/\\eta . Our experiments demonstrate otherwise . '' I do n't believe this is a conclusion of that paper ( progress happens when sharpness is above 2/\\eta ) . ( Note also that the paper tends to study wide networks . ) Could the authors elaborate on what they mean here ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Here , we respond to your points regarding Lewkowycz , et al . ( 2020 ) . > A comment on relation to prior work : the authors write that Lewkowycz , et al . ( 2020 ) imply that `` actual progress would occur in regions where the sharpness remains strictly less than 2/\\eta . Our experiments demonstrate otherwise . '' I do n't believe this is a conclusion of that paper ( progress happens when sharpness is above 2/\\eta ) . ( Note also that the paper tends to study wide networks . ) Could the authors elaborate on what they mean here ? It is true that Lewkowycz , et al . ( 2020 ) exhibits a stage of training in which the sharpness is greater than $ 2 / \\eta $ , and where the training loss behaves non-monotonically . However , in Lewkowycz , et al . ( 2020 ) this \u201c catapult \u201d stage of training is understood to be temporary and fleeting , and actual optimization ( in the sense of the loss consistently going down ) is only supposed to commence after this \u201c catapult \u201d stage is definitively over . By contrast , our paper details how the sharpness is $ \\approx 2 / \\eta $ ( and the loss is non-monotonic ) for the bulk of training notably , at the same time as the loss is consistently decreasing . In more detail , Lewkowycz , et al . ( 2020 ) examined what happens when the step size $ \\eta $ is set greater than $ 2 /\\lambda_0 $ , where $ \\lambda_0 $ is the initial sharpness . In this scenario , at the beginning of training , gradient descent oscillates unstably along the leading Hessian eigenvector until it is flung from its initial position . What Lewkowycz , et al . ( 2020 ) showed is that , rather than diverge entirely , on some architectures gradient descent will \u201c land on its feet \u201d ( our words ) in a new region that is flat enough to accommodate the step size , i.e.a region where the sharpness is $ < 2 /\\eta $ . To quote from that paper : if $ \\eta > 2/\\lambda_0 $ , then > optimization begins with a period of exponential growth in the loss , coupled with a rapid decrease in the curvature [ sharpness ] , until curvature * * stabilizes at a value $ \\lambda_ { \\text { final } } < 2 / \\eta $ . Once the curvature drops below $ 2 / \\eta $ , training converges * * \u2026 Note that the new sharpness is called $ \\lambda_ { \\text { final } } $ , indicating that once gradient descent finds a region where the sharpness is $ < 2 /\\eta $ , the sharpness is supposed to stay there for the remainder of training . Indeed , this is stated explicitly in their section 4.1 : > one striking prediction of the model is that after a period of excursion , the logic differences settle back to O ( 1 ) values , the * * NTK stops changing * * [ emphasis ours ] , and evolution is again well approximated by a linear model with constant kernel at large with . Notice also that they write : \u201c once the curvature drops below $ 2 / \\eta $ , training converges. \u201d This is what we meant when we wrote that \u201c actual progress would occur in regions where the sharpness remains strictly less than $ 2 / \\eta $ . \u201d Their overall story is illustrated by their Figure 2 ( a ) and 2 ( b ) . In Figure 2 ( b ) , which plots the sharpness , we can see that after an initial drop ( the catapult ) , the sharpness ceases to change for the remainder of training . In Figure 2 ( a ) , which plots the train loss , we can see that after an initial spike ( the catapult ) , the train loss monotonically decreases for the remainder of training . To summarize , the overall story in Lewkowycz , et al . ( 2020 ) is fully consistent with the conventional local-quadratic view of optimization . In their paper , the main function of the large learning rate is to trigger a switch from a sharp initialization to a flat initialization . After this initialization switch , the conventional optimization wisdom concerning step sizes is supposed to kick in again . -- > However , prior papers have pointed out that GD optimization can stably proceed for learning rates above 2/\\eta and are accompanied with nonmonotonic loss : one example is Lewkowycz , et al . ( 2020 ) .We are not aware of any such papers apart from Lewkowycz , et al . ( 2020 ) ( which we discussed above ) , as well Xing et al . ( 2018 ) [ 1 ] , which we discussed in our Related Work section . Xing et al . ( 2018 ) empirically studied full-batch gradient descent , and noted that the train loss eventually starts behaving non-monotonically . However , that paper didn \u2019 t relate this phenomenon to dynamics of the sharpness , or discuss implications for optimization theory . If there are other papers which observe either of these two effects , we are happy to add the appropriate citations . [ 1 ] Chen Xing , Devansh Arpit , Christos Tsirigotis , and Yoshua Bengio . A walk with sgd ."}], "0": {"review_id": "jh-rTtvkGeM-0", "review_text": "This work identifies a new empirical phenomenon in the training dynamics of deep nets : when trained with full-batch GD , the curvature of the train loss increases up to a critical value of 2/ ( step size ) , at which point it plateaus for the remainder of training . This phenomenon is demonstrated robustly for networks trained with MSE loss , across various architectures and datasets , and a slightly weaker version of this holds for cross-entropy loss as well . This work contributes to our understanding of deep network dynamics -- it is a precise and apparently robust phenomenon that was surprisingly not noticed before ( perhaps because of the requirement of GD vs SGD ) . In terms of impact : This work will be instructive for DL optimization theory , since it points out that certain assumptions which are usually made in theoretical works ( e.g.step size < < curvature ) are far from true in practice -- moreover , it guides theory towards more realistic assumptions . It may also have later impact in practice , by leading to a better understanding of the interaction between optimization algorithm , step size , and architecture . Thus I recommend acceptance . Weaknesses and desired clarifications : - It should be mentioned more prominently that these results are primarily for networks trained with MSE loss -- and that a similar but weaker phenomena holds for cross-entropy loss . - Why is the main example in Section 3 given for a non-standard network for CIFAR-10 ? A 2-layer MLP with ELU activation . Why not a standard network with standard activation ? ( VGG-11 or ResNet-18 , etc ) . - The distinction between SGD and GD seems crucial for this phenomenon , so more discussion would be good . In particular , as noted in the related works , some papers using SGD claim an opposite effect . This is especially important to clarify since SGD is most often used in practice . If time allows , experiments with increasing batch size could shed light on the importance of GD vs SGD . - The Related Works is currently written as an account of what previous works do * not * do , as opposed to what they do . It would help contextualize this work to relate it to prior works which are consistent ( or inconsistent ) with this phenomena -- especially works studying the Hessian of deep nets . Some of the mechanisms proposed in prior works ( eg Lewkowycz et al 2020 and works on deep linear networks ) may also be helpful to understand the phenomena in this work . Comments which do not affect the score : - I wonder if you have measured the 2nd eigenvalue during training as well ? In particular , after the 1st eigenvalue has saturated at 2/eta , does the 2nd eigenvalue also `` progressively sharpen '' up to 2/eta ? ( And so on for later eigenvals ) . - I am glad to see the experiments on deep linear networks , it suggests that it may be possible to theoretically understand this phenomenon in such simple settings . This would be a nice topic for future work .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thank you very much for all of your insightful comments ! Here we respond to : > It should be mentioned more prominently that these results are primarily for networks trained with MSE loss -- and that a similar but weaker phenomena holds for cross-entropy loss . We \u2019 d first like to clarify that your comment applies to just one of the results in our paper . Our paper has essentially three main results : ( 1 ) negative results for optimization theory , ( 2 ) a new qualitative characterization of the behavior of gradient descent ( that GD is constantly `` trying to '' increase the sharpness , but is constantly being blocked from doing so ) , and , finally , ( 3 ) `` a simple rule that can numerically predict , to respectable accuracy , the value of '' the sharpness during gradient descent training . This simple rule is that `` the sharpness hovers right at , or just above , the value $ 2 / \\eta $ . '' Your comment applies to Result ( 3 ) , but not ( 1 ) or ( 2 ) . Regarding Result ( 3 ) , you correctly observe that for some networks/losses , the gap between the sharpness and the value $ 2 / \\eta $ is minuscule ( e.g.Figure 3 , or the square loss networks in Figure 6 ) , while for other networks/losses , the gap between the sharpness and the value 2 / eta is small yet non-miniscule ( e.g.Figure 1 , or the cross-entropy networks in Figure 6 ) . We tried to choose wording -- \u201d just above \u201d -- that applies to both cases , the only difference being one of degree , i.e.of how big is \u201c just above. \u201d If you think that different wording would be better than \u201c the sharpness hovers right at , or just above , the value $ 2/\\eta $ \u201d , then we would be happy to make a change . Additionally , in section 3 we will insert a sentence noting that the gap between the sharpness and the value $ 2/\\eta $ is not always as miniscule as in Figure 3 . As you point out , for the networks in Figure 6 , the loss function makes a difference : for MSE loss , the gap is minuscule , while for cross-entropy loss , the gap is non-minuscule . In our experience , this cross entropy-vs.-MSE pattern is true for smallish networks like the ones in Figure 6 , but not always for big complicated architectures like VGG . When training big complicated architectures like VGG , we sometimes observe that the gap is non-minuscule even when training with the MSE loss . ( In other words , the MSE version of Figure 1 looks much like Figure 1 . ) In the case of big vs. small , we think this is because for big , complicated networks like VGG , the training objective is just generally less well-behaved , so the quadratic Taylor approximation is not as accurate over long distances . Ultimately , while it would definitely be preferable if the gap were always minuscule ( as in Figure 3 , or the top row of Figure 6 ) , we think that even the level of agreement exhibited in Figure 1 is noteworthy , considering that little is known quantitatively about the neural network training process ."}, "1": {"review_id": "jh-rTtvkGeM-1", "review_text": "Summary : This submission numerically shows that during exploring the neural network landscape , GD flow keeps increasing the sharpness . As a result , GD with a fixed learning rate will exhibit two phases during the dynamics . Denote by $ \\eta $ the fixed learning rate . In the first phase , GD follows closely to the GD flow , and it finally converges to a region where the sharpness is roughly $ 2/\\eta $ . Then , it transits into the second phase during which the sharpness hovers right at or above $ 2/\\eta $ . In the second phase , GD can not increase the sharpness anymore due to the dynamical stability constraint . Thus , the authors name it the Edge of Stability phase . What is interesting is that in the edge of stability phase , the loss is still decreasing steadily although not monotonically . Pros : I enjoy reading this submission . It is clearly written and the numerical evaluation is also sufficient . To my best of knowledge , the observation that the edge of stability happens during the whole late phase of GD dynamics is new . It reveals a very complicated dynamical behavior of GD for training neural networks , which has not been systematically investigated before . Thus , I think this submission made a very important and original contribution to the understanding of GD dynamics in deep learning . Cons : The relationship with the previous study on the dynamical stability of ( S ) GD is not sufficient discussed . In my opinion , just saying `` previous works have argued that the stability properties of optimization algorithms could potentially serve as a form of implicit bias in deep learning '' is obviously not precise and enough . A large number of numerical results in [ 1,2 ] already showed that the edge of stability happens for the convergent solutions , which implies that the edge of stability must happen at least in the very late phase of GD dynamics . The new finds of this submission are that the edge of stability actually holds for a large portion of GD dynamics , which is very unexpected . The authors should explicitly mention that the edge of stability was already observed in these previous works . Giving the right credit to the right references does not harm the contribution of this submission . Especially , the jargon `` Edge of stability '' was first used in [ 1 ] , and the authors even did not mention it . [ 1 ] Giladi , Niv , et al . `` At Stability 's Edge : How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks ? . '' arXiv preprint arXiv:1909.12340 ( 2019 ) . [ 2 ] Wu , Lei , Chao Ma , and E. Weinan . `` How sgd selects the global minima in over-parameterized learning : A dynamical stability perspective . '' Advances in Neural Information Processing Systems . 2018 .", "rating": "8: Top 50% of accepted papers, clear accept", "reply_text": "Thanks very much for your review ! We agree that our Related Work section does not currently provide sufficient discussion on prior works pertaining to dynamical stability . In the revision , when we have an extra page , we will be sure to discuss prior work on this subject in a much greater level of detail . In response to your specific comments : > A large number of numerical results in [ Wu et al ( 2018 ) , Giladi et al ( 2019 ) ] already showed that the edge of stability happens for the convergent solutions * * Wu et al ( 2018 ) * * : Yes , as you point out , Wu et al ( 2018 ) previously observed that the sharpness of the convergent solution learned by gradient descent was not just * bounded by * $ 2 /\\eta $ ( as expected ) , but was , mysteriously , * approximately equal * to $ 2 / \\eta $ . In the early stages of our research , this finding was very helpful to us : we 'd observed `` Edge of Stability '' ourselves on a few architectures , but we were n't sure how general the phenomenon was . The fact that it had also appeared in the experiments in Wu et al ( 2018 ) gave us confidence that the phenomenon might be quite general , which motivated us to investigate further . We 'd noted in earlier drafts of our paper that Wu et al ( 2018 ) made this observation , but lack of space forced us to condense the section on dynamical stability . In the revision , we will make sure to note that Wu et al ( 2018 ) previously made this observation . * * Giladi et al ( 2019 ) * * : Giladi et al ( 2019 ) did not observe the effect we call `` Edge of Stability '' , neither during training nor at convergence . That paper did argue that the convergent solution reached by gradient descent would have sharpness _less than or equal to_ $ 2 / \\eta $ , but did not suggest or demonstrate that the sharpness would be at the very top of that allowable range . Perhaps you were referring to their Figure 4 ( though if you had something else in mind , we \u2019 d greatly appreciate any pointers ) . In their Figure 4 , they empirically validated that the maximum stable learning rate derived using a quadratic Taylor approximation also can be used to predict local divergence on the real neural network training objective . Namely , they first trained a network to a low-loss iterate ( they call it a `` minimum '' ) $ \\theta_0 $ using some original learning rate $ \\eta_0 $ . Then they computed the sharpness at $ \\theta_0 $ \u2014 call this sharpness $ a $ . However , they did * not * empirically examine the relationship between $ a $ and $ \\eta_0 $ , as we do in our paper . Instead , they demonstrated that they could use $ a $ to accurately predict the learning rates at which gradient descent initialized at $ \\theta_0 $ would escape that immediate neighborhood . That is , they analytically computed the maximum stable learning rate for ( asynchronous ) gradient descent on a quadratic function with sharpness $ a $ , and they plotted this on the left side of Figure 4 . Then they empirically found the maximum learning rate at which gradient descent initialized at $ \\theta_0 $ would remain in the neighborhood of $ \\theta_0 $ ( rather than immediately escaping ) , and they plotted this on the right side of Figure 4 . The point of this experiment was to demonstrate , both theoretically ( left ) and empirically ( right ) , the dependence of gradient descent 's stability properties on both the momentum parameter $ m $ and the time lag $ \\tau $ . > Especially , the jargon `` Edge of stability '' was first used in [ 1 ] , and the authors even did not mention it . As we will clarify in the revision , our jargon `` Edge of Stability '' is indeed inspired by Giladi et al ( 2019 ) . That paper used this term to refer to the well-known fact that for gradient descent , there is a step size threshold for which step sizes over this threshold will trigger ( local ) divergence . However , just to reiterate , their paper did not suggest or demonstrate our main contribution : that , no matter the step size , gradient descent always eventually navigates to a region where _that step size_ is on the edge of stability ."}, "2": {"review_id": "jh-rTtvkGeM-2", "review_text": "This paper presents an interesting observation for GD . That is , the sharpness of the learnt model in the final phase of the training ( measured by the largest eigenvalue of the training loss Hessian ) hovers right at the value 2/\\eta while the training loss . At the same time , the loss goes to unstable and non-monotonically decreasing . This pattern is consistent across architecture , activation functions , tasks , loss functions and BN . Comprehensive experiments are conducted to show this common observation . The paper is easy to follow . Besides the empirical results in the main body , authors give insightful discussions in Intro and related work section . Specifically , authors propose a novel guess , that GD eventually transitions to \u201c Edge of stability \u201d , where GD can finally succeed with non-small enough step size . Although I am not sure how GD can do this , the concept of \u201c Edge of stability \u201d is still attractive . I have two concerns for this work . 1 ) Authors did not investigate why sharpness finally hover over 2/\\eta . Is it a trivial consequence followed by some relationship between the update rule of GD and the definition of sharpness , without any condition ? Even if yes , we may further think about how to leverage it along the existing discussions in this paper . Hope to have authors ' feedbacks on this later . 2 ) Given people use SGD to train neural networks , discussions about the insight from the observation of GD to SGD will enhance the impact of this paper .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your helpful feedback ! We don \u2019 t have a rigorous mathematical proof for why the sharpness hovers just above $ 2/\\eta $ , but we do have a partial explanation which we believe makes intuitive sense : 1 . On the one hand , we empirically observe that in neural network training , the sharpness always \u201c wants to \u201d increase ( progressive sharpening ) . 2.On the other hand , gradient descent with step size $ \\eta $ on a quadratic function will * provably * diverge if the sharpness is greater than $ 2 / \\eta $ . In part # 2 of this response ( a separate comment ) , we show why this is true for _one-dimensional_ quadratic functions ; Appendix B of our paper covers the more general case of a multidimensional quadratic function . Now , neural network training objectives are not globally quadratic , but around any iterate one can make a second-order Taylor approximation to get a * local * quadratic approximation . If the sharpness at some iterate is greater than $ 2/\\eta $ , then gradient descent would provably diverge if run on the quadratic Taylor approximation around that iterate . Therefore , to the extent that gradient descent on the real training objective resembles gradient descent on the quadratic Taylor approximation , gradient descent will be unstable in neighborhoods where the sharpness exceeds $ 2/\\eta $ . In summary , on the one hand , gradient descent always \u201c wants to \u201d increase the sharpness ; but on the other hand , gradient descent can not linger for long in any neighborhood where the sharpness exceeds $ 2/\\eta $ . Thus , it makes sense that gradient descent spends its time in regions where the sharpness is approximately equal to the maximum stable sharpness , $ 2/\\eta $ . Indeed , one can non-rigorously argue that by process of elimination , this is the only possible outcome : the sharpness couldn \u2019 t be consistently much greater than $ 2/\\eta $ , because gradient descent would leave the neighborhood , yet the sharpness couldn \u2019 t be consistently less than $ 2/\\eta $ , because the sharpness would increase . A question you might have is : if gradient descent on quadratic functions diverges whenever the sharpness is greater than $ 2/\\eta $ , how can it be that in Figure 1 ( and many other figures ) the sharpness is consistently a little bit bigger than $ 2/\\eta $ ? The answer likely lies in the fact that on quadratic functions , the * speed * of divergence depends on the margin between the sharpness and the value $ 2/\\eta $ . If the sharpness exceeds $ 2/\\eta $ by a lot , then this divergence will be fast , but if the sharpness exceeds $ 2/\\eta $ by a little , then this divergence will be slow . So , if you run gradient descent in some region of the loss landscape in where the sharpness is just a little bit greater than $ 2/\\eta $ , then yes it \u2019 s true that gradient descent would * eventually * diverge if run on the quadratic Taylor approximation around that point , but this divergence would occur slowly \u2014 slower than the speed at which the quadratic Taylor approximation is itself changing . You can view the momentum experiments in Figure 5 as further confirmation of our hypothesis that the reason why the sharpness hovers at $ 2/\\eta $ during vanilla gradient descent is because $ 2/\\eta $ is the maximum stable sharpness of gradient descent on quadratic functions . For momentum gradient descent ( with either Polyak or Nesterov momentum ) , the maximum stable sharpness is given not by $ 2/\\eta $ but by a more complicated expression that is printed analytically in Equation ( 1 ) and plotted as the dashed horizontal lines in Figure 5 . Sure enough , during momentum gradient descent , the sharpness hovers just above these values . It is true that we do not provide a rigorous mathematical proof for why the sharpness hovers just above $ 2/\\eta $ , nor do we have an explanation for why progressive sharpening occurs . Rather , one major goal of our paper is to bring these phenomena to the attention of the ML theory / optimization theory communities so that future work can make our observations mathematically ironclad . As we write in the conclusion : \u201c We think that understanding _precisely_ [ emphasis new ] how gradient descent is able to succeed at the Edge of Stability should be viewed as an important open problem in optimization theory. \u201d We are eager to hear any more questions or comments that you have ."}, "3": {"review_id": "jh-rTtvkGeM-3", "review_text": "Summary of results : This empirical paper finds that deep neural network `` sharpness '' ( as measured by the top eigenvalue of the Hessian ) tends to saturate at or hover just above the value 2/\\eta , where \\eta is the step size in gradient descent ( GD ) , during the course of optimization . This is accompanied by non-monotonicity in the loss . ( Results also hold for gradient descent with momentum . ) This phenomena occurs for full-batch GD , a variety of tasks , and two different loss functions ( square loss and cross-entropy , although in the latter case late-time dynamics of the sharpness is different , with the sharpness decreasing ) . The result also holds across a variety of architectures ( VGG-11 , with and without batch norm , convolutional networks and fully-connected networks with different nonlinearities , a deep linear network , a Transformer model -- although I comment on the architecture dependence below . ) The authors refer to this phenomenon of sharpness hovering at or above the 2/\\eta bound as optimization on the `` edge of stability . '' The authors posit that this observation goes against our current understanding of optimization in deep learning : ( i ) theoretical work may use assumptions such as monotonic descent or L-smoothness , which is in violation of the `` edge of stability '' regime where the loss behavior is not monotonic ; ( ii ) relatedly , it corresponds to a regime of instability in a quadratic Taylor approximation , so this is not a good assumption either . The empirical observations do not appear to straightway carry over to SGD ( although there is some discussion of similarities in the appendix ) -- the authors leave this is as an open problem . Quality and clarity : The work is of good quality . The experiments that are presented clearly demonstrate the described phenomenon , attempt to sample from a variety of problems ( ranging from a simple 1D regression task to training VGG-11 on CIFAR-10 in Fig.1 ) , and are well-explained . Originality : The observed phenomena ( of sharpness progressively increasing and then hovering at or near 2/\\eta through much of GD dynamics ) appears new to the best of my knowledge . ( However , prior papers have pointed out that GD optimization can stably proceed for learning rates above 2/\\eta and are accompanied with nonmonotonic loss : one example is Lewkowycz , et al. ( 2020 ) . ) Significance : I think the observations are somewhat important , although perhaps not as much as the authors seem to stress in writing ( e.g.in the abstract , `` Our results ... shed significant light on the dynamics of gradient descent with a fixed step size ... '' -- this paper focuses only on empirical observations rather than explanation of the mechanism behind the stability ) . The results are for full-batch GD which somewhat limits the applicability of the results to practice where SGD is more common . Nonetheless , I do think understanding how optimization is stabilized in this regime is an interesting problem . Other comments : I think that a key point not fully understood ( at least , as expressed in the writing ) or investigated in the experiments is the role of network width ( i.e.I 'm skeptical that the observations will be `` consistent across architectures '' as expressed in the opening paragraph in the Introduction if wider networks are also investigated empirically ) . This is a main reason for not giving the paper a higher score . In Appendix D , the authors try to reconcile the results with existing theory on infinite-width limits . Fig.11 ( c ) shows that across networks of varying width but trained at the same learning rate , wider networks end up with smaller values of sharpness at the end of training ( here , say that all experiments are stopped at the same value of the training loss ) . Hence if a narrower network saturates the 2/\\eta bound at late times in gradient descent , the wider networks will fall below this value . ( This is , of course , consistent with no evolution in the curvature in the infinite width limit . ) That is to say , whether or not the `` edge of stability '' regime is reached depends quite strongly on how wide the networks are . While some realistic networks are explored ( e.g.VGG-11 , * narrow * Transformers ) , which is a positive point , all of the remaining networks used in the experiments ( as far as I can gather ) are on the narrow side ( e.g.layer widths of order 100 or 200 ) . I think a shortcoming of the paper is that the strong width ( i.e.architecture ) dependence of the phenomenon is not fully appreciated or discussed by the authors ( e.g.for instance , by discussing in the main text that it only sets in for narrow networks ) or investigated empirically . ( Alternatively , noting for readers that all the networks chosen are rather narrow , out of transparency . ) I note that the authors do mention in Appendix D : `` ... one might hypothesize that progressive sharpening might attenuate as networks ( with NTK parameterization ) become increasingly wide . '' However , I do n't think that NTK parameterization is necessary for this to be true . In short , I think the width dependence of the phenomenon is an important factor that affects the significance and applicability of the observations and could have been treated with greater transparency ( with additional experiments and additions to the main text and abstract ) . A comment on relation to prior work : the authors write that Lewkowycz , et al . ( 2020 ) imply that `` actual progress would occur in regions where the sharpness remains strictly less than 2/\\eta . Our experiments demonstrate otherwise . '' I do n't believe this is a conclusion of that paper ( progress happens when sharpness is above 2/\\eta ) . ( Note also that the paper tends to study wide networks . ) Could the authors elaborate on what they mean here ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "Here , we respond to your points regarding Lewkowycz , et al . ( 2020 ) . > A comment on relation to prior work : the authors write that Lewkowycz , et al . ( 2020 ) imply that `` actual progress would occur in regions where the sharpness remains strictly less than 2/\\eta . Our experiments demonstrate otherwise . '' I do n't believe this is a conclusion of that paper ( progress happens when sharpness is above 2/\\eta ) . ( Note also that the paper tends to study wide networks . ) Could the authors elaborate on what they mean here ? It is true that Lewkowycz , et al . ( 2020 ) exhibits a stage of training in which the sharpness is greater than $ 2 / \\eta $ , and where the training loss behaves non-monotonically . However , in Lewkowycz , et al . ( 2020 ) this \u201c catapult \u201d stage of training is understood to be temporary and fleeting , and actual optimization ( in the sense of the loss consistently going down ) is only supposed to commence after this \u201c catapult \u201d stage is definitively over . By contrast , our paper details how the sharpness is $ \\approx 2 / \\eta $ ( and the loss is non-monotonic ) for the bulk of training notably , at the same time as the loss is consistently decreasing . In more detail , Lewkowycz , et al . ( 2020 ) examined what happens when the step size $ \\eta $ is set greater than $ 2 /\\lambda_0 $ , where $ \\lambda_0 $ is the initial sharpness . In this scenario , at the beginning of training , gradient descent oscillates unstably along the leading Hessian eigenvector until it is flung from its initial position . What Lewkowycz , et al . ( 2020 ) showed is that , rather than diverge entirely , on some architectures gradient descent will \u201c land on its feet \u201d ( our words ) in a new region that is flat enough to accommodate the step size , i.e.a region where the sharpness is $ < 2 /\\eta $ . To quote from that paper : if $ \\eta > 2/\\lambda_0 $ , then > optimization begins with a period of exponential growth in the loss , coupled with a rapid decrease in the curvature [ sharpness ] , until curvature * * stabilizes at a value $ \\lambda_ { \\text { final } } < 2 / \\eta $ . Once the curvature drops below $ 2 / \\eta $ , training converges * * \u2026 Note that the new sharpness is called $ \\lambda_ { \\text { final } } $ , indicating that once gradient descent finds a region where the sharpness is $ < 2 /\\eta $ , the sharpness is supposed to stay there for the remainder of training . Indeed , this is stated explicitly in their section 4.1 : > one striking prediction of the model is that after a period of excursion , the logic differences settle back to O ( 1 ) values , the * * NTK stops changing * * [ emphasis ours ] , and evolution is again well approximated by a linear model with constant kernel at large with . Notice also that they write : \u201c once the curvature drops below $ 2 / \\eta $ , training converges. \u201d This is what we meant when we wrote that \u201c actual progress would occur in regions where the sharpness remains strictly less than $ 2 / \\eta $ . \u201d Their overall story is illustrated by their Figure 2 ( a ) and 2 ( b ) . In Figure 2 ( b ) , which plots the sharpness , we can see that after an initial drop ( the catapult ) , the sharpness ceases to change for the remainder of training . In Figure 2 ( a ) , which plots the train loss , we can see that after an initial spike ( the catapult ) , the train loss monotonically decreases for the remainder of training . To summarize , the overall story in Lewkowycz , et al . ( 2020 ) is fully consistent with the conventional local-quadratic view of optimization . In their paper , the main function of the large learning rate is to trigger a switch from a sharp initialization to a flat initialization . After this initialization switch , the conventional optimization wisdom concerning step sizes is supposed to kick in again . -- > However , prior papers have pointed out that GD optimization can stably proceed for learning rates above 2/\\eta and are accompanied with nonmonotonic loss : one example is Lewkowycz , et al . ( 2020 ) .We are not aware of any such papers apart from Lewkowycz , et al . ( 2020 ) ( which we discussed above ) , as well Xing et al . ( 2018 ) [ 1 ] , which we discussed in our Related Work section . Xing et al . ( 2018 ) empirically studied full-batch gradient descent , and noted that the train loss eventually starts behaving non-monotonically . However , that paper didn \u2019 t relate this phenomenon to dynamics of the sharpness , or discuss implications for optimization theory . If there are other papers which observe either of these two effects , we are happy to add the appropriate citations . [ 1 ] Chen Xing , Devansh Arpit , Christos Tsirigotis , and Yoshua Bengio . A walk with sgd ."}}