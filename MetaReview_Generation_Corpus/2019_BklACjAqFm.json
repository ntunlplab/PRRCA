{"year": "2019", "forum": "BklACjAqFm", "title": "Successor Uncertainties: exploration and uncertainty in temporal difference learning", "decision": "Reject", "meta_review": "Pros:\n- interesting algorithmic idea for using successor features to propagate uncertainty for use in epxloration\n- clarity\n\nCons:\n- moderate novelty\n- initially only simplistic experiments (later complemented with Atari results)\n- initially missing baseline comparisons\n- no regret-based analysis\n- questionable soundness because uncertainty is not guaranteed to go down\n\nAll the reviewers found the initial submission to be insufficient for acceptance, and the one reviewer who read the rebuttal/revision did not change their mind, despite the addition of some large-scale results (Atari).", "reviews": [{"review_id": "BklACjAqFm-0", "review_text": "Summary/contribution: This paper focuses on the problem of incorporating uncertainty into RL. The primary contribution is exploring the use of successor features for uncertainty prediction over Q-values. The proposed approach builds on O\u2019Donoghue et al. (2018). The authors provide experiments that demonstrate improved performance on a chain MDP environment and a Tree environment. Pros: - I found this paper to be above average in terms of clarity. Cons: - The experiments evaluation is restricted to simplistic environments. The authors make an argument for why using successor features would be more \"stable\", but I found the experimental evidence to support this claim to be underwhelming. Justification for rating: This paper does a good job of articulating an interesting approach to the exploration problem using successor representations. In the current form however, it is really lacking in experimental evidence to support the main claims/contributions. Currently the domains considered are somewhat toy which I do not find convincing enough to demonstrate the effectiveness of their approach. Other: - I would appreciate a discussion on the relationship to Machado et al. 2018 which explored count based exploration with successor representations. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear AnonReviewer1 , We \u2019 ve posted a revised version of the paper and a rebuttal . Specifically we \u2019 ve now got a stronger set of experiments , and a stronger argument for the reasons why our method may be preferred to O \u2019 Donoghue et al . ( 2018 ) .Have you had a chance to have a look at these ? If so , might these influence your rating of our paper ?"}, {"review_id": "BklACjAqFm-1", "review_text": "The paper proposes an exploration approach (either based on posterior sampling or optimism) based on successor features representation. A high probability ellipsoid confidence set (defined by the Gram matrix \\Sigma_t) is estimated based on linear regression (using some features \\phi) to the immediate reward function. Now for any policy \\pi, a confidence interval for the Q-value of any policy \\pi can be derived by application of the \\psi transformation, where \\psi = expected sum of discounted future \\phi under \\pi. The algorithm selects action by posterior sampling (or UCB) in the \\psi-space. It would be interesting to see if this approach would converge to a good policy, maybe by doing a regret-based analysis. Unfortunately there is no such analysis in the paper. However my main complaint is the soundness of the approach, for two reasons: - First it is not clear that the uncertainty in the Q-values decreases with time. Indeed the uncertainty on Q^{\\pi}(s,a) corresponds to the width of the confidence ellipsoid in the direction of the successor features \\psi^{\\pi}(s,a). However, although we know that the uncertainty shrinks in the directions of the features \\phi_t (when action a_t is chosen in state s_t) because we do regression of the reward function, we do not have the same property for \\psi_t, which defines the Q-function. And it is not obvious that the confidence set in the direction of \\psi_t would shrink at all. Thus it could be the case that the uncertainty on the Q-values will never decrease. - Second, since the successor features are learnt on-policy, the uncertainty on the Q-values (assuming we can estimate them) corresponds to a mixture of the policies which have been used in the past, but not to the policy that will be used from there on, because the policy is non stationary (since the uncertainty decreases as more information is collected). I would recommend to be very careful when defining and using the successor features by emphasizing the policy under which those features are defined. So in the end the contribution is mainly algorithmic. However I find it hard to say anything about the proposed approach, whether it improves over previous ones or not, specially because the experiments are limited to toy problems. Theoretical analysis or more complex experiments would make the paper stronger. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear AnonReviewer3 , Thank you for your insightful comments . We have used these , and those of the other reviewers , to significantly improve the paper . We \u2019 ve included references in our rebuttal to each specific change we made . We now have a much stronger and better analysed set of experiments . Might you have a moment to look over these and let us know if this changes the rating you believe our paper deserves ?"}, {"review_id": "BklACjAqFm-2", "review_text": "This paper tackles the classical exploration / exploitation problem in reinforcement learning. The paper argues that it is necessary to propagate uncertainty correctly and argue that they can do so using the successor representation to compute the Bayesian posterior over Q-values conditioned on the data already observed. Novelty: This work is similar to \u201cThe uncertainty bellman equation\u201d (UBE) (O\u2019Donoghue et al. 2018) which adds a head to a regular DQN agent to predict a function u which is an upper bound of the variance of the posterior distribution over Q-values. The difference here is that the successor features are used here to predict Q-values and the function u. The successor features can be seen as a discounted state occupancy of the current policy and carry information about the future. While the relation with the UBE is highlighted in section 4.6 an empirical evaluation between the two methods would also be needed. Clarity: The method is detailed comparing to contextual bandit methods, the authors then argue that applying directly these methods to reinforcement learning case does not propagate uncertainty over several timesteps. While this indeed true it is misleading and imply that propagating uncertainty is not considered by current exploration methods in RL. Soundness: The method presented here is relatively reductive. The estimated posterior over the value function is correct only if the transition model P is already known, otherwise Equation 4 would also need to incorporate uncertainty over P. Similarly, as the authors point out, this doesn\u2019t include the max operation. At best, we are learning a posterior over the value function for a fixed policy, for when only the reward is unknown. As a whole, the authors argue that their method allows a better propagation of Q-values uncertainty but provide little theoretical or experimental evidence that would back this claim. From a deep RL perspective, the features \\phi^l only carry local information. The authors argue that this leads to more stable features as these feature do not depend on the current policy. However it also means that in a sparse reward setting the reward observed would be zero most of the time and no useful features would be learned. In practice methods using the successor representation usually share parts of the network with other tasks to improve representation learning (see e.g. Figure 1 of Machado et al., Eigenoption discovery through the deep successor representation, 2017, & also their 2018 paper). Experiments: The experiment are disappointing as they are only limited to tabular and deterministic problems. An obvious missing comparison is to UBE, at the minimum; and other \u201cdeep\u201d algorithms such as BDQN, Bootstrap DQN, etc. Some of these algorithms have been shown to perform well on the Atari benchmark, and that seems like a reasonable point of comparison also. The method also relies on knowing the successor features. While they can be learned easily in a tabular, deterministic MDP it is not clear how the posterior would behave in larger and/or non stochastic domains when it takes more time to learn these successor features. Overall, I am not sure what I learned from reading this paper. While the idea of using the successor representation in exploration is interesting and has been considered recently, the method presented in this paper needs to be better justified and evaluated on more challenging tasks. Minor comments I would like to see a proof of Equation 4, which may be simple but is not immediate. Some papers of relevance here: An analysis of model-based Interval Estimation for Markov Decision Processes, Strehl & Littman (2008) (More) efficient reinforcement learning via posterior sampling. Osband et al (2013) Count-Based Exploration with the Successor Representation, Machado et al. (2018) ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear AnonReviewer2 , We \u2019 ve updated the paper , incorporating a great deal of your feedback and improving the experiments . To make it easy to check the changes , our rebuttal contains references to the sections where we address each point . When you have a moment to look over those changes , could you please let us know if these influence your rating of our paper ?"}], "0": {"review_id": "BklACjAqFm-0", "review_text": "Summary/contribution: This paper focuses on the problem of incorporating uncertainty into RL. The primary contribution is exploring the use of successor features for uncertainty prediction over Q-values. The proposed approach builds on O\u2019Donoghue et al. (2018). The authors provide experiments that demonstrate improved performance on a chain MDP environment and a Tree environment. Pros: - I found this paper to be above average in terms of clarity. Cons: - The experiments evaluation is restricted to simplistic environments. The authors make an argument for why using successor features would be more \"stable\", but I found the experimental evidence to support this claim to be underwhelming. Justification for rating: This paper does a good job of articulating an interesting approach to the exploration problem using successor representations. In the current form however, it is really lacking in experimental evidence to support the main claims/contributions. Currently the domains considered are somewhat toy which I do not find convincing enough to demonstrate the effectiveness of their approach. Other: - I would appreciate a discussion on the relationship to Machado et al. 2018 which explored count based exploration with successor representations. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear AnonReviewer1 , We \u2019 ve posted a revised version of the paper and a rebuttal . Specifically we \u2019 ve now got a stronger set of experiments , and a stronger argument for the reasons why our method may be preferred to O \u2019 Donoghue et al . ( 2018 ) .Have you had a chance to have a look at these ? If so , might these influence your rating of our paper ?"}, "1": {"review_id": "BklACjAqFm-1", "review_text": "The paper proposes an exploration approach (either based on posterior sampling or optimism) based on successor features representation. A high probability ellipsoid confidence set (defined by the Gram matrix \\Sigma_t) is estimated based on linear regression (using some features \\phi) to the immediate reward function. Now for any policy \\pi, a confidence interval for the Q-value of any policy \\pi can be derived by application of the \\psi transformation, where \\psi = expected sum of discounted future \\phi under \\pi. The algorithm selects action by posterior sampling (or UCB) in the \\psi-space. It would be interesting to see if this approach would converge to a good policy, maybe by doing a regret-based analysis. Unfortunately there is no such analysis in the paper. However my main complaint is the soundness of the approach, for two reasons: - First it is not clear that the uncertainty in the Q-values decreases with time. Indeed the uncertainty on Q^{\\pi}(s,a) corresponds to the width of the confidence ellipsoid in the direction of the successor features \\psi^{\\pi}(s,a). However, although we know that the uncertainty shrinks in the directions of the features \\phi_t (when action a_t is chosen in state s_t) because we do regression of the reward function, we do not have the same property for \\psi_t, which defines the Q-function. And it is not obvious that the confidence set in the direction of \\psi_t would shrink at all. Thus it could be the case that the uncertainty on the Q-values will never decrease. - Second, since the successor features are learnt on-policy, the uncertainty on the Q-values (assuming we can estimate them) corresponds to a mixture of the policies which have been used in the past, but not to the policy that will be used from there on, because the policy is non stationary (since the uncertainty decreases as more information is collected). I would recommend to be very careful when defining and using the successor features by emphasizing the policy under which those features are defined. So in the end the contribution is mainly algorithmic. However I find it hard to say anything about the proposed approach, whether it improves over previous ones or not, specially because the experiments are limited to toy problems. Theoretical analysis or more complex experiments would make the paper stronger. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear AnonReviewer3 , Thank you for your insightful comments . We have used these , and those of the other reviewers , to significantly improve the paper . We \u2019 ve included references in our rebuttal to each specific change we made . We now have a much stronger and better analysed set of experiments . Might you have a moment to look over these and let us know if this changes the rating you believe our paper deserves ?"}, "2": {"review_id": "BklACjAqFm-2", "review_text": "This paper tackles the classical exploration / exploitation problem in reinforcement learning. The paper argues that it is necessary to propagate uncertainty correctly and argue that they can do so using the successor representation to compute the Bayesian posterior over Q-values conditioned on the data already observed. Novelty: This work is similar to \u201cThe uncertainty bellman equation\u201d (UBE) (O\u2019Donoghue et al. 2018) which adds a head to a regular DQN agent to predict a function u which is an upper bound of the variance of the posterior distribution over Q-values. The difference here is that the successor features are used here to predict Q-values and the function u. The successor features can be seen as a discounted state occupancy of the current policy and carry information about the future. While the relation with the UBE is highlighted in section 4.6 an empirical evaluation between the two methods would also be needed. Clarity: The method is detailed comparing to contextual bandit methods, the authors then argue that applying directly these methods to reinforcement learning case does not propagate uncertainty over several timesteps. While this indeed true it is misleading and imply that propagating uncertainty is not considered by current exploration methods in RL. Soundness: The method presented here is relatively reductive. The estimated posterior over the value function is correct only if the transition model P is already known, otherwise Equation 4 would also need to incorporate uncertainty over P. Similarly, as the authors point out, this doesn\u2019t include the max operation. At best, we are learning a posterior over the value function for a fixed policy, for when only the reward is unknown. As a whole, the authors argue that their method allows a better propagation of Q-values uncertainty but provide little theoretical or experimental evidence that would back this claim. From a deep RL perspective, the features \\phi^l only carry local information. The authors argue that this leads to more stable features as these feature do not depend on the current policy. However it also means that in a sparse reward setting the reward observed would be zero most of the time and no useful features would be learned. In practice methods using the successor representation usually share parts of the network with other tasks to improve representation learning (see e.g. Figure 1 of Machado et al., Eigenoption discovery through the deep successor representation, 2017, & also their 2018 paper). Experiments: The experiment are disappointing as they are only limited to tabular and deterministic problems. An obvious missing comparison is to UBE, at the minimum; and other \u201cdeep\u201d algorithms such as BDQN, Bootstrap DQN, etc. Some of these algorithms have been shown to perform well on the Atari benchmark, and that seems like a reasonable point of comparison also. The method also relies on knowing the successor features. While they can be learned easily in a tabular, deterministic MDP it is not clear how the posterior would behave in larger and/or non stochastic domains when it takes more time to learn these successor features. Overall, I am not sure what I learned from reading this paper. While the idea of using the successor representation in exploration is interesting and has been considered recently, the method presented in this paper needs to be better justified and evaluated on more challenging tasks. Minor comments I would like to see a proof of Equation 4, which may be simple but is not immediate. Some papers of relevance here: An analysis of model-based Interval Estimation for Markov Decision Processes, Strehl & Littman (2008) (More) efficient reinforcement learning via posterior sampling. Osband et al (2013) Count-Based Exploration with the Successor Representation, Machado et al. (2018) ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Dear AnonReviewer2 , We \u2019 ve updated the paper , incorporating a great deal of your feedback and improving the experiments . To make it easy to check the changes , our rebuttal contains references to the sections where we address each point . When you have a moment to look over those changes , could you please let us know if these influence your rating of our paper ?"}}