{"year": "2018", "forum": "SJx9GQb0-", "title": "Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect", "decision": "Accept (Poster)", "meta_review": "The paper proposes various improvements to Wasserstein distance based GAN training. Reviewers agree that the method produces good quality samples and are impressed by the state of the art results in several semi-supervised learning benchmarks. The paper is well written and the authors have further improved the empirical analysis in the paper based on reviewer comments.", "reviews": [{"review_id": "SJx9GQb0--0", "review_text": "Updates: thanks for the authors' hard rebuttal work, which addressed some of my problems/concerns. But still, without the analysis of the temporal ensembling trick [Samuli & Timo, 2017] and data augmentation, it is difficult to figure out the real effectiveness of the proposed GAN. I would insist my previous argument and score. Original review: ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- This paper presented an improved approach for training WGANs, by applying some Lipschitz constraint close to the real manifold in the pixel level. The framework can also be integrated to boost the SSL performances. In experiments, the generated data showed very good qualities, measured by inception score. Meanwhile, the SSL-GANs results were impressive on MNIST and CIFAR-10, demonstrating its effectiveness. However, the paper has the following weakness: Missing citations: the most related work of this one is the DRAGAN work. However, it did not cite it. I think the author should cite it, make a clear justification for the comparison and emphasize the main contribution of the method. Also, it suggested that the paper should discuss its relation to other important work, [Arjovsky & Bottou 2017], [Wu et al. 2016]. Experiments: as for the experimental part, it is not solid. Firstly, although the SSL results are very good, it is guaranteed the proposed GAN is good [Dai & Almahairi, et al. 2017]. Secondly, the paper missed several details, such as settings, model configuration, hyper-parameters, making it is difficult to justify which part of the model works. Since the paper using the temporal ensembling trick [Samuli & Timo, 2017], most of the gain might be from there. Data augmentation might also help to improve. Finally, except CIFAR-10, it is better to evaluate it on more datasets. Given the above reason, I think this paper is not ready to be published in ICLR. The author can submit it to the workshop and prepare for next conference. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We are pleased to see that the reviewer thinks our `` generated data showed very good qualities '' and `` the SSL-GANs results were impressive '' . =Q1= `` Missing citations : the most related work of this one is the DRAGAN '' We would consider WGAN and WGAN-GP as the most related works to ours and DRAGAN ranks after them . As a matter of fact , DRAGAN is an unpublished work and has not been peer-reviewed . As another matter of fact , the gradient penalty in DRAGAN is the same as in WGAN-GP except that it is imposed around the real data while WGAN-GP applies it to the points sampled between the real and the generated ones . Next , we highlight some key differences between DRAGAN and ours . We propose to improve Wasserstein GAN , while DRAGAN works with GAN . DRAGAN aims to reduce the non-optimal saddle points in the minmax two-player training of GANs . In contrast , we propose an approach to enforcing the 1-Lipschitz continuity over the critic of WGANs . One of our key observations is that it blurs the generated samples if we add noise directly to the data points , as done in DRAGAN . Instead , we perturb the hidden layers of the discriminator . DRAGAN perturbs a data point once while we do it twice in each iteration . After the perturbations , DRAGAN penalizes the gradients while we enforce the consistency of the outputs . One of the most distinct features of our approach is that it seamlessly integrates the semi-supervised learning method by Laine & Aila ( 2016 ) with GANs . =Q2= `` the paper should discuss ... [ Arjovsky & Bottou 2017 ] , [ Wu et al.2016 ] '' We had included both in our paper . Arjovsky & Bottou 2017 analyzes some distribution divergences and their effects in training GANs . Wu et al.2016 propose to quantitatively evaluate the decoder-based generative models by annealed importance sampling . In our paper , we focus on a different subject , i.e. , to design an algorithmic solution to the difficulty of training GANs . =Q3= `` the paper missed several details '' Please see either Appendices A and B of the revised paper or the following for our answer to this question . Given the context of the question , we believe it is about SSL . We follow the experiment setups in the prior works so that our results are directly comparable to theirs . Please see below for more details . If you are interested , you may also check out our code : https : //github.com/biuyq/CT-GAN/blob/master/CT-GANs/Theano_classifier/CT_CIFAR-10_TE.py . MNIST : There are 60,000 images in total . We randomly choose 10 data points for each digit as the labeled set . No data augmentation is used . CIFAR-10 : There are 50,000 image in total . We randomly choose 400 images for each class as the labeled set . We augment the data by horizontally flipping the images and randomly translating the images within [ -2,2 ] pixels . No ZCA whitening is used . Model Configurations : We had included them in the appendix . Hyper-parameters : We set lambda = 1.0 in Eq . ( 7 ) in all our experiments . For CIFAR-10 , the number of training epochs is set to 1,000 with a constant learning rate of 0.0003 . For MNIST , the number of training epochs is set to 300 with a constant learning rate of 0.003 . The other hyper-parameters are exactly the same as in the improved GAN ( Salimans et al. , 2016 ) . =Q4= `` ... which part of the model works '' Please see either Appendix C of the revised paper or the following for our answer to this question . We have done some ablation studies about our semi-supervised learning approach on CIFAR-10 . Method , Error w/o CT , 15.0 w/o GAN ( note 1 ) , 12.0 w batch norm ( note 2 ) , -- w/o D_ , 10.7 OURS , 10.0 Note 1 : This almost reduces to TE ( Laine & Aila , 2016 ) . All the settings here are the same as TE except that we use the extra regularization ( $ D\\_ ( . , . ) $ in CT ) over the second-to-last layer . Note 2 : We use the weight normalization as in ( Salimans et al. , 2016 ) , which becomes a core constituent of our approach . The batch normalization would actually invalidate the feature matching in ( Salimans et al. , 2016 ) . We can see that both GAN and the temporal ensembling effectively contribute to our final results . The results without our consistency regularization ( w/o CT ) drop more than those without GAN . We are running the experiments without any data augmentation and will include the corresponding results in the paper . =Q5= `` ... it is better to evaluate it on more datasets '' We have run some new experiments on the SVHN dataset . Ours is the best among all the GAN based semi-supervised learning methods , and is on par with the state of the arts . Method , Error PI Laine & Aila 2016 , 4.8 TE Laine & Aila 2016 , 4.4 Tarvainen & Valpola 2017 , 4.0 Miyato et al.2017 , 3.86 Salimans et al.2016 , 8.1 Dumoulin et al.2016 , 7.4 Kumar et al.2017 , 5.9 Ours , 4.2"}, {"review_id": "SJx9GQb0--1", "review_text": "Summary: The paper proposes a new regularizer for wgans, to be combined with the traditional gradient penalty. The theoretical motivation is bleak, and the analysis contains some important mistakes. The results are very good, as noticed by the comments, the fact that the method is also less susceptible to overfitting is also an important result, though this might be purely due to dropout. One of the main problems is that the largest dataset used is CIFAR, which is small. Experiments on something like bedrooms or imagenet would make the paper much stronger. If the authors fix the theoretical analysis and add evidence in a larger dataset I will raise the score. Detailed comments: - The motivation of 1.2 and the sentence \"Arguably, it is fairly safe to limit our scope to the manifold that supports the real data distribution P_r and its surrounding regions\" are incredibly wrong. First of all, it should be noted that the duality uses 1-Lip in the entire space between Pr and Pg, not in Pr alone. If the manifolds are not extremely close (such as in the beginning of training), then the discriminator can be almost exactly 1 in the real data, and 0 on the fake. Thus the discriminator would be almost exactly constant (0-Lip) near the real manifold, but will fail to be 1-lip in the decision boundary, this is where interpolations fix this issue. See Figure 2 of the wgan paper for example, in this simple example an almost perfect discriminator would have almost 0 penalty. - In the 'Potential caveats' section, the implication that 1-Lip may not be enforced in non-examined samples is checkable by an easy experiment, which is to look for samples that have gradients of the critic wrt the input with norm > 1. I performed the exp in figure 8 and saw that by taking a slightly higher lambda, one reaches gradients that are as close to 1 as with ct-gan. Since ct-gan uses an extra regularizer, I think the authors need some stronger evidence to support the claim that ct-gan better battles this 'potential caveat'. - It's important to realize that the CT regularizer with M' = 1 (1-Lip constraint) will only be positive for an almost 1-Lip function if x and x' are sampled when x - x' has a very similar direction than the gradient at x. This is very hard in high dimensional spaces, and when I implemented a CT regularizer indeed the ration of eq (4) was quite less than the norm of the gradient. It would be useful to plot the value of the CT regularizer (the eq 4 version) as the training iterations progresses. Thus the CT regularizer works as an overall Lipschitz penalty, as opposed to penalizing having more than 1 for the Lipschitz constant. This difference is non-trivial and should be discussed. - Line 11 of the algorithm is missing L^(i) inside the sum. - One shouldn't use MNIST for anything else than deliberately testing an overfitting problem. Figure 4 is thus relevant, but the semi-supervised results of MNIST or the sample quality experiments give hardly any evidence to support the method. - The overfitting result is very important, but one should disambiguate this from being due to dropout. Comparing with wgangp + dropout is thus important in this experiment. - The authors should provide experiments in at least one larger dataset like bedrooms or imagenet (not faces, which is known to be very easy). This would strengthen the paper quite a bit.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the insightful comments and suggestions ! The paper has been revised accordingly . Next , we answer the questions in detail . == Q1 : The motivation == We acknowledge that the duality uses 1-Lipschitz continuity in the entire space between Pr and Pg , and it is impossible to visit everywhere of the space in the experiments . We instead focus on the region around the real data manifold to complement the region checked by GP-WGAN -- - the gradient penalty term is kept in our overall approach . We have clarified this point by the following in the revised paper . Arguably , it is fairly safe to limit our scope to the manifold that supports the real data distribution $ \\mathbb { P } _r $ and its surrounding regions mainly for two reasons . First , we keep the gradient penalty term and improve it by the proposed consistency term in our overall approach . While the former enforces the continuity over the points sampled between the real and generated points , the latter complement the former by focusing on the region around the real data manifold instead . Second , the distribution of the generative model $ \\mathbb { P } _G $ is virtually desired to be as close as possible to $ \\mathbb { P } _r $ . == Q2 : That 1-Lip may not be enforced in non-examined samples is checkable == The non-examined samples can refer to all the possible samples in the continuous space which can not be traversed in a discrete manner . Figure 8 plots the norm of the gradients ( of the critic with respect to the input ) over the real data points only . In other words , Figure 8 is only part of the consequence , and certainly not the cause , of the discriminators trained by GP-WGAN and our CT-GAN , respectively . It is not surprising that the norms by CT-GAN are closer to 1 than by GP-WGAN because we explicitly enforce the continuity around the real data . We have run more experiments with larger \\lambda values in GP-GAN , and found the gradient norms can indeed reach those of CT-GAN when the \\lambda is four times larger than the original one used in the authors \u2019 code . However , the inception score on CIFAR-10 drops a little , and the overfitting remains . Stronger evidence ? In addition to the gradient norm , we have also examined the 1-Lipschitz continuity of the critic using the basic definition . For any two inputs x and x ' , the difference of the critic 's outputs should be no more than M * |x-x'| . This notion is captured by our CT term defined in eq . ( 4 ) .We plot the CT versus the training iterations as Figure 9 in the revised paper . In particular , for every 100 iterations , we randomly pick up 64 real examples and split them into two subsets of the same size . We compute d ( D ( x1 ) -D ( x2 ) ) /d ( x1-x2 ) for all the ( x1 , x2 ) pairs , where x1 is from the first subset and x2 is from the second . The maximum of d ( D ( x1 ) -D ( x2 ) ) /d ( x1-x2 ) is plotted in Figure 9 . We can see that the CT-GAN curve converges under a certain value much faster than GP-WGAN . == Q3 : Plot the value of the CT regularizer == Please see Figures 9 and 10 in the revised paper for the plots . Note that M \u2019 has absorbed the term d ( x \u2019 , x \u2019 \u2019 ) in the final consistency term ( eq . ( 5 ) ) , so we have to tune its value as opposed to fixing it to 1 . Also , because of this fact , we agree with the comment that \u201c Thus the CT regularizer works as an overall Lipschitz penalty , as opposed to penalizing having more than 1 for the Lipschitz constant. \u201d We will clarify this part in the final paper , if it is accepted . == Q4 : Line 11 == It is correct and is another way of denoting the gradient . == Q5 : MNIST == We understand your concern with the use of MNIST and appreciate that you agree the overfitting experiments ( Figure 4 ) are relevant . The other results ( e.g. , the generated samples and the test error in semi-supervised learning ) can give the readers a concrete understanding about our model , but we agree one should not use MNIST to compare different algorithms . == Q6 : GP-WGAN + Dropout == Please see Appendix E for the experimental results of GP-WGAN+Dropout on CIFAR-10 using 1000 training images . The corresponding inception score is better than GP-WGAN and yet still significantly lower than ours ( 2.98+-0.11 vs. 4.29+-0.12 vs. 5.13+-0.12 ) . Figure 12 , which is about the convergence curves of the discriminator cost over both training and testing sets , shows that dropout is indeed able to reduce the overfitting , but it is not as effective as ours . == Q7 : Experiments in larger datasets == In Appendix F of the revised paper , we present results on the ImageNet and LSUN bedroom datasets following the experiment setup of GP-WGAN . After 200,000 generator iterations on ImageNet , the inception score of CT-GAN is 10.27+-0.15 , whereas GP-WGAN 's is 9.85+-0.17 . Since there is only one class in LSUN bedroom , the inception score is not a proper evaluation metric for the experiments on this dataset . Visually , there is no clear difference between the generated samples of GP-WGAN and CT-GAN up to the 124,000th generator iteration ."}, {"review_id": "SJx9GQb0--2", "review_text": "This paper continues a trend of incremental improvements to Wasserstein GANs (WGAN), where the latter were proposed in order to alleviate the difficulties encountered in training GANs. Originally, Arjovsky et al. [1] argued that the Wasserstein distance was superior to many others typically used for GANs. An important feature of WGANs is the requirement for the discriminator to be 1-Lipschitz, which [1] achieved simply by clipping the network weights. Recently, Gulrajani et al. [2] proposed a gradient penalty \"encouraging\" the discriminator to be 1-Lipschitz. However, their approach estimated continuity on points between the generated and the real samples, and thus could fail to guarantee Lipschitz-ness at the early training stages. The paper under review overcomes this drawback by estimating the continuity on perturbations of the real samples. Together with various technical improvements, this leads to state-of-the-art practical performance both in terms of generated images and in semi-supervised learning. In terms of novelty, the paper provides one core conceptual idea followed by several tweaks aimed at improving the practical performance of GANs. The key conceptual idea is to perturb each data point twice and use a Lipschitz constant to bound the difference in the discriminator\u2019s response on the perturbed points. The proposed method is used in eq. (6) together with the gradient penalty from [2]. The authors found that directly perturbing the data with Gaussian noise led to inferior results and therefore propose to perturb the hidden layers using dropout. For supervised learning they demonstrate less overfitting for both MNIST and CIFAR 10. They also extend their framework to the semi-supervised setting of Salismans et al 2016 and report improved image generation. The authors do an excellent comparative job in presenting their experiments. They compare numerous techniques (e.g., Gaussian noise, dropout) and demonstrates the applicability of the approach for a wide range of tasks. They use several criteria to evaluate their performance (images, inception score, semi-supervised learning, overfitting, weight histogram) and compare against a wide range of competing papers. Where the paper could perhaps be slightly improved is writing clarity. In particular, the discussion of M and M' is vital to the point of the paper, but could be written in a more transparent manner. The same goes for the semi-supervised experiment details and the CIFAR-10 augmentation process. Finally, the title seems uninformative. Almost all progress is incremental, and the authors modestly give credit to both [1] and [2], but the title is neither memorable nor useful in expressing the novel idea. [1] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. [2] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the very positive and affirmative comments about our work . We also appreciate the suggestions for improving the writing clarify of the paper . The following has been incorporated in the revised paper . == M vs. M ' == We use the notation $ M $ in eq . ( 3 ) and a different $ M ' $ in eq . ( 4 ) to reflect the fact that the continuity will be checked only sparsely at some data points in practice . ... ... Note that , however , it becomes impossible to compute the distance $ d ( \\bm { x } ' , \\bm { x } '' ) $ between the two virtual data points . In this work , we assume it is bounded by a constant and absorb the constant to $ M ' $ . Accordingly , we tune $ M ' $ in our experiments to take account of this unknown constant ; the best results are obtained between $ M'=0 $ and $ M'=0.2 $ . == Semi-supervised experiment details and the CIFAR-10 augmentation process == MNIST : There are 60,000 images in total . We randomly choose 10 data points for each digit as the labeled set . No data augmentation is used . CIFAR-10 : There are 50,000 image in total . We randomly choose 400 images for each class as the labeled set . We augment the data by horizontally flipping the images and randomly translating the images within [ -2,2 ] pixels . No ZCA whitening is used . Model Configuration Table 1 : MNIST -- -- -- -- -- -- -- Classifier C | Generator G Input : Labels y , 28 * 28 Images x | Input : Noise 100 z Gaussian noise 0.3 , MLP 1000 , ReLU | MLP 500 , Softplus , Batch norm Gaussian noise 0.5 , MLP 500 , ReLU | MLP 500 , Softplus , Batch norm Gaussian noise 0.5 , MLP 250 , ReLU | MLP 784 , Sigmoid , Weight norm Gaussian noise 0.5 , MLP 250 , ReLU | Gaussian noise 0.5 , MLP 250 , ReLU | Gaussian noise 0.5 , MLP 10 , Softmax | Table 2 : CIFAR-10 -- -- -- -- -- -- -- -- - Input : Labels y , 32 * 32 * 3 Colored Image x , | Input : Noise 50 z -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 0.2 Dropout | MLP 8192 , ReLU , BN 3 * 3 conv . 128 , Pad =1 , Stride =1 , lReLU , Weight norm | Reshape 512 * 4 * 4 3 * 3 conv . 128 , Pad =1 , Stride =1 , lReLU , Weight norm | 5 * 5 deconv . 256 * 8 * 8 , 3 * 3 conv . 128 , Pad =1 , Stride =2 , lReLU , Weight norm | ReLU , Batch norm -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 0.5 Dropout | 3 * 3 conv . 256 , Pad =1 , Stride =1 , lReLU , Weight norm | 3 * 3 conv . 256 , Pad =1 , Stride =1 , lReLU , Weight norm | 5 * 5 deconv . 128 * 16 * 16 , 3 * 3 conv . 256 , Pad =1 , Stride =2 , lReLU , Weight norm | ReLU , Batch norm -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 0.5 Dropout | 3 * 3 conv . 512 , Pad =0 , Stride =1 , lReLU , Weight norm | 3 * 3 conv . 256 , Pad =0 , Stride =1 , lReLU , Weight norm | 5 * 5 deconv . 3 * 32 * 32 , 3 * 3 conv . 128 , Pad =0 , Stride =1 , lReLU , Weight norm | Tanh , Weight norm -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Global pool | MLP 10 , Weight norm , Softmax | == Hyper-parameters == We set \\lambda = 1.0 in Eq . ( 7 ) in all our experiments . For CIFAR-10 , the number of training epochs is set to 1,000 with a constant learning rate of 0.0003 . For MNIST , the number of training epochs is set to 300 with a constant learning rate of 0.003 . The other hyper-parameters are exactly the same as in the improved GAN ( Salimans et al. , 2016 ) . == New Title == Improving the Improved Training of Wasserstein GANs : A Consistency Term and Its Dual Effect"}], "0": {"review_id": "SJx9GQb0--0", "review_text": "Updates: thanks for the authors' hard rebuttal work, which addressed some of my problems/concerns. But still, without the analysis of the temporal ensembling trick [Samuli & Timo, 2017] and data augmentation, it is difficult to figure out the real effectiveness of the proposed GAN. I would insist my previous argument and score. Original review: ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- This paper presented an improved approach for training WGANs, by applying some Lipschitz constraint close to the real manifold in the pixel level. The framework can also be integrated to boost the SSL performances. In experiments, the generated data showed very good qualities, measured by inception score. Meanwhile, the SSL-GANs results were impressive on MNIST and CIFAR-10, demonstrating its effectiveness. However, the paper has the following weakness: Missing citations: the most related work of this one is the DRAGAN work. However, it did not cite it. I think the author should cite it, make a clear justification for the comparison and emphasize the main contribution of the method. Also, it suggested that the paper should discuss its relation to other important work, [Arjovsky & Bottou 2017], [Wu et al. 2016]. Experiments: as for the experimental part, it is not solid. Firstly, although the SSL results are very good, it is guaranteed the proposed GAN is good [Dai & Almahairi, et al. 2017]. Secondly, the paper missed several details, such as settings, model configuration, hyper-parameters, making it is difficult to justify which part of the model works. Since the paper using the temporal ensembling trick [Samuli & Timo, 2017], most of the gain might be from there. Data augmentation might also help to improve. Finally, except CIFAR-10, it is better to evaluate it on more datasets. Given the above reason, I think this paper is not ready to be published in ICLR. The author can submit it to the workshop and prepare for next conference. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "We are pleased to see that the reviewer thinks our `` generated data showed very good qualities '' and `` the SSL-GANs results were impressive '' . =Q1= `` Missing citations : the most related work of this one is the DRAGAN '' We would consider WGAN and WGAN-GP as the most related works to ours and DRAGAN ranks after them . As a matter of fact , DRAGAN is an unpublished work and has not been peer-reviewed . As another matter of fact , the gradient penalty in DRAGAN is the same as in WGAN-GP except that it is imposed around the real data while WGAN-GP applies it to the points sampled between the real and the generated ones . Next , we highlight some key differences between DRAGAN and ours . We propose to improve Wasserstein GAN , while DRAGAN works with GAN . DRAGAN aims to reduce the non-optimal saddle points in the minmax two-player training of GANs . In contrast , we propose an approach to enforcing the 1-Lipschitz continuity over the critic of WGANs . One of our key observations is that it blurs the generated samples if we add noise directly to the data points , as done in DRAGAN . Instead , we perturb the hidden layers of the discriminator . DRAGAN perturbs a data point once while we do it twice in each iteration . After the perturbations , DRAGAN penalizes the gradients while we enforce the consistency of the outputs . One of the most distinct features of our approach is that it seamlessly integrates the semi-supervised learning method by Laine & Aila ( 2016 ) with GANs . =Q2= `` the paper should discuss ... [ Arjovsky & Bottou 2017 ] , [ Wu et al.2016 ] '' We had included both in our paper . Arjovsky & Bottou 2017 analyzes some distribution divergences and their effects in training GANs . Wu et al.2016 propose to quantitatively evaluate the decoder-based generative models by annealed importance sampling . In our paper , we focus on a different subject , i.e. , to design an algorithmic solution to the difficulty of training GANs . =Q3= `` the paper missed several details '' Please see either Appendices A and B of the revised paper or the following for our answer to this question . Given the context of the question , we believe it is about SSL . We follow the experiment setups in the prior works so that our results are directly comparable to theirs . Please see below for more details . If you are interested , you may also check out our code : https : //github.com/biuyq/CT-GAN/blob/master/CT-GANs/Theano_classifier/CT_CIFAR-10_TE.py . MNIST : There are 60,000 images in total . We randomly choose 10 data points for each digit as the labeled set . No data augmentation is used . CIFAR-10 : There are 50,000 image in total . We randomly choose 400 images for each class as the labeled set . We augment the data by horizontally flipping the images and randomly translating the images within [ -2,2 ] pixels . No ZCA whitening is used . Model Configurations : We had included them in the appendix . Hyper-parameters : We set lambda = 1.0 in Eq . ( 7 ) in all our experiments . For CIFAR-10 , the number of training epochs is set to 1,000 with a constant learning rate of 0.0003 . For MNIST , the number of training epochs is set to 300 with a constant learning rate of 0.003 . The other hyper-parameters are exactly the same as in the improved GAN ( Salimans et al. , 2016 ) . =Q4= `` ... which part of the model works '' Please see either Appendix C of the revised paper or the following for our answer to this question . We have done some ablation studies about our semi-supervised learning approach on CIFAR-10 . Method , Error w/o CT , 15.0 w/o GAN ( note 1 ) , 12.0 w batch norm ( note 2 ) , -- w/o D_ , 10.7 OURS , 10.0 Note 1 : This almost reduces to TE ( Laine & Aila , 2016 ) . All the settings here are the same as TE except that we use the extra regularization ( $ D\\_ ( . , . ) $ in CT ) over the second-to-last layer . Note 2 : We use the weight normalization as in ( Salimans et al. , 2016 ) , which becomes a core constituent of our approach . The batch normalization would actually invalidate the feature matching in ( Salimans et al. , 2016 ) . We can see that both GAN and the temporal ensembling effectively contribute to our final results . The results without our consistency regularization ( w/o CT ) drop more than those without GAN . We are running the experiments without any data augmentation and will include the corresponding results in the paper . =Q5= `` ... it is better to evaluate it on more datasets '' We have run some new experiments on the SVHN dataset . Ours is the best among all the GAN based semi-supervised learning methods , and is on par with the state of the arts . Method , Error PI Laine & Aila 2016 , 4.8 TE Laine & Aila 2016 , 4.4 Tarvainen & Valpola 2017 , 4.0 Miyato et al.2017 , 3.86 Salimans et al.2016 , 8.1 Dumoulin et al.2016 , 7.4 Kumar et al.2017 , 5.9 Ours , 4.2"}, "1": {"review_id": "SJx9GQb0--1", "review_text": "Summary: The paper proposes a new regularizer for wgans, to be combined with the traditional gradient penalty. The theoretical motivation is bleak, and the analysis contains some important mistakes. The results are very good, as noticed by the comments, the fact that the method is also less susceptible to overfitting is also an important result, though this might be purely due to dropout. One of the main problems is that the largest dataset used is CIFAR, which is small. Experiments on something like bedrooms or imagenet would make the paper much stronger. If the authors fix the theoretical analysis and add evidence in a larger dataset I will raise the score. Detailed comments: - The motivation of 1.2 and the sentence \"Arguably, it is fairly safe to limit our scope to the manifold that supports the real data distribution P_r and its surrounding regions\" are incredibly wrong. First of all, it should be noted that the duality uses 1-Lip in the entire space between Pr and Pg, not in Pr alone. If the manifolds are not extremely close (such as in the beginning of training), then the discriminator can be almost exactly 1 in the real data, and 0 on the fake. Thus the discriminator would be almost exactly constant (0-Lip) near the real manifold, but will fail to be 1-lip in the decision boundary, this is where interpolations fix this issue. See Figure 2 of the wgan paper for example, in this simple example an almost perfect discriminator would have almost 0 penalty. - In the 'Potential caveats' section, the implication that 1-Lip may not be enforced in non-examined samples is checkable by an easy experiment, which is to look for samples that have gradients of the critic wrt the input with norm > 1. I performed the exp in figure 8 and saw that by taking a slightly higher lambda, one reaches gradients that are as close to 1 as with ct-gan. Since ct-gan uses an extra regularizer, I think the authors need some stronger evidence to support the claim that ct-gan better battles this 'potential caveat'. - It's important to realize that the CT regularizer with M' = 1 (1-Lip constraint) will only be positive for an almost 1-Lip function if x and x' are sampled when x - x' has a very similar direction than the gradient at x. This is very hard in high dimensional spaces, and when I implemented a CT regularizer indeed the ration of eq (4) was quite less than the norm of the gradient. It would be useful to plot the value of the CT regularizer (the eq 4 version) as the training iterations progresses. Thus the CT regularizer works as an overall Lipschitz penalty, as opposed to penalizing having more than 1 for the Lipschitz constant. This difference is non-trivial and should be discussed. - Line 11 of the algorithm is missing L^(i) inside the sum. - One shouldn't use MNIST for anything else than deliberately testing an overfitting problem. Figure 4 is thus relevant, but the semi-supervised results of MNIST or the sample quality experiments give hardly any evidence to support the method. - The overfitting result is very important, but one should disambiguate this from being due to dropout. Comparing with wgangp + dropout is thus important in this experiment. - The authors should provide experiments in at least one larger dataset like bedrooms or imagenet (not faces, which is known to be very easy). This would strengthen the paper quite a bit.", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for the insightful comments and suggestions ! The paper has been revised accordingly . Next , we answer the questions in detail . == Q1 : The motivation == We acknowledge that the duality uses 1-Lipschitz continuity in the entire space between Pr and Pg , and it is impossible to visit everywhere of the space in the experiments . We instead focus on the region around the real data manifold to complement the region checked by GP-WGAN -- - the gradient penalty term is kept in our overall approach . We have clarified this point by the following in the revised paper . Arguably , it is fairly safe to limit our scope to the manifold that supports the real data distribution $ \\mathbb { P } _r $ and its surrounding regions mainly for two reasons . First , we keep the gradient penalty term and improve it by the proposed consistency term in our overall approach . While the former enforces the continuity over the points sampled between the real and generated points , the latter complement the former by focusing on the region around the real data manifold instead . Second , the distribution of the generative model $ \\mathbb { P } _G $ is virtually desired to be as close as possible to $ \\mathbb { P } _r $ . == Q2 : That 1-Lip may not be enforced in non-examined samples is checkable == The non-examined samples can refer to all the possible samples in the continuous space which can not be traversed in a discrete manner . Figure 8 plots the norm of the gradients ( of the critic with respect to the input ) over the real data points only . In other words , Figure 8 is only part of the consequence , and certainly not the cause , of the discriminators trained by GP-WGAN and our CT-GAN , respectively . It is not surprising that the norms by CT-GAN are closer to 1 than by GP-WGAN because we explicitly enforce the continuity around the real data . We have run more experiments with larger \\lambda values in GP-GAN , and found the gradient norms can indeed reach those of CT-GAN when the \\lambda is four times larger than the original one used in the authors \u2019 code . However , the inception score on CIFAR-10 drops a little , and the overfitting remains . Stronger evidence ? In addition to the gradient norm , we have also examined the 1-Lipschitz continuity of the critic using the basic definition . For any two inputs x and x ' , the difference of the critic 's outputs should be no more than M * |x-x'| . This notion is captured by our CT term defined in eq . ( 4 ) .We plot the CT versus the training iterations as Figure 9 in the revised paper . In particular , for every 100 iterations , we randomly pick up 64 real examples and split them into two subsets of the same size . We compute d ( D ( x1 ) -D ( x2 ) ) /d ( x1-x2 ) for all the ( x1 , x2 ) pairs , where x1 is from the first subset and x2 is from the second . The maximum of d ( D ( x1 ) -D ( x2 ) ) /d ( x1-x2 ) is plotted in Figure 9 . We can see that the CT-GAN curve converges under a certain value much faster than GP-WGAN . == Q3 : Plot the value of the CT regularizer == Please see Figures 9 and 10 in the revised paper for the plots . Note that M \u2019 has absorbed the term d ( x \u2019 , x \u2019 \u2019 ) in the final consistency term ( eq . ( 5 ) ) , so we have to tune its value as opposed to fixing it to 1 . Also , because of this fact , we agree with the comment that \u201c Thus the CT regularizer works as an overall Lipschitz penalty , as opposed to penalizing having more than 1 for the Lipschitz constant. \u201d We will clarify this part in the final paper , if it is accepted . == Q4 : Line 11 == It is correct and is another way of denoting the gradient . == Q5 : MNIST == We understand your concern with the use of MNIST and appreciate that you agree the overfitting experiments ( Figure 4 ) are relevant . The other results ( e.g. , the generated samples and the test error in semi-supervised learning ) can give the readers a concrete understanding about our model , but we agree one should not use MNIST to compare different algorithms . == Q6 : GP-WGAN + Dropout == Please see Appendix E for the experimental results of GP-WGAN+Dropout on CIFAR-10 using 1000 training images . The corresponding inception score is better than GP-WGAN and yet still significantly lower than ours ( 2.98+-0.11 vs. 4.29+-0.12 vs. 5.13+-0.12 ) . Figure 12 , which is about the convergence curves of the discriminator cost over both training and testing sets , shows that dropout is indeed able to reduce the overfitting , but it is not as effective as ours . == Q7 : Experiments in larger datasets == In Appendix F of the revised paper , we present results on the ImageNet and LSUN bedroom datasets following the experiment setup of GP-WGAN . After 200,000 generator iterations on ImageNet , the inception score of CT-GAN is 10.27+-0.15 , whereas GP-WGAN 's is 9.85+-0.17 . Since there is only one class in LSUN bedroom , the inception score is not a proper evaluation metric for the experiments on this dataset . Visually , there is no clear difference between the generated samples of GP-WGAN and CT-GAN up to the 124,000th generator iteration ."}, "2": {"review_id": "SJx9GQb0--2", "review_text": "This paper continues a trend of incremental improvements to Wasserstein GANs (WGAN), where the latter were proposed in order to alleviate the difficulties encountered in training GANs. Originally, Arjovsky et al. [1] argued that the Wasserstein distance was superior to many others typically used for GANs. An important feature of WGANs is the requirement for the discriminator to be 1-Lipschitz, which [1] achieved simply by clipping the network weights. Recently, Gulrajani et al. [2] proposed a gradient penalty \"encouraging\" the discriminator to be 1-Lipschitz. However, their approach estimated continuity on points between the generated and the real samples, and thus could fail to guarantee Lipschitz-ness at the early training stages. The paper under review overcomes this drawback by estimating the continuity on perturbations of the real samples. Together with various technical improvements, this leads to state-of-the-art practical performance both in terms of generated images and in semi-supervised learning. In terms of novelty, the paper provides one core conceptual idea followed by several tweaks aimed at improving the practical performance of GANs. The key conceptual idea is to perturb each data point twice and use a Lipschitz constant to bound the difference in the discriminator\u2019s response on the perturbed points. The proposed method is used in eq. (6) together with the gradient penalty from [2]. The authors found that directly perturbing the data with Gaussian noise led to inferior results and therefore propose to perturb the hidden layers using dropout. For supervised learning they demonstrate less overfitting for both MNIST and CIFAR 10. They also extend their framework to the semi-supervised setting of Salismans et al 2016 and report improved image generation. The authors do an excellent comparative job in presenting their experiments. They compare numerous techniques (e.g., Gaussian noise, dropout) and demonstrates the applicability of the approach for a wide range of tasks. They use several criteria to evaluate their performance (images, inception score, semi-supervised learning, overfitting, weight histogram) and compare against a wide range of competing papers. Where the paper could perhaps be slightly improved is writing clarity. In particular, the discussion of M and M' is vital to the point of the paper, but could be written in a more transparent manner. The same goes for the semi-supervised experiment details and the CIFAR-10 augmentation process. Finally, the title seems uninformative. Almost all progress is incremental, and the authors modestly give credit to both [1] and [2], but the title is neither memorable nor useful in expressing the novel idea. [1] Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. [2] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. ", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for the very positive and affirmative comments about our work . We also appreciate the suggestions for improving the writing clarify of the paper . The following has been incorporated in the revised paper . == M vs. M ' == We use the notation $ M $ in eq . ( 3 ) and a different $ M ' $ in eq . ( 4 ) to reflect the fact that the continuity will be checked only sparsely at some data points in practice . ... ... Note that , however , it becomes impossible to compute the distance $ d ( \\bm { x } ' , \\bm { x } '' ) $ between the two virtual data points . In this work , we assume it is bounded by a constant and absorb the constant to $ M ' $ . Accordingly , we tune $ M ' $ in our experiments to take account of this unknown constant ; the best results are obtained between $ M'=0 $ and $ M'=0.2 $ . == Semi-supervised experiment details and the CIFAR-10 augmentation process == MNIST : There are 60,000 images in total . We randomly choose 10 data points for each digit as the labeled set . No data augmentation is used . CIFAR-10 : There are 50,000 image in total . We randomly choose 400 images for each class as the labeled set . We augment the data by horizontally flipping the images and randomly translating the images within [ -2,2 ] pixels . No ZCA whitening is used . Model Configuration Table 1 : MNIST -- -- -- -- -- -- -- Classifier C | Generator G Input : Labels y , 28 * 28 Images x | Input : Noise 100 z Gaussian noise 0.3 , MLP 1000 , ReLU | MLP 500 , Softplus , Batch norm Gaussian noise 0.5 , MLP 500 , ReLU | MLP 500 , Softplus , Batch norm Gaussian noise 0.5 , MLP 250 , ReLU | MLP 784 , Sigmoid , Weight norm Gaussian noise 0.5 , MLP 250 , ReLU | Gaussian noise 0.5 , MLP 250 , ReLU | Gaussian noise 0.5 , MLP 10 , Softmax | Table 2 : CIFAR-10 -- -- -- -- -- -- -- -- - Input : Labels y , 32 * 32 * 3 Colored Image x , | Input : Noise 50 z -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 0.2 Dropout | MLP 8192 , ReLU , BN 3 * 3 conv . 128 , Pad =1 , Stride =1 , lReLU , Weight norm | Reshape 512 * 4 * 4 3 * 3 conv . 128 , Pad =1 , Stride =1 , lReLU , Weight norm | 5 * 5 deconv . 256 * 8 * 8 , 3 * 3 conv . 128 , Pad =1 , Stride =2 , lReLU , Weight norm | ReLU , Batch norm -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 0.5 Dropout | 3 * 3 conv . 256 , Pad =1 , Stride =1 , lReLU , Weight norm | 3 * 3 conv . 256 , Pad =1 , Stride =1 , lReLU , Weight norm | 5 * 5 deconv . 128 * 16 * 16 , 3 * 3 conv . 256 , Pad =1 , Stride =2 , lReLU , Weight norm | ReLU , Batch norm -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 0.5 Dropout | 3 * 3 conv . 512 , Pad =0 , Stride =1 , lReLU , Weight norm | 3 * 3 conv . 256 , Pad =0 , Stride =1 , lReLU , Weight norm | 5 * 5 deconv . 3 * 32 * 32 , 3 * 3 conv . 128 , Pad =0 , Stride =1 , lReLU , Weight norm | Tanh , Weight norm -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Global pool | MLP 10 , Weight norm , Softmax | == Hyper-parameters == We set \\lambda = 1.0 in Eq . ( 7 ) in all our experiments . For CIFAR-10 , the number of training epochs is set to 1,000 with a constant learning rate of 0.0003 . For MNIST , the number of training epochs is set to 300 with a constant learning rate of 0.003 . The other hyper-parameters are exactly the same as in the improved GAN ( Salimans et al. , 2016 ) . == New Title == Improving the Improved Training of Wasserstein GANs : A Consistency Term and Its Dual Effect"}}