{"year": "2019", "forum": "BygREjC9YQ", "title": "A unified theory of adaptive stochastic gradient descent as Bayesian filtering", "decision": "Reject", "meta_review": "The aim of this paper is to interpret various optimizers such as RMSprop, Adam, and NAG, as approximate Kalman filtering of the optimal parameters. These algorithms are derived as inference procedures in various dynamical systems. The main empirical result is the algorithms achieve slightly better test accuracy on MNIST compared to an unregularized network trained with Adam or RMSprop.\n\nThis was a controversial paper, and each of the reviewers had a significant back-and-forth with the authors. The controversy reflects that this is a pretty interesting and relevant topic: a proper Bayesian framework could provide significant guidance for developing better optimizers and regularizers. Unfortunately, I don't think this paper delivers on its promise of a unifying Bayesian framework for these various methods, and I don't think it's quite ready for publication at ICLR.\n\nThere was some controversy about relationships to various recently published papers giving Bayesian interpretations of optimizers. The authors believe the added value of this submission is that it recovers features such as momentum and root-mean-square normalization. This would be a very interesting contribution beyond those works. But R2 and R3 feel like these particular features were derived using fairly ad-hoc assumptions or approximations almost designed to obtain existing algorithms, and from reading the paper I have to say I agree with the reviewers.\n\nThere was a lot of back-and-forth about the correctness of various theoretical claims. But overall, my impression is that the theoretical arguments in this paper exceed the bar for a primarily practical/empirical paper, but aren't rigorous enough for the paper to stand purely based on the theoretical contributions. \n\nUnfortunately, the empirical part of the paper is rather lacking. The only experiment reported is on MNIST, and the only result is improved test error. The baseline gets below 99% test accuracy, below the level achieved by the original LeNet, suggesting the baseline may be somehow broken. Simply measuring test error doesn't really get at the benefits of Bayesian approaches, as it doesn't distinguish it from the many other regularizers that have been proposed. Since the proposed method is nearly identical to things like Adam or NAG, I don't see any reason it can't be evaluated on more challenging problems (as reviewers have asked for). \n\nOverall, while I find the ideas promising, I think the paper needs considerable work before it is ready for publication at ICLR.\n", "reviews": [{"review_id": "BygREjC9YQ-0", "review_text": "Paper summary: The authors analyze stochastic gradient descent through the lens of Bayesian filtering. In doing so they (approximately) recover several common adaptive gradient optimization schemes. The paper focuses on a theoretical construction of this framework and offers a limited empirical study. Detailed comments: I thought that the paper presented some interesting ideas but amongst the many things discussed there is very little which is empirically gratified. While the Bayesian filtering framework is interesting in that it recovers slight variations of existing algorithms, and also caters for some recent practical tricks, I do not feel that it substantially improves our theoretical understanding of these methods. 1) I found the notation difficult to follow in the introduction and parts of section 2. I have highlighted several places explicitly below. I found paragraphs 2 and 3 of the introduction particularly challenging. 2) I found the introduction of Bayesian filtering challenging to follow. For example, which form of the likelihood is assumed for the Taylor expansion? How/why is $\\mu_{like}$ identified using the gradient? Linking to Kalman filtering made things easier to follow. 3) I think that the related work, and possibly a chunk of section 2, should include a discussion of Noisy Natural Gradient [1]. While the derivation differs, the motivation and final form of the updates seem to have a large overlap but this work is not cited. 4) Start of 2.1: \"z will have on element representing a single parameter\", after which z is treated as a vector. I believe this sentence is present to distinguish RMSProp from Adam when momentum is added but I found it confusing at first. 5) I found the comparisons between BRMSProp-vs-RMSProp and BAdam-vs-Adam fairly unconvincing. The assumptions are not clearly demonstrated to have little practical significance and Figure 2. does not seem to support the claim that these methods are strongly related. Is it possible to demonstrate empirically that these algorithms have equivalent behaviour under some limiting factors? And if not, is there a good reason for this that still justifies the comparison? I would appreciate some clarifications on these points. 6) I am not sure what you mean by \"We now assume that the data is strong enough to reduce the uncertainty in the momentum below its levels under the prior\". I believe that I am following the mathematical arguments correctly but I find this phrasing misleading. Furthermore, this section uses e.g. ppth and wpth to refer to coordinates, I think it would be clearer to simply write Sigma_{pp}, etc. 7) Section 4.2 is lacking justification in my opinion (am I missing something?). I think that this section needs to have the derivation clearly laid out (in the appendix would be fine). Furthermore, the NAWD algorithm is not explored empirically, or analyzed theoretically at all. I would argue that more evidence is needed that this is a reasonable thing to do before it is meaningful to include it in the final print of this paper. In general, sections 4.4 - 4.7 feel a little out-of-place and thrown together. I think there are interesting comments here which are certainly worth including but their presentation should be rethought and some empirical investigation would be valuable. Minor comments: - In introduction, how exactly does $w'_i$ differ from $w_i$? - In introduction, after para 2, the notation in the equation is confusing, e.g. overloading w_i(t) and w_i(mu_{-i}(t)). - In introduction, para 3, \"must depend on other parameters\" - this seems like an obvious statement but it is presented as being crucial - Should \"Related Work\" start at 1 or 2? - (VERY MINOR) In section 2.2 and 2.3, \"christen\" seems like an add choice of word. Perhaps just \"call\"? - Equations 10 and 11 introduce an independence assumption on the dimensions of the parameter vector. I think this should be explicitly stated. - Section 7.2 heading typo: MOMEMTUM Clarity: I found the paper challenging to follow in places due to choices of notation (and a weak background in Kalman filtering and related techniques). Significance: I do not feel that this work offers a strong case for significance. The empirical evaluation is very limited. The theoretical framework introduces is interesting but is not justified particularly well in the paper and does not directly offer explanations for many of the observations noted in this paper and elsewhere. Originality: To my knowledge, the ideas presented in the paper are original and hint at potentially interesting viewpoints of optimization. References: [1] Zhang et al. \"Noisy Natural Gradient as Variational Inference\" https://arxiv.org/pdf/1712.02390.pdf", "rating": "5: Marginally below acceptance threshold", "reply_text": "7 ) We have added an Appendix section spelling out how inference in this filtering model implements NAG . The empirical evaluations in Fig.2 include NAWD as part of the full BRMSprop and BAdam algorithms . We agree that it would be great to do an empirical investigation of all the features discussed in this paper , but this becomes an empirical analysis of a pretty large swathe of techniques for adaptive stochastic gradient descent , which is out of scope for the present paper . Minor comments : - w ' has been removed . - While we have rewritten this section , we have retained the notation . w is the underlying unknown random variable , whereas mu is our current mean estimate of that variable . Hopefully this distinction is clearer in the new version . We are also using the recommended DL book notation for random variables . - I agree , it is obvious . But it is also crucial : it is the key for why we need to introduce a rich dynamical prior . Again , hopefully this discussion has been improved in the new version . - Related work has been incorporated into the introduction . I 've also explicitly introduced an Introduction section , so that next section is 2 . - I have replaced christen - The independence assumption comes right at the start when we write down a factorised generative model ( as we explain in the new section `` Factorisation implies a rich dynamical prior '' in Eq.10 in the original submission , we 're doing filtering in a 1D inference problem . - Fixed"}, {"review_id": "BygREjC9YQ-1", "review_text": "In this work, the authors attempt to unify existing adaptive gradient methods under the Bayesian filtering framework with the dynamical prior. In Ollivier, 2017, a framework is proposed to connect Bayesian filtering and natural gradient. On the other hand, in Khan et al., 2018. an approach is proposed to connect natural gradient and adaptive gradient methods. The main contributions of this work are (1) introducing a dynamical prior and (2) recovering RMSProp and Adam as special cases. However, the proposed dynamical prior is very similar to the fading memory technique used in Ollivier, 2017. (see Proposition 3 of Ollivier, 2017) Furthermore, the authors argue that this work recovers a root-mean-square form while Khan et al., 2018 recovers a different sum-square form. Unfortunately, the authors have to use a series of unnatural approximations to recover the root-mean-square form. In fact, as mentioned in Khan, 2017b this proposed method without these approximations is also a mean-square form. (also see Eq (2.28-2.29) of Ollivier, 2017) Since the authors mainly follow Ollivier, 2017 and make unnatural approximations, the work has a limited impact. To get a higher rating, the authors should clearly give justifications and insights of these approximations. Detailed comments: (1) On Page 1, \"The typical approach to Bayesian filtering, where we infer a distribution, ... jointly, forces us to use extremely strong, factorised approximations, and it is legitimate to worry that these strong approximations might meaningfully disrupt the ability of Bayesian filtering to give close-to-optimal updates. ... we instead consider ... that incorporates factorisation into the problem setting, and therefore requires fewer approximations downstream. \" The proposed method is equivalent to jointly perform Kalman filtering with full-covariance with an additional diagonal-approximation step. This additional step might also meaningfully disrupt the ability of Bayesian filtering. Furthermore, such approximation ignores the off-diagonal terms in the low-rank approximation at Eq (8). Minor: You should use \\approx at Eq (8) since a rank-1 approximation is used. (2) On page 2, \"It has been noted that under specific circumstances, natural gradient is approximate Bayesian filtering (Ollivier, 2017), allowing us to link Bayesian filtering to the rich literature on natural gradients. However, this only occurs when the dynamical prior in the Bayesian filtering problem has a specific form: the parameters being fixed over time (i.e. arguably an online data, rather than a true Bayesian filtering setting).\" The authors should comment the difference between the dynamical prior and the fading memory technique (see Proposition 3 of Ollivier, 2017) where at page 14 of Ollivier, 2017, Ollivier mentions that \"this is equivalent ... or to the addition of an artificial process noise ... in the model\". I think Ollivier's idea is very similar to the dynamical prior used at Eq (1) of this submission. Furthermore, the second-order Taylor expansion with a Fisher information-based estimation of Hessian (see the equation below Eq(1) of this submission) is exactly the same as Ollivier's Extended Kalman filter (see Eq 2.25 at Lemma 9 and Lemma 10 of Ollivier, 2017). The authors should cite Ollivier, 2017. Minor: Eq (6) should be E_p [ - \\nabla_z^2 \\log p(d|z) ] = E_p [ e e^T ], where \"-\", the negative sign is missing. Please see the definition of the Fisher information matrix. (3) On page 2, \"While there have been attempts to use natural gradients to recover the Adam or RMSprop root-mean-square form for the gradient normalizer, in practice a different sum-square form emerges (Khan & Lin, 2017; Khan et al., 2018). In contrast, we show that to recover the Adam or RMSprop form for the gradient normalizer.\" Khan et al., 2018 is a mean-square form for variational inference due to the entropy term of the variational distribution. (see Sec 3 and 5 of Khan et al., 2018 and Khan, 2017b ) Unfortunately, the \"root-mean-square form\" does not appear naturally in this submission. In practice, the proposed update is also a mean-square form (see Eq (2.28-2.29) of Ollivier, 2017 and Khan, 2017b) without a series of unnatural approximations used in this submission. To justify these assumptions, the authors should explain when \"the steady state posterior variance\" (see sec 2.21) and \"a self-consistent solution\" (see sec 7.1) achieve. As far as I know, \\sigma^2_t = \\sigma^2_{t+1} in sec 2.2.1 only holds in the limit case when t-> \\inifity. Why does the equality hold at each time step t? The authors should give a justification or an intuition about these approximations since this paper is a theory paper. Please also see my next point. (4) Section 7.1 is also confusing. In sec 7.1, the authors assume that A \\in O(\\eta). However, A=\\eta^2/(2\\sigma^2) in sec 2.2 and A_{1,1} = ( \\eta_w^2+\\eta^2 )/ (2\\sigma^2) at Eq (14). In both cases, A can be \\in O(\\eta^2). This is very *critical* since the authors argue that O(\\eta^3) can be neglected in sec 7.1. The authors use this point to show that Adam is a special case. If A \\in O(\\eta^2), we know that \"A \\Sigma_{post}\" \\in O(\\eta^3) should be neglected. At the last equation on page 10, the authors do not neglect \"A \\Sigma_{post}\". Why? The authors should clarify this point to avoid doing *selective* neglection. Again, the impact of this paper should be inspiring new adaptive methods. The authors also mention that the second-order term in A is neglected in sec 7.2. Any justification? References [1] Ollivier, Yann. \"Online Natural Gradient as a Kalman Filter.\" arXiv preprint arXiv:1703.00209 (2017). [2] Khan, Mohammad Emtiyaz, and Wu Lin. \"Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models.\" arXiv preprint arXiv:1703.04265 (2017). [3] Khan, Mohammad Emtiyaz, et al. \"Vprop: Variational Inference using RMSprop.\" arXiv preprint arXiv:1712.01038 (2017b). [4] Khan, Mohammad Emtiyaz, et al. \"Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam\" (2018) ", "rating": "5: Marginally below acceptance threshold", "reply_text": "( 2 ) We have included a paragraph in the introduction on the difference with Olliver 's ( 2017 ) fading memory technique . To reiterate , they use a highly unnatural generative process under which the process noise depends on inferences under that generative model ( in particular , the posterior covariance ) . In contrast , the process noise under our model is `` meaningful '' in the sense that it does not depend on inferences under the model . Minor : we have delete this equation . ( 3 ) Steady-state is indeed only reached as t - > infinity , and we have added a note to this effect . At the very least , we do n't expect to obtain a mean-square form for the normalizer , because that would require us to follow Olliver ( 2017 ) in using process noise proportional to our uncertainty , which we categorically do not do . ( 4 ) The first point is that we agree , the impact of this paper should be inspiring new adaptive methods . And we do just that , with BRMSprop and BAdam . Importantly , these approximations are never used in simulations/updates for BRMSprop and BAdam . Instead , they are only used to think about the similarities and differences between our Bayesian approaches ( BRMSprop and BAdam ) and classical methods such as RMSprop or Adam . As such , in some sense , the stronger the approximations we need to make BAdam close to Adam , the more scope we have for developing improved adaptive methods ! In response to your specific questions . Here , we are considering the limit of \\eta - > 0 , so assuming A scales with \\eta is weaker than assuming A scales with \\eta^2 . While A for RMSprop indeed scales with \\eta^2 , A for Adam scales with \\eta ( the top-right element is -eta ) . As A in general scales with \\eta , the terms you refer to , A S_post , scale with \\eta^2 , and can not be neglected in general . However , for the case of RMSprop , A indeed scales with eta^2 , and so we can neglect these terms , and indeed we do just that in the original draft ( the approximate variance updates that we solve for in steady state do not include any weight-decay terms ) . We have clarified this point in the supplementary , including both the RMSprop and Adam cases . As a technical issue , the usual definition of big-O notation considers the limit as \\eta - > \\infty , whereas the relevant limit in our case is \\eta - > 0 . I have therefore replaced the \\in big-O notation , with the physics-inspired \\sim ( for `` scales with '' ) ."}, {"review_id": "BygREjC9YQ-2", "review_text": "* Description The paper considers the following random process on the parameters z (modeled as Gaussians): - shrink z towards zero and add Gaussian i.i.d. noise to it. - update the parameters to the posterior w.r.t. a batch, where the likelihood is approximated as a diagonal multivariate normal distribution. This results in a Kalman filter like updates. There have been related methods proposed performing Bayesian learning in the form of assumed density filtering, considered as separate learning algorithms. At the same time methods such as RMSprop and Adam were previously derived from completely different considerations. The work can derive these methods in the Bayesian framework with certain additional assumptions / simplifications. It allows to naturally explain tracking the gradient statistics as uncertainties and the normalization of the gradient in the existing methods as the update of the mean parameters in the Kalman filter taking into account these uncertainties. The experiments on MNIST show that derived more Bayesian variants of RMSprop and Adam can improve generalization in terms of test likelihood and test error. * Assessment The provided derivation of Bayes like learning algorithms is relatively simple and could be very useful in practice and in further improvement of the learning methods. The approximations used are not completely clear. The clarification of the idea of a separate optimization problem per variable is necessary. The provided experiments, if there is nothing subtle, are clearly done and would be sufficient. There are some open questions such as: does the method in fact learn useful variances of the parameters, i.e. really performs an approximate Bayesian learning? Overall if find it a promising novel research direction of high practical relevance. * Clarity Intro: Why is the unnumbered equation on page 1 is called a \u201cBayesian optimization problem\u201d? There is so many sings called Bayesian that one cannot be sure what it means. In the context of the paper it should be a Bayesian learning problem, but I do not see a posterior distribution over the parameters. Overall, I did not get the point of the discussion in the introduction and Figure 1 altogether. Everything it says to me is that global minimize coordinates are dependent through the objective. I do not see what the unnumbered equation on page 1 has to do with Bayesian inference and how the correlation of parameters in the posterior distribution is related to the dependencies in the minimizer. Could authors please seriously consider clarifying this section? In what follows the paper keeps a factorize approximation to the posterior of parameters of a NN in the form of a Gaussian distribution per coordinate. It thus does not in any way avoid making this restrictive assumption. Results: Sorry, I am not familiar with the background behind (6). Which value of z is assumed in the conditional expectation, is it conditioning on \u201cz = \\mu_{prior}\u201d? How come the approximation to the variance of the data likelihood does not depend on the data? If we make this approximation, how much it is still relevant to the Bayesian learning? What are the overheads of the proposed methods? I expect they scale as easily to large problems as SGD? * Experiments From Figure 2 it seems that BRMSprop and BAdam can achieve relatively good results for large range of eta in 10^-5 to 10^-2 and it seems from the trend that even smaller eta would work. Does it mean they do not need in fact tuning of the learning rate? The experiment uses 50 epochs, do the compared methods reach the convergence? Could the authors consider an experiment running best setting of parameters per method with twice as many epochs? Some artificial toy experiments could be of interest. For example, consider a classification problem with a 1D Gaussian data distribution in each class and the logistic regression model with 2 parameters. Does the method approximate the posterior distribution? * Related work The approach to Bayesian learning taken in the paper needs to be better discussed. I think it is from the family of methods known as \u201cassumed density filtering\u201d, occurring in: Ghosh et al. \u201cAssumed Density Filtering Methods for Scalable Learning of Bayesian Neural Networks\u201d with earlier works well described in Minka T. \u201cExpectation propagation for approximate Bayesian inference\u201d. In particular equation (5) of the submission is well known. The work Khan et al. 2018 \u201cFast and scalable Bayesian deep learning by weight-perturbation in Adam\u201d also derives Bayesian learning algorithms in the forms closely similar to RMSprop and Adam and interprets the running statistics as uncertainties. However it takes the variational Bayesian learning approach, which means the reverse KL divergence is used somewhere. Could the authors discuss conceptual similarities and differences to this work? ", "rating": "7: Good paper, accept", "reply_text": "Experiments : usually , when we set the learning rate to be too small , it is impossible to move the parameters far enough from their initializations , and so we obtain very poor performance . One of the interesting things about our framework is that as eta goes to zero , it converges to Bayesian inference without the dynamical prior , which , in effect , gives an adaptive learning rate that goes as the sum-of-square gradients . In practice , we still expect to need to tune the learning rate to find the minimum of the curves in Fig.2 ( now Fig.3 ) .As regards toy experiments , we expect the benefits of our approach to become more evident in complex models with strong posterior correlations , so the simplest relevant toy experiment is linear regression with highly correlated inputs -- - which is n't all that simple . Regarding convergence , tried a range of numbers of epochs , and found that the displayed curves are pretty stable at 50 epochs , but that if anything , the difference between the Bayesian and classical methods actually increased for larger number of epochs . We have discussed the ADF-style approach ( Ghosh et al . ) , and have noted that the Kalman filtering approach is well-understood . Finally , it should be noted that Khan et al . ( 2018 ) does not recover the root-mean-square-gradient form for the normalizer . To quote from Khan et al . ( 2018 ) : `` Using ... an additional modification in the VON update , we can make the VON update very similar to RMSprop . Our modification involves taking the square-root over s_ { t+1 } in ( 7 ) '' Both they and Zhang et al . ( 2017 ) of these approches use natural gradient VI , and they both encounter the same problem : that natural gradient gives you a mean-square-gradient , rather than a root-mean-square gradient form for the normalizer . Khan et al . ( 2018 ) deal with this by reaching in and replacing the mean-square with a root-mean-square normalizer ( without a principled justification based on approximate inference ) , and Zhang et al . ( 2017 ) regard the difference between a root-mean-square and a mean-square gradient normalizer as `` inessential '' ."}], "0": {"review_id": "BygREjC9YQ-0", "review_text": "Paper summary: The authors analyze stochastic gradient descent through the lens of Bayesian filtering. In doing so they (approximately) recover several common adaptive gradient optimization schemes. The paper focuses on a theoretical construction of this framework and offers a limited empirical study. Detailed comments: I thought that the paper presented some interesting ideas but amongst the many things discussed there is very little which is empirically gratified. While the Bayesian filtering framework is interesting in that it recovers slight variations of existing algorithms, and also caters for some recent practical tricks, I do not feel that it substantially improves our theoretical understanding of these methods. 1) I found the notation difficult to follow in the introduction and parts of section 2. I have highlighted several places explicitly below. I found paragraphs 2 and 3 of the introduction particularly challenging. 2) I found the introduction of Bayesian filtering challenging to follow. For example, which form of the likelihood is assumed for the Taylor expansion? How/why is $\\mu_{like}$ identified using the gradient? Linking to Kalman filtering made things easier to follow. 3) I think that the related work, and possibly a chunk of section 2, should include a discussion of Noisy Natural Gradient [1]. While the derivation differs, the motivation and final form of the updates seem to have a large overlap but this work is not cited. 4) Start of 2.1: \"z will have on element representing a single parameter\", after which z is treated as a vector. I believe this sentence is present to distinguish RMSProp from Adam when momentum is added but I found it confusing at first. 5) I found the comparisons between BRMSProp-vs-RMSProp and BAdam-vs-Adam fairly unconvincing. The assumptions are not clearly demonstrated to have little practical significance and Figure 2. does not seem to support the claim that these methods are strongly related. Is it possible to demonstrate empirically that these algorithms have equivalent behaviour under some limiting factors? And if not, is there a good reason for this that still justifies the comparison? I would appreciate some clarifications on these points. 6) I am not sure what you mean by \"We now assume that the data is strong enough to reduce the uncertainty in the momentum below its levels under the prior\". I believe that I am following the mathematical arguments correctly but I find this phrasing misleading. Furthermore, this section uses e.g. ppth and wpth to refer to coordinates, I think it would be clearer to simply write Sigma_{pp}, etc. 7) Section 4.2 is lacking justification in my opinion (am I missing something?). I think that this section needs to have the derivation clearly laid out (in the appendix would be fine). Furthermore, the NAWD algorithm is not explored empirically, or analyzed theoretically at all. I would argue that more evidence is needed that this is a reasonable thing to do before it is meaningful to include it in the final print of this paper. In general, sections 4.4 - 4.7 feel a little out-of-place and thrown together. I think there are interesting comments here which are certainly worth including but their presentation should be rethought and some empirical investigation would be valuable. Minor comments: - In introduction, how exactly does $w'_i$ differ from $w_i$? - In introduction, after para 2, the notation in the equation is confusing, e.g. overloading w_i(t) and w_i(mu_{-i}(t)). - In introduction, para 3, \"must depend on other parameters\" - this seems like an obvious statement but it is presented as being crucial - Should \"Related Work\" start at 1 or 2? - (VERY MINOR) In section 2.2 and 2.3, \"christen\" seems like an add choice of word. Perhaps just \"call\"? - Equations 10 and 11 introduce an independence assumption on the dimensions of the parameter vector. I think this should be explicitly stated. - Section 7.2 heading typo: MOMEMTUM Clarity: I found the paper challenging to follow in places due to choices of notation (and a weak background in Kalman filtering and related techniques). Significance: I do not feel that this work offers a strong case for significance. The empirical evaluation is very limited. The theoretical framework introduces is interesting but is not justified particularly well in the paper and does not directly offer explanations for many of the observations noted in this paper and elsewhere. Originality: To my knowledge, the ideas presented in the paper are original and hint at potentially interesting viewpoints of optimization. References: [1] Zhang et al. \"Noisy Natural Gradient as Variational Inference\" https://arxiv.org/pdf/1712.02390.pdf", "rating": "5: Marginally below acceptance threshold", "reply_text": "7 ) We have added an Appendix section spelling out how inference in this filtering model implements NAG . The empirical evaluations in Fig.2 include NAWD as part of the full BRMSprop and BAdam algorithms . We agree that it would be great to do an empirical investigation of all the features discussed in this paper , but this becomes an empirical analysis of a pretty large swathe of techniques for adaptive stochastic gradient descent , which is out of scope for the present paper . Minor comments : - w ' has been removed . - While we have rewritten this section , we have retained the notation . w is the underlying unknown random variable , whereas mu is our current mean estimate of that variable . Hopefully this distinction is clearer in the new version . We are also using the recommended DL book notation for random variables . - I agree , it is obvious . But it is also crucial : it is the key for why we need to introduce a rich dynamical prior . Again , hopefully this discussion has been improved in the new version . - Related work has been incorporated into the introduction . I 've also explicitly introduced an Introduction section , so that next section is 2 . - I have replaced christen - The independence assumption comes right at the start when we write down a factorised generative model ( as we explain in the new section `` Factorisation implies a rich dynamical prior '' in Eq.10 in the original submission , we 're doing filtering in a 1D inference problem . - Fixed"}, "1": {"review_id": "BygREjC9YQ-1", "review_text": "In this work, the authors attempt to unify existing adaptive gradient methods under the Bayesian filtering framework with the dynamical prior. In Ollivier, 2017, a framework is proposed to connect Bayesian filtering and natural gradient. On the other hand, in Khan et al., 2018. an approach is proposed to connect natural gradient and adaptive gradient methods. The main contributions of this work are (1) introducing a dynamical prior and (2) recovering RMSProp and Adam as special cases. However, the proposed dynamical prior is very similar to the fading memory technique used in Ollivier, 2017. (see Proposition 3 of Ollivier, 2017) Furthermore, the authors argue that this work recovers a root-mean-square form while Khan et al., 2018 recovers a different sum-square form. Unfortunately, the authors have to use a series of unnatural approximations to recover the root-mean-square form. In fact, as mentioned in Khan, 2017b this proposed method without these approximations is also a mean-square form. (also see Eq (2.28-2.29) of Ollivier, 2017) Since the authors mainly follow Ollivier, 2017 and make unnatural approximations, the work has a limited impact. To get a higher rating, the authors should clearly give justifications and insights of these approximations. Detailed comments: (1) On Page 1, \"The typical approach to Bayesian filtering, where we infer a distribution, ... jointly, forces us to use extremely strong, factorised approximations, and it is legitimate to worry that these strong approximations might meaningfully disrupt the ability of Bayesian filtering to give close-to-optimal updates. ... we instead consider ... that incorporates factorisation into the problem setting, and therefore requires fewer approximations downstream. \" The proposed method is equivalent to jointly perform Kalman filtering with full-covariance with an additional diagonal-approximation step. This additional step might also meaningfully disrupt the ability of Bayesian filtering. Furthermore, such approximation ignores the off-diagonal terms in the low-rank approximation at Eq (8). Minor: You should use \\approx at Eq (8) since a rank-1 approximation is used. (2) On page 2, \"It has been noted that under specific circumstances, natural gradient is approximate Bayesian filtering (Ollivier, 2017), allowing us to link Bayesian filtering to the rich literature on natural gradients. However, this only occurs when the dynamical prior in the Bayesian filtering problem has a specific form: the parameters being fixed over time (i.e. arguably an online data, rather than a true Bayesian filtering setting).\" The authors should comment the difference between the dynamical prior and the fading memory technique (see Proposition 3 of Ollivier, 2017) where at page 14 of Ollivier, 2017, Ollivier mentions that \"this is equivalent ... or to the addition of an artificial process noise ... in the model\". I think Ollivier's idea is very similar to the dynamical prior used at Eq (1) of this submission. Furthermore, the second-order Taylor expansion with a Fisher information-based estimation of Hessian (see the equation below Eq(1) of this submission) is exactly the same as Ollivier's Extended Kalman filter (see Eq 2.25 at Lemma 9 and Lemma 10 of Ollivier, 2017). The authors should cite Ollivier, 2017. Minor: Eq (6) should be E_p [ - \\nabla_z^2 \\log p(d|z) ] = E_p [ e e^T ], where \"-\", the negative sign is missing. Please see the definition of the Fisher information matrix. (3) On page 2, \"While there have been attempts to use natural gradients to recover the Adam or RMSprop root-mean-square form for the gradient normalizer, in practice a different sum-square form emerges (Khan & Lin, 2017; Khan et al., 2018). In contrast, we show that to recover the Adam or RMSprop form for the gradient normalizer.\" Khan et al., 2018 is a mean-square form for variational inference due to the entropy term of the variational distribution. (see Sec 3 and 5 of Khan et al., 2018 and Khan, 2017b ) Unfortunately, the \"root-mean-square form\" does not appear naturally in this submission. In practice, the proposed update is also a mean-square form (see Eq (2.28-2.29) of Ollivier, 2017 and Khan, 2017b) without a series of unnatural approximations used in this submission. To justify these assumptions, the authors should explain when \"the steady state posterior variance\" (see sec 2.21) and \"a self-consistent solution\" (see sec 7.1) achieve. As far as I know, \\sigma^2_t = \\sigma^2_{t+1} in sec 2.2.1 only holds in the limit case when t-> \\inifity. Why does the equality hold at each time step t? The authors should give a justification or an intuition about these approximations since this paper is a theory paper. Please also see my next point. (4) Section 7.1 is also confusing. In sec 7.1, the authors assume that A \\in O(\\eta). However, A=\\eta^2/(2\\sigma^2) in sec 2.2 and A_{1,1} = ( \\eta_w^2+\\eta^2 )/ (2\\sigma^2) at Eq (14). In both cases, A can be \\in O(\\eta^2). This is very *critical* since the authors argue that O(\\eta^3) can be neglected in sec 7.1. The authors use this point to show that Adam is a special case. If A \\in O(\\eta^2), we know that \"A \\Sigma_{post}\" \\in O(\\eta^3) should be neglected. At the last equation on page 10, the authors do not neglect \"A \\Sigma_{post}\". Why? The authors should clarify this point to avoid doing *selective* neglection. Again, the impact of this paper should be inspiring new adaptive methods. The authors also mention that the second-order term in A is neglected in sec 7.2. Any justification? References [1] Ollivier, Yann. \"Online Natural Gradient as a Kalman Filter.\" arXiv preprint arXiv:1703.00209 (2017). [2] Khan, Mohammad Emtiyaz, and Wu Lin. \"Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models.\" arXiv preprint arXiv:1703.04265 (2017). [3] Khan, Mohammad Emtiyaz, et al. \"Vprop: Variational Inference using RMSprop.\" arXiv preprint arXiv:1712.01038 (2017b). [4] Khan, Mohammad Emtiyaz, et al. \"Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam\" (2018) ", "rating": "5: Marginally below acceptance threshold", "reply_text": "( 2 ) We have included a paragraph in the introduction on the difference with Olliver 's ( 2017 ) fading memory technique . To reiterate , they use a highly unnatural generative process under which the process noise depends on inferences under that generative model ( in particular , the posterior covariance ) . In contrast , the process noise under our model is `` meaningful '' in the sense that it does not depend on inferences under the model . Minor : we have delete this equation . ( 3 ) Steady-state is indeed only reached as t - > infinity , and we have added a note to this effect . At the very least , we do n't expect to obtain a mean-square form for the normalizer , because that would require us to follow Olliver ( 2017 ) in using process noise proportional to our uncertainty , which we categorically do not do . ( 4 ) The first point is that we agree , the impact of this paper should be inspiring new adaptive methods . And we do just that , with BRMSprop and BAdam . Importantly , these approximations are never used in simulations/updates for BRMSprop and BAdam . Instead , they are only used to think about the similarities and differences between our Bayesian approaches ( BRMSprop and BAdam ) and classical methods such as RMSprop or Adam . As such , in some sense , the stronger the approximations we need to make BAdam close to Adam , the more scope we have for developing improved adaptive methods ! In response to your specific questions . Here , we are considering the limit of \\eta - > 0 , so assuming A scales with \\eta is weaker than assuming A scales with \\eta^2 . While A for RMSprop indeed scales with \\eta^2 , A for Adam scales with \\eta ( the top-right element is -eta ) . As A in general scales with \\eta , the terms you refer to , A S_post , scale with \\eta^2 , and can not be neglected in general . However , for the case of RMSprop , A indeed scales with eta^2 , and so we can neglect these terms , and indeed we do just that in the original draft ( the approximate variance updates that we solve for in steady state do not include any weight-decay terms ) . We have clarified this point in the supplementary , including both the RMSprop and Adam cases . As a technical issue , the usual definition of big-O notation considers the limit as \\eta - > \\infty , whereas the relevant limit in our case is \\eta - > 0 . I have therefore replaced the \\in big-O notation , with the physics-inspired \\sim ( for `` scales with '' ) ."}, "2": {"review_id": "BygREjC9YQ-2", "review_text": "* Description The paper considers the following random process on the parameters z (modeled as Gaussians): - shrink z towards zero and add Gaussian i.i.d. noise to it. - update the parameters to the posterior w.r.t. a batch, where the likelihood is approximated as a diagonal multivariate normal distribution. This results in a Kalman filter like updates. There have been related methods proposed performing Bayesian learning in the form of assumed density filtering, considered as separate learning algorithms. At the same time methods such as RMSprop and Adam were previously derived from completely different considerations. The work can derive these methods in the Bayesian framework with certain additional assumptions / simplifications. It allows to naturally explain tracking the gradient statistics as uncertainties and the normalization of the gradient in the existing methods as the update of the mean parameters in the Kalman filter taking into account these uncertainties. The experiments on MNIST show that derived more Bayesian variants of RMSprop and Adam can improve generalization in terms of test likelihood and test error. * Assessment The provided derivation of Bayes like learning algorithms is relatively simple and could be very useful in practice and in further improvement of the learning methods. The approximations used are not completely clear. The clarification of the idea of a separate optimization problem per variable is necessary. The provided experiments, if there is nothing subtle, are clearly done and would be sufficient. There are some open questions such as: does the method in fact learn useful variances of the parameters, i.e. really performs an approximate Bayesian learning? Overall if find it a promising novel research direction of high practical relevance. * Clarity Intro: Why is the unnumbered equation on page 1 is called a \u201cBayesian optimization problem\u201d? There is so many sings called Bayesian that one cannot be sure what it means. In the context of the paper it should be a Bayesian learning problem, but I do not see a posterior distribution over the parameters. Overall, I did not get the point of the discussion in the introduction and Figure 1 altogether. Everything it says to me is that global minimize coordinates are dependent through the objective. I do not see what the unnumbered equation on page 1 has to do with Bayesian inference and how the correlation of parameters in the posterior distribution is related to the dependencies in the minimizer. Could authors please seriously consider clarifying this section? In what follows the paper keeps a factorize approximation to the posterior of parameters of a NN in the form of a Gaussian distribution per coordinate. It thus does not in any way avoid making this restrictive assumption. Results: Sorry, I am not familiar with the background behind (6). Which value of z is assumed in the conditional expectation, is it conditioning on \u201cz = \\mu_{prior}\u201d? How come the approximation to the variance of the data likelihood does not depend on the data? If we make this approximation, how much it is still relevant to the Bayesian learning? What are the overheads of the proposed methods? I expect they scale as easily to large problems as SGD? * Experiments From Figure 2 it seems that BRMSprop and BAdam can achieve relatively good results for large range of eta in 10^-5 to 10^-2 and it seems from the trend that even smaller eta would work. Does it mean they do not need in fact tuning of the learning rate? The experiment uses 50 epochs, do the compared methods reach the convergence? Could the authors consider an experiment running best setting of parameters per method with twice as many epochs? Some artificial toy experiments could be of interest. For example, consider a classification problem with a 1D Gaussian data distribution in each class and the logistic regression model with 2 parameters. Does the method approximate the posterior distribution? * Related work The approach to Bayesian learning taken in the paper needs to be better discussed. I think it is from the family of methods known as \u201cassumed density filtering\u201d, occurring in: Ghosh et al. \u201cAssumed Density Filtering Methods for Scalable Learning of Bayesian Neural Networks\u201d with earlier works well described in Minka T. \u201cExpectation propagation for approximate Bayesian inference\u201d. In particular equation (5) of the submission is well known. The work Khan et al. 2018 \u201cFast and scalable Bayesian deep learning by weight-perturbation in Adam\u201d also derives Bayesian learning algorithms in the forms closely similar to RMSprop and Adam and interprets the running statistics as uncertainties. However it takes the variational Bayesian learning approach, which means the reverse KL divergence is used somewhere. Could the authors discuss conceptual similarities and differences to this work? ", "rating": "7: Good paper, accept", "reply_text": "Experiments : usually , when we set the learning rate to be too small , it is impossible to move the parameters far enough from their initializations , and so we obtain very poor performance . One of the interesting things about our framework is that as eta goes to zero , it converges to Bayesian inference without the dynamical prior , which , in effect , gives an adaptive learning rate that goes as the sum-of-square gradients . In practice , we still expect to need to tune the learning rate to find the minimum of the curves in Fig.2 ( now Fig.3 ) .As regards toy experiments , we expect the benefits of our approach to become more evident in complex models with strong posterior correlations , so the simplest relevant toy experiment is linear regression with highly correlated inputs -- - which is n't all that simple . Regarding convergence , tried a range of numbers of epochs , and found that the displayed curves are pretty stable at 50 epochs , but that if anything , the difference between the Bayesian and classical methods actually increased for larger number of epochs . We have discussed the ADF-style approach ( Ghosh et al . ) , and have noted that the Kalman filtering approach is well-understood . Finally , it should be noted that Khan et al . ( 2018 ) does not recover the root-mean-square-gradient form for the normalizer . To quote from Khan et al . ( 2018 ) : `` Using ... an additional modification in the VON update , we can make the VON update very similar to RMSprop . Our modification involves taking the square-root over s_ { t+1 } in ( 7 ) '' Both they and Zhang et al . ( 2017 ) of these approches use natural gradient VI , and they both encounter the same problem : that natural gradient gives you a mean-square-gradient , rather than a root-mean-square gradient form for the normalizer . Khan et al . ( 2018 ) deal with this by reaching in and replacing the mean-square with a root-mean-square normalizer ( without a principled justification based on approximate inference ) , and Zhang et al . ( 2017 ) regard the difference between a root-mean-square and a mean-square gradient normalizer as `` inessential '' ."}}