{"year": "2021", "forum": "o29tNZZqGcN", "title": "Bridging Graph Network to Lifelong Learning with Feature Interaction", "decision": "Reject", "meta_review": "The reviewers initially assessed this paper as slightly below the acceptance threshold. The reviewers seem to agree on the novelty and potential impact of this project, but they also highlighted the lack of clarity of the manuscript including lack of clarity in the method used to encode the graph data. \n\nAs the authors noted, graph-related questions were the focus of most of the comments and questions from the reviewers. This is not because the reviewers did not understand and assess the method from the continual-learning side (I am also meta-reviewing several continual-learning papers and I believe that I can assess the novelty of this work). As I wrote above, reviewers were convinced of the paper's motivation. \n\nThe authors provided good responses and discussed with at least one reviewer thoroughly. These interactions seem to have clarified important aspects of your proposed methodology and notably the properties of your graph-construction method. I found that your new results on larger datasets also provide an improvement. However, to be properly assessed, this number of clarifications regarding the core method requires a new round of reviews. The discussions have also highlighted some of the limits of your approach which do not seem to be acknowledged in your paper. This includes the discussion with reviewer2 regarding constraints on L & K, node classification (also I find that one to less important), and comparison to GraphSage on the non-lifelong learning scenario.\n\nOverall, and while I agree that continual learning from graph data is an important and unexplored problem, I also find that the current manuscript lacks clarity and, even though the ICLR discussion allowed reviewers to discuss these with the authors, there are still significant ways to improve the clarity of the current manuscript. As a result, I do not recommend acceptance of the current manuscript. \n\nI strongly suggest the authors keep on working on their manuscript as their idea seems to have potential and I would imagine that it may become one of the first works in a new interesting line of research.", "reviews": [{"review_id": "o29tNZZqGcN-0", "review_text": "This paper aims to extend GCN to the lifelong learning setting . The idea is to transform the nodes of a graph into feature graphs where each feature is a node and the edges represent feature correlations estimated from the K-hop neighborhood of a node in the original graph . [ Pros ] The idea of converting nodes of a graph into feature graphs is novel and interesting ( as far as I know ) . In this way , the model depends on feature graphs rather than the original graph which continually grows in the online learning setting . This breaks down the original graph into individual nodes and their contexts which are then fed to the model to generate results . Very interesting . [ Cons ] However , some major issues still hinder the paper from publication : ( 1 ) There is a lack of necessary details . First , the constructed adjacency matrix $ A^F_ { k , c } $ ( Eq . ( 5 ) ) has two subscripts $ k $ and $ c $ . But the subscript $ c $ just disappears in Eq . ( 9 ) without any explanation . Therefore , it is not known how the final adjacency matrix is built in Eq . ( 9 ) from Eq . ( 5 ) . Second , how do you set the edge wights of the original graph ? Moreover , at first the problem formulation says each edge is associated with a weight vector , but it seems finally it is just a scalar . This is a little confusing . The above issues affect the reproducibility of this work . ( 2 ) The notations are hard to follow . For example , in the definition of the graph lifelong learning , the weight vector set $ \\mathcal { W } $ is said to contain weight vectors associated with the $ K $ -hop ( Strictly speaking , I think here it should be $ K $ rather than $ k $ .More about this issue below ) neighbors . Does this mean these weights are associated with nodes rather than edges ? According to the notation of $ \\mathcal { W } $ ( $ k=1 : K $ ) , I think $ \\mathcal { W } $ contains neighbors up to $ K $ -hop rather than only the $ K $ -th hop . The use of $ k $ and $ K $ is very confusing , which makes related content hard to understand . Another question is , how do you set the parameter $ K $ ? The last sentence said \u201c a maximum $ k=1 $ is required ... \u201d . I am not sure whether this is how $ K $ is set . Furthermore , what do you mean by maximum ? Is it possible that $ K=0 $ ? The last question is , how do you set the target vector $ z $ for the training data ? I can not find the related content . ( 3 ) What is the motivation of the design of the feature transform layer ? Why do we need to change the number of feature nodes ? The motivation is not clear . Furthermore , there is no comparison to using broadcast layers in experiments . In the appendix F , why do you use the Flickr dataset for evaluating transform layers ? Why not use this dataset in the main experiments ? Some minor issues : ( 1 ) The writing needs some improvement . Some language errors include \u201c an importance \u201d , two sentences in a sentence . ( 2 ) What is the motivation of using sgnroot in Eq . ( 5 ) ? ( 3 ) Traditional methods also exploit the interactions between features in a neighborhood , by concatenating feature vector of the target node and the aggregated neighborhood feature vector and do affine transformation . Hence , I think \u201c existing methods can not model feature interactions well \u201d is not precise . Besides , \u201c useful information might be encoded in one \u2019 s neighbor features \u201d is vague . Some concrete examples are needed to motivate the modeling of feature interactions in node neighborhoods .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We are excited that R4 believes that the idea of converting nodes of a graph into feature graphs is novel and very interesting . We next address your questions , respectively . Q1 : The constructed adjacency matrix $ A_ { k , c } ^F $ in ( 5 ) has subscripts $ k $ and $ c $ . But $ c $ disappears in Eq . ( 9 ) without explanation . Therefore , it is not known how the final adjacency matrix is built in ( 9 ) from ( 5 ) . - In ( 5 ) , $ c $ is the channel index . Equation ( 9 ) is the definition of feature broadcast layer for * * a single channel * * , thus $ c $ is not included in ( 9 ) for simplicity . The layer computation is conducted in separated channels then the outputs are concatenated to produce the signal output . - The graph data in experiment contains only 1 channel . However , we still give the multi-channel definition in ( 5 ) for compatibility with multi-channel signals , e.g. , images have 3 channels , which will be useful for future works . - Thanks for your suggestions , we have updated Sec 4.3 to make this more clear . Q2 : How do you set the edge weights of the original graph ? Moreover , at first the problem formulation says each edge is associated with a weight vector , but it seems finally it is just a scalar . This is a little confusing . The above issues affect the reproducibility of this work . - The citation graph used in experiments have a predefined edge weight : 1 or 0 indicates whether two articles ( nodes ) are references of each other . The flickr dataset also has a predefined weight : 1 or 0 indicate whether two images ( nodes ) are from the same location , submitted to the same gallery , group , or set , etc . - We define the multi-channel edge weights in ( 5 ) also for compatibility with such graph data , but not used in this paper . In this way , we expect our method can inspire more research in the future . - We have updated the paper to clarify . We will release all source codes and pre-trained models to ensure the reproducibility of this work . Q3 : In the definition of the graph lifelong learning , the weight vector set $ \\mathcal { W } $ is said to contain weight vectors associated with the K-hop neighbors . Does this mean these weights are associated with nodes rather than edges ? - The weights are still associated with edges , while its availability in the continuum in Eq . ( 1 ) is associated with the nodes , as defined in the continuum . Q4 : According to the notation of W ( k=1 : K ) , I think W contains neighbors up to K-hop rather than only the K-th hop . The use of k and K is very confusing , which makes related content hard to understand . How do you set the parameter K ? The last sentence said \u201c a maximum k=1 is required ... \u201d . I am not sure whether this is how K is set . Furthermore , what do you mean by maximum ? Is it possible that K=0 ? - Yes , $ \\mathcal { W } $ contains information up to K-th hop neighbor . - $ K $ is always 1 in the experiments . We have made this more clear this in Sec.3.Q5 : How do you set the target vector z for the training data ? - Since it is a classification problem , we simply use the one-hot vector as the target vector . We have updated Appendix C the clarify it . Q6 : What is the motivation of the design of the feature transform layer ? Why do we need to change the number of feature nodes ? In the appendix F , why do you use the Flickr dataset for evaluating feature transform layers ? - The feature transform layer defined in App F is able to further reduce the number of parameters if the number of feature nodes are reduced/changed layer by layer . ( Although we shown in Sec.5.4 that the featur broadcast layer requires the least number of parameters . ) - Flickr is a very large dataset . For efficiency , we use the feature transform layer to further reduce the number of parameters . - We have updated the Appendix F to make this motivation more clear . Some minor issues : Q7 : What is the motivation of using $ \\text { sgnroot } $ in Eq . ( 5 ) ? - Inside the operator , it is $ x\\cdot y^T $ , thus we use $ \\text { root } $ to get the number magnitude of $ x $ and $ y $ . - We want to keep the sign of the correlation $ x\\cdot y^T $ , but $ \\text { root } $ can not get real values for negative inputs , thus we put the operator $ \\text { sign } $ outside of the $ \\text { root } $ operator . - The above insights inspired us to define the operator $ \\text { sgnroot } =\\text { sign } ( x ) \\sqrt ( \\left|x\\right| ) $ . Q8 : Some language errors and some statement are not precise and vague . - Thanks for your reminder , we have revised those statements in Sec 1 ."}, {"review_id": "o29tNZZqGcN-1", "review_text": "Summary : This paper aims to bridge GNNs with life-long learning so that the catastrophic forgetting problem in graph-structured tasks is alleviated . Specifically , the major contribution seems to be transforming the original graph into a feature graph so that the node classification problem is transferred into a graph classification problem with isolated samples . Meanwhile , feature interactions are modeled in constructing edges of the feature graph . Experiments on three citation graphs demonstrate the effectiveness of the proposed method . Pros : ( + ) GNNs + lifelong learning seems to be a novel problem that has been seldomly studied . ( + ) The proposed framework can be applied to different GNNs . ( + ) The paper is overall well written and easy to follow . Negative points are as follows : ( 1 ) Converting the original graph into a feature graph is not adequately justified and may have severe drawbacks . The main motivation for such a conversion is to transform connected nodes in the original graph into isolated samples so that the existing lifelong learning methods can be applied . However , the whole point of using GNNs is to pass and exchange messages between different nodes . If each node is regarded as an isolated sample , the relationships between nodes are completed ignored ( except the feature co-occurrence statistics ) . In other words , the resulted model is essentially feature-centric and basically does not preserve any structural information in the original graph . For example , it is highly likely that the proposed model can not preserve motifs or structural roles of nodes , nor to handle structural-driven tasks such as link prediction . The authors need to further clarify this major model design . ( 2 ) Following how to convert nodes into isolated samples in GNNs , a well-known method is to regard each node as an ego-graph , since the representation of a node in GNNs only depends on its k-hop neighbors ( see GraphSAGE and [ 1-2 ] ) . Such a conversion will also transform the node classification problem into a graph classification problem , similar to the paper \u2019 s arguments , but without losing structural information . Thus I am wondering whether or why such a method can not be directly applied in the lifelong setting ( from Section 3 , the k-hop neighbors \u2019 information should be available ) . ( 3 ) In experiments , the authors adopt three citations graphs . Though I acknowledge they were commonly used as benchmarks in GNNs , recent studies suggest these small datasets may be not adequate in comparing different methods [ 3-5 ] . Thus , more experiments on larger datasets may be needed . ( The experimental results on the Flickr dataset in the Appendix are puzzling since the results show that not using memory outperforms using memory , indicating that graph lifelong learning may even not be a proper setting . ) ( 4 ) There are also a few missing related works [ 6-7 ] . Minor : ( 1 ) In related works , JK-Net and DiffPool are GNN architectures ( proposing jumping connections and differentiable pooling ) rather than new sampling techniques . ( 2 ) It should be noted that building feature graphs and considering feature interactions in GNNs have also been studied recently , see [ 8-10 ] . But since they are informal publications or very recent w.r.t.the submission , I only suggest the authors compare them in an updated version and do not consider this as a negative point . [ 1 ] Link Prediction Based on Graph Neural Networks , NeurIPS 2018 [ 2 ] Graph Meta Learning via Local Subgraphs , arXiv:2006.07889 [ 3 ] Pitfalls of Graph Neural Network Evaluation , arXiv:1811.05868 [ 4 ] Open Graph Benchmark : Datasets for Machine Learning on Graphs , arXiv : 2005.00687 . [ 5 ] Benchmarking Graph Neural Networks , arXiv : 2003.00982 [ 6 ] Lifelong representation learning in dynamic attributed networks , Neurocomputing 2019 . [ 7 ] Streaming Graph Neural Networks via Continual Learning , CIKM 2019 . [ 8 ] Cross-GCN : Enhancing Graph Convolutional Network with k-Order Feature Interactions , arXiv:2003.02587 . [ 9 ] CatGCN : Graph Convolutional Networks with Categorical Node Features , arXiv:2009.05303 . [ 10 ] AM-GCN : Adaptive Multi-channel Graph Convolutional Networks , KDD 2020 . Based on the above comments , I am currently leaning towards rejection . I am happy to improve my scores if the authors can further justify their proposed method .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for agreeing that \u201c GNNs + lifelong learning seems to be * * a novel problem * * that has been seldomly studied '' , `` The proposed framework * * can be applied to different GNNs * * . '' and `` The paper is * * overall well written and easy to follow * * . '' We next address your concerns respectively . ( 1 ) This method does not preserve any structural information in the original graph . - This is not true . Feature graph encodes the structural information of the original graph into the feature adjacence matrix $ A_ { k , c } ^ { \\mathcal { F } } $ as defined in Eq . ( 5 ) . $ $ A_ { k , c } ^ { \\mathcal { F } } ( x ) \\triangleq \\operatorname { sgnroot } \\left ( \\mathbb { E } \\left [ w_ { x , y } x_ { [ : ,c ] } y_ { [ : ,c ] } ^ { \\rm { T } } \\right ] \\right ) , \\quad \\forall y \\in N_k ( x ) $ $ where $ k $ is the k-hop neighbors of the original graph . In this setting , the structural information is encoded by the cross-correlation with neighbors $ \\mathbf { y } $ , meaning that * * the structural information is retained in another form * * . ( 2 ) Methods like GraphSAGE will not lose structural information and I am wondering whether or why such a method can not be directly applied in the lifelong setting . - As answered in the first question , we do n't lose the structural information . - As mentioned in 2nd paragraph of Sec 5 , GraphSAGE needs to traverse the entire graph for every layer which is impossible for lifelong learning , but it is possible to be applied to the lifelong learning via slight modification ( see more details in Sec 5 ) . However , its performance is much lower than our method . This was what we showed in Sec 5 : - In Table 2 , the feature graph achieves 5.5 % , 1.9 % , and 5.6 % higher accuracy than GraphSAGE for data-incremental learning tasks . - In Table 3 , we achieves an average of an average of 5.8 % higher accuracy than GraphSAGE for class incremental tasks . - Table 4 shows that our method only requires $ \\frac { 1 } { 10 } $ parameters of GraphSAGE . ( 3 ) More datasets test excluding Cora , Citeseer , and Pubmed are needed , since the Flickr dataset test in Appendix is puzzling : the results show that not using memory outperforms using memory . - We explained this phenomenon in * * Page 14 * * : the overfitting effect . This is because we adopt a simple memory reply strategy following ( Aljundi et al. , 2019b ) : replaying the memory samples ( updating the parameters by the memory samples ) after each time we update parameters by new samples from the continuum , thus a larger memory or dataset leads to more learning steps and easier overfitting . - We could overcome this phenomenon , for example , by updating parameters from memory after receiving * * more * * samples in the continuum . We believe that a better-designed updating strategy is able to further improve the performance , but it is out the scope of this paper , as we only focus on the feature graphs . - In * * Sec F * * : Even a non-continual learning method GraphSIANT achieves only an upper bound accuracy of 0.51 on * * Flickr * * , while we achieve an average accuracy of 0.470 and 0.468 for the data-incremental and class-incremental tasks , respectively . * * The performance on Flickr verifies the effectiveness of our method . * * ( 4-6 ) Missing related works [ 6,7 ] and other minor issues . - We have added the related work [ 6 ] in the new version and find [ 7 ] is published in CIKM 2020 ( 2020 Oct , arXiv is 2020 Sep 23rd ) . - Thanks for your suggestions and we have corrected the context of JK-Net and DiffPool in the new version ."}, {"review_id": "o29tNZZqGcN-2", "review_text": "This paper aims to solve the problem of lifelong graph learning . Thus far , the topic about graph learning and lifelong learning is still underexplored . This paper proposes a new graph topology based on feature interaction , which takes the features as nodes and turns the nodes into graphs , and thus formulates a regular lifelong learning problem by defining the feature graph continuum . The authors conduct experiments on three popular citation graph datasets including Cora , Citeseer , and Pubmed . Pros : 1.This paper presents a novel strategy to transform the regular graph to a feature graph . This converts the original problem of node classification to graph classification , where the increasing nodes are turned into training samples . It makes the graph learning applicable to the continual learning . Cons : 1.In Section 1 , the authors mention that -- \u201c It takes the features as nodes and turns the nodes into graphs . This converts the problem of node classification to graph classification . In this way , the increasing nodes become training samples \u201d . In the lifelong learning , this strategy will not only increase the node samples but also the edges between new and old nodes . Although the feature graph continuum and random sample rehearsal strategy are proposed , scalability might still be a concern . 2.It is unclear whether the proposed feature graph and feature adjacency matrix could effectively capture useful information from neighbors . More justifications and theoretical analysis shall be provided . In addition , evaluating the performance of feature graph in some traditional graph learning tasks would be helpful . 3.In the experiments , the authors only compare their method with the modified GraphSAGE method . Although this topic about the combination of graph learning and continual learning is relatively new , there exists several relevant papers such as Continual Graph Learning ( March , 2020 ) . It would be more convincing if the authors could conduct comparisons with other graph continual learning or lifelong learning methods .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for agreeing that \u201c this paper presents * * a novel strategy * * to transform the regular graph to a feature graph \u201d and \u201c * * It makes the graph learning applicable to the continual learning * * . \u201d We next address your concerns respectively . ( 1 ) This strategy will not only increase the node samples but also the edges between new and old nodes . Although the feature graph continuum and random sample rehearsal strategy are proposed , scalability might still be a concern . - Noted that each regular node $ x\\in R^ { F\\times C } $ is associated to $ F $ feature nodes $ x_i\\in R^C $ . This means that a feature node is just one feature of a regular node . Therefore , our strategy does * * not * * increase the total number of data element $ E=FC $ . - Feature graphs have * * constant * * ( $ F\\times ( F-1 ) /2 $ ) number of edges and we provided complexity analysis in * * Sec 4.4 * * : feature graph roughly has complexity of $ O ( E^2+nF^2 ) $ , where $ n $ is the expected number of neighbors in the continuum . Note that they are irrelevant to the task number , thus it has * * constant complexity * * with the increased learning tasks/samples . - We also provided model size comparison in * * Sec 5.4 * * , which shown that our method only requires $ \\frac { 1 } { 10 } $ parameters , which improves the scalability . - The rehearsal strategy we used has * * constant memory consumption * * ( fixed number of samples ) and we show in * * Sec 5 * * that it reduces the forgetting issue by a large margin . - Our strategy converts the increasing samples into a continuum , which is a basic setting of continual learning , in which the forgetting issue rather than the scalability is the challenge . Other continual learning methods that do not require memory are also applicable to our methods . ( 2 ) It is unclear whether the proposed feature graph and feature adjacency matrix could effectively capture useful information from neighbors . More justifications and theoretical analysis shall be provided . Evaluating the performance of feature graph in some traditional graph learning tasks would be helpful . - We used the cross-correlation matrix to capture the relationship from neighbors . To demonstarte its * * effectiveness * * , we first assume that it can effectively capture the useful information by taking it as an adjacency matrix , we then established the model and * * verified this assumption * * by showing that this structure achieves an average of * * 5 % higher * * accuracy in graph continual learning ( Sec 5 ) . - We agree it is still an open challenge to theoretically evaluate whether a method could effectively capture useful information , and this challenge exists for many other machine learning algorithms ( Emperical justifications are widely used , which is also what we provided ) . It is a very interesting topic and we would like to conduct it in the future . - As in the * * third paragraph of Sec 5 * * : we achieved a little bit higher performance than GraphSage in the * * traditional graph learning tasks * * , but much higher performance in continual graph learning . This further demonstrated the effectiveness of our method in continual learning . ( 3 ) It would be more convincing if the authors could conduct comparisons with other graph continual learning or lifelong learning methods , such as Continual Graph Learning ( March , 2020 ) . - Thanks for your suggetion , but `` Continual Graph Learning ( * * CGN * * ) '' is an arXiv paper and not published yet . It has a very different formulation and tested several exemplar selection methods ( * * Sec 2.1 * * ) . Moreover , it does n't provide implementation details , which makes it difficult to compare . - To the best our knowledge , feature graph is one of the first methods that make continual learning techniques applicable to graph learning ."}, {"review_id": "o29tNZZqGcN-3", "review_text": "The continuous learning on the graph neural network is restricted by the mechanism of the graph neural network itself . In order to enable the continuous learning approaches to be directly applied to GNN , the authors propose Feature Graph , which converts the node classification problem into graph classification by converting nodes into graphs . Therefore , we can apply current lifelong learning techniques to GNNs . Experimental results show that the proposed method overperforms the baselines . The main contribution of the paper is the idea of the converting technique , which makes it possible to employ existing methods to deal with the problem of continuous learning on the graph neural network . Overall , the paper is well written and is easy to follow . Concerns : 1 . Using the feature 's cross-correlation matrix as the adjacency matrix of the neural network on the node sounds reasonable , but the validity of this idea lacks experimental explanation . In addition , how much do features contain the structure information of a graph ? It is not discussed in this paper . 2.The dynamic increase of nodes is the main problem that limits the application of continuous learning approaches in graph neural networks . This problem still exists in Feature Graph . The author should discuss it . 3.The usage of task descriptors should be clarified , since their different usages may have a great impact on difficulty of problem 4 . The experiments are not solid enough . Although Table 2 & 3 show positive result supporting proposed method , the experimental results are too brief to be convincing . Other results such as accuracy curves , or other baselines such as joint training should be provided . In addition , Cora , Citeseer and Pubmed are small graph datasets . Experiments on big graph datasets should be conducted . Minor comments : 1 . In Formula ( 9 ) , the parentheses are redundant .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks very much for accepting our paper ! We next address your concerns respectively . ( 1 ) Using the feature 's cross-correlation matrix as the adjacency matrix of the neural network on the node sounds reasonable , but the validity of this idea lacks experimental explanation . In addition , how much do features contain the structure information of a graph ? - We agree that it is very difficult to directly measure how much do features contain structural information . Therefore , we resort to emperical validation : - We first assume that the cross-correlation matrix is able to capture useful information for graph lifelong learning . - To verify this assumption , we first establish the model following the assumption and then show in Sec 5 : with the cross-correlation matrix as the adjacency matrix , our feature graphs achieve much higher accuracy in lifelong graph learning than the method without taking the cross-correlatioin as adjacency matrix . These results successfully validate our assumption and provides the experimental explanation . ( 2 ) The dynamic increase of nodes is the main problem that limits the application of continuous learning approaches in graph neural networks . This problem still exists in Feature Graph . The author should discuss it . - The dynamic increase of nodes is converted into a continuum of feature graphs using our method . - The continuum is a basic setting of continual learning ( Aljundi et al. , 2019b ) , thus the problem of dynamic increase nodes can be solved/alleviated by applying the continual learining techniques . - The main challenge of continual learning from the continuum is the * * forgetting issue * * : the learned knowledge from earlier continuum may be easily forgotten since the earlier data can not be available anymore . - We show in Sec.5 that feature graphs achieve very low forgetting rate . - We have made this more clear in the new version ( Sec 4.2 ) . ( 3 ) The usage of task descriptors $ t_i $ should be clarified , since their different usages may have a great impact on difficulty of problem . - The task decriptors $ t_i $ in the experiments are integers numerating the tasks ( at the end of Sec.3 of initial version ) , which is equivalent to the class labels of samples $ y_i $ . In other words , we do n't need $ t_i $ during both training and testing ( we only need sample labels $ y_i $ during training ) . - However , we still define the task descriptors for compatibility with some continual learning techniques , which require such information . - We have made this more clear in the new version ( Sec.3 ) . ( 4 ) Although Table 2 & 3 show positive result supporting proposed method , the experimental results are too brief to be convincing . Other results such as accuracy curves , or other baselines such as joint training should be provided . In addition , Cora , Citeseer and Pubmed are small graph datasets . Experiments on big graph datasets should be conducted . - We added a per-class precision figure and a per-class forgetting rate comparison figure in Fig 3 ( a ) and Fig.3 ( b ) in the new version . - We also showed the training accuracy curves in Fig 2 ( a ) and Fig.2 ( b ) .A large dataset experiment , Flickr , was conducted in Appendix F. - Thanks for your suggestion , we will conduct more experiments in the future ."}], "0": {"review_id": "o29tNZZqGcN-0", "review_text": "This paper aims to extend GCN to the lifelong learning setting . The idea is to transform the nodes of a graph into feature graphs where each feature is a node and the edges represent feature correlations estimated from the K-hop neighborhood of a node in the original graph . [ Pros ] The idea of converting nodes of a graph into feature graphs is novel and interesting ( as far as I know ) . In this way , the model depends on feature graphs rather than the original graph which continually grows in the online learning setting . This breaks down the original graph into individual nodes and their contexts which are then fed to the model to generate results . Very interesting . [ Cons ] However , some major issues still hinder the paper from publication : ( 1 ) There is a lack of necessary details . First , the constructed adjacency matrix $ A^F_ { k , c } $ ( Eq . ( 5 ) ) has two subscripts $ k $ and $ c $ . But the subscript $ c $ just disappears in Eq . ( 9 ) without any explanation . Therefore , it is not known how the final adjacency matrix is built in Eq . ( 9 ) from Eq . ( 5 ) . Second , how do you set the edge wights of the original graph ? Moreover , at first the problem formulation says each edge is associated with a weight vector , but it seems finally it is just a scalar . This is a little confusing . The above issues affect the reproducibility of this work . ( 2 ) The notations are hard to follow . For example , in the definition of the graph lifelong learning , the weight vector set $ \\mathcal { W } $ is said to contain weight vectors associated with the $ K $ -hop ( Strictly speaking , I think here it should be $ K $ rather than $ k $ .More about this issue below ) neighbors . Does this mean these weights are associated with nodes rather than edges ? According to the notation of $ \\mathcal { W } $ ( $ k=1 : K $ ) , I think $ \\mathcal { W } $ contains neighbors up to $ K $ -hop rather than only the $ K $ -th hop . The use of $ k $ and $ K $ is very confusing , which makes related content hard to understand . Another question is , how do you set the parameter $ K $ ? The last sentence said \u201c a maximum $ k=1 $ is required ... \u201d . I am not sure whether this is how $ K $ is set . Furthermore , what do you mean by maximum ? Is it possible that $ K=0 $ ? The last question is , how do you set the target vector $ z $ for the training data ? I can not find the related content . ( 3 ) What is the motivation of the design of the feature transform layer ? Why do we need to change the number of feature nodes ? The motivation is not clear . Furthermore , there is no comparison to using broadcast layers in experiments . In the appendix F , why do you use the Flickr dataset for evaluating transform layers ? Why not use this dataset in the main experiments ? Some minor issues : ( 1 ) The writing needs some improvement . Some language errors include \u201c an importance \u201d , two sentences in a sentence . ( 2 ) What is the motivation of using sgnroot in Eq . ( 5 ) ? ( 3 ) Traditional methods also exploit the interactions between features in a neighborhood , by concatenating feature vector of the target node and the aggregated neighborhood feature vector and do affine transformation . Hence , I think \u201c existing methods can not model feature interactions well \u201d is not precise . Besides , \u201c useful information might be encoded in one \u2019 s neighbor features \u201d is vague . Some concrete examples are needed to motivate the modeling of feature interactions in node neighborhoods .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We are excited that R4 believes that the idea of converting nodes of a graph into feature graphs is novel and very interesting . We next address your questions , respectively . Q1 : The constructed adjacency matrix $ A_ { k , c } ^F $ in ( 5 ) has subscripts $ k $ and $ c $ . But $ c $ disappears in Eq . ( 9 ) without explanation . Therefore , it is not known how the final adjacency matrix is built in ( 9 ) from ( 5 ) . - In ( 5 ) , $ c $ is the channel index . Equation ( 9 ) is the definition of feature broadcast layer for * * a single channel * * , thus $ c $ is not included in ( 9 ) for simplicity . The layer computation is conducted in separated channels then the outputs are concatenated to produce the signal output . - The graph data in experiment contains only 1 channel . However , we still give the multi-channel definition in ( 5 ) for compatibility with multi-channel signals , e.g. , images have 3 channels , which will be useful for future works . - Thanks for your suggestions , we have updated Sec 4.3 to make this more clear . Q2 : How do you set the edge weights of the original graph ? Moreover , at first the problem formulation says each edge is associated with a weight vector , but it seems finally it is just a scalar . This is a little confusing . The above issues affect the reproducibility of this work . - The citation graph used in experiments have a predefined edge weight : 1 or 0 indicates whether two articles ( nodes ) are references of each other . The flickr dataset also has a predefined weight : 1 or 0 indicate whether two images ( nodes ) are from the same location , submitted to the same gallery , group , or set , etc . - We define the multi-channel edge weights in ( 5 ) also for compatibility with such graph data , but not used in this paper . In this way , we expect our method can inspire more research in the future . - We have updated the paper to clarify . We will release all source codes and pre-trained models to ensure the reproducibility of this work . Q3 : In the definition of the graph lifelong learning , the weight vector set $ \\mathcal { W } $ is said to contain weight vectors associated with the K-hop neighbors . Does this mean these weights are associated with nodes rather than edges ? - The weights are still associated with edges , while its availability in the continuum in Eq . ( 1 ) is associated with the nodes , as defined in the continuum . Q4 : According to the notation of W ( k=1 : K ) , I think W contains neighbors up to K-hop rather than only the K-th hop . The use of k and K is very confusing , which makes related content hard to understand . How do you set the parameter K ? The last sentence said \u201c a maximum k=1 is required ... \u201d . I am not sure whether this is how K is set . Furthermore , what do you mean by maximum ? Is it possible that K=0 ? - Yes , $ \\mathcal { W } $ contains information up to K-th hop neighbor . - $ K $ is always 1 in the experiments . We have made this more clear this in Sec.3.Q5 : How do you set the target vector z for the training data ? - Since it is a classification problem , we simply use the one-hot vector as the target vector . We have updated Appendix C the clarify it . Q6 : What is the motivation of the design of the feature transform layer ? Why do we need to change the number of feature nodes ? In the appendix F , why do you use the Flickr dataset for evaluating feature transform layers ? - The feature transform layer defined in App F is able to further reduce the number of parameters if the number of feature nodes are reduced/changed layer by layer . ( Although we shown in Sec.5.4 that the featur broadcast layer requires the least number of parameters . ) - Flickr is a very large dataset . For efficiency , we use the feature transform layer to further reduce the number of parameters . - We have updated the Appendix F to make this motivation more clear . Some minor issues : Q7 : What is the motivation of using $ \\text { sgnroot } $ in Eq . ( 5 ) ? - Inside the operator , it is $ x\\cdot y^T $ , thus we use $ \\text { root } $ to get the number magnitude of $ x $ and $ y $ . - We want to keep the sign of the correlation $ x\\cdot y^T $ , but $ \\text { root } $ can not get real values for negative inputs , thus we put the operator $ \\text { sign } $ outside of the $ \\text { root } $ operator . - The above insights inspired us to define the operator $ \\text { sgnroot } =\\text { sign } ( x ) \\sqrt ( \\left|x\\right| ) $ . Q8 : Some language errors and some statement are not precise and vague . - Thanks for your reminder , we have revised those statements in Sec 1 ."}, "1": {"review_id": "o29tNZZqGcN-1", "review_text": "Summary : This paper aims to bridge GNNs with life-long learning so that the catastrophic forgetting problem in graph-structured tasks is alleviated . Specifically , the major contribution seems to be transforming the original graph into a feature graph so that the node classification problem is transferred into a graph classification problem with isolated samples . Meanwhile , feature interactions are modeled in constructing edges of the feature graph . Experiments on three citation graphs demonstrate the effectiveness of the proposed method . Pros : ( + ) GNNs + lifelong learning seems to be a novel problem that has been seldomly studied . ( + ) The proposed framework can be applied to different GNNs . ( + ) The paper is overall well written and easy to follow . Negative points are as follows : ( 1 ) Converting the original graph into a feature graph is not adequately justified and may have severe drawbacks . The main motivation for such a conversion is to transform connected nodes in the original graph into isolated samples so that the existing lifelong learning methods can be applied . However , the whole point of using GNNs is to pass and exchange messages between different nodes . If each node is regarded as an isolated sample , the relationships between nodes are completed ignored ( except the feature co-occurrence statistics ) . In other words , the resulted model is essentially feature-centric and basically does not preserve any structural information in the original graph . For example , it is highly likely that the proposed model can not preserve motifs or structural roles of nodes , nor to handle structural-driven tasks such as link prediction . The authors need to further clarify this major model design . ( 2 ) Following how to convert nodes into isolated samples in GNNs , a well-known method is to regard each node as an ego-graph , since the representation of a node in GNNs only depends on its k-hop neighbors ( see GraphSAGE and [ 1-2 ] ) . Such a conversion will also transform the node classification problem into a graph classification problem , similar to the paper \u2019 s arguments , but without losing structural information . Thus I am wondering whether or why such a method can not be directly applied in the lifelong setting ( from Section 3 , the k-hop neighbors \u2019 information should be available ) . ( 3 ) In experiments , the authors adopt three citations graphs . Though I acknowledge they were commonly used as benchmarks in GNNs , recent studies suggest these small datasets may be not adequate in comparing different methods [ 3-5 ] . Thus , more experiments on larger datasets may be needed . ( The experimental results on the Flickr dataset in the Appendix are puzzling since the results show that not using memory outperforms using memory , indicating that graph lifelong learning may even not be a proper setting . ) ( 4 ) There are also a few missing related works [ 6-7 ] . Minor : ( 1 ) In related works , JK-Net and DiffPool are GNN architectures ( proposing jumping connections and differentiable pooling ) rather than new sampling techniques . ( 2 ) It should be noted that building feature graphs and considering feature interactions in GNNs have also been studied recently , see [ 8-10 ] . But since they are informal publications or very recent w.r.t.the submission , I only suggest the authors compare them in an updated version and do not consider this as a negative point . [ 1 ] Link Prediction Based on Graph Neural Networks , NeurIPS 2018 [ 2 ] Graph Meta Learning via Local Subgraphs , arXiv:2006.07889 [ 3 ] Pitfalls of Graph Neural Network Evaluation , arXiv:1811.05868 [ 4 ] Open Graph Benchmark : Datasets for Machine Learning on Graphs , arXiv : 2005.00687 . [ 5 ] Benchmarking Graph Neural Networks , arXiv : 2003.00982 [ 6 ] Lifelong representation learning in dynamic attributed networks , Neurocomputing 2019 . [ 7 ] Streaming Graph Neural Networks via Continual Learning , CIKM 2019 . [ 8 ] Cross-GCN : Enhancing Graph Convolutional Network with k-Order Feature Interactions , arXiv:2003.02587 . [ 9 ] CatGCN : Graph Convolutional Networks with Categorical Node Features , arXiv:2009.05303 . [ 10 ] AM-GCN : Adaptive Multi-channel Graph Convolutional Networks , KDD 2020 . Based on the above comments , I am currently leaning towards rejection . I am happy to improve my scores if the authors can further justify their proposed method .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for agreeing that \u201c GNNs + lifelong learning seems to be * * a novel problem * * that has been seldomly studied '' , `` The proposed framework * * can be applied to different GNNs * * . '' and `` The paper is * * overall well written and easy to follow * * . '' We next address your concerns respectively . ( 1 ) This method does not preserve any structural information in the original graph . - This is not true . Feature graph encodes the structural information of the original graph into the feature adjacence matrix $ A_ { k , c } ^ { \\mathcal { F } } $ as defined in Eq . ( 5 ) . $ $ A_ { k , c } ^ { \\mathcal { F } } ( x ) \\triangleq \\operatorname { sgnroot } \\left ( \\mathbb { E } \\left [ w_ { x , y } x_ { [ : ,c ] } y_ { [ : ,c ] } ^ { \\rm { T } } \\right ] \\right ) , \\quad \\forall y \\in N_k ( x ) $ $ where $ k $ is the k-hop neighbors of the original graph . In this setting , the structural information is encoded by the cross-correlation with neighbors $ \\mathbf { y } $ , meaning that * * the structural information is retained in another form * * . ( 2 ) Methods like GraphSAGE will not lose structural information and I am wondering whether or why such a method can not be directly applied in the lifelong setting . - As answered in the first question , we do n't lose the structural information . - As mentioned in 2nd paragraph of Sec 5 , GraphSAGE needs to traverse the entire graph for every layer which is impossible for lifelong learning , but it is possible to be applied to the lifelong learning via slight modification ( see more details in Sec 5 ) . However , its performance is much lower than our method . This was what we showed in Sec 5 : - In Table 2 , the feature graph achieves 5.5 % , 1.9 % , and 5.6 % higher accuracy than GraphSAGE for data-incremental learning tasks . - In Table 3 , we achieves an average of an average of 5.8 % higher accuracy than GraphSAGE for class incremental tasks . - Table 4 shows that our method only requires $ \\frac { 1 } { 10 } $ parameters of GraphSAGE . ( 3 ) More datasets test excluding Cora , Citeseer , and Pubmed are needed , since the Flickr dataset test in Appendix is puzzling : the results show that not using memory outperforms using memory . - We explained this phenomenon in * * Page 14 * * : the overfitting effect . This is because we adopt a simple memory reply strategy following ( Aljundi et al. , 2019b ) : replaying the memory samples ( updating the parameters by the memory samples ) after each time we update parameters by new samples from the continuum , thus a larger memory or dataset leads to more learning steps and easier overfitting . - We could overcome this phenomenon , for example , by updating parameters from memory after receiving * * more * * samples in the continuum . We believe that a better-designed updating strategy is able to further improve the performance , but it is out the scope of this paper , as we only focus on the feature graphs . - In * * Sec F * * : Even a non-continual learning method GraphSIANT achieves only an upper bound accuracy of 0.51 on * * Flickr * * , while we achieve an average accuracy of 0.470 and 0.468 for the data-incremental and class-incremental tasks , respectively . * * The performance on Flickr verifies the effectiveness of our method . * * ( 4-6 ) Missing related works [ 6,7 ] and other minor issues . - We have added the related work [ 6 ] in the new version and find [ 7 ] is published in CIKM 2020 ( 2020 Oct , arXiv is 2020 Sep 23rd ) . - Thanks for your suggestions and we have corrected the context of JK-Net and DiffPool in the new version ."}, "2": {"review_id": "o29tNZZqGcN-2", "review_text": "This paper aims to solve the problem of lifelong graph learning . Thus far , the topic about graph learning and lifelong learning is still underexplored . This paper proposes a new graph topology based on feature interaction , which takes the features as nodes and turns the nodes into graphs , and thus formulates a regular lifelong learning problem by defining the feature graph continuum . The authors conduct experiments on three popular citation graph datasets including Cora , Citeseer , and Pubmed . Pros : 1.This paper presents a novel strategy to transform the regular graph to a feature graph . This converts the original problem of node classification to graph classification , where the increasing nodes are turned into training samples . It makes the graph learning applicable to the continual learning . Cons : 1.In Section 1 , the authors mention that -- \u201c It takes the features as nodes and turns the nodes into graphs . This converts the problem of node classification to graph classification . In this way , the increasing nodes become training samples \u201d . In the lifelong learning , this strategy will not only increase the node samples but also the edges between new and old nodes . Although the feature graph continuum and random sample rehearsal strategy are proposed , scalability might still be a concern . 2.It is unclear whether the proposed feature graph and feature adjacency matrix could effectively capture useful information from neighbors . More justifications and theoretical analysis shall be provided . In addition , evaluating the performance of feature graph in some traditional graph learning tasks would be helpful . 3.In the experiments , the authors only compare their method with the modified GraphSAGE method . Although this topic about the combination of graph learning and continual learning is relatively new , there exists several relevant papers such as Continual Graph Learning ( March , 2020 ) . It would be more convincing if the authors could conduct comparisons with other graph continual learning or lifelong learning methods .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for agreeing that \u201c this paper presents * * a novel strategy * * to transform the regular graph to a feature graph \u201d and \u201c * * It makes the graph learning applicable to the continual learning * * . \u201d We next address your concerns respectively . ( 1 ) This strategy will not only increase the node samples but also the edges between new and old nodes . Although the feature graph continuum and random sample rehearsal strategy are proposed , scalability might still be a concern . - Noted that each regular node $ x\\in R^ { F\\times C } $ is associated to $ F $ feature nodes $ x_i\\in R^C $ . This means that a feature node is just one feature of a regular node . Therefore , our strategy does * * not * * increase the total number of data element $ E=FC $ . - Feature graphs have * * constant * * ( $ F\\times ( F-1 ) /2 $ ) number of edges and we provided complexity analysis in * * Sec 4.4 * * : feature graph roughly has complexity of $ O ( E^2+nF^2 ) $ , where $ n $ is the expected number of neighbors in the continuum . Note that they are irrelevant to the task number , thus it has * * constant complexity * * with the increased learning tasks/samples . - We also provided model size comparison in * * Sec 5.4 * * , which shown that our method only requires $ \\frac { 1 } { 10 } $ parameters , which improves the scalability . - The rehearsal strategy we used has * * constant memory consumption * * ( fixed number of samples ) and we show in * * Sec 5 * * that it reduces the forgetting issue by a large margin . - Our strategy converts the increasing samples into a continuum , which is a basic setting of continual learning , in which the forgetting issue rather than the scalability is the challenge . Other continual learning methods that do not require memory are also applicable to our methods . ( 2 ) It is unclear whether the proposed feature graph and feature adjacency matrix could effectively capture useful information from neighbors . More justifications and theoretical analysis shall be provided . Evaluating the performance of feature graph in some traditional graph learning tasks would be helpful . - We used the cross-correlation matrix to capture the relationship from neighbors . To demonstarte its * * effectiveness * * , we first assume that it can effectively capture the useful information by taking it as an adjacency matrix , we then established the model and * * verified this assumption * * by showing that this structure achieves an average of * * 5 % higher * * accuracy in graph continual learning ( Sec 5 ) . - We agree it is still an open challenge to theoretically evaluate whether a method could effectively capture useful information , and this challenge exists for many other machine learning algorithms ( Emperical justifications are widely used , which is also what we provided ) . It is a very interesting topic and we would like to conduct it in the future . - As in the * * third paragraph of Sec 5 * * : we achieved a little bit higher performance than GraphSage in the * * traditional graph learning tasks * * , but much higher performance in continual graph learning . This further demonstrated the effectiveness of our method in continual learning . ( 3 ) It would be more convincing if the authors could conduct comparisons with other graph continual learning or lifelong learning methods , such as Continual Graph Learning ( March , 2020 ) . - Thanks for your suggetion , but `` Continual Graph Learning ( * * CGN * * ) '' is an arXiv paper and not published yet . It has a very different formulation and tested several exemplar selection methods ( * * Sec 2.1 * * ) . Moreover , it does n't provide implementation details , which makes it difficult to compare . - To the best our knowledge , feature graph is one of the first methods that make continual learning techniques applicable to graph learning ."}, "3": {"review_id": "o29tNZZqGcN-3", "review_text": "The continuous learning on the graph neural network is restricted by the mechanism of the graph neural network itself . In order to enable the continuous learning approaches to be directly applied to GNN , the authors propose Feature Graph , which converts the node classification problem into graph classification by converting nodes into graphs . Therefore , we can apply current lifelong learning techniques to GNNs . Experimental results show that the proposed method overperforms the baselines . The main contribution of the paper is the idea of the converting technique , which makes it possible to employ existing methods to deal with the problem of continuous learning on the graph neural network . Overall , the paper is well written and is easy to follow . Concerns : 1 . Using the feature 's cross-correlation matrix as the adjacency matrix of the neural network on the node sounds reasonable , but the validity of this idea lacks experimental explanation . In addition , how much do features contain the structure information of a graph ? It is not discussed in this paper . 2.The dynamic increase of nodes is the main problem that limits the application of continuous learning approaches in graph neural networks . This problem still exists in Feature Graph . The author should discuss it . 3.The usage of task descriptors should be clarified , since their different usages may have a great impact on difficulty of problem 4 . The experiments are not solid enough . Although Table 2 & 3 show positive result supporting proposed method , the experimental results are too brief to be convincing . Other results such as accuracy curves , or other baselines such as joint training should be provided . In addition , Cora , Citeseer and Pubmed are small graph datasets . Experiments on big graph datasets should be conducted . Minor comments : 1 . In Formula ( 9 ) , the parentheses are redundant .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks very much for accepting our paper ! We next address your concerns respectively . ( 1 ) Using the feature 's cross-correlation matrix as the adjacency matrix of the neural network on the node sounds reasonable , but the validity of this idea lacks experimental explanation . In addition , how much do features contain the structure information of a graph ? - We agree that it is very difficult to directly measure how much do features contain structural information . Therefore , we resort to emperical validation : - We first assume that the cross-correlation matrix is able to capture useful information for graph lifelong learning . - To verify this assumption , we first establish the model following the assumption and then show in Sec 5 : with the cross-correlation matrix as the adjacency matrix , our feature graphs achieve much higher accuracy in lifelong graph learning than the method without taking the cross-correlatioin as adjacency matrix . These results successfully validate our assumption and provides the experimental explanation . ( 2 ) The dynamic increase of nodes is the main problem that limits the application of continuous learning approaches in graph neural networks . This problem still exists in Feature Graph . The author should discuss it . - The dynamic increase of nodes is converted into a continuum of feature graphs using our method . - The continuum is a basic setting of continual learning ( Aljundi et al. , 2019b ) , thus the problem of dynamic increase nodes can be solved/alleviated by applying the continual learining techniques . - The main challenge of continual learning from the continuum is the * * forgetting issue * * : the learned knowledge from earlier continuum may be easily forgotten since the earlier data can not be available anymore . - We show in Sec.5 that feature graphs achieve very low forgetting rate . - We have made this more clear in the new version ( Sec 4.2 ) . ( 3 ) The usage of task descriptors $ t_i $ should be clarified , since their different usages may have a great impact on difficulty of problem . - The task decriptors $ t_i $ in the experiments are integers numerating the tasks ( at the end of Sec.3 of initial version ) , which is equivalent to the class labels of samples $ y_i $ . In other words , we do n't need $ t_i $ during both training and testing ( we only need sample labels $ y_i $ during training ) . - However , we still define the task descriptors for compatibility with some continual learning techniques , which require such information . - We have made this more clear in the new version ( Sec.3 ) . ( 4 ) Although Table 2 & 3 show positive result supporting proposed method , the experimental results are too brief to be convincing . Other results such as accuracy curves , or other baselines such as joint training should be provided . In addition , Cora , Citeseer and Pubmed are small graph datasets . Experiments on big graph datasets should be conducted . - We added a per-class precision figure and a per-class forgetting rate comparison figure in Fig 3 ( a ) and Fig.3 ( b ) in the new version . - We also showed the training accuracy curves in Fig 2 ( a ) and Fig.2 ( b ) .A large dataset experiment , Flickr , was conducted in Appendix F. - Thanks for your suggestion , we will conduct more experiments in the future ."}}