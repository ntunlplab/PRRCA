{"year": "2020", "forum": "rkeJRhNYDH", "title": "TabFact: A Large-scale Dataset for Table-based Fact Verification", "decision": "Accept (Poster)", "meta_review": "This paper presents a new dataset for fact verification in text from tables. The task is to identify whether a given claim is supported by the information presented in the table. The authors have also presented two baseline models, one based on BERT and based on symbolic reasoning which have an ok performance on the dataset but still very behind the human performance. The paper is well-written and the arguments and experiments presented in the paper are sound.\n\nAfter reviewer comments, the authors have incorporated major changes in the paper. I recommend an Accept for the paper in its current form.", "reviews": [{"review_id": "rkeJRhNYDH-0", "review_text": "This work proposes the problem of fact verification with semi-structured data source such as tables. Specifically, the authors created a new dataset TabFact and evaluated two baseline models with different variations. They applied two criteria and different rewards for workers to collect two subsets of different levels (\u201csimple\u201d and \u201ccomplex\u201d). They also applied a negative rewriting strategy to avoid exploitable cues or patterns in the annotations. They evaluated two baselines models: (1) latent program algorithm (LPA), which makes use of simple string match entity linking and systematic search and trains a neural network discriminator, and (2) Table-BERT, which linearize the table into a sequence through concatenation or template, and treat it as a classfication problem. Both showed reasonable accuracy (~68%), but still below human performance (92.1%) on a held out test set. I would like to see the paper accepted because: (1) it proposes an interesting task (table fact verification) with a clean dataset, and the experiments evaluated the ability of the current neural network models, such as BERT, or hybrid models, such as the LPA baseline, to perform (symbolic) reasoning; (2) special care is done to ensure the dataset doesn't contain simple cues or patterns, which is a common pitfall in dataset collection, and the dataset is also validated through two reasonable baseline models. Some weakness and concerns are: (1) some error analysis of the baseline models are missing. For example, what types questions are hard/simple for table-BERT and what are hard/simple for LPA, and some comparison between them. This helps point out where the difficulty come from, for example, whether the difficulty is language understanding or symbolic reasoning. (2) \"To focus on statement verification against the table, we do not feed the caption to the model and simply mask the phrases in the statements which links to the caption with placeholders.\" I am not sure this is the right thing to do since even the human annotators requires the caption to understand the context, why not also feed the caption into the model? (3) Although the Figure 2 showed that the higher order operations are indeed used in a majority of questions, which measures the breadth of the reasoning, it is unclear about the depths of the required reasoning, i.e., how many operations / steps are required to achieve the correct answer. It would help if the average number of steps / operations required for answering the questions are shown. If the above concerns are addressed, I will be willing to raise my score. Minor comments: The paper, especially the Appendix, requires some proof reading, for example, I believe the caption for Figure 5 in Appendix A is misplaced. \"... they are explicitly banned bring ...\" -> \"banned from bringing\" \u201cAs illustrated in Figure 6, the title only acts as a placeholder in the statements to make it sound more natural.\u201d -> From the example, it seems the name of the player is kept unchanged in the sentence, which is different from a placeholder. Suggestions: I understand this might require some works, but it would be really helpful to add more comments and maybe examples for the functions shown in Figure 5 (you might need to split the table into two pages or have another table for examples), so that it can be used by works that follows the LPA approach. \u201cDuring the annotation, the workers are explicitly guided to modify the words, phrases or sentence structures but retain the sentence style/length to prevent from artificial cues\u201d Avoiding simple cues or patterns are important, so it will be good if more details can be shared (for example, the instructions/guidelines you showed to the workers). ================================== Post rebuttal update: Thanks for the update to the paper. I think this work provides a good dataset and some reasonable baselines, thus should be accepted. However, I am still a bit concerned about the categorization of questions and the analysis on reasoning depth, because they are all based on the programs generated by LPA, while systematic search's recall is just 77% and the programs potentially contains a lot of spurious ones. So the categorization of the questions in Figure 11 and the number of reasoning steps in Figure 12 has to be taken with a grain of salt. It is worth annotating a few hundred, say 100-300 questions, manually with the ground truth programs for question distribution analysis or reasoning depth analysis, and confirm the numbers estimated using programs from LPA. Those manually annotated examples would also be a great addition to the dataset to aid the researchers for better analysis. A good example is the WikiTableQuestions dataset https://github.com/ppasupat/WikiTableQuestions/releases, which contains 300 manually annotated examples (annotated-all.examples), and they are used to analyze the distribution of the questions in the paper. ", "rating": "6: Weak Accept", "reply_text": "Thanks for your constructive feedback , here are the answers to your questions : 1 . Error Analysis : We have put the detailed error analysis in Appendix C ( Error Analysis ) in the revision . Specifically , we first qualitatively discussed the bottlenecks of LPA and BERT model . Then we held-out some data from the validation set to analyze the performance difference between the two models . It turns out that the linguistic inference case is much better handled by BERT while max/min/argmax/argmin/count cases are better handled by LPA . 2.Feeding Caption to model : During annotation , we have given special notice to the worker not to rewrite the part copied from the Wikipedia table title during the phase of annotation for refuted statements . It implies that the captions shouldn \u2019 t be regarded as evidence for verification . Thus , we masked them out in the experiments . However , we did do some auxiliary experiments to provide the caption as an additional input to the discriminator for the LPA method . We found that the performance is roughly the same without much change , we added the exact numbers to Table 2 . For Table-BERT , the title is already given in the template linearization . 3.Reasoning Depth : We have added Appendix D ( Reasoning Depth ) in the revision , which aims to visualize the reasoning depth of LPA method . As can be seen from the figure therein , the reasoning steps are concentrated between 4-7 steps . It implies the difficulty and compositionality of the statements in TabFact . 4.Minor comments : 1 ) I have replaced the original table with a detailed version . Thanks for your reminder . 2 ) regarding the sentence \u201c as illustrated in \u2026 \u201d , I would like to clarify that the annotators can only rely on the title to gain background knowledge . They can not rewrite information about the title to create a \u201c refuted \u201d statement . 5.Function Description : We added detailed function description and input/output examples in Appendix A and also list the function triggers we adopted to shrink the search space . 6.Instruction to workers : The detailed guidelines are added to Appendix H , which is the HIT interface we used for the workers to follow during annotation ."}, {"review_id": "rkeJRhNYDH-1", "review_text": "This paper proposes a new dataset for table-based fact verification and introduces a couple of methods for the task. I think that the dataset would be a useful resource (see some comments nevertheless on its construction), however the methods proposed are not particularly interesting, and the contributions to ML and NLP are overstated in my opinion. In addition the paper needs proof reading as there are many typos, some of which make comprehension problematic. In detail: - The dataset is the main contribution of this paper. Its size is great. However I have some concerns on its construction. The guidelines described for constructing simple and complex claims are rathe vague, e.g. how does one define \"too much symbolic reasoning\" and explain it to crowdworkers? How was the linguistic complexity difference between the two channels measured? How were the claims sanity checked? In the beginning of section 2.3 it is stated that quality control filtered a substantial proportion of the statements. How was this done? - A troublesome aspect in my opinion of the dataset is that it only allows for evaluation of the Entailed/Refuted binary classification task, but not on whether the model used the right evidence to reach its conclusion. Thus it will always be possible for models to score highly without doing the right thing. Avoiding trivial re-writes helps, but it is unlikely to be enough as human crowd workers will try to optimize their earnings. - The two models discussed are OK as baselines, but not particularly interesting or appropriate. Both require substantial rule-based processing (named entity linking, latent program construction. templates) and eventually linearize structured data (the program or the table). I understand that this is not the main contribution of the paper, but given the substantial amount of work on semantic parsing and question answering I was expecting more appropriate baselines taking previous work into account. The performance of the model is not great either, especially if we consider that they are only evaluated on returning a binary label, not the correct evidence from the table. - While it is true that most of the fact verification work has focus on textual sources, the challenge of combining reasoning over continuous and discrete representations is not new. The various QA works mentioned in the related work section address this, as well as work on theorem proving: https://arxiv.org/abs/1705.11040 Furthermore, there has been at least one more previous work on table based verification against FreeBase tables: https://www.aclweb.org/anthology/D15-1312/. Thus I believe the discussion of the challenges posed by this dataset should be re-framed, especially given that the kinds of programs that need to be constructed are of similar complexity to previous work like the WikiTableQuestions. - writing: \"the model is expected to excel... but to fall short\", \"we follow the human subject research protocols\" (which ones?), \"in case of obvious stylistic patterns\" (which ones). On the whole it is understandable, but the writing should be improved. ---- Post author response I have responded to the author response and I have revised my score.", "rating": "6: Weak Accept", "reply_text": "1.Guideline : The guideline for annotating complex claims are given in Appendix H , where we display the HIT interface for dataset collection . Since it was hard to explain these scientific terms to the crowdworkers , we made the following efforts to make sure the collected data follows our expectation : 1 ) we provide comprehensive examples for different logic operations to the annotators to help them understand what different operations represent . 2 ) we iterate a lot of rounds to refine our instruction to gradually improve annotator \u2019 s annotation skills . We reward good workers and encourage them to annotate more high-quality claims while penalizing low-quality workers . 3 ) our team employs 8 experts to perform the quality control during the annotation and after the annotation , where each expert spent over 50 hours on this project to ensure the quality of the dataset . We describe the details of the quality control step in Sec 2.2 . The `` too much reasoning ... `` sentence is a bit vague , what we want to emphasize is to only involve unary facts in the statement rather than having compound facts within the statement , we revised it in the revision . 2.Linguistic/Logic Difference from two channels : The two channels have different instructions and examples to guide the crowd workers to behave differently . In the simple channel , our instruction explicitly encourages workers to annotate simple and straightforward claims about certain entities without strong paraphrasing and inference . To maximize the reward , the workers tend to annotate rather simple claims without too strong reasoning . In the complex channel , as depicted in the HIT interface in the Appendix , we explicitly enforce the workers to follow the examples to annotate claims with higher-order logic and linguistic paraphrase/inference . This channel involves more dedicated quality control to ensure quality . 3.Evaluation problem : we follow the standard NLI/Verification problems ( e.g. , SNLI , GLUE , SuperGLUE , FEVER ) to use classification accuracy as one of our evaluation metrics . The binary classification gives a very objective and accurate evaluation of the end-task goal . To provide additional evaluation , in our original submission , we have already employed three additional metrics in Table 3 to give fine-grained analysis about different model components like entity linking/search/discriminator , etc . 4.Higher performance without learning well : We have already applied different state-of-the-art methods like BERT , but it still can not achieve plausible accuracy and exhibits unstable training behavior . It indicates that the proposed new task is a challenging problem . To make sure the negative samples do not have explicit cues , we hired 8 experts to adopt strict quality control to enforce the workers to annotate high-quality and challenging negative sentences . 5.Previous semantic parsing baselines : to the best of our knowledge , the previous semantic parsing papers can be divided into two groups : 1 ) with logic form annotation : ( Lu 2008 , Artzi 2014 , Dong 2016 , Yin 2017 , Zhong 2017 , Cheng 2017 , etc ) , these methods rely on annotated logic forms to learn the parsing model , which does not fit our case as we do not have logic-form annotation . 2 ) weakly-supervised : Traditional parsing models ( Luke 2013 , Liang 2013 , Berant 2014 , Wang 2015 , Pasupat 2015 ) mostly rely on rules/lexicon/grammar and feature-based log-linear ranking models , etc . The core ideas of these methods are based on BFS/DFS/DP search with strong pruning strategies to harvest the plausible candidates and then score the most likely parses for answer prediction . Our LPA model inherits such a pipeline to first do BFS with memorization to accelerate the search speed , and then leverage a neural discriminator to rank the plausible parses . Recently , people have invented neural parsing models ( Liang 2017 , Rishabh 2019 ) , which implements the parsing as a sequence generation architecture and leverages reinforcement learning to address the weakly supervised challenge . We have implemented NSM ( Liang 2017 ) baseline under two settings , and we have updated the revision to include the experimental results in Table 2 . Since we don \u2019 t have any annotated programs for bootstrapping , directly using RL to train NSM does not work well given the spurious program problem . By leveraging the LPA-searched programs to teacher-force NSM and then fine-tune it with RL can yield more reasonable performance . Through this comparison , we found that LPA actually performs better under our verification problem with under-specified rewards . We are not sure if I missed some publicly available methods , please remind us of that and we will try to implement it as a baseline for TabFact ."}, {"review_id": "rkeJRhNYDH-2", "review_text": "Updated review: Thank you for addressing the comments and making relevant edits. Additionally, the HIT section provides a lot more insights into the data collection process. I've updated my score based on the responses/edits made. ------ This paper is about a dataset (TABFACT) aimed at promoting research for fact-verification using semi-structured data as evidence. The paper highlights how the existing fact-verification studies have been restricted to work with unstructured evidence, and hence lack generalization to use-cases where the evidence is in a structured format (eg. databases). The paper also highlights how fact-verification with semi-structured evidence is challenging, since it involves both linguistic reasoning (for paraphrasing, entailment etc.) and symbolic reasoning (for operations like count, min, max etc.). To tackle this, the authors suggest two approaches as baselines on the dataset - one uses off-the-shelf BERT model for NLI; the other one focuses on symbolic reasoning and is based on program execution - which primarily uses lexical matching and a set of predefined operations (like count/max/min) to construct a program. Apart from a few issues (mentioned below), the paper is well written. The authors have provided a detailed overview of their data collection/verification pipeline and related model/experiments. Overall, it seems like an interesting dataset and I'm inclined towards accepting the paper. A few remarks/concerns are: 1. Usefulness of the dataset: It seems limiting for a fact-verification dataset to restrict itself to a binary space i.e. entailed vs refuted. It is often the case, that statements are not completely true or false. For example, the 3rd refuted statement in Figure 1 is partially true (\u2018there are five candidates in total\u2019). With a binary space for supervision, we don\u2019t really know if the system is actually able to capture the linguistic and symbolic nuances present in the task. It is entirely possible for the system to \u201cdo well\u201d without \u201clearning well\u201d, if the learning/output space is this coarse (as opposed to a dataset like Vlachos and Riedel, 2014). 2. Related work: The paper talks about introducing a new \u2018format\u2019 of evidence (structured text) and talks about \u2018unstructured text\u2019 as the only \u2018other\u2019 format of evidence. It misses out on a highly related task that uses image as evidence (notable datasets being: CLEVR-Humans, NLVR/2, GQA). Either these should be included in the related work, or the authors should make it explicit that this work only deals with \u2018textual\u2019 evidence. 3. The dataset statistics in Table 1 don\u2019t seem to add up (train+dev+test = (92,283 + 12,792 + 12,779) = 117,854 != 118,275 (=Total #Sentence)). 4. Page 3, Section 2.3: \u201cwe further perform quality control\u201d -> a line or two to explain quality control? 5. Appendix C: No data/statistics have been provided to support the conclusion of the ablation study. Minor remarks: - Page 2: Section 2: 1. overtly -> overly 2. huge tables(e.g. -> huge tables (e.g. - Page 3: Section 2.3 1. to filter 18% entailed of entailed statements -> to filter 18% entailed statements - Page 4: 1. candidate) . we need to -> candidate), we need to", "rating": "8: Accept", "reply_text": "Thanks for your constructive feedback , here are the answers to your questions : 1 . Usefulness of the dataset : we follow the standard NLI/Fact-Checking setting to divide the label into entailed/refuted , where a given claim with \u201c any part of the claim containing misinformation \u201d is viewed as refuted . In order to deal with the coarse space of output label , we propose to analyze the accuracy/F1/HITS @ K score of entity linking/search/discriminator to further evaluate the fine-grained performance of the model . To encourage automatic evaluation , we plan to annotate a smaller amount of gold data for these components to enrich the evaluation metric . 2.Missing reference : I added some references and talked about their relatedness in the revision . Among the listed datasets , NLVR/2 is especially related to our task . 3.Mismatch Set size : We filtered roughly 400 sentences from some abnormal tables ( which contains special tokens/hyperlink/mathematical formula , etc ) after merging simple/complex channels and divide the 117,854 into train/val/test . 4.Quality Control : We added a Section 2.2 \u201c quality control \u201d to talk about the details of our quality control strategy , which is very critical during collection . We distribute the quality control workload to 8 different experts , where each one spent over 50 hours to ensure the data quality . 5. w/ vs. w/o caption ablation : We performed a user study before the collection , where we randomly match 50 pairs from two annotation tasks ( w/ title and w/o title ) and distribute them to our 8 experts to measure the language fluency and informativeness without telling them their sources . The user study indicates that the 8 experts consistently agree to favor the statements annotated with Wikipedia title/caption . The gap is significant and we immediately made the decision to provide workers with table titles as context ."}], "0": {"review_id": "rkeJRhNYDH-0", "review_text": "This work proposes the problem of fact verification with semi-structured data source such as tables. Specifically, the authors created a new dataset TabFact and evaluated two baseline models with different variations. They applied two criteria and different rewards for workers to collect two subsets of different levels (\u201csimple\u201d and \u201ccomplex\u201d). They also applied a negative rewriting strategy to avoid exploitable cues or patterns in the annotations. They evaluated two baselines models: (1) latent program algorithm (LPA), which makes use of simple string match entity linking and systematic search and trains a neural network discriminator, and (2) Table-BERT, which linearize the table into a sequence through concatenation or template, and treat it as a classfication problem. Both showed reasonable accuracy (~68%), but still below human performance (92.1%) on a held out test set. I would like to see the paper accepted because: (1) it proposes an interesting task (table fact verification) with a clean dataset, and the experiments evaluated the ability of the current neural network models, such as BERT, or hybrid models, such as the LPA baseline, to perform (symbolic) reasoning; (2) special care is done to ensure the dataset doesn't contain simple cues or patterns, which is a common pitfall in dataset collection, and the dataset is also validated through two reasonable baseline models. Some weakness and concerns are: (1) some error analysis of the baseline models are missing. For example, what types questions are hard/simple for table-BERT and what are hard/simple for LPA, and some comparison between them. This helps point out where the difficulty come from, for example, whether the difficulty is language understanding or symbolic reasoning. (2) \"To focus on statement verification against the table, we do not feed the caption to the model and simply mask the phrases in the statements which links to the caption with placeholders.\" I am not sure this is the right thing to do since even the human annotators requires the caption to understand the context, why not also feed the caption into the model? (3) Although the Figure 2 showed that the higher order operations are indeed used in a majority of questions, which measures the breadth of the reasoning, it is unclear about the depths of the required reasoning, i.e., how many operations / steps are required to achieve the correct answer. It would help if the average number of steps / operations required for answering the questions are shown. If the above concerns are addressed, I will be willing to raise my score. Minor comments: The paper, especially the Appendix, requires some proof reading, for example, I believe the caption for Figure 5 in Appendix A is misplaced. \"... they are explicitly banned bring ...\" -> \"banned from bringing\" \u201cAs illustrated in Figure 6, the title only acts as a placeholder in the statements to make it sound more natural.\u201d -> From the example, it seems the name of the player is kept unchanged in the sentence, which is different from a placeholder. Suggestions: I understand this might require some works, but it would be really helpful to add more comments and maybe examples for the functions shown in Figure 5 (you might need to split the table into two pages or have another table for examples), so that it can be used by works that follows the LPA approach. \u201cDuring the annotation, the workers are explicitly guided to modify the words, phrases or sentence structures but retain the sentence style/length to prevent from artificial cues\u201d Avoiding simple cues or patterns are important, so it will be good if more details can be shared (for example, the instructions/guidelines you showed to the workers). ================================== Post rebuttal update: Thanks for the update to the paper. I think this work provides a good dataset and some reasonable baselines, thus should be accepted. However, I am still a bit concerned about the categorization of questions and the analysis on reasoning depth, because they are all based on the programs generated by LPA, while systematic search's recall is just 77% and the programs potentially contains a lot of spurious ones. So the categorization of the questions in Figure 11 and the number of reasoning steps in Figure 12 has to be taken with a grain of salt. It is worth annotating a few hundred, say 100-300 questions, manually with the ground truth programs for question distribution analysis or reasoning depth analysis, and confirm the numbers estimated using programs from LPA. Those manually annotated examples would also be a great addition to the dataset to aid the researchers for better analysis. A good example is the WikiTableQuestions dataset https://github.com/ppasupat/WikiTableQuestions/releases, which contains 300 manually annotated examples (annotated-all.examples), and they are used to analyze the distribution of the questions in the paper. ", "rating": "6: Weak Accept", "reply_text": "Thanks for your constructive feedback , here are the answers to your questions : 1 . Error Analysis : We have put the detailed error analysis in Appendix C ( Error Analysis ) in the revision . Specifically , we first qualitatively discussed the bottlenecks of LPA and BERT model . Then we held-out some data from the validation set to analyze the performance difference between the two models . It turns out that the linguistic inference case is much better handled by BERT while max/min/argmax/argmin/count cases are better handled by LPA . 2.Feeding Caption to model : During annotation , we have given special notice to the worker not to rewrite the part copied from the Wikipedia table title during the phase of annotation for refuted statements . It implies that the captions shouldn \u2019 t be regarded as evidence for verification . Thus , we masked them out in the experiments . However , we did do some auxiliary experiments to provide the caption as an additional input to the discriminator for the LPA method . We found that the performance is roughly the same without much change , we added the exact numbers to Table 2 . For Table-BERT , the title is already given in the template linearization . 3.Reasoning Depth : We have added Appendix D ( Reasoning Depth ) in the revision , which aims to visualize the reasoning depth of LPA method . As can be seen from the figure therein , the reasoning steps are concentrated between 4-7 steps . It implies the difficulty and compositionality of the statements in TabFact . 4.Minor comments : 1 ) I have replaced the original table with a detailed version . Thanks for your reminder . 2 ) regarding the sentence \u201c as illustrated in \u2026 \u201d , I would like to clarify that the annotators can only rely on the title to gain background knowledge . They can not rewrite information about the title to create a \u201c refuted \u201d statement . 5.Function Description : We added detailed function description and input/output examples in Appendix A and also list the function triggers we adopted to shrink the search space . 6.Instruction to workers : The detailed guidelines are added to Appendix H , which is the HIT interface we used for the workers to follow during annotation ."}, "1": {"review_id": "rkeJRhNYDH-1", "review_text": "This paper proposes a new dataset for table-based fact verification and introduces a couple of methods for the task. I think that the dataset would be a useful resource (see some comments nevertheless on its construction), however the methods proposed are not particularly interesting, and the contributions to ML and NLP are overstated in my opinion. In addition the paper needs proof reading as there are many typos, some of which make comprehension problematic. In detail: - The dataset is the main contribution of this paper. Its size is great. However I have some concerns on its construction. The guidelines described for constructing simple and complex claims are rathe vague, e.g. how does one define \"too much symbolic reasoning\" and explain it to crowdworkers? How was the linguistic complexity difference between the two channels measured? How were the claims sanity checked? In the beginning of section 2.3 it is stated that quality control filtered a substantial proportion of the statements. How was this done? - A troublesome aspect in my opinion of the dataset is that it only allows for evaluation of the Entailed/Refuted binary classification task, but not on whether the model used the right evidence to reach its conclusion. Thus it will always be possible for models to score highly without doing the right thing. Avoiding trivial re-writes helps, but it is unlikely to be enough as human crowd workers will try to optimize their earnings. - The two models discussed are OK as baselines, but not particularly interesting or appropriate. Both require substantial rule-based processing (named entity linking, latent program construction. templates) and eventually linearize structured data (the program or the table). I understand that this is not the main contribution of the paper, but given the substantial amount of work on semantic parsing and question answering I was expecting more appropriate baselines taking previous work into account. The performance of the model is not great either, especially if we consider that they are only evaluated on returning a binary label, not the correct evidence from the table. - While it is true that most of the fact verification work has focus on textual sources, the challenge of combining reasoning over continuous and discrete representations is not new. The various QA works mentioned in the related work section address this, as well as work on theorem proving: https://arxiv.org/abs/1705.11040 Furthermore, there has been at least one more previous work on table based verification against FreeBase tables: https://www.aclweb.org/anthology/D15-1312/. Thus I believe the discussion of the challenges posed by this dataset should be re-framed, especially given that the kinds of programs that need to be constructed are of similar complexity to previous work like the WikiTableQuestions. - writing: \"the model is expected to excel... but to fall short\", \"we follow the human subject research protocols\" (which ones?), \"in case of obvious stylistic patterns\" (which ones). On the whole it is understandable, but the writing should be improved. ---- Post author response I have responded to the author response and I have revised my score.", "rating": "6: Weak Accept", "reply_text": "1.Guideline : The guideline for annotating complex claims are given in Appendix H , where we display the HIT interface for dataset collection . Since it was hard to explain these scientific terms to the crowdworkers , we made the following efforts to make sure the collected data follows our expectation : 1 ) we provide comprehensive examples for different logic operations to the annotators to help them understand what different operations represent . 2 ) we iterate a lot of rounds to refine our instruction to gradually improve annotator \u2019 s annotation skills . We reward good workers and encourage them to annotate more high-quality claims while penalizing low-quality workers . 3 ) our team employs 8 experts to perform the quality control during the annotation and after the annotation , where each expert spent over 50 hours on this project to ensure the quality of the dataset . We describe the details of the quality control step in Sec 2.2 . The `` too much reasoning ... `` sentence is a bit vague , what we want to emphasize is to only involve unary facts in the statement rather than having compound facts within the statement , we revised it in the revision . 2.Linguistic/Logic Difference from two channels : The two channels have different instructions and examples to guide the crowd workers to behave differently . In the simple channel , our instruction explicitly encourages workers to annotate simple and straightforward claims about certain entities without strong paraphrasing and inference . To maximize the reward , the workers tend to annotate rather simple claims without too strong reasoning . In the complex channel , as depicted in the HIT interface in the Appendix , we explicitly enforce the workers to follow the examples to annotate claims with higher-order logic and linguistic paraphrase/inference . This channel involves more dedicated quality control to ensure quality . 3.Evaluation problem : we follow the standard NLI/Verification problems ( e.g. , SNLI , GLUE , SuperGLUE , FEVER ) to use classification accuracy as one of our evaluation metrics . The binary classification gives a very objective and accurate evaluation of the end-task goal . To provide additional evaluation , in our original submission , we have already employed three additional metrics in Table 3 to give fine-grained analysis about different model components like entity linking/search/discriminator , etc . 4.Higher performance without learning well : We have already applied different state-of-the-art methods like BERT , but it still can not achieve plausible accuracy and exhibits unstable training behavior . It indicates that the proposed new task is a challenging problem . To make sure the negative samples do not have explicit cues , we hired 8 experts to adopt strict quality control to enforce the workers to annotate high-quality and challenging negative sentences . 5.Previous semantic parsing baselines : to the best of our knowledge , the previous semantic parsing papers can be divided into two groups : 1 ) with logic form annotation : ( Lu 2008 , Artzi 2014 , Dong 2016 , Yin 2017 , Zhong 2017 , Cheng 2017 , etc ) , these methods rely on annotated logic forms to learn the parsing model , which does not fit our case as we do not have logic-form annotation . 2 ) weakly-supervised : Traditional parsing models ( Luke 2013 , Liang 2013 , Berant 2014 , Wang 2015 , Pasupat 2015 ) mostly rely on rules/lexicon/grammar and feature-based log-linear ranking models , etc . The core ideas of these methods are based on BFS/DFS/DP search with strong pruning strategies to harvest the plausible candidates and then score the most likely parses for answer prediction . Our LPA model inherits such a pipeline to first do BFS with memorization to accelerate the search speed , and then leverage a neural discriminator to rank the plausible parses . Recently , people have invented neural parsing models ( Liang 2017 , Rishabh 2019 ) , which implements the parsing as a sequence generation architecture and leverages reinforcement learning to address the weakly supervised challenge . We have implemented NSM ( Liang 2017 ) baseline under two settings , and we have updated the revision to include the experimental results in Table 2 . Since we don \u2019 t have any annotated programs for bootstrapping , directly using RL to train NSM does not work well given the spurious program problem . By leveraging the LPA-searched programs to teacher-force NSM and then fine-tune it with RL can yield more reasonable performance . Through this comparison , we found that LPA actually performs better under our verification problem with under-specified rewards . We are not sure if I missed some publicly available methods , please remind us of that and we will try to implement it as a baseline for TabFact ."}, "2": {"review_id": "rkeJRhNYDH-2", "review_text": "Updated review: Thank you for addressing the comments and making relevant edits. Additionally, the HIT section provides a lot more insights into the data collection process. I've updated my score based on the responses/edits made. ------ This paper is about a dataset (TABFACT) aimed at promoting research for fact-verification using semi-structured data as evidence. The paper highlights how the existing fact-verification studies have been restricted to work with unstructured evidence, and hence lack generalization to use-cases where the evidence is in a structured format (eg. databases). The paper also highlights how fact-verification with semi-structured evidence is challenging, since it involves both linguistic reasoning (for paraphrasing, entailment etc.) and symbolic reasoning (for operations like count, min, max etc.). To tackle this, the authors suggest two approaches as baselines on the dataset - one uses off-the-shelf BERT model for NLI; the other one focuses on symbolic reasoning and is based on program execution - which primarily uses lexical matching and a set of predefined operations (like count/max/min) to construct a program. Apart from a few issues (mentioned below), the paper is well written. The authors have provided a detailed overview of their data collection/verification pipeline and related model/experiments. Overall, it seems like an interesting dataset and I'm inclined towards accepting the paper. A few remarks/concerns are: 1. Usefulness of the dataset: It seems limiting for a fact-verification dataset to restrict itself to a binary space i.e. entailed vs refuted. It is often the case, that statements are not completely true or false. For example, the 3rd refuted statement in Figure 1 is partially true (\u2018there are five candidates in total\u2019). With a binary space for supervision, we don\u2019t really know if the system is actually able to capture the linguistic and symbolic nuances present in the task. It is entirely possible for the system to \u201cdo well\u201d without \u201clearning well\u201d, if the learning/output space is this coarse (as opposed to a dataset like Vlachos and Riedel, 2014). 2. Related work: The paper talks about introducing a new \u2018format\u2019 of evidence (structured text) and talks about \u2018unstructured text\u2019 as the only \u2018other\u2019 format of evidence. It misses out on a highly related task that uses image as evidence (notable datasets being: CLEVR-Humans, NLVR/2, GQA). Either these should be included in the related work, or the authors should make it explicit that this work only deals with \u2018textual\u2019 evidence. 3. The dataset statistics in Table 1 don\u2019t seem to add up (train+dev+test = (92,283 + 12,792 + 12,779) = 117,854 != 118,275 (=Total #Sentence)). 4. Page 3, Section 2.3: \u201cwe further perform quality control\u201d -> a line or two to explain quality control? 5. Appendix C: No data/statistics have been provided to support the conclusion of the ablation study. Minor remarks: - Page 2: Section 2: 1. overtly -> overly 2. huge tables(e.g. -> huge tables (e.g. - Page 3: Section 2.3 1. to filter 18% entailed of entailed statements -> to filter 18% entailed statements - Page 4: 1. candidate) . we need to -> candidate), we need to", "rating": "8: Accept", "reply_text": "Thanks for your constructive feedback , here are the answers to your questions : 1 . Usefulness of the dataset : we follow the standard NLI/Fact-Checking setting to divide the label into entailed/refuted , where a given claim with \u201c any part of the claim containing misinformation \u201d is viewed as refuted . In order to deal with the coarse space of output label , we propose to analyze the accuracy/F1/HITS @ K score of entity linking/search/discriminator to further evaluate the fine-grained performance of the model . To encourage automatic evaluation , we plan to annotate a smaller amount of gold data for these components to enrich the evaluation metric . 2.Missing reference : I added some references and talked about their relatedness in the revision . Among the listed datasets , NLVR/2 is especially related to our task . 3.Mismatch Set size : We filtered roughly 400 sentences from some abnormal tables ( which contains special tokens/hyperlink/mathematical formula , etc ) after merging simple/complex channels and divide the 117,854 into train/val/test . 4.Quality Control : We added a Section 2.2 \u201c quality control \u201d to talk about the details of our quality control strategy , which is very critical during collection . We distribute the quality control workload to 8 different experts , where each one spent over 50 hours to ensure the data quality . 5. w/ vs. w/o caption ablation : We performed a user study before the collection , where we randomly match 50 pairs from two annotation tasks ( w/ title and w/o title ) and distribute them to our 8 experts to measure the language fluency and informativeness without telling them their sources . The user study indicates that the 8 experts consistently agree to favor the statements annotated with Wikipedia title/caption . The gap is significant and we immediately made the decision to provide workers with table titles as context ."}}