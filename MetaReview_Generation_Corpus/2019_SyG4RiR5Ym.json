{"year": "2019", "forum": "SyG4RiR5Ym", "title": "Neural Distribution Learning for generalized time-to-event prediction", "decision": "Reject", "meta_review": "All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.", "reviews": [{"review_id": "SyG4RiR5Ym-0", "review_text": "The authors propose a parametric framework (HazardNet) for survival analysis with deep learning where they mainly focus on the discrete-time case. The framework allows different popular architectures to learn the representation of the past events with different explicit features. Then, it considers a bunch of parametric families and their mixtures for the distribution of inter-event time. Experiments include a comprehensive comparison between HazardNet and different binary classifiers trained separately at each target time duration. Overall, the paper is well-written and easy to follow. It seeks to build a strong baseline for the deep survival analysis, which is an hot ongoing research topic recently in literature. However, there are a few weaknesses that should be addressed. 1. In the beginning, the paper motivates the mixtures of distributions from MDN. Because most existing work focuses on the formulation of the intensity function, it is very interesting to approach the problem from the cumulative intensity function instead. Originally, it looks like the paper seeks to formulate a general parametric form based on MDN. However, it is disappointing that in the experiments, it still only considers a few classic parametric distributions. There is lack of solid technical connection between Sec 3.1, 3.2 and Sec 4. 2. The discretization discussion of Sec 3.4 is not clear. Normally, the major motivation for discretization is application-driven, say, in hospital, the doctor regularly triggers the inspection event. However, how to optimally choose a bin-size and how to aggregate the multiple events within each bin is still not clear, which is not sufficiently discussed in the paper. Why is taking the summation of the events in a bin a proper way of aggregation? What if we have highly skewed bins? 3. Although the comparison and experimental setting in Figure 4 is comprehensive, the paper misses a very related work \"Deep Recurrent Survival Analysis, https://arxiv.org/abs/1809.02403\", which also considers the discrete-time version of survival analysis. Only comparing with the binary classifiers is not quite convincing without referring to other survival analysis work. 4. Finally, the authors state that existing temporal point process work \"have little meaning without taking into account censored data\". However, if inspecting the loss function of these work closely, we can see there is a survival term exactly the same as the log-cumulative hazard in Equation 3 that handles the censored case. 5. A typo on the bottom of page 3, should be p(t) = F(t + 1) - F(t) ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your very relevant and insightful comments . We appreciate the comments and tried small changes to tie named sections together better . Some things were originally kept orthogonal by design , as distributions ( 3.1-3.3 ) and loss functions is quite independent from what kind of feature/target engineering takes places ( 3.4 , 4 ) which we found previous work being unclear about . A small comment on the summary ; > The framework allows different popular architectures to learn the representation of the past events with different explicit features . While this is true , we mainly focus on using features to learn representations of * future * events . # 1.This was also pointed out by other reviewers . We tried to fix this as our presentation was not clear . We did in fact run experiments for ParetoMixedHazards , LogisticMixedhazards , WeibullMixedHazards but did not have space to present results broken down by distributions other than in appendix . The was more to show that the CHF-perspective makes it easy to compose positive distributions that effortlessly interfaces with density networks , than to look into details of individual distributions . We tried to make this clearer with some of the edits . See comment to AnonReviewer1 and AnonReviewer2 . In the accompanying github repository ( awaiting publication ) , we will release all logs from each individual experiment . With code released , it should also be clear how this machinery looks like in practice . We did not find a good method to release it yet without breaking a double-blind policy . # 2.We hope not to make a case for any optimal method of choosing the bin width or how to aggregate * * features * * while discretizing . We think this is an application & data-specific question and want to leave it as such , this is why we wanted to make it easy to make different choices . In the experiments in Section 4 , one of the * features * was 'log ( 1+count of events in prior timestep ) ' . While experience shows this is usually a good feature , it should be seen as an arbitrary feature generation step that we chose only because it could be done consistently for all datasets under consideration . To aggregate/discretize * * events * * , we consider a timestep with * many events * as a time step with at least one event in . This also naturally fits into the discretization strategy . If a future timestep ( say in ' y ' steps ) has many events ( i.e. , is highly skewed ) , we hope that the current time to event distribution is predicted such that the hazard around that future timestep ( equivalently ; ' \u039b ( y+1 ) -\u039b ( y ) ' ) is high . In the presented framework , this is left to the modeller as the problem of choosing a reasonable distribution for the task , good feature engineering and the choice of neural networks . We hope our framework makes this easy . Through experiments , we can say that using our approach they should get a calibrated prediction when querying 'Pr ( Y < =y ) ' and that this approach is better or at least as good as modelling it as the binary task of whether events will happen in ' y ' timesteps . There are many design choices that have been found through hard gotten experience that are more style than science . It was hard to motivate them in the paper due to the space limit . We put much effort into harmonizing the notation and the perspective on time to event problems . ( Too ) Much can be said about this ."}, {"review_id": "SyG4RiR5Ym-1", "review_text": "This paper proposes to use a mixture of distributions for hazard modeling. They use the standard censored loss and binning-based discretization for handling irregularities in the time series. The evaluation is quite sub-par. Instead of reporting the standard ranking/concordance metrics, the authors report the accuracy of binary classification in certain future timestamps ahead. If we are measuring the classification accuracy, there is a little justification for using survival analysis; we could use just a classification algorithm instead. Moreover, the authors do not compare to the many existing deep hazard model such as Deep Survival [1], DeepSurv [2], DeepHit [3], or many variations based on deep point process modeling. The authors also don\u2019t report the result for non-mixture versions, so we cannot see the true advantages of the proposed mixture modeling. A major baseline for mixture modeling is always non-parametric modeling. In this case, given that there are existing works on deep Cox hazard modeling, the authors need to show the advantages of their proposed mixture modeling against deep Cox models. Overall, the methodology in this paper is quite limited and the evaluation is non-standard. Thus, I vote for rejection of the paper. [1] Ranganath, Rajesh, et al. \"Deep Survival Analysis.\" Machine Learning for Healthcare Conference. 2016. [2] Katzman, Jared L., et al. \"DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network.\" BMC medical research methodology 18.1 (2018): 24. [3] Lee, Changhee, et al. \"Deephit: A deep learning approach to survival analysis with competing risks.\" AAAI, 2018.", "rating": "3: Clear rejection", "reply_text": "Thank you for your feedback . > If we are measuring the classification accuracy , there is a little justification for using survival analysis ; we could use just a classification algorithm instead . A theme in the paper is to point out that predicting a distribution CDF is the same thing as making * all * classification predictions 'Pr ( Y < y ) ' for every timestep ' y > 0 ' ahead . We think that our experimental results shows ( surprisingly ) that our survival model outperforms the classification algorithm on the classification task . Both for the arguably contrieved task of predicting specific timesteps ahead ( Section 4 ) or whether a certain timeframe contains an event ( Predicting zero steps ahead , section 5 ) . The latter is a well known binary task in its domain which we solve with an arguably novel multivariate survival-formulation . > The evaluation is quite sub-par . Instead of reporting the standard ranking/concordance metrics , the authors report the accuracy of binary classification in certain future timestamps ahead . We make a point of our evaluation approach to be non-standard , but we hope that our arguments for it and our critique against the standard evaluation methods for censored sequential problems ( Section 3.4 , Section 4 and results Section 6 ) makes sense . We understand the standard Concordance Index ( CI ) to estimate how well two predictions are expected to be ordered . A good metric for the dominant paradigm of pointwise-predicting TTE ( regression/ranking ) . In contrast , our model predicts a distribution so to answer questions of performance and calibration its arguably not a relevant/helpful metric in its commonly known form . To this goal we found the Binary model a good choice of baseline ( Section 4 ) , and since CI is not defined for binary predictions , we omitted it . As a sidenote , evaluating AUC on different predicted time-windows ahead as we do is tightly related to the time-specific AUC [ 1 ] [ 2 ] [ 3 ] , in turn related to CI . While possible ( but non-standard ) , we could have generalized CI to compare two predicted distributions directly as 'Pr ( Y_i < Y_j ) ' with ground truth 'y_i < y_j ' when available ( `` concordant '' ) . This however restrics the parametric form of the distributions and is hence less general . > The authors also don \u2019 t report the result for non-mixture versions , so we can not see the true advantages of the proposed mixture modeling . This critique was pointed out by other reviewers , and we tried to edit to make this clearer . The main questions we wanted to answer was not which network architecture or distributions where the best . It was more ; - Does our Parametric Survival model produce calibrated & good predictions independent of choice of architecture and distribution ? - Is the this approach better or at least as good as explicitly modeling its binary subqueries ? ( classification approach ) The conclusions was a resounding * yes * . We tested ( but did n't report ) all of the following : ( 3 datasets ) x ( 4 evaluation thresholds ) x ( 3 network architectures ) x [ Binary x ( Use last timesteps or not ) , HazardNet x ( 4 distributions ) x ( MixedHazards or not ) ] Some per-distribution results can be found in Appendix ( see Figure 9 ) but we could report all tabular statistics broken up by distributions too if interesting . While not the main question , one conclusion was no significant improvement ( see Appendix fig 9 ) for the more complex multimodal MixedHazards-distributions . The reasons can only be speculated about , but it may give hints about the need for predicting fine grained/expressive target distributions , which seems to be quite a concern of current research ( Consider DeepHit , Luck et al [ 0 ] , etc ) . While we found this interesting and surprising in its own right , the main benefit we wanted to show was the ease of testing this in the first place using our framework . > the authors do not compare to the many existing deep hazard model such as Deep Survival [ 1 ] , DeepSurv [ 2 ] , DeepHit [ 3 ] , or many variations based on deep point process modeling . We do not explicitly test against these , see answer to reviewer # 3 . It should be noted on the other hand , that one of our findings - that binary models will be biased unless removing last timesteps - has consequences for all methods employing what we call `` classification '' or `` multitask '' approaches ( Ex DeepHit , [ 0 ] and more ) . We tried to clarify this in the revised version . Our results implies that unless they preprocessed data as we suggest , their results risks being heavily biased and uncalibrated . If not evaluated as we suggests they wo n't see this . We ca n't find any paper commenting on this issue . [ 0 ] https : //arxiv.org/abs/1705.10245 [ 1 ] https : //www.ncbi.nlm.nih.gov/pmc/articles/PMC5384160/ [ 2 ] https : //academic.oup.com/bib/article/16/1/153/238328 [ 3 ] https : //www.mayo.edu/research/documents/biostat-80pdf/doc-10027891"}, {"review_id": "SyG4RiR5Ym-2", "review_text": "The paper \"Neural Distribution Learning for generalized time-to-event prediction\" proposes HazardNet, a neural network framework for time-to-event prediction with right-censored data. First of all, this paper should be more clear from the begining of the kind of problems it aim to tackle. The tasks the proposal is able to consider is not easy to realize, at least before the experiments part. The problem should be clearly formalized in the begining of the paper (for instance in the introduction of section 3). It the current form, it is very hard to know what are the inputs, are they sequences of various kinds of events or only one type of event per sequence. It is either not clear to me wether the censoring time is constant or not and wether it is given as input (censoring time looks to be known from section 3.4 but in that case I do not really understand the contribution : does it not correspond to a very classical problem where events from outside of the observation window should be considered during training ? classical EM approaches can be developped for this). The problem of unevenly spaced sequences should also be more formally defined. Also, while the HazardNet framework looks convenient, by using hazard and survival functions as discusses by the authors, it is not clear to me what are the benefits from recent works in neural temporal point processes which also define a general framework for temporal predictions of events. Approaches such at least like \"Modeling the intensity function of point process via recurrent neural networks\" should be considered in the experiments, though they do not explicitely model censoring but with slight adapations should be able to work well of experimental data. ", "rating": "3: Clear rejection", "reply_text": "> First of all , this paper should be more clear from the begining of the kind of problems it aim to tackle . We highly value the feedback . An overarching theme was trying to be as general as possible , as we found much work being too specific when there 's a much wider set of data , problems , and network architectures that can be utilized once we understand the fundamentals of predicting a parametric survival distribution . There is however a very common problem domain that we are explicitly working on ( Section 4 ) we also try to generalize the problem to the general task of predicting a probability distribution with possibly censored or discrete target data , and show how it can even improve on common sparse classification tasks ( Section 5 ) . Currently we can \u2019 t release implementation and all experimental data without breaking double blind . Once this is done , with corresponding visualizations and data-manipulation should be a bit easier . We tried to make some amends to make it clearer . > It the current form , it is very hard to know what are the inputs , are they sequences of various kinds of events or only one type of event per sequence . It is either not clear to me wether the censoring time is constant or not and wether it is given as input ( censoring time looks to be known from section 3.4.Censoring time is used to calculate censoring indicators , itself used for training . It typically varies with time . Input to the neural network ( features ) can be anything , output are parameters of a distribution . Specifically , for the experiment in section 4 it 's sequences of features . The target ( supplied during training ) is a sequence of time to event ( which will look like a countdown/sawtooth wave as in figure 2 ) and censoring indicators used for the loss function . Censoring time for a sequence would be the time to the end of the sequence ( so it 's a countdown ) . Censoring indicators will thus vary . In section 5 , input is a 50 ms time-window of a spectrogram . Target is bivariate ; the time to event and time since event ( each with their respective censoring indicators ) . This was supposed to exemplify that with just a change in the output dimension and feature transformation , the same model may be used for something seemingly different like making multivariate predictions . > but in that case I do not really understand the contribution : does it not correspond to a very classical problem where events from outside of the observation window should be considered during training ? classical EM approaches can be developped for this ) . It is true that this corresponds closely to the classical problem of , during training , considering whether events were * not * in the observation/data window ( i.e censored ) . While we heard of no relevant EM-methods but consider our method as exactly developing on a classical approach Parametric Survival Analysis ( PSA ) which we found other work not sufficiently recognizing . The purpose is to say that - while there seems to be many shiny and complex solutions out there - let 's first do an in-depth discussion of the classical approach . It 's easy to see that most other papers can be considered as derivative work of PSA , but we could n't find anyone going into depth on the idea itself . Our general reasoning is that by thinking from the classical idea of PSA , many variations ( our contributions ) immediately follows and are easy to implement such as predicting all parameters of the distribution like other Density Networks do , being able to discretize TTE , composing distributions to make other distributions , multivariate predictions , architecture agnosticism all the while making it fit well with popular probabilistic programming paradigms ( Edward , PyTorch Distributions , Pyro ) . > The problem of unevenly spaced sequences should also be more formally defined . We thought this was clear in the context of event-generated time series no ? Asyncronous measurements vs Evenly spaced measurements for temporal models is a classic such problem we wanted to address discussing in section 3.4 . In the context of TTE an additional confounding factor is that the lag between observations may be connected to what we want to predict ."}], "0": {"review_id": "SyG4RiR5Ym-0", "review_text": "The authors propose a parametric framework (HazardNet) for survival analysis with deep learning where they mainly focus on the discrete-time case. The framework allows different popular architectures to learn the representation of the past events with different explicit features. Then, it considers a bunch of parametric families and their mixtures for the distribution of inter-event time. Experiments include a comprehensive comparison between HazardNet and different binary classifiers trained separately at each target time duration. Overall, the paper is well-written and easy to follow. It seeks to build a strong baseline for the deep survival analysis, which is an hot ongoing research topic recently in literature. However, there are a few weaknesses that should be addressed. 1. In the beginning, the paper motivates the mixtures of distributions from MDN. Because most existing work focuses on the formulation of the intensity function, it is very interesting to approach the problem from the cumulative intensity function instead. Originally, it looks like the paper seeks to formulate a general parametric form based on MDN. However, it is disappointing that in the experiments, it still only considers a few classic parametric distributions. There is lack of solid technical connection between Sec 3.1, 3.2 and Sec 4. 2. The discretization discussion of Sec 3.4 is not clear. Normally, the major motivation for discretization is application-driven, say, in hospital, the doctor regularly triggers the inspection event. However, how to optimally choose a bin-size and how to aggregate the multiple events within each bin is still not clear, which is not sufficiently discussed in the paper. Why is taking the summation of the events in a bin a proper way of aggregation? What if we have highly skewed bins? 3. Although the comparison and experimental setting in Figure 4 is comprehensive, the paper misses a very related work \"Deep Recurrent Survival Analysis, https://arxiv.org/abs/1809.02403\", which also considers the discrete-time version of survival analysis. Only comparing with the binary classifiers is not quite convincing without referring to other survival analysis work. 4. Finally, the authors state that existing temporal point process work \"have little meaning without taking into account censored data\". However, if inspecting the loss function of these work closely, we can see there is a survival term exactly the same as the log-cumulative hazard in Equation 3 that handles the censored case. 5. A typo on the bottom of page 3, should be p(t) = F(t + 1) - F(t) ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your very relevant and insightful comments . We appreciate the comments and tried small changes to tie named sections together better . Some things were originally kept orthogonal by design , as distributions ( 3.1-3.3 ) and loss functions is quite independent from what kind of feature/target engineering takes places ( 3.4 , 4 ) which we found previous work being unclear about . A small comment on the summary ; > The framework allows different popular architectures to learn the representation of the past events with different explicit features . While this is true , we mainly focus on using features to learn representations of * future * events . # 1.This was also pointed out by other reviewers . We tried to fix this as our presentation was not clear . We did in fact run experiments for ParetoMixedHazards , LogisticMixedhazards , WeibullMixedHazards but did not have space to present results broken down by distributions other than in appendix . The was more to show that the CHF-perspective makes it easy to compose positive distributions that effortlessly interfaces with density networks , than to look into details of individual distributions . We tried to make this clearer with some of the edits . See comment to AnonReviewer1 and AnonReviewer2 . In the accompanying github repository ( awaiting publication ) , we will release all logs from each individual experiment . With code released , it should also be clear how this machinery looks like in practice . We did not find a good method to release it yet without breaking a double-blind policy . # 2.We hope not to make a case for any optimal method of choosing the bin width or how to aggregate * * features * * while discretizing . We think this is an application & data-specific question and want to leave it as such , this is why we wanted to make it easy to make different choices . In the experiments in Section 4 , one of the * features * was 'log ( 1+count of events in prior timestep ) ' . While experience shows this is usually a good feature , it should be seen as an arbitrary feature generation step that we chose only because it could be done consistently for all datasets under consideration . To aggregate/discretize * * events * * , we consider a timestep with * many events * as a time step with at least one event in . This also naturally fits into the discretization strategy . If a future timestep ( say in ' y ' steps ) has many events ( i.e. , is highly skewed ) , we hope that the current time to event distribution is predicted such that the hazard around that future timestep ( equivalently ; ' \u039b ( y+1 ) -\u039b ( y ) ' ) is high . In the presented framework , this is left to the modeller as the problem of choosing a reasonable distribution for the task , good feature engineering and the choice of neural networks . We hope our framework makes this easy . Through experiments , we can say that using our approach they should get a calibrated prediction when querying 'Pr ( Y < =y ) ' and that this approach is better or at least as good as modelling it as the binary task of whether events will happen in ' y ' timesteps . There are many design choices that have been found through hard gotten experience that are more style than science . It was hard to motivate them in the paper due to the space limit . We put much effort into harmonizing the notation and the perspective on time to event problems . ( Too ) Much can be said about this ."}, "1": {"review_id": "SyG4RiR5Ym-1", "review_text": "This paper proposes to use a mixture of distributions for hazard modeling. They use the standard censored loss and binning-based discretization for handling irregularities in the time series. The evaluation is quite sub-par. Instead of reporting the standard ranking/concordance metrics, the authors report the accuracy of binary classification in certain future timestamps ahead. If we are measuring the classification accuracy, there is a little justification for using survival analysis; we could use just a classification algorithm instead. Moreover, the authors do not compare to the many existing deep hazard model such as Deep Survival [1], DeepSurv [2], DeepHit [3], or many variations based on deep point process modeling. The authors also don\u2019t report the result for non-mixture versions, so we cannot see the true advantages of the proposed mixture modeling. A major baseline for mixture modeling is always non-parametric modeling. In this case, given that there are existing works on deep Cox hazard modeling, the authors need to show the advantages of their proposed mixture modeling against deep Cox models. Overall, the methodology in this paper is quite limited and the evaluation is non-standard. Thus, I vote for rejection of the paper. [1] Ranganath, Rajesh, et al. \"Deep Survival Analysis.\" Machine Learning for Healthcare Conference. 2016. [2] Katzman, Jared L., et al. \"DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network.\" BMC medical research methodology 18.1 (2018): 24. [3] Lee, Changhee, et al. \"Deephit: A deep learning approach to survival analysis with competing risks.\" AAAI, 2018.", "rating": "3: Clear rejection", "reply_text": "Thank you for your feedback . > If we are measuring the classification accuracy , there is a little justification for using survival analysis ; we could use just a classification algorithm instead . A theme in the paper is to point out that predicting a distribution CDF is the same thing as making * all * classification predictions 'Pr ( Y < y ) ' for every timestep ' y > 0 ' ahead . We think that our experimental results shows ( surprisingly ) that our survival model outperforms the classification algorithm on the classification task . Both for the arguably contrieved task of predicting specific timesteps ahead ( Section 4 ) or whether a certain timeframe contains an event ( Predicting zero steps ahead , section 5 ) . The latter is a well known binary task in its domain which we solve with an arguably novel multivariate survival-formulation . > The evaluation is quite sub-par . Instead of reporting the standard ranking/concordance metrics , the authors report the accuracy of binary classification in certain future timestamps ahead . We make a point of our evaluation approach to be non-standard , but we hope that our arguments for it and our critique against the standard evaluation methods for censored sequential problems ( Section 3.4 , Section 4 and results Section 6 ) makes sense . We understand the standard Concordance Index ( CI ) to estimate how well two predictions are expected to be ordered . A good metric for the dominant paradigm of pointwise-predicting TTE ( regression/ranking ) . In contrast , our model predicts a distribution so to answer questions of performance and calibration its arguably not a relevant/helpful metric in its commonly known form . To this goal we found the Binary model a good choice of baseline ( Section 4 ) , and since CI is not defined for binary predictions , we omitted it . As a sidenote , evaluating AUC on different predicted time-windows ahead as we do is tightly related to the time-specific AUC [ 1 ] [ 2 ] [ 3 ] , in turn related to CI . While possible ( but non-standard ) , we could have generalized CI to compare two predicted distributions directly as 'Pr ( Y_i < Y_j ) ' with ground truth 'y_i < y_j ' when available ( `` concordant '' ) . This however restrics the parametric form of the distributions and is hence less general . > The authors also don \u2019 t report the result for non-mixture versions , so we can not see the true advantages of the proposed mixture modeling . This critique was pointed out by other reviewers , and we tried to edit to make this clearer . The main questions we wanted to answer was not which network architecture or distributions where the best . It was more ; - Does our Parametric Survival model produce calibrated & good predictions independent of choice of architecture and distribution ? - Is the this approach better or at least as good as explicitly modeling its binary subqueries ? ( classification approach ) The conclusions was a resounding * yes * . We tested ( but did n't report ) all of the following : ( 3 datasets ) x ( 4 evaluation thresholds ) x ( 3 network architectures ) x [ Binary x ( Use last timesteps or not ) , HazardNet x ( 4 distributions ) x ( MixedHazards or not ) ] Some per-distribution results can be found in Appendix ( see Figure 9 ) but we could report all tabular statistics broken up by distributions too if interesting . While not the main question , one conclusion was no significant improvement ( see Appendix fig 9 ) for the more complex multimodal MixedHazards-distributions . The reasons can only be speculated about , but it may give hints about the need for predicting fine grained/expressive target distributions , which seems to be quite a concern of current research ( Consider DeepHit , Luck et al [ 0 ] , etc ) . While we found this interesting and surprising in its own right , the main benefit we wanted to show was the ease of testing this in the first place using our framework . > the authors do not compare to the many existing deep hazard model such as Deep Survival [ 1 ] , DeepSurv [ 2 ] , DeepHit [ 3 ] , or many variations based on deep point process modeling . We do not explicitly test against these , see answer to reviewer # 3 . It should be noted on the other hand , that one of our findings - that binary models will be biased unless removing last timesteps - has consequences for all methods employing what we call `` classification '' or `` multitask '' approaches ( Ex DeepHit , [ 0 ] and more ) . We tried to clarify this in the revised version . Our results implies that unless they preprocessed data as we suggest , their results risks being heavily biased and uncalibrated . If not evaluated as we suggests they wo n't see this . We ca n't find any paper commenting on this issue . [ 0 ] https : //arxiv.org/abs/1705.10245 [ 1 ] https : //www.ncbi.nlm.nih.gov/pmc/articles/PMC5384160/ [ 2 ] https : //academic.oup.com/bib/article/16/1/153/238328 [ 3 ] https : //www.mayo.edu/research/documents/biostat-80pdf/doc-10027891"}, "2": {"review_id": "SyG4RiR5Ym-2", "review_text": "The paper \"Neural Distribution Learning for generalized time-to-event prediction\" proposes HazardNet, a neural network framework for time-to-event prediction with right-censored data. First of all, this paper should be more clear from the begining of the kind of problems it aim to tackle. The tasks the proposal is able to consider is not easy to realize, at least before the experiments part. The problem should be clearly formalized in the begining of the paper (for instance in the introduction of section 3). It the current form, it is very hard to know what are the inputs, are they sequences of various kinds of events or only one type of event per sequence. It is either not clear to me wether the censoring time is constant or not and wether it is given as input (censoring time looks to be known from section 3.4 but in that case I do not really understand the contribution : does it not correspond to a very classical problem where events from outside of the observation window should be considered during training ? classical EM approaches can be developped for this). The problem of unevenly spaced sequences should also be more formally defined. Also, while the HazardNet framework looks convenient, by using hazard and survival functions as discusses by the authors, it is not clear to me what are the benefits from recent works in neural temporal point processes which also define a general framework for temporal predictions of events. Approaches such at least like \"Modeling the intensity function of point process via recurrent neural networks\" should be considered in the experiments, though they do not explicitely model censoring but with slight adapations should be able to work well of experimental data. ", "rating": "3: Clear rejection", "reply_text": "> First of all , this paper should be more clear from the begining of the kind of problems it aim to tackle . We highly value the feedback . An overarching theme was trying to be as general as possible , as we found much work being too specific when there 's a much wider set of data , problems , and network architectures that can be utilized once we understand the fundamentals of predicting a parametric survival distribution . There is however a very common problem domain that we are explicitly working on ( Section 4 ) we also try to generalize the problem to the general task of predicting a probability distribution with possibly censored or discrete target data , and show how it can even improve on common sparse classification tasks ( Section 5 ) . Currently we can \u2019 t release implementation and all experimental data without breaking double blind . Once this is done , with corresponding visualizations and data-manipulation should be a bit easier . We tried to make some amends to make it clearer . > It the current form , it is very hard to know what are the inputs , are they sequences of various kinds of events or only one type of event per sequence . It is either not clear to me wether the censoring time is constant or not and wether it is given as input ( censoring time looks to be known from section 3.4.Censoring time is used to calculate censoring indicators , itself used for training . It typically varies with time . Input to the neural network ( features ) can be anything , output are parameters of a distribution . Specifically , for the experiment in section 4 it 's sequences of features . The target ( supplied during training ) is a sequence of time to event ( which will look like a countdown/sawtooth wave as in figure 2 ) and censoring indicators used for the loss function . Censoring time for a sequence would be the time to the end of the sequence ( so it 's a countdown ) . Censoring indicators will thus vary . In section 5 , input is a 50 ms time-window of a spectrogram . Target is bivariate ; the time to event and time since event ( each with their respective censoring indicators ) . This was supposed to exemplify that with just a change in the output dimension and feature transformation , the same model may be used for something seemingly different like making multivariate predictions . > but in that case I do not really understand the contribution : does it not correspond to a very classical problem where events from outside of the observation window should be considered during training ? classical EM approaches can be developped for this ) . It is true that this corresponds closely to the classical problem of , during training , considering whether events were * not * in the observation/data window ( i.e censored ) . While we heard of no relevant EM-methods but consider our method as exactly developing on a classical approach Parametric Survival Analysis ( PSA ) which we found other work not sufficiently recognizing . The purpose is to say that - while there seems to be many shiny and complex solutions out there - let 's first do an in-depth discussion of the classical approach . It 's easy to see that most other papers can be considered as derivative work of PSA , but we could n't find anyone going into depth on the idea itself . Our general reasoning is that by thinking from the classical idea of PSA , many variations ( our contributions ) immediately follows and are easy to implement such as predicting all parameters of the distribution like other Density Networks do , being able to discretize TTE , composing distributions to make other distributions , multivariate predictions , architecture agnosticism all the while making it fit well with popular probabilistic programming paradigms ( Edward , PyTorch Distributions , Pyro ) . > The problem of unevenly spaced sequences should also be more formally defined . We thought this was clear in the context of event-generated time series no ? Asyncronous measurements vs Evenly spaced measurements for temporal models is a classic such problem we wanted to address discussing in section 3.4 . In the context of TTE an additional confounding factor is that the lag between observations may be connected to what we want to predict ."}}