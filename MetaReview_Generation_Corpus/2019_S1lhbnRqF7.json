{"year": "2019", "forum": "S1lhbnRqF7", "title": "Building Dynamic Knowledge Graphs from Text using Machine Reading Comprehension", "decision": "Accept (Poster)", "meta_review": "This paper investigates a new approach to machine reading for procedural text, where the task of reading comprehension is formulated as dynamic construction of a procedural knowledge graph. The proposed model constructs a recurrent knowledge graph (as a bipartite graph between entities and location nodes) and tracks the entity states for two domains: scientific processes and recipes.\n\nPros:\nThe idea of formulating reading comprehension as dynamic construction of a knowledge graph is novel and interesting. The proposed model is tested on two different domains: scientific processes (ProPara) and cooking recipes.\n\nCons:\nThe initial submission didn't have the experimental results on the full recipe dataset and also had several clarity issues, all of which have been resolved through the rebuttal. \n\nVerdict:\nAccept. An interesting task & models with solid empirical results.\n", "reviews": [{"review_id": "S1lhbnRqF7-0", "review_text": "The paper proposes a recurrent knowledge graph (bipartite graph between entities and location nodes) construction & updating mechanism for entity state tracking datasets such as (two) ProPara tasks and Recipes. The model goes through the following three steps: 1) it reads a sentence at each time step t and identifies the location of each entity via machine reading comprehension model such as DrQA (entities are predefined). 2) Co-reference module adjusts relationship scores (soft adjacency matrix) among nodes, including possibly new nodes introduced by the MRC model. 3) to propagate the relational information across all the nodes, the model performs L layers of LSTM for each entity that attend on other nodes via attention (where the weights come from the adjacency matrix). The model repeats the three steps for each sentence. The model is trained by directly supervising for the correct span by the MRC model at each time step, which is possible because the data provides strong supervision for each sentence (not just the answer at the end). The model achieves the state of the art in the two tasks of ProPara and Recipes dataset. Strengths: The paper provides an elegant solution for tracking relationship between entities as time (sentence) progresses. I also agree with the authors that this line of work (dynamic KG construction and modification) is an important area of research. While the model shares a similar spirit to EntNet, I think the model has enough distinctions / contributions, especially given that it outperforms EntNet by a large margin. The model also obtains non-trivial improvement over previous SOTA models. Weaknesses: Paper could have been written better. I had hard time understanding it. The notations are overall confusing and not explained well. Also there are a few unclear parts which I discuss in questions below. Questions: 1. Are e_{i,t} and lambda_{i,t} vectors? Scalars? Abstract node notations? It is not clear in the model section. Also, it took me a long time to figure out that \u2018i\u2019 is used to index each entity (it is mentioned later). 2. The paper says v_i (initial representation of each entity) is obtained by looking at the contextualized representations (LSTM outputs) of entity mention in the context. What happens if there are multiple mentions in the text? Which one does it look at? 3. For the LSTM in the graph update, why does it have only one input? Shouldn\u2019t it have two inputs, one for previous hidden state and the other for input? 4. Regarding Recipe experiments, the paper says it reaches a better performance than the baseline using just 10k examples out of 60k. This is great, but could you also report the number when the full dataset is used? 5. What does it mean that in training time the model \u201cupdates\u201d the location node representation with the encoding of correct span. Do you mean you use the encoding instead? 6. For ProPara task 2, what threshold did you choose to obtain the P/R/F1 score? Is it the threshold that maximizes F1? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the useful feedback . We \u2019 ve updated our paper to take it into account -- we \u2019 ve updated the model description and the notation in Section 4 to clarify our method . Two important additions are a high-level summary of the model , which we give at the beginning of Section 4 , and a table ( Table 2 ) that lists what each symbol represents along with its dimensions . We also made several updates that address your specific questions . 1.Are e_ { i , t } and lambda_ { i , t } vectors ? Scalars ? Abstract node notations ? It is not clear in the model section . Also , it took me a long time to figure out that \u2018 i \u2019 is used to index each entity ( it is mentioned later ) . The entity and location embeddings e_ { i , t } and lambda_ { i , t } are d-dimensional vectors , although we also overload the symbols to refer to abstract nodes in the model \u2019 s knowledge graphs . In the updated manuscript we state both these facts explicitly and state much earlier that \u2018 i \u2019 is the index for entities . 2.The paper says v_i ( initial representation of each entity ) is obtained by looking at the contextualized representations ( LSTM outputs ) of entity mention in the context . What happens if there are multiple mentions in the text ? Which one does it look at ? When there are multiple mentions of entity i , the initial representation v_i is formed by summing the representations of each mention . We have updated the paper to clarify this ( Sec 4.1 ) . 3.For the LSTM in the graph update , why does it have only one input ? Shouldn \u2019 t it have two inputs , one for previous hidden state and the other for input ? Good point ! We \u2019 ve improved the notation used to describe the model in Section 4 . The update equation now shows clearly that the LSTM takes in the concatenation of two node inputs ( entity and location embeddings ) along with the previous hidden state . 4.Regarding Recipe experiments , the paper says it reaches a better performance than the baseline using just 10k examples out of 60k . This is great , but could you also report the number when the full dataset is used ? We \u2019 ve completed an experiment on the full Recipes dataset and updated the paper to describe the result ( this experiment did not finish in time for the initial submission ) . The model \u2019 s F1 score improves from 51.64 on the partial data to 54.27 on the full data , surpassing the previous state of the art by a more significant margin . 5.What does it mean that in training time the model \u201c updates \u201d the location node representation with the encoding of the correct span . Do you mean you use the encoding instead ? We meant that we perform teacher-forcing to train the model . During training , we extract the context encodings for the groundtruth span and use these in downstream operations to obtain the node representations . At test time , we use the MRC module \u2019 s predicted span rather than the groundtruth . 6.For ProPara task 2 , what threshold did you choose to obtain the P/R/F1 score ? Is it the threshold that maximizes F1 ? For ProPara task 2 , our model was optimized for micro averaged F1 on the development set . Tandon et al . ( 2018 ) were kind enough to provide us with their evaluation script ."}, {"review_id": "S1lhbnRqF7-1", "review_text": "* Summary This paper addresses machine reading tasks involving tracking the states of entities over text. To this end, it proposes constructing a knowledge graph using recurrent updates over the sentences of the text, and using the graph representation to condition a reading comprehension module. The paper reports positive evaluations on three different tasks. * Review This is an interesting paper. The key technical component in the proposed approach is the idea that keeping track of entity states requires (soft) coreference between newly read entities and locations and the ones existing in the knowledge graph constructed so far. The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says. This is especially the case in a few places involving coreference: 1. The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation. 2. The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities. While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference. Why does the graph update require coreference pooling again? Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right? Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time? That the model implicitly learns constraints from data is interesting! Bottomline: The paper presents interesting ideas and good results, but would be better if the modeling choices were better explored/motivated. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the insightful comments . We \u2019 ve tried to improve our paper based on your feedback . Most significantly , we \u2019 ve performed additional ablation studies to confirm that our modeling choices improve performance , and we provide further empirical insight on what the coreference operations do . We \u2019 ve also updated the model description and the notation in Section 4 to clarify modeling mechanisms and choices . Two important additions are a high-level summary of the model , which we give at the beginning of Section 4 , and a table ( Table 2 ) that lists what each symbol represents along with its dimensions . Below we address your concerns point-by-point . The proposed method seems plausible , but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says . This is especially the case in a few places involving coreference : 1 . The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation . 2.The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities . While these may indeed be working as advertised , it would be good to see some evaluation that verifies that after learning , what is actually happening is coreference . ====== Based on your comments , we \u2019 ve performed additional ablations to measure the impact of the co-reference mechanisms . We find that removing any of them leads to a decrease in performance ( Rows 2 , 3 , 4 of Table 5 ) . To provide more than just this quantitative insight , we \u2019 ll expand here on how KG-MRC handles coreference to better motivate the modeling choices : The construction of graph G_t from G_ { t-1 } uses co-reference disambiguation of nodes to prevent node duplication and to enforce temporal dependencies . We perform coreference disambiguation between location nodes of G_t and G_ { t-1 } via Eq.1 ( call this inter-graph coreference ) and between the location nodes in the same graph Gt ( call this intra-graph coreference ) via Eq.2.The inter-graph coreference yields new , intermediate representations for the nodes in G_t . These are further updated via the intra-graph coreference step . Inter-graph Co-ref : One way to think about this is that we construct a new graph G_t at every time step . Now the graph G_ { t-1 } might contain some location nodes which are predicted again at time step \u2018 t \u2019 ( e.g. , in Figure 2 , leaf node already existed in G_ { t-1 } ) . Instead of replacing an old node with an entirely new node at \u2018 t \u2019 , we take a recurrent approach and do a gated update that preserves some information stored in the node in previous time steps while adding new information unique to time step \u2018 t \u2019 . Intra-graph Co-ref : Inter-graph co-ref isn \u2019 t enough since the MRC module makes its span predictions independently . This means that , at time step t , the model could predict the same span/location for multiple entities and add all these duplicates to the graph . Moreover , a single location might have the same surface form but be from different parts of the paragraph ( e.g. \u201c leaf \u201d in the 1st and the 5th sentence of the para in figure 2 ) . The operations in Eq.2 resolve this by performing self-attention ( i.e. , the predicted locations of all entities are compared to each other ) . ====="}, {"review_id": "S1lhbnRqF7-2", "review_text": "The paper addresses a challenging problem of predicting the states of entities over the description of a process. The paper is very well written, and easily understandable. The authors propose a graph structure for entity states, which is updated at each step using the outputs of a machine comprehension system. The approach is novel and well motivated. I will suggest a few improvements: 1. the NPN model seems a good alternative, will be good to have a discussion about why your model is better than NPN. Also, NPN can probably be modified to output spans of a sentence. I will be curious to know how it performs. 2. A more detailed illustration of the system / network is needed. Would have made it much easier to understand the paper. 3. What are the results when using the whole training set of Recipes ? ", "rating": "7: Good paper, accept", "reply_text": "We \u2019 re glad that you found the paper interesting and well-written . To address your comments and questions : 1. the NPN model seems a good alternative , will be good to have a discussion about why your model is better than NPN . Also , NPN can probably be modified to output spans of a sentence . I will be curious to know how it performs . The NPN model requires a pre-defined lexicon of action types ( i.e. , verbs ) , such as cut , bake , boil , etc . For the recipes dataset , the action types and their causal effects were manually collected and defined . Since the ProPara dataset does not have these annotations , we would have to manually identify action types to apply NPN to it . Also , NPN treats the state change as a classification problem ( of about 260 classes that are also manually defined ) . In contrast , KG-MRC finds the state-describing span in the text directly , which we believe is a more generic approach . 2.A more detailed illustration of the system / network is needed . Would have made it much easier to understand the paper . We agree that more detail would help readers to understand the model better . We \u2019 ve made some hopefully significant updates to Section 4 ( model description and notation ) to improve clarity , and we hope you \u2019 ll take the time to read the new manuscript . Two important additions are a high-level summary of the model , which we give at the beginning of Section 4 , and a table ( Table 2 ) that lists what each symbol represents along with its dimensions . 3.What are the results when using the whole training set of Recipes ? We \u2019 ve completed an experiment on the full Recipes dataset and updated the paper to describe the result ( this experiment did not finish in time for the initial submission ) . The model \u2019 s F1 score improves from 51.64 on the partial data to 54.27 on the full data , surpassing the previous state of the art by a more significant margin ."}], "0": {"review_id": "S1lhbnRqF7-0", "review_text": "The paper proposes a recurrent knowledge graph (bipartite graph between entities and location nodes) construction & updating mechanism for entity state tracking datasets such as (two) ProPara tasks and Recipes. The model goes through the following three steps: 1) it reads a sentence at each time step t and identifies the location of each entity via machine reading comprehension model such as DrQA (entities are predefined). 2) Co-reference module adjusts relationship scores (soft adjacency matrix) among nodes, including possibly new nodes introduced by the MRC model. 3) to propagate the relational information across all the nodes, the model performs L layers of LSTM for each entity that attend on other nodes via attention (where the weights come from the adjacency matrix). The model repeats the three steps for each sentence. The model is trained by directly supervising for the correct span by the MRC model at each time step, which is possible because the data provides strong supervision for each sentence (not just the answer at the end). The model achieves the state of the art in the two tasks of ProPara and Recipes dataset. Strengths: The paper provides an elegant solution for tracking relationship between entities as time (sentence) progresses. I also agree with the authors that this line of work (dynamic KG construction and modification) is an important area of research. While the model shares a similar spirit to EntNet, I think the model has enough distinctions / contributions, especially given that it outperforms EntNet by a large margin. The model also obtains non-trivial improvement over previous SOTA models. Weaknesses: Paper could have been written better. I had hard time understanding it. The notations are overall confusing and not explained well. Also there are a few unclear parts which I discuss in questions below. Questions: 1. Are e_{i,t} and lambda_{i,t} vectors? Scalars? Abstract node notations? It is not clear in the model section. Also, it took me a long time to figure out that \u2018i\u2019 is used to index each entity (it is mentioned later). 2. The paper says v_i (initial representation of each entity) is obtained by looking at the contextualized representations (LSTM outputs) of entity mention in the context. What happens if there are multiple mentions in the text? Which one does it look at? 3. For the LSTM in the graph update, why does it have only one input? Shouldn\u2019t it have two inputs, one for previous hidden state and the other for input? 4. Regarding Recipe experiments, the paper says it reaches a better performance than the baseline using just 10k examples out of 60k. This is great, but could you also report the number when the full dataset is used? 5. What does it mean that in training time the model \u201cupdates\u201d the location node representation with the encoding of correct span. Do you mean you use the encoding instead? 6. For ProPara task 2, what threshold did you choose to obtain the P/R/F1 score? Is it the threshold that maximizes F1? ", "rating": "7: Good paper, accept", "reply_text": "Thank you for the useful feedback . We \u2019 ve updated our paper to take it into account -- we \u2019 ve updated the model description and the notation in Section 4 to clarify our method . Two important additions are a high-level summary of the model , which we give at the beginning of Section 4 , and a table ( Table 2 ) that lists what each symbol represents along with its dimensions . We also made several updates that address your specific questions . 1.Are e_ { i , t } and lambda_ { i , t } vectors ? Scalars ? Abstract node notations ? It is not clear in the model section . Also , it took me a long time to figure out that \u2018 i \u2019 is used to index each entity ( it is mentioned later ) . The entity and location embeddings e_ { i , t } and lambda_ { i , t } are d-dimensional vectors , although we also overload the symbols to refer to abstract nodes in the model \u2019 s knowledge graphs . In the updated manuscript we state both these facts explicitly and state much earlier that \u2018 i \u2019 is the index for entities . 2.The paper says v_i ( initial representation of each entity ) is obtained by looking at the contextualized representations ( LSTM outputs ) of entity mention in the context . What happens if there are multiple mentions in the text ? Which one does it look at ? When there are multiple mentions of entity i , the initial representation v_i is formed by summing the representations of each mention . We have updated the paper to clarify this ( Sec 4.1 ) . 3.For the LSTM in the graph update , why does it have only one input ? Shouldn \u2019 t it have two inputs , one for previous hidden state and the other for input ? Good point ! We \u2019 ve improved the notation used to describe the model in Section 4 . The update equation now shows clearly that the LSTM takes in the concatenation of two node inputs ( entity and location embeddings ) along with the previous hidden state . 4.Regarding Recipe experiments , the paper says it reaches a better performance than the baseline using just 10k examples out of 60k . This is great , but could you also report the number when the full dataset is used ? We \u2019 ve completed an experiment on the full Recipes dataset and updated the paper to describe the result ( this experiment did not finish in time for the initial submission ) . The model \u2019 s F1 score improves from 51.64 on the partial data to 54.27 on the full data , surpassing the previous state of the art by a more significant margin . 5.What does it mean that in training time the model \u201c updates \u201d the location node representation with the encoding of the correct span . Do you mean you use the encoding instead ? We meant that we perform teacher-forcing to train the model . During training , we extract the context encodings for the groundtruth span and use these in downstream operations to obtain the node representations . At test time , we use the MRC module \u2019 s predicted span rather than the groundtruth . 6.For ProPara task 2 , what threshold did you choose to obtain the P/R/F1 score ? Is it the threshold that maximizes F1 ? For ProPara task 2 , our model was optimized for micro averaged F1 on the development set . Tandon et al . ( 2018 ) were kind enough to provide us with their evaluation script ."}, "1": {"review_id": "S1lhbnRqF7-1", "review_text": "* Summary This paper addresses machine reading tasks involving tracking the states of entities over text. To this end, it proposes constructing a knowledge graph using recurrent updates over the sentences of the text, and using the graph representation to condition a reading comprehension module. The paper reports positive evaluations on three different tasks. * Review This is an interesting paper. The key technical component in the proposed approach is the idea that keeping track of entity states requires (soft) coreference between newly read entities and locations and the ones existing in the knowledge graph constructed so far. The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says. This is especially the case in a few places involving coreference: 1. The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation. 2. The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities. While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference. Why does the graph update require coreference pooling again? Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right? Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time? That the model implicitly learns constraints from data is interesting! Bottomline: The paper presents interesting ideas and good results, but would be better if the modeling choices were better explored/motivated. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for the insightful comments . We \u2019 ve tried to improve our paper based on your feedback . Most significantly , we \u2019 ve performed additional ablation studies to confirm that our modeling choices improve performance , and we provide further empirical insight on what the coreference operations do . We \u2019 ve also updated the model description and the notation in Section 4 to clarify modeling mechanisms and choices . Two important additions are a high-level summary of the model , which we give at the beginning of Section 4 , and a table ( Table 2 ) that lists what each symbol represents along with its dimensions . Below we address your concerns point-by-point . The proposed method seems plausible , but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says . This is especially the case in a few places involving coreference : 1 . The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation . 2.The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities . While these may indeed be working as advertised , it would be good to see some evaluation that verifies that after learning , what is actually happening is coreference . ====== Based on your comments , we \u2019 ve performed additional ablations to measure the impact of the co-reference mechanisms . We find that removing any of them leads to a decrease in performance ( Rows 2 , 3 , 4 of Table 5 ) . To provide more than just this quantitative insight , we \u2019 ll expand here on how KG-MRC handles coreference to better motivate the modeling choices : The construction of graph G_t from G_ { t-1 } uses co-reference disambiguation of nodes to prevent node duplication and to enforce temporal dependencies . We perform coreference disambiguation between location nodes of G_t and G_ { t-1 } via Eq.1 ( call this inter-graph coreference ) and between the location nodes in the same graph Gt ( call this intra-graph coreference ) via Eq.2.The inter-graph coreference yields new , intermediate representations for the nodes in G_t . These are further updated via the intra-graph coreference step . Inter-graph Co-ref : One way to think about this is that we construct a new graph G_t at every time step . Now the graph G_ { t-1 } might contain some location nodes which are predicted again at time step \u2018 t \u2019 ( e.g. , in Figure 2 , leaf node already existed in G_ { t-1 } ) . Instead of replacing an old node with an entirely new node at \u2018 t \u2019 , we take a recurrent approach and do a gated update that preserves some information stored in the node in previous time steps while adding new information unique to time step \u2018 t \u2019 . Intra-graph Co-ref : Inter-graph co-ref isn \u2019 t enough since the MRC module makes its span predictions independently . This means that , at time step t , the model could predict the same span/location for multiple entities and add all these duplicates to the graph . Moreover , a single location might have the same surface form but be from different parts of the paragraph ( e.g. \u201c leaf \u201d in the 1st and the 5th sentence of the para in figure 2 ) . The operations in Eq.2 resolve this by performing self-attention ( i.e. , the predicted locations of all entities are compared to each other ) . ====="}, "2": {"review_id": "S1lhbnRqF7-2", "review_text": "The paper addresses a challenging problem of predicting the states of entities over the description of a process. The paper is very well written, and easily understandable. The authors propose a graph structure for entity states, which is updated at each step using the outputs of a machine comprehension system. The approach is novel and well motivated. I will suggest a few improvements: 1. the NPN model seems a good alternative, will be good to have a discussion about why your model is better than NPN. Also, NPN can probably be modified to output spans of a sentence. I will be curious to know how it performs. 2. A more detailed illustration of the system / network is needed. Would have made it much easier to understand the paper. 3. What are the results when using the whole training set of Recipes ? ", "rating": "7: Good paper, accept", "reply_text": "We \u2019 re glad that you found the paper interesting and well-written . To address your comments and questions : 1. the NPN model seems a good alternative , will be good to have a discussion about why your model is better than NPN . Also , NPN can probably be modified to output spans of a sentence . I will be curious to know how it performs . The NPN model requires a pre-defined lexicon of action types ( i.e. , verbs ) , such as cut , bake , boil , etc . For the recipes dataset , the action types and their causal effects were manually collected and defined . Since the ProPara dataset does not have these annotations , we would have to manually identify action types to apply NPN to it . Also , NPN treats the state change as a classification problem ( of about 260 classes that are also manually defined ) . In contrast , KG-MRC finds the state-describing span in the text directly , which we believe is a more generic approach . 2.A more detailed illustration of the system / network is needed . Would have made it much easier to understand the paper . We agree that more detail would help readers to understand the model better . We \u2019 ve made some hopefully significant updates to Section 4 ( model description and notation ) to improve clarity , and we hope you \u2019 ll take the time to read the new manuscript . Two important additions are a high-level summary of the model , which we give at the beginning of Section 4 , and a table ( Table 2 ) that lists what each symbol represents along with its dimensions . 3.What are the results when using the whole training set of Recipes ? We \u2019 ve completed an experiment on the full Recipes dataset and updated the paper to describe the result ( this experiment did not finish in time for the initial submission ) . The model \u2019 s F1 score improves from 51.64 on the partial data to 54.27 on the full data , surpassing the previous state of the art by a more significant margin ."}}