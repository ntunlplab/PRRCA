{"year": "2019", "forum": "rJlEojAqFm", "title": "Relational Forward Models for Multi-Agent Learning", "decision": "Accept (Poster)", "meta_review": "\npros:\n- interesting application of graph networks for relational inference in MARL, allowing interpretability and, as the results show, increasing performance\n- better learning curves in several games\n- somewhat better forward prediction than baselines\n\ncons:\n- perhaps some lingering confusion about the amount of improvement over the LSTM+MLP baseline\n\nMany of the reviewer's other issues have been addressed in revision and I recommend acceptance.", "reviews": [{"review_id": "rJlEojAqFm-0", "review_text": " This paper used graph neural networks to do relational reasoning of multi-agent systems to predict the actions and returns of MARL agents that they call Relational Forward Modeling. They used RFM to analyze and assess the coordination between agents in three different multi-agent environments. They then constructed an RFM-aumented RL agent and showed improved training speeds over non relational reasoning baseline methods. I think the overall approach is interesting and a novel way to address the growing concern of how to access coordination between agents in multi-agent systems. I also like how they authors immediately incorporated the relational reasoning approach to improve the training of the MARL agents. I wonder how dependent this approach is to the semantic representation of the environment. These semantic descriptions are similar to hand crafted features and thus will require some prior knowledge about the environment or task and will be harder to obtain on more difficult environment and tasks. Will this approach work on continuous tasks? For example, the continuous state and action space of the predator-prey tasks that use the multi-agent particle environment from OpenAi. I think one of the biggest selling points from this paper is using this method to assess the coordination/collaboration between agents (i.e. the social influence amongst agents). I would have liked to see more visualizations or analysis into these learned representations. The bottom row of Figure 3 shows that \"when stags become available, agents care about each other more than just before that happens\". While this is very interesting and an important result, i think that this allows one to see what features of the environment (including other agents) are important to a particular agents decision making but it doesn't really answer whether the agents are truly coordinated, i.e. whether there are any causal dependencies between agents. For the RFM augmented agents, I like that you are able to train the policy as well as the RFM simultaneously from scratch, however, it seems that this requires you to only train a single agent in the multi-agent environment. If I understand correctly, for a given multi-agent environment, you first pre-trained A2C agents to play the three MARL games and then you paired one of the pre-trained (expert) agents with the RFM-augmented learning agents during training. This seems to limit the practicality and usability of this method as it requires you to have pre-trained agents that have already solved the task. I would like to know why the authors didn't try to train two (or four) RFM-augmented agents from scratch together. When you use one of the agents as a pre-trained agent, this might make the training of the RFM module a bit easier since you have at least one agent with a fixed policy to predict actions from. It could be challenging when trying to train both RFM modules on two learning agents as the behaviors of learning agents are changing over time and thus the learning might be unstable. Overall, i think this is an interesting approach and especially for probing what information drives agents' behaviors. However, I don't see the benefit of the RFM-augmented agent provides. It's clearly shown to learn faster than non RFM-augmented agents (which is good), however, unless I'm mistaken, the RFM-augmented agent requires a pre-trained agent to be able to learn in the first place. --edit: The authors have sufficiently addressed my questions and concerns and have performed additional analysis. My biggest concern of weather or not the RFM-augmented agent was capable of learning without a pre-trained agent has been addressed with additional experiments and analysis (Figure 8). Based on this, i have adjusted my rating to a 7. ", "rating": "7: Good paper, accept", "reply_text": "3 ) Are the agents truly coordinated ? Can we measure causal influence between agents ? This is a good question . There are many potential definitions of coordination . From a game theoretic perspective , the agents have definitely found a coordinative equilibrium . They coordinate to consume stags , which is reflected in their overall return . Another sense of coordination is whether agents \u2019 behaviors are mutually interdependent in service of a common goal . The RFM analysis in the bottom of Figure 3 demonstrates that it is statistically appropriate to describe the agents \u2019 behavior as mutually interdependent when stags are present . The common goal , of course , is stag consumption . A final sense might be whether there are ground-truth causal influences between agents . We note that the RFM approach we pursue here is not designed to answer causal questions , per se : it is a statistical fit to time-series data , and does not traffic directly with interventions or counterfactuals . We are currently exploring such possibilities in ongoing research . If there \u2019 s an additional sense of coordination that you \u2019 re interested , please feel free to suggest a specific experiment to falsify the conjecture that we \u2019 re picking up on something other than coordination here . As a further test of whether the agents are coordinated in these ways , we ran two additional experiments ( 1 ) where there is no scope for coordination between the agents and ( 2 ) when no interdependent behavior is required . For additional experiment ( 1 ) We trained deep RL agents on a modified version of Stag Hunt where stags yielded no reward at all ( i.e.the only rewards are for consuming apples ) . We then trained the RFM on rollouts of these agents \u2019 behavior . In contrast to the standard case , we found that the edge norms between agents was not modulated by the appearance of a stag ( Figure 9a ) . In additional experiment ( 2 ) we obtained similar results in a version of the environment where stags could be consumed by single agents , without the need for coordination ( Figure 9b ) . 4 ) Does the RFM-augmented agent require pre-trained agents ? Not at all . We only chose to include this experiment previously to isolate the benefit of including the RFM in the augmented agent and because this situation is relevant to artificial agnets learning to act in an environment shared with human experts . However , the RFM-augmented agent can be trained in just the same way alongside learning agents too , with similar benefits . We have included this in Figure 8 in a revised version of the manuscript . We note that when the RFM-augmented agent is trained alongside teammates which are also learning agents , the relative benefit of the RFM on the agent \u2019 s return is smaller in magnitude than when this agent is trained alongside expert teammates . We suspect the reason for this is that the RFM model is initially modeling the behavior of untrained teammates , and there are fewer opportunities for rewarded coordination . Since the teammates are learning at roughly the same rate as the RFM-augmented agent , the RFM only has a chance to provide useful information later on during training . References : Battaglia et al , ( 2016 ) . Interaction networks for learning about objects , relations and physics . NIPS.Santoro , et al . ( 2017 ) .A simple neural network module for relational reasoning . NIPS.Watters , et al . ( 2017 ) .Visual interaction networks : Learning a physics simulator from video . NIPS.Barrett , et al . ( 2018 ) .Measuring abstract reasoning in neural networks . arXiv:1807.04225 . Zambaldi , et al . ( 2018 ) .Relational Deep Reinforcement Learning . arXiv:1806.01830 ."}, {"review_id": "rJlEojAqFm-1", "review_text": "RELATIONAL FORWARD MODELS FOR MULTI-AGENT LEARNING Summary: Model free learning is hard, especially in multi-agent systems. The authors consider a way of reducing variance which is to have an explicit model of actions that other agents will take. The model uses a graphical structure and the authors argue it is a) interpretable, b) predicts actions better and further forward than competing models, c) can increase learning speed. Strong Points: - The main innovation here is that the model uses a graph conv net-like architecture which also allows for interpretable outputs of \u201cwhat is going on\u201d in a game. - The authors show that the RFM increases learning speed in several games - The authors show that the RFM does somewhat better at forward action prediction than a na\u00efve LSTM+MLP setup and other competing models Weak Point - The RFM is compared to other models in predicting forwards actions but is not compared to other models in Figure 5, so it is not clear that the graphical structure is actually required to speed up learning. I would like to see these experiments added before we can say that the RFM is adding to performance. - Related: The authors argue that an advantage of the RFM is that it is interpretable, but I thought a main argument of Rabinowitz et. al. was that simple forward models similar to the LSTM+MLP here were also interpretable? If the RFM does not improve learning above and beyond the LSTM+MLP then the argument comes down to more accurate action prediction (ok) and more interpretability (maybe) which is less compelling. Clarifying Questions - How does the 4 player Stag Hunt work? Do all 4 agents have to step on the Stag together or just 2 of them? How are rewards distributed? Is there a negative payoff for Hunting the stag alone as in the Peysakhovich & Lerer paper? - Related: In the Stag Hunt there are multiple equilibria, either agents learn to get plants (which is safe but low payoff) or they learn to Hunt (which is risky but high payoff). Is the RFM leading to more convergence to the Hunting state or is it simple leading agents to learn the safe but low payoff strategies faster? - The choice of metric in Figure 2 (# exactly correct prediction) is non-standard (not saying it is wrong). I think it would be good to also see a plot of a more standard metric such as loglikelihood of the model's for each of X possible steps ahead. It would help to clarify where the RFM is doing better (is it better at any horizon or is it just able to look further forward more accurately than the competitors?) ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your insightful questions and suggestions on the submission . We hope we have addressed your questions below . In particular , we have addressed your two major weak points : the graphical structure of the embedded RFM is indeed helpful for speeding up learning ; and the type and degree of interpretability differs substantially from previous work . We hope you will revise your rating accordingly . 1 ) Can we augment agents with models other than the RFM ( e.g.MLP + LSTM ) ? Yes ! We have performed your suggested experiments , which we show in Figure 7 . Indeed , the RFM-augmented agents outperform the MLP+LSTM-augmented agents ( i.e.those with forward models that are non-relational ) . 2 ) How does interpretability compare with ToMNet ( Rabinowitz et al , 2018 ) ? This is a good question . To begin , interpretability is not a binary property of a model ; there are a number of advantages that RFMs offer over the ToMNet construction in Rabinowitz et al , 2018 . Most importantly , the ToMNet embeds sequences of behavior as points in an unstructured , high-dimensional Euclidean space , relying on the inherent structure of the data ( and optionally an information bottleneck ) to yield interpretable representations . In contrast , RFMs shape behavioral representations through the structure of entities and relations ( via the graph net ) . These representations are very natural for humans to interface with , as they conform to representation schemas of human cognition ( Spelke & Kinzler , 2007 ) . Moreover , the representations of the RFM also allow us to easily ask directed questions about how different entities influence agent behavior ( e.g.Figure 3 ) , which is not something that the ToMnet enables . We also note that the ToMNet was designed as a single-agent forward model , while RFMs naturally scale to the multi-agent setting . This allows us to augment agents with the RFM module , which is not something pursued in this previous work . 3 ) Clarifications about Stag Hunt 4 players - do all 4 need to step on the stag to capture it ? No , only 2 Is there a negative reward for hunting alone ? No Does including the RFM leads to more Stags being captured ? We have only made qualitative observations , but anecdotally , the RFM-augmented agents go for stags like maniacs . 4 ) Choice of metric in Figure 2 . We have now provided an alternative metric in Figure 10 ( next-step action classification accuracy ) which shows the same qualitative results . The reason we use the particular metric in the main text is that is gives us a measure of how long the model remains useful . In particular , we are learning a simulator of the agents dynamics ; the metric gives an indication of how many steps one can simulate before making a mistake . Alternative rollout metrics are hard to define beyond the first mistake when the true environment and the predictions have diverged . There most likely isn \u2019 t a perfect metric that covers all bases , and in particular alternative rollout metrics are hard to define after the model makes a mistake , since the ground-truth observations and predictions no longer match . Nonetheless , between this and the new Figure 10 we believe there \u2019 s a strong case that the RFM-based model is better . References : Spelke & Kinzler . ( 2007 ) .Core knowledge . Developmental science , 10 ( 1 ) , 89-96 ."}, {"review_id": "rJlEojAqFm-2", "review_text": "This paper studies predicting multi-agent behavior using a proposed neural network architecture. The architecture, called a relational forward model (RFM) is the same graph network proposed by Battaglia et al., 2018, but adds a recurrent component. Two tasks are define: predict the next action of each agent, and predict the sum of future rewards. The paper demonstrates that RFMs outperform two baselines and two ablations. The authors also show that edge activation magnitudes are correlated with certain phenomenons (e.g. an agent walking towards an entity, or an entity being \u201con\u201d or \u201coff\u201d). The authors also show that appending the output of a pre-trained RFM to the state of a policy can help it learn faster. Overall, this paper presents some interesting ideas and is easy to follow, but the significance of the paper is not clear. The architecture is a rather straightforward extensions of previous work, and using graph networks for predictive modeling in multi-agent settings has been examined in the past, making the technical contributions not particularly novel. Examining the correlation between edge activation magnitudes and certain events is intriguing and perhaps the most novel aspect of this paper, but it is not clear how or why this information would be useful. There a few unsubstantiated claims that are concerning. There are also some odd experimental decisions and results that should be addressed. For specific comments: 1. Why would using a recurrent network help (i.e. RFM vs Feedforward)? Unless the policies are non-Markovian, the entire prediction problem should Markovian. I suspect that most of the gains are coming from the fact that the RFM method simply has more parameters than the Feedforward method (e.g. it can amortize some of the computation into the recurrent part of the network). Suggestion: train a Feedforward model that has more parameters (with appropriate hyperparameter sweeps) to see if this is the cause. If not, provide some analysis for why \u201cmemories of the relations between entities\u201d would be any more beneficial than simply recomputing those relations. 2. The other potential reason that the recurrent method did better is that policy actions are highly correlated (e.g. because agents move in straight lines to locations). If so, then recurrent methods can outperform feedforward methods without having to learn anything about what actually causes policies to move in certain directions. Suggestion: measure the correlation between consecutive actions. If there is non-trivial correlation, than this suggests that RFM does better than Feedforward (which is basically prior work of Battaglia et. al.) is for the wrong reasons. 3. If I understand the evaluation metric correctly, for each rollout, it counts how many steps from the beginning of the rollout match perfectly before the first error occurs. Then it averages this \u201cminimum time to failure\u201d across all evaluation rollouts. If this is correct, why was this evaluation metric chosen? A much more natural metric would be to just compute the average number of errors on a test data-set (and if this is what is actually reported, please update the description to disambiguate the two). The current metric could be very deceptive: Methods that do very well on states around the initial-state distribution but poorly near the end of trajectories (e.g. perfectly predicts the actions in the first 10 steps, but then resorts to random guessing for the last 99999 time steps) will outperform methods that have lower average error rate (e.g. a model that is correct 50% of the time). Suggestion: change the metrics to average number of errors, or report both, or provide a convincing argument why this metric is meaningful. 4. Unless I misunderstood, the results in Section 2.2.3 seem spurious and the claims seem unsubstantiated. For one, if we look at Equations (1) and (2), when we average over s_a1 and s_a2, they should both give the same average for R_a1. Put another way: the prune graph should (in theory) marginalize out s_a2. On average, its expected output should be the same as the output of the full graph (after marginalizing out s_a1 and s_a2). Obviously, it is possible to find specific rollouts where the full graph has higher value than the prune graph (and it seems Figure 4 does this), but it should equally be possible to find rollouts where the opposite is true. I\u2019m hoping I misunderstood this section, but otherwise this seems to invalidate all the claims made in this section. 5. Even if concern #4 is addressed, the following sentence would still seem false: \u201cThis figure shows that teammates\u2019 influence on each other during this time is beneficial to their return.\u201d The figure simply shows predictions of the RFM, and not of the ground truth. Moreover, it\u2019s not clear what \u201cteammates\u2019 influence\u201d actually means. 6. The comparison to NRI seems rather odd, since that method uses strictly less information than RFM. 7. For Section 3, is the RFM module pretrained and then fine-tuned with the new policy? If so, this gives the \u201cRFM + A2C\u201d agent extra information indirectly via the pretrained weights of the RFM module. 8. I\u2019m not sure what to make of the correlation analysis. It is not too surprising that there is some correlation (in fact, it\u2019d be quite an interesting paper if the findings were that there wasn\u2019t a correlation!), and it\u2019s not clear to me how this could be used for debugging, visualizations, etc. If someone wanted to analyze the correlation between two entities and a policy\u2019s action, it seems like they could directly model this correlation. Some minor comments: - In Figure 3C, right, why isn\u2019t the magnitude 0 at time=1? Based on the other plots in Figure 3c, it seems like it should be 0. - The month/year in many of the citations seems odd. - The use of the word \u201cvalence\u201d seems unnecessarily flowery and distracting. My main concern with this paper is that it is not particularly novel and the contribution seems questionable. I have some concerns over the experimental metric and Section 2.2.3, but even if that is clarified, it is not clear how impactful this paper would be. The use of a recurrent network seems unnecessary, unjustified, and not analyzed. The analysis of correlations is interesting, but not particularly compelling or surprising. And lastly, the RFM-augmented results are not very strong. -- Edit: After discussing with the authors, I have changed my rating. The authors have adjusted some of the language, which I previously thought overstated the contributions and was misleading. They have added a number of experiments which valid the claim that their method is proposing a reasonable way of measuring collaboration. I also realized that I misunderstood one of the sections, and I encourage the authors to improve the presentation to (1) present the significance of the experiments more clear, (2) not overstate the results, and (3) emphasize the contribution more clearly. Overall, the paper presents convincing evidence that factors in a graph neural networks do capture some notion of collaboration. I do not feel that the paper is particularly novel, but the experiments are thorough. Furthermore, their experiments show that adding an RFM module to an agent consistently helps (albeit not by much). Given that the multi-agent community is still trying to decide how to best quantify and use metrics for collaboration, I find it difficult to access the long-term impact of this paper. However, given the thoroughness of the experiments and analysis, I suspect that this will be valuable for the community and deserves some visibility.", "rating": "7: Good paper, accept", "reply_text": "5 ) Even if concern # 4 is addressed , the following sentence would still seem false : \u201c This figure shows that teammates \u2019 influence on each other during this time is beneficial to their return. \u201d The figure simply shows predictions of the RFM , and not of the ground truth . Moreover , it \u2019 s not clear what \u201c teammates \u2019 influence \u201d actually means . Good catch . We can not actually compute the ground truth . We have rephrased this to : \u201c Thus the model estimates that teammates ' specific interactions during this time are beneficial to their return. \u201d 6 ) The comparison to NRI seems rather odd , since that method uses strictly less information than RFM . NRI has access to the same set of information as the RFM . In comparison however , NRI actively discards information : it infers a connectivity map from the past trajectory , and may choose to do inference using any number of edges . This might actually confer advantages if the ground-truth process is indeed sparse and relatively stationary . Overall , NRI is a good method and it has many advantages . Our results nevertheless demonstrate that these design decisions are less well-suited to the patterns of multi-agent interaction in these environments . 7 ) For Section 3 , is the RFM module pretrained and then fine-tuned with the new policy ? If so , this gives the \u201c RFM + A2C \u201d agent extra information indirectly via the pretrained weights of the RFM module . The on-board RFM is trained * from scratch * alongside the policy network . There is no pre-training . 8 ) I \u2019 m not sure what to make of the correlation analysis . It is not too surprising that there is some correlation ( in fact , it \u2019 d be quite an interesting paper if the findings were that there wasn \u2019 t a correlation ! ) , and it \u2019 s not clear to me how this could be used for debugging , visualizations , etc . If someone wanted to analyze the correlation between two entities and a policy \u2019 s action , it seems like they could directly model this correlation . Measuring the emergence of coordinated behavior in multi-agent systems is an important open problem in this field ( see also AnonReviewer3 \u2019 s comment to this effect ) . Especially in the case of * learning * systems , assessing whether or not agents are able to coordinate , what drives their behavior , whether they learn to help or hinder one another and how they modify their behavior in response to changes in the environment are all crucial aspects of multi-agent analysis that we struggle to quantify . Here we show that learning relational models of multi-agent systems might be a good place to look . With respect to the particular suggestion that one could directly model correlations between entities and a policy \u2019 s actions , we wish it were that simple ! The influence of one agent \u2019 s state on another \u2019 s behavior can be highly contextual , so one would need to factor in the state to tease apart the appropriate effect . This amounts to fitting a parameterized model of the interaction , which is precisely what we \u2019 re doing here . 9 ) Some minor comments a . In Figure 3C , right , why isn \u2019 t the magnitude 0 at time=1 ? Based on the other plots in Figure 3c , it seems like it should be 0 . Agents tend to rapidly move away from fruit or stags they just consumed . The probability of respawning is small ( 0.05 ) so there will be nothing there for some time ( on average ) . This means that there is some information that a recently-consumed entity can provide to the agent that just consumed it : when the entity is adjacent to the agent , its chances of respawning are lower than otherwise . This predictive information drops sharply after the first step as agents will move towards available entities ( which might be anywhere ) rather than directly away from unavailable ones ( see also Figure 3 top row the two right panels ) . b.The month/year in many of the citations seems odd . We have fixed this . c. The use of the word \u201c valence \u201d seems unnecessarily flowery and distracting . \u201c Valence \u201d is a standard technical term from psychology , denoting the attractiveness or aversiveness of a stimulus ( e.g.Frijda , 1986 ) . References Frijda , N. H. ( 1986 ) . The emotions . Cambridge University Press ."}, {"review_id": "rJlEojAqFm-3", "review_text": "This paper proposes to use graph neural networks in the scenario of multi-agent reinforcement learning (MARL). It tackles two current challenges, learning coordinated behaviours and measuring such coordination. At the core of the approach are graph neural networks (a cite to Scarselli 2009 would be reasonable): acting and non-acting entities are represented by a graph (with (binary) edges between acting-acting and acting-nonacting entities) and the graph network produces a graph where these edges are transformed into a vectorial representation, which then can be used by a downstream task, e.g. a policy algorithm (as in this paper) that uses it to coordinate behavour. Because the output of the graph network is a structurally identical graph to the input, it is possible to interpret this output. The paper is well written, the main ideas are clearly described. I'm uncertain about the novelty of the approach, at least the way the RFM is utilized in the policy is a nice idea (albeit, a-posteriori, sounds straight forward in the context of MARL). Similarly, using the graph output for interpretations is an obvious choice). Nevertheless, showing empirically that the ideas actually work gives the paper a lot of credibility for being a stepping stone in the area of MARL.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . We take it as a good sign that you see the ideas as obvious a posteriori . To the best of our knowledge , though , no one has actually explored them . When others have modelled the dynamics of multi-agent systems ( e.g.NRI , VAIN , ToMnet ) , they have not attempted to integrate these models into the agents themselves . Conversely , in papers that do put models of opponents in MARL , these models are not relational , and they model goals in agents identical to oneself ( e.g.Raileanu et al.2018 ) , policy representations or Q-values ( He et al.2016 ) , but not future actions . In a similar manner , no one has developed this method to interpret multi-agent behavior to the extent that we do here . The event-based analysis and value-based analyses are novel applications of this framework . Thank you for pointing out that we were missing a citation to Scarselli , we included it the revised manuscript ."}], "0": {"review_id": "rJlEojAqFm-0", "review_text": " This paper used graph neural networks to do relational reasoning of multi-agent systems to predict the actions and returns of MARL agents that they call Relational Forward Modeling. They used RFM to analyze and assess the coordination between agents in three different multi-agent environments. They then constructed an RFM-aumented RL agent and showed improved training speeds over non relational reasoning baseline methods. I think the overall approach is interesting and a novel way to address the growing concern of how to access coordination between agents in multi-agent systems. I also like how they authors immediately incorporated the relational reasoning approach to improve the training of the MARL agents. I wonder how dependent this approach is to the semantic representation of the environment. These semantic descriptions are similar to hand crafted features and thus will require some prior knowledge about the environment or task and will be harder to obtain on more difficult environment and tasks. Will this approach work on continuous tasks? For example, the continuous state and action space of the predator-prey tasks that use the multi-agent particle environment from OpenAi. I think one of the biggest selling points from this paper is using this method to assess the coordination/collaboration between agents (i.e. the social influence amongst agents). I would have liked to see more visualizations or analysis into these learned representations. The bottom row of Figure 3 shows that \"when stags become available, agents care about each other more than just before that happens\". While this is very interesting and an important result, i think that this allows one to see what features of the environment (including other agents) are important to a particular agents decision making but it doesn't really answer whether the agents are truly coordinated, i.e. whether there are any causal dependencies between agents. For the RFM augmented agents, I like that you are able to train the policy as well as the RFM simultaneously from scratch, however, it seems that this requires you to only train a single agent in the multi-agent environment. If I understand correctly, for a given multi-agent environment, you first pre-trained A2C agents to play the three MARL games and then you paired one of the pre-trained (expert) agents with the RFM-augmented learning agents during training. This seems to limit the practicality and usability of this method as it requires you to have pre-trained agents that have already solved the task. I would like to know why the authors didn't try to train two (or four) RFM-augmented agents from scratch together. When you use one of the agents as a pre-trained agent, this might make the training of the RFM module a bit easier since you have at least one agent with a fixed policy to predict actions from. It could be challenging when trying to train both RFM modules on two learning agents as the behaviors of learning agents are changing over time and thus the learning might be unstable. Overall, i think this is an interesting approach and especially for probing what information drives agents' behaviors. However, I don't see the benefit of the RFM-augmented agent provides. It's clearly shown to learn faster than non RFM-augmented agents (which is good), however, unless I'm mistaken, the RFM-augmented agent requires a pre-trained agent to be able to learn in the first place. --edit: The authors have sufficiently addressed my questions and concerns and have performed additional analysis. My biggest concern of weather or not the RFM-augmented agent was capable of learning without a pre-trained agent has been addressed with additional experiments and analysis (Figure 8). Based on this, i have adjusted my rating to a 7. ", "rating": "7: Good paper, accept", "reply_text": "3 ) Are the agents truly coordinated ? Can we measure causal influence between agents ? This is a good question . There are many potential definitions of coordination . From a game theoretic perspective , the agents have definitely found a coordinative equilibrium . They coordinate to consume stags , which is reflected in their overall return . Another sense of coordination is whether agents \u2019 behaviors are mutually interdependent in service of a common goal . The RFM analysis in the bottom of Figure 3 demonstrates that it is statistically appropriate to describe the agents \u2019 behavior as mutually interdependent when stags are present . The common goal , of course , is stag consumption . A final sense might be whether there are ground-truth causal influences between agents . We note that the RFM approach we pursue here is not designed to answer causal questions , per se : it is a statistical fit to time-series data , and does not traffic directly with interventions or counterfactuals . We are currently exploring such possibilities in ongoing research . If there \u2019 s an additional sense of coordination that you \u2019 re interested , please feel free to suggest a specific experiment to falsify the conjecture that we \u2019 re picking up on something other than coordination here . As a further test of whether the agents are coordinated in these ways , we ran two additional experiments ( 1 ) where there is no scope for coordination between the agents and ( 2 ) when no interdependent behavior is required . For additional experiment ( 1 ) We trained deep RL agents on a modified version of Stag Hunt where stags yielded no reward at all ( i.e.the only rewards are for consuming apples ) . We then trained the RFM on rollouts of these agents \u2019 behavior . In contrast to the standard case , we found that the edge norms between agents was not modulated by the appearance of a stag ( Figure 9a ) . In additional experiment ( 2 ) we obtained similar results in a version of the environment where stags could be consumed by single agents , without the need for coordination ( Figure 9b ) . 4 ) Does the RFM-augmented agent require pre-trained agents ? Not at all . We only chose to include this experiment previously to isolate the benefit of including the RFM in the augmented agent and because this situation is relevant to artificial agnets learning to act in an environment shared with human experts . However , the RFM-augmented agent can be trained in just the same way alongside learning agents too , with similar benefits . We have included this in Figure 8 in a revised version of the manuscript . We note that when the RFM-augmented agent is trained alongside teammates which are also learning agents , the relative benefit of the RFM on the agent \u2019 s return is smaller in magnitude than when this agent is trained alongside expert teammates . We suspect the reason for this is that the RFM model is initially modeling the behavior of untrained teammates , and there are fewer opportunities for rewarded coordination . Since the teammates are learning at roughly the same rate as the RFM-augmented agent , the RFM only has a chance to provide useful information later on during training . References : Battaglia et al , ( 2016 ) . Interaction networks for learning about objects , relations and physics . NIPS.Santoro , et al . ( 2017 ) .A simple neural network module for relational reasoning . NIPS.Watters , et al . ( 2017 ) .Visual interaction networks : Learning a physics simulator from video . NIPS.Barrett , et al . ( 2018 ) .Measuring abstract reasoning in neural networks . arXiv:1807.04225 . Zambaldi , et al . ( 2018 ) .Relational Deep Reinforcement Learning . arXiv:1806.01830 ."}, "1": {"review_id": "rJlEojAqFm-1", "review_text": "RELATIONAL FORWARD MODELS FOR MULTI-AGENT LEARNING Summary: Model free learning is hard, especially in multi-agent systems. The authors consider a way of reducing variance which is to have an explicit model of actions that other agents will take. The model uses a graphical structure and the authors argue it is a) interpretable, b) predicts actions better and further forward than competing models, c) can increase learning speed. Strong Points: - The main innovation here is that the model uses a graph conv net-like architecture which also allows for interpretable outputs of \u201cwhat is going on\u201d in a game. - The authors show that the RFM increases learning speed in several games - The authors show that the RFM does somewhat better at forward action prediction than a na\u00efve LSTM+MLP setup and other competing models Weak Point - The RFM is compared to other models in predicting forwards actions but is not compared to other models in Figure 5, so it is not clear that the graphical structure is actually required to speed up learning. I would like to see these experiments added before we can say that the RFM is adding to performance. - Related: The authors argue that an advantage of the RFM is that it is interpretable, but I thought a main argument of Rabinowitz et. al. was that simple forward models similar to the LSTM+MLP here were also interpretable? If the RFM does not improve learning above and beyond the LSTM+MLP then the argument comes down to more accurate action prediction (ok) and more interpretability (maybe) which is less compelling. Clarifying Questions - How does the 4 player Stag Hunt work? Do all 4 agents have to step on the Stag together or just 2 of them? How are rewards distributed? Is there a negative payoff for Hunting the stag alone as in the Peysakhovich & Lerer paper? - Related: In the Stag Hunt there are multiple equilibria, either agents learn to get plants (which is safe but low payoff) or they learn to Hunt (which is risky but high payoff). Is the RFM leading to more convergence to the Hunting state or is it simple leading agents to learn the safe but low payoff strategies faster? - The choice of metric in Figure 2 (# exactly correct prediction) is non-standard (not saying it is wrong). I think it would be good to also see a plot of a more standard metric such as loglikelihood of the model's for each of X possible steps ahead. It would help to clarify where the RFM is doing better (is it better at any horizon or is it just able to look further forward more accurately than the competitors?) ", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your insightful questions and suggestions on the submission . We hope we have addressed your questions below . In particular , we have addressed your two major weak points : the graphical structure of the embedded RFM is indeed helpful for speeding up learning ; and the type and degree of interpretability differs substantially from previous work . We hope you will revise your rating accordingly . 1 ) Can we augment agents with models other than the RFM ( e.g.MLP + LSTM ) ? Yes ! We have performed your suggested experiments , which we show in Figure 7 . Indeed , the RFM-augmented agents outperform the MLP+LSTM-augmented agents ( i.e.those with forward models that are non-relational ) . 2 ) How does interpretability compare with ToMNet ( Rabinowitz et al , 2018 ) ? This is a good question . To begin , interpretability is not a binary property of a model ; there are a number of advantages that RFMs offer over the ToMNet construction in Rabinowitz et al , 2018 . Most importantly , the ToMNet embeds sequences of behavior as points in an unstructured , high-dimensional Euclidean space , relying on the inherent structure of the data ( and optionally an information bottleneck ) to yield interpretable representations . In contrast , RFMs shape behavioral representations through the structure of entities and relations ( via the graph net ) . These representations are very natural for humans to interface with , as they conform to representation schemas of human cognition ( Spelke & Kinzler , 2007 ) . Moreover , the representations of the RFM also allow us to easily ask directed questions about how different entities influence agent behavior ( e.g.Figure 3 ) , which is not something that the ToMnet enables . We also note that the ToMNet was designed as a single-agent forward model , while RFMs naturally scale to the multi-agent setting . This allows us to augment agents with the RFM module , which is not something pursued in this previous work . 3 ) Clarifications about Stag Hunt 4 players - do all 4 need to step on the stag to capture it ? No , only 2 Is there a negative reward for hunting alone ? No Does including the RFM leads to more Stags being captured ? We have only made qualitative observations , but anecdotally , the RFM-augmented agents go for stags like maniacs . 4 ) Choice of metric in Figure 2 . We have now provided an alternative metric in Figure 10 ( next-step action classification accuracy ) which shows the same qualitative results . The reason we use the particular metric in the main text is that is gives us a measure of how long the model remains useful . In particular , we are learning a simulator of the agents dynamics ; the metric gives an indication of how many steps one can simulate before making a mistake . Alternative rollout metrics are hard to define beyond the first mistake when the true environment and the predictions have diverged . There most likely isn \u2019 t a perfect metric that covers all bases , and in particular alternative rollout metrics are hard to define after the model makes a mistake , since the ground-truth observations and predictions no longer match . Nonetheless , between this and the new Figure 10 we believe there \u2019 s a strong case that the RFM-based model is better . References : Spelke & Kinzler . ( 2007 ) .Core knowledge . Developmental science , 10 ( 1 ) , 89-96 ."}, "2": {"review_id": "rJlEojAqFm-2", "review_text": "This paper studies predicting multi-agent behavior using a proposed neural network architecture. The architecture, called a relational forward model (RFM) is the same graph network proposed by Battaglia et al., 2018, but adds a recurrent component. Two tasks are define: predict the next action of each agent, and predict the sum of future rewards. The paper demonstrates that RFMs outperform two baselines and two ablations. The authors also show that edge activation magnitudes are correlated with certain phenomenons (e.g. an agent walking towards an entity, or an entity being \u201con\u201d or \u201coff\u201d). The authors also show that appending the output of a pre-trained RFM to the state of a policy can help it learn faster. Overall, this paper presents some interesting ideas and is easy to follow, but the significance of the paper is not clear. The architecture is a rather straightforward extensions of previous work, and using graph networks for predictive modeling in multi-agent settings has been examined in the past, making the technical contributions not particularly novel. Examining the correlation between edge activation magnitudes and certain events is intriguing and perhaps the most novel aspect of this paper, but it is not clear how or why this information would be useful. There a few unsubstantiated claims that are concerning. There are also some odd experimental decisions and results that should be addressed. For specific comments: 1. Why would using a recurrent network help (i.e. RFM vs Feedforward)? Unless the policies are non-Markovian, the entire prediction problem should Markovian. I suspect that most of the gains are coming from the fact that the RFM method simply has more parameters than the Feedforward method (e.g. it can amortize some of the computation into the recurrent part of the network). Suggestion: train a Feedforward model that has more parameters (with appropriate hyperparameter sweeps) to see if this is the cause. If not, provide some analysis for why \u201cmemories of the relations between entities\u201d would be any more beneficial than simply recomputing those relations. 2. The other potential reason that the recurrent method did better is that policy actions are highly correlated (e.g. because agents move in straight lines to locations). If so, then recurrent methods can outperform feedforward methods without having to learn anything about what actually causes policies to move in certain directions. Suggestion: measure the correlation between consecutive actions. If there is non-trivial correlation, than this suggests that RFM does better than Feedforward (which is basically prior work of Battaglia et. al.) is for the wrong reasons. 3. If I understand the evaluation metric correctly, for each rollout, it counts how many steps from the beginning of the rollout match perfectly before the first error occurs. Then it averages this \u201cminimum time to failure\u201d across all evaluation rollouts. If this is correct, why was this evaluation metric chosen? A much more natural metric would be to just compute the average number of errors on a test data-set (and if this is what is actually reported, please update the description to disambiguate the two). The current metric could be very deceptive: Methods that do very well on states around the initial-state distribution but poorly near the end of trajectories (e.g. perfectly predicts the actions in the first 10 steps, but then resorts to random guessing for the last 99999 time steps) will outperform methods that have lower average error rate (e.g. a model that is correct 50% of the time). Suggestion: change the metrics to average number of errors, or report both, or provide a convincing argument why this metric is meaningful. 4. Unless I misunderstood, the results in Section 2.2.3 seem spurious and the claims seem unsubstantiated. For one, if we look at Equations (1) and (2), when we average over s_a1 and s_a2, they should both give the same average for R_a1. Put another way: the prune graph should (in theory) marginalize out s_a2. On average, its expected output should be the same as the output of the full graph (after marginalizing out s_a1 and s_a2). Obviously, it is possible to find specific rollouts where the full graph has higher value than the prune graph (and it seems Figure 4 does this), but it should equally be possible to find rollouts where the opposite is true. I\u2019m hoping I misunderstood this section, but otherwise this seems to invalidate all the claims made in this section. 5. Even if concern #4 is addressed, the following sentence would still seem false: \u201cThis figure shows that teammates\u2019 influence on each other during this time is beneficial to their return.\u201d The figure simply shows predictions of the RFM, and not of the ground truth. Moreover, it\u2019s not clear what \u201cteammates\u2019 influence\u201d actually means. 6. The comparison to NRI seems rather odd, since that method uses strictly less information than RFM. 7. For Section 3, is the RFM module pretrained and then fine-tuned with the new policy? If so, this gives the \u201cRFM + A2C\u201d agent extra information indirectly via the pretrained weights of the RFM module. 8. I\u2019m not sure what to make of the correlation analysis. It is not too surprising that there is some correlation (in fact, it\u2019d be quite an interesting paper if the findings were that there wasn\u2019t a correlation!), and it\u2019s not clear to me how this could be used for debugging, visualizations, etc. If someone wanted to analyze the correlation between two entities and a policy\u2019s action, it seems like they could directly model this correlation. Some minor comments: - In Figure 3C, right, why isn\u2019t the magnitude 0 at time=1? Based on the other plots in Figure 3c, it seems like it should be 0. - The month/year in many of the citations seems odd. - The use of the word \u201cvalence\u201d seems unnecessarily flowery and distracting. My main concern with this paper is that it is not particularly novel and the contribution seems questionable. I have some concerns over the experimental metric and Section 2.2.3, but even if that is clarified, it is not clear how impactful this paper would be. The use of a recurrent network seems unnecessary, unjustified, and not analyzed. The analysis of correlations is interesting, but not particularly compelling or surprising. And lastly, the RFM-augmented results are not very strong. -- Edit: After discussing with the authors, I have changed my rating. The authors have adjusted some of the language, which I previously thought overstated the contributions and was misleading. They have added a number of experiments which valid the claim that their method is proposing a reasonable way of measuring collaboration. I also realized that I misunderstood one of the sections, and I encourage the authors to improve the presentation to (1) present the significance of the experiments more clear, (2) not overstate the results, and (3) emphasize the contribution more clearly. Overall, the paper presents convincing evidence that factors in a graph neural networks do capture some notion of collaboration. I do not feel that the paper is particularly novel, but the experiments are thorough. Furthermore, their experiments show that adding an RFM module to an agent consistently helps (albeit not by much). Given that the multi-agent community is still trying to decide how to best quantify and use metrics for collaboration, I find it difficult to access the long-term impact of this paper. However, given the thoroughness of the experiments and analysis, I suspect that this will be valuable for the community and deserves some visibility.", "rating": "7: Good paper, accept", "reply_text": "5 ) Even if concern # 4 is addressed , the following sentence would still seem false : \u201c This figure shows that teammates \u2019 influence on each other during this time is beneficial to their return. \u201d The figure simply shows predictions of the RFM , and not of the ground truth . Moreover , it \u2019 s not clear what \u201c teammates \u2019 influence \u201d actually means . Good catch . We can not actually compute the ground truth . We have rephrased this to : \u201c Thus the model estimates that teammates ' specific interactions during this time are beneficial to their return. \u201d 6 ) The comparison to NRI seems rather odd , since that method uses strictly less information than RFM . NRI has access to the same set of information as the RFM . In comparison however , NRI actively discards information : it infers a connectivity map from the past trajectory , and may choose to do inference using any number of edges . This might actually confer advantages if the ground-truth process is indeed sparse and relatively stationary . Overall , NRI is a good method and it has many advantages . Our results nevertheless demonstrate that these design decisions are less well-suited to the patterns of multi-agent interaction in these environments . 7 ) For Section 3 , is the RFM module pretrained and then fine-tuned with the new policy ? If so , this gives the \u201c RFM + A2C \u201d agent extra information indirectly via the pretrained weights of the RFM module . The on-board RFM is trained * from scratch * alongside the policy network . There is no pre-training . 8 ) I \u2019 m not sure what to make of the correlation analysis . It is not too surprising that there is some correlation ( in fact , it \u2019 d be quite an interesting paper if the findings were that there wasn \u2019 t a correlation ! ) , and it \u2019 s not clear to me how this could be used for debugging , visualizations , etc . If someone wanted to analyze the correlation between two entities and a policy \u2019 s action , it seems like they could directly model this correlation . Measuring the emergence of coordinated behavior in multi-agent systems is an important open problem in this field ( see also AnonReviewer3 \u2019 s comment to this effect ) . Especially in the case of * learning * systems , assessing whether or not agents are able to coordinate , what drives their behavior , whether they learn to help or hinder one another and how they modify their behavior in response to changes in the environment are all crucial aspects of multi-agent analysis that we struggle to quantify . Here we show that learning relational models of multi-agent systems might be a good place to look . With respect to the particular suggestion that one could directly model correlations between entities and a policy \u2019 s actions , we wish it were that simple ! The influence of one agent \u2019 s state on another \u2019 s behavior can be highly contextual , so one would need to factor in the state to tease apart the appropriate effect . This amounts to fitting a parameterized model of the interaction , which is precisely what we \u2019 re doing here . 9 ) Some minor comments a . In Figure 3C , right , why isn \u2019 t the magnitude 0 at time=1 ? Based on the other plots in Figure 3c , it seems like it should be 0 . Agents tend to rapidly move away from fruit or stags they just consumed . The probability of respawning is small ( 0.05 ) so there will be nothing there for some time ( on average ) . This means that there is some information that a recently-consumed entity can provide to the agent that just consumed it : when the entity is adjacent to the agent , its chances of respawning are lower than otherwise . This predictive information drops sharply after the first step as agents will move towards available entities ( which might be anywhere ) rather than directly away from unavailable ones ( see also Figure 3 top row the two right panels ) . b.The month/year in many of the citations seems odd . We have fixed this . c. The use of the word \u201c valence \u201d seems unnecessarily flowery and distracting . \u201c Valence \u201d is a standard technical term from psychology , denoting the attractiveness or aversiveness of a stimulus ( e.g.Frijda , 1986 ) . References Frijda , N. H. ( 1986 ) . The emotions . Cambridge University Press ."}, "3": {"review_id": "rJlEojAqFm-3", "review_text": "This paper proposes to use graph neural networks in the scenario of multi-agent reinforcement learning (MARL). It tackles two current challenges, learning coordinated behaviours and measuring such coordination. At the core of the approach are graph neural networks (a cite to Scarselli 2009 would be reasonable): acting and non-acting entities are represented by a graph (with (binary) edges between acting-acting and acting-nonacting entities) and the graph network produces a graph where these edges are transformed into a vectorial representation, which then can be used by a downstream task, e.g. a policy algorithm (as in this paper) that uses it to coordinate behavour. Because the output of the graph network is a structurally identical graph to the input, it is possible to interpret this output. The paper is well written, the main ideas are clearly described. I'm uncertain about the novelty of the approach, at least the way the RFM is utilized in the policy is a nice idea (albeit, a-posteriori, sounds straight forward in the context of MARL). Similarly, using the graph output for interpretations is an obvious choice). Nevertheless, showing empirically that the ideas actually work gives the paper a lot of credibility for being a stepping stone in the area of MARL.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . We take it as a good sign that you see the ideas as obvious a posteriori . To the best of our knowledge , though , no one has actually explored them . When others have modelled the dynamics of multi-agent systems ( e.g.NRI , VAIN , ToMnet ) , they have not attempted to integrate these models into the agents themselves . Conversely , in papers that do put models of opponents in MARL , these models are not relational , and they model goals in agents identical to oneself ( e.g.Raileanu et al.2018 ) , policy representations or Q-values ( He et al.2016 ) , but not future actions . In a similar manner , no one has developed this method to interpret multi-agent behavior to the extent that we do here . The event-based analysis and value-based analyses are novel applications of this framework . Thank you for pointing out that we were missing a citation to Scarselli , we included it the revised manuscript ."}}