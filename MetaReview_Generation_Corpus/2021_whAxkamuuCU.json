{"year": "2021", "forum": "whAxkamuuCU", "title": "Symbol-Shift Equivariant Neural Networks", "decision": "Reject", "meta_review": "This paper proposed a  new type of models that are invariant to entities by exploring the symbolic property of entities. This problem is important in language modeling since it gives intrinsically more proper representation of sentences, which can better generalize to new entities.  However I still suggest to reject this paper for the following reasons \n1. The description of model is not clear enough which can certainly use a serious round of revision.\n2. The experiments on bAbi is not convincing enough since it is an overly simple and toyish data-set with many ways to hack\n3. Similar entity-invariant idea has been explored long time ago by  (https://arxiv.org/pdf/1508.05508.pdf) which attempted to represent entities as \u201cvariables\u201d\n\n", "reviews": [{"review_id": "whAxkamuuCU-0", "review_text": "* * Summary * * . This paper proposes a new type of models that are equivariant to entity permutations , which is an important criterion to build language models that can easily generalize to new entities . The authors modified a Memory-Network and a Third-order tensor product RNN to make them symbolic-shit invariant . The new models were evaluated and compared on the 20 bAbi tasks . Results show that the symbolic versions of the models yield better performance than the original ones . * * Positives * * . The topic is of great interest and it is indeed crutial that neural language models become symbol-shift invariant to allow them to better generalize . This work is clearly motivated . * * Confusions * * . The beginning of Section4 mentions that the main idea of this work is to concatenate a regular `` semantic '' word vector with a `` symbolic '' representation essentially corresponding to a one-hot vector of the token order of appearance . In the following paragraphs , the work presented lacks clarity and seems to over-complicate concepts with hard-to-follow math notations . For instance , the \u201c * Mapping words into and from symbolic representations * \u201d paragraph introduces tedious math notations to describes something simple that was clear before , namely , the mapping from tokens to their respective symbolic vector , which is simply defined as the one-hot vector position appearance of this token in the context . Similarly , the `` * Hybrid semantic-symbolic embeddings * '' paragraph uses again tedious math notations to describe how semantic and symbolic embedding are concatenated . Given the confusion presented in Section4 , it is currently not clear how adding a one-hot vector to the input embedding can make a neural model symbol-shift equivariant . In particular , below are the two things I could not understand : 1 ) The paper mentions that `` * all parameters are differentiable * '' . It is not clear if that also includes the symbolic representation or not ? If so , then the initial one-hot vector may not be a one-hot vector after the gradient updates performed during training , which would result in a non-symbolic representation ? if it is kept fix during training , then it is not clear how it is used by the network . 2 ) In addition , assuming that the symbolic representation of all tokens stays the same during training , I do n't see how `` _permuted symbols share the same latent representations_ '' if the latent representations are made of both on-hot vectors * * and * * regular word vectors . I understand that the symbolic representation does not change for a permuted word since it will appear at the same place as the original word . But the semantic representation will be different . For instance , the semantic word vector of \u201c banana \u201d is similar but still different than the word vector of \u201c apple \u201d . * * Conclusion * * . I would suggest the authors to simplify their mathematical notation and make their paper easier to read . As of now , I could not fully understand the paper and unfortunately for that reason could only put a score of 4 with a low confidence of 2 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . To answer your questions : 1 . The only free-parameter of the symbolic representation $ \\alpha_x e_\\varphi ( x ) $ is $ \\alpha_x \\in [ 0 , 1 ] $ given that $ e_\\varphi ( x ) \\in \\mathbb { R } ^m $ is set to the one-hot vector of the order of appearance of $ x $ . With the parameter $ \\alpha_x $ , the model can learn if a word $ x $ should behave as a symbol or not . 2.In our definition of symbol-shift , permutation of symbols must happen without changing semantic embeddings so that \u201c banana \u201d and \u201c apple \u201d can be changed only if they share the same semantic word vector . This is a theoretical requirement which allows us to prove formally that the models we introduce are equivariant under symbol shift . However , in practice as you mention we can not expect the semantic vector of two entities of the same group to be exactly the same , they would be close but different . Our claim is that the added inductive bias helps in practice , even if the theoretical requirement does not hold , which we aimed to demonstrate in our experiments as the models can generalize to a larger number of entities ( for instance in the experiment of Fig 1 , the semantic vectors of all entities are close but different ) , to unseen one and also converges faster than models that do not have the symbolic representation we propose . About your other points on notations , we will do our best to simplify them and we are grateful for your feedback . We agree that the symbolic vector can be described in simple words , however it is a bit more difficult to express the projection of a symbolic vector to the vocabulary in words without equations . Please let us know of any other questions you may have ."}, {"review_id": "whAxkamuuCU-1", "review_text": "This work proposes to improve the generalizability of bAbi models through [ entity permutations ] . More specifically the approach assumes domain knowledge of word/entity type equivalences , which helps restricting possible permutations between word POS ( e.g. , \u201c John \u201d vs \u201c why \u201d ) or gender ( e.g. , \u201c John \u201d vs \u201c Mary \u201d ) . Each word type has its own embedding param and is concatenated with normal word embeddings to form the final word representation . Experiment with memory networks and Third-order Tensor Product RNN shows that the proposed approach indeed enables the models ( especially TPR ) to handle artificial data sets with large number of entity names . Overall I find the proposed research not very well motivated . Leveraging word type knowledge to improve the generalizability of NLP models has been a popular and effective approach . Commonly used strategy is to replace named entities in sentences with their word type tokens . e.g. , from [ how old is Obama ] to [ how old is PERSON ] https : //arxiv.org/abs/1601.01280 https : //arxiv.org/abs/1611.00020 The proposed approach seems to achieve a similar effect , but is a lot more complex .", "rating": "3: Clear rejection", "reply_text": "Thank you for your review . There seems to be an important misunderstanding that we would like to clarify . We do not assume that we have \u201c domain knowledge of word/entity type equivalences \u201d . This is in fact the main motivation and contribution of this paper : having compositionality and symbol abstraction * * without * * having to specify/detect entities in advance . We hope you can adapt your review as the point you raised is the main motivation and contribution of the paper ( as stated in the abstract \u201c we define a class of models whose outputs are equivariant to entity permutations * without requiring to specify or detect entities * in a pre-processing step \u201d or in the introduction \u201c The main advantage and novelty of our approach is that * entities are not required to be identified in advance * as we rely solely on differentiation to determine whether a word acts like an entity \u201d ) . In case there is one sentence that is misleading and indicates that we are assuming entities equivalences are given , we would really appreciate it if you could point us to it . We can add the references you mentioned but we would like to point-out that we already highlighted the fact that replacing entities by token helps compositionality in our related work section ( \u201c compositionality becomes much easier if symbols ( or entities ) are detected before-hand . For instance , [ Li2015 ] showed that replacing entities by dedicated token placeholders leads to significant improvement in question answering. \u201d ) ."}, {"review_id": "whAxkamuuCU-2", "review_text": "The authors propose a network that is equivariant to entity permutations without requiring the pre-specification of the set of entities . To this end , the authors propose a hybrid semantic-symbolic embedding which they integrate into two QA models . Finally , the authors show significant gains on the bAbi tasks , with especially impressive gains in the 1K setting . The problem is quite interesting and challenging in the setting where entities are not prespecified . However , given the model description it is not clear at all how the model is able to learn a symbol-shift equivariant embedding . I do n't understand how the model is able to determine that `` apple '' and `` orange '' have the same embedding while `` apple '' and `` John '' have different embeddings . What is the loss/model architecture/data augmentation guiding this ? How is the model able to figure out that `` John '' and `` Sasha '' share embedding ? Apart from the high level details , I do n't understand the following notations and operations : * In Section 4 , what is $ n $ ? Is it total number of words in the sequence ? * If $ B_\\varphi \\in R^ { m \\times n } $ and $ e^m_j \\in R^m $ , the multiplication $ B_\\varphi e^m_j $ does n't make sense . * How exactly is $ \\alpha_x e_ { \\varphi ( x ) } \\in R^m $ ? What exactly is $ \\alpha_x $ and what is it 's shape ? The notation and the working of the model is not clear to me , hence , I am giving a low rating for now . Apart from this I also doubt the proposed method 's generalizability beyond toy settings .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review . \u201c I do n't understand how the model is able to determine that `` apple '' and `` orange '' have the same embedding while `` apple '' and `` John '' have different embeddings. \u201d The model learns the semantic embeddings in the same way as a standard model ( in fact , if $ \\alpha_x = 0 $ for all word $ x $ , the model will reduce to a \u201c semantic \u201d model ) . Thus the model is able to learn similar embeddings to \u201c orange \u201d and \u201c apple \u201d that would be close but different ( for instance if semantic embedding encodes color ) the same way than any word embedding model relying on differentiation . While we assume that vectors of entities in the same group are equal , this is only required to prove formally that models are equivariant . In practice as noted by R2 , the semantic vectors are only * close * but not equal between entities of the same group . However , our claim is that this additional inductive bias still allows the model to generalize as seen in our experiments where \u201c symbolic \u201d models are able to learn with large number of entities or unseen ones even if the condition of the theorem does not strictly apply . In regards to your specific points : * $ n $ is the number of words in the vocabulary , it is introduced in Section 3 paragraph 2 , we will recall its definition in Section 4 to ease readability . * We did a typo when indicating the dimension of $ B_\\varphi $ which is $ B_\\varphi \\in R^ { n \\times m } $ * $ \\alpha_x $ is the $ x $ -th component of $ \\alpha \\in [ 0 , 1 ] ^n $ and hence $ \\alpha_x \\in \\mathbb { R } $ . $ e_ { \\varphi ( x ) } \\in R^m $ as it is the one-hot vector of the order of appearance of $ x $ in the context ( which has $ m $ words ) . We hope this clarifies the points raised , please let us know of other questions you may have ."}], "0": {"review_id": "whAxkamuuCU-0", "review_text": "* * Summary * * . This paper proposes a new type of models that are equivariant to entity permutations , which is an important criterion to build language models that can easily generalize to new entities . The authors modified a Memory-Network and a Third-order tensor product RNN to make them symbolic-shit invariant . The new models were evaluated and compared on the 20 bAbi tasks . Results show that the symbolic versions of the models yield better performance than the original ones . * * Positives * * . The topic is of great interest and it is indeed crutial that neural language models become symbol-shift invariant to allow them to better generalize . This work is clearly motivated . * * Confusions * * . The beginning of Section4 mentions that the main idea of this work is to concatenate a regular `` semantic '' word vector with a `` symbolic '' representation essentially corresponding to a one-hot vector of the token order of appearance . In the following paragraphs , the work presented lacks clarity and seems to over-complicate concepts with hard-to-follow math notations . For instance , the \u201c * Mapping words into and from symbolic representations * \u201d paragraph introduces tedious math notations to describes something simple that was clear before , namely , the mapping from tokens to their respective symbolic vector , which is simply defined as the one-hot vector position appearance of this token in the context . Similarly , the `` * Hybrid semantic-symbolic embeddings * '' paragraph uses again tedious math notations to describe how semantic and symbolic embedding are concatenated . Given the confusion presented in Section4 , it is currently not clear how adding a one-hot vector to the input embedding can make a neural model symbol-shift equivariant . In particular , below are the two things I could not understand : 1 ) The paper mentions that `` * all parameters are differentiable * '' . It is not clear if that also includes the symbolic representation or not ? If so , then the initial one-hot vector may not be a one-hot vector after the gradient updates performed during training , which would result in a non-symbolic representation ? if it is kept fix during training , then it is not clear how it is used by the network . 2 ) In addition , assuming that the symbolic representation of all tokens stays the same during training , I do n't see how `` _permuted symbols share the same latent representations_ '' if the latent representations are made of both on-hot vectors * * and * * regular word vectors . I understand that the symbolic representation does not change for a permuted word since it will appear at the same place as the original word . But the semantic representation will be different . For instance , the semantic word vector of \u201c banana \u201d is similar but still different than the word vector of \u201c apple \u201d . * * Conclusion * * . I would suggest the authors to simplify their mathematical notation and make their paper easier to read . As of now , I could not fully understand the paper and unfortunately for that reason could only put a score of 4 with a low confidence of 2 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your review . To answer your questions : 1 . The only free-parameter of the symbolic representation $ \\alpha_x e_\\varphi ( x ) $ is $ \\alpha_x \\in [ 0 , 1 ] $ given that $ e_\\varphi ( x ) \\in \\mathbb { R } ^m $ is set to the one-hot vector of the order of appearance of $ x $ . With the parameter $ \\alpha_x $ , the model can learn if a word $ x $ should behave as a symbol or not . 2.In our definition of symbol-shift , permutation of symbols must happen without changing semantic embeddings so that \u201c banana \u201d and \u201c apple \u201d can be changed only if they share the same semantic word vector . This is a theoretical requirement which allows us to prove formally that the models we introduce are equivariant under symbol shift . However , in practice as you mention we can not expect the semantic vector of two entities of the same group to be exactly the same , they would be close but different . Our claim is that the added inductive bias helps in practice , even if the theoretical requirement does not hold , which we aimed to demonstrate in our experiments as the models can generalize to a larger number of entities ( for instance in the experiment of Fig 1 , the semantic vectors of all entities are close but different ) , to unseen one and also converges faster than models that do not have the symbolic representation we propose . About your other points on notations , we will do our best to simplify them and we are grateful for your feedback . We agree that the symbolic vector can be described in simple words , however it is a bit more difficult to express the projection of a symbolic vector to the vocabulary in words without equations . Please let us know of any other questions you may have ."}, "1": {"review_id": "whAxkamuuCU-1", "review_text": "This work proposes to improve the generalizability of bAbi models through [ entity permutations ] . More specifically the approach assumes domain knowledge of word/entity type equivalences , which helps restricting possible permutations between word POS ( e.g. , \u201c John \u201d vs \u201c why \u201d ) or gender ( e.g. , \u201c John \u201d vs \u201c Mary \u201d ) . Each word type has its own embedding param and is concatenated with normal word embeddings to form the final word representation . Experiment with memory networks and Third-order Tensor Product RNN shows that the proposed approach indeed enables the models ( especially TPR ) to handle artificial data sets with large number of entity names . Overall I find the proposed research not very well motivated . Leveraging word type knowledge to improve the generalizability of NLP models has been a popular and effective approach . Commonly used strategy is to replace named entities in sentences with their word type tokens . e.g. , from [ how old is Obama ] to [ how old is PERSON ] https : //arxiv.org/abs/1601.01280 https : //arxiv.org/abs/1611.00020 The proposed approach seems to achieve a similar effect , but is a lot more complex .", "rating": "3: Clear rejection", "reply_text": "Thank you for your review . There seems to be an important misunderstanding that we would like to clarify . We do not assume that we have \u201c domain knowledge of word/entity type equivalences \u201d . This is in fact the main motivation and contribution of this paper : having compositionality and symbol abstraction * * without * * having to specify/detect entities in advance . We hope you can adapt your review as the point you raised is the main motivation and contribution of the paper ( as stated in the abstract \u201c we define a class of models whose outputs are equivariant to entity permutations * without requiring to specify or detect entities * in a pre-processing step \u201d or in the introduction \u201c The main advantage and novelty of our approach is that * entities are not required to be identified in advance * as we rely solely on differentiation to determine whether a word acts like an entity \u201d ) . In case there is one sentence that is misleading and indicates that we are assuming entities equivalences are given , we would really appreciate it if you could point us to it . We can add the references you mentioned but we would like to point-out that we already highlighted the fact that replacing entities by token helps compositionality in our related work section ( \u201c compositionality becomes much easier if symbols ( or entities ) are detected before-hand . For instance , [ Li2015 ] showed that replacing entities by dedicated token placeholders leads to significant improvement in question answering. \u201d ) ."}, "2": {"review_id": "whAxkamuuCU-2", "review_text": "The authors propose a network that is equivariant to entity permutations without requiring the pre-specification of the set of entities . To this end , the authors propose a hybrid semantic-symbolic embedding which they integrate into two QA models . Finally , the authors show significant gains on the bAbi tasks , with especially impressive gains in the 1K setting . The problem is quite interesting and challenging in the setting where entities are not prespecified . However , given the model description it is not clear at all how the model is able to learn a symbol-shift equivariant embedding . I do n't understand how the model is able to determine that `` apple '' and `` orange '' have the same embedding while `` apple '' and `` John '' have different embeddings . What is the loss/model architecture/data augmentation guiding this ? How is the model able to figure out that `` John '' and `` Sasha '' share embedding ? Apart from the high level details , I do n't understand the following notations and operations : * In Section 4 , what is $ n $ ? Is it total number of words in the sequence ? * If $ B_\\varphi \\in R^ { m \\times n } $ and $ e^m_j \\in R^m $ , the multiplication $ B_\\varphi e^m_j $ does n't make sense . * How exactly is $ \\alpha_x e_ { \\varphi ( x ) } \\in R^m $ ? What exactly is $ \\alpha_x $ and what is it 's shape ? The notation and the working of the model is not clear to me , hence , I am giving a low rating for now . Apart from this I also doubt the proposed method 's generalizability beyond toy settings .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for the review . \u201c I do n't understand how the model is able to determine that `` apple '' and `` orange '' have the same embedding while `` apple '' and `` John '' have different embeddings. \u201d The model learns the semantic embeddings in the same way as a standard model ( in fact , if $ \\alpha_x = 0 $ for all word $ x $ , the model will reduce to a \u201c semantic \u201d model ) . Thus the model is able to learn similar embeddings to \u201c orange \u201d and \u201c apple \u201d that would be close but different ( for instance if semantic embedding encodes color ) the same way than any word embedding model relying on differentiation . While we assume that vectors of entities in the same group are equal , this is only required to prove formally that models are equivariant . In practice as noted by R2 , the semantic vectors are only * close * but not equal between entities of the same group . However , our claim is that this additional inductive bias still allows the model to generalize as seen in our experiments where \u201c symbolic \u201d models are able to learn with large number of entities or unseen ones even if the condition of the theorem does not strictly apply . In regards to your specific points : * $ n $ is the number of words in the vocabulary , it is introduced in Section 3 paragraph 2 , we will recall its definition in Section 4 to ease readability . * We did a typo when indicating the dimension of $ B_\\varphi $ which is $ B_\\varphi \\in R^ { n \\times m } $ * $ \\alpha_x $ is the $ x $ -th component of $ \\alpha \\in [ 0 , 1 ] ^n $ and hence $ \\alpha_x \\in \\mathbb { R } $ . $ e_ { \\varphi ( x ) } \\in R^m $ as it is the one-hot vector of the order of appearance of $ x $ in the context ( which has $ m $ words ) . We hope this clarifies the points raised , please let us know of other questions you may have ."}}