{"year": "2019", "forum": "HkfYOoCcYX", "title": "Double Viterbi: Weight Encoding for High Compression Ratio and Fast On-Chip Reconstruction for Deep Neural Network", "decision": "Accept (Poster)", "meta_review": "The authors propose an efficient scheme for encoding sparse matrices which allow weights to be compressed efficiently. At the same time, the proposed scheme allows for fast parallelizable decompression into a dense matrix using Viterbi-based pruning. \nThe reviewers noted that the techniques address an important problem relevant to deploying neural networks on resource-constrained platforms, and although the work builds on previous work, it is important from a practical standpoint. \nThe reviewers noted a number of concerns on the initial draft of this work related to the experimental methodology and the absence of runtime comparison against the baseline, which the reviewers have since fixed in the revised draft. The reviewers were unanimous in recommending that the revision be accepted, and the authors are requested to incorporate the final changes that they said they would make in the camera-ready version.\n", "reviews": [{"review_id": "HkfYOoCcYX-0", "review_text": "This paper presents a new way to represent a dense matrix in a compact format. First, the method prunes a dense matrix based on the Viterbi-based pruning. Then, the pruned matrix is quantized with alternating multi-bit quantization. Finally, the binary vectors produced by the quantization algorithm are further compressed with the Viterbi-based algorithm. It spots the problem of each existing approach and solve the problems by combining each method. The combination is new and the result is encouraging. I find this paper is interesting and I like the strong results. It is an interesting combination of methods. However, the experiments are not enough to show that the proposed method is really needed to achieve the results. If these are answered well, I'd be happy to change my evaluation. 1. The method should be compared with other combinations of components. At least, it should be compared with \"Multi-bit quantization only (Xu et al., 2018)\" and \"Multi-bit-quantization + Viterbi-based binary code encoding\". 2. The experiments with \"Don't Care\" should go to the experiment section, and the end-to-end results should be present but not the ratio of incorrect bits. 3. Similarly, the paper will become stronger if it has some experimental results that compare quantization methods. In Section 3.3., it mentions that the conventional k-bit quantization was tried and significant accuracy drops were observed. I feel that this is a kind of things which support the proposed method if it is properly assessed. 4. When you say \"slow\" form something and propose a method to address it, I'd like to see some benchmark numbers. There is an experiment with simulation, but that does not seem to simulate the slow \"sequential sparse matrix decoding process\". Minor comments: * It was a bit hard to understand how a matrix is processed through the flowchart in Fig. 1 at first glance. It would help readers to understand it better if it has a corresponding figure which shows how a matrix is processed through the flowchart.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for the constructive comments . We tried to strengthen our claims by adding more experimental data which the Reviewer requested . 1.The proposed `` Multi-bit-quantization + Viterbi-based binary code encoding '' requires slightly larger memory footprint than `` Multi-bit quantization only ( [ 4 ] ) '' because some of the Viterbi encoded bits have different indices from their corresponding quantization bits . Hence , the `` Multi-bit quantization only '' requires 10 % to 20 % smaller memory footprint than `` Multi-bit-quantization + Viterbi-based binary code encoding '' case . However , the main reason why we apply the Viterbi weight encoding is that parallel sparse-to-dense matrix conversion can be done by applying same Viterbi encoding process to the non-zero values and indices of the non-zero values in parallel . This parallel sparse-to-dense conversion makes the speed of feeding parameters to PEs 10 % to 40 % faster compared to [ 1 ] ( Figure 6c ) . 2.Per Reviewer \u2019 s suggestion , the experimental results for the effectiveness of `` Don \u2019 t Care '' term have been moved to Section 4.1 . 3.Per Reviewer 's suggestion , we measured accuracy differences before and after Viterbi encoding for several quantization methods such as linear quantization ( [ 2 ] ) , logarithmic quantization ( [ 3 ] ) , and alternating quantization ( [ 4 ] ) methods with the same quantization bits ( 3-bit ) . The result shows that combination with alternating quantization and Viterbi weight encoding had only 2 % validation accuracy degradation after the Viterbi encoding was applied first right after the quantization and the accuracy was easily recovered with retraining . On the other hand , the combination with the other quantization methods and Viterbi weight encoding showed accuracy degradation as much as 71 % , which was too large to recover the accuracy with retraining . The accuracy difference mainly results from the uneven weight distribution . Because weights of neural networks usually are normally distributed , the composition ratio of ' 0 ' and ' 1 ' is not equal when the linear or logarithmic quantization is applied to the weights of neural networks . As we stated in the manuscript , Viterbi encoder tends to produce similar number of ' 0 ' and ' 1 ' . Therefore , we can conclude that under the same bit condition , alternating quantization method shows best accuracy and compatibility with our bit-by-bit Viterbi encoding scheme regardless of the type of neural networks . 4.We conducted additional simulations to compare sparse matrix reconstruction speed of [ 1 ] and the proposed method . We used a random 512-by-512 size matrix with various pruning rate ranging from 75 % to 95 % . We conducted the simulations under the assumptions described in Figure 6c . The simulation results are shown in Figure 6c in updated manuscript . We could observe that the proposed method could feed 10 % to 40 % more nonzero weights and input activations to PEs in same 10000 cycles compared to [ 1 ] . Proposed method could also feed parameters to PEs 20 % to 106 % faster compared to baseline method , which reads dense weight and activation matrices directly from DRAM . The improvement in the proposed scheme mainly comes from the parallelized process of assigning non-zero values to their corresponding indices in the weight matrix . While preparing addition data for the rebuttal , we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method . After further optimization , we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript . Therefore , we updated Figure 7 in original manuscript to Figure 6c in updated manuscript according to the new data . 5.We added the change of the exact weight representation at each process in Figure 1 to clarify the flowchart . Reference [ 1 ] Dongsoo Lee , Daehyun Ahn , Taesu Kim , Pierce I. Chuang , and Jae-Joon Kim . Viterbi-based pruning for sparse matrix with fixed and high index compression ratio . International Conference on Learning Representations ( ICLR ) , 2018 . [ 2 ] Darryl D. Lin , Sachin S. Talathi , and V. Sreekanth Annapureddy . Fixed point quantization of deep convolutional networks . In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48 , ICML \u2019 16 , pp . 2849\u20132858 . 2016 . [ 3 ] Daisuke Miyashita , Edward H. Lee , and Boris Murmann . Convolutional Neural Networks using Logarithmic Data Representation . CoRR , abs/1603.01025 , 2016 . URL https : //arxiv.org/abs/1603.01025 . [ 4 ] Chen Xu , Jianqiang Yao , Zouchen Lin , Wenwu Qu , Yuanbin Cao , Zhirong Wang , and Hongbin Zha . Alternating multi-bit quantization for recurrent neural networks . International Conference on Learning Representations ( ICLR ) , 2018 ."}, {"review_id": "HkfYOoCcYX-1", "review_text": "Summary: This paper addresses the computational aspects of Viterbi-based encoding for neural networks. In usual Viterbi codes, input messages are encoded via a convolution with a codeword, and then decoded using a trellis. Now consider a codebook with n convolutional codes, of rate 1/k. Then a vector of length n is represented by inputing a message of length k and receiving n encoded bits. Then the memory footprint (in terms of messages) is reduced by rate k/n. This is the format that will be used to encode the row indices in a matrix, with n columns. (The value of each nonzero is stored separately.) However, it is clear that not all messages are possible, only those in the \"range space\" of my codes. (This part is previous work Lee 2018.) The \"Double Viterbi\" (new contribution) refers to the storage of the nonzero values themselves. A weakness of CSR and CSC (carried over to the previous work) is that since each row may have a different number of nonzeros, then finding the value of any particular nonzero requires going through the list to find the right corresponding nonzero, a sequential task. Instead, m new Viterbi decompressers are included, where each row becomes (s_1*codeword_1 + s_2*codeword2 + ...) cdot mask, and the new scalar are the results of the linear combinations of the codewords. Pros: - I think the work addressed here is important, and though the details are hard to parse and the new contributions seemingly small, it is important enough for practical performance. - The idea is theoretically sound and interesting. Cons: - My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor. Compressability is evaluated, but that was already present in the previous work. Therefore the novel contribution of this paper over Lee 2018 is not clearly outlined. - It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract. - Minor grammatical mistakes (missing \"a\" or \"the\" in front of some terms, suggest proofread.) ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for the positive comments . We added the more experimental data of runtime analysis to address the Reviewer 's main concern . Q1.My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor . Compressability is evaluated , but that was already present in the previous work . Therefore the novel contribution of this paper over [ 1 ] is not clearly outlined . We conducted additional simulations to evaluate the runtime benefit of the proposed method compared to that of the method in [ 1 ] . We generated random 512-by-512 matrices with pruning rate ranging from 70 % to 95 % and simulated the number of parameters fed to PEs in 10000 cycles . The assumptions used for the simulation and analysis data have been updated in Figure 6c of the revised manuscript . We could observe that proposed parallel weight decoding based on the second Viterbi decompressor allowed 10 % to 40 % more parameters to be fed to PEs than the previous design [ 1 ] . The proposed method outperformed both baseline method and [ 1 ] in all simulation results . Please note that the data described in Figure 6c has been updated from Figure 7 , and our method shows better performance in new data compared to the data shown in the original manuscript . While preparing for the rebuttal , we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method . After further optimization , we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript . Therefore , we updated Figure 7 in original manuscript to Figure 6c in the updated manuscript according to the new data . Q2.It is extremely hard to follow what exactly is going on ; I believe a few illustrative examples would help make the paper much clearer ; in fact the idea is not that abstract . In the revision , we added the more precise mathematical description of the input and output of each block in Figure 1 and showed the change of the exact weight representation at each process . We first prune weights in a neural network with the Viterbi-based pruning scheme [ 1 ] , then we quantize the pruned weights with the alternating quantization method [ 2 ] . Our main contribution is the third process , which includes encoding each weight with the Viterbi algorithm , and retraining for the recovery of accuracy . With our proposed method , the sparse and encoded weights are reconstructed to a dense matrix as described in Figure 2 . Figure 2 illustrates the purpose of our proposed scheme , which is the parallelization of the whole sparse-to-dense conversion process with the VDs while maintaining the high compression rate . Q3.Minor grammatical mistakes ( missing `` a '' or `` the '' in front of some terms , suggest proofread . ) Thanks very much for the suggestions . We tried to fix grammatical mistakes as much as possible in the revision . Reference [ 1 ] Dongsoo Lee , Daehyun Ahn , Taesu Kim , Pierce I. Chuang , and Jae-Joon Kim . Viterbi-based pruning for sparse matrix with fixed and high index compression ratio . International Conference on Learning Representations ( ICLR ) , 2018 . [ 2 ] Chen Xu , Jianqiang Yao , Zouchen Lin , Wenwu Qu , Yuanbin Cao , Zhirong Wang , and Hongbin Zha . Alternating multi-bit quantization for recurrent neural networks . International Conference on Learning Representations ( ICLR ) , 2018 ."}, {"review_id": "HkfYOoCcYX-2", "review_text": "The paper proposes two additional steps to improve the compression of weights in deep neural networks. The first is to quantize the weights after pruning, and the second is to further encode the quantized weights. There are several weaknesses in this paper. The first one is clarity. The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper. The paper can be made more mathematically precise. The input and output types of each block in Figure 1. should be clearly stated. For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits. Since the paper relies so heavily on Lee et al., the authors should make an effort to summarize the approach in a mathematically precise way. The figures are almost useless, because the captions contain very little information. For example, the authors should at least say that the \"D\" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned. Many more can be said in all the figures. The second weakness is experimental design. There are two conflicting qualities that need to be optimized--performance and compression rate. When optimizing the compression rate, it is important not to look at the test set error. If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set. The test set is typically small compared to the training set, so it is no surprise that the compression rate can be as high as 90%. Optimizing compression rates should be done on the training set with a separate development set. The test set should not used before the best compression scheme is selected. Both the results on the development set and on the test set should be reported for the validity of the experiments. I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning. Lee et al. seem to make similar mistakes, and it is likely that their experimental design is also flawed.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for the comments . We believe that this response can help the Reviewer to be more convinced about the validness of our experiments ; in particular , the validness of our retraining methodology . Q1.The paper is not very self-contained , and I have to constantly go back to [ 1 ] and [ 2 ] in order to read through the paper . In the original manuscript , we had to limit the detailed information of the previous work due to the page limit . Based on the Reviewer \u2019 s comments , we added more description about the schemes we adopted from [ 1 ] and [ 2 ] in Appendix A.1 and A.2 of the revised manuscript . Q2.The input and output types of each block in Figure 1. should be clearly stated , and the figures are almost useless because the captions contain very little information . We tried to add more information to the figures in the revision . First , in Figure 1 , we added the more mathematically precise description of input and output of each block to show how the exact weight representation is changed at each process . We also added additional explanation for 'D ' of Figure 2 in its caption . For the Figure 4 , we added the description of the underlined numbers . Q3.Optimizing compression rates should be done on the training set with a separate development set . The test set should not used before the best compression scheme is selected . Both the results on the development set and on the test set should be reported for the validity of the experiments . Thanks for pointing this out . We believe that this is the Reviewer 1 's core question so would like to justify our results more in detail in this response and try to convince the Reviewer . We agree that optimizing compression rates should not use the test set before the best compression scheme is selected . In fact , in case of PTB and Wikitext-2 corpus , we already used the provided validation set and measured the test PPW only once after training ( Table 2 ) in the original manuscript . From the Table 2 , we can see that our proposed scheme maintains the accuracy of the uncompressed baseline network . On the other hand , the CIFAR-10 dataset does not include a separate validation set , so we had to use the test set in the retraining process . To avoid using the test set in the retraining process as the Reviewer pointed out , we randomly selected 5K validation images among the original 50K training images in CIFAR-10 dataset , and applied our scheme . Then , we observed the training and validation accuracy at each training epoch , and measured the test accuracy once after training . The accuracy results are as shown in the following table . Note the compression rates are the same as the data in Table 3 in the original manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Compression scheme Validation Error ( % ) Test Error ( % ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - -- -- -- -- -- -- -- -- -- -- - Baseline 11.5 12.2 Pruning [ 1 ] 11.4 12.2 VWM ( Ours ) 11.4 12.4 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- The test accuracy in the above table is about 1 % less than the accuracy which we reported in the originally submitted manuscript because the number of training data was decreased as part of the data set is used as a validation set . However , the results show that our proposed method does not make the network be overfitted to test data as the Reviewer doubted because the difference between the accuracy for validation set and test set are consistent with the values from the previous works . Note that even the uncompressed baseline network exhibits similar accuracy difference between the validation error and the test error compared with the compressed networks . Therefore , we believe that our proposed compression method does not suffer from the concerned overfitting problem regardless of the types of neural networks or dataset . Reference [ 1 ] Dongsoo Lee , Daehyun Ahn , Taesu Kim , Pierce I. Chuang , and Jae-Joon Kim . Viterbi-based pruning for sparse matrix with fixed and high index compression ratio . International Conference on Learning Representations ( ICLR ) , 2018 . [ 2 ] Chen Xu , Jianqiang Yao , Zouchen Lin , Wenwu Qu , Yuanbin Cao , Zhirong Wang , and Hongbin Zha . Alternating multi-bit quantization for recurrent neural networks . International Conference on Learning Representations ( ICLR ) , 2018 ."}], "0": {"review_id": "HkfYOoCcYX-0", "review_text": "This paper presents a new way to represent a dense matrix in a compact format. First, the method prunes a dense matrix based on the Viterbi-based pruning. Then, the pruned matrix is quantized with alternating multi-bit quantization. Finally, the binary vectors produced by the quantization algorithm are further compressed with the Viterbi-based algorithm. It spots the problem of each existing approach and solve the problems by combining each method. The combination is new and the result is encouraging. I find this paper is interesting and I like the strong results. It is an interesting combination of methods. However, the experiments are not enough to show that the proposed method is really needed to achieve the results. If these are answered well, I'd be happy to change my evaluation. 1. The method should be compared with other combinations of components. At least, it should be compared with \"Multi-bit quantization only (Xu et al., 2018)\" and \"Multi-bit-quantization + Viterbi-based binary code encoding\". 2. The experiments with \"Don't Care\" should go to the experiment section, and the end-to-end results should be present but not the ratio of incorrect bits. 3. Similarly, the paper will become stronger if it has some experimental results that compare quantization methods. In Section 3.3., it mentions that the conventional k-bit quantization was tried and significant accuracy drops were observed. I feel that this is a kind of things which support the proposed method if it is properly assessed. 4. When you say \"slow\" form something and propose a method to address it, I'd like to see some benchmark numbers. There is an experiment with simulation, but that does not seem to simulate the slow \"sequential sparse matrix decoding process\". Minor comments: * It was a bit hard to understand how a matrix is processed through the flowchart in Fig. 1 at first glance. It would help readers to understand it better if it has a corresponding figure which shows how a matrix is processed through the flowchart.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for the constructive comments . We tried to strengthen our claims by adding more experimental data which the Reviewer requested . 1.The proposed `` Multi-bit-quantization + Viterbi-based binary code encoding '' requires slightly larger memory footprint than `` Multi-bit quantization only ( [ 4 ] ) '' because some of the Viterbi encoded bits have different indices from their corresponding quantization bits . Hence , the `` Multi-bit quantization only '' requires 10 % to 20 % smaller memory footprint than `` Multi-bit-quantization + Viterbi-based binary code encoding '' case . However , the main reason why we apply the Viterbi weight encoding is that parallel sparse-to-dense matrix conversion can be done by applying same Viterbi encoding process to the non-zero values and indices of the non-zero values in parallel . This parallel sparse-to-dense conversion makes the speed of feeding parameters to PEs 10 % to 40 % faster compared to [ 1 ] ( Figure 6c ) . 2.Per Reviewer \u2019 s suggestion , the experimental results for the effectiveness of `` Don \u2019 t Care '' term have been moved to Section 4.1 . 3.Per Reviewer 's suggestion , we measured accuracy differences before and after Viterbi encoding for several quantization methods such as linear quantization ( [ 2 ] ) , logarithmic quantization ( [ 3 ] ) , and alternating quantization ( [ 4 ] ) methods with the same quantization bits ( 3-bit ) . The result shows that combination with alternating quantization and Viterbi weight encoding had only 2 % validation accuracy degradation after the Viterbi encoding was applied first right after the quantization and the accuracy was easily recovered with retraining . On the other hand , the combination with the other quantization methods and Viterbi weight encoding showed accuracy degradation as much as 71 % , which was too large to recover the accuracy with retraining . The accuracy difference mainly results from the uneven weight distribution . Because weights of neural networks usually are normally distributed , the composition ratio of ' 0 ' and ' 1 ' is not equal when the linear or logarithmic quantization is applied to the weights of neural networks . As we stated in the manuscript , Viterbi encoder tends to produce similar number of ' 0 ' and ' 1 ' . Therefore , we can conclude that under the same bit condition , alternating quantization method shows best accuracy and compatibility with our bit-by-bit Viterbi encoding scheme regardless of the type of neural networks . 4.We conducted additional simulations to compare sparse matrix reconstruction speed of [ 1 ] and the proposed method . We used a random 512-by-512 size matrix with various pruning rate ranging from 75 % to 95 % . We conducted the simulations under the assumptions described in Figure 6c . The simulation results are shown in Figure 6c in updated manuscript . We could observe that the proposed method could feed 10 % to 40 % more nonzero weights and input activations to PEs in same 10000 cycles compared to [ 1 ] . Proposed method could also feed parameters to PEs 20 % to 106 % faster compared to baseline method , which reads dense weight and activation matrices directly from DRAM . The improvement in the proposed scheme mainly comes from the parallelized process of assigning non-zero values to their corresponding indices in the weight matrix . While preparing addition data for the rebuttal , we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method . After further optimization , we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript . Therefore , we updated Figure 7 in original manuscript to Figure 6c in updated manuscript according to the new data . 5.We added the change of the exact weight representation at each process in Figure 1 to clarify the flowchart . Reference [ 1 ] Dongsoo Lee , Daehyun Ahn , Taesu Kim , Pierce I. Chuang , and Jae-Joon Kim . Viterbi-based pruning for sparse matrix with fixed and high index compression ratio . International Conference on Learning Representations ( ICLR ) , 2018 . [ 2 ] Darryl D. Lin , Sachin S. Talathi , and V. Sreekanth Annapureddy . Fixed point quantization of deep convolutional networks . In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48 , ICML \u2019 16 , pp . 2849\u20132858 . 2016 . [ 3 ] Daisuke Miyashita , Edward H. Lee , and Boris Murmann . Convolutional Neural Networks using Logarithmic Data Representation . CoRR , abs/1603.01025 , 2016 . URL https : //arxiv.org/abs/1603.01025 . [ 4 ] Chen Xu , Jianqiang Yao , Zouchen Lin , Wenwu Qu , Yuanbin Cao , Zhirong Wang , and Hongbin Zha . Alternating multi-bit quantization for recurrent neural networks . International Conference on Learning Representations ( ICLR ) , 2018 ."}, "1": {"review_id": "HkfYOoCcYX-1", "review_text": "Summary: This paper addresses the computational aspects of Viterbi-based encoding for neural networks. In usual Viterbi codes, input messages are encoded via a convolution with a codeword, and then decoded using a trellis. Now consider a codebook with n convolutional codes, of rate 1/k. Then a vector of length n is represented by inputing a message of length k and receiving n encoded bits. Then the memory footprint (in terms of messages) is reduced by rate k/n. This is the format that will be used to encode the row indices in a matrix, with n columns. (The value of each nonzero is stored separately.) However, it is clear that not all messages are possible, only those in the \"range space\" of my codes. (This part is previous work Lee 2018.) The \"Double Viterbi\" (new contribution) refers to the storage of the nonzero values themselves. A weakness of CSR and CSC (carried over to the previous work) is that since each row may have a different number of nonzeros, then finding the value of any particular nonzero requires going through the list to find the right corresponding nonzero, a sequential task. Instead, m new Viterbi decompressers are included, where each row becomes (s_1*codeword_1 + s_2*codeword2 + ...) cdot mask, and the new scalar are the results of the linear combinations of the codewords. Pros: - I think the work addressed here is important, and though the details are hard to parse and the new contributions seemingly small, it is important enough for practical performance. - The idea is theoretically sound and interesting. Cons: - My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor. Compressability is evaluated, but that was already present in the previous work. Therefore the novel contribution of this paper over Lee 2018 is not clearly outlined. - It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract. - Minor grammatical mistakes (missing \"a\" or \"the\" in front of some terms, suggest proofread.) ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for the positive comments . We added the more experimental data of runtime analysis to address the Reviewer 's main concern . Q1.My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor . Compressability is evaluated , but that was already present in the previous work . Therefore the novel contribution of this paper over [ 1 ] is not clearly outlined . We conducted additional simulations to evaluate the runtime benefit of the proposed method compared to that of the method in [ 1 ] . We generated random 512-by-512 matrices with pruning rate ranging from 70 % to 95 % and simulated the number of parameters fed to PEs in 10000 cycles . The assumptions used for the simulation and analysis data have been updated in Figure 6c of the revised manuscript . We could observe that proposed parallel weight decoding based on the second Viterbi decompressor allowed 10 % to 40 % more parameters to be fed to PEs than the previous design [ 1 ] . The proposed method outperformed both baseline method and [ 1 ] in all simulation results . Please note that the data described in Figure 6c has been updated from Figure 7 , and our method shows better performance in new data compared to the data shown in the original manuscript . While preparing for the rebuttal , we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method . After further optimization , we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript . Therefore , we updated Figure 7 in original manuscript to Figure 6c in the updated manuscript according to the new data . Q2.It is extremely hard to follow what exactly is going on ; I believe a few illustrative examples would help make the paper much clearer ; in fact the idea is not that abstract . In the revision , we added the more precise mathematical description of the input and output of each block in Figure 1 and showed the change of the exact weight representation at each process . We first prune weights in a neural network with the Viterbi-based pruning scheme [ 1 ] , then we quantize the pruned weights with the alternating quantization method [ 2 ] . Our main contribution is the third process , which includes encoding each weight with the Viterbi algorithm , and retraining for the recovery of accuracy . With our proposed method , the sparse and encoded weights are reconstructed to a dense matrix as described in Figure 2 . Figure 2 illustrates the purpose of our proposed scheme , which is the parallelization of the whole sparse-to-dense conversion process with the VDs while maintaining the high compression rate . Q3.Minor grammatical mistakes ( missing `` a '' or `` the '' in front of some terms , suggest proofread . ) Thanks very much for the suggestions . We tried to fix grammatical mistakes as much as possible in the revision . Reference [ 1 ] Dongsoo Lee , Daehyun Ahn , Taesu Kim , Pierce I. Chuang , and Jae-Joon Kim . Viterbi-based pruning for sparse matrix with fixed and high index compression ratio . International Conference on Learning Representations ( ICLR ) , 2018 . [ 2 ] Chen Xu , Jianqiang Yao , Zouchen Lin , Wenwu Qu , Yuanbin Cao , Zhirong Wang , and Hongbin Zha . Alternating multi-bit quantization for recurrent neural networks . International Conference on Learning Representations ( ICLR ) , 2018 ."}, "2": {"review_id": "HkfYOoCcYX-2", "review_text": "The paper proposes two additional steps to improve the compression of weights in deep neural networks. The first is to quantize the weights after pruning, and the second is to further encode the quantized weights. There are several weaknesses in this paper. The first one is clarity. The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper. The paper can be made more mathematically precise. The input and output types of each block in Figure 1. should be clearly stated. For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits. Since the paper relies so heavily on Lee et al., the authors should make an effort to summarize the approach in a mathematically precise way. The figures are almost useless, because the captions contain very little information. For example, the authors should at least say that the \"D\" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned. Many more can be said in all the figures. The second weakness is experimental design. There are two conflicting qualities that need to be optimized--performance and compression rate. When optimizing the compression rate, it is important not to look at the test set error. If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set. The test set is typically small compared to the training set, so it is no surprise that the compression rate can be as high as 90%. Optimizing compression rates should be done on the training set with a separate development set. The test set should not used before the best compression scheme is selected. Both the results on the development set and on the test set should be reported for the validity of the experiments. I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning. Lee et al. seem to make similar mistakes, and it is likely that their experimental design is also flawed.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you very much for the comments . We believe that this response can help the Reviewer to be more convinced about the validness of our experiments ; in particular , the validness of our retraining methodology . Q1.The paper is not very self-contained , and I have to constantly go back to [ 1 ] and [ 2 ] in order to read through the paper . In the original manuscript , we had to limit the detailed information of the previous work due to the page limit . Based on the Reviewer \u2019 s comments , we added more description about the schemes we adopted from [ 1 ] and [ 2 ] in Appendix A.1 and A.2 of the revised manuscript . Q2.The input and output types of each block in Figure 1. should be clearly stated , and the figures are almost useless because the captions contain very little information . We tried to add more information to the figures in the revision . First , in Figure 1 , we added the more mathematically precise description of input and output of each block to show how the exact weight representation is changed at each process . We also added additional explanation for 'D ' of Figure 2 in its caption . For the Figure 4 , we added the description of the underlined numbers . Q3.Optimizing compression rates should be done on the training set with a separate development set . The test set should not used before the best compression scheme is selected . Both the results on the development set and on the test set should be reported for the validity of the experiments . Thanks for pointing this out . We believe that this is the Reviewer 1 's core question so would like to justify our results more in detail in this response and try to convince the Reviewer . We agree that optimizing compression rates should not use the test set before the best compression scheme is selected . In fact , in case of PTB and Wikitext-2 corpus , we already used the provided validation set and measured the test PPW only once after training ( Table 2 ) in the original manuscript . From the Table 2 , we can see that our proposed scheme maintains the accuracy of the uncompressed baseline network . On the other hand , the CIFAR-10 dataset does not include a separate validation set , so we had to use the test set in the retraining process . To avoid using the test set in the retraining process as the Reviewer pointed out , we randomly selected 5K validation images among the original 50K training images in CIFAR-10 dataset , and applied our scheme . Then , we observed the training and validation accuracy at each training epoch , and measured the test accuracy once after training . The accuracy results are as shown in the following table . Note the compression rates are the same as the data in Table 3 in the original manuscript . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Compression scheme Validation Error ( % ) Test Error ( % ) -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - -- -- -- -- -- -- -- -- -- -- - Baseline 11.5 12.2 Pruning [ 1 ] 11.4 12.2 VWM ( Ours ) 11.4 12.4 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- The test accuracy in the above table is about 1 % less than the accuracy which we reported in the originally submitted manuscript because the number of training data was decreased as part of the data set is used as a validation set . However , the results show that our proposed method does not make the network be overfitted to test data as the Reviewer doubted because the difference between the accuracy for validation set and test set are consistent with the values from the previous works . Note that even the uncompressed baseline network exhibits similar accuracy difference between the validation error and the test error compared with the compressed networks . Therefore , we believe that our proposed compression method does not suffer from the concerned overfitting problem regardless of the types of neural networks or dataset . Reference [ 1 ] Dongsoo Lee , Daehyun Ahn , Taesu Kim , Pierce I. Chuang , and Jae-Joon Kim . Viterbi-based pruning for sparse matrix with fixed and high index compression ratio . International Conference on Learning Representations ( ICLR ) , 2018 . [ 2 ] Chen Xu , Jianqiang Yao , Zouchen Lin , Wenwu Qu , Yuanbin Cao , Zhirong Wang , and Hongbin Zha . Alternating multi-bit quantization for recurrent neural networks . International Conference on Learning Representations ( ICLR ) , 2018 ."}}