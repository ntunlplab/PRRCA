{"year": "2021", "forum": "5mhViEOQxaV", "title": "Controllable Pareto Multi-Task Learning", "decision": "Reject", "meta_review": "The paper first aims to propose a new controllable Pareto multi-task learning framework to find pareto-optimal solutions. But after the revision according the comments, the paper claims to find finite Pareto stationary solutions. But the paper still can not prove their proposed method can find the Pareto stationary solutions. Even if they can find the  Pareto stationary solutions, they can not guarantee find the pareto front which is conflict with the experiments and claims. There are major flaws in the paper.", "reviews": [{"review_id": "5mhViEOQxaV-0", "review_text": "The paper proposes a method to controllably generate models on the trade-off front of multi-task learning problems . The key idea is to use a hypernetwork that will generate the MTL parameters on demand conditioned on the desired trade-off . The hypernetwork can be trained along with the MTL in an end-to-end manner . Strengths : + Paper addresses an important practical problem . Instead of training many models independently to span the trade-off front , it can generate an MTL model on demand for any desired trade-off point . + The idea of using a hypernetwork in the context of MTL is new . Weaknesses : - Conceptually and technically the improvements over [ 1 ] are minimal . The main contribution of the paper is to use a hypernetwork that will generate the weights . Although this is new in the context of MTL , it is in fact a common idea in the context of Neural Architecture Search . In NAS we have a supernet which is equivalent to the hypernet and then perform reference based multi-objective optimization over it . See [ 2 ] for an example . - Method does not seem to be very scalable in terms of MTL model size . The experimental evaluation is on a small scale . For larger models the hypernet needs to generate a large output for the desired trade-off . In that case training the hypernet gets more challenging . - In terms of the exposition , firstly the main paper has very few details apart from the high level idea , there is a lot of repetition of the same points . Secondly , a lot of the actual paper and the appendix itself is background material that is already well known in the multi-objective optimization literature . - The experiments are a bit disappointing . The advantage of the proposed method in comparison to linear scalarization would be on concave pareto fronts as shown in Fig.6.Unless I am missing something , looks like in none of the actual experiments is the trade-off a concave curve . Overall , in the current form the paper looks more like a proof of concept . I would encourage the authors to demonstrate or discuss the scalability of the solution . Clarifications : 1 ) I guess one of the benefits of this approach over training multiple models is in terms of the total number of parameters and computational complexity . But there is no discussion of these aspects . How does the HyperNet compare in size wrt to the MTL model part ? I would imagine the HyperNet is much bigger since it has to learn to predict the MTL parameters . 2 ) How about inference ? How long does it take for the HyperNet to generate the parameters since you still have to solve an optimization problem ? 3 ) The paper claims real-time but there is no discussion of this claim . 4 ) In Fig.7 there seem to be some solutions that are better than the trade-off front obtained by the proposed method . Any comments on what is limiting the current approach from reaching/surpassing those solutions ? [ 1 ] Pareto Multi-Task Learning [ 2 ] Neural Architecture Transfer", "rating": "7: Good paper, accept", "reply_text": "Thank you for your efforts in reviewing our paper . We provide our responses to your concerns point by point as follow : > Improvements and Contributions 1 . We totally agree with the reviewer that the use of hypernetwork to generate the weights for the main network itself is not a new idea , and our contribution is to use it to generate the whole trade-off curve for MTL . 2.We think there are some critical misunderstandings on the scalability and the inference behavior of our work . In short , our hypernetwork-based model has * * a similar number of parameters with the MTL model * * , and can scale well for large models . Thanks to the end-to-end training , our model * * does not need extra optimization steps at the inference time * * . We will address these two important concerns in detail in the following response and hope they can make our contribution much clearer . We ~~will also revise~~ have revised our manuscript carefully to avoid misunderstanding . 3.The idea of using multiobjective optimization methods to solve MTL is a timely research direction , and has drawn attention from researchers in the MTL community [ 1,3-5 ] . Learning the entire trade-off curve , instead of a finite set of separate solutions , would be a novel contribution to multiobjective optimization , and also an important improvement over the current related works that use multiobjective optimization to solve MTL problem . We find there is a concurrent ICLR submissions that shares similar idea with our work ( https : //openreview.net/forum ? id=NjF772F4ZZR ) to learn the entire Pareto front by hypernetwork . We ~~will add~~ have added discussions with this work in our manuscript . See above response to AnonReviewer3 for more detailed discussions on the contribution . 4.Thank you for pointing out the closely related work ( NAT ) [ 2 ] , which is a novel and efficient MO-NAS method . We have cited and discussed this work in the revised paper . NAT simultaneously optimizes a set of neural networks , which are sampled from a large supernet , to approximate the optimal trade-off curve among different objectives via a single run . The obtained supernet also supports fast adaption to new tasks and domains . Our proposed method is different from NAT , and it learns a direct mapping from a specific preference to the corresponding Pareto stationary solution without any search/selection/optimization , which is good for real-time trade-off adjustment . Our proposed method might further improve the preference-based fast adaption ability for the current NAS methods , and searching different architectures for different preferences is also an important improvement for MTL . > Scalability and Experiments 1 . Scalability : With the model compression ability powered by the hypernetwork ( e.g. , chunking [ 7,8 ] ) , our proposed hypernetwork-based model can scale well to large scale problems . In this paper , our proposed model has a comparable number of parameters with the corresponding single MTL model in all experiments . For example , for the CityScape and NYUv2 experiments , we use the SegNet-based MTL model proposed in [ 6 ] , and both hypernetwork-based model and a single MTL model have around 26 million parameters . Given the current works [ 1,4,5 ] need to train and store multiple MTL models ( say , 10 models , total 260M parameters ) , our proposed model is much more parameter-efficient while learning the whole trade-off curve . In this sense , it is much more scalable than the current methods which need to train multiple MTL models . 2.End-to-End Traning : Thanks to the proposed end-to-end gradient-based optimization method , training our model only requires the same number of optimization iterations ( epochs ) to train a single traditional MTL network , while our model can learn the entire trade-off curve . In other words , our proposed method provides an efficient way to solve MTL problems with different trade-off preferences by a single model . The properties of multiobjective optimization , such as the trade-off curve is a low dimensional manifold , and similar preference vectors have similar hypernetwork outputs , make it possible to learn the curve to cover different ( infinite ) trade-off preferences with a single model . Simultaneously training the hypernetwork-based model with different preferences is also good for cross-preference regularization . See the below responses to AnonReviewer1 on model structure and regularization . 3.Experiments : We follow similar experimental settings ( datasets , problems and models ) with the most related current works [ 1,3-6 ] . The model size is up to 26 million parameters , and the number of tasks is up to 20 . We ~~will add~~ have added more experimental results with analysis to demonstrate the scalability of our proposed method ( e.g. , model with pretrained feature extractor , up to 100M parameters ) in the revised manuscript . We will also open-source the code for all experiments to let other researchers easily reproduce our results ( see code examples in supp.zip ) ."}, {"review_id": "5mhViEOQxaV-1", "review_text": "This paper proposes a novel controllable Pareto multi-task learning framework , which aims to learn the whole Pareto optimal front for all tasks with a single model . The motivation is straight forward and the proposed method is inspiring . However , the proposed technique does not match with the announced contribution . 1.This paper announces that it proposes a novel Pareto solution generator that can learn the whole Pareto front for MTL . However , actually , this paper adopts fixed shared parameters ( pretrained feature extractor ) and only optimizes a part of parameters of a MTL , which degrade the solution space and the solutions may not be the Pareto solutions . The Pareto front generated by such method may not be the real Pareto front for MTL . 2.Using fixed shared parameters conflicts with the essence Pareto MTL . In MTL , the tasks mutually regularized by the feature extractor , which improve the generalization ability of each task . This paper is globally well organized and clearly written . However , some important details are missing . 1.The details about the hypernetwork are unclear . 2.The paper lacks of analysis on the experimental result . 3.Some notations are not clear , e.g. , does the loss used in the paper denotes empirical loss ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your efforts in reviewing our paper . We provide our responses to your concerns point by point as follow : > Fixed Shared Parameters We believe there are some misunderstandings on the model we proposed in this paper . We want to clarify them here and will revise our paper to make these points clear : 1 . For all experiments in the current paper , we * * do not use any pretrained feature extractor * * , and all multi-task deep neural network ( including our proposed hypernetwork-based model ) are * * trained from scratch * * . In our proposed method , we use the hypernetwork to * * generate all parameters * * for the main MTL network . With the model compression ability powered by the hypernetwork ( like chunking [ 1,2 ] ) , our hypernetwork-based model has a similar number of trainable parameters with the traditional MTL network ( e.g. , both have 26 million parameters for the SegNet-based MTL model in the CityScape and NYUv2 problems , with the same structure as in [ 3 ] ) . ~~We will make this point clear , add details of our hypernetwork-based model in the revised manuscript.~~ We have added the details of our hypernetwork-based model in Section 5 to make this point clear . We will also open-source the code for all experiments , to let other researchers easily reproduce our results by training the models from scratch ( see code examples in supp.zip ) . 2.The misunderstanding might also come from the parameter sharing method ( e.g. , shared frozen or unfrozen pretrained feature extractor ) we discussed in Appendix C ( Fig.16 ( b ) in the original manuscript ) . This is the technique which can further improve the scalability of our proposed method , but it is * * not used * * in ~~this paper~~ the original manuscript . The use of pretrained feature extractor ( on a large scale dataset ) can improve the performance of an MTL model ( see a recent survey [ 4 ] ) . In our proposed method , such a shared feature extractor ( frozen or unfrozen ) can be treated as a structure constraint across all generated solutions . As correctly pointed out by the reviewer , with the shared feature extractor ~~ ( which is not used yet in the paper ) ~~ , our method will generate a constrained Pareto front but not the real ( unconstrained ) Pareto front . We ~~will add~~ have added discussions and experiments in the revised manuscript . 3.Since the objective functions for a multi-task neural network could be highly non-convex , the best solutions we can find is the Pareto stationary solutions rather than the global Pareto optimal solutions ( similar to the local/stationary optimal solution for single-objective a deep neural network ) . It is also the case for all current work . More details can be found in response to AnonReviewer3 above . We ~~will carefully revise~~ have carefully revised our announced contribution accordingly in the manuscript . > MTL as Multi-Objective Optimization 1 . The idea of reformulating the MTL problem as multiobjective optimization is a timely research direction , and have drawn attention from researchers in the MTL community [ 5-8 ] . The current works directly use existing methods , and can only find a single [ 5 ] or a finite set of Pareto stationary solutions for an MTL problem [ 6-8 ] . In addition , these methods need to train and store a large number of deep neural networks with different preferences , which is expensive and prohibited in many real-world applications . 2.In contrast , we propose a novel method to use a single model to learn the whole trade-off curve ( which might contain infinite solutions with different preferences ) . Thanks to our proposed end-to-end training procedure , the hypernetwork-based model can simultaneously learn to generate the corresponding Pareto stationary solutions for different preferences . In other words , our proposed method provides an efficient way to solve MTL problems with different trade-off preferences by a single model . There is a concurrent ICLR submission shares similar idea with our work ( https : //openreview.net/forum ? id=NjF772F4ZZR ) . They also demonstrate the benefit of reformulating MTL as solving multiobjective optimization problem , and propose to use hypernetwork to learn the entire trade-off curve . We ~~will add~~ have added discussion with this work in our revised manuscript ."}, {"review_id": "5mhViEOQxaV-2", "review_text": "This paper proposes a controllable Pareto multi-task learning model by generating the Pareto-stationary solutions . Even though the idea to generate a Pareto-stationary solution seems interesting , the proposed method is overstated . It is well known that the MGDA can find Pareto-stationary solutions , which however are NOT Pareto-optimal solutions . The steepest gradient descent method to solve problem ( 10 ) is just the primal form of the MGDA method and hence it can not find the Pareto-optimal solution . To the best of my knowledge , there is no method which can guarantee to find a Pareto-optimal solution for general multi-objective optimization problems . Authors claim that their method can generate Pareto-optimal solutions but actually they can generate only Pareto-stationary solutions at most . Authors confuse the Pareto-optimum with Pareto-stationary solutions and this is misleading . In this sense , the proposed method is not so appealing as Pareto-stationary solutions are easy to obtain in many methods .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your efforts in reviewing our paper . We provide our responses to your concerns point by point as follow : > Pareto-stationary solution 1 . As you correctly point out , the solutions our method can generate are Pareto stationary solutions rather than global Pareto optimal solutions . The Pareto stationary is a necessary condition for Pareto optimality [ 1,2 ] , and more strong requirements are needed to make it a sufficient condition ( e.g. , all functions are convex and all $ \\lambda $ are strictly positive in the dual problem ) [ 1,3 ] . For a general multiobjective optimization problem , no method can guarantee to find a Pareto optimal solution . We will modify our statements accordingly and carefully revise our manuscript to make this point clear . We believe our proposed method is useful for solving the MTL problem with deep neural networks : 2 . This Pareto stationary v.s . Pareto optimal situation is similar to the well-known stationary/local optimal v.s . global optimal situation for training a single-objective deep neural network . The objective function of a deep neural network could be highly non-convex , and the gradient-based methods ( e.g. , SGD and Adam ) can not guarantee to find a global optimal solution . However , these methods are widely-used to train deep neural networks , and achieve promising performance in practice . The experimental results show that our proposed method can generate reasonable trade-off curves for different MTL problems , which could be useful for many real-world MTL applications . 3.Non-convex optimization on deep neural networks [ 4-6 ] is an important and on-going research topic . How to generalize the results from single-objective optimization to multiobjective optimization would be an interesting research direction in future work . > Contribution : `` The proposed method is not so appealing as Pareto-stationary solutions are easy to obtain in many methods . '' 1.We think there is a misunderstanding here . Instead of obtaining a single or a set of finite Pareto stationary solutions by other multiobjective optimization methods , we propose to * * learn the entire trade-off curve for a given multiobjective optimization problem ( which might contain infinite Pareto stationary solutions ) by a single model * * . The result we provide to the user/decision-maker is the model but not a set of finite Pareto stationary solutions . By adjusting the preference vector , decision-makers can obtain their preferred Pareto stationary solution ( s ) in real-time , rather than rerunning the optimization algorithm if none of the finite solutions satisfy their preference . This is a novel contribution for solving multiobjective optimization problems , and could be valuable for many real-world applications . ( There are a few concurrent works , see 5 ) . 2.Under mild conditions , the Pareto set/front for a continuous multiobjective optimization problem is an ( m-1 ) -dimensional manifold in the solution/objective space [ 7 ] . In our proposed methods , the valid set of preference vector is also an ( m-1 ) -dimensional manifold , where m is number of tasks . Our proposed hypernetwork-based model provides a natural mapping from the preference vector to the trade-off curve and supports an efficient end-to-end gradient-based method to train the model . By going through the preference vectors , we can reconstruct the entire trade-off curve as shown in our experiments ( e.g. , Fig 7,8 ) , which provides an intuitive and efficient way to analyze the trade-off among different tasks . The proposed model structure is also a novel contribution of this paper ."}], "0": {"review_id": "5mhViEOQxaV-0", "review_text": "The paper proposes a method to controllably generate models on the trade-off front of multi-task learning problems . The key idea is to use a hypernetwork that will generate the MTL parameters on demand conditioned on the desired trade-off . The hypernetwork can be trained along with the MTL in an end-to-end manner . Strengths : + Paper addresses an important practical problem . Instead of training many models independently to span the trade-off front , it can generate an MTL model on demand for any desired trade-off point . + The idea of using a hypernetwork in the context of MTL is new . Weaknesses : - Conceptually and technically the improvements over [ 1 ] are minimal . The main contribution of the paper is to use a hypernetwork that will generate the weights . Although this is new in the context of MTL , it is in fact a common idea in the context of Neural Architecture Search . In NAS we have a supernet which is equivalent to the hypernet and then perform reference based multi-objective optimization over it . See [ 2 ] for an example . - Method does not seem to be very scalable in terms of MTL model size . The experimental evaluation is on a small scale . For larger models the hypernet needs to generate a large output for the desired trade-off . In that case training the hypernet gets more challenging . - In terms of the exposition , firstly the main paper has very few details apart from the high level idea , there is a lot of repetition of the same points . Secondly , a lot of the actual paper and the appendix itself is background material that is already well known in the multi-objective optimization literature . - The experiments are a bit disappointing . The advantage of the proposed method in comparison to linear scalarization would be on concave pareto fronts as shown in Fig.6.Unless I am missing something , looks like in none of the actual experiments is the trade-off a concave curve . Overall , in the current form the paper looks more like a proof of concept . I would encourage the authors to demonstrate or discuss the scalability of the solution . Clarifications : 1 ) I guess one of the benefits of this approach over training multiple models is in terms of the total number of parameters and computational complexity . But there is no discussion of these aspects . How does the HyperNet compare in size wrt to the MTL model part ? I would imagine the HyperNet is much bigger since it has to learn to predict the MTL parameters . 2 ) How about inference ? How long does it take for the HyperNet to generate the parameters since you still have to solve an optimization problem ? 3 ) The paper claims real-time but there is no discussion of this claim . 4 ) In Fig.7 there seem to be some solutions that are better than the trade-off front obtained by the proposed method . Any comments on what is limiting the current approach from reaching/surpassing those solutions ? [ 1 ] Pareto Multi-Task Learning [ 2 ] Neural Architecture Transfer", "rating": "7: Good paper, accept", "reply_text": "Thank you for your efforts in reviewing our paper . We provide our responses to your concerns point by point as follow : > Improvements and Contributions 1 . We totally agree with the reviewer that the use of hypernetwork to generate the weights for the main network itself is not a new idea , and our contribution is to use it to generate the whole trade-off curve for MTL . 2.We think there are some critical misunderstandings on the scalability and the inference behavior of our work . In short , our hypernetwork-based model has * * a similar number of parameters with the MTL model * * , and can scale well for large models . Thanks to the end-to-end training , our model * * does not need extra optimization steps at the inference time * * . We will address these two important concerns in detail in the following response and hope they can make our contribution much clearer . We ~~will also revise~~ have revised our manuscript carefully to avoid misunderstanding . 3.The idea of using multiobjective optimization methods to solve MTL is a timely research direction , and has drawn attention from researchers in the MTL community [ 1,3-5 ] . Learning the entire trade-off curve , instead of a finite set of separate solutions , would be a novel contribution to multiobjective optimization , and also an important improvement over the current related works that use multiobjective optimization to solve MTL problem . We find there is a concurrent ICLR submissions that shares similar idea with our work ( https : //openreview.net/forum ? id=NjF772F4ZZR ) to learn the entire Pareto front by hypernetwork . We ~~will add~~ have added discussions with this work in our manuscript . See above response to AnonReviewer3 for more detailed discussions on the contribution . 4.Thank you for pointing out the closely related work ( NAT ) [ 2 ] , which is a novel and efficient MO-NAS method . We have cited and discussed this work in the revised paper . NAT simultaneously optimizes a set of neural networks , which are sampled from a large supernet , to approximate the optimal trade-off curve among different objectives via a single run . The obtained supernet also supports fast adaption to new tasks and domains . Our proposed method is different from NAT , and it learns a direct mapping from a specific preference to the corresponding Pareto stationary solution without any search/selection/optimization , which is good for real-time trade-off adjustment . Our proposed method might further improve the preference-based fast adaption ability for the current NAS methods , and searching different architectures for different preferences is also an important improvement for MTL . > Scalability and Experiments 1 . Scalability : With the model compression ability powered by the hypernetwork ( e.g. , chunking [ 7,8 ] ) , our proposed hypernetwork-based model can scale well to large scale problems . In this paper , our proposed model has a comparable number of parameters with the corresponding single MTL model in all experiments . For example , for the CityScape and NYUv2 experiments , we use the SegNet-based MTL model proposed in [ 6 ] , and both hypernetwork-based model and a single MTL model have around 26 million parameters . Given the current works [ 1,4,5 ] need to train and store multiple MTL models ( say , 10 models , total 260M parameters ) , our proposed model is much more parameter-efficient while learning the whole trade-off curve . In this sense , it is much more scalable than the current methods which need to train multiple MTL models . 2.End-to-End Traning : Thanks to the proposed end-to-end gradient-based optimization method , training our model only requires the same number of optimization iterations ( epochs ) to train a single traditional MTL network , while our model can learn the entire trade-off curve . In other words , our proposed method provides an efficient way to solve MTL problems with different trade-off preferences by a single model . The properties of multiobjective optimization , such as the trade-off curve is a low dimensional manifold , and similar preference vectors have similar hypernetwork outputs , make it possible to learn the curve to cover different ( infinite ) trade-off preferences with a single model . Simultaneously training the hypernetwork-based model with different preferences is also good for cross-preference regularization . See the below responses to AnonReviewer1 on model structure and regularization . 3.Experiments : We follow similar experimental settings ( datasets , problems and models ) with the most related current works [ 1,3-6 ] . The model size is up to 26 million parameters , and the number of tasks is up to 20 . We ~~will add~~ have added more experimental results with analysis to demonstrate the scalability of our proposed method ( e.g. , model with pretrained feature extractor , up to 100M parameters ) in the revised manuscript . We will also open-source the code for all experiments to let other researchers easily reproduce our results ( see code examples in supp.zip ) ."}, "1": {"review_id": "5mhViEOQxaV-1", "review_text": "This paper proposes a novel controllable Pareto multi-task learning framework , which aims to learn the whole Pareto optimal front for all tasks with a single model . The motivation is straight forward and the proposed method is inspiring . However , the proposed technique does not match with the announced contribution . 1.This paper announces that it proposes a novel Pareto solution generator that can learn the whole Pareto front for MTL . However , actually , this paper adopts fixed shared parameters ( pretrained feature extractor ) and only optimizes a part of parameters of a MTL , which degrade the solution space and the solutions may not be the Pareto solutions . The Pareto front generated by such method may not be the real Pareto front for MTL . 2.Using fixed shared parameters conflicts with the essence Pareto MTL . In MTL , the tasks mutually regularized by the feature extractor , which improve the generalization ability of each task . This paper is globally well organized and clearly written . However , some important details are missing . 1.The details about the hypernetwork are unclear . 2.The paper lacks of analysis on the experimental result . 3.Some notations are not clear , e.g. , does the loss used in the paper denotes empirical loss ?", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your efforts in reviewing our paper . We provide our responses to your concerns point by point as follow : > Fixed Shared Parameters We believe there are some misunderstandings on the model we proposed in this paper . We want to clarify them here and will revise our paper to make these points clear : 1 . For all experiments in the current paper , we * * do not use any pretrained feature extractor * * , and all multi-task deep neural network ( including our proposed hypernetwork-based model ) are * * trained from scratch * * . In our proposed method , we use the hypernetwork to * * generate all parameters * * for the main MTL network . With the model compression ability powered by the hypernetwork ( like chunking [ 1,2 ] ) , our hypernetwork-based model has a similar number of trainable parameters with the traditional MTL network ( e.g. , both have 26 million parameters for the SegNet-based MTL model in the CityScape and NYUv2 problems , with the same structure as in [ 3 ] ) . ~~We will make this point clear , add details of our hypernetwork-based model in the revised manuscript.~~ We have added the details of our hypernetwork-based model in Section 5 to make this point clear . We will also open-source the code for all experiments , to let other researchers easily reproduce our results by training the models from scratch ( see code examples in supp.zip ) . 2.The misunderstanding might also come from the parameter sharing method ( e.g. , shared frozen or unfrozen pretrained feature extractor ) we discussed in Appendix C ( Fig.16 ( b ) in the original manuscript ) . This is the technique which can further improve the scalability of our proposed method , but it is * * not used * * in ~~this paper~~ the original manuscript . The use of pretrained feature extractor ( on a large scale dataset ) can improve the performance of an MTL model ( see a recent survey [ 4 ] ) . In our proposed method , such a shared feature extractor ( frozen or unfrozen ) can be treated as a structure constraint across all generated solutions . As correctly pointed out by the reviewer , with the shared feature extractor ~~ ( which is not used yet in the paper ) ~~ , our method will generate a constrained Pareto front but not the real ( unconstrained ) Pareto front . We ~~will add~~ have added discussions and experiments in the revised manuscript . 3.Since the objective functions for a multi-task neural network could be highly non-convex , the best solutions we can find is the Pareto stationary solutions rather than the global Pareto optimal solutions ( similar to the local/stationary optimal solution for single-objective a deep neural network ) . It is also the case for all current work . More details can be found in response to AnonReviewer3 above . We ~~will carefully revise~~ have carefully revised our announced contribution accordingly in the manuscript . > MTL as Multi-Objective Optimization 1 . The idea of reformulating the MTL problem as multiobjective optimization is a timely research direction , and have drawn attention from researchers in the MTL community [ 5-8 ] . The current works directly use existing methods , and can only find a single [ 5 ] or a finite set of Pareto stationary solutions for an MTL problem [ 6-8 ] . In addition , these methods need to train and store a large number of deep neural networks with different preferences , which is expensive and prohibited in many real-world applications . 2.In contrast , we propose a novel method to use a single model to learn the whole trade-off curve ( which might contain infinite solutions with different preferences ) . Thanks to our proposed end-to-end training procedure , the hypernetwork-based model can simultaneously learn to generate the corresponding Pareto stationary solutions for different preferences . In other words , our proposed method provides an efficient way to solve MTL problems with different trade-off preferences by a single model . There is a concurrent ICLR submission shares similar idea with our work ( https : //openreview.net/forum ? id=NjF772F4ZZR ) . They also demonstrate the benefit of reformulating MTL as solving multiobjective optimization problem , and propose to use hypernetwork to learn the entire trade-off curve . We ~~will add~~ have added discussion with this work in our revised manuscript ."}, "2": {"review_id": "5mhViEOQxaV-2", "review_text": "This paper proposes a controllable Pareto multi-task learning model by generating the Pareto-stationary solutions . Even though the idea to generate a Pareto-stationary solution seems interesting , the proposed method is overstated . It is well known that the MGDA can find Pareto-stationary solutions , which however are NOT Pareto-optimal solutions . The steepest gradient descent method to solve problem ( 10 ) is just the primal form of the MGDA method and hence it can not find the Pareto-optimal solution . To the best of my knowledge , there is no method which can guarantee to find a Pareto-optimal solution for general multi-objective optimization problems . Authors claim that their method can generate Pareto-optimal solutions but actually they can generate only Pareto-stationary solutions at most . Authors confuse the Pareto-optimum with Pareto-stationary solutions and this is misleading . In this sense , the proposed method is not so appealing as Pareto-stationary solutions are easy to obtain in many methods .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you for your efforts in reviewing our paper . We provide our responses to your concerns point by point as follow : > Pareto-stationary solution 1 . As you correctly point out , the solutions our method can generate are Pareto stationary solutions rather than global Pareto optimal solutions . The Pareto stationary is a necessary condition for Pareto optimality [ 1,2 ] , and more strong requirements are needed to make it a sufficient condition ( e.g. , all functions are convex and all $ \\lambda $ are strictly positive in the dual problem ) [ 1,3 ] . For a general multiobjective optimization problem , no method can guarantee to find a Pareto optimal solution . We will modify our statements accordingly and carefully revise our manuscript to make this point clear . We believe our proposed method is useful for solving the MTL problem with deep neural networks : 2 . This Pareto stationary v.s . Pareto optimal situation is similar to the well-known stationary/local optimal v.s . global optimal situation for training a single-objective deep neural network . The objective function of a deep neural network could be highly non-convex , and the gradient-based methods ( e.g. , SGD and Adam ) can not guarantee to find a global optimal solution . However , these methods are widely-used to train deep neural networks , and achieve promising performance in practice . The experimental results show that our proposed method can generate reasonable trade-off curves for different MTL problems , which could be useful for many real-world MTL applications . 3.Non-convex optimization on deep neural networks [ 4-6 ] is an important and on-going research topic . How to generalize the results from single-objective optimization to multiobjective optimization would be an interesting research direction in future work . > Contribution : `` The proposed method is not so appealing as Pareto-stationary solutions are easy to obtain in many methods . '' 1.We think there is a misunderstanding here . Instead of obtaining a single or a set of finite Pareto stationary solutions by other multiobjective optimization methods , we propose to * * learn the entire trade-off curve for a given multiobjective optimization problem ( which might contain infinite Pareto stationary solutions ) by a single model * * . The result we provide to the user/decision-maker is the model but not a set of finite Pareto stationary solutions . By adjusting the preference vector , decision-makers can obtain their preferred Pareto stationary solution ( s ) in real-time , rather than rerunning the optimization algorithm if none of the finite solutions satisfy their preference . This is a novel contribution for solving multiobjective optimization problems , and could be valuable for many real-world applications . ( There are a few concurrent works , see 5 ) . 2.Under mild conditions , the Pareto set/front for a continuous multiobjective optimization problem is an ( m-1 ) -dimensional manifold in the solution/objective space [ 7 ] . In our proposed methods , the valid set of preference vector is also an ( m-1 ) -dimensional manifold , where m is number of tasks . Our proposed hypernetwork-based model provides a natural mapping from the preference vector to the trade-off curve and supports an efficient end-to-end gradient-based method to train the model . By going through the preference vectors , we can reconstruct the entire trade-off curve as shown in our experiments ( e.g. , Fig 7,8 ) , which provides an intuitive and efficient way to analyze the trade-off among different tasks . The proposed model structure is also a novel contribution of this paper ."}}