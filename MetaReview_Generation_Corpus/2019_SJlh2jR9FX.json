{"year": "2019", "forum": "SJlh2jR9FX", "title": "Learning with Reflective Likelihoods", "decision": "Reject", "meta_review": "The proposed \u201cinput forgetting\u201d problem is interesting, and the reflective likelihood can come to be seen as a natural solution, however the reviewers overall are concerned about the rigor of the paper. Reviewer 2 pointed out a technical flaw and this was addressed, however the reviewers remain unconvinced about the theoretical justification for the approach. One suggestion made by reviewer 1 is to focus on simpler models that can be studied more rigorously. Alternatively, it could be useful to focus on stronger empirical results. The method works in the experiments given, but for example in the imbalanced data experiments, only MLE is compared to as a baseline. I think it would be more convincing to compare against stronger baselines from the literature. If they are orthogonal to the choice of estimator, then it would be even better to show that these baselines + RLL outperforms the baselines + MLE. Alternatively, you mention some challenging tasks like seq2seq, where a convincing demonstration would greatly strengthen the paper. While the paper is not yet ready in its current form, it seems like a promising approach that is worth further exploration.", "reviews": [{"review_id": "SJlh2jR9FX-0", "review_text": "Summary: This paper proposes maximizing the \u201creflective likelihood,\u201d which the authors define as: E_x E_y [log q(y|x) - \\alpha log q(y)] where the expectations are taken over the data, q is the classifier, and \\alpha is a weight on the log q(y) term. The paper derives the reflective likelihood for classification models and unsupervised latent variable models. Choices for \\alpha are also discussed, and connections are made to ranking losses. Results show superior F1 and perplexity in MNIST classification and 20NewsGroups modeling. Pros: I like how the paper frames the reflective likelihood as a ranking loss. It does seem like subtracting off the marginal probability of y from the conditional likelihood should indeed \u2018focus\u2019 the model on the dependent relationship y|x. Can this be further formalized? I would be very interested in seeing a derivation of this kind. I like that the authors test under class imbalance and report F1 metrics in the experiments as it does seem the proposed method operates through better calibration. Cons: My biggest issue with the paper is that I find much of the discussion lacks rigor. I followed the argument through to Equation 3, but then I became confused when the discussion turned to \u2018dependence paths\u2019: \u201cwe want our learning procedure to follow the dependence path\u2014the subspace in \u0398 for which inputs and outputs are dependent. However this dependence path is unknown to us; there is nothing in Eq. 1 that guides learning to follow this dependence path instead of following Eq. 3\u2014the independence path\u201d (p 3). What are these dependence paths? Can they be defined mathematically in a way that is more direct than switching around the KLD directions in Equations 1-3? Surely any conditional model x-->y has a \u2018dependence path\u2019 flowing from y to x, so it seems the paper is trying to make some stronger statement about the conditional structure? Moving on to the proposed reflective likelihood in Equation 4, I could see some connections to Equations 1-3, but I\u2019m not sure how exactly that final form was settled upon. There seems to be a connection to maximum entropy methods? That is, E_x E_y [log q(y|x) - \\alpha log q(y)] = E_x E_y [log q(y|x)] + \\alpha E_y [ -log q(y)] \\approx E_x E_y [log q(y|x)] + \\alpha H[y], if we assume q(y) approximates the empirical distribution of y well. Thus, the objective can be thought of as maximizing the traditional log model probability plus an estimate of the entropy. As there is a long history of maximum entropy methods / classifiers, I\u2019m surprised there were no mentions or references to this literature. Also, I believe there might be some connections to Bayesian loss calibration / risk by viewing \\alpha as a utility function (which is easy to do when it is defined to be data dependent). I\u2019m less sure about this connection though; see Cobb et al. (2018) (https://arxiv.org/abs/1805.03901) and its citations for references. The data sets used in the experiments are also somewhat dissatisfying as MNIST and 20NewsGroups are fairly easy to get high-performing models for. I would have liked to have seen more direct analysis / simulation of what we expect from the reflective likelihood. As I mentioned above, I suspect its really providing gains through better calibration---which the authors may recognize as F1 scores are reported and class imbalance tested---but the word \u2018calibration\u2019 is never mentioned. More direction comparison against calibration methods such as Platt scaling would be make the experiments have better focus. It would be great to show that this method provides good calibration directly during optimization and doesn\u2019t need the post-hoc calibration steps that most methods require. Evaluation: While the paper has some interesting ideas, they are not well defined, making the paper unready for publication. Discussion of the connections to calibration and maximum entropy seems like a large piece missing from the paper\u2019s argument. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your in depth review . We are very happy that you enjoyed the connections we made to ranking losses . We think this was a cool finding as well ! In fact there might be more connections : the coefficient \\alpha can be data-dependent and in that sense it induces a family of regularizers . The step-schedule for alpha ( as defined in Eq.11 of the paper ) corresponds to ranking loss . We conjecture that there might be other connections when carefully choosing other schedules for \\alpha . We think the \u201c finite sample dilemma \u201d derivations address your remark about formalizing the fact that subtracting the marginal will make the optimization focus on capturing the dependencies . Please let us know if you think otherwise . We agree with you on the calibration remark . We also found classification under imbalance to be a natural fit for testing the RLL objective . The results suggest RLL is doing what it is supposed to be doing i.e.strengthen the dependence between x and y . Regarding your remarks on terminology and lack of rigor in the discussion : we clarified the exposition of the paper and formalized the explanation as to why the proposed objective works . Please see the general rebuttal above for details on what changed in this new version . We also hope you will find the time to read the revision . Regarding your comment on connections : thank you for bringing this up ! Although it might seem that there is a connection to maximum entropy methods , there is no such connection unfortunately . We would like to point out that in your derivation , the quantity you define as entropy is not the entropy of q ( y ) . This is because the expectation is not taken under q ( y ) but under p * ( y | x ) -- -the conditional distribution of y given x under the population distribution -- -which is different from q ( y ) . This is why we did not mention connections to maximum entropy methods . Thank you for the reference ! We have not looked into connections to Bayesian loss calibration . We hope we have addressed your concerns . Please let us know if you have other remarks . We would appreciate it if you could read the revision and let us know if we have addressed all your concerns ."}, {"review_id": "SJlh2jR9FX-1", "review_text": "This paper is technically flawed. Here are three key equations from Section 2. The notations are simplified for textual presentation: d \u2013 p_data; d(y|x) \u2013 p_d(y|x); m(y|x) \u2013 p_theta(y|x) max E_x~d E_y~d(y|x) [ log m (y|x) ] (1) max E_x~d { E_y~d(y|x) ) [ log d(y|x) ]} - E_y~d(y|x) [ log m (y|x) ]} (2) max { E_y~d [ log (y) ] - E_y~d log E_x~d(x|y) [ m (y|x) ]} (3) First error is that the \u201cmax\u201d in (2) and (3) should be \u201cmin\u201d. I will assume this minor error is corrected in the following. The equivalence between (1) and (2) is correct and well-known. The reason is that the first entropy term in (2) does not depend on model. The MAJOR ERROR is that (1) is NOT equivalent to (3). Instead, it is equivalent to the following: min { E_y~d [ log d (y) ] - E_y~d E_x~d(x|y) [ log m (y|x) ]} (3\u2019) Notice the swap of \u201cE_x\u201d and \u201clog\u201d. By Jensen\u2019s nequality, we have log E_x~d(x|y) m (y|x) ] > E_x~d(x|y) [ log m (y|x) - E_y~d log E_x~d(x|y) [ m (y|x) ] < - E_y~d E_x~d(x|y) [ log m (y|x) ] So, minimizing (3) amounts to minimizing a lower bound of the correct objective (3\u2019). It does not make sense at all. ", "rating": "2: Strong rejection", "reply_text": "Thank you for taking the time to review the paper . We corrected the statement and refactored the paper to reflect this . We hope you will be able to read the revision and verify that your concerns have been addressed . Please let us know if you have any further questions ."}, {"review_id": "SJlh2jR9FX-2", "review_text": "The paper proposes a modification of maximum likelihood estimation that encourages estimated predictive/conditional models p(z|x) to have low entropy and/or to maximize mutual information between z and x under the model p_{data}(x)p_{model}(z|x). There is pre-existing literature on encouraging low entropy / high mutual information in predictive models, suggesting this can indeed be a good idea. The experiments in the current paper are preliminary but encouraging. However, the setting in which the paper presents this approach (section 2.1) does not make any sense. Also see my previous comments. - Please reconsider your motivation for the proposed method. Why does it work? Also please try to make the connection with the existing literature on minimum entropy priors and maximizing mutual information. - Please try to provide some guarantees for the method. Maximum likelihood estimation is consistent: given enough data and a powerful model it will eventually do the right thing. What will your estimator converge to for infinite data and infinitely powerful models?", "rating": "3: Clear rejection", "reply_text": "Thank you for reviewing our paper . We are very glad you find the idea potentially interesting and that the experimental results are encouraging . In the first version of our paper , we made a wrong statement when justifying our proposed objective . This made the paper very confusing as you mentioned in your review . In light of this feedback , we refactored the paper with a new perspective on why regularizing maximum likelihood with the log reflective probability is a good thing to do . The results did not change because the method is the same . It is the explanation behind why our proposed method works that changed . We replaced the section 2 of version 1 with the paragraph titled \u201c the finite sample dilemma of maximum likelihood \u201d in the introduction . We chose to add this paragraph in the introduction for sake of clarity and because we want the reader to grasp the intuition behind the proposed objective early on . We hope these changes address your earlier concerns . Regarding your question on connections to mutual information : our objective increases the dependence between inputs and outputs as evidenced in the empirical study section and as motivated by the finite sample dilemma of maximum likelihood . In this sense the RLL objective is implicitly related to mutual information which is a measure of dependence . However , there is no mathematical equation that directly relates RLL and mutual information . Regarding your question on asymptotics : for infinite data the criterion in Eq.8 ( which is the RLL ... the one we use ) converges in probability to the difference between the true maximum likelihood objective ( the one using expectations under the population distribution ) and the log reflective-probability ( we define this in the paper ... it is some marginal over the output y ) . In this sense our finite-data objective in Eq.8 is a consistent estimator of the true objective . You get this result using the law of large numbers and continuity of logarithm . We hope we have addressed your concerns . Please let us know if you have further remarks ."}], "0": {"review_id": "SJlh2jR9FX-0", "review_text": "Summary: This paper proposes maximizing the \u201creflective likelihood,\u201d which the authors define as: E_x E_y [log q(y|x) - \\alpha log q(y)] where the expectations are taken over the data, q is the classifier, and \\alpha is a weight on the log q(y) term. The paper derives the reflective likelihood for classification models and unsupervised latent variable models. Choices for \\alpha are also discussed, and connections are made to ranking losses. Results show superior F1 and perplexity in MNIST classification and 20NewsGroups modeling. Pros: I like how the paper frames the reflective likelihood as a ranking loss. It does seem like subtracting off the marginal probability of y from the conditional likelihood should indeed \u2018focus\u2019 the model on the dependent relationship y|x. Can this be further formalized? I would be very interested in seeing a derivation of this kind. I like that the authors test under class imbalance and report F1 metrics in the experiments as it does seem the proposed method operates through better calibration. Cons: My biggest issue with the paper is that I find much of the discussion lacks rigor. I followed the argument through to Equation 3, but then I became confused when the discussion turned to \u2018dependence paths\u2019: \u201cwe want our learning procedure to follow the dependence path\u2014the subspace in \u0398 for which inputs and outputs are dependent. However this dependence path is unknown to us; there is nothing in Eq. 1 that guides learning to follow this dependence path instead of following Eq. 3\u2014the independence path\u201d (p 3). What are these dependence paths? Can they be defined mathematically in a way that is more direct than switching around the KLD directions in Equations 1-3? Surely any conditional model x-->y has a \u2018dependence path\u2019 flowing from y to x, so it seems the paper is trying to make some stronger statement about the conditional structure? Moving on to the proposed reflective likelihood in Equation 4, I could see some connections to Equations 1-3, but I\u2019m not sure how exactly that final form was settled upon. There seems to be a connection to maximum entropy methods? That is, E_x E_y [log q(y|x) - \\alpha log q(y)] = E_x E_y [log q(y|x)] + \\alpha E_y [ -log q(y)] \\approx E_x E_y [log q(y|x)] + \\alpha H[y], if we assume q(y) approximates the empirical distribution of y well. Thus, the objective can be thought of as maximizing the traditional log model probability plus an estimate of the entropy. As there is a long history of maximum entropy methods / classifiers, I\u2019m surprised there were no mentions or references to this literature. Also, I believe there might be some connections to Bayesian loss calibration / risk by viewing \\alpha as a utility function (which is easy to do when it is defined to be data dependent). I\u2019m less sure about this connection though; see Cobb et al. (2018) (https://arxiv.org/abs/1805.03901) and its citations for references. The data sets used in the experiments are also somewhat dissatisfying as MNIST and 20NewsGroups are fairly easy to get high-performing models for. I would have liked to have seen more direct analysis / simulation of what we expect from the reflective likelihood. As I mentioned above, I suspect its really providing gains through better calibration---which the authors may recognize as F1 scores are reported and class imbalance tested---but the word \u2018calibration\u2019 is never mentioned. More direction comparison against calibration methods such as Platt scaling would be make the experiments have better focus. It would be great to show that this method provides good calibration directly during optimization and doesn\u2019t need the post-hoc calibration steps that most methods require. Evaluation: While the paper has some interesting ideas, they are not well defined, making the paper unready for publication. Discussion of the connections to calibration and maximum entropy seems like a large piece missing from the paper\u2019s argument. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you for your in depth review . We are very happy that you enjoyed the connections we made to ranking losses . We think this was a cool finding as well ! In fact there might be more connections : the coefficient \\alpha can be data-dependent and in that sense it induces a family of regularizers . The step-schedule for alpha ( as defined in Eq.11 of the paper ) corresponds to ranking loss . We conjecture that there might be other connections when carefully choosing other schedules for \\alpha . We think the \u201c finite sample dilemma \u201d derivations address your remark about formalizing the fact that subtracting the marginal will make the optimization focus on capturing the dependencies . Please let us know if you think otherwise . We agree with you on the calibration remark . We also found classification under imbalance to be a natural fit for testing the RLL objective . The results suggest RLL is doing what it is supposed to be doing i.e.strengthen the dependence between x and y . Regarding your remarks on terminology and lack of rigor in the discussion : we clarified the exposition of the paper and formalized the explanation as to why the proposed objective works . Please see the general rebuttal above for details on what changed in this new version . We also hope you will find the time to read the revision . Regarding your comment on connections : thank you for bringing this up ! Although it might seem that there is a connection to maximum entropy methods , there is no such connection unfortunately . We would like to point out that in your derivation , the quantity you define as entropy is not the entropy of q ( y ) . This is because the expectation is not taken under q ( y ) but under p * ( y | x ) -- -the conditional distribution of y given x under the population distribution -- -which is different from q ( y ) . This is why we did not mention connections to maximum entropy methods . Thank you for the reference ! We have not looked into connections to Bayesian loss calibration . We hope we have addressed your concerns . Please let us know if you have other remarks . We would appreciate it if you could read the revision and let us know if we have addressed all your concerns ."}, "1": {"review_id": "SJlh2jR9FX-1", "review_text": "This paper is technically flawed. Here are three key equations from Section 2. The notations are simplified for textual presentation: d \u2013 p_data; d(y|x) \u2013 p_d(y|x); m(y|x) \u2013 p_theta(y|x) max E_x~d E_y~d(y|x) [ log m (y|x) ] (1) max E_x~d { E_y~d(y|x) ) [ log d(y|x) ]} - E_y~d(y|x) [ log m (y|x) ]} (2) max { E_y~d [ log (y) ] - E_y~d log E_x~d(x|y) [ m (y|x) ]} (3) First error is that the \u201cmax\u201d in (2) and (3) should be \u201cmin\u201d. I will assume this minor error is corrected in the following. The equivalence between (1) and (2) is correct and well-known. The reason is that the first entropy term in (2) does not depend on model. The MAJOR ERROR is that (1) is NOT equivalent to (3). Instead, it is equivalent to the following: min { E_y~d [ log d (y) ] - E_y~d E_x~d(x|y) [ log m (y|x) ]} (3\u2019) Notice the swap of \u201cE_x\u201d and \u201clog\u201d. By Jensen\u2019s nequality, we have log E_x~d(x|y) m (y|x) ] > E_x~d(x|y) [ log m (y|x) - E_y~d log E_x~d(x|y) [ m (y|x) ] < - E_y~d E_x~d(x|y) [ log m (y|x) ] So, minimizing (3) amounts to minimizing a lower bound of the correct objective (3\u2019). It does not make sense at all. ", "rating": "2: Strong rejection", "reply_text": "Thank you for taking the time to review the paper . We corrected the statement and refactored the paper to reflect this . We hope you will be able to read the revision and verify that your concerns have been addressed . Please let us know if you have any further questions ."}, "2": {"review_id": "SJlh2jR9FX-2", "review_text": "The paper proposes a modification of maximum likelihood estimation that encourages estimated predictive/conditional models p(z|x) to have low entropy and/or to maximize mutual information between z and x under the model p_{data}(x)p_{model}(z|x). There is pre-existing literature on encouraging low entropy / high mutual information in predictive models, suggesting this can indeed be a good idea. The experiments in the current paper are preliminary but encouraging. However, the setting in which the paper presents this approach (section 2.1) does not make any sense. Also see my previous comments. - Please reconsider your motivation for the proposed method. Why does it work? Also please try to make the connection with the existing literature on minimum entropy priors and maximizing mutual information. - Please try to provide some guarantees for the method. Maximum likelihood estimation is consistent: given enough data and a powerful model it will eventually do the right thing. What will your estimator converge to for infinite data and infinitely powerful models?", "rating": "3: Clear rejection", "reply_text": "Thank you for reviewing our paper . We are very glad you find the idea potentially interesting and that the experimental results are encouraging . In the first version of our paper , we made a wrong statement when justifying our proposed objective . This made the paper very confusing as you mentioned in your review . In light of this feedback , we refactored the paper with a new perspective on why regularizing maximum likelihood with the log reflective probability is a good thing to do . The results did not change because the method is the same . It is the explanation behind why our proposed method works that changed . We replaced the section 2 of version 1 with the paragraph titled \u201c the finite sample dilemma of maximum likelihood \u201d in the introduction . We chose to add this paragraph in the introduction for sake of clarity and because we want the reader to grasp the intuition behind the proposed objective early on . We hope these changes address your earlier concerns . Regarding your question on connections to mutual information : our objective increases the dependence between inputs and outputs as evidenced in the empirical study section and as motivated by the finite sample dilemma of maximum likelihood . In this sense the RLL objective is implicitly related to mutual information which is a measure of dependence . However , there is no mathematical equation that directly relates RLL and mutual information . Regarding your question on asymptotics : for infinite data the criterion in Eq.8 ( which is the RLL ... the one we use ) converges in probability to the difference between the true maximum likelihood objective ( the one using expectations under the population distribution ) and the log reflective-probability ( we define this in the paper ... it is some marginal over the output y ) . In this sense our finite-data objective in Eq.8 is a consistent estimator of the true objective . You get this result using the law of large numbers and continuity of logarithm . We hope we have addressed your concerns . Please let us know if you have further remarks ."}}