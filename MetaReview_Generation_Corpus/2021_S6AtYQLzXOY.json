{"year": "2021", "forum": "S6AtYQLzXOY", "title": "Learning Self-Similarity in Space and Time as a Generalized Motion for Action Recognition", "decision": "Reject", "meta_review": "This paper presents an approach to use spatio-temporal self-similarity (STSS) as a feature for a convolutional neural network for video understanding. The proposed approach extracts STSS as a descriptor capturing similarities between local spatio-temporal regions, and adds conventional layers such as soft-argmax, fully connected layers, and conv. layers on top of it.\n\nOn one hand, all of the reviewers agree that the novelty of the paper is limited. On the other hand, most of the reviewers (except R1) appreciated thoroughness of the experiments and ablations. In the end, the reviewers gave 3 marginally above the acceptance threshold ratings and 1 marginally below the threshold rating.\n\nThe AC views this paper as a borderline paper. None of the reviewers are excited about the paper, and it is a typical \"Nice experiments with limited novelty\" (by R1) paper. The concept of the STSS itself was already proposed in prior studies as mentioned in the paper and by the reviewers, and this paper 'engineers' a new way to take advantage of STSS without further theoratical or conceptual justifications on why it should work. The newly added Kinetics and HMDB results in the rebuttal are nice, but the impact of STSS seems to be minimal in these results.\n\nOverall, the AC find the paper slighly lacking to be considered for ICLR.", "reviews": [{"review_id": "S6AtYQLzXOY-0", "review_text": "The paper proposes a novel nerual block based on the classifical spatio-temporal self-similarity ( STSS ) , named SELFY , which can be easily inserted into nerual architectures and learned end-to-end without additional supervision . SELFY could capture long-term interaction and fast motion in the video with a sufficient volume of the neighborhood in time and space . The nice point of the method is that it is heavily investigtated through experiments . Evaluated on three standard action recognition benchmark datasets , the proposed SELFY is demonstrated the superiority over previous methods for motion modeling as well as the complementarity to spatio-temporal features from direct convolution . Moreover , the paper is clear and seems correct , technically . comments : -The first concern is about the limited novelty of the method . Despite revisting the self-similarity , from section 3.1 , the learning of generalized and long-term information is a property of the STSS rather than a contribution of this work . If necessary , authors should give more details on the classifical STSS to show the improvement . -The authors briefly mention the differences between the new proposed and non-local approaches ( Wang et al. , 2018 ; Liu et al. , 2019 ) and correlation-based methods ( Wang et al. , 2020 ; Kwon et al. , 2020 ) , which appears inadequate and is better to integrate them into a comparison in the form of Figure 1 . -How are parameters ( 5 , 9 , 9 ) and ( 9 , 9 ,9 ) chosen ? The best result occurs as the temporal offsets being chosen from { -3 , ... , 3 } . Will there be better accuracy when the range is larger ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "> [ R1 ] The authors briefly mention the differences between the new proposed and non-local approaches ( Wang et al. , 2018 ; Liu et al. , 2019 ) and correlation-based methods ( Wang et al. , 2020 ; Kwon et al. , 2020 ) , which appears inadequate and is better to integrate them into a comparison in the form of Figure 1 . While computing the self-similarity values as ours , the non-local approaches [ 3,4 ] use them as attention weights for feature aggregation by multiplying them to the visual features [ 3 ] or aligning top- $ K $ corresponding features [ 4 ] ; they both do not use STSS itself as a relational representation . In contrast , our method does it indeed and learns a more powerful relational feature from STSS . As for the performance comparison , Table 7a of our appendix shows that our method clearly outperforms non-local approaches in top-1 , top-5 accuracy and FLOPs . Furthermore , while the correlation-based methods [ 5,6 ] extract motion features between two adjacent frames only and are thus limited to short-term motion , our method effectively captures bi-directional and long-term motion information via learning with the sufficient volume of STSS . Our method can also exploit richer information from the self-similarity values than other methods . MS module [ 5 ] only focuses on the maximal similarity value of the $ ( U , V ) $ dimension to extract flow information , and Correlation block [ 6 ] uses an $ 1 \\times 1 $ convolution layer for extracting motion features from the similarity values . In contrast to the two methods , we introduce a generalized motion learning framework using the self-similarity tensor at Sec 3.2 of our paper and demonstrate the effectiveness of our method through ablation studies . Following the reviewers \u2019 suggestion , we will add a figure that describes these differences between the previous ones and ours in our final manuscript . [ 3 ] X. Wang , R. Girshick , A. Gupta , and K. He . \u201c Non-local neural networks. \u201d CVPR . 2018.\\ [ 4 ] X. Liu , J.Y . Lee , and H. Jin . \u201c Learning video representations from correspondence proposals. \u201d CVPR . 2019.\\ [ 5 ] H. Wang , D. Tran , L. Torrensani , and M. Feiszli . \u201c Video modeling with correlation networks. \u201d CVPR . 2020.\\ [ 6 ] H. Kwon , M. Kim , S. Kwak , and M. Cho . \u201c MotionSqueeze : neural motion feature learning for video understanding. \u201d ECCV . 2020. > [ R1 ] How are parameters ( 5 , 9 , 9 ) and ( 9 , 9 ,9 ) chosen ? The best result occurs as the temporal offsets being chosen from { -3 , ... , 3 } . Will there be better accuracy when the range is larger ? We set the matching region $ ( L , U , V ) $ to $ ( 5,9,9 ) $ and $ ( 9,9,9 ) $ since they perform the best accuracy-computation trade-offs . When we use 8 frames as the input , the accuracy saturates as the $ ( L , U , V ) $ volume becomes larger than $ ( 5,9,9 ) $ , resulting in worse trade-offs . When increasing the input frames from 8 to 16 , we simply extend $ L $ to 9 maintaining the time span of STSS . Table B summarizes the SS-V1 results of FLOPs and accuracies varying $ ( L , U , V ) $ given 8 RGB frames . Table B : * * Performance comparison with different matching regions of SELFY block . * * |model| $ ( L , U , V ) $ |FLOPs ( G ) |top-1|top-5| | : | : :| : :| : :| : :| |TSM-R18 |-|14.6|43.0|72.3| |SELFYNet| $ ( 1,9,9 ) $ |15.3|47.1|76.3| |SELFYNet| $ ( 3,9,9 ) $ |16.3|47.8|76.7| |SELFYNet| $ ( 5,9,9 ) $ |17.3|48.4|77.6| |SELFYNet| $ ( 7,9,9 ) $ |18.3| * * 48.6 * * |77.7| |SELFYNet| $ ( 11,9,9 ) $ |20.2|48.4|77.6| |SELFYNet| $ ( 5,5,5 ) $ |17.1|47.8|77.1| |SELFYNet| $ ( 5,13,13 ) $ |18.4|48.4|77.8| |SELFYNet| $ ( 5,17,17 ) $ |19.8| * * 48.6 * * | * * 78.3 * * |"}, {"review_id": "S6AtYQLzXOY-1", "review_text": "# # # Summary : This submission proposed a motion representation method based on spatio-temporal self-similarity ( STSS ) , which represents each local region as similarities to its neighbors in both spatial and temporal dimension . There are previous works ( e.g. , Ref [ 1 ] , [ 2 ] , [ 5 ] listed here ) which utilize STSS for feature extractions , authors claim that this work is the first one to learn STSS representation based on modern CNN architecture . The proposed method is implemented as a neural block , i.e. , SELFY , which can be applied into neural architectures and learned end-to-end without additional supervision . On 3 standard human action recognition data sets , Something-Something-V1 & V2 , Diving-48 , and FineGym , the proposed method achieves quite good empirical results . # # # Strengths & Originality : The idea of utilizing patio-temporal self-similarity ( STSS ) for feature representation in the modern CNN framework for human action recognition is interesting . I also like the concept that `` fix our attention to the similarity map to the very next frame within STSS and attempt to extract a single displacement vector to the most likely position at the frame , the problem reduces to optical flow ...... In contrast , we leverage the whole volume of STSS ...... '' , as to view the proposed method as a generalized & rich optical flow . Furthermore , STSS should be helpful for view invariant action recognition , i.e. , one of the fundamental challenges from video data recognition . Empirical results on Something-Something V1 & V2 , Diving-48 , FineGym show that the proposed achieves the state-of-the-art results , though seems marginally . For example , the proposed Ensemble model ( SELFYNet-TSM-R50EN ) achieved 56.6 % and 67.7 % attop-1 accuracy for V1 & V2 , vs. the MSNet-TSM-R50EN got quite similar performance as 55.1 % and 67.1 % ( Kwon et al. , 2020 , Ref . [ 3 ] ) .On FineGym , SELFYNet-TSM-R50 achieves 49.5 % and 87.7 % , these do look better & more clearly , compared with 46.5 % , 81.2 % reported from ( Shao et al , 2020 , Ref . [ 4 ] ) . # # # Weakness : From Figure . 2 or Section 3.2.2 , the final STSS representation Z is the combination of original input V and the STSS feature F ( S ( V ) ) , so kind of unclear how much contribution is made from the original V in terms of the final recognition performance . Maybe this part is addressed in the experimental section somewhere . The proposed neural block , SELFY , includes 3 parts as self-similarity transformation , feature extraction , and feature integration , it seems none of them is original but the combination of these put into the end-of-end NN structure is new as claimed by authors . There are some additional experiment results presented in section 4.3 ( for different types of similarity , and for different feature extraction ) , still , it seems unclear how important or sensitive for each step within the whole framework based on TSN ResNets and TSM ResNets , given the complexity here . Self-similarity can be applied for both image and videos ( i.e. , Section 3 or Ref [ 5 ] , etc ) , I know the focus of this submission is video action recognition , still , seems interesting to know the proposed framework `` 3 steps combined + CNN end-of-end '' apply to image object recognition . If that also achieved good results compared with state-of-the-art for object recognition , it will be a strong support for the proposed methodology , and if not , which is also good to know & could be multiple reasons behind . Besides these video based action benchmarks , it should be interesting to see results on a depth enriched data set ( i.e. , RGB-D ) , as missing depth information is one of the limitations from video data . Ideally , we should see similar good performance if the proposed methodology is effective for representation learning . # # # Reference : 1 . Videos as Space-Time Region Graphs , Xiaolong Wang , Abhinav Gupta , Proc . European Conference on Computer Vision ( ECCV ) 2018 2 . TSM : Temporal Shift Module for Efficient Video Understanding , Ji Lin , Chuang Gan , and Song Han , Proc . IEEE International Conference on Computer Vision ( ICCV ) , 2019 3 . MotionSqueeze : Neural Motion Feature Learning for Video Understanding , Heeseung Kwon , Manjin Kim , Suha Kwak , Minsu Cho , Proc . European Conference on Computer Vision ( ECCV ) , 2020 4 . Dian Shao , Yue Zhao , Bo Dai , and Dahua Lin . Finegym : A hierarchical video dataset for finegrained action understanding . In Proc.IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , 2020 5 . Imran N. Junejo , Emilie Dexter , Ivan Laptev and Patrick Perez , Cross-View Action Recognition from TemporalSelf-Similarities , Proc . European Conference on Computer Vision ( ECCV ) 2008", "rating": "6: Marginally above acceptance threshold", "reply_text": "> [ R3 ] Self-similarity can be applied for both image and videos ( i.e. , Section 3 or Ref [ 5 ] , etc ) , I know the focus of this submission is video action recognition , still , seems interesting to know the proposed framework `` 3 steps combined + CNN end-of-end '' apply to image object recognition . If that also achieved good results compared with state-of-the-art for object recognition , it will be a strong support for the proposed methodology , and if not , which is also good to know & could be multiple reasons behind . We agree with the point and find that learning self-similarity is also effective in image classification . We validated it on CIFAR-10 and -100 , which are the popular image classification benchmarks . For the experiments , we organize two basic blocks : a light-weighted version of SELFY block ( SELFY $ _l $ ) and a spatial convolutional block ( SCB ) with four 3x3 spatial convolutional layers as the counterpart . Both blocks have the same receptive field as ( $ 9 \\times 9 $ ) , and the similar FLOPs . We insert each of the basic blocks and their combinations ( please refer to Fig.4c,4d , and 4f of our paper ) into the middle of ResNet20 . Table D summarizes the results on CIFAR-10 and -100 . ResNet20 , which is our baseline , performs as 8.44 \\ % p and 30.34 \\ % p at top-1 error on CIFAR-10 and -100 , respectively . ResNet20 with SELFY $ _l $ reduces the top-1 errors to 1.1 . \\ % p and 1.98 \\ % p on the both benchmarks . The results are in alignment with those in Table 3 of our paper ; SELFYNet with $ l=\\ { 0\\ } $ that learns spatial self-similarity improves the accuracy on both TSN and TSM backbones . Interestingly , ResNet20 with SEFLY $ _l $ is more accurate than ResNet with SCB . Furthermore , in terms of the block combinations ( 4th - 6th rows ) , SELFY $ _l $ + SCB and SELFY $ _l $ + SELFY $ _l $ show the lowest top-1 error rate as 64.7 \\ % p and 27.50 \\ % p on CIFAR-10 and -100 , respectively . These results indicate that the spatial self-similarity features and the ordinary visual features are complementary to each other also on image classification , which is consistent with the results in Table 5 of our paper . Table D : * * The effect of spatial self-similarity for image classification . * * |model|FLOPS ( G ) | # params ( K ) |CIFAR-10 err-1|CIFAR-100 err-1| | : | : :| : :| : :| : :| |baseline ( ResNet20 ) |35.79|220|8.44|30.34| | $ + $ SCB|50.41|277|7.81|28.77| | $ + $ SELFY $ _l $ |49.99|254|7.34|28.36| | $ + $ SCB $ + $ SCB |65.02|333|7.17|27.83| | $ + $ SELFY $ _l $ $ + $ SELFY $ _l $ |64.16|287|6.79| * * 27.50 * * | | $ + $ SELFY $ _l $ $ + $ SCB |64.59|310| * * 6.47 * * |27.59| > [ R3 ] Besides these video based action benchmarks , it should be interesting to see results on a depth enriched data set ( i.e. , RGB-D ) , as missing depth information is one of the limitations from video data . Ideally , we should see similar good performance if the proposed methodology is effective for representation learning . We also agree with the point , and thus , we \u2019 ve been conducting the experiments on the depth enriched dataset , e.g.NTU RGB+D , to validate our methods . We will leave the comment as soon as the experiments are finished ."}, {"review_id": "S6AtYQLzXOY-2", "review_text": "The paper introduces SELFY , a neural module that learns spatio-temporal self-similarity across longer timescales in both directions to obtain visual features that provide consistent empirical gains on three action recognition datasets . Ablation studies show that modeling longer , bi-directional motion similarity can help handle motion blurs and ( artificially induced ) occlusion . Strengths : + The paper is well written and shows strong empirical gains over prior SOTA . + The ablation experiments on the effect of temporal length and the different feature extraction methods are nicely done and show the effect of different design choices . + The experiments for testing the robustness of the learned representations is nice to see and helps highlight the strength of the proposed approach . Concerns : - From what I can see , the contribution is in the modeling of ( motion ) similarity in both directions , across a longer time scale . My concern is that all of the chosen datasets are very motion-centric where temporal relationships are more important and this makes it somewhat advantageous to the proposed model and does not really allow us to ascertain its applicability to datasets where appearance plays a bigger role such as Kinetics or HMDB-51 . In fact , the results on SS-V1 and SS-V2 show some gains ( ~2-3 % ) compared to the very closely related MotionSqueeze Network ( Kwon et al , ECCV 2020 ) which computes correlation ( rather than similarity ) between adjacent frames ( t and t+1 ) . MSNet did not see any significant advances in datasets ( HMDB-51 and Kinetics ) where appearance plays a greater role and I am not sure how this proposed approach would help in those conditions . - I assume the function sim ( . ) from Equation 1 is cosine similarity since it was not explicitly mentioned anywhere . Is there any specific reason the correlation function was not used ? What is the effect of using correlation in place of cosine similarity ? It would interesting to see the effect of this since it would essentially allow us to see how modeling the longer temporal context would affect MS Net ( the most closely related network ) and help highlight the contribution of the proposed model beyond the obvious empirical gains . - While the robustness experiments test out the occlusion setting by cutting out a rectangular patch of a single center-frame , that does not , IMHO , shows robustness to occlusion since the features from surrounding frames ( on both sides ) will help mitigate this artificial occlusion . In practice , the occlusion can have a larger effect since it would not zero out the appearance but will add some noise into the feature extraction and hence provide a stronger challenge . Experiments on HMDB51 , which has strong camera motion and hence introduces natural occlusions , would be a stronger experiment to show the generalization of the model beyond datasets with a strong motion-centric bias .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for appreciating our work and providing constructive comments . Below are our responses and we will reflect them in our final manuscript . > [ R2 ] My concern is that all of the chosen datasets are very motion-centric where temporal relationships are more important and this makes it somewhat advantageous to the proposed model and does not really allow us to ascertain its applicability to datasets where appearance plays a bigger role such as Kinetics or HMDB-51 . We are conducting the experiments on the appearance-oriented datasets . We will leave the comment as soon as the experiments are finished . > [ R2 ] I assume the function sim ( . ) from Equation 1 is cosine similarity since it was not explicitly mentioned anywhere . Is there any specific reason the correlation function was not used ? What is the effect of using correlation in place of cosine similarity ? As pointed out , we use the cosine similarity as our default similarity function . We have explored the effect of different similarity functions on learning STSS , and found that the cosine similarity performs the best . Table A compares the performance on SS-V1 with different similarity functions : cosine similarity ( ours ) , dot product [ 1 ] , and embedded Gaussian [ 2 ] . The experimental details are the same as those in Appendix A , except that we reduce the channel dimension $ C $ of appearance feature $ \\mathbf { V } $ to 32 for the less GPU memory consumption . The cosine similarity gives better accuracy , but not much , by 0.1 \\ % p and 0.2 \\ % p than the dot product and the embedded Gaussian at top-1 accuracy . We will clarify this point in our final manuscript . Table A : * * Results of different similarity functions . * * |model|\\ # channels|similarity function|top-1|top-5| | : | : :| : :| : :| : :| |TSM-R18|-|-|43.0|72.3| |SELFYNet ( ours ) |32|cosine| * * 47.8 * * | * * 77.1 * * | |SELFYNet|32|dot product|47.7|76.6| |SELFYNet|32|embedded Gaussian|47.6|76.8| [ 1 ] P. Fischer , et al. \u201c FlowNet : learning optical flow with convolutional networks. \u201d ICCV . 2015.\\ [ 2 ] X. Wang , R. Girshick , A. Gupta , and K. He . \u201c Non-local neural networks. \u201d CVPR . 2018. > [ R2 ] It would be interesting to see the effect of this since it would essentially allow us to see how modeling the longer temporal context would affect MS Net ( the most closely related network ) and help highlight the contribution of the proposed model beyond the obvious empirical gains . Below are the comparisons between SELFYNet and MSNet [ 3 ] to demonstrate the effectiveness of STSS as well as our feature extraction methods . For an apple-to-apple comparison , we apply the kernel soft-argmax and the max-pooling operations ( * KS+CM * in [ 3 ] ) to the feature extraction method by following their official github code . Please note that their official code also uses the cosine similarity as the same as ours . As we restrict the temporal offset $ l $ of the SELFY variant to $ \\ { +1 \\ } $ , it is equivalent to MS module of which * feature transformation layers * are the standard 2D conv layers . All experimental details are the same as those in Appendix A . Table B summarizes the results on SS-V1 . KS+CM method achieves 46.1 \\ % p at top-1 accuracy . As we enlarge the temporal window $ L $ to 5 , we obtain an additional gain as 1.3 \\ % p . The learnable convolution layers improve top-1 accuracy by 1.0 \\ % p in both cases . We will add this experiment in our final manuscript . Table B : * * Performance comparison with MSNet . * * | model | feature extraction method | $ ( L , U , V ) $ | top-1 | top-5 | | : | : :| : :| : | : | |TSM-R18|-|-|43.0|72.3| |SELFYNet|KS+CM| $ ( 1,9,9 ) $ |46.1|75.3| |SELFYNet|KS+CM| $ ( 5,9,9 ) $ |47.4|76.8| |SELFYNet|conv| $ ( 1,9,9 ) $ |47.1|76.3| |SELFYNet|conv| $ ( 5,9,9 ) $ | * * 48.4 * * | * * 77.6 * * | [ 3 ] H. Kwon , M. Kim , S. Kwak , and M. Cho . \u201c MotionSqueeze : neural motion feature learning for video understanding. \u201d ECCV . 2020. > [ R2 ] the occlusion can have a larger effect since it would not zero out the appearance but will add some noise into the feature extraction and hence provide a stronger challenge . Experiments on HMDB51 , which has strong camera motion and hence introduces natural occlusions , would be a stronger experiment to show the generalization of the model beyond datasets with a strong motion-centric bias . We are conducting the experiments on HMDB51 . We will leave the comment as soon as the experiments are finished ."}, {"review_id": "S6AtYQLzXOY-3", "review_text": "# # # # General This paper proposes spatio-temporal self-similarity ( STSS ) , which captures structural patterns in space and time , for action recognition from videos . Overall , I would like to recommend ICLR to accept the paper . Pros and Cons I found in the paper are summarized as follows . # # # # Pros . 1.SELFY , the proposed neural block that implements STSS , is a good extention and adjustment of prior self-similarity works to modern CNNs , resulted in achieving state-of-the-art performance on different datasets . 1.The ablation study reveals that SELFY with a long temporal range successfully incorporate the temporal information without explicitly exploiting optical flows . 1.The paper is generally well and clearly written . 1.The literature review is thorough , and the work is well contextualized in the literatures . # # # # Cons 1 . The novelty and the originality of the paper may be relatively low because the concept of the STSS itself was already proposed in prior studies as mentioned in the paper . 1.The reasoning or intuition behind some of the design choices are not always clear . For example , - The output of the feature extraction block is $ F \\in \\mathbb { R } ^ { T \\times X \\times Y \\times L \\times C_F } $ , and the authors say > The dimension of L is preserved to extract motion information across different temporal offsets in a consistent manner . But the motion information across different temporal offsets can be extracted in the feature extraction module if MLP or convolution is employed in the feature extraction block . In other words , even if the output is $ F \\in \\mathbb { R } ^ { T \\times X \\times Y \\times C_F } $ , the motion information can be encoded in the output tensor . Possibly the authors empirically found the present design choice is better . If so , how does the performance change if $ L $ is not preserved ? - In the feature integration block , firstly $ 3 \\times 3 $ spatial convolution kernel is applied , then the temporal offset ( $ L $ ) and the channels ( $ C^ * _F $ ) dimension is flattened , and finally $ 1 \\times 1 \\times 1 $ spatio-temporal convolution is applied . Is this design chosen experimentally or is there any intuition behind ? # # # # Minor comments 1 . It may be interesting to discuss the relationship with self-attention based networks such as [ 1-3 ] 1 . About Table 7 in Appendix , how is the result if SELFY block is used in $ res_1 $ and $ res_5 $ ? I guess these option do not work well , but I think it is beneficial to list all the results . [ 1 ] Hu+ , Local Relation Networks for Image Recognition , ICCV 2019 [ 2 ] Ramachandran+ , Stand-Alone Self-Attention in Vision Models , NeurIPS 2019 [ 3 ] Zhao+ , Exploring Self-attention for Image Recognition , CVPR 2020", "rating": "6: Marginally above acceptance threshold", "reply_text": "> [ R4 ] In the feature integration block , firstly 3\u00d73 spatial convolution kernel is applied , then the temporal offset ( L ) and the channels ( CF\u2217 ) dimension is flattened , and finally 1\u00d71\u00d71 spatio-temporal convolution is applied . Is this design chosen experimentally or is there any intuition behind ? Our intuition is that each of $ L $ channel-groups in the flattened $ LC_ { F^\u2217 } $ -dimensional STSS representation can be viewed as a spatial self-similarity feature within each relative temporal offset so that the $ 1 \\times 1 \\times 1 $ convolutional layer , i.e.fully connected layer , is able to reduce the flattened feature to the $ C $ -dimensional feature vector considering the information of different temporal offsets . We also empirically found that a pooling operation such as max pooling or average pooling , which reduces $ \\mathbf { F } \\in \\mathbb { R } ^ { T \\times X \\times Y \\times L \\times C_ { F^ * } } $ along $ L $ dimension , significantly degrades the performance . We conjecture that such pooling operations , which get rid of the information of temporal offsets , damage long-range temporal modeling . > [ R4 ] It may be interesting to discuss the relationship with self-attention based networks such as [ 1-3 ] . The local self-attention [ 6,7,8 ] and our method have a common denominator of using the self-similarity tensor but use it in a very different way and purpose . The local self-attention mechanism aims to aggregate the local context features using the self-similarity tensor and it thus uses the self-similarity values as attention weights for feature aggregation . However , our method aims to learn a generalized motion representation from the local STSS , so the final STSS representation is directly fed into the neural network instead of multiplying it to local context features . For a clear comparison , we conduct an ablation experiment as follows . We add a single * spatio-temporal * local self-attention layer , extended from [ 7 ] , after $ res_3 $ followed by feature integration layers in Sec.3.2.2 . All experimental details are the same as those in Appendix A , except that we reduce the channel dimension $ C $ of appearance feature $ \\mathbf { V } $ to 32 . Table C summarizes the results on SS-V1 . The spatio-temporal local self-attention layer is accurate as 43.8 \\ % p at top-1 accuracy , and both of SELFY blocks using the embedded Gaussian and the cosine similarity outperform the local self-attention by achieving top-1 accuracy as 47.6 \\ % p and 47.8 \\ % p , respectively . These results are in alignment with the prior work [ 9 ] , which reveals that the self-attention mechanism hardly captures motion features in video . We will add this to the discussion section in our final manuscript . Table C : * * Performance comparison with the local self-attention . * * R.P.E.is an abbreviation for relative positional embeddings . |model|similarity function|feature extraction method|top-1|top-5| | : | : :| : :| : :| : :| |TSM-R18|-|-|43.0|72.3| |SELFYNet|embedded Gaussian|multiplication with $ \\mathbf { V } $ +R.P.E|43.8|72.9| |SELFYNet|embedded Gaussian|conv|47.6|76.8| |SELFYNet ( ours ) |cosine|conv| * * 47.8 * * | * * 77.1 * * | [ 6 ] H. Hu , Z. Zhang , Z. Xie , and S. Lin . \u201c Local relation networks for image recognition. \u201d ICCV . 2019.\\ [ 7 ] P. Ramachandran , N. Parmar , A. Vaswani , I. Vello , A. Levskaya , and J. Shlens . \u201c Stand-alone self-attention in vision models. \u201d NIPS . 2019.\\ [ 8 ] H. Zhao , J. Jia , and V. Koltun . \u201c Exploring self-attention for image recognition. \u201d CVPR . 2020.\\ [ 9 ] X. Liu , J.Y . Lee , and H. Jin . \u201c Learning video representations from correspondence proposals. \u201d CVPR . 2019. > [ R4 ] About Table 7 in Appendix , how is the result if SELFY block is used in res1 and res5 ? I guess these options do not work well , but I think it is beneficial to list all the results . We conducted the additional experiments and the results on SS-V1 are in Table D below , where we add the SELFY block to the different places . Here , we set the spatial resolution of $ \\mathbf { V } $ to $ 14 \\times 14 $ . The SELFY block after $ pool_1 $ improves 2.7 \\ % p at top-1 accuracy , while the SELFY block after $ res_5 $ does not . We conjecture that the spatial resolution ( $ 7 \\times 7 $ ) is too small to extract meaningful motion features for action recognition . We will update Table 7 in our manuscript . Table D : * * Performance comparison with different positions of SELFY block . * * |model|position|top-1|top-5| | : |||| |TSM-R18|-|43.0|72.3| |SELFYNet |after $ pool_1 $ |45.7|74.6| | SELFYNet|after $ res_2 $ |47.2|76.6| |SELFYNet |after $ res_3 $ | * * 48.4 * * | * * 77.6 * * | |SELFYNet |after $ res_4 $ |46.6|76.0| |SELFYNet |after $ res_5 $ |42.8|72.6|"}], "0": {"review_id": "S6AtYQLzXOY-0", "review_text": "The paper proposes a novel nerual block based on the classifical spatio-temporal self-similarity ( STSS ) , named SELFY , which can be easily inserted into nerual architectures and learned end-to-end without additional supervision . SELFY could capture long-term interaction and fast motion in the video with a sufficient volume of the neighborhood in time and space . The nice point of the method is that it is heavily investigtated through experiments . Evaluated on three standard action recognition benchmark datasets , the proposed SELFY is demonstrated the superiority over previous methods for motion modeling as well as the complementarity to spatio-temporal features from direct convolution . Moreover , the paper is clear and seems correct , technically . comments : -The first concern is about the limited novelty of the method . Despite revisting the self-similarity , from section 3.1 , the learning of generalized and long-term information is a property of the STSS rather than a contribution of this work . If necessary , authors should give more details on the classifical STSS to show the improvement . -The authors briefly mention the differences between the new proposed and non-local approaches ( Wang et al. , 2018 ; Liu et al. , 2019 ) and correlation-based methods ( Wang et al. , 2020 ; Kwon et al. , 2020 ) , which appears inadequate and is better to integrate them into a comparison in the form of Figure 1 . -How are parameters ( 5 , 9 , 9 ) and ( 9 , 9 ,9 ) chosen ? The best result occurs as the temporal offsets being chosen from { -3 , ... , 3 } . Will there be better accuracy when the range is larger ?", "rating": "5: Marginally below acceptance threshold", "reply_text": "> [ R1 ] The authors briefly mention the differences between the new proposed and non-local approaches ( Wang et al. , 2018 ; Liu et al. , 2019 ) and correlation-based methods ( Wang et al. , 2020 ; Kwon et al. , 2020 ) , which appears inadequate and is better to integrate them into a comparison in the form of Figure 1 . While computing the self-similarity values as ours , the non-local approaches [ 3,4 ] use them as attention weights for feature aggregation by multiplying them to the visual features [ 3 ] or aligning top- $ K $ corresponding features [ 4 ] ; they both do not use STSS itself as a relational representation . In contrast , our method does it indeed and learns a more powerful relational feature from STSS . As for the performance comparison , Table 7a of our appendix shows that our method clearly outperforms non-local approaches in top-1 , top-5 accuracy and FLOPs . Furthermore , while the correlation-based methods [ 5,6 ] extract motion features between two adjacent frames only and are thus limited to short-term motion , our method effectively captures bi-directional and long-term motion information via learning with the sufficient volume of STSS . Our method can also exploit richer information from the self-similarity values than other methods . MS module [ 5 ] only focuses on the maximal similarity value of the $ ( U , V ) $ dimension to extract flow information , and Correlation block [ 6 ] uses an $ 1 \\times 1 $ convolution layer for extracting motion features from the similarity values . In contrast to the two methods , we introduce a generalized motion learning framework using the self-similarity tensor at Sec 3.2 of our paper and demonstrate the effectiveness of our method through ablation studies . Following the reviewers \u2019 suggestion , we will add a figure that describes these differences between the previous ones and ours in our final manuscript . [ 3 ] X. Wang , R. Girshick , A. Gupta , and K. He . \u201c Non-local neural networks. \u201d CVPR . 2018.\\ [ 4 ] X. Liu , J.Y . Lee , and H. Jin . \u201c Learning video representations from correspondence proposals. \u201d CVPR . 2019.\\ [ 5 ] H. Wang , D. Tran , L. Torrensani , and M. Feiszli . \u201c Video modeling with correlation networks. \u201d CVPR . 2020.\\ [ 6 ] H. Kwon , M. Kim , S. Kwak , and M. Cho . \u201c MotionSqueeze : neural motion feature learning for video understanding. \u201d ECCV . 2020. > [ R1 ] How are parameters ( 5 , 9 , 9 ) and ( 9 , 9 ,9 ) chosen ? The best result occurs as the temporal offsets being chosen from { -3 , ... , 3 } . Will there be better accuracy when the range is larger ? We set the matching region $ ( L , U , V ) $ to $ ( 5,9,9 ) $ and $ ( 9,9,9 ) $ since they perform the best accuracy-computation trade-offs . When we use 8 frames as the input , the accuracy saturates as the $ ( L , U , V ) $ volume becomes larger than $ ( 5,9,9 ) $ , resulting in worse trade-offs . When increasing the input frames from 8 to 16 , we simply extend $ L $ to 9 maintaining the time span of STSS . Table B summarizes the SS-V1 results of FLOPs and accuracies varying $ ( L , U , V ) $ given 8 RGB frames . Table B : * * Performance comparison with different matching regions of SELFY block . * * |model| $ ( L , U , V ) $ |FLOPs ( G ) |top-1|top-5| | : | : :| : :| : :| : :| |TSM-R18 |-|14.6|43.0|72.3| |SELFYNet| $ ( 1,9,9 ) $ |15.3|47.1|76.3| |SELFYNet| $ ( 3,9,9 ) $ |16.3|47.8|76.7| |SELFYNet| $ ( 5,9,9 ) $ |17.3|48.4|77.6| |SELFYNet| $ ( 7,9,9 ) $ |18.3| * * 48.6 * * |77.7| |SELFYNet| $ ( 11,9,9 ) $ |20.2|48.4|77.6| |SELFYNet| $ ( 5,5,5 ) $ |17.1|47.8|77.1| |SELFYNet| $ ( 5,13,13 ) $ |18.4|48.4|77.8| |SELFYNet| $ ( 5,17,17 ) $ |19.8| * * 48.6 * * | * * 78.3 * * |"}, "1": {"review_id": "S6AtYQLzXOY-1", "review_text": "# # # Summary : This submission proposed a motion representation method based on spatio-temporal self-similarity ( STSS ) , which represents each local region as similarities to its neighbors in both spatial and temporal dimension . There are previous works ( e.g. , Ref [ 1 ] , [ 2 ] , [ 5 ] listed here ) which utilize STSS for feature extractions , authors claim that this work is the first one to learn STSS representation based on modern CNN architecture . The proposed method is implemented as a neural block , i.e. , SELFY , which can be applied into neural architectures and learned end-to-end without additional supervision . On 3 standard human action recognition data sets , Something-Something-V1 & V2 , Diving-48 , and FineGym , the proposed method achieves quite good empirical results . # # # Strengths & Originality : The idea of utilizing patio-temporal self-similarity ( STSS ) for feature representation in the modern CNN framework for human action recognition is interesting . I also like the concept that `` fix our attention to the similarity map to the very next frame within STSS and attempt to extract a single displacement vector to the most likely position at the frame , the problem reduces to optical flow ...... In contrast , we leverage the whole volume of STSS ...... '' , as to view the proposed method as a generalized & rich optical flow . Furthermore , STSS should be helpful for view invariant action recognition , i.e. , one of the fundamental challenges from video data recognition . Empirical results on Something-Something V1 & V2 , Diving-48 , FineGym show that the proposed achieves the state-of-the-art results , though seems marginally . For example , the proposed Ensemble model ( SELFYNet-TSM-R50EN ) achieved 56.6 % and 67.7 % attop-1 accuracy for V1 & V2 , vs. the MSNet-TSM-R50EN got quite similar performance as 55.1 % and 67.1 % ( Kwon et al. , 2020 , Ref . [ 3 ] ) .On FineGym , SELFYNet-TSM-R50 achieves 49.5 % and 87.7 % , these do look better & more clearly , compared with 46.5 % , 81.2 % reported from ( Shao et al , 2020 , Ref . [ 4 ] ) . # # # Weakness : From Figure . 2 or Section 3.2.2 , the final STSS representation Z is the combination of original input V and the STSS feature F ( S ( V ) ) , so kind of unclear how much contribution is made from the original V in terms of the final recognition performance . Maybe this part is addressed in the experimental section somewhere . The proposed neural block , SELFY , includes 3 parts as self-similarity transformation , feature extraction , and feature integration , it seems none of them is original but the combination of these put into the end-of-end NN structure is new as claimed by authors . There are some additional experiment results presented in section 4.3 ( for different types of similarity , and for different feature extraction ) , still , it seems unclear how important or sensitive for each step within the whole framework based on TSN ResNets and TSM ResNets , given the complexity here . Self-similarity can be applied for both image and videos ( i.e. , Section 3 or Ref [ 5 ] , etc ) , I know the focus of this submission is video action recognition , still , seems interesting to know the proposed framework `` 3 steps combined + CNN end-of-end '' apply to image object recognition . If that also achieved good results compared with state-of-the-art for object recognition , it will be a strong support for the proposed methodology , and if not , which is also good to know & could be multiple reasons behind . Besides these video based action benchmarks , it should be interesting to see results on a depth enriched data set ( i.e. , RGB-D ) , as missing depth information is one of the limitations from video data . Ideally , we should see similar good performance if the proposed methodology is effective for representation learning . # # # Reference : 1 . Videos as Space-Time Region Graphs , Xiaolong Wang , Abhinav Gupta , Proc . European Conference on Computer Vision ( ECCV ) 2018 2 . TSM : Temporal Shift Module for Efficient Video Understanding , Ji Lin , Chuang Gan , and Song Han , Proc . IEEE International Conference on Computer Vision ( ICCV ) , 2019 3 . MotionSqueeze : Neural Motion Feature Learning for Video Understanding , Heeseung Kwon , Manjin Kim , Suha Kwak , Minsu Cho , Proc . European Conference on Computer Vision ( ECCV ) , 2020 4 . Dian Shao , Yue Zhao , Bo Dai , and Dahua Lin . Finegym : A hierarchical video dataset for finegrained action understanding . In Proc.IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , 2020 5 . Imran N. Junejo , Emilie Dexter , Ivan Laptev and Patrick Perez , Cross-View Action Recognition from TemporalSelf-Similarities , Proc . European Conference on Computer Vision ( ECCV ) 2008", "rating": "6: Marginally above acceptance threshold", "reply_text": "> [ R3 ] Self-similarity can be applied for both image and videos ( i.e. , Section 3 or Ref [ 5 ] , etc ) , I know the focus of this submission is video action recognition , still , seems interesting to know the proposed framework `` 3 steps combined + CNN end-of-end '' apply to image object recognition . If that also achieved good results compared with state-of-the-art for object recognition , it will be a strong support for the proposed methodology , and if not , which is also good to know & could be multiple reasons behind . We agree with the point and find that learning self-similarity is also effective in image classification . We validated it on CIFAR-10 and -100 , which are the popular image classification benchmarks . For the experiments , we organize two basic blocks : a light-weighted version of SELFY block ( SELFY $ _l $ ) and a spatial convolutional block ( SCB ) with four 3x3 spatial convolutional layers as the counterpart . Both blocks have the same receptive field as ( $ 9 \\times 9 $ ) , and the similar FLOPs . We insert each of the basic blocks and their combinations ( please refer to Fig.4c,4d , and 4f of our paper ) into the middle of ResNet20 . Table D summarizes the results on CIFAR-10 and -100 . ResNet20 , which is our baseline , performs as 8.44 \\ % p and 30.34 \\ % p at top-1 error on CIFAR-10 and -100 , respectively . ResNet20 with SELFY $ _l $ reduces the top-1 errors to 1.1 . \\ % p and 1.98 \\ % p on the both benchmarks . The results are in alignment with those in Table 3 of our paper ; SELFYNet with $ l=\\ { 0\\ } $ that learns spatial self-similarity improves the accuracy on both TSN and TSM backbones . Interestingly , ResNet20 with SEFLY $ _l $ is more accurate than ResNet with SCB . Furthermore , in terms of the block combinations ( 4th - 6th rows ) , SELFY $ _l $ + SCB and SELFY $ _l $ + SELFY $ _l $ show the lowest top-1 error rate as 64.7 \\ % p and 27.50 \\ % p on CIFAR-10 and -100 , respectively . These results indicate that the spatial self-similarity features and the ordinary visual features are complementary to each other also on image classification , which is consistent with the results in Table 5 of our paper . Table D : * * The effect of spatial self-similarity for image classification . * * |model|FLOPS ( G ) | # params ( K ) |CIFAR-10 err-1|CIFAR-100 err-1| | : | : :| : :| : :| : :| |baseline ( ResNet20 ) |35.79|220|8.44|30.34| | $ + $ SCB|50.41|277|7.81|28.77| | $ + $ SELFY $ _l $ |49.99|254|7.34|28.36| | $ + $ SCB $ + $ SCB |65.02|333|7.17|27.83| | $ + $ SELFY $ _l $ $ + $ SELFY $ _l $ |64.16|287|6.79| * * 27.50 * * | | $ + $ SELFY $ _l $ $ + $ SCB |64.59|310| * * 6.47 * * |27.59| > [ R3 ] Besides these video based action benchmarks , it should be interesting to see results on a depth enriched data set ( i.e. , RGB-D ) , as missing depth information is one of the limitations from video data . Ideally , we should see similar good performance if the proposed methodology is effective for representation learning . We also agree with the point , and thus , we \u2019 ve been conducting the experiments on the depth enriched dataset , e.g.NTU RGB+D , to validate our methods . We will leave the comment as soon as the experiments are finished ."}, "2": {"review_id": "S6AtYQLzXOY-2", "review_text": "The paper introduces SELFY , a neural module that learns spatio-temporal self-similarity across longer timescales in both directions to obtain visual features that provide consistent empirical gains on three action recognition datasets . Ablation studies show that modeling longer , bi-directional motion similarity can help handle motion blurs and ( artificially induced ) occlusion . Strengths : + The paper is well written and shows strong empirical gains over prior SOTA . + The ablation experiments on the effect of temporal length and the different feature extraction methods are nicely done and show the effect of different design choices . + The experiments for testing the robustness of the learned representations is nice to see and helps highlight the strength of the proposed approach . Concerns : - From what I can see , the contribution is in the modeling of ( motion ) similarity in both directions , across a longer time scale . My concern is that all of the chosen datasets are very motion-centric where temporal relationships are more important and this makes it somewhat advantageous to the proposed model and does not really allow us to ascertain its applicability to datasets where appearance plays a bigger role such as Kinetics or HMDB-51 . In fact , the results on SS-V1 and SS-V2 show some gains ( ~2-3 % ) compared to the very closely related MotionSqueeze Network ( Kwon et al , ECCV 2020 ) which computes correlation ( rather than similarity ) between adjacent frames ( t and t+1 ) . MSNet did not see any significant advances in datasets ( HMDB-51 and Kinetics ) where appearance plays a greater role and I am not sure how this proposed approach would help in those conditions . - I assume the function sim ( . ) from Equation 1 is cosine similarity since it was not explicitly mentioned anywhere . Is there any specific reason the correlation function was not used ? What is the effect of using correlation in place of cosine similarity ? It would interesting to see the effect of this since it would essentially allow us to see how modeling the longer temporal context would affect MS Net ( the most closely related network ) and help highlight the contribution of the proposed model beyond the obvious empirical gains . - While the robustness experiments test out the occlusion setting by cutting out a rectangular patch of a single center-frame , that does not , IMHO , shows robustness to occlusion since the features from surrounding frames ( on both sides ) will help mitigate this artificial occlusion . In practice , the occlusion can have a larger effect since it would not zero out the appearance but will add some noise into the feature extraction and hence provide a stronger challenge . Experiments on HMDB51 , which has strong camera motion and hence introduces natural occlusions , would be a stronger experiment to show the generalization of the model beyond datasets with a strong motion-centric bias .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thanks for appreciating our work and providing constructive comments . Below are our responses and we will reflect them in our final manuscript . > [ R2 ] My concern is that all of the chosen datasets are very motion-centric where temporal relationships are more important and this makes it somewhat advantageous to the proposed model and does not really allow us to ascertain its applicability to datasets where appearance plays a bigger role such as Kinetics or HMDB-51 . We are conducting the experiments on the appearance-oriented datasets . We will leave the comment as soon as the experiments are finished . > [ R2 ] I assume the function sim ( . ) from Equation 1 is cosine similarity since it was not explicitly mentioned anywhere . Is there any specific reason the correlation function was not used ? What is the effect of using correlation in place of cosine similarity ? As pointed out , we use the cosine similarity as our default similarity function . We have explored the effect of different similarity functions on learning STSS , and found that the cosine similarity performs the best . Table A compares the performance on SS-V1 with different similarity functions : cosine similarity ( ours ) , dot product [ 1 ] , and embedded Gaussian [ 2 ] . The experimental details are the same as those in Appendix A , except that we reduce the channel dimension $ C $ of appearance feature $ \\mathbf { V } $ to 32 for the less GPU memory consumption . The cosine similarity gives better accuracy , but not much , by 0.1 \\ % p and 0.2 \\ % p than the dot product and the embedded Gaussian at top-1 accuracy . We will clarify this point in our final manuscript . Table A : * * Results of different similarity functions . * * |model|\\ # channels|similarity function|top-1|top-5| | : | : :| : :| : :| : :| |TSM-R18|-|-|43.0|72.3| |SELFYNet ( ours ) |32|cosine| * * 47.8 * * | * * 77.1 * * | |SELFYNet|32|dot product|47.7|76.6| |SELFYNet|32|embedded Gaussian|47.6|76.8| [ 1 ] P. Fischer , et al. \u201c FlowNet : learning optical flow with convolutional networks. \u201d ICCV . 2015.\\ [ 2 ] X. Wang , R. Girshick , A. Gupta , and K. He . \u201c Non-local neural networks. \u201d CVPR . 2018. > [ R2 ] It would be interesting to see the effect of this since it would essentially allow us to see how modeling the longer temporal context would affect MS Net ( the most closely related network ) and help highlight the contribution of the proposed model beyond the obvious empirical gains . Below are the comparisons between SELFYNet and MSNet [ 3 ] to demonstrate the effectiveness of STSS as well as our feature extraction methods . For an apple-to-apple comparison , we apply the kernel soft-argmax and the max-pooling operations ( * KS+CM * in [ 3 ] ) to the feature extraction method by following their official github code . Please note that their official code also uses the cosine similarity as the same as ours . As we restrict the temporal offset $ l $ of the SELFY variant to $ \\ { +1 \\ } $ , it is equivalent to MS module of which * feature transformation layers * are the standard 2D conv layers . All experimental details are the same as those in Appendix A . Table B summarizes the results on SS-V1 . KS+CM method achieves 46.1 \\ % p at top-1 accuracy . As we enlarge the temporal window $ L $ to 5 , we obtain an additional gain as 1.3 \\ % p . The learnable convolution layers improve top-1 accuracy by 1.0 \\ % p in both cases . We will add this experiment in our final manuscript . Table B : * * Performance comparison with MSNet . * * | model | feature extraction method | $ ( L , U , V ) $ | top-1 | top-5 | | : | : :| : :| : | : | |TSM-R18|-|-|43.0|72.3| |SELFYNet|KS+CM| $ ( 1,9,9 ) $ |46.1|75.3| |SELFYNet|KS+CM| $ ( 5,9,9 ) $ |47.4|76.8| |SELFYNet|conv| $ ( 1,9,9 ) $ |47.1|76.3| |SELFYNet|conv| $ ( 5,9,9 ) $ | * * 48.4 * * | * * 77.6 * * | [ 3 ] H. Kwon , M. Kim , S. Kwak , and M. Cho . \u201c MotionSqueeze : neural motion feature learning for video understanding. \u201d ECCV . 2020. > [ R2 ] the occlusion can have a larger effect since it would not zero out the appearance but will add some noise into the feature extraction and hence provide a stronger challenge . Experiments on HMDB51 , which has strong camera motion and hence introduces natural occlusions , would be a stronger experiment to show the generalization of the model beyond datasets with a strong motion-centric bias . We are conducting the experiments on HMDB51 . We will leave the comment as soon as the experiments are finished ."}, "3": {"review_id": "S6AtYQLzXOY-3", "review_text": "# # # # General This paper proposes spatio-temporal self-similarity ( STSS ) , which captures structural patterns in space and time , for action recognition from videos . Overall , I would like to recommend ICLR to accept the paper . Pros and Cons I found in the paper are summarized as follows . # # # # Pros . 1.SELFY , the proposed neural block that implements STSS , is a good extention and adjustment of prior self-similarity works to modern CNNs , resulted in achieving state-of-the-art performance on different datasets . 1.The ablation study reveals that SELFY with a long temporal range successfully incorporate the temporal information without explicitly exploiting optical flows . 1.The paper is generally well and clearly written . 1.The literature review is thorough , and the work is well contextualized in the literatures . # # # # Cons 1 . The novelty and the originality of the paper may be relatively low because the concept of the STSS itself was already proposed in prior studies as mentioned in the paper . 1.The reasoning or intuition behind some of the design choices are not always clear . For example , - The output of the feature extraction block is $ F \\in \\mathbb { R } ^ { T \\times X \\times Y \\times L \\times C_F } $ , and the authors say > The dimension of L is preserved to extract motion information across different temporal offsets in a consistent manner . But the motion information across different temporal offsets can be extracted in the feature extraction module if MLP or convolution is employed in the feature extraction block . In other words , even if the output is $ F \\in \\mathbb { R } ^ { T \\times X \\times Y \\times C_F } $ , the motion information can be encoded in the output tensor . Possibly the authors empirically found the present design choice is better . If so , how does the performance change if $ L $ is not preserved ? - In the feature integration block , firstly $ 3 \\times 3 $ spatial convolution kernel is applied , then the temporal offset ( $ L $ ) and the channels ( $ C^ * _F $ ) dimension is flattened , and finally $ 1 \\times 1 \\times 1 $ spatio-temporal convolution is applied . Is this design chosen experimentally or is there any intuition behind ? # # # # Minor comments 1 . It may be interesting to discuss the relationship with self-attention based networks such as [ 1-3 ] 1 . About Table 7 in Appendix , how is the result if SELFY block is used in $ res_1 $ and $ res_5 $ ? I guess these option do not work well , but I think it is beneficial to list all the results . [ 1 ] Hu+ , Local Relation Networks for Image Recognition , ICCV 2019 [ 2 ] Ramachandran+ , Stand-Alone Self-Attention in Vision Models , NeurIPS 2019 [ 3 ] Zhao+ , Exploring Self-attention for Image Recognition , CVPR 2020", "rating": "6: Marginally above acceptance threshold", "reply_text": "> [ R4 ] In the feature integration block , firstly 3\u00d73 spatial convolution kernel is applied , then the temporal offset ( L ) and the channels ( CF\u2217 ) dimension is flattened , and finally 1\u00d71\u00d71 spatio-temporal convolution is applied . Is this design chosen experimentally or is there any intuition behind ? Our intuition is that each of $ L $ channel-groups in the flattened $ LC_ { F^\u2217 } $ -dimensional STSS representation can be viewed as a spatial self-similarity feature within each relative temporal offset so that the $ 1 \\times 1 \\times 1 $ convolutional layer , i.e.fully connected layer , is able to reduce the flattened feature to the $ C $ -dimensional feature vector considering the information of different temporal offsets . We also empirically found that a pooling operation such as max pooling or average pooling , which reduces $ \\mathbf { F } \\in \\mathbb { R } ^ { T \\times X \\times Y \\times L \\times C_ { F^ * } } $ along $ L $ dimension , significantly degrades the performance . We conjecture that such pooling operations , which get rid of the information of temporal offsets , damage long-range temporal modeling . > [ R4 ] It may be interesting to discuss the relationship with self-attention based networks such as [ 1-3 ] . The local self-attention [ 6,7,8 ] and our method have a common denominator of using the self-similarity tensor but use it in a very different way and purpose . The local self-attention mechanism aims to aggregate the local context features using the self-similarity tensor and it thus uses the self-similarity values as attention weights for feature aggregation . However , our method aims to learn a generalized motion representation from the local STSS , so the final STSS representation is directly fed into the neural network instead of multiplying it to local context features . For a clear comparison , we conduct an ablation experiment as follows . We add a single * spatio-temporal * local self-attention layer , extended from [ 7 ] , after $ res_3 $ followed by feature integration layers in Sec.3.2.2 . All experimental details are the same as those in Appendix A , except that we reduce the channel dimension $ C $ of appearance feature $ \\mathbf { V } $ to 32 . Table C summarizes the results on SS-V1 . The spatio-temporal local self-attention layer is accurate as 43.8 \\ % p at top-1 accuracy , and both of SELFY blocks using the embedded Gaussian and the cosine similarity outperform the local self-attention by achieving top-1 accuracy as 47.6 \\ % p and 47.8 \\ % p , respectively . These results are in alignment with the prior work [ 9 ] , which reveals that the self-attention mechanism hardly captures motion features in video . We will add this to the discussion section in our final manuscript . Table C : * * Performance comparison with the local self-attention . * * R.P.E.is an abbreviation for relative positional embeddings . |model|similarity function|feature extraction method|top-1|top-5| | : | : :| : :| : :| : :| |TSM-R18|-|-|43.0|72.3| |SELFYNet|embedded Gaussian|multiplication with $ \\mathbf { V } $ +R.P.E|43.8|72.9| |SELFYNet|embedded Gaussian|conv|47.6|76.8| |SELFYNet ( ours ) |cosine|conv| * * 47.8 * * | * * 77.1 * * | [ 6 ] H. Hu , Z. Zhang , Z. Xie , and S. Lin . \u201c Local relation networks for image recognition. \u201d ICCV . 2019.\\ [ 7 ] P. Ramachandran , N. Parmar , A. Vaswani , I. Vello , A. Levskaya , and J. Shlens . \u201c Stand-alone self-attention in vision models. \u201d NIPS . 2019.\\ [ 8 ] H. Zhao , J. Jia , and V. Koltun . \u201c Exploring self-attention for image recognition. \u201d CVPR . 2020.\\ [ 9 ] X. Liu , J.Y . Lee , and H. Jin . \u201c Learning video representations from correspondence proposals. \u201d CVPR . 2019. > [ R4 ] About Table 7 in Appendix , how is the result if SELFY block is used in res1 and res5 ? I guess these options do not work well , but I think it is beneficial to list all the results . We conducted the additional experiments and the results on SS-V1 are in Table D below , where we add the SELFY block to the different places . Here , we set the spatial resolution of $ \\mathbf { V } $ to $ 14 \\times 14 $ . The SELFY block after $ pool_1 $ improves 2.7 \\ % p at top-1 accuracy , while the SELFY block after $ res_5 $ does not . We conjecture that the spatial resolution ( $ 7 \\times 7 $ ) is too small to extract meaningful motion features for action recognition . We will update Table 7 in our manuscript . Table D : * * Performance comparison with different positions of SELFY block . * * |model|position|top-1|top-5| | : |||| |TSM-R18|-|43.0|72.3| |SELFYNet |after $ pool_1 $ |45.7|74.6| | SELFYNet|after $ res_2 $ |47.2|76.6| |SELFYNet |after $ res_3 $ | * * 48.4 * * | * * 77.6 * * | |SELFYNet |after $ res_4 $ |46.6|76.0| |SELFYNet |after $ res_5 $ |42.8|72.6|"}}