{"year": "2017", "forum": "H178hw9ex", "title": "Dynamic Steerable Frame Networks", "decision": "Reject", "meta_review": "This paper studies how to incorporate local invariance to geometric transformations into a CNN pipeline. It proposes steerable filter banks as the ground-bed to measure and produce such local invariance, building on previous work from the same authors as well as the Spatial Transformer Networks. Preliminary experiments on several tasks requiring different levels of local invariance are presented. \n \n The reviewers had varying opinions about this work; all acknowledged the potential benefits of the approach, while some of them raised questions about the significance and usefulness of the approach. The authors were very responsive during the rebuttal phase and took into account all the feedback. \n \n Based on the technical content of the paper and the reviewers opinion, the AC recommends rejection. Since this decision is not consensual among all reviewers, please let me explain it in more detail.\n \n - The current manuscript does not provide a clear description of the new model in the context of the related works it builds upon (the so-called dynamic filter networks and the spatial transformer networks). The paper spends almost 4 pages with a technical exposition on steerable frames, covering basic material from signal processing. While this might indeed be a good introduction to readers not familiar with the concept of steerable filters, the fact is that it obfuscates the real contributions of the paper. which are not clearly stated. In fact, the model is presented between equations (10) and (11), but it is not clear from these equations what specifically differentiates the dsfn from the other two models -- the reader has to do some digging in order to uncover the differences (which are important). \n \n Besides this clarity issue, the paper does not offer any insight as to how the 'Pose generating network' Psi is supposed to estimate the pose parameters. Which architecture? what is the underlying estimation problem it is trying to solve, and why do we expect this problem to be efficiently estimated with a neural network? when are the pose parameters uniquely determined? how does this network deal with the aperture effects (i.e. the situations where there is no unicity in determining a specific pose) ?\n Currently, the reader has no access to these questions, which are to some extent at the core of the proposed technique.", "reviews": [{"review_id": "H178hw9ex-0", "review_text": "This paper presents an improved formulation of CNN, aiming to separate geometric transformation from inherent features. The network can estimate the transformation of filters given the input images. This work is based on a solid technical foundation and is motivated by a plausible rationale. Yet, the value of this work in practice is subject to questions: (1) It relies on the assumption that the input image is subject to a transformation on a certain Lie group (locally). Do such transformations constitute real challenges in practice? State-of-the-art CNNs, e.g. ResNet, are already quite resilient to such local deformations. What such components would add to the state of the art? Limited experiments on Cifar-10 does not seem to provide a very strong argument. (2) The computational cost is not discussed.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your comments and questions . The main goal of the Cifar10 experiments was to show , that one can replace the pixel-basis with a frame that has additional desirable properties like the steerability without decreasing performance . Additionally , we were able to show that steerable frames can consistently increase the performance of SOTA networks given they are suitable for natural image data . Note that we have added SOTA Densenet models to table 2 , in which we were able to improve performance by substituting the pixel-basis with frames as well as in the Resnet models , substantiating the generality of our observations . ( 1 ) Do such transformations constitute real challenges in practice ? What such components would add to the state of the art ? Being able to replace the pixel-basis by steerable frames leads us to our proposed Dynamic Steerable Frame Networks ( DSFNs ) , a method that continuously transforms filters conditioned on the input . DSFNs are locally adaptive , interpretable and data-efficient . We compare our DSFNs to other adaptive methods . In this domain Dynamic Filter Networks ( DFNs ) and Spatial Transformer Networks ( STNs ) constitute the state of the art . Dynamic Filter Networks learn location varying filters in an unconstrained manner and generate any type of filter kernel for each location in the input , this approach is very data-inefficient as it has many unconstrained parameters and the transformations have no clear geometrical meaning . Spatial Transformer Networks transform the whole feature stack globally under predefined geometrical parameters . The method regularizes the additional parameters introduced by it effectively and achieves global transformation invariance . STNs are not locally adaptive , thus they fail in many cases where it is not beneficial to transform the image globally as it would destroy discriminative information ( deformable objects , many objects , dynamic movements ) or where global registration is anyway performed as a standard preprocessing step ( medical imaging data ) . The hand-gesture experiment is an example of a whole range of tasks where STNs fail , as they are not suitable for moving deformable objects . DFNs are black boxes and not data-efficient . They introduce many unconstrained parameters and are thus not suited for limited-data scenarios . At the same time , it is not possible to check if the model converges or does something meaningful . Such a behavior is undesirable when data is limited and interpretability is key , like in medical imaging . Further , they fail to generate very fine-grained local adaption as can be achieved with a well-regularized DSFN , illustrated with the edge-detection experiment . This makes them inferior for tasks like segmentation , even in unlimited data scenarios . Our proposed DSFNs transform filters locally in a continuous manner . This allows the model to precisely adapt filters to local features in the image . It can do so in a data-efficient manner . One learned filter can be applied to all its transformations , alleviating the necessity to learn one filter for each orientation or each different scale , effectively representing infinitely many geometrical variants of the same filter . In both experiments , our proposed method substantially outperform DFNs and STNs and we are able to explain why this is the case . In conclusion , our proposed Dynamic Steerable Frame Networks are able to fill the gap between adaptive state-of-the-art Dynamic Filter Networks and Spatial Transformer Networks . ( 2 ) The computational cost is not discussed . Frame-based CNNs have the same runtime as vanilla CNNs . The Dynamic Steerable Frame Networks have the same runtime as vanilla Dynamic Filter Networks . Besides that , there is a whole body of research on how to speed up convolution with analytic frame functions , with strategies like recursive filtering , showing promise to potentially decrease runtime . Thank you once again for your review , we have updated the manuscript to include our answers . Let us know if this answers your questions !"}, {"review_id": "H178hw9ex-1", "review_text": "I sincerely apologize for the late review! The first part has a strong emphasis on the technical part. It could benefit from some high level arguments on what the method aims to achieve, what limitation is there to overcome. I may have misunderstood the contribution (in which case please correct me) that the main novel part of the paper is the suggestion to learn the group parameterizations instead of pre-fixing them. So instead of applying it to common spatial filters as in De Brabandere et al., it is applied to Steerable Frames? The first contribution suggests that \"general frame bases are better suited to represent sensory input data than the commonly used pixel basis.\". The experiments on Cifar10+ indicate that this is not true in general. Considering the basis as a hyper-parameter, expensive search has to be conducted to find that the Gauss-Frame gives better results. I assume this does not suggest that the Gauss-Frame is always better, at least there is weak evidence on a single network presented. Maybe the first contribution has to be re-stated. Further is the \"Pixel\" network representation corrected for the larger number of parameters. As someone who is interested in using this, what are the runtime considerations? I would strongly suggest to improve Fig.3. The Figure uses \"w\" several times in different notations and depictions. It mixes boxes, single symbols and illustrative figures. It took some time to decipher the Figure and its flow. Summary: The paper is sufficiently clear, technical at many places and readability can be improved. E.g., the introduction of frames in the beginning lacks motivation and is rather unclear to someone new to this concept. The work falls in the general category of methods that impose knowledge about filter transformations into the network architecture. For me that has always two sides, the algorithmic and technical part (there are several ways to do this) and the practical side (should I do it)? This is a possible approach to this problem but after the paper I was a bit wondering what I have learned, I am certainly not inspired based on the content of the paper to integrate or build on this work. I am lacking insights into transformational parameters that are relevant for a problem. While the spatial transformer network paper was weaker on the technical elegance side, it provided exactly this: an insight into the feature transformation learned by the algorithm. I am missing this here, e.g., from Table 2 I learn that among four choices one works empirically better. What is destroyed by the x^py^p and Hermite frames that the ResNet is *not* able to recover from? You can construct network architectures that are the superset of both, so that inferior performance could be avoided. The algorithm is clear but it is similar to the Dynamic Filter Networks paper. And I am unfortunately not convinced about the usefulness of this particular formulation. I'd expect a stronger paper with more insights into transformations and comparisons to standard techniques, a clear delineation of when this is advised. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you very much for the insightful comments and questions . You asked : What are the feature transformations learned by the algorithm ? The algorithm learns location varying filters that transform under continuous transformation groups , such as rotations , changes in size , shearing , changes of color . In fact , it provides a framework to transform filters under arbitrary local transformations that can be formulated as a Lie group . Locally adaptive filters are advantageous in many tasks . In natural images and video , many things change locally . For instance , the leaves of a tree can all move , but the tree remains a tree . Humans and animals are changing locally all the time , even though their overall appearance remains largely the same . Medical imaging is almost purely concerned with local features that don \u2019 t necessarily have a preferred orientation . In all of these cases , data-augmentation and global methods like Spatial Transformer Networks can only get rid of global variabilities . Ideally the amount of local invariance should not be pre-defined , but learned to preserve all important information for the task at hand . Our proposed Dynamic Steerable Frame Networks ( DSFNs ) provide a framework to do so . DSFNs transform filters locally in a continuous manner . This allows the model to precisely adapt filters to local features in the image . It can do so in a data-efficient manner . One learned filter can be applied to all its transformations , alleviating the necessity to learn one filter for each orientation or each different scale , effectively representing infinitely many geometrical variants of the same filter . When precise local adaption is necessary , the usefulness of continuously transformable filters is illustrated by the edge-detection experiment . The Dynamic Filter Network ( DFN ) fails to learn a continuously transforming basis , while the DSFN outperforms it by an order of magnitude . The DFN improves performance when it receives a steerable frame as input , but the error it makes is still double compared to the well-regularized DSFN , see figure 4 . We visualize the pose space of the DSFN to highlight that it indeed learns rotation invariant filters in the same figure . Note , that the DSFN yields an interpretable pose space , while the DFN acts as an uninterpretable black box . Another example of local adaption are the hand-gesture recognition experiments as an example where local invariance is key . The DSFN allows us to learn locally varying filters in a data-efficient and interpretable manner , as substantiated by 18 % higher accuracy than the baseline . We also evaluated Spatial Transformer Networks on this task in two settings , one that allows the full-affine transformations and one that only allows rotation and uniform scalings . The affine STN fails to find meaningful warps of the inputs and achieves almost random performance , only classifying one static class correctly . The rotation scaling STN still decreases performance compared to the baseline , but it performance substantially better than the affine STN , indicating , that partially removing global invariances increases preserved information content . We are also adding visualizations of the warps and filters learned by STN and DSFN to the appendix to increase insight into the trained models . The STN learns more or less random warps , while the DSFN indeed discovers locally invariant filters that move with the boundaries of the object in a continuous manner , whereas the STN approach fails . The paper could benefit from some high-level arguments . Our proposed Dynamic Steerable Frame Networks fill the gap in-between STNs and DFNs . DSFNs are locally adaptive , interpretable and data-efficient . They can be applied to a range of problems where neither STNs nor DFNs are a good fit , but one would rather like to use a hybrid approach of the two . DFNs learn location varying filters in an unconstrained manner and generate any type of filter kernel for each location in the input , this approach is very data-inefficient as it has many unconstrained parameters and the transformations have no clear geometrical meaning . STNs transform the whole feature stack globally under predefined geometrical parameters . The method regularizes the additional parameters introduced by it effectively and achieves global transformation invariance . STNs are not locally adaptive , thus they fail in many cases where it is not beneficial to transform the image globally as it would destroy discriminative information ( deformable objects , many objects , dynamic movements ) or where global registration is anyway performed as a standard preprocessing step ( medical imaging data ) . The hand-gesture experiment is an example of a whole range of tasks where STNs fail , as they are not suitable for moving deformable objects . DFNs are black boxes and not data-efficient . They introduce many unconstrained parameters and are thus not suited for limited-data scenarios . At the same time , it is not possible to check if the model converges or does something meaningful . Such a behavior is undesirable when data is limited and interpretability is key , like in medical imaging . Further , they fail to generate very fine-grained local adaption as can be achieved with a well-regularized DSFN , illustrated with the edge-detection experiment . This makes them inferior for tasks like segmentation , even in unlimited data scenarios . Why are certain frames more suitable than others ? Suitable frames are the ones have been designed with general image properties in mind . Each frame performs a change of basis that can be seen as a rotation of the inputs basis . Natural image statistics based frames are thus a good idea ( like the Gauss-Frame ) while rotating to an arbitrary frame ( like polynomials ) can hurt performance , but depending on the task at hand a decrease in 1 % of classification performance can also be acceptable , given that different frames exhibit properties the pixel-basis does not have , for example steerability . We have made the Cifar10 experiments section clearer in this regard and added the SOTA Densenet to the table . We show that it behaves similar to the ResNets , thus our observations generalize to different models and model sizes . Our main purpose of this experiment is to show that suitable frames can outperform the standard pixel-basis , while they also exhibit multiple beneficial properties like steerability . We thus do agree with the reviewer , that the contribution associated with this insight has to be restated . We have changed it to : \u201c Suitable choices of frames lead to improved performance. \u201d What are runtime considerations ? Frame-based CNNs have the same runtime as vanilla CNNs . The Dynamic Steerable Frame Networks have the same runtime as vanilla Dynamic Filter Networks . However , due to separability and strategies like recursive filtering , frame-bases show much promise to be faster than standard filter formulations , but we leave this for future work . Figure 3 We have simplified figure 3 based on the reviewer 's suggestions . We believe merging different blocks and staying very close to the main equation of the paper ( eq.7 ) and further simplification helped to increase the readability of it significantly , we thank the reviewer for the suggestions . Thank you once again . We have reworked the manuscript according to your comments and believe they were instrumental for improving it substantially . Let us know if we have answered all your questions !"}, {"review_id": "H178hw9ex-2", "review_text": "This works applies steerable frames for various tasks where convolutional neural networks with location invariant operators are traditionally applied. Authors provide a detailed overview of steerable frames followed with an experimental section which applies dynamic steerable network to small machine learning problems where the steerability is conceptually useful. Even though the evaluation is performed only on few small tasks, the reason why more tasks were not evaluated is that piece-wise pose invariance is needed only for a subset of tasks. The fact, that simply using overcomplete bases as a sort of \"feature pre-processing\" improves the results for already highly optimized ResNet and DenseNet architectures is quite interesting achievement. For the edge detection, a relatively hard baseline is selected - the Dynamic Filter Networks, which already attempts to achieve position invariant filters. The fact that DSFN improves the performance on this task verifies that regressing the parametrization of the steerable filters yields better results than regressing the filters directly. In the last experiment authors apply the network to video classification using LSTMs and they show that the improved performance is not due to increased capacity of the network. In general, it is quite interesting work. Even though it does not offer ground-breaking results (mainly in a sense of not performing experiments on larger tasks), it is theoretically interesting and shows promising results. There are few minor issues and suggestions related to the paper: * For the LSTM experiment, in order to be more exact, it would be useful to include information about total number of parameters, as the network which estimates the pose also increases the number of parameters. * Would it be possible to provide more details about how the back-propagation is done through the steerable filters? * For the Edge Detection experiment, it would be useful to provide results for some standard baseline - e.g. CNN with a similar number of parameters. Simply to see how useful it is to have location-variant filters for this task. * The last sentence in second paragraph on page 1 is missing a verb. Also it is maybe unnecessary. * The hyphenation for ConvNet is incorrect on multiple places (probably `\\hyphenation{Conv-Net}` would fix it). ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your positive review and for acknowledging the usefulness of the approach . We have included your comments into the revision of the paper . * For the LSTM experiment , in order to be more exact , it would be useful to include information about total number of parameters , as the network which estimates the pose also increases the number of parameters . We have added the number of parameters to the table . The 1-Layer convLSTM has 905k parameters and the DSFN convLSTM has 907k parameters , while the 2-Layer convLSTM has 913k parameters . Thus , our proposed method comes with a very small parameter cost , as it mainly consists of 1x1 convolutions performed on the output of a convolution with fixed frame functions . * Would it be possible to provide more details about how the back-propagation is done through the steerable filters ? We will add an example for the backpropagation to the appendix . * For the Edge Detection experiment , it would be useful to provide results for some standard baseline - e.g.CNN with a similar number of parameters . Simply to see how useful it is to have location-variant filters for this task . We have added an Autoencoder baseline , it performance substantially worse than the locally adaptive methods . Further , we have added a Dynamic Filter Network who 's input is a steerable frame , doing so improves the performance of the DFN significantly , illustrating that the DFN does not seem to be capable of discovering a continuously steerable frame by itself . Most effective , however , is the full DSFN , cutting the error of the frame-based DFN again in half . * The last sentence in second paragraph on page 1 is missing a verb . Also it is maybe unnecessary . Thank you very much for pointing this out . We have rewritten this paragraph incorporating the comments of reviewer 3 and thus the sentence has been removed . * The hyphenation for ConvNet is incorrect on multiple places ( probably ` \\hyphenation { Conv-Net } ` would fix it ) . We have removed the term ConvNet and replaced it with CNN . Thank you once again for your constructive and positive feedback . Let us know if this answers all your questions !"}], "0": {"review_id": "H178hw9ex-0", "review_text": "This paper presents an improved formulation of CNN, aiming to separate geometric transformation from inherent features. The network can estimate the transformation of filters given the input images. This work is based on a solid technical foundation and is motivated by a plausible rationale. Yet, the value of this work in practice is subject to questions: (1) It relies on the assumption that the input image is subject to a transformation on a certain Lie group (locally). Do such transformations constitute real challenges in practice? State-of-the-art CNNs, e.g. ResNet, are already quite resilient to such local deformations. What such components would add to the state of the art? Limited experiments on Cifar-10 does not seem to provide a very strong argument. (2) The computational cost is not discussed.", "rating": "5: Marginally below acceptance threshold", "reply_text": "Thank you very much for your comments and questions . The main goal of the Cifar10 experiments was to show , that one can replace the pixel-basis with a frame that has additional desirable properties like the steerability without decreasing performance . Additionally , we were able to show that steerable frames can consistently increase the performance of SOTA networks given they are suitable for natural image data . Note that we have added SOTA Densenet models to table 2 , in which we were able to improve performance by substituting the pixel-basis with frames as well as in the Resnet models , substantiating the generality of our observations . ( 1 ) Do such transformations constitute real challenges in practice ? What such components would add to the state of the art ? Being able to replace the pixel-basis by steerable frames leads us to our proposed Dynamic Steerable Frame Networks ( DSFNs ) , a method that continuously transforms filters conditioned on the input . DSFNs are locally adaptive , interpretable and data-efficient . We compare our DSFNs to other adaptive methods . In this domain Dynamic Filter Networks ( DFNs ) and Spatial Transformer Networks ( STNs ) constitute the state of the art . Dynamic Filter Networks learn location varying filters in an unconstrained manner and generate any type of filter kernel for each location in the input , this approach is very data-inefficient as it has many unconstrained parameters and the transformations have no clear geometrical meaning . Spatial Transformer Networks transform the whole feature stack globally under predefined geometrical parameters . The method regularizes the additional parameters introduced by it effectively and achieves global transformation invariance . STNs are not locally adaptive , thus they fail in many cases where it is not beneficial to transform the image globally as it would destroy discriminative information ( deformable objects , many objects , dynamic movements ) or where global registration is anyway performed as a standard preprocessing step ( medical imaging data ) . The hand-gesture experiment is an example of a whole range of tasks where STNs fail , as they are not suitable for moving deformable objects . DFNs are black boxes and not data-efficient . They introduce many unconstrained parameters and are thus not suited for limited-data scenarios . At the same time , it is not possible to check if the model converges or does something meaningful . Such a behavior is undesirable when data is limited and interpretability is key , like in medical imaging . Further , they fail to generate very fine-grained local adaption as can be achieved with a well-regularized DSFN , illustrated with the edge-detection experiment . This makes them inferior for tasks like segmentation , even in unlimited data scenarios . Our proposed DSFNs transform filters locally in a continuous manner . This allows the model to precisely adapt filters to local features in the image . It can do so in a data-efficient manner . One learned filter can be applied to all its transformations , alleviating the necessity to learn one filter for each orientation or each different scale , effectively representing infinitely many geometrical variants of the same filter . In both experiments , our proposed method substantially outperform DFNs and STNs and we are able to explain why this is the case . In conclusion , our proposed Dynamic Steerable Frame Networks are able to fill the gap between adaptive state-of-the-art Dynamic Filter Networks and Spatial Transformer Networks . ( 2 ) The computational cost is not discussed . Frame-based CNNs have the same runtime as vanilla CNNs . The Dynamic Steerable Frame Networks have the same runtime as vanilla Dynamic Filter Networks . Besides that , there is a whole body of research on how to speed up convolution with analytic frame functions , with strategies like recursive filtering , showing promise to potentially decrease runtime . Thank you once again for your review , we have updated the manuscript to include our answers . Let us know if this answers your questions !"}, "1": {"review_id": "H178hw9ex-1", "review_text": "I sincerely apologize for the late review! The first part has a strong emphasis on the technical part. It could benefit from some high level arguments on what the method aims to achieve, what limitation is there to overcome. I may have misunderstood the contribution (in which case please correct me) that the main novel part of the paper is the suggestion to learn the group parameterizations instead of pre-fixing them. So instead of applying it to common spatial filters as in De Brabandere et al., it is applied to Steerable Frames? The first contribution suggests that \"general frame bases are better suited to represent sensory input data than the commonly used pixel basis.\". The experiments on Cifar10+ indicate that this is not true in general. Considering the basis as a hyper-parameter, expensive search has to be conducted to find that the Gauss-Frame gives better results. I assume this does not suggest that the Gauss-Frame is always better, at least there is weak evidence on a single network presented. Maybe the first contribution has to be re-stated. Further is the \"Pixel\" network representation corrected for the larger number of parameters. As someone who is interested in using this, what are the runtime considerations? I would strongly suggest to improve Fig.3. The Figure uses \"w\" several times in different notations and depictions. It mixes boxes, single symbols and illustrative figures. It took some time to decipher the Figure and its flow. Summary: The paper is sufficiently clear, technical at many places and readability can be improved. E.g., the introduction of frames in the beginning lacks motivation and is rather unclear to someone new to this concept. The work falls in the general category of methods that impose knowledge about filter transformations into the network architecture. For me that has always two sides, the algorithmic and technical part (there are several ways to do this) and the practical side (should I do it)? This is a possible approach to this problem but after the paper I was a bit wondering what I have learned, I am certainly not inspired based on the content of the paper to integrate or build on this work. I am lacking insights into transformational parameters that are relevant for a problem. While the spatial transformer network paper was weaker on the technical elegance side, it provided exactly this: an insight into the feature transformation learned by the algorithm. I am missing this here, e.g., from Table 2 I learn that among four choices one works empirically better. What is destroyed by the x^py^p and Hermite frames that the ResNet is *not* able to recover from? You can construct network architectures that are the superset of both, so that inferior performance could be avoided. The algorithm is clear but it is similar to the Dynamic Filter Networks paper. And I am unfortunately not convinced about the usefulness of this particular formulation. I'd expect a stronger paper with more insights into transformations and comparisons to standard techniques, a clear delineation of when this is advised. ", "rating": "4: Ok but not good enough - rejection", "reply_text": "Thank you very much for the insightful comments and questions . You asked : What are the feature transformations learned by the algorithm ? The algorithm learns location varying filters that transform under continuous transformation groups , such as rotations , changes in size , shearing , changes of color . In fact , it provides a framework to transform filters under arbitrary local transformations that can be formulated as a Lie group . Locally adaptive filters are advantageous in many tasks . In natural images and video , many things change locally . For instance , the leaves of a tree can all move , but the tree remains a tree . Humans and animals are changing locally all the time , even though their overall appearance remains largely the same . Medical imaging is almost purely concerned with local features that don \u2019 t necessarily have a preferred orientation . In all of these cases , data-augmentation and global methods like Spatial Transformer Networks can only get rid of global variabilities . Ideally the amount of local invariance should not be pre-defined , but learned to preserve all important information for the task at hand . Our proposed Dynamic Steerable Frame Networks ( DSFNs ) provide a framework to do so . DSFNs transform filters locally in a continuous manner . This allows the model to precisely adapt filters to local features in the image . It can do so in a data-efficient manner . One learned filter can be applied to all its transformations , alleviating the necessity to learn one filter for each orientation or each different scale , effectively representing infinitely many geometrical variants of the same filter . When precise local adaption is necessary , the usefulness of continuously transformable filters is illustrated by the edge-detection experiment . The Dynamic Filter Network ( DFN ) fails to learn a continuously transforming basis , while the DSFN outperforms it by an order of magnitude . The DFN improves performance when it receives a steerable frame as input , but the error it makes is still double compared to the well-regularized DSFN , see figure 4 . We visualize the pose space of the DSFN to highlight that it indeed learns rotation invariant filters in the same figure . Note , that the DSFN yields an interpretable pose space , while the DFN acts as an uninterpretable black box . Another example of local adaption are the hand-gesture recognition experiments as an example where local invariance is key . The DSFN allows us to learn locally varying filters in a data-efficient and interpretable manner , as substantiated by 18 % higher accuracy than the baseline . We also evaluated Spatial Transformer Networks on this task in two settings , one that allows the full-affine transformations and one that only allows rotation and uniform scalings . The affine STN fails to find meaningful warps of the inputs and achieves almost random performance , only classifying one static class correctly . The rotation scaling STN still decreases performance compared to the baseline , but it performance substantially better than the affine STN , indicating , that partially removing global invariances increases preserved information content . We are also adding visualizations of the warps and filters learned by STN and DSFN to the appendix to increase insight into the trained models . The STN learns more or less random warps , while the DSFN indeed discovers locally invariant filters that move with the boundaries of the object in a continuous manner , whereas the STN approach fails . The paper could benefit from some high-level arguments . Our proposed Dynamic Steerable Frame Networks fill the gap in-between STNs and DFNs . DSFNs are locally adaptive , interpretable and data-efficient . They can be applied to a range of problems where neither STNs nor DFNs are a good fit , but one would rather like to use a hybrid approach of the two . DFNs learn location varying filters in an unconstrained manner and generate any type of filter kernel for each location in the input , this approach is very data-inefficient as it has many unconstrained parameters and the transformations have no clear geometrical meaning . STNs transform the whole feature stack globally under predefined geometrical parameters . The method regularizes the additional parameters introduced by it effectively and achieves global transformation invariance . STNs are not locally adaptive , thus they fail in many cases where it is not beneficial to transform the image globally as it would destroy discriminative information ( deformable objects , many objects , dynamic movements ) or where global registration is anyway performed as a standard preprocessing step ( medical imaging data ) . The hand-gesture experiment is an example of a whole range of tasks where STNs fail , as they are not suitable for moving deformable objects . DFNs are black boxes and not data-efficient . They introduce many unconstrained parameters and are thus not suited for limited-data scenarios . At the same time , it is not possible to check if the model converges or does something meaningful . Such a behavior is undesirable when data is limited and interpretability is key , like in medical imaging . Further , they fail to generate very fine-grained local adaption as can be achieved with a well-regularized DSFN , illustrated with the edge-detection experiment . This makes them inferior for tasks like segmentation , even in unlimited data scenarios . Why are certain frames more suitable than others ? Suitable frames are the ones have been designed with general image properties in mind . Each frame performs a change of basis that can be seen as a rotation of the inputs basis . Natural image statistics based frames are thus a good idea ( like the Gauss-Frame ) while rotating to an arbitrary frame ( like polynomials ) can hurt performance , but depending on the task at hand a decrease in 1 % of classification performance can also be acceptable , given that different frames exhibit properties the pixel-basis does not have , for example steerability . We have made the Cifar10 experiments section clearer in this regard and added the SOTA Densenet to the table . We show that it behaves similar to the ResNets , thus our observations generalize to different models and model sizes . Our main purpose of this experiment is to show that suitable frames can outperform the standard pixel-basis , while they also exhibit multiple beneficial properties like steerability . We thus do agree with the reviewer , that the contribution associated with this insight has to be restated . We have changed it to : \u201c Suitable choices of frames lead to improved performance. \u201d What are runtime considerations ? Frame-based CNNs have the same runtime as vanilla CNNs . The Dynamic Steerable Frame Networks have the same runtime as vanilla Dynamic Filter Networks . However , due to separability and strategies like recursive filtering , frame-bases show much promise to be faster than standard filter formulations , but we leave this for future work . Figure 3 We have simplified figure 3 based on the reviewer 's suggestions . We believe merging different blocks and staying very close to the main equation of the paper ( eq.7 ) and further simplification helped to increase the readability of it significantly , we thank the reviewer for the suggestions . Thank you once again . We have reworked the manuscript according to your comments and believe they were instrumental for improving it substantially . Let us know if we have answered all your questions !"}, "2": {"review_id": "H178hw9ex-2", "review_text": "This works applies steerable frames for various tasks where convolutional neural networks with location invariant operators are traditionally applied. Authors provide a detailed overview of steerable frames followed with an experimental section which applies dynamic steerable network to small machine learning problems where the steerability is conceptually useful. Even though the evaluation is performed only on few small tasks, the reason why more tasks were not evaluated is that piece-wise pose invariance is needed only for a subset of tasks. The fact, that simply using overcomplete bases as a sort of \"feature pre-processing\" improves the results for already highly optimized ResNet and DenseNet architectures is quite interesting achievement. For the edge detection, a relatively hard baseline is selected - the Dynamic Filter Networks, which already attempts to achieve position invariant filters. The fact that DSFN improves the performance on this task verifies that regressing the parametrization of the steerable filters yields better results than regressing the filters directly. In the last experiment authors apply the network to video classification using LSTMs and they show that the improved performance is not due to increased capacity of the network. In general, it is quite interesting work. Even though it does not offer ground-breaking results (mainly in a sense of not performing experiments on larger tasks), it is theoretically interesting and shows promising results. There are few minor issues and suggestions related to the paper: * For the LSTM experiment, in order to be more exact, it would be useful to include information about total number of parameters, as the network which estimates the pose also increases the number of parameters. * Would it be possible to provide more details about how the back-propagation is done through the steerable filters? * For the Edge Detection experiment, it would be useful to provide results for some standard baseline - e.g. CNN with a similar number of parameters. Simply to see how useful it is to have location-variant filters for this task. * The last sentence in second paragraph on page 1 is missing a verb. Also it is maybe unnecessary. * The hyphenation for ConvNet is incorrect on multiple places (probably `\\hyphenation{Conv-Net}` would fix it). ", "rating": "7: Good paper, accept", "reply_text": "Thank you very much for your positive review and for acknowledging the usefulness of the approach . We have included your comments into the revision of the paper . * For the LSTM experiment , in order to be more exact , it would be useful to include information about total number of parameters , as the network which estimates the pose also increases the number of parameters . We have added the number of parameters to the table . The 1-Layer convLSTM has 905k parameters and the DSFN convLSTM has 907k parameters , while the 2-Layer convLSTM has 913k parameters . Thus , our proposed method comes with a very small parameter cost , as it mainly consists of 1x1 convolutions performed on the output of a convolution with fixed frame functions . * Would it be possible to provide more details about how the back-propagation is done through the steerable filters ? We will add an example for the backpropagation to the appendix . * For the Edge Detection experiment , it would be useful to provide results for some standard baseline - e.g.CNN with a similar number of parameters . Simply to see how useful it is to have location-variant filters for this task . We have added an Autoencoder baseline , it performance substantially worse than the locally adaptive methods . Further , we have added a Dynamic Filter Network who 's input is a steerable frame , doing so improves the performance of the DFN significantly , illustrating that the DFN does not seem to be capable of discovering a continuously steerable frame by itself . Most effective , however , is the full DSFN , cutting the error of the frame-based DFN again in half . * The last sentence in second paragraph on page 1 is missing a verb . Also it is maybe unnecessary . Thank you very much for pointing this out . We have rewritten this paragraph incorporating the comments of reviewer 3 and thus the sentence has been removed . * The hyphenation for ConvNet is incorrect on multiple places ( probably ` \\hyphenation { Conv-Net } ` would fix it ) . We have removed the term ConvNet and replaced it with CNN . Thank you once again for your constructive and positive feedback . Let us know if this answers all your questions !"}}