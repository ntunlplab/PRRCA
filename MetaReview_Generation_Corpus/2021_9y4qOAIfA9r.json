{"year": "2021", "forum": "9y4qOAIfA9r", "title": "Does injecting linguistic structure into language models lead to better alignment with brain recordings?", "decision": "Reject", "meta_review": "This paper explores the effect on decoding accuracy (predicting hidden representations from fMRI datasets) from fine tuning models by injecting structural bias.  This paper specifically focuses the attention of BERT on syntactic features of the text, which (for one dataset) appears to improve the decoding performance.  The paper's motivation is strong, and complex concepts are communicated clearly.\n\nThe review period was very productive.  There were some questions about analyses, and the validity of the statistical tests, but through some very thorough back and forth with the reviewers, this seems to have been resolved.  There is a good amount of analysis done on the resulting language models to try and determine the impact of finetuning or attention on the models. However, the results on the fMRI two datasets appear to be very different, and it's unclear why (and isn't clearly related back to the extensive language model analyses).  We would have liked to have seen a more thorough analysis of the stark difference in performance, and some convincing explanations for the difference based on the analyses.  \n\n\nP.s. A minor point, but the Wehbe paper uses Chapter 9 of Harry potter, not chapter 2.\n", "reviews": [{"review_id": "9y4qOAIfA9r-0", "review_text": "Summary of paper : the authors explore adding a soft structural attention constraint to BERT , by penalizing attention weights that are substantially different from a head\u2013dependent `` adjacency '' matrix derived from dependency parses . BERT is then fine-tuned with and without ( `` domain-finetuned '' ) this constraint on corpus data for which fMRI recordings from participants during reading are available . A linear classifier from the final layer of BERT 's embedding ( mean-pooled ) is then learned to the fMRI data . Within this pipeline , domain-finetuned models are not an improvement over unfinetuned BERT , but fine-tuning with the structural attention constraint improves decoding to fMRI data , especially for word-level data ( the Wehbe2014 dataset ) . Assessment : this is a nice paper that investigates an intuitive method of incorporating syntax-based , structural soft attention constraints into Transformer encoder models for language . What makes the contribution fairly distinctive is evaluation on alignment with human fMRI recordings during comprehension of the texts . The results show improvements in decoding relative to baseline models that involve no fine-tuning and/or domain-adaptation fine-tuning alone ( no structural attention constraints ) , especially for fMRI data that are recorded below the sentence level . The authors also evaluate the effect of fine-tuning on targeted syntactic evaluations from Marvin & Linzen ; the results here are not particularly conclusive . Overall , this is a potentially solid , if not ground-breaking , contribution . However , there are a number of technical questions that are left unclear in the submission , and some of the results are cause for some concern . These concerns need to be addresed in order for the submission to be fully satisfactory . The single biggest concern is the extraordinarily high word perplexity scores in Table 2 for Wehbe2014 -- which get much , much worse after fine-tuning . It is important to understand what 's going on here in order to make sense of the core potential contribution of the paper , because it 's only in the Wehbe2014 dataset where there seem to be appreciable improvements in decoding performance . I would guess that the high perplexity comes from poor prediction of the proper nouns in the Harry Potter book chapter . Maybe there needs to be some amount of fine-tuning of the models to the domain of the test-set corpus . Overall , the paper needs more clarity on why it is only the Wehbe2014 dataset where the perplexity is so high and the fine tuning affects decoding performance so much . Additional technical questions : 1 ) How is the split of a word into word pieces handled in the adjacency matrix representing word\u2013word dependencies ? 2 ) How are the adjacency matrix and each head 's attention weight matrix converted into a distribution for computing cross-entropy loss ? Are the entries normalized globally ? By row ? By column ? 3 ) What are the perplexities like for domain-finetuned ( no structural attention constraint ) BERT ? These are missing from Table 2 ( Appendix B ) , but are potentially important in interpreting your results . 4 ) What words are pooled over for the Wehbe2014 analyses -- the four words in the 2-second window ? 5 ) Section 4.1 reports that UD and DM finetuned models are significantly better in brain decoding than the un-finetuned baseline , at p < 0.0001 , but the 95 % confidence intervals for subject scores look very different . And the difference in mean decoding performance for DM finetuning is barely visible . How are you computing your confidence intervals and your p-values ? Why are they so different , and how are you getting such high confidence in improvements over the unfinetuned baseline here ? 6 ) How do your results compare to those using the best fine-tuning methods from Gauthier & Levy ( 2019 ) , which involve scrambling the input sentences ? 7 ) Given that in Wehbe2014 each fMRI image corresponds to four words , most of which probably contain both function and content words , how is the content/function word analysis defined and performed ? Additional comments : * the authors write that `` increase in perplexity roughly corresponds to lower brain decoding scores '' , but this does n't look consistent with Table 2 and Figure 3 . For Wehbe2014 , UCCA data yield the worst decoding accuracy but yield better perplexity than DM data , which yield decoding accuracy only slightly worse than the UD data . The monotonicity is cleaner for Pereira2018 but the overall differences in decoding performance are much smaller .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear reviewer # 3 , we truly appreciate your comprehensive and thoughtful review , which has already helped us improve this work . Please let us know if you have any additional comments or questions . # # # # Regarding word perplexity , there are two important clarifications to make : # # # # - We have found an explanation for the anomalously high perplexity scores . In the results reported in Table 2 , the exponentiation of the log-likelihood term is being applied per sentence ( i.e.over the average word log-likelihood per sentence ) , rather than over the entire dataset . - The results in Table 2 are actually for the domain-finetuned baselines , i.e.the models fine-tuned on each formalism \u2019 s corpus without the structural attention constraint . This was not sufficiently clear . We have now adjusted the method by which the perplexity was being calculated , and included the results for both the domain-fintuned baselines and the structurally biased models . Please find the results below ( we will also update them in the appendix ) : * PRE . : pretrained * DF-B : domain-finetuned baseline * GA : guided attention finetuning * Pereira et al . ( 2018 ) * -- * * PRE * * 14.09 -- * * DF-B DM * * 19.11 * * DF-B UD * * 19.08 * * DF-B UCCA * * 20.67 -- * * GA DM * * 20.82 * * GA UD * * 17.15 * * GA UCCA * * 17.47 * Wehbe et al . ( 2014 ) * -- * * PRE * * 34.79 -- * * DF-B DM * * 36.11 * * DF-B UD * * 38.41 * * DF-B UCCA * * 40.45 -- * * GA DM * * 33.24 * * GA UD * * 37.16 * * GA UCCA * * 33.60 We now observe the following : - Our main conclusion re . the effect of domain remains unchanged : simply running MLM finetuning on each of the texts of the three datasets ( UD , DM , and UCCA ) leads to higher perplexity scores on the fMRI stimuli texts . Moreover , except in the case of DM for Pereira 2018 , the models finetuned via MLM + guided attention ( GA ) , have lower perplexities than their domain-finetuned baseline counterparts . - As you correctly note , there is , overall , no clear correspondence between lower perplexity and higher brain decoding scores -- although we find a tendency for the domain-finetuned baselines , where a higher decoding score ( descending rank , P2018 : UD > DM > UCCA ; W2014 : DM > UD > UCCA ) corresponds to a lower perplexity ( ascending ranking , P2018 : UD > DM > UCCA ; W2014 : DM > UD > UCCA ) . This does not hold for the structurally biased models ( as domain is , perhaps , no longer the primary factor involved ) ."}, {"review_id": "9y4qOAIfA9r-1", "review_text": "This paper tests whether fine-tuning large pre-trained language models ( LMs ) with structural information can increase the correlation between these representations and the representations of brain activity measured while processing the same stimuli . The injection of the structural information is done through fine-tuning of the pre-trained model by `` guided attention '' , which makes use of binary relations between the words according to three different syntactic or semantic formalisms . The authors map the brain activity to each of the alternative LM representations via a regression model , and measure the alignment by using correlation between the predicted ( from brain activity ) and actual output of the alternative models . The results show that under certain conditions representations learned through guided attention aligns better with the representations of brain activity . In general the investigates an interesting question which may be ( eventually ) relevant to both understanding the way humans process language , and possibly building better computational models . The method followed in the study is ( mostly ) sound , and the paper is written well . A potential issue with the method is the direction of the prediction in `` brain decoding '' regression ( section 3.5 ) . Authors predict the model representations from the `` brain representation '' ( this seems to be based on earlier studies , but I did not verify ) . In my opinion the reverse is more meaningful . Since the invariant quantity in this study is the representations coming from the brain imaging . This is important , because the success of the regression is not only about the amount of information in the predictors , but also simplicity of the task . Hence , an alteration of the model representations that simplifies them may result in better predictions , and hence , higher correlations Except the above , I have some ( mostly minor ) comments : - I would be happier with a bit more explicit discussion of the main results . After reading the articles , I am still not sure what to take away from the main experiments . The effect on two different data sets ( also means representations at different levels/units ) are quite different - not allowing a clear conclusion . Side issues discussed ( the effects of the use of different formalisms , the effect of domain , particular syntactic patterns , content vs. function words are also relatively brief and far from being conclusive ) . I think a clearer discussion of the main results , and investigation of reasons for the discrepancy between the data sets would make the paper stronger . - It would help if the data is explained slightly better . Particularly , it would make the paper more self contained if the authors specify whether any of the data sets ( section 3.3 ) had automatic annotation . On a somewhat related note , comparisons between the formalisms seem to correlate with the data sizes , which is not pointed out in the paper . - A few language/typography issues/suggestions : - I am not sure about the ICLR guidelines , but avoiding citations in the abstract is a good idea ( abstracts should stand alone ) . - Footnote marks should go after punctuation ( footnote mark 8 ) - Conclusions line 3 : `` attention guided '' - > `` guided attention '' ? - There are case ( normalization ) issues in the references : `` groningen '' , `` erp '' , `` bert '' ( not exhaustive , a through check is recommended ) .", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer # 2 , we thank you for your appreciation of our work and your helpful comments/suggestions , which we aim to address : - Regarding the direction of prediction in the regression between the brain and model representations : in addition to comparing the regression performance during brain decoding , we have also evaluated all models on a range of syntactic probing tasks proposed by Marvin & Linzen ( 2019 ) . From these evaluations , we observe that after attention-guided fine-tuning : a ) two of the guided-attention models have a higher score than the pretrained baseline and the domain-finetuned baselines for most tasks and b ) the ranking of the models corresponds to their ranking on the brain decoding task ( DM > UD > UCCA ) . Taking both the brain decoding results and these syntactic probing results together , we argue that the guided-attention has altered the model representations in a beneficial way that is beyond just simplifying the representations in a task-irrelevant way . However , we agree with the reviewer that investigating the opposite direction of prediction ( from the model representations to the brain representations ) is also informative , and indeed this is a recently popular direction ( Toneva and Wehbe , 2019 ; Schwartz et al.2019 , Schrimpf et al.2020 ) that will make for excellent future application of our proposed method . - We will clarify and consolidate our discussion sections to better highlight the conclusions . - Regarding the data , it is all manually annotated by expert annotators .. We had cut the section short to save space , but will now include the additional information . Fine-tuning data size is indeed correspondent to decoding score ( for Wehbe 2014 ) and even to performance on ( most of ) the subject-verb agreement tasks . We will add mention of this ."}, {"review_id": "9y4qOAIfA9r-2", "review_text": "This paper describes experiments that inject linguistic information ( for example dependency structures ) into BERT , then measure improvements in correlation with FMRI measurements of humans reading an underlying sentence ( which is also analyzed by BERT ) . Linguistic information is incorporated by biasing attention heads to line up with dependency ( or other ) structures . Positives about the paper : it 's an interesting experiment to try , and an important direction of work . Negatives : * It 's a somewhat small increment over previous work , not sure it merits a full conference paper . As it stands the paper presents the approach and results , with little inspection of why improvements are seen . I would like the authors to go much deeper with the analysis . Are there particular syntactic constructions that are being better modeled ? Is the new model much more sensitive to long range dependencies , as found in syntactic structures ? Are particular classes of words effected more than others ? Answering these questions will be challenging but would add a lot to the paper . * Most importantly , the evaluation metrics are unclear . The critical sentence in the paper is `` To evaluate the regression models , Pearson \u2019 s correlation coefficient between the predicted and the corresponding heldout true sentence or word representations is computed '' . This is a terse description of a critical part of the approach , and I ca n't make sense of it . Part of my unease about the evaluation is the following . The matrix $ D_ { fr } $ is the output from BERT . Importantly in The definition of L_ { ifr } this matrix is predicted from the `` brain '' matrix B_i . If $ D_ { fr } $ was all zeros it would be trivially predictable ( and hence gameable ) . In the original Gauthier and Levy paper they appear to use metrics in addition to MSE . In this paper some variant of Pearson 's correlation coefficient is used - but I ca n't understand what exactly this is , and my worry is that it is trivially gameable in the same way as MSE .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer # 4 , we thank you for your helpful comments and feedback . Regarding the evaluation metrics : We report pearson \u2019 s r correlation , employing it as a bounded , invariant measure of representational similarity . In general , this is of course , yes , vulnerable to \u2018 trivial gaming \u2019 , as instanced in your all zeroes example . In our case , however , there is little risk of that occurring , as : A ) The models are not directly fine-tuned to become more similar to B_i , so should not learn a 'trivial solution ' . B ) Even if there could still , theoretically , be a confound where D_fr becomes more `` simple '' /trivially predictable due to fine-tuning , we believe this is clearly not the case , as the fine-tuned models are able to induce representations which outperform the non-fine-tuned BERT on the targeted-syntactic evaluation tasks . Furthermore , we have also computed the rank-based metric from Gauthier and Levy which gives the rank of a ground-truth sentence representation in the list of nearest neighbors ( computed via cosine similarity ) of a predicted sentence representation . We found a strong correspondence between this and the metric we have reported in the paper ( which was more stable across subjects , and between datasets ) , therefore omitted it from the paper for the sake of clarity and space . However , you are correct that including it would offer a more complete picture . We thank you for raising this point . Please find these results for Wehbe 2014 in the table below ( we will add this and a similar table for Pereira 2018 to the appendix ) : Pre . : pretrained Df : domain-finetuned Ag : attention-guided finetuning Wehbe 2014 ( Mean and Median ranks are out of a total of 4369 words in dataset ) : | Model/Metric | Pearson r | Mean Rank | Median Rank | | | -- | -- |- | | Pre . | 0.225 | 436.70 | 53.13 | | | -- | -- |- | | Df-baseline-dm | 0.204 | 493.11 | 89.32 | | Df-baseline-ud | 0.206 | 497.24 | 81.69 | | Df-baseline-ucca | 0.164 | 689.89 | 227.30 | | | -- | -- |- | | Ag-dm | 0.343 | 172.45 | 10.96 | | Ag-ud | 0.280 | 255.127 | 18.28 | | Ag-ucca | 0.261 | 315.73 | 25.78 | The table shows that the models which have higher Pearson r scores , also have a lower average ground truth word/sentence nearest neighbour rank i.e.induce representations that better support contrasts between sentences/words which are relevant to the brain recordings . We hope that this adequately addresses your unease re . the methodology of evaluation . Regarding the first point , we would like to respectfully dispute the characterization of the work \u2019 s contribution as incremental : A ) we present a novel approach which allows for targeted evaluation of particular structural hypotheses from linguistic theory regarding the composition of meaning in the brain ; B ) utilising this , we conduct a carefully controlled evaluation involving three different syntactic and semantic linguistic formalisms across two fMRI datasets of different granularities ; C ) we then present an analysis of a variety of factors including textual domain , ability to model different syntactic constructions , and word class ( content vs. function ) . Naturally , we agree that a deeper analysis is of interest . The scope of our analysis is necessarily restricted both by space and the amount of information one can reasonably include in an already packed work . We are currently conducting a fine grained analysis of the fine-tuned and non-fintuned models \u2019 representation of semantic information , as suggested by Reviewer # 1 , and will include it ."}, {"review_id": "9y4qOAIfA9r-3", "review_text": "An interesting paper that discusses whether injecting three types of syntactic and semantic formalisms lead to better alignment with how language is processed in the brain . The authors conduct experiments with the BERT model and two fMRI datasets and show that including linguistic structure through fine-tuning can improve brain decoding performance . The paper would be improved by experimenting with language models other than BERT , as it is not clear at the moment whether the produced results are generalizable to different language models or are BERT-specific . For example , additional experiments with AlBert , distilBert and RoBerta would provide additional insights on the effect of size of the model , in terms of the number of parameters . Comparison of Bert to GPT and XLNet would emphasize the advantages/disadvantages of autoencoder-based vs autoregressive models and could potentially provide additional insight on how attention is represented in human brain . It would also be interesting to read a discussion of semantic analysis , as currently the paper concentrates the most on syntactic formalism as represented in both BERT and fMRI data . Specifically , it would be interesting to know if the injection of syntax impacts the semantic representations . One of the possible methods to measure that would be probing for semantic classes ( as in Yaghoobzadeh et al. , 2019.Probing for Semantic Classes )", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer # 1 , we thank you for your appreciation of our work and for your helpful suggestions on how to improve it . We agree that expanding the scope of the experiments to other language models could potentially yield interesting conclusions regarding the interaction of structural bias with model size and architecture . We made a conscious choice to focus in this work on evaluating across multiple linguistic formalisms and on presenting results for more than one imaging dataset , since these two facets of our investigation were more immediately crucial to the core research questions . However , we see extensions along the \u2018 architecture/training objective \u2019 dimension as an important next step that we would very much like to address in follow-up work . Analysing the effect of structural bias on the models \u2019 encoding of semantics could indeed potentially allow for a deeper understanding of the factors which lead to a better alignment with the brain recording data . The task proposed in Yaghoobzadeh et al. , 2019 appears to perhaps be better suited for non-contextualized word embeddings , than for contextualized ones . However , we are currently running an analysis using the Semantic Tagging task ( [ 1 ] ) , which involves assigning a one of 80 fine-grained \u2018 semantic tags \u2019 which cover a broad range of semantic classes ( e.g . : discourse relations , logical semantics Anaphora , named entities , etc . ) , and describe the \u201c semantic contribution of the token with respect to the meaning of the source expression \u201d . We will report the results of this analysis and include it in the paper over the next few days . We thank you for the suggestion ! 1 : https : //www.aclweb.org/anthology/W17-6901/"}], "0": {"review_id": "9y4qOAIfA9r-0", "review_text": "Summary of paper : the authors explore adding a soft structural attention constraint to BERT , by penalizing attention weights that are substantially different from a head\u2013dependent `` adjacency '' matrix derived from dependency parses . BERT is then fine-tuned with and without ( `` domain-finetuned '' ) this constraint on corpus data for which fMRI recordings from participants during reading are available . A linear classifier from the final layer of BERT 's embedding ( mean-pooled ) is then learned to the fMRI data . Within this pipeline , domain-finetuned models are not an improvement over unfinetuned BERT , but fine-tuning with the structural attention constraint improves decoding to fMRI data , especially for word-level data ( the Wehbe2014 dataset ) . Assessment : this is a nice paper that investigates an intuitive method of incorporating syntax-based , structural soft attention constraints into Transformer encoder models for language . What makes the contribution fairly distinctive is evaluation on alignment with human fMRI recordings during comprehension of the texts . The results show improvements in decoding relative to baseline models that involve no fine-tuning and/or domain-adaptation fine-tuning alone ( no structural attention constraints ) , especially for fMRI data that are recorded below the sentence level . The authors also evaluate the effect of fine-tuning on targeted syntactic evaluations from Marvin & Linzen ; the results here are not particularly conclusive . Overall , this is a potentially solid , if not ground-breaking , contribution . However , there are a number of technical questions that are left unclear in the submission , and some of the results are cause for some concern . These concerns need to be addresed in order for the submission to be fully satisfactory . The single biggest concern is the extraordinarily high word perplexity scores in Table 2 for Wehbe2014 -- which get much , much worse after fine-tuning . It is important to understand what 's going on here in order to make sense of the core potential contribution of the paper , because it 's only in the Wehbe2014 dataset where there seem to be appreciable improvements in decoding performance . I would guess that the high perplexity comes from poor prediction of the proper nouns in the Harry Potter book chapter . Maybe there needs to be some amount of fine-tuning of the models to the domain of the test-set corpus . Overall , the paper needs more clarity on why it is only the Wehbe2014 dataset where the perplexity is so high and the fine tuning affects decoding performance so much . Additional technical questions : 1 ) How is the split of a word into word pieces handled in the adjacency matrix representing word\u2013word dependencies ? 2 ) How are the adjacency matrix and each head 's attention weight matrix converted into a distribution for computing cross-entropy loss ? Are the entries normalized globally ? By row ? By column ? 3 ) What are the perplexities like for domain-finetuned ( no structural attention constraint ) BERT ? These are missing from Table 2 ( Appendix B ) , but are potentially important in interpreting your results . 4 ) What words are pooled over for the Wehbe2014 analyses -- the four words in the 2-second window ? 5 ) Section 4.1 reports that UD and DM finetuned models are significantly better in brain decoding than the un-finetuned baseline , at p < 0.0001 , but the 95 % confidence intervals for subject scores look very different . And the difference in mean decoding performance for DM finetuning is barely visible . How are you computing your confidence intervals and your p-values ? Why are they so different , and how are you getting such high confidence in improvements over the unfinetuned baseline here ? 6 ) How do your results compare to those using the best fine-tuning methods from Gauthier & Levy ( 2019 ) , which involve scrambling the input sentences ? 7 ) Given that in Wehbe2014 each fMRI image corresponds to four words , most of which probably contain both function and content words , how is the content/function word analysis defined and performed ? Additional comments : * the authors write that `` increase in perplexity roughly corresponds to lower brain decoding scores '' , but this does n't look consistent with Table 2 and Figure 3 . For Wehbe2014 , UCCA data yield the worst decoding accuracy but yield better perplexity than DM data , which yield decoding accuracy only slightly worse than the UD data . The monotonicity is cleaner for Pereira2018 but the overall differences in decoding performance are much smaller .", "rating": "6: Marginally above acceptance threshold", "reply_text": "Dear reviewer # 3 , we truly appreciate your comprehensive and thoughtful review , which has already helped us improve this work . Please let us know if you have any additional comments or questions . # # # # Regarding word perplexity , there are two important clarifications to make : # # # # - We have found an explanation for the anomalously high perplexity scores . In the results reported in Table 2 , the exponentiation of the log-likelihood term is being applied per sentence ( i.e.over the average word log-likelihood per sentence ) , rather than over the entire dataset . - The results in Table 2 are actually for the domain-finetuned baselines , i.e.the models fine-tuned on each formalism \u2019 s corpus without the structural attention constraint . This was not sufficiently clear . We have now adjusted the method by which the perplexity was being calculated , and included the results for both the domain-fintuned baselines and the structurally biased models . Please find the results below ( we will also update them in the appendix ) : * PRE . : pretrained * DF-B : domain-finetuned baseline * GA : guided attention finetuning * Pereira et al . ( 2018 ) * -- * * PRE * * 14.09 -- * * DF-B DM * * 19.11 * * DF-B UD * * 19.08 * * DF-B UCCA * * 20.67 -- * * GA DM * * 20.82 * * GA UD * * 17.15 * * GA UCCA * * 17.47 * Wehbe et al . ( 2014 ) * -- * * PRE * * 34.79 -- * * DF-B DM * * 36.11 * * DF-B UD * * 38.41 * * DF-B UCCA * * 40.45 -- * * GA DM * * 33.24 * * GA UD * * 37.16 * * GA UCCA * * 33.60 We now observe the following : - Our main conclusion re . the effect of domain remains unchanged : simply running MLM finetuning on each of the texts of the three datasets ( UD , DM , and UCCA ) leads to higher perplexity scores on the fMRI stimuli texts . Moreover , except in the case of DM for Pereira 2018 , the models finetuned via MLM + guided attention ( GA ) , have lower perplexities than their domain-finetuned baseline counterparts . - As you correctly note , there is , overall , no clear correspondence between lower perplexity and higher brain decoding scores -- although we find a tendency for the domain-finetuned baselines , where a higher decoding score ( descending rank , P2018 : UD > DM > UCCA ; W2014 : DM > UD > UCCA ) corresponds to a lower perplexity ( ascending ranking , P2018 : UD > DM > UCCA ; W2014 : DM > UD > UCCA ) . This does not hold for the structurally biased models ( as domain is , perhaps , no longer the primary factor involved ) ."}, "1": {"review_id": "9y4qOAIfA9r-1", "review_text": "This paper tests whether fine-tuning large pre-trained language models ( LMs ) with structural information can increase the correlation between these representations and the representations of brain activity measured while processing the same stimuli . The injection of the structural information is done through fine-tuning of the pre-trained model by `` guided attention '' , which makes use of binary relations between the words according to three different syntactic or semantic formalisms . The authors map the brain activity to each of the alternative LM representations via a regression model , and measure the alignment by using correlation between the predicted ( from brain activity ) and actual output of the alternative models . The results show that under certain conditions representations learned through guided attention aligns better with the representations of brain activity . In general the investigates an interesting question which may be ( eventually ) relevant to both understanding the way humans process language , and possibly building better computational models . The method followed in the study is ( mostly ) sound , and the paper is written well . A potential issue with the method is the direction of the prediction in `` brain decoding '' regression ( section 3.5 ) . Authors predict the model representations from the `` brain representation '' ( this seems to be based on earlier studies , but I did not verify ) . In my opinion the reverse is more meaningful . Since the invariant quantity in this study is the representations coming from the brain imaging . This is important , because the success of the regression is not only about the amount of information in the predictors , but also simplicity of the task . Hence , an alteration of the model representations that simplifies them may result in better predictions , and hence , higher correlations Except the above , I have some ( mostly minor ) comments : - I would be happier with a bit more explicit discussion of the main results . After reading the articles , I am still not sure what to take away from the main experiments . The effect on two different data sets ( also means representations at different levels/units ) are quite different - not allowing a clear conclusion . Side issues discussed ( the effects of the use of different formalisms , the effect of domain , particular syntactic patterns , content vs. function words are also relatively brief and far from being conclusive ) . I think a clearer discussion of the main results , and investigation of reasons for the discrepancy between the data sets would make the paper stronger . - It would help if the data is explained slightly better . Particularly , it would make the paper more self contained if the authors specify whether any of the data sets ( section 3.3 ) had automatic annotation . On a somewhat related note , comparisons between the formalisms seem to correlate with the data sizes , which is not pointed out in the paper . - A few language/typography issues/suggestions : - I am not sure about the ICLR guidelines , but avoiding citations in the abstract is a good idea ( abstracts should stand alone ) . - Footnote marks should go after punctuation ( footnote mark 8 ) - Conclusions line 3 : `` attention guided '' - > `` guided attention '' ? - There are case ( normalization ) issues in the references : `` groningen '' , `` erp '' , `` bert '' ( not exhaustive , a through check is recommended ) .", "rating": "7: Good paper, accept", "reply_text": "Dear reviewer # 2 , we thank you for your appreciation of our work and your helpful comments/suggestions , which we aim to address : - Regarding the direction of prediction in the regression between the brain and model representations : in addition to comparing the regression performance during brain decoding , we have also evaluated all models on a range of syntactic probing tasks proposed by Marvin & Linzen ( 2019 ) . From these evaluations , we observe that after attention-guided fine-tuning : a ) two of the guided-attention models have a higher score than the pretrained baseline and the domain-finetuned baselines for most tasks and b ) the ranking of the models corresponds to their ranking on the brain decoding task ( DM > UD > UCCA ) . Taking both the brain decoding results and these syntactic probing results together , we argue that the guided-attention has altered the model representations in a beneficial way that is beyond just simplifying the representations in a task-irrelevant way . However , we agree with the reviewer that investigating the opposite direction of prediction ( from the model representations to the brain representations ) is also informative , and indeed this is a recently popular direction ( Toneva and Wehbe , 2019 ; Schwartz et al.2019 , Schrimpf et al.2020 ) that will make for excellent future application of our proposed method . - We will clarify and consolidate our discussion sections to better highlight the conclusions . - Regarding the data , it is all manually annotated by expert annotators .. We had cut the section short to save space , but will now include the additional information . Fine-tuning data size is indeed correspondent to decoding score ( for Wehbe 2014 ) and even to performance on ( most of ) the subject-verb agreement tasks . We will add mention of this ."}, "2": {"review_id": "9y4qOAIfA9r-2", "review_text": "This paper describes experiments that inject linguistic information ( for example dependency structures ) into BERT , then measure improvements in correlation with FMRI measurements of humans reading an underlying sentence ( which is also analyzed by BERT ) . Linguistic information is incorporated by biasing attention heads to line up with dependency ( or other ) structures . Positives about the paper : it 's an interesting experiment to try , and an important direction of work . Negatives : * It 's a somewhat small increment over previous work , not sure it merits a full conference paper . As it stands the paper presents the approach and results , with little inspection of why improvements are seen . I would like the authors to go much deeper with the analysis . Are there particular syntactic constructions that are being better modeled ? Is the new model much more sensitive to long range dependencies , as found in syntactic structures ? Are particular classes of words effected more than others ? Answering these questions will be challenging but would add a lot to the paper . * Most importantly , the evaluation metrics are unclear . The critical sentence in the paper is `` To evaluate the regression models , Pearson \u2019 s correlation coefficient between the predicted and the corresponding heldout true sentence or word representations is computed '' . This is a terse description of a critical part of the approach , and I ca n't make sense of it . Part of my unease about the evaluation is the following . The matrix $ D_ { fr } $ is the output from BERT . Importantly in The definition of L_ { ifr } this matrix is predicted from the `` brain '' matrix B_i . If $ D_ { fr } $ was all zeros it would be trivially predictable ( and hence gameable ) . In the original Gauthier and Levy paper they appear to use metrics in addition to MSE . In this paper some variant of Pearson 's correlation coefficient is used - but I ca n't understand what exactly this is , and my worry is that it is trivially gameable in the same way as MSE .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Dear Reviewer # 4 , we thank you for your helpful comments and feedback . Regarding the evaluation metrics : We report pearson \u2019 s r correlation , employing it as a bounded , invariant measure of representational similarity . In general , this is of course , yes , vulnerable to \u2018 trivial gaming \u2019 , as instanced in your all zeroes example . In our case , however , there is little risk of that occurring , as : A ) The models are not directly fine-tuned to become more similar to B_i , so should not learn a 'trivial solution ' . B ) Even if there could still , theoretically , be a confound where D_fr becomes more `` simple '' /trivially predictable due to fine-tuning , we believe this is clearly not the case , as the fine-tuned models are able to induce representations which outperform the non-fine-tuned BERT on the targeted-syntactic evaluation tasks . Furthermore , we have also computed the rank-based metric from Gauthier and Levy which gives the rank of a ground-truth sentence representation in the list of nearest neighbors ( computed via cosine similarity ) of a predicted sentence representation . We found a strong correspondence between this and the metric we have reported in the paper ( which was more stable across subjects , and between datasets ) , therefore omitted it from the paper for the sake of clarity and space . However , you are correct that including it would offer a more complete picture . We thank you for raising this point . Please find these results for Wehbe 2014 in the table below ( we will add this and a similar table for Pereira 2018 to the appendix ) : Pre . : pretrained Df : domain-finetuned Ag : attention-guided finetuning Wehbe 2014 ( Mean and Median ranks are out of a total of 4369 words in dataset ) : | Model/Metric | Pearson r | Mean Rank | Median Rank | | | -- | -- |- | | Pre . | 0.225 | 436.70 | 53.13 | | | -- | -- |- | | Df-baseline-dm | 0.204 | 493.11 | 89.32 | | Df-baseline-ud | 0.206 | 497.24 | 81.69 | | Df-baseline-ucca | 0.164 | 689.89 | 227.30 | | | -- | -- |- | | Ag-dm | 0.343 | 172.45 | 10.96 | | Ag-ud | 0.280 | 255.127 | 18.28 | | Ag-ucca | 0.261 | 315.73 | 25.78 | The table shows that the models which have higher Pearson r scores , also have a lower average ground truth word/sentence nearest neighbour rank i.e.induce representations that better support contrasts between sentences/words which are relevant to the brain recordings . We hope that this adequately addresses your unease re . the methodology of evaluation . Regarding the first point , we would like to respectfully dispute the characterization of the work \u2019 s contribution as incremental : A ) we present a novel approach which allows for targeted evaluation of particular structural hypotheses from linguistic theory regarding the composition of meaning in the brain ; B ) utilising this , we conduct a carefully controlled evaluation involving three different syntactic and semantic linguistic formalisms across two fMRI datasets of different granularities ; C ) we then present an analysis of a variety of factors including textual domain , ability to model different syntactic constructions , and word class ( content vs. function ) . Naturally , we agree that a deeper analysis is of interest . The scope of our analysis is necessarily restricted both by space and the amount of information one can reasonably include in an already packed work . We are currently conducting a fine grained analysis of the fine-tuned and non-fintuned models \u2019 representation of semantic information , as suggested by Reviewer # 1 , and will include it ."}, "3": {"review_id": "9y4qOAIfA9r-3", "review_text": "An interesting paper that discusses whether injecting three types of syntactic and semantic formalisms lead to better alignment with how language is processed in the brain . The authors conduct experiments with the BERT model and two fMRI datasets and show that including linguistic structure through fine-tuning can improve brain decoding performance . The paper would be improved by experimenting with language models other than BERT , as it is not clear at the moment whether the produced results are generalizable to different language models or are BERT-specific . For example , additional experiments with AlBert , distilBert and RoBerta would provide additional insights on the effect of size of the model , in terms of the number of parameters . Comparison of Bert to GPT and XLNet would emphasize the advantages/disadvantages of autoencoder-based vs autoregressive models and could potentially provide additional insight on how attention is represented in human brain . It would also be interesting to read a discussion of semantic analysis , as currently the paper concentrates the most on syntactic formalism as represented in both BERT and fMRI data . Specifically , it would be interesting to know if the injection of syntax impacts the semantic representations . One of the possible methods to measure that would be probing for semantic classes ( as in Yaghoobzadeh et al. , 2019.Probing for Semantic Classes )", "rating": "7: Good paper, accept", "reply_text": "Dear Reviewer # 1 , we thank you for your appreciation of our work and for your helpful suggestions on how to improve it . We agree that expanding the scope of the experiments to other language models could potentially yield interesting conclusions regarding the interaction of structural bias with model size and architecture . We made a conscious choice to focus in this work on evaluating across multiple linguistic formalisms and on presenting results for more than one imaging dataset , since these two facets of our investigation were more immediately crucial to the core research questions . However , we see extensions along the \u2018 architecture/training objective \u2019 dimension as an important next step that we would very much like to address in follow-up work . Analysing the effect of structural bias on the models \u2019 encoding of semantics could indeed potentially allow for a deeper understanding of the factors which lead to a better alignment with the brain recording data . The task proposed in Yaghoobzadeh et al. , 2019 appears to perhaps be better suited for non-contextualized word embeddings , than for contextualized ones . However , we are currently running an analysis using the Semantic Tagging task ( [ 1 ] ) , which involves assigning a one of 80 fine-grained \u2018 semantic tags \u2019 which cover a broad range of semantic classes ( e.g . : discourse relations , logical semantics Anaphora , named entities , etc . ) , and describe the \u201c semantic contribution of the token with respect to the meaning of the source expression \u201d . We will report the results of this analysis and include it in the paper over the next few days . We thank you for the suggestion ! 1 : https : //www.aclweb.org/anthology/W17-6901/"}}