{"year": "2021", "forum": "rRFIni1CYmy", "title": "End-to-End Egospheric Spatial Memory", "decision": "Accept (Poster)", "meta_review": "In this paper, the authors combine ideas from SLAM (using an Extended Kalman Filter and a state with nonlinear transitions and warping) and differentiable memory networks that store a spherical representation of the state (from the ego-centric point of view of an RL agent moving in an environment) with depth and visual features stored at each pixel and dynamics transitions corresponding to warping.\n\nThe main idea in the paper is very simple and elegant, but I will concur with the reviewers that the writing of the first version of the paper was extremely hard to understand and that the experimental section was too dense. Two subsequent revisions of the paper have dramatically improved the paper.\n\nGiven the spread of scores (R1: 6, R2: 7 and R3: 4) and the fact that only R1 and R2 have acknowledged the revisions, I will veer towards acceptance.\n", "reviews": [{"review_id": "rRFIni1CYmy-0", "review_text": "The paper considers the problem of creating spatial memory representations , which play important roles in robotics and are crucial for real-world applications of intelligent agents . The paper proposes an ego-centric representation that stores depth values and features at each pixel in a panorama . Given the relative pose between frames , the representation from the previous frame is transformed via forward warping ( using known depth values ) to the viewpoint of the current frame . The proposed approach has no learnable parameters . Experiments on a wide range of tasks show that the proposed approach outperforms baselines such as LSTM and NTM . On the positive side , the approach is positively simple , in the sense that it relies on known techniques ( EKF , forward warping , etc . ) that ensure that it is easy to implement while achieving good results in the experiments . Up to Sec.4 , I found the paper easy to follow , although some design choices could be better motivated ( e.g. , I assume that diagonal covariances are assumed for simplicity ) . The paper evaluates the proposed approach on multiple tasks and in various configurations , which is another strength of the paper . While I like the proposed approach , I also see multiple significant weaknesses : 1 ) I found the experimental evaluation nearly impossible to understand . My main problem is that I do n't understand what the different method that are evaluated are : * Given the abbreviation ESMN introduced in the abstract , I assume that ESMN is the proposed approach . ESM seems to be a variant of ESMN , but I am not sure how ESM and ESMN differ as the difference is never clearly described ( or if it is , I seem to have missed it ) . Sec.4.1.1 mentions training with a convolutional encoder in the context of ESMN , Sec.4.1.3 and Sec.4.1.4 only evaluate ESM but not ESMN , while Sec.4.2 states that `` ESM represents map-only inference , while ESMN includes convolutions for both the image-level and map-level inference '' . Unfortunately , the term `` map-level '' inference is not well-defined . Overall , I do n't understand the difference between ESM and ESMN . As a result , it is unclear to me why ESM performs worse than ESMN in Tab . 1 for DR-Ego-S but comparable for all other tasks in the table , or why ESM and not ESMN is used for some of the experiments . Similarly , what is the `` ESM-DepthAvoid '' baseline ? Does it only use depth and no features ? * Tab.1 contains a baseline called `` PO '' and I do n't understand how it works . The abstract introduces PO as an abbreviation for partial observability , but that does not seem to be an explanation for a baseline . The PO baseline performs similarly well as ESM and ESMN in Tab . 1 , which makes me wonder whether Tab . 1 really shows the superiority of the proposed approach . * I am confused by the statement `` In contrast , ESM by design stores features in the memory with meaningful indexing . The inclusion of relative cartesian co-ordinates in the memory image also effectively aligns each pixel with an associated relative translation . '' since the inclusion of such coordinates is never mentioned before . I assume ESM uses some form of handcrafted features ? 2 ) Some of the statements made in the paper seem too strong : * I do n't see how the claim `` Our memory is much more expressive than these 2D examples , with the ability to represent detailed 3D geometry in all directions around the agent . '' ( Sec.2.2 ) holds . I agree that being able to store information in 2.5D ( panorama and depth ) is more powerful than storing only top-down 2D maps . However , the allocentric maps of the references can store larger and more complicated scene parts . E.g. , if an agent would turn around a corner , ESMN would essentially be forced to forget about everything that is not directly visible anymore as it is occluded and thus not included in the memory structure anymore . As such , a point can be made that ESMN is much less expressive than for example Henriques & Vedaldi . Given that the latter evaluate on significantly more complex scenes compared to the ones used in this paper ( which do not have strong occlusions ) strengthens this impression . * Regarding the statement `` Although the most recent depth frame is also a strong signal for local obstacle avoidance , we show that the avoidance based on the full ESM geometry results in fewer collisions when tested on a variant of the drone task with the inclusion of 25 obstacles . `` : Looking at Tab . 2 , it seems to me that the standard deviation is so large for both ESM and ESM-DepthAvoid that it is unclear to me whether one is consistently better than the other . 3 ) The CodeSLAM approach from Bloesch et al.also provides a form of memory representation and I do n't understand why the paper , and its follow-up ( Zhi et al. , SceneCode : Monocular Dense Semantic Reconstruction using Learned Encoded Scene Representations , CVPR 2019 ) , is not discussed in the related work section . Overall , I believe that the paper has potential . My main criticism is that I do not feel able to properly understand the experimental evaluation . As such , it is hard to recommend acceptance . However , I am willing to increase my score if the information necessary to understand this part of the paper is provided . # # # After rebuttal phase # # # The answers provided by the authors and the revised version of the paper sufficiently address my concerns . As such , I recommend to accept the paper . I am still concerned that the experimental evaluation is very packed with multiple experiments while lacking details on the experimental setup and explanations of the baselines . Still , I feel that in the current form , the paper can be accepted .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their helpful comments and feedback . We address the main points of concern below . * * I am not sure how ESM and ESMN differ as the difference is never clearly described * * We acknowledge that this was unclear in the original submission , and have now added network diagrams clearly outlining the differences in Figure 2 . We have also slightly changed terminology to avoid such confusion . Before , we used ESM interchangeably to refer to both the standalone module and to a network using the module without convolutions beforehand , with RGB values projected into the module . We now refer to ESM purely as the standalone parameter-free memory module , and refer to the two network variants which make use of this module as ESMN-RGB and ESMN , to keep the distinctions clear . In terms of the difference between ESMN and ESMN-RGB , ESMN projects convolutional features into the ESM module , whereas ESMN-RGB directly projects RGB features from the acquired images into the ESM module . Again , Figure 2 outlines this architectural difference more clearly . * * What is the `` ESM-DepthAvoid '' baseline ? Does it only use depth and no features ? * * We have renamed this baseline as \u201c Single Depth Frame Avoidance \u201d for better clarity , and added an explanation in the obstacle avoidance section . To answer your question directly , this baseline performs obstacle avoidance using only the geometry available from the most recent depth frame . In contrast , \u201c ESM Depth Map Avoidance \u201d uses the depth information stored in the full ESM memory . * * Tab.1 contains a baseline called `` PO '' and I do n't understand how it works . * * In the original submission , PO in this context referred to a baseline which we called Partial-Omni . It did not stand for partial observability as introduced in the abstract . To avoid this clash of terminology , we now call this baseline Partial-Omni-Oracle ( PO2 ) . We have also moved the explanation of PO2 from the appendices into the main text , in section 4.1.1 . PO2 is a baseline which uses a ground-truth omni-directional camera of the same dimensions as ESM ( 90x180 ) , but with ESM re-projections used to mask the observed pixels throughout the image sequence . Quoting the revised paper : \u201c PO2 can not see regions where the monocular camera has not looked , but it maintains a pixel-perfect memory of anywhere it has looked. \u201d * * I am confused by the statement `` In contrast , ESM by design stores features in the memory with meaningful indexing . The inclusion of relative cartesian coordinates in the memory image also effectively aligns each pixel with an associated relative translation . '' since the inclusion of such coordinates is never mentioned before . I assume ESM uses some form of handcrafted features ? * * This statement in the original submission was actually intended to say \u201c the inclusion of polar coordinates in memory \u201d . We apologise for this mistake . We have modified the method section to bring earlier attention to these coordinates . We now consider the polar coordinates as part of the egosphere state . Quoting the beginning of our revised section 3 : \u201c The egosphere image consists of 2 channels for the polar and azimuthal angles , 1 for radial depth , and n for encoded features . The angles are not included in the covariance , as their values are implicit in the egosphere image pixel indices . The covariance only represents the uncertainty in depth and features at these fixed equidistant indices. \u201d We then refer to these again in the revised section 3.3 : \u201c The inclusion of polar angles , azimuthal angles and depth means the full relative polar coordinates are explicitly represented for each pixel in memory \u201d Finally , quoting our revision of the sentence which caused confusion : \u201c ESM by design stores the encoded features in memory with meaningful indexing . The ESM structure ensures that the encoded features for each pixel are aligned with the associated relative polar translation , represented as an additional feature in memory. \u201d We hope that these revisions clarify the original confusion . * * I do n't see how the claim `` Our memory is much more expressive than these 2D examples , with the ability to represent detailed 3D geometry in all directions around the agent . '' ( Sec.2.2 ) holds . * * We agree that this statement was too simplistic and strong , and we have modified the statement with a more balanced comparison . Indeed , the benefits of our approach ( detailed immediate unoccluded local geometry with orientation aware indexing ) are complementary to 2D methods ( occlusion aware planar understanding for navigation ) . The sentence now reads : \u201c Our memory instead focuses on local perception , with the ability to represent detailed 3D geometry in all directions around the agent . The benefits of our module are complementary to existing 2D methods , which instead focus on occlusion aware planar understanding suitable for navigation . \u201d"}, {"review_id": "rRFIni1CYmy-1", "review_text": "# Summary - This paper presents a method to build an ego-centric spatial memory map from an agent 's viewpoint . This map module is differentiable and can be used for a variety of tasks , such as object segmentation , or image-to-action learning in different control tasks . # Pros - + The idea of combining the ego-centric representation seems interesting and novel . + Evaluation in Image-to-Action learning shows good performance compared to baselines . # Cons - * _ [ Major ] _ The Method section is very confusing . The method is based on the EKF pipeline and modifies the particular parts of the pipeline . The authors only briefly outline the overview of the method and then jump into specific details . The text mostly focuses on improvement , and hence it is hard to judge which parts are novel and what the contribution is . Furthermore , there are errors in notation and variables that are not properly explained . u_t is first defined as incremental pose measurements and then as a control vector . Kamera intrinsic matrix K_1 is never introduced . The method section would benefit from restructuring , especially giving a clear overview of the method would help the reader to understand the method better . * _ [ Major ] _ Experiments are not well explained , and more baselines are necessary for proper evaluation . In all experiments , the experimental set-up is not well defined . For example , in the first experiment , we find out that the authors use imitation learning only from the appendix . In the second experiment , the environment used for the evaluation is not introduced . The experimental set-up needs to be clearly outlined , and the goal of the experiment needs to be provided . The choice of baselines is not well motivated . Why were LSTM and NTM chosen in the first experiment ? There is very little rationale for the choice in the submission . Do these approaches show the state-of-the-art performance on the examined tasks ? In the second experiment , the authors only use the LSTM network to train the RL agent . Various methods tackle the navigation problem in the RL domain , and hence the authors should choose better baselines for comparison . I suppose that the PO task is some form of navigation task , although the authors never explain what the PO task is . * _ [ Minor ] _ The simple approach to directly quantize pixel projections leads to artifacts in the map . It is a bit unclear why this option was chosen , especially now that progress on differentiable renderers has been made .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their helpful comments and feedback . We address the main points of concern below . * * The authors only briefly outline the overview of the method and then jump into specific details . The method section would benefit from restructuring , especially giving a clear overview of the method would help the reader to understand the method better . * * We thank the reviewer for pointing this out . We have modified the method section to introduce the module more clearly , and also extended the section to explain the integration of our ESM module in the network architectures used for the experiments . We hope this clarifies confusion , otherwise we are very happy to make further changes to benefit understanding . * * The text mostly focuses on improvement , and hence it is hard to judge which parts are novel and what the contribution is . * * We thank the reviewer for pointing this out . We have added a sentence in the introduction clearly outlining the key contribution : \u201c To the best of our knowledge , ESM is the first end-to-end trainable egocentric memory with full panoramic representation , enabling direct encoding of the surrounding scene in a 2.5D image. \u201d We hope this helps to clarify the central novelty of the method , but we are happy to make further changes to the method section if this remains unclear . * * u_t is first defined as incremental pose measurements and then as a control vector . * * We thank the reviewer for pointing this out . Our reasoning behind this was that control vectors in state estimation are usually measurements of some kind , for example , velocity or odometry measurements . Therefore , these two descriptions of u_t are not contradictory . However , to improve clarity , we now only use the term : \u201c incremental pose measurement \u201d . * * Kamera intrinsic matrix K_1 is never introduced . * * We thank the reviewer for pointing this out . The term K_1^ ( -1 ) was introduced as the inverse camera intrinsic matrix in the text just before Eq.1.We assume the reviewer would prefer K_1 be introduced before K_1^ ( -1 ) , and so we have now added a more explicit introduction to the K_1 parameter earlier in the same sentence . * * Experiments are not well explained . In all experiments , the experimental set-up is not well defined . * * We agree that the experiments were not clear in the original submission . The experiment descriptions have now been improved , and some important content has been moved from the appendices to the main body . We hope these changes help the reviewer to better understand the experiments - we are very happy to make further changes to benefit understanding . * * The experimental set-up needs to be clearly outlined , and the goal of the experiment needs to be provided . * * We have added a new paragraph at the beginning of the experiments section ( Sec 4 ) , which outlines the overall goal of the experiments . We have also improved the clarity of the experiments section . Quoting the new introductory paragraph at the beginning of section 4 : \u201c The goal of our experiments is to show the wide applicability of ESM to different embodied 3D learning tasks , where ESM outperforms existing memory baselines . We test two different applications : 1.Image-to-action learning for multi-DOF control ( Sec 4.1 ) . Here we consider drone and robot manipulator target reacher tasks using either ego-centric or scene-centric cameras.We then assess the ability for ESMN policies to generalize between these different camera modalities , and assess the utility of the ESM geometry for obstacle avoidance . We train policies both using imitation learning ( IL ) and reinforcement learning ( RL ) . 2.Object segmentation ( Sec 4.2 ) . Here we explore the task of constructing a semantic map , and the effect of changing the ESM module location in the computation graph on performance. \u201d * * In the first experiment , we find out that the authors use imitation learning only from the appendix . * * We thank the reviewer for pointing this out , this has now been moved to the main body . * * In the second experiment , the environment used for the evaluation is not introduced . * * We thank the reviewer for pointing this out . The description of the reinforcement learning experiment has now been extended , to properly introduce the environment . Quoting the revision : \u201c We therefore train both ESMN-RGB and an LSTM baseline on a simpler variant of the MR task via DQN . We refer to this variant as MR-Seq , where the manipulator must reach red , blue and then yellow spherical targets from egocentric observations , after which the episode terminates . The only other difference to MR is that MR-Seq uses 128x128 images as opposed to 32x32. \u201d We emphasize that the original MR task is modified from the RLBench reacher task , and further details about the task setup can be found in the code and paper for this benchmark ."}, {"review_id": "rRFIni1CYmy-2", "review_text": "The paper introduced Egocentric Spatial Memory Networks ( ESMN ) , a novel learning paradigm and architecture for encoding spatial memory in a sphere representation . The representation can be used for several downstream applications ranging from semantic segmentation as well as action learning for robotics applications . The formulation of the proposed approach builds on a Kalman Filter setting and encodes memory in a spherical structure . Both the formulation and design makes sense , and the downstream applications show that the proposed approach is indeed plausible . While I think the value of this paper is well justified , I do have a few comments on the paper : - I think it 'll be worthwhile to elaborate on the `` Mono '' method , including the network architectures as well as training details and such . It will also help understand which part of ESMN improves over this baseline method . - In the related work section , one argument made by authors is that ESMN and MemNN/NTM are two different paradigms for learning spatial memory . While the proposed approach ESMN is reasonable , I wonder if it is possible to have a concrete experiment comparing those two different designs . - Could the authors elaborate on Sec 4.1.1 Fig 5 ? What 's the meaning of the colorings in this figure , and how to interpret that ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their helpful comments and feedback . We address the main points of concern below . * * I think it 'll be worthwhile to elaborate on the `` Mono '' method , including the network architectures as well as training details and such . It will also help understand which part of ESMN improves over this baseline method . * * We agree , and we have added a high-level schematic of the architectural differences between Mono , LSTM/NTM , ESMN-RGB , and ESMN in Figure 2 . We have also provided the full network architectures for the imitation learning and object segmentation experiments in the Appendices . * * In the related work section , one argument made by authors is that ESMN and MemNN/NTM are two different paradigms for learning spatial memory . While the proposed approach ESMN is reasonable , I wonder if it is possible to have a concrete experiment comparing those two different designs . * * We do compare ESMN against NTM in our imitation learning experiments , the results are presented in Table . 1.For improved performance , we implement the same changes proposed by ( Wayne et al. , Unsupervised Predictive Memory in a Goal-Directed Agent , 2018 ) . These changes are fully explained in the Appendices . * * Could the authors elaborate on Sec 4.1.1 Fig 5 ? What 's the meaning of the colorings in this figure , and how to interpret that ? * * We thank the reviewer for pointing this out . We have improved the explanations of this figure both in the main text and in the figure caption . To answer the question directly , these images show the 2D representation of the features stored in the spherical ESM memory ( in the same way that an atlas shows a 2D representation of a spherical globe ) , for adjacent timesteps in different reacher tasks , going chronologically from left to right . The rows correspond to separate chronological image sequences ."}], "0": {"review_id": "rRFIni1CYmy-0", "review_text": "The paper considers the problem of creating spatial memory representations , which play important roles in robotics and are crucial for real-world applications of intelligent agents . The paper proposes an ego-centric representation that stores depth values and features at each pixel in a panorama . Given the relative pose between frames , the representation from the previous frame is transformed via forward warping ( using known depth values ) to the viewpoint of the current frame . The proposed approach has no learnable parameters . Experiments on a wide range of tasks show that the proposed approach outperforms baselines such as LSTM and NTM . On the positive side , the approach is positively simple , in the sense that it relies on known techniques ( EKF , forward warping , etc . ) that ensure that it is easy to implement while achieving good results in the experiments . Up to Sec.4 , I found the paper easy to follow , although some design choices could be better motivated ( e.g. , I assume that diagonal covariances are assumed for simplicity ) . The paper evaluates the proposed approach on multiple tasks and in various configurations , which is another strength of the paper . While I like the proposed approach , I also see multiple significant weaknesses : 1 ) I found the experimental evaluation nearly impossible to understand . My main problem is that I do n't understand what the different method that are evaluated are : * Given the abbreviation ESMN introduced in the abstract , I assume that ESMN is the proposed approach . ESM seems to be a variant of ESMN , but I am not sure how ESM and ESMN differ as the difference is never clearly described ( or if it is , I seem to have missed it ) . Sec.4.1.1 mentions training with a convolutional encoder in the context of ESMN , Sec.4.1.3 and Sec.4.1.4 only evaluate ESM but not ESMN , while Sec.4.2 states that `` ESM represents map-only inference , while ESMN includes convolutions for both the image-level and map-level inference '' . Unfortunately , the term `` map-level '' inference is not well-defined . Overall , I do n't understand the difference between ESM and ESMN . As a result , it is unclear to me why ESM performs worse than ESMN in Tab . 1 for DR-Ego-S but comparable for all other tasks in the table , or why ESM and not ESMN is used for some of the experiments . Similarly , what is the `` ESM-DepthAvoid '' baseline ? Does it only use depth and no features ? * Tab.1 contains a baseline called `` PO '' and I do n't understand how it works . The abstract introduces PO as an abbreviation for partial observability , but that does not seem to be an explanation for a baseline . The PO baseline performs similarly well as ESM and ESMN in Tab . 1 , which makes me wonder whether Tab . 1 really shows the superiority of the proposed approach . * I am confused by the statement `` In contrast , ESM by design stores features in the memory with meaningful indexing . The inclusion of relative cartesian co-ordinates in the memory image also effectively aligns each pixel with an associated relative translation . '' since the inclusion of such coordinates is never mentioned before . I assume ESM uses some form of handcrafted features ? 2 ) Some of the statements made in the paper seem too strong : * I do n't see how the claim `` Our memory is much more expressive than these 2D examples , with the ability to represent detailed 3D geometry in all directions around the agent . '' ( Sec.2.2 ) holds . I agree that being able to store information in 2.5D ( panorama and depth ) is more powerful than storing only top-down 2D maps . However , the allocentric maps of the references can store larger and more complicated scene parts . E.g. , if an agent would turn around a corner , ESMN would essentially be forced to forget about everything that is not directly visible anymore as it is occluded and thus not included in the memory structure anymore . As such , a point can be made that ESMN is much less expressive than for example Henriques & Vedaldi . Given that the latter evaluate on significantly more complex scenes compared to the ones used in this paper ( which do not have strong occlusions ) strengthens this impression . * Regarding the statement `` Although the most recent depth frame is also a strong signal for local obstacle avoidance , we show that the avoidance based on the full ESM geometry results in fewer collisions when tested on a variant of the drone task with the inclusion of 25 obstacles . `` : Looking at Tab . 2 , it seems to me that the standard deviation is so large for both ESM and ESM-DepthAvoid that it is unclear to me whether one is consistently better than the other . 3 ) The CodeSLAM approach from Bloesch et al.also provides a form of memory representation and I do n't understand why the paper , and its follow-up ( Zhi et al. , SceneCode : Monocular Dense Semantic Reconstruction using Learned Encoded Scene Representations , CVPR 2019 ) , is not discussed in the related work section . Overall , I believe that the paper has potential . My main criticism is that I do not feel able to properly understand the experimental evaluation . As such , it is hard to recommend acceptance . However , I am willing to increase my score if the information necessary to understand this part of the paper is provided . # # # After rebuttal phase # # # The answers provided by the authors and the revised version of the paper sufficiently address my concerns . As such , I recommend to accept the paper . I am still concerned that the experimental evaluation is very packed with multiple experiments while lacking details on the experimental setup and explanations of the baselines . Still , I feel that in the current form , the paper can be accepted .", "rating": "7: Good paper, accept", "reply_text": "We thank the reviewer for their helpful comments and feedback . We address the main points of concern below . * * I am not sure how ESM and ESMN differ as the difference is never clearly described * * We acknowledge that this was unclear in the original submission , and have now added network diagrams clearly outlining the differences in Figure 2 . We have also slightly changed terminology to avoid such confusion . Before , we used ESM interchangeably to refer to both the standalone module and to a network using the module without convolutions beforehand , with RGB values projected into the module . We now refer to ESM purely as the standalone parameter-free memory module , and refer to the two network variants which make use of this module as ESMN-RGB and ESMN , to keep the distinctions clear . In terms of the difference between ESMN and ESMN-RGB , ESMN projects convolutional features into the ESM module , whereas ESMN-RGB directly projects RGB features from the acquired images into the ESM module . Again , Figure 2 outlines this architectural difference more clearly . * * What is the `` ESM-DepthAvoid '' baseline ? Does it only use depth and no features ? * * We have renamed this baseline as \u201c Single Depth Frame Avoidance \u201d for better clarity , and added an explanation in the obstacle avoidance section . To answer your question directly , this baseline performs obstacle avoidance using only the geometry available from the most recent depth frame . In contrast , \u201c ESM Depth Map Avoidance \u201d uses the depth information stored in the full ESM memory . * * Tab.1 contains a baseline called `` PO '' and I do n't understand how it works . * * In the original submission , PO in this context referred to a baseline which we called Partial-Omni . It did not stand for partial observability as introduced in the abstract . To avoid this clash of terminology , we now call this baseline Partial-Omni-Oracle ( PO2 ) . We have also moved the explanation of PO2 from the appendices into the main text , in section 4.1.1 . PO2 is a baseline which uses a ground-truth omni-directional camera of the same dimensions as ESM ( 90x180 ) , but with ESM re-projections used to mask the observed pixels throughout the image sequence . Quoting the revised paper : \u201c PO2 can not see regions where the monocular camera has not looked , but it maintains a pixel-perfect memory of anywhere it has looked. \u201d * * I am confused by the statement `` In contrast , ESM by design stores features in the memory with meaningful indexing . The inclusion of relative cartesian coordinates in the memory image also effectively aligns each pixel with an associated relative translation . '' since the inclusion of such coordinates is never mentioned before . I assume ESM uses some form of handcrafted features ? * * This statement in the original submission was actually intended to say \u201c the inclusion of polar coordinates in memory \u201d . We apologise for this mistake . We have modified the method section to bring earlier attention to these coordinates . We now consider the polar coordinates as part of the egosphere state . Quoting the beginning of our revised section 3 : \u201c The egosphere image consists of 2 channels for the polar and azimuthal angles , 1 for radial depth , and n for encoded features . The angles are not included in the covariance , as their values are implicit in the egosphere image pixel indices . The covariance only represents the uncertainty in depth and features at these fixed equidistant indices. \u201d We then refer to these again in the revised section 3.3 : \u201c The inclusion of polar angles , azimuthal angles and depth means the full relative polar coordinates are explicitly represented for each pixel in memory \u201d Finally , quoting our revision of the sentence which caused confusion : \u201c ESM by design stores the encoded features in memory with meaningful indexing . The ESM structure ensures that the encoded features for each pixel are aligned with the associated relative polar translation , represented as an additional feature in memory. \u201d We hope that these revisions clarify the original confusion . * * I do n't see how the claim `` Our memory is much more expressive than these 2D examples , with the ability to represent detailed 3D geometry in all directions around the agent . '' ( Sec.2.2 ) holds . * * We agree that this statement was too simplistic and strong , and we have modified the statement with a more balanced comparison . Indeed , the benefits of our approach ( detailed immediate unoccluded local geometry with orientation aware indexing ) are complementary to 2D methods ( occlusion aware planar understanding for navigation ) . The sentence now reads : \u201c Our memory instead focuses on local perception , with the ability to represent detailed 3D geometry in all directions around the agent . The benefits of our module are complementary to existing 2D methods , which instead focus on occlusion aware planar understanding suitable for navigation . \u201d"}, "1": {"review_id": "rRFIni1CYmy-1", "review_text": "# Summary - This paper presents a method to build an ego-centric spatial memory map from an agent 's viewpoint . This map module is differentiable and can be used for a variety of tasks , such as object segmentation , or image-to-action learning in different control tasks . # Pros - + The idea of combining the ego-centric representation seems interesting and novel . + Evaluation in Image-to-Action learning shows good performance compared to baselines . # Cons - * _ [ Major ] _ The Method section is very confusing . The method is based on the EKF pipeline and modifies the particular parts of the pipeline . The authors only briefly outline the overview of the method and then jump into specific details . The text mostly focuses on improvement , and hence it is hard to judge which parts are novel and what the contribution is . Furthermore , there are errors in notation and variables that are not properly explained . u_t is first defined as incremental pose measurements and then as a control vector . Kamera intrinsic matrix K_1 is never introduced . The method section would benefit from restructuring , especially giving a clear overview of the method would help the reader to understand the method better . * _ [ Major ] _ Experiments are not well explained , and more baselines are necessary for proper evaluation . In all experiments , the experimental set-up is not well defined . For example , in the first experiment , we find out that the authors use imitation learning only from the appendix . In the second experiment , the environment used for the evaluation is not introduced . The experimental set-up needs to be clearly outlined , and the goal of the experiment needs to be provided . The choice of baselines is not well motivated . Why were LSTM and NTM chosen in the first experiment ? There is very little rationale for the choice in the submission . Do these approaches show the state-of-the-art performance on the examined tasks ? In the second experiment , the authors only use the LSTM network to train the RL agent . Various methods tackle the navigation problem in the RL domain , and hence the authors should choose better baselines for comparison . I suppose that the PO task is some form of navigation task , although the authors never explain what the PO task is . * _ [ Minor ] _ The simple approach to directly quantize pixel projections leads to artifacts in the map . It is a bit unclear why this option was chosen , especially now that progress on differentiable renderers has been made .", "rating": "4: Ok but not good enough - rejection", "reply_text": "We thank the reviewer for their helpful comments and feedback . We address the main points of concern below . * * The authors only briefly outline the overview of the method and then jump into specific details . The method section would benefit from restructuring , especially giving a clear overview of the method would help the reader to understand the method better . * * We thank the reviewer for pointing this out . We have modified the method section to introduce the module more clearly , and also extended the section to explain the integration of our ESM module in the network architectures used for the experiments . We hope this clarifies confusion , otherwise we are very happy to make further changes to benefit understanding . * * The text mostly focuses on improvement , and hence it is hard to judge which parts are novel and what the contribution is . * * We thank the reviewer for pointing this out . We have added a sentence in the introduction clearly outlining the key contribution : \u201c To the best of our knowledge , ESM is the first end-to-end trainable egocentric memory with full panoramic representation , enabling direct encoding of the surrounding scene in a 2.5D image. \u201d We hope this helps to clarify the central novelty of the method , but we are happy to make further changes to the method section if this remains unclear . * * u_t is first defined as incremental pose measurements and then as a control vector . * * We thank the reviewer for pointing this out . Our reasoning behind this was that control vectors in state estimation are usually measurements of some kind , for example , velocity or odometry measurements . Therefore , these two descriptions of u_t are not contradictory . However , to improve clarity , we now only use the term : \u201c incremental pose measurement \u201d . * * Kamera intrinsic matrix K_1 is never introduced . * * We thank the reviewer for pointing this out . The term K_1^ ( -1 ) was introduced as the inverse camera intrinsic matrix in the text just before Eq.1.We assume the reviewer would prefer K_1 be introduced before K_1^ ( -1 ) , and so we have now added a more explicit introduction to the K_1 parameter earlier in the same sentence . * * Experiments are not well explained . In all experiments , the experimental set-up is not well defined . * * We agree that the experiments were not clear in the original submission . The experiment descriptions have now been improved , and some important content has been moved from the appendices to the main body . We hope these changes help the reviewer to better understand the experiments - we are very happy to make further changes to benefit understanding . * * The experimental set-up needs to be clearly outlined , and the goal of the experiment needs to be provided . * * We have added a new paragraph at the beginning of the experiments section ( Sec 4 ) , which outlines the overall goal of the experiments . We have also improved the clarity of the experiments section . Quoting the new introductory paragraph at the beginning of section 4 : \u201c The goal of our experiments is to show the wide applicability of ESM to different embodied 3D learning tasks , where ESM outperforms existing memory baselines . We test two different applications : 1.Image-to-action learning for multi-DOF control ( Sec 4.1 ) . Here we consider drone and robot manipulator target reacher tasks using either ego-centric or scene-centric cameras.We then assess the ability for ESMN policies to generalize between these different camera modalities , and assess the utility of the ESM geometry for obstacle avoidance . We train policies both using imitation learning ( IL ) and reinforcement learning ( RL ) . 2.Object segmentation ( Sec 4.2 ) . Here we explore the task of constructing a semantic map , and the effect of changing the ESM module location in the computation graph on performance. \u201d * * In the first experiment , we find out that the authors use imitation learning only from the appendix . * * We thank the reviewer for pointing this out , this has now been moved to the main body . * * In the second experiment , the environment used for the evaluation is not introduced . * * We thank the reviewer for pointing this out . The description of the reinforcement learning experiment has now been extended , to properly introduce the environment . Quoting the revision : \u201c We therefore train both ESMN-RGB and an LSTM baseline on a simpler variant of the MR task via DQN . We refer to this variant as MR-Seq , where the manipulator must reach red , blue and then yellow spherical targets from egocentric observations , after which the episode terminates . The only other difference to MR is that MR-Seq uses 128x128 images as opposed to 32x32. \u201d We emphasize that the original MR task is modified from the RLBench reacher task , and further details about the task setup can be found in the code and paper for this benchmark ."}, "2": {"review_id": "rRFIni1CYmy-2", "review_text": "The paper introduced Egocentric Spatial Memory Networks ( ESMN ) , a novel learning paradigm and architecture for encoding spatial memory in a sphere representation . The representation can be used for several downstream applications ranging from semantic segmentation as well as action learning for robotics applications . The formulation of the proposed approach builds on a Kalman Filter setting and encodes memory in a spherical structure . Both the formulation and design makes sense , and the downstream applications show that the proposed approach is indeed plausible . While I think the value of this paper is well justified , I do have a few comments on the paper : - I think it 'll be worthwhile to elaborate on the `` Mono '' method , including the network architectures as well as training details and such . It will also help understand which part of ESMN improves over this baseline method . - In the related work section , one argument made by authors is that ESMN and MemNN/NTM are two different paradigms for learning spatial memory . While the proposed approach ESMN is reasonable , I wonder if it is possible to have a concrete experiment comparing those two different designs . - Could the authors elaborate on Sec 4.1.1 Fig 5 ? What 's the meaning of the colorings in this figure , and how to interpret that ?", "rating": "6: Marginally above acceptance threshold", "reply_text": "We thank the reviewer for their helpful comments and feedback . We address the main points of concern below . * * I think it 'll be worthwhile to elaborate on the `` Mono '' method , including the network architectures as well as training details and such . It will also help understand which part of ESMN improves over this baseline method . * * We agree , and we have added a high-level schematic of the architectural differences between Mono , LSTM/NTM , ESMN-RGB , and ESMN in Figure 2 . We have also provided the full network architectures for the imitation learning and object segmentation experiments in the Appendices . * * In the related work section , one argument made by authors is that ESMN and MemNN/NTM are two different paradigms for learning spatial memory . While the proposed approach ESMN is reasonable , I wonder if it is possible to have a concrete experiment comparing those two different designs . * * We do compare ESMN against NTM in our imitation learning experiments , the results are presented in Table . 1.For improved performance , we implement the same changes proposed by ( Wayne et al. , Unsupervised Predictive Memory in a Goal-Directed Agent , 2018 ) . These changes are fully explained in the Appendices . * * Could the authors elaborate on Sec 4.1.1 Fig 5 ? What 's the meaning of the colorings in this figure , and how to interpret that ? * * We thank the reviewer for pointing this out . We have improved the explanations of this figure both in the main text and in the figure caption . To answer the question directly , these images show the 2D representation of the features stored in the spherical ESM memory ( in the same way that an atlas shows a 2D representation of a spherical globe ) , for adjacent timesteps in different reacher tasks , going chronologically from left to right . The rows correspond to separate chronological image sequences ."}}