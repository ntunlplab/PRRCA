{"year": "2021", "forum": "6VPl9khIMz", "title": "Adaptive Stacked Graph Filter", "decision": "Reject", "meta_review": "The topic covered by the paper is timely, and the way the authors have addressed the problem seems correct. The provided empirical evidence seems to be sufficient to support the main claim of the paper. Presentation is well structured and clear.\nNotwithstanding the above merits, the proposed approach seems to confirm other similar proposals presented in the literature, so the contribution of the paper seems to be limited. Although presentation is good, it is not highlighting enough the differences w.r.t. those proposals and the basic approximation result given by Chebyshev polynomials. Especially a better theoretical characterisation w.r.t. to approximation capabilities by Chebyshev polynomials (with no truncation) would have helped to better gain understanding of the merits of the proposed approach. Finally, some of the experimental results do not seem to have a significant  statistical difference w.r.t to the baselines, so it would have helped to have the result of a statistical test.", "reviews": [{"review_id": "6VPl9khIMz-0", "review_text": "Adaptive stacked graph filter The paper proposes a relatively simple formulation for a graph convolutional filter , that has the advantage of providing useful insights on the characteristic of the considered datasets . Many points of the paper are however not convincing in the present form , mainly regarding the novelty of the proposed formulation . The paper proposes a graph convolution operator that is inspired by the well-known approximation of a graph filter using polynomials of the graph Laplacian . Pros : - The paper proposes a simple filter formulation that allows to study the dependency on the neighborhood radius on different datasets . - The visualisation of the filters is interesting . -The reported experimental results are positive , even though in many cases the improvement does not seem significant . Cons : -The proposed model is very similar to GCNII : Graph convolution by Kipf and Welling with a single scalar parameters instead of a parameter matrix + skip connections . The main difference with GCNII is the lack of the identity mapping . In fact , eq.of H^l in page 4 is very similar to eq.5 in https : //arxiv.org/pdf/2007.02133.pdf . Authors should deeply discuss the differences between their proposal and other works in literature , clarifying their novel contribution . Comments about specific sections follow . Experimental section : -In page 6 , authors state that they fix the \\theta hyper-parameter of GCNII to 0.5 , even though the recommended values are around 1.5 . Can you justify this choice ? Also , since you run the experiments on GCNII , it would be interesting to see its performance on the bipartite dataset with \\theta = 1.5 -In Table 3 , the results from literature do not report the variance . In general , it seems like the results of the proposed method and baselines are pretty close , and in many cases inside the variance range . Appendix A : the horizontal stacking variant is not explained in detail . From the figure it looks like several stacked layers with an aggregation that sums the weighted representation computed at each layer . I do n't see why this should be `` horizontal '' . Probably writing down the equations of this model would help . B.2.While authors state that for each dataset and for each run they select the hyper-parameters using the validation set , later in the same section they state that the results in the main paper are referred to the hyper-parameters in bold . I do n't understand how the hyper-parameter selection procedure is adopted . Minor : Table 3 , Chamaleon dataset . Missing bold on SGC . Texas : MLP is in bold while it should n't be Page 6 : `` Note that we also the extact '' - > we use the -- REBUTTAL I acknowledge having checked authors ' rebuttal and the revised version of the manuscript", "rating": "5: Marginally below acceptance threshold", "reply_text": "Let us first summarize and assign numbers to your comments : 1 . Too similar to GCNII . Need more discussion . 2.Page 6 , why GCNII is set at 0.5 ? Why not 1.5 ? 3.Run at 1.5 for bipartite . 4.Literature results does n't have variance . 5.Appendix A : Why `` horizontal '' ? Write down formula . 6.Discrepancy of hyperparameters . 7.Fix the results in Page 6 . 8.Improvement does n't seem significant . = We would like to say that we understand and agree with your suggestions . However , there are some minor points that we believe there is a slight misunderstanding so we hope to clarify them here . = Misunderstanding points : ( 2 , 6 ) While \\theta=1.5 is a recommended hyper-parameters for Chameleon and Texas , it is not recommended for other datasets . Actually , GCNII has different \u201c recommended \u201d hyperparameters ( \\alpha , \\theta , number of layers , weight decay ) to each dataset . In \u201c Our Experiment \u201d sections , we demonstrate that if we fix a hyperparameter , it does not adapt . We emphasize that , as specified by the end of page 6 , we reported the best results ( with recommended hyper-params by the authors ) in the literature sections of Table 2 and 3 . Please visit ` https : //github.com/chennnM/GCNII/blob/master/full.sh ` to see that GCNII uses different hyper-parameter settings for each dataset . ( 4 ) Indeed , the literature results do not have variance . In most other works , authors do not report their variance . ( 6 ) See ( 2 ) . ( 7 ) The results for SGC is due to a data entry error . We have fixed it in the updated manuscript . = Rebuttal points ( 1 ) It is true that our model is similar to GCNII , but based on that logic , ours is more similar to APPNP . The main difference to both these modes , as pointed out in page 4 , is that we train the polynomial parameters instead of the weight matrices . ( 3 ) We believe by construction of GCNII , setting $ \\theta=1.5 $ do not make it `` high-frequency '' , here is the result for bipartite from original GCNII code : `` ` cuda:0 pretrained/6be005effcde4e60abfaaf3a29de50f6.pt bipartite 0 : 50.25 1 : 51.25 2 : 49.75 3 : 46.50 4 : 52.00 5 : 49.25 6 : 44.50 7 : 51.00 8 : 44.50 9 : 49.00 Train cost : 29.2409s Test acc . :48.80 `` ` ( 5 ) We have clarified the horizontal stacking experiment in the updated manuscript . ( 8 ) Indeed some improvement is not statistically significant ( i.e. , comparable ) ; however our main contribution is that we can obtain such results by using one initialization setting rather than changing hyper-params according to datasets . We would like to express our thanks for your time and your thoughtful comments . It would be great if you can check our updated manuscript and let us know if the updated version clarified some of your concerns . A yes/no answer for each point would be suffice !"}, {"review_id": "6VPl9khIMz-1", "review_text": "Summary : The authors proposed to learn the polynomial graph filter in their model . It can be viewed as adaptively learning the propagation part of APPNP and follows by a linear transformation ( in features ) . They show the proposed model can perform well on both homophilic and heterophilic graphs . Pros : 1.The idea of adaptively learn the polynomial filter seems correct and reasonable . 2.Results on filter visualization and structural noise are interesting . Cons : 1.The proposed methodology is not novel . A very similar idea has been proposed previously . ( See Detail comments ) 2 . Problems of over-smoothing . 3.Results on experiment section ( Table 2 and 3 ) are questionable . Detail comments : While the proposed idea of adaptively learning the polynomial graph filter is interesting , it has been proposed previously in not only the GNN literature [ 2 ] but also PageRank based methods [ 3 ] . Both of them proposed the idea of adaptively learn the polynomial graph filter , or equivalently the generalized PageRank weights . Hence , I do not think the current paper is completely novel . Nevertheless , the proposed methodology seems to be the correct answer for GNN to adapt to both homophilic and heterophilic graphs . One problem of the current proposed method is that why it can avoid over-smoothing when stacking many layers ? The authors use a fixed initialization $ \\alpha = 0.5 $ which is the same as APPNP so at least at the very beginning it won \u2019 t suffer from over-smoothing . However , it is unclear how will the coefficients behave during and after training . Also , it is not clear how to initialize $ \\beta $ in the model . Furthermore , if the proposed model can indeed adaptively learn the good polynomial graph filter , why doesn \u2019 t the random initialization work ? Does that mean the implicit bias of the specific initialization proposed in the paper is necessary ? If that is the case , then I do not see why the claim of \u201c adaptive learning \u201d is correct since it is actually sensitive to the initialization . Beside the methodology and novelty , I also find the experiment section questionable . Firstly , since the main theme of the paper is learning the polynomial filter , the authors should at least compare their method with ChebNet ( GCN-Cheby ) [ 5 ] which also use polynomial filter . Note that in both [ 4 ] and [ 2 ] , they all show that ChebNet can better adapt to heterophilic graphs compare to GCN and GAT . On the other hand , according to Appendix B.4 , the authors use $ K=2 $ ( propagation step ) for APPNP . This is * NOT * the suggested hyperparameter reported in [ 1 ] ( $ K=10 $ ) . Note that the authors of [ 1 ] even show that if we choose a larger $ K\\geq 10 $ , the performance can be slightly improved on Cora , Citeseer and PubMed . In contrast , SGF use $ K=16 $ which is not a fair comparison to APPNP . There should be a experiment that compares APPNP with SGF under the same $ K $ . Finally , the authors claim the performance of most baseline methods are found in the literature . However , this is also problematic to me . Note that in the original GCN and GAT paper , the date split is much sparse then the $ 0.6/0.2/0.2 $ split proposed by the authors . Also , in the Geom-GCN paper they do test their model on Chameleon in the split $ 0.6/0.2/0.2 $ . Why is it stated as not available ? Even if we assume all the problem above can be well explained , the improvement of the proposed model seems not statistically significant . For example , on Wisconsin , Cornell and Texas , although SGF has the highest accuracy in average , the standard deviation is very large . MLP is within 1 standard deviation . Please report the confidence interval to show that the gain of SGF is indeed statistically significant . On the other hand , SGF is worse than not only SGC but also GCNII by a large margin on Chameleon . If SGF can indeed learn the near-optimal polynomial filter , then why this is the case ? At last , in the original Geom-GCN paper , they also have the Actor dataset . I think it would be great if the authors can put this result at least in the Appendix . Besides these weaknesses , I still find the paper well written . Also , the experiment on filter visualization and structural noise are quite interesting . I believe the paper can be greatly improved if all the concerns above can be addressed . Minor comments : In page 2 , the authors state that the normalized adjacency matrix with added self-loops is $ \\tilde { A } = I-D^ { -1/2 } A D^ { -1/2 } + c $ , where $ c $ is some diagonal matrix . This is incorrect . Note that when we add self-loops , the degree matrix $ D $ has to changed accordingly . Please see the correct expression in [ 1 ] for example . In page 2 , the Rayleigh quotient $ r ( \\mathcal { L } , x ) $ is defined with two input arguments but later the authors ignore $ \\mathcal { L } $ . While it is clear from the context , the notation is not rigorous . In page 1 introduction section , the authors mention that the model does not need to tune the hyper-parameters . However , in the same page contribution section , the authors mention that they use one hyper-parameter setting . According to their experiment section , I think what they mean is the previous . It would be great to clarify the ambiguity here . Reference : [ 1 ] \u201c Predict then Propagate : Graph Neural Networks meet Personalized PageRank , \u201d Klicpera et al. , ICLR 2018 . [ 2 ] \u201c Adaptive Universal Generalized PageRank Graph Neural Network , \u201d Chien et al. , arXiv:2006.07988 . [ 3 ] \u201c Adaptive diffusions for scalable learning over graphs , \u201d Berberidis et al. , In Mining and Learning with Graphs Workshop @ ACM KDD 2018 , pp . 1 , 8 2018 . [ 4 ] \u201c Generalizing Graph Neural Networks Beyond Homophily , \u201d Zhu et al. , NeurIPS 2020 . ( arXiv:2006.11468 ) [ 5 ] \u201c Convolutional neural networkson graphs with fast localized spectral filtering , \u201d Defferrard et al. , NeurIPS 2016 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Let us first summarize and assign numbers to your comments : 1 . Not novel ( pointed out one arxiv and one adaptive diffusion in 2018 ) 2 . Compare with ChebNet . 3.K=12 for APPNP ( K=2 is not fair ! ) 4.Add Actor dataset , review Chameleon . 5.How to init alpha , beta ; how do they behave during training ? 6.A = I - L + c is wrong ? 7. r ( L , x ) = r ( x ) . 8.Introduction has repetitive part about hyper-parameter setting . 9.Why can it avoid over-smoothing ? = ( 1 ) ( 2018 ) is feature less and ( 2020 ) is very similar to our Horizontal stacking case and the main difference is that they use the pagerank matrix while we provide arguments and experimental results for a general polynomial basis of both A and L. ( 2 ) It is not `` better '' than ChebNet , we included the detailed discussion in page 4 and compared with ChebNet ( our implementation ) . Both our model and ChebNet learn filters ; however , the implementation of ChebNet is sensitive to the choice of \\lambda_ { max } in the sense that if \\lambda_ { max } > 1 then the leading polynomial coefficient will be arbitrarily large as more layers are added . Also , ChebNet-GCN is clearly unfair , our implementation showed that ChebNet is fully capable of going deeper . ( 3 ) We find that K=12 and K=2 for APPNP does not differ much , so we reported the experimental result for K=2 . In the literature result section , it was the reported result for K=12 by GCNII paper . ( 4 ) We added the Actor datasets . For Chameleon , we find that graph parameters ( GCNII ) does n't affect the accuracy , but weight decay is the main contributor for higher accuracy . Hence , Chameleon result is not ideal for our case . ( 5 ) We included the behavior of \\alpha and \\beta in our updated manuscript . ( 6 ) Thank you very much for pointing out the mistake , we fixed the formulation . ( 7 ) We believe r ( L , x ) = r ( x ) is quite clear . ( 9 ) It avoids over-smoothing thanks to the parameter \\beta . As seen in figure 1 , \\beta can `` lift '' each component filter horizontally , which can migrate the over-smoothing effect automatically . Another explanation is that over-smoothing is reported for models having no skip connections . Our model has the \\beta-skip-connections to deal with the over-smoothing effect . We would like to express our thanks for your time and your thoughtful comments . It would be great if you can check our updated manuscript and let us know if the updated version clarified some of your concerns . A yes/no answer for each point would be suffice !"}, {"review_id": "6VPl9khIMz-2", "review_text": "This paper proposes to stack the graph filters with learnable polynomial parameters to construct the new graph neural network model . Generally , this paper is well organized and easy to read . Here are my concerns . 1.Essentially , this paper argues that the approximation of Chebyshev polynomials in GCN can only capture the low-frequency features in the spectral domain , and proposes a more general approximation scheme by stacking the graph filter in the spatial domain . However , the low-frequency property of GCN is highly related to the localized first-order approximation of graph convolutions . Without this first-order approximation , GCN model can capture the high-frequency information in graphs , e.g , ChebyNet [ 2 ] with large enough order K. It 's better to add more discussions/comparisons with this kind of GCNs . Moreover , my core concern is the superiority of why the proposed polynomial approximation ( in Equation 7 ) is better than the previous Chebyshev approximation from both theoretical and practical justifications . In graph signal processing , using a polynomial series to approximate the graph filter has been well studied in the literature . As pointed out by [ 1 ] , Chebyshev polynomial is a good approximator to approximate graph filters . It is better to add more justifications ( e.g. , numerical analysis ) about the proposed approximation scheme . 2.Another concern is the experiment . Dataset splitting : It seems like that this paper adopts the new splitting plan ( stratified 0.6/0.2/0.2 splits ) for all datasets . Meanwhile , the paper also reports the best results reported in the literature . However , I think it \u2019 s improper to put them in the same table since we can \u2019 t make a fair comparison under different data splitting . Moreover , I would like to see the results of SGF on the public splitting of these datasets . Hyperprameters : In Appendix B.4 , the authors claim that they follow the hyperparameter recommendation in the original paper of baselines . However , it seems that some of the given hyperparameters are not the best hyper-parameters . For example , for Cora , \\alpha of GCNII is set to 0.2 , while in Appendix B.4 , \\alpha=0.5 which inconsistent with the original paper [ 3 ] . On the other hand , In Appendix B.2 , the authors adopt the random strategy to search the hyperparameters of SGF . Since the authors re-run all the experiments of baselines in the new splits , it \u2019 s better to conduct the same hyper-parameter search process for each baseline to ensure a fair comparison . The filter parameters visualization : From the model construction perspective , since the only difference between SGF and GCNII/APPNP is the trainable filter parameters . Therefore , I \u2019 m curious about the value of \\alpha and \\beta after the training . Could you visualize the value of two parameters in each layer from SGF ? Overall , I think this paper is marginally below the acceptance threshold . [ 1 ] David K. Hammond , Pierre Vandergheynst , and Re \u0301mi Gribonval . Wavelets on graphs via spectral graph theory . Applied and Computational Harmonic Analysis , 30 ( 2 ) :129\u2013150 , 2011 . [ 2 ] Defferrard , Micha\u00ebl , Xavier Bresson , and Pierre Vandergheynst . `` Convolutional neural networks on graphs with fast localized spectral filtering . '' Advances in neural information processing systems . 2016 . [ 3 ] Chen , M. , Wei , Z. , Huang , Z. , Ding , B. , & Li , Y . ( 2020 ) .Simple and deep graph convolutional networks . arXiv preprint arXiv:2007.02133 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Let us first summarize and assign numbers to your comments : 1 . Why is it better than ChebNet ? 2.Justify the `` new '' splitting plan . 3.Justify the hyper-parameter scheme . 4.Visualize alpha , beta for each layers . = Missing a more detailed discussion on ChebNet ( as pointed out by other reviewers ) is indeed our oversight and we agree with your comments on our manuscripts . Here , we would like to clarify some of your concerns : ( 1 ) It is not `` better '' than ChebNet , we included the detailed discussion in page 4 and compared with ChebNet ( our implementation ) . Both our model and ChebNet learn filters ; however , the implementation of ChebNet is sensitive to the choice of \\lambda_ { max } in the sense that if \\lambda_ { max } > 1 then the leading polynomial coefficient will be arbitrarily large as more layers are added . ( 2 ) We think this is a slight misunderstanding . This splitting plan is not new , it is used by Geom-GCN , GCNII ( they called it full-supervised , we just say straightforward what it is ) . ( https : //openreview.net/pdf ? id=S1e2agrFvS , page 8 , paragraph 4 ) . ( 3 ) While \\theta=1.5 is a recommended hyper-parameters for Chameleon and Texas , it is not recommended for other datasets . Actually , GCNII has different \u201c recommended \u201d hyperparameters ( \\alpha , \\theta , number of layers , weight decay ) to each dataset . In \u201c Our Experiment \u201d sections , we demonstrate that if we fix a hyperparameter , it does not adapt . We emphasize that , as specified by the end of page 6 , we reported the best results ( with recommended hyper-params by the authors ) in the literature sections of Table 2 and 3 . ( 4 ) We included the visualization on page 9 and the appendix . We would like to express our thanks for your time and your thoughtful comments . It would be great if you can check our updated manuscript and let us know if the updated version clarified some of your concerns . A yes/no answer for each point would be suffice !"}, {"review_id": "6VPl9khIMz-3", "review_text": "SUMMARY : This paper addresses the problem of vertex classification using a new Graph Convolutional Neural Network ( NN ) architecture . The linear operator within each of the layers of the GNNN is formed by a polynomial graph filter ( i.e. , a matrix polynomial of either the adjacency or the Laplacian novelty ) . Rather than working on the frequency domain , the paper focuses on learning the polynomial coefficients of the filter on the vertex domain . The key novelty is the consideration of a stack architecture for which the polynomial filter is formed by the successive application ( i.e. , matrix multiplication ) of filters of order one . Numerical experiments with real datasets showcase the merits , including superior classification performance , of the proposed architecture . STRONG POINTS : The paper is timely and fits nicely the scope of the conference . The numerical experiments are convincing , offering insights , and demonstrating some of the advantages of the proposed architecture . The writing is clear , making the paper easy to follow . WEAK POINTS : Except for the numerical experiments , I find that the contribution is quite limited . The postulation of GCNN architectures based on polynomial graph filters where the focus is on learning the polynomial coefficients has been studied thoroughly in the literature . In general , the paper does a good job listing relevant works in that area , although some are missing ( e.g. , Gama - Ribeiro ) . Some of the existing works look at ARMA structures and recursive order-one filter implementations . I acknowledge that the architecture considered in those papers may not be exactly the same as the one proposed by the authors in this paper . I also appreciate that the application at hand ( vertex classification ) was not the goal of many of those papers . However , I still feel that the contribution falls short , especially for a top conference such as ICLR . In any case , I am open to change my mind if the authors are able to strengthen their theoretical claims or address my concerns in their rebuttal . I believe that the title should be changed . GCNN are not mentioned . The current title places the focus on Stacked Graph Filters . My first concern is that , within the linear paradigm ( i.e. , as polynomials of the adjacency/Laplacian matrix ) , this type of architectures have already been investigated . More importantly , the paper focuses on NN architectures , so I think it is reasonable to have that on the title . OVERALL RECOMMENDATION : Marginal reject . The paper is topical , timely , and nicely written . It addresses a problem of interest and does so with contemporary machine learning tools . The results in real-world datasets are convincing . However , the contribution and novelty are limited , falling short of the average contribution at ICLR . ADDITIONAL RECOMMENDATIONS : Being able to obtain additional theoretical results would make the contribution more solid . Further elaborating on the robustness of the architecture it is another change that would strengthen the manuscript .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Let us first summarize and assign numbers to your comments : 1 . Polynomial interpolation has been well-studied . 2.Lack of theoretical results . 3.Title should be changed . 4.Elaborate on the robustness of the model . = We agree with your constructive comments and here we just want to justify our motivation for the work . ( 1 ) It is true that polynomial interpolation has been well-studied , and it is our oversight that we did not give representative methods like ChebNet a deeper discussion . We have updated our manuscript ( page 4 and Table 2,3 ) to elaborate this point . Our main argument here is that since we are not finding interpolation points but learning the coefficients , all polynomial bases are equivalent . ( 2 ) Indeed , we are still developing theoretical results from these experiments . However , we hope that reader can see the merit of our paper that `` going back to learning polynomial '' is beneficial , especially when the training data is abundant ( Appendix A discussion ) . ( 3 ) How do you feel about the new title `` Vertex classification - Returns of the polynomials '' ? ( 4 ) We will indeed go beyond our discussion in `` 6.4 ADAPTIVITY TO STRUCTURAL NOISE '' , but perhaps it can be an entire topic of its own because of the adversarial attack literature . The new update to our manuscript is marked with dark red text for clarity . We hope this update will clarify some of your questions . We understand that going through manuscripts is time-consuming , so we truly appreciate your comments on our work . If possible , we would like to know if this updated version has clarified your concerns ( a simple yes/no for each point would be sufficient ) ."}], "0": {"review_id": "6VPl9khIMz-0", "review_text": "Adaptive stacked graph filter The paper proposes a relatively simple formulation for a graph convolutional filter , that has the advantage of providing useful insights on the characteristic of the considered datasets . Many points of the paper are however not convincing in the present form , mainly regarding the novelty of the proposed formulation . The paper proposes a graph convolution operator that is inspired by the well-known approximation of a graph filter using polynomials of the graph Laplacian . Pros : - The paper proposes a simple filter formulation that allows to study the dependency on the neighborhood radius on different datasets . - The visualisation of the filters is interesting . -The reported experimental results are positive , even though in many cases the improvement does not seem significant . Cons : -The proposed model is very similar to GCNII : Graph convolution by Kipf and Welling with a single scalar parameters instead of a parameter matrix + skip connections . The main difference with GCNII is the lack of the identity mapping . In fact , eq.of H^l in page 4 is very similar to eq.5 in https : //arxiv.org/pdf/2007.02133.pdf . Authors should deeply discuss the differences between their proposal and other works in literature , clarifying their novel contribution . Comments about specific sections follow . Experimental section : -In page 6 , authors state that they fix the \\theta hyper-parameter of GCNII to 0.5 , even though the recommended values are around 1.5 . Can you justify this choice ? Also , since you run the experiments on GCNII , it would be interesting to see its performance on the bipartite dataset with \\theta = 1.5 -In Table 3 , the results from literature do not report the variance . In general , it seems like the results of the proposed method and baselines are pretty close , and in many cases inside the variance range . Appendix A : the horizontal stacking variant is not explained in detail . From the figure it looks like several stacked layers with an aggregation that sums the weighted representation computed at each layer . I do n't see why this should be `` horizontal '' . Probably writing down the equations of this model would help . B.2.While authors state that for each dataset and for each run they select the hyper-parameters using the validation set , later in the same section they state that the results in the main paper are referred to the hyper-parameters in bold . I do n't understand how the hyper-parameter selection procedure is adopted . Minor : Table 3 , Chamaleon dataset . Missing bold on SGC . Texas : MLP is in bold while it should n't be Page 6 : `` Note that we also the extact '' - > we use the -- REBUTTAL I acknowledge having checked authors ' rebuttal and the revised version of the manuscript", "rating": "5: Marginally below acceptance threshold", "reply_text": "Let us first summarize and assign numbers to your comments : 1 . Too similar to GCNII . Need more discussion . 2.Page 6 , why GCNII is set at 0.5 ? Why not 1.5 ? 3.Run at 1.5 for bipartite . 4.Literature results does n't have variance . 5.Appendix A : Why `` horizontal '' ? Write down formula . 6.Discrepancy of hyperparameters . 7.Fix the results in Page 6 . 8.Improvement does n't seem significant . = We would like to say that we understand and agree with your suggestions . However , there are some minor points that we believe there is a slight misunderstanding so we hope to clarify them here . = Misunderstanding points : ( 2 , 6 ) While \\theta=1.5 is a recommended hyper-parameters for Chameleon and Texas , it is not recommended for other datasets . Actually , GCNII has different \u201c recommended \u201d hyperparameters ( \\alpha , \\theta , number of layers , weight decay ) to each dataset . In \u201c Our Experiment \u201d sections , we demonstrate that if we fix a hyperparameter , it does not adapt . We emphasize that , as specified by the end of page 6 , we reported the best results ( with recommended hyper-params by the authors ) in the literature sections of Table 2 and 3 . Please visit ` https : //github.com/chennnM/GCNII/blob/master/full.sh ` to see that GCNII uses different hyper-parameter settings for each dataset . ( 4 ) Indeed , the literature results do not have variance . In most other works , authors do not report their variance . ( 6 ) See ( 2 ) . ( 7 ) The results for SGC is due to a data entry error . We have fixed it in the updated manuscript . = Rebuttal points ( 1 ) It is true that our model is similar to GCNII , but based on that logic , ours is more similar to APPNP . The main difference to both these modes , as pointed out in page 4 , is that we train the polynomial parameters instead of the weight matrices . ( 3 ) We believe by construction of GCNII , setting $ \\theta=1.5 $ do not make it `` high-frequency '' , here is the result for bipartite from original GCNII code : `` ` cuda:0 pretrained/6be005effcde4e60abfaaf3a29de50f6.pt bipartite 0 : 50.25 1 : 51.25 2 : 49.75 3 : 46.50 4 : 52.00 5 : 49.25 6 : 44.50 7 : 51.00 8 : 44.50 9 : 49.00 Train cost : 29.2409s Test acc . :48.80 `` ` ( 5 ) We have clarified the horizontal stacking experiment in the updated manuscript . ( 8 ) Indeed some improvement is not statistically significant ( i.e. , comparable ) ; however our main contribution is that we can obtain such results by using one initialization setting rather than changing hyper-params according to datasets . We would like to express our thanks for your time and your thoughtful comments . It would be great if you can check our updated manuscript and let us know if the updated version clarified some of your concerns . A yes/no answer for each point would be suffice !"}, "1": {"review_id": "6VPl9khIMz-1", "review_text": "Summary : The authors proposed to learn the polynomial graph filter in their model . It can be viewed as adaptively learning the propagation part of APPNP and follows by a linear transformation ( in features ) . They show the proposed model can perform well on both homophilic and heterophilic graphs . Pros : 1.The idea of adaptively learn the polynomial filter seems correct and reasonable . 2.Results on filter visualization and structural noise are interesting . Cons : 1.The proposed methodology is not novel . A very similar idea has been proposed previously . ( See Detail comments ) 2 . Problems of over-smoothing . 3.Results on experiment section ( Table 2 and 3 ) are questionable . Detail comments : While the proposed idea of adaptively learning the polynomial graph filter is interesting , it has been proposed previously in not only the GNN literature [ 2 ] but also PageRank based methods [ 3 ] . Both of them proposed the idea of adaptively learn the polynomial graph filter , or equivalently the generalized PageRank weights . Hence , I do not think the current paper is completely novel . Nevertheless , the proposed methodology seems to be the correct answer for GNN to adapt to both homophilic and heterophilic graphs . One problem of the current proposed method is that why it can avoid over-smoothing when stacking many layers ? The authors use a fixed initialization $ \\alpha = 0.5 $ which is the same as APPNP so at least at the very beginning it won \u2019 t suffer from over-smoothing . However , it is unclear how will the coefficients behave during and after training . Also , it is not clear how to initialize $ \\beta $ in the model . Furthermore , if the proposed model can indeed adaptively learn the good polynomial graph filter , why doesn \u2019 t the random initialization work ? Does that mean the implicit bias of the specific initialization proposed in the paper is necessary ? If that is the case , then I do not see why the claim of \u201c adaptive learning \u201d is correct since it is actually sensitive to the initialization . Beside the methodology and novelty , I also find the experiment section questionable . Firstly , since the main theme of the paper is learning the polynomial filter , the authors should at least compare their method with ChebNet ( GCN-Cheby ) [ 5 ] which also use polynomial filter . Note that in both [ 4 ] and [ 2 ] , they all show that ChebNet can better adapt to heterophilic graphs compare to GCN and GAT . On the other hand , according to Appendix B.4 , the authors use $ K=2 $ ( propagation step ) for APPNP . This is * NOT * the suggested hyperparameter reported in [ 1 ] ( $ K=10 $ ) . Note that the authors of [ 1 ] even show that if we choose a larger $ K\\geq 10 $ , the performance can be slightly improved on Cora , Citeseer and PubMed . In contrast , SGF use $ K=16 $ which is not a fair comparison to APPNP . There should be a experiment that compares APPNP with SGF under the same $ K $ . Finally , the authors claim the performance of most baseline methods are found in the literature . However , this is also problematic to me . Note that in the original GCN and GAT paper , the date split is much sparse then the $ 0.6/0.2/0.2 $ split proposed by the authors . Also , in the Geom-GCN paper they do test their model on Chameleon in the split $ 0.6/0.2/0.2 $ . Why is it stated as not available ? Even if we assume all the problem above can be well explained , the improvement of the proposed model seems not statistically significant . For example , on Wisconsin , Cornell and Texas , although SGF has the highest accuracy in average , the standard deviation is very large . MLP is within 1 standard deviation . Please report the confidence interval to show that the gain of SGF is indeed statistically significant . On the other hand , SGF is worse than not only SGC but also GCNII by a large margin on Chameleon . If SGF can indeed learn the near-optimal polynomial filter , then why this is the case ? At last , in the original Geom-GCN paper , they also have the Actor dataset . I think it would be great if the authors can put this result at least in the Appendix . Besides these weaknesses , I still find the paper well written . Also , the experiment on filter visualization and structural noise are quite interesting . I believe the paper can be greatly improved if all the concerns above can be addressed . Minor comments : In page 2 , the authors state that the normalized adjacency matrix with added self-loops is $ \\tilde { A } = I-D^ { -1/2 } A D^ { -1/2 } + c $ , where $ c $ is some diagonal matrix . This is incorrect . Note that when we add self-loops , the degree matrix $ D $ has to changed accordingly . Please see the correct expression in [ 1 ] for example . In page 2 , the Rayleigh quotient $ r ( \\mathcal { L } , x ) $ is defined with two input arguments but later the authors ignore $ \\mathcal { L } $ . While it is clear from the context , the notation is not rigorous . In page 1 introduction section , the authors mention that the model does not need to tune the hyper-parameters . However , in the same page contribution section , the authors mention that they use one hyper-parameter setting . According to their experiment section , I think what they mean is the previous . It would be great to clarify the ambiguity here . Reference : [ 1 ] \u201c Predict then Propagate : Graph Neural Networks meet Personalized PageRank , \u201d Klicpera et al. , ICLR 2018 . [ 2 ] \u201c Adaptive Universal Generalized PageRank Graph Neural Network , \u201d Chien et al. , arXiv:2006.07988 . [ 3 ] \u201c Adaptive diffusions for scalable learning over graphs , \u201d Berberidis et al. , In Mining and Learning with Graphs Workshop @ ACM KDD 2018 , pp . 1 , 8 2018 . [ 4 ] \u201c Generalizing Graph Neural Networks Beyond Homophily , \u201d Zhu et al. , NeurIPS 2020 . ( arXiv:2006.11468 ) [ 5 ] \u201c Convolutional neural networkson graphs with fast localized spectral filtering , \u201d Defferrard et al. , NeurIPS 2016 .", "rating": "4: Ok but not good enough - rejection", "reply_text": "Let us first summarize and assign numbers to your comments : 1 . Not novel ( pointed out one arxiv and one adaptive diffusion in 2018 ) 2 . Compare with ChebNet . 3.K=12 for APPNP ( K=2 is not fair ! ) 4.Add Actor dataset , review Chameleon . 5.How to init alpha , beta ; how do they behave during training ? 6.A = I - L + c is wrong ? 7. r ( L , x ) = r ( x ) . 8.Introduction has repetitive part about hyper-parameter setting . 9.Why can it avoid over-smoothing ? = ( 1 ) ( 2018 ) is feature less and ( 2020 ) is very similar to our Horizontal stacking case and the main difference is that they use the pagerank matrix while we provide arguments and experimental results for a general polynomial basis of both A and L. ( 2 ) It is not `` better '' than ChebNet , we included the detailed discussion in page 4 and compared with ChebNet ( our implementation ) . Both our model and ChebNet learn filters ; however , the implementation of ChebNet is sensitive to the choice of \\lambda_ { max } in the sense that if \\lambda_ { max } > 1 then the leading polynomial coefficient will be arbitrarily large as more layers are added . Also , ChebNet-GCN is clearly unfair , our implementation showed that ChebNet is fully capable of going deeper . ( 3 ) We find that K=12 and K=2 for APPNP does not differ much , so we reported the experimental result for K=2 . In the literature result section , it was the reported result for K=12 by GCNII paper . ( 4 ) We added the Actor datasets . For Chameleon , we find that graph parameters ( GCNII ) does n't affect the accuracy , but weight decay is the main contributor for higher accuracy . Hence , Chameleon result is not ideal for our case . ( 5 ) We included the behavior of \\alpha and \\beta in our updated manuscript . ( 6 ) Thank you very much for pointing out the mistake , we fixed the formulation . ( 7 ) We believe r ( L , x ) = r ( x ) is quite clear . ( 9 ) It avoids over-smoothing thanks to the parameter \\beta . As seen in figure 1 , \\beta can `` lift '' each component filter horizontally , which can migrate the over-smoothing effect automatically . Another explanation is that over-smoothing is reported for models having no skip connections . Our model has the \\beta-skip-connections to deal with the over-smoothing effect . We would like to express our thanks for your time and your thoughtful comments . It would be great if you can check our updated manuscript and let us know if the updated version clarified some of your concerns . A yes/no answer for each point would be suffice !"}, "2": {"review_id": "6VPl9khIMz-2", "review_text": "This paper proposes to stack the graph filters with learnable polynomial parameters to construct the new graph neural network model . Generally , this paper is well organized and easy to read . Here are my concerns . 1.Essentially , this paper argues that the approximation of Chebyshev polynomials in GCN can only capture the low-frequency features in the spectral domain , and proposes a more general approximation scheme by stacking the graph filter in the spatial domain . However , the low-frequency property of GCN is highly related to the localized first-order approximation of graph convolutions . Without this first-order approximation , GCN model can capture the high-frequency information in graphs , e.g , ChebyNet [ 2 ] with large enough order K. It 's better to add more discussions/comparisons with this kind of GCNs . Moreover , my core concern is the superiority of why the proposed polynomial approximation ( in Equation 7 ) is better than the previous Chebyshev approximation from both theoretical and practical justifications . In graph signal processing , using a polynomial series to approximate the graph filter has been well studied in the literature . As pointed out by [ 1 ] , Chebyshev polynomial is a good approximator to approximate graph filters . It is better to add more justifications ( e.g. , numerical analysis ) about the proposed approximation scheme . 2.Another concern is the experiment . Dataset splitting : It seems like that this paper adopts the new splitting plan ( stratified 0.6/0.2/0.2 splits ) for all datasets . Meanwhile , the paper also reports the best results reported in the literature . However , I think it \u2019 s improper to put them in the same table since we can \u2019 t make a fair comparison under different data splitting . Moreover , I would like to see the results of SGF on the public splitting of these datasets . Hyperprameters : In Appendix B.4 , the authors claim that they follow the hyperparameter recommendation in the original paper of baselines . However , it seems that some of the given hyperparameters are not the best hyper-parameters . For example , for Cora , \\alpha of GCNII is set to 0.2 , while in Appendix B.4 , \\alpha=0.5 which inconsistent with the original paper [ 3 ] . On the other hand , In Appendix B.2 , the authors adopt the random strategy to search the hyperparameters of SGF . Since the authors re-run all the experiments of baselines in the new splits , it \u2019 s better to conduct the same hyper-parameter search process for each baseline to ensure a fair comparison . The filter parameters visualization : From the model construction perspective , since the only difference between SGF and GCNII/APPNP is the trainable filter parameters . Therefore , I \u2019 m curious about the value of \\alpha and \\beta after the training . Could you visualize the value of two parameters in each layer from SGF ? Overall , I think this paper is marginally below the acceptance threshold . [ 1 ] David K. Hammond , Pierre Vandergheynst , and Re \u0301mi Gribonval . Wavelets on graphs via spectral graph theory . Applied and Computational Harmonic Analysis , 30 ( 2 ) :129\u2013150 , 2011 . [ 2 ] Defferrard , Micha\u00ebl , Xavier Bresson , and Pierre Vandergheynst . `` Convolutional neural networks on graphs with fast localized spectral filtering . '' Advances in neural information processing systems . 2016 . [ 3 ] Chen , M. , Wei , Z. , Huang , Z. , Ding , B. , & Li , Y . ( 2020 ) .Simple and deep graph convolutional networks . arXiv preprint arXiv:2007.02133 .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Let us first summarize and assign numbers to your comments : 1 . Why is it better than ChebNet ? 2.Justify the `` new '' splitting plan . 3.Justify the hyper-parameter scheme . 4.Visualize alpha , beta for each layers . = Missing a more detailed discussion on ChebNet ( as pointed out by other reviewers ) is indeed our oversight and we agree with your comments on our manuscripts . Here , we would like to clarify some of your concerns : ( 1 ) It is not `` better '' than ChebNet , we included the detailed discussion in page 4 and compared with ChebNet ( our implementation ) . Both our model and ChebNet learn filters ; however , the implementation of ChebNet is sensitive to the choice of \\lambda_ { max } in the sense that if \\lambda_ { max } > 1 then the leading polynomial coefficient will be arbitrarily large as more layers are added . ( 2 ) We think this is a slight misunderstanding . This splitting plan is not new , it is used by Geom-GCN , GCNII ( they called it full-supervised , we just say straightforward what it is ) . ( https : //openreview.net/pdf ? id=S1e2agrFvS , page 8 , paragraph 4 ) . ( 3 ) While \\theta=1.5 is a recommended hyper-parameters for Chameleon and Texas , it is not recommended for other datasets . Actually , GCNII has different \u201c recommended \u201d hyperparameters ( \\alpha , \\theta , number of layers , weight decay ) to each dataset . In \u201c Our Experiment \u201d sections , we demonstrate that if we fix a hyperparameter , it does not adapt . We emphasize that , as specified by the end of page 6 , we reported the best results ( with recommended hyper-params by the authors ) in the literature sections of Table 2 and 3 . ( 4 ) We included the visualization on page 9 and the appendix . We would like to express our thanks for your time and your thoughtful comments . It would be great if you can check our updated manuscript and let us know if the updated version clarified some of your concerns . A yes/no answer for each point would be suffice !"}, "3": {"review_id": "6VPl9khIMz-3", "review_text": "SUMMARY : This paper addresses the problem of vertex classification using a new Graph Convolutional Neural Network ( NN ) architecture . The linear operator within each of the layers of the GNNN is formed by a polynomial graph filter ( i.e. , a matrix polynomial of either the adjacency or the Laplacian novelty ) . Rather than working on the frequency domain , the paper focuses on learning the polynomial coefficients of the filter on the vertex domain . The key novelty is the consideration of a stack architecture for which the polynomial filter is formed by the successive application ( i.e. , matrix multiplication ) of filters of order one . Numerical experiments with real datasets showcase the merits , including superior classification performance , of the proposed architecture . STRONG POINTS : The paper is timely and fits nicely the scope of the conference . The numerical experiments are convincing , offering insights , and demonstrating some of the advantages of the proposed architecture . The writing is clear , making the paper easy to follow . WEAK POINTS : Except for the numerical experiments , I find that the contribution is quite limited . The postulation of GCNN architectures based on polynomial graph filters where the focus is on learning the polynomial coefficients has been studied thoroughly in the literature . In general , the paper does a good job listing relevant works in that area , although some are missing ( e.g. , Gama - Ribeiro ) . Some of the existing works look at ARMA structures and recursive order-one filter implementations . I acknowledge that the architecture considered in those papers may not be exactly the same as the one proposed by the authors in this paper . I also appreciate that the application at hand ( vertex classification ) was not the goal of many of those papers . However , I still feel that the contribution falls short , especially for a top conference such as ICLR . In any case , I am open to change my mind if the authors are able to strengthen their theoretical claims or address my concerns in their rebuttal . I believe that the title should be changed . GCNN are not mentioned . The current title places the focus on Stacked Graph Filters . My first concern is that , within the linear paradigm ( i.e. , as polynomials of the adjacency/Laplacian matrix ) , this type of architectures have already been investigated . More importantly , the paper focuses on NN architectures , so I think it is reasonable to have that on the title . OVERALL RECOMMENDATION : Marginal reject . The paper is topical , timely , and nicely written . It addresses a problem of interest and does so with contemporary machine learning tools . The results in real-world datasets are convincing . However , the contribution and novelty are limited , falling short of the average contribution at ICLR . ADDITIONAL RECOMMENDATIONS : Being able to obtain additional theoretical results would make the contribution more solid . Further elaborating on the robustness of the architecture it is another change that would strengthen the manuscript .", "rating": "5: Marginally below acceptance threshold", "reply_text": "Let us first summarize and assign numbers to your comments : 1 . Polynomial interpolation has been well-studied . 2.Lack of theoretical results . 3.Title should be changed . 4.Elaborate on the robustness of the model . = We agree with your constructive comments and here we just want to justify our motivation for the work . ( 1 ) It is true that polynomial interpolation has been well-studied , and it is our oversight that we did not give representative methods like ChebNet a deeper discussion . We have updated our manuscript ( page 4 and Table 2,3 ) to elaborate this point . Our main argument here is that since we are not finding interpolation points but learning the coefficients , all polynomial bases are equivalent . ( 2 ) Indeed , we are still developing theoretical results from these experiments . However , we hope that reader can see the merit of our paper that `` going back to learning polynomial '' is beneficial , especially when the training data is abundant ( Appendix A discussion ) . ( 3 ) How do you feel about the new title `` Vertex classification - Returns of the polynomials '' ? ( 4 ) We will indeed go beyond our discussion in `` 6.4 ADAPTIVITY TO STRUCTURAL NOISE '' , but perhaps it can be an entire topic of its own because of the adversarial attack literature . The new update to our manuscript is marked with dark red text for clarity . We hope this update will clarify some of your questions . We understand that going through manuscripts is time-consuming , so we truly appreciate your comments on our work . If possible , we would like to know if this updated version has clarified your concerns ( a simple yes/no for each point would be sufficient ) ."}}