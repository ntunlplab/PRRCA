{"year": "2020", "forum": "HJxyZkBKDr", "title": "NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search", "decision": "Accept (Spotlight)", "meta_review": "This paper presents a new benchmark for architecture search. Reviewers put this paper in the top tier. I encourage the authors to also cite https://openreview.net/forum?id=SJx9ngStPH in their final version. ", "reviews": [{"review_id": "HJxyZkBKDr-0", "review_text": "Edit after rebuttals: I have read all other reviews and rebuttals and maintain my assessment. ---- Summary: Comparison of neural architecture search algorithms is hindered by the lack of a common measurement procedure. This paper describes a publicly available benchmark on which most recent types of NAS algorithms can be evaluated. It does so by exhaustive calculation of performance metrics on the full combinatorial space of select architectures, on two select datasets. NAS algorithms can then perform search without having to perform evaluation on each node, which shrinks the computational cost of experimentation and benchmarking drastically. I recommend acceptance, as the resource described in the paper has been created thoughtfully and is useful to the research community, as well as to users of NAS algorithms. The paper is clear about restrictions too, which doesn't hurt. The technical details are laid out clearly especially in sec 2.1. It would be interesting to know the computational cost of producing the data. It is useful in practice to have access to different metrics (validation, training and test) for each node, as well as extra diagnostic information. The usefulness of the resources hinges on a few elements, which make its strength and also weakness: - choice of tasks and datasets - choice of skeleton architecture, fig 1 - choice of hyperparameters, sec 2.3 (I note there is no regularisation, as discussed in the paper) All of these seem reasonable to me. It is clearly a limitation that hyperparameter search is infeasible to conduct in parallel with architecture search, as pointed out sec 6. The principal competitor NAS-Bench-101 is only applicable to specific NAS algorithms, which evidences the need for the present resource. The discussion and comparison in sec3 is fair. The discussion of weaknesses, such as possible overfitting patterns, or technical choices, is balanced. # Minor English proofreading is required. - Maybe you can attempt a pun on Ananas in the naming? - I'm not sure \"fairness\" as in the abstract is the exact core problem; I would call this comparability. - sec2 head: \"side information\", I usggest diagnostic information - sec2.2 \"and etc\" is a redundant: etc stands for \"and the others\" - sec2.4 almost involves almost; target on computation cost; stabability - sec 4: has impacts on, parameters keeps the same -> stays, which serves as testing -> to test - sec6 tricky ways-> insidious?", "rating": "8: Accept", "reply_text": "We appreciate your recognition of our paper and valuable comments regarding writing . Please find our response to each of your questions/comments in the following . - Try to attempt a pun on Ananas in the naming ? We are brainstorming this problem . Do you have some suggestions ? - I 'm not sure `` fairness '' as in the abstract is the exact core problem ; I would call this comparability . Nice correction . We have revised the paper according to your suggestion . - sec2 head : `` side information '' , I suggest diagnostic information Thanks for this constructive suggestion . We have replaced all \u201c side information \u201d with \u201c diagnostic information \u201d . - sec2.2 `` and etc '' is redundant : etc stands for `` and the others '' We have revised the sentence to \u201c The test set is to evaluate the performance of each searching algorithm by comparing the indicators ( e.g. , accuracy , model size , speed ) of their selected architectures. \u201d . - sec2.4 almost involves almost ; target on computation cost ; stabability Thanks for your comments . We have revised the sentences as : ( 1 ) Collecting these statistics almost involves no extra computation cost ( 2 ) Algorithms that target on searching architectures with computational constraints , such as models on edge devices , can use these metrics directly in their algorithm designs without extra calculations . ( 2 ) the stability - sec 4 : has impacts on , parameters keeps the same - > stays , which serves as testing - > to test Thanks for your comments . We have revised the sentences as : ( 1 ) Results show that a different number of parameters will affect the performance of the architectures , which indicates that the choices of operations are essential in NAS . ( 2 ) We also observe that the performance of the architecture can vary even when the number of parameters stays the same . ( 3 ) The performance of the architectures shows a generally consistent ranking over the three datasets with slightly different variance , which serves to test the generality of the searching algorithm . - sec6 tricky ways- > insidious ? Good suggestion . We have revised \u201c tricky \u201d by \u201c insidious \u201d ."}, {"review_id": "HJxyZkBKDr-1", "review_text": "--- Updated during response period --- Authors successfully answers all my questions. I revise my rating to Accept. ----- Summary: This paper proposes another benchmark dataset for neural architecture search. The idea is following the NASBench-101 dataset, that in a given search space, densely sampled all existing architectures and train each of them on three tasks for multiple times, and using the obtained metrics as a tool to evaluate an arbitrary neural architecture search algorithm. The paper also presents comprehensive reports on the statistics, revealing a strong performance correlation between tasks, and evaluate some baseline NAS algorithms. I think this paper will be valuable to the research community for these reasons: (1) the dataset contains a more geologically complex search space comparing to the original NASBench-101, whose search space is restrained in certain ways; (2) released metrics include more meaningful information rather than single point value in NASbench; (3) it uses 3 datasets rather than 1. My major concerns, which I will detail later, is the phrasing \"algorithm-agnostic\" does not truly reflect the difference between their approach and NASBench-101, and about the architecture search space design details. Altogether, I think even the technical novelty is incremental, the work is not trivial considering the computational cost. I am willing to improve my score if my concerns are addressed during the rebuttal period. Nevertheless, this dataset is a strong subsidy of existing NASBench-101 and can benefit the research community and serves as an important baseline to evaluate a NAS algorithm. Strength + Clear motivation to use an operation-on-the-edge search space that is widely used in NAS domain. + Extensive experiments on evaluating 15K architectures over 3 datasets + Detailed statistics on the search space + Good baseline experiments comparison Main concerns about this dataset: - Comparing to NASBench-101 in terms of \"Algorithm Agnostic\", it is in a \"more-or-less\" game but not a \"yes-or-no\" one, so that AA-NAS-Bench does not seem appropriate. In my perspective, this dataset has not shown significant differences for the following reasons. 1. With proper adaptation, both NASBench-101 and the one in this paper are \"algorithm agnostic\". For example, original ENAS is training a reinforcement learning sampler that learns to predict a string with encoding [id1, op1, id2, op2] for each node, where id1, id2 is the IDs of the previous node to connect, op1, op2 is the operation choice for each edge. Since NASBench has operation on output node, one could simply make RL sampler to predict [id1, id2, op1], or another string encoding that suits the search space better. In my perspective, Ying et al. mentioned that many NAS algorithms cannot be directly evaluated on NASBench-101 are because the search space is different, but it does not mean using NASBench-101 is impossible. On the other hand, for some other state-of-the-art algorithms, like Proxyless-NAS on ImageNet, the search space is also different from the one proposed in this paper, but likewise, it does not indicate evaluating Proxyless-NAS on this dataset is impossible. 2. NASBench-101 does impose constraints on maximum edge number equals to 9 with 7 nodes in their space and results in 423K architectures. However, this constraint is no longer applied if you reduce the number of node to 6 (i.e. all possible architectures can be sampled), yet it still contains around 64K architectures, which is more than 15K in the proposed dataset. In this perspective, NASBench is a larger dataset and \"algorithm agnostic\". To summarize, I acknowledge the paper's contribution is using an operation-on-the-edge search space that is widely used in previous NAS algorithms while NASBench-101 is using operation-on-the-node space. However, it only makes the proposed dataset \"more algorithm agnostic\" with less effort, and it does not make the previous NASBench-101 \"not\" algorithm agnostic. If using the current name AA-NAS-Bench, I think it is not fair for the NASBench-101, specifically they are 4 times larger after removing the edge number constraints. - Questions about architecture space design 1. Why using average pooling instead of max pooling? 2. How do you compute the total architecture number 15,625 in Table 3? In your setting, with the number of node V=4 densely connected DAG, it should have 6 edges as depicted in Figure 1, and each edge has 5 possible operations, i.e. total number = 6^5 = 7776. I am confused about this point, could author comment more on this number? 3. Is there any topologically equal architectures in this space? For example, let's name the node 1,2,3,4, and the following two architectures should be the same since input edges are summed before passed to the next node. I listed **non-zeroed** edge as, id1->id2: op Architecture 1: 1->2: conv3x3 2->4: skip 1->3: conv1x1 3->4: skip Architecture 2: 1->2: conv1x1 2->4: skip 1->3: conv3x3 3->4: skip If the pruning is not effectively conducted, my worry is the actual number of architectures is smaller. Minor comments: 1. DARTS results on the are quite poor as mentioned in this paper that, DARTS will eventually converge to an architecture with all skip connection. However, it could be a simple fix, by tracking the architecture evolution during the search and report the best like early-stopping. Will this improve DARTS results? 2. Since ENAS is the first work using parameter sharing on the NAS problem, could the author add it to the baseline? 3. In table 4, what is the average (94.37 for CIFAR-10) mean in the \"optimal\" column? Is this the mean performance of all architectures? If so, it is quite strange to see all the baselines are selecting architectures worse than the average performance. Or it is the best architecture performance as indicated in the caption? This \"average\" column for \"optimal\" seems confusing. 4. The dynamic ranking of architecture in Figure 5 is very interesting. Architecture ranking seems stable after the 190th epoch. Could the author provide another visualization, showing when stabilization happens in between epoch number 150 and 190? 5. Figure 4, correlation matrix for top 4743 architectures are significantly lower than the full and 1387 ones, is this possible because of repetitive architectures in the space are not pruned? And, what is the reason for number 4743 and 1387? 6. ResNet (star in Figure 2) seems to perform very well. Does this indicates the proposed search space is not much meaningful, considering there are only 1~2% for NAS to improve? ", "rating": "8: Accept", "reply_text": "Thank you for your constructive and detailed review . We have updated the paper according to your comments and suggestions . Detailed responses are shown in a point-to-point manner below . Q1.More detailed discussion and comparison with NASBench-101 . R1.We agree with the reviewer \u2019 s statement : with some modification to both NASBench-101 ( a reduced one ) and NAS algorithms , most algorithms could also be evaluated on the modified NASBench-101 . To the best of our knowledge , such modification is non-trivial and might be beyond the scope of the original NASBench-101 paper . Also , the modifications might need extra tedious effort , which is no longer convenient to use and against the main motivation of the benchmark . A subset of NASBench-101 with all possible architectures included needs to have 4 or fewer nodes , which sum to only less than 500 architectures . This is because a complete DAG with n nodes has n * ( n-1 ) /2 edges and NASBench-101 limits the maximum number of edges to 9 , therefore , the number of nodes ( n ) should be < = 4 . Q2.Why using average pooling instead of max pooling ? R2.It is inspired by the typical architectures , such as ResNet and ResNeXt ( https : //github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua # L38 ) , which use average pooling in their residual blocks . Q3.How do you compute the total architecture number 15,625 in Table 3 ? R3.There are 6 edges when we use the number of node V=4 . Each edge has 5 possible operations . Therefore , the total number is 5^6 = 15625 . Q4.Are there any topologically equal architectures in this space ? Is the actual number of architectures smaller ? R4.Yes , the number of unique architectures is 12751 . Q5.How about early stopping on DARTS to avoid finding the architecture with all skip connection . R5.We follow the original training strategy in the DARTS paper . Even if the early stopping may improve the performance , it is not the focus of this paper . Q6.Add ENAS as a baseline NAS algorithm . R6.Thanks for this suggestion . We have included ENAS in Table 4 . Q7.Clarify the \u201c optimal \u201d column in Table 4 . R7.We average the accuracy results of all trials for each architecture . The \u201c optimal \u201d means the highest mean accuracy . We have revised Table 4 to clarify it . Q8.Could the author provide another visualization , showing when stabilization happens in between epoch number 150 and 190 ? R8.We have made a video to show the ranking over training epochs . Please see the video at https : //drive.google.com/open ? id=1rp58l5FM-3Q-S7tPSYX003BVekWkj8X7 Q9 . Figure 4 , correlation matrix for top 4743 architectures are significantly lower than the full and 1387 ones , is this possible because of repetitive architectures in the space are not pruned ? And , what is the reason for the number 4743 and 1387 ? R9.Thanks for pointing out this problem . Figure 4b and 4c should be exchanged . We have updated it . The number of architectures is derived by the number of top architectures with accuracy > 92 % ( 4743 ) and 93 % ( 1387 ) . Q10.ResNet ( star in Figure 2 ) seems to perform very well . Does this indicate the proposed search space is not much meaningful , considering there are only 1~2 % for NAS to improve ? R10.This is also true for NAS-Bench-101 : ResNet is competitive and most NAS algorithms just find a worse architecture than ResNet . As shown in Table 4 , the best architecture found by 10 NAS algorithms is still far from the best architecture in the search space ( > 1 % on CIFAR-10 and > 3 % on CIFAR-100 and ImageNet-16-120 ) . Also , the stability of the NAS algorithms should also be considered since the evolution-based and RL-based methods usually suffer from high variance . In Section 6 , we included the rules to use the benchmark to avoid boosting performance using priors , e.g. , hard-code rules ."}, {"review_id": "HJxyZkBKDr-2", "review_text": "Summary: Research into Neural Architecture Search (NAS) has exploded in recent times. But unfortunately the entry barrier into the field is high due to the computational demands of running experiments on even cifar10/100 let alone ImageNet sized datasets. Furthermore there is a reproducibility and fair comparision crisis due to differences in search spaces, training routine hyperparameters, stochasticity in gpu training, etc. This paper proposes a benchmark cell-search space (resnet backbone, 4-node cell space, 5 possible operations) which is algorithm agnostic. They train all possible architectures (15625) in this search space on cifar10/100/Imagenet-16-120 (a reduced version of ImageNet with 120 classes). Thus anyone can now use this pretrained lookup-table to benchmark their search algorithm in seconds on a tiny laptop instead of having to get access to a cluster with hundreds of gpus. By also proposing reference implementations of training architectures the community can use this to fairly benchmark their search algorithms. The other such benchmark is NASBench-101 which uses a much more expansive search space but by imposing a limit on the number of edges in the cell (to keep the search space manageable with respect to how many of them they have to train) they leave out algorithms which do weight-sharing (ENAS, DARTS, RANDNas) from being able to use their benchmark. This paper alleviates those constraints and thus brings important algorithm classes to their fold. Comments: - The paper is very well written. Thanks! - Minor clarification question: One nice thing of the NASBench paper was the fact that they also reported variance in training with differnt random seeds. I see a line in the 'Metrics' section saying that this is also done but did not find any details on number of trials and whether this was part of the benchmark lookup. I might have missed it somewhere. - There is another class of search algorithms which grow from small to big cells (if using a cell search space) like EFAS (Efficient Forward Architecture Search by Dey et al and AutoGrow by Wen et al.). Can such algorithms take advantage of this benchmark? I think the answer is yes, because of the 'zeroise' operation but wanted to get the authors' answer. - Overall I think this is an important contribution to the field and I am assuming that the authors plan to release the benchmark and reference implementations if accepted?", "rating": "8: Accept", "reply_text": "We appreciate your constructive comments and suggestions . Please find our response to each of your questions/comments in the following . Q1.More details on the number of trials and whether this was part of the benchmark lookup . R1.In the current version of our AA-NAS-Bench , every architecture is trained at least once . To be specific , 7433 architectures are trained once , 782 architectures are trained twice , 7410 architectures are trained three times with different random seeds . Our API supports returning the metrics of a specific trial . Moreover , we are actively training all architectures with more seeds and will continue updating our AA-NAS-Bench . We have clarified this information in the footnote on Page 4 . We plan to finish the training of all architectures for 3 trials in 4 months . Q2.Can the searching algorithms which grow from small to big cells take the advantages of this benchmark ? R2.Yes , they can take advantages of this benchmark , because each small cell is equivalent to one big cell by adding some \u201c skip-connect \u201d and \u201c zeroize \u201d operations . Please see the following example . A small cell with 3 nodes : node-1 - > node-2 : 3x3conv node-1 - > node-3 : 3x3conv node-2 - > node-3 : 3x3conv The corresponding big cell with 4 nodes : node-1 - > node-2 : 3x3conv node-1 - > node-3 : 3x3conv node-2 - > node-3 : 3x3conv node-1 - > node-4 : zeroize node-2 - > node-4 : zeroize node-3 - > node-4 : skip-connect Therefore , our AA-NAS-Bench can also provide the metrics for all small cells , and benefit to searching algorithms that grow from small to big cells , e.g. , EFAS and AutoGrow . Q3.Release the benchmark and reference implementations . R3.Of course . We will release all source codes for training each architecture candidate and baseline searching algorithms during the rebuttal period . We would also provide convenient APIs to access our benchmark ."}], "0": {"review_id": "HJxyZkBKDr-0", "review_text": "Edit after rebuttals: I have read all other reviews and rebuttals and maintain my assessment. ---- Summary: Comparison of neural architecture search algorithms is hindered by the lack of a common measurement procedure. This paper describes a publicly available benchmark on which most recent types of NAS algorithms can be evaluated. It does so by exhaustive calculation of performance metrics on the full combinatorial space of select architectures, on two select datasets. NAS algorithms can then perform search without having to perform evaluation on each node, which shrinks the computational cost of experimentation and benchmarking drastically. I recommend acceptance, as the resource described in the paper has been created thoughtfully and is useful to the research community, as well as to users of NAS algorithms. The paper is clear about restrictions too, which doesn't hurt. The technical details are laid out clearly especially in sec 2.1. It would be interesting to know the computational cost of producing the data. It is useful in practice to have access to different metrics (validation, training and test) for each node, as well as extra diagnostic information. The usefulness of the resources hinges on a few elements, which make its strength and also weakness: - choice of tasks and datasets - choice of skeleton architecture, fig 1 - choice of hyperparameters, sec 2.3 (I note there is no regularisation, as discussed in the paper) All of these seem reasonable to me. It is clearly a limitation that hyperparameter search is infeasible to conduct in parallel with architecture search, as pointed out sec 6. The principal competitor NAS-Bench-101 is only applicable to specific NAS algorithms, which evidences the need for the present resource. The discussion and comparison in sec3 is fair. The discussion of weaknesses, such as possible overfitting patterns, or technical choices, is balanced. # Minor English proofreading is required. - Maybe you can attempt a pun on Ananas in the naming? - I'm not sure \"fairness\" as in the abstract is the exact core problem; I would call this comparability. - sec2 head: \"side information\", I usggest diagnostic information - sec2.2 \"and etc\" is a redundant: etc stands for \"and the others\" - sec2.4 almost involves almost; target on computation cost; stabability - sec 4: has impacts on, parameters keeps the same -> stays, which serves as testing -> to test - sec6 tricky ways-> insidious?", "rating": "8: Accept", "reply_text": "We appreciate your recognition of our paper and valuable comments regarding writing . Please find our response to each of your questions/comments in the following . - Try to attempt a pun on Ananas in the naming ? We are brainstorming this problem . Do you have some suggestions ? - I 'm not sure `` fairness '' as in the abstract is the exact core problem ; I would call this comparability . Nice correction . We have revised the paper according to your suggestion . - sec2 head : `` side information '' , I suggest diagnostic information Thanks for this constructive suggestion . We have replaced all \u201c side information \u201d with \u201c diagnostic information \u201d . - sec2.2 `` and etc '' is redundant : etc stands for `` and the others '' We have revised the sentence to \u201c The test set is to evaluate the performance of each searching algorithm by comparing the indicators ( e.g. , accuracy , model size , speed ) of their selected architectures. \u201d . - sec2.4 almost involves almost ; target on computation cost ; stabability Thanks for your comments . We have revised the sentences as : ( 1 ) Collecting these statistics almost involves no extra computation cost ( 2 ) Algorithms that target on searching architectures with computational constraints , such as models on edge devices , can use these metrics directly in their algorithm designs without extra calculations . ( 2 ) the stability - sec 4 : has impacts on , parameters keeps the same - > stays , which serves as testing - > to test Thanks for your comments . We have revised the sentences as : ( 1 ) Results show that a different number of parameters will affect the performance of the architectures , which indicates that the choices of operations are essential in NAS . ( 2 ) We also observe that the performance of the architecture can vary even when the number of parameters stays the same . ( 3 ) The performance of the architectures shows a generally consistent ranking over the three datasets with slightly different variance , which serves to test the generality of the searching algorithm . - sec6 tricky ways- > insidious ? Good suggestion . We have revised \u201c tricky \u201d by \u201c insidious \u201d ."}, "1": {"review_id": "HJxyZkBKDr-1", "review_text": "--- Updated during response period --- Authors successfully answers all my questions. I revise my rating to Accept. ----- Summary: This paper proposes another benchmark dataset for neural architecture search. The idea is following the NASBench-101 dataset, that in a given search space, densely sampled all existing architectures and train each of them on three tasks for multiple times, and using the obtained metrics as a tool to evaluate an arbitrary neural architecture search algorithm. The paper also presents comprehensive reports on the statistics, revealing a strong performance correlation between tasks, and evaluate some baseline NAS algorithms. I think this paper will be valuable to the research community for these reasons: (1) the dataset contains a more geologically complex search space comparing to the original NASBench-101, whose search space is restrained in certain ways; (2) released metrics include more meaningful information rather than single point value in NASbench; (3) it uses 3 datasets rather than 1. My major concerns, which I will detail later, is the phrasing \"algorithm-agnostic\" does not truly reflect the difference between their approach and NASBench-101, and about the architecture search space design details. Altogether, I think even the technical novelty is incremental, the work is not trivial considering the computational cost. I am willing to improve my score if my concerns are addressed during the rebuttal period. Nevertheless, this dataset is a strong subsidy of existing NASBench-101 and can benefit the research community and serves as an important baseline to evaluate a NAS algorithm. Strength + Clear motivation to use an operation-on-the-edge search space that is widely used in NAS domain. + Extensive experiments on evaluating 15K architectures over 3 datasets + Detailed statistics on the search space + Good baseline experiments comparison Main concerns about this dataset: - Comparing to NASBench-101 in terms of \"Algorithm Agnostic\", it is in a \"more-or-less\" game but not a \"yes-or-no\" one, so that AA-NAS-Bench does not seem appropriate. In my perspective, this dataset has not shown significant differences for the following reasons. 1. With proper adaptation, both NASBench-101 and the one in this paper are \"algorithm agnostic\". For example, original ENAS is training a reinforcement learning sampler that learns to predict a string with encoding [id1, op1, id2, op2] for each node, where id1, id2 is the IDs of the previous node to connect, op1, op2 is the operation choice for each edge. Since NASBench has operation on output node, one could simply make RL sampler to predict [id1, id2, op1], or another string encoding that suits the search space better. In my perspective, Ying et al. mentioned that many NAS algorithms cannot be directly evaluated on NASBench-101 are because the search space is different, but it does not mean using NASBench-101 is impossible. On the other hand, for some other state-of-the-art algorithms, like Proxyless-NAS on ImageNet, the search space is also different from the one proposed in this paper, but likewise, it does not indicate evaluating Proxyless-NAS on this dataset is impossible. 2. NASBench-101 does impose constraints on maximum edge number equals to 9 with 7 nodes in their space and results in 423K architectures. However, this constraint is no longer applied if you reduce the number of node to 6 (i.e. all possible architectures can be sampled), yet it still contains around 64K architectures, which is more than 15K in the proposed dataset. In this perspective, NASBench is a larger dataset and \"algorithm agnostic\". To summarize, I acknowledge the paper's contribution is using an operation-on-the-edge search space that is widely used in previous NAS algorithms while NASBench-101 is using operation-on-the-node space. However, it only makes the proposed dataset \"more algorithm agnostic\" with less effort, and it does not make the previous NASBench-101 \"not\" algorithm agnostic. If using the current name AA-NAS-Bench, I think it is not fair for the NASBench-101, specifically they are 4 times larger after removing the edge number constraints. - Questions about architecture space design 1. Why using average pooling instead of max pooling? 2. How do you compute the total architecture number 15,625 in Table 3? In your setting, with the number of node V=4 densely connected DAG, it should have 6 edges as depicted in Figure 1, and each edge has 5 possible operations, i.e. total number = 6^5 = 7776. I am confused about this point, could author comment more on this number? 3. Is there any topologically equal architectures in this space? For example, let's name the node 1,2,3,4, and the following two architectures should be the same since input edges are summed before passed to the next node. I listed **non-zeroed** edge as, id1->id2: op Architecture 1: 1->2: conv3x3 2->4: skip 1->3: conv1x1 3->4: skip Architecture 2: 1->2: conv1x1 2->4: skip 1->3: conv3x3 3->4: skip If the pruning is not effectively conducted, my worry is the actual number of architectures is smaller. Minor comments: 1. DARTS results on the are quite poor as mentioned in this paper that, DARTS will eventually converge to an architecture with all skip connection. However, it could be a simple fix, by tracking the architecture evolution during the search and report the best like early-stopping. Will this improve DARTS results? 2. Since ENAS is the first work using parameter sharing on the NAS problem, could the author add it to the baseline? 3. In table 4, what is the average (94.37 for CIFAR-10) mean in the \"optimal\" column? Is this the mean performance of all architectures? If so, it is quite strange to see all the baselines are selecting architectures worse than the average performance. Or it is the best architecture performance as indicated in the caption? This \"average\" column for \"optimal\" seems confusing. 4. The dynamic ranking of architecture in Figure 5 is very interesting. Architecture ranking seems stable after the 190th epoch. Could the author provide another visualization, showing when stabilization happens in between epoch number 150 and 190? 5. Figure 4, correlation matrix for top 4743 architectures are significantly lower than the full and 1387 ones, is this possible because of repetitive architectures in the space are not pruned? And, what is the reason for number 4743 and 1387? 6. ResNet (star in Figure 2) seems to perform very well. Does this indicates the proposed search space is not much meaningful, considering there are only 1~2% for NAS to improve? ", "rating": "8: Accept", "reply_text": "Thank you for your constructive and detailed review . We have updated the paper according to your comments and suggestions . Detailed responses are shown in a point-to-point manner below . Q1.More detailed discussion and comparison with NASBench-101 . R1.We agree with the reviewer \u2019 s statement : with some modification to both NASBench-101 ( a reduced one ) and NAS algorithms , most algorithms could also be evaluated on the modified NASBench-101 . To the best of our knowledge , such modification is non-trivial and might be beyond the scope of the original NASBench-101 paper . Also , the modifications might need extra tedious effort , which is no longer convenient to use and against the main motivation of the benchmark . A subset of NASBench-101 with all possible architectures included needs to have 4 or fewer nodes , which sum to only less than 500 architectures . This is because a complete DAG with n nodes has n * ( n-1 ) /2 edges and NASBench-101 limits the maximum number of edges to 9 , therefore , the number of nodes ( n ) should be < = 4 . Q2.Why using average pooling instead of max pooling ? R2.It is inspired by the typical architectures , such as ResNet and ResNeXt ( https : //github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua # L38 ) , which use average pooling in their residual blocks . Q3.How do you compute the total architecture number 15,625 in Table 3 ? R3.There are 6 edges when we use the number of node V=4 . Each edge has 5 possible operations . Therefore , the total number is 5^6 = 15625 . Q4.Are there any topologically equal architectures in this space ? Is the actual number of architectures smaller ? R4.Yes , the number of unique architectures is 12751 . Q5.How about early stopping on DARTS to avoid finding the architecture with all skip connection . R5.We follow the original training strategy in the DARTS paper . Even if the early stopping may improve the performance , it is not the focus of this paper . Q6.Add ENAS as a baseline NAS algorithm . R6.Thanks for this suggestion . We have included ENAS in Table 4 . Q7.Clarify the \u201c optimal \u201d column in Table 4 . R7.We average the accuracy results of all trials for each architecture . The \u201c optimal \u201d means the highest mean accuracy . We have revised Table 4 to clarify it . Q8.Could the author provide another visualization , showing when stabilization happens in between epoch number 150 and 190 ? R8.We have made a video to show the ranking over training epochs . Please see the video at https : //drive.google.com/open ? id=1rp58l5FM-3Q-S7tPSYX003BVekWkj8X7 Q9 . Figure 4 , correlation matrix for top 4743 architectures are significantly lower than the full and 1387 ones , is this possible because of repetitive architectures in the space are not pruned ? And , what is the reason for the number 4743 and 1387 ? R9.Thanks for pointing out this problem . Figure 4b and 4c should be exchanged . We have updated it . The number of architectures is derived by the number of top architectures with accuracy > 92 % ( 4743 ) and 93 % ( 1387 ) . Q10.ResNet ( star in Figure 2 ) seems to perform very well . Does this indicate the proposed search space is not much meaningful , considering there are only 1~2 % for NAS to improve ? R10.This is also true for NAS-Bench-101 : ResNet is competitive and most NAS algorithms just find a worse architecture than ResNet . As shown in Table 4 , the best architecture found by 10 NAS algorithms is still far from the best architecture in the search space ( > 1 % on CIFAR-10 and > 3 % on CIFAR-100 and ImageNet-16-120 ) . Also , the stability of the NAS algorithms should also be considered since the evolution-based and RL-based methods usually suffer from high variance . In Section 6 , we included the rules to use the benchmark to avoid boosting performance using priors , e.g. , hard-code rules ."}, "2": {"review_id": "HJxyZkBKDr-2", "review_text": "Summary: Research into Neural Architecture Search (NAS) has exploded in recent times. But unfortunately the entry barrier into the field is high due to the computational demands of running experiments on even cifar10/100 let alone ImageNet sized datasets. Furthermore there is a reproducibility and fair comparision crisis due to differences in search spaces, training routine hyperparameters, stochasticity in gpu training, etc. This paper proposes a benchmark cell-search space (resnet backbone, 4-node cell space, 5 possible operations) which is algorithm agnostic. They train all possible architectures (15625) in this search space on cifar10/100/Imagenet-16-120 (a reduced version of ImageNet with 120 classes). Thus anyone can now use this pretrained lookup-table to benchmark their search algorithm in seconds on a tiny laptop instead of having to get access to a cluster with hundreds of gpus. By also proposing reference implementations of training architectures the community can use this to fairly benchmark their search algorithms. The other such benchmark is NASBench-101 which uses a much more expansive search space but by imposing a limit on the number of edges in the cell (to keep the search space manageable with respect to how many of them they have to train) they leave out algorithms which do weight-sharing (ENAS, DARTS, RANDNas) from being able to use their benchmark. This paper alleviates those constraints and thus brings important algorithm classes to their fold. Comments: - The paper is very well written. Thanks! - Minor clarification question: One nice thing of the NASBench paper was the fact that they also reported variance in training with differnt random seeds. I see a line in the 'Metrics' section saying that this is also done but did not find any details on number of trials and whether this was part of the benchmark lookup. I might have missed it somewhere. - There is another class of search algorithms which grow from small to big cells (if using a cell search space) like EFAS (Efficient Forward Architecture Search by Dey et al and AutoGrow by Wen et al.). Can such algorithms take advantage of this benchmark? I think the answer is yes, because of the 'zeroise' operation but wanted to get the authors' answer. - Overall I think this is an important contribution to the field and I am assuming that the authors plan to release the benchmark and reference implementations if accepted?", "rating": "8: Accept", "reply_text": "We appreciate your constructive comments and suggestions . Please find our response to each of your questions/comments in the following . Q1.More details on the number of trials and whether this was part of the benchmark lookup . R1.In the current version of our AA-NAS-Bench , every architecture is trained at least once . To be specific , 7433 architectures are trained once , 782 architectures are trained twice , 7410 architectures are trained three times with different random seeds . Our API supports returning the metrics of a specific trial . Moreover , we are actively training all architectures with more seeds and will continue updating our AA-NAS-Bench . We have clarified this information in the footnote on Page 4 . We plan to finish the training of all architectures for 3 trials in 4 months . Q2.Can the searching algorithms which grow from small to big cells take the advantages of this benchmark ? R2.Yes , they can take advantages of this benchmark , because each small cell is equivalent to one big cell by adding some \u201c skip-connect \u201d and \u201c zeroize \u201d operations . Please see the following example . A small cell with 3 nodes : node-1 - > node-2 : 3x3conv node-1 - > node-3 : 3x3conv node-2 - > node-3 : 3x3conv The corresponding big cell with 4 nodes : node-1 - > node-2 : 3x3conv node-1 - > node-3 : 3x3conv node-2 - > node-3 : 3x3conv node-1 - > node-4 : zeroize node-2 - > node-4 : zeroize node-3 - > node-4 : skip-connect Therefore , our AA-NAS-Bench can also provide the metrics for all small cells , and benefit to searching algorithms that grow from small to big cells , e.g. , EFAS and AutoGrow . Q3.Release the benchmark and reference implementations . R3.Of course . We will release all source codes for training each architecture candidate and baseline searching algorithms during the rebuttal period . We would also provide convenient APIs to access our benchmark ."}}