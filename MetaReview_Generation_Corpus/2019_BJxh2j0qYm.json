{"year": "2019", "forum": "BJxh2j0qYm", "title": "Dynamic Channel Pruning: Feature Boosting and Suppression", "decision": "Accept (Poster)", "meta_review": "The authors propose a dynamic inference technique for accelerating neural network prediction with minimal accuracy loss. The method are simple and effective. The paper is clear and easy to follow. However, the real speedup on CPU/GPU is not demonstrated beyond the theoretical FLOPs reduction. Reviewers are also concerned that the idea of dynamic channel pruning is not novel. The evaluation is on fairly old networks.", "reviews": [{"review_id": "BJxh2j0qYm-0", "review_text": "Summary: This paper proposed a feature boosting and suppression method for dynamic channel pruning. To be specific, the proposed method firstly predicts the importance of each channel and then use an affine function to amplify/suppress the importance of different channels. However, the idea of dynamic channel pruning is not novel. Moreover, the comparisons in the experiments are quite limited. My detailed comments are as follows. Strengths: 1. The motivation for this paper is reasonable and very important. 2. The authors proposed a new method for dynamic channel pruning. Weaknesses: 1. The idea of dynamic channel pruning is not novel. In my opinion, this paper is only an extension to Network Slimming (Liu et al., 2017). What is the essential difference between the proposed method and Network Slimming? 2. The writing and organization of this paper need to be significantly improved. There are many grammatical errors and this paper should be carefully proof-read. 3. The authors argued that the importance of features is highly input-dependent. This problem is reasonable but the proposed method still cannot handle it. According to Eqn. (7), the prediction of channel saliency relies on a data batch rather than a single data. Given different inputs in a batch, the selected channels should be different for each input rather than a general one for the whole batch. Please comment on this issue. 4. The proposed method does not remove any channels from the original model. As a result, both the memory and the computational cost will not be reduced. It is confusing why the proposed method can yield a significant speed-up in the experiments. 5. The authors only evaluate the proposed method on shallow models, e.g., VGG and ResNet18. What about the deeper model like ResNet50 on ImageNet? 6. It is very confusing why the authors only reported top-5 error of VGG. The results of top-1 error for VGG should be compared in the experiments. 7. Several state-of-the-art channel pruning methods should be considered as the baselines, such as ThiNet (Luo et al., 2017), Channel pruning (He et al., 2017) and DCP (Zhuang et al., 2018) [1] Channel pruning for accelerating very deep neural networks. CVPR 2017. [2] Thinet: A filter level pruning method for deep neural network compression. CVPR 2017. [3] Discrimination-aware Channel Pruning for Deep Neural Networks. NIPS 2018. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "3 . `` The authors argued that the importance of features is highly input-dependent . This problem is reasonable but the proposed method still can not handle it . According to Eqn . ( 7 ) , the prediction of channel saliency relies on a data batch rather than a single data . Given different inputs in a batch , the selected channels should be different for each input rather than a general one for the whole batch . Please comment on this issue . '' The prediction of channel saliency * does not * rely on a batch of data . In equation ( 7 ) , x_ ( l-1 ) is the output of the ( l-1 ) -th layer , which comprises of C_ ( l-1 ) features , each feature has the spatial dimensions H_ ( l-1 ) * W_ ( l-1 ) , as defined in Section 3.1 . Throughout this paper , x_l for all layers is a single input image , which consists of multiple channels . Equation ( 7 ) reduces each channel in an image to a scalar , which is then used to predict the output channel saliencies in equation ( 8 ) . Although this process is identical for each input image , each evaluation of equation ( 8 ) may produce drastically different predicted channel saliencies dependent on the input image . We would like to update this section to remove any sources of ambiguity , would it be possible for you to describe how our intended meaning was lost ? 4 . `` The proposed method does not remove any channels from the original model . As a result , both the memory and the computational cost will not be reduced . It is confusing why the proposed method can yield a significant speed-up in the experiments. \u201d It is hopefully clear from previous comments that this is not the case . Typically , convolutional layers are stacked to form a sequential convolutional network . Prior to computing the costly convolution , FBS uses the input ( or the output from the previous layer ) to predict the saliencies of output channels of the costly convolution . If an output channel is predicted to have a zero saliency , the evaluation of this output channel can be entirely skipped , as the entire output channel is predicted to contain only zero entries . In addition , each convolutional layer takes as its input the output of the previous layer . This input can have channel-wise sparsity ( channels consisting of only zero entries ) , if the previous layer is a convolutional layer . It is clear that these inactive input channels can always be skipped when computing the convolution . The input- and output-side sparsities therefore doubly accelerate the expensive convolution and thus achieve a huge reduction in compute . Such reduction in computation is also seen in [ 2 ] , as it shares the same goal but uses an entirely different method . 5 . `` The authors only evaluate the proposed method on shallow models , e.g. , VGG and ResNet18 . What about the deeper model like ResNet50 on ImageNet ? '' The method we propose is a per-layer method , which should not make a difference when targeting deeper models . Unlike NS , we do not rank channel importance globally to produce pruning decisions . We are working on generating results on deeper models , but this might be limited by the amount of time available . 6 . `` It is very confusing why the authors only reported top-5 error of VGG . The results of top-1 error for VGG should be compared in the experiments . '' We will update Table 2 to include top-1 errors . However , some works we compare to , e.g.He et al . 's channel pruning [ 4 ] , may have missing top-1 errors as they were not reported . 7 . `` Several state-of-the-art channel pruning methods should be considered as the baselines , such as ThiNet ( Luo et al. , 2017 ) , Channel pruning ( He et al. , 2017 ) and DCP ( Zhuang et al. , 2018 ) . '' Thank you for pointing out these works . These are all static techniques . We will be including them in our comparisons . In addition , it should be noted that Channel pruning [ 4 ] is already in our comparison of Table 2 . We thank the reviewer for providing this review . We are in the process of updating this paper , and will notify you by comment of the new revision and its changes . [ 1 ] : Squeeze-and-Excitation Networks , CVPR 2018 , https : //arxiv.org/abs/1709.01507 [ 2 ] : Runtime Neural Pruning , NIPS 2017 , https : //papers.nips.cc/paper/6813-runtime-neural-pruning [ 3 ] : Conditional Computation in Neural Networks for Faster Models , ICLR 2016 , https : //arxiv.org/abs/1511.06297 [ 4 ] : Channel pruning for accelerating very deep neural networks , ICCV 2017 , https : //arxiv.org/abs/1707.06168"}, {"review_id": "BJxh2j0qYm-1", "review_text": "This paper propose a channel pruning method for dynamically selecting channels during testing. The analysis has shown that some channels are not always active. Pros: - The results on ImageNet are promising. FBS achieves state-of-the-art results on VGG-16 and ResNet-18. - The method is simple yet effective. - The paper is clear and easy to follow. Cons: - Lack of experiments on mobile networks like shufflenets and mobilenets - Missing citations of some state-of-the-art methods [1] [2]. - The speed-up ratios on GPU or CPU are not demonstrated. The dynamic design of Dong et al., 2017 did not achieve good GPU speedup. - Some small typos. [1] Amc: Automl for model compression and acceleration on mobile devices [2] Netadapt: Platform-aware neural network adaptation for mobile applications ", "rating": "7: Good paper, accept", "reply_text": "We would like to thank the reviewer for the positive comments . A comparison to AMC [ 1 ] is included in Table 2 , it is difficult for us to compare to Netadapt [ 2 ] since the networks considered are different . We would like to point out that Dong et al . [ 3 ] considered spatial dynamic execution , which eliminates computations at a finer granularity and is thus harder to accelerate compared to our channel-wise dynamic execution . On a CPU , we recently found that a single layer using FBS can increase inference speed by 3.91x , given a theoretical speedup of 3.98x . [ 3 ] More is Less : A More Complicated Network with Less Inference Complexity , CVPR 2017 , https : //arxiv.org/pdf/1703.08651.pdf"}, {"review_id": "BJxh2j0qYm-2", "review_text": "The authors propose a dynamic inference technique for accelerating neural network prediction with minimal accuracy loss. The technique prunes channels in an input-dependent way through the addition of auxiliary channel saliency prediction+pruning connections. Pros: - The paper is well-written and clearly explains the technique, and Figure 1 nicely summarizes the weakness of static channel pruning - The technique itself is simple and memory-efficient - The performance decrease is small Cons: - There is no clear motivation for the setting (keeping model accuracy while increasing inference speed by 2x or 5x) - In contrast to methods that prune weights, the model size is not reduced, decreasing the utility in many settings where faster inference and smaller models are desired (e.g. mobile, real-time) - The experiments are limited to classification and fairly dated architectures (VGG16, ResNet-18) Overall, the method is nicely explained but the motivation is not clear. Provided that speeding up inference without reducing the size of the model is desirable, this paper gives a good technique for preserving accuracy.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . 1.Re.motivation , to clarify we do increase performance as you state ( 2 -- 5x ) but in addition also make significant savings in terms of compute and memory bandwidth . These savings would be beneficial in almost all scenarios , either to reduce power , increase performance or trade for better accuracy . We have clarified this in our introduction . 2.I think there is some misunderstanding here . By dynamically gating computation , FBS reduces both compute and memory requirements . We simply do n't load/store the weights/activations for the suppressed channels . The newly added Table 3 quantifies these savings . 3.We are working on generating data for newer models , but this might be limited by the amount of time available ."}, {"review_id": "BJxh2j0qYm-3", "review_text": "This manuscript presents a nice method that can dynamically prune some channels in a CNN network to speed up the training. The main strength of the proposed method is to determine which channels to be suppressed based upon each data sample without incurring too much computational burden or too much memory consumption. The good thing is that the proposed pruning strategy does not result in a big performance decrease. Overall, this is a nicely written paper and may be empirically useful for training a very large CNN. Nevertheless, the authors did not present a real-world application in which it is important to speed up by 2 or 3 times at a small cost, so it is hard to judge the real impact of the proposed method.", "rating": "7: Good paper, accept", "reply_text": "> `` the authors did not present a real-world application in > which it is important to speed up by 2 or 3 times at a small > cost , so it is hard to judge the real > impact of the proposed method . '' Of course , all real systems are constrained by power and memory bandwidth . The proposed scheme offers very significant savings ( 2-3X in both compute and memory bandwidth ) that would be beneficial in almost all scenarios , either to reduce power , increase performance or trade for better accuracy . Additionally , we would like to point out that FBS works as an technique to accelerate network inference . Although it is entirely feasible to use it to accelerate training , we have not conducted relevant experiments ."}], "0": {"review_id": "BJxh2j0qYm-0", "review_text": "Summary: This paper proposed a feature boosting and suppression method for dynamic channel pruning. To be specific, the proposed method firstly predicts the importance of each channel and then use an affine function to amplify/suppress the importance of different channels. However, the idea of dynamic channel pruning is not novel. Moreover, the comparisons in the experiments are quite limited. My detailed comments are as follows. Strengths: 1. The motivation for this paper is reasonable and very important. 2. The authors proposed a new method for dynamic channel pruning. Weaknesses: 1. The idea of dynamic channel pruning is not novel. In my opinion, this paper is only an extension to Network Slimming (Liu et al., 2017). What is the essential difference between the proposed method and Network Slimming? 2. The writing and organization of this paper need to be significantly improved. There are many grammatical errors and this paper should be carefully proof-read. 3. The authors argued that the importance of features is highly input-dependent. This problem is reasonable but the proposed method still cannot handle it. According to Eqn. (7), the prediction of channel saliency relies on a data batch rather than a single data. Given different inputs in a batch, the selected channels should be different for each input rather than a general one for the whole batch. Please comment on this issue. 4. The proposed method does not remove any channels from the original model. As a result, both the memory and the computational cost will not be reduced. It is confusing why the proposed method can yield a significant speed-up in the experiments. 5. The authors only evaluate the proposed method on shallow models, e.g., VGG and ResNet18. What about the deeper model like ResNet50 on ImageNet? 6. It is very confusing why the authors only reported top-5 error of VGG. The results of top-1 error for VGG should be compared in the experiments. 7. Several state-of-the-art channel pruning methods should be considered as the baselines, such as ThiNet (Luo et al., 2017), Channel pruning (He et al., 2017) and DCP (Zhuang et al., 2018) [1] Channel pruning for accelerating very deep neural networks. CVPR 2017. [2] Thinet: A filter level pruning method for deep neural network compression. CVPR 2017. [3] Discrimination-aware Channel Pruning for Deep Neural Networks. NIPS 2018. ", "rating": "6: Marginally above acceptance threshold", "reply_text": "3 . `` The authors argued that the importance of features is highly input-dependent . This problem is reasonable but the proposed method still can not handle it . According to Eqn . ( 7 ) , the prediction of channel saliency relies on a data batch rather than a single data . Given different inputs in a batch , the selected channels should be different for each input rather than a general one for the whole batch . Please comment on this issue . '' The prediction of channel saliency * does not * rely on a batch of data . In equation ( 7 ) , x_ ( l-1 ) is the output of the ( l-1 ) -th layer , which comprises of C_ ( l-1 ) features , each feature has the spatial dimensions H_ ( l-1 ) * W_ ( l-1 ) , as defined in Section 3.1 . Throughout this paper , x_l for all layers is a single input image , which consists of multiple channels . Equation ( 7 ) reduces each channel in an image to a scalar , which is then used to predict the output channel saliencies in equation ( 8 ) . Although this process is identical for each input image , each evaluation of equation ( 8 ) may produce drastically different predicted channel saliencies dependent on the input image . We would like to update this section to remove any sources of ambiguity , would it be possible for you to describe how our intended meaning was lost ? 4 . `` The proposed method does not remove any channels from the original model . As a result , both the memory and the computational cost will not be reduced . It is confusing why the proposed method can yield a significant speed-up in the experiments. \u201d It is hopefully clear from previous comments that this is not the case . Typically , convolutional layers are stacked to form a sequential convolutional network . Prior to computing the costly convolution , FBS uses the input ( or the output from the previous layer ) to predict the saliencies of output channels of the costly convolution . If an output channel is predicted to have a zero saliency , the evaluation of this output channel can be entirely skipped , as the entire output channel is predicted to contain only zero entries . In addition , each convolutional layer takes as its input the output of the previous layer . This input can have channel-wise sparsity ( channels consisting of only zero entries ) , if the previous layer is a convolutional layer . It is clear that these inactive input channels can always be skipped when computing the convolution . The input- and output-side sparsities therefore doubly accelerate the expensive convolution and thus achieve a huge reduction in compute . Such reduction in computation is also seen in [ 2 ] , as it shares the same goal but uses an entirely different method . 5 . `` The authors only evaluate the proposed method on shallow models , e.g. , VGG and ResNet18 . What about the deeper model like ResNet50 on ImageNet ? '' The method we propose is a per-layer method , which should not make a difference when targeting deeper models . Unlike NS , we do not rank channel importance globally to produce pruning decisions . We are working on generating results on deeper models , but this might be limited by the amount of time available . 6 . `` It is very confusing why the authors only reported top-5 error of VGG . The results of top-1 error for VGG should be compared in the experiments . '' We will update Table 2 to include top-1 errors . However , some works we compare to , e.g.He et al . 's channel pruning [ 4 ] , may have missing top-1 errors as they were not reported . 7 . `` Several state-of-the-art channel pruning methods should be considered as the baselines , such as ThiNet ( Luo et al. , 2017 ) , Channel pruning ( He et al. , 2017 ) and DCP ( Zhuang et al. , 2018 ) . '' Thank you for pointing out these works . These are all static techniques . We will be including them in our comparisons . In addition , it should be noted that Channel pruning [ 4 ] is already in our comparison of Table 2 . We thank the reviewer for providing this review . We are in the process of updating this paper , and will notify you by comment of the new revision and its changes . [ 1 ] : Squeeze-and-Excitation Networks , CVPR 2018 , https : //arxiv.org/abs/1709.01507 [ 2 ] : Runtime Neural Pruning , NIPS 2017 , https : //papers.nips.cc/paper/6813-runtime-neural-pruning [ 3 ] : Conditional Computation in Neural Networks for Faster Models , ICLR 2016 , https : //arxiv.org/abs/1511.06297 [ 4 ] : Channel pruning for accelerating very deep neural networks , ICCV 2017 , https : //arxiv.org/abs/1707.06168"}, "1": {"review_id": "BJxh2j0qYm-1", "review_text": "This paper propose a channel pruning method for dynamically selecting channels during testing. The analysis has shown that some channels are not always active. Pros: - The results on ImageNet are promising. FBS achieves state-of-the-art results on VGG-16 and ResNet-18. - The method is simple yet effective. - The paper is clear and easy to follow. Cons: - Lack of experiments on mobile networks like shufflenets and mobilenets - Missing citations of some state-of-the-art methods [1] [2]. - The speed-up ratios on GPU or CPU are not demonstrated. The dynamic design of Dong et al., 2017 did not achieve good GPU speedup. - Some small typos. [1] Amc: Automl for model compression and acceleration on mobile devices [2] Netadapt: Platform-aware neural network adaptation for mobile applications ", "rating": "7: Good paper, accept", "reply_text": "We would like to thank the reviewer for the positive comments . A comparison to AMC [ 1 ] is included in Table 2 , it is difficult for us to compare to Netadapt [ 2 ] since the networks considered are different . We would like to point out that Dong et al . [ 3 ] considered spatial dynamic execution , which eliminates computations at a finer granularity and is thus harder to accelerate compared to our channel-wise dynamic execution . On a CPU , we recently found that a single layer using FBS can increase inference speed by 3.91x , given a theoretical speedup of 3.98x . [ 3 ] More is Less : A More Complicated Network with Less Inference Complexity , CVPR 2017 , https : //arxiv.org/pdf/1703.08651.pdf"}, "2": {"review_id": "BJxh2j0qYm-2", "review_text": "The authors propose a dynamic inference technique for accelerating neural network prediction with minimal accuracy loss. The technique prunes channels in an input-dependent way through the addition of auxiliary channel saliency prediction+pruning connections. Pros: - The paper is well-written and clearly explains the technique, and Figure 1 nicely summarizes the weakness of static channel pruning - The technique itself is simple and memory-efficient - The performance decrease is small Cons: - There is no clear motivation for the setting (keeping model accuracy while increasing inference speed by 2x or 5x) - In contrast to methods that prune weights, the model size is not reduced, decreasing the utility in many settings where faster inference and smaller models are desired (e.g. mobile, real-time) - The experiments are limited to classification and fairly dated architectures (VGG16, ResNet-18) Overall, the method is nicely explained but the motivation is not clear. Provided that speeding up inference without reducing the size of the model is desirable, this paper gives a good technique for preserving accuracy.", "rating": "6: Marginally above acceptance threshold", "reply_text": "Thank you for your comments . 1.Re.motivation , to clarify we do increase performance as you state ( 2 -- 5x ) but in addition also make significant savings in terms of compute and memory bandwidth . These savings would be beneficial in almost all scenarios , either to reduce power , increase performance or trade for better accuracy . We have clarified this in our introduction . 2.I think there is some misunderstanding here . By dynamically gating computation , FBS reduces both compute and memory requirements . We simply do n't load/store the weights/activations for the suppressed channels . The newly added Table 3 quantifies these savings . 3.We are working on generating data for newer models , but this might be limited by the amount of time available ."}, "3": {"review_id": "BJxh2j0qYm-3", "review_text": "This manuscript presents a nice method that can dynamically prune some channels in a CNN network to speed up the training. The main strength of the proposed method is to determine which channels to be suppressed based upon each data sample without incurring too much computational burden or too much memory consumption. The good thing is that the proposed pruning strategy does not result in a big performance decrease. Overall, this is a nicely written paper and may be empirically useful for training a very large CNN. Nevertheless, the authors did not present a real-world application in which it is important to speed up by 2 or 3 times at a small cost, so it is hard to judge the real impact of the proposed method.", "rating": "7: Good paper, accept", "reply_text": "> `` the authors did not present a real-world application in > which it is important to speed up by 2 or 3 times at a small > cost , so it is hard to judge the real > impact of the proposed method . '' Of course , all real systems are constrained by power and memory bandwidth . The proposed scheme offers very significant savings ( 2-3X in both compute and memory bandwidth ) that would be beneficial in almost all scenarios , either to reduce power , increase performance or trade for better accuracy . Additionally , we would like to point out that FBS works as an technique to accelerate network inference . Although it is entirely feasible to use it to accelerate training , we have not conducted relevant experiments ."}}