{"year": "2019", "forum": "r1espiA9YQ", "title": "Towards More Theoretically-Grounded Particle Optimization Sampling for Deep Learning", "decision": "Reject", "meta_review": "This paper proposes a combination of SVGD and SLGD and analyzes its non-asymptotic properties based on gradient flow. This is an interesting direction to explore. Unfortunately, two major concerns have been raised regarding this paper:  1) the reviewers identified multiple technical flaws. Authors provided rebuttal and addressed some of the problems. But the reviewers think it requires significantly more improvement and clarification to fully address the issues. 2) the motivation of the combination of SVGD and SLGD, despite of being very interesting, is not very clearly motivated; by combining SVGD and SLGD, one get convergence rate for free from the SLGD part, but not much insight is shed on the SVGD part (meaning if the contribution of SLGD is zero, then the bound because vacuum). This could be misleading given that one of the claimed contribution is non-asymptotic theory of ''SVGD-style algorithms\" (rather than SLGD style..). We encourage the authors to addresses the technical questions and clarify the contribution and motivation of the paper in revision for future submissions.  \n", "reviews": [{"review_id": "r1espiA9YQ-0", "review_text": "Two promising methods for scalable sample-based Bayesian inference are: 1) SGLD: simply discretize a standard Langevin dynamics to construct a Markov chain that approximate the correct invariant distribution. This reads: x_{t+1} = x_t + \\nabla \\log \\pi(x_t) \\delta + \\sqrt(2 \\delta) \\xi 2) SVGD: the method can be expressed as a type of gradient descent of an appropriate functional on the space of probability distributions. A cloud of particles {x_i}_{I=1}^M evolves according to: x^i_{t+1} = x^i_t + (some functional of all the particles) \\, \\delta The method proposed in the article is not very different from alternating the two above mentioned update, which is indeed quite a natural idea, and can work pretty well I think. The method reads: x^i_{t+1} = x^i_t + [ \\nabla \\log \\pi(x_t) \\delta + (some functional of all the particles) \\, \\delta ] + \\sqrt(2 \\delta) \\xi. PROS: - yes, I think that the method can work quite OK since it may be borrowing the strengths of both SGLD and SVGD. - It seems that the meat of the paper consists in proving some (non-asymptotic) convergence result. Unfortunately, this went above my head and I cannot claim that I have read the details of the proofs. CONS: - it is (very) difficult to fairly evaluate this type of methods in high-dimensional settings. I thus appreciate that the numerical section starts with a toy very simple Gaussian model. I would have been much more interested in fair and extensive simulations in this type of settings where it is relatively easy to compare the proposed method with SGLD and SGVD. In other words, after reading the paper, I must say that I am not at all convinced that the method does bring something over SGLD or SVGD (although it is very possible that it does). For example, comprehensive and fair comparisons with SGLD and SVGD in Gaussian settings (not necessarily one-dimensional) could have been presented. The delicate tuning of the different methods, the speed of convergence wrt algorithmic time, the speed of comparison wrt the number of particles, etc.. could have been investigated numerically: this would have been, I think, much more convincing. MINOR comments: - I did check the proof of Theorem 2, which seems hand-wavy and overly complicated. What is the function G? It seems that the proof of Theorem 2 simply consists in establishing that if each particle x_i follows the dynamics dx = F(x)*dt then the associated densities satisfy \\partial_t \\mu_t = -\\partial_x(F(x) * \\mu_t(x)) , which is obvious. But the situation in the paper is indeed more delicate since the particles are interacting, etc... Reading this proof got me very worried and did not motivate me to read the rest of the paper. SUMMARY: - the method is not terribly original -- this is a simple hybrid SVGD / SGLD -- but may work very well. - unfortunately, the numerical experiments are not convincing. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for his comments , however , the reviewer seems to miss our key point . First , we would like to stress that our paper does not try to show our proposed method is better than either SVGD or SGLD . The motivation of our method is to help better understand the nonasymptotic convergence behavior of SVGD . Since there is no/limited nonasymptotic theory for SVGD , it is hard to understand its convergence behavior . To overcome this difficulty , we combine SGLD with SVGD , and for the first time successfully develop nonasymptotic convergence theory for a SVGD-style algorithm . Because there no nonasymptotic theory for SVGD ( except for some restrict results of the recent work [ 1 ] ) , nothing can be said about SVGD and our algorithm in theory . Similarly , it is hard to compare to SGLD as well because our algorithm is particle based . That said , even though we can perform other experiments on simple toy data , nothing can be expected by comparing our algorithm and SVGD , except the pitfall property of SVGD described in Sec 4.4 , which has been shown in Figure 1 . For the proof of Theorem 2 , G is defined in the equation below eq.20 , which is related to LHS of eq.7 . It is unfair to say that `` Reading this proof got me very worried and did not motivate me to read the rest of the paper '' because the proof techniques of Theorem 2 and other theorems are complete independent ."}, {"review_id": "r1espiA9YQ-1", "review_text": "This paper considers the problem of Bayesian inference using particle optimization sampler. Similarly to SGLD, authors propose Stochastic Particle Optimization Sampler (SPOS), augmenting Stein Variational Gradient Descent (SVGD) with diminishing Gaussian noise, replacing the hard-to-compute term of the Chen et al. (2018) formulation. Various theoretical results are given. This paper was a pleasant read until I decided to check the proof of Theorem 3. I was not able to understand transitions in some of the steps and certain statements in the proof seem wrong. Theorem 3: \"Note that $\\theta^i_t$ and $\\hat \\theta^i_t$ are initialized with the same initial distribution \u00b50 = \u03bd0 and we can also set $\\theta^i_0$ to be independent of $\\hat \\theta^i_0$, we can have $\\gamma(0) = 0$. $\\gamma(0) = E \\|\\theta^i_0 - \\hat \\theta^i_0 \\|^2$.\" - this doesn't seem right to me. Expectation of squared difference of two independent and identically distributed random variables is not 0, assuming expectation is with respect to their joint density. \"Then according to the Gronwall Lemma, we have\" - I don't see how the resulting inequality was obtained. When I tried applying Gronwall Lemma, it seems that authors forgot to multiply by $t$ and $\\lambda_1$. Could you please elaborate how exactly Gronwall Lemma was used in this case. \"... some positive constants c1 and c2 independent of (M, d)$ - in the proof authors introduce additional assumption \"We can tune the bandwidth of the RBF kernel to make \u2207K \u2264 H_\u2207K, which is omitted in the Assumption due to the space limit.\" First, there is a missing norm, since \u2207K is a vector and H_\u2207K is I believe a scalar constant. Second, c1 = H_\u2207K + H_F, which both bound norm of d-dimensional vector and hence depend on d. I also suggest that all assumptions are included in the theorem statements, especially since authors have another assumption requiring large bandwidth. Additionally, feasibility of these both assumptions being satisfied should be explored (it seems to me that they can hold together, but it doesn't mean that part of assumptions can be moved to the supplement). I find using Wasserstein-1 metric misleading in the theorem statement . This is not what authors really bound - from the proof it can be seen that they bound W_1 with W_2 and then with just an expectation of l2 norm. Moreover I don't understand the meaning of this bound. Theorem is concerned with W_1 distance between two atomic measures. What is the expectation over? Note that atom locations are supposed to be fixed for the W_1 to make sense in this context (and the expectation is over the coupling of discrete measures defined by weights of the atoms, not atom locations). \"Note the first bullet indicates U to be a convex function and W to be ... \" I think it should be K, not W. Theorems 3-6 could be lemmas, while there should be a unifying theorem for the bound. Finally, I think notation should be changed - same letter is used for Wasserstein distance and Wiener process. Other comments: Example in Figure 1 is somewhat contrived - clearly gradient based particle sampler will never escape the mode since all modes are disconnected by regions with 0 density. Proposed method on the other hand will eventually jump out due to noise, but it doesn't necessarily mean it produces better posterior estimate. Something more realistic like a mixture of Gaussians, with density bounded away from zero across domain space, will be more informative. It is not sufficient to report RMSE and test log likelihood for BNNs. One of the key motivating points is posterior uncertainty estimation. Hence important metric, when comparing to other posterior inference techniques, is to show high uncertainty for out of distribution samples and low for training/test data. ", "rating": "3: Clear rejection", "reply_text": "1.We will change the \u201c W \u201d to \u201c K \u201d . 2.Yes , we understand your suggestions that Theorems 3-6 could be lemmas and there should be a unifying theorem for the bound . However , it is worth noting that the \u201c unifying theorems for the bounds \u201d are provided in the appendix H as mentioned at the end of Section 4.3 . And we hope not to move them to the context due to the space limit . Our paper is already 10 pages long and can not fit these two long theorems . Besides , we have no problems to change the \u201c Theorem \u201d to \u201c Lemma \u201d . Unfortunately , as our responses to other reviewers , our paper \u2019 s main contribution lies in the theoretical analysis . We think the techniques and ideas of Theorems 3-6 are also very important , which provide some guides for other researchers in the field . Hence , we think Theorems 3-6 worth the name \u201c Theorem \u201d , not just \u201c Lemmas \u201d ( which mean they are only affiliated to the \u201c unifying theorems for the bounds \u201d ) . But if you still disagree with our opinions , we are willing to make the changes since this issue is quite minor and should not be the reason for your rejection . 3.We will change the notation of Wasserstein metric from $ \\mathcal { W } $ to $ W $ . 4.We don \u2019 t think \u201c Example in Figure 1 is contrived \u201d . The distribution form used in Figure 1 is given in Appendix A . It is obvious that the distribution are nonzero everywhere ( the probabilities are just very small somewhere ) , thus it does not have the problem of \u201c disconnected modes \u201d . We use this example to show failure case of SVGD , which induces no problem with our SPOS . Using RMSE and log-likelihood is the gold standard in Bayesian learning of DNNs . Instead of directly showing uncertainty for in/out distribution samples as suggested by you , we test It in the more direct scenario of reinforcement learning . The reason is that it is well-accepted that RL performance directly measures how well the uncertainty is learned , as there is an exploration stage in the learning , requiring uncertainty to explore the environment . As a result , we believe our measure in the experiments are standard . At last , we really hope you could reconsider your scoring as it seems to be quite unfair based on your comments . In our response , we have fully address the minor problem which you pointed out . Besides , we have resolved your concerns about the W_1 metric excessively . And we have shared our opinions with you on the name of our Theorem and decides to change the notations as you suggested . We think it is really unfair to reject our paper merely for some minor problems , which has been fully addressed . Although we have explained much to your concerns about W_1 metric , most of them are not needed to add to our paper , which means that we do no need to make much revision . If you like , we are willing to add our explanations for your concerns into the Appendix . Thank you so much for your time and re-consideration !"}, {"review_id": "r1espiA9YQ-2", "review_text": "This paper proposes a particle-based inference algorithm, the optimal update for each particle is the summation of the standard SGLD direction and SVGD velocity. The work further analyzes non-asymptotic properties of SPOS. The results appear theoretically interesting and of potential practical value in designing inference algorithms. I did not go through the proofs in the supplementary. [Experimental results are not convincing] [BNN] I noticed the test RMSE and test LL of SVGD are directly copied from the original SVGD paper. However, the performance critically depends on: 1. Running time, or training epochs 2. Data partitions To be a fair comparison, the authors should keep at least the training epochs and random partitions the same. Especially for the dataset Year, for which only one random partition is conducted. It\u2019s highly likely that the performance gain is due to favored data partition rather than the superiority of the algorithm. [RL] Average rewards are significantly lower than the scores reported in the original SVPG paper? 1. From figure 3, SPOS only outperforms SVPG on envs Cartpole Swing Up and Double Pendulum. The best reward for env Cartpole Swing Up reported in this paper is around 200. However, the score is ~400 in the original SVPG paper. For the env Double Pendulum, there\u2019s also very large performance gap. I am aware the code for SVPG is now publicly available, the authors may consider conducting the experiments with the same settings (e.g. same seed?). Otherwise, it\u2019s hard to tell whether the performance gain is significant while the baseline is much worse than it should be. 2. Only 3 envs are reported, the authors may also consider reporting all the envs are used in the SVPG paper [Figure 1] The authors may consider reporting the exact settings of this case, otherwise, it\u2019s hard to believe that SVGD would collapse on a simple 1D case. If the authors can fully address the concerns above, I will consider changing the scores. Other comments: - Related papers: Stein Variational Message Passing for Continuous Graphical Models, Wang et al., ICML18 (https://arxiv.org/abs/1711.07168) Stein Variational Gradient Descent as Moment Matching, Liu et al., NIPS18 (https://arxiv.org/abs/1810.11693) - Page 30 crashes my browser all the time ", "rating": "4: Ok but not good enough - rejection", "reply_text": "To eliminate the confusion of the reviewer , we re-run the experiments for SVGD and SPOS , the same split of data ( train , val and test ) are used for SVGD and SPOS . The test results are reported on the best model on the validation set . The results are as follows : Boston_housing : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 2.829 \\pm 0.126 & 2.961 \\pm 0.109 & -2.532 \\pm 0.082 & -2.591 \\pm 0.029 Concrete : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 5.071 \\pm 0.1495 & 6.157 \\pm 0.082 & -3.062 \\pm 0.037 & -3.247 \\pm 0.01 Energy : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 0.752 \\pm 0.0285 & 1.291 \\pm 0.029 & -1.158 \\pm 0.073 & -1.534 \\pm 0.026 Kin8nm : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 0.079 \\pm 0.001 & 0.075 \\pm 0.001 & 1.092 \\pm 0.013 & 1.138 \\pm 0.004 Naval : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 0.004 \\pm 0.0 & 0.004 \\pm 0.000 & 4.145 \\pm 0.02 & 4.032 \\pm 0.008 CCPP : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 3.939 \\pm 0.0495 & 4.127 \\pm 0.027 & -2.794 \\pm 0.025 & -2.843 \\pm 0.006 Winequality : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 0.598 \\pm 0.014 & 0.604 \\pm 0.007 & -0.911 \\pm 0.041 & -0.926 For RL results , the four benchmarks are the simplest benchmarks for reinforcement learning ; thus it is obviously not necessary to use a 400-400 MLP as a policy . Even in much more complex benchmarks , e.g. , humanoid and walker , previous methods such as soft-Q learning and SAC used 128-128 MLP or 256-256 MLP as the policy network , TRPO used one-layer MLP . We followed the settings of VIME , and think it is more reasonable to use a simpler policy network . 400-400 MLP as a policy is too complex to be a good choice . We used the released code of SVPG and the same settings for both methods ( also same seeds ) , thus the comparisons are fair for both methods . For the other environments , Mountain car is a very simple environment compared with CartpoleSwingUp and Double Pendulum , and we encountered errors from the framework when running the algorithms . We will try to fix the problem and incorporate results into our next revision . As in our response to Reviewer 1 , we did not claim a better algorithm than SVGD in theory because there is no nonasymptotic theory for SVGD ( though we did observed better empirical performance ) , but a better way to understand the nonasymptotic convergence behavior of particle optimization algorithms , e.g. , SPOS , providing a non-asymptotic bound for an SVGD-style algorithm the first time ."}], "0": {"review_id": "r1espiA9YQ-0", "review_text": "Two promising methods for scalable sample-based Bayesian inference are: 1) SGLD: simply discretize a standard Langevin dynamics to construct a Markov chain that approximate the correct invariant distribution. This reads: x_{t+1} = x_t + \\nabla \\log \\pi(x_t) \\delta + \\sqrt(2 \\delta) \\xi 2) SVGD: the method can be expressed as a type of gradient descent of an appropriate functional on the space of probability distributions. A cloud of particles {x_i}_{I=1}^M evolves according to: x^i_{t+1} = x^i_t + (some functional of all the particles) \\, \\delta The method proposed in the article is not very different from alternating the two above mentioned update, which is indeed quite a natural idea, and can work pretty well I think. The method reads: x^i_{t+1} = x^i_t + [ \\nabla \\log \\pi(x_t) \\delta + (some functional of all the particles) \\, \\delta ] + \\sqrt(2 \\delta) \\xi. PROS: - yes, I think that the method can work quite OK since it may be borrowing the strengths of both SGLD and SVGD. - It seems that the meat of the paper consists in proving some (non-asymptotic) convergence result. Unfortunately, this went above my head and I cannot claim that I have read the details of the proofs. CONS: - it is (very) difficult to fairly evaluate this type of methods in high-dimensional settings. I thus appreciate that the numerical section starts with a toy very simple Gaussian model. I would have been much more interested in fair and extensive simulations in this type of settings where it is relatively easy to compare the proposed method with SGLD and SGVD. In other words, after reading the paper, I must say that I am not at all convinced that the method does bring something over SGLD or SVGD (although it is very possible that it does). For example, comprehensive and fair comparisons with SGLD and SVGD in Gaussian settings (not necessarily one-dimensional) could have been presented. The delicate tuning of the different methods, the speed of convergence wrt algorithmic time, the speed of comparison wrt the number of particles, etc.. could have been investigated numerically: this would have been, I think, much more convincing. MINOR comments: - I did check the proof of Theorem 2, which seems hand-wavy and overly complicated. What is the function G? It seems that the proof of Theorem 2 simply consists in establishing that if each particle x_i follows the dynamics dx = F(x)*dt then the associated densities satisfy \\partial_t \\mu_t = -\\partial_x(F(x) * \\mu_t(x)) , which is obvious. But the situation in the paper is indeed more delicate since the particles are interacting, etc... Reading this proof got me very worried and did not motivate me to read the rest of the paper. SUMMARY: - the method is not terribly original -- this is a simple hybrid SVGD / SGLD -- but may work very well. - unfortunately, the numerical experiments are not convincing. ", "rating": "5: Marginally below acceptance threshold", "reply_text": "We thank the reviewer for his comments , however , the reviewer seems to miss our key point . First , we would like to stress that our paper does not try to show our proposed method is better than either SVGD or SGLD . The motivation of our method is to help better understand the nonasymptotic convergence behavior of SVGD . Since there is no/limited nonasymptotic theory for SVGD , it is hard to understand its convergence behavior . To overcome this difficulty , we combine SGLD with SVGD , and for the first time successfully develop nonasymptotic convergence theory for a SVGD-style algorithm . Because there no nonasymptotic theory for SVGD ( except for some restrict results of the recent work [ 1 ] ) , nothing can be said about SVGD and our algorithm in theory . Similarly , it is hard to compare to SGLD as well because our algorithm is particle based . That said , even though we can perform other experiments on simple toy data , nothing can be expected by comparing our algorithm and SVGD , except the pitfall property of SVGD described in Sec 4.4 , which has been shown in Figure 1 . For the proof of Theorem 2 , G is defined in the equation below eq.20 , which is related to LHS of eq.7 . It is unfair to say that `` Reading this proof got me very worried and did not motivate me to read the rest of the paper '' because the proof techniques of Theorem 2 and other theorems are complete independent ."}, "1": {"review_id": "r1espiA9YQ-1", "review_text": "This paper considers the problem of Bayesian inference using particle optimization sampler. Similarly to SGLD, authors propose Stochastic Particle Optimization Sampler (SPOS), augmenting Stein Variational Gradient Descent (SVGD) with diminishing Gaussian noise, replacing the hard-to-compute term of the Chen et al. (2018) formulation. Various theoretical results are given. This paper was a pleasant read until I decided to check the proof of Theorem 3. I was not able to understand transitions in some of the steps and certain statements in the proof seem wrong. Theorem 3: \"Note that $\\theta^i_t$ and $\\hat \\theta^i_t$ are initialized with the same initial distribution \u00b50 = \u03bd0 and we can also set $\\theta^i_0$ to be independent of $\\hat \\theta^i_0$, we can have $\\gamma(0) = 0$. $\\gamma(0) = E \\|\\theta^i_0 - \\hat \\theta^i_0 \\|^2$.\" - this doesn't seem right to me. Expectation of squared difference of two independent and identically distributed random variables is not 0, assuming expectation is with respect to their joint density. \"Then according to the Gronwall Lemma, we have\" - I don't see how the resulting inequality was obtained. When I tried applying Gronwall Lemma, it seems that authors forgot to multiply by $t$ and $\\lambda_1$. Could you please elaborate how exactly Gronwall Lemma was used in this case. \"... some positive constants c1 and c2 independent of (M, d)$ - in the proof authors introduce additional assumption \"We can tune the bandwidth of the RBF kernel to make \u2207K \u2264 H_\u2207K, which is omitted in the Assumption due to the space limit.\" First, there is a missing norm, since \u2207K is a vector and H_\u2207K is I believe a scalar constant. Second, c1 = H_\u2207K + H_F, which both bound norm of d-dimensional vector and hence depend on d. I also suggest that all assumptions are included in the theorem statements, especially since authors have another assumption requiring large bandwidth. Additionally, feasibility of these both assumptions being satisfied should be explored (it seems to me that they can hold together, but it doesn't mean that part of assumptions can be moved to the supplement). I find using Wasserstein-1 metric misleading in the theorem statement . This is not what authors really bound - from the proof it can be seen that they bound W_1 with W_2 and then with just an expectation of l2 norm. Moreover I don't understand the meaning of this bound. Theorem is concerned with W_1 distance between two atomic measures. What is the expectation over? Note that atom locations are supposed to be fixed for the W_1 to make sense in this context (and the expectation is over the coupling of discrete measures defined by weights of the atoms, not atom locations). \"Note the first bullet indicates U to be a convex function and W to be ... \" I think it should be K, not W. Theorems 3-6 could be lemmas, while there should be a unifying theorem for the bound. Finally, I think notation should be changed - same letter is used for Wasserstein distance and Wiener process. Other comments: Example in Figure 1 is somewhat contrived - clearly gradient based particle sampler will never escape the mode since all modes are disconnected by regions with 0 density. Proposed method on the other hand will eventually jump out due to noise, but it doesn't necessarily mean it produces better posterior estimate. Something more realistic like a mixture of Gaussians, with density bounded away from zero across domain space, will be more informative. It is not sufficient to report RMSE and test log likelihood for BNNs. One of the key motivating points is posterior uncertainty estimation. Hence important metric, when comparing to other posterior inference techniques, is to show high uncertainty for out of distribution samples and low for training/test data. ", "rating": "3: Clear rejection", "reply_text": "1.We will change the \u201c W \u201d to \u201c K \u201d . 2.Yes , we understand your suggestions that Theorems 3-6 could be lemmas and there should be a unifying theorem for the bound . However , it is worth noting that the \u201c unifying theorems for the bounds \u201d are provided in the appendix H as mentioned at the end of Section 4.3 . And we hope not to move them to the context due to the space limit . Our paper is already 10 pages long and can not fit these two long theorems . Besides , we have no problems to change the \u201c Theorem \u201d to \u201c Lemma \u201d . Unfortunately , as our responses to other reviewers , our paper \u2019 s main contribution lies in the theoretical analysis . We think the techniques and ideas of Theorems 3-6 are also very important , which provide some guides for other researchers in the field . Hence , we think Theorems 3-6 worth the name \u201c Theorem \u201d , not just \u201c Lemmas \u201d ( which mean they are only affiliated to the \u201c unifying theorems for the bounds \u201d ) . But if you still disagree with our opinions , we are willing to make the changes since this issue is quite minor and should not be the reason for your rejection . 3.We will change the notation of Wasserstein metric from $ \\mathcal { W } $ to $ W $ . 4.We don \u2019 t think \u201c Example in Figure 1 is contrived \u201d . The distribution form used in Figure 1 is given in Appendix A . It is obvious that the distribution are nonzero everywhere ( the probabilities are just very small somewhere ) , thus it does not have the problem of \u201c disconnected modes \u201d . We use this example to show failure case of SVGD , which induces no problem with our SPOS . Using RMSE and log-likelihood is the gold standard in Bayesian learning of DNNs . Instead of directly showing uncertainty for in/out distribution samples as suggested by you , we test It in the more direct scenario of reinforcement learning . The reason is that it is well-accepted that RL performance directly measures how well the uncertainty is learned , as there is an exploration stage in the learning , requiring uncertainty to explore the environment . As a result , we believe our measure in the experiments are standard . At last , we really hope you could reconsider your scoring as it seems to be quite unfair based on your comments . In our response , we have fully address the minor problem which you pointed out . Besides , we have resolved your concerns about the W_1 metric excessively . And we have shared our opinions with you on the name of our Theorem and decides to change the notations as you suggested . We think it is really unfair to reject our paper merely for some minor problems , which has been fully addressed . Although we have explained much to your concerns about W_1 metric , most of them are not needed to add to our paper , which means that we do no need to make much revision . If you like , we are willing to add our explanations for your concerns into the Appendix . Thank you so much for your time and re-consideration !"}, "2": {"review_id": "r1espiA9YQ-2", "review_text": "This paper proposes a particle-based inference algorithm, the optimal update for each particle is the summation of the standard SGLD direction and SVGD velocity. The work further analyzes non-asymptotic properties of SPOS. The results appear theoretically interesting and of potential practical value in designing inference algorithms. I did not go through the proofs in the supplementary. [Experimental results are not convincing] [BNN] I noticed the test RMSE and test LL of SVGD are directly copied from the original SVGD paper. However, the performance critically depends on: 1. Running time, or training epochs 2. Data partitions To be a fair comparison, the authors should keep at least the training epochs and random partitions the same. Especially for the dataset Year, for which only one random partition is conducted. It\u2019s highly likely that the performance gain is due to favored data partition rather than the superiority of the algorithm. [RL] Average rewards are significantly lower than the scores reported in the original SVPG paper? 1. From figure 3, SPOS only outperforms SVPG on envs Cartpole Swing Up and Double Pendulum. The best reward for env Cartpole Swing Up reported in this paper is around 200. However, the score is ~400 in the original SVPG paper. For the env Double Pendulum, there\u2019s also very large performance gap. I am aware the code for SVPG is now publicly available, the authors may consider conducting the experiments with the same settings (e.g. same seed?). Otherwise, it\u2019s hard to tell whether the performance gain is significant while the baseline is much worse than it should be. 2. Only 3 envs are reported, the authors may also consider reporting all the envs are used in the SVPG paper [Figure 1] The authors may consider reporting the exact settings of this case, otherwise, it\u2019s hard to believe that SVGD would collapse on a simple 1D case. If the authors can fully address the concerns above, I will consider changing the scores. Other comments: - Related papers: Stein Variational Message Passing for Continuous Graphical Models, Wang et al., ICML18 (https://arxiv.org/abs/1711.07168) Stein Variational Gradient Descent as Moment Matching, Liu et al., NIPS18 (https://arxiv.org/abs/1810.11693) - Page 30 crashes my browser all the time ", "rating": "4: Ok but not good enough - rejection", "reply_text": "To eliminate the confusion of the reviewer , we re-run the experiments for SVGD and SPOS , the same split of data ( train , val and test ) are used for SVGD and SPOS . The test results are reported on the best model on the validation set . The results are as follows : Boston_housing : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 2.829 \\pm 0.126 & 2.961 \\pm 0.109 & -2.532 \\pm 0.082 & -2.591 \\pm 0.029 Concrete : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 5.071 \\pm 0.1495 & 6.157 \\pm 0.082 & -3.062 \\pm 0.037 & -3.247 \\pm 0.01 Energy : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 0.752 \\pm 0.0285 & 1.291 \\pm 0.029 & -1.158 \\pm 0.073 & -1.534 \\pm 0.026 Kin8nm : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 0.079 \\pm 0.001 & 0.075 \\pm 0.001 & 1.092 \\pm 0.013 & 1.138 \\pm 0.004 Naval : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 0.004 \\pm 0.0 & 0.004 \\pm 0.000 & 4.145 \\pm 0.02 & 4.032 \\pm 0.008 CCPP : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 3.939 \\pm 0.0495 & 4.127 \\pm 0.027 & -2.794 \\pm 0.025 & -2.843 \\pm 0.006 Winequality : SPOS ( MSE ) & SVGD ( MSE ) & SPOS ( LL ) & SVGD ( LL ) 0.598 \\pm 0.014 & 0.604 \\pm 0.007 & -0.911 \\pm 0.041 & -0.926 For RL results , the four benchmarks are the simplest benchmarks for reinforcement learning ; thus it is obviously not necessary to use a 400-400 MLP as a policy . Even in much more complex benchmarks , e.g. , humanoid and walker , previous methods such as soft-Q learning and SAC used 128-128 MLP or 256-256 MLP as the policy network , TRPO used one-layer MLP . We followed the settings of VIME , and think it is more reasonable to use a simpler policy network . 400-400 MLP as a policy is too complex to be a good choice . We used the released code of SVPG and the same settings for both methods ( also same seeds ) , thus the comparisons are fair for both methods . For the other environments , Mountain car is a very simple environment compared with CartpoleSwingUp and Double Pendulum , and we encountered errors from the framework when running the algorithms . We will try to fix the problem and incorporate results into our next revision . As in our response to Reviewer 1 , we did not claim a better algorithm than SVGD in theory because there is no nonasymptotic theory for SVGD ( though we did observed better empirical performance ) , but a better way to understand the nonasymptotic convergence behavior of particle optimization algorithms , e.g. , SPOS , providing a non-asymptotic bound for an SVGD-style algorithm the first time ."}}